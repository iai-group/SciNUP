{"id": "0707.1133", "contents": "Title: Stochastic Differential Games with Reflection and Related Obstacle\n  Problems for Isaacs Equations Abstract: In this paper we first investigate zero-sum two-player stochastic\ndifferential games with reflection with the help of theory of Reflected\nBackward Stochastic Differential Equations (RBSDEs). We will establish the\ndynamic programming principle for the upper and the lower value functions of\nthis kind of stochastic differential games with reflection in a\nstraight-forward way. Then the upper and the lower value functions are proved\nto be the unique viscosity solutions of the associated upper and the lower\nHamilton-Jacobi-Bellman-Isaacs equations with obstacles, respectively. The\nmethod differs heavily from those used for control problems with reflection, it\nhas its own techniques and its own interest. On the other hand, we also prove a\nnew estimate for RBSDEs being sharper than that in El Karoui, Kapoudjian,\nPardoux, Peng and Quenez [7], which turns out to be very useful because it\nallows to estimate the $L^p$-distance of the solutions of two different RBSDEs\nby the $p$-th power of the distance of the initial values of the driving\nforward equations. We also show that the unique viscosity solution of the\napproximating Isaacs equation which is constructed by the penalization method\nconverges to the viscosity solution of the Isaacs equation with obstacle. \n\n"}
{"id": "0802.3250", "contents": "Title: Valuation of Mortality Risk via the Instantaneous Sharpe Ratio:\n  Applications to Life Annuities Abstract: We develop a theory for valuing non-diversifiable mortality risk in an\nincomplete market. We do this by assuming that the company issuing a\nmortality-contingent claim requires compensation for this risk in the form of a\npre-specified instantaneous Sharpe ratio. We apply our method to value life\nannuities. One result of our paper is that the value of the life annuity is\n{\\it identical} to the upper good deal bound of Cochrane and Sa\\'{a}-Requejo\n(2000) and of Bj\\\"{o}rk and Slinko (2006) applied to our setting. A second\nresult of our paper is that the value per contract solves a {\\it linear}\npartial differential equation as the number of contracts approaches infinity.\nOne can represent the limiting value as an expectation with respect to an\nequivalent martingale measure (as in Blanchet-Scalliet, El Karoui, and\nMartellini (2005)), and from this representation, one can interpret the\ninstantaneous Sharpe ratio as an annuity market's price of mortality risk. \n\n"}
{"id": "0805.0910", "contents": "Title: Lyapunov control of a quantum particle in a decaying potential Abstract: A Lyapunov-based approach for the trajectory generation of an $N$-dimensional\nSchr{\\\"o}dinger equation in whole $\\RR^N$ is proposed. For the case of a\nquantum particle in an $N$-dimensional decaying potential the convergence is\nprecisely analyzed. The free system admitting a mixed spectrum, the dispersion\nthrough the absolutely continuous part is the main obstacle to ensure such a\nstabilization result. Whenever, the system is completely initialized in the\ndiscrete part of the spectrum, a Lyapunov strategy encoding both the distance\nwith respect to the target state and the penalization of the passage through\nthe continuous part of the spectrum, ensures the approximate stabilization. \n\n"}
{"id": "0806.1785", "contents": "Title: Energy-efficient motion camouflage in three dimensions Abstract: Recent observations suggest that one insect may camouflage its own motion\nwhilst tracking another. Here we present a strategy by which one agent can\nefficiently track and intercept another agent, whilst camouflaging its own\nmotion and minimizing its energy consumption \n\n"}
{"id": "0809.0545", "contents": "Title: Frequency Locking of an Optical Cavity using LQG Integral Control Abstract: This paper considers the application of integral Linear Quadratic Gaussian\n(LQG) optimal control theory to a problem of cavity locking in quantum optics.\nThe cavity locking problem involves controlling the error between the laser\nfrequency and the resonant frequency of the cavity. A model for the cavity\nsystem, which comprises a piezo-electric actuator and an optical cavity is\nexperimentally determined using a subspace identification method. An LQG\ncontroller which includes integral action is synthesized to stabilize the\nfrequency of the cavity to the laser frequency and to reject low frequency\nnoise. The controller is successfully implemented in the laboratory using a\ndSpace DSP board. \n\n"}
{"id": "0809.3170", "contents": "Title: A New Framework of Multistage Hypothesis Tests Abstract: In this paper, we have established a general framework of multistage\nhypothesis tests which applies to arbitrarily many mutually exclusive and\nexhaustive composite hypotheses. Within the new framework, we have constructed\nspecific multistage tests which rigorously control the risk of committing\ndecision errors and are more efficient than previous tests in terms of average\nsample number and the number of sampling operations. Without truncation, the\nsample numbers of our testing plans are absolutely bounded. \n\n"}
{"id": "0903.1287", "contents": "Title: A convex polynomial that is not sos-convex Abstract: A multivariate polynomial $p(x)=p(x_1,...,x_n)$ is sos-convex if its Hessian\n$H(x)$ can be factored as $H(x)= M^T(x) M(x)$ with a possibly nonsquare\npolynomial matrix $M(x)$. It is easy to see that sos-convexity is a sufficient\ncondition for convexity of $p(x)$. Moreover, the problem of deciding\nsos-convexity of a polynomial can be cast as the feasibility of a semidefinite\nprogram, which can be solved efficiently. Motivated by this computational\ntractability, it has been recently speculated whether sos-convexity is also a\nnecessary condition for convexity of polynomials. In this paper, we give a\nnegative answer to this question by presenting an explicit example of a\ntrivariate homogeneous polynomial of degree eight that is convex but not\nsos-convex. Interestingly, our example is found with software using sum of\nsquares programming techniques and the duality theory of semidefinite\noptimization. As a byproduct of our numerical procedure, we obtain a simple\nmethod for searching over a restricted family of nonnegative polynomials that\nare not sums of squares. \n\n"}
{"id": "0903.4368", "contents": "Title: Convergent relaxations of polynomial optimization problems with\n  non-commuting variables Abstract: We consider optimization problems with polynomial inequality constraints in\nnon-commuting variables. These non-commuting variables are viewed as bounded\noperators on a Hilbert space whose dimension is not fixed and the associated\npolynomial inequalities as semidefinite positivity constraints. Such problems\narise naturally in quantum theory and quantum information science. To solve\nthem, we introduce a hierarchy of semidefinite programming relaxations which\ngenerates a monotone sequence of lower bounds that converges to the optimal\nsolution. We also introduce a criterion to detect whether the global optimum is\nreached at a given relaxation step and show how to extract a global optimizer\nfrom the solution of the corresponding semidefinite programming problem. \n\n"}
{"id": "0908.0364", "contents": "Title: Polynomial Matrix Inequality and Semidefinite Representation Abstract: Consider a convex set S defined by a matrix inequality of polynomials or\nrational functions over a domain. The set S is called semidefinite programming\n(SDP) representable or just semidefinite representable if it equals the\nprojection of a higher dimensional set which is defined by a linear matrix\ninequality (LMI). This paper studies sufficient conditions guaranteeing\nsemidefinite representability of S. We prove that S is semidefinite\nrepresentable in the following cases: (i) the domain is the whole space and the\nmatrix polynomial is matrix sos-concave; (ii) the domain is compact convex and\nthe matrix polynomial is strictly matrix concave; (iii) the rational matrix\nfunction is q-module matrix concave on the domain. Explicit constructions of\nSDP representations are given. Some examples are illustrated. \n\n"}
{"id": "0910.3034", "contents": "Title: Linear State Feedback Stabilization on Time Scales Abstract: For a general class of dynamical systems (of which the canonical continuous\nand uniform discrete versions are but special cases), we prove that there is a\nstate feedback gain such that the resulting closed-loop system is uniformly\nexponentially stable with a prescribed rate. The methods here generalize and\nextend Gramian-based linear state feedback control to much more general time\ndomains, e.g. nonuniform discrete or a combination of continuous and discrete\ntime. In conclusion, we discuss an experimental implementation of this theory. \n\n"}
{"id": "0910.5673", "contents": "Title: Synchronization and Transient Stability in Power Networks and\n  Non-Uniform Kuramoto Oscillators Abstract: Motivated by recent interest for multi-agent systems and smart power grid\narchitectures, we discuss the synchronization problem for the network-reduced\nmodel of a power system with non-trivial transfer conductances. Our key insight\nis to exploit the relationship between the power network model and a\nfirst-order model of coupled oscillators. Assuming overdamped generators\n(possibly due to local excitation controllers), a singular perturbation\nanalysis shows the equivalence between the classic swing equations and a\nnon-uniform Kuramoto model. Here, non-uniform Kuramoto oscillators are\ncharacterized by multiple time constants, non-homogeneous coupling, and\nnon-uniform phase shifts. Extending methods from transient stability,\nsynchronization theory, and consensus protocols, we establish sufficient\nconditions for synchronization of non-uniform Kuramoto oscillators. These\nconditions reduce to and improve upon previously-available tests for the\nstandard Kuramoto model. Combining our singular perturbation and Kuramoto\nanalyses, we derive concise and purely algebraic conditions that relate\nsynchronization and transient stability of a power network to the underlying\nsystem parameters and initial conditions. \n\n"}
{"id": "0910.5761", "contents": "Title: Which graphical models are difficult to learn? Abstract: We consider the problem of learning the structure of Ising models (pairwise\nbinary Markov random fields) from i.i.d. samples. While several methods have\nbeen proposed to accomplish this task, their relative merits and limitations\nremain somewhat obscure. By analyzing a number of concrete examples, we show\nthat low-complexity algorithms systematically fail when the Markov random field\ndevelops long-range correlations. More precisely, this phenomenon appears to be\nrelated to the Ising model phase transition (although it does not coincide with\nit). \n\n"}
{"id": "0912.3033", "contents": "Title: When is multidimensional screening a convex program? Abstract: A principal wishes to transact business with a multidimensional distribution\nof agents whose preferences are known only in the aggregate. Assuming a twist\n(= generalized Spence-Mirrlees single-crossing) hypothesis and that agents can\nchoose only pure strategies, we identify a structural condition on the\npreference b(x,y) of agent type x for product type y -- and on the principal's\ncosts c(y) -- which is necessary and sufficient for reducing the profit\nmaximization problem faced by the principal to a convex program. This is a key\nstep toward making the principal's problem theoretically and computationally\ntractable; in particular, it allows us to derive uniqueness and stability of\nthe principal's optimum strategy -- and similarly of the strategy maximizing\nthe expected welfare of the agents when the principal's profitability is\nconstrained. We call this condition non-negative cross-curvature: it is also\n(i) necessary and sufficient to guarantee convexity of the set of b-convex\nfunctions, (ii) invariant under reparametrization of agent and/or product types\nby diffeomorphisms, and (iii) a strengthening of Ma, Trudinger and Wang's\nnecessary and sufficient condition (A3w) for continuity of the correspondence\nbetween an exogenously prescribed distribution of agents and of products. We\nderive the persistence of economic effects such as the desirability for a\nmonopoly to establish prices so high they effectively exclude a positive\nfraction of its potential customers, in nearly the full range of non-negatively\ncross-curved models. \n\n"}
{"id": "1003.5819", "contents": "Title: A unified controllability/observability theory for some stochastic and\n  deterministic partial differential equations Abstract: The purpose of this paper is to present a universal approach to the study of\ncontrollability/observability problems for infinite dimensional systems\ngoverned by some stochastic/deterministic partial differential equations. The\ncrucial analytic tool is a class of fundamental weighted identities for\nstochastic/deterministic partial differential operators, via which one can\nderive the desired global Carleman estimates. This method can also give a\nunified treatment of the stabilization, global unique continuation, and inverse\nproblems for some stochastic/deterministic partial differential equations. \n\n"}
{"id": "1004.3361", "contents": "Title: From open quantum systems to open quantum maps Abstract: For a class of quantized open chaotic systems satisfying a natural dynamical\nassumption, we show that the study of the resolvent, and hence of scattering\nand resonances, can be reduced to the study of a family of open quantum maps,\nthat is of finite dimensional operators obtained by quantizing the Poincar\\'e\nmap associated with the flow near the set of trapped trajectories. \n\n"}
{"id": "1005.2638", "contents": "Title: Hierarchical Clustering for Finding Symmetries and Other Patterns in\n  Massive, High Dimensional Datasets Abstract: Data analysis and data mining are concerned with unsupervised pattern finding\nand structure determination in data sets. \"Structure\" can be understood as\nsymmetry and a range of symmetries are expressed by hierarchy. Such symmetries\ndirectly point to invariants, that pinpoint intrinsic properties of the data\nand of the background empirical domain of interest. We review many aspects of\nhierarchy here, including ultrametric topology, generalized ultrametric,\nlinkages with lattices and other discrete algebraic structures and with p-adic\nnumber representations. By focusing on symmetries in data we have a powerful\nmeans of structuring and analyzing massive, high dimensional data stores. We\nillustrate the powerfulness of hierarchical clustering in case studies in\nchemistry and finance, and we provide pointers to other published case studies. \n\n"}
{"id": "1005.5273", "contents": "Title: Holonomic Gradient Descent and its Application to Fisher-Bingham\n  Integral Abstract: We give a new algorithm to find local maximum and minimum of a holonomic\nfunction and apply it for the Fisher-Bingham integral on the sphere $S^n$,\nwhich is used in the directional statistics. The method utilizes the theory and\nalgorithms of holonomic systems. \n\n"}
{"id": "1006.4895", "contents": "Title: On the complexity of nonlinear mixed-integer optimization Abstract: This is a survey on the computational complexity of nonlinear mixed-integer\noptimization. It highlights a selection of important topics, ranging from\nincomputability results that arise from number theory and logic, to recently\nobtained fully polynomial time approximation schemes in fixed dimension, and to\nstrongly polynomial-time algorithms for special cases. \n\n"}
{"id": "1007.0743", "contents": "Title: Fractional variational calculus in terms of a combined Caputo derivative Abstract: We generalize the fractional Caputo derivative to the fractional derivative\n${^CD^{\\alpha,\\beta}_{\\gamma}}$, which is a convex combination of the left\nCaputo fractional derivative of order $\\alpha$ and the right Caputo fractional\nderivative of order $\\beta$. The fractional variational problems under our\nconsideration are formulated in terms of ${^CD^{\\alpha,\\beta}_{\\gamma}}$. The\nEuler-Lagrange equations for the basic and isoperimetric problems, as well as\ntransversality conditions, are proved. \n\n"}
{"id": "1007.5087", "contents": "Title: Calculus of Variations on Time Scales and Discrete Fractional Calculus Abstract: We study problems of the calculus of variations and optimal control within\nthe framework of time scales. Specifically, we obtain Euler-Lagrange type\nequations for both Lagrangians depending on higher order delta derivatives and\nisoperimetric problems. We also develop some direct methods to solve certain\nclasses of variational problems via dynamic inequalities. In the last chapter\nwe introduce fractional difference operators and propose a new discrete-time\nfractional calculus of variations. Corresponding Euler-Lagrange and Legendre\nnecessary optimality conditions are derived and some illustrative examples\nprovided. \n\n"}
{"id": "1009.1185", "contents": "Title: Toward the Universal Rigidity of General Frameworks Abstract: Let (G,P) be a bar framework of n vertices in general position in R^d, d <=\nn-1, where G is a (d+1)-lateration graph. In this paper, we present a\nconstructive proof that (G,P) admits a positive semi-definite stress matrix\nwith rank n-d-1. We also prove a similar result for a sensor network where the\ngraph consists of m(>= d+1) anchors. \n\n"}
{"id": "1010.3125", "contents": "Title: A Quasi-separation Principle and Newton-like Scheme for Coherent Quantum\n  LQG Control Abstract: This paper is concerned with constructing an optimal controller in the\ncoherent quantum Linear Quadratic Gaussian problem. A coherent quantum\ncontroller is itself a quantum system and is required to be physically\nrealizable. The use of coherent control avoids the need for classical\nmeasurements, which inherently entail the loss of quantum information. Physical\nrealizability corresponds to the equivalence of the controller to an open\nquantum harmonic oscillator and relates its state-space matrices to the\nHamiltonian, coupling and scattering operators of the oscillator. The\nHamiltonian parameterization of the controller is combined with Frechet\ndifferentiation of the LQG cost with respect to the state-space matrices to\nobtain equations for the optimal controller. A quasi-separation principle for\nthe gain matrices of the quantum controller is established, and a Newton-like\niterative scheme for numerical solution of the equations is outlined. \n\n"}
{"id": "1010.5742", "contents": "Title: Stochastic Verification Theorem of Forward-Backward Controlled Systems\n  for Viscosity Solutions Abstract: In this paper, we investigate the controlled system described by\nforward-backward stochastic differential equations with the control contained\nin drift, diffusion and generator of BSDE. A new verification theorem is\nderived within the framework of viscosity solutions without involving any\nderivatives of the value functions. It is worth to pointing out that this\ntheorem has wider applicability than the restrictive classical verification\ntheorems. As a relevant problem, the optimal stochastic feedback controls for\nforward-backward system are discussed as well. \n\n"}
{"id": "1012.1908", "contents": "Title: NP-hardness of Deciding Convexity of Quartic Polynomials and Related\n  Problems Abstract: We show that unless P=NP, there exists no polynomial time (or even\npseudo-polynomial time) algorithm that can decide whether a multivariate\npolynomial of degree four (or higher even degree) is globally convex. This\nsolves a problem that has been open since 1992 when N. Z. Shor asked for the\ncomplexity of deciding convexity for quartic polynomials. We also prove that\ndeciding strict convexity, strong convexity, quasiconvexity, and\npseudoconvexity of polynomials of even degree four or higher is strongly\nNP-hard. By contrast, we show that quasiconvexity and pseudoconvexity of odd\ndegree polynomials can be decided in polynomial time. \n\n"}
{"id": "1101.4957", "contents": "Title: Aircraft Proximity Maps Based on Data-Driven Flow Modeling Abstract: With the forecast increase in air traffic demand over the next decades, it is\nimperative to develop tools to provide traffic flow managers with the\ninformation required to support decision making. In particular,\ndecision-support tools for traffic flow management should aid in limiting\ncontroller workload and complexity, while supporting increases in air traffic\nthroughput. While many decision-support tools exist for short-term traffic\nplanning, few have addressed the strategic needs for medium- and long-term\nplanning for time horizons greater than 30 minutes. This paper seeks to address\nthis gap through the introduction of 3D aircraft proximity maps that evaluate\nthe future probability of presence of at least one or two aircraft at any given\npoint of the airspace. Three types of proximity maps are presented: presence\nmaps that indicate the local density of traffic; conflict maps that determine\nlocations and probabilities of potential conflicts; and outliers maps that\nevaluate the probability of conflict due to aircraft not belonging to dominant\ntraffic patterns. These maps provide traffic flow managers with information\nrelating to the complexity and difficulty of managing an airspace. The intended\npurpose of the maps is to anticipate how aircraft flows will interact, and how\noutliers impact the dominant traffic flow for a given time period. This\nformulation is able to predict which \"critical\" regions may be subject to\nconflicts between aircraft, thereby requiring careful monitoring. These\nprobabilities are computed using a generative aircraft flow model. Time-varying\nflow characteristics, such as geometrical configuration, speed, and probability\ndensity function of aircraft spatial distribution within the flow, are\ndetermined from archived Enhanced Traffic Management System data, using a\ntailored clustering algorithm. Aircraft not belonging to flows are identified\nas outliers. \n\n"}
{"id": "1102.0059", "contents": "Title: Statistical methods for tissue array images - algorithmic scoring and\n  co-training Abstract: Recent advances in tissue microarray technology have allowed\nimmunohistochemistry to become a powerful medium-to-high throughput analysis\ntool, particularly for the validation of diagnostic and prognostic biomarkers.\nHowever, as study size grows, the manual evaluation of these assays becomes a\nprohibitive limitation; it vastly reduces throughput and greatly increases\nvariability and expense. We propose an algorithm - Tissue Array Co-Occurrence\nMatrix Analysis (TACOMA) - for quantifying cellular phenotypes based on\ntextural regularity summarized by local inter-pixel relationships. The\nalgorithm can be easily trained for any staining pattern, is absent of\nsensitive tuning parameters and has the ability to report salient pixels in an\nimage that contribute to its score. Pathologists' input via informative\ntraining patches is an important aspect of the algorithm that allows the\ntraining for any specific marker or cell type. With co-training, the error rate\nof TACOMA can be reduced substantially for a very small training sample (e.g.,\nwith size 30). We give theoretical insights into the success of co-training via\nthinning of the feature set in a high-dimensional setting when there is\n\"sufficient\" redundancy among the features. TACOMA is flexible, transparent and\nprovides a scoring process that can be evaluated with clarity and confidence.\nIn a study based on an estrogen receptor (ER) marker, we show that TACOMA is\ncomparable to, or outperforms, pathologists' performance in terms of accuracy\nand repeatability. \n\n"}
{"id": "1102.1107", "contents": "Title: Robust Distributed Routing in Dynamical Flow Networks - Part I: Locally\n  Responsive Policies and Weak Resilience Abstract: Robustness of distributed routing policies is studied for dynamical flow\nnetworks, with respect to adversarial disturbances that reduce the link flow\ncapacities. A dynamical flow network is modeled as a system of ordinary\ndifferential equations derived from mass conservation laws on a directed\nacyclic graph with a single origin-destination pair and a constant inflow at\nthe origin. Routing policies regulate the way the inflow at a non-destination\nnode gets split among its outgoing links as a function of the current particle\ndensity, while the outflow of a link is modeled to depend on the current\nparticle density on that link through a flow function. The dynamical flow\nnetwork is called partially transferring if the total inflow at the destination\nnode is asymptotically bounded away from zero, and its weak resilience is\nmeasured as the minimum sum of the link-wise magnitude of all disturbances that\nmake it not partially transferring. The weak resilience of a dynamical flow\nnetwork with arbitrary routing policy is shown to be upper-bounded by the\nnetwork's min-cut capacity, independently of the initial flow conditions.\nMoreover, a class of distributed routing policies that rely exclusively on\nlocal information on the particle densities, and are locally responsive to\nthat, is shown to yield such maximal weak resilience. These results imply that\nlocality constraints on the information available to the routing policies do\nnot cause loss of weak resilience. Some fundamental properties of dynamical\nflow networks driven by locally responsive distributed policies are analyzed in\ndetail, including global convergence to a unique limit flow. \n\n"}
{"id": "1102.2950", "contents": "Title: Kron Reduction of Graphs with Applications to Electrical Networks Abstract: Consider a weighted and undirected graph, possibly with self-loops, and its\ncorresponding Laplacian matrix, possibly augmented with additional diagonal\nelements corresponding to the self-loops. The Kron reduction of this graph is\nagain a graph whose Laplacian matrix is obtained by the Schur complement of the\noriginal Laplacian matrix with respect to a subset of nodes. The Kron reduction\nprocess is ubiquitous in classic circuit theory and in related disciplines such\nas electrical impedance tomography, smart grid monitoring, transient stability\nassessment in power networks, or analysis and simulation of induction motors\nand power electronics. More general applications of Kron reduction occur in\nsparse matrix algorithms, multi-grid solvers, finite--element analysis, and\nMarkov chains. The Schur complement of a Laplacian matrix and related concepts\nhave also been studied under different names and as purely theoretic problems\nin the literature on linear algebra. In this paper we propose a general\ngraph-theoretic framework for Kron reduction that leads to novel and deep\ninsights both on the mathematical and the physical side. We show the\napplicability of our framework to various practical problem setups arising in\nengineering applications and computation. Furthermore, we provide a\ncomprehensive and detailed graph-theoretic analysis of the Kron reduction\nprocess encompassing topological, algebraic, spectral, resistive, and\nsensitivity analyses. Throughout our theoretic elaborations we especially\nemphasize the practical applicability of our results. \n\n"}
{"id": "1103.1178", "contents": "Title: A Simplified Approach to Recovery Conditions for Low Rank Matrices Abstract: Recovering sparse vectors and low-rank matrices from noisy linear\nmeasurements has been the focus of much recent research. Various reconstruction\nalgorithms have been studied, including $\\ell_1$ and nuclear norm minimization\nas well as $\\ell_p$ minimization with $p<1$. These algorithms are known to\nsucceed if certain conditions on the measurement map are satisfied. Proofs of\nrobust recovery for matrices have so far been much more involved than in the\nvector case.\n  In this paper, we show how several robust classes of recovery conditions can\nbe extended from vectors to matrices in a simple and transparent way, leading\nto the best known restricted isometry and nullspace conditions for matrix\nrecovery. Our results rely on the ability to \"vectorize\" matrices through the\nuse of a key singular value inequality. \n\n"}
{"id": "1103.1365", "contents": "Title: Design of Strict Control-Lyapunov Functions for Quantum Systems with QND\n  Measurements Abstract: We consider discrete-time quantum systems subject to Quantum Non-Demolition\n(QND) measurements and controlled by an adjustable unitary evolution between\ntwo successive QND measures. In open-loop, such QND measurements provide a\nnon-deterministic preparation tool exploiting the back-action of the\nmeasurement on the quantum state. We propose here a systematic method based on\nelementary graph theory and inversion of Laplacian matrices to construct strict\ncontrol-Lyapunov functions. This yields an appropriate feedback law that\nstabilizes globally the system towards a chosen target state among the\nopen-loop stable ones, and that makes in closed-loop this preparation\ndeterministic. We illustrate such feedback laws through simulations\ncorresponding to an experimental setup with QND photon counting. \n\n"}
{"id": "1103.1665", "contents": "Title: The Role of Singular Control in Frictionless Atom Cooling in a Harmonic\n  Trapping Potential Abstract: In this article we study the frictionless cooling of atoms trapped in a\nharmonic potential, while minimizing the transient energy of the system. We\nshow that in the case of unbounded control, this goal is achieved by a singular\ncontrol, which is also the time-minimal solution for a \"dual\" problem, where\nthe energy is held fixed. In addition, we examine briefly how the solution is\nmodified when there are bounds on the control. The results presented here have\na broad range of applications, from the cooling of a Bose-Einstein condensate\nconfined in a harmonic trap to adiabatic quantum computing and finite time\nthermodynamic processes. \n\n"}
{"id": "1103.4893", "contents": "Title: Robust Distributed Routing in Dynamical Flow Networks - Part II: Strong\n  Resilience, Equilibrium Selection and Cascaded Failures Abstract: Strong resilience properties of dynamical flow networks are analyzed for\ndistributed routing policies. The latter are characterized by the property that\nthe way the inflow at a non-destination node gets split among its outgoing\nlinks is allowed to depend only on local information about the current particle\ndensities on the outgoing links. The strong resilience of the network is\ndefined as the infimum sum of link-wise flow capacity reductions under which\nthe network cannot maintain the asymptotic total inflow to the destination node\nto be equal to the inflow at the origin. A class of distributed routing\npolicies that are locally responsive to local information is shown to yield the\nmaximum possible strong resilience under such local information constraints for\nan acyclic dynamical flow network with a single origin-destination pair. The\nmaximal strong resilience achievable is shown to be equal to the minimum node\nresidual capacity of the network. The latter depends on the limit flow of the\nunperturbed network and is defined as the minimum, among all the\nnon-destination nodes, of the sum, over all the links outgoing from the node,\nof the differences between the maximum flow capacity and the limit flow of the\nunperturbed network. We propose a simple convex optimization problem to solve\nfor equilibrium limit flows of the unperturbed network that minimize average\ndelay subject to strong resilience guarantees, and discuss the use of tolls to\ninduce such an equilibrium limit flow in transportation networks. Finally, we\npresent illustrative simulations to discuss the connection between cascaded\nfailures and the resilience properties of the network. \n\n"}
{"id": "1103.5431", "contents": "Title: Identification of Nonlinear Systems with Stable Limit Cycles via Convex\n  Optimization Abstract: We propose a convex optimization procedure for black-box identification of\nnonlinear state-space models for systems that exhibit stable limit cycles\n(unforced periodic solutions). It extends the \"robust identification error\"\nframework in which a convex upper bound on simulation error is optimized to fit\nrational polynomial models with a strong stability guarantee. In this work, we\nrelax the stability constraint using the concepts of transverse dynamics and\norbital stability, thus allowing systems with autonomous oscillations to be\nidentified. The resulting optimization problem is convex, and can be formulated\nas a semidefinite program. A simulation-error bound is proved without assuming\nthat the true system is in the model class, or that the number of measurements\ngoes to infinity. Conditions which guarantee existence of a unique limit cycle\nof the model are proved and related to the model class that we search over. The\nmethod is illustrated by identifying a high-fidelity model from experimental\nrecordings of a live rat hippocampal neuron in culture. \n\n"}
{"id": "1104.0654", "contents": "Title: Block-Sparse Recovery via Convex Optimization Abstract: Given a dictionary that consists of multiple blocks and a signal that lives\nin the range space of only a few blocks, we study the problem of finding a\nblock-sparse representation of the signal, i.e., a representation that uses the\nminimum number of blocks. Motivated by signal/image processing and computer\nvision applications, such as face recognition, we consider the block-sparse\nrecovery problem in the case where the number of atoms in each block is\narbitrary, possibly much larger than the dimension of the underlying subspace.\nTo find a block-sparse representation of a signal, we propose two classes of\nnon-convex optimization programs, which aim to minimize the number of nonzero\ncoefficient blocks and the number of nonzero reconstructed vectors from the\nblocks, respectively. Since both classes of problems are NP-hard, we propose\nconvex relaxations and derive conditions under which each class of the convex\nprograms is equivalent to the original non-convex formulation. Our conditions\ndepend on the notions of mutual and cumulative subspace coherence of a\ndictionary, which are natural generalizations of existing notions of mutual and\ncumulative coherence. We evaluate the performance of the proposed convex\nprograms through simulations as well as real experiments on face recognition.\nWe show that treating the face recognition problem as a block-sparse recovery\nproblem improves the state-of-the-art results by 10% with only 25% of the\ntraining data. \n\n"}
{"id": "1104.3221", "contents": "Title: On the geometry of higher-order variational problems on Lie groups Abstract: In this paper, we describe a geometric setting for higher-order lagrangian\nproblems on Lie groups. Using left-trivialization of the higher-order tangent\nbundle of a Lie group and an adaptation of the classical Skinner-Rusk\nformalism, we deduce an intrinsic framework for this type of dynamical systems.\nInteresting applications as, for instance, a geometric derivation of the\nhigher-order Euler-Poincar\\'e equations, optimal control of underactuated\ncontrol systems whose configuration space is a Lie group are shown, among\nothers, along the paper. \n\n"}
{"id": "1105.0247", "contents": "Title: Liquidation in Limit Order Books with Controlled Intensity Abstract: We consider a framework for solving optimal liquidation problems in limit\norder books. In particular, order arrivals are modeled as a point process whose\nintensity depends on the liquidation price. We set up a stochastic control\nproblem in which the goal is to maximize the expected revenue from liquidating\nthe entire position held. We solve this optimal liquidation problem for\npower-law and exponential-decay order book models and discuss several\nextensions. We also consider the continuous selling (or fluid) limit when the\ntrading units are ever smaller and the intensity is ever larger. This limit\nprovides an analytical approximation to the value function and the optimal\nsolution. Using techniques from viscosity solutions we show that the discrete\nstate problem and its optimal solution converge to the corresponding quantities\nin the continuous selling limit uniformly on compacts. \n\n"}
{"id": "1105.2013", "contents": "Title: Weyl theory and explicit solutions of direct and inverse problems for a\n  Dirac system with rectangular matrix potential Abstract: A non-classical Weyl theory is developed for Dirac systems with rectangular\nmatrix potentials. The notion of the Weyl function is introduced and the\ncorresponding direct problem is treated. Furthermore, explicit solutions of the\ndirect and inverse problems are obtained for the case of rational Weyl matrix\nfunctions. \n\n"}
{"id": "1105.2924", "contents": "Title: On the derivative cones of polyhedral cones Abstract: Hyperbolic polynomials elegantly encode a rich class of convex cones that\nincludes polyhedral and spectrahedral cones. Hyperbolic polynomials are closed\nunder taking polars and the corresponding cones, the derivative cones, yield\nrelaxations for the associated optimization problem and exhibit interesting\nfacial properties. While it is unknown if every hyperbolicity cone is a section\nof the positive semidefinite cone, it is natural to ask whether spectrahedral\ncones are closed under taking polars. In this note we give an affirmative\nanswer for polyhedral cones by exhibiting an explicit spectrahedral\nrepresentation for the first derivative cone. We also proof that higher polars\ndo not have an determinantal representation which shows that the problem for\ngeneral spectrahedral cones is considerably more difficult. \n\n"}
{"id": "1105.4272", "contents": "Title: Calibration with Changing Checking Rules and Its Application to\n  Short-Term Trading Abstract: We provide a natural learning process in which a financial trader without a\nrisk receives a gain in case when Stock Market is inefficient. In this process,\nthe trader rationally choose his gambles using a prediction made by a\nrandomized calibrated algorithm. Our strategy is based on Dawid's notion of\ncalibration with more general changing checking rules and on some modification\nof Kakade and Foster's randomized algorithm for computing calibrated forecasts. \n\n"}
{"id": "1106.1412", "contents": "Title: Control for Schroedinger operators on tori Abstract: A well known result of Jaffard states that an arbitrary region on a torus\ncontrols, in the L2 sense, solutions of the free stationary and dynamical\nSchroedinger equations. In this note we show that the same result is valid in\nthe presence of a smooth time-independent potential. The methods apply to\ncontinuous potentials as well and we conjecture that the L2 control is valid\nfor any bounded time dependent potential. \n\n"}
{"id": "1106.1651", "contents": "Title: Sparse Principal Component of a Rank-deficient Matrix Abstract: We consider the problem of identifying the sparse principal component of a\nrank-deficient matrix. We introduce auxiliary spherical variables and prove\nthat there exists a set of candidate index-sets (that is, sets of indices to\nthe nonzero elements of the vector argument) whose size is polynomially\nbounded, in terms of rank, and contains the optimal index-set, i.e. the\nindex-set of the nonzero elements of the optimal solution. Finally, we develop\nan algorithm that computes the optimal sparse principal component in polynomial\ntime for any sparsity degree. \n\n"}
{"id": "1106.2781", "contents": "Title: Optimal Dividend Payments for the Piecewise-Deterministic Poisson Risk\n  Model Abstract: This paper considers the optimal dividend payment problem in\npiecewise-deterministic compound Poisson risk models. The objective is to\nmaximize the expected discounted dividend payout up to the time of ruin. We\nprovide a comparative study in this general framework of both restricted and\nunrestricted payment schemes, which were only previously treated separately in\ncertain special cases of risk models in the literature. In the case of\nrestricted payment scheme, the value function is shown to be a classical\nsolution of the corresponding HJB equation, which in turn leads to an optimal\nrestricted payment policy known as the threshold strategy. In the case of\nunrestricted payment scheme, by solving the associated integro-differential\nquasi-variational inequality, we obtain the value function as well as an\noptimal unrestricted dividend payment scheme known as the barrier strategy.\nWhen claim sizes are exponentially distributed, we provide easily verifiable\nconditions under which the threshold and barrier strategies are optimal\nrestricted and unrestricted dividend payment policies, respectively. The main\nresults are illustrated with several examples, including a new example\nconcerning regressive growth rates. \n\n"}
{"id": "1106.3273", "contents": "Title: A Quasi-Sure Approach to the Control of Non-Markovian Stochastic\n  Differential Equations Abstract: We study stochastic differential equations (SDEs) whose drift and diffusion\ncoefficients are path-dependent and controlled. We construct a value process on\nthe canonical path space, considered simultaneously under a family of singular\nmeasures, rather than the usual family of processes indexed by the controls.\nThis value process is characterized by a second order backward SDE, which can\nbe seen as a non-Markovian analogue of the Hamilton-Jacobi-Bellman partial\ndifferential equation. Moreover, our value process yields a generalization of\nthe G-expectation to the context of SDEs. \n\n"}
{"id": "1107.1639", "contents": "Title: Injectivity and flatness of semitopological modules Abstract: The spaces D, S and E' over \\mathbb{R}^(n) are known to be flat modules over\nA=\\mathbb{C}[\\partial_{1},...,\\partial_{n}], whereas their duals D', S' and E\nare known to be injective modules over the same ring. Let A be a Noetherian\nk-algebra (k=\\mathbb{R} or \\mathbb{C}). The above observation leads us to study\nin this paper the link existing between the flatness of an A-module E which is\na locally convex topological k-vector space and the injectivity of its dual. We\nshow that, for dual pairs (E,E') which are (K) over A--a notion which is\nexplained in the paper--, injectivity of E' is a stronger condition than\nflatness of E. A preprint of this paper (dated September 2009) has been quoted\nand discussed by Shankar. \n\n"}
{"id": "1108.3025", "contents": "Title: Optimal control of a dengue epidemic model with vaccination Abstract: We present a SIR+ASI epidemic model to describe the interaction between human\nand dengue fever mosquito populations. A control strategy in the form of\nvaccination, to decrease the number of infected individuals, is used. An\noptimal control approach is applied in order to find the best way to fight the\ndisease. \n\n"}
{"id": "1108.4114", "contents": "Title: Collaborative Network Formation in Spatial Oligopolies Abstract: Recently, it has been shown that networks with an arbitrary degree sequence\nmay be a stable solution to a network formation game. Further, in recent years\nthere has been a rise in the number of firms participating in collaborative\nefforts. In this paper, we show conditions under which a graph with an\narbitrary degree sequence is admitted as a stable firm collaboration graph. \n\n"}
{"id": "1109.0105", "contents": "Title: Differentially Private Online Learning Abstract: In this paper, we consider the problem of preserving privacy in the online\nlearning setting. We study the problem in the online convex programming (OCP)\nframework---a popular online learning setting with several interesting\ntheoretical and practical implications---while using differential privacy as\nthe formal privacy measure. For this problem, we distill two critical\nattributes that a private OCP algorithm should have in order to provide\nreasonable privacy as well as utility guarantees: 1) linearly decreasing\nsensitivity, i.e., as new data points arrive their effect on the learning model\ndecreases, 2) sub-linear regret bound---regret bound is a popular\ngoodness/utility measure of an online learning algorithm.\n  Given an OCP algorithm that satisfies these two conditions, we provide a\ngeneral framework to convert the given algorithm into a privacy preserving OCP\nalgorithm with good (sub-linear) regret. We then illustrate our approach by\nconverting two popular online learning algorithms into their differentially\nprivate variants while guaranteeing sub-linear regret ($O(\\sqrt{T})$). Next, we\nconsider the special case of online linear regression problems, a practically\nimportant class of online learning problems, for which we generalize an\napproach by Dwork et al. to provide a differentially private algorithm with\njust $O(\\log^{1.5} T)$ regret. Finally, we show that our online learning\nframework can be used to provide differentially private algorithms for offline\nlearning as well. For the offline learning problem, our approach obtains better\nerror bounds as well as can handle larger class of problems than the existing\nstate-of-the-art methods Chaudhuri et al. \n\n"}
{"id": "1109.1900", "contents": "Title: Weakly-coupled systems in quantum control Abstract: This paper provides rigorous definitions and analysis of the dynamics of\nweakly-coupled systems and gives sufficient conditions for an infinite\ndimensional quantum control system to be weakly-coupled. As an illustration we\nprovide examples chosen among common physical systems. \n\n"}
{"id": "1109.4184", "contents": "Title: A (k+1)-Slope Theorem for the k-Dimensional Infinite Group Relaxation Abstract: We prove that any minimal valid function for the k-dimensional infinite group\nrelaxation that is piecewise linear with at most k+1 slopes and does not factor\nthrough a linear map with non-trivial kernel is extreme. This generalizes a\ntheorem of Gomory and Johnson for k=1, and Cornuejols and Molinaro for k=2. \n\n"}
{"id": "1112.2328", "contents": "Title: Exact Safety Verification of Hybrid Systems Using Sums-Of-Squares\n  Representation Abstract: In this paper we discuss how to generate inductive invariants for safety\nverification of hybrid systems. A hybrid symbolic-numeric method is presented\nto compute inequality inductive invariants of the given systems. A numerical\ninvariant of the given system can be obtained by solving a parameterized\npolynomial optimization problem via sum-of-squares (SOS) relaxation. And a\nmethod based on Gauss-Newton refinement and rational vector recovery is\ndeployed to obtain the invariants with rational coefficients, which exactly\nsatisfy the conditions of invariants. Several examples are given to illustrate\nour algorithm. \n\n"}
{"id": "1112.6234", "contents": "Title: Sparse Recovery from Nonlinear Measurements with Applications in Bad\n  Data Detection for Power Networks Abstract: In this paper, we consider the problem of sparse recovery from nonlinear\nmeasurements, which has applications in state estimation and bad data detection\nfor power networks. An iterative mixed $\\ell_1$ and $\\ell_2$ convex program is\nused to estimate the true state by locally linearizing the nonlinear\nmeasurements. When the measurements are linear, through using the almost\nEuclidean property for a linear subspace, we derive a new performance bound for\nthe state estimation error under sparse bad data and additive observation\nnoise. As a byproduct, in this paper we provide sharp bounds on the almost\nEuclidean property of a linear subspace, using the \"escape-through-the-mesh\"\ntheorem from geometric functional analysis. When the measurements are\nnonlinear, we give conditions under which the solution of the iterative\nalgorithm converges to the true state even though the locally linearized\nmeasurements may not be the actual nonlinear measurements. We numerically\nevaluate our iterative convex programming approach to perform bad data\ndetections in nonlinear electrical power networks problems. We are able to use\nsemidefinite programming to verify the conditions for convergence of the\nproposed iterative sparse recovery algorithms from nonlinear measurements. \n\n"}
{"id": "1201.3212", "contents": "Title: On asymptotic properties of matrix semigroups with an invariant cone Abstract: Recently, several research efforts showed that the analysis of joint spectral\ncharacteristics of sets of matrices is greatly eased when these matrices share\nan invariant cone. In this short note we prove two new results in this\ndirection. We prove that the joint spectral subradius is continuous in the\nneighborhood of sets of matrices that leave an embedded pair of cones\ninvariant. We show that the (averaged) maximal spectral radius, as well as the\nmaximal trace, of products of length t, converge towards the joint spectral\nradius when the matrices share an invariant cone, and addi- tionally one of\nthem is primitive. \n\n"}
{"id": "1203.2031", "contents": "Title: Design of modular wireless sensor Abstract: The paper addresses combinatorial approach to design of modular wireless\nsensor as composition of the sensor element from its component alternatives and\naggregation of the obtained solutions into a resultant aggregated solution. A\nhierarchical model is used for the wireless sensor element. The solving process\nconsists of three stages: (i) multicriteria ranking of design alternatives for\nsystem components/parts, (ii) composing the selected design alternatives into\ncomposite solution(s) while taking into account ordinal quality of the design\nalternatives above and their compatibility (this stage is based on Hierarchical\nMorphological Multicriteria Design - HMMD), and (iii) aggregation of the\nobtained composite solutions into a resultant aggregated solution(s). A\nnumerical example describes the problem structuring and solving processes for\nmodular alarm wireless sensor element. \n\n"}
{"id": "1203.2995", "contents": "Title: Marginal multi-Bernoulli filters: RFS derivation of MHT, JIPDA and\n  association-based MeMBer Abstract: Recent developments in random finite sets (RFSs) have yielded a variety of\ntracking methods that avoid data association. This paper derives a form of the\nfull Bayes RFS filter and observes that data association is implicitly present,\nin a data structure similar to MHT. Subsequently, algorithms are obtained by\napproximating the distribution of associations. Two algorithms result: one\nnearly identical to JIPDA, and another related to the MeMBer filter. Both\nimprove performance in challenging environments. \n\n"}
{"id": "1203.3258", "contents": "Title: QoE-aware Media Streaming in Technology and Cost Heterogeneous Networks Abstract: We present a framework for studying the problem of media streaming in\ntechnology and cost heterogeneous environments. We first address the problem of\nefficient streaming in a technology-heterogeneous setting. We employ random\nlinear network coding to simplify the packet selection strategies and alleviate\nissues such as duplicate packet reception. Then, we study the problem of media\nstreaming from multiple cost-heterogeneous access networks. Our objective is to\ncharacterize analytically the trade-off between access cost and user\nexperience. We model the Quality of user Experience (QoE) as the probability of\ninterruption in playback as well as the initial waiting time. We design and\ncharacterize various control policies, and formulate the optimal control\nproblem using a Markov Decision Process (MDP) with a probabilistic constraint.\nWe present a characterization of the optimal policy using the\nHamilton-Jacobi-Bellman (HJB) equation. For a fluid approximation model, we\nprovide an exact and explicit characterization of a threshold policy and prove\nits optimality using the HJB equation.\n  Our simulation results show that under properly designed control policy, the\nexistence of alternative access technology as a complement for a primary access\nnetwork can significantly improve the user experience without any bandwidth\nover-provisioning. \n\n"}
{"id": "1203.4523", "contents": "Title: On the Equivalence between Herding and Conditional Gradient Algorithms Abstract: We show that the herding procedure of Welling (2009) takes exactly the form\nof a standard convex optimization algorithm--namely a conditional gradient\nalgorithm minimizing a quadratic moment discrepancy. This link enables us to\ninvoke convergence results from convex optimization and to consider faster\nalternatives for the task of approximating integrals in a reproducing kernel\nHilbert space. We study the behavior of the different variants through\nnumerical simulations. The experiments indicate that while we can improve over\nherding on the task of approximating integrals, the original herding algorithm\ntends to approach more often the maximum entropy distribution, shedding more\nlight on the learning bias behind herding. \n\n"}
{"id": "1203.5161", "contents": "Title: Effect of correlations on network controllability Abstract: A dynamical system is controllable if by imposing appropriate external\nsignals on a subset of its nodes, it can be driven from any initial state to\nany desired state in finite time. Here we study the impact of various network\ncharacteristics on the minimal number of driver nodes required to control a\nnetwork. We find that clustering and modularity have no discernible impact, but\nthe symmetries of the underlying matching problem can produce linear, quadratic\nor no dependence on degree correlation coefficients, depending on the nature of\nthe underlying correlations. The results are supported by numerical simulations\nand help narrow the observed gap between the predicted and the observed number\nof driver nodes in real networks. \n\n"}
{"id": "1204.0343", "contents": "Title: Comments on \"Prediction of Subharmonic Oscillation in Switching\n  Converters Under Different Control Strategies\" Abstract: A recent paper [1] (El Aroudi, 2012) misapplied a critical condition (Fang\nand Abed, 2001) to a well-known example. Even if the mistake is corrected, the\nresults in [1] are applicable only to buck converters and period-doubling\nbifurcation. Actually, these results are known in Fang's works a decade ago\nwhich have broader critical conditions applicable to other converters and\nbifurcations. The flaws in [1] are identified. \n\n"}
{"id": "1204.3259", "contents": "Title: Combinatorial Evolution and Forecasting of Communication Protocol ZigBee Abstract: The article addresses combinatorial evolution and forecasting of\ncommunication protocol for wireless sensor networks (ZigBee). Morphological\ntree structure (a version of and-or tree) is used as a hierarchical model for\nthe protocol. Three generations of ZigBee protocol are examined. A set of\nprotocol change operations is generated and described. The change operations\nare used as items for forecasting based on combinatorial problems (e.g.,\nclustering, knapsack problem, multiple choice knapsack problem). Two kinds of\npreliminary forecasts for the examined communication protocol are considered:\n(i) direct expert (expert judgment) based forecast, (ii) computation of the\nforecast(s) (usage of multicriteria decision making and combinatorial\noptimization problems). Finally, aggregation of the obtained preliminary\nforecasts is considered (two aggregation strategies are used). \n\n"}
{"id": "1204.4865", "contents": "Title: Branch Flow Model: Relaxations and Convexification (Parts I, II) Abstract: We propose a branch flow model for the anal- ysis and optimization of mesh as\nwell as radial networks. The model leads to a new approach to solving optimal\npower flow (OPF) that consists of two relaxation steps. The first step\neliminates the voltage and current angles and the second step approximates the\nresulting problem by a conic program that can be solved efficiently. For radial\nnetworks, we prove that both relaxation steps are always exact, provided there\nare no upper bounds on loads. For mesh networks, the conic relaxation is always\nexact but the angle relaxation may not be exact, and we provide a simple way to\ndetermine if a relaxed solution is globally optimal. We propose convexification\nof mesh networks using phase shifters so that OPF for the convexified network\ncan always be solved efficiently for an optimal solution. We prove that\nconvexification requires phase shifters only outside a spanning tree of the\nnetwork and their placement depends only on network topology, not on power\nflows, generation, loads, or operating constraints. Part I introduces our\nbranch flow model, explains the two relaxation steps, and proves the conditions\nfor exact relaxation. Part II describes convexification of mesh networks, and\npresents simulation results. \n\n"}
{"id": "1204.5046", "contents": "Title: Instantaneous Relaying: Optimal Strategies and Interference\n  Neutralization Abstract: In a multi-user wireless network equipped with multiple relay nodes, some\nrelays are more intelligent than other relay nodes. The intelligent relays are\nable to gather channel state information, perform linear processing and forward\nsignals whereas the dumb relays is only able to serve as amplifiers. As the\ndumb relays are oblivious to the source and destination nodes, the wireless\nnetwork can be modeled as a relay network with *smart instantaneous relay*\nonly: the signals of source-destination arrive at the same time as\nsource-relay-destination. Recently, instantaneous relaying is shown to improve\nthe degrees-of-freedom of the network as compared to classical cut-set bound.\nIn this paper, we study an achievable rate region and its boundary of the\ninstantaneous interference relay channel in the scenario of (a) uninformed\nnon-cooperative source-destination nodes (source and destination nodes are not\naware of the existence of the relay and are non-cooperative) and (b) informed\nand cooperative source-destination nodes. Further, we examine the performance\nof interference neutralization: a relay strategy which is able to cancel\ninterference signals at each destination node in the air. We observe that\ninterference neutralization, although promise to achieve desired\ndegrees-of-freedom, may not be feasible if relay has limited power. Simulation\nresults show that the optimal relay strategies improve the achievable rate\nregion and provide better user-fairness in both uninformed non-cooperative and\ninformed cooperative scenarios. \n\n"}
{"id": "1204.6174", "contents": "Title: Efficient Computations of a Security Index for False Data Attacks in\n  Power Networks Abstract: The resilience of Supervisory Control and Data Acquisition (SCADA) systems\nfor electric power networks for certain cyber-attacks is considered. We analyze\nthe vulnerability of the measurement system to false data attack on\ncommunicated measurements. The vulnerability analysis problem is shown to be\nNP-hard, meaning that unless $P = NP$ there is no polynomial time algorithm to\nanalyze the vulnerability of the system. Nevertheless, we identify situations,\nsuch as the full measurement case, where it can be solved efficiently. In such\ncases, we show indeed that the problem can be cast as a generalization of the\nminimum cut problem involving costly nodes. We further show that it can be\nreformulated as a standard minimum cut problem (without costly nodes) on a\nmodified graph of proportional size. An important consequence of this result is\nthat our approach provides the first exact efficient algorithm for the\nvulnerability analysis problem under the full measurement assumption.\nFurthermore, our approach also provides an efficient heuristic algorithm for\nthe general NP-hard problem. Our results are illustrated by numerical studies\non benchmark systems including the IEEE 118-bus system. \n\n"}
{"id": "1204.6345", "contents": "Title: Efficient Computation of a Canonical Form for a Generalized P-matrix Abstract: We use recent results on algorithms for Markov decision problems to show that\na canonical form for a generalized P-matrix can be computed, in some important\ncases, by a strongly polynomial algorithm. \n\n"}
{"id": "1204.6624", "contents": "Title: Theorems about Ergodicity and Class-Ergodicity of Chains with\n  Applications in Known Consensus Models Abstract: In a multi-agent system, unconditional (multiple) consensus is the property\nof reaching to (multiple) consensus irrespective of the instant and values at\nwhich states are initialized. For linear algorithms, occurrence of\nunconditional (multiple) consensus turns out to be equivalent to (class-)\nergodicity of the transition chain (A_n). For a wide class of chains, chains\nwith so-called balanced asymmetry property, necessary and sufficient conditions\nfor ergodicity and class-ergodicity are derived. The results are employed to\nanalyze the limiting behavior of agents' states in the JLM model, the Krause\nmodel, and the Cucker-Smale model. In particular, unconditional single or\nmultiple consensus occurs in all three models. Moreover, a necessary and\nsufficient condition for unconditional consensus in the JLM model and a\nsufficient condition for consensus in the Cucker-Smale model are obtained. \n\n"}
{"id": "1205.0050", "contents": "Title: On the dynamic programming principle for uniformly nondegenerate\n  stochastic differential games in domains and the Isaacs equations Abstract: We prove the dynamic programming principe for uniformly nondegenerate\nstochastic differential games in the framework of time-homogeneous diffusion\nprocesses considered up to the first exit time from a domain. In contrast with\nprevious results established for constant stopping times we allow arbitrary\nstopping times and randomized ones as well. There is no assumption about\nsolvability of the the Isaacs equation in any sense (classical or viscosity).\nThe zeroth-order \"coefficient\" and the \"free\" term are only assumed to be\nmeasurable in the space variable. We also prove that value functions are\nuniquely determined by the functions defining the corresponding Isaacs\nequations and thus stochastic games with the same Isaacs equation have the same\nvalue functions. \n\n"}
{"id": "1205.0207", "contents": "Title: Shortest Path Set Induced Vertex Ordering and its Application to\n  Distributed Distance Optimal Multi-agent Formation Path Planning Abstract: For the task of moving a group of indistinguishable agents on a connected\ngraph with unit edge lengths into an arbitrary goal formation, it was\npreviously shown that distance optimal paths can be scheduled to complete with\na tight convergence time guarantee, using a fully centralized algorithm. In\nthis study, we show that the problem formulation in fact induces a more\nfundamental ordering of the vertices on the underlying graph network, which\ndirectly leads to a more intuitive scheduling algorithm that assures the same\nconvergence time and runs faster. More importantly, this structure enables a\ndistributed scheduling algorithm once individual paths are assigned to the\nagents, which was not possible before. The vertex ordering also readily extends\nto more general graphs - those with non-unit capacities and edge lengths - for\nwhich we again guarantee the convergence time until the desired formation is\nachieved. \n\n"}
{"id": "1205.1863", "contents": "Title: Intrinsic volumes of symmetric cones Abstract: We compute the intrinsic volumes of the cone of positive semidefinite\nmatrices over the real numbers, over the complex numbers, and over the\nquaternions, in terms of integrals related to Mehta's integral. Several\napplications for the probabilistic analysis of semidefinite programming are\ngiven. \n\n"}
{"id": "1205.2039", "contents": "Title: Hyperbolicity and exponential long-time convergence for space-time\n  periodic Hamilton-Jacobi equations Abstract: We prove exponential convergence to time-periodic states of the solutions of\ntime-periodic Hamilton-Jacobi equations on the torus, assuming that the Aubry\nset is the union of a finite number of hyperbolic periodic orbits of the the\nEuler Lagrange flow. The period of limiting solutions is the least common\nmultiple of the periods of the orbits in the Aubry set. This extends a result\nthat we obtained in the autonomous case. \n\n"}
{"id": "1205.5927", "contents": "Title: An Approximate Projected Consensus Algorithm for Computing Intersection\n  of Convex Sets Abstract: In this paper, we propose an approximate projected consensus algorithm for a\nnetwork to cooperatively compute the intersection of convex sets. Instead of\nassuming the exact convex projection proposed in the literature, we allow each\nnode to compute an approximate projection and communicate it to its neighbors.\nThe communication graph is directed and time-varying. Nodes update their states\nby weighted averaging. Projection accuracy conditions are presented for the\nconsidered algorithm. They indicate how much projection accuracy is required to\nensure global consensus to a point in the intersection set when the\ncommunication graph is uniformly jointly strongly connected. We show that\n$\\pi/4$ is a critical angle error of the projection approximation to ensure a\nbounded state. A numerical example indicates that this approximate projected\nconsensus algorithm may achieve better performance than the exact projected\nconsensus algorithm in some cases. \n\n"}
{"id": "1206.3065", "contents": "Title: Stability Analysis and Controller Design for a Linear System with Duhem\n  Hysteresis Nonlinearity Abstract: In this paper, we investigate the stability of a feedback interconnection\nbetween a linear system and a Duhem hysteresis operator, where the linear\nsystem and the Duhem hysteresis operator satisfy either the counter-clockwise\n(CCW) or clockwise (CW) input-output dynamics. More precisely, we present\nsufficient conditions for the stability of the interconnected system that\ndepend on the CW or CCW properties of the linear system and the Duhem operator.\nBased on these results we introduce a control design methodology for\nstabilizing a linear plant with a hysteretic actuator or sensor without\nrequiring precise information on the hysteresis operator. \n\n"}
{"id": "1206.4900", "contents": "Title: Robust Power System State Estimation for the Nonlinear AC Flow Model Abstract: An important monitoring task for power systems is accurate estimation of the\nsystem operation state. Under the nonlinear AC power flow model, the state\nestimation (SE) problem is inherently nonconvex giving rise to many local\noptima. In addition to nonconvexity, SE is challenged by data integrity and\ncyber-security issues. Unfortunately, existing robust (R-) SE schemes employed\nroutinely in practice rely on iterative solvers, which are sensitive to\ninitialization and cannot ensure global optimality. A novel R-SE approach is\nformulated here by capitalizing on the sparsity of an overcomplete outlier\nvector model. Observability and identifiability issues of this model are\ninvestigated, and neat links are established between R-SE and error control\ncoding. The \\emph{convex} semidefinite relaxation (SDR) technique is further\npursued to render the nonconvex R-SE problem efficiently solvable. The\nresultant algorithm markedly outperforms existing iterative alternatives, as\ncorroborated through numerical tests on the standard IEEE 30-bus system. \n\n"}
{"id": "1206.5376", "contents": "Title: Note on stochastic control problems related with general fully coupled\n  forward-backward stochastic differential equations Abstract: In this paper we study stochastic optimal control problems of general fully\ncoupled forward-backward stochastic differential equations (FBSDEs). In Li and\nWei [8] the authors studied two cases of diffusion coefficients $\\sigma$ of\nFSDEs, in one case when $\\sigma$\\ depends on the control and does not depend on\nthe second component of the solution $(Y, Z)$ of the BSDE, and in the other\ncase $\\sigma$ depends on $Z$ and doesn't depend on the control. Here we study\nthe general case when $\\sigma$ depends on both $Z$ and the control at the same\ntime. The recursive cost functionals are defined by controlled general fully\ncoupled FBSDEs, then the value functions are given by taking the essential\nsupremum of the cost functionals over all admissible controls. We give the\nformulation of related generalized Hamilton-Jacobi-Bellman (HJB) equations, and\nprove the value function is its viscosity solution. \n\n"}
{"id": "1207.0563", "contents": "Title: Kron Reduction of Generalized Electrical Networks Abstract: Kron reduction is used to simplify the analysis of multi-machine power\nsystems under certain steady state assumptions that underly the usage of\nphasors. In this paper we show how to perform Kron reduction for a class of\nelectrical networks without steady state assumptions. The reduced models can\nthus be used to analyze the transient as well as the steady state behavior of\nthese electrical networks. \n\n"}
{"id": "1207.4592", "contents": "Title: Differentially Private Kalman Filtering Abstract: This paper studies the H2 (Kalman) filtering problem in the situation where a\nsignal estimate must be constructed based on inputs from individual\nparticipants, whose data must remain private. This problem arises in emerging\napplications such as smart grids or intelligent transportation systems, where\nusers continuously send data to third-party aggregators performing global\nmonitoring or control tasks, and require guarantees that this data cannot be\nused to infer additional personal information. To provide strong formal privacy\nguarantees against adversaries with arbitrary side information, we rely on the\nnotion of differential privacy introduced relatively recently in the database\nliterature. This notion is extended to dynamic systems with many participants\ncontributing independent input signals, and mechanisms are then proposed to\nsolve the H2 filtering problem with a differential privacy constraint. A method\nfor mitigating the impact of the privacy-inducing mechanism on the estimation\nperformance is described, which relies on controlling the Hinfinity norm of the\nfilter. Finally, we discuss an application to a privacy-preserving traffic\nmonitoring system. \n\n"}
{"id": "1207.5119", "contents": "Title: Feedback stabilization of dynamical systems with switched delays Abstract: We analyze a classification of two main families of controllers that are of\ninterest when the feedback loop is subject to switching propagation delays due\nto routing via a wireless multi-hop communication network. We show that we can\ncast this problem as a subclass of classical switching systems, which is a\nnon-trivial generalization of classical LTI systems with timevarying delays. We\nconsider both cases where delay-dependent and delay independent controllers are\nused, and show that both can be modeled as switching systems with unconstrained\nswitchings. We provide NP-hardness results for the stability verification\nproblem, and propose a general methodology for approximate stability analysis\nwith arbitrary precision. We finally give evidence that non-trivial design\nproblems arise for which new algorithmic methods are needed. \n\n"}
{"id": "1207.5123", "contents": "Title: Lifted polytope methods for stability analysis of switching systems Abstract: We describe new methods for deciding the stability of switching systems. The\nmethods build on two ideas previously appeared in the literature: the polytope\nnorm iterative construction, and the lifting procedure. Moreover, the\ncombination of these two ideas allows us to introduce a pruning algorithm which\ncan importantly reduce the computational burden. We prove several appealing\ntheoretical properties of our methods like a finiteness computational result\nwhich extends a known result for unlifted sets of matrices, and provide\nnumerical examples of their good behaviour. \n\n"}
{"id": "1207.5286", "contents": "Title: Backward stochastic partial differential equations with quadratic growth Abstract: This paper is concerned with the existence and uniqueness of weak solutions\nto the Cauchy-Dirichlet problem of backward stochastic partial differential\nequations (BSPDEs) with nonhomogeneous terms of quadratic growth in both the\ngradient of the first unknown and the second unknown. As an example, we\nconsider a non-Markovian stochastic optimal control problem with cost\nfunctional formulated by a quadratic BSDE, where the corresponding value\nfunction satisfies the above quadratic BSPDE. \n\n"}
{"id": "1209.0229", "contents": "Title: Efficiency-Risk Tradeoffs in Dynamic Oligopoly Markets - with\n  application to electricity markets Abstract: In this paper, we examine in an abstract framework, how a tradeoff between\nefficiency and robustness arises in different dynamic oligopolistic market\narchitectures. We consider a market in which there is a monopolistic resource\nprovider and agents that enter and exit the market following a random process.\nSelf-interested and fully rational agents dynamically update their resource\nconsumption decisions over a finite time horizon, under the constraint that the\ntotal resource consumption requirements are met before each individual's\ndeadline. We then compare the statistics of the stationary aggregate demand\nprocesses induced by the non-cooperative and cooperative load scheduling\nschemes. We show that although the non-cooperative load scheduling scheme leads\nto an efficiency loss - widely known as the \"price of anarchy\" - the stationary\ndistribution of the corresponding aggregate demand process has a smaller tail.\nThis tail, which corresponds to rare and undesirable demand spikes, is\nimportant in many applications of interest. On the other hand, when the agents\ncan cooperate with each other in optimizing their total cost, a higher market\nefficiency is achieved at the cost of a higher probability of demand spikes. We\nthus posit that the origins of endogenous risk in such systems may lie in the\nmarket architecture, which is an inherent characteristic of the system. \n\n"}
{"id": "1209.2910", "contents": "Title: Community Detection in the Labelled Stochastic Block Model Abstract: We consider the problem of community detection from observed interactions\nbetween individuals, in the context where multiple types of interaction are\npossible. We use labelled stochastic block models to represent the observed\ndata, where labels correspond to interaction types. Focusing on a two-community\nscenario, we conjecture a threshold for the problem of reconstructing the\nhidden communities in a way that is correlated with the true partition. To\nsubstantiate the conjecture, we prove that the given threshold correctly\nidentifies a transition on the behaviour of belief propagation from insensitive\nto sensitive. We further prove that the same threshold corresponds to the\ntransition in a related inference problem on a tree model from infeasible to\nfeasible. Finally, numerical results using belief propagation for community\ndetection give further support to the conjecture. \n\n"}
{"id": "1209.3834", "contents": "Title: Low-rank matrix completion by Riemannian optimization---extended version Abstract: The matrix completion problem consists of finding or approximating a low-rank\nmatrix based on a few samples of this matrix. We propose a new algorithm for\nmatrix completion that minimizes the least-square distance on the sampling set\nover the Riemannian manifold of fixed-rank matrices. The algorithm is an\nadaptation of classical non-linear conjugate gradients, developed within the\nframework of retraction-based optimization on manifolds. We describe all the\nnecessary objects from differential geometry necessary to perform optimization\nover this low-rank matrix manifold, seen as a submanifold embedded in the space\nof matrices. In particular, we describe how metric projection can be used as\nretraction and how vector transport lets us obtain the conjugate search\ndirections. Finally, we prove convergence of a regularized version of our\nalgorithm under the assumption that the restricted isometry property holds for\nincoherent matrices throughout the iterations. The numerical experiments\nindicate that our approach scales very well for large-scale problems and\ncompares favorably with the state-of-the-art, while outperforming most existing\nsolvers. \n\n"}
{"id": "1209.5805", "contents": "Title: Memoryless Control Design for Persistent Surveillance under Safety\n  Constraints Abstract: This paper deals with the design of time-invariant memoryless control\npolicies for robots that move in a finite two- dimensional lattice and are\ntasked with persistent surveillance of an area in which there are forbidden\nregions. We model each robot as a controlled Markov chain whose state comprises\nits position in the lattice and the direction of motion. The goal is to find\nthe minimum number of robots and an associated time-invariant memoryless\ncontrol policy that guarantees that the largest number of states are\npersistently surveilled without ever visiting a forbidden state. We propose a\ndesign method that relies on a finitely parametrized convex program inspired by\nentropy maximization principles. Numerical examples are provided. \n\n"}
{"id": "1210.0198", "contents": "Title: Maximum Likelihood for Matrices with Rank Constraints Abstract: Maximum likelihood estimation is a fundamental optimization problem in\nstatistics. We study this problem on manifolds of matrices with bounded rank.\nThese represent mixtures of distributions of two independent discrete random\nvariables. We determine the maximum likelihood degree for a range of\ndeterminantal varieties, and we apply numerical algebraic geometry to compute\nall critical points of their likelihood functions. This led to the discovery of\nmaximum likelihood duality between matrices of complementary ranks, a result\nproved subsequently by Draisma and Rodriguez. \n\n"}
{"id": "1210.0729", "contents": "Title: An Algorithm to Solve Polyhedral Convex Set Optimization Problems Abstract: An algorithm which computes a solution of a set optimization problem is\nprovided. The graph of the objective map is assumed to be given by finitely\nmany linear inequalities. A solution is understood to be a set of points in the\ndomain satisfying two conditions: the attainment of the infimum and minimality\nwith respect to a set relation. In the first phase of the algorithm, a linear\nvector optimization problem, called the vectorial relaxation, is solved. The\nresulting pre-solution yields the attainment of the infimum but, in general,\nnot minimality. In the second phase of the algorithm, minimality is established\nby solving certain linear programs in combination with vertex enumeration of\nsome values of the objective map. \n\n"}
{"id": "1210.5196", "contents": "Title: Matrix reconstruction with the local max norm Abstract: We introduce a new family of matrix norms, the \"local max\" norms,\ngeneralizing existing methods such as the max norm, the trace norm (nuclear\nnorm), and the weighted or smoothed weighted trace norms, which have been\nextensively used in the literature as regularizers for matrix reconstruction\nproblems. We show that this new family can be used to interpolate between the\n(weighted or unweighted) trace norm and the more conservative max norm. We test\nthis interpolation on simulated data and on the large-scale Netflix and\nMovieLens ratings data, and find improved accuracy relative to the existing\nmatrix norms. We also provide theoretical results showing learning guarantees\nfor some of the new norms. \n\n"}
{"id": "1210.7719", "contents": "Title: Robustness, Canalyzing Functions and Systems Design Abstract: We study a notion of robustness of a Markov kernel that describes a system of\nseveral input random variables and one output random variable. Robustness\nrequires that the behaviour of the system does not change if one or several of\nthe input variables are knocked out. If the system is required to be robust\nagainst too many knockouts, then the output variable cannot distinguish\nreliably between input states and must be independent of the input. We study\nhow many input states the output variable can distinguish as a function of the\nrequired level of robustness.\n  Gibbs potentials allow a mechanistic description of the behaviour of the\nsystem after knockouts. Robustness imposes structural constraints on these\npotentials. We show that interaction families of Gibbs potentials allow to\ndescribe robust systems.\n  Given a distribution of the input random variables and the Markov kernel\ndescribing the system, we obtain a joint probability distribution. Robustness\nimplies a number of conditional independence statements for this joint\ndistribution. The set of all probability distributions corresponding to robust\nsystems can be decomposed into a finite union of components, and we find\nparametrizations of the components. The decomposition corresponds to a primary\ndecomposition of the conditional independence ideal and can be derived from\nmore general results about generalized binomial edge ideals. \n\n"}
{"id": "1211.3128", "contents": "Title: Non-asymptotic Upper Bounds for Deletion Correcting Codes Abstract: Explicit non-asymptotic upper bounds on the sizes of multiple-deletion\ncorrecting codes are presented. In particular, the largest single-deletion\ncorrecting code for $q$-ary alphabet and string length $n$ is shown to be of\nsize at most $\\frac{q^n-q}{(q-1)(n-1)}$. An improved bound on the asymptotic\nrate function is obtained as a corollary. Upper bounds are also derived on\nsizes of codes for a constrained source that does not necessarily comprise of\nall strings of a particular length, and this idea is demonstrated by\napplication to sets of run-length limited strings.\n  The problem of finding the largest deletion correcting code is modeled as a\nmatching problem on a hypergraph. This problem is formulated as an integer\nlinear program. The upper bound is obtained by the construction of a feasible\npoint for the dual of the linear programming relaxation of this integer linear\nprogram.\n  The non-asymptotic bounds derived imply the known asymptotic bounds of\nLevenshtein and Tenengolts and improve on known non-asymptotic bounds.\nNumerical results support the conjecture that in the binary case, the\nVarshamov-Tenengolts codes are the largest single-deletion correcting codes. \n\n"}
{"id": "1211.3412", "contents": "Title: Network Sampling: From Static to Streaming Graphs Abstract: Network sampling is integral to the analysis of social, information, and\nbiological networks. Since many real-world networks are massive in size,\ncontinuously evolving, and/or distributed in nature, the network structure is\noften sampled in order to facilitate study. For these reasons, a more thorough\nand complete understanding of network sampling is critical to support the field\nof network science. In this paper, we outline a framework for the general\nproblem of network sampling, by highlighting the different objectives,\npopulation and units of interest, and classes of network sampling methods. In\naddition, we propose a spectrum of computational models for network sampling\nmethods, ranging from the traditionally studied model based on the assumption\nof a static domain to a more challenging model that is appropriate for\nstreaming domains. We design a family of sampling methods based on the concept\nof graph induction that generalize across the full spectrum of computational\nmodels (from static to streaming) while efficiently preserving many of the\ntopological properties of the input graphs. Furthermore, we demonstrate how\ntraditional static sampling algorithms can be modified for graph streams for\neach of the three main classes of sampling methods: node, edge, and\ntopology-based sampling. Our experimental results indicate that our proposed\nfamily of sampling methods more accurately preserves the underlying properties\nof the graph for both static and streaming graphs. Finally, we study the impact\nof network sampling algorithms on the parameter estimation and performance\nevaluation of relational classification algorithms. \n\n"}
{"id": "1212.1788", "contents": "Title: Approximate discrete-time schemes for the estimation of diffusion\n  processes from complete observations Abstract: In this paper, a modification of the conventional approximations to the\nquasi-maximum likelihood method is introduced for the parameter estimation of\ndiffusion processes from discrete observations. This is based on a convergent\napproximation to the first two conditional moments of the diffusion process\nthrough discrete-time schemes. It is shown that, for finite samples, the\nresulting approximate estimators converge to the quasi-maximum likelihood one\nwhen the error between the discrete-time approximation and the diffusion\nprocess decreases. For an increasing number of observations, the approximate\nestimators are asymptotically normal distributed and their bias decreases when\nthe mentioned error does it. A simulation study is provided to illustrate the\nperformance of the new estimators. The results show that, with respect to the\nconventional approximate estimators, the new ones significantly enhance the\nparameter estimation of the test equations. The proposed estimators are\nintended for the recurrent practical situation where a nonlinear stochastic\nsystem should be identified from a reduced number of complete observations\ndistant in time. \n\n"}
{"id": "1212.2287", "contents": "Title: Runtime Optimizations for Prediction with Tree-Based Models Abstract: Tree-based models have proven to be an effective solution for web ranking as\nwell as other problems in diverse domains. This paper focuses on optimizing the\nruntime performance of applying such models to make predictions, given an\nalready-trained model. Although exceedingly simple conceptually, most\nimplementations of tree-based models do not efficiently utilize modern\nsuperscalar processor architectures. By laying out data structures in memory in\na more cache-conscious fashion, removing branches from the execution flow using\na technique called predication, and micro-batching predictions using a\ntechnique called vectorization, we are able to better exploit modern processor\narchitectures and significantly improve the speed of tree-based models over\nhard-coded if-else blocks. Our work contributes to the exploration of\narchitecture-conscious runtime implementations of machine learning algorithms. \n\n"}
{"id": "1212.3170", "contents": "Title: Improving Macrocell - Small Cell Coexistence through Adaptive\n  Interference Draining Abstract: The deployment of underlay small base stations (SBSs) is expected to\nsignificantly boost the spectrum efficiency and the coverage of next-generation\ncellular networks. However, the coexistence of SBSs underlaid to an existing\nmacro-cellular network faces important challenges, notably in terms of spectrum\nsharing and interference management. In this paper, we propose a novel\ngame-theoretic model that enables the SBSs to optimize their transmission rates\nby making decisions on the resource occupation jointly in the frequency and\nspatial domains. This procedure, known as interference draining, is performed\namong cooperative SBSs and allows to drastically reduce the interference\nexperienced by both macro- and small cell users. At the macrocell side, we\nconsider a modified water-filling policy for the power allocation that allows\neach macrocell user (MUE) to focus the transmissions on the degrees of freedom\nover which the MUE experiences the best channel and interference conditions.\nThis approach not only represents an effective way to decrease the received\ninterference at the MUEs but also grants the SBSs tier additional transmission\nopportunities and allows for a more agile interference management. Simulation\nresults show that the proposed approach yields significant gains at both\nmacrocell and small cell tiers, in terms of average achievable rate per user,\nreaching up to 37%, relative to the non-cooperative case, for a network with\n150 MUEs and 200 SBSs. \n\n"}
{"id": "1212.3471", "contents": "Title: Optimal Cuts and Partitions in Tree Metrics in Polynomial Time Abstract: We present a polynomial time dynamic programming algorithm for optimal\npartitions in the shortest path metric induced by a tree. This resolves, among\nother things, the exact complexity status of the optimal partition problems in\none dimensional geometric metric settings. Our method of solution could be also\nof independent interest in other applications. We discuss also an extension of\nour method to the class of metrics induced by the bounded treewidth graphs. \n\n"}
{"id": "1212.5860", "contents": "Title: A short note on the tail bound of Wishart distribution Abstract: We study the tail bound of the emperical covariance of multivariate normal\ndistribution. Following the work of (Gittens & Tropp, 2011), we provide a tail\nbound with a small constant. \n\n"}
{"id": "1301.0091", "contents": "Title: On the Robust Optimal Stopping Problem Abstract: We study a robust optimal stopping problem with respect to a set $\\cP$ of\nmutually singular probabilities. This can be interpreted as a zero-sum\ncontroller-stopper game in which the stopper is trying to maximize its pay-off\nwhile an adverse player wants to minimize this payoff by choosing an evaluation\ncriteria from $\\cP$. We show that the \\emph{upper Snell envelope $\\ol{Z}$} of\nthe reward process $Y$ is a supermartingale with respect to an appropriately\ndefined nonlinear expectation $\\ul{\\sE}$, and $\\ol{Z}$ is further an\n$\\ul{\\sE}-$martingale up to the first time $\\t^*$ when $\\ol{Z}$ meets $Y$.\nConsequently, $\\t^*$ is the optimal stopping time for the robust optimal\nstopping problem and the corresponding zero-sum game has a value. Although the\nresult seems similar to the one obtained in the classical optimal stopping\ntheory, the mutual singularity of probabilities and the game aspect of the\nproblem give rise to major technical hurdles, which we circumvent using some\nnew methods. \n\n"}
{"id": "1301.2194", "contents": "Title: Network-based clustering with mixtures of L1-penalized Gaussian\n  graphical models: an empirical investigation Abstract: In many applications, multivariate samples may harbor previously unrecognized\nheterogeneity at the level of conditional independence or network structure.\nFor example, in cancer biology, disease subtypes may differ with respect to\nsubtype-specific interplay between molecular components. Then, both subtype\ndiscovery and estimation of subtype-specific networks present important and\nrelated challenges. To enable such analyses, we put forward a mixture model\nwhose components are sparse Gaussian graphical models. This brings together\nmodel-based clustering and graphical modeling to permit simultaneous estimation\nof cluster assignments and cluster-specific networks. We carry out estimation\nwithin an L1-penalized framework, and investigate several specific penalization\nregimes. We present empirical results on simulated data and provide general\nrecommendations for the formulation and use of mixtures of L1-penalized\nGaussian graphical models. \n\n"}
{"id": "1301.5332", "contents": "Title: Online Learning with Pairwise Loss Functions Abstract: Efficient online learning with pairwise loss functions is a crucial component\nin building large-scale learning system that maximizes the area under the\nReceiver Operator Characteristic (ROC) curve. In this paper we investigate the\ngeneralization performance of online learning algorithms with pairwise loss\nfunctions. We show that the existing proof techniques for generalization bounds\nof online algorithms with a univariate loss can not be directly applied to\npairwise losses. In this paper, we derive the first result providing\ndata-dependent bounds for the average risk of the sequence of hypotheses\ngenerated by an arbitrary online learner in terms of an easily computable\nstatistic, and show how to extract a low risk hypothesis from the sequence. We\ndemonstrate the generality of our results by applying it to two important\nproblems in machine learning. First, we analyze two online algorithms for\nbipartite ranking; one being a natural extension of the perceptron algorithm\nand the other using online convex optimization. Secondly, we provide an\nanalysis for the risk bound for an online algorithm for supervised metric\nlearning. \n\n"}
{"id": "1301.5898", "contents": "Title: Phase Diagram and Approximate Message Passing for Blind Calibration and\n  Dictionary Learning Abstract: We consider dictionary learning and blind calibration for signals and\nmatrices created from a random ensemble. We study the mean-squared error in the\nlimit of large signal dimension using the replica method and unveil the\nappearance of phase transitions delimiting impossible, possible-but-hard and\npossible inference regions. We also introduce an approximate message passing\nalgorithm that asymptotically matches the theoretical performance, and show\nthrough numerical tests that it performs very well, for the calibration\nproblem, for tractable system sizes. \n\n"}
{"id": "1302.0286", "contents": "Title: Stochastic maximum principle for optimal control of SPDEs Abstract: We prove a version of the maximum principle, in the sense of Pontryagin, for\nthe optimal control of a stochastic partial differential equation driven by a\nfinite dimensional Wiener process. The equation is formulated in a\nsemi-abstract form that allows direct applications to a large class of\ncontrolled stochastic parabolic equations. We allow for a diffusion coefficient\ndependent on the control parameter, and the space of control actions is\ngeneral, so that in particular we need to introduce two adjoint processes. The\nsecond adjoint process takes values in a suitable space of operators on $L^4$. \n\n"}
{"id": "1302.0908", "contents": "Title: The Traffic Phases of Road Networks Abstract: We study the relation between the average traffic flow and the vehicle\ndensity on road networks that we call 2D-traffic fundamental diagram. We show\nthat this diagram presents mainly four phases. We analyze different cases.\nFirst, the case of a junction managed with a priority rule is presented, four\ntraffic phases are identified and described, and a good analytic approximation\nof the fundamental diagram is obtained by computing a generalized eigenvalue of\nthe dynamics of the system. Then, the model is extended to the case of two\njunctions, and finally to a regular city. The system still presents mainly four\nphases. The role of a critical circuit of non-priority roads appears clearly in\nthe two junctions case. In Section 4, we use traffic light controls to improve\nthe traffic diagram. We present the improvements obtained by open-loop, local\nfeedback, and global feedback strategies. A comparison based on the response\ntimes to reach the stationary regime is also given. Finally, we show the\nimportance of the design of the junction. It appears that if the junction is\nenough large, the traffic is almost not slowed down by the junction. \n\n"}
{"id": "1302.0935", "contents": "Title: Optimal control problems of fully coupled FBSDEs and viscosity solutions\n  of Hamilton-Jacobi-Bellman equations Abstract: In this paper we study stochastic optimal control problems of fully coupled\nforward-backward stochastic differential equations (FBSDEs). The recursive cost\nfunctionals are defined by controlled fully coupled FBSDEs. We study two cases\nof diffusion coefficients $\\sigma$ of FSDEs. We use a new method to prove that\nthe value functions are deterministic, satisfy the dynamic programming\nprinciple (DPP), and are viscosity solutions to the associated generalized\nHamilton-Jacobi-Bellman (HJB) equations. The associated generalized HJB\nequations are related with algebraic equations when $\\sigma$ depends on the\nsecond component of the solution $(Y, Z)$ of the BSDE and doesn't depend on the\ncontrol. For this we adopt Peng's BSDE method, and so in particular, the notion\nof stochastic backward semigroup in [16]. We emphasize that the fact that\n$\\sigma$ also depends on $Z$ makes the stochastic control much more complicate\nand has as consequence that the associated HJB equation is combined with an\nalgebraic equation, which is inspired by Wu and Yu [19]. We use the\ncontinuation method combined with the fixed point theorem to prove that the\nalgebraic equation has a unique solution, and moreover, we also give the\nrepresentation for this solution. On the other hand, we prove some new basic\nestimates for fully coupled FBSDEs under the monotonic assumptions. In\nparticular, we prove under the Lipschitz and linear growth conditions that\nfully coupled FBSDEs have a unique solution on the small time interval, if the\nLipschitz constant of $\\sigma$\\ with respect to $z$ is sufficiently small. We\nalso establish a generalized comparison theorem for such fully coupled FBSDEs. \n\n"}
{"id": "1302.2017", "contents": "Title: Cooperative Environmental Monitoring for PTZ Visual Sensor Networks: A\n  Payoff-based Learning Approach Abstract: This paper investigates cooperative environmental monitoring for\nPan-Tilt-Zoom (PTZ) visual sensor networks. We first present a novel\nformulation of the optimal environmental monitoring problem, whose objective\nfunction is intertwined with the uncertain state of the environment. In\naddition, due to the large volume of vision data, it is desired for each sensor\nto execute processing through local computation and communication. To address\nthe issues, we present a distributed solution to the problem based on game\ntheoretic cooperative control and payoff-based learning. At the first stage, a\nutility function is designed so that the resulting game constitutes a potential\ngame with potential function equal to the group objective function, where the\ndesigned utility is shown to be computable through local image processing and\ncommunication. Then, we present a payoff-based learning algorithm so that the\nsensors are led to the global objective function maximizers without using any\nprior information on the environmental state. Finally, we run experiments to\ndemonstrate the effectiveness of the present approach. \n\n"}
{"id": "1303.1090", "contents": "Title: Embedded Online Optimization for Model Predictive Control at Megahertz\n  Rates Abstract: Faster, cheaper, and more power efficient optimization solvers than those\ncurrently offered by general-purpose solutions are required for extending the\nuse of model predictive control (MPC) to resource-constrained embedded\nplatforms. We propose several custom computational architectures for different\nfirst-order optimization methods that can handle linear-quadratic MPC problems\nwith input, input-rate, and soft state constraints. We provide analysis\nensuring the reliable operation of the resulting controller under reduced\nprecision fixed-point arithmetic. Implementation of the proposed architectures\nin FPGAs shows that satisfactory control performance at a sample rate beyond 1\nMHz is achievable even on low-end devices, opening up new possibilities for the\napplication of MPC on embedded systems. \n\n"}
{"id": "1303.1707", "contents": "Title: Moment LMI approach to LTV impulsive control Abstract: In the 1960s, a moment approach to linear time varying (LTV) minimal norm\nimpulsive optimal control was developed, as an alternative to direct approaches\n(based on discretization of the equations of motion and linear programming) or\nindirect approaches (based on Pontryagin's maximum principle). This paper\nrevisits these classical results in the light of recent advances in convex\noptimization, in particular the use of measures jointly with hierarchy of\nlinear matrix inequality (LMI) relaxations. Linearity of the dynamics allows us\nto integrate system trajectories and to come up with a simplified LMI hierarchy\nwhere the only unknowns are moments of a vector of control measures of time. In\nparticular, occupation measures of state and control variables do not appear in\nthis formulation. This is in stark contrast with LMI relaxations arising\nusually in polynomial optimal control, where size grows quickly as a function\nof the relaxation order. Jointly with the use of Chebyshev polynomials (as a\nnumerically more stable polynomial basis), this allows LMI relaxations of high\norder (up to a few hundreds) to be solved numerically. \n\n"}
{"id": "1303.7173", "contents": "Title: Distributed reactive power feedback control for voltage regulation and\n  loss minimization Abstract: We consider the problem of exploiting the microgenerators dispersed in the\npower distribution network in order to provide distributed reactive power\ncompensation for power losses minimization and voltage regulation. In the\nproposed strategy, microgenerators are smart agents that can measure their\nphasorial voltage, share these data with the other agents on a cyber layer, and\nadjust the amount of reactive power injected into the grid, according to a\nfeedback control law that descends from duality-based methods applied to the\noptimal reactive power flow problem. Convergence to the configuration of\nminimum losses and feasible voltages is proved analytically for both a\nsynchronous and an asynchronous version of the algorithm, where agents update\ntheir state independently one from the other. Simulations are provided in order\nto illustrate the performance and the robustness of the algorithm, and the\ninnovative feedback nature of such strategy is discussed. \n\n"}
{"id": "1304.0678", "contents": "Title: Randomized Methods for Design of Uncertain Systems: Sample Complexity\n  and Sequential Algorithms Abstract: In this paper, we study randomized methods for feedback design of uncertain\nsystems. The first contribution is to derive the sample complexity of various\nconstrained control problems. In particular, we show the key role played by the\nbinomial distribution and related tail inequalities, and compute the sample\ncomplexity. This contribution significantly improves the existing results by\nreducing the number of required samples in the randomized algorithm. These\nresults are then applied to the analysis of worst-case performance and design\nwith robust optimization. The second contribution of the paper is to introduce\na general class of sequential algorithms, denoted as Sequential Probabilistic\nValidation (SPV). In these sequential algorithms, at each iteration, a\ncandidate solution is probabilistically validated, and corrected if necessary,\nto meet the required specifications. The results we derive provide the sample\ncomplexity which guarantees that the solutions obtained with SPV algorithms\nmeet some pre-specified probabilistic accuracy and confidence. The performance\nof these algorithms is illustrated and compared with other existing methods\nusing a numerical example dealing with robust system identification. \n\n"}
{"id": "1304.2313", "contents": "Title: On Differentially Private Filtering for Event Streams Abstract: Rigorous privacy mechanisms that can cope with dynamic data are required to\nencourage a wider adoption of large-scale monitoring and decision systems\nrelying on end-user information. A promising approach to develop these\nmechanisms is to specify quantitative privacy requirements at design time\nrather than as an afterthought, and to rely on signal processing techniques to\nachieve satisfying trade-offs between privacy and performance specifications.\nThis paper discusses, from the signal processing point of view, an event stream\nanalysis problem introduced in the database and cryptography literature. A\ndiscrete-valued input signal describes the occurrence of events contributed by\nend-users, and a system is supposed to provide some output signal based on this\ninformation, while preserving the privacy of the participants. The notion of\nprivacy adopted here is that of event-level differential privacy, which\nprovides strong privacy guarantees and has important operational advantages.\nSeveral mechanisms are described to provide differentially private output\nsignals while minimizing the impact on performance. These mechanisms\ndemonstrate the benefits of leveraging system theoretic techniques to provide\nprivacy guarantees for dynamic systems. \n\n"}
{"id": "1304.2329", "contents": "Title: Socially optimal charging strategies for electric vehicles Abstract: Electric vehicles represent a promising technology for reducing emissions and\ndependence on fossil fuels and have started entering different automotive\nmarkets. In order to bolster their adoption by consumers and hence enhance\ntheir penetration rate, a charging station infrastructure needs to be deployed.\nThis paper studies decentralized policies that assign electric vehicles to a\nnetwork of charging stations with the goal to achieve little to no queueing.\nThis objective is especially important for electric vehicles, whose charging\ntimes are fairly long. The social optimality of the proposed policies is\nestablished in the many-server regime, where each station is equipped with\nmultiple charging slots. Further, convergence issues of the algorithm that\nachieves the optimal policy are examined. Finally, the results provide insight\non how to address questions related to the optimal location deployment of the\ninfrastructure. \n\n"}
{"id": "1304.2581", "contents": "Title: Stability and performance of stochastic predictive control Abstract: This article is concerned with stability and performance of controlled\nstochastic processes under receding horizon policies. We carry out a systematic\nstudy of methods to guarantee stability under receding horizon policies via\nappropriate selections of cost functions in the underlying finite-horizon\noptimal control problem. We also obtain quantitative bounds on the performance\nof the system under receding horizon policies as measured by the long-run\nexpected average cost. The results are illustrated with the help of several\nsimple examples. \n\n"}
{"id": "1304.5286", "contents": "Title: Elastic demand dynamic network user equilibrium: Formulation, existence\n  and computation Abstract: This paper is concerned with dynamic user equilibrium with elastic travel\ndemand (E-DUE) when the trip demand matrix is determined endogenously. We\npresent an infinite-dimensional variational inequality (VI) formulation that is\nequivalent to the conditions defining a continuous-time E-DUE problem. An\nexistence result for this VI is established by applying a fixed-point existence\ntheorem (Browder, 1968) in an extended Hilbert space. We present three\nalgorithms based on the aforementioned VI and its re-expression as a\ndifferential variational inequality (DVI): a projection method, a self-adaptive\nprojection method, and a proximal point method. Rigorous convergence results\nare provided for these methods, which rely on increasingly relaxed notions of\ngeneralized monotonicity, namely mixed strongly-weakly monotonicity for the\nprojection method; pseudomonotonicity for the self-adaptive projection method,\nand quasimonotonicity for the proximal point method. These three algorithms are\ntested and their solution quality, convergence, and computational efficiency\ncompared. Our convergence results, which transcend the transportation\napplications studied here, apply to a broad family of infinite-dimensional VIs\nand DVIs, and are the weakest reported to date. \n\n"}
{"id": "1304.6800", "contents": "Title: Approximation Hardness of Graphic TSP on Cubic Graphs Abstract: We prove explicit approximation hardness results for the Graphic TSP on cubic\nand subcubic graphs as well as the new inapproximability bounds for the\ncorresponding instances of the (1,2)-TSP. The proof technique uses new modular\nconstructions of simulating gadgets for the restricted cubic and subcubic\ninstances. The modular constructions used in the paper could be also of\nindependent interest. \n\n"}
{"id": "1305.3252", "contents": "Title: Fault-tolerant control under controller-driven sampling using virtual\n  actuator strategy Abstract: We present a new output feedback fault tolerant control strategy for\ncontinuous-time linear systems. The strategy combines a digital nominal\ncontroller under controller-driven (varying) sampling with virtual-actuator\n(VA)-based controller reconfiguration to compensate for actuator faults. In the\nproposed scheme, the controller controls both the plant and the sampling\nperiod, and performs controller reconfiguration by engaging in the loop the VA\nadapted to the diagnosed fault. The VA also operates under controller-driven\nsampling. Two independent objectives are considered: (a) closed-loop stability\nwith setpoint tracking and (b) controller reconfiguration under faults. Our\nmain contribution is to extend an existing VA-based controller reconfiguration\nstrategy to systems under controller-driven sampling in such a way that if\nobjective (a) is possible under controller-driven sampling (without VA) and\nobjective (b) is possible under uniform sampling (without controller-driven\nsampling), then closed-loop stability and setpoint tracking will be preserved\nunder both healthy and faulty operation for all possible sampling rate\nevolutions that may be selected by the controller. \n\n"}
{"id": "1306.0386", "contents": "Title: Improved and Generalized Upper Bounds on the Complexity of Policy\n  Iteration Abstract: Given a Markov Decision Process (MDP) with $n$ states and a totalnumber $m$\nof actions, we study the number of iterations needed byPolicy Iteration (PI)\nalgorithms to converge to the optimal$\\gamma$-discounted policy. We consider\ntwo variations of PI: Howard'sPI that changes the actions in all states with a\npositive advantage,and Simplex-PI that only changes the action in the state\nwith maximaladvantage. We show that Howard's PI terminates after at most\n$O\\left(\\frac{m}{1-\\gamma}\\log\\left(\\frac{1}{1-\\gamma}\\right)\\right)$iterations,\nimproving by a factor $O(\\log n)$ a result by Hansen etal., while Simplex-PI\nterminates after at most\n$O\\left(\\frac{nm}{1-\\gamma}\\log\\left(\\frac{1}{1-\\gamma}\\right)\\right)$iterations,\nimproving by a factor $O(\\log n)$ a result by Ye. Undersome structural\nproperties of the MDP, we then consider bounds thatare independent of the\ndiscount factor~$\\gamma$: quantities ofinterest are bounds $\\tau\\_t$ and\n$\\tau\\_r$---uniform on all states andpolicies---respectively on the\n\\emph{expected time spent in transientstates} and \\emph{the inverse of the\nfrequency of visits in recurrentstates} given that the process starts from the\nuniform distribution.Indeed, we show that Simplex-PI terminates after at most\n$\\tilde O\\left(n^3 m^2 \\tau\\_t \\tau\\_r \\right)$ iterations. This extends\narecent result for deterministic MDPs by Post & Ye, in which $\\tau\\_t\\le 1$ and\n$\\tau\\_r \\le n$, in particular it shows that Simplex-PI isstrongly polynomial\nfor a much larger class of MDPs. We explain whysimilar results seem hard to\nderive for Howard's PI. Finally, underthe additional (restrictive) assumption\nthat the state space ispartitioned in two sets, respectively states that are\ntransient andrecurrent for all policies, we show that both Howard's PI\nandSimplex-PI terminate after at most $\\tilde\nO(m(n^2\\tau\\_t+n\\tau\\_r))$iterations. \n\n"}
{"id": "1306.2972", "contents": "Title: Synchronization-Aware and Algorithm-Efficient Chance Constrained Optimal\n  Power Flow Abstract: One of the most common control decisions faced by power system operators is\nthe question of how to dispatch generation to meet demand for power. This is a\ncomplex optimization problem that includes many nonlinear, non convex\nconstraints as well as inherent uncertainties about future demand for power and\navailable generation. In this paper we develop convex formulations to\nappropriately model crucial classes of nonlinearities and stochastic effects.\nWe focus on solving a nonlinear optimal power flow (OPF) problem that includes\nloss of synchrony constraints and models wind-farm caused fluctuations. In\nparticular, we develop (a) a convex formulation of the deterministic\nphase-difference nonlinear Optimum Power Flow (OPF) problem; and (b) a\nprobabilistic chance constrained OPF for angular stability, thermal overloads\nand generation limits that is computationally tractable. \n\n"}
{"id": "1306.3774", "contents": "Title: Under-determined linear systems and $\\ell_q$-optimization thresholds Abstract: Recent studies of under-determined linear systems of equations with sparse\nsolutions showed a great practical and theoretical efficiency of a particular\ntechnique called $\\ell_1$-optimization. Seminal works \\cite{CRT,DOnoho06CS}\nrigorously confirmed it for the first time. Namely, \\cite{CRT,DOnoho06CS}\nshowed, in a statistical context, that $\\ell_1$ technique can recover sparse\nsolutions of under-determined systems even when the sparsity is linearly\nproportional to the dimension of the system. A followup \\cite{DonohoPol} then\nprecisely characterized such a linearity through a geometric approach and a\nseries of work\\cite{StojnicCSetam09,StojnicUpper10,StojnicEquiv10} reaffirmed\nstatements of \\cite{DonohoPol} through a purely probabilistic approach. A\ntheoretically interesting alternative to $\\ell_1$ is a more general version\ncalled $\\ell_q$ (with an essentially arbitrary $q$). While $\\ell_1$ is\ntypically considered as a first available convex relaxation of sparsity norm\n$\\ell_0$, $\\ell_q,0\\leq q\\leq 1$, albeit non-convex, should technically be a\ntighter relaxation of $\\ell_0$. Even though developing polynomial (or close to\nbe polynomial) algorithms for non-convex problems is still in its initial\nphases one may wonder what would be the limits of an $\\ell_q,0\\leq q\\leq 1$,\nrelaxation even if at some point one can develop algorithms that could handle\nits non-convexity. A collection of answers to this and a few realted questions\nis precisely what we present in this paper. \n\n"}
{"id": "1307.1940", "contents": "Title: Reinforcing Power Grid Transmission with FACTS Devices Abstract: We explore optimization methods for planning the placement, sizing and\noperations of Flexible Alternating Current Transmission System (FACTS) devices\ninstalled into the grid to relieve congestion created by load growth or\nfluctuations of intermittent renewable generation. We limit our selection of\nFACTS devices to those that can be represented by modification of the\ninductance of the transmission lines. Our master optimization problem minimizes\nthe $l_1$ norm of the FACTS-associated inductance correction subject to\nconstraints enforcing that no line of the system exceeds its thermal limit. We\ndevelop off-line heuristics that reduce this non-convex optimization to a\nsuccession of Linear Programs (LP) where at each step the constraints are\nlinearized analytically around the current operating point. The algorithm is\naccelerated further with a version of the cutting plane method greatly reducing\nthe number of active constraints during the optimization, while checking\nfeasibility of the non-active constraints post-factum. This hybrid algorithm\nsolves a typical single-contingency problem over the MathPower Polish Grid\nmodel (3299 lines and 2746 nodes) in 40 seconds per iteration on a standard\nlaptop---a speed up that allows the sizing and placement of a family of FACTS\ndevices to correct a large set of anticipated contingencies. From testing of\nmultiple examples, we observe that our algorithm finds feasible solutions that\nare always sparse, i.e., FACTS devices are placed on only a few lines. The\noptimal FACTS are not always placed on the originally congested lines, however\ntypically the correction(s) is made at line(s) positioned in a relative\nproximity of the overload line(s). \n\n"}
{"id": "1307.4847", "contents": "Title: Efficient Reinforcement Learning in Deterministic Systems with Value\n  Function Generalization Abstract: We consider the problem of reinforcement learning over episodes of a\nfinite-horizon deterministic system and as a solution propose optimistic\nconstraint propagation (OCP), an algorithm designed to synthesize efficient\nexploration and value function generalization. We establish that when the true\nvalue function lies within a given hypothesis class, OCP selects optimal\nactions over all but at most K episodes, where K is the eluder dimension of the\ngiven hypothesis class. We establish further efficiency and asymptotic\nperformance guarantees that apply even if the true value function does not lie\nin the given hypothesis class, for the special case where the hypothesis class\nis the span of pre-specified indicator functions over disjoint sets. We also\ndiscuss the computational complexity of OCP and present computational results\ninvolving two illustrative examples. \n\n"}
{"id": "1307.5944", "contents": "Title: Online Optimization in Dynamic Environments Abstract: High-velocity streams of high-dimensional data pose significant \"big data\"\nanalysis challenges across a range of applications and settings. Online\nlearning and online convex programming play a significant role in the rapid\nrecovery of important or anomalous information from these large datastreams.\nWhile recent advances in online learning have led to novel and rapidly\nconverging algorithms, these methods are unable to adapt to nonstationary\nenvironments arising in real-world problems. This paper describes a dynamic\nmirror descent framework which addresses this challenge, yielding low\ntheoretical regret bounds and accurate, adaptive, and computationally efficient\nalgorithms which are applicable to broad classes of problems. The methods are\ncapable of learning and adapting to an underlying and possibly time-varying\ndynamical model. Empirical results in the context of dynamic texture analysis,\nsolar flare detection, sequential compressed sensing of a dynamic scene,\ntraffic surveillance,tracking self-exciting point processes and network\nbehavior in the Enron email corpus support the core theoretical findings. \n\n"}
{"id": "1308.4585", "contents": "Title: Fractional Calculus of Variations of Several Independent Variables Abstract: We prove multidimensional integration by parts formulas for generalized\nfractional derivatives and integrals. The new results allow us to obtain\noptimality conditions for multidimensional fractional variational problems with\nLagrangians depending on generalized partial integrals and derivatives. A\ngeneralized fractional Noether's theorem, a formulation of Dirichlet's\nprinciple and an uniqueness result are given. \n\n"}
{"id": "1308.5964", "contents": "Title: Automated, Credible Autocoding of An Unmanned Aggressive Maneuvering Car\n  Controller Abstract: This article describes the application of a credible autocoding framework for\ncontrol systems towards a nonlinear car controller example. The framework\ngenerates code, along with guarantees of high level functional properties about\nthe code that can be independently verified. These high-level functional\nproperties not only serves as a certificate of good system behvaior but also\ncan be used to guarantee the absence of runtime errors. In one of our previous\nworks, we have constructed a prototype autocoder with proofs that demonstrates\nthis framework in a fully automatic fashion for linear and quasi-nonlinear\ncontrollers. With the nonlinear car example, we propose to further extend the\nprototype's dataflow annotation language environment with with several new\nannotation symbols to enable the expression of general predicates and dynamical\nsystems. We demonstrate manually how the new extensions to the prototype\nautocoder work on the car controller using the output language Matlab. Finally,\nwe discuss the requirements and scalability issues of the automatic analysis\nand verification of the documented output code. \n\n"}
{"id": "1308.6833", "contents": "Title: Stability of Polynomial Differential Equations: Complexity and Converse\n  Lyapunov Questions Abstract: We consider polynomial differential equations and make a number of\ncontributions to the questions of (i) complexity of deciding stability, (ii)\nexistence of polynomial Lyapunov functions, and (iii) existence of sum of\nsquares (sos) Lyapunov functions.\n  (i) We show that deciding local or global asymptotic stability of cubic\nvector fields is strongly NP-hard. Simple variations of our proof are shown to\nimply strong NP-hardness of several other decision problems: testing local\nattractivity of an equilibrium point, stability of an equilibrium point in the\nsense of Lyapunov, invariance of the unit ball, boundedness of trajectories,\nconvergence of all trajectories in a ball to a given equilibrium point,\nexistence of a quadratic Lyapunov function, local collision avoidance, and\nexistence of a stabilizing control law.\n  (ii) We present a simple, explicit example of a globally asymptotically\nstable quadratic vector field on the plane which does not admit a polynomial\nLyapunov function (joint work with M. Krstic). For the subclass of homogeneous\nvector fields, we conjecture that asymptotic stability implies existence of a\npolynomial Lyapunov function, but show that the minimum degree of such a\nLyapunov function can be arbitrarily large even for vector fields in fixed\ndimension and degree. For the same class of vector fields, we further establish\nthat there is no monotonicity in the degree of polynomial Lyapunov functions.\n  (iii) We show via an explicit counterexample that if the degree of the\npolynomial Lyapunov function is fixed, then sos programming may fail to find a\nvalid Lyapunov function even though one exists. On the other hand, if the\ndegree is allowed to increase, we prove that existence of a polynomial Lyapunov\nfunction for a planar or a homogeneous vector field implies existence of a\npolynomial Lyapunov function that is sos and that the negative of its\nderivative is also sos. \n\n"}
{"id": "1309.0474", "contents": "Title: Smooth solutions to portfolio liquidation problems under price-sensitive\n  market impact Abstract: We consider the stochastic control problem of a financial trader that needs\nto unwind a large asset portfolio within a short period of time. The trader can\nsimultaneously submit active orders to a primary market and passive orders to a\ndark pool. Our framework is flexible enough to allow for price-dependent impact\nfunctions describing the trading costs in the primary market and\nprice-dependent adverse selection costs associated with dark pool trading. We\nprove that the value function can be characterized in terms of the unique\nsmooth solution to a PDE with singular terminal value, establish its explicit\nasymptotic behavior at the terminal time, and give the optimal trading strategy\nin feedback form. \n\n"}
{"id": "1309.0578", "contents": "Title: Coherent-Classical Estimation for Quantum Linear Systems Abstract: This paper introduces a problem of coherent-classical estimation for a class\nof linear quantum systems. In this problem, the estimator is a mixed\nquantum-classical system which produces a classical estimate of a system\nvariable. The coherent-classical estimator may also involve coherent feedback.\nAn example involving optical squeezers is given to illustrate the efficacy of\nthis idea. \n\n"}
{"id": "1309.1349", "contents": "Title: Ergodic Randomized Algorithms and Dynamics over Networks Abstract: Algorithms and dynamics over networks often involve randomization, and\nrandomization may result in oscillating dynamics which fail to converge in a\ndeterministic sense. In this paper, we observe this undesired feature in three\napplications, in which the dynamics is the randomized asynchronous counterpart\nof a well-behaved synchronous one. These three applications are network\nlocalization, PageRank computation, and opinion dynamics. Motivated by their\nformal similarity, we show the following general fact, under the assumptions of\nindependence across time and linearities of the updates: if the expected\ndynamics is stable and converges to the same limit of the original synchronous\ndynamics, then the oscillations are ergodic and the desired limit can be\nlocally recovered via time-averaging. \n\n"}
{"id": "1309.5823", "contents": "Title: A Kernel Classification Framework for Metric Learning Abstract: Learning a distance metric from the given training samples plays a crucial\nrole in many machine learning tasks, and various models and optimization\nalgorithms have been proposed in the past decade. In this paper, we generalize\nseveral state-of-the-art metric learning methods, such as large margin nearest\nneighbor (LMNN) and information theoretic metric learning (ITML), into a kernel\nclassification framework. First, doublets and triplets are constructed from the\ntraining samples, and a family of degree-2 polynomial kernel functions are\nproposed for pairs of doublets or triplets. Then, a kernel classification\nframework is established, which can not only generalize many popular metric\nlearning methods such as LMNN and ITML, but also suggest new metric learning\nmethods, which can be efficiently implemented, interestingly, by using the\nstandard support vector machine (SVM) solvers. Two novel metric learning\nmethods, namely doublet-SVM and triplet-SVM, are then developed under the\nproposed framework. Experimental results show that doublet-SVM and triplet-SVM\nachieve competitive classification accuracies with state-of-the-art metric\nlearning methods such as ITML and LMNN but with significantly less training\ntime. \n\n"}
{"id": "1309.7367", "contents": "Title: Stochastic Online Shortest Path Routing: The Value of Feedback Abstract: This paper studies online shortest path routing over multi-hop networks. Link\ncosts or delays are time-varying and modeled by independent and identically\ndistributed random processes, whose parameters are initially unknown. The\nparameters, and hence the optimal path, can only be estimated by routing\npackets through the network and observing the realized delays. Our aim is to\nfind a routing policy that minimizes the regret (the cumulative difference of\nexpected delay) between the path chosen by the policy and the unknown optimal\npath. We formulate the problem as a combinatorial bandit optimization problem\nand consider several scenarios that differ in where routing decisions are made\nand in the information available when making the decisions. For each scenario,\nwe derive a tight asymptotic lower bound on the regret that has to be satisfied\nby any online routing policy. These bounds help us to understand the\nperformance improvements we can expect when (i) taking routing decisions at\neach hop rather than at the source only, and (ii) observing per-link delays\nrather than end-to-end path delays. In particular, we show that (i) is of no\nuse while (ii) can have a spectacular impact. Three algorithms, with a\ntrade-off between computational complexity and performance, are proposed. The\nregret upper bounds of these algorithms improve over those of the existing\nalgorithms, and they significantly outperform state-of-the-art algorithms in\nnumerical experiments. \n\n"}
{"id": "1310.0432", "contents": "Title: Online Learning of Dynamic Parameters in Social Networks Abstract: This paper addresses the problem of online learning in a dynamic setting. We\nconsider a social network in which each individual observes a private signal\nabout the underlying state of the world and communicates with her neighbors at\neach time period. Unlike many existing approaches, the underlying state is\ndynamic, and evolves according to a geometric random walk. We view the scenario\nas an optimization problem where agents aim to learn the true state while\nsuffering the smallest possible loss. Based on the decomposition of the global\nloss function, we introduce two update mechanisms, each of which generates an\nestimate of the true state. We establish a tight bound on the rate of change of\nthe underlying state, under which individuals can track the parameter with a\nbounded variance. Then, we characterize explicit expressions for the steady\nstate mean-square deviation(MSD) of the estimates from the truth, per\nindividual. We observe that only one of the estimators recovers the optimal\nMSD, which underscores the impact of the objective function decomposition on\nthe learning quality. Finally, we provide an upper bound on the regret of the\nproposed methods, measured as an average of errors in estimating the parameter\nin a finite time. \n\n"}
{"id": "1310.5748", "contents": "Title: Optimal Distributed Control of Reactive Power via the Alternating\n  Direction Method of Multipliers Abstract: We formulate the control of reactive power generation by photovoltaic\ninverters in a power distribution circuit as a constrained optimization that\naims to minimize reactive power losses subject to finite inverter capacity and\nupper and lower voltage limits at all nodes in the circuit. When voltage\nvariations along the circuit are small and losses of both real and reactive\npowers are small compared to the respective flows, the resulting optimization\nproblem is convex. Moreover, the cost function is separable enabling a\ndistributed, on-line implementation with node-local computations using only\nlocal measurements augmented with limited information from the neighboring\nnodes communicated over cyber channels. Such an approach lies between the fully\ncentralized and local policy approaches previously considered. We explore\nprotocols based on the dual ascent method and on the Alternating Direction\nMethod of Multipliers (ADMM) and find that the ADMM protocol performs\nsignificantly better. \n\n"}
{"id": "1311.0701", "contents": "Title: On Fast Dropout and its Applicability to Recurrent Networks Abstract: Recurrent Neural Networks (RNNs) are rich models for the processing of\nsequential data. Recent work on advancing the state of the art has been focused\non the optimization or modelling of RNNs, mostly motivated by adressing the\nproblems of the vanishing and exploding gradients. The control of overfitting\nhas seen considerably less attention. This paper contributes to that by\nanalyzing fast dropout, a recent regularization method for generalized linear\nmodels and neural networks from a back-propagation inspired perspective. We\nshow that fast dropout implements a quadratic form of an adaptive,\nper-parameter regularizer, which rewards large weights in the light of\nunderfitting, penalizes them for overconfident predictions and vanishes at\nminima of an unregularized training loss. The derivatives of that regularizer\nare exclusively based on the training error signal. One consequence of this is\nthe absense of a global weight attractor, which is particularly appealing for\nRNNs, since the dynamics are not biased towards a certain regime. We positively\ntest the hypothesis that this improves the performance of RNNs on four musical\ndata sets. \n\n"}
{"id": "1311.1885", "contents": "Title: Verifiable Control System Development for Gas Turbine Engines Abstract: A control software verification framework for gas turbine engines is\ndeveloped. A stability proof is presented for gain scheduled closed-loop engine\nsystem based on global linearization and linear matrix inequality (LMI)\ntechniques. Using convex optimization tools, a single quadratic Lyapunov\nfunction is computed for multiple linearizations near equilibrium points of the\nclosed-loop system. With the computed stability matrices, ellipsoid invariant\nsets are constructed, which are used efficiently for DGEN turbofan engine\ncontrol code stability analysis. Then a verifiable linear gain scheduled\ncontroller for DGEN engine is developed based on formal methods, and tested on\nthe engine virtual test bench. Simulation results show that the developed\nverifiable gain scheduled controller is capable of regulating the engine in a\nstable fashion with proper tracking performance. \n\n"}
{"id": "1311.2009", "contents": "Title: On conjugate times of LQ optimal control problems Abstract: Motivated by the study of linear quadratic optimal control problems, we\nconsider a dynamical system with a constant, quadratic Hamiltonian, and we\ncharacterize the number of conjugate times in terms of the spectrum of the\nHamiltonian vector field $\\vec{H}$. We prove the following dichotomy: the\nnumber of conjugate times is identically zero or grows to infinity. The latter\ncase occurs if and only if $\\vec{H}$ has at least one Jordan block of odd\ndimension corresponding to a purely imaginary eigenvalue. As a byproduct, we\nobtain bounds from below on the number of conjugate times contained in an\ninterval in terms of the spectrum of $\\vec{H}$. \n\n"}
{"id": "1311.2796", "contents": "Title: Mixed Human-Robot Team Surveillance Abstract: We study the mixed human-robot team design in a system theoretic setting\nusing the context of a surveillance mission. The three key coupled components\nof a mixed team design are (i) policies for the human operator, (ii) policies\nto account for erroneous human decisions, and (iii) policies to control the\nautomaton. In this paper, we survey elements of human decision-making,\nincluding evidence aggregation, situational awareness, fatigue, and memory\neffects. We bring together the models for these elements in human\ndecision-making to develop a single coherent model for human decision-making in\na two-alternative choice task. We utilize the developed model to design\nefficient attention allocation policies for the human operator. We propose an\nanomaly detection algorithm that utilizes potentially erroneous decision by the\noperator to ascertain an anomalous region among the set of regions surveilled.\nFinally, we propose a stochastic vehicle routing policy that surveils an\nanomalous region with high probability. Our mixed team design relies on the\ncertainty-equivalent receding-horizon control framework. \n\n"}
{"id": "1311.4291", "contents": "Title: Minimum $n$-Rank Approximation via Iterative Hard Thresholding Abstract: The problem of recovering a low $n$-rank tensor is an extension of sparse\nrecovery problem from the low dimensional space (matrix space) to the high\ndimensional space (tensor space) and has many applications in computer vision\nand graphics such as image inpainting and video inpainting. In this paper, we\nconsider a new tensor recovery model, named as minimum $n$-rank approximation\n(MnRA), and propose an appropriate iterative hard thresholding algorithm with\ngiving the upper bound of the $n$-rank in advance. The convergence analysis of\nthe proposed algorithm is also presented. Particularly, we show that for the\nnoiseless case, the linear convergence with rate $\\frac{1}{2}$ can be obtained\nfor the proposed algorithm under proper conditions. Additionally, combining an\neffective heuristic for determining $n$-rank, we can also apply the proposed\nalgorithm to solve MnRA when $n$-rank is unknown in advance. Some preliminary\nnumerical results on randomly generated and real low $n$-rank tensor completion\nproblems are reported, which show the efficiency of the proposed algorithms. \n\n"}
{"id": "1311.7099", "contents": "Title: Probabilistic and Set-based Model Invalidation and Estimation using LMIs Abstract: Probabilistic and set-based methods are two approaches for model\ninvalidation, parameter and state estimation. Both classes of methods use\ndifferent types of data, i.e. deterministic or probabilistic data, which allow\ndifferent statements and applications. Ideally, however, all available data\nshould be used in estimation and model invalidation methods. This paper\npresents an estimation and model invalidation framework combining set-based and\nprobabilistically uncertain data for polynomial continuous-time systems. In\nparticular, uncertain data on the moments and the support is used without the\nneed to make explicit assumptions on the type of probability densities. The\npaper derives pointwise-in-time outer approximations of the moments of the\nprobability densities associated with the states and parameters of the system.\nThese approximations can be interpreted as guaranteed confidence intervals for\nthe moment estimates. Furthermore, guaranteed bounds on the probability masses\non subsets are derived and allow an estimation of the unknown probability\ndensities. To calculate the estimates, the dynamics of the probability\ndensities of the state trajectories are found by occupation measures of the\nnonlinear dynamics. This allows the construction of an infinite-dimensional\nlinear program which incorporates the set- and moment-based data. This linear\nprogram is relaxed by a hierarchy of LMI problems providing, as shown\nelsewhere, an almost uniformly convergent sequence of outer approximations of\nthe estimated sets. The approach is demonstrated with numerical examples. \n\n"}
{"id": "1311.7130", "contents": "Title: Convex Optimal Uncertainty Quantification Abstract: Optimal uncertainty quantification (OUQ) is a framework for numerical\nextreme-case analysis of stochastic systems with imperfect knowledge of the\nunderlying probability distribution. This paper presents sufficient conditions\nunder which an OUQ problem can be reformulated as a finite-dimensional convex\noptimization problem, for which efficient numerical solutions can be obtained.\nThe sufficient conditions include that the objective function is piecewise\nconcave and the constraints are piecewise convex. In particular, we show that\npiecewise concave objective functions may appear in applications where the\nobjective is defined by the optimal value of a parameterized linear program. \n\n"}
{"id": "1312.0336", "contents": "Title: A Unifying Framework for the Electrical Structure-Based Approach to PMU\n  Placement in Electric Power Systems Abstract: The electrical structure of the power grid is utilized to address the phasor\nmeasurement unit (PMU) placement problem. First, we derive the connectivity\nmatrix of the network using the resistance distance metric and employ it in the\nlinear program formulation to obtain the optimal number of PMUs, for complete\nnetwork observability without zero injection measurements. This approach was\ndeveloped by the author in an earlier work, but the solution methodology to\naddress the location problem did not fully utilize the electrical properties of\nthe network, resulting in an ambiguity. In this paper, we settle this issue by\nexploiting the coupling structure of the grid derived using the singular value\ndecomposition (SVD)-based analysis of the resistance distance matrix to solve\nthe location problem. Our study, which is based on recent advances in complex\nnetworks that promote the electrical structure of the grid over its topological\nstructure and the SVD analysis which throws light on the electrical coupling of\nthe network, results in a unified framework for the electrical structure-based\nPMU placement. The proposed method is tested on IEEE bus systems, and the\nresults uncover intriguing connections between the singular vectors and average\nresistance distance between buses in the network. \n\n"}
{"id": "1312.2039", "contents": "Title: Active Classification for POMDPs: a Kalman-like State Estimator Abstract: The problem of state tracking with active observation control is considered\nfor a system modeled by a discrete-time, finite-state Markov chain observed\nthrough conditionally Gaussian measurement vectors. The measurement model\nstatistics are shaped by the underlying state and an exogenous control input,\nwhich influence the observations' quality. Exploiting an innovations approach,\nan approximate minimum mean-squared error (MMSE) filter is derived to estimate\nthe Markov chain system state. To optimize the control strategy, the associated\nmean-squared error is used as an optimization criterion in a partially\nobservable Markov decision process formulation. A stochastic dynamic\nprogramming algorithm is proposed to solve for the optimal solution. To enhance\nthe quality of system state estimates, approximate MMSE smoothing estimators\nare also derived. Finally, the performance of the proposed framework is\nillustrated on the problem of physical activity detection in wireless body\nsensing networks. The power of the proposed framework lies within its ability\nto accommodate a broad spectrum of active classification applications including\nsensor management for object classification and tracking, estimation of sparse\nsignals and radar scheduling. \n\n"}
{"id": "1312.2482", "contents": "Title: Automatic recognition and tagging of topologically different regimes in\n  dynamical systems Abstract: Complex systems are commonly modeled using nonlinear dynamical systems. These\nmodels are often high-dimensional and chaotic. An important goal in studying\nphysical systems through the lens of mathematical models is to determine when\nthe system undergoes changes in qualitative behavior. A detailed description of\nthe dynamics can be difficult or impossible to obtain for high-dimensional and\nchaotic systems. Therefore, a more sensible goal is to recognize and mark\ntransitions of a system between qualitatively different regimes of behavior. In\npractice, one is interested in developing techniques for detection of such\ntransitions from sparse observations, possibly contaminated by noise. In this\npaper we develop a framework to accurately tag different regimes of complex\nsystems based on topological features. In particular, our framework works with\na high degree of success in picking out a cyclically orbiting regime from a\nstationary equilibrium regime in high-dimensional stochastic dynamical systems. \n\n"}
{"id": "1312.4885", "contents": "Title: Rolling Manifolds of Different Dimensions Abstract: If $(M,g)$ and $(\\hM,\\hg)$ are two smooth connected complete oriented\nRiemannian manifolds of dimensions $n$ and $\\hn$ respectively, we model the\nrolling of $(M,g)$ onto $(\\hM,\\hg)$ as a driftless control affine systems\ndescribing two possible constraints of motion: the first rolling motion\n$\\Sigma_{NS}$ captures the no-spinning condition only and the second rolling\nmotion $\\Sigma_{R}$ corresponds to rolling without spinning nor slipping. Two\ndistributions of dimensions $(n + \\hn)$ and $n$, respectively, are then\nassociated to the rolling motions $\\Sigma_{NS}$ and $\\Sigma_{R}$ respectively.\nThis generalizes the rolling problems considered in \\cite{ChitourKokkonen1}\nwhere both manifolds had the same dimension. The controllability issue is then\naddressed for both $\\Sigma_{NS}$ and $\\Sigma_{R}$ and completely solved for\n$\\Sigma_{NS}$. As regards to $\\Sigma_{R}$, basic properties for the reachable\nsets are provided as well as the complete study of the case $(n,\\hn)=(3,2)$ and\nsome sufficient conditions for non-controllability. \n\n"}
{"id": "1312.5371", "contents": "Title: Investigating early warning signs of oscillatory instability in\n  simulated phasor measurements Abstract: This paper shows that the variance of load bus voltage magnitude in a small\npower system test case increases monotonically as the system approaches a Hopf\nbifurcation. This property can potentially be used as a method for monitoring\noscillatory stability in power grid using high-resolution phasor measurements.\nIncreasing variance in data from a dynamical system is a common sign of a\nphenomenon known as critical slowing down (CSD). CSD is slower recovery of\ndynamical systems from perturbations as they approach critical transitions.\nEarlier work has focused on studying CSD in systems approaching voltage\ncollapse; In this paper, we investigate its occurrence as a power system\napproaches a Hopf bifurcation. \n\n"}
{"id": "1312.5434", "contents": "Title: Asynchronous Adaptation and Learning over Networks --- Part I: Modeling\n  and Stability Analysis Abstract: In this work and the supporting Parts II [2] and III [3], we provide a rather\ndetailed analysis of the stability and performance of asynchronous strategies\nfor solving distributed optimization and adaptation problems over networks. We\nexamine asynchronous networks that are subject to fairly general sources of\nuncertainties, such as changing topologies, random link failures, random data\narrival times, and agents turning on and off randomly. Under this model, agents\nin the network may stop updating their solutions or may stop sending or\nreceiving information in a random manner and without coordination with other\nagents. We establish in Part I conditions on the first and second-order moments\nof the relevant parameter distributions to ensure mean-square stable behavior.\nWe derive in Part II expressions that reveal how the various parameters of the\nasynchronous behavior influence network performance. We compare in Part III the\nperformance of asynchronous networks to the performance of both centralized\nsolutions and synchronous networks. One notable conclusion is that the\nmean-square-error performance of asynchronous networks shows a degradation only\nof the order of $O(\\nu)$, where $\\nu$ is a small step-size parameter, while the\nconvergence rate remains largely unaltered. The results provide a solid\njustification for the remarkable resilience of cooperative networks in the face\nof random failures at multiple levels: agents, links, data arrivals, and\ntopology. \n\n"}
{"id": "1312.6421", "contents": "Title: Output Synchronization of Nonlinear Systems under Input Disturbances Abstract: We study synchronization of nonlinear systems that satisfy an incremental\npassivity property. We consider the case where the control input is subject to\na class of disturbances, including constant and sinusoidal disturbances with\nunknown phases and magnitudes and known frequencies. We design a distributed\ncontrol law that recovers the synchronization of the nonlinear systems in the\npresence of the disturbances. Simulation results of Goodwin oscillators\nillustrate the effectiveness of the control law. Finally, we highlight the\nconnection of the proposed control law to the dynamic average consensus\nestimator developed in [1]. \n\n"}
{"id": "1312.6677", "contents": "Title: Path Finding I :Solving Linear Programs with \\~O(sqrt(rank)) Linear\n  System Solves Abstract: In this paper we present a new algorithm for solving linear programs that\nrequires only $\\tilde{O}(\\sqrt{rank(A)}L)$ iterations to solve a linear program\nwith $m$ constraints, $n$ variables, and constraint matrix $A$, and bit\ncomplexity $L$. Each iteration of our method consists of solving $\\tilde{O}(1)$\nlinear systems and additional nearly linear time computation.\n  Our method improves upon the previous best iteration bound by factor of\n$\\tilde{\\Omega}((m/rank(A))^{1/4})$ for methods with polynomial time computable\niterations and by $\\tilde{\\Omega}((m/rank(A))^{1/2})$ for methods which solve\nat most $\\tilde{O}(1)$ linear systems in each iteration. Our method is\nparallelizable and amenable to linear algebraic techniques for accelerating the\nlinear system solver. As such, up to polylogarithmic factors we either match or\nimprove upon the best previous running times in both depth and work for\ndifferent ratios of $m$ and $rank(A)$.\n  Moreover, our method matches up to polylogarithmic factors a theoretical\nlimit established by Nesterov and Nemirovski in 1994 regarding the use of a\n\"universal barrier\" for interior point methods, thereby resolving a\nlong-standing open question regarding the running time of polynomial time\ninterior point methods for linear programming. \n\n"}
{"id": "1312.7323", "contents": "Title: Norm Convergence of Realistic Projection and Reflection Methods Abstract: We provide sufficient conditions for norm convergence of various projection\nand reflection methods, as well as giving limiting examples regarding\nconvergence rates. \n\n"}
{"id": "1312.7580", "contents": "Title: On the Learning Behavior of Adaptive Networks - Part II: Performance\n  Analysis Abstract: Part I of this work examined the mean-square stability and convergence of the\nlearning process of distributed strategies over graphs. The results identified\nconditions on the network topology, utilities, and data in order to ensure\nstability; the results also identified three distinct stages in the learning\nbehavior of multi-agent networks related to transient phases I and II and the\nsteady-state phase. This Part II examines the steady-state phase of distributed\nlearning by networked agents. Apart from characterizing the performance of the\nindividual agents, it is shown that the network induces a useful equalization\neffect across all agents. In this way, the performance of noisier agents is\nenhanced to the same level as the performance of agents with less noisy data.\nIt is further shown that in the small step-size regime, each agent in the\nnetwork is able to achieve the same performance level as that of a centralized\nstrategy corresponding to a fully connected network. The results in this part\nreveal explicitly which aspects of the network topology and operation influence\nperformance and provide important insights into the design of effective\nmechanisms for the processing and diffusion of information over networks. \n\n"}
{"id": "1312.7581", "contents": "Title: On the Learning Behavior of Adaptive Networks - Part I: Transient\n  Analysis Abstract: This work carries out a detailed transient analysis of the learning behavior\nof multi-agent networks, and reveals interesting results about the learning\nabilities of distributed strategies. Among other results, the analysis reveals\nhow combination policies influence the learning process of networked agents,\nand how these policies can steer the convergence point towards any of many\npossible Pareto optimal solutions. The results also establish that the learning\nprocess of an adaptive network undergoes three (rather than two) well-defined\nstages of evolution with distinctive convergence rates during the first two\nstages, while attaining a finite mean-square-error (MSE) level in the last\nstage. The analysis reveals what aspects of the network topology influence\nperformance directly and suggests design procedures that can optimize\nperformance by adjusting the relevant topology parameters. Interestingly, it is\nfurther shown that, in the adaptation regime, each agent in a sparsely\nconnected network is able to achieve the same performance level as that of a\ncentralized stochastic-gradient strategy even for left-stochastic combination\nstrategies. These results lead to a deeper understanding and useful insights on\nthe convergence behavior of coupled distributed learners. The results also lead\nto effective design mechanisms to help diffuse information more thoroughly over\nnetworks. \n\n"}
{"id": "1312.7602", "contents": "Title: A Martingale Approach and Time-Consistent Sampling-based Algorithms for\n  Risk Management in Stochastic Optimal Control Abstract: In this paper, we consider a class of stochastic optimal control problems\nwith risk constraints that are expressed as bounded probabilities of failure\nfor particular initial states. We present here a martingale approach that\ndiffuses a risk constraint into a martingale to construct time-consistent\ncontrol policies. The martingale stands for the level of risk tolerance over\ntime. By augmenting the system dynamics with the controlled martingale, the\noriginal risk-constrained problem is transformed into a stochastic target\nproblem. We extend the incremental Markov Decision Process (iMDP) algorithm to\napproximate arbitrarily well an optimal feedback policy of the original problem\nby sampling in the augmented state space and computing proper boundary\nconditions for the reformulated problem. We show that the algorithm is both\nprobabilistically sound and asymptotically optimal. The performance of the\nproposed algorithm is demonstrated on motion planning and control problems\nsubject to bounded probability of collision in uncertain cluttered\nenvironments. \n\n"}
{"id": "1401.1549", "contents": "Title: Optimal Demand Response Using Device Based Reinforcement Learning Abstract: Demand response (DR) for residential and small commercial buildings is\nestimated to account for as much as 65% of the total energy savings potential\nof DR, and previous work shows that a fully automated Energy Management System\n(EMS) is a necessary prerequisite to DR in these areas. In this paper, we\npropose a novel EMS formulation for DR problems in these sectors. Specifically,\nwe formulate a fully automated EMS's rescheduling problem as a reinforcement\nlearning (RL) problem, and argue that this RL problem can be approximately\nsolved by decomposing it over device clusters. Compared with existing\nformulations, our new formulation (1) does not require explicitly modeling the\nuser's dissatisfaction on job rescheduling, (2) enables the EMS to\nself-initiate jobs, (3) allows the user to initiate more flexible requests and\n(4) has a computational complexity linear in the number of devices. We also\ndemonstrate the simulation results of applying Q-learning, one of the most\npopular and classical RL algorithms, to a representative example. \n\n"}
{"id": "1401.1876", "contents": "Title: Equivalent relaxations of optimal power flow Abstract: Several convex relaxations of the optimal power flow (OPF) problem have\nrecently been developed using both bus injection models and branch flow models.\nIn this paper, we prove relations among three convex relaxations: a\nsemidefinite relaxation that computes a full matrix, a chordal relaxation based\non a chordal extension of the network graph, and a second-order cone relaxation\nthat computes the smallest partial matrix. We prove a bijection between the\nfeasible sets of the OPF in the bus injection model and the branch flow model,\nestablishing the equivalence of these two models and their second-order cone\nrelaxations. Our results imply that, for radial networks, all these relaxations\nare equivalent and one should always solve the second-order cone relaxation.\nFor mesh networks, the semidefinite relaxation is tighter than the second-order\ncone relaxation but requires a heavier computational effort, and the chordal\nrelaxation strikes a good balance. Simulations are used to illustrate these\nresults. \n\n"}
{"id": "1401.5492", "contents": "Title: Alternating direction method of multipliers for penalized zero-variance\n  discriminant analysis Abstract: We consider the task of classification in the high dimensional setting where\nthe number of features of the given data is significantly greater than the\nnumber of observations. To accomplish this task, we propose a heuristic, called\nsparse zero-variance discriminant analysis (SZVD), for simultaneously\nperforming linear discriminant analysis and feature selection on high\ndimensional data. This method combines classical zero-variance discriminant\nanalysis, where discriminant vectors are identified in the null space of the\nsample within-class covariance matrix, with penalization applied to induce\nsparse structures in the resulting vectors. To approximately solve the\nresulting nonconvex problem, we develop a simple algorithm based on the\nalternating direction method of multipliers. Further, we show that this\nalgorithm is applicable to a larger class of penalized generalized eigenvalue\nproblems, including a particular relaxation of the sparse principal component\nanalysis problem. Finally, we establish theoretical guarantees for convergence\nof our algorithm to stationary points of the original nonconvex problem, and\nempirically demonstrate the effectiveness of our heuristic for classifying\nsimulated data and data drawn from applications in time-series classification. \n\n"}
{"id": "1401.6396", "contents": "Title: Symbolic Abstractions of Networked Control Systems Abstract: The last decade has witnessed significant attention on networked control\nsystems (NCS) due to their ubiquitous presence in industrial applications, and,\nin the particular case of wireless NCS, because of their architectural\nflexibility and low installation and maintenance costs. In wireless NCS the\ncommunication between sensors, controllers, and actuators is supported by a\ncommunication channel that is likely to introduce variable communication\ndelays, packet losses, limited bandwidth, and other practical non-idealities\nleading to numerous technical challenges. Although stability properties of NCS\nhave been investigated extensively in the literature, results for NCS under\nmore complex and general objectives, and in particular results dealing with\nverification or controller synthesis for logical specifications, are much more\nlimited. This work investigates how to address such complex objectives by\nconstructively deriving symbolic models of NCS, while encompassing the\nmentioned network non-idealities. The obtained abstracted (symbolic) models can\nthen be employed to synthesize hybrid controllers enforcing rich logical\nspecifications over the concrete NCS models. Examples of such general\nspecifications include properties expressed as formulae in linear temporal\nlogic (LTL) or as automata on infinite strings. We thus provide a general\nsynthesis framework that can be flexibly adapted to a number of NCS setups. We\nillustrate the effectiveness of the results over some case studies. \n\n"}
{"id": "1401.7079", "contents": "Title: A Block Successive Upper Bound Minimization Method of Multipliers for\n  Linearly Constrained Convex Optimization Abstract: Consider the problem of minimizing the sum of a smooth convex function and a\nseparable nonsmooth convex function subject to linear coupling constraints.\nProblems of this form arise in many contemporary applications including signal\nprocessing, wireless networking and smart grid provisioning. Motivated by the\nhuge size of these applications, we propose a new class of first order\nprimal-dual algorithms called the block successive upper-bound minimization\nmethod of multipliers (BSUM-M) to solve this family of problems. The BSUM-M\nupdates the primal variable blocks successively by minimizing locally tight\nupper-bounds of the augmented Lagrangian of the original problem, followed by a\ngradient type update for the dual variable in closed form. We show that under\ncertain regularity conditions, and when the primal block variables are updated\nin either a deterministic or a random fashion, the BSUM-M converges to the set\nof optimal solutions. Moreover, in the absence of linear constraints, we show\nthat the BSUM-M, which reduces to the block successive upper-bound minimization\n(BSUM) method, is capable of linear convergence without strong convexity. \n\n"}
{"id": "1401.7279", "contents": "Title: Optimal control and numerical software: an overview Abstract: Optimal Control (OC) is the process of determining control and state\ntrajectories for a dynamic system, over a period of time, in order to optimize\na given performance index. With the increasing of variables and complexity, OC\nproblems can no longer be solved analytically and, consequently, numerical\nmethods are required. For this purpose, direct and indirect methods are used.\nDirect methods consist in the discretization of the OC problem, reducing it to\na nonlinear constrained optimization problem. Indirect methods are based on the\nPontryagin Maximum Principle, which in turn reduces to a boundary value\nproblem. In order to have a more reliable solution, one can solve the same\nproblem through different approaches. Here, as an illustrative example, an\nepidemiological application related to the rubella disease is solved using\nseveral software packages, such as the routine ode45 of Matlab, OC-ODE, DOTcvp\ntoolbox, IPOPT and Snopt, showing the state of the art of numerical software\nfor OC. \n\n"}
{"id": "1402.4618", "contents": "Title: Passive Dynamics in Mean Field Control Abstract: Mean-field models are a popular tool in a variety of fields. They provide an\nunderstanding of the impact of interactions among a large number of particles\nor people or other \"self-interested agents\", and are an increasingly popular\ntool in distributed control.\n  This paper considers a particular randomized distributed control architecture\nintroduced in our own recent work. In numerical results it was found that the\nassociated mean-field model had attractive properties for purposes of control.\nIn particular, when viewed as an input-output system, its linearization was\nfound to be minimum phase.\n  In this paper we take a closer look at the control model. The results are\nsummarized as follows:\n  (i) The Markov Decision Process framework of Todorov is extended to\ncontinuous time models, in which the \"control cost\" is based on relative\nentropy. This is the basis of the construction of a family of controlled\nMarkovian generators.\n  (ii) A decentralized control architecture is proposed in which each agent\nevolves as a controlled Markov process. A central authority broadcasts a common\ncontrol signal to each agent. The central authority chooses this signal based\non an aggregate scalar output of the Markovian agents.\n  (iii) Provided the control-free system is a reversible Markov process, the\nfollowing identity holds for the linearization, \\[ \\text{Real} (G(j\\omega)) =\n\\text{PSD}_Y(\\omega)\\ge 0, \\quad \\omega\\in\\Re, \\] where the right hand side\ndenotes the power spectral density for the output of any one of the individual\n(control-free) Markov processes. \n\n"}
{"id": "1402.4844", "contents": "Title: Subspace Learning with Partial Information Abstract: The goal of subspace learning is to find a $k$-dimensional subspace of\n$\\mathbb{R}^d$, such that the expected squared distance between instance\nvectors and the subspace is as small as possible. In this paper we study\nsubspace learning in a partial information setting, in which the learner can\nonly observe $r \\le d$ attributes from each instance vector. We propose several\nefficient algorithms for this task, and analyze their sample complexity \n\n"}
{"id": "1403.1639", "contents": "Title: Optimal Patching in Clustered Malware Epidemics Abstract: Studies on the propagation of malware in mobile networks have revealed that\nthe spread of malware can be highly inhomogeneous. Platform diversity, contact\nlist utilization by the malware, clustering in the network structure, etc. can\nalso lead to differing spreading rates. In this paper, a general formal\nframework is proposed for leveraging such heterogeneity to derive optimal\npatching policies that attain the minimum aggregate cost due to the spread of\nmalware and the surcharge of patching. Using Pontryagin's Maximum Principle for\na stratified epidemic model, it is analytically proven that in the mean-field\ndeterministic regime, optimal patch disseminations are simple single-threshold\npolicies. Through numerical simulations, the behavior of optimal patching\npolicies is investigated in sample topologies and their advantages are\ndemonstrated. \n\n"}
{"id": "1403.1863", "contents": "Title: Statistical Structure Learning, Towards a Robust Smart Grid Abstract: Robust control and maintenance of the grid relies on accurate data. Both PMUs\nand state estimators are prone to false data injection attacks. Thus, it is\ncrucial to have a mechanism for fast and accurate detection of an agent\nmaliciously tampering with the data---for both preventing attacks that may lead\nto blackouts, and for routine monitoring and control tasks of current and\nfuture grids. We propose a decentralized false data injection detection scheme\nbased on Markov graph of the bus phase angles. We utilize the Conditional\nCovariance Test (CCT) to learn the structure of the grid. Using the DC power\nflow model, we show that under normal circumstances, and because of\nwalk-summability of the grid graph, the Markov graph of the voltage angles can\nbe determined by the power grid graph. Therefore, a discrepancy between\ncalculated Markov graph and learned structure should trigger the alarm. Local\ngrid topology is available online from the protection system and we exploit it\nto check for mismatch. Should a mismatch be detected, we use correlation\nanomaly score to detect the set of attacked nodes. Our method can detect the\nmost recent stealthy deception attack on the power grid that assumes knowledge\nof bus-branch model of the system and is capable of deceiving the state\nestimator, damaging power network observatory, control, monitoring, demand\nresponse and pricing schemes. Specifically, under the stealthy deception\nattack, the Markov graph of phase angles changes. In addition to detect a state\nof attack, our method can detect the set of attacked nodes. To the best of our\nknowledge, our remedy is the first to comprehensively detect this sophisticated\nattack and it does not need additional hardware. Moreover, our detection scheme\nis successful no matter the size of the attacked subset. Simulation of various\npower networks confirms our claims. \n\n"}
{"id": "1403.2898", "contents": "Title: A Minty variational principle for set optimization Abstract: Extremal problems are studied involving an objective function with values in\n(order) complete lattices of sets generated by so called set relations.\nContrary to the popular paradigm in vector optimization, the solution concept\nfor such problems, introduced by F. Heyde and A. L\\\"ohne, comprises the\nattainment of the infimum as well as a minimality property. The main result is\na Minty type variational inequality for set optimization problems which\nprovides a sufficient optimality condition under lower semicontinuity\nassumptions and a necessary condition under appropriate generalized convexity\nassumptions. The variational inequality is based on a new Dini directional\nderivative for set-valued functions which is defined in terms of a \"lattice\ndifference quotient\": A residual operation in a lattice of sets replaces the\ninverse addition in linear spaces. Relationships to families of scalar problems\nare pointed out and used for proofs: The appearance of improper scalarizations\nposes a major difficulty which is dealt with by extending known scalar results\nsuch as Diewert's theorem to improper functions. \n\n"}
{"id": "1403.4789", "contents": "Title: Structure-preserving model reduction of physical network systems by\n  clustering Abstract: In this paper, we establish a method for model order reduction of a certain\nclass of physical network systems. The proposed method is based on clustering\nof the vertices of the underlying graph, and yields a reduced order model\nwithin the same class. To capture the physical properties of the network, we\nallow for weights associated to both the edges as well as the vertices of the\ngraph. We extend the notion of almost equitable partitions to this class of\ngraphs. Consequently, an explicit model reduction error expression in the sense\nof H2-norm is provided for clustering arising from almost equitable partitions.\nFinally the method is extended to second-order systems. \n\n"}
{"id": "1403.4879", "contents": "Title: A Compressive Sensing Based Approach to Sparse Wideband Array Design Abstract: Sparse wideband sensor array design for sensor location optimisation is\nhighly nonlinear and it is traditionally solved by genetic algorithms,\nsimulated annealing or other similar optimization methods. However, this is an\nextremely time-consuming process and more efficient solutions are needed. In\nthis work, this problem is studied from the viewpoint of compressive sensing\nand a formulation based on a modified $l_1$ norm is derived. As there are\nmultiple coefficients associated with each sensor, the key is to make sure that\nthese coefficients are simultaneously minimized in order to discard the\ncorresponding sensor locations. Design examples are provided to verify the\neffectiveness of the proposed methods. \n\n"}
{"id": "1403.5339", "contents": "Title: Spacecraft Position and Attitude Formation Control using Line-of-Sight\n  Observations Abstract: This paper studies formation control of an arbitrary number of spacecraft\nbased on a serial network structure. The leader controls its absolute position\nand absolute attitude with respect to an inertial frame, and the followers\ncontrol its relative position and attitude with respect to another spacecraft\nassigned by the serial network. The unique feature is that both the absolute\nattitude and the relative attitude control systems are developed directly in\nterms of the line-of-sight observations between spacecraft, without need for\nestimating the full absolute and relative attitudes, to improve accuracy and\nefficiency. Control systems are developed on the nonlinear configuration\nmanifold, guaranteeing exponential stability. Numerical examples are presented\nto illustrate the desirable properties of the proposed control system. \n\n"}
{"id": "1403.5374", "contents": "Title: Transverse Contraction Criteria for Stability of Nonlinear Hybrid Limit\n  Cycles Abstract: In this paper, we derive differential conditions guaranteeing the orbital\nstability of nonlinear hybrid limit cycles. These conditions are represented as\na series of pointwise linear matrix inequalities (LMI), enabling the search for\nstability certificates via convex optimization tools such as sum-of-squares\nprogramming. Unlike traditional Lyapunov-based methods, the transverse\ncontraction framework developed in this paper enables proof of stability for\nhybrid systems, without prior knowledge of the exact location of the stable\nlimit cycle in state space. This methodology is illustrated on a dynamic\nwalking example. \n\n"}
{"id": "1403.5521", "contents": "Title: Scenario optimization with certificates and applications to anti-windup\n  design Abstract: In this paper, we introduce a significant extension, called scenario with\ncertificates (SwC), of the so-called scenario approach for uncertain\noptimization problems. This extension is motivated by the observation that in\nmany control problems only some of the optimization variables are used in the\ndesign phase, while the other variables play the role of certificates. Examples\nare all those control problems that can be reformulated in terms of linear\nmatrix inequalities involving parameter-dependent Lyapunov functions. These\ncontrol problems include static anti-windup compensator design for uncertain\nlinear systems with input saturation, where the goal is the minimization of the\nnonlinear gain from an exogenous input to a performance output. The main\ncontribution of this paper is to show that randomization is a useful tool,\nspecifically for anti-windup design, to make the overall approach less\nconservative compared to its robust counterpart. In particular, we demonstrate\nthat the scenario with certificates reformulation is appealing because it\nprovides a way to implicitly design the parameter-dependent Lyapunov functions.\nFinally, to further reduce the computational cost of this one-shot approach, we\npresent a sequential randomized algorithm for iteratively solving this problem. \n\n"}
{"id": "1403.6281", "contents": "Title: Exponential decay properties of a mathematical model for a certain\n  fluid-structure interaction Abstract: In this work, we derive a result of exponential stability for a coupled\nsystem of partial differential equations (PDEs) which governs a certain\nfluid-structure interaction. In particular, a three-dimensional Stokes flow\ninteracts across a boundary interface with a two-dimensional mechanical plate\nequation. In the case that the PDE plate component is rotational inertia-free,\none will have that solutions of this fluid-structure PDE system exhibit an\nexponential rate of decay. By way of proving this decay, an estimate is\nobtained for the resolvent of the associated semigroup generator, an estimate\nwhich is uniform for frequency domain values along the imaginary axis.\nSubsequently, we proceed to discuss relevant point control and boundary control\nscenarios for this fluid-structure PDE model, with an ultimate view to optimal\ncontrol studies on both finite and infinite horizon. (Because of said\nexponential stability result, optimal control of the PDE on time interval\n$(0,\\infty)$ becomes a reasonable problem for contemplation.) \n\n"}
{"id": "1403.7175", "contents": "Title: Low-Rank and Low-Order Decompositions for Local System Identification Abstract: As distributed systems increase in size, the need for scalable algorithms\nbecomes more and more important. We argue that in the context of system\nidentification, an essential building block of any scalable algorithm is the\nability to estimate local dynamics within a large interconnected system. We\nshow that in what we term the \"full interconnection measurement\" setting, this\ntask is easily solved using existing system identification methods. We also\npropose a promising heuristic for the \"hidden interconnection measurement\"\ncase, in which contributions to local measurements from both local and global\ndynamics need to be separated. Inspired by the machine learning literature, and\nin particular by convex approaches to rank minimization and matrix\ndecomposition, we exploit the fact that the transfer function of the local\ndynamics is low-order, but full-rank, while the transfer function of the global\ndynamics is high-order, but low-rank, to formulate this separation task as a\nnuclear norm minimization. \n\n"}
{"id": "1404.0237", "contents": "Title: Design of Symbolic Controllers for Networked Control Systems Abstract: Networked Control Systems (NCS) are distributed systems where plants,\nsensors, actuators and controllers communicate over shared networks. Non-ideal\nbehaviors of the communication network include variable sampling/transmission\nintervals and communication delays, packet losses, communication constraints\nand quantization errors. NCS have been the object of intensive study in the\nlast few years. However, due to the inherent complexity of NCS, current\nliterature focuses on a subset of these non-idealities and mostly considers\nstability and stabilizability problems. Recent technology advances need\ndifferent and more complex control objectives to be considered. In this paper\nwe present first a general model of NCS, including most relevant non-idealities\nof the communication network; then, we propose a symbolic model approach to the\ncontrol design with objectives expressed in terms of non-deterministic\ntransition systems. The presented results are based on recent advances in\nsymbolic control design of continuous and hybrid systems. An example in the\ncontext of robot motion planning with remote control is included, showing the\neffectiveness of the proposed approach. \n\n"}
{"id": "1404.1972", "contents": "Title: Regularization for Design Abstract: When designing controllers for large-scale systems, the architectural aspects\nof the controller such as the placement of actuators, sensors, and the\ncommunication links between them can no longer be taken as given. The task of\ndesigning this architecture is now as important as the design of the control\nlaws themselves. By interpreting controller synthesis (in a model matching\nsetup) as the solution of a particular linear inverse problem, we view the\nchallenge of obtaining a controller with a desired architecture as one of\nfinding a structured solution to an inverse problem. Building on this\nconceptual connection, we formulate and analyze a framework called\n\\textit{Regularization for Design (RFD)}, in which we augment the variational\nformulations of controller synthesis problems with convex penalty functions\nthat induce a desired controller architecture. The resulting regularized\nformulations are convex optimization problems that can be solved efficiently,\nthese convex programs provide a unified computationally tractable approach for\nthe simultaneous co-design of a structured optimal controller and the\nactuation, sensing and communication architecture required to implement it.\nFurther, these problems are natural control-theoretic analogs of prominent\napproaches such as the Lasso, the Group Lasso, the Elastic Net, and others that\nare employed in statistical modeling. In analogy to that literature, we show\nthat our approach identifies optimally structured controllers under a suitable\ncondition on a \"signal-to-noise\" type ratio. \n\n"}
{"id": "1404.1978", "contents": "Title: An Abrupt Change Detection Heuristic with Applications to Cyber Data\n  Attacks on Power Systems Abstract: We present an analysis of a heuristic for abrupt change detection of systems\nwith bounded state variations. The proposed analysis is based on the Singular\nValue Decomposition (SVD) of a history matrix built from system observations.\nWe show that monitoring the largest singular value of the history matrix can be\nused as a heuristic for detecting abrupt changes in the system outputs. We\nprovide sufficient detectability conditions for the proposed heuristic. As an\napplication, we consider detecting malicious cyber data attacks on power\nsystems and test our proposed heuristic on the IEEE 39-bus testbed. \n\n"}
{"id": "1404.2655", "contents": "Title: Open problem: Tightness of maximum likelihood semidefinite relaxations Abstract: We have observed an interesting, yet unexplained, phenomenon: Semidefinite\nprogramming (SDP) based relaxations of maximum likelihood estimators (MLE) tend\nto be tight in recovery problems with noisy data, even when MLE cannot exactly\nrecover the ground truth. Several results establish tightness of SDP based\nrelaxations in the regime where exact recovery from MLE is possible. However,\nto the best of our knowledge, their tightness is not understood beyond this\nregime. As an illustrative example, we focus on the generalized Procrustes\nproblem. \n\n"}
{"id": "1404.2862", "contents": "Title: Tangle Machines Abstract: Tangle machines are topologically inspired diagrammatic models. Their novel\nfeature is their natural notion of equivalence. Equivalent tangle machines may\ndiffer locally, but globally they are considered to share the same information\ncontent. The goal of tangle machine equivalence is to provide a\ncontext-independent method to select, from among many ways to perform a task,\nthe `best' way to perform the task. The concept of equivalent tangle machines\nis illustrated through examples in which they represent recursive computations,\nnetworks of adiabatic quantum computations, and networks of distributed\ninformation processing. \n\n"}
{"id": "1404.5222", "contents": "Title: Self-Averaging Property of Minimal Investment Risk of Mean-Variance\n  Model Abstract: In portfolio optimization problems, the minimum expected investment risk is\nnot always smaller than the expected minimal investment risk. That is, using a\nwell-known approach from operations research, it is possible to derive a\nstrategy that minimizes the expected investment risk, but this strategy does\nnot always result in the best rate of return on assets. Prior to making\ninvestment decisions, it is important to an investor to know the potential\nminimal investment risk (or the expected minimal investment risk) and to\ndetermine the strategy that will maximize the return on assets. We use the\nself-averaging property to analyze the potential minimal investment risk and\nthe concentrated investment level for the strategy that gives the best rate of\nreturn. We compare the results from our method with the results obtained by the\noperations research approach and with those obtained by a numerical simulation\nusing the optimal portfolio. The results of our method and the numerical\nsimulation are in agreement, but they differ from that of the operations\nresearch approach. \n\n"}
{"id": "1404.6813", "contents": "Title: Diffusion LMS over Multitask Networks Abstract: The diffusion LMS algorithm has been extensively studied in recent years.\nThis efficient strategy allows to address distributed optimization problems\nover networks in the case where nodes have to collaboratively estimate a single\nparameter vector. Problems of this type are referred to as single-task\nproblems. Nevertheless, there are several problems in practice that are\nmultitask-oriented in the sense that the optimum parameter vector may not be\nthe same for every node. This brings up the issue of studying the performance\nof the diffusion LMS algorithm when it is run, either intentionally or\nunintentionally, in a multitask environment. In this paper, we conduct a\ntheoretical analysis on the stochastic behavior of diffusion LMS in the case\nwhere the so-called single-task hypothesis is violated. We explain under what\nconditions diffusion LMS continues to deliver performance superior to\nnon-cooperative strategies in the multitask environment. When the conditions\nare violated, we explain how to endow the nodes with the ability to cluster\nwith other similar nodes to remove bias. We propose an unsupervised clustering\nstrategy that allows each node to select, via adaptive adjustments of\ncombination weights, the neighboring nodes with which it can collaborate to\nestimate a common parameter vector. Simulations are presented to illustrate the\ntheoretical results, and to demonstrate the efficiency of the proposed\nclustering strategy. The framework is applied to a useful problem involving a\nmulti-target tracking task. \n\n"}
{"id": "1404.7282", "contents": "Title: Formal Proofs for Nonlinear Optimization Abstract: We present a formally verified global optimization framework. Given a\nsemialgebraic or transcendental function $f$ and a compact semialgebraic domain\n$K$, we use the nonlinear maxplus template approximation algorithm to provide a\ncertified lower bound of $f$ over $K$. This method allows to bound in a modular\nway some of the constituents of $f$ by suprema of quadratic forms with a well\nchosen curvature. Thus, we reduce the initial goal to a hierarchy of\nsemialgebraic optimization problems, solved by sums of squares relaxations. Our\nimplementation tool interleaves semialgebraic approximations with sums of\nsquares witnesses to form certificates. It is interfaced with Coq and thus\nbenefits from the trusted arithmetic available inside the proof assistant. This\nfeature is used to produce, from the certificates, both valid underestimators\nand lower bounds for each approximated constituent. The application range for\nsuch a tool is widespread; for instance Hales' proof of Kepler's conjecture\nyields thousands of multivariate transcendental inequalities. We illustrate the\nperformance of our formal framework on some of these inequalities as well as on\nexamples from the global optimization literature. \n\n"}
{"id": "1404.7801", "contents": "Title: Duality results for Iterated Function Systems with a general family of\n  branches Abstract: For $X$, $Y$, $Z$ and $W$ compact metric spaces, consider two uniformly\ncontractive IFS $\\{\\tau_x: Z\\to Z,\\, x\\in x\\}$ and $\\{\\tau_y:W\\to W,\\, y\\in\nY\\}$. For a fixed $\\alpha \\in \\mathcal{P}(X)$ with $supp(\\alpha)=X$ we define\nthe entropy of a holonomic measure $\\pi \\in \\mathcal{P}(X\\times Z)$ relative to\n$\\alpha$, the pressure of a continuous cost function $c(x,z)$ and show that for\n$c$ Lipschitz this pressure coincides with the spectral radius of the\nassociated transfer operator. The same approach can be applied to the pair\n$Y,W$. For fixed probabilities $\\alpha \\in \\mathcal{P}(X)$ and $\\beta \\in\n\\mathcal{P}(Y)$ with $supp(\\alpha)=X,\\,supp(\\beta)=Y$ we denote by\n$H_\\alpha(\\pi), \\pi \\in \\Pi(\\cdot,\\cdot,\\tau)$, the entropy of the\n$(X,Z)-$marginal of $\\pi$ relative to $\\alpha$ and denote by $H_\\beta(\\pi)$,\nthe entropy of the $(Y,W)-$marginal of $\\pi$ relative to $\\beta$. The marginal\npressure of a continuous cost function $c \\in C(X\\times Y \\times Z \\times W)$\nrelative to $(\\alpha,\\beta)$ will be defined by $P^{m}(c) =\n\\sup_{\\pi\\in\\Pi(\\cdot,\\cdot,\\tau)} \\int c\\, d\\pi + H_{\\alpha}(\\pi)\n+H_{\\beta}(\\pi)$ and we will show the following duality result: \\[\\inf_{P^{m}(c\n-\\varphi(x) -\\psi(y))=0} \\int \\varphi(x)\\,d\\mu +\\int \\psi(y)\\,d\\nu =\n\\sup_{\\pi\\in\\Pi(\\mu,\\nu,\\tau)} \\int c\\, d\\pi + H_{\\alpha}(\\pi)\n+H_{\\beta}(\\pi).\\] When $Z$ and $W$ have only one point and the entropy is\nunconsidered this equality can be rewritten as the Kantorovich Duality for\ncompact spaces $X,Y$ and continuous cost $-c$: \\[\\inf_{c -\\varphi(x)\n-\\psi(y)\\leq 0} \\int \\varphi(x)\\,d\\mu +\\int \\psi(y)\\,d\\nu =\n\\sup_{\\pi\\in\\Pi(\\mu,\\nu)} \\int c\\, d\\pi .\\] \n\n"}
{"id": "1405.1483", "contents": "Title: Recovering rank-one matrices via rank-r matrices relaxation Abstract: PhaseLift, proposed by E.J. Cand\\`{e}s et al., is one convex relaxation\napproach for phase retrieval. The relaxation enlarges the solution set from\nrank one matrices to positive semidefinite matrices.\n  In this paper, a relaxation is employed to nonconvex alternating minimization\nmethods to recover the rank-one matrices.\n  A generic measurement matrix can be standardized to a matrix consisting of\northonormal columns. To recover the rank-one matrix, the standardized frames\nare used to select the matrix with the maximal leading eigenvalue among the\nrank-$r$ matrices. Empirical studies are conducted to validate the\neffectiveness of this relaxation approach. In the case of Gaussian random\nmatrices with a sufficient number of nearly orthogonal sensing vectors, we show\nthat the singular vector corresponding to the least singular value is close to\nthe unknown signal, and thus it can be a good initialization for the nonconvex\nminimization algorithm. \n\n"}
{"id": "1405.3162", "contents": "Title: Circulant Binary Embedding Abstract: Binary embedding of high-dimensional data requires long codes to preserve the\ndiscriminative power of the input space. Traditional binary coding methods\noften suffer from very high computation and storage costs in such a scenario.\nTo address this problem, we propose Circulant Binary Embedding (CBE) which\ngenerates binary codes by projecting the data with a circulant matrix. The\ncirculant structure enables the use of Fast Fourier Transformation to speed up\nthe computation. Compared to methods that use unstructured matrices, the\nproposed method improves the time complexity from $\\mathcal{O}(d^2)$ to\n$\\mathcal{O}(d\\log{d})$, and the space complexity from $\\mathcal{O}(d^2)$ to\n$\\mathcal{O}(d)$ where $d$ is the input dimensionality. We also propose a novel\ntime-frequency alternating optimization to learn data-dependent circulant\nprojections, which alternatively minimizes the objective in original and\nFourier domains. We show by extensive experiments that the proposed approach\ngives much better performance than the state-of-the-art approaches for fixed\ntime, and provides much faster computation with no performance degradation for\nfixed number of bits. \n\n"}
{"id": "1405.3363", "contents": "Title: Weakly Coupled Dynamic Program: Information and Lagrangian Relaxations Abstract: \"Weakly coupled dynamic program\" describes a broad class of stochastic\noptimization problems in which multiple controlled stochastic processes evolve\nindependently but subject to a set of linking constraints imposed on the\ncontrols. One feature of the weakly coupled dynamic program is that it\ndecouples into lower-dimensional dynamic programs by dualizing the linking\nconstraint via the Lagrangian relaxation, which also yields a bound on the\noptimal value of the original dynamic program. Together with the Lagrangian\nbound, we utilize the information relaxation approach that relaxes the\nnon-anticipative constraint on the controls to obtain a tighter dual bound. We\nalso investigate other combinations of the relaxations and place the resulting\nbounds in order. To tackle large-scale problems, we further propose a\ncomputationally tractable method based on information relaxation, and provide\ninsightful interpretation and performance guarantee. We implement our method\nand demonstrate its use through two numerical examples. \n\n"}
{"id": "1405.4543", "contents": "Title: A Distributed Algorithm for Training Nonlinear Kernel Machines Abstract: This paper concerns the distributed training of nonlinear kernel machines on\nMap-Reduce. We show that a re-formulation of Nystr\\\"om approximation based\nsolution which is solved using gradient based techniques is well suited for\nthis, especially when it is necessary to work with a large number of basis\npoints. The main advantages of this approach are: avoidance of computing the\npseudo-inverse of the kernel sub-matrix corresponding to the basis points;\nsimplicity and efficiency of the distributed part of the computations; and,\nfriendliness to stage-wise addition of basis points. We implement the method\nusing an AllReduce tree on Hadoop and demonstrate its value on a few large\nbenchmark datasets. \n\n"}
{"id": "1406.0728", "contents": "Title: A Game-theoretic Machine Learning Approach for Revenue Maximization in\n  Sponsored Search Abstract: Sponsored search is an important monetization channel for search engines, in\nwhich an auction mechanism is used to select the ads shown to users and\ndetermine the prices charged from advertisers. There have been several pieces\nof work in the literature that investigate how to design an auction mechanism\nin order to optimize the revenue of the search engine. However, due to some\nunrealistic assumptions used, the practical values of these studies are not\nvery clear. In this paper, we propose a novel \\emph{game-theoretic machine\nlearning} approach, which naturally combines machine learning and game theory,\nand learns the auction mechanism using a bilevel optimization framework. In\nparticular, we first learn a Markov model from historical data to describe how\nadvertisers change their bids in response to an auction mechanism, and then for\nany given auction mechanism, we use the learnt model to predict its\ncorresponding future bid sequences. Next we learn the auction mechanism through\nempirical revenue maximization on the predicted bid sequences. We show that the\nempirical revenue will converge when the prediction period approaches infinity,\nand a Genetic Programming algorithm can effectively optimize this empirical\nrevenue. Our experiments indicate that the proposed approach is able to produce\na much more effective auction mechanism than several baselines. \n\n"}
{"id": "1406.1089", "contents": "Title: A variational approach to stable principal component pursuit Abstract: We introduce a new convex formulation for stable principal component pursuit\n(SPCP) to decompose noisy signals into low-rank and sparse representations. For\nnumerical solutions of our SPCP formulation, we first develop a convex\nvariational framework and then accelerate it with quasi-Newton methods. We\nshow, via synthetic and real data experiments, that our approach offers\nadvantages over the classical SPCP formulations in scalability and practical\nparameter selection. \n\n"}
{"id": "1406.2075", "contents": "Title: Stochastic Gradient-Push for Strongly Convex Functions on Time-Varying\n  Directed Graphs Abstract: We investigate the convergence rate of the recently proposed subgradient-push\nmethod for distributed optimization over time-varying directed graphs. The\nsubgradient-push method can be implemented in a distributed way without\nrequiring knowledge of either the number of agents or the graph sequence; each\nnode is only required to know its out-degree at each time. Our main result is a\nconvergence rate of $O \\left((\\ln t)/t \\right)$ for strongly convex functions\nwith Lipschitz gradients even if only stochastic gradient samples are\navailable; this is asymptotically faster than the $O \\left((\\ln t)/\\sqrt{t}\n\\right)$ rate previously known for (general) convex functions. \n\n"}
{"id": "1406.2528", "contents": "Title: Denosing Using Wavelets and Projections onto the L1-Ball Abstract: Both wavelet denoising and denosing methods using the concept of sparsity are\nbased on soft-thresholding. In sparsity based denoising methods, it is assumed\nthat the original signal is sparse in some transform domains such as the\nwavelet domain and the wavelet subsignals of the noisy signal are projected\nonto L1-balls to reduce noise. In this lecture note, it is shown that the size\nof the L1-ball or equivalently the soft threshold value can be determined using\nlinear algebra. The key step is an orthogonal projection onto the epigraph set\nof the L1-norm cost function. \n\n"}
{"id": "1406.3228", "contents": "Title: On Existence of $L^1$-solutions for Coupled Boltzmann Transport Equation\n  and Radiation Therapy Treatment Optimization Abstract: The paper considers a linear system of Boltzmann transport equations\nmodelling the evolution of three species of particles, photons, electrons and\npositrons. The system is coupled because of the collision term (an integral\noperator). The model is intended especially for dose calculation (forward\nproblem) in radiation therapy. It, however, does not apply to all relevant\ninteractions in its present form. We show under physically relevant assumptions\nthat the system has a unique solution in appropriate ($L^1$-based) spaces and\nthat the solution is non-negative when the data (internal source and inflow\nboundary source) is non-negative. In order to be self-contained as much as is\npractically possible, many (basic) results and proofs have been reproduced in\nthe paper. Existence, uniqueness and non-negativity of solutions for the\nrelated time-dependent coupled system are also proven. Moreover, we deal with\ninverse radiation treatment planning problem (inverse problem) as an optimal\ncontrol problem both for external and internal therapy (in general\n$L^p$-spaces). Especially, in the case $p=2$ variational equations for an\noptimal control related to an appropriate differentiable convex object function\nare verified. Its solution can be used as an initial point for an actual\n(global) optimization. \n\n"}
{"id": "1406.5362", "contents": "Title: Predicting the Future Behavior of a Time-Varying Probability\n  Distribution Abstract: We study the problem of predicting the future, though only in the\nprobabilistic sense of estimating a future state of a time-varying probability\ndistribution. This is not only an interesting academic problem, but solving\nthis extrapolation problem also has many practical application, e.g. for\ntraining classifiers that have to operate under time-varying conditions. Our\nmain contribution is a method for predicting the next step of the time-varying\ndistribution from a given sequence of sample sets from earlier time steps. For\nthis we rely on two recent machine learning techniques: embedding probability\ndistributions into a reproducing kernel Hilbert space, and learning operators\nby vector-valued regression. We illustrate the working principles and the\npractical usefulness of our method by experiments on synthetic and real data.\nWe also highlight an exemplary application: training a classifier in a domain\nadaptation setting without having access to examples from the test time\ndistribution at training time. \n\n"}
{"id": "1406.5378", "contents": "Title: Faa di Bruno Hopf Algebra of the Output Feedback Group for Multivariable\n  Fliess Operators Abstract: Given two nonlinear input-output systems written in terms of Chen-Fliess\nfunctional expansions, it is known that the feedback interconnected system is\nalways well defined and in the same class. An explicit formula for the\ngenerating series of a single-input, single-output closed-loop system was\nprovided by the first two authors in earlier work via Hopf algebra methods.\nThis paper is a sequel. It has four main innovations. First, the full\nmultivariable extension of the theory is presented. Next, a major\nsimplification of the basic set up is introduced using a new type of grading\nthat has recently appeared in the literature. This grading also facilitates a\nfully recursive algorithm to compute the antipode of the Hopf algebra of the\noutput feedback group, and thus, the corresponding feedback product can be\ncomputed much more efficiently. The final innovation is an improved convergence\nanalysis of the antipode operation, namely, the radius of convergence of the\nantipode is computed. \n\n"}
{"id": "1406.6332", "contents": "Title: Combined Global and Local Search for the Falsification of Hybrid Systems Abstract: In this paper we solve the problem of finding a trajectory that shows that a\ngiven hybrid dynamical system with deterministic evolution leaves a given set\nof states considered to be safe. The algorithm combines local with global\nsearch for achieving both efficiency and global convergence. In local search,\nit exploits derivatives for efficient computation. Unlike other methods for\nfalsification of hybrid systems with deterministic evolution, we do not\nrestrict our search to trajectories of a certain bounded length but search for\nerror trajectories of arbitrary length. \n\n"}
{"id": "1407.1569", "contents": "Title: Joint Centrality Distinguishes Optimal Leaders in Noisy Networks Abstract: We study the performance of a network of agents tasked with tracking an\nexternal unknown signal in the presence of stochastic disturbances and under\nthe condition that only a limited subset of agents, known as leaders, can\nmeasure the signal directly. We investigate the optimal leader selection\nproblem for a prescribed maximum number of leaders, where the optimal leader\nset minimizes total system error defined as steady-state variance about the\nexternal signal. In contrast to previously established greedy algorithms for\noptimal leader selection, our results rely on an expression of total system\nerror in terms of properties of the underlying network graph. We demonstrate\nthat the performance of any given set of leaders depends on their influence as\ndetermined by a new graph measure of centrality of a set. We define the $joint\n\\; centrality$ of a set of nodes in a network graph such that a leader set with\nmaximal joint centrality is an optimal leader set. In the case of a single\nleader, we prove that the optimal leader is the node with maximal information\ncentrality. In the case of multiple leaders, we show that the nodes in the\noptimal leader set balance high information centrality with a coverage of the\ngraph. For special cases of graphs, we solve explicitly for optimal leader\nsets. We illustrate with examples. \n\n"}
{"id": "1407.2657", "contents": "Title: Beyond Disagreement-based Agnostic Active Learning Abstract: We study agnostic active learning, where the goal is to learn a classifier in\na pre-specified hypothesis class interactively with as few label queries as\npossible, while making no assumptions on the true function generating the\nlabels. The main algorithms for this problem are {\\em{disagreement-based active\nlearning}}, which has a high label requirement, and {\\em{margin-based active\nlearning}}, which only applies to fairly restricted settings. A major challenge\nis to find an algorithm which achieves better label complexity, is consistent\nin an agnostic setting, and applies to general classification problems.\n  In this paper, we provide such an algorithm. Our solution is based on two\nnovel contributions -- a reduction from consistent active learning to\nconfidence-rated prediction with guaranteed error, and a novel confidence-rated\npredictor. \n\n"}
{"id": "1407.5396", "contents": "Title: Symblicit algorithms for optimal strategy synthesis in monotonic Markov\n  decision processes Abstract: When treating Markov decision processes (MDPs) with large state spaces, using\nexplicit representations quickly becomes unfeasible. Lately, Wimmer et al. have\nproposed a so-called symblicit algorithm for the synthesis of optimal\nstrategies in MDPs, in the quantitative setting of expected mean-payoff. This\nalgorithm, based on the strategy iteration algorithm of Howard and Veinott,\nefficiently combines symbolic and explicit data structures, and uses binary\ndecision diagrams as symbolic representation. The aim of this paper is to show\nthat the new data structure of pseudo-antichains (an extension of antichains)\nprovides another interesting alternative, especially for the class of monotonic\nMDPs. We design efficient pseudo-antichain based symblicit algorithms (with\nopen source implementations) for two quantitative settings: the expected\nmean-payoff and the stochastic shortest path. For two practical applications\ncoming from automated planning and LTL synthesis, we report promising\nexperimental results w.r.t. both the run time and the memory consumption. \n\n"}
{"id": "1407.5553", "contents": "Title: Privacy-Preserving Filtering for Event Streams Abstract: Many large-scale information systems such as intelligent transportation\nsystems, smart grids or smart buildings collect data about the activities of\ntheir users to optimize their operations. To encourage participation and\nadoption of these systems, it is becoming increasingly important that the\ndesign process take privacy issues into consideration. In a typical scenario,\nsignals originate from many sensors capturing events involving the users, and\nseveral statistics of interest need to be continuously published in real-time.\nThis paper considers the problem of providing differential privacy guarantees\nfor such multi-input multi-output systems processing event streams. We show how\nto construct and optimize various extensions of the zero-forcing equalization\nmechanism, which we previously proposed for single-input single-output systems.\nSome of these extensions can take a model of the input signals into account. We\nillustrate our privacy-preserving filter design methodology through the problem\nof privately monitoring and forecasting occupancy in a building equipped with\nmultiple motion detection sensors. \n\n"}
{"id": "1407.7810", "contents": "Title: Models and Feedback Stabilization of Open Quantum Systems Abstract: At the quantum level, feedback-loops have to take into account measurement\nback-action. We present here the structure of the Markovian models including\nsuch back-action and sketch two stabilization methods: measurement-based\nfeedback where an open quantum system is stabilized by a classical controller;\ncoherent or autonomous feedback where a quantum system is stabilized by a\nquantum controller with decoherence (reservoir engineering). We begin to\nexplain these models and methods for the photon box experiments realized in the\ngroup of Serge Haroche (Nobel Prize 2012). We present then these models and\nmethods for general open quantum systems. \n\n"}
{"id": "1407.8186", "contents": "Title: Exploration vs. Exploitation in the Information Filtering Problem Abstract: We consider information filtering, in which we face a stream of items too\nvoluminous to process by hand (e.g., scientific articles, blog posts, emails),\nand must rely on a computer system to automatically filter out irrelevant\nitems. Such systems face the exploration vs. exploitation tradeoff, in which it\nmay be beneficial to present an item despite a low probability of relevance,\njust to learn about future items with similar content. We present a Bayesian\nsequential decision-making model of this problem, show how it may be solved to\noptimality using a decomposition to a collection of two-armed bandit problems,\nand show structural results for the optimal policy. We show that the resulting\nmethod is especially useful when facing the cold start problem, i.e., when\nfiltering items for new users without a long history of past interactions. We\nthen present an application of this information filtering method to a\nhistorical dataset from the arXiv.org repository of scientific articles. \n\n"}
{"id": "1408.0578", "contents": "Title: A Cyclic Coordinate Descent Algorithm for lq Regularization Abstract: In recent studies on sparse modeling, $l_q$ ($0<q<1$) regularization has\nreceived considerable attention due to its superiorities on sparsity-inducing\nand bias reduction over the $l_1$ regularization.In this paper, we propose a\ncyclic coordinate descent (CCD) algorithm for $l_q$ regularization. Our main\nresult states that the CCD algorithm converges globally to a stationary point\nas long as the stepsize is less than a positive constant. Furthermore, we\ndemonstrate that the CCD algorithm converges to a local minimizer under certain\nadditional conditions. Our numerical experiments demonstrate the efficiency of\nthe CCD algorithm. \n\n"}
{"id": "1408.4072", "contents": "Title: Indexing Cost Sensitive Prediction Abstract: Predictive models are often used for real-time decision making. However,\ntypical machine learning techniques ignore feature evaluation cost, and focus\nsolely on the accuracy of the machine learning models obtained utilizing all\nthe features available. We develop algorithms and indexes to support\ncost-sensitive prediction, i.e., making decisions using machine learning models\ntaking feature evaluation cost into account. Given an item and a online\ncomputation cost (i.e., time) budget, we present two approaches to return an\nappropriately chosen machine learning model that will run within the specified\ntime on the given item. The first approach returns the optimal machine learning\nmodel, i.e., one with the highest accuracy, that runs within the specified\ntime, but requires significant up-front precomputation time. The second\napproach returns a possibly sub- optimal machine learning model, but requires\nlittle up-front precomputation time. We study these two algorithms in detail\nand characterize the scenarios (using real and synthetic data) in which each\nperforms well. Unlike prior work that focuses on a narrow domain or a specific\nalgorithm, our techniques are very general: they apply to any cost-sensitive\nprediction scenario on any machine learning algorithm. \n\n"}
{"id": "1409.1889", "contents": "Title: Lyapunov Functions Family Approach to Transient Stability Assessment Abstract: Analysis of transient stability of strongly nonlinear post-fault dynamics is\none of the most computationally challenging parts of Dynamic Security\nAssessment. This paper proposes a novel approach for assessment of transient\nstability of the system. The approach generalizes the idea of energy methods,\nand extends the concept of energy function to a more general Lyapunov Functions\nFamily (LFF) constructed via Semi-Definite-Programming techniques. Unlike the\ntraditional energy function and its variations, the constructed Lyapunov\nfunctions are proven to be decreasing only in a finite neighborhood of the\nequilibrium point. However, we show that they can still certify stability of a\nbroader set of initial conditions in comparison to the traditional energy\nfunction in the closest-UEP method. Moreover, the certificates of stability can\nbe constructed via a sequence of convex optimization problems that are\ntractable even for large scale systems. We also propose specific algorithms for\nadaptation of the Lyapunov functions to specific initial conditions and\ndemonstrate the effectiveness of the approach on a number of IEEE test cases. \n\n"}
{"id": "1409.2594", "contents": "Title: A Direct Coupling Coherent Quantum Observer for a Single Qubit Finite\n  Level Quantum System Abstract: This paper considers the problem of constructing a direct coupling quantum\nobserver for a single qubit finite level quantum system plant. The proposed\nobserver is a single mode linear quantum system which is shown to be able to\nestimate one of the plant variables in a time averaged sense. A numerical\nexample and simulations are included to illustrate the properties of the\nobserver. \n\n"}
{"id": "1409.4377", "contents": "Title: A quantum mechanical version of Price's theorem for Gaussian states Abstract: This paper is concerned with integro-differential identities which are known\nin statistical signal processing as Price's theorem for expectations of\nnonlinear functions of jointly Gaussian random variables. We revisit these\nrelations for classical variables by using the Frechet differentiation with\nrespect to covariance matrices, and then show that Price's theorem carries over\nto a quantum mechanical setting. The quantum counterpart of the theorem is\nestablished for Gaussian quantum states in the framework of the Weyl functional\ncalculus for quantum variables satisfying the Heisenberg canonical commutation\nrelations. The quantum mechanical version of Price's theorem relates the\nFrechet derivative of the generalized moment of such variables with respect to\nthe real part of their quantum covariance matrix with other moments. As an\nillustrative example, we consider these relations for quadratic-exponential\nmoments which are relevant to risk-sensitive quantum control. \n\n"}
{"id": "1409.6354", "contents": "Title: A Compartmental Model for Traffic Networks and its Dynamical Behavior Abstract: We propose a macroscopic traffic network flow model suitable for analysis as\na dynamical system, and we qualitatively analyze equilibrium flows as well as\nconvergence. Flows at a junction are determined by downstream supply of\ncapacity as well as upstream demand of traffic wishing to flow through the\njunction. This approach is rooted in the celebrated Cell Transmission Model for\nfreeway traffic flow. Unlike related results which rely on certain system\ncooperativity properties, our model generally does not possess these\nproperties. We show that the lack of cooperativity is in fact a useful feature\nthat allows traffic control methods, such as ramp metering, to be effective.\nFinally, we leverage the results of the paper to develop a linear program for\noptimal ramp metering. \n\n"}
{"id": "1409.7638", "contents": "Title: Edges vs Circuits: a Hierarchy of Diameters in Polyhedra Abstract: The study of the graph diameter of polytopes is a classical open problem in\npolyhedral geometry and the theory of linear optimization. In this paper we\ncontinue the investigation initiated in [4] by introducing a vast hierarchy of\ngeneralizations to the notion of graph diameter. This hierarchy provides some\ninteresting lower bounds for the usual graph diameter. After explaining the\nstructure of the hierarchy and discussing these bounds, we focus on clearly\nexplaining the differences and similarities among the many diameter notions of\nour hierarchy. Finally, we fully characterize the hierarchy in dimension two.\nIt collapses into fewer categories, for which we exhibit the ranges of values\nthat can be realized as diameters. \n\n"}
{"id": "1409.7963", "contents": "Title: MoDeep: A Deep Learning Framework Using Motion Features for Human Pose\n  Estimation Abstract: In this work, we propose a novel and efficient method for articulated human\npose estimation in videos using a convolutional network architecture, which\nincorporates both color and motion features. We propose a new human body pose\ndataset, FLIC-motion, that extends the FLIC dataset with additional motion\nfeatures. We apply our architecture to this dataset and report significantly\nbetter performance than current state-of-the-art pose detection systems. \n\n"}
{"id": "1410.0630", "contents": "Title: Deep Directed Generative Autoencoders Abstract: For discrete data, the likelihood $P(x)$ can be rewritten exactly and\nparametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$\nhas enough capacity to put no probability mass on any $x'$ for which $f(x')\\neq\nf(x)$, where $f(\\cdot)$ is a deterministic discrete function. The log of the\nfirst factor gives rise to the log-likelihood reconstruction error of an\nautoencoder with $f(\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)\ndecoder. The log of the second term can be seen as a regularizer on the encoded\nactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder\ncan be represented by a deep neural network and trained to maximize the average\nof the optimal log-likelihood $\\log p(x)$. The objective is to learn an encoder\n$f(\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than\n$X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates\nprobability mass in a smaller number of (relevant) dimensions over which the\ndistribution factorizes. Generating samples from the model is straightforward\nusing ancestral sampling. One challenge is that regular back-propagation cannot\nbe used to obtain the gradient on the parameters of the encoder, but we find\nthat using the straight-through estimator works well here. We also find that\nalthough optimizing a single level of such architecture may be difficult, much\nbetter results can be obtained by pre-training and stacking them, gradually\ntransforming the data distribution into one that is more easily captured by a\nsimple parametric model. \n\n"}
{"id": "1410.1710", "contents": "Title: A Cost / Speed / Reliability Trade-off to Erasing Abstract: We present a KL-control treatment of the fundamental problem of erasing a\nbit. We introduce notions of \"reliability\" of information storage via a\nreliability timescale $\\tau_r$, and \"speed\" of erasing via an erasing timescale\n$\\tau_e$. Our problem formulation captures the tradeoff between speed,\nreliability, and the Kullback-Leibler (KL) cost required to erase a bit. We\nshow that rapid erasing of a reliable bit costs at least $\\log 2 - \\log\\left(1\n- \\operatorname{e}^{-\\frac{\\tau_e}{\\tau_r}}\\right) > \\log 2$, which goes to\n$\\frac{1}{2} \\log\\frac{2\\tau_r}{\\tau_e}$ when $\\tau_r>>\\tau_e$. \n\n"}
{"id": "1410.2006", "contents": "Title: Compositional Performance Certification of Interconnected Systems using\n  ADMM Abstract: A compositional performance certification method is presented for\ninterconnected systems using subsystem dissipativity properties and the\ninterconnection structure. A large-scale optimization problem is formulated to\nsearch for the most relevant dissipativity properties. The alternating\ndirection method of multipliers (ADMM) is employed to decompose and solve this\nproblem, and is demonstrated on several examples. \n\n"}
{"id": "1410.5179", "contents": "Title: A surgery result for the spectrum of the Dirichlet Laplacian Abstract: In this paper we give a method to geometrically modify an open set such that\nthe first $k$ eigenvalues of the Dirichlet Laplacian and its perimeter are not\nincreasing, its measure remains constant, and both perimeter and diameter\ndecrease below a certain threshold. The key point of the analysis relies on the\nproperties of the shape subsolutions for the torsion energy. \n\n"}
{"id": "1410.6956", "contents": "Title: Success and Failure of Adaptation-Diffusion Algorithms for Consensus in\n  Multi-Agent Networks Abstract: This paper investigates the problem of distributed stochastic approximation\nin multi-agent systems. The algorithm under study consists of two steps: a\nlocal stochastic approximation step and a diffusion step which drives the\nnetwork to a consensus. The diffusion step uses row-stochastic matrices to\nweight the network exchanges. As opposed to previous works, exchange matrices\nare not supposed to be doubly stochastic, and may also depend on the past\nestimate.\n  We prove that non-doubly stochastic matrices generally influence the limit\npoints of the algorithm. Nevertheless, the limit points are not affected by the\nchoice of the matrices provided that the latter are doubly-stochastic in\nexpectation. This conclusion legitimates the use of broadcast-like diffusion\nprotocols, which are easier to implement. Next, by means of a central limit\ntheorem, we prove that doubly stochastic protocols perform asymptotically as\nwell as centralized algorithms and we quantify the degradation caused by the\nuse of non doubly stochastic matrices. Throughout the paper, a special emphasis\nis put on the special case of distributed non-convex optimization as an\nillustration of our results. \n\n"}
{"id": "1410.7868", "contents": "Title: Dynamic Optimal Power Flow in Microgrids using the Alternating Direction\n  Method of Multipliers Abstract: Smart devices, storage and other distributed technologies have the potential\nto greatly improve the utilisation of network infrastructure and renewable\ngeneration. Decentralised control of these technologies overcomes many\nscalability and privacy concerns, but in general still requires the underlying\nproblem to be convex in order to guarantee convergence to a global optimum.\nConsidering that AC power flows are non-convex in nature, and the operation of\nhousehold devices often requires discrete decisions, there has been uncertainty\nsurrounding the use of distributed methods in a realistic setting. This paper\nextends prior work on the alternating direction method of multipliers (ADMM)\nfor solving the dynamic optimal power flow (D-OPF) problem. We utilise more\nrealistic line and load models, and introduce a two-stage approach to managing\ndiscrete decisions and uncertainty. Our experiments on a suburb-sized microgrid\nshow that this approach provides near optimal results, in a time that is fast\nenough for receding horizon control. This work brings distributed control of\nsmart-grid technologies closer to reality. \n\n"}
{"id": "1410.8060", "contents": "Title: ProbReach: Verified Probabilistic Delta-Reachability for Stochastic\n  Hybrid Systems Abstract: We present ProbReach, a tool for verifying probabilistic reachability for\nstochastic hybrid systems, i.e., computing the probability that the system\nreaches an unsafe region of the state space. In particular, ProbReach will\ncompute an arbitrarily small interval which is guaranteed to contain the\nrequired probability. Standard (non-probabilistic) reachability is undecidable\neven for linear hybrid systems. In ProbReach we adopt the weaker notion of\ndelta-reachability, in which the unsafe region is overapproximated by a\nuser-defined parameter (delta). This choice leads to false alarms, but also\nmakes the reachability problem decidable for virtually any hybrid system. In\nProbReach we have implemented a probabilistic version of delta-reachability\nthat is suited for hybrid systems whose stochastic behaviour is given in terms\nof random initial conditions. In this paper we introduce the capabilities of\nProbReach, give an overview of the parallel implementation, and present results\nfor several benchmarks involving highly non-linear hybrid systems. \n\n"}
{"id": "1411.0728", "contents": "Title: Approachability in Stackelberg Stochastic Games with Vector Costs Abstract: The notion of approachability was introduced by Blackwell [1] in the context\nof vector-valued repeated games. The famous Blackwell's approachability theorem\nprescribes a strategy for approachability, i.e., for `steering' the average\ncost of a given agent towards a given target set, irrespective of the\nstrategies of the other agents. In this paper, motivated by the multi-objective\noptimization/decision making problems in dynamically changing environments, we\naddress the approachability problem in Stackelberg stochastic games with vector\nvalued cost functions. We make two main contributions. Firstly, we give a\nsimple and computationally tractable strategy for approachability for\nStackelberg stochastic games along the lines of Blackwell's. Secondly, we give\na reinforcement learning algorithm for learning the approachable strategy when\nthe transition kernel is unknown. We also recover as a by-product Blackwell's\nnecessary and sufficient condition for approachability for convex sets in this\nset up and thus a complete characterization. We also give sufficient conditions\nfor non-convex sets. \n\n"}
{"id": "1411.1701", "contents": "Title: The Hierarchy of Circuit Diameters and Transportation Polytopes Abstract: The study of the diameter of the graph of polyhedra is a classical problem in\nthe theory of linear programming. While transportation polytopes are at the\ncore of operations research and statistics it is still open whether the Hirsch\nconjecture is true for general $m{\\times}n$--transportation polytopes. In\nearlier work the first three authors introduced a hierarchy of variations to\nthe notion of graph diameter in polyhedra. The key reason was that this\nhierarchy provides some interesting lower bounds for the usual graph diameter.\n  This paper has three contributions: First, we compare the hierarchy of\ndiameters for the $m{\\times}n$--transportation polytopes. We show that the\nHirsch conjecture bound of $m+n-1$ is actually valid in most of these diameter\nnotions. Second, we prove that for $3{\\times}n$--transportation polytopes the\nHirsch conjecture holds in the classical graph diameter. Third, we show for\n$2{\\times}n$--transportation polytopes that the stronger monotone Hirsch\nconjecture holds and improve earlier bounds on the graph diameter. \n\n"}
{"id": "1411.1971", "contents": "Title: Power-Law Graph Cuts Abstract: Algorithms based on spectral graph cut objectives such as normalized cuts,\nratio cuts and ratio association have become popular in recent years because\nthey are widely applicable and simple to implement via standard eigenvector\ncomputations. Despite strong performance for a number of clustering tasks,\nspectral graph cut algorithms still suffer from several limitations: first,\nthey require the number of clusters to be known in advance, but this\ninformation is often unknown a priori; second, they tend to produce clusters\nwith uniform sizes. In some cases, the true clusters exhibit a known size\ndistribution; in image segmentation, for instance, human-segmented images tend\nto yield segment sizes that follow a power-law distribution. In this paper, we\npropose a general framework of power-law graph cut algorithms that produce\nclusters whose sizes are power-law distributed, and also does not fix the\nnumber of clusters upfront. To achieve our goals, we treat the Pitman-Yor\nexchangeable partition probability function (EPPF) as a regularizer to graph\ncut objectives. Because the resulting objectives cannot be solved by relaxing\nvia eigenvectors, we derive a simple iterative algorithm to locally optimize\nthe objectives. Moreover, we show that our proposed algorithm can be viewed as\nperforming MAP inference on a particular Pitman-Yor mixture model. Our\nexperiments on various data sets show the effectiveness of our algorithms. \n\n"}
{"id": "1411.2664", "contents": "Title: Preserving Statistical Validity in Adaptive Data Analysis Abstract: A great deal of effort has been devoted to reducing the risk of spurious\nscientific discoveries, from the use of sophisticated validation techniques, to\ndeep statistical methods for controlling the false discovery rate in multiple\nhypothesis testing. However, there is a fundamental disconnect between the\ntheoretical results and the practice of data analysis: the theory of\nstatistical inference assumes a fixed collection of hypotheses to be tested, or\nlearning algorithms to be applied, selected non-adaptively before the data are\ngathered, whereas in practice data is shared and reused with hypotheses and new\nanalyses being generated on the basis of data exploration and the outcomes of\nprevious analyses.\n  In this work we initiate a principled study of how to guarantee the validity\nof statistical inference in adaptive data analysis. As an instance of this\nproblem, we propose and investigate the question of estimating the expectations\nof $m$ adaptively chosen functions on an unknown distribution given $n$ random\nsamples.\n  We show that, surprisingly, there is a way to estimate an exponential in $n$\nnumber of expectations accurately even if the functions are chosen adaptively.\nThis gives an exponential improvement over standard empirical estimators that\nare limited to a linear number of estimates. Our result follows from a general\ntechnique that counter-intuitively involves actively perturbing and\ncoordinating the estimates, using techniques developed for privacy\npreservation. We give additional applications of this technique to our\nquestion. \n\n"}
{"id": "1411.4433", "contents": "Title: Stochastic HYPE: Flow-based modelling of stochastic hybrid systems Abstract: Stochastic HYPE is a novel process algebra that models stochastic,\ninstantaneous and continuous behaviour. It develops the flow-based approach of\nthe hybrid process algebra HYPE by replacing non-urgent events with events with\nexponentially-distributed durations and also introduces random resets. The\nrandom resets allow for general stochasticity, and in particular allow for the\nuse of event durations drawn from distributions other than the exponential\ndistribution. To account for stochasticity, the semantics of stochastic HYPE\ntarget piecewise deterministic Markov processes (PDMPs), via intermediate\ntransition-driven stochastic hybrid automata (TDSHA) in contrast to the hybrid\nautomata used as semantic target for HYPE. Stochastic HYPE models have a\nspecific structure where the controller of a system is separate from the\ncontinuous aspect of this system providing separation of concerns and\nsupporting reasoning. A novel equivalence is defined which captures when two\nmodels have the same stochastic behaviour (as in stochastic bisimulation),\ninstantaneous behaviour (as in classical bisimulation) and continuous\nbehaviour. These techniques are illustrated via an assembly line example. \n\n"}
{"id": "1412.0683", "contents": "Title: Robust Stability Assessment in the Presence of Load Dynamics Uncertainty Abstract: Dynamic response of loads has a significant effect on system stability and\ndirectly determines the stability margin of the operating point. Inherent\nuncertainty and natural variability of load models make the stability\nassessment especially difficult and may compromise the security of the system.\nWe propose a novel mathematical \"robust stability\" criterion for the assessment\nof small-signal stability of operating points. Whenever the criterion is\nsatisfied for a given operating point, it provides mathematical guarantees that\nthe operating point will be stable with respect to small disturbances for any\ndynamic response of the loads. The criterion can be naturally used for\nidentification of operating regions secure from the occurrence of Hopf\nbifurcation. Several possible applications of the criterion are discussed, most\nimportantly the concept of Robust Stability Assessment (RSA) that could be\nintegrated in dynamic security assessment packages and used in contingency\nscreening and other planning and operational studies. \n\n"}
{"id": "1412.2817", "contents": "Title: Diffusion Estimation Over Cooperative Multi-Agent Networks With Missing\n  Data Abstract: In many fields, and especially in the medical and social sciences and in\nrecommender systems, data are gathered through clinical studies or targeted\nsurveys. Participants are generally reluctant to respond to all questions in a\nsurvey or they may lack information to respond adequately to some questions.\nThe data collected from these studies tend to lead to linear regression models\nwhere the regression vectors are only known partially: some of their entries\nare either missing completely or replaced randomly by noisy values. In this\nwork, assuming missing positions are replaced by noisy values, we examine how a\nconnected network of agents, with each one of them subjected to a stream of\ndata with incomplete regression information, can cooperate with each other\nthrough local interactions to estimate the underlying model parameters in the\npresence of missing data. We explain how to adjust the distributed diffusion\nthrough (de)regularization in order to eliminate the bias introduced by the\nincomplete model. We also propose a technique to recursively estimate the\n(de)regularization parameter and examine the performance of the resulting\nstrategy. We illustrate the results by considering two applications: one\ndealing with a mental health survey and the other dealing with a household\nconsumption survey. \n\n"}
{"id": "1412.4056", "contents": "Title: Blind system identification using kernel-based methods Abstract: We propose a new method for blind system identification. Resorting to a\nGaussian regression framework, we model the impulse response of the unknown\nlinear system as a realization of a Gaussian process. The structure of the\ncovariance matrix (or kernel) of such a process is given by the stable spline\nkernel, which has been recently introduced for system identification purposes\nand depends on an unknown hyperparameter. We assume that the input can be\nlinearly described by few parameters. We estimate these parameters, together\nwith the kernel hyperparameter and the noise variance, using an empirical Bayes\napproach. The related optimization problem is efficiently solved with a novel\niterative scheme based on the Expectation-Maximization method. In particular,\nwe show that each iteration consists of a set of simple update rules. We show,\nthrough some numerical experiments, very promising performance of the proposed\nmethod. \n\n"}
{"id": "1412.4927", "contents": "Title: Consensus of Multi-agent Systems Under State-dependent Information\n  Transmission Abstract: In this paper, we study the consensus problem for continuous-time and\ndiscrete-time multi-agent systems in state-dependent switching networks. In\neach case, we first consider the networks with fixed connectivity, in which the\ncommunication between adjacent agents always exists but the influence could\npossibly become negligible if the transmission distance is long enough. It is\nobtained that consensus can be reached under a restriction of either the\ndecaying rate of the transmission weight or the initial states of the agents.\nAfter then we investigate the networks with state-dependent connectivity, in\nwhich the information transmission between adjacent agents gradually vanishes\nif their distance exceeds a fixed range. In such networks, we prove that the\nrealization of consensus requires the validity of some initial conditions.\nFinally, the conclusions are applied to models with the transmission law of C-S\nmodel, opinion dynamics and the rendezvous problem, the corresponding\nsimulations are also presented. \n\n"}
{"id": "1412.4986", "contents": "Title: A Scalable Asynchronous Distributed Algorithm for Topic Modeling Abstract: Learning meaningful topic models with massive document collections which\ncontain millions of documents and billions of tokens is challenging because of\ntwo reasons: First, one needs to deal with a large number of topics (typically\nin the order of thousands). Second, one needs a scalable and efficient way of\ndistributing the computation across multiple machines. In this paper we present\na novel algorithm F+Nomad LDA which simultaneously tackles both these problems.\nIn order to handle large number of topics we use an appropriately modified\nFenwick tree. This data structure allows us to sample from a multinomial\ndistribution over $T$ items in $O(\\log T)$ time. Moreover, when topic counts\nchange the data structure can be updated in $O(\\log T)$ time. In order to\ndistribute the computation across multiple processor we present a novel\nasynchronous framework inspired by the Nomad algorithm of\n\\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperform\nstate-of-the-art on massive problems which involve millions of documents,\nbillions of words, and thousands of topics. \n\n"}
{"id": "1412.6115", "contents": "Title: Compressing Deep Convolutional Networks using Vector Quantization Abstract: Deep convolutional neural networks (CNN) has become the most promising method\nfor object recognition, repeatedly demonstrating record breaking results for\nimage classification and object detection in recent years. However, a very deep\nCNN generally involves many layers with millions of parameters, making the\nstorage of the network model to be extremely large. This prohibits the usage of\ndeep CNNs on resource limited hardware, especially cell phones or other\nembedded devices. In this paper, we tackle this model storage issue by\ninvestigating information theoretical vector quantization methods for\ncompressing the parameters of CNNs. In particular, we have found in terms of\ncompressing the most storage demanding dense connected layers, vector\nquantization methods have a clear gain over existing matrix factorization\nmethods. Simply applying k-means clustering to the weights or conducting\nproduct quantization can lead to a very good balance between model size and\nrecognition accuracy. For the 1000-category classification task in the ImageNet\nchallenge, we are able to achieve 16-24 times compression of the network with\nonly 1% loss of classification accuracy using the state-of-the-art CNN. \n\n"}
{"id": "1412.6621", "contents": "Title: Why does Deep Learning work? - A perspective from Group Theory Abstract: Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called pre-training: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper. \n\n"}
{"id": "1412.7840", "contents": "Title: Value Function in Maximum Hands-off Control Abstract: In this brief paper, we study the value function in maximum hands-off\ncontrol. Maximum hands-off control, also known as sparse control, is the\nL0-optimal control among the admissible controls. Although the L0 measure is\ndiscontinuous and non- convex, we prove that the value function, or the minimum\nL0 norm of the control, is a continuous and strictly convex function of the\ninitial state in the reachable set, under an assumption on the controlled plant\nmodel. This property is important, in particular, for discussing the\nsensitivity of the optimality against uncertainties in the initial state, and\nalso for investigating the stability by using the value function as a Lyapunov\nfunction in model predictive control. \n\n"}
{"id": "1412.8105", "contents": "Title: Kalman Filtering over Fading Channels: Zero-One Laws and Almost Sure\n  Stabilities Abstract: In this paper, we investigate probabilistic stability of Kalman filtering\nover fading channels modeled by $\\ast$-mixing random processes, where channel\nfading is allowed to generate non-stationary packet dropouts with temporal\nand/or spatial correlations. Upper/lower almost sure (a.s.) stabilities and\nabsolutely upper/lower a.s. stabilities are defined for characterizing the\nsample-path behaviors of the Kalman filtering. We prove that both upper and\nlower a.s. stabilities follow a zero-one law, i.e., these stabilities must\nhappen with a probability either zero or one, and when the filtering system is\none-step observable, the absolutely upper and lower a.s. stabilities can also\nbe interpreted using a zero-one law. We establish general stability conditions\nfor (absolutely) upper and lower a.s. stabilities. In particular, with one-step\nobservability, we show the equivalence between absolutely a.s. stabilities and\na.s. ones, and necessary and sufficient conditions in terms of packet arrival\nrate are derived; for the so-called non-degenerate systems, we also manage to\ngive a necessary and sufficient condition for upper a.s. stability. \n\n"}
{"id": "1501.00892", "contents": "Title: On the trade-off between control performance and communication cost in\n  event-triggered control Abstract: We consider a stochastic system where the communication between the\ncontroller and the actuator is triggered by a threshold-based rule. The\ncommunication is performed across an unreliable link that stochastically erases\ntransmitted packets. To decrease the communication burden, and as a partial\nprotection against dropped packets, the controller sends a sequence of control\ncommands to the actuator in each packet. These commands are stored in a buffer\nand applied sequentially until the next control packet arrives. In this\ncontext, we study dead-beat control laws and compute the expected\nlinear-quadratic loss of the closed-loop system for any given event-threshold.\nFurthermore, we provide analytical expressions that quantify the trade-off\nbetween the communication cost and the control performance of event-triggered\ncontrol systems. Numerical examples demonstrate the effectiveness of the\nproposed framework. \n\n"}
{"id": "1501.01324", "contents": "Title: An Evolution Strategy Method for Optimizing Machining Parameters of\n  Milling Operations Abstract: In this paper, an evolutionary strategy (ES) method is introduced as an\noptimization approach to solve problems in the manufacturing area. The ES\nmethod is applied to a case study for milling operations. The results show that\nit can effectively yield good results. \n\n"}
{"id": "1501.01701", "contents": "Title: Distributed Resource Allocation for Epidemic control Abstract: We present a distributed resource allocation strategy to control an epidemic\noutbreak in a networked population based on a Distributed Alternating Direction\nMethod of Multipliers (D-ADMM) algorithm. We consider a linearized Susceptible-\nInfected-Susceptible (SIS) epidemic spreading model in which agents in the\nnetwork are able to allocate vaccination resources (for prevention) and\nantidotes (for treatment) in the presence of a contagion. We express our\nepidemic control condition as a spectral constraint involving the\nPerron-Frobenius eigenvalue, and formulate the resource allocation problem as a\nGeometric Program (GP). Next, we separate the network-wide optimization problem\ninto subproblems optimally solved by each agent in a fully distributed way. We\nconclude the paper by illustrating performance of our solution framework with\nnumerical simulations. \n\n"}
{"id": "1501.03975", "contents": "Title: Stochastic Gradient Based Extreme Learning Machines For Online Learning\n  of Advanced Combustion Engines Abstract: In this article, a stochastic gradient based online learning algorithm for\nExtreme Learning Machines (ELM) is developed (SG-ELM). A stability criterion\nbased on Lyapunov approach is used to prove both asymptotic stability of\nestimation error and stability in the estimated parameters suitable for\nidentification of nonlinear dynamic systems. The developed algorithm not only\nguarantees stability, but also reduces the computational demand compared to the\nOS-ELM approach based on recursive least squares. In order to demonstrate the\neffectiveness of the algorithm on a real-world scenario, an advanced combustion\nengine identification problem is considered. The algorithm is applied to two\ncase studies: An online regression learning for system identification of a\nHomogeneous Charge Compression Ignition (HCCI) Engine and an online\nclassification learning (with class imbalance) for identifying the dynamic\noperating envelope of the HCCI Engine. The results indicate that the accuracy\nof the proposed SG-ELM is comparable to that of the state-of-the-art but adds\nstability and a reduction in computational effort. \n\n"}
{"id": "1501.05469", "contents": "Title: An Improved Stability Condition for Kalman Filtering with Bounded\n  Markovian Packet Losses Abstract: In this paper, we consider the peak-covariance stability of Kalman filtering\nsubject to packet losses. The length of consecutive packet losses is governed\nby a time-homogeneous finite-state Markov chain. We establish a sufficient\ncondition for peak-covariance stability and show that this stability check can\nbe recast as a linear matrix inequality (LMI) feasibility problem. Comparing\nwith the literature, the stability condition given in this paper is invariant\nwith respect to similarity state transformations; moreover, our condition is\nproved to be less conservative than the existing results. Numerical examples\nare provided to demonstrate the effectiveness of our result. \n\n"}
{"id": "1501.06006", "contents": "Title: Stochastic Analysis of Synchronization in a Supermarket Refrigeration\n  System Abstract: Display cases in supermarket systems often exhibit synchronization, in which\nthe expansion valves in the display cases turn on and off at exactly the same\ntime. The study of the influence of switching noise on synchronization in\nsupermarket refrigeration systems is the subject matter of this work. For this\npurpose, we model it as a hybrid system, for which synchronization corresponds\nto a periodic trajectory. Subsequently, we investigate the influence of\nswitching noise. We develop a statistical method for computing an intensity\nfunction, which measures how often the refrigeration system stays synchronized.\nBy analyzing the intensity, we conclude that the increase in measurement\nuncertainty yields the decrease at the prevalence of synchronization. \n\n"}
{"id": "1501.06095", "contents": "Title: Between Pure and Approximate Differential Privacy Abstract: We show a new lower bound on the sample complexity of $(\\varepsilon,\n\\delta)$-differentially private algorithms that accurately answer statistical\nqueries on high-dimensional databases. The novelty of our bound is that it\ndepends optimally on the parameter $\\delta$, which loosely corresponds to the\nprobability that the algorithm fails to be private, and is the first to\nsmoothly interpolate between approximate differential privacy ($\\delta > 0$)\nand pure differential privacy ($\\delta = 0$).\n  Specifically, we consider a database $D \\in \\{\\pm1\\}^{n \\times d}$ and its\n\\emph{one-way marginals}, which are the $d$ queries of the form \"What fraction\nof individual records have the $i$-th bit set to $+1$?\" We show that in order\nto answer all of these queries to within error $\\pm \\alpha$ (on average) while\nsatisfying $(\\varepsilon, \\delta)$-differential privacy, it is necessary that\n$$ n \\geq \\Omega\\left( \\frac{\\sqrt{d \\log(1/\\delta)}}{\\alpha \\varepsilon}\n\\right), $$ which is optimal up to constant factors. To prove our lower bound,\nwe build on the connection between \\emph{fingerprinting codes} and lower bounds\nin differential privacy (Bun, Ullman, and Vadhan, STOC'14).\n  In addition to our lower bound, we give new purely and approximately\ndifferentially private algorithms for answering arbitrary statistical queries\nthat improve on the sample complexity of the standard Laplace and Gaussian\nmechanisms for achieving worst-case accuracy guarantees by a logarithmic\nfactor. \n\n"}
{"id": "1502.01053", "contents": "Title: Quantized Consensus by the ADMM: Probabilistic versus Deterministic\n  Quantizers Abstract: This paper develops efficient algorithms for distributed average consensus\nwith quantized communication using the alternating direction method of\nmultipliers (ADMM). We first study the effects of probabilistic and\ndeterministic quantizations on a distributed ADMM algorithm. With probabilistic\nquantization, this algorithm yields linear convergence to the desired average\nin the mean sense with a bounded variance. When deterministic quantization is\nemployed, the distributed ADMM either converges to a consensus or cycles with a\nfinite period after a finite-time iteration. In the cyclic case, local\nquantized variables have the same mean over one period and hence each node can\nalso reach a consensus. We then obtain an upper bound on the consensus error\nwhich depends only on the quantization resolution and the average degree of the\nnetwork. Finally, we propose a two-stage algorithm which combines both\nprobabilistic and deterministic quantizations. Simulations show that the\ntwo-stage algorithm, without picking small algorithm parameter, has consensus\nerrors that are typically less than one quantization resolution for all\nconnected networks where agents' data can be of arbitrary magnitudes. \n\n"}
{"id": "1502.01057", "contents": "Title: Personalized Web Search Abstract: Personalization is important for search engines to improve user experience.\nMost of the existing work do pure feature engineering and extract a lot of\nsession-style features and then train a ranking model. Here we proposed a novel\nway to model both long term and short term user behavior using Multi-armed\nbandit algorithm. Our algorithm can generalize session information across users\nwell, and as an Explore-Exploit style algorithm, it can generalize to new urls\nand new users well. Experiments show that our algorithm can improve performance\nover the default ranking and outperforms several popular Multi-armed bandit\nalgorithms. \n\n"}
{"id": "1502.01852", "contents": "Title: Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n  ImageNet Classification Abstract: Rectified activation units (rectifiers) are essential for state-of-the-art\nneural networks. In this work, we study rectifier neural networks for image\nclassification from two aspects. First, we propose a Parametric Rectified\nLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLU\nimproves model fitting with nearly zero extra computational cost and little\noverfitting risk. Second, we derive a robust initialization method that\nparticularly considers the rectifier nonlinearities. This method enables us to\ntrain extremely deep rectified models directly from scratch and to investigate\ndeeper or wider network architectures. Based on our PReLU networks\n(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012\nclassification dataset. This is a 26% relative improvement over the ILSVRC 2014\nwinner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass\nhuman-level performance (5.1%, Russakovsky et al.) on this visual recognition\nchallenge. \n\n"}
{"id": "1502.02074", "contents": "Title: Counting real critical points of the distance to orthogonally invariant\n  matrix sets Abstract: Minimizing the Euclidean distance to a set arises frequently in applications.\nWhen the set is algebraic, a measure of complexity of this optimization problem\nis its number of critical points. In this paper we provide a general framework\nto compute and count the real smooth critical points of a data matrix on an\northogonally invariant set of matrices. The technique relies on \"transfer\nprinciples\" that allow calculations to be done in the space of singular values\nof the matrices in the orthogonally invariant set. The calculations often\nsimplify greatly and yield transparent formulas. We illustrate the method on\nseveral examples, and compare our results to the recently introduced notion of\nEuclidean distance degree of an algebraic variety. \n\n"}
{"id": "1502.02558", "contents": "Title: K2-ABC: Approximate Bayesian Computation with Kernel Embeddings Abstract: Complicated generative models often result in a situation where computing the\nlikelihood of observed data is intractable, while simulating from the\nconditional density given a parameter value is relatively easy. Approximate\nBayesian Computation (ABC) is a paradigm that enables simulation-based\nposterior inference in such cases by measuring the similarity between simulated\nand observed data in terms of a chosen set of summary statistics. However,\nthere is no general rule to construct sufficient summary statistics for complex\nmodels. Insufficient summary statistics will \"leak\" information, which leads to\nABC algorithms yielding samples from an incorrect (partial) posterior. In this\npaper, we propose a fully nonparametric ABC paradigm which circumvents the need\nfor manually selecting summary statistics. Our approach, K2-ABC, uses maximum\nmean discrepancy (MMD) as a dissimilarity measure between the distributions\nover observed and simulated data. MMD is easily estimated as the squared\ndifference between their empirical kernel embeddings. Experiments on a\nsimulated scenario and a real-world biological problem illustrate the\neffectiveness of the proposed algorithm. \n\n"}
{"id": "1502.02704", "contents": "Title: Learning Reductions that Really Work Abstract: We provide a summary of the mathematical and computational techniques that\nhave enabled learning reductions to effectively address a wide class of\nproblems, and show that this approach to solving machine learning problems can\nbe broadly useful. \n\n"}
{"id": "1502.03296", "contents": "Title: Statistical laws in linguistics Abstract: Zipf's law is just one out of many universal laws proposed to describe\nstatistical regularities in language. Here we review and critically discuss how\nthese laws can be statistically interpreted, fitted, and tested (falsified).\nThe modern availability of large databases of written text allows for tests\nwith an unprecedent statistical accuracy and also a characterization of the\nfluctuations around the typical behavior. We find that fluctuations are usually\nmuch larger than expected based on simplifying statistical assumptions (e.g.,\nindependence and lack of correlations between observations).These\nsimplifications appear also in usual statistical tests so that the large\nfluctuations can be erroneously interpreted as a falsification of the law.\nInstead, here we argue that linguistic laws are only meaningful (falsifiable)\nif accompanied by a model for which the fluctuations can be computed (e.g., a\ngenerative model of the text). The large fluctuations we report show that the\nconstraints imposed by linguistic laws on the creativity process of text\ngeneration are not as tight as one could expect. \n\n"}
{"id": "1502.03630", "contents": "Title: Ordering-sensitive and Semantic-aware Topic Modeling Abstract: Topic modeling of textual corpora is an important and challenging problem. In\nmost previous work, the \"bag-of-words\" assumption is usually made which ignores\nthe ordering of words. This assumption simplifies the computation, but it\nunrealistically loses the ordering information and the semantic of words in the\ncontext. In this paper, we present a Gaussian Mixture Neural Topic Model\n(GMNTM) which incorporates both the ordering of words and the semantic meaning\nof sentences into topic modeling. Specifically, we represent each topic as a\ncluster of multi-dimensional vectors and embed the corpus into a collection of\nvectors generated by the Gaussian mixture model. Each word is affected not only\nby its topic, but also by the embedding vector of its surrounding words and the\ncontext. The Gaussian mixture components and the topic of documents, sentences\nand words can be learnt jointly. Extensive experiments show that our model can\nlearn better topics and more accurate word distributions for each topic.\nQuantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM\nobtains significantly better performance in terms of perplexity, retrieval\naccuracy and classification accuracy. \n\n"}
{"id": "1502.04548", "contents": "Title: Real-Time Stochastic Optimal Control for Multi-agent Quadrotor Systems Abstract: This paper presents a novel method for controlling teams of unmanned aerial\nvehicles using Stochastic Optimal Control (SOC) theory. The approach consists\nof a centralized high-level planner that computes optimal state trajectories as\nvelocity sequences, and a platform-specific low-level controller which ensures\nthat these velocity sequences are met. The planning task is expressed as a\ncentralized path-integral control problem, for which optimal control\ncomputation corresponds to a probabilistic inference problem that can be solved\nby efficient sampling methods. Through simulation we show that our SOC approach\n(a) has significant benefits compared to deterministic control and other SOC\nmethods in multimodal problems with noise-dependent optimal solutions, (b) is\ncapable of controlling a large number of platforms in real-time, and (c) yields\ncollective emergent behaviour in the form of flight formations. Finally, we\nshow that our approach works for real platforms, by controlling a team of three\nquadrotors in outdoor conditions. \n\n"}
{"id": "1502.04622", "contents": "Title: Particle Gibbs for Bayesian Additive Regression Trees Abstract: Additive regression trees are flexible non-parametric models and popular\noff-the-shelf tools for real-world non-linear regression. In application\ndomains, such as bioinformatics, where there is also demand for probabilistic\npredictions with measures of uncertainty, the Bayesian additive regression\ntrees (BART) model, introduced by Chipman et al. (2010), is increasingly\npopular. As data sets have grown in size, however, the standard\nMetropolis-Hastings algorithms used to perform inference in BART are proving\ninadequate. In particular, these Markov chains make local changes to the trees\nand suffer from slow mixing when the data are high-dimensional or the best\nfitting trees are more than a few layers deep. We present a novel sampler for\nBART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a\ntop-down particle filtering algorithm for Bayesian decision trees\n(Lakshminarayanan et al., 2013). Rather than making local changes to individual\ntrees, the PG sampler proposes a complete tree to fit the residual. Experiments\nshow that the PG sampler outperforms existing samplers in many settings. \n\n"}
{"id": "1502.04846", "contents": "Title: Variational analysis and regularity of the minimum time function for\n  differential inclusions Abstract: We study the time optimal control problem for differential inclusions with a\ngeneral closed target. We first give the representation of the proximal\nhorizontal subgradients of the minimum time function $\\mathcal{T}$ and then,\ntogether with the representation of the proximal subgradients, we obtain some\nrelationships between the normal cones to the sublevel of $\\mathcal{T}$ and the\nnormal cones to its epigraph. The relationships allow us to get the propagation\nof the proximal subdifferential as well as of the proximal horizontal\nsubdifferential of $\\mathcal{T}$ along optimal trajectories. Finally, we show,\nunder suitable assumptions, that the epigraph of $\\mathcal{T}$ is\n$\\varphi$-convex near the target. This is the first nonlinear\n$\\varphi$-convexity result valid in any dimension. \n\n"}
{"id": "1502.06131", "contents": "Title: Unimodular Binary Hierarchical Models Abstract: Associated to each simplicial complex is a binary hierarchical model. We\nclassify the simplicial complexes that yield unimodular binary hierarchical\nmodels. Our main theorem provides both a construction of all unimodular binary\nhierarchical models, together with a characterization in terms of excluded\nminors, where our definition of a minor allows the taking of links and induced\ncomplexes. A key tool in the proof is the lemma that the class of unimodular\nbinary hierarchical models is closed under the Alexander duality operation on\nsimplicial complexes. \n\n"}
{"id": "1503.00215", "contents": "Title: Optimal mass transport over bridges Abstract: We present an overview of our recent work on implementable solutions to the\nSchroedinger bridge problem and their potential application to optimal\ntransport and various generalizations. \n\n"}
{"id": "1503.00411", "contents": "Title: Vector Fitting for Matrix-valued Rational Approximation Abstract: Vector Fitting (VF) is a popular method of constructing rational approximants\nthat provides a least squares fit to frequency response measurements. In an\nearlier work, we provided an analysis of VF for scalar-valued rational\nfunctions and established a connection with optimal $H_2$ approximation. We\nbuild on this work and extend the previous framework to include the\nconstruction of effective rational approximations to matrix-valued functions, a\nproblem which presents significant challenges that do not appear in the scalar\ncase. Transfer functions associated with multi-input/multi-output (MIMO)\ndynamical systems typify the class of functions that we consider here. Others\nhave also considered extensions of VF to matrix-valued functions and related\nnumerical implementations are readily available. However to our knowledge, a\ndetailed analysis of numerical issues that arise does not yet exist. We offer\nsuch an analysis including critical implementation details here.\n  One important issue that arises for VF on matrix-valued functions that has\nremained largely unaddressed is the control of the McMillan degree of the\nresulting rational approximant; the McMillan degree can grow very high in the\ncase of large input/output dimensions. We introduce two new mechanisms for\ncontrolling the McMillan degree of the final approximant, one based on\nalternating least-squares minimization and one based on ancillary\nsystem-theoretic reduction methods. Motivated in part by our earlier work on\nthe scalar VF problem as well as by recent innovations for computing optimal\n$H_2$ approximation, we establish a connection with optimal $H_2$\napproximation, and are able to improve significantly the fidelity of VF through\nnumerical quadrature, with virtually no increase in cost or complexity. We\nprovide several numerical examples to support the theoretical discussion and\nproposed algorithms. \n\n"}
{"id": "1503.00765", "contents": "Title: LDDMM Surface Registration with Atrophy Constraints Abstract: Diffeomorphic registration using optimal control on the diffeomorphism group\nand on shape spaces has become widely used since the development of the Large\nDeformation Diffeomorphic Metric Mapping (LDDMM) algorithm. More recently, a\nseries of algorithms involving sub-riemannian constraints have been introduced,\nin which the velocity fields that control the shapes in the LDDMM framework are\nconstrained in accordance with a specific deformation model. Here, we extend\nthis setting by considering, for the first time, inequality constraints, in\norder to estimate surface deformations that only allow for atrophy, introducing\nfor this purpose an algorithm that uses the augmented lagrangian method. We\nalso provide a version of our approach that uses a weaker constraint in which\nonly the total volume is forced to decrease. These developments are illustrated\nby numerical experiments on brain data. \n\n"}
{"id": "1503.00923", "contents": "Title: An Interoperable Realization of Smart Cities with Plug and Play based\n  Device Management Abstract: The primal problem with Internet of Things (IoT) solutions for smart cities\nis the lack of interoperability at various levels, and more predominately at\nthe device level. While there exist multitude of platforms from multiple\nmanufacturers, the existing ecosystem still remains highly closed. In this\npaper, we propose SNaaS or Sensor/Network as a Service: a service layer that\nenables the creation of the plug-n-play infrastructure, across platforms from\nmultiple vendors, necessary for interoperability and successful deployment of\nlarge-scale city wide systems. In order to correctly position the new service\nlayer, we present a high level reference IoT architecture for smart city\nimplementations, and follow it up with the workflow details of SNaaS along with\npreliminary microbenchmarks. \n\n"}
{"id": "1503.01407", "contents": "Title: Invariant EKF Design for Scan Matching-aided Localization Abstract: Localization in indoor environments is a technique which estimates the\nrobot's pose by fusing data from onboard motion sensors with readings of the\nenvironment, in our case obtained by scan matching point clouds captured by a\nlow-cost Kinect depth camera. We develop both an Invariant Extended Kalman\nFilter (IEKF)-based and a Multiplicative Extended Kalman Filter (MEKF)-based\nsolution to this problem. The two designs are successfully validated in\nexperiments and demonstrate the advantage of the IEKF design. \n\n"}
{"id": "1503.01436", "contents": "Title: Class Probability Estimation via Differential Geometric Regularization Abstract: We study the problem of supervised learning for both binary and multiclass\nclassification from a unified geometric perspective. In particular, we propose\na geometric regularization technique to find the submanifold corresponding to a\nrobust estimator of the class probability $P(y|\\pmb{x})$. The regularization\nterm measures the volume of this submanifold, based on the intuition that\noverfitting produces rapid local oscillations and hence large volume of the\nestimator. This technique can be applied to regularize any classification\nfunction that satisfies two requirements: firstly, an estimator of the class\nprobability can be obtained; secondly, first and second derivatives of the\nclass probability estimator can be calculated. In experiments, we apply our\nregularization technique to standard loss functions for classification, our\nRBF-based implementation compares favorably to widely used regularization\nmethods for both binary and multiclass classification. \n\n"}
{"id": "1503.01457", "contents": "Title: Time Averaged Consensus in a Direct Coupled Distributed Coherent Quantum\n  Observer Abstract: This paper considers the problem of constructing a distributed direct\ncoupling quantum observer for a closed linear quantum system. The proposed\ndistributed observer consists of a network of quantum harmonic oscillators and\nit is shown that the distributed observer converges to a consensus in a time\naveraged sense in which each component of the observer estimates the specified\noutput of the quantum plant. An example and simulations are included to\nillustrate the properties of the distributed observer. \n\n"}
{"id": "1503.01563", "contents": "Title: Convex Optimization for Parallel Energy Minimization Abstract: Energy minimization has been an intensely studied core problem in computer\nvision. With growing image sizes (2D and 3D), it is now highly desirable to run\nenergy minimization algorithms in parallel. But many existing algorithms, in\nparticular, some efficient combinatorial algorithms, are difficult to\npar-allelize. By exploiting results from convex and submodular theory, we\nreformulate the quadratic energy minimization problem as a total variation\ndenoising problem, which, when viewed geometrically, enables the use of\nprojection and reflection based convex methods. The resulting min-cut algorithm\n(and code) is conceptually very simple, and solves a sequence of TV denoising\nproblems. We perform an extensive empirical evaluation comparing\nstate-of-the-art combinatorial algorithms and convex optimization techniques.\nOn small problems the iterative convex methods match the combinatorial max-flow\nalgorithms, while on larger problems they offer other flexibility and important\ngains: (a) their memory footprint is small; (b) their straightforward\nparallelizability fits multi-core platforms; (c) they can easily be\nwarm-started; and (d) they quickly reach approximately good solutions, thereby\nenabling faster \"inexact\" solutions. A key consequence of our approach based on\nsubmodularity and convexity is that it is allows to combine any arbitrary\ncombinatorial or convex methods as subroutines, which allows one to obtain\nhybrid combinatorial and convex optimization algorithms that benefit from the\nstrengths of both. \n\n"}
{"id": "1503.02427", "contents": "Title: Syntax-based Deep Matching of Short Texts Abstract: Many tasks in natural language processing, ranging from machine translation\nto question answering, can be reduced to the problem of matching two sentences\nor more generally two short texts. We propose a new approach to the problem,\ncalled Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The\napproach consists of two components, 1) a mining algorithm to discover patterns\nfor matching two short-texts, defined in the product space of dependency trees,\nand 2) a deep neural network for matching short texts using the mined patterns,\nas well as a learning algorithm to build the network having a sparse structure.\nWe test our algorithm on the problem of matching a tweet and a response in\nsocial media, a hard matching problem proposed in [Wang et al., 2013], and show\nthat DeepMatch$_{tree}$ can outperform a number of competitor models including\none without using dependency trees and one based on word-embedding, all with\nlarge margins \n\n"}
{"id": "1503.02510", "contents": "Title: Compositional Distributional Semantics with Long Short Term Memory Abstract: We are proposing an extension of the recursive neural network that makes use\nof a variant of the long short-term memory architecture. The extension allows\ninformation low in parse trees to be stored in a memory register (the `memory\ncell') and used much later higher up in the parse tree. This provides a\nsolution to the vanishing gradient problem and allows the network to capture\nlong range dependencies. Experimental results show that our composition\noutperformed the traditional neural-network composition on the Stanford\nSentiment Treebank. \n\n"}
{"id": "1503.03033", "contents": "Title: On the Complexity of Parallel Coordinate Descent Abstract: In this work we study the parallel coordinate descent method (PCDM) proposed\nby Richt\\'arik and Tak\\'a\\v{c} [26] for minimizing a regularized convex\nfunction. We adopt elements from the work of Xiao and Lu [39], and combine them\nwith several new insights, to obtain sharper iteration complexity results for\nPCDM than those presented in [26]. Moreover, we show that PCDM is monotonic in\nexpectation, which was not confirmed in [26], and we also derive the first high\nprobability iteration complexity result where the initial levelset is\nunbounded. \n\n"}
{"id": "1503.03715", "contents": "Title: Feedback Refinement Relations for the Synthesis of Symbolic Controllers Abstract: We present an abstraction and refinement methodology for the automated\ncontroller synthesis to enforce general predefined specifications. The designed\ncontrollers require quantized (or symbolic) state information only and can be\ninterfaced with the system via a static quantizer. Both features are\nparticularly important with regard to any practical implementation of the\ndesigned controllers and, as we prove, are characterized by the existence of a\nfeedback refinement relation between plant and abstraction. Feedback refinement\nrelations are a novel concept introduced in this paper. Our work builds on a\ngeneral notion of system with set-valued dynamics and possibly\nnon-deterministic quantizers to permit the synthesis of controllers that\nrobustly, and provably, enforce the specification in the presence of various\ntypes of uncertainties and disturbances. We identify a class of abstractions\nthat is canonical in a well-defined sense, and provide a method to efficiently\ncompute canonical abstractions. We demonstrate the practicality of our approach\non two examples. \n\n"}
{"id": "1503.03952", "contents": "Title: A Switched Dynamical System Framework for Analysis of Massively Parallel\n  Asynchronous Numerical Algorithms Abstract: In the near future, massively parallel computing systems will be necessary to\nsolve computation intensive applications. The key bottleneck in massively\nparallel implementation of numerical algorithms is the synchronization of data\nacross processing elements (PEs) after each iteration, which results in\nsignificant idle time. Thus, there is a trend towards relaxing the\nsynchronization and adopting an asynchronous model of computation to reduce\nidle time. However, it is not clear what is the effect of this relaxation on\nthe stability and accuracy of the numerical algorithm. In this paper we present\na new framework to analyze such algorithms. We treat the computation in each PE\nas a dynamical system and model the asynchrony as stochastic switching. The\noverall system is then analyzed as a switched dynamical system. However,\nmodeling of massively parallel numerical algorithms as switched dynamical\nsystems results in a very large number of modes, which makes current analysis\ntools available for such systems computationally intractable. We develop new\ntechniques that circumvent this scalability issue. The framework is presented\non a one-dimensional heat equation as a case study and the proposed analysis\nframework is verified by solving the partial differential equation (PDE) in a\n$\\mathtt{nVIDIA\\: Tesla^{\\scriptsize{TM}}}$ GPU machine, with asynchronous\ncommunication between cores. \n\n"}
{"id": "1503.06408", "contents": "Title: Automated Linear Function Submission-based Double Auction as Bottom-up\n  Real-Time Pricing in a Regional Prosumers' Electricity Network Abstract: A linear function submission-based double-auction (LFS-DA) mechanism for a\nregional electricity network is proposed in this paper. Each agent in the\nnetwork is equipped with a battery and a generator. Each agent simultaneously\nbecomes a producer and consumer of electricity, i.e., a prosumer and trades\nelectricity in the regional market at a variable price. In the LFS-DA, each\nagent uses linear demand and supply functions when they submit bids and asks to\nan auctioneer in the regional market.The LFS-DA can achieve an exact balance\nbetween electricity demand and supply for each time slot throughout the\nlearning phase and was shown capable of solving the primal problem of\nmaximizing the social welfare of the network without any central price setter,\ne.g., a utility or a large electricity company, in contrast with conventional\nreal-time pricing (RTP). This paper presents a clarification of the\nrelationship between the RTP algorithm derived on the basis of a dual\ndecomposition framework and LFS-DA. Specifically, we proved that the changes in\nthe price profile of the LFS-DA mechanism are equal to those achieved by the\nRTP mechanism derived from the dual decomposition framework except for a\nconstant factor. \n\n"}
{"id": "1503.07561", "contents": "Title: Primal robustness and semidefinite cones Abstract: This paper reformulates and streamlines the core tools of robust stability\nand performance for LTI systems using now-standard methods in convex\noptimization. In particular, robustness analysis can be formulated directly as\na primal convex (semidefinite program or SDP) optimization problem using sets\nof gramians whose closure is a semidefinite cone. This allows various\nconstraints such as structured uncertainty to be included directly, and\nworst-case disturbances and perturbations constructed directly from the primal\nvariables. Well known results such as the KYP lemma and various scaled small\ngain tests can also be obtained directly through standard SDP duality. To\nreaders familiar with robustness and SDPs, the framework should appear obvious,\nif only in retrospect. But this is also part of its appeal and should enhance\npedagogy, and we hope suggest new research. There is a key lemma proving\nclosure of a grammian that is also obvious but our current proof appears\nunnecessarily cumbersome, and a final aim of this paper is to enlist the help\nof experts in robust control and convex optimization in finding simpler\nalternatives. \n\n"}
{"id": "1503.07621", "contents": "Title: The Evolution of Network Entropy in Classical and Quantum Consensus\n  Dynamics Abstract: In this paper, we investigate the evolution of the network entropy for\nconsensus dynamics in classical or quantum networks. We show that in the\nclassical case, the network entropy decreases at the consensus limit if the\nnode initial values are i.i.d. Bernoulli random variables, and the network\ndifferential entropy is monotonically non-increasing if the node initial values\nare i.i.d. Gaussian. While for quantum consensus dynamics, the network's von\nNeumann entropy is in contrast non-decreasing. In light of this inconsistency,\nwe compare several gossiping algorithms with random or deterministic\ncoefficients for classical or quantum networks, and show that quantum gossiping\nalgorithms with deterministic coefficients are physically related to classical\ngossiping algorithms with random coefficients. \n\n"}
{"id": "1503.08085", "contents": "Title: Evolutionary Poisson Games for Controlling Large Population Behaviors Abstract: Emerging applications in engineering such as crowd-sourcing and\n(mis)information propagation involve a large population of heterogeneous users\nor agents in a complex network who strategically make dynamic decisions. In\nthis work, we establish an evolutionary Poisson game framework to capture the\nrandom, dynamic and heterogeneous interactions of agents in a holistic fashion,\nand design mechanisms to control their behaviors to achieve a system-wide\nobjective. We use the antivirus protection challenge in cyber security to\nmotivate the framework, where each user in the network can choose whether or\nnot to adopt the software. We introduce the notion of evolutionary Poisson\nstable equilibrium for the game, and show its existence and uniqueness. Online\nalgorithms are developed using the techniques of stochastic approximation\ncoupled with the population dynamics, and they are shown to converge to the\noptimal solution of the controller problem. Numerical examples are used to\nillustrate and corroborate our results. \n\n"}
{"id": "1503.08244", "contents": "Title: Distribution System Outage Detection using Consumer Load and Line Flow\n  Measurements Abstract: An outage detection framework for power distribution networks is proposed.\nGiven the tree structure of the distribution system, a method is developed\ncombining the use of real-time power flow measurements on edges of the tree\nwith load forecasts at the nodes of the tree. A maximum a posteriori detector\n{\\color{black} (MAP)} is formulated for arbitrary number and location of\noutages on trees which is shown to have an efficient detector. A framework\nrelying on the maximum missed detection probability is used for optimal sensor\nplacement and is solved for tree networks. Finally, a set of case studies is\nconsidered using feeder data from the Pacific Northwest National Laboratories.\nWe show that a 10\\% loss in mean detection reliability network wide reduces the\nrequired sensor density by 60 \\% for a typical feeder if efficient use of\nmeasurements is performed. \n\n"}
{"id": "1503.08473", "contents": "Title: Bearing-Based Distributed Control and Estimation of Multi-Agent Systems Abstract: This paper studies the distributed control and estimation of multi-agent\nsystems based on bearing information. In particular, we consider two problems:\n(i) the distributed control of bearing-constrained formations using relative\nposition measurements and (ii) the distributed localization of sensor networks\nusing bearing measurements. Both of the two problems are considered in\narbitrary dimensional spaces. The analyses of the two problems rely on the\nrecently developed bearing rigidity theory. We show that the two problems have\nthe same mathematical formulation and can be solved by identical protocols. The\nproposed controller and estimator can globally solve the two problems without\nambiguity. The results are supported with illustrative simulations. \n\n"}
{"id": "1504.00057", "contents": "Title: Optimal Power Flow with Weighted Chance Constraints and General Policies\n  for Generation Control Abstract: Due to the increasing amount of electricity generated from renewable sources,\nuncertainty in power system operation will grow. This has implications for\ntools such as Optimal Power Flow (OPF), an optimization problem widely used in\npower system operations and planning, which should be adjusted to account for\nthis uncertainty. One way to handle the uncertainty is to formulate a Chance\nConstrained OPF (CC-OPF) which limits the probability of constraint violation\nto a predefined value. However, existing CC-OPF formulations and solutions are\nnot immune to drawbacks. On one hand, they only consider affine policies for\ngeneration control, which are not always realistic and may be sub-optimal. On\nthe other hand, the standard CC-OPF formulations do not distinguish between\nlarge and small violations, although those might carry significantly different\nrisk. In this paper, we introduce the Weighted CC-OPF (WCC-OPF) that can handle\ngeneral control policies while preserving convexity and allowing for efficient\ncomputation. The weighted chance constraints account for the size of violations\nthrough a weighting function, which assigns a higher risk to a higher\noverloads. We prove that the problem remains convex for any convex weighting\nfunction, and for very general generation control policies. In a case study, we\ncompare the performance of the new WCC-OPF and the standard CC-OPF and\ndemonstrate that WCC-OPF effectively reduces the number of severe overloads.\nFurthermore, we compare an affine generation control policy with a more general\npolicy, and show that the additional flexibility allow for a lower cost while\nmaintaining the same level of risk. \n\n"}
{"id": "1504.01032", "contents": "Title: A Three-Operator Splitting Scheme and its Optimization Applications Abstract: Operator splitting schemes have been successfully used in computational\nsciences to reduce complex problems into a series of simpler subproblems. Since\n1950s, these schemes have been widely used to solve problems in PDE and\ncontrol. Recently, large-scale optimization problems in machine learning,\nsignal processing, and imaging have created a resurgence of interest in\noperator-splitting based algorithms because they often have simple\ndescriptions, are easy to code, and have (nearly) state-of-the-art performance\nfor large-scale optimization problems. Although operator splitting techniques\nwere introduced over 60 years ago, their importance has significantly increased\nin the past decade.\n  This paper introduces a new operator-splitting scheme for solving a variety\nof problems that are reduced to a monotone inclusion of three operators, one of\nwhich is cocoercive. Our scheme is very simple, and it does not reduce to any\nexisting splitting schemes. Our scheme recovers the existing forward-backward,\nDouglas-Rachford, and forward-Douglas-Rachford splitting schemes as special\ncases.\n  Our new splitting scheme leads to a set of new and simple algorithms for a\nvariety of other problems, including the 3-set split feasibility problems,\n3-objective minimization problems, and doubly and multiple regularization\nproblems, as well as the simplest extension of the classic ADMM from 2 to 3\nblocks of variables. In addition to the basic scheme, we introduce several\nmodifications and enhancements that can improve the convergence rate in\npractice, including an acceleration that achieves the optimal rate of\nconvergence for strongly monotone inclusions. Finally, we evaluate the\nalgorithm on several applications. \n\n"}
{"id": "1504.01515", "contents": "Title: Simultaneously sparse and low-rank abundance matrix estimation for\n  hyperspectral image unmixing Abstract: In a plethora of applications dealing with inverse problems, e.g. in image\nprocessing, social networks, compressive sensing, biological data processing\netc., the signal of interest is known to be structured in several ways at the\nsame time. This premise has recently guided the research to the innovative and\nmeaningful idea of imposing multiple constraints on the parameters involved in\nthe problem under study. For instance, when dealing with problems whose\nparameters form sparse and low-rank matrices, the adoption of suitably combined\nconstraints imposing sparsity and low-rankness, is expected to yield\nsubstantially enhanced estimation results. In this paper, we address the\nspectral unmixing problem in hyperspectral images. Specifically, two novel\nunmixing algorithms are introduced, in an attempt to exploit both spatial\ncorrelation and sparse representation of pixels lying in homogeneous regions of\nhyperspectral images. To this end, a novel convex mixed penalty term is first\ndefined consisting of the sum of the weighted $\\ell_1$ and the weighted nuclear\nnorm of the abundance matrix corresponding to a small area of the image\ndetermined by a sliding square window. This penalty term is then used to\nregularize a conventional quadratic cost function and impose simultaneously\nsparsity and row-rankness on the abundance matrix. The resulting regularized\ncost function is minimized by a) an incremental proximal sparse and low-rank\nunmixing algorithm and b) an algorithm based on the alternating minimization\nmethod of multipliers (ADMM). The effectiveness of the proposed algorithms is\nillustrated in experiments conducted both on simulated and real data. \n\n"}
{"id": "1504.02602", "contents": "Title: Algebraic solution of tropical optimization problems via matrix\n  sparsification with application to scheduling Abstract: Optimization problems are considered in the framework of tropical algebra to\nminimize and maximize a nonlinear objective function defined on vectors over an\nidempotent semifield, and calculated using multiplicative conjugate\ntransposition. To find the minimum of the function, we first obtain a partial\nsolution, which explicitly represents a subset of solution vectors. We\ncharacterize all solutions by a system of simultaneous equation and inequality,\nand show that the solution set is closed under vector addition and scalar\nmultiplication. A matrix sparsification technique is proposed to extend the\npartial solution, and then to obtain a complete solution described as a family\nof subsets. We offer a backtracking procedure that generates all members of the\nfamily, and derive an explicit representation for the complete solution. As\nanother result, we deduce a complete solution of the maximization problem,\ngiven in a compact vector form by the use of sparsified matrices. The results\nobtained are illustrated with illuminating examples and graphical\nrepresentations. We apply the results to solve real-world problems drawn from\nproject (machine) scheduling, and give numerical examples. \n\n"}
{"id": "1504.03517", "contents": "Title: Bearing-Based Formation Maneuvering Abstract: This paper studies the problem of multi-agent formation maneuver control\nwhere both of the centroid and scale of a formation are required to track given\nvelocity references while maintaining the formation shape. Unlike the\nconventional approaches where the target formation is defined by inter-neighbor\nrelative positions or distances, we propose a bearing-based approach where the\ntarget formation is defined by inter-neighbor bearings. Due to the invariance\nof the bearings, the bearing-based approach provides a natural solution to\nformation scale control. We assume the dynamics of each agent as a single\nintegrator and propose a globally stable proportional-integral formation\nmaneuver control law. It is shown that at least two leaders are required to\ncollaborate in order to control the centroid and scale of the formation whereas\nthe followers are not required to have access to any global information, such\nas the velocities of the leaders. \n\n"}
{"id": "1504.04319", "contents": "Title: The Kirchhoff-Braess Paradox and Its Implications for Smart Microgrids Abstract: Well known in the theory of network flows, Braess paradox states that in a\ncongested network, it may happen that adding a new path between destinations\ncan increase the level of congestion. In transportation networks the phenomenon\nresults from the decisions of network participants who selfishly seek to\noptimize their own performance metrics. In an electric power distribution\nnetwork, an analogous increase in congestion can arise as a consequence\nKirchhoff's laws. Even for the simplest linear network of resistors and voltage\nsources, the sudden appearance of congestion due to an additional conductive\nline is a nonlinear phenomenon that results in a discontinuous change in the\nnetwork state. It is argued that the phenomenon can occur in almost any grid in\nwhich they are loops, and with the increasing penetration of small-scale\ndistributed generation it suggests challenges ahead in the operation of\nmicrogrids. \n\n"}
{"id": "1504.04684", "contents": "Title: A Framework for Robust Assessment of Power Grid Stability and Resiliency Abstract: Security assessment of large-scale, strongly nonlinear power grids containing\nthousands to millions of interacting components is a computationally expensive\ntask. Targeting at reducing the computational cost, this paper introduces a\nframework for constructing a robust assessment toolbox that can provide\nmathematically rigorous certificates for the grids' stability in the presence\nof variations in power injections, and for the grids' ability to withstand a\nbunch sources of faults. By this toolbox we can \"off-line\" screen a wide range\nof contingencies or power injection profiles, without reassessing the system\nstability on a regular basis. In particular, we formulate and solve two novel\nrobust stability and resiliency assessment problems of power grids subject to\nthe uncertainty in equilibrium points and uncertainty in fault-on dynamics.\nFurthermore, we bring in the quadratic Lyapunov functions approach to transient\nstability assessment, offering real-time construction of stability/resiliency\ncertificates and real-time stability assessment. The effectiveness of the\nproposed techniques is numerically illustrated on a number of IEEE test cases. \n\n"}
{"id": "1504.04920", "contents": "Title: From Weak Learning to Strong Learning in Fictitious Play Type Algorithms Abstract: The paper studies the highly prototypical Fictitious Play (FP) algorithm, as\nwell as a broad class of learning processes based on best-response dynamics,\nthat we refer to as FP-type algorithms. A well-known shortcoming of FP is that,\nwhile players may learn an equilibrium strategy in some abstract sense, there\nare no guarantees that the period-by-period strategies generated by the\nalgorithm actually converge to equilibrium themselves. This issue is\nfundamentally related to the discontinuous nature of the best response\ncorrespondence and is inherited by many FP-type algorithms. Not only does it\ncause problems in the interpretation of such algorithms as a mechanism for\neconomic and social learning, but it also greatly diminishes the practical\nvalue of these algorithms for use in distributed control. We refer to forms of\nlearning in which players learn equilibria in some abstract sense only (to be\ndefined more precisely in the paper) as weak learning, and we refer to forms of\nlearning where players' period-by-period strategies converge to equilibrium as\nstrong learning. An approach is presented for modifying an FP-type algorithm\nthat achieves weak learning in order to construct a variant that achieves\nstrong learning. Theoretical convergence results are proved. \n\n"}
{"id": "1504.06068", "contents": "Title: Analysis on Non-negative Factorizations and Applications Abstract: In this work we perform some mathematical analysis on non-negative matrix\nfactorizations (NMF) and apply NMF to some imaging and inverse problems. We\nwill propose a sparse low-rank approximation of big positive data and images in\nterms of tensor products of positive vectors, and investigate its effectiveness\nin terms of the number of tensor products to be used in the approximation. A\nnew concept of multi-level analysis (MLA) framework is also suggested to\nextract major components in the matrix representing structures of different\nresolutions, but still preserving the positivity of the basis and sparsity of\nthe approximation. We will also propose a semi-smooth Newton method based on\nprimal-dual active sets for the non-negative factorization. Numerical results\nare given to demonstrate the effectiveness of the proposed method to capture\nfeatures in images and structures of inverse problems under no a-priori\nassumption on the data structure, as well as to provide a sparse low-rank\nrepresentation of the data. \n\n"}
{"id": "1504.06937", "contents": "Title: Algorithms with Logarithmic or Sublinear Regret for Constrained\n  Contextual Bandits Abstract: We study contextual bandits with budget and time constraints, referred to as\nconstrained contextual bandits.The time and budget constraints significantly\ncomplicate the exploration and exploitation tradeoff because they introduce\ncomplex coupling among contexts over time.Such coupling effects make it\ndifficult to obtain oracle solutions that assume known statistics of bandits.\nTo gain insight, we first study unit-cost systems with known context\ndistribution. When the expected rewards are known, we develop an approximation\nof the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves\nnear-optimality and only requires the ordering of expected rewards. With these\nhighly desirable features, we then combine ALP with the upper-confidence-bound\n(UCB) method in the general case where the expected rewards are unknown {\\it a\npriori}. We show that the proposed UCB-ALP algorithm achieves logarithmic\nregret except for certain boundary cases. Further, we design algorithms and\nobtain similar regret analysis results for more general systems with unknown\ncontext distribution and heterogeneous costs. To the best of our knowledge,\nthis is the first work that shows how to achieve logarithmic regret in\nconstrained contextual bandits. Moreover, this work also sheds light on the\nstudy of computationally efficient algorithms for general constrained\ncontextual bandits. \n\n"}
{"id": "1505.00290", "contents": "Title: Algorithms for Lipschitz Learning on Graphs Abstract: We develop fast algorithms for solving regression problems on graphs where\none is given the value of a function at some vertices, and must find its\nsmoothest possible extension to all vertices. The extension we compute is the\nabsolutely minimal Lipschitz extension, and is the limit for large $p$ of\n$p$-Laplacian regularization. We present an algorithm that computes a minimal\nLipschitz extension in expected linear time, and an algorithm that computes an\nabsolutely minimal Lipschitz extension in expected time $\\widetilde{O} (m n)$.\nThe latter algorithm has variants that seem to run much faster in practice.\nThese extensions are particularly amenable to regularization: we can perform\n$l_{0}$-regularization on the given values in polynomial time and\n$l_{1}$-regularization on the initial function values and on graph edge weights\nin time $\\widetilde{O} (m^{3/2})$. \n\n"}
{"id": "1505.05114", "contents": "Title: Solving Random Quadratic Systems of Equations Is Nearly as Easy as\n  Solving Linear Systems Abstract: We consider the fundamental problem of solving quadratic systems of equations\nin $n$ variables, where $y_i = |\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\n\\rangle|^2$, $i = 1, \\ldots, m$ and $\\boldsymbol{x} \\in \\mathbb{R}^n$ is\nunknown. We propose a novel method, which starting with an initial guess\ncomputed by means of a spectral method, proceeds by minimizing a nonconvex\nfunctional as in the Wirtinger flow approach. There are several key\ndistinguishing features, most notably, a distinct objective functional and\nnovel update rules, which operate in an adaptive fashion and drop terms bearing\ntoo much influence on the search direction. These careful selection rules\nprovide a tighter initial guess, better descent directions, and thus enhanced\npractical performance. On the theoretical side, we prove that for certain\nunstructured models of quadratic systems, our algorithms return the correct\nsolution in linear time, i.e. in time proportional to reading the data\n$\\{\\boldsymbol{a}_i\\}$ and $\\{y_i\\}$ as soon as the ratio $m/n$ between the\nnumber of equations and unknowns exceeds a fixed numerical constant. We extend\nthe theory to deal with noisy systems in which we only have $y_i \\approx\n|\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$ and prove that our\nalgorithms achieve a statistical accuracy, which is nearly un-improvable. We\ncomplement our theoretical study with numerical examples showing that solving\nrandom quadratic systems is both computationally and statistically not much\nharder than solving linear systems of the same size---hence the title of this\npaper. For instance, we demonstrate empirically that the computational cost of\nour algorithm is about four times that of solving a least-squares problem of\nthe same size. \n\n"}
{"id": "1505.05921", "contents": "Title: Identifying Modes of Intent from Driver Behaviors in Dynamic\n  Environments Abstract: In light of growing attention of intelligent vehicle systems, we propose\ndeveloping a driver model that uses a hybrid system formulation to capture the\nintent of the driver. This model hopes to capture human driving behavior in a\nway that can be utilized by semi- and fully autonomous systems in heterogeneous\nenvironments. We consider a discrete set of high level goals or intent modes,\nthat is designed to encompass the decision making process of the human. A\ndriver model is derived using a dataset of lane changes collected in a\nrealistic driving simulator, in which the driver actively labels data to give\nus insight into her intent. By building the labeled dataset, we are able to\nutilize classification tools to build the driver model using features of based\non her perception of the environment, and achieve high accuracy in identifying\ndriver intent. Multiple algorithms are presented and compared on the dataset,\nand a comparison of the varying behaviors between drivers is drawn. Using this\nmodeling methodology, we present a model that can be used to assess driver\nbehaviors and to develop human-inspired safety metrics that can be utilized in\nintelligent vehicular systems. \n\n"}
{"id": "1505.06807", "contents": "Title: MLlib: Machine Learning in Apache Spark Abstract: Apache Spark is a popular open-source platform for large-scale data\nprocessing that is well-suited for iterative machine learning tasks. In this\npaper we present MLlib, Spark's open-source distributed machine learning\nlibrary. MLlib provides efficient functionality for a wide range of learning\nsettings and includes several underlying statistical, optimization, and linear\nalgebra primitives. Shipped with Spark, MLlib supports several languages and\nprovides a high-level API that leverages Spark's rich ecosystem to simplify the\ndevelopment of end-to-end machine learning pipelines. MLlib has experienced a\nrapid growth due to its vibrant open-source community of over 140 contributors,\nand includes extensive documentation to support further growth and to let users\nquickly get up to speed. \n\n"}
{"id": "1505.06810", "contents": "Title: On the Reachability of Networked Systems Abstract: In this paper, we study networks of discrete-time linear time-invariant\nsubsystems. Our focus is on situations where subsystems are connected to each\nother through a time-invariant topology and where there exists a base-station\nwhose aim is to control the subsystems into any desired destinations. However,\nthe base-station can only communicate with some of the subsystems that we refer\nto as leaders. There are no direct links between the base-station and the rest\nof subsystems, known as followers, as they are only able to liaise among\nthemselves and with some of the leaders.\n  The current paper formulates this framework as the well-known reachability\nproblem for linear systems. Then to address this problem, we introduce notions\nof leader-reachability and base-reachability. We present algebraic conditions\nunder which these notions hold. It turns out that if subsystems are represented\nby minimal state space representations, then base-reachability always holds.\nHence, we focus on leader-reachability and investigate the corresponding\nconditions in detail. We further demonstrate that when the networked system\nparameters i.e. subsystems' parameters and interconnection matrices, assume\ngeneric values then the whole network is both leader-reachable and\nbase-reachable. \n\n"}
{"id": "1506.00935", "contents": "Title: Discovering Valuable Items from Massive Data Abstract: Suppose there is a large collection of items, each with an associated cost\nand an inherent utility that is revealed only once we commit to selecting it.\nGiven a budget on the cumulative cost of the selected items, how can we pick a\nsubset of maximal value? This task generalizes several important problems such\nas multi-arm bandits, active search and the knapsack problem. We present an\nalgorithm, GP-Select, which utilizes prior knowledge about similarity be- tween\nitems, expressed as a kernel function. GP-Select uses Gaussian process\nprediction to balance exploration (estimating the unknown value of items) and\nexploitation (selecting items of high value). We extend GP-Select to be able to\ndiscover sets that simultaneously have high utility and are diverse. Our\npreference for diversity can be specified as an arbitrary monotone submodular\nfunction that quantifies the diminishing returns obtained when selecting\nsimilar items. Furthermore, we exploit the structure of the model updates to\nachieve an order of magnitude (up to 40X) speedup in our experiments without\nresorting to approximations. We provide strong guarantees on the performance of\nGP-Select and apply it to three real-world case studies of industrial\nrelevance: (1) Refreshing a repository of prices in a Global Distribution\nSystem for the travel industry, (2) Identifying diverse, binding-affine\npeptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale\nrecommender system by recommending items to users. \n\n"}
{"id": "1506.02005", "contents": "Title: Robust $H_\\infty$ Estimation of Uncertain Linear Quantum Systems Abstract: We consider classical estimators for a class of physically realizable linear\nquantum systems. Optimal estimation using a complex Kalman filter for this\nproblem has been previously explored. Here, we study robust $H_\\infty$\nestimation for uncertain linear quantum systems. The estimation problem is\nsolved by converting it to a suitably scaled $H_\\infty$ control problem. The\nsolution is obtained in the form of two algebraic Riccati equations. Relevant\nexamples involving dynamic squeezers are presented to illustrate the efficacy\nof our method. \n\n"}
{"id": "1506.02158", "contents": "Title: Bayesian Convolutional Neural Networks with Bernoulli Approximate\n  Variational Inference Abstract: Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10. \n\n"}
{"id": "1506.03591", "contents": "Title: Optimal Control of a Semidiscrete Cahn-Hilliard-Navier-Stokes System\n  with Non-Matched Fluid Densities Abstract: This paper is concerned with the distributed optimal control of a\ntime-discrete Cahn--Hilliard/Navier--Stokes system with variable densities. It\nfocuses on the double-obstacle potential which yields an optimal control\nproblem for a family of coupled systems in each time instance of a variational\ninequality of fourth order and the Navier--Stokes equation. By proposing a\nsuitable time-discretization, energy estimates are proved and the existence of\nsolutions to the primal system and of optimal controls is established for the\noriginal problem as well as for a family of regularized problems. The latter\ncorrespond to Moreau--Yosida type approximations of the double-obstacle\npotential. The consistency of these approximations is shown and first order\noptimality conditions for the regularized problems are derived. Through a limit\nprocess, a stationarity system for the original problem is established which is\nrelated to a function space version of C-stationarity. \n\n"}
{"id": "1506.03963", "contents": "Title: Efficient algorithm for computing large scale systems of differential\n  algebraic equations Abstract: In many mathematical models of physical phenomenons and engineering fields,\nsuch as electrical circuits or mechanical multibody systems, which generate the\ndifferential algebraic equations (DAEs) systems naturally. In general, the\nfeature of DAEs is a sparse large scale system of fully nonlinear and high\nindex. To make use of its sparsity, this paper provides a simple and efficient\nalgorithm for computing the large scale DAEs system. We exploit the shortest\naugmenting path algorithm for finding maximum value transversal (MVT) as well\nas block triangular forms (BTF). We also present the extended signature matrix\nmethod with the block fixed point iteration and its complexity results.\nFurthermore, a range of nontrivial problems are demonstrated by our algorithm. \n\n"}
{"id": "1506.04089", "contents": "Title: Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to\n  Action Sequences Abstract: We propose a neural sequence-to-sequence model for direction following, a\ntask that is essential to realizing effective autonomous agents. Our\nalignment-based encoder-decoder model with long short-term memory recurrent\nneural networks (LSTM-RNN) translates natural language instructions to action\nsequences based upon a representation of the observable world state. We\nintroduce a multi-level aligner that empowers our model to focus on sentence\n\"regions\" salient to the current world state by using multiple abstractions of\nthe input sentence. In contrast to existing methods, our model uses no\nspecialized linguistic resources (e.g., parsers) or task-specific annotations\n(e.g., seed lexicons). It is therefore generalizable, yet still achieves the\nbest results reported to-date on a benchmark single-sentence dataset and\ncompetitive results for the limited-training multi-sentence setting. We analyze\nour model through a series of ablations that elucidate the contributions of the\nprimary components of our model. \n\n"}
{"id": "1506.04255", "contents": "Title: Entropic and displacement interpolation: a computational approach using\n  the Hilbert metric Abstract: Monge-Kantorovich optimal mass transport (OMT) provides a blueprint for\ngeometries in the space of positive densities -- it quantifies the cost of\ntransporting a mass distribution into another. In particular, it provides\nnatural options for interpolation of distributions (displacement interpolation)\nand for modeling flows. As such it has been the cornerstone of recent\ndevelopments in physics, probability theory, image processing, time-series\nanalysis, and several other fields. In spite of extensive work and theoretical\ndevelopments, the computation of OMT for large scale problems has remained a\nchallenging task. An alternative framework for interpolating distributions,\nrooted in statistical mechanics and large deviations, is that of Schroedinger\nbridges (entropic interpolation). This may be seen as a stochastic\nregularization of OMT and can be cast as the stochastic control problem of\nsteering the probability density of the state-vector of a dynamical system\nbetween two marginals. In this approach, however, the actual computation of\nflows had hardly received any attention. In recent work on Schroedinger bridges\nfor Markov chains and quantum evolutions, we noted that the solution can be\nefficiently obtained from the fixed-point of a map which is contractive in the\nHilbert metric. Thus, the purpose of this paper is to show that a similar\napproach can be taken in the context of diffusion processes which i) leads to a\nnew proof of a classical result on Schroedinger bridges and ii) provides an\nefficient computational scheme for both, Schroedinger bridges and OMT. We\nillustrate this new computational approach by obtaining interpolation of\ndensities in representative examples such as interpolation of images. \n\n"}
{"id": "1506.04773", "contents": "Title: DistFlow Extensions for AC Transmission Systems Abstract: Convex relaxations of the power flow equations and, in particular, the\nSemi-Definite Programming (SDP), Second-Order Cone (SOC), and Convex DistFlow\n(CDF) relaxations, have attracted significant interest in recent years. Thus\nfar, studies of the CDF model and its connection to the other relaxations have\nbeen limited to power distribution systems, which omit several parameters\nnecessary for modeling transmission systems. To increase the applicability of\nthe CDF relaxation, this paper develops an extended CDF model that is suitable\nfor transmission systems by incorporating bus shunts, line charging, and\ntransformers. Additionally, a theoretical result shows that the established\nequivalence of the SOC and CDF models for distribution systems also holds in\nthis transmission system extension. \n\n"}
{"id": "1506.07603", "contents": "Title: Analytic MMSE Bounds in Linear Dynamic Systems with Gaussian Mixture\n  Noise Statistics Abstract: Using state-space representation, mobile object positioning problems can be\ndescribed as dynamic systems, with the state representing the unknown location\nand the observations being the information gathered from the location sensors.\nFor linear dynamic systems with Gaussian noise, the Kalman filter provides the\nMinimum Mean-Square Error (MMSE) state estimation by tracking the posterior.\nHence, by approximating non-Gaussian noise distributions with Gaussian Mixtures\n(GM), a bank of Kalman filters or Gaussian Sum Filter (GSF), can provide the\nMMSE state estimation. However, the MMSE itself is not analytically tractable.\nMoreover, the general analytic bounds proposed in the literature are not\ntractable for GM noise statistics. Hence, in this work, we evaluate the MMSE of\nlinear dynamic systems with GM noise statistics and propose its analytic lower\nand upper bounds. We provide two analytic upper bounds which are the\nMean-Square Errors (MSE) of implementable filters, and we show that based on\nthe shape of the GM noise distributions, the tighter upper bound can be\nselected. We also show that for highly multimodal GM noise distributions, the\nbounds and the MMSE converge. Simulation results support the validity of the\nproposed bounds and their behavior in limits. \n\n"}
{"id": "1506.08019", "contents": "Title: Multiobjective approach to optimal control for a dengue transmission\n  model Abstract: During the last decades, the global prevalence of dengue progressed\ndramatically. It is a disease which is now endemic in more than one hundred\ncountries of Africa, America, Asia and the Western Pacific. This study\naddresses a mathematical model for the dengue disease transmission and finding\nthe most effective ways of controlling the disease. The model is described by a\nsystem of ordinary differential equations representing human and vector\ndynamics. Multiobjective optimization is applied to find the optimal control\nstrategies, considering the simultaneous minimization of infected humans and\ncosts due to insecticide application. The obtained results show that\nmultiobjective optimization is an effective tool for finding the optimal\ncontrol. The set of trade-off solutions encompasses a whole range of optimal\nscenarios, providing valuable information about the dynamics of infection\ntransmissions. The results are discussed for different values of model\nparameters. \n\n"}
{"id": "1506.08234", "contents": "Title: Robust Online Monitoring of Signal Temporal Logic Abstract: Signal Temporal Logic (STL) is a formalism used to rigorously specify\nrequirements of cyberphysical systems (CPS), i.e., systems mixing digital or\ndiscrete components in interaction with a continuous environment or analog com-\nponents. STL is naturally equipped with a quantitative semantics which can be\nused for various purposes: from assessing the robustness of a specification to\nguiding searches over the input and parameter space with the goal of falsifying\nthe given property over system behaviors. Algorithms have been proposed and\nimplemented for offline computation of such quantitative semantics, but only\nfew methods exist for an online setting, where one would want to monitor the\nsatisfaction of a formula during simulation. In this paper, we formalize a\nsemantics for robust online monitoring of partial traces, i.e., traces for\nwhich there might not be enough data to decide the Boolean satisfaction (and to\ncompute its quantitative counterpart). We propose an efficient algorithm to\ncompute it and demonstrate its usage on two large scale real-world case studies\ncoming from the automotive domain and from CPS education in a Massively Open\nOnline Course (MOOC) setting. We show that savings in computationally expensive\nsimulations far outweigh any overheads incurred by an online approach. \n\n"}
{"id": "1507.00210", "contents": "Title: Natural Neural Networks Abstract: We introduce Natural Neural Networks, a novel family of algorithms that speed\nup convergence by adapting their internal representation during training to\nimprove conditioning of the Fisher matrix. In particular, we show a specific\nexample that employs a simple and efficient reparametrization of the neural\nnetwork weights by implicitly whitening the representation obtained at each\nlayer, while preserving the feed-forward computation of the network. Such\nnetworks can be trained efficiently via the proposed Projected Natural Gradient\nDescent algorithm (PRONG), which amortizes the cost of these reparametrizations\nover many parameter updates and is closely related to the Mirror Descent online\nlearning algorithm. We highlight the benefits of our method on both\nunsupervised and supervised learning tasks, and showcase its scalability by\ntraining on the large-scale ImageNet Challenge dataset. \n\n"}
{"id": "1507.00664", "contents": "Title: On the Reduction of Total-Cost and Average-Cost MDPs to Discounted MDPs Abstract: This paper provides conditions under which total-cost and average-cost Markov\ndecision processes (MDPs) can be reduced to discounted ones. Results are given\nfor transient total-cost MDPs with tran- sition rates whose values may be\ngreater than one, as well as for average-cost MDPs with transition\nprobabilities satisfying the condition that there is a state such that the\nexpected time to reach it is uniformly bounded for all initial states and\nstationary policies. In particular, these reductions imply sufficient\nconditions for the validity of optimality equations and the existence of\nstationary optimal poli- cies for MDPs with undiscounted total cost and\naverage-cost criteria. When the state and action sets are finite, these\nreductions lead to linear programming formulations and complexity estimates for\nMDPs under the aforementioned criteria. \n\n"}
{"id": "1507.01569", "contents": "Title: Emphatic Temporal-Difference Learning Abstract: Emphatic algorithms are temporal-difference learning algorithms that change\ntheir effective state distribution by selectively emphasizing and\nde-emphasizing their updates on different time steps. Recent works by Sutton,\nMahmood and White (2015), and Yu (2015) show that by varying the emphasis in a\nparticular way, these algorithms become stable and convergent under off-policy\ntraining with linear function approximation. This paper serves as a unified\nsummary of the available results from both works. In addition, we demonstrate\nthe empirical benefits from the flexibility of emphatic algorithms, including\nstate-dependent discounting, state-dependent bootstrapping, and the\nuser-specified allocation of function approximation resources. \n\n"}
{"id": "1507.02528", "contents": "Title: Faster Convex Optimization: Simulated Annealing with an Efficient\n  Universal Barrier Abstract: This paper explores a surprising equivalence between two seemingly-distinct\nconvex optimization methods. We show that simulated annealing, a well-studied\nrandom walk algorithms, is directly equivalent, in a certain sense, to the\ncentral path interior point algorithm for the the entropic universal barrier\nfunction. This connection exhibits several benefits. First, we are able improve\nthe state of the art time complexity for convex optimization under the\nmembership oracle model. We improve the analysis of the randomized algorithm of\nKalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that\nunderly the central path following interior point algorithm. We are able to\ntighten the temperature schedule for simulated annealing which gives an\nimproved running time, reducing by square root of the dimension in certain\ninstances. Second, we get an efficient randomized interior point method with an\nefficiently computable universal barrier for any convex set described by a\nmembership oracle. Previously, efficiently computable barriers were known only\nfor particular convex sets. \n\n"}
{"id": "1507.03454", "contents": "Title: Stability for the Brunn-Minkowski and Riesz rearrangement inequalities,\n  with applications to Gaussian concentration and finite range non-local\n  isoperimetry Abstract: We provide a simple, general argument to obtain improvements of\nconcentration-type inequalities starting from improvements of their\ncorresponding isoperimetric-type inequalities. We apply this argument to obtain\nrobust improvements of the Brunn-Minkowski inequality (for Minkowski sums\nbetween generic sets and convex sets) and of the Gaussian concentration\ninequality. The former inequality is then used to obtain a robust improvement\nof the Riesz rearrangement inequality under certain natural conditions. These\nconditions are compatible with the applications to a finite-range nonlocal\nisoperimetric problem arising in statistical mechanics. \n\n"}
{"id": "1507.03566", "contents": "Title: Low-rank Solutions of Linear Matrix Equations via Procrustes Flow Abstract: In this paper we study the problem of recovering a low-rank matrix from\nlinear measurements. Our algorithm, which we call Procrustes Flow, starts from\nan initial estimate obtained by a thresholding scheme followed by gradient\ndescent on a non-convex objective. We show that as long as the measurements\nobey a standard restricted isometry property, our algorithm converges to the\nunknown matrix at a geometric rate. In the case of Gaussian measurements, such\nconvergence occurs for a $n_1 \\times n_2$ matrix of rank $r$ when the number of\nmeasurements exceeds a constant times $(n_1+n_2)r$. \n\n"}
{"id": "1507.04374", "contents": "Title: Uniform-Price Mechanism Design for a Large Population of Dynamic Agents Abstract: This paper focuses on the coordination of a large population of dynamic\nagents with private information over multiple periods. Each agent maximizes the\nindividual utility, while the coordinator determines the market rule to achieve\ngroup objectives. The coordination problem is formulated as a dynamic mechanism\ndesign problem. A mechanism is proposed based on the competitive equilibrium of\nthe large population game. We derive the conditions for the general nonlinear\ndynamic systems under which the proposed mechanism is incentive compatible and\ncan implement the social choice function in $\\epsilon$-Nash equilibrium. In\naddition, we show that for linear quadratic problems with bounded parameters,\nthe proposed mechanism can maximize the social welfare subject to a total\nresource constraint in $\\epsilon$-dominant strategy equilibrium. \n\n"}
{"id": "1507.05125", "contents": "Title: On the Convergence of Optimal Actions for Markov Decision Processes and\n  the Optimality of $(s,S)$ Inventory Policies Abstract: This paper studies convergence properties of optimal values and actions for\ndiscounted and average-cost Markov Decision Processes (MDPs) with weakly\ncontinuous transition probabilities and applies these properties to the\nstochastic periodic-review inventory control problem with backorders, positive\nsetup costs, and convex holding/backordering costs. The following results are\nestablished for MDPs with possibly noncompact action sets and unbounded cost\nfunctions: (i) convergence of value iterations to optimal values for discounted\nproblems with possibly non-zero terminal costs, (ii) convergence of optimal\nfinite-horizon actions to optimal infinite-horizon actions for total discounted\ncosts, as the time horizon tends to infinity, and (iii) convergence of optimal\ndiscount-cost actions to optimal average-cost actions for infinite-horizon\nproblems, as the discount factor tends to 1.\n  Being applied to the setup-cost inventory control problem, the general\nresults on MDPs imply the optimality of $(s,S)$ policies and convergence\nproperties of optimal thresholds. In particular this paper analyzes the\nsetup-cost inventory control problem without two assumptions often used in the\nliterature: (a) the demand is either discrete or continuous or (b) the\nbackordering cost is higher than the cost of backordered inventory if the\namount of backordered inventory is large. \n\n"}
{"id": "1507.05367", "contents": "Title: Structured Sparsity: Discrete and Convex approaches Abstract: Compressive sensing (CS) exploits sparsity to recover sparse or compressible\nsignals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity\nis also used to enhance interpretability in machine learning and statistics\napplications: While the ambient dimension is vast in modern data analysis\nproblems, the relevant information therein typically resides in a much lower\ndimensional space. However, many solutions proposed nowadays do not leverage\nthe true underlying structure. Recent results in CS extend the simple sparsity\nidea to more sophisticated {\\em structured} sparsity models, which describe the\ninterdependency between the nonzero components of a signal, allowing to\nincrease the interpretability of the results and lead to better recovery\nperformance. In order to better understand the impact of structured sparsity,\nin this chapter we analyze the connections between the discrete models and\ntheir convex relaxations, highlighting their relative advantages. We start with\nthe general group sparse model and then elaborate on two important special\ncases: the dispersive and the hierarchical models. For each, we present the\nmodels in their discrete nature, discuss how to solve the ensuing discrete\nproblems and then describe convex relaxations. We also consider more general\nstructures as defined by set functions and present their convex proxies.\nFurther, we discuss efficient optimization solutions for structured sparsity\nproblems and illustrate structured sparsity in action via three applications. \n\n"}
{"id": "1507.07095", "contents": "Title: Stochastic Approximations and Perturbations in Forward-Backward\n  Splitting for Monotone Operators Abstract: We investigate the asymptotic behavior of a stochastic version of the\nforward-backward splitting algorithm for finding a zero of the sum of a\nmaximally monotone set-valued operator and a cocoercive operator in Hilbert\nspaces. Our general setting features stochastic approximations of the\ncocoercive operator and stochastic perturbations in the evaluation of the\nresolvents of the set-valued operator. In addition, relaxations and not\nnecessarily vanishing proximal parameters are allowed. Weak and strong almost\nsure convergence properties of the iterates is established under mild\nconditions on the underlying stochastic processes. Leveraging these results, we\nalso establish the almost sure convergence of the iterates of a stochastic\nvariant of a primal-dual proximal splitting method for composite minimization\nproblems. \n\n"}
{"id": "1507.07145", "contents": "Title: Nearly convex sets: fine properties and domains or ranges of\n  subdifferentials of convex functions Abstract: Nearly convex sets play important roles in convex analysis, optimization and\ntheory of monotone operators. We give a systematic study of nearly convex sets,\nand construct examples of subdifferentials of lower semicontinuous convex\nfunctions whose domain or ranges are nonconvex. \n\n"}
{"id": "1507.07595", "contents": "Title: Distributed Stochastic Variance Reduced Gradient Methods and A Lower\n  Bound for Communication Complexity Abstract: We study distributed optimization algorithms for minimizing the average of\nconvex functions. The applications include empirical risk minimization problems\nin statistical machine learning where the datasets are large and have to be\nstored on different machines. We design a distributed stochastic variance\nreduced gradient algorithm that, under certain conditions on the condition\nnumber, simultaneously achieves the optimal parallel runtime, amount of\ncommunication and rounds of communication among all distributed first-order\nmethods up to constant factors. Our method and its accelerated extension also\noutperform existing distributed algorithms in terms of the rounds of\ncommunication as long as the condition number is not too large compared to the\nsize of data in each machine. We also prove a lower bound for the number of\nrounds of communication for a broad class of distributed first-order methods\nincluding the proposed algorithms in this paper. We show that our accelerated\ndistributed stochastic variance reduced gradient algorithm achieves this lower\nbound so that it uses the fewest rounds of communication among all distributed\nfirst-order algorithms. \n\n"}
{"id": "1507.08592", "contents": "Title: Sparse Linear-Quadratic Feedback Design Using Affine Approximation Abstract: We consider a class of $\\ell_0$-regularized linear-quadratic (LQ) optimal\ncontrol problems. This class of problems is obtained by augmenting a penalizing\nsparsity measure to the cost objective of the standard linear-quadratic\nregulator (LQR) problem in order to promote sparsity pattern of the state\nfeedback controller. This class of problems is generally NP hard and\ncomputationally intractable. First, we apply a $\\ell_1$-relaxation and consider\nthe $\\ell_1$-regularized LQ version of this class of problems, which is still\nnonconvex. Then, we convexify the resulting $\\ell_1$-regularized LQ problem by\napplying affine approximation techniques. An iterative algorithm is proposed to\nsolve the $\\ell_1$-regularized LQ problem using a series of convexified\n$\\ell_1$-regularized LQ problems. By means of several numerical experiments, we\nshow that our proposed algorithm is comparable to the existing algorithms in\nthe literature, and in some cases it even returns solutions with superior\nperformance and sparsity pattern. \n\n"}
{"id": "1507.08751", "contents": "Title: Beyond Low Rank + Sparse: Multi-scale Low Rank Matrix Decomposition Abstract: We present a natural generalization of the recent low rank + sparse matrix\ndecomposition and consider the decomposition of matrices into components of\nmultiple scales. Such decomposition is well motivated in practice as data\nmatrices often exhibit local correlations in multiple scales. Concretely, we\npropose a multi-scale low rank modeling that represents a data matrix as a sum\nof block-wise low rank matrices with increasing scales of block sizes. We then\nconsider the inverse problem of decomposing the data matrix into its\nmulti-scale low rank components and approach the problem via a convex\nformulation. Theoretically, we show that under various incoherence conditions,\nthe convex program recovers the multi-scale low rank components \\revised{either\nexactly or approximately}. Practically, we provide guidance on selecting the\nregularization parameters and incorporate cycle spinning to reduce blocking\nartifacts. Experimentally, we show that the multi-scale low rank decomposition\nprovides a more intuitive decomposition than conventional low rank methods and\ndemonstrate its effectiveness in four applications, including illumination\nnormalization for face images, motion separation for surveillance videos,\nmulti-scale modeling of the dynamic contrast enhanced magnetic resonance\nimaging and collaborative filtering exploiting age information. \n\n"}
{"id": "1508.01000", "contents": "Title: Uniform Quadratic Optimization and Extensions Abstract: The uniform quadratic optimizatin problem (UQ) is a nonconvex quadratic\nconstrained quadratic programming (QCQP) sharing the same Hessian matrix. Based\non the second-order cone programming (SOCP) relaxation, we establish a new\nsufficient condition to guarantee strong duality for (UQ) and then extend it to\n(QCQP), which not only covers several well-known results in literature but also\npartially gives answers to a few open questions. For convex constrained\nnonconvex (UQ), we propose an improved approximation algorithm based on (SOCP).\nOur approximation bound is dimensional independent. As an application, we\nestablish the first approximation bound for the problem of finding the\nChebyshev center of the intersection of several balls. \n\n"}
{"id": "1508.02087", "contents": "Title: A Linearly-Convergent Stochastic L-BFGS Algorithm Abstract: We propose a new stochastic L-BFGS algorithm and prove a linear convergence\nrate for strongly convex and smooth functions. Our algorithm draws heavily from\na recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as\na recent approach to variance reduction for stochastic gradient descent from\nJohnson and Zhang (2013). We demonstrate experimentally that our algorithm\nperforms well on large-scale convex and non-convex optimization problems,\nexhibiting linear convergence and rapidly solving the optimization problems to\nhigh levels of precision. Furthermore, we show that our algorithm performs well\nfor a wide-range of step sizes, often differing by several orders of magnitude. \n\n"}
{"id": "1508.03715", "contents": "Title: Exact algorithms for linear matrix inequalities Abstract: Let $A(x)=A\\_0+x\\_1A\\_1+...+x\\_nA\\_n$ be a linear matrix, or pencil,\ngenerated by given symmetric matrices $A\\_0,A\\_1,...,A\\_n$ of size $m$ with\nrational entries. The set of real vectors x such that the pencil is positive\nsemidefinite is a convex semi-algebraic set called spectrahedron, described by\na linear matrix inequality (LMI). We design an exact algorithm that, up to\ngenericity assumptions on the input matrices, computes an exact algebraic\nrepresentation of at least one point in the spectrahedron, or decides that it\nis empty. The algorithm does not assume the existence of an interior point, and\nthe computed point minimizes the rank of the pencil on the spectrahedron. The\ndegree $d$ of the algebraic representation of the point coincides\nexperimentally with the algebraic degree of a generic semidefinite program\nassociated to the pencil. We provide explicit bounds for the complexity of our\nalgorithm, proving that the maximum number of arithmetic operations that are\nperformed is essentially quadratic in a multilinear B\\'ezout bound of $d$. When\n$m$ (resp. $n$) is fixed, such a bound, and hence the complexity, is polynomial\nin $n$ (resp. $m$). We conclude by providing results of experiments showing\npractical improvements with respect to state-of-the-art computer algebra\nalgorithms. \n\n"}
{"id": "1508.04210", "contents": "Title: Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors Abstract: We present a scalable Bayesian model for low-rank factorization of massive\ntensors with binary observations. The proposed model has the following key\nproperties: (1) in contrast to the models based on the logistic or probit\nlikelihood, using a zero-truncated Poisson likelihood for binary data allows\nour model to scale up in the number of \\emph{ones} in the tensor, which is\nespecially appealing for massive but sparse binary tensors; (2)\nside-information in form of binary pairwise relationships (e.g., an adjacency\nnetwork) between objects in any tensor mode can also be leveraged, which can be\nespecially useful in \"cold-start\" settings; and (3) the model admits simple\nBayesian inference via batch, as well as \\emph{online} MCMC; the latter allows\nscaling up even for \\emph{dense} binary data (i.e., when the number of ones in\nthe tensor/network is also massive). In addition, non-negative factor matrices\nin our model provide easy interpretability, and the tensor rank can be inferred\nfrom the data. We evaluate our model on several large-scale real-world binary\ntensors, achieving excellent computational scalability, and also demonstrate\nits usefulness in leveraging side-information provided in form of\nmode-network(s). \n\n"}
{"id": "1508.07065", "contents": "Title: A dual descent algorithm for node-capacitated multiflow problems and its\n  applications Abstract: In this paper, we develop an $O((m \\log k) {\\rm MSF} (n,m,1))$-time algorithm\nto find a half-integral node-capacitated multiflow of the maximum total\nflow-value in a network with $n$ nodes, $m$ edges, and $k$ terminals, where\n${\\rm MSF} (n',m',\\gamma)$ denotes the time complexity of solving the maximum\nsubmodular flow problem in a network with $n'$ nodes, $m'$ edges, and the\ncomplexity $\\gamma$ of computing the exchange capacity of the submodular\nfunction describing the problem. By using Fujishige-Zhang algorithm for\nsubmodular flow, we can find a maximum half-integral multiflow in $O(m n^3 \\log\nk)$ time. This is the first combinatorial strongly polynomial time algorithm\nfor this problem. Our algorithm is built on a developing theory of discrete\nconvex functions on certain graph structures. Applications include\n\"ellipsoid-free\" combinatorial implementations of a 2-approximation algorithm\nfor the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis. \n\n"}
{"id": "1508.07096", "contents": "Title: Partitioning Large Scale Deep Belief Networks Using Dropout Abstract: Deep learning methods have shown great promise in many practical\napplications, ranging from speech recognition, visual object recognition, to\ntext processing. However, most of the current deep learning methods suffer from\nscalability problems for large-scale applications, forcing researchers or users\nto focus on small-scale problems with fewer parameters.\n  In this paper, we consider a well-known machine learning model, deep belief\nnetworks (DBNs) that have yielded impressive classification performance on a\nlarge number of benchmark machine learning tasks. To scale up DBN, we propose\nan approach that can use the computing clusters in a distributed environment to\ntrain large models, while the dense matrix computations within a single machine\nare sped up using graphics processors (GPU). When training a DBN, each machine\nrandomly drops out a portion of neurons in each hidden layer, for each training\ncase, making the remaining neurons only learn to detect features that are\ngenerally helpful for producing the correct answer. Within our approach, we\nhave developed four methods to combine outcomes from each machine to form a\nunified model. Our preliminary experiment on the mnst handwritten digit\ndatabase demonstrates that our approach outperforms the state of the art test\nerror rate. \n\n"}
{"id": "1508.07933", "contents": "Title: Coordinate Dual Averaging for Decentralized Online Optimization with\n  Nonseparable Global Objectives Abstract: We consider a decentralized online convex optimization problem in a network\nof agents, where each agent controls only a coordinate (or a part) of the\nglobal decision vector. For such a problem, we propose two decentralized\nvariants (ODA-C and ODA-PS) of Nesterov's primal-dual algorithm with dual\naveraging. In ODA-C, to mitigate the disagreements on the primal-vector\nupdates, the agents implement a generalization of the local\ninformation-exchange dynamics recently proposed by Li and Marden over a static\nundirected graph. In ODA-PS, the agents implement the broadcast-based push-sum\ndynamics over a time-varying sequence of uniformly connected digraphs. We show\nthat the regret bounds in both cases have sublinear growth of $O(\\sqrt{T})$,\nwith the time horizon $T$, when the stepsize is of the form $1/\\sqrt{t}$ and\nthe objective functions are Lipschitz-continuous convex functions with\nLipschitz gradients. We also implement the proposed algorithms on a sensor\nnetwork to complement our theoretical analysis. \n\n"}
{"id": "1509.01817", "contents": "Title: On collapsed representation of hierarchical Completely Random Measures Abstract: The aim of the paper is to provide an exact approach for generating a Poisson\nprocess sampled from a hierarchical CRM, without having to instantiate the\ninfinitely many atoms of the random measures. We use completely random\nmeasures~(CRM) and hierarchical CRM to define a prior for Poisson processes. We\nderive the marginal distribution of the resultant point process, when the\nunderlying CRM is marginalized out. Using well known properties unique to\nPoisson processes, we were able to derive an exact approach for instantiating a\nPoisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs\nsampling strategies for hierarchical CRM models based on Chinese restaurant\nfranchise sampling scheme. As an example, we present the sum of generalized\ngamma process (SGGP), and show its application in topic-modelling. We show that\none can determine the power-law behaviour of the topics and words in a Bayesian\nfashion, by defining a prior on the parameters of SGGP. \n\n"}
{"id": "1509.01846", "contents": "Title: Sample Efficient Path Integral Control under Uncertainty Abstract: We present a data-driven optimal control framework that can be viewed as a\ngeneralization of the path integral (PI) control approach. We find iterative\nfeedback control laws without parameterization based on probabilistic\nrepresentation of learned dynamics model. The proposed algorithm operates in a\nforward-backward manner which differentiate from other PI-related methods that\nperform forward sampling to find optimal controls. Our method uses\nsignificantly less samples to find optimal controls compared to other\napproaches within the PI control family that relies on extensive sampling from\ngiven dynamics models or trials on physical systems in model-free fashions. In\naddition, the learned controllers can be generalized to new tasks without\nre-sampling based on the compositionality theory for the linearly-solvable\noptimal control framework. We provide experimental results on three different\nsystems and comparisons with state-of-the-art model-based methods to\ndemonstrate the efficiency and generalizability of the proposed framework. \n\n"}
{"id": "1509.01898", "contents": "Title: A Possible Implementation of a Direct Coupling Coherent Quantum Observer Abstract: This paper considers the problem of implementing a previously proposed direct\ncoupling quantum observer for a closed linear quantum system. This observer is\nshown to be able to estimate some but not all of the plant variables in a time\naveraged sense. The paper proposes a possible experimental implementation of\nthe observer plant system using a non-degenerate parametric amplifier. \n\n"}
{"id": "1509.02763", "contents": "Title: Performance Enhancement of Parameter Estimators via Dynamic Regressor\n  Extension and Mixing Abstract: A new way to design parameter estimators with enhanced performance is\nproposed in the paper. The procedure consists of two stages, first, the\ngeneration of new regression forms via the application of a dynamic operator to\nthe original regression. Second, a suitable mix of these new regressors to\nobtain the final desired regression form. For classical linear regression forms\nthe procedure yields a new parameter estimator whose convergence is established\nwithout the usual requirement of regressor persistency of excitation. The\ntechnique is also applied to nonlinear regressions with \"partially\" monotonic\nparameter dependence---giving rise again to estimators with enhanced\nperformance. Simulation results illustrate the advantages of the proposed\nprocedure in both scenarios. \n\n"}
{"id": "1509.03351", "contents": "Title: A sub-optimal solution for optimal control of linear systems with\n  unmeasurable switching delays Abstract: We consider the optimal control design problem for discrete-time LTI systems\nwith state feedback, when the actuation signal is subject to unmeasurable\nswitching propagation delays, due to e.g. the routing in a multi-hop\ncommunication network and/or jitter. In particular, we set up a constrained\noptimization problem where the cost function is the worst-case $\\mathcal{L}_2$\nnorm for all admissible switching delays. We first show how to model these\nsystems as pure switching linear systems, and as main contribution of the paper\nwe provide an algorithm to compute a sub-optimal solution. \n\n"}
{"id": "1509.03427", "contents": "Title: Observer-based correct-by-design controller synthesis Abstract: Current state-of-the-art correct-by-design controllers are designed for\nfull-state measurable systems. This work first extends the applicability of\ncorrect-by-design controllers to partially observable LTI systems. Leveraging\n2nd order bounds we give a design method that has a quantifiable robustness to\nprobabilistic disturbances on state transitions and on output measurements. In\na case study from smart buildings we evaluate the new output-based\ncorrect-by-design controller on a physical system with limited sensor\ninformation. \n\n"}
{"id": "1509.03742", "contents": "Title: Error Bounds for Parametric Polynomial Systems with Applications to\n  Higher-Order Stability Analysis and Convergence Rates Abstract: The paper addresses parametric inequality systems described by polynomial\nfunctions in finite dimensions, where state-dependent infinite parameter sets\nare given by finitely many polynomial inequalities and equalities. Such systems\ncan be viewed, in particular, as solution sets to problems of generalized\nsemi-infinite programming with polynomial data. Exploiting the imposed\npolynomial structure together with powerful tools of variational analysis and\nsemialgebraic geometry, we establish a far-going extension of the \\L ojasiewicz\ngradient inequality to the general nonsmooth class of supremum marginal\nfunctions as well as higher-order (H\\\"older type) local error bounds results\nwith explicitly calculated exponents. The obtained results are applied to\nhigher-order quantitative stability analysis for various classes of\noptimization problems including generalized semi-infinite programming with\npolynomial data, optimization of real polynomials under polynomial matrix\ninequality constraints, and polynomial second-order cone programming. Other\napplications provide explicit convergence rate estimates for the cyclic\nprojection algorithm to find common points of convex sets described by matrix\npolynomial inequalities and for the asymptotic convergence of trajectories of\nsubgradient dynamical systems in semialgebraic settings. \n\n"}
{"id": "1509.06542", "contents": "Title: Adaptive-Robust Control of a Class of Nonlinear Systems with Unknown\n  Input Delay Abstract: In this paper, the tracking control problem of a class of uncertain\nEuler-Lagrange systems subjected to unknown input delay and bounded\ndisturbances is addressed. To this front, a novel delay dependent control law,\nreferred as Adaptive Robust Outer Loop Control (AROLC) is proposed. Compared to\nthe conventional predictor based approaches, the proposed controller is capable\nof negotiating any input delay, within a stipulated range, without knowing the\ndelay or its variation. The maximum allowable input delay is computed through\nRazumikhin-type stability analysis. AROLC also provides robustness against the\ndisturbances due to input delay, parametric variations and unmodelled dynamics\nthrough switching control law. The novel adaptive law allows the switching gain\nto modify itself online in accordance with the tracking error without any\nprerequisite of the uncertainties. The uncertain system, employing AROLC, is\nshown to be Uniformly Ultimately Bounded (UUB). As a proof of concept,\nexperimentation is carried out on a nonholonomic wheeled mobile robot with\nvarious time varying as well as fixed input delay, and better tracking accuracy\nof the proposed controller is noted compared to predictor based methodology. \n\n"}
{"id": "1509.06812", "contents": "Title: Learning Wake-Sleep Recurrent Attention Models Abstract: Despite their success, convolutional neural networks are computationally\nexpensive because they must examine all image locations. Stochastic\nattention-based models have been shown to improve computational efficiency at\ntest time, but they remain difficult to train because of intractable posterior\ninference and high variance in the stochastic gradient estimates. Borrowing\ntechniques from the literature on training deep generative models, we present\nthe Wake-Sleep Recurrent Attention Model, a method for training stochastic\nattention networks which improves posterior inference and which reduces the\nvariability in the stochastic gradients. We show that our method can greatly\nspeed up the training time for stochastic attention networks in the domains of\nimage classification and caption generation. \n\n"}
{"id": "1509.08990", "contents": "Title: Learning without Recall: A Case for Log-Linear Learning Abstract: We analyze a model of learning and belief formation in networks in which\nagents follow Bayes rule yet they do not recall their history of past\nobservations and cannot reason about how other agents' beliefs are formed. They\ndo so by making rational inferences about their observations which include a\nsequence of independent and identically distributed private signals as well as\nthe beliefs of their neighboring agents at each time. Fully rational agents\nwould successively apply Bayes rule to the entire history of observations. This\nleads to forebodingly complex inferences due to lack of knowledge about the\nglobal network structure that causes those observations. To address these\ncomplexities, we consider a Learning without Recall model, which in addition to\nproviding a tractable framework for analyzing the behavior of rational agents\nin social networks, can also provide a behavioral foundation for the variety of\nnon-Bayesian update rules in the literature. We present the implications of\nvarious choices for time-varying priors of such agents and how this choice\naffects learning and its rate. \n\n"}
{"id": "1510.00109", "contents": "Title: Confinement Control of Double Integrators using Partially Periodic\n  Leader Trajectories Abstract: We consider a multi-agent confinement control problem in which a single\nleader has a purely repulsive effect on follower agents with double-integrator\ndynamics. By decomposing the leader's control inputs into periodic and\naperiodic components, we show that the leader can be driven so as to guarantee\nconfinement of the followers about a time-dependent trajectory in the plane. We\nuse tools from averaging theory and an input-to-state stability type argument\nto derive conditions on the model parameters that guarantee confinement of the\nfollowers about the trajectory. For the case of a single follower, we show that\nif the follower starts at the origin, then the error in trajectory tracking can\nbe made arbitrarily small depending on the frequency of the periodic control\ncomponents and the rate of change of the trajectory. We validate our approach\nusing simulations and experiments with a small mobile robot. \n\n"}
{"id": "1510.01597", "contents": "Title: Sum of Squares Basis Pursuit with Linear and Second Order Cone\n  Programming Abstract: We devise a scheme for solving an iterative sequence of linear programs (LPs)\nor second order cone programs (SOCPs) to approximate the optimal value of any\nsemidefinite program (SDP) or sum of squares (SOS) program. The first LP and\nSOCP-based bounds in the sequence come from the recent work of Ahmadi and\nMajumdar on diagonally dominant sum of squares (DSOS) and scaled diagonally\ndominant sum of squares (SDSOS) polynomials. We then iteratively improve on\nthese bounds by pursuing better bases in which more relevant SOS polynomials\nadmit a DSOS or SDSOS representation. Different interpretations of the\nprocedure from primal and dual perspectives are given. While the approach is\napplicable to SDP relaxations of general polynomial programs, we apply it to\ntwo problems of discrete optimization: the maximum independent set problem and\nthe partition problem. We further show that some completely trivial instances\nof the partition problem lead to strictly positive polynomials on the boundary\nof the sum of squares cone and hence make the SOS relaxation fail. \n\n"}
{"id": "1510.07111", "contents": "Title: Dynamic programming approach to principal-agent problems Abstract: We consider a general formulation of the Principal-Agent problem with a\nlump-sum payment on a finite horizon, providing a systematic method for solving\nsuch problems. Our approach is the following: we first find the contract that\nis optimal among those for which the agent's value process allows a dynamic\nprogramming representation, for which the agent's optimal effort is\nstraightforward to find. We then show that the optimization over the restricted\nfamily of contracts represents no loss of generality. As a consequence, we have\nreduced this non-zero sum stochastic differential game to a stochastic control\nproblem which may be addressed by the standard tools of control theory. Our\nproofs rely on the backward stochastic differential equations approach to\nnon-Markovian stochastic control, and more specifically, on the recent\nextensions to the second order case. \n\n"}
{"id": "1510.08792", "contents": "Title: Centralized versus Decentralized Infrastructure Networks Abstract: While many large infrastructure networks, such as power, water, and natural\ngas systems, have similar physical properties governing flows, these systems\ntend to have distinctly different sizes and topological structures. This paper\nseeks to understand how these different size-scales and topological features\ncan emerge from relatively simple design principles. Specifically, we seek to\ndescribe the conditions under which it is optimal to build decentralized\nnetwork infrastructures, such as a microgrid, rather than centralized ones,\nsuch as a large high-voltage power system. While our method is simple it is\nuseful in explaining why sometimes, but not always, it is economical to build\nlarge, interconnected networks and in other cases it is preferable to use\nsmaller, distributed systems. The results indicate that there is not a single\nset of infrastructure cost conditions under which optimally-designed networks\nwill have highly centralized architectures. Instead, as costs increase we find\nthat average network sizes increase gradually according to a power-law. When we\nconsider the reliability costs, however, we do observe a transition point at\nwhich optimally designed networks become more centralized with larger\ngeographic scope. As the losses associated with node and edge failures become\nmore costly, this transition becomes more sudden. \n\n"}
{"id": "1510.09202", "contents": "Title: Generating Text with Deep Reinforcement Learning Abstract: We introduce a novel schema for sequence to sequence learning with a Deep\nQ-Network (DQN), which decodes the output sequence iteratively. The aim here is\nto enable the decoder to first tackle easier portions of the sequences, and\nthen turn to cope with difficult parts. Specifically, in each iteration, an\nencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the\ninput sequence, automatically create features to represent the internal states\nof and formulate a list of potential actions for the DQN. Take rephrasing a\nnatural sentence as an example. This list can contain ranked potential words.\nNext, the DQN learns to make decision on which action (e.g., word) will be\nselected from the list to modify the current decoded sequence. The newly\nmodified output sequence is subsequently used as the input to the DQN for the\nnext decoding iteration. In each iteration, we also bias the reinforcement\nlearning's attention to explore sequence portions which are previously\ndifficult to be decoded. For evaluation, the proposed strategy was trained to\ndecode ten thousands natural sentences. Our experiments indicate that, when\ncompared to a left-to-right greedy beam search LSTM decoder, the proposed\nmethod performed competitively well when decoding sentences from the training\nset, but significantly outperformed the baseline when decoding unseen\nsentences, in terms of BLEU score obtained. \n\n"}
{"id": "1511.00146", "contents": "Title: Faster Stochastic Variational Inference using Proximal-Gradient Methods\n  with General Divergence Functions Abstract: Several recent works have explored stochastic gradient methods for\nvariational inference that exploit the geometry of the variational-parameter\nspace. However, the theoretical properties of these methods are not\nwell-understood and these methods typically only apply to\nconditionally-conjugate models. We present a new stochastic method for\nvariational inference which exploits the geometry of the variational-parameter\nspace and also yields simple closed-form updates even for non-conjugate models.\nWe also give a convergence-rate analysis of our method and many other previous\nmethods which exploit the geometry of the space. Our analysis generalizes\nexisting convergence results for stochastic mirror-descent on non-convex\nobjectives by using a more general class of divergence functions. Beyond giving\na theoretical justification for a variety of recent methods, our experiments\nshow that new algorithms derived in this framework lead to state of the art\nresults on a variety of problems. Further, due to its generality, we expect\nthat our theoretical analysis could also apply to other applications. \n\n"}
{"id": "1511.00205", "contents": "Title: Eigenvalue Clustering, Control Energy, and Logarithmic Capacity Abstract: We prove two bounds showing that if the eigenvalues of a matrix are clustered\nin a region of the complex plane then the corresponding discrete-time linear\nsystem requires significant energy to control. A curious feature of one of our\nbounds is that the dependence on the region is via its logarithmic capacity,\nwhich is a measure of how well a unit of mass may be spread out over the region\nto minimize a logarithmic potential. \n\n"}
{"id": "1511.02963", "contents": "Title: Analysis and Design of Actuation-Sensing-Communication Interconnection\n  Structures towards Secured/Resilient Closed-loop Systems Abstract: This paper considers the analysis and design of resilient/robust\ndecentralized control systems. Specifically, we aim to assess how the pairing\nof sensors and actuators lead to architectures that are resilient to\nattacks/hacks for industrial control systems and other complex cyber-physical\nsystems. We consider inherent structural properties such as internal fixed\nmodes of a dynamical system depending on actuation, sensing, and\ninterconnection/communication structure for linear discrete time-invariant\ndynamical systems. We introduce the notion of resilient fixed-modes free system\nthat ensures the non-existence of fixed modes when the\nactuation-sensing-communication structure is compromised due to attacks by a\nmalicious agent on actuators, sensors, or communication components and natural\nfailures. Also, we provide a graph-theoretical characterization for the\nresilient structurally fixed modes that enables to capture the non-existence of\nresilient fixed modes for almost all possible systems' realizations.\nAdditionally, we address the minimum actuation-sensing-communication co-design\nensuring the non-existence of resiliently structurally fixed modes, which we\nshow to be NP-hard. Notwithstanding, we identify conditions that are often\nsatisfied in engineering settings and under which the co-design problem is\nsolvable in polynomial-time complexity. Furthermore, we leverage the structural\ninsights and properties to provide a convex optimization method to design the\ngain for a parametrized system and satisfying the sparsity of a given\ninformation pattern. Thus, exploring the interplay between structural and\nnon-structural systems to ensure their resilience. Finally, the efficacy of the\nproposed approach is demonstrated on a power grid example. \n\n"}
{"id": "1511.03414", "contents": "Title: A dynamic state transition algorithm with application to sensor network\n  localization Abstract: The sensor network localization (SNL) problem is to reconstruct the positions\nof all the sensors in a network with the given distance between pairs of\nsensors and within the radio range between them. It is proved that the\ncomputational complexity of the SNL problem is NP-hard, and semi-definite\nprogramming or second-order cone programming relaxation methods are only able\nto solve some special problems of this kind. In this study, a stochastic global\noptimization method called the state transition algorithm is introduced to\nsolve the SNL problem without additional assumptions and conditions of the\nproblem structure. To transcend local optimality, a novel dynamic adjustment\nstrategy called \"risk and restoration in probability\" is incorporated into the\nstate transition algorithm. An empirical study is investigated to appropriately\nchoose the \"risk probability\" and \"restoration probability\", yielding the\ndynamic state transition algorithm, which is further improved by gradient-based\nrefinement. The dynamic state transition algorithm with refinement is applied\nto the SNL problem, and satisfactory experimental results have testified the\neffectiveness of the proposed approach. \n\n"}
{"id": "1511.04561", "contents": "Title: 8-Bit Approximations for Parallelism in Deep Learning Abstract: The creation of practical deep learning data-products often requires\nparallelization across processors and computers to make deep learning feasible\non large data sets, but bottlenecks in communication bandwidth make it\ndifficult to attain good speedups through parallelism. Here we develop and test\n8-bit approximation algorithms which make better use of the available bandwidth\nby compressing 32-bit gradients and nonlinear activations to 8-bit\napproximations. We show that these approximations do not decrease predictive\nperformance on MNIST, CIFAR10, and ImageNet for both model and data parallelism\nand provide a data transfer speedup of 2x relative to 32-bit parallelism. We\nbuild a predictive model for speedups based on our experimental data, verify\nits validity on known speedup data, and show that we can obtain a speedup of\n50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We\ncompare our data types with other methods and show that 8-bit approximations\nachieve state-of-the-art speedups for model parallelism. Thus 8-bit\napproximation is an efficient method to parallelize convolutional networks on\nvery large systems of GPUs. \n\n"}
{"id": "1511.04813", "contents": "Title: Budget Online Multiple Kernel Learning Abstract: Online learning with multiple kernels has gained increasing interests in\nrecent years and found many applications. For classification tasks, Online\nMultiple Kernel Classification (OMKC), which learns a kernel based classifier\nby seeking the optimal linear combination of a pool of single kernel\nclassifiers in an online fashion, achieves superior accuracy and enjoys great\nflexibility compared with traditional single-kernel classifiers. Despite being\nstudied extensively, existing OMKC algorithms suffer from high computational\ncost due to their unbounded numbers of support vectors. To overcome this\ndrawback, we present a novel framework of Budget Online Multiple Kernel\nLearning (BOMKL) and propose a new Sparse Passive Aggressive learning to\nperform effective budget online learning. Specifically, we adopt a simple yet\neffective Bernoulli sampling to decide if an incoming instance should be added\nto the current set of support vectors. By limiting the number of support\nvectors, our method can significantly accelerate OMKC while maintaining\nsatisfactory accuracy that is comparable to that of the existing OMKC\nalgorithms. We theoretically prove that our new method achieves an optimal\nregret bound in expectation, and empirically found that the proposed algorithm\noutperforms various OMKC algorithms and can easily scale up to large-scale\ndatasets. \n\n"}
{"id": "1511.06566", "contents": "Title: Acceleration of the PDHGM on strongly convex subspaces Abstract: We propose several variants of the primal-dual method due to Chambolle and\nPock. Without requiring full strong convexity of the objective functions, our\nmethods are accelerated on subspaces with strong convexity. This yields mixed\nrates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect to\nthe dual sequence, and the residual part of the primal sequence. We demonstrate\nthe efficacy of the proposed methods on image processing problems lacking\nstrong convexity, such as total generalised variation denoising and total\nvariation deblurring. \n\n"}
{"id": "1511.07562", "contents": "Title: Efficient Dynamic Compressor Optimization in Natural Gas Transmission\n  Systems Abstract: The growing reliance of electric power systems on gas-fired generation to\nbalance intermittent sources of renewable energy has increased the variation\nand volume of flows through natural gas transmission pipelines. Adapting\npipeline operations to maintain efficiency and security under these new\nconditions requires optimization methods that account for transients and that\ncan quickly compute solutions in reaction to generator re-dispatch. This paper\npresents an efficient scheme to minimize compression costs under dynamic\nconditions where deliveries to customers are described by time-dependent mass\nflow. The optimization scheme relies on a compact representation of gas flow\nphysics, a trapezoidal discretization in time and space, and a two-stage\napproach to minimize energy costs and maximize smoothness. The resulting\nlarge-scale nonlinear programs are solved using a modern interior-point method.\nThe proposed optimization scheme is validated against an integration of dynamic\nequations with adaptive time-stepping, as well as a recently proposed\nstate-of-the-art optimal control method. The comparison shows that the\nsolutions are feasible for the continuous problem and also practical from an\noperational standpoint. The results also indicate that our scheme provides at\nleast an order of magnitude reduction in computation time relative to the\nstate-of-the-art and scales to large gas transmission networks with more than\n6000 kilometers of total pipeline. \n\n"}
{"id": "1511.08032", "contents": "Title: Learning to detect video events from zero or very few video examples Abstract: In this work we deal with the problem of high-level event detection in video.\nSpecifically, we study the challenging problems of i) learning to detect video\nevents from solely a textual description of the event, without using any\npositive video examples, and ii) additionally exploiting very few positive\ntraining samples together with a small number of ``related'' videos. For\nlearning only from an event's textual description, we first identify a general\nlearning framework and then study the impact of different design choices for\nvarious stages of this framework. For additionally learning from example\nvideos, when true positive training samples are scarce, we employ an extension\nof the Support Vector Machine that allows us to exploit ``related'' event\nvideos by automatically introducing different weights for subsets of the videos\nin the overall training set. Experimental evaluations performed on the\nlarge-scale TRECVID MED 2014 video dataset provide insight on the effectiveness\nof the proposed methods. \n\n"}
{"id": "1512.00583", "contents": "Title: Central-limit approach to risk-aware Markov decision processes Abstract: Whereas classical Markov decision processes maximize the expected reward, we\nconsider minimizing the risk. We propose to evaluate the risk associated to a\ngiven policy over a long-enough time horizon with the help of a central limit\ntheorem. The proposed approach works whether the transition probabilities are\nknown or not. We also provide a gradient-based policy improvement algorithm\nthat converges to a local optimum of the risk objective. \n\n"}
{"id": "1512.01218", "contents": "Title: Optimal Sizing and Placement of Distributed Storage in Low Voltage\n  Networks Abstract: This paper proposes a novel algorithm to optimally size and place storage in\nlow voltage (LV) networks based on a linearized multiperiod optimal power flow\nmethod which we call forward backward sweep optimal power flow (FBS-OPF). We\nshow that this method has good convergence properties, its solution deviates\nslightly from the optimum and makes the storage sizing and placement problem\ntractable for longer investment horizons. We demonstrate the usefulness of our\nmethod by assessing the economic viability of distributed and centralized\nstorage in LV grids with a high photovoltaic penetration (PV). As a main\nresult, we quantify that for the CIGRE LV test grid distributed storage\nconfigurations are preferable, since they allow for less PV curtailment due to\ngrid constraints. \n\n"}
{"id": "1512.02549", "contents": "Title: Facial Reduction and Partial Polyhedrality Abstract: We present FRA-Poly, a facial reduction algorithm (FRA) for conic linear\nprograms that is sensitive to the presence of polyhedral faces in the cone. The\nmain goals of FRA and FRA-Poly are the same, i.e., finding the minimal face\ncontaining the feasible region and detecting infeasibility, but FRA-Poly treats\npolyhedral constraints separately. This idea enables us to reduce the number of\niterations drastically when there are many linear inequality constraints. The\nworst case number of iterations for FRA-poly is written in the terms of a\n\"distance to polyhedrality\" quantity and provides better bounds than FRA under\nmild conditions. In particular, in the case of the doubly nonnegative cone,\nFRA-Poly gives a worst case bound of $n$ whereas the classical FRA is\n$\\mathcal{O}(n^2)$. Of possible independent interest, we prove a variant of\nGordan-Stiemke's Theorem and a proper separation theorem that takes into\naccount partial polyhedrality. We provide a discussion on the optimal facial\nreduction strategy and an instance that forces FRAs to perform many steps. We\nalso present a few applications. In particular, we will use FRA-poly to improve\nthe bounds recently obtained by Liu and Pataki on the dimension of certain\naffine subspaces which appear in weakly infeasible problems. \n\n"}
{"id": "1512.04702", "contents": "Title: Second order dynamical systems associated to variational inequalities Abstract: We investigate the asymptotic convergence of the trajectories generated by\nthe second order dynamical system $\\ddot x(t) + \\gamma\\dot x(t) + \\nabla\n\\phi(x(t))+\\beta(t)\\nabla \\psi(x(t))=0$, where $\\phi,\\psi:{\\cal H}\\rightarrow\n\\R$ are convex and smooth functions defined on a real Hilbert space ${\\cal H}$,\n$\\gamma>0$ and $\\beta$ is a function of time which controls the penalty term.\nWe show weak convergence of the trajectories to a minimizer of the function\n$\\phi$ over the (nonempty) set of minima of $\\psi$ as well as convergence for\nthe objective function values along the trajectories, provided a condition\nexpressed via the Fenchel conjugate of $\\psi$ is fulfilled. When the function\n$\\phi$ is assumed to be strongly convex, we can even show strong convergence of\nthe trajectories. The results can be seen as the second order counterparts of\nthe ones given by Attouch and Czarnecki (Journal of Differential Equations\n248(6), 1315--1344, 2010) for first order dynamical systems associated to\nconstrained variational inequalities. At the same time we give a positive\nanswer to an open problem posed in \\cite{att-cza-16} by the same authors. \n\n"}
{"id": "1512.06427", "contents": "Title: Towards Integrated Glance To Restructuring in Combinatorial Optimization Abstract: The paper focuses on a new class of combinatorial problems which consists in\nrestructuring of solutions (as sets/structures) in combinatorial optimization.\nTwo main features of the restructuring process are examined: (i) a cost of the\nrestructuring, (ii) a closeness to a goal solution. Three types of the\nrestructuring problems are under study: (a) one-stage structuring, (b)\nmulti-stage structuring, and (c) structuring over changed element set.\nOne-criterion and multicriteria problem formulations can be considered. The\nrestructuring problems correspond to redesign (improvement, upgrade) of modular\nsystems or solutions. The restructuring approach is described and illustrated\n(problem statements, solving schemes, examples) for the following combinatorial\noptimization problems: knapsack problem, multiple choice problem, assignment\nproblem, spanning tree problems, clustering problem, multicriteria ranking\n(sorting) problem, morphological clique problem. Numerical examples illustrate\nthe restructuring problems and solving schemes. \n\n"}
{"id": "1601.00034", "contents": "Title: Stochastic Neural Networks with Monotonic Activation Functions Abstract: We propose a Laplace approximation that creates a stochastic unit from any\nsmooth monotonic activation function, using only Gaussian noise. This paper\ninvestigates the application of this stochastic approximation in training a\nfamily of Restricted Boltzmann Machines (RBM) that are closely linked to\nBregman divergences. This family, that we call exponential family RBM\n(Exp-RBM), is a subset of the exponential family Harmoniums that expresses\nfamily members through a choice of smooth monotonic non-linearity for each\nneuron. Using contrastive divergence along with our Gaussian approximation, we\nshow that Exp-RBM can learn useful representations using novel stochastic\nunits. \n\n"}
{"id": "1601.02039", "contents": "Title: Informational Braess' Paradox: The Effect of Information on Traffic\n  Congestion Abstract: To systematically study the implications of additional information about\nroutes provided to certain users (e.g., via GPS-based route guidance systems),\nwe introduce a new class of congestion games in which users have differing\ninformation sets about the available edges and can only use routes consisting\nof edges in their information set. After defining the notion of Information\nConstrained Wardrop Equilibrium (ICWE) for this class of congestion games and\nstudying its basic properties, we turn to our main focus: whether additional\ninformation can be harmful (in the sense of generating greater equilibrium\ncosts/delays). We formulate this question in the form of Informational Braes'\nParadox (IBP), which extends the classic Braess' Paradox in traffic equilibria,\nand asks whether users receiving additional information can become worse off.\nWe provide a comprehensive answer to this question showing that in any network\nin the series of linearly independent (SLI) class, which is a strict subset of\nseries-parallel networks, IBP cannot occur, and in any network that is not in\nthe SLI class, there exists a configuration of edge-specific cost functions for\nwhich IBP will occur. In the process, we establish several properties of the\nSLI class of networks, which include the characterization of the complement of\nthe SLI class in terms of embedding a specific set of networks, and also an\nalgorithm which determines whether a graph is SLI in linear time. We further\nprove that the worst-case inefficiency performance of ICWE is no worse than the\nstandard Wardrop equilibrium. \n\n"}
{"id": "1601.02239", "contents": "Title: On minimax theorems for lower semicontinuous functions in Hilbert spaces Abstract: We prove minimax theorems for lower semicontinuous functions defined on a\nHilbert space. The main tool is the theory of $\\Phi$-convex functions and\nsufficient and necessary conditions for the minimax equality to hold for\n$\\Phi$-convex functions. These conditions are expressed in terms of abstract\n$\\Phi$-subgradients. \n\n"}
{"id": "1601.03249", "contents": "Title: Optimal trajectory tracking Abstract: This thesis investigates optimal trajectory tracking of nonlinear dynamical\nsystems with affine controls. The control task is to enforce the system state\nto follow a prescribed desired trajectory as closely as possible. The concept\nof so-called exactly realizable trajectories is proposed. For exactly\nrealizable desired trajectories exists a control signal which enforces the\nstate to exactly follow the desired trajectory. For a given affine control\nsystem, these trajectories are characterized by the so-called constraint\nequation. This approach does not only yield an explicit expression for the\ncontrol signal in terms of the desired trajectory, but also identifies a\nparticularly simple class of nonlinear control systems. Based on that insight,\nthe regularization parameter is used as the small parameter for a perturbation\nexpansion. This results in a reinterpretation of affine optimal control\nproblems with small regularization term as singularly perturbed differential\nequations. The small parameter originates from the formulation of the control\nproblem and does not involve simplifying assumptions about the system dynamics.\nCombining this approach with the linearizing assumption, approximate and partly\nlinear equations for the optimal trajectory tracking of arbitrary desired\ntrajectories are derived. For vanishing regularization parameter, the state\ntrajectory becomes discontinuous and the control signal diverges. On the other\nhand, the analytical treatment becomes exact and the solutions are exclusively\ngoverned by linear differential equations. Thus, the possibility of linear\nstructures underlying nonlinear optimal control is revealed. This fact enables\nthe derivation of exact analytical solutions to an entire class of nonlinear\ntrajectory tracking problems with affine controls. This class comprises\nmechanical control systems in one spatial dimension and the FitzHugh-Nagumo\nmodel. \n\n"}
{"id": "1601.03479", "contents": "Title: Collective Circular Motion of Multi-Agent Systems in Synchronized and\n  Balanced Formations With Second-Order Rotational Dynamics Abstract: This paper considers the collective circular motion of multi-agent systems in\nwhich all the agents are required to traverse different circles or a common\ncircle at a prescribed angular velocity. It is required to achieve these\ncollective motions with the heading angles of the agents synchronized or\nbalanced. In synchronization, the agents and their centroid have a common\nvelocity direction, while in balancing, the movement of agents causes the\nlocation of the centroid to become stationary. The agents considered are\ninitially moving at unit speed around individual circles at different angular\nvelocities. It is assumed that the agents are subjected to limited\ncommunication constraints, and exchange relative information according to a\ntime-invariant undirected graph. We present suitable feedback control laws for\neach of these motion coordination tasks by considering a second-order\nrotational dynamics of the agent. Simulations are given to illustrate the\ntheoretical findings. \n\n"}
{"id": "1601.04891", "contents": "Title: Stochastic control, entropic interpolation and gradient flows on\n  Wasserstein product spaces Abstract: Since the early nineties, it has been observed that the Schroedinger bridge\nproblem can be formulated as a stochastic control problem with atypical\nboundary constraints. This in turn has a fluid dynamic counterpart where the\nflow of probability densities represents an entropic interpolation between the\ngiven initial and final marginals. In the zero noise limit, such entropic\ninterpolation converges in a suitable sense to the displacement interpolation\nof optimal mass transport (OMT). We consider two absolutely continuous curves\nin Wasserstein space ${\\cal W}_2$ and study the evolution of the relative\nentropy on ${\\cal W}_2\\times {\\cal W}_2$ on a finite time interval. Thus, this\nstudy differs from previous work in OMT theory concerning relative entropy from\na fixed (often equilibrium) distribution (density). We derive a gradient flow\non Wasserstein product space. We find the remarkable property that fluxes in\nthe two components are opposite. Plugging in the \"steepest descent\" into the\nevolution of the relative entropy we get what appears to be a new formula: The\ntwo flows approach each other at a faster rate than that of two solutions of\nthe same Fokker-Planck. We then study the evolution of relative entropy in the\ncase of uncontrolled-controlled diffusions. In two special cases of the\nSchroedinger bridge problem, we show that such relative entropy may be\nmonotonically decreasing or monotonically increasing. \n\n"}
{"id": "1601.07277", "contents": "Title: A General System for Heuristic Solution of Convex Problems over\n  Nonconvex Sets Abstract: We describe general heuristics to approximately solve a wide variety of\nproblems with convex objective and decision variables from a nonconvex set. The\nheuristics, which employ convex relaxations, convex restrictions, local\nneighbor search methods, and the alternating direction method of multipliers\n(ADMM), require the solution of a modest number of convex problems, and are\nmeant to apply to general problems, without much tuning. We describe an\nimplementation of these methods in a package called NCVX, as an extension of\nCVXPY, a Python package for formulating and solving convex optimization\nproblems. We study several examples of well known nonconvex problems, and show\nthat our general purpose heuristics are effective in finding approximate\nsolutions to a wide variety of problems. \n\n"}
{"id": "1602.02454", "contents": "Title: Efficient Algorithms for Adversarial Contextual Learning Abstract: We provide the first oracle efficient sublinear regret algorithms for\nadversarial versions of the contextual bandit problem. In this problem, the\nlearner repeatedly makes an action on the basis of a context and receives\nreward for the chosen action, with the goal of achieving reward competitive\nwith a large class of policies. We analyze two settings: i) in the transductive\nsetting the learner knows the set of contexts a priori, ii) in the small\nseparator setting, there exists a small set of contexts such that any two\npolicies behave differently in one of the contexts in the set. Our algorithms\nfall into the follow the perturbed leader family \\cite{Kalai2005} and achieve\nregret $O(T^{3/4}\\sqrt{K\\log(N)})$ in the transductive setting and $O(T^{2/3}\nd^{3/4} K\\sqrt{\\log(N)})$ in the separator setting, where $K$ is the number of\nactions, $N$ is the number of baseline policies, and $d$ is the size of the\nseparator. We actually solve the more general adversarial contextual\nsemi-bandit linear optimization problem, whilst in the full information setting\nwe address the even more general contextual combinatorial optimization. We\nprovide several extensions and implications of our algorithms, such as\nswitching regret and efficient learning with predictable sequences. \n\n"}
{"id": "1602.02823", "contents": "Title: Poor starting points in machine learning Abstract: Poor (even random) starting points for learning/training/optimization are\ncommon in machine learning. In many settings, the method of Robbins and Monro\n(online stochastic gradient descent) is known to be optimal for good starting\npoints, but may not be optimal for poor starting points -- indeed, for poor\nstarting points Nesterov acceleration can help during the initial iterations,\neven though Nesterov methods not designed for stochastic approximation could\nhurt during later iterations. The common practice of training with nontrivial\nminibatches enhances the advantage of Nesterov acceleration. \n\n"}
{"id": "1602.04436", "contents": "Title: Autoregressive Moving Average Graph Filtering Abstract: One of the cornerstones of the field of signal processing on graphs are graph\nfilters, direct analogues of classical filters, but intended for signals\ndefined on graphs. This work brings forth new insights on the distributed graph\nfiltering problem. We design a family of autoregressive moving average (ARMA)\nrecursions, which (i) are able to approximate any desired graph frequency\nresponse, and (ii) give exact solutions for tasks such as graph signal\ndenoising and interpolation. The design philosophy, which allows us to design\nthe ARMA coefficients independently from the underlying graph, renders the ARMA\ngraph filters suitable in static and, particularly, time-varying settings. The\nlatter occur when the graph signal and/or graph are changing over time. We show\nthat in case of a time-varying graph signal our approach extends naturally to a\ntwo-dimensional filter, operating concurrently in the graph and regular time\ndomains. We also derive sufficient conditions for filter stability when the\ngraph and signal are time-varying. The analytical and numerical results\npresented in this paper illustrate that ARMA graph filters are practically\nappealing for static and time-varying settings, as predicted by theoretical\nderivations. \n\n"}
{"id": "1602.04785", "contents": "Title: Approximate solutions of continuous-time stochastic games Abstract: The paper is concerned with a zero-sum continuous-time stochastic\ndifferential game with a dynamics controlled by a Markov process and a terminal\npayoff. The value function of the original game is estimated using the value\nfunction of a model game. The dynamics of the model game differs from the\noriginal one. The general result applied to differential games yields the\napproximation of value function of differential game by the solution of\ncountable system of ODEs. \n\n"}
{"id": "1602.05130", "contents": "Title: Risk Aversion in Finite Markov Decision Processes Using Total Cost\n  Criteria and Average Value at Risk Abstract: In this paper we present an algorithm to compute risk averse policies in\nMarkov Decision Processes (MDP) when the total cost criterion is used together\nwith the average value at risk (AVaR) metric. Risk averse policies are needed\nwhen large deviations from the expected behavior may have detrimental effects,\nand conventional MDP algorithms usually ignore this aspect. We provide\nconditions for the structure of the underlying MDP ensuring that approximations\nfor the exact problem can be derived and solved efficiently. Our findings are\nnovel inasmuch as average value at risk has not previously been considered in\nassociation with the total cost criterion. Our method is demonstrated in a\nrapid deployment scenario, whereby a robot is tasked with the objective of\nreaching a target location within a temporal deadline where increased speed is\nassociated with increased probability of failure. We demonstrate that the\nproposed algorithm not only produces a risk averse policy reducing the\nprobability of exceeding the expected temporal deadline, but also provides the\nstatistical distribution of costs, thus offering a valuable analysis tool. \n\n"}
{"id": "1602.06604", "contents": "Title: Detection of Cyber-Physical Faults and Intrusions from Physical\n  Correlations Abstract: Cyber-physical systems are critical infrastructures that are crucial both to\nthe reliable delivery of resources such as energy, and to the stable\nfunctioning of automatic and control architectures. These systems are composed\nof interdependent physical, control and communications networks described by\ndisparate mathematical models creating scientific challenges that go well\nbeyond the modeling and analysis of the individual networks. A key challenge in\ncyber-physical defense is a fast online detection and localization of faults\nand intrusions without prior knowledge of the failure type. We describe a set\nof techniques for the efficient identification of faults from correlations in\nphysical signals, assuming only a minimal amount of available system\ninformation. The performance of our detection method is illustrated on data\ncollected from a large building automation system. \n\n"}
{"id": "1602.07570", "contents": "Title: Bayesian Exploration: Incentivizing Exploration in Bayesian Games Abstract: We consider a ubiquitous scenario in the Internet economy when individual\ndecision-makers (henceforth, agents) both produce and consume information as\nthey make strategic choices in an uncertain environment. This creates a\nthree-way tradeoff between exploration (trying out insufficiently explored\nalternatives to help others in the future), exploitation (making optimal\ndecisions given the information discovered by other agents), and incentives of\nthe agents (who are myopically interested in exploitation, while preferring the\nothers to explore). We posit a principal who controls the flow of information\nfrom agents that came before, and strives to coordinate the agents towards a\nsocially optimal balance between exploration and exploitation, not using any\nmonetary transfers. The goal is to design a recommendation policy for the\nprincipal which respects agents' incentives and minimizes a suitable notion of\nregret.\n  We extend prior work in this direction to allow the agents to interact with\none another in a shared environment: at each time step, multiple agents arrive\nto play a Bayesian game, receive recommendations, choose their actions, receive\ntheir payoffs, and then leave the game forever. The agents now face two sources\nof uncertainty: the actions of the other agents and the parameters of the\nuncertain game environment.\n  Our main contribution is to show that the principal can achieve constant\nregret when the utilities are deterministic (where the constant depends on the\nprior distribution, but not on the time horizon), and logarithmic regret when\nthe utilities are stochastic. As a key technical tool, we introduce the concept\nof explorable actions, the actions which some incentive-compatible policy can\nrecommend with non-zero probability. We show how the principal can identify\n(and explore) all explorable actions, and use the revealed information to\nperform optimally. \n\n"}
{"id": "1602.08372", "contents": "Title: Explicit Conditions on Existence and Uniqueness of Load-Flow Solutions\n  in Distribution Networks Abstract: We present explicit sufficient conditions that guarantee the existence and\nuniqueness of the feasible load-flow solution for distribution networks with a\ngeneric topology (radial or meshed) modeled with positive sequence equivalents.\nIn the problem, we also account for the presence of shunt elements. The\nconditions have low computational complexity and thus can be efficiently\nverified in a real system. Once the conditions are satisfied, the unique\nload-flow solution can be reached by a given fixed point iteration method of\napproximately linear complexity. Therefore, the proposed approach is of\nparticular interest for modern active distribution network (ADN) setup in the\ncontext of real-time control. The theory has been confirmed through numerical\nexperiments. \n\n"}
{"id": "1602.08509", "contents": "Title: Estimating Distribution Grid Topologies: A Graphical Learning based\n  Approach Abstract: Distribution grids represent the final tier in electric networks consisting\nof medium and low voltage lines that connect the distribution substations to\nthe end-users. Traditionally, distribution networks have been operated in a\nradial topology that may be changed from time to time. Due to absence of a\nsignificant number of real-time line monitoring devices in the distribution\ngrid, estimation of the topology is a problem critical for its observability\nand control. This paper develops a novel graphical learning based approach to\nestimate the radial operational grid structure using voltage measurements\ncollected from the grid loads. The learning algorithm is based on conditional\nindependence tests for continuous variables over chordal graphs and has wide\napplicability. It is proven that the scheme can be used for several power flow\nlaws (DC or AC approximations) and more importantly is independent of the\nspecific probability distribution controlling individual bus power usage. The\ncomplexity of the algorithm is discussed and its performance is demonstrated by\nsimulations on distribution test cases. \n\n"}
{"id": "1603.01121", "contents": "Title: Deep Reinforcement Learning from Self-Play in Imperfect-Information\n  Games Abstract: Many real-world applications can be described as large-scale games of\nimperfect information. To deal with these challenging domains, prior work has\nfocused on computing Nash equilibria in a handcrafted abstraction of the\ndomain. In this paper we introduce the first scalable end-to-end approach to\nlearning approximate Nash equilibria without prior domain knowledge. Our method\ncombines fictitious self-play with deep reinforcement learning. When applied to\nLeduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,\nwhereas common reinforcement learning methods diverged. In Limit Texas Holdem,\na poker game of real-world scale, NFSP learnt a strategy that approached the\nperformance of state-of-the-art, superhuman algorithms based on significant\ndomain expertise. \n\n"}
{"id": "1603.01545", "contents": "Title: Minimum-Time Transitions between Thermal Equilibrium States of the\n  Quantum Parametric Oscillator Abstract: In this article, we use geometric optimal control to completely solve the\nproblem of minimum-time transitions between thermal equilibrium states of the\nquantum parametric oscillator, which finds applications in various physical\ncontexts. We discover a new kind of optimal solutions, absent from all the\nprevious treatments of the problem. \n\n"}
{"id": "1603.01681", "contents": "Title: A single-phase, proximal path-following framework Abstract: We propose a new proximal, path-following framework for a class of\nconstrained convex problems. We consider settings where the nonlinear---and\npossibly non-smooth---objective part is endowed with a proximity operator, and\nthe constraint set is equipped with a self-concordant barrier. Our approach\nrelies on the following two main ideas. First, we re-parameterize the\noptimality condition as an auxiliary problem, such that a good initial point is\navailable; by doing so, a family of alternative paths towards the optimum is\ngenerated. Second, we combine the proximal operator with path-following ideas\nto design a single-phase, proximal, path-following algorithm. Our method has\nseveral advantages. First, it allows handling non-smooth objectives via\nproximal operators; this avoids lifting the problem dimension in order to\naccommodate non-smooth components in optimization. Second, it consists of only\na \\emph{single phase}: While the overall convergence rate of classical\npath-following schemes for self-concordant objectives does not suffer from the\ninitialization phase, proximal path-following schemes undergo slow convergence,\nin order to obtain a good starting point \\cite{TranDinh2013e}. In this work, we\nshow how to overcome this limitation in the proximal setting and prove that our\nscheme has the same $\\mathcal{O}(\\sqrt{\\nu}\\log(1/\\varepsilon))$ worst-case\niteration-complexity with standard approaches \\cite{Nesterov2004,Nesterov1994}\nwithout requiring an initial phase, where $\\nu$ is the barrier parameter and\n$\\varepsilon$ is a desired accuracy. Finally, our framework allows errors in\nthe calculation of proximal-Newton directions, without sacrificing the\nworst-case iteration complexity. We demonstrate the merits of our algorithm via\nthree numerical examples, where proximal operators play a key role. \n\n"}
{"id": "1603.02038", "contents": "Title: Unscented Bayesian Optimization for Safe Robot Grasping Abstract: We address the robot grasp optimization problem of unknown objects\nconsidering uncertainty in the input space. Grasping unknown objects can be\nachieved by using a trial and error exploration strategy. Bayesian optimization\nis a sample efficient optimization algorithm that is especially suitable for\nthis setups as it actively reduces the number of trials for learning about the\nfunction to optimize. In fact, this active object exploration is the same\nstrategy that infants do to learn optimal grasps. One problem that arises while\nlearning grasping policies is that some configurations of grasp parameters may\nbe very sensitive to error in the relative pose between the object and robot\nend-effector. We call these configurations unsafe because small errors during\ngrasp execution may turn good grasps into bad grasps. Therefore, to reduce the\nrisk of grasp failure, grasps should be planned in safe areas. We propose a new\nalgorithm, Unscented Bayesian optimization that is able to perform sample\nefficient optimization while taking into consideration input noise to find safe\noptima. The contribution of Unscented Bayesian optimization is twofold as if\nprovides a new decision process that drives exploration to safe regions and a\nnew selection procedure that chooses the optimal in terms of its safety without\nextra analysis or computational cost. Both contributions are rooted on the\nstrong theory behind the unscented transformation, a popular nonlinear\napproximation method. We show its advantages with respect to the classical\nBayesian optimization both in synthetic problems and in realistic robot grasp\nsimulations. The results highlights that our method achieves optimal and robust\ngrasping policies after few trials while the selected grasps remain in safe\nregions. \n\n"}
{"id": "1603.02595", "contents": "Title: Connection between MP and DPP for Stochastic Recursive Optimal Control\n  Problems: Viscosity Solution Framework in General Case Abstract: This paper deals with a stochastic recursive optimal control problem, where\nthe diffusion coefficient depends on the control variable and the control\ndomain is not necessarily convex. We focus on the connection between the\ngeneral maximum principle and the dynamic programming principle for such\ncontrol problem without the assumption that the value is smooth enough, the set\ninclusions among the sub- and super-jets of the value function and the\nfirst-order and second-order adjoint processes as well as the generalized\nHamiltonian function are established. Moreover, by comparing these results with\nthe classical ones in Yong and Zhou [{\\em Stochastic Controls: Hamiltonian\nSystems and HJB Equations, Springer-Verlag, New York, 1999}], it is natural to\nobtain the first- and second-order adjoint equations of Hu [{\\em Direct method\non stochastic maximum principle for optimization with recursive utilities,\narXiv:1507.03567v1 [math.OC], 13 Jul. 2015}]. \n\n"}
{"id": "1603.03265", "contents": "Title: Dynamics and optimal control of Ebola transmission Abstract: A major Ebola outbreak occurs in West Africa since March 2014, being the\ndeadliest epidemic in history. As an infectious disease epidemiology, Ebola is\nthe most lethal and is moving faster than in previous outbreaks. On 8 August\n2014, the World Health Organization (WHO) declared the outbreak a public health\nemergency of international concern. Last update on 7 July 2015 by WHO reports\n27 609 cases of Ebola with a total of 11 261 deaths. In this work, we present a\nmathematical description of the spread of Ebola virus based on the SEIR\n(Susceptible-Exposed-Infective-Recovered) model and optimal strategies for\nEbola control. In order to control the propagation of the virus and to predict\nthe impact of vaccine programmes, we investigate several strategies of optimal\ncontrol of the spread of Ebola: control infection by vaccination of\nsusceptible; minimize exposed and infected; reduce Ebola infection by\nvaccination and education. \n\n"}
{"id": "1603.03657", "contents": "Title: Efficient forward propagation of time-sequences in convolutional neural\n  networks using Deep Shifting Abstract: When a Convolutional Neural Network is used for on-the-fly evaluation of\ncontinuously updating time-sequences, many redundant convolution operations are\nperformed. We propose the method of Deep Shifting, which remembers previously\ncalculated results of convolution operations in order to minimize the number of\ncalculations. The reduction in complexity is at least a constant and in the\nbest case quadratic. We demonstrate that this method does indeed save\nsignificant computation time in a practical implementation, especially when the\nnetworks receives a large number of time-frames. \n\n"}
{"id": "1603.04416", "contents": "Title: Criteria of efficiency for conformal prediction Abstract: We study optimal conformity measures for various criteria of efficiency of\nclassification in an idealised setting. This leads to an important class of\ncriteria of efficiency that we call probabilistic; it turns out that the most\nstandard criteria of efficiency used in literature on conformal prediction are\nnot probabilistic unless the problem of classification is binary. We consider\nboth unconditional and label-conditional conformal prediction. \n\n"}
{"id": "1603.04467", "contents": "Title: TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems Abstract: TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org. \n\n"}
{"id": "1603.04667", "contents": "Title: Stationary Graph Processes and Spectral Estimation Abstract: Stationarity is a cornerstone property that facilitates the analysis and\nprocessing of random signals in the time domain. Although time-varying signals\nare abundant in nature, in many practical scenarios the information of interest\nresides in more irregular graph domains. This lack of regularity hampers the\ngeneralization of the classical notion of stationarity to graph signals. The\ncontribution in this paper is twofold. Firstly, we propose a definition of weak\nstationarity for random graph signals that takes into account the structure of\nthe graph where the random process takes place, while inheriting many of the\nmeaningful properties of the classical definition in the time domain. Our\ndefinition requires that stationary graph processes can be modeled as the\noutput of a linear graph filter applied to a white input. We will show that\nthis is equivalent to requiring the correlation matrix to be diagonalized by\nthe graph Fourier transform. Secondly, we analyze the properties of the power\nspectral density and propose a number of methods to estimate it. We start with\nnonparametric approaches, including periodograms, window-based average\nperiodograms, and filter banks. We then shift the focus to parametric\napproaches, discussing the estimation of moving-average (MA), autoregressive\n(AR) and ARMA processes. Finally, we illustrate the power spectral density\nestimation in synthetic and real-world graphs. \n\n"}
{"id": "1603.04914", "contents": "Title: Boundary Control of Coupled Reaction-Advection-Diffusion Systems with\n  Spatially-Varying Coefficients Abstract: Recently, the problem of boundary stabilization for unstable linear\nconstant-coefficient coupled reaction-diffusion systems was solved by means of\nthe backstepping method. The extension of this result to systems with advection\nterms and spatially-varying coefficients is challenging due to complex boundary\nconditions that appear in the equations verified by the control kernels. In\nthis paper we address this issue by showing that these equations are\nessentially equivalent to those verified by the control kernels for first-order\nhyperbolic coupled systems, which were recently found to be well-posed. The\nresult therefore applies in this case, allowing us to prove H^1 stability for\nthe closed-loop system. It also shows an interesting connection between\nbackstepping kernels for coupled parabolic and hyperbolic problems. \n\n"}
{"id": "1603.05307", "contents": "Title: Cooperative Robust Estimation with Local Performance Guarantees Abstract: The paper considers the problem of cooperative estimation for a linear\nuncertain plant observed by a network of communicating sensors. We take a novel\napproach by treating the filtering problem from the view point of local sensors\nwhile the network interconnections are accounted for via an uncertain signals\nmodelling of estimation performance of other nodes. That is, the information\ncommunicated between the nodes is treated as the true plant information subject\nto perturbations, and each node is endowed with certain believes about these\nperturbations during the filter design. The proposed distributed filter\nachieves a suboptimal $H_\\infty$ consensus performance. Furthermore, local\nperformance of each estimator is also assessed given additional constraints on\nthe performance of the other nodes. These conditions are shown to be useful in\ntuning the desired estimation performance of the sensor network. \n\n"}
{"id": "1603.05962", "contents": "Title: Document Neural Autoregressive Distribution Estimation Abstract: We present an approach based on feed-forward neural networks for learning the\ndistribution of textual documents. This approach is inspired by the Neural\nAutoregressive Distribution Estimator(NADE) model, which has been shown to be a\ngood estimator of the distribution of discrete-valued igh-dimensional vectors.\nIn this paper, we present how NADE can successfully be adapted to the case of\ntextual data, retaining from NADE the property that sampling or computing the\nprobability of observations can be done exactly and efficiently. The approach\ncan also be used to learn deep representations of documents that are\ncompetitive to those learned by the alternative topic modeling approaches.\nFinally, we describe how the approach can be combined with a regular neural\nnetwork N-gram model and substantially improve its performance, by making its\nlearned representation sensitive to the larger, document-specific context. \n\n"}
{"id": "1603.06263", "contents": "Title: Data-Driven Robust Taxi Dispatch under Demand Uncertainties Abstract: In modern taxi networks, large amounts of taxi occupancy status and location\ndata are collected from networked in-vehicle sensors in real-time. They provide\nknowledge of system models on passenger demand and mobility patterns for\nefficient taxi dispatch and coordination strategies. Such approaches face new\nchallenges: how to deal with uncertainties of predicted customer demand while\nfulfilling the system's performance requirements, including minimizing taxis'\ntotal idle mileage and maintaining service fairness across the whole city; how\nto formulate a computationally tractable problem. To address this problem, we\ndevelop a data-driven robust taxi dispatch framework to consider\nspatial-temporally correlated demand uncertainties. The robust vehicle dispatch\nproblem we formulate is concave in the uncertain demand and convex in the\ndecision variables. Uncertainty sets of random demand vectors are constructed\nfrom data based on theories in hypothesis testing, and provide a desired\nprobabilistic guarantee level for the performance of robust taxi dispatch\nsolutions. We prove equivalent computationally tractable forms of the robust\ndispatch problem using the minimax theorem and strong duality. Evaluations on\nfour years of taxi trip data for New York City show that by selecting a\nprobabilistic guarantee level at 75%, the average demand-supply ratio error is\nreduced by 31.7%, and the average total idle driving distance is reduced by\n10.13% or about 20 million miles annually, compared with non-robust dispatch\nsolutions. \n\n"}
{"id": "1603.06872", "contents": "Title: Building Model Identification during Regular Operation - Empirical\n  Results and Challenges Abstract: The inter-temporal consumption flexibility of commercial buildings can be\nharnessed to improve the energy efficiency of buildings, or to provide\nancillary service to the power grid. To do so, a predictive model of the\nbuilding's thermal dynamics is required. In this paper, we identify a\nphysics-based model of a multi-purpose commercial building including its\nheating, ventilation and air conditioning system during regular operation. We\npresent our empirical results and show that large uncertainties in internal\nheat gains, due to occupancy and equipment, present several challenges in\nutilizing the building model for long-term prediction. In addition, we show\nthat by learning these uncertain loads online and dynamically updating the\nbuilding model, prediction accuracy is improved significantly. \n\n"}
{"id": "1603.07195", "contents": "Title: A Decentralized Quasi-Newton Method for Dual Formulations of Consensus\n  Optimization Abstract: This paper considers consensus optimization problems where each node of a\nnetwork has access to a different summand of an aggregate cost function. Nodes\ntry to minimize the aggregate cost function, while they exchange information\nonly with their neighbors. We modify the dual decomposition method to\nincorporate a curvature correction inspired by the\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dual\nD-BFGS method is a fully decentralized algorithm in which nodes approximate\ncurvature information of themselves and their neighbors through the\nsatisfaction of a secant condition. Dual D-BFGS is of interest in consensus\noptimization problems that are not well conditioned, making first order\ndecentralized methods ineffective, and in which second order information is not\nreadily available, making decentralized second order methods infeasible.\nAsynchronous implementation is discussed and convergence of D-BFGS is\nestablished formally for both synchronous and asynchronous implementations.\nPerformance advantages relative to alternative decentralized algorithms are\nshown numerically. \n\n"}
{"id": "1603.07235", "contents": "Title: Global-Local Face Upsampling Network Abstract: Face hallucination, which is the task of generating a high-resolution face\nimage from a low-resolution input image, is a well-studied problem that is\nuseful in widespread application areas. Face hallucination is particularly\nchallenging when the input face resolution is very low (e.g., 10 x 12 pixels)\nand/or the image is captured in an uncontrolled setting with large pose and\nillumination variations. In this paper, we revisit the algorithm introduced in\n[1] and present a deep interpretation of this framework that achieves\nstate-of-the-art under such challenging scenarios. In our deep network\narchitecture the global and local constraints that define a face can be\nefficiently modeled and learned end-to-end using training data. Conceptually\nour network design can be partitioned into two sub-networks: the first one\nimplements the holistic face reconstruction according to global constraints,\nand the second one enhances face-specific details and enforces local patch\nstatistics. We optimize the deep network using a new loss function for\nsuper-resolution that combines reconstruction error with a learned face quality\nmeasure in adversarial setting, producing improved visual results. We conduct\nextensive experiments in both controlled and uncontrolled setups and show that\nour algorithm improves the state of the art both numerically and visually. \n\n"}
{"id": "1603.08129", "contents": "Title: Robust transport over networks Abstract: We consider transport over a strongly connected, directed graph. The\nscheduling amounts to selecting transition probabilities for a discrete-time\nMarkov evolution which is designed to be consistent with certain initial and\nfinal marginals. The random evolution is selected to be closest to a prior\nmeasure on paths in the relative entropy sense, i.e., a Schroedinger bridge\nbetween the two marginals. This is an atypical stochastic control problem where\nthe control consists in suitably modifying the transition mechanism. The prior\ncan incorporate cost of traversing edges or allocate equal probability to all\npaths of equal length connecting any two given nodes, i.e., a uniform measure\non paths. This latter choice relies on the so-called Ruelle-Bowen random walk\nand gives rise to a scheduling that tends to utilize all paths as uniformly as\nthe topology allows. Thus, when the Ruelle-Bowen law is taken as prior, the\ntransportation plan tends to lessen congestion and ensure a level of\nrobustness. We show that the Ruelle-Bowen law is itself a Schroedinger bridge\nalbeit with a prior that is not a probability measure. The paradigm of\nSchroedinger bridges as a mechanism for scheduling transport on networks can be\nadapted to graphs that are not strongly connected as well as to weighted\ngraphs. The latter leads to transportation plans that effect a compromise\nbetween robustness and transportation cost. \n\n"}
{"id": "1603.08355", "contents": "Title: Multi-Sensor Control for Multi-Target Tracking Using Cauchy-Schwarz\n  Divergence Abstract: The paper addresses the problem of multi-sensor control for multi-target\ntracking via labelled random finite sets (RFS) in the sensor network systems.\nBased on an information theoretic divergence measure, namely Cauchy-Schwarz\n(CS) divergence which admits a closed form solution for GLMB densities, we\npropose two novel multi-sensor control approaches in the framework of\ngeneralized Covariance Intersection (GCI). The first joint decision making\n(JDM) method is optimal and can achieve overall good performance, while the\nsecond independent decision making (IDM) method is suboptimal as a fast\nrealization with smaller amount of computations. Simulation in challenging\nsituation is presented to verify the effectiveness of the two proposed\napproaches. \n\n"}
{"id": "1603.09672", "contents": "Title: Lyapunov stability of a rigid body with two frictional contacts Abstract: Lyapunov stability of a mechanical system means that the dynamic response\nstays bounded in an arbitrarily small neighborhood of a static equilibrium\nconfiguration under small perturbations in positions and velocities. This type\nof stability is highly desired in robotic applications that involve multiple\nunilateral contacts. Nevertheless, Lyapunov stability analysis of such systems\nis extremely difficult, because even small perturbations may result in hybrid\ndynamics where the solution involves many nonsmooth transitions between\ndifferent contact states. This paper concerns with Lyapunov stability analysis\nof a planar rigid body with two frictional unilateral contacts under inelastic\nimpacts, for a general class of equilibrium configurations under a constant\nexternal load. The hybrid dynamics of the system under contact transitions and\nimpacts is formulated, and a \\Poincare map at two-contact states is introduced.\nUsing invariance relations, this \\Poincare map is reduced into two\nsemi-analytic scalar functions that entirely encode the dynamic behavior of\nsolutions under any small initial perturbation. These two functions enable\ndetermination of Lyapunov stability or instability for almost any equilibrium\nstate. The results are demonstrated via simulation examples and by plotting\nstability and instability regions in two-dimensional parameter spaces that\ndescribe the contact geometry and external load. \n\n"}
{"id": "1604.00082", "contents": "Title: A track-before-detect labelled multi-Bernoulli particle filter with\n  label switching Abstract: This paper presents a multitarget tracking particle filter (PF) for general\ntrack-before-detect measurement models. The PF is presented in the random\nfinite set framework and uses a labelled multi-Bernoulli approximation. We also\npresent a label switching improvement algorithm based on Markov chain Monte\nCarlo that is expected to increase filter performance if targets get in close\nproximity for a sufficiently long time. The PF is tested in two challenging\nnumerical examples. \n\n"}
{"id": "1604.01802", "contents": "Title: Learning to Track at 100 FPS with Deep Regression Networks Abstract: Machine learning techniques are often used in computer vision due to their\nability to leverage large amounts of training data to improve performance.\nUnfortunately, most generic object trackers are still trained from scratch\nonline and do not benefit from the large number of videos that are readily\navailable for offline training. We propose a method for offline training of\nneural networks that can track novel objects at test-time at 100 fps. Our\ntracker is significantly faster than previous methods that use neural networks\nfor tracking, which are typically very slow to run and not practical for\nreal-time applications. Our tracker uses a simple feed-forward network with no\nonline training required. The tracker learns a generic relationship between\nobject motion and appearance and can be used to track novel objects that do not\nappear in the training set. We test our network on a standard tracking\nbenchmark to demonstrate our tracker's state-of-the-art performance. Further,\nour performance improves as we add more videos to our offline training set. To\nthe best of our knowledge, our tracker is the first neural-network tracker that\nlearns to track generic objects at 100 fps. \n\n"}
{"id": "1604.02216", "contents": "Title: A Primal-Dual Type Algorithm with the $O(1/t)$ Convergence Rate for\n  Large Scale Constrained Convex Programs Abstract: This paper considers large scale constrained convex programs, which are\nusually not solvable by interior point methods or other Newton-type methods due\nto the prohibitive computation and storage complexity for Hessians and matrix\ninversions. Instead, large scale constrained convex programs are often solved\nby gradient based methods or decomposition based methods. The conventional\nprimal-dual subgradient method, aka, Arrow-Hurwicz-Uzawa subgradient method, is\na low complexity algorithm with the $O(1/\\sqrt{t})$ convergence rate, where $t$\nis the number of iterations. If the objective and constraint functions are\nseparable, the Lagrangian dual type method can decompose a large scale convex\nprogram into multiple parallel small scale convex programs. The classical dual\ngradient algorithm is an example of Lagrangian dual type methods and has\nconvergence rate $O(1/\\sqrt{t})$. Recently, a new Lagrangian dual type\nalgorithm with faster $O(1/t)$ convergence is proposed in Yu and Neely (2015).\nHowever, if the objective or constraint functions are not separable, each\niteration of the Lagrangian dual type method in Yu and Neely (2015) requires to\nsolve a large scale unconstrained convex program, which can have huge\ncomplexity. This paper proposes a new primal-dual type algorithm, which only\ninvolves simple gradient updates at each iteration and has the $O(1/t)$\nconvergence rate. \n\n"}
{"id": "1604.03182", "contents": "Title: Cascade and locally dissipative realizations of linear quantum systems\n  for pure Gaussian state covariance assignment Abstract: This paper presents two realizations of linear quantum systems for covariance\nassignment corresponding to pure Gaussian states. The first one is called a\ncascade realization; given any covariance matrix corresponding to a pure\nGaussian state, we can construct a cascaded quantum system generating that\nstate. The second one is called a locally dissipative realization; given a\ncovariance matrix corresponding to a pure Gaussian state, if it satisfies\ncertain conditions, we can construct a linear quantum system that has only\nlocal interactions with its environment and achieves the assigned covariance\nmatrix. Both realizations are illustrated by examples from quantum optics. \n\n"}
{"id": "1604.03199", "contents": "Title: From sample to knowledge: Towards an integrated approach for\n  neuroscience discovery Abstract: Imaging methods used in modern neuroscience experiments are quickly producing\nlarge amounts of data capable of providing increasing amounts of knowledge\nabout neuroanatomy and function. A great deal of information in these datasets\nis relatively unexplored and untapped. One of the bottlenecks in knowledge\nextraction is that often there is no feedback loop between the knowledge\nproduced (e.g., graph, density estimate, or other statistic) and the earlier\nstages of the pipeline (e.g., acquisition). We thus advocate for the\ndevelopment of sample-to-knowledge discovery pipelines that one can use to\noptimize acquisition and processing steps with a particular end goal (i.e.,\npiece of knowledge) in mind. We therefore propose that optimization takes place\nnot just within each processing stage but also between adjacent (and\nnon-adjacent) steps of the pipeline. Furthermore, we explore the existing\ncategories of knowledge representation and models to motivate the types of\nexperiments and analysis needed to achieve the ultimate goal. To illustrate\nthis approach, we provide an experimental paradigm to answer questions about\nlarge-scale synaptic distributions through a multimodal approach combining\nX-ray microtomography and electron microscopy. \n\n"}
{"id": "1604.04144", "contents": "Title: Self-taught learning of a deep invariant representation for visual\n  tracking via temporal slowness principle Abstract: Visual representation is crucial for a visual tracking method's performances.\nConventionally, visual representations adopted in visual tracking rely on\nhand-crafted computer vision descriptors. These descriptors were developed\ngenerically without considering tracking-specific information. In this paper,\nwe propose to learn complex-valued invariant representations from tracked\nsequential image patches, via strong temporal slowness constraint and stacked\nconvolutional autoencoders. The deep slow local representations are learned\noffline on unlabeled data and transferred to the observational model of our\nproposed tracker. The proposed observational model retains old training samples\nto alleviate drift, and collect negative samples which are coherent with\ntarget's motion pattern for better discriminative tracking. With the learned\nrepresentation and online training samples, a logistic regression classifier is\nadopted to distinguish target from background, and retrained online to adapt to\nappearance changes. Subsequently, the observational model is integrated into a\nparticle filter framework to peform visual tracking. Experimental results on\nvarious challenging benchmark sequences demonstrate that the proposed tracker\nperforms favourably against several state-of-the-art trackers. \n\n"}
{"id": "1604.06823", "contents": "Title: Completely positive and completely positive semidefinite tensor\n  relaxations for polynomial optimization Abstract: Completely positive (CP) tensors, which correspond to a generalization of CP\nmatrices, allow to reformulate or approximate a general polynomial optimization\nproblem (POP) with a conic optimization problem over the cone of CP tensors.\nSimilarly, completely positive semidefinite (CPSD) tensors, which correspond to\na generalization of positive semidefinite (PSD) matrices, can be used to\napproximate general POPs with a conic optimization problem over the cone of\nCPSD tensors. In this paper, we study CP and CPSD tensor relaxations for\ngeneral POPs and compare them with the bounds obtained via a Lagrangian\nrelaxation of the POPs. This shows that existing results in this direction for\nquadratic POPs extend to general POPs. Also, we provide some tractable\napproximation strategies for CP and CPSD tensor relaxations. These\napproximation strategies show that, with a similar computational effort, bounds\nobtained from them for general POPs can be tighter than bounds for these\nproblems obtained by reformulating the POP as a quadratic POP, which\nsubsequently can be approximated using CP and PSD matrices. To illustrate our\nresults, we numerically compare the bounds obtained from these relaxation\napproaches on small scale fourth-order degree POPs. \n\n"}
{"id": "1604.08382", "contents": "Title: Convolutional Neural Networks For Automatic State-Time Feature\n  Extraction in Reinforcement Learning Applied to Residential Load Control Abstract: Direct load control of a heterogeneous cluster of residential demand\nflexibility sources is a high-dimensional control problem with partial\nobservability. This work proposes a novel approach that uses a convolutional\nneural network to extract hidden state-time features to mitigate the curse of\npartial observability. More specific, a convolutional neural network is used as\na function approximator to estimate the state-action value function or\nQ-function in the supervised learning step of fitted Q-iteration. The approach\nis evaluated in a qualitative simulation, comprising a cluster of\nthermostatically controlled loads that only share their air temperature, whilst\ntheir envelope temperature remains hidden. The simulation results show that the\npresented approach is able to capture the underlying hidden features and\nsuccessfully reduce the electricity cost the cluster. \n\n"}
{"id": "1605.00267", "contents": "Title: Distributed Algorithms for Aggregative Games on Graphs Abstract: We consider a class of Nash games, termed as aggregative games, being played\nover a networked system. In an aggregative game, a player's objective is a\nfunction of the aggregate of all the players' decisions. Every player maintains\nan estimate of this aggregate, and the players exchange this information with\ntheir local neighbors over a connected network. We study distributed\nsynchronous and asynchronous algorithms for information exchange and\nequilibrium computation over such a network. Under standard conditions, we\nestablish the almost-sure convergence of the obtained sequences to the\nequilibrium point. We also consider extensions of our schemes to aggregative\ngames where the players' objectives are coupled through a more general form of\naggregate function. Finally, we present numerical results that demonstrate the\nperformance of the proposed schemes. \n\n"}
{"id": "1605.00322", "contents": "Title: Adaptive Modulation in Network-coded Two-way Relay Channel: A\n  Supermodular Game Approach Abstract: We study the adaptive modulation (AM) problem in a network-coded two-way\nrelay channel (NC-TWRC), where each of the two users controls its own bit rate\nin the $m$-ary quadrature amplitude modulation ($m$-QAM) to minimize the\ntransmission error rate and enhance the spectral efficiency. We show that there\nexists a strategic complementarity, one user tends to transmit while the other\ndecides to do so in order to enhance the overall spectral efficiency, which is\nbeyond the scope of the conventional single-agent AM scheduling method. We\npropose a two-player game model parameterized by the signal-to-noise ratios\n(SNRs) of two user-to-user channels and prove that it is a supermodular game\nwhere there always exist the extremal pure strategy Nash equilibria (PSNEs),\nthe largest and smallest PSNEs. We show by simulation results that the extremal\nPSNEs incur a similar bit error rate (BER) as the conventional single-agent AM\nscheme, but significantly improve the spectral efficiency in the NC-TWRC\nsystem. The study also reveals the Pareto order of the extremal PSNEs: The\nlargest and smallest PSNEs are Pareto worst and best PSNEs, respectively.\nFinally, we derive the sufficient conditions for the extremal PSNEs to be\nsymmetric and monotonic in channel SNRs. We also discuss how to utilize the\nsymmetry and monotonicity to relieve the complexity in the PSNE learning\nprocess. \n\n"}
{"id": "1605.00451", "contents": "Title: Towards a characterization of the uncertainty curve for graphs Abstract: Signal processing on graphs is a recent research domain that aims at\ngeneralizing classical tools in signal processing, in order to analyze signals\nevolving on complex domains. Such domains are represented by graphs, for which\none can compute a particular matrix, called the normalized Laplacian. It was\nshown that the eigenvalues of this Laplacian correspond to the frequencies of\nthe Fourier domain in classical signal processing. Therefore, the frequency\ndomain is not the same for every support graph. A consequence of this is that\nthere is no non-trivial generalization of Heisenberg's uncertainty principle,\nthat states that a signal cannot be fully localized both in the time domain and\nin the frequency domain. A way to generalize this principle, introduced by\nAgaskar and Lu, consists in determining a curve that represents a lower bound\non the compromise between precision in the graph domain and precision in the\nspectral domain. The aim of this paper is to propose a characterization of the\nsignals achieving this curve, for a larger class of graphs than the one studied\nby Agaskar and Lu. \n\n"}
{"id": "1605.00690", "contents": "Title: Optimal Remote Estimation Over Use-Dependent Packet-Drop Channels -\n  Extended Version Abstract: Consider a discrete-time remote estimation system formed by an encoder, a\ntransmission policy, a channel, and a remote estimator. The encoder assesses a\nrandom process that the remote estimator seeks to estimate based on information\nsent to it by the encoder via the channel. The channel is affected by Bernoulli\ndrops. The instantaneous probability of a drop is governed by a finite state\nmachine (FSM). The state of the FSM is denoted as the channel state. At each\ntime step, the encoder decides whether to attempt a transmission through the\npacket-drop link. The sequence of encoder decisions is the input to the FSM.\nThis paper seeks to design an encoder, transmission policy and remote estimator\nthat minimize a finite-horizon mean squared error cost. We present two\nstructural results. The first result in which we assume that the process to be\nestimated is white and Gaussian, we show that there is an optimal transmission\npolicy governed by a threshold on the estimation error. The second result\ncharacterizes optimal symmetric transmission policies for the case when the\nmeasured process is the state of a scalar linear time-invariant plant driven by\nwhite Gaussian noise. Use-dependent packet-drop channels can be used to\nquantify the effect of transmission on channel quality when the encoder is\npowered by energy harvesting. An application to a mixed initiative system in\nwhich a human operator performs visual search tasks is also presented. \n\n"}
{"id": "1605.02268", "contents": "Title: Rate-Distortion Bounds on Bayes Risk in Supervised Learning Abstract: We present an information-theoretic framework for bounding the number of\nlabeled samples needed to train a classifier in a parametric Bayesian setting.\nWe derive bounds on the average $L_p$ distance between the learned classifier\nand the true maximum a posteriori classifier, which are well-established\nsurrogates for the excess classification error due to imperfect learning. We\nprovide lower and upper bounds on the rate-distortion function, using $L_p$\nloss as the distortion measure, of a maximum a priori classifier in terms of\nthe differential entropy of the posterior distribution and a quantity called\nthe interpolation dimension, which characterizes the complexity of the\nparametric distribution family. In addition to expressing the information\ncontent of a classifier in terms of lossy compression, the rate-distortion\nfunction also expresses the minimum number of bits a learning machine needs to\nextract from training data to learn a classifier to within a specified $L_p$\ntolerance. We use results from universal source coding to express the\ninformation content in the training data in terms of the Fisher information of\nthe parametric family and the number of training samples available. The result\nis a framework for computing lower bounds on the Bayes $L_p$ risk. This\nframework complements the well-known probably approximately correct (PAC)\nframework, which provides minimax risk bounds involving the Vapnik-Chervonenkis\ndimension or Rademacher complexity. Whereas the PAC framework provides upper\nbounds the risk for the worst-case data distribution, the proposed\nrate-distortion framework lower bounds the risk averaged over the data\ndistribution. We evaluate the bounds for a variety of data models, including\ncategorical, multinomial, and Gaussian models. In each case the bounds are\nprovably tight orderwise, and in two cases we prove that the bounds are tight\nup to multiplicative constants. \n\n"}
{"id": "1605.02408", "contents": "Title: Structured Nonconvex and Nonsmooth Optimization: Algorithms and\n  Iteration Complexity Analysis Abstract: Nonconvex and nonsmooth optimization problems are frequently encountered in\nmuch of statistics, business, science and engineering, but they are not yet\nwidely recognized as a technology in the sense of scalability. A reason for\nthis relatively low degree of popularity is the lack of a well developed system\nof theory and algorithms to support the applications, as is the case for its\nconvex counterpart. This paper aims to take one step in the direction of\ndisciplined nonconvex and nonsmooth optimization. In particular, we consider in\nthis paper some constrained nonconvex optimization models in block decision\nvariables, with or without coupled affine constraints. In the case of without\ncoupled constraints, we show a sublinear rate of convergence to an\n$\\epsilon$-stationary solution in the form of variational inequality for a\ngeneralized conditional gradient method, where the convergence rate is shown to\nbe dependent on the H\\\"olderian continuity of the gradient of the smooth part\nof the objective. For the model with coupled affine constraints, we introduce\ncorresponding $\\epsilon$-stationarity conditions, and apply two proximal-type\nvariants of the ADMM to solve such a model, assuming the proximal ADMM updates\ncan be implemented for all the block variables except for the last block, for\nwhich either a gradient step or a majorization-minimization step is\nimplemented. We show an iteration complexity bound of $O(1/\\epsilon^2)$ to\nreach an $\\epsilon$-stationary solution for both algorithms. Moreover, we show\nthat the same iteration complexity of a proximal BCD method follows\nimmediately. Numerical results are provided to illustrate the efficacy of the\nproposed algorithms for tensor robust PCA. \n\n"}
{"id": "1605.03651", "contents": "Title: Minimum-Rank Dynamic Output Consensus Design for Heterogeneous Nonlinear\n  Multi-Agent Systems Abstract: In this paper, we propose a new and systematic design framework for output\nconsensus in heterogeneous Multi-Input Multi-Output (MIMO) general nonlinear\nMulti-Agent Systems (MASs) subjected to directed communication topology. First,\nthe input-output feedback linearization method is utilized assuming that the\ninternal dynamics is Input-to-State Stable (ISS) to obtain linearized\nsubsystems of agents. Consequently, we propose local dynamic controllers for\nagents such that the linearized subsystems have an identical closed-loop\ndynamics which has a single pole at the origin whereas other poles are on the\nopen left half complex plane. This allows us to deal with distinct agents\nhaving arbitrarily vector relative degrees and to derive rank-$1$ cooperative\ncontrol inputs for those homogeneous linearized dynamics which results in a\nminimum rank distributed dynamic consensus controller for the initial nonlinear\nMAS. Moreover, we prove that the coupling strength in the consensus protocol\ncan be arbitrarily small but positive and hence our consensus design is\nnon-conservative. Next, our design approach is further strengthened by tackling\nthe problem of randomly switching communication topologies among agents where\nwe relax the assumption on the balance of each switched graph and derive a\ndistributed rank-$1$ dynamic consensus controller. Lastly, a numerical example\nis introduced to illustrate the effectiveness of our proposed framework. \n\n"}
{"id": "1605.04070", "contents": "Title: A Reinforcement Learning System to Encourage Physical Activity in\n  Diabetes Patients Abstract: Regular physical activity is known to be beneficial to people suffering from\ndiabetes type 2. Nevertheless, most such people are sedentary. Smartphones\ncreate new possibilities for helping people to adhere to their physical\nactivity goals, through continuous monitoring and communication, coupled with\npersonalized feedback.\n  We provided 27 sedentary diabetes type 2 patients with a smartphone-based\npedometer and a personal plan for physical activity. Patients were sent SMS\nmessages to encourage physical activity between once a day and once per week.\nMessages were personalized through a Reinforcement Learning (RL) algorithm\nwhich optimized messages to improve each participant's compliance with the\nactivity regimen. The RL algorithm was compared to a static policy for sending\nmessages and to weekly reminders.\n  Our results show that participants who received messages generated by the RL\nalgorithm increased the amount of activity and pace of walking, while the\ncontrol group patients did not. Patients assigned to the RL algorithm group\nexperienced a superior reduction in blood glucose levels (HbA1c) compared to\ncontrol policies, and longer participation caused greater reductions in blood\nglucose levels. The learning algorithm improved gradually in predicting which\nmessages would lead participants to exercise.\n  Our results suggest that a mobile phone application coupled with a learning\nalgorithm can improve adherence to exercise in diabetic patients. As a learning\nalgorithm is automated, and delivers personalized messages, it could be used in\nlarge populations of diabetic patients to improve health and glycemic control.\nOur results can be expanded to other areas where computer-led health coaching\nof humans may have a positive impact. \n\n"}
{"id": "1605.04131", "contents": "Title: Barzilai-Borwein Step Size for Stochastic Gradient Descent Abstract: One of the major issues in stochastic gradient descent (SGD) methods is how\nto choose an appropriate step size while running the algorithm. Since the\ntraditional line search technique does not apply for stochastic optimization\nalgorithms, the common practice in SGD is either to use a diminishing step\nsize, or to tune a fixed step size by hand, which can be time consuming in\npractice. In this paper, we propose to use the Barzilai-Borwein (BB) method to\nautomatically compute step sizes for SGD and its variant: stochastic variance\nreduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and\nSVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective\nfunctions. As a by-product, we prove the linear convergence result of SVRG with\nOption I proposed in [10], whose convergence result is missing in the\nliterature. Numerical experiments on standard data sets show that the\nperformance of SGD-BB and SVRG-BB is comparable to and sometimes even better\nthan SGD and SVRG with best-tuned step sizes, and is superior to some advanced\nSGD variants. \n\n"}
{"id": "1605.06394", "contents": "Title: Bayesian Hyperparameter Optimization for Ensemble Learning Abstract: In this paper, we bridge the gap between hyperparameter optimization and\nensemble learning by performing Bayesian optimization of an ensemble with\nregards to its hyperparameters. Our method consists in building a fixed-size\nensemble, optimizing the configuration of one classifier of the ensemble at\neach iteration of the hyperparameter optimization algorithm, taking into\nconsideration the interaction with the other models when evaluating potential\nperformances. We also consider the case where the ensemble is to be\nreconstructed at the end of the hyperparameter optimization phase, through a\ngreedy selection over the pool of models generated during the optimization. We\nstudy the performance of our proposed method on three different hyperparameter\nspaces, showing that our approach is better than both the best single model and\na greedy ensemble construction over the models produced by a standard Bayesian\noptimization. \n\n"}
{"id": "1605.07221", "contents": "Title: Global Optimality of Local Search for Low Rank Matrix Recovery Abstract: We show that there are no spurious local minima in the non-convex factorized\nparametrization of low-rank matrix recovery from incoherent linear\nmeasurements. With noisy measurements we show all local minima are very close\nto a global optimum. Together with a curvature bound at saddle points, this\nyields a polynomial time global convergence guarantee for stochastic gradient\ndescent {\\em from random initialization}. \n\n"}
{"id": "1605.07784", "contents": "Title: Fast Algorithms for Robust PCA via Gradient Descent Abstract: We consider the problem of Robust PCA in the fully and partially observed\nsettings. Without corruptions, this is the well-known matrix completion\nproblem. From a statistical standpoint this problem has been recently\nwell-studied, and conditions on when recovery is possible (how many\nobservations do we need, how many corruptions can we tolerate) via\npolynomial-time algorithms is by now understood. This paper presents and\nanalyzes a non-convex optimization approach that greatly reduces the\ncomputational complexity of the above problems, compared to the best available\nalgorithms. In particular, in the fully observed case, with $r$ denoting rank\nand $d$ dimension, we reduce the complexity from\n$\\mathcal{O}(r^2d^2\\log(1/\\varepsilon))$ to\n$\\mathcal{O}(rd^2\\log(1/\\varepsilon))$ -- a big savings when the rank is big.\nFor the partially observed case, we show the complexity of our algorithm is no\nmore than $\\mathcal{O}(r^4d \\log d \\log(1/\\varepsilon))$. Not only is this the\nbest-known run-time for a provable algorithm under partial observation, but in\nthe setting where $r$ is small compared to $d$, it also allows for\nnear-linear-in-$d$ run-time that can be exploited in the fully-observed case as\nwell, by simply running our algorithm on a subset of the observations. \n\n"}
{"id": "1605.09721", "contents": "Title: CYCLADES: Conflict-free Asynchronous Machine Learning Abstract: We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms. \n\n"}
{"id": "1606.01947", "contents": "Title: Minimum-Information LQG Control - Part II: Retentive Controllers Abstract: Retentive (memory-utilizing) sensing-acting agents may operate under\nlimitations on the communication between their sensing, memory and acting\ncomponents, requiring them to trade off the external cost that they incur with\nthe capacity of their communication channels. In this paper we formulate this\nproblem as a sequential rate-distortion problem of minimizing the rate of\ninformation required for the controller's operation under a constraint on its\nexternal cost. We reduce this bounded retentive control problem to the\nmemoryless one, studied in Part I of this work, by viewing the memory reader as\none more sensor and the memory writer as one more actuator. We further\ninvestigate the structure of the resulting optimal solution and demonstrate its\ninteresting phenomenology. \n\n"}
{"id": "1606.02228", "contents": "Title: Systematic evaluation of CNN advances on the ImageNet Abstract: The paper systematically studies the impact of a range of recent advances in\nCNN architectures and learning methods on the object categorization (ILSVRC)\nproblem. The evalution tests the influence of the following choices of the\narchitecture: non-linearity (ReLU, ELU, maxout, compatibility with batch\nnormalization), pooling variants (stochastic, max, average, mixed), network\nwidth, classifier design (convolutional, fully-connected, SPP), image\npre-processing, and of learning parameters: learning rate, batch size,\ncleanliness of the data, etc.\n  The performance gains of the proposed modifications are first tested\nindividually and then in combination. The sum of individual gains is bigger\nthan the observed improvement when all modifications are introduced, but the\n\"deficit\" is small suggesting independence of their benefits. We show that the\nuse of 128x128 pixel images is sufficient to make qualitative conclusions about\noptimal network structure that hold for the full size Caffe and VGG nets. The\nresults are obtained an order of magnitude faster than with the standard 224\npixel images. \n\n"}
{"id": "1606.02635", "contents": "Title: Feedback Scheduling for Energy-Efficient Real-Time Homogeneous\n  Multiprocessor Systems Abstract: Real-time scheduling algorithms proposed in the literature are often based on\nworst-case estimates of task parameters. The performance of an open-loop scheme\ncan be degraded significantly if there are uncertainties in task parameters,\nsuch as the execution times of the tasks. Therefore, to cope with such a\nsituation, a closed-loop scheme, where feedback is exploited to adjust the\nsystem parameters, can be applied. We propose an optimal control framework that\ntakes advantage of feeding back information of finished tasks to solve a\nreal-time multiprocessor scheduling problem with uncertainty in task execution\ntimes, with the objective of minimizing the total energy consumption.\nSpecifically, we propose a linear programming based algorithm to solve a\nworkload partitioning problem and adopt McNaughton's wrap around algorithm to\nfind the task execution order. The simulation results illustrate that our\nfeedback scheduling algorithm can save energy by as much as 40% compared to an\nopen-loop method for two processor models, i.e. a PowerPC 405LP and an XScale\nprocessor. \n\n"}
{"id": "1606.03199", "contents": "Title: Distributed stabilization control of rigid formations with prescribed\n  orientation Abstract: Most rigid formation controllers reported in the literature aim to only\nstabilize a rigid formation shape, while the formation orientation is not\ncontrolled. This paper studies the problem of controlling rigid formations with\nprescribed orientations in both 2-D and 3-D spaces. The proposed controllers\ninvolve the commonly-used gradient descent control for shape stabilization, and\nan additional term to control the directions of certain relative position\nvectors associated with certain chosen agents. In this control framework, we\nshow the minimal number of agents which should have knowledge of a global\ncoordinate system (2 agents for a 2-D rigid formation and 3 agents for a 3-D\nrigid formation), while all other agents do not require any global coordinate\nknowledge or any coordinate frame alignment to implement the proposed control.\nThe exponential convergence to the desired rigid shape and formation\norientation is also proved. Typical simulation examples are shown to support\nthe analysis and performance of the proposed formation controllers. \n\n"}
{"id": "1606.05168", "contents": "Title: Energy Conservation and Coupling Error Reduction in Non-Iterative\n  Co-Simulations Abstract: When simulators are energetically coupled in a co-simulation, residual\nenergies alter the total energy of the full coupled system. This distorts the\nsystem dynamics, lowers the quality of the results, and can lead to\ninstability. By using power bonds to realize simulator coupling, the\nEnergy-Conservation-based Co-Simulation method (ECCO) [Sadjina et al. 2016]\nexploits these concepts to define non-iterative global error estimation and\nadaptive step size control relying on coupling variable data alone. Following\nsimilar argumentation, the Nearly Energy Preserving Coupling Element (NEPCE)\n[Benedikt et al. 2013] uses corrections to the simulator inputs to\napproximately ensure energy conservation. Here, we discuss a modification to\nNEPCE for when direct feed-through is present in one of the coupled simulators.\nWe further demonstrate how accuracy and efficiency in non-iterative\nco-simulations are substantially enhanced when combining NEPCE with ECCO's\nadaptive step size controller. A quarter car model with linear and nonlinear\ndamping characteristics serves as a co-simulation benchmark, and we observe\nreductions of the coupling errors of up to 98% utilizing the concepts discussed\nhere. \n\n"}
{"id": "1606.05361", "contents": "Title: Impact of storage competition on energy markets Abstract: We study how storage, operating as a price maker within a market environment,\nmay be optimally operated over an extended period of time. The optimality\ncriterion may be the maximisation of the profit of the storage itself, where\nthis profit results from the exploitation of the differences in market clearing\nprices at different times. Alternatively it may be the minimisation of the cost\nof generation, or the maximisation of consumer surplus or social welfare. In\nall cases there is calculated for each successive time-step the cost function\nmeasuring the total impact of whatever action is taken by the storage. The\nsuccession of such cost functions provides the information for the storage to\ndetermine how to behave over time, forming the basis of the appropriate\noptimisation problem. Further, optimal decision making, even over a very long\nor indefinite time period, usually depends on a knowledge of costs over a\nrelatively short running time horizon -- for storage of electrical energy\ntypically of the order of a day or so.\n  We study particularly competition between multiple stores, where the\nobjective of each store is to maximise its own income given the activities of\nthe remainder. We show that, at the Cournot Nash equilibrium, multiple large\nstores collectively erode their own abilities to make profits: essentially each\nstore attempts to increase its own profit over time by overcompeting at the\nexpense of the remainder. We quantify this for linear price functions\n  We give examples throughout based on Great Britain spot-price market data. \n\n"}
{"id": "1606.05834", "contents": "Title: Convergence of Nonlinear Observers on R^n with a Riemannian Metric (Part\n  II) Abstract: In [1], it is established that a convergent observer with an infinite gain\nmargin can be designed for a given nonlinear system when a Riemannian metric\nshowing that the system is differentially detectable (i.e., the Lie derivative\nof the Riemannian metric along the system vector field is negative in the space\ntangent to the output function level sets) and the level sets of the output\nfunction are geodesically convex is available. In this paper, we propose\ntechniques for designing a Riemannian metric satisfying the first property in\nthe case where the system is strongly infinitesimally observable (i.e., each\ntime-varying linear system resulting from the linearization along a solution to\nthe system satisfies a uniform observability property) or where it is strongly\ndifferentially observable (i.e. the mapping state to output derivatives is an\ninjective immersion) or where it is Lagrangian. Also, we give results that are\ncomplementary to those in [1]. In particular, we provide a locally convergent\nobserver and make a link to the existence of a reduced order observer. Examples\nillustrating the results are presented. \n\n"}
{"id": "1606.06512", "contents": "Title: Graphical Models for Optimal Power Flow Abstract: Optimal power flow (OPF) is the central optimization problem in electric\npower grids. Although solved routinely in the course of power grid operations,\nit is known to be strongly NP-hard in general, and weakly NP-hard over tree\nnetworks. In this paper, we formulate the optimal power flow problem over tree\nnetworks as an inference problem over a tree-structured graphical model where\nthe nodal variables are low-dimensional vectors. We adapt the standard dynamic\nprogramming algorithm for inference over a tree-structured graphical model to\nthe OPF problem. Combining this with an interval discretization of the nodal\nvariables, we develop an approximation algorithm for the OPF problem. Further,\nwe use techniques from constraint programming (CP) to perform interval\ncomputations and adaptive bound propagation to obtain practically efficient\nalgorithms. Compared to previous algorithms that solve OPF with optimality\nguarantees using convex relaxations, our approach is able to work for arbitrary\ndistribution networks and handle mixed-integer optimization problems. Further,\nit can be implemented in a distributed message-passing fashion that is scalable\nand is suitable for \"smart grid\" applications like control of distributed\nenergy resources. We evaluate our technique numerically on several benchmark\nnetworks and show that practical OPF problems can be solved effectively using\nthis approach. \n\n"}
{"id": "1606.06709", "contents": "Title: Guaranteed Cost Model Predictive Control-based Driver Assistance System\n  for Vehicle Stabilization Under Tire Parameters Uncertainties Abstract: Road traffic crashes have been the leading cause of death among young people.\nMost of these accidents occur when the driver becomes distracted and a\nloss-of-control situation occurs. Steer-by-Wire systems were recently proposed\nas an alternative to mitigate such accidents. This technology enables the\ndecoupling of the front wheel steering angles from the driver hand wheel angle\nand, consequently, the measurement of road/tire friction limits and the\ndevelopment of novel control systems capable of ensuring vehicle stabilization\nand safety. However, vehicle safety boundaries are highly dependent on tire\ncharacteristics which vary significantly with temperature, wear and the tire\nmanufacturing process. Therefore, design of autonomous vehicle and driver\nassistance controllers cannot assume that these characteristics are constant or\nknown. Thus, this paper proposes a Guaranteed Cost Model Predictive Controller\nDriver Assistance System able to avoid front and rear tire saturation and to\ntrack the drivers intent up to the limits of handling for a vehicle with\nuncertain tire parameters. Simulation results show the performance of the\nproposed approach under time-varying uniformly distributed disturbances. \n\n"}
{"id": "1606.07276", "contents": "Title: Dual theory of transmission line outages Abstract: A new graph dual formalism is presented for the analysis of line outages in\nelectricity networks. The dual formalism is based on a consideration of the\nflows around closed cycles in the network. After some exposition of the theory\nis presented, a new formula for the computation of Line Outage Distribution\nFactors (LODFs) is derived, which is not only computationally faster than\nexisting methods, but also generalizes easily for multiple line outages and\narbitrary changes to line series reactance. In addition, the dual formalism\nprovides new physical insight for how the effects of line outages propagate\nthrough the network. For example, in a planar network a single line outage can\nbe shown to induce monotonically decreasing flow changes, which are\nmathematically equivalent to an electrostatic dipole field. \n\n"}
{"id": "1606.08061", "contents": "Title: Exact gradient updates in time independent of output size for the\n  spherical loss family Abstract: An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g.\n200,000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the $D \\times d$ output weight matrix and computing the gradient\nneeded for backpropagation to previous layers. While efficient handling of\nlarge sparse network inputs is trivial, the case of large sparse targets is\nnot, and has thus so far been sidestepped with approximate alternatives such as\nhierarchical softmax or sampling-based approximations during training. In this\nwork we develop an original algorithmic approach which, for a family of loss\nfunctions that includes squared error and spherical softmax, can compute the\nexact loss, gradient update for the output weights, and gradient for\nbackpropagation, all in $O(d^{2})$ per example instead of $O(Dd)$, remarkably\nwithout ever computing the D-dimensional output. The proposed algorithm yields\na speedup of up to $D/4d$ i.e. two orders of magnitude for typical sizes, for\nthat critical part of the computations that often dominates the training time\nin this kind of network architecture. \n\n"}
{"id": "1606.08939", "contents": "Title: Distributed Optimization Under Adversarial Nodes Abstract: We investigate the vulnerabilities of consensus-based distributed\noptimization protocols to nodes that deviate from the prescribed update rule\n(e.g., due to failures or adversarial attacks). We first characterize certain\nfundamental limitations on the performance of any distributed optimization\nalgorithm in the presence of adversaries. We then propose a resilient\ndistributed optimization algorithm that guarantees that the non-adversarial\nnodes converge to the convex hull of the minimizers of their local functions\nunder certain conditions on the graph topology, regardless of the actions of a\ncertain number of adversarial nodes. In particular, we provide sufficient\nconditions on the graph topology to tolerate a bounded number of adversaries in\nthe neighborhood of every non-adversarial node, and necessary and sufficient\nconditions to tolerate a globally bounded number of adversaries. For situations\nwhere there are up to F adversaries in the neighborhood of every node, we use\nthe concept of maximal F-local sets of graphs to provide lower bounds on the\ndistance-to-optimality of achievable solutions under any algorithm. We show\nthat finding the size of such sets is NP-hard. \n\n"}
{"id": "1606.09365", "contents": "Title: On the worst-case complexity of the gradient method with exact line\n  search for smooth strongly convex functions Abstract: We consider the gradient (or steepest) descent method with exact line search\napplied to a strongly convex function with Lipschitz continuous gradient. We\nestablish the exact worst-case rate of convergence of this scheme, and show\nthat this worst-case behavior is exhibited by a certain convex quadratic\nfunction. We also give the tight worst-case complexity bound for a noisy\nvariant of gradient descent method, where exact line-search is performed in a\nsearch direction that differs from negative gradient by at most a prescribed\nrelative tolerance.\n  The proofs are computer-assisted, and rely on the resolutions of semidefinite\nprogramming performance estimation problems as introduced in the paper [Y.\nDrori and M. Teboulle. Performance of first-order methods for smooth convex\nminimization: a novel approach. Mathematical Programming, 145(1-2):451-482,\n2014]. \n\n"}
{"id": "1607.00036", "contents": "Title: Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes Abstract: We extend neural Turing machine (NTM) model into a dynamic neural Turing\nmachine (D-NTM) by introducing a trainable memory addressing scheme. This\naddressing scheme maintains for each memory cell two separate vectors, content\nand address vectors. This allows the D-NTM to learn a wide variety of\nlocation-based addressing strategies including both linear and nonlinear ones.\nWe implement the D-NTM with both continuous, differentiable and discrete,\nnon-differentiable read/write mechanisms. We investigate the mechanisms and\neffects of learning to read and write into a memory through experiments on\nFacebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is\nevaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM\nbaselines. We have done extensive analysis of our model and different\nvariations of NTM on bAbI task. We also provide further experimental results on\nsequential pMNIST, Stanford Natural Language Inference, associative recall and\ncopy tasks. \n\n"}
{"id": "1607.02480", "contents": "Title: Real-Time Anomaly Detection for Streaming Analytics Abstract: Much of the worlds data is streaming, time-series data, where anomalies give\nsignificant information in critical situations. Yet detecting anomalies in\nstreaming data is a difficult task, requiring detectors to process data in\nreal-time, and learn while simultaneously making predictions. We present a\nnovel anomaly detection technique based on an on-line sequence memory algorithm\ncalled Hierarchical Temporal Memory (HTM). We show results from a live\napplication that detects anomalies in financial metrics in real-time. We also\ntest the algorithm on NAB, a published benchmark for real-time anomaly\ndetection, where our algorithm achieves best-in-class results. \n\n"}
{"id": "1607.03140", "contents": "Title: Privacy-Enhanced Architecture for Occupancy-based HVAC Control Abstract: Large-scale sensing and actuation infrastructures have allowed buildings to\nachieve significant energy savings; at the same time, these technologies\nintroduce significant privacy risks that must be addressed. In this paper, we\npresent a framework for modeling the trade-off between improved control\nperformance and increased privacy risks due to occupancy sensing. More\nspecifically, we consider occupancy-based HVAC control as the control objective\nand the location traces of individual occupants as the private variables.\nPrevious studies have shown that individual location information can be\ninferred from occupancy measurements. To ensure privacy, we design an\narchitecture that distorts the occupancy data in order to hide individual\noccupant location information while maintaining HVAC performance. Using mutual\ninformation between the individual's location trace and the reported occupancy\nmeasurement as a privacy metric, we are able to optimally design a scheme to\nminimize privacy risk subject to a control performance guarantee. We evaluate\nour framework using real-world occupancy data: first, we verify that our\nprivacy metric accurately assesses the adversary's ability to infer private\nvariables from the distorted sensor measurements; then, we show that control\nperformance is maintained through simulations of building operations using\nthese distorted occupancy readings. \n\n"}
{"id": "1607.03815", "contents": "Title: Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than\n  $O(1/\\epsilon)$ Abstract: In this paper, we develop a novel {\\bf ho}moto{\\bf p}y {\\bf s}moothing (HOPS)\nalgorithm for solving a family of non-smooth problems that is composed of a\nnon-smooth term with an explicit max-structure and a smooth term or a simple\nnon-smooth term whose proximal mapping is easy to compute. The best known\niteration complexity for solving such non-smooth optimization problems is\n$O(1/\\epsilon)$ without any assumption on the strong convexity. In this work,\nwe will show that the proposed HOPS achieved a lower iteration complexity of\n$\\widetilde O(1/\\epsilon^{1-\\theta})$\\footnote{$\\widetilde O()$ suppresses a\nlogarithmic factor.} with $\\theta\\in(0,1]$ capturing the local sharpness of the\nobjective function around the optimal solutions. To the best of our knowledge,\nthis is the lowest iteration complexity achieved so far for the considered\nnon-smooth optimization problems without strong convexity assumption. The HOPS\nalgorithm employs Nesterov's smoothing technique and Nesterov's accelerated\ngradient method and runs in stages, which gradually decreases the smoothing\nparameter in a stage-wise manner until it yields a sufficiently good\napproximation of the original function. We show that HOPS enjoys a linear\nconvergence for many well-known non-smooth problems (e.g., empirical risk\nminimization with a piece-wise linear loss function and $\\ell_1$ norm\nregularizer, finding a point in a polyhedron, cone programming, etc).\nExperimental results verify the effectiveness of HOPS in comparison with\nNesterov's smoothing algorithm and the primal-dual style of first-order\nmethods. \n\n"}
{"id": "1607.04683", "contents": "Title: On the efficient representation and execution of deep acoustic models Abstract: In this paper we present a simple and computationally efficient quantization\nscheme that enables us to reduce the resolution of the parameters of a neural\nnetwork from 32-bit floating point values to 8-bit integer values. The proposed\nquantization scheme leads to significant memory savings and enables the use of\noptimized hardware instructions for integer arithmetic, thus significantly\nreducing the cost of inference. Finally, we propose a \"quantization aware\"\ntraining process that applies the proposed scheme during network training and\nfind that it allows us to recover most of the loss in accuracy introduced by\nquantization. We validate the proposed techniques by applying them to a long\nshort-term memory-based acoustic model on an open-ended large vocabulary speech\nrecognition task. \n\n"}
{"id": "1607.07019", "contents": "Title: Robust Control for Signal Temporal Logic Specifications using Average\n  Space Robustness Abstract: Control systems that satisfy temporal logic specifications have become\nincreasingly popular due to their applicability to robotic systems. Existing\ncontrol methods, however, are computationally demanding, especially when the\nproblem size becomes too large. In this paper, a robust and computationally\nefficient model predictive control framework for signal temporal logic\nspecifications is proposed. We introduce discrete average space robustness, a\nnovel quantitative semantic for signal temporal logic, that is directly\nincorporated into the cost function of the model predictive controller. The\noptimization problem entailed in this framework can be written as a convex\nquadratic program when no disjunctions are considered and results in a robust\nsatisfaction of the specification. Furthermore, we define the predicate\nrobustness degree as a new robustness notion. Simulations of a multi-agent\nsystem subject to complex specifications demonstrate the efficacy of the\nproposed method. \n\n"}
{"id": "1607.07405", "contents": "Title: gvnn: Neural Network Library for Geometric Computer Vision Abstract: We introduce gvnn, a neural network library in Torch aimed towards bridging\nthe gap between classic geometric computer vision and deep learning. Inspired\nby the recent success of Spatial Transformer Networks, we propose several new\nlayers which are often used as parametric transformations on the data in\ngeometric computer vision. These layers can be inserted within a neural network\nmuch in the spirit of the original spatial transformers and allow\nbackpropagation to enable end-to-end learning of a network involving any domain\nknowledge in geometric computer vision. This opens up applications in learning\ninvariance to 3D geometric transformation for place recognition, end-to-end\nvisual odometry, depth estimation and unsupervised learning through warping\nwith a parametric transformation for image reconstruction error. \n\n"}
{"id": "1607.07684", "contents": "Title: The Price of Anarchy in Auctions Abstract: This survey outlines a general and modular theory for proving approximation\nguarantees for equilibria of auctions in complex settings. This theory\ncomplements traditional economic techniques, which generally focus on exact and\noptimal solutions and are accordingly limited to relatively stylized settings.\n  We highlight three user-friendly analytical tools: smoothness-type\ninequalities, which immediately yield approximation guarantees for many auction\nformats of interest in the special case of complete information and\ndeterministic strategies; extension theorems, which extend such guarantees to\nrandomized strategies, no-regret learning outcomes, and incomplete-information\nsettings; and composition theorems, which extend such guarantees from simpler\nto more complex auctions. Combining these tools yields tight worst-case\napproximation guarantees for the equilibria of many widely-used auction\nformats. \n\n"}
{"id": "1608.00242", "contents": "Title: Input-Output Non-Linear Dynamical Systems applied to Physiological\n  Condition Monitoring Abstract: We present a non-linear dynamical system for modelling the effect of drug\ninfusions on the vital signs of patients admitted in Intensive Care Units\n(ICUs). More specifically we are interested in modelling the effect of a widely\nused anaesthetic drug (Propofol) on a patient's monitored depth of anaesthesia\nand haemodynamics. We compare our approach with one from the\nPharmacokinetics/Pharmacodynamics (PK/PD) literature and show that we can\nprovide significant improvements in performance without requiring the\nincorporation of expert physiological knowledge in our system. \n\n"}
{"id": "1608.00663", "contents": "Title: Simulation Optimization of Risk Measures with Adaptive Risk Levels Abstract: Optimizing risk measures such as Value-at-Risk (VaR) and Conditional\nValue-at-Risk (CVaR) of a general loss distribution is usually difficult,\nbecause 1) the loss function might lack structural properties such as convexity\nor differentiability since it is often generated via black-box simulation of a\nstochastic system; 2) evaluation of risk measures often requires rare-event\nsimulation, which is computationally expensive. In this paper, we study the\nextension of the recently proposed gradient-based adaptive stochastic search\n(GASS) to the optimization of risk measures VaR and CVaR. Instead of optimizing\nVaR or CVaR at the target risk level directly, we incorporate an adaptive\nupdating scheme on the risk level, by initializing the algorithm at a small\nrisk level and adaptively increasing it until the target risk level is achieved\nwhile the algorithm converges at the same time. This enables us to adaptively\nreduce the number of samples required to estimate the risk measure at each\niteration, and thus improving the overall efficiency of the algorithm. \n\n"}
{"id": "1608.03798", "contents": "Title: Exponential convergence under distributed averaging integral frequency\n  control Abstract: We investigate the performance and robustness of distributed averaging\nintegral controllers used in the optimal frequency regulation of power\nnetworks. We construct a strict Lyapunov function that allows us to quantify\nthe exponential convergence rate of the closed-loop system. As an application,\nwe study the stability of the system in the presence of disruptions to the\ncontrollers' communication network, and investigate how the convergence rate is\naffected by these disruptions. \n\n"}
{"id": "1608.04091", "contents": "Title: Functions with uniform sublevel sets and scalarization in linear spaces Abstract: Functions with uniform sublevel sets can represent orders, preference\nrelations or other binary relations and thus turn out to be a tool for\nscalarization that can be used in multicriteria optimization, decision theory,\nmathematical finance, production theory and operator theory. Sets which are not\nnecessarily convex can be separated by functions with uniform sublevel sets.\nThis report focuses on properties of real-valued and extended-real-valued\nfunctions with uniform sublevel sets which are defined on a linear space\nwithout assuming topological properties. The functions may be convex or\nsublinear. They can coincide with a Minkowski functional or with an order unit\nnorm on a subset of the space. The considered functionals are applied to the\nscalarization of vector optimization problems. These vector optimization\nproblems refer to arbitrary domination sets. The consideration of such sets is\nmotivated by their relationship to domination structures in decision making. \n\n"}
{"id": "1608.05269", "contents": "Title: Two-Timescale Stochastic Dispatch of Smart Distribution Grids Abstract: Smart distribution grids should efficiently integrate stochastic renewable\nresources while effecting voltage regulation. The design of energy management\nschemes is challenging, one of the reasons being that energy management is a\nmultistage problem where decisions are not all made at the same timescale and\nmust account for the variability during real-time operation. The joint dispatch\nof slow- and fast-timescale controls in a smart distribution grid is considered\nhere. The substation voltage, the energy exchanged with a main grid, and the\ngeneration schedules for small diesel generators have to be decided on a slow\ntimescale; whereas optimal photovoltaic inverter setpoints are found on a more\nfrequent basis. While inverter and looser voltage regulation limits are imposed\nat all times, tighter bus voltage constraints are enforced on the average or in\nprobability, thus enabling more efficient renewable integration. Upon\nreformulating the two-stage grid dispatch as a stochastic convex-concave\nproblem, two distribution-free schemes are put forth. An average dispatch\nalgorithm converges provably to the optimal two-stage decisions via a sequence\nof convex quadratic programs. Its non-convex probabilistic alternative entails\nsolving two slightly different convex problems and is numerically shown to\nconverge. Numerical tests on a real-world distribution feeder verify that both\nnovel data-driven schemes yield lower costs over competing alternatives. \n\n"}
{"id": "1608.06475", "contents": "Title: Stabilization Control for Linear Continuous-time Mean-field Systems Abstract: This paper investigates the stabilization and control problems for linear\ncontinuous-time mean-field systems (MFS). Under standard assumptions, necessary\nand sufficient conditions to stabilize the mean-field systems in the mean\nsquare sense are explored for the first time. It is shown that, under the\nassumption of exact detectability (exact observability), the mean-field system\nis stabilizable if and only if a coupled algebraic Riccati equation (ARE)\nadmits a unique positive semi-definite solution (positive definite solution),\nwhich coincides with the classical stabilization results for standard\ndeterministic systems and stochastic systems.\n  One of the key techniques in the paper is the obtained solution to the\nforward and backward stochastic differential equation (FBSDE) associated with\nthe maximum principle for an optimal control problem. Actually, with the\nanalytical FBSDE solution, a necessary and sufficient solvability condition of\nthe optimal control, under mild conditions, is derived. Accordingly, the\nstabilization condition is presented by defining an Lyaponuv functional via the\nsolution to the FBSDE and the optimal cost function.\n  It is worth of pointing out that the presented results are different from the\nprevious works for stabilization and also different from the works on optimal\ncontrol. \n\n"}
{"id": "1608.07690", "contents": "Title: A Boundary Tilting Persepective on the Phenomenon of Adversarial\n  Examples Abstract: Deep neural networks have been shown to suffer from a surprising weakness:\ntheir classification outputs can be changed by small, non-random perturbations\nof their inputs. This adversarial example phenomenon has been explained as\noriginating from deep networks being \"too linear\" (Goodfellow et al., 2014). We\nshow here that the linear explanation of adversarial examples presents a number\nof limitations: the formal argument is not convincing, linear classifiers do\nnot always suffer from the phenomenon, and when they do their adversarial\nexamples are different from the ones affecting deep networks.\n  We propose a new perspective on the phenomenon. We argue that adversarial\nexamples exist when the classification boundary lies close to the submanifold\nof sampled data, and present a mathematical analysis of this new perspective in\nthe linear case. We define the notion of adversarial strength and show that it\ncan be reduced to the deviation angle between the classifier considered and the\nnearest centroid classifier. Then, we show that the adversarial strength can be\nmade arbitrarily high independently of the classification performance due to a\nmechanism that we call boundary tilting. This result leads us to defining a new\ntaxonomy of adversarial examples. Finally, we show that the adversarial\nstrength observed in practice is directly dependent on the level of\nregularisation used and the strongest adversarial examples, symptomatic of\noverfitting, can be avoided by using a proper level of regularisation. \n\n"}
{"id": "1609.01458", "contents": "Title: On a Distributed Computation of Supervisors in Modular Supervisory\n  Control Abstract: In this paper, we discuss a supervisory control problem of modular\ndiscrete-event systems that allows for a distributed computation of\nsupervisors. We provide a characterization and an algorithm to compute the\nsupervisors. If the specification does not satisfy the properties, we make use\nof a relaxation of coordination control to compute a sublanguage of the\nspecification for which the supervisors can be computed in a distributed way. \n\n"}
{"id": "1609.02456", "contents": "Title: Voltage stabilization in DC microgrids: an approach based on\n  line-independent plug-and-play controllers Abstract: We consider the problem of stabilizing voltages in DC microGrids (mGs) given\nby the interconnection of Distributed Generation Units (DGUs), power lines and\nloads. We propose a decentralized control architecture where the primary\ncontroller of each DGU can be designed in a Plug-and-Play (PnP) fashion,\nallowing the seamless addition of new DGUs. Differently from several other\napproaches to primary control, local design is independent of the parameters of\npower lines. Moreover, differently from the PnP control scheme in [1], the\nplug-in of a DGU does not require to update controllers of neighboring DGUs.\nLocal control design is cast into a Linear Matrix Inequality (LMI) problem\nthat, if unfeasible, allows one to deny plug-in requests that might be\ndangerous for mG stability. The proof of closed-loop stability of voltages\nexploits structured Lyapunov functions, the LaSalle invariance theorem and\nproperties of graph Laplacians. Theoretical results are backed up by\nsimulations in PSCAD. \n\n"}
{"id": "1609.02963", "contents": "Title: Event-triggered second-moment stabilization of linear systems under\n  packet drops Abstract: This paper deals with the stabilization of linear systems with process noise\nunder packet drops between the sensor and the controller. Our aim is to ensure\nexponential convergence of the second moment of the plant state to a given\nbound in finite time. Motivated by considerations about the efficient use of\nthe available resources, we adopt an event-triggering approach to design the\ntransmission policy. In our design, the sensor's decision to transmit or not\nthe state to the controller is based on an online evaluation of the future\nsatisfaction of the control objective. The resulting event-triggering policy is\nhence specifically tailored to the control objective. We formally establish\nthat the proposed event-triggering policy meets the desired objective and\nquantify its efficiency by providing an upper bound on the fraction of expected\nnumber of transmissions in an infinite time interval. Simulations for scalar\nand vector systems illustrate the results. \n\n"}
{"id": "1609.04508", "contents": "Title: Column Networks for Collective Classification Abstract: Relational learning deals with data that are characterized by relational\nstructures. An important task is collective classification, which is to jointly\nclassify networked objects. While it holds a great promise to produce a better\naccuracy than non-collective classifiers, collective classification is\ncomputational challenging and has not leveraged on the recent breakthroughs of\ndeep learning. We present Column Network (CLN), a novel deep learning model for\ncollective classification in multi-relational domains. CLN has many desirable\ntheoretical properties: (i) it encodes multi-relations between any two\ninstances; (ii) it is deep and compact, allowing complex functions to be\napproximated at the network level with a small set of free parameters; (iii)\nlocal and relational features are learned simultaneously; (iv) long-range,\nhigher-order dependencies between instances are supported naturally; and (v)\ncrucially, learning and inference are efficient, linear in the size of the\nnetwork and the number of relations. We evaluate CLN on multiple real-world\napplications: (a) delay prediction in software projects, (b) PubMed Diabetes\npublication classification and (c) film genre classification. In all\napplications, CLN demonstrates a higher accuracy than state-of-the-art rivals. \n\n"}
{"id": "1609.04557", "contents": "Title: Structured Dropout for Weak Label and Multi-Instance Learning and Its\n  Application to Score-Informed Source Separation Abstract: Many success stories involving deep neural networks are instances of\nsupervised learning, where available labels power gradient-based learning\nmethods. Creating such labels, however, can be expensive and thus there is\nincreasing interest in weak labels which only provide coarse information, with\nuncertainty regarding time, location or value. Using such labels often leads to\nconsiderable challenges for the learning process. Current methods for\nweak-label training often employ standard supervised approaches that\nadditionally reassign or prune labels during the learning process. The\ninformation gain, however, is often limited as only the importance of labels\nwhere the network already yields reasonable results is boosted. We propose\ntreating weak-label training as an unsupervised problem and use the labels to\nguide the representation learning to induce structure. To this end, we propose\ntwo autoencoder extensions: class activity penalties and structured dropout. We\ndemonstrate the capabilities of our approach in the context of score-informed\nsource separation of music. \n\n"}
{"id": "1609.05011", "contents": "Title: Convex separation from convex optimization for large-scale problems Abstract: We present a scheme, based on Gilbert's algorithm for quadratic minimization\n[SIAM J. Contrl., vol. 4, pp. 61-80, 1966], to prove separation between a point\nand an arbitrary convex set $S\\subset\\mathbb{R}^{n}$ via calls to an oracle\nable to perform linear optimizations over $S$. Compared to other methods, our\nscheme has almost negligible memory requirements and the number of calls to the\noptimization oracle does not depend on the dimensionality $n$ of the underlying\nspace. We study the speed of convergence of the scheme under different promises\non the shape of the set $S$ and/or the location of the point, validating the\naccuracy of our theoretical bounds with numerical examples. Finally, we present\nsome applications of the scheme in quantum information theory. There we find\nthat our algorithm out-performs existing linear programming methods for certain\nlarge scale problems, allowing us to certify nonlocality in bipartite scenarios\nwith upto $42$ measurement settings. We apply the algorithm to upper bound the\nvisibility of two-qubit Werner states, hence improving known lower bounds on\nGrothendieck's constant $K_G(3)$. Similarly, we compute new upper bounds on the\nvisibility of GHZ states and on the steerability limit of Werner states for a\nfixed number of measurement settings. \n\n"}
{"id": "1609.05879", "contents": "Title: Online Output-Feedback Parameter and State Estimation for Second Order\n  Linear Systems Abstract: In this paper, a concurrent learning based adaptive observer is developed for\na class of second-order linear time-invariant systems with uncertain system\nmatrices. The developed technique yields an exponentially convergent state\nestimator and an exponentially convergent parameter estimator. As opposed to\npersistent excitation required for parameter convergence in traditional\nadaptive methods, excitation over a finite time-interval is sufficient for the\ndeveloped technique to achieve exponential convergence. Simulation results in\nboth noise-free and noisy environments are presented to validate the design. \n\n"}
{"id": "1609.06119", "contents": "Title: FastBDT: A speed-optimized and cache-friendly implementation of\n  stochastic gradient-boosted decision trees for multivariate classification Abstract: Stochastic gradient-boosted decision trees are widely employed for\nmultivariate classification and regression tasks. This paper presents a\nspeed-optimized and cache-friendly implementation for multivariate\nclassification called FastBDT. FastBDT is one order of magnitude faster during\nthe fitting-phase and application-phase, in comparison with popular\nimplementations in software frameworks like TMVA, scikit-learn and XGBoost. The\nconcepts used to optimize the execution time and performance studies are\ndiscussed in detail in this paper. The key ideas include: An equal-frequency\nbinning on the input data, which allows replacing expensive floating-point with\ninteger operations, while at the same time increasing the quality of the\nclassification; a cache-friendly linear access pattern to the input data, in\ncontrast to usual implementations, which exhibit a random access pattern.\nFastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used\nin the field of high energy physics by the Belle II experiment. \n\n"}
{"id": "1609.06662", "contents": "Title: On Efficient Computation of Shortest Dubins Paths Through Three\n  Consecutive Points Abstract: In this paper, we address the problem of computing optimal paths through\nthree consecutive points for the curvature-constrained forward moving Dubins\nvehicle. Given initial and final configurations of the Dubins vehicle, and a\nmidpoint with an unconstrained heading, the objective is to compute the\nmidpoint heading that minimizes the total Dubins path length. We provide a\nnovel geometrical analysis of the optimal path, and establish new properties of\nthe optimal Dubins' path through three points. We then show how our method can\nbe used to quickly refine Dubins TSP tours produced using state-of-the-art\ntechniques. We also provide extensive simulation results showing the\nimprovement of the proposed approach in both runtime and solution quality over\nthe conventional method of uniform discretization of the heading at the\nmid-point, followed by solving the minimum Dubins path for each discrete\nheading. \n\n"}
{"id": "1609.06804", "contents": "Title: Decoupled Asynchronous Proximal Stochastic Gradient Descent with\n  Variance Reduction Abstract: In the era of big data, optimizing large scale machine learning problems\nbecomes a challenging task and draws significant attention. Asynchronous\noptimization algorithms come out as a promising solution. Recently, decoupled\nasynchronous proximal stochastic gradient descent (DAP-SGD) is proposed to\nminimize a composite function. It is claimed to be able to off-loads the\ncomputation bottleneck from server to workers by allowing workers to evaluate\nthe proximal operators, therefore, server just need to do element-wise\noperations. However, it still suffers from slow convergence rate because of the\nvariance of stochastic gradient is nonzero. In this paper, we propose a faster\nmethod, decoupled asynchronous proximal stochastic variance reduced gradient\ndescent method (DAP-SVRG). We prove that our method has linear convergence for\nstrongly convex problem. Large-scale experiments are also conducted in this\npaper, and results demonstrate our theoretical analysis. \n\n"}
{"id": "1609.07082", "contents": "Title: Large Margin Nearest Neighbor Classification using Curved Mahalanobis\n  Distances Abstract: We consider the supervised classification problem of machine learning in\nCayley-Klein projective geometries: We show how to learn a curved Mahalanobis\nmetric distance corresponding to either the hyperbolic geometry or the elliptic\ngeometry using the Large Margin Nearest Neighbor (LMNN) framework. We report on\nour experimental results, and further consider the case of learning a mixed\ncurved Mahalanobis distance. Besides, we show that the Cayley-Klein Voronoi\ndiagrams are affine, and can be built from an equivalent (clipped) power\ndiagrams, and that Cayley-Klein balls have Mahalanobis shapes with displaced\ncenters. \n\n"}
{"id": "1609.07221", "contents": "Title: Convergence analysis of the direct extension of ADMM for multiple-block\n  separable convex minimization Abstract: Recently, the alternating direction method of multipliers (ADMM) has found\nmany efficient applications in various areas; and it has been shown that the\nconvergence is not guaranteed when it is directly extended to the\nmultiple-block case of separable convex minimization problems where there are\n$m\\ge 3$ functions without coupled variables in the objective. This fact has\ngiven great impetus to investigate various conditions on both the model and the\nalgorithm's parameter that can ensure the convergence of the direct extension\nof ADMM (abbreviated as \"e-ADMM\"). Despite some results under very strong\nconditions (e.g., at least $(m-1)$ functions should be strongly convex) that\nare applicable to the generic case with a general $m$, some others concentrate\non the special case of $m=3$ under the relatively milder condition that only\none function is assumed to be strongly convex. We focus on extending the\nconvergence analysis from the case of $m=3$ to the more general case of\n$m\\ge3$. That is, we show the convergence of e-ADMM for the case of $m\\ge 3$\nwith the assumption of only $(m-2)$ functions being strongly convex; and\nestablish its convergence rates in different scenarios such as the worst-case\nconvergence rates measured by iteration complexity and the asymptotically\nlinear convergence rate under stronger assumptions. Thus the convergence of\ne-ADMM for the general case of $m\\ge 4$ is proved; this result seems to be\nstill unknown even though it is intuitive given the known result of the case of\n$m=3$. Even for the special case of $m=3$, our convergence results turn out to\nbe more general than the exiting results that are derived specifically for the\ncase of $m=3$. \n\n"}
{"id": "1609.07261", "contents": "Title: Existence of tangent lines to Carnot-Carath\\'eodory geodesics Abstract: We show that length minimizing curves in Carnot-Carath\\'eodory spaces possess\nat any point at least one tangent curve (i.e., a blow-up in the nilpotent\napproximation) equal to a straight horizontal line. This is the first\nregularity result for length minimizers that holds with no assumption on either\nthe space (e.g., its rank, step, or analyticity) or the curve, and it is novel\neven in the setting of Carnot groups. \n\n"}
{"id": "1609.08221", "contents": "Title: Simultaneous Low-rank Component and Graph Estimation for\n  High-dimensional Graph Signals: Application to Brain Imaging Abstract: We propose an algorithm to uncover the intrinsic low-rank component of a\nhigh-dimensional, graph-smooth and grossly-corrupted dataset, under the\nsituations that the underlying graph is unknown. Based on a model with a\nlow-rank component plus a sparse perturbation, and an initial graph estimation,\nour proposed algorithm simultaneously learns the low-rank component and refines\nthe graph. Our evaluations using synthetic and real brain imaging data in\nunsupervised and supervised classification tasks demonstrate encouraging\nperformance. \n\n"}
{"id": "1609.08665", "contents": "Title: A Bayesian Risk Approach to Data-driven Stochastic Optimization:\n  Formulations and Asymptotics Abstract: A large class of stochastic programs involve optimizing an expectation taken\nwith respect to an underlying distribution that is unknown in practice. One\npopular approach to addressing the distributional uncertainty, known as the\ndistributionally robust optimization (DRO), is to hedge against the worst case\nover an uncertainty set of candidate distributions. However, it has been\nobserved that inappropriate construction of the uncertainty set can sometimes\nresult in over-conservative solutions. To explore the middle ground between\noptimistically ignoring the distributional uncertainty and pessimistically\nfixating on the worst-case scenario, we propose a Bayesian risk optimization\n(BRO) framework for parametric underlying distributions, which is to optimize a\nrisk functional applied to the posterior distribution of an unknown\ndistribution parameter. Of our particular interest are four risk functionals:\nmean, mean-variance, value-at-risk, and conditional value-at-risk. To unravel\nthe implication of BRO, we establish the consistency of objective functions and\noptimal solutions, as well as the asymptotic normality of objective functions\nand optimal values. More importantly, our analysis reveals a hidden\ninterpretation: the objectives of BRO can be approximately viewed as a weighted\nsum of posterior mean objective and the (squared) half-width of the true\nobjective's confidence interval. \n\n"}
{"id": "1610.00151", "contents": "Title: A compact representation for minimizers of $k$-submodular functions Abstract: A $k$-submodular function is a generalization of submodular and bisubmodular\nfunctions. This paper establishes a compact representation for minimizers of a\n$k$-submodular function by a poset with inconsistent pairs (PIP). This is a\ngeneralization of Ando-Fujishige's signed poset representation for minimizers\nof a bisubmodular function. We completely characterize the class of PIPs\n(elementary PIPs) arising from $k$-submodular functions. We give algorithms to\nconstruct the elementary PIP of minimizers of a $k$-submodular function $f$ for\nthree cases: (i) a minimizing oracle of $f$ is available, (ii) $f$ is\nnetwork-representable, and (iii) $f$ arises from a Potts energy function.\nFurthermore, we provide an efficient enumeration algorithm for all maximal\nminimizers of a Potts $k$-submodular function. Our results are applicable to\nobtain all maximal persistent labelings in actual computer vision problems. We\npresent experimental results for real vision instances. \n\n"}
{"id": "1610.00564", "contents": "Title: End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural\n  Networks Abstract: We investigate sequence machine learning techniques on raw radio signal\ntime-series data. By applying deep recurrent neural networks we learn to\ndiscriminate between several application layer traffic types on top of a\nconstant envelope modulation without using an expert demodulation algorithm. We\nshow that complex protocol sequences can be learned and used for both\nclassification and generation tasks using this approach. \n\n"}
{"id": "1610.00999", "contents": "Title: Exponential utility maximization under model uncertainty for unbounded\n  endowments Abstract: We consider the robust exponential utility maximization problem in discrete\ntime: An investor maximizes the worst case expected exponential utility with\nrespect to a family of nondominated probabilistic models of her endowment by\ndynamically investing in a financial market, and statically in available\noptions. We show that, for any measurable random endowment (regardless of\nwhether the problem is finite or not) an optimal strategy exists, a dual\nrepresentation in terms of (calibrated) martingale measures holds true, and\nthat the problem satisfies the dynamic programming principle (in case of no\noptions). Further it is shown that the value of the utility maximization\nproblem converges to the robust superhedging price as the risk aversion\nparameter gets large, and examples of nondominated probabilistic models are\ndiscussed. \n\n"}
{"id": "1610.01050", "contents": "Title: An Efficient High-Dimensional Sparse Fourier Transform Abstract: We propose RSFT, which is an extension of the one dimensional Sparse Fourier\nTransform algorithm to higher dimensions in a way that it can be applied to\nreal, noisy data. The RSFT allows for off-grid frequencies. Furthermore, by\nincorporating Neyman-Pearson detection, the frequency detection stages in RSFT\ndo not require knowledge of the exact sparsity of the signal and are more\nrobust to noise. We analyze the asymptotic performance of RSFT, and study the\ncomputational complexity versus the worst case signal SNR tradeoff. We show\nthat by choosing the proper parameters, the optimal tradeoff can be achieved.\nWe discuss the application of RSFT on short range ubiquitous radar signal\nprocessing, and demonstrate its feasibility via simulations. \n\n"}
{"id": "1610.01206", "contents": "Title: A Survey of Multi-View Representation Learning Abstract: Recently, multi-view representation learning has become a rapidly growing\ndirection in machine learning and data mining areas. This paper introduces two\ncategories for multi-view representation learning: multi-view representation\nalignment and multi-view representation fusion. Consequently, we first review\nthe representative methods and theories of multi-view representation learning\nbased on the perspective of alignment, such as correlation-based alignment.\nRepresentative examples are canonical correlation analysis (CCA) and its\nseveral extensions. Then from the perspective of representation fusion we\ninvestigate the advancement of multi-view representation learning that ranges\nfrom generative methods including multi-modal topic learning, multi-view sparse\ncoding, and multi-view latent space Markov networks, to neural network-based\nmethods including multi-modal autoencoders, multi-view convolutional neural\nnetworks, and multi-modal recurrent neural networks. Further, we also\ninvestigate several important applications of multi-view representation\nlearning. Overall, this survey aims to provide an insightful overview of\ntheoretical foundation and state-of-the-art developments in the field of\nmulti-view representation learning and to help researchers find the most\nappropriate tools for particular applications. \n\n"}
{"id": "1610.01871", "contents": "Title: Solutions to inexact resolvent inclusion problems with applications to\n  nonlinear analysis and optimization Abstract: Many problems in nonlinear analysis and optimization, among them variational\ninequalities and minimization of convex functions, can be reduced to finding\nzeros (namely, roots) of set-valued operators. Hence numerous algorithms have\nbeen devised in order to achieve this task. A lot of these algorithms are\ninexact in the sense that they allow perturbations to appear during the\niterative process, and hence they enable one to better deal with noise and\ncomputational errors, as well as superiorization. For many years a certain\nfundamental question has remained open regarding many of these known inexact\nalgorithmic schemes in various finite and infinite dimensional settings, namely\nwhether there exist sequences satisfying these inexact schemes when errors\nappear. We provide a positive answer to this question. Our results also show\nthat various theorems discussing the convergence of these inexact schemes have\na genuine merit beyond the exact case. As a by-product we solve the standard\nand the strongly implicit inexact resolvent inclusion problems, introduce a\npromising class of functions (fully Legendre functions), establish continuous\ndependence (stability) properties of the solution of the inexact resolvent\ninclusion problem and continuity properties of the protoresolvent, and\ngeneralize the notion of strong monotonicity. \n\n"}
{"id": "1610.04514", "contents": "Title: The proximal augmented Lagrangian method for nonsmooth composite\n  optimization Abstract: We study a class of optimization problems in which the objective function is\ngiven by the sum of a differentiable but possibly nonconvex component and a\nnondifferentiable convex regularization term. We introduce an auxiliary\nvariable to separate the objective function components and utilize the Moreau\nenvelope of the regularization term to derive the proximal augmented Lagrangian\n$-$ a continuously differentiable function obtained by constraining the\naugmented Lagrangian to the manifold that corresponds to the explicit\nminimization over the variable in the nonsmooth term. The continuous\ndifferentiability of this function with respect to both primal and dual\nvariables allows us to leverage the method of multipliers (MM) to compute\noptimal primal-dual pairs by solving a sequence of differentiable problems. The\nMM algorithm is applicable to a broader class of problems than proximal\ngradient methods and it has stronger convergence guarantees and a more refined\nstep-size update rules than the alternating direction method of multipliers.\nThese features make it an attractive option for solving structured optimal\ncontrol problems. We also develop an algorithm based on the primal-descent\ndual-ascent gradient method and prove global (exponential) asymptotic stability\nwhen the differentiable component of the objective function is (strongly)\nconvex and the regularization term is convex. Finally, we identify classes of\nproblems for which the primal-dual gradient flow dynamics are convenient for\ndistributed implementation and compare/contrast our framework to the existing\napproaches. \n\n"}
{"id": "1610.05984", "contents": "Title: Particle Swarm Optimization for Generating Interpretable Fuzzy\n  Reinforcement Learning Policies Abstract: Fuzzy controllers are efficient and interpretable system controllers for\ncontinuous state and action spaces. To date, such controllers have been\nconstructed manually or trained automatically either using expert-generated\nproblem-specific cost functions or incorporating detailed knowledge about the\noptimal control strategy. Both requirements for automatic training processes\nare not found in most real-world reinforcement learning (RL) problems. In such\napplications, online learning is often prohibited for safety reasons because\nonline learning requires exploration of the problem's dynamics during policy\ntraining. We introduce a fuzzy particle swarm reinforcement learning (FPSRL)\napproach that can construct fuzzy RL policies solely by training parameters on\nworld models that simulate real system dynamics. These world models are created\nby employing an autonomous machine learning technique that uses previously\ngenerated transition samples of a real system. To the best of our knowledge,\nthis approach is the first to relate self-organizing fuzzy controllers to\nmodel-based batch RL. Therefore, FPSRL is intended to solve problems in domains\nwhere online learning is prohibited, system dynamics are relatively easy to\nmodel from previously generated default policy transition samples, and it is\nexpected that a relatively easily interpretable control policy exists. The\nefficiency of the proposed approach with problems from such domains is\ndemonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole\nbalancing, and cart-pole swing-up. Our experimental results demonstrate\nhigh-performing, interpretable fuzzy policies. \n\n"}
{"id": "1610.06077", "contents": "Title: On the Security of Carrier Phase-based Ranging Abstract: Multicarrier phase-based ranging is fast emerging as a cost-optimized\nsolution for a wide variety of proximity-based applications due to its low\npower requirement, low hardware complexity and compatibility with existing\nstandards such as ZigBee and 6LoWPAN. Given potentially critical nature of the\napplications in which phase-based ranging can be deployed (e.g., access\ncontrol, asset tracking), it is important to evaluate its security guarantees.\nTherefore, in this work, we investigate the security of multicarrier\nphase-based ranging systems and specifically focus on distance decreasing relay\nattacks that have proven detrimental to the security of proximity-based access\ncontrol systems (e.g., vehicular passive keyless entry and start systems). We\nshow that phase-based ranging, as well as its implementations, are vulnerable\nto a variety of distance reduction attacks. We describe different attack\nrealizations and verify their feasibility by simulations and experiments on a\ncommercial ranging system. Specifically, we successfully reduced the estimated\nrange to less than 3 m even though the devices were more than 50 m apart. We\ndiscuss possible countermeasures against such attacks and illustrate their\nlimitations, therefore demonstrating that phase-based ranging cannot be fully\nsecured against distance decreasing attacks. \n\n"}
{"id": "1610.06283", "contents": "Title: Deep Neural Networks for Improved, Impromptu Trajectory Tracking of\n  Quadrotors Abstract: Trajectory tracking control for quadrotors is important for applications\nranging from surveying and inspection, to film making. However, designing and\ntuning classical controllers, such as proportional-integral-derivative (PID)\ncontrollers, to achieve high tracking precision can be time-consuming and\ndifficult, due to hidden dynamics and other non-idealities. The Deep Neural\nNetwork (DNN), with its superior capability of approximating abstract,\nnonlinear functions, proposes a novel approach for enhancing trajectory\ntracking control. This paper presents a DNN-based algorithm as an add-on module\nthat improves the tracking performance of a classical feedback controller.\nGiven a desired trajectory, the DNNs provide a tailored reference input to the\ncontroller based on their gained experience. The input aims to achieve a unity\nmap between the desired and the output trajectory. The motivation for this work\nis an interactive \"fly-as-you-draw\" application, in which a user draws a\ntrajectory on a mobile device, and a quadrotor instantly flies that trajectory\nwith the DNN-enhanced control system. Experimental results demonstrate that the\nproposed approach improves the tracking precision for user-drawn trajectories\nafter the DNNs are trained on selected periodic trajectories, suggesting the\nmethod's potential in real-world applications. Tracking errors are reduced by\naround 40-50% for both training and testing trajectories from users,\nhighlighting the DNNs' capability of generalizing knowledge. \n\n"}
{"id": "1610.06974", "contents": "Title: Optimal Control for Network Coding Broadcast Abstract: Random linear network coding (RLNC) has been shown to efficiently improve the\nnetwork performance in terms of reducing transmission delays and increasing the\nthroughput in broadcast and multicast communications. However, it can result in\nincreased storage and computational complexity at the receivers end. In our\nprevious work we considered the broadcast transmission of large file to N\nreceivers. We showed that the storage and complexity requirements at the\nreceivers end can be greatly reduced when segmenting the file into smaller\nblocks and applying RLNC to these blocks. To that purpose, we proposed a packet\nscheduling policy, namely the Least Received. In this work we will prove the\noptimality of our previously proposed policy, in terms of file transfer\ncompletion time, when N = 2. We will model our system as a Markov Decision\nProcess and prove the optimality of the policy using Dynamic Programming. Our\nintuition is that the Least Received policy may be optimal regardless of the\nnumber of receivers. Towards that end, we will provide experimental results\nthat verify that ntuition. \n\n"}
{"id": "1610.08120", "contents": "Title: Image Segmentation for Fruit Detection and Yield Estimation in Apple\n  Orchards Abstract: Ground vehicles equipped with monocular vision systems are a valuable source\nof high resolution image data for precision agriculture applications in\norchards. This paper presents an image processing framework for fruit detection\nand counting using orchard image data. A general purpose image segmentation\napproach is used, including two feature learning algorithms; multi-scale\nMulti-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These\nnetworks were extended by including contextual information about how the image\ndata was captured (metadata), which correlates with some of the appearance\nvariations and/or class distributions observed in the data. The pixel-wise\nfruit segmentation output is processed using the Watershed Segmentation (WS)\nand Circular Hough Transform (CHT) algorithms to detect and count individual\nfruits. Experiments were conducted in a commercial apple orchard near\nMelbourne, Australia. The results show an improvement in fruit segmentation\nperformance with the inclusion of metadata on the previously benchmarked MLP\nnetwork. We extend this work with CNNs, bringing agrovision closer to the\nstate-of-the-art in computer vision, where although metadata had negligible\ninfluence, the best pixel-wise F1-score of $0.791$ was achieved. The WS\nalgorithm produced the best apple detection and counting results, with a\ndetection F1-score of $0.858$. As a final step, image fruit counts were\naccumulated over multiple rows at the orchard and compared against the\npost-harvest fruit counts that were obtained from a grading and counting\nmachine. The count estimates using CNN and WS resulted in the best performance\nfor this dataset, with a squared correlation coefficient of $r^2=0.826$. \n\n"}
{"id": "1610.08401", "contents": "Title: Universal adversarial perturbations Abstract: Given a state-of-the-art deep neural network classifier, we show the\nexistence of a universal (image-agnostic) and very small perturbation vector\nthat causes natural images to be misclassified with high probability. We\npropose a systematic algorithm for computing universal perturbations, and show\nthat state-of-the-art deep neural networks are highly vulnerable to such\nperturbations, albeit being quasi-imperceptible to the human eye. We further\nempirically analyze these universal perturbations and show, in particular, that\nthey generalize very well across neural networks. The surprising existence of\nuniversal perturbations reveals important geometric correlations among the\nhigh-dimensional decision boundary of classifiers. It further outlines\npotential security breaches with the existence of single directions in the\ninput space that adversaries can possibly exploit to break a classifier on most\nnatural images. \n\n"}
{"id": "1610.09450", "contents": "Title: Evaluation of Automated Vehicles in the Frontal Cut-in Scenario - an\n  Enhanced Approach using Piecewise Mixture Models Abstract: Evaluation and testing are critical for the development of Automated Vehicles\n(AVs). Currently, companies test AVs on public roads, which is very\ntime-consuming and inefficient. We proposed the Accelerated Evaluation concept\nwhich uses a modified statistics of the surrounding vehicles and the Importance\nSampling theory to reduce the evaluation time by several orders of magnitude,\nwhile ensuring the final evaluation results are accurate. In this paper, we\nfurther extend this idea by using Piecewise Mixture Distribution models instead\nof Single Distribution models. We demonstrate this idea to evaluate vehicle\nsafety in lane change scenarios. The behavior of the cut-in vehicles was\nmodeled based on more than 400,000 naturalistic driving lane changes collected\nby the University of Michigan Safety Pilot Model Deployment Program. Simulation\nresults confirm that the accuracy and efficiency of the Piecewise Mixture\nDistribution method are better than the single distribution. \n\n"}
{"id": "1611.00347", "contents": "Title: Surpassing Gradient Descent Provably: A Cyclic Incremental Method with\n  Linear Convergence Rate Abstract: Recently, there has been growing interest in developing optimization methods\nfor solving large-scale machine learning problems. Most of these problems boil\ndown to the problem of minimizing an average of a finite set of smooth and\nstrongly convex functions where the number of functions $n$ is large. Gradient\ndescent method (GD) is successful in minimizing convex problems at a fast\nlinear rate; however, it is not applicable to the considered large-scale\noptimization setting because of the high computational complexity. Incremental\nmethods resolve this drawback of gradient methods by replacing the required\ngradient for the descent direction with an incremental gradient approximation.\nThey operate by evaluating one gradient per iteration and executing the average\nof the $n$ available gradients as a gradient approximate. Although, incremental\nmethods reduce the computational cost of GD, their convergence rates do not\njustify their advantage relative to GD in terms of the total number of gradient\nevaluations until convergence. In this paper, we introduce a Double Incremental\nAggregated Gradient method (DIAG) that computes the gradient of only one\nfunction at each iteration, which is chosen based on a cyclic scheme, and uses\nthe aggregated average gradient of all the functions to approximate the full\ngradient. The iterates of the proposed DIAG method uses averages of both\niterates and gradients in oppose to classic incremental methods that utilize\ngradient averages but do not utilize iterate averages. We prove that not only\nthe proposed DIAG method converges linearly to the optimal solution, but also\nits linear convergence factor justifies the advantage of incremental methods on\nGD. In particular, we prove that the worst case performance of DIAG is better\nthan the worst case performance of GD. \n\n"}
{"id": "1611.00589", "contents": "Title: Stochastic Control and Differential Games with Path-Dependent Influence\n  of Controls on Dynamics and Running Cost Abstract: In this paper, we consider the functional It\\^o calculus framework to find a\npath-dependent version of the Hamilton-Jacobi-Bellman equation for stochastic\ncontrol problems that feature dynamics and running cost that depend on the path\nof the control. We also prove a Dynamic Programming Principle for such\nproblems. We apply our results to path-dependence of the delay type. We further\nstudy Stochastic Differential Games in this context. \n\n"}
{"id": "1611.02320", "contents": "Title: Adversarial Ladder Networks Abstract: The use of unsupervised data in addition to supervised data in training\ndiscriminative neural networks has improved the performance of this clas-\nsification scheme. However, the best results were achieved with a training\nprocess that is divided in two parts: first an unsupervised pre-training step\nis done for initializing the weights of the network and after these weights are\nrefined with the use of supervised data. On the other hand adversarial noise\nhas improved the results of clas- sical supervised learning. Recently, a new\nneural network topology called Ladder Network, where the key idea is based in\nsome properties of hierar- chichal latent variable models, has been proposed as\na technique to train a neural network using supervised and unsupervised data at\nthe same time with what is called semi-supervised learning. This technique has\nreached state of the art classification. In this work we add adversarial noise\nto the ladder network and get state of the art classification, with several\nimportant conclusions on how adversarial noise can help in addition with new\npossible lines of investi- gation. We also propose an alternative to add\nadversarial noise to unsu- pervised data. \n\n"}
{"id": "1611.02338", "contents": "Title: Line failure probability bounds for power grids Abstract: We develop upper bounds for line failure probabilities in power grids, under\nthe DC approximation and assuming Gaussian noise for the power injections. Our\nupper bounds are explicit, and lead to characterization of safe operational\ncapacity regions that are convex and polyhedral, making our tools compatible\nwith existing planning methods. Our probabilistic bounds are derived through\nthe use of powerful concentration inequalities. \n\n"}
{"id": "1611.02408", "contents": "Title: Real-time Algorithm for Self-Reflective Model Predictive Control Abstract: This paper is about a real-time model predictive control (MPC) algorithm for\na particular class of model based controllers, whose objective consists of a\nnominal tracking objective and an additional learning objective. Here, the\nconstruction of the learning term is based on economic optimal experiment\ndesign criteria. It is added to the MPC objective in order to excite the system\nfrom time-to-time on purpose in order to improve the accuracy of the state and\nparameter estimates in the presence of incomplete or noise affected\nmeasurements. A particular focus of this paper is on so-called self-reflective\nmodel predictive control schemes, which have the property that the additional\nlearning term can be interpreted as the expected loss of optimality of the\ncontroller in the presence of random measurement errors. The main contribution\nof this paper is a formulation-tailored algorithm, which exploits the\nparticular structure of self-reflective MPC problems in order to speed-up the\nonline computation. It is shown that, in contrast to generic state-of-the-art\noptimal control problem solvers, the proposed algorithm can solve the\nself-reflective optimization problems with reasonable additional computational\neffort and in real-time. The advantages of the proposed real-time scheme are\nillustrated by applying the algorithm to a nonlinear process control problem in\nthe presence of measurement errors and process noise. \n\n"}
{"id": "1611.02431", "contents": "Title: Distributed recovery of jointly sparse signals under communication\n  constraints Abstract: The problem of the distributed recovery of jointly sparse signals has\nattracted much attention recently. Let us assume that the nodes of a network\nobserve different sparse signals with common support; starting from linear,\ncompressed measurements, and exploiting network communication, each node aims\nat reconstructing the support and the non-zero values of its observed signal.\nIn the literature, distributed greedy algorithms have been proposed to tackle\nthis problem, among which the most reliable ones require a large amount of\ntransmitted data, which barely adapts to realistic network communication\nconstraints. In this work, we address the problem through a reweighted $\\ell_1$\nsoft thresholding technique, in which the threshold is iteratively tuned based\non the current estimate of the support. The proposed method adapts to\nconstrained networks, as it requires only local communication among neighbors,\nand the transmitted messages are indices from a finite set. We analytically\nprove the convergence of the proposed algorithm and we show that it outperforms\nthe state-of-the-art greedy methods in terms of balance between recovery\naccuracy and communication load. \n\n"}
{"id": "1611.02654", "contents": "Title: Sentence Ordering and Coherence Modeling using Recurrent Neural Networks Abstract: Modeling the structure of coherent texts is a key NLP problem. The task of\ncoherently organizing a given set of sentences has been commonly used to build\nand evaluate models that understand such structure. We propose an end-to-end\nunsupervised deep learning approach based on the set-to-sequence framework to\naddress this problem. Our model strongly outperforms prior methods in the order\ndiscrimination task and a novel task of ordering abstracts from scientific\narticles. Furthermore, our work shows that useful text representations can be\nobtained by learning to order sentences. Visualizing the learned sentence\nrepresentations shows that the model captures high-level logical structure in\nparagraphs. Our representations perform comparably to state-of-the-art\npre-training methods on sentence similarity and paraphrase detection tasks. \n\n"}
{"id": "1611.04642", "contents": "Title: Link Prediction using Embedded Knowledge Graphs Abstract: Since large knowledge bases are typically incomplete, missing facts need to\nbe inferred from observed facts in a task called knowledge base completion. The\nmost successful approaches to this task have typically explored explicit paths\nthrough sequences of triples. These approaches have usually resorted to\nhuman-designed sampling procedures, since large knowledge graphs produce\nprohibitively large numbers of possible paths, most of which are uninformative.\nAs an alternative approach, we propose performing a single, short sequence of\ninteractive lookup operations on an embedded knowledge graph which has been\ntrained through end-to-end backpropagation to be an optimized and compressed\nversion of the initial knowledge base. Our proposed model, called Embedded\nKnowledge Graph Network (EKGN), achieves new state-of-the-art results on\npopular knowledge base completion benchmarks. \n\n"}
{"id": "1611.05377", "contents": "Title: Fully-adaptive Feature Sharing in Multi-Task Networks with Applications\n  in Person Attribute Classification Abstract: Multi-task learning aims to improve generalization performance of multiple\nprediction tasks by appropriately sharing relevant information across them. In\nthe context of deep neural networks, this idea is often realized by\nhand-designed network architectures with layers that are shared across tasks\nand branches that encode task-specific features. However, the space of possible\nmulti-task deep architectures is combinatorially large and often the final\narchitecture is arrived at by manual exploration of this space subject to\ndesigner's bias, which can be both error-prone and tedious. In this work, we\npropose a principled approach for designing compact multi-task deep learning\narchitectures. Our approach starts with a thin network and dynamically widens\nit in a greedy manner during training using a novel criterion that promotes\ngrouping of similar tasks together. Our Extensive evaluation on person\nattributes classification tasks involving facial and clothing attributes\nsuggests that the models produced by the proposed method are fast, compact and\ncan closely match or exceed the state-of-the-art accuracy from strong baselines\nby much more expensive models. \n\n"}
{"id": "1611.05961", "contents": "Title: Stochastic Recursive Inclusions in two timescales with non-additive\n  iterate dependent Markov noise Abstract: In this paper we study the asymptotic behavior of a stochastic approximation\nscheme on two timescales with set-valued drift functions and in the presence of\nnon-additive iterate-dependent Markov noise. It is shown that the recursion on\neach timescale tracks the flow of a differential inclusion obtained by\naveraging the set-valued drift function in the recursion with respect to a set\nof measures which take into account both the averaging with respect to the\nstationary distributions of the Markov noise terms and the interdependence\nbetween the two recursions on different timescales. The framework studied in\nthis paper builds on the works of \\it{A. Ramaswamy et al. }\\rm by allowing for\nthe presence of non-additive iterate-dependent Markov noise. As an application,\nwe consider the problem of computing the optimum in a constrained convex\noptimization problem where the objective function and the constraints are\naveraged with respect to the stationary distribution of an underlying Markov\nchain. Further the proposed scheme neither requires the differentiability of\nthe objective function nor the knowledge of the averaging measure. \n\n"}
{"id": "1611.06830", "contents": "Title: Linear quadratic stochastic control problems with stochastic terminal\n  constraint Abstract: We study a linear quadratic optimal control problem with stochastic\ncoefficients and a terminal state constraint, which may be in force merely on a\nset with positive, but not necessarily full probability. Under such a partial\nterminal constraint, the usual approach via a coupled system of a backward\nstochastic Riccati equation and a linear backward equation breaks down. As a\nremedy, we introduce a family of auxiliary problems parametrized by the\nsupersolutions to this Riccati equation alone. The target functional of these\nproblems dominates the original constrained one and allows for an explicit\ndescription of both the optimal control policy and the auxiliary problem's\nvalue in terms of a suitably constructed optimal signal process. This suggests\nthat, for the minimal supersolution of the Riccati equation, the minimizers of\nthe auxiliary problem coincide with those of the original problem, a conjecture\nthat we see confirmed in all examples understood so far. \n\n"}
{"id": "1611.07119", "contents": "Title: Max-Margin Deep Generative Models for (Semi-)Supervised Learning Abstract: Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, it is relatively insufficient to\nempower the discriminative ability of DGMs on making accurate predictions. This\npaper presents max-margin deep generative models (mmDGMs) and a\nclass-conditional variant (mmDCGMs), which explore the strongly discriminative\nprinciple of max-margin learning to improve the predictive performance of DGMs\nin both supervised and semi-supervised learning, while retaining the generative\ncapability. In semi-supervised learning, we use the predictions of a max-margin\nclassifier as the missing labels instead of performing full posterior inference\nfor efficiency; we also introduce additional max-margin and label-balance\nregularization terms of unlabeled data for effectiveness. We develop an\nefficient doubly stochastic subgradient algorithm for the piecewise linear\nobjectives in different settings. Empirical results on various datasets\ndemonstrate that: (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; (2)\nin supervised learning, mmDGMs are competitive to the best fully discriminative\nnetworks when employing convolutional neural networks as the generative and\nrecognition models; and (3) in semi-supervised learning, mmDCGMs can perform\nefficient inference and achieve state-of-the-art classification results on\nseveral benchmarks. \n\n"}
{"id": "1611.07661", "contents": "Title: Multigrid Neural Architectures Abstract: We propose a multigrid extension of convolutional neural networks (CNNs).\nRather than manipulating representations living on a single spatial grid, our\nnetwork layers operate across scale space, on a pyramid of grids. They consume\nmultigrid inputs and produce multigrid outputs; convolutional filters\nthemselves have both within-scale and cross-scale extent. This aspect is\ndistinct from simple multiscale designs, which only process the input at\ndifferent scales. Viewed in terms of information flow, a multigrid network\npasses messages across a spatial pyramid. As a consequence, receptive field\nsize grows exponentially with depth, facilitating rapid integration of context.\nMost critically, multigrid structure enables networks to learn internal\nattention and dynamic routing mechanisms, and use them to accomplish tasks on\nwhich modern CNNs fail.\n  Experiments demonstrate wide-ranging performance advantages of multigrid. On\nCIFAR and ImageNet classification tasks, flipping from a single grid to\nmultigrid within the standard CNN paradigm improves accuracy, while being\ncompute and parameter efficient. Multigrid is independent of other\narchitectural choices; we show synergy in combination with residual\nconnections. Multigrid yields dramatic improvement on a synthetic semantic\nsegmentation dataset. Most strikingly, relatively shallow multigrid networks\ncan learn to directly perform spatial transformation tasks, where, in contrast,\ncurrent CNNs fail. Together, our results suggest that continuous evolution of\nfeatures on a multigrid pyramid is a more powerful alternative to existing CNN\ndesigns on a flat grid. \n\n"}
{"id": "1611.08930", "contents": "Title: Deep attractor network for single-microphone speaker separation Abstract: Despite the overwhelming success of deep learning in various speech\nprocessing tasks, the problem of separating simultaneous speakers in a mixture\nremains challenging. Two major difficulties in such systems are the arbitrary\nsource permutation and unknown number of sources in the mixture. We propose a\nnovel deep learning framework for single channel speech separation by creating\nattractor points in high dimensional embedding space of the acoustic signals\nwhich pull together the time-frequency bins corresponding to each source.\nAttractor points in this study are created by finding the centroids of the\nsources in the embedding space, which are subsequently used to determine the\nsimilarity of each bin in the mixture to each source. The network is then\ntrained to minimize the reconstruction error of each source by optimizing the\nembeddings. The proposed model is different from prior works in that it\nimplements an end-to-end training, and it does not depend on the number of\nsources in the mixture. Two strategies are explored in the test time, K-means\nand fixed attractor points, where the latter requires no post-processing and\ncan be implemented in real-time. We evaluated our system on Wall Street Journal\ndataset and show 5.49\\% improvement over the previous state-of-the-art methods. \n\n"}
{"id": "1611.09946", "contents": "Title: Vector-Valued Optimal Mass Transport Abstract: We introduce the problem of transporting vector-valued distributions. In\nthis, a salient feature is that mass may flow between vectorial entries as well\nas across space (discrete or continuous). The theory relies on a first step\ntaken to define an appropriate notion of optimal transport on a graph. The\ncorresponding distance between distributions is readily computable via convex\noptimization and provides a suitable generalization of Wasserstein-type\nmetrics. Building on this, we define Wasserstein-type metrics on vector-valued\ndistributions supported on continuous spaces as well as graphs. Motivation for\ndeveloping vector-valued mass transport is provided by applications such as\nmulti-color image processing, polarimetric radar, as well as network problems\nwhere resources may be vectorial. \n\n"}
{"id": "1611.10191", "contents": "Title: Is Non-Neutrality Profitable for the Stakeholders of the Internet\n  Market? Abstract: Net neutrality on the Internet is perceived as the policy that mandates\nInternet Service Providers (ISPs) to treat all data equally, regardless of the\nsource, destination, or type of transmitted data. In this work, we consider a\nscheme in which the decision makers of the market are two ISPs, one \"big\"\nContent Provider (CP), and a continuum of end-users. One of the ISPs is neutral\nand the other is non-neutral, i.e. she offers a premium quality to a CP in\nexchange for a side-payment. In addition, we assume that the CP can\ndifferentiate between ISPs by controlling the quality of the content she is\noffering on each one. In this part of the paper, we consider a scenario in\nwhich end-users are not locked in with the ISPs and can switch between ISPs\neasily. We formulate a sequential game, and show that there exists a unique\nSub-game Perfect Nash Equilibrium (SPNE) for the game, where the CP pays the\nside-payment to the non-neutral ISP and offers her content with the premium\nquality. In addition, the CP does not offer her content on the neutral ISP.\nThus, driving this ISP out of the market. \n\n"}
{"id": "1612.02795", "contents": "Title: Safety Verification and Control for Collision Avoidance at Road\n  Intersections Abstract: This paper presents the design of a supervisory algorithm that monitors\nsafety at road intersections and overrides drivers with a safe input when\nnecessary. The design of the supervisor consists of two parts: safety\nverification and control design. Safety verification is the problem to\ndetermine if vehicles will be able to cross the intersection without colliding\nwith current drivers' inputs. We translate this safety verification problem\ninto a jobshop scheduling problem, which minimizes the maximum lateness and\nevaluates if the optimal cost is zero. The zero optimal cost corresponds to the\ncase in which all vehicles can cross each conflict area without collisions.\nComputing the optimal cost requires solving a Mixed Integer Nonlinear\nProgramming (MINLP) problem due to the nonlinear second-order dynamics of the\nvehicles. We therefore estimate this optimal cost by formulating two related\nMixed Integer Linear Programming (MILP) problems that assume simpler vehicle\ndynamics. We prove that these two MILP problems yield lower and upper bounds of\nthe optimal cost. We also quantify the worst case approximation errors of these\nMILP problems. We design the supervisor to override the vehicles with a safe\ncontrol input if the MILP problem that computes the upper bound yields a\npositive optimal cost. We theoretically demonstrate that the supervisor keeps\nthe intersection safe and is non-blocking. Computer simulations further\nvalidate that the algorithms can run in real time for problems of realistic\nsize. \n\n"}
{"id": "1612.03815", "contents": "Title: Gradient-Based Multiobjective Optimization with Uncertainties Abstract: In this article we develop a gradient-based algorithm for the solution of\nmultiobjective optimization problems with uncertainties. To this end, an\nadditional condition is derived for the descent direction in order to account\nfor inaccuracies in the gradients and then incorporated in a subdivison\nalgorithm for the computation of global solutions to multiobjective\noptimization problems. Convergence to a superset of the Pareto set is proved\nand an upper bound for the maximal distance to the set of substationary points\nis given. Besides the applicability to problems with uncertainties, the\nalgorithm is developed with the intention to use it in combination with model\norder reduction techniques in order to efficiently solve PDE-constrained\nmultiobjective optimization problems. \n\n"}
{"id": "1612.04082", "contents": "Title: What Lies Beneath the Surface: Topological-Shape Optimization With the\n  Kernel-Independent Fast Multipole Method Abstract: The paper presents a new method for shape and topology optimization based on\nan efficient and scalable boundary integral formulation for elasticity. To\noptimize topology, our approach uses iterative extraction of isosurfaces of a\ntopological derivative. The numerical solution of the elasticity boundary value\nproblem at every iteration is performed with the boundary element formulation\nand the kernel-independent fast multipole method. Providing excellent single\nnode performance, scalable parallelization and the best available asymptotic\ncomplexity, our method is among the fastest optimization tools available today.\nThe performance of our approach is studied on few illustrative examples,\nincluding the optimization of engineered constructions for the minimum\ncompliance and the optimization of the microstructure of a metamaterial for the\ndesired macroscopic tensor of elasticity. \n\n"}
{"id": "1612.04119", "contents": "Title: Rigidity for critical points in the Levy-Gromov inequality Abstract: The Levy-Gromov inequality states that round spheres have the least\nisoperimetric profile (normalized by total volume) among Riemannian manifolds\nwith a fixed positive lower bound on the Ricci tensor. In this note we study\ncritical metrics corresponding to the Levy-Gromov inequality and prove that, in\ntwo-dimensions, this criticality condition is quite rigid, as it characterizes\nround spheres and projective planes. \n\n"}
{"id": "1612.05044", "contents": "Title: A non-autonomous stochastic discrete time system with uniform\n  disturbances Abstract: The main objective of this article is to present Bayesian optimal control\nover a class of non-autonomous linear stochastic discrete time systems with\ndisturbances belonging to a family of the one parameter uniform distributions.\nIt is proved that the Bayes control for the Pareto priors is the solution of a\nlinear system of algebraic equations. For the case that this linear system is\nsingular, we apply optimization techniques to gain the Bayesian optimal\ncontrol. These results are extended to generalized linear stochastic systems of\ndifference equations and provide the Bayesian optimal control for the case\nwhere the coefficients of these type of systems are non-square matrices. The\npaper extends the results of the authors developed for system with disturbances\nbelonging to the exponential family. \n\n"}
{"id": "1612.05812", "contents": "Title: Decentralized Robust Inverter-based Control in Power Systems Abstract: This paper develops a novel framework for power system stability analysis,\nthat allows for the decentralized design of inverter based controllers. The\nmethod requires that each individual inverter satisfies a standard $H^\\infty$\ndesign requirement. Critically each requirement depends only on the dynamics of\nthe components and inverters at each individual bus and the aggregate\nsusceptance of the transmission lines connected to it. The method is both\nrobust to network and delay uncertainties, as well as heterogeneous network\ncomponents, and when no network information is available it reduces to the\nstandard decentralized passivity sufficient condition for stability. We\nillustrate the novelty and strength of our approach by studying the design of\ninverter-based control laws in the presence of delays. \n\n"}
{"id": "1612.08515", "contents": "Title: Compositional Abstraction-Based Controller Synthesis for Continuous-Time\n  Systems Abstract: Controller synthesis techniques for continuous systems with respect to\ntemporal logic specifications typically use a finite-state symbolic abstraction\nof the system model. Constructing this abstraction for the entire system is\ncomputationally expensive, and does not exploit natural decompositions of many\nsystems into interacting components. We describe a methodology for\ncompositional symbolic abstraction to help scale controller synthesis for\ntemporal logic to larger systems.\n  We introduce a new relation, called (approximate) disturbance bisimulation,\nas the basis for compositional symbolic abstractions. Disturbance bisimulation\nstrengthens the standard approximate alternating bisimulation relation used in\ncontrol. It extends naturally to systems which are composed of weakly\ninterconnected sub-components possibly connected in feedback, and models the\ncoupling signals as disturbances. After proving this composability of\ndisturbance bisimulation for metric systems we apply this result to the\ncompositional abstraction of networks of input-to-state stable deterministic\nnon-linear control systems. We give conditions that allow to construct\nfinite-state abstractions compositionally for each component in such a network,\nso that the abstractions are simultaneously disturbance bisimilar to their\ncontinuous counterparts. Combining these two results, we show conditions under\nwhich one can compositionally abstract a network of non-linear control systems\nin a modular way while ensuring that the final composed abstraction is\ndisturbance bisimilar to the original system. We discuss how we get a\ncompositional abstraction-based controller synthesis methodology for networks\nof such systems against local temporal specifications as a by-product of our\nconstruction. \n\n"}
{"id": "1701.01290", "contents": "Title: Approximate Value Iteration for Risk-aware Markov Decision Processes Abstract: We consider large-scale Markov decision processes (MDPs) with a risk measure\nof variability in cost, under the risk-aware MDPs paradigm. Previous studies\nshowed that risk-aware MDPs, based on a minimax approach to handling risk, can\nbe solved using dynamic programming for small to medium sized problems.\nHowever, due to the \"curse of dimensionality\", MDPs that model real-life\nproblems are typically prohibitively large for such approaches. In this paper,\nwe employ an approximate dynamic programming approach, and develop a family of\nsimulation-based algorithms to approximately solve large-scale risk-aware MDPs.\nIn parallel, we develop a unified convergence analysis technique to derive\nsample complexity bounds for this new family of algorithms. \n\n"}
{"id": "1701.01722", "contents": "Title: Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU Abstract: The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem. \n\n"}
{"id": "1701.04537", "contents": "Title: Cloud Resource Allocation for Cloud-Based Automotive Applications Abstract: There is a rapidly growing interest in the use of cloud computing for\nautomotive vehicles to facilitate computation and data intensive tasks.\nEfficient utilization of on-demand cloud resources holds a significant\npotential to improve future vehicle safety, comfort, and fuel economy. In the\nmeanwhile, issues like cyber security and resource allocation pose great\nchallenges. In this paper, we treat the resource allocation problem for\ncloud-based automotive systems. Both private and public cloud paradigms are\nconsidered where a private cloud provides an internal, company-owned internet\nservice dedicated to its own vehicles while a public cloud serves all\nsubscribed vehicles. This paper establishes comprehensive models of cloud\nresource provisioning for both private and public cloud- based automotive\nsystems. Complications such as stochastic communication delays and task\ndeadlines are explicitly considered. In particular, a centralized resource\nprovisioning model is developed for private cloud and chance constrained\noptimization is exploited to utilize the cloud resources for best Quality of\nServices. On the other hand, a decentralized auction-based model is developed\nfor public cloud and reinforcement learning is employed to obtain an optimal\nbidding policy for a \"selfish\" agent. Numerical examples are presented to\nillustrate the effectiveness of the developed techniques. \n\n"}
{"id": "1701.06700", "contents": "Title: Extreme functions with an arbitrary number of slopes Abstract: For the one dimensional infinite group relaxation, we construct a sequence of\nextreme valid functions that are piecewise linear and such that for every\nnatural number $k\\geq 2$, there is a function in the sequence with $k$ slopes.\nThis settles an open question in this area regarding a universal bound on the\nnumber of slopes for extreme functions. The function which is the pointwise\nlimit of this sequence is an extreme valid function that is continuous and has\nan infinite number of slopes. This provides a new and more refined\ncounterexample to an old conjecture of Gomory and Johnson stating that all\nextreme functions are piecewise linear. These constructions are extended to\nobtain functions for the higher dimensional group problems via the\nsequential-merge operation of Dey and Richard. \n\n"}
{"id": "1701.08074", "contents": "Title: Model-Free Control of Thermostatically Controlled Loads Connected to a\n  District Heating Network Abstract: Optimal control of thermostatically controlled loads connected to a district\nheating network is considered a sequential decision- making problem under\nuncertainty. The practicality of a direct model-based approach is compromised\nby two challenges, namely scalability due to the large dimensionality of the\nproblem and the system identification required to identify an accurate model.\nTo help in mitigating these problems, this paper leverages on recent\ndevelopments in reinforcement learning in combination with a market-based\nmulti-agent system to obtain a scalable solution that obtains a significant\nperformance improvement in a practical learning time. The control approach is\napplied on a scenario comprising 100 thermostatically controlled loads\nconnected to a radial district heating network supplied by a central combined\nheat and power plant. Both for an energy arbitrage and a peak shaving\nobjective, the control approach requires 60 days to obtain a performance within\n65% of a theoretical lower bound on the cost. \n\n"}
{"id": "1701.08498", "contents": "Title: Efficient DC Algorithm for Constrained Sparse Optimization Abstract: We address the minimization of a smooth objective function under an\n$\\ell_0$-constraint and simple convex constraints. When the problem has no\nconstraints except the $\\ell_0$-constraint, some efficient algorithms are\navailable; for example, Proximal DC (Difference of Convex functions) Algorithm\n(PDCA) repeatedly evaluates closed-form solutions of convex subproblems,\nleading to a stationary point of the $\\ell_0$-constrained problem. However,\nwhen the problem has additional convex constraints, they become inefficient\nbecause it is difficult to obtain closed-form solutions of the associated\nsubproblems. In this paper, we reformulate the problem by employing a new DC\nrepresentation of the $\\ell_0$-constraint, so that PDCA can retain the\nefficiency by reducing its subproblems to the projection operation onto a\nconvex set. Moreover, inspired by the Nesterov's acceleration technique for\nproximal methods, we propose the Accelerated PDCA (APDCA), which attains the\noptimal convergence rate if applied to convex programs, and performs well in\nnumerical experiments. \n\n"}
{"id": "1702.00001", "contents": "Title: Learning the distribution with largest mean: two bandit frameworks Abstract: Over the past few years, the multi-armed bandit model has become increasingly\npopular in the machine learning community, partly because of applications\nincluding online content optimization. This paper reviews two different\nsequential learning tasks that have been considered in the bandit literature ;\nthey can be formulated as (sequentially) learning which distribution has the\nhighest mean among a set of distributions, with some constraints on the\nlearning process. For both of them (regret minimization and best arm\nidentification) we present recent, asymptotically optimal algorithms. We\ncompare the behaviors of the sampling rule of each algorithm as well as the\ncomplexity terms associated to each problem. \n\n"}
{"id": "1702.00155", "contents": "Title: Asymptotically Efficient Identification of Known-Sensor Hidden Markov\n  Models Abstract: We consider estimating the transition probability matrix of a finite-state\nfinite-observation alphabet hidden Markov model with known observation\nprobabilities. The main contribution is a two-step algorithm; a method of\nmoments estimator (formulated as a convex optimization problem) followed by a\nsingle iteration of a Newton-Raphson maximum likelihood estimator. The two-fold\ncontribution of this letter is, firstly, to theoretically show that the\nproposed estimator is consistent and asymptotically efficient, and secondly, to\nnumerically show that the method is computationally less demanding than\nconventional methods - in particular for large data sets. \n\n"}
{"id": "1702.01890", "contents": "Title: Graphical Models and Belief Propagation-hierarchy for Optimal\n  Physics-Constrained Network Flows Abstract: In this manuscript we review new ideas and first results on application of\nthe Graphical Models approach, originated from Statistical Physics, Information\nTheory, Computer Science and Machine Learning, to optimization problems of\nnetwork flow type with additional constraints related to the physics of the\nflow. We illustrate the general concepts on a number of enabling examples from\npower system and natural gas transmission (continental scale) and distribution\n(district scale) systems. \n\n"}
{"id": "1702.02241", "contents": "Title: Adapting Regularized Low Rank Models for Parallel Architectures Abstract: We introduce a reformulation of regularized low-rank recovery models to take\nadvantage of GPU, multiple CPU, and hybridized architectures. Low-rank recovery\noften involves nuclear-norm minimization through iterative thresholding of\nsingular values. These models are slow to fit and difficult to parallelize\nbecause of their dependence on computing a singular value decomposition at each\niteration. Regularized low-rank recovery models also incorporate non-smooth\nterms to separate structured components (e.g. sparse outliers) from the\nlow-rank component, making these problems more difficult.\n  Using Burer-Monteiro splitting and marginalization, we develop a smooth,\nnon-convex formulation of regularized low-rank recovery models that can be fit\nwith first-order solvers. We develop a computable certificate of convergence\nfor this non-convex program, and use it to establish bounds on the\nsuboptimality of any point. Using robust principal component analysis (RPCA) as\nan example, we include numerical experiments showing that this approach is an\norder-of-magnitude faster than existing RPCA solvers on the GPU. We also show\nthat this acceleration allows new applications for RPCA, including real-time\nbackground subtraction and MR image analysis. \n\n"}
{"id": "1702.04939", "contents": "Title: A Bayesian framework for distributed estimation of arrival rates in\n  asynchronous networks Abstract: In this paper we consider a network of agents monitoring a spatially\ndistributed arrival process. Each node measures the number of arrivals seen at\nits monitoring point in a given time-interval with the objective of estimating\nthe unknown local arrival rate. We propose an asynchronous distributed approach\nbased on a Bayesian model with unknown hyperparameter, where each node computes\nthe minimum mean square error (MMSE) estimator of its local arrival rate in a\ndistributed way. As a result, the estimation at each node \"optimally\" fuses the\ninformation from the whole network through a distributed optimization\nalgorithm. Moreover, we propose an ad-hoc distributed estimator, based on a\nconsensus algorithm for time-varying and directed graphs, which exhibits\nreduced complexity and exponential convergence. We analyze the performance of\nthe proposed distributed estimators, showing that they: (i) are reliable even\nin presence of limited local data, and (ii) improve the estimation accuracy\ncompared to the purely decentralized setup. Finally, we provide a statistical\ncharacterization of the proposed estimators. In particular, for the ad-hoc\nestimator, we show that as the number of nodes goes to infinity its mean square\nerror converges to the optimal one. Numerical Monte Carlo simulations confirm\nthe theoretical characterization and highlight the appealing performances of\nthe estimators. \n\n"}
{"id": "1702.05551", "contents": "Title: A Distributed Online Pricing Strategy for Demand Response Programs Abstract: We study a demand response problem from utility (also referred to as\noperator)'s perspective with realistic settings, in which the utility faces\nuncertainty and limited communication. Specifically, the utility does not know\nthe cost function of consumers and cannot have multiple rounds of information\nexchange with consumers. We formulate an optimization problem for the utility\nto minimize its operational cost considering time-varying demand response\ntargets and responses of consumers. We develop a joint online learning and\npricing algorithm. In each time slot, the utility sends out a price signal to\nall consumers and estimates the cost functions of consumers based on their\nnoisy responses. We measure the performance of our algorithm using regret\nanalysis and show that our online algorithm achieves logarithmic regret with\nrespect to the operating horizon. In addition, our algorithm employs linear\nregression to estimate the aggregate response of consumers, making it easy to\nimplement in practice. Simulation experiments validate the theoretic results\nand show that the performance gap between our algorithm and the offline\noptimality decays quickly. \n\n"}
{"id": "1702.06329", "contents": "Title: Towards a Common Implementation of Reinforcement Learning for Multiple\n  Robotic Tasks Abstract: Mobile robots are increasingly being employed for performing complex tasks in\ndynamic environments. Reinforcement learning (RL) methods are recognized to be\npromising for specifying such tasks in a relatively simple manner. However, the\nstrong dependency between the learning method and the task to learn is a\nwell-known problem that restricts practical implementations of RL in robotics,\noften requiring major modifications of parameters and adding other techniques\nfor each particular task. In this paper we present a practical core\nimplementation of RL which enables the learning process for multiple robotic\ntasks with minimal per-task tuning or none. Based on value iteration methods,\nthis implementation includes a novel approach for action selection, called\nQ-biased softmax regression (QBIASSR), which avoids poor performance of the\nlearning process when the robot reaches new unexplored states. Our approach\ntakes advantage of the structure of the state space by attending the physical\nvariables involved (e.g., distances to obstacles, X,Y,{\\theta} pose, etc.),\nthus experienced sets of states may favor the decision-making process of\nunexplored or rarely-explored states. This improvement has a relevant role in\nreducing the tuning of the algorithm for particular tasks. Experiments with\nreal and simulated robots, performed with the software framework also\nintroduced here, show that our implementation is effectively able to learn\ndifferent robotic tasks without tuning the learning method. Results also\nsuggest that the combination of true online SARSA({\\lambda}) with QBIASSR can\noutperform the existing RL core algorithms in low-dimensional robotic tasks. \n\n"}
{"id": "1702.06463", "contents": "Title: Predicting non-linear dynamics by stable local learning in a recurrent\n  spiking neural network Abstract: Brains need to predict how the body reacts to motor commands. It is an open\nquestion how networks of spiking neurons can learn to reproduce the non-linear\nbody dynamics caused by motor commands, using local, online and stable learning\nrules. Here, we present a supervised learning scheme for the feedforward and\nrecurrent connections in a network of heterogeneous spiking neurons. The error\nin the output is fed back through fixed random connections with a negative\ngain, causing the network to follow the desired dynamics, while an online and\nlocal rule changes the weights. The rule for Feedback-based Online Local\nLearning Of Weights (FOLLOW) is local in the sense that weight changes depend\non the presynaptic activity and the error signal projected onto the\npostsynaptic neuron. We provide examples of learning linear, non-linear and\nchaotic dynamics, as well as the dynamics of a two-link arm. Using the Lyapunov\nmethod, and under reasonable assumptions and approximations, we show that\nFOLLOW learning is stable uniformly, with the error going to zero\nasymptotically. \n\n"}
{"id": "1702.06971", "contents": "Title: Online Ranking with Constraints: A Primal-Dual Algorithm and\n  Applications to Web Traffic-Shaping Abstract: We study the online constrained ranking problem motivated by an application\nto web-traffic shaping: an online stream of sessions arrive in which, within\neach session, we are asked to rank items. The challenge involves optimizing the\nranking in each session so that local vs. global objectives are controlled:\nwithin each session one wishes to maximize a reward (local) while satisfying\ncertain constraints over the entire set of sessions (global). A typical\napplication of this setup is that of page optimization in a web portal. We wish\nto rank items so that not only is user engagement maximized in each session,\nbut also other business constraints (such as the number of views/clicks\ndelivered to various publishing partners) are satisfied.\n  We describe an online algorithm for performing this optimization. A novel\nelement of our approach is the use of linear programming duality and\nconnections to the celebrated Hungarian algorithm. This framework enables us to\ndetermine a set of \\emph{shadow prices} for each traffic-shaping constraint\nthat can then be used directly in the final ranking function to assign\nnear-optimal rankings. The (dual) linear program can be solved off-line\nperiodically to determine the prices. At serving time these prices are used as\nweights to compute weighted rank-scores for the items, and the simplicity of\nthe approach facilitates scalability to web applications. We provide rigorous\ntheoretical guarantees for the performance of our online algorithm and validate\nour approach using numerical experiments on real web-traffic data from a\nprominent internet portal. \n\n"}
{"id": "1702.07076", "contents": "Title: Data-Driven Fuzzy Modeling Using Deep Learning Abstract: Fuzzy modeling has many advantages over the non-fuzzy methods, such as\nrobustness against uncertainties and less sensitivity to the varying dynamics\nof nonlinear systems. Data-driven fuzzy modeling needs to extract fuzzy rules\nfrom the input/output data, and train the fuzzy parameters. This paper takes\nadvantages from deep learning, probability theory, fuzzy modeling, and extreme\nlearning machines. We use the restricted Boltzmann machine (RBM) and\nprobability theory to overcome some common problems in data based modeling\nmethods. The RBM is modified such that it can be trained with continuous\nvalues. A probability based clustering method is proposed to partition the\nhidden features from the RBM, and extract fuzzy rules with probability\nmeasurement. An extreme learning machine and an optimization method are applied\nto train the consequent part of the fuzzy rules and the probability parameters.\nThe proposed method is validated with two benchmark problems. \n\n"}
{"id": "1702.07125", "contents": "Title: Automatic Representation for Lifetime Value Recommender Systems Abstract: Many modern commercial sites employ recommender systems to propose relevant\ncontent to users. While most systems are focused on maximizing the immediate\ngain (clicks, purchases or ratings), a better notion of success would be the\nlifetime value (LTV) of the user-system interaction. The LTV approach considers\nthe future implications of the item recommendation, and seeks to maximize the\ncumulative gain over time. The Reinforcement Learning (RL) framework is the\nstandard formulation for optimizing cumulative successes over time. However, RL\nis rarely used in practice due to its associated representation, optimization\nand validation techniques which can be complex. In this paper we propose a new\narchitecture for combining RL with recommendation systems which obviates the\nneed for hand-tuned features, thus automating the state-space representation\nconstruction process. We analyze the practical difficulties in this formulation\nand test our solutions on batch off-line real-world recommendation data. \n\n"}
{"id": "1702.07241", "contents": "Title: Kalman Filter and its Modern Extensions for the Continuous-time\n  Nonlinear Filtering Problem Abstract: This paper is concerned with the filtering problem in continuous-time. Three\nalgorithmic solution approaches for this problem are reviewed: (i) the\nclassical Kalman-Bucy filter which provides an exact solution for the linear\nGaussian problem, (ii) the ensemble Kalman-Bucy filter (EnKBF) which is an\napproximate filter and represents an extension of the Kalman-Bucy filter to\nnonlinear problems, and (iii) the feedback particle filter (FPF) which\nrepresents an extension of the EnKBF and furthermore provides for an consistent\nsolution in the general nonlinear, non-Gaussian case. The common feature of the\nthree algorithms is the gain times error formula to implement the update step\n(to account for conditioning due to the observations) in the filter. In\ncontrast to the commonly used sequential Monte Carlo methods, the EnKBF and FPF\navoid the resampling of the particles in the importance sampling update step.\nMoreover, the feedback control structure provides for error correction\npotentially leading to smaller simulation variance and improved stability\nproperties. The paper also discusses the issue of non-uniqueness of the filter\nupdate formula and formulates a novel approximation algorithm based on ideas\nfrom optimal transport and coupling of measures. Performance of this and other\nalgorithms is illustrated for a numerical example. \n\n"}
{"id": "1702.07335", "contents": "Title: Approximately Optimal Continuous-Time Motion Planning and Control via\n  Probabilistic Inference Abstract: The problem of optimal motion planing and control is fundamental in robotics.\nHowever, this problem is intractable for continuous-time stochastic systems in\ngeneral and the solution is difficult to approximate if non-instantaneous\nnonlinear performance indices are present. In this work, we provide an\nefficient algorithm, PIPC (Probabilistic Inference for Planning and Control),\nthat yields approximately optimal policies with arbitrary higher-order\nnonlinear performance indices. Using probabilistic inference and a Gaussian\nprocess representation of trajectories, PIPC exploits the underlying sparsity\nof the problem such that its complexity scales linearly in the number of\nnonlinear factors. We demonstrate the capabilities of our algorithm in a\nreceding horizon setting with multiple systems in simulation. \n\n"}
{"id": "1702.07933", "contents": "Title: Efficient Learning of Mixed Membership Models Abstract: We present an efficient algorithm for learning mixed membership models when\nthe number of variables $p$ is much larger than the number of hidden components\n$k$. This algorithm reduces the computational complexity of state-of-the-art\ntensor methods, which require decomposing an $O\\left(p^3\\right)$ tensor, to\nfactorizing $O\\left(p/k\\right)$ sub-tensors each of size $O\\left(k^3\\right)$.\nIn addition, we address the issue of negative entries in the empirical method\nof moments based estimators. We provide sufficient conditions under which our\napproach has provable guarantees. Our approach obtains competitive empirical\nresults on both simulated and real data. \n\n"}
{"id": "1702.07948", "contents": "Title: Mapping Rule Estimation for Power Flow Analysis in Distribution Grids Abstract: The increasing integration of distributed energy resources (DERs) calls for\nnew monitoring and operational planning tools to ensure stability and\nsustainability in distribution grids. One idea is to use existing monitoring\ntools in transmission grids and some primary distribution grids. However, they\nusually depend on the knowledge of the system model, e.g., the topology and\nline parameters, which may be unavailable in primary and secondary distribution\ngrids. Furthermore, a utility usually has limited modeling ability of active\ncontrollers for solar panels as they may belong to a third party like\nresidential customers. To solve the modeling problem in traditional power flow\nanalysis, we propose a support vector regression (SVR) approach to reveal the\nmapping rules between different variables and recover useful variables based on\nphysical understanding and data mining. We illustrate the advantages of using\nthe SVR model over traditional regression method which finds line parameters in\ndistribution grids. Specifically, the SVR model is robust enough to recover the\nmapping rules while the regression method fails when 1) there are measurement\noutliers and missing data, 2) there are active controllers, or 3) measurements\nare only available at some part of a distribution grid. We demonstrate the\nsuperior performance of our method through extensive numerical validation on\ndifferent scales of distribution grids. \n\n"}
{"id": "1702.08169", "contents": "Title: Communication-efficient Algorithms for Distributed Stochastic Principal\n  Component Analysis Abstract: We study the fundamental problem of Principal Component Analysis in a\nstatistical distributed setting in which each machine out of $m$ stores a\nsample of $n$ points sampled i.i.d. from a single unknown distribution. We\nstudy algorithms for estimating the leading principal component of the\npopulation covariance matrix that are both communication-efficient and achieve\nestimation error of the order of the centralized ERM solution that uses all\n$mn$ samples. On the negative side, we show that in contrast to results\nobtained for distributed estimation under convexity assumptions, for the PCA\nobjective, simply averaging the local ERM solutions cannot guarantee error that\nis consistent with the centralized ERM. We show that this unfortunate phenomena\ncan be remedied by performing a simple correction step which correlates between\nthe individual solutions, and provides an estimator that is consistent with the\ncentralized ERM for sufficiently-large $n$. We also introduce an iterative\ndistributed algorithm that is applicable in any regime of $n$, which is based\non distributed matrix-vector products. The algorithm gives significant\nacceleration in terms of communication rounds over previous distributed\nalgorithms, in a wide regime of parameters. \n\n"}
{"id": "1702.08591", "contents": "Title: The Shattered Gradients Problem: If resnets are the answer, then what is\n  the question? Abstract: A long-standing obstacle to progress in deep learning is the problem of\nvanishing and exploding gradients. Although, the problem has largely been\novercome via carefully constructed initializations and batch normalization,\narchitectures incorporating skip-connections such as highway and resnets\nperform much better than standard feedforward architectures despite well-chosen\ninitialization and batch normalization. In this paper, we identify the\nshattered gradients problem. Specifically, we show that the correlation between\ngradients in standard feedforward networks decays exponentially with depth\nresulting in gradients that resemble white noise whereas, in contrast, the\ngradients in architectures with skip-connections are far more resistant to\nshattering, decaying sublinearly. Detailed empirical evidence is presented in\nsupport of the analysis, on both fully-connected networks and convnets.\nFinally, we present a new \"looks linear\" (LL) initialization that prevents\nshattering, with preliminary experiments showing the new initialization allows\nto train very deep networks without the addition of skip-connections. \n\n"}
{"id": "1702.08648", "contents": "Title: Auto-clustering Output Layer: Automatic Learning of Latent Annotations\n  in Neural Networks Abstract: In this paper, we discuss a different type of semi-supervised setting: a\ncoarse level of labeling is available for all observations but the model has to\nlearn a fine level of latent annotation for each one of them. Problems in this\nsetting are likely to be encountered in many domains such as text\ncategorization, protein function prediction, image classification as well as in\nexploratory scientific studies such as medical and genomics research. We\nconsider this setting as simultaneously performed supervised classification\n(per the available coarse labels) and unsupervised clustering (within each one\nof the coarse labels) and propose a novel output layer modification called\nauto-clustering output layer (ACOL) that allows concurrent classification and\nclustering based on Graph-based Activity Regularization (GAR) technique. As the\nproposed output layer modification duplicates the softmax nodes at the output\nlayer for each class, GAR allows for competitive learning between these\nduplicates on a traditional error-correction learning framework to ultimately\nenable a neural network to learn the latent annotations in this partially\nsupervised setup. We demonstrate how the coarse label supervision impacts\nperformance and helps propagate useful clustering information between\nsub-classes. Comparative tests on three of the most popular image datasets\nMNIST, SVHN and CIFAR-100 rigorously demonstrate the effectiveness and\ncompetitiveness of the proposed approach. \n\n"}
{"id": "1702.08820", "contents": "Title: Computing non-stationary $(s, S)$ policies using mixed integer linear\n  programming Abstract: This paper addresses the single-item single-stocking location stochastic lot\nsizing problem under the $(s, S) $ policy. We first present a mixed integer\nnon-linear programming (MINLP) formulation for determining near-optimal $(s,\nS)$ policy parameters. To tackle larger instances, we then combine the\npreviously introduced MINLP model and a binary search approach. These models\ncan be reformulated as mixed integer linear programming (MILP) models which can\nbe easily implemented and solved by using off-the-shelf optimisation software.\nComputational experiments demonstrate that optimality gaps of these models are\naround $0.3\\%$ of the optimal policy cost and computational times are\nreasonable. \n\n"}
{"id": "1703.00272", "contents": "Title: Incremental constraint projection methods for monotone stochastic\n  variational inequalities Abstract: We consider stochastic variational inequalities with monotone operators\ndefined as the expected value of a random operator. We assume the feasible set\nis the intersection of a large family of convex sets. We propose a method that\ncombines stochastic approximation with incremental constraint projections\nmeaning that at each iteration, a step similar to some variant of a\ndeterministic projection method is taken after the random operator is sampled\nand a component of the intersection defining the feasible set is chosen at\nrandom. Such sequential scheme is well suited for applications involving large\ndata sets, online optimization and distributed learning. First, we assume that\nthe variational inequality is weak-sharp. We provide asymptotic convergence,\nfeasibility rate of $O(1/k)$ in terms of the mean squared distance to the\nfeasible set and solvability rate of $O(1/\\sqrt{k})$ (up to first order\nlogarithmic terms) in terms of the mean distance to the solution set for a\nbounded or unbounded feasible set. Then, we assume just monotonicity of the\noperator and introduce an explicit iterative Tykhonov regularization to the\nmethod. We consider Cartesian variational inequalities so as to encompass the\ndistributed solution of stochastic Nash games or multi-agent optimization\nproblems under a limited coordination. We provide asymptotic convergence,\nfeasibility rate of $O(1/k)$ in terms of the mean squared distance to the\nfeasible set and, in the case of a compact set, we provide a near-optimal\nsolvability convergence rate of $O\\left(\\frac{k^\\delta\\ln k}{\\sqrt{k}}\\right)$\nin terms of the mean dual gap-function of the SVI for arbitrarily small\n$\\delta>0$. \n\n"}
{"id": "1703.00405", "contents": "Title: Stability and performance analysis of linear positive systems with\n  delays using input-output methods Abstract: It is known that input-output approaches based on scaled small-gain theorems\nwith constant $D$-scalings and integral linear constraints are non-conservative\nfor the analysis of some classes of linear positive systems interconnected with\nuncertain linear operators. This dramatically contrasts with the case of\ngeneral linear systems with delays where input-output approaches provide, in\ngeneral, sufficient conditions only. Using these results we provide simple\nalternative proofs for many of the existing results on the stability of linear\npositive systems with discrete/distributed/neutral time-invariant/-varying\ndelays and linear difference equations. In particular, we give a simple proof\nfor the characterization of diagonal Riccati stability for systems with\ndiscrete-delays and generalize this equation to other types of delay systems.\nThe fact that all those results can be reproved in a very simple way\ndemonstrates the importance and the efficiency of the input-output framework\nfor the analysis of linear positive systems. The approach is also used to\nderive performance results evaluated in terms of the $L_1$-, $L_2$- and\n$L_\\infty$-gains. It is also flexible enough to be used for design purposes. \n\n"}
{"id": "1703.00658", "contents": "Title: Time-varying Bang-bang Property of Minimal Controls for Approximately\n  Null-controllable Heat Equations Abstract: In this paper, optimal time control problems and optimal target control\nproblems are studied for the approximately null-controllable heat equations.\nCompared with the existed results on these problems, the boundary of control\nvariables are not constants but time varying functions. The time-varying\nbang-bang property for optimal time control problem, and an equivalence theorem\nfor optimal control problem and optimal target problem are obtained. \n\n"}
{"id": "1703.00956", "contents": "Title: A Laplacian Framework for Option Discovery in Reinforcement Learning Abstract: Representation learning and option discovery are two of the biggest\nchallenges in reinforcement learning (RL). Proto-value functions (PVFs) are a\nwell-known approach for representation learning in MDPs. In this paper we\naddress the option discovery problem by showing how PVFs implicitly define\noptions. We do it by introducing eigenpurposes, intrinsic reward functions\nderived from the learned representations. The options discovered from\neigenpurposes traverse the principal directions of the state space. They are\nuseful for multiple tasks because they are discovered without taking the\nenvironment's rewards into consideration. Moreover, different options act at\ndifferent time scales, making them helpful for exploration. We demonstrate\nfeatures of eigenpurposes in traditional tabular domains as well as in Atari\n2600 games. \n\n"}
{"id": "1703.02336", "contents": "Title: A scalable line-independent design algorithm for voltage and frequency\n  control in AC islanded microgrids Abstract: We propose a decentralized control synthesis procedure for stabilizing\nvoltage and frequency in AC Islanded microGrids (ImGs) composed of Distributed\nGeneration Units (DGUs) and loads interconnected through power lines. The\npresented approach enables Plug-and-Play (PnP) operations, meaning that DGUs\ncan be added or removed without compromising the overall ImG stability. The\nmain feature of our approach is that the proposed design algorithm is\nline-independent. This implies that (i) the synthesis of each local controller\nrequires only the parameters of the corresponding DGU and not the model of\npower lines connecting neighboring DGUs, and (ii) whenever a new DGU is plugged\nin, DGUs physically coupled with it do not have to retune their regulators\nbecause of the new power line connected to them. Moreover, we formally prove\nthat stabilizing local controllers can be always computed, independently of the\nelectrical parameters. Theoretical results are validated by simulating in PSCAD\nthe behavior of a 10-DGUs ImG. \n\n"}
{"id": "1703.03374", "contents": "Title: PMU-Based Estimation of Dynamic State Jacobian Matrix Abstract: In this paper, a hybrid measurement- and model-based method is proposed which\ncan estimate the dynamic state Jacobian matrix in near real-time. The proposed\nmethod is computationally efficient and robust to the variation of network\ntopology. A numerical example is given to show that the proposed method is able\nto provide good estimation for the dynamic state Jacobian matrix and is\nsuperior to the model-based method under undetectable network topology change.\nThe proposed method may also help identify big discrepancy in the assumed\nnetwork model. \n\n"}
{"id": "1703.03470", "contents": "Title: Deep Radial Kernel Networks: Approximating Radially Symmetric Functions\n  with Deep Networks Abstract: We prove that a particular deep network architecture is more efficient at\napproximating radially symmetric functions than the best known 2 or 3 layer\nnetworks. We use this architecture to approximate Gaussian kernel SVMs, and\nsubsequently improve upon them with further training. The architecture and\ninitial weights of the Deep Radial Kernel Network are completely specified by\nthe SVM and therefore sidesteps the problem of empirically choosing an\nappropriate deep network architecture. \n\n"}
{"id": "1703.03669", "contents": "Title: Special cases of pairwise comparisons matrices represented by Toeplitz\n  matrices Abstract: This study presents special cases of inconsistent pairwise comparisons PC\nmatrices and analysis of their eigenvalue-based inconsistency index using\nmathematical methods. All studied special cases of PC matrices are Toeplitz\nmatrices with only three different entries $1$, $x$, and $1/x$. A new type of\ncirculant pairwise comparisons matrix has been introduced. Although this class\nof PC matrices may be perceived as restricted, it is general enough to cover\nnumerous levels of eigenvalue-based inconsistency index from the lowest to the\nhighest. Both exact mathematical expressions and estimations, where the exact\nexpression was impossible to find, are provided \n\n"}
{"id": "1703.04664", "contents": "Title: Optimal Densification for Fast and Accurate Minwise Hashing Abstract: Minwise hashing is a fundamental and one of the most successful hashing\nalgorithm in the literature. Recent advances based on the idea of\ndensification~\\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown\nthat it is possible to compute $k$ minwise hashes, of a vector with $d$\nnonzeros, in mere $(d + k)$ computations, a significant improvement over the\nclassical $O(dk)$. These advances have led to an algorithmic improvement in the\nquery complexity of traditional indexing algorithms based on minwise hashing.\nUnfortunately, the variance of the current densification techniques is\nunnecessarily high, which leads to significantly poor accuracy compared to\nvanilla minwise hashing, especially when the data is sparse. In this paper, we\nprovide a novel densification scheme which relies on carefully tailored\n2-universal hashes. We show that the proposed scheme is variance-optimal, and\nwithout losing the runtime efficiency, it is significantly more accurate than\nexisting densification techniques. As a result, we obtain a significantly\nefficient hashing scheme which has the same variance and collision probability\nas minwise hashing. Experimental evaluations on real sparse and\nhigh-dimensional datasets validate our claims. We believe that given the\nsignificant advantages, our method will replace minwise hashing implementations\nin practice. \n\n"}
{"id": "1703.04703", "contents": "Title: Variational obstacle avoidance problem on Riemannian manifolds Abstract: We introduce variational obstacle avoidance problems on Riemannian manifolds\nand derive necessary conditions for the existence of their normal extremals.\nThe problem consists of minimizing an energy functional depending on the\nvelocity and covariant acceleration, among a set of admissible curves, and also\ndepending on a navigation function used to avoid an obstacle on the workspace,\na Riemannian manifold.\n  We study two different scenarios, a general one on a Riemannian manifold and,\na sub-Riemannian problem. By introducing a left-invariant metric on a Lie\ngroup, we also study the variational obstacle avoidance problem on a Lie group.\nWe apply the results to the obstacle avoidance problem of a planar rigid body\nand an unicycle. \n\n"}
{"id": "1703.04813", "contents": "Title: Learned Optimizers that Scale and Generalize Abstract: Learning to learn has emerged as an important direction for achieving\nartificial intelligence. Two of the primary barriers to its adoption are an\ninability to scale to larger problems and a limited ability to generalize to\nnew tasks. We introduce a learned gradient descent optimizer that generalizes\nwell to new tasks, and which has significantly reduced memory and computation\noverhead. We achieve this by introducing a novel hierarchical RNN architecture,\nwith minimal per-parameter overhead, augmented with additional architectural\nfeatures that mirror the known structure of optimization tasks. We also develop\na meta-training ensemble of small, diverse optimization tasks capturing common\nproperties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM\non problems in this corpus. More importantly, it performs comparably or better\nwhen applied to small convolutional neural networks, despite seeing no neural\nnetworks in its meta-training set. Finally, it generalizes to train Inception\nV3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps,\noptimization problems that are of a vastly different scale than those it was\ntrained on. We release an open source implementation of the meta-training\nalgorithm. \n\n"}
{"id": "1703.04933", "contents": "Title: Sharp Minima Can Generalize For Deep Nets Abstract: Despite their overwhelming capacity to overfit, deep learning architectures\ntend to generalize relatively well to unseen data, allowing them to be deployed\nin practice. However, explaining why this is the case is still an open area of\nresearch. One standing hypothesis that is gaining popularity, e.g. Hochreiter &\nSchmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the\nloss function found by stochastic gradient based methods results in good\ngeneralization. This paper argues that most notions of flatness are problematic\nfor deep models and can not be directly applied to explain generalization.\nSpecifically, when focusing on deep networks with rectifier units, we can\nexploit the particular geometry of parameter space induced by the inherent\nsymmetries that these architectures exhibit to build equivalent models\ncorresponding to arbitrarily sharper minima. Furthermore, if we allow to\nreparametrize a function, the geometry of its parameters can change drastically\nwithout affecting its generalization properties. \n\n"}
{"id": "1703.06114", "contents": "Title: Deep Sets Abstract: We study the problem of designing models for machine learning tasks defined\non \\emph{sets}. In contrast to traditional approach of operating on fixed\ndimensional vectors, we consider objective functions defined on sets that are\ninvariant to permutations. Such problems are widespread, ranging from\nestimation of population statistics \\cite{poczos13aistats}, to anomaly\ndetection in piezometer data of embankment dams \\cite{Jung15Exploration}, to\ncosmology \\cite{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem\ncharacterizes the permutation invariant functions and provides a family of\nfunctions to which any permutation invariant objective function must belong.\nThis family of functions has a special structure which enables us to design a\ndeep network architecture that can operate on sets and which can be deployed on\na variety of scenarios including both unsupervised and supervised learning\ntasks. We also derive the necessary and sufficient conditions for permutation\nequivariance in deep models. We demonstrate the applicability of our method on\npopulation statistic estimation, point cloud classification, set expansion, and\noutlier detection. \n\n"}
{"id": "1703.06273", "contents": "Title: Distributed Stochastic Model Predictive Control for Large-Scale Linear\n  Systems with Private and Common Uncertainty Sources Abstract: This paper presents a distributed stochastic model predictive control (SMPC)\napproach for large-scale linear systems with private and common uncertainties\nin a plug-and-play framework. Using the so-called scenario approach, the\ncentralized SMPC involves formulating a large-scale finite-horizon scenario\noptimization problem at each sampling time, which is in general computationally\ndemanding, due to the large number of required scenarios. We present two novel\nideas in this paper to address this issue. We first develop a technique to\ndecompose the large-scale scenario program into distributed scenario programs\nthat exchange a certain number of scenarios with each other in order to compute\nlocal decisions using the alternating direction method of multipliers (ADMM).\nWe show the exactness of the decomposition with a-priori probabilistic\nguarantees for the desired level of constraint fulfillment for both uncertainty\nsources. As our second contribution, we develop an inter-agent soft\ncommunication scheme based on a set parametrization technique together with the\nnotion of probabilistically reliable sets to reduce the required communication\nbetween the subproblems. We show how to incorporate the probabilistic\nreliability notion into existing results and provide new guarantees for the\ndesired level of constraint violations. Two different simulation studies of two\ntypes of systems interactions, dynamically coupled and coupling constraints,\nare presented to illustrate the advantages of the proposed framework. \n\n"}
{"id": "1703.06490", "contents": "Title: Generating Multi-label Discrete Patient Records using Generative\n  Adversarial Networks Abstract: Access to electronic health record (EHR) data has motivated computational\nadvances in medical research. However, various concerns, particularly over\nprivacy, can limit access to and collaborative use of EHR data. Sharing\nsynthetic EHR data could mitigate risk. In this paper, we propose a new\napproach, medical Generative Adversarial Network (medGAN), to generate\nrealistic synthetic patient records. Based on input real patient records,\nmedGAN can generate high-dimensional discrete variables (e.g., binary and count\nfeatures) via a combination of an autoencoder and generative adversarial\nnetworks. We also propose minibatch averaging to efficiently avoid mode\ncollapse, and increase the learning efficiency with batch normalization and\nshortcut connections. To demonstrate feasibility, we showed that medGAN\ngenerates synthetic patient records that achieve comparable performance to real\ndata on many experiments including distribution statistics, predictive modeling\ntasks and a medical expert review. We also empirically observe a limited\nprivacy risk in both identity and attribute disclosure using medGAN. \n\n"}
{"id": "1703.06807", "contents": "Title: Guaranteed Sufficient Decrease for Variance Reduced Stochastic Gradient\n  Descent Abstract: In this paper, we propose a novel sufficient decrease technique for variance\nreduced stochastic gradient descent methods such as SAG, SVRG and SAGA. In\norder to make sufficient decrease for stochastic optimization, we design a new\nsufficient decrease criterion, which yields sufficient decrease versions of\nvariance reduction algorithms such as SVRG-SD and SAGA-SD as a byproduct. We\nintroduce a coefficient to scale current iterate and satisfy the sufficient\ndecrease property, which takes the decisions to shrink, expand or move in the\nopposite direction, and then give two specific update rules of the coefficient\nfor Lasso and ridge regression. Moreover, we analyze the convergence properties\nof our algorithms for strongly convex problems, which show that both of our\nalgorithms attain linear convergence rates. We also provide the convergence\nguarantees of our algorithms for non-strongly convex problems. Our experimental\nresults further verify that our algorithms achieve significantly better\nperformance than their counterparts. \n\n"}
{"id": "1703.06853", "contents": "Title: Modulus consensus in discrete-time signed networks and properties of\n  special recurrent inequalities Abstract: Recently the dynamics of signed networks, where the ties among the agents can\nbe both positive (attractive) or negative (repulsive) have attracted\nsubstantial attention of the research community. Examples of such networks are\nmodels of opinion dynamics over signed graphs, recently introduced by Altafini\n(2012,2013) and extended to discrete-time case by Meng et al. (2014). It has\nbeen shown that under mild connectivity assumptions these protocols provide the\nconvergence of opinions in absolute value, whereas their signs may differ. This\n\"modulus consensus\" may correspond to the polarization of the opinions (or\nbipartite consensus, including the usual consensus as a special case), or their\nconvergence to zero. In this paper, we demonstrate that the phenomenon of\nmodulus consensus in the discrete-time Altafini model is a manifestation of a\nmore general and profound fact, regarding the solutions of a special recurrent\ninequality. Although such a recurrent inequality does not provide the\nuniqueness of a solution, it can be shown that, under some natural assumptions,\neach of its bounded solutions has a limit and, moreover, converges to\nconsensus. A similar property has previously been established for special\ncontinuous-time differential inequalities (Proskurnikov, Cao, 2016). Besides\nanalysis of signed networks, we link the consensus properties of recurrent\ninequalities to the convergence analysis of distributed optimization algorithms\nand the problems of Schur stability of substochastic matrices. \n\n"}
{"id": "1703.07612", "contents": "Title: Networked Systems under Denial-of-Service: Co-located vs. Remote Control\n  Architectures Abstract: In this paper, we study networked systems in the presence of\nDenial-of-Service (DoS) attacks, namely attacks that prevent transmissions over\nthe communication network. Previous studies have shown that co-located\narchitectures (control unit co-located with the actuators and networked sensor\nchannel) can ensure a high level of robustness against DoS. However,\nco-location requires a wired or dedicated actuator channel, which could not\nmeet flexibility and cost requirements. In this paper we consider a control\narchitecture that approximates co-location while enabling remote implementation\n(networked sensor and actuator channels). We analyze closed-loop stability and\nquantify the robustness \"gap\" between this architecture and the co-located one. \n\n"}
{"id": "1703.07698", "contents": "Title: Characterization of Deterministic and Probabilistic Sampling Patterns\n  for Finite Completability of Low Tensor-Train Rank Tensor Abstract: In this paper, we analyze the fundamental conditions for low-rank tensor\ncompletion given the separation or tensor-train (TT) rank, i.e., ranks of\nunfoldings. We exploit the algebraic structure of the TT decomposition to\nobtain the deterministic necessary and sufficient conditions on the locations\nof the samples to ensure finite completability. Specifically, we propose an\nalgebraic geometric analysis on the TT manifold that can incorporate the whole\nrank vector simultaneously in contrast to the existing approach based on the\nGrassmannian manifold that can only incorporate one rank component. Our\nproposed technique characterizes the algebraic independence of a set of\npolynomials defined based on the sampling pattern and the TT decomposition,\nwhich is instrumental to obtaining the deterministic condition on the sampling\npattern for finite completability. In addition, based on the proposed analysis,\nassuming that the entries of the tensor are sampled independently with\nprobability $p$, we derive a lower bound on the sampling probability $p$, or\nequivalently, the number of sampled entries that ensures finite completability\nwith high probability. Moreover, we also provide the deterministic and\nprobabilistic conditions for unique completability. \n\n"}
{"id": "1703.07704", "contents": "Title: Formal Methods for Adaptive Control of Dynamical Systems Abstract: We develop a method to control discrete-time systems with constant but\ninitially unknown parameters from linear temporal logic (LTL) specifications.\nWe introduce the notions of (non-deterministic) parametric and adaptive\ntransition systems and show how to use tools from formal methods to compute\nadaptive control strategies for finite systems. For infinite systems, we first\ncompute abstractions in the form of parametric finite quotient transition\nsystems and then apply the techniques for finite systems. Unlike traditional\nadaptive control methods, our approach is correct by design, does not require a\nreference model, and can deal with a much wider range of systems and\nspecifications. Illustrative case studies are included. \n\n"}
{"id": "1703.07955", "contents": "Title: Dimensional-invariance principles in coupled dynamical systems-- A\n  unified analysis and applications Abstract: In this paper we study coupled dynamical systems and investigate dimension\nproperties of the subspace spanned by solutions of each individual system.\nRelevant problems on \\textit{collinear dynamical systems} and their variations\nare discussed recently by Montenbruck et. al. in \\cite{collinear2017SCL}, while\nin this paper we aim to provide a unified analysis to derive the\ndimensional-invariance principles for networked coupled systems, and to\ngeneralize the invariance principles for networked systems with more general\nforms of coupling terms. To be specific, we consider two types of coupled\nsystems, one with scalar couplings and the other with matrix couplings. Via the\n\\textit{rank-preserving flow theory}, we show that any scalar-coupled dynamical\nsystem (with constant, time-varying or state-dependent couplings) possesses the\ndimensional-invariance principles, in that the dimension of the subspace\nspanned by the individual systems' solutions remains invariant. For coupled\ndynamical systems with matrix coefficients/couplings, necessary and sufficient\nconditions (for constant, time-varying and state-dependent couplings) are given\nto characterize dimensional-invariance principles. The proofs via a\nrank-preserving matrix flow theory in this paper simplify the analysis in\n\\cite{collinear2017SCL}, and we also extend the invariance principles to the\ncases of time-varying couplings and state-dependent couplings. Furthermore,\nsubspace-preserving property and signature-preserving flows are also developed\nfor coupled networked systems with particular coupling terms. These invariance\nprinciples provide insightful characterizations to analyze transient behaviors\nand solution evolutions for a large family of coupled systems, such as\nmulti-agent consensus dynamics, distributed coordination systems, formation\ncontrol systems, among others. \n\n"}
{"id": "1703.08240", "contents": "Title: Efficient regularization with wavelet sparsity constraints in PAT Abstract: In this paper we consider the reconstruction problem of photoacoustic\ntomography (PAT) with a flat observation surface. We develop a direct\nreconstruction method that employs regularization with wavelet sparsity\nconstraints. To that end, we derive a wavelet-vaguelette decomposition (WVD)\nfor the PAT forward operator and a corresponding explicit reconstruction\nformula in the case of exact data. In the case of noisy data, we combine the\nWVD reconstruction formula with soft-thresholding which yields a spatially\nadaptive estimation method. We demonstrate that our method is statistically\noptimal for white random noise if the unknown function is assumed to lie in any\nBesov-ball. We present generalizations of this approach and, in particular, we\ndiscuss the combination of vaguelette soft-thresholding with a TV prior. We\nalso provide an efficient implementation of the vaguelette transform that leads\nto fast image reconstruction algorithms supported by numerical results. \n\n"}
{"id": "1703.08976", "contents": "Title: Hybrid Filtering for a Class of Quantum Systems with Classical\n  Disturbances Abstract: A filtering problem for a class of quantum systems disturbed by a classical\nstochastic process is investigated in this paper. The classical disturbance\nprocess, which is assumed to be described by a linear stochastic differential\nequation, is modeled by a quantum cavity model. Then the hybrid\nquantum-classical system is described by a combined quantum system consisting\nof two quantum cavity subsystems. Quantum filtering theory and a quantum\nextended Kalman filter method are employed to estimate the states of the\ncombined quantum system. An estimate of the classical stochastic process is\nderived from the estimate of the combined quantum system. The effectiveness and\nperformance of the proposed methods are illustrated by numerical results. \n\n"}
{"id": "1703.09310", "contents": "Title: Adaptive Simulation-based Training of AI Decision-makers using Bayesian\n  Optimization Abstract: This work studies how an AI-controlled dog-fighting agent with tunable\ndecision-making parameters can learn to optimize performance against an\nintelligent adversary, as measured by a stochastic objective function evaluated\non simulated combat engagements. Gaussian process Bayesian optimization (GPBO)\ntechniques are developed to automatically learn global Gaussian Process (GP)\nsurrogate models, which provide statistical performance predictions in both\nexplored and unexplored areas of the parameter space. This allows a learning\nengine to sample full-combat simulations at parameter values that are most\nlikely to optimize performance and also provide highly informative data points\nfor improving future predictions. However, standard GPBO methods do not provide\na reliable surrogate model for the highly volatile objective functions found in\naerial combat, and thus do not reliably identify global maxima. These issues\nare addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point\nSampling (HRMS) techniques. Simulation studies show that HRMS improves the\naccuracy of GP surrogate models, allowing AI decision-makers to more accurately\npredict performance and efficiently tune parameters. \n\n"}
{"id": "1703.09877", "contents": "Title: Robust Consensus for Multi-Agent Systems Communicating over Stochastic\n  Uncertain Networks Abstract: In this paper, we study the robust consensus problem for a set of\ndiscrete-time linear agents to coordinate over an uncertain communication\nnetwork, which is to achieve consensus against the transmission errors and\nnoises resulted from the information exchange between the agents. We model the\nnetwork by means of communication links subject to multiplicative stochastic\nuncertainties, which are susceptible to describing packet dropout, random\ndelay, and fading phenomena. Different communication topologies, such as\nundirected graphs and leader-follower graphs, are considered. We derive\nsufficient conditions for robust consensus in the mean square sense. This\nresults unveil intrinsic constraints on consensus attainment imposed by the\nnetwork synchronizability, the unstable agent dynamics, and the channel\nuncertainty variances. Consensus protocols are designed based on the state\ninformation transmitted over the uncertain channels, by solving a modified\nalgebraic Riccati equation. \n\n"}
{"id": "1703.10211", "contents": "Title: Connections between Mean-Field Game and Social Welfare Optimization Abstract: This paper studies the connection between a class of mean-field games and a\nsocial welfare optimization problem. We consider a mean-field game in function\nspaces with a large population of agents, and each agent seeks to minimize an\nindividual cost function. The cost functions of different agents are coupled\nthrough a mean-field term that depends on the mean of the population states. We\nshow that although the mean-field game is not a potential game, under some mild\ncondition the $\\epsilon$-Nash equilibrium of the mean-field game coincides with\nthe optimal solution to a social welfare optimization problem, and this is true\neven when the individual cost functions are non-convex. The connection enables\nus to evaluate and promote the efficiency of the mean-field equilibrium. In\naddition, it also leads to several important implications on the existence,\nuniqueness, and computation of the mean-field equilibrium. Numerical results\nare presented to validate the solution, and examples are provided to show the\napplicability of the proposed approach. \n\n"}
{"id": "1704.01545", "contents": "Title: Stability and Frequency Regulation of Inverters with Capacitive Inertia Abstract: In this paper, we address the problem of stability and frequency regulation\nof a recently proposed inverter. In this type of inverter, the DC-side\ncapacitor emulates the inertia of a synchronous generator. First, we remodel\nthe dynamics from the electrical power perspective. Second, using this model,\nwe show that the system is stable if connected to a constant power load, and\nthe frequency can be regulated by a suitable choice of the controller. Next,\nand as the main focus of this paper, we analyze the stability of a network of\nthese inverters, and show that frequency regulation can be achieved by using an\nappropriate controller design. Finally, a numerical example is provided which\nillustrates the effectiveness of the method. \n\n"}
{"id": "1704.02038", "contents": "Title: Treatment-Response Models for Counterfactual Reasoning with\n  Continuous-time, Continuous-valued Interventions Abstract: Treatment effects can be estimated from observational data as the difference\nin potential outcomes. In this paper, we address the challenge of estimating\nthe potential outcome when treatment-dose levels can vary continuously over\ntime. Further, the outcome variable may not be measured at a regular frequency.\nOur proposed solution represents the treatment response curves using linear\ntime-invariant dynamical systems---this provides a flexible means for modeling\nresponse over time to highly variable dose curves. Moreover, for multivariate\ndata, the proposed method: uncovers shared structure in treatment response and\nthe baseline across multiple markers; and, flexibly models challenging\ncorrelation structure both across and within signals over time. For this, we\nbuild upon the framework of multiple-output Gaussian Processes. On simulated\nand a challenging clinical dataset, we show significant gains in accuracy over\nstate-of-the-art models. \n\n"}
{"id": "1704.02232", "contents": "Title: Rapid Mixing Swendsen-Wang Sampler for Stochastic Partitioned Attractive\n  Models Abstract: The Gibbs sampler is a particularly popular Markov chain used for learning\nand inference problems in Graphical Models (GMs). These tasks are\ncomputationally intractable in general, and the Gibbs sampler often suffers\nfrom slow mixing. In this paper, we study the Swendsen-Wang dynamics which is a\nmore sophisticated Markov chain designed to overcome bottlenecks that impede\nthe Gibbs sampler. We prove O(\\log n) mixing time for attractive binary\npairwise GMs (i.e., ferromagnetic Ising models) on stochastic partitioned\ngraphs having n vertices, under some mild conditions, including low temperature\nregions where the Gibbs sampler provably mixes exponentially slow. Our\nexperiments also confirm that the Swendsen-Wang sampler significantly\noutperforms the Gibbs sampler when they are used for learning parameters of\nattractive GMs. \n\n"}
{"id": "1704.03976", "contents": "Title: Virtual Adversarial Training: A Regularization Method for Supervised and\n  Semi-Supervised Learning Abstract: We propose a new regularization method based on virtual adversarial loss: a\nnew measure of local smoothness of the conditional label distribution given\ninput. Virtual adversarial loss is defined as the robustness of the conditional\nlabel distribution around each input data point against local perturbation.\nUnlike adversarial training, our method defines the adversarial direction\nwithout label information and is hence applicable to semi-supervised learning.\nBecause the directions in which we smooth the model are only \"virtually\"\nadversarial, we call our method virtual adversarial training (VAT). The\ncomputational cost of VAT is relatively low. For neural networks, the\napproximated gradient of virtual adversarial loss can be computed with no more\nthan two pairs of forward- and back-propagations. In our experiments, we\napplied VAT to supervised and semi-supervised learning tasks on multiple\nbenchmark datasets. With a simple enhancement of the algorithm based on the\nentropy minimization principle, our VAT achieves state-of-the-art performance\nfor semi-supervised learning tasks on SVHN and CIFAR-10. \n\n"}
{"id": "1704.04163", "contents": "Title: Spectrum Approximation Beyond Fast Matrix Multiplication: Algorithms and\n  Hardness Abstract: Understanding the singular value spectrum of a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ is a fundamental task in countless applications. In matrix\nmultiplication time, it is possible to perform a full SVD and directly compute\nthe singular values $\\sigma_1,...,\\sigma_n$. However, little is known about\nalgorithms that break this runtime barrier.\n  Using tools from stochastic trace estimation, polynomial approximation, and\nfast system solvers, we show how to efficiently isolate different ranges of\n$A$'s spectrum and approximate the number of singular values in these ranges.\nWe thus effectively compute a histogram of the spectrum, which can stand in for\nthe true singular values in many applications.\n  We use this primitive to give the first algorithms for approximating a wide\nclass of symmetric matrix norms in faster than matrix multiplication time. For\nexample, we give a $(1 + \\epsilon)$ approximation algorithm for the\nSchatten-$1$ norm (the nuclear norm) running in just $\\tilde O((nnz(A)n^{1/3} +\nn^2)\\epsilon^{-3})$ time for $A$ with uniform row sparsity or $\\tilde\nO(n^{2.18} \\epsilon^{-3})$ time for dense matrices. The runtime scales smoothly\nfor general Schatten-$p$ norms, notably becoming $\\tilde O (p \\cdot nnz(A)\n\\epsilon^{-3})$ for any $p \\ge 2$.\n  At the same time, we show that the complexity of spectrum approximation is\ninherently tied to fast matrix multiplication in the small $\\epsilon$ regime.\nWe prove that achieving milder $\\epsilon$ dependencies in our algorithms would\nimply faster than matrix multiplication time triangle detection for general\ngraphs. This further implies that highly accurate algorithms running in\nsubcubic time yield subcubic time matrix multiplication. As an application of\nour bounds, we show that precisely computing all effective resistances in a\ngraph in less than matrix multiplication time is likely difficult, barring a\nmajor algorithmic breakthrough. \n\n"}
{"id": "1704.04792", "contents": "Title: Locating Power Flow Solution Space Boundaries: A Numerical Polynomial\n  Homotopy Approach Abstract: The solution space of any set of power flow equations may contain different\nnumber of real-valued solutions. The boundaries that separate these regions are\nreferred to as power flow solution space boundaries. Knowledge of these\nboundaries is important as they provide a measure for voltage stability.\nTraditionally, continuation based methods have been employed to compute these\nboundaries on the basis of initial guesses for the solution. However, with\nrapid growth of renewable energy sources these boundaries will be increasingly\naffected by variable parameters such as penetration levels, locations of the\nrenewable sources, and voltage set-points, making it difficult to generate an\ninitial guess that can guarantee all feasible solutions for the power flow\nproblem. In this paper we solve this problem by applying a numerical polynomial\nhomotopy based continuation method. The proposed method guarantees to find all\nsolution boundaries within a given parameter space up to a chosen level of\ndiscretization, independent of any initial guess. Power system operators can\nuse this computational tool conveniently to plan the penetration levels of\nrenewable sources at different buses. We illustrate the proposed method through\nsimulations on 3-bus and 10-bus power system examples with renewable\ngeneration. \n\n"}
{"id": "1704.05566", "contents": "Title: Simultaneous Policy Learning and Latent State Inference for Imitating\n  Driver Behavior Abstract: In this work, we propose a method for learning driver models that account for\nvariables that cannot be observed directly. When trained on a synthetic\ndataset, our models are able to learn encodings for vehicle trajectories that\ndistinguish between four distinct classes of driver behavior. Such encodings\nare learned without any knowledge of the number of driver classes or any\nobjective that directly requires the models to learn encodings for each class.\nWe show that driving policies trained with knowledge of latent variables are\nmore effective than baseline methods at imitating the driver behavior that they\nare trained to replicate. Furthermore, we demonstrate that the actions chosen\nby our policy are heavily influenced by the latent variable settings that are\nprovided to them. \n\n"}
{"id": "1704.06209", "contents": "Title: ADMM Penalty Parameter Selection by Residual Balancing Abstract: Appropriate selection of the penalty parameter is crucial to obtaining good\nperformance from the Alternating Direction Method of Multipliers (ADMM). While\nanalytic results for optimal selection of this parameter are very limited,\nthere is a heuristic method that appears to be relatively successful in a\nnumber of different problems. The contribution of this paper is to demonstrate\nthat their is a potentially serious flaw in this heuristic approach, and to\npropose a modification that at least partially addresses it. \n\n"}
{"id": "1704.06325", "contents": "Title: Trajectory Planning Under Vehicle Dimension Constraints Using Sequential\n  Linear Programming Abstract: This paper presents a spatial-based trajectory planning method for automated\nvehicles under actuator, obstacle avoidance, and vehicle dimension constraints.\nStarting from a nonlinear kinematic bicycle model, vehicle dynamics are\ntransformed to a road-aligned coordinate frame with path along the road\ncenterline replacing time as the dependent variable. Space-varying vehicle\ndimension constraints are linearized around a reference path to pose convex\noptimization problems. Such constraints do not require to inflate obstacles by\nsafety-margins and therefore maximize performance in very constrained\nenvironments. A sequential linear programming (SLP) algorithm is motivated. A\nlinear program (LP) is solved at each SLP-iteration. The relation between LP\nformulation and maximum admissible traveling speeds within vehicle tire\nfriction limits is discussed. The proposed method is evaluated in a roomy and\nin a tight maneuvering driving scenario, whereby a comparison to a\nsemi-analytical clothoid-based path planner is given. Effectiveness is\ndemonstrated particularly for very constrained environments, requiring to\naccount for constraints and planning over the entire obstacle constellation\nspace. \n\n"}
{"id": "1705.00607", "contents": "Title: Determinantal Point Processes for Mini-Batch Diversification Abstract: We study a mini-batch diversification scheme for stochastic gradient descent\n(SGD). While classical SGD relies on uniformly sampling data points to form a\nmini-batch, we propose a non-uniform sampling scheme based on the Determinantal\nPoint Process (DPP). The DPP relies on a similarity measure between data points\nand gives low probabilities to mini-batches which contain redundant data, and\nhigher probabilities to mini-batches with more diverse data. This\nsimultaneously balances the data and leads to stochastic gradients with lower\nvariance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show\nthat regular SGD and a biased version of stratified sampling emerge as special\ncases. Furthermore, DM-SGD generalizes stratified sampling to cases where no\ndiscrete features exist to bin the data into groups. We show experimentally\nthat our method results more interpretable and diverse features in unsupervised\nsetups, and in better classification accuracies in supervised setups. \n\n"}
{"id": "1705.01079", "contents": "Title: Analysis, simulation and optimal control of a SEIR model for Ebola virus\n  with demographic effects Abstract: Ebola virus is one of the most virulent pathogens for humans. We present a\nmathematical description of different Susceptible-Exposed-Infectious-Recovered\n(SEIR) models. By using mathematical modeling and analysis, the latest major\noutbreak of Ebola virus in West Africa is described. Our aim is to study and\ndiscuss the properties of SEIR models with respect to Ebola virus, the\ninformation they provide, and when the models make sense. We added to the basic\nSEIR model demographic effects in order to analyze the equilibria with vital\ndynamics. Numerical simulations confirm the theoretical analysis. The control\nof the propagation of the virus through vaccination is investigated and the\ncase study of Liberia is discussed in detail. \n\n"}
{"id": "1705.01471", "contents": "Title: Active Sampling for Constrained Simulation-based Verification of\n  Uncertain Nonlinear Systems Abstract: Increasingly demanding performance requirements for dynamical systems\nmotivates the adoption of nonlinear and adaptive control techniques. One\nchallenge is the nonlinearity of the resulting closed-loop system complicates\nverification that the system does satisfy the requirements at all possible\noperating conditions. This paper presents a data-driven procedure for efficient\nsimulation-based, statistical verification without the reliance upon exhaustive\nsimulations. In contrast to previous work, this approach introduces a method\nfor online estimation of prediction accuracy without the use of external\nvalidation sets. This work also develops a novel active sampling algorithm that\niteratively selects additional training points in order to maximize the\naccuracy of the predictions while still limited to a sample budget. Three case\nstudies demonstrate the utility of the new approach and the results show up to\na 50% improvement over state-of-the-art techniques. \n\n"}
{"id": "1705.01482", "contents": "Title: An Incentive-Based Online Optimization Framework for Distribution Grids Abstract: This paper formulates a time-varying social-welfare maximization problem for\ndistribution grids with distributed energy resources (DERs) and develops online\ndistributed algorithms to identify (and track) its solutions. In the considered\nsetting, network operator and DER-owners pursue given operational and economic\nobjectives, while concurrently ensuring that voltages are within prescribed\nlimits. The proposed algorithm affords an online implementation to enable\ntracking of the solutions in the presence of time-varying operational\nconditions and changing optimization objectives. It involves a strategy where\nthe network operator collects voltage measurements throughout the feeder to\nbuild incentive signals for the DER-owners in real time; DERs then adjust the\ngenerated/consumed powers in order to avoid the violation of the voltage\nconstraints while maximizing given objectives. The stability of the proposed\nschemes is analytically established and numerically corroborated. \n\n"}
{"id": "1705.01849", "contents": "Title: Adaptive Pressure Control for Use in Variable-Thrust Rocket Development Abstract: The precise control of gas generator pressure is important to obtain variable\nthrust in ducted rockets. A delay resistant closed loop reference model\nadaptive control is proposed in this paper to address this problem. The\nproposed controller combines delay compensation and adaptation with improved\ntransient response. The controller is successfully implemented using an\nindustrial grade cold air test setup, which is a milestone towards obtaining a\nfully developed throttleable rocket gas generator controller. Simulation and\nexperimental comparisons with alternative adaptive approaches and a fixed\ncontroller demonstrate improved performance and effective handling of time\ndelays and uncertainties. A step by step design methodology, covering\nrobustfying schemes, selection of adaptation rates and initial controller\nparameters, is also provided to facilitate implementations. \n\n"}
{"id": "1705.02036", "contents": "Title: Approximate Nash Equilibria in Partially Observed Stochastic Games with\n  Mean-Field Interactions Abstract: Establishing the existence of Nash equilibria for partially observed\nstochastic dynamic games is known to be quite challenging, with the\ndifficulties stemming from the noisy nature of the measurements available to\nindividual players (agents) and the decentralized nature of this information.\nWhen the number of players is sufficiently large and the interactions among\nagents is of the mean-field type, one way to overcome this challenge is to\ninvestigate the infinite-population limit of the problem, which leads to a\nmean-field game. In this paper, we consider discrete-time partially observed\nmean-field games with infinite-horizon discounted cost criteria. Using the\ntechnique of converting the original partially observed stochastic control\nproblem to a fully observed one on the belief space and the dynamic programming\nprinciple, we establish the existence of Nash equilibria for these game models\nunder very mild technical conditions. Then, we show that the mean-field\nequilibrium policy, when adopted by each agent, forms an approximate Nash\nequilibrium for games with sufficiently many agents. \n\n"}
{"id": "1705.03805", "contents": "Title: Smart Routing of Electric Vehicles for Load Balancing in Smart Grids Abstract: Electric vehicles (EVs) are expected to be a major component of the smart\ngrid. The rapid proliferation of EVs will introduce an unprecedented load on\nthe existing electric grid due to the charging/discharging behavior of the EVs,\nthus motivating the need for novel approaches for routing EVs across the grid.\nIn this paper, a novel gametheoretic framework for smart routing of EVs within\nthe smart grid is proposed. The goal of this framework is to balance the\nelectricity load across the grid while taking into account the traffic\ncongestion and the waiting time at charging stations. The EV routing problem is\nformulated as a noncooperative game. For this game, it is shown that selfish\nbehavior of EVs will result in a pure-strategy Nash equilibrium with the price\nof anarchy upper bounded by the variance of the ground load induced by the\nresidential, industrial, or commercial users. Moreover, the results are\nextended to capture the stochastic nature of induced ground load as well as the\nsubjective behavior of the owners of EVs as captured by using notions from the\nbehavioral framework of prospect theory. Simulation results provide new\ninsights on more efficient energy pricing at charging stations and under more\nrealistic grid conditions. \n\n"}
{"id": "1705.04031", "contents": "Title: Power System State Estimation via Feasible Point Pursuit: Algorithms and\n  Cramer-Rao Bound Abstract: Accurately monitoring the system's operating point is central to the reliable\nand economic operation of an electric power grid. Power system state estimation\n(PSSE) aims to obtain complete voltage magnitude and angle information at each\nbus given a number of system variables at selected buses and lines. Power flow\nanalysis is a special case of PSSE, and amounts to solving a set of noise-free\npower flow equations. Physical laws dictate quadratic relationships between\navailable quantities and unknown voltages, rendering general instances of power\nflow and PSSE nonconvex and NP-hard. Past approaches are largely based on\ngradient-type iterative procedures or semidefinite relaxation (SDR). Due to\nnonconvexity, the solution obtained via gradient-type schemes depends on\ninitialization, while SDR methods do not perform as desired in challenging\nscenarios. This paper puts forth novel \\emph{feasible point pursuit}\n(FPP)-based solvers for power flow and PSSE, which iteratively seek feasible\nsolutions for a nonconvex quadratically constrained quadratic programming\n(QCQP) reformulation of the weighted least-squares (WLS) problem. Relative to\nthe prior art, the developed solvers offer superior performance at the cost of\nhigher complexity. Furthermore, they converge to a stationary point of the WLS\nproblem. As a baseline for comparing different estimators, the Cram{\\' e}r-Rao\nlower bound (CRLB) is derived for the fundamental PSSE problem in this paper.\nJudicious numerical tests on several IEEE benchmark systems showcase markedly\nimproved performance of our FPP-based solvers for both power flow and PSSE\ntasks over popular WLS-based Gauss-Newton iterations and SDR approaches. \n\n"}
{"id": "1705.06671", "contents": "Title: Efficient optimization of the quantum relative entropy Abstract: Many quantum information measures can be written as an optimization of the\nquantum relative entropy between sets of states. For example, the relative\nentropy of entanglement of a state is the minimum relative entropy to the set\nof separable states. The various capacities of quantum channels can also be\nwritten in this way. We propose a unified framework to numerically compute\nthese quantities using off-the-shelf semidefinite programming solvers,\nexploiting the approximation method proposed in [Fawzi, Saunderson, Parrilo,\nSemidefinite approximations of the matrix logarithm, arXiv:1705.00812]. As a\nnotable application, this method allows us to provide numerical counterexamples\nfor a proposed lower bound on the quantum conditional mutual information in\nterms of the relative entropy of recovery. \n\n"}
{"id": "1705.07262", "contents": "Title: Batch Reinforcement Learning on the Industrial Benchmark: First\n  Experiences Abstract: The Particle Swarm Optimization Policy (PSO-P) has been recently introduced\nand proven to produce remarkable results on interacting with academic\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\nfurther investigate the properties and feasibility on real-world applications,\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\nincluding a variety of aspects found in industrial applications, like\ncontinuous state and action spaces, a high dimensional, partially observable\nstate space, delayed effects, and complex stochasticity. The experimental\nresults of PSO-P on IB are compared to results of closed-form control policies\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\nonly of interest for academic benchmarks, but also for real-world industrial\napplications, since it also yielded the best performing policy in our IB\nsetting. Compared to other well established RL techniques, PSO-P produced\noutstanding results in performance and robustness, requiring only a relatively\nlow amount of effort in finding adequate parameters or making complex design\ndecisions. \n\n"}
{"id": "1705.07445", "contents": "Title: Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep\n  Reinforcement Learning Abstract: Reinforcement Learning (RL) can model complex behavior policies for\ngoal-directed sequential decision making tasks. A hallmark of RL algorithms is\nTemporal Difference (TD) learning: value function for the current state is\nmoved towards a bootstrapped target that is estimated using next state's value\nfunction. $\\lambda$-returns generalize beyond 1-step returns and strike a\nbalance between Monte Carlo and TD learning methods. While lambda-returns have\nbeen extensively studied in RL, they haven't been explored a lot in Deep RL.\nThis paper's first contribution is an exhaustive benchmarking of\nlambda-returns. Although mathematically tractable, the use of exponentially\ndecaying weighting of n-step returns based targets in lambda-returns is a\nrather ad-hoc design choice. Our second major contribution is that we propose a\ngeneralization of lambda-returns called Confidence-based Autodidactic Returns\n(CAR), wherein the RL agent learns the weighting of the n-step returns in an\nend-to-end manner. This allows the agent to learn to decide how much it wants\nto weigh the n-step returns based targets. In contrast, lambda-returns restrict\nRL agents to use an exponentially decaying weighting scheme. Autodidactic\nreturns can be used for improving any RL algorithm which uses TD learning. We\nempirically demonstrate that using sophisticated weighted mixtures of\nmulti-step returns (like CAR and lambda-returns) considerably outperforms the\nuse of n-step returns. We perform our experiments on the Asynchronous Advantage\nActor Critic (A3C) algorithm in the Atari 2600 domain. \n\n"}
{"id": "1705.07904", "contents": "Title: Semantically Decomposing the Latent Spaces of Generative Adversarial\n  Networks Abstract: We propose a new algorithm for training generative adversarial networks that\njointly learns latent codes for both identities (e.g. individual humans) and\nobservations (e.g. specific photographs). By fixing the identity portion of the\nlatent codes, we can generate diverse images of the same subject, and by fixing\nthe observation portion, we can traverse the manifold of subjects while\nmaintaining contingent aspects such as lighting and pose. Our algorithm\nfeatures a pairwise training scheme in which each sample from the generator\nconsists of two images with a common identity code. Corresponding samples from\nthe real dataset consist of two distinct photographs of the same subject. In\norder to fool the discriminator, the generator must produce pairs that are\nphotorealistic, distinct, and appear to depict the same individual. We augment\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\npairwise training. Experiments with human judges and an off-the-shelf face\nverification system demonstrate our algorithm's ability to generate convincing,\nidentity-matched photographs. \n\n"}
{"id": "1705.08161", "contents": "Title: Computational Methods for Path-based Robust Flows Abstract: Real world networks are often subject to severe uncertainties which need to\nbe addressed by any reliable prescriptive model. In the context of the maximum\nflow problem subject to arc failure, robust models have gained particular\nattention. For a path-based model, the resulting optimization problem is\nassumed to be difficult in the literature, yet the complexity status is widely\nunknown. We present a computational approach to solve the robust flow problem\nto optimality by simultaneous primal and dual separation, the practical\nefficacy of which is shown by a computational study.\n  Furthermore, we introduce a novel model of robust flows which provides a\ncompromise between stochastic and robust optimization by assigning\nprobabilities to groups of scenarios. The new model can be solved by the same\ncomputational techniques as the robust model. A bound on the generalization\nerror is proven for the case that the probabilities are determined empirically.\nThe suggested model as well as the computational approach extend to linear\noptimization problems more general than robust flows. \n\n"}
{"id": "1705.08494", "contents": "Title: Asynchronous Coordinate Descent under More Realistic Assumptions Abstract: Asynchronous-parallel algorithms have the potential to vastly speed up\nalgorithms by eliminating costly synchronization. However, our understanding to\nthese algorithms is limited because the current convergence of asynchronous\n(block) coordinate descent algorithms are based on somewhat unrealistic\nassumptions. In particular, the age of the shared optimization variables being\nused to update a block is assumed to be independent of the block being updated.\nAlso, it is assumed that the updates are applied to randomly chosen blocks. In\nthis paper, we argue that these assumptions either fail to hold or will imply\nless efficient implementations. We then prove the convergence of\nasynchronous-parallel block coordinate descent under more realistic\nassumptions, in particular, always without the independence assumption. The\nanalysis permits both the deterministic (essentially) cyclic and random rules\nfor block choices. Because a bound on the asynchronous delays may or may not be\navailable, we establish convergence for both bounded delays and unbounded\ndelays. The analysis also covers nonconvex, weakly convex, and strongly convex\nfunctions. We construct Lyapunov functions that directly model both objective\nprogress and delays, so delays are not treated errors or noise. A\ncontinuous-time ODE is provided to explain the construction at a high level. \n\n"}
{"id": "1705.08818", "contents": "Title: A hierarchical multirate MPC scheme for interconnected systems -\n  $\\textit{extended version}$ Abstract: This paper presents a hierarchical control scheme for interconnected linear\nsystems. At the higher layer of the control structure a robust centralized\nModel Predictive Control (MPC) algorithm based on a reduced order dynamic model\nof the overall system optimizes a long-term performance index penalizing the\ndeviation of the state and the control input from their nominal values. At the\nlower layer local MPC regulators, possibly working at different rates, are\ndesigned for the full order models of the subsystems to refine the control\naction computed at the higher layer. A simulation experiment is presented to\ndescribe the implementation aspects and the potentialities of the proposed\napproach. \n\n"}
{"id": "1705.08870", "contents": "Title: PaToPa: A Data-Driven Parameter and Topology Joint Estimation Framework\n  in Distribution Grids Abstract: The increasing integration of distributed energy resources (DERs) calls for\nnew planning and operational tools. However, such tools depend on system\ntopology and line parameters, which may be missing or inaccurate in\ndistribution grids. With abundant data, one idea is to use linear regression to\nfind line parameters, based on which topology can be identified. Unfortunately,\nthe linear regression method is accurate only if there is no noise in both the\ninput measurements (e.g., voltage magnitude and phase angle) and output\nmeasurements (e.g., active and reactive power). For topology estimation, even\nwith a small error in measurements, the regression-based method is incapable of\nfinding the topology using non-zero line parameters with a proper metric. To\nmodel input and output measurement errors simultaneously, we propose the\nerror-in-variables (EIV) model in a maximum likelihood estimation (MLE)\nframework for joint line parameter and topology estimation. While directly\nsolving the problem is NP-hard, we successfully adapt the problem into a\ngeneralized low-rank approximation problem via variable transformation and\nnoise decorrelation. For accurate topology estimation, we let it interact with\nparameter estimation in a fashion that is similar to expectation-maximization\nfashion in machine learning. The proposed PaToPa approach does not require a\nradial network setting and works for mesh networks. We demonstrate the superior\nperformance in accuracy for our method on IEEE test cases with actual feeder\ndata from South California Edison. \n\n"}
{"id": "1705.10757", "contents": "Title: A Multi-Layer K-means Approach for Multi-Sensor Data Pattern Recognition\n  in Multi-Target Localization Abstract: Data-target association is an important step in multi-target localization for\nthe intelligent operation of un- manned systems in numerous applications such\nas search and rescue, traffic management and surveillance. The objective of\nthis paper is to present an innovative data association learning approach named\nmulti-layer K-means (MLKM) based on leveraging the advantages of some existing\nmachine learning approaches, including K-means, K-means++, and deep neural\nnetworks. To enable the accurate data association from different sensors for\nefficient target localization, MLKM relies on the clustering capabilities of\nK-means++ structured in a multi-layer framework with the error correction\nfeature that is motivated by the backpropogation that is well-known in deep\nlearning research. To show the effectiveness of the MLKM method, numerous\nsimulation examples are conducted to compare its performance with K-means,\nK-means++, and deep neural networks. \n\n"}
{"id": "1705.10786", "contents": "Title: Semi-Supervised Learning for Detecting Human Trafficking Abstract: Human trafficking is one of the most atrocious crimes and among the\nchallenging problems facing law enforcement which demands attention of global\nmagnitude. In this study, we leverage textual data from the website \"Backpage\"-\nused for classified advertisement- to discern potential patterns of human\ntrafficking activities which manifest online and identify advertisements of\nhigh interest to law enforcement. Due to the lack of ground truth, we rely on a\nhuman analyst from law enforcement, for hand-labeling a small portion of the\ncrawled data. We extend the existing Laplacian SVM and present S3VM-R, by\nadding a regularization term to exploit exogenous information embedded in our\nfeature space in favor of the task at hand. We train the proposed method using\nlabeled and unlabeled data and evaluate it on a fraction of the unlabeled data,\nherein referred to as unseen data, with our expert's further verification.\nResults from comparisons between our method and other semi-supervised and\nsupervised approaches on the labeled data demonstrate that our learner is\neffective in identifying advertisements of high interest to law enforcement \n\n"}
{"id": "1705.10883", "contents": "Title: Optimization of Tree Ensembles Abstract: Tree ensemble models such as random forests and boosted trees are among the\nmost widely used and practically successful predictive models in applied\nmachine learning and business analytics. Although such models have been used to\nmake predictions based on exogenous, uncontrollable independent variables, they\nare increasingly being used to make predictions where the independent variables\nare controllable and are also decision variables. In this paper, we study the\nproblem of tree ensemble optimization: given a tree ensemble that predicts some\ndependent variable using controllable independent variables, how should we set\nthese variables so as to maximize the predicted value? We formulate the problem\nas a mixed-integer optimization problem. We theoretically examine the strength\nof our formulation, provide a hierarchy of approximate formulations with bounds\non approximation quality and exploit the structure of the problem to develop\ntwo large-scale solution methods, one based on Benders decomposition and one\nbased on iteratively generating tree split constraints. We test our methodology\non real data sets, including two case studies in drug design and customized\npricing, and show that our methodology can efficiently solve large-scale\ninstances to near or full optimality, and outperforms solutions obtained by\nheuristic approaches. In our drug design case, we show how our approach can\nidentify compounds that efficiently trade-off predicted performance and novelty\nwith respect to existing, known compounds. In our customized pricing case, we\nshow how our approach can efficiently determine optimal store-level prices\nunder a random forest model that delivers excellent predictive accuracy. \n\n"}
{"id": "1706.00234", "contents": "Title: Optimality conditions for minimizers at infinity in polynomial\n  programming Abstract: In this paper we study necessary optimality conditions for the optimization\nproblem $$\\textrm{infimum}f_0(x) \\quad \\textrm{ subject to } \\quad x \\in S,$$\nwhere $f_0 \\colon \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a polynomial function\nand $S \\subset \\mathbb{R}^n$ is a set defined by polynomial inequalities.\nAssume that the problem is bounded below and has the Mangasarian--Fromovitz\nproperty at infinity. We first show that if the problem does {\\em not} have an\noptimal solution, then a version at infinity of the Fritz-John optimality\nconditions holds. From this we derive a version at infinity of the\nKarush--Kuhn--Tucker optimality conditions. As applications, we obtain a\nFrank--Wolfe type theorem which states that the optimal solution set of the\nproblem is nonempty provided the objective function $f_0$ is convenient.\nFinally, in the unconstrained case, we show that the optimal value of the\nproblem is the smallest critical value of some polynomial. All the results are\npresented in terms of the Newton polyhedra of the polynomials defining the\nproblem. \n\n"}
{"id": "1706.00550", "contents": "Title: On Unifying Deep Generative Models Abstract: Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques. \n\n"}
{"id": "1706.01628", "contents": "Title: Optimal Attack against Cyber-Physical Control Systems with Reactive\n  Attack Mitigation Abstract: This paper studies the performance and resilience of a cyber-physical control\nsystem (CPCS) with attack detection and reactive attack mitigation. It\naddresses the problem of deriving an optimal sequence of false data injection\nattacks that maximizes the state estimation error of the system. The results\nprovide basic understanding about the limit of the attack impact. The design of\nthe optimal attack is based on a Markov decision process (MDP) formulation,\nwhich is solved efficiently using the value iteration method. Using the\nproposed framework, we quantify the effect of false positives and\nmis-detections on the system performance, which can help the joint design of\nthe attack detection and mitigation. To demonstrate the use of the proposed\nframework in a real-world CPCS, we consider the voltage control system of power\ngrids, and run extensive simulations using PowerWorld, a high-fidelity power\nsystem simulator, to validate our analysis. The results show that by carefully\ndesigning the attack sequence using our proposed approach, the attacker can\ncause a large deviation of the bus voltages from the desired setpoint. Further,\nthe results verify the optimality of the derived attack sequence and show that,\nto cause maximum impact, the attacker must carefully craft his attack to strike\na balance between the attack magnitude and stealthiness, due to the\nsimultaneous presence of attack detection and mitigation. \n\n"}
{"id": "1706.02869", "contents": "Title: Adaptive Consensus ADMM for Distributed Optimization Abstract: The alternating direction method of multipliers (ADMM) is commonly used for\ndistributed model fitting problems, but its performance and reliability depend\nstrongly on user-defined penalty parameters. We study distributed ADMM methods\nthat boost performance by using different fine-tuned algorithm parameters on\neach worker node. We present a O(1/k) convergence rate for adaptive ADMM\nmethods with node-specific parameters, and propose adaptive consensus ADMM\n(ACADMM), which automatically tunes parameters without user oversight. \n\n"}
{"id": "1706.03041", "contents": "Title: Learning optimal wavelet bases using a neural network approach Abstract: A novel method for learning optimal, orthonormal wavelet bases for\nrepresenting 1- and 2D signals, based on parallels between the wavelet\ntransform and fully connected artificial neural networks, is described. The\nstructural similarities between these two concepts are reviewed and combined to\na \"wavenet\", allowing for the direct learning of optimal wavelet filter\ncoefficient through stochastic gradient descent with back-propagation over\nensembles of training inputs, where conditions on the filter coefficients for\nconstituting orthonormal wavelet bases are cast as quadratic regularisations\nterms. We describe the practical implementation of this method, and study its\nperformance for high-energy physics collision events for QCD $2 \\to 2$\nprocesses. It is shown that an optimal solution is found, even in a\nhigh-dimensional search space, and the implications of the result are\ndiscussed. \n\n"}
{"id": "1706.03200", "contents": "Title: Critical Hyper-Parameters: No Random, No Cry Abstract: The selection of hyper-parameters is critical in Deep Learning. Because of\nthe long training time of complex models and the availability of compute\nresources in the cloud, \"one-shot\" optimization schemes - where the sets of\nhyper-parameters are selected in advance (e.g. on a grid or in a random manner)\nand the training is executed in parallel - are commonly used. It is known that\ngrid search is sub-optimal, especially when only a few critical parameters\nmatter, and suggest to use random search instead. Yet, random search can be\n\"unlucky\" and produce sets of values that leave some part of the domain\nunexplored. Quasi-random methods, such as Low Discrepancy Sequences (LDS) avoid\nthese issues. We show that such methods have theoretical properties that make\nthem appealing for performing hyperparameter search, and demonstrate that, when\napplied to the selection of hyperparameters of complex Deep Learning models\n(such as state-of-the-art LSTM language models and image classification\nmodels), they yield suitable hyperparameters values with much fewer runs than\nrandom search. We propose a particularly simple LDS method which can be used as\na drop-in replacement for grid or random search in any Deep Learning pipeline,\nboth as a fully one-shot hyperparameter search or as an initializer in\niterative batch optimization. \n\n"}
{"id": "1706.03741", "contents": "Title: Deep reinforcement learning from human preferences Abstract: For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent's interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback. \n\n"}
{"id": "1706.04304", "contents": "Title: Dueling Bandits With Weak Regret Abstract: We consider online content recommendation with implicit feedback through\npairwise comparisons, formalized as the so-called dueling bandit problem. We\nstudy the dueling bandit problem in the Condorcet winner setting, and consider\ntwo notions of regret: the more well-studied strong regret, which is 0 only\nwhen both arms pulled are the Condorcet winner; and the less well-studied weak\nregret, which is 0 if either arm pulled is the Condorcet winner. We propose a\nnew algorithm for this problem, Winner Stays (WS), with variations for each\nkind of regret: WS for weak regret (WS-W) has expected cumulative weak regret\nthat is $O(N^2)$, and $O(N\\log(N))$ if arms have a total order; WS for strong\nregret (WS-S) has expected cumulative strong regret of $O(N^2 + N \\log(T))$,\nand $O(N\\log(N)+N\\log(T))$ if arms have a total order. WS-W is the first\ndueling bandit algorithm with weak regret that is constant in time. WS is\nsimple to compute, even for problems with many arms, and we demonstrate through\nnumerical experiments on simulated and real data that WS has significantly\nsmaller regret than existing algorithms in both the weak- and strong-regret\nsettings. \n\n"}
{"id": "1706.05335", "contents": "Title: Unsupervised Domain Adaptation with Random Walks on Target Labelings Abstract: Unsupervised Domain Adaptation (DA) is used to automatize the task of\nlabeling data: an unlabeled dataset (target) is annotated using a labeled\ndataset (source) from a related domain. We cast domain adaptation as the\nproblem of finding stable labels for target examples. A new definition of label\nstability is proposed, motivated by a generalization error bound for large\nmargin linear classifiers: a target labeling is stable when, with high\nprobability, a classifier trained on a random subsample of the target with that\nlabeling yields the same labeling. We find stable labelings using a random walk\non a directed graph with transition probabilities based on labeling stability.\nThe majority vote of those labelings visited by the walk yields a stable label\nfor each target example. The resulting domain adaptation algorithm is\nstrikingly easy to implement and apply: It does not rely on data\ntransformations, which are in general computational prohibitive in the presence\nof many input features, and does not need to access the source data, which is\nadvantageous when data sharing is restricted. By acting on the original feature\nspace, our method is able to take full advantage of deep features from external\npre-trained neural networks, as demonstrated by the results of our experiments. \n\n"}
{"id": "1706.06348", "contents": "Title: Frank-Wolfe Optimization for Symmetric-NMF under Simplicial Constraint Abstract: Symmetric nonnegative matrix factorization has found abundant applications in\nvarious domains by providing a symmetric low-rank decomposition of nonnegative\nmatrices. In this paper we propose a Frank-Wolfe (FW) solver to optimize the\nsymmetric nonnegative matrix factorization problem under a simplicial\nconstraint, which has recently been proposed for probabilistic clustering.\nCompared with existing solutions, this algorithm is simple to implement, and\nhas no hyperparameters to be tuned. Building on the recent advances of FW\nalgorithms in nonconvex optimization, we prove an $O(1/\\varepsilon^2)$\nconvergence rate to $\\varepsilon$-approximate KKT points, via a tight bound\n$\\Theta(n^2)$ on the curvature constant, which matches the best known result in\nunconstrained nonconvex setting using gradient methods. Numerical results\ndemonstrate the effectiveness of our algorithm. As a side contribution, we\nconstruct a simple nonsmooth convex problem where the FW algorithm fails to\nconverge to the optimum. This result raises an interesting question about\nnecessary conditions of the success of the FW algorithm on convex problems. \n\n"}
{"id": "1706.06619", "contents": "Title: Most Ligand-Based Classification Benchmarks Reward Memorization Rather\n  than Generalization Abstract: Undetected overfitting can occur when there are significant redundancies\nbetween training and validation data. We describe AVE, a new measure of\ntraining-validation redundancy for ligand-based classification problems that\naccounts for the similarity amongst inactive molecules as well as active. We\ninvestigated seven widely-used benchmarks for virtual screening and\nclassification, and show that the amount of AVE bias strongly correlates with\nthe performance of ligand-based predictive methods irrespective of the\npredicted property, chemical fingerprint, similarity measure, or\npreviously-applied unbiasing techniques. Therefore, it may be that the\npreviously-reported performance of most ligand-based methods can be explained\nby overfitting to benchmarks rather than good prospective accuracy. \n\n"}
{"id": "1706.06843", "contents": "Title: Optimal control of non-autonomous SEIRS models with vaccination and\n  treatment Abstract: We study an optimal control problem for a non-autonomous SEIRS model with\nincidence given by a general function of the infective, the susceptible and the\ntotal population, and with vaccination and treatment as control variables. We\nprove existence and uniqueness results for our problem and, for the case of\nmass-action incidence, we present some simulation results designed to compare\nan autonomous and corresponding periodic model, as well as the controlled\nversus uncontrolled models. \n\n"}
{"id": "1706.07001", "contents": "Title: Improved Optimization of Finite Sums with Minibatch Stochastic Variance\n  Reduced Proximal Iterations Abstract: We present novel minibatch stochastic optimization methods for empirical risk\nminimization problems, the methods efficiently leverage variance reduced\nfirst-order and sub-sampled higher-order information to accelerate the\nconvergence speed. For quadratic objectives, we prove improved iteration\ncomplexity over state-of-the-art under reasonable assumptions. We also provide\nempirical evidence of the advantages of our method compared to existing\napproaches in the literature. \n\n"}
{"id": "1706.07832", "contents": "Title: Growing Linear Consensus Networks Endowed by Spectral Systemic\n  Performance Measures Abstract: We propose an axiomatic approach for design and performance analysis of noisy\nlinear consensus networks by introducing a notion of systemic performance\nmeasure. This class of measures are spectral functions of Laplacian eigenvalues\nof the network that are monotone, convex, and orthogonally invariant with\nrespect to the Laplacian matrix of the network. It is shown that several\nexisting gold-standard and widely used performance measures in the literature\nbelong to this new class of measures. We build upon this new notion and\ninvestigate a general form of combinatorial problem of growing a linear\nconsensus network via minimizing a given systemic performance measure. Two\nefficient polynomial-time approximation algorithms are devised to tackle this\nnetwork synthesis problem: a linearization-based method and a simple greedy\nalgorithm based on rank-one updates. Several theoretical fundamental limits on\nthe best achievable performance for the combinatorial problem is derived that\nassist us to evaluate optimality gaps of our proposed algorithms. A detailed\ncomplexity analysis confirms the effectiveness and viability of our algorithms\nto handle large-scale consensus networks. \n\n"}
{"id": "1706.09597", "contents": "Title: Path Integral Networks: End-to-End Differentiable Optimal Control Abstract: In this paper, we introduce Path Integral Networks (PI-Net), a recurrent\nnetwork representation of the Path Integral optimal control algorithm. The\nnetwork includes both system dynamics and cost models, used for optimal control\nbased planning. PI-Net is fully differentiable, learning both dynamics and cost\nmodels end-to-end by back-propagation and stochastic gradient descent. Because\nof this, PI-Net can learn to plan. PI-Net has several advantages: it can\ngeneralize to unseen states thanks to planning, it can be applied to continuous\ncontrol tasks, and it allows for a wide variety learning schemes, including\nimitation and reinforcement learning. Preliminary experiment results show that\nPI-Net, trained by imitation learning, can mimic control demonstrations for two\nsimulated problems; a linear system and a pendulum swing-up problem. We also\nshow that PI-Net is able to learn dynamics and cost models latent in the\ndemonstrations. \n\n"}
{"id": "1706.09820", "contents": "Title: Structural Analysis and Optimal Design of Distributed System Throttlers Abstract: In this paper, we investigate the performance analysis and synthesis of\ndistributed system throttlers (DST). A throttler is a mechanism that limits the\nflow rate of incoming metrics, e.g., byte per second, network bandwidth usage,\ncapacity, traffic, etc. This can be used to protect a service's backend/clients\nfrom getting overloaded, or to reduce the effects of uncertainties in demand\nfor shared services. We study performance deterioration of DSTs subject to\ndemand uncertainty. We then consider network synthesis problems that aim to\nimprove the performance of noisy DSTs via communication link modifications as\nwell as server update cycle modifications. \n\n"}
{"id": "1707.00622", "contents": "Title: Rank Determination for Low-Rank Data Completion Abstract: Recently, fundamental conditions on the sampling patterns have been obtained\nfor finite completability of low-rank matrices or tensors given the\ncorresponding ranks. In this paper, we consider the scenario where the rank is\nnot given and we aim to approximate the unknown rank based on the location of\nsampled entries and some given completion. We consider a number of data models,\nincluding single-view matrix, multi-view matrix, CP tensor, tensor-train tensor\nand Tucker tensor. For each of these data models, we provide an upper bound on\nthe rank when an arbitrary low-rank completion is given. We characterize these\nbounds both deterministically, i.e., with probability one given that the\nsampling pattern satisfies certain combinatorial properties, and\nprobabilistically, i.e., with high probability given that the sampling\nprobability is above some threshold. Moreover, for both single-view matrix and\nCP tensor, we are able to show that the obtained upper bound is exactly equal\nto the unknown rank if the lowest-rank completion is given. Furthermore, we\nprovide numerical experiments for the case of single-view matrix, where we use\nnuclear norm minimization to find a low-rank completion of the sampled data and\nwe observe that in most of the cases the proposed upper bound on the rank is\nequal to the true rank. \n\n"}
{"id": "1707.00802", "contents": "Title: PBODL : Parallel Bayesian Online Deep Learning for Click-Through Rate\n  Prediction in Tencent Advertising System Abstract: We describe a parallel bayesian online deep learning framework (PBODL) for\nclick-through rate (CTR) prediction within today's Tencent advertising system,\nwhich provides quick and accurate learning of user preferences. We first\nexplain the framework with a deep probit regression model, which is trained\nwith probabilistic back-propagation in the mode of assumed Gaussian density\nfiltering. Then we extend the model family to a variety of bayesian online\nmodels with increasing feature embedding capabilities, such as Sparse-MLP,\nFM-MLP and FFM-MLP. Finally, we implement a parallel training system based on a\nstream computing infrastructure and parameter servers. Experiments with public\navailable datasets and Tencent industrial datasets show that models within our\nframework perform better than several common online models, such as\nAdPredictor, FTRL-Proximal and MatchBox. Online A/B test within Tencent\nadvertising system further proves that our framework could achieve CTR and CPM\nlift by learning more quickly and accurately. \n\n"}
{"id": "1707.01475", "contents": "Title: Complex and Holographic Embeddings of Knowledge Graphs: A Comparison Abstract: Embeddings of knowledge graphs have received significant attention due to\ntheir excellent performance for tasks like link prediction and entity\nresolution. In this short paper, we are providing a comparison of two\nstate-of-the-art knowledge graph embeddings for which their equivalence has\nrecently been established, i.e., ComplEx and HolE [Nickel, Rosasco, and Poggio,\n2016; Trouillon et al., 2016; Hayashi and Shimbo, 2017]. First, we briefly\nreview both models and discuss how their scoring functions are equivalent. We\nthen analyze the discrepancy of results reported in the original articles, and\nshow experimentally that they are likely due to the use of different loss\nfunctions. In further experiments, we evaluate the ability of both models to\nembed symmetric and antisymmetric patterns. Finally, we discuss advantages and\ndisadvantages of both models and under which conditions one would be preferable\nto the other. \n\n"}
{"id": "1707.02201", "contents": "Title: Learning human behaviors from motion capture by adversarial imitation Abstract: Rapid progress in deep reinforcement learning has made it increasingly\nfeasible to train controllers for high-dimensional humanoid bodies. However,\nmethods that use pure reinforcement learning with simple reward functions tend\nto produce non-humanlike and overly stereotyped movement behaviors. In this\nwork, we extend generative adversarial imitation learning to enable training of\ngeneric neural network policies to produce humanlike movement patterns from\nlimited demonstrations consisting only of partially observed state features,\nwithout access to actions, even when the demonstrations come from a body with\ndifferent and unknown physical parameters. We leverage this approach to build\nsub-skill policies from motion capture data and show that they can be reused to\nsolve tasks when controlled by a higher level controller. \n\n"}
{"id": "1707.02845", "contents": "Title: Resistance distance criterion for optimal slack bus selection Abstract: We investigate the dependence of transmission losses on the choice of a slack\nbus in high voltage AC transmission networks. We formulate a transmission loss\nminimization problem in terms of slack variables representing the additional\npower injection that each generator provides to compensate the transmission\nlosses. We show analytically that for transmission lines having small,\nhomogeneous resistance over reactance ratios ${r/x\\ll1}$, transmission losses\nare generically minimal in the case of a unique \\textit{slack bus} instead of a\ndistributed slack bus. For the unique slack bus scenario, to lowest order in\n${r/x}$, transmission losses depend linearly on a resistance distance based\nindicator measuring the separation of the slack bus candidate from the rest of\nthe network. We confirm these results numerically for several IEEE and Pegase\ntestcases, and show that our predictions qualitatively hold also in the case of\nlines having inhomogeneous ${r/x}$ ratios, with optimal slack bus choices\nreducing transmission losses by ${10}\\%$ typically. \n\n"}
{"id": "1707.03141", "contents": "Title: A Simple Neural Attentive Meta-Learner Abstract: Deep neural networks excel in regimes with large amounts of data, but tend to\nstruggle when data is scarce or when they need to adapt quickly to changes in\nthe task. In response, recent work in meta-learning proposes training a\nmeta-learner on a distribution of similar tasks, in the hopes of generalization\nto novel but related tasks by learning a high-level strategy that captures the\nessence of the problem it is asked to solve. However, many recent meta-learning\napproaches are extensively hand-designed, either using architectures\nspecialized to a particular application, or hard-coding algorithmic components\nthat constrain how the meta-learner solves the task. We propose a class of\nsimple and generic meta-learner architectures that use a novel combination of\ntemporal convolutions and soft attention; the former to aggregate information\nfrom past experience and the latter to pinpoint specific pieces of information.\nIn the most extensive set of meta-learning experiments to date, we evaluate the\nresulting Simple Neural AttentIve Learner (or SNAIL) on several\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\nlearning, SNAIL attains state-of-the-art performance by significant margins. \n\n"}
{"id": "1707.03631", "contents": "Title: Adversarial Dropout for Supervised and Semi-supervised Learning Abstract: Recently, the training with adversarial examples, which are generated by\nadding a small but worst-case perturbation on input examples, has been proved\nto improve generalization performance of neural networks. In contrast to the\nindividually biased inputs to enhance the generality, this paper introduces\nadversarial dropout, which is a minimal set of dropouts that maximize the\ndivergence between the outputs from the network with the dropouts and the\ntraining supervisions. The identified adversarial dropout are used to\nreconfigure the neural network to train, and we demonstrated that training on\nthe reconfigured sub-network improves the generalization performance of\nsupervised and semi-supervised learning tasks on MNIST and CIFAR-10. We\nanalyzed the trained model to reason the performance improvement, and we found\nthat adversarial dropout increases the sparsity of neural networks more than\nthe standard dropout does. \n\n"}
{"id": "1707.03686", "contents": "Title: Methodology for Multi-stage, Operations- and Uncertainty-Aware Placement\n  and Sizing of FACTS Devices in a Large Power Transmission System Abstract: We develop new optimization methodology for planning installation of Flexible\nAlternating Current Transmission System (FACTS) devices of the parallel and\nshunt types into large power transmission systems, which allows to delay or\navoid installations of generally much more expensive power lines. Methodology\ntakes as an input projected economic development, expressed through a paced\ngrowth of the system loads, as well as uncertainties, expressed through\nmultiple scenarios of the growth. We price new devices according to their\ncapacities. Installation cost contributes to the optimization objective in\ncombination with the cost of operations integrated over time and averaged over\nthe scenarios. The multi-stage (-time-frame) optimization aims to achieve a\ngradual distribution of new resources in space and time. Constraints on the\ninvestment budget, or equivalently constraint on building capacity, is\nintroduced at each time frame. Our approach adjusts operationally not only\nnewly installed FACTS devices but also other already existing flexible degrees\nof freedom. This complex optimization problem is stated using the most general\nAC Power Flows. Non-linear, non-convex, multiple-scenario and multi-time-frame\noptimization is resolved via efficient heuristics, consisting of a sequence of\nalternating Linear Programmings or Quadratic Programmings (depending on the\ngeneration cost) and AC-PF solution steps designed to maintain operational\nfeasibility for all scenarios. Computational scalability and application of the\nnewly developed approach is illustrated on the example of the 2736-nodes large\nPolish system. One most important advantage of the framework is that the\noptimal capacity of FACTS is build up gradually at each time frame in a limited\nnumber of locations, thus allowing to prepare the system better for possible\ncongestion due to future economic and other uncertainties. \n\n"}
{"id": "1707.03804", "contents": "Title: Source-Target Inference Models for Spatial Instruction Understanding Abstract: Models that can execute natural language instructions for situated robotic\ntasks such as assembly and navigation have several useful applications in\nhomes, offices, and remote scenarios. We study the semantics of\nspatially-referred configuration and arrangement instructions, based on the\nchallenging Bisk-2016 blank-labeled block dataset. This task involves finding a\nsource block and moving it to the target position (mentioned via a reference\nblock and offset), where the blocks have no names or colors and are just\nreferred to via spatial location features. We present novel models for the\nsubtasks of source block classification and target position regression, based\non joint-loss language and spatial-world representation learning, as well as\nCNN-based and dual attention models to compute the alignment between the world\nblocks and the instruction phrases. For target position prediction, we compare\ntwo inference approaches: annealed sampling via policy gradient versus\nexpectation inference via supervised regression. Our models achieve the new\nstate-of-the-art on this task, with an improvement of 47% on source block\naccuracy and 22% on target position distance. \n\n"}
{"id": "1707.04064", "contents": "Title: A Pad\\'e-Weierstrass technique for the rigorous enforcement of control\n  limits in power flow studies Abstract: A new technique is presented for solving the problem of enforcing control\nlimits in power flow studies. As an added benefit, it greatly increases the\nachievable precision at nose points. The method is exemplified for the case of\nMvar limits in generators regulating voltage on both local and remote buses.\nBased on the framework of the Holomorphic Embedding Loadflow Method (HELM), it\nprovides a rigorous solution to this fundamental problem by framing it in terms\nof \\emph{optimization}. A novel Lagrangian formulation of power-flow, which is\nexact for lossless networks, leads to a natural physics-based minimization\ncriterion that yields the correct solution. For networks with small losses, as\nis the case in transmission, the AC power flow problem cannot be framed exactly\nin terms of optimization, but the criterion still retains its ability to select\nthe correct solution. This foundation then provides a way to design a HELM\nscheme to solve for the minimizing solution. Although the use of barrier\nfunctions evokes interior point optimization, this method, like HELM, is based\non the analytic continuation of a germ (of a particular branch) of the\nalgebraic curve representing the solutions of the system. In this case, since\nthe constraint equations given by limits result in an unavoidable singularity\nat $s=1$, direct analytic continuation by means of standard Pad\\'e\napproximation is fraught with numerical instabilities. This has been overcome\nby means of a new analytic continuation procedure, denominated\nPad\\'e-Weierstrass, that exploits the covariant nature of the power flow\nequations under certain changes of variables. One colateral benefit of this\nprocedure is that it can also be used when limits are not being enforced, in\norder to increase the achievable numerical precision in highly stressed cases. \n\n"}
{"id": "1707.05173", "contents": "Title: Trial without Error: Towards Safe Reinforcement Learning via Human\n  Intervention Abstract: AI systems are increasingly applied to complex tasks that involve interaction\nwith humans. During training, such systems are potentially dangerous, as they\nhaven't yet learned to avoid actions that could cause serious harm. How can an\nAI system explore and learn without making a single mistake that harms humans\nor otherwise causes serious damage? For model-free reinforcement learning,\nhaving a human \"in the loop\" and ready to intervene is currently the only way\nto prevent all catastrophes. We formalize human intervention for RL and show\nhow to reduce the human labor required by training a supervised learner to\nimitate the human's intervention decisions. We evaluate this scheme on Atari\ngames, with a Deep RL agent being overseen by a human for four hours. When the\nclass of catastrophes is simple, we are able to prevent all catastrophes\nwithout affecting the agent's learning (whereas an RL baseline fails due to\ncatastrophic forgetting). However, this scheme is less successful when\ncatastrophes are more complex: it reduces but does not eliminate catastrophes\nand the supervised learner fails on adversarial examples found by the agent.\nExtrapolating to more challenging environments, we show that our implementation\nwould not scale (due to the infeasible amount of human labor required). We\noutline extensions of the scheme that are necessary if we are to train\nmodel-free agents without a single catastrophe. \n\n"}
{"id": "1707.05714", "contents": "Title: A Multi-Armed Bandit Approach for Online Expert Selection in Markov\n  Decision Processes Abstract: We formulate a multi-armed bandit (MAB) approach to choosing expert policies\nonline in Markov decision processes (MDPs). Given a set of expert policies\ntrained on a state and action space, the goal is to maximize the cumulative\nreward of our agent. The hope is to quickly find the best expert in our set.\nThe MAB formulation allows us to quantify the performance of an algorithm in\nterms of the regret incurred from not choosing the best expert from the\nbeginning. We first develop the theoretical framework for MABs in MDPs, and\nthen present a basic regret decomposition identity. We then adapt the classical\nUpper Confidence Bounds algorithm to the problem of choosing experts in MDPs\nand prove that the expected regret grows at worst at a logarithmic rate.\nLastly, we validate the theory on a small MDP. \n\n"}
{"id": "1707.06484", "contents": "Title: Deep Layer Aggregation Abstract: Visual recognition requires rich representations that span levels from low to\nhigh, scales from small to large, and resolutions from fine to coarse. Even\nwith the depth of features in a convolutional network, a layer in isolation is\nnot enough: compounding and aggregating these representations improves\ninference of what and where. Architectural efforts are exploring many\ndimensions for network backbones, designing deeper or wider architectures, but\nhow to best aggregate layers and blocks across a network deserves further\nattention. Although skip connections have been incorporated to combine layers,\nthese connections have been \"shallow\" themselves, and only fuse by simple,\none-step operations. We augment standard architectures with deeper aggregation\nto better fuse information across layers. Our deep layer aggregation structures\niteratively and hierarchically merge the feature hierarchy to make networks\nwith better accuracy and fewer parameters. Experiments across architectures and\ntasks show that deep layer aggregation improves recognition and resolution\ncompared to existing branching and merging schemes. The code is at\nhttps://github.com/ucbdrive/dla. \n\n"}
{"id": "1707.06658", "contents": "Title: RAIL: Risk-Averse Imitation Learning Abstract: Imitation learning algorithms learn viable policies by imitating an expert's\nbehavior when reward signals are not available. Generative Adversarial\nImitation Learning (GAIL) is a state-of-the-art algorithm for learning policies\nwhen the expert's behavior is available as a fixed set of trajectories. We\nevaluate in terms of the expert's cost function and observe that the\ndistribution of trajectory-costs is often more heavy-tailed for GAIL-agents\nthan the expert at a number of benchmark continuous-control tasks. Thus,\nhigh-cost trajectories, corresponding to tail-end events of catastrophic\nfailure, are more likely to be encountered by the GAIL-agents than the expert.\nThis makes the reliability of GAIL-agents questionable when it comes to\ndeployment in risk-sensitive applications like robotic surgery and autonomous\ndriving. In this work, we aim to minimize the occurrence of tail-end events by\nminimizing tail risk within the GAIL framework. We quantify tail risk by the\nConditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse\nImitation Learning (RAIL) algorithm. We observe that the policies learned with\nRAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed\nRAIL algorithm appears as a potent alternative to GAIL for improved reliability\nin risk-sensitive applications. \n\n"}
{"id": "1707.07476", "contents": "Title: About Extensions of the Extremal Principle Abstract: In this article, after recalling and discussing the conventional extremality,\nlocal extremality, stationarity and approximate stationarity properties of\ncollections of sets and the corresponding (extended) extremal principle, we\nfocus on extensions of these properties and the corresponding dual conditions\nwith the goal to refine the main arguments used in this type of results,\nclarify the relationships between different extensions and expand the\napplicability of the generalised separability results. We introduce and study\nnew more universal concepts of relative extremality and stationarity and\nformulate the relative extended extremal principle. Among other things, certain\nstability of the relative approximate stationarity is proved. Some links are\nestablished between the relative extremality and stationarity properties of\ncollections of sets and (the absence of) certain regularity, lower\nsemicontinuity and Lipschitz-like properties of set-valued mappings. \n\n"}
{"id": "1707.07605", "contents": "Title: Share your Model instead of your Data: Privacy Preserving Mimic Learning\n  for Ranking Abstract: Deep neural networks have become a primary tool for solving problems in many\nfields. They are also used for addressing information retrieval problems and\nshow strong performance in several tasks. Training these models requires large,\nrepresentative datasets and for most IR tasks, such data contains sensitive\ninformation from users. Privacy and confidentiality concerns prevent many data\nowners from sharing the data, thus today the research community can only\nbenefit from research on large-scale datasets in a limited manner. In this\npaper, we discuss privacy preserving mimic learning, i.e., using predictions\nfrom a privacy preserving trained model instead of labels from the original\nsensitive training data as a supervision signal. We present the results of\npreliminary experiments in which we apply the idea of mimic learning and\nprivacy preserving mimic learning for the task of document re-ranking as one of\nthe core IR tasks. This research is a step toward laying the ground for\nenabling researchers from data-rich environments to share knowledge learned\nfrom actual users' data, which should facilitate research collaborations. \n\n"}
{"id": "1707.09726", "contents": "Title: Spectral Compressed Sensing via Projected Gradient Descent Abstract: Let $x\\in\\mathbb{C}^n$ be a spectrally sparse signal consisting of $r$\ncomplex sinusoids with or without damping. We consider the spectral compressed\nsensing problem, which is about reconstructing $x$ from its partial revealed\nentries. By utilizing the low rank structure of the Hankel matrix corresponding\nto $x$, we develop a computationally efficient algorithm for this problem. The\nalgorithm starts from an initial guess computed via one-step hard thresholding\nfollowed by projection, and then proceeds by applying projected gradient\ndescent iterations to a non-convex functional. Based on the sampling with\nreplacement model, we prove that $O(r^2\\log(n))$ observed entries are\nsufficient for our algorithm to achieve the successful recovery of a spectrally\nsparse signal. Moreover, extensive empirical performance comparisons show that\nour algorithm is competitive with other state-of-the-art spectral compressed\nsensing algorithms in terms of phase transitions and overall computational\ntime. \n\n"}
{"id": "1707.09863", "contents": "Title: Dimensionality reduction of SDPs through sketching Abstract: We show how to sketch semidefinite programs (SDPs) using positive maps in\norder to reduce their dimension. More precisely, we use\nJohnson\\hyp{}Lindenstrauss transforms to produce a smaller SDP whose solution\npreserves feasibility or approximates the value of the original problem with\nhigh probability. These techniques allow to improve both complexity and storage\nspace requirements. They apply to problems in which the Schatten 1-norm of the\nmatrices specifying the SDP and also of a solution to the problem is constant\nin the problem size. Furthermore, we provide some results which clarify the\nlimitations of positive, linear sketches in this setting. \n\n"}
{"id": "1708.00065", "contents": "Title: Time-Dependent Representation for Neural Event Sequence Prediction Abstract: Existing sequence prediction methods are mostly concerned with\ntime-independent sequences, in which the actual time span between events is\nirrelevant and the distance between events is simply the difference between\ntheir order positions in the sequence. While this time-independent view of\nsequences is applicable for data such as natural languages, e.g., dealing with\nwords in a sentence, it is inappropriate and inefficient for many real world\nevents that are observed and collected at unequally spaced points of time as\nthey naturally arise, e.g., when a person goes to a grocery store or makes a\nphone call. The time span between events can carry important information about\nthe sequence dependence of human behaviors. In this work, we propose a set of\nmethods for using time in sequence prediction. Because neural sequence models\nsuch as RNN are more amenable for handling token-like input, we propose two\nmethods for time-dependent event representation, based on the intuition on how\ntime is tokenized in everyday life and previous work on embedding\ncontextualization. We also introduce two methods for using next event duration\nas regularization for training a sequence prediction model. We discuss these\nmethods based on recurrent neural nets. We evaluate these methods as well as\nbaseline models on five datasets that resemble a variety of sequence prediction\ntasks. The experiments revealed that the proposed methods offer accuracy gain\nover baseline models in a range of settings. \n\n"}
{"id": "1708.00117", "contents": "Title: Compiling Deep Learning Models for Custom Hardware Accelerators Abstract: Convolutional neural networks (CNNs) are the core of most state-of-the-art\ndeep learning algorithms specialized for object detection and classification.\nCNNs are both computationally complex and embarrassingly parallel. Two\nproperties that leave room for potential software and hardware optimizations\nfor embedded systems. Given a programmable hardware accelerator with a CNN\noriented custom instructions set, the compiler's task is to exploit the\nhardware's full potential, while abiding with the hardware constraints and\nmaintaining generality to run different CNN models with varying workload\nproperties. Snowflake is an efficient and scalable hardware accelerator\nimplemented on programmable logic devices. It implements a control pipeline for\na custom instruction set. The goal of this paper is to present Snowflake's\ncompiler that generates machine level instructions from Torch7 model\ndescription files. The main software design points explored in this work are:\nmodel structure parsing, CNN workload breakdown, loop rearrangement for memory\nbandwidth optimizations and memory access balancing. The performance achieved\nby compiler generated instructions matches against hand optimized code for\nconvolution layers. Generated instructions also efficiently execute AlexNet and\nResNet18 inference on Snowflake. Snowflake with $256$ processing units was\nsynthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in\n$93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$\nframes/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W. \n\n"}
{"id": "1708.00842", "contents": "Title: Latent Parameter Estimation in Fusion Networks Using Separable\n  Likelihoods Abstract: Multi-sensor state space models underpin fusion applications in networks of\nsensors. Estimation of latent parameters in these models has the potential to\nprovide highly desirable capabilities such as network self-calibration.\nConventional solutions to the problem pose difficulties in scaling with the\nnumber of sensors due to the joint multi-sensor filtering involved when\nevaluating the parameter likelihood. In this article, we propose a separable\npseudo-likelihood which is a more accurate approximation compared to a\npreviously proposed alternative under typical operating conditions. In\naddition, we consider using separable likelihoods in the presence of many\nobjects and ambiguity in associating measurements with objects that originated\nthem. To this end, we use a state space model with a hypothesis based\nparameterisation, and, develop an empirical Bayesian perspective in order to\nevaluate separable likelihoods on this model using local filtering. Bayesian\ninference with this likelihood is carried out using belief propagation on the\nassociated pairwise Markov random field. We specify a particle algorithm for\nlatent parameter estimation in a linear Gaussian state space model and\ndemonstrate its efficacy for network self-calibration using measurements from\nnon-cooperative targets in comparison with alternatives. \n\n"}
{"id": "1708.01289", "contents": "Title: Independently Controllable Factors Abstract: It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal. \n\n"}
{"id": "1708.01298", "contents": "Title: Effective sketching methods for value function approximation Abstract: High-dimensional representations, such as radial basis function networks or\ntile coding, are common choices for policy evaluation in reinforcement\nlearning. Learning with such high-dimensional representations, however, can be\nexpensive, particularly for matrix methods, such as least-squares temporal\ndifference learning or quasi-Newton methods that approximate matrix step-sizes.\nIn this work, we explore the utility of sketching for these two classes of\nalgorithms. We highlight issues with sketching the high-dimensional features\ndirectly, which can incur significant bias. As a remedy, we demonstrate how to\nuse sketching more sparingly, with only a left-sided sketch, that can still\nenable significant computational gains and the use of these matrix-based\nlearning algorithms that are less sensitive to parameters. We empirically\ninvestigate these algorithms, in four domains with a variety of\nrepresentations. Our aim is to provide insights into effective use of sketching\nin practice. \n\n"}
{"id": "1708.01867", "contents": "Title: An Information-Theoretic Optimality Principle for Deep Reinforcement\n  Learning Abstract: We methodologically address the problem of Q-value overestimation in deep\nreinforcement learning to handle high-dimensional state spaces efficiently. By\nadapting concepts from information theory, we introduce an intrinsic penalty\nsignal encouraging reduced Q-value estimates. The resultant algorithm\nencompasses a wide range of learning outcomes containing deep Q-networks as a\nspecial case. Different learning outcomes can be demonstrated by tuning a\nLagrange multiplier accordingly. We furthermore propose a novel scheduling\nscheme for this Lagrange multiplier to ensure efficient and robust learning. In\nexperiments on Atari, our algorithm outperforms other algorithms (e.g. deep and\ndouble deep Q-networks) in terms of both game-play performance and sample\ncomplexity. These results remain valid under the recently proposed dueling\narchitecture. \n\n"}
{"id": "1708.04680", "contents": "Title: Augmentor: An Image Augmentation Library for Machine Learning Abstract: The generation of artificial data based on existing observations, known as\ndata augmentation, is a technique used in machine learning to improve model\naccuracy, generalisation, and to control overfitting. Augmentor is a software\npackage, available in both Python and Julia versions, that provides a high\nlevel API for the expansion of image data using a stochastic, pipeline-based\napproach which effectively allows for images to be sampled from a distribution\nof augmented images at runtime. Augmentor provides methods for most standard\naugmentation practices as well as several advanced features such as\nlabel-preserving, randomised elastic distortions, and provides many helper\nfunctions for typical augmentation tasks used in machine learning. \n\n"}
{"id": "1708.07311", "contents": "Title: Generalized maximum entropy estimation Abstract: We consider the problem of estimating a probability distribution that\nmaximizes the entropy while satisfying a finite number of moment constraints,\npossibly corrupted by noise. Based on duality of convex programming, we present\na novel approximation scheme using a smoothed fast gradient method that is\nequipped with explicit bounds on the approximation error. We further\ndemonstrate how the presented scheme can be used for approximating the chemical\nmaster equation through the zero-information moment closure method, and for an\napproximate dynamic programming approach in the context of constrained Markov\ndecision processes with uncountable state and action spaces. \n\n"}
{"id": "1709.02268", "contents": "Title: Phylogenetic Convolutional Neural Networks in Metagenomics Abstract: Background: Convolutional Neural Networks can be effectively used only when\ndata are endowed with an intrinsic concept of neighbourhood in the input space,\nas is the case of pixels in images. We introduce here Ph-CNN, a novel deep\nlearning architecture for the classification of metagenomics data based on the\nConvolutional Neural Networks, with the patristic distance defined on the\nphylogenetic tree being used as the proximity measure. The patristic distance\nbetween variables is used together with a sparsified version of\nMultiDimensional Scaling to embed the phylogenetic tree in a Euclidean space.\nResults: Ph-CNN is tested with a domain adaptation approach on synthetic data\nand on a metagenomics collection of gut microbiota of 38 healthy subjects and\n222 Inflammatory Bowel Disease patients, divided in 6 subclasses.\nClassification performance is promising when compared to classical algorithms\nlike Support Vector Machines and Random Forest and a baseline fully connected\nneural network, e.g. the Multi-Layer Perceptron. Conclusion: Ph-CNN represents\na novel deep learning approach for the classification of metagenomics data.\nOperatively, the algorithm has been implemented as a custom Keras layer taking\ncare of passing to the following convolutional layer not only the data but also\nthe ranked list of neighbourhood of each sample, thus mimicking the case of\nimage data, transparently to the user. Keywords: Metagenomics; Deep learning;\nConvolutional Neural Networks; Phylogenetic trees \n\n"}
{"id": "1709.02878", "contents": "Title: TensorFlow Agents: Efficient Batched Reinforcement Learning in\n  TensorFlow Abstract: We introduce TensorFlow Agents, an efficient infrastructure paradigm for\nbuilding parallel reinforcement learning algorithms in TensorFlow. We simulate\nmultiple environments in parallel, and group them to perform the neural network\ncomputation on a batch rather than individual observations. This allows the\nTensorFlow execution engine to parallelize computation, without the need for\nmanual synchronization. Environments are stepped in separate Python processes\nto progress them in parallel without interference of the global interpreter\nlock. As part of this project, we introduce BatchPPO, an efficient\nimplementation of the proximal policy optimization algorithm. By open sourcing\nTensorFlow Agents, we hope to provide a flexible starting point for future\nprojects that accelerates future research in the field. \n\n"}
{"id": "1709.03465", "contents": "Title: Online Learning in Weakly Coupled Markov Decision Processes: A\n  Convergence Time Study Abstract: We consider multiple parallel Markov decision processes (MDPs) coupled by\nglobal constraints, where the time varying objective and constraint functions\ncan only be observed after the decision is made. Special attention is given to\nhow well the decision maker can perform in $T$ slots, starting from any state,\ncompared to the best feasible randomized stationary policy in hindsight. We\ndevelop a new distributed online algorithm where each MDP makes its own\ndecision each slot after observing a multiplier computed from past information.\nWhile the scenario is significantly more challenging than the classical online\nlearning context, the algorithm is shown to have a tight $O(\\sqrt{T})$ regret\nand constraint violations simultaneously. To obtain such a bound, we combine\nseveral new ingredients including ergodicity and mixing time bound in weakly\ncoupled MDPs, a new regret analysis for online constrained optimization, a\ndrift analysis for queue processes, and a perturbation analysis based on\nFarkas' Lemma. \n\n"}
{"id": "1709.03668", "contents": "Title: Branch-and-bound for biobjective mixed-integer linear programming Abstract: We present a generic branch-and-bound algorithm for finding all the Pareto\nsolutions of a biobjective mixed-integer linear program. The main contributions\nare new algorithms for obtaining dual bounds at a node, for checking node\nfathoming, presolve and duality gap measurement. Our branch-and-bound is\npredominantly a decision space search method since the branching is performed\non the decision variables, akin to single objective problems, although we also\nsometimes split gaps and branch in the objective space. The various algorithms\nare implemented using a data structure for storing Pareto sets. Computational\nexperiments are carried out on literature instances and also on a new set of\ninstances that we generate using the MIPLIB benchmark library for single\nobjective problems. We also perform comparisons against the triangle splitting\nmethod from literature, which is an objective space search algorithm. \n\n"}
{"id": "1709.03726", "contents": "Title: Adaptive Graph Signal Processing: Algorithms and Optimal Sampling\n  Strategies Abstract: The goal of this paper is to propose novel strategies for adaptive learning\nof signals defined over graphs, which are observed over a (randomly\ntime-varying) subset of vertices. We recast two classical adaptive algorithms\nin the graph signal processing framework, namely, the least mean squares (LMS)\nand the recursive least squares (RLS) adaptive estimation strategies. For both\nmethods, a detailed mean-square analysis illustrates the effect of random\nsampling on the adaptive reconstruction capability and the steady-state\nperformance. Then, several probabilistic sampling strategies are proposed to\ndesign the sampling probability at each node in the graph, with the aim of\noptimizing the tradeoff between steady-state performance, graph sampling rate,\nand convergence rate of the adaptive algorithms. Finally, a distributed RLS\nstrategy is derived and is shown to be convergent to its centralized\ncounterpart. Numerical simulations carried out over both synthetic and real\ndata illustrate the good performance of the proposed sampling and\nreconstruction strategies for (possibly distributed) adaptive learning of\nsignals defined over graphs. \n\n"}
{"id": "1709.04073", "contents": "Title: Linear Stochastic Approximation: Constant Step-Size and Iterate\n  Averaging Abstract: We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)\nwith a constant step-size and the so called Polyak-Ruppert (PR) averaging of\niterates. LSAs are widely applied in machine learning and reinforcement\nlearning (RL), where the aim is to compute an appropriate $\\theta_{*} \\in\n\\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$\nupdates per iteration. In this paper, we are motivated by the problem (in RL)\nof policy evaluation from experience replay using the \\emph{temporal\ndifference} (TD) class of learning algorithms that are also LSAs. For LSAs with\na constant step-size, and PR averaging, we provide bounds for the mean squared\nerror (MSE) after $t$ iterations. We assume that data is \\iid with finite\nvariance (underlying distribution being $P$) and that the expected dynamics is\nHurwitz. For a given LSA with PR averaging, and data distribution $P$\nsatisfying the said assumptions, we show that there exists a range of constant\nstep-sizes such that its MSE decays as $O(\\frac{1}{t})$.\n  We examine the conditions under which a constant step-size can be chosen\nuniformly for a class of data distributions $\\mathcal{P}$, and show that not\nall data distributions `admit' such a uniform constant step-size. We also\nsuggest a heuristic step-size tuning algorithm to choose a constant step-size\nof a given LSA for a given data distribution $P$. We compare our results with\nrelated work and also discuss the implication of our results in the context of\nTD algorithms that are LSAs. \n\n"}
{"id": "1709.05843", "contents": "Title: Decentralized Collision-Free Control of Multiple Robots in 2D and 3D\n  Spaces Abstract: Decentralized control of robots has attracted huge research interests.\nHowever, some of the research used unrealistic assumptions without collision\navoidance. This report focuses on the collision-free control for multiple\nrobots in both complete coverage and search tasks in 2D and 3D areas which are\narbitrary unknown. All algorithms are decentralized as robots have limited\nabilities and they are mathematically proved.\n  The report starts with the grid selection in the two tasks. Grid patterns\nsimplify the representation of the area and robots only need to move straightly\nbetween neighbor vertices. For the 100% complete 2D coverage, the equilateral\ntriangular grid is proposed. For the complete coverage ignoring the boundary\neffect, the grid with the fewest vertices is calculated in every situation for\nboth 2D and 3D areas.\n  The second part is for the complete coverage in 2D and 3D areas. A\ndecentralized collision-free algorithm with the above selected grid is\npresented driving robots to sections which are furthest from the reference\npoint. The area can be static or expanding, and the algorithm is simulated in\nMATLAB.\n  Thirdly, three grid-based decentralized random algorithms with collision\navoidance are provided to search targets in 2D or 3D areas. The number of\ntargets can be known or unknown. In the first algorithm, robots choose vacant\nneighbors randomly with priorities on unvisited ones while the second one adds\nthe repulsive force to disperse robots if they are close. In the third\nalgorithm, if surrounded by visited vertices, the robot will use the\nbreadth-first search algorithm to go to one of the nearest unvisited vertices\nvia the grid. The second search algorithm is verified on Pioneer 3-DX robots.\nThe general way to generate the formula to estimate the search time is\ndemonstrated. Algorithms are compared with five other algorithms in MATLAB to\nshow their effectiveness. \n\n"}
{"id": "1709.06011", "contents": "Title: Guided Deep Reinforcement Learning for Swarm Systems Abstract: In this paper, we investigate how to learn to control a group of cooperative\nagents with limited sensing capabilities such as robot swarms. The agents have\nonly very basic sensor capabilities, yet in a group they can accomplish\nsophisticated tasks, such as distributed assembly or search and rescue tasks.\nLearning a policy for a group of agents is difficult due to distributed partial\nobservability of the state. Here, we follow a guided approach where a critic\nhas central access to the global state during learning, which simplifies the\npolicy evaluation problem from a reinforcement learning point of view. For\nexample, we can get the positions of all robots of the swarm using a camera\nimage of a scene. This camera image is only available to the critic and not to\nthe control policies of the robots. We follow an actor-critic approach, where\nthe actors base their decisions only on locally sensed information. In\ncontrast, the critic is learned based on the true global state. Our algorithm\nuses deep reinforcement learning to approximate both the Q-function and the\npolicy. The performance of the algorithm is evaluated on two tasks with simple\nsimulated 2D agents: 1) finding and maintaining a certain distance to each\nothers and 2) locating a target. \n\n"}
{"id": "1709.06533", "contents": "Title: Summable Reparameterizations of Wasserstein Critics in the\n  One-Dimensional Setting Abstract: Generative adversarial networks (GANs) are an exciting alternative to\nalgorithms for solving density estimation problems---using data to assess how\nlikely samples are to be drawn from the same distribution. Instead of\nexplicitly computing these probabilities, GANs learn a generator that can match\nthe given probabilistic source. This paper looks particularly at this matching\ncapability in the context of problems with one-dimensional outputs. We identify\na class of function decompositions with properties that make them well suited\nto the critic role in a leading approach to GANs known as Wasserstein GANs. We\nshow that Taylor and Fourier series decompositions belong to our class, provide\nexamples of these critics outperforming standard GAN approaches, and suggest\nhow they can be scaled to higher dimensional problems in the future. \n\n"}
{"id": "1709.06645", "contents": "Title: Closed-Loop Statistical Verification of Stochastic Nonlinear Systems\n  Subject to Parametric Uncertainties Abstract: This paper proposes a statistical verification framework using Gaussian\nprocesses (GPs) for simulation-based verification of stochastic nonlinear\nsystems with parametric uncertainties. Given a small number of stochastic\nsimulations, the proposed framework constructs a GP regression model and\npredicts the system's performance over the entire set of possible\nuncertainties. Included in the framework is a new metric to estimate the\nconfidence in those predictions based on the variance of the GP's cumulative\ndistribution function. This variance-based metric forms the basis of active\nsampling algorithms that aim to minimize prediction error through careful\nselection of simulations. In three case studies, the new active sampling\nalgorithms demonstrate up to a 35% improvement in prediction error over other\napproaches and are able to correctly identify regions with low prediction\nconfidence through the variance metric. \n\n"}
{"id": "1709.06934", "contents": "Title: REACT to Cyber Attacks on Power Grids Abstract: Motivated by the recent cyber attack on the Ukrainian power grid, we study\ncyber attacks on power grids that affect both the physical infrastructure and\nthe data at the control center. In particular, we assume that an adversary\nattacks an area by: (i) remotely disconnecting some lines within the attacked\narea, and (ii) modifying the information received from the attacked area to\nmask the line failures and hide the attacked area from the control center. For\nthe latter, we consider two types of attacks: (i) data distortion: which\ndistorts the data by adding powerful noise to the actual data, and (ii) data\nreplay: which replays a locally consistent old data instead of the actual data.\nWe use the DC power flow model and prove that the problem of finding the set of\nline failures given the phase angles of the nodes outside of the attacked area\nis strongly NP-hard, even when the attacked area is known. However, we\nintroduce the polynomial time REcurrent Attack Containment and deTection\n(REACT) Algorithm to approximately detect the attacked area and line failures\nafter a cyber attack. We numerically show that it performs very well in\ndetecting the attacked area, and detecting single, double, and triple line\nfailures in small and large attacked areas. \n\n"}
{"id": "1709.06994", "contents": "Title: Structured Probabilistic Pruning for Convolutional Neural Network\n  Acceleration Abstract: In this paper, we propose a novel progressive parameter pruning method for\nConvolutional Neural Network acceleration, named Structured Probabilistic\nPruning (SPP), which effectively prunes weights of convolutional layers in a\nprobabilistic manner. Unlike existing deterministic pruning approaches, where\nunimportant weights are permanently eliminated, SPP introduces a pruning\nprobability for each weight, and pruning is guided by sampling from the pruning\nprobabilities. A mechanism is designed to increase and decrease pruning\nprobabilities based on importance criteria in the training process. Experiments\nshow that, with 4x speedup, SPP can accelerate AlexNet with only 0.3% loss of\ntop-5 accuracy and VGG-16 with 0.8% loss of top-5 accuracy in ImageNet\nclassification. Moreover, SPP can be directly applied to accelerate\nmulti-branch CNN networks, such as ResNet, without specific adaptations. Our 2x\nspeedup ResNet-50 only suffers 0.8% loss of top-5 accuracy on ImageNet. We\nfurther show the effectiveness of SPP on transfer learning tasks. \n\n"}
{"id": "1709.07544", "contents": "Title: Robust Detection of Biasing Attacks on Misappropriated Distributed\n  Observers via Decentralized $H_\\infty$ synthesis Abstract: We develop a decentralized $H_\\infty$ synthesis approach to detection of\nbiasing misappropriation attacks on distributed observers. Its starting point\nis to equip the observer with an attack model which is then used in the design\nof attack detectors. A two-step design procedure is proposed. First, an initial\ncentralized setup is carried out which enables each node to compute the\nparameters of its attack detector online in a decentralized manner, without\ninteracting with other nodes. Each such detector is designed using the\n$H_\\infty$ approach. Next, the attack detectors are embedded into the network,\nwhich allows them to detect misappropriated nodes from innovation in the\nnetwork interconnections. \n\n"}
{"id": "1709.07911", "contents": "Title: Avoidance of Manual Labeling in Robotic Autonomous Navigation Through\n  Multi-Sensory Semi-Supervised Learning Abstract: Imitation learning holds the promise to address challenging robotic tasks\nsuch as autonomous navigation. It however requires a human supervisor to\noversee the training process and send correct control commands to robots\nwithout feedback, which is always prone to error and expensive. To minimize\nhuman involvement and avoid manual labeling of data in the robotic autonomous\nnavigation with imitation learning, this paper proposes a novel semi-supervised\nimitation learning solution based on a multi-sensory design. This solution\nincludes a suboptimal sensor policy based on sensor fusion to automatically\nlabel states encountered by a robot to avoid human supervision during training.\nIn addition, a recording policy is developed to throttle the adversarial affect\nof learning too much from the suboptimal sensor policy. This solution allows\nthe robot to learn a navigation policy in a self-supervised manner. With\nextensive experiments in indoor environments, this solution can achieve near\nhuman performance in most of the tasks and even surpasses human performance in\ncase of unexpected events such as hardware failures or human operation errors.\nTo best of our knowledge, this is the first work that synthesizes sensor fusion\nand imitation learning to enable robotic autonomous navigation in the real\nworld without human supervision. \n\n"}
{"id": "1709.08274", "contents": "Title: Learning Graph-Structured Sum-Product Networks for Probabilistic\n  Semantic Maps Abstract: We introduce Graph-Structured Sum-Product Networks (GraphSPNs), a\nprobabilistic approach to structured prediction for problems where dependencies\nbetween latent variables are expressed in terms of arbitrary, dynamic graphs.\nWhile many approaches to structured prediction place strict constraints on the\ninteractions between inferred variables, many real-world problems can be only\ncharacterized using complex graph structures of varying size, often\ncontaminated with noise when obtained from real data. Here, we focus on one\nsuch problem in the domain of robotics. We demonstrate how GraphSPNs can be\nused to bolster inference about semantic, conceptual place descriptions using\nnoisy topological relations discovered by a robot exploring large-scale office\nspaces. Through experiments, we show that GraphSPNs consistently outperform the\ntraditional approach based on undirected graphical models, successfully\ndisambiguating information in global semantic maps built from uncertain, noisy\nlocal evidence. We further exploit the probabilistic nature of the model to\ninfer marginal distributions over semantic descriptions of as yet unexplored\nplaces and detect spatial environment configurations that are novel and\nincongruent with the known evidence. \n\n"}
{"id": "1709.08740", "contents": "Title: On the error of a priori sampling: zero forcing sets and propagation\n  time Abstract: Zero forcing is an iterative process on a graph used to bound the maximum\nnullity. The process begins with select vertices as colored, and the remaining\nvertices can become colored under a specific color change rule. The goal is to\nfind a minimum set of vertices such that after iteratively applying the rule,\nall of the vertices become colored (i.e., a minimum zero forcing set). Of\nparticular interest is the propagation time of a chosen set which is the number\nof steps the rule must be applied in order to color all the vertices of a\ngraph.\n  We give a purely linear algebraic interpretation of zero forcing: Find a set\nof vertices $S$ such that for any weighted adjacency matrix $\\mathbf{A}$,\nwhenever $\\mathbf{Ax} = \\mathbf{0}$, the entirety of of $\\mathbf{x}$ can be\nrecovered using only $\\mathbf{x}_S$, the entries corresponding to $S$. The key\nhere is that $S$ must be chosen before $\\mathbf{A}$. In this light, we are able\nto give a linear algebraic interpretation of the propagation time: Any error in\n$\\mathbf{x}_S$ effects the error of $\\mathbf{x}$ exponentially in the\npropagation time. This error can be quantitatively measured using newly defined\nzero forcing-related parameters, the error polynomial vector and the variance\npolynomial vector. In this sense, the quality of two zero forcing sets can\nobjectively be compared even if the sets are the same size and their\npropagation time is the same. Examples and constructions are given. \n\n"}
{"id": "1709.08830", "contents": "Title: Catching Anomalous Distributed Photovoltaics: An Edge-based Multi-modal\n  Anomaly Detection Abstract: A significant challenge in energy system cyber security is the current\ninability to detect cyber-physical attacks targeting and originating from\ndistributed grid-edge devices such as photovoltaics (PV) panels, smart flexible\nloads, and electric vehicles. We address this concern by designing and\ndeveloping a distributed, multi-modal anomaly detection approach that can sense\nthe health of the device and the electric power grid from the edge. This is\nrealized by exploiting unsupervised machine learning algorithms on multiple\nsources of time-series data, fusing these multiple local observations and\nflagging anomalies when a deviation from the normal behavior is observed.\n  We particularly focus on the cyber-physical threats to the distributed PVs\nthat has the potential to cause local disturbances or grid instabilities by\ncreating supply-demand mismatch, reverse power flow conditions etc. We use an\nopen source power system simulation tool called GridLAB-D, loaded with real\nsmart home and solar datasets to simulate the smart grid scenarios and to\nillustrate the impact of PV attacks on the power system. Various attacks\ntargeting PV panels that create voltage fluctuations, reverse power flow etc\nwere designed and performed. We observe that while individual unsupervised\nlearning algorithms such as OCSVMs, Corrupt RF and PCA surpasses in identifying\nparticular attack type, PCA with Convex Hull outperforms all algorithms in\nidentifying all designed attacks with a true positive rate of 83.64% and an\naccuracy of 95.78%. Our key insight is that due to the heterogeneous nature of\nthe distribution grid and the uncertainty in the type of the attack being\nlaunched, relying on single mode of information for defense can lead to\nincreased false alarms and missed detection rates as one can design attacks to\nhide within those uncertainties and remain stealthy. \n\n"}
{"id": "1709.08840", "contents": "Title: Nonlinear Mapping Convergence and Application to Social Networks Abstract: This paper discusses discrete-time maps of the form $x(k + 1) = F(x(k))$,\nfocussing on equilibrium points of such maps. Under some circumstances,\nLefschetz fixed-point theory can be used to establish the existence of a single\nlocally attractive equilibrium (which is sometimes globally attractive) when a\ngeneral property of local attractivity is known for any equilibrium. Problems\nin social networks often involve such discrete-time systems, and we make an\napplication to one such problem. \n\n"}
{"id": "1709.08999", "contents": "Title: Optimal Stationary Synchronization of Heterogeneous Linear Multi-Agent\n  Systems Abstract: In this paper, we address the output synchronization of heterogeneous linear\nnetworks. In the literature, all agents are typically required to synchronize\nexactly to a common trajectory. Here, we introduce optimal stationary\nsynchronization (OSS) instead which permits non-zero steady-state\nsynchronization errors. As a benefit, we are able to relax standard\nrequirements. E.g., agents are allowed to participate in the network even when\nthey usually cannot synchronize exactly. In addition, OSS enables agents to\nsave input-energy by synchronizing within tolerable error-bounds. Our new\nmethod combines the synchronization of bounded exosystems with local\ninfinite-time linear quadratic tracking (LQT). This results in an optimal\nbalance of each agent's synchronization error versus its consumed input-energy.\nMoreover, we extend recent results in LQT such that the derived time-invariant\noptimal control guarantees that the synchronization error satisfies given\nstrict bounds. All these aspects are demonstrated by an illustrative simulation\nexample with a detailed analysis. \n\n"}
{"id": "1709.09820", "contents": "Title: Generative Adversarial Mapping Networks Abstract: Generative Adversarial Networks (GANs) have shown impressive performance in\ngenerating photo-realistic images. They fit generative models by minimizing\ncertain distance measure between the real image distribution and the generated\ndata distribution. Several distance measures have been used, such as\nJensen-Shannon divergence, $f$-divergence, and Wasserstein distance, and\nchoosing an appropriate distance measure is very important for training the\ngenerative network. In this paper, we choose to use the maximum mean\ndiscrepancy (MMD) as the distance metric, which has several nice theoretical\nguarantees. In fact, generative moment matching network (GMMN) (Li, Swersky,\nand Zemel 2015) is such a generative model which contains only one generator\nnetwork $G$ trained by directly minimizing MMD between the real and generated\ndistributions. However, it fails to generate meaningful samples on challenging\nbenchmark datasets, such as CIFAR-10 and LSUN. To improve on GMMN, we propose\nto add an extra network $F$, called mapper. $F$ maps both real data\ndistribution and generated data distribution from the original data space to a\nfeature representation space $\\mathcal{R}$, and it is trained to maximize MMD\nbetween the two mapped distributions in $\\mathcal{R}$, while the generator $G$\ntries to minimize the MMD. We call the new model generative adversarial mapping\nnetworks (GAMNs). We demonstrate that the adversarial mapper $F$ can help $G$\nto better capture the underlying data distribution. We also show that GAMN\nsignificantly outperforms GMMN, and is also superior to or comparable with\nother state-of-the-art GAN based methods on MNIST, CIFAR-10 and LSUN-Bedrooms\ndatasets. \n\n"}
{"id": "1709.09844", "contents": "Title: Distance-based Confidence Score for Neural Network Classifiers Abstract: The reliable measurement of confidence in classifiers' predictions is very\nimportant for many applications and is, therefore, an important part of\nclassifier design. Yet, although deep learning has received tremendous\nattention in recent years, not much progress has been made in quantifying the\nprediction confidence of neural network classifiers. Bayesian models offer a\nmathematically grounded framework to reason about model uncertainty, but\nusually come with prohibitive computational costs. In this paper we propose a\nsimple, scalable method to achieve a reliable confidence score, based on the\ndata embedding derived from the penultimate layer of the network. We\ninvestigate two ways to achieve desirable embeddings, by using either a\ndistance-based loss or Adversarial Training. We then test the benefits of our\nmethod when used for classification error prediction, weighting an ensemble of\nclassifiers, and novelty detection. In all tasks we show significant\nimprovement over traditional, commonly used confidence scores. \n\n"}
{"id": "1710.00104", "contents": "Title: Small Satellite Constellation Separation using Linear Programming based\n  Differential Drag Commands Abstract: We study the optimal control of an arbitrarily large constellation of small\nsatellites operating in low Earth orbit. Simulating the lack of on-board\npropulsion, we limit our actuation to the use of differential drag maneuvers to\nmake in-plane changes to the satellite orbits. We propose an efficient method\nto separate a cluster of satellites into a desired constellation shape while\nrespecting actuation constraints and maximizing the operational lifetime of the\nconstellation. By posing the problem as a linear program, we solve for the\noptimal drag commands for each of the satellites on a daily basis with a\nshrinking-horizon model predictive control approach. We then apply this control\nstrategy in a nonlinear orbital dynamics simulation with a simple, varying\natmospheric density model. We demonstrate the ability to control a cluster of\n100+ satellites starting at the same initial conditions in a circular low Earth\norbit to form an equally spaced constellation (with a relative angular\nseparation error tolerance of one-tenth a degree). The constellation separation\ntask can be executed in 71 days, a time frame that is competitive for the\nstate-of-the-practice. This method allows us to trade the time required to\nconverge to the desired constellation with a sacrifice in the overall\nconstellation lifetime, measured as the maximum altitude loss experienced by\none of the satellites in the group after the separation maneuvers. \n\n"}
{"id": "1710.01105", "contents": "Title: A Bernoulli-Gaussian Physical Watermark for Detecting Integrity Attacks\n  in Control Systems Abstract: We examine the merit of Bernoulli packet drops in actively detecting\nintegrity attacks on control systems. The aim is to detect an adversary who\ndelivers fake sensor measurements to a system operator in order to conceal\ntheir effect on the plant. Physical watermarks, or noisy additive Gaussian\ninputs, have been previously used to detect several classes of integrity\nattacks in control systems. In this paper, we consider the analysis and design\nof Gaussian physical watermarks in the presence of packet drops at the control\ninput. On one hand, this enables analysis in a more general network setting. On\nthe other hand, we observe that in certain cases, Bernoulli packet drops can\nimprove detection performance relative to a purely Gaussian watermark. This\nmotivates the joint design of a Bernoulli-Gaussian watermark which incorporates\nboth an additive Gaussian input and a Bernoulli drop process. We characterize\nthe effect of such a watermark on system performance as well as attack\ndetectability in two separate design scenarios. Here, we consider a correlation\ndetector for attack recognition. We then propose efficiently solvable\noptimization problems to intelligently select parameters of the Gaussian input\nand the Bernoulli drop process while addressing security and performance\ntrade-offs. Finally, we provide numerical results which illustrate that a\nwatermark with packet drops can indeed outperform a Gaussian watermark. \n\n"}
{"id": "1710.02338", "contents": "Title: Projection Based Weight Normalization for Deep Neural Networks Abstract: Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned\nproblem. We observe that the scaling-based weight space symmetry property in\nrectified nonlinear network will cause this negative effect. Therefore, we\npropose to constrain the incoming weights of each neuron to be unit-norm, which\nis formulated as an optimization problem over Oblique manifold. A simple yet\nefficient method referred to as projection based weight normalization (PBWN) is\nalso developed to solve this problem. PBWN executes standard gradient updates,\nfollowed by projecting the updated weight back to Oblique manifold. This\nproposed method has the property of regularization and collaborates well with\nthe commonly used batch normalization technique. We conduct comprehensive\nexperiments on several widely-used image datasets including CIFAR-10,\nCIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art\nconvolutional neural networks, such as Inception, VGG and residual networks.\nThe results show that our method is able to improve the performance of DNNs\nwith different architectures consistently. We also apply our method to Ladder\nnetwork for semi-supervised learning on permutation invariant MNIST dataset,\nand our method outperforms the state-of-the-art methods: we obtain test errors\nas 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples,\nrespectively. \n\n"}
{"id": "1710.04062", "contents": "Title: Decentralized Online Learning with Kernels Abstract: We consider multi-agent stochastic optimization problems over reproducing\nkernel Hilbert spaces (RKHS). In this setting, a network of interconnected\nagents aims to learn decision functions, i.e., nonlinear statistical models,\nthat are optimal in terms of a global convex functional that aggregates data\nacross the network, with only access to locally and sequentially observed\nsamples. We propose solving this problem by allowing each agent to learn a\nlocal regression function while enforcing consensus constraints. We use a\npenalized variant of functional stochastic gradient descent operating\nsimultaneously with low-dimensional subspace projections. These subspaces are\nconstructed greedily by applying orthogonal matching pursuit to the sequence of\nkernel dictionaries and weights. By tuning the projection-induced bias, we\npropose an algorithm that allows for each individual agent to learn, based upon\nits locally observed data stream and message passing with its neighbors only, a\nregression function that is close to the globally optimal regression function.\nThat is, we establish that with constant step-size selections agents' functions\nconverge to a neighborhood of the globally optimal one while satisfying the\nconsensus constraints as the penalty parameter is increased. Moreover, the\ncomplexity of the learned regression functions is guaranteed to remain finite.\nOn both multi-class kernel logistic regression and multi-class kernel support\nvector classification with data generated from class-dependent Gaussian mixture\nmodels, we observe stable function estimation and state of the art performance\nfor distributed online multi-class classification. Experiments on the Brodatz\ntextures further substantiate the empirical validity of this approach. \n\n"}
{"id": "1710.05472", "contents": "Title: Safe Learning of Quadrotor Dynamics Using Barrier Certificates Abstract: To effectively control complex dynamical systems, accurate nonlinear models\nare typically needed. However, these models are not always known. In this\npaper, we present a data-driven approach based on Gaussian processes that\nlearns models of quadrotors operating in partially unknown environments. What\nmakes this challenging is that if the learning process is not carefully\ncontrolled, the system will go unstable, i.e., the quadcopter will crash. To\nthis end, barrier certificates are employed for safe learning. The barrier\ncertificates establish a non-conservative forward invariant safe region, in\nwhich high probability safety guarantees are provided based on the statistics\nof the Gaussian Process. A learning controller is designed to efficiently\nexplore those uncertain states and expand the barrier certified safe region\nbased on an adaptive sampling scheme. In addition, a recursive Gaussian Process\nprediction method is developed to learn the complex quadrotor dynamics in\nreal-time. Simulation results are provided to demonstrate the effectiveness of\nthe proposed approach. \n\n"}
{"id": "1710.05521", "contents": "Title: On the Hybrid Minimum Principle Abstract: The Hybrid Minimum Principle (HMP) is established for the optimal control of\ndeterministic hybrid systems with both autonomous and controlled switchings and\njumps where state jumps at the switching instants are permitted to be\naccompanied by changes in the dimension of the state space. First order\nvariational analysis is performed via the needle variation methodology and the\nnecessary optimality conditions are established in the form of the HMP. A\nfeature of special interest in this work is the explicit presentations of\nboundary conditions on the Hamiltonians and the adjoint processes before and\nafter switchings and jumps. In addition to an analytic example, the HMP results\nare illustrated for the optimal control of an electric vehicle with\ntransmission, where the modelling of the powertrain requires the consideration\nof both autonomous and controlled switchings accompanied by dimension changes. \n\n"}
{"id": "1710.06302", "contents": "Title: Robustly Maximal Utilisation of Energy-Constrained Distributed Resources Abstract: We consider the problem of dispatching a fleet of distributed energy reserve\ndevices to collectively meet a sequence of power requests over time. Under the\nrestriction that reserves cannot be replenished, we aim to maximise the\nsurvival time of an energy-constrained islanded electrical system; and we\ndiscuss realistic scenarios in which this might be the ultimate goal of the\ngrid operator. We present a policy that achieves this optimality, and\ngeneralise this into a set-theoretic result that implies there is no better\npolicy available, regardless of the realised energy requirement scenario. \n\n"}
{"id": "1710.06570", "contents": "Title: A Correspondence Between Random Neural Networks and Statistical Field\n  Theory Abstract: A number of recent papers have provided evidence that practical design\nquestions about neural networks may be tackled theoretically by studying the\nbehavior of random networks. However, until now the tools available for\nanalyzing random neural networks have been relatively ad-hoc. In this work, we\nshow that the distribution of pre-activations in random neural networks can be\nexactly mapped onto lattice models in statistical physics. We argue that\nseveral previous investigations of stochastic networks actually studied a\nparticular factorial approximation to the full lattice model. For random linear\nnetworks and random rectified linear networks we show that the corresponding\nlattice models in the wide network limit may be systematically approximated by\na Gaussian distribution with covariance between the layers of the network. In\neach case, the approximate distribution can be diagonalized by Fourier\ntransformation. We show that this approximation accurately describes the\nresults of numerical simulations of wide random neural networks. Finally, we\ndemonstrate that in each case the large scale behavior of the random networks\ncan be approximated by an effective field theory. \n\n"}
{"id": "1710.06832", "contents": "Title: The Origins of Computational Mechanics: A Brief Intellectual History and\n  Several Clarifications Abstract: The principle goal of computational mechanics is to define pattern and\nstructure so that the organization of complex systems can be detected and\nquantified. Computational mechanics developed from efforts in the 1970s and\nearly 1980s to identify strange attractors as the mechanism driving weak fluid\nturbulence via the method of reconstructing attractor geometry from measurement\ntime series and in the mid-1980s to estimate equations of motion directly from\ncomplex time series. In providing a mathematical and operational definition of\nstructure it addressed weaknesses of these early approaches to discovering\npatterns in natural systems.\n  Since then, computational mechanics has led to a range of results from\ntheoretical physics and nonlinear mathematics to diverse applications---from\nclosed-form analysis of Markov and non-Markov stochastic processes that are\nergodic or nonergodic and their measures of information and intrinsic\ncomputation to complex materials and deterministic chaos and intelligence in\nMaxwellian demons to quantum compression of classical processes and the\nevolution of computation and language.\n  This brief review clarifies several misunderstandings and addresses concerns\nrecently raised regarding early works in the field (1980s). We show that\nmisguided evaluations of the contributions of computational mechanics are\ngroundless and stem from a lack of familiarity with its basic goals and from a\nfailure to consider its historical context. For all practical purposes, its\nmodern methods and results largely supersede the early works. This not only\nrenders recent criticism moot and shows the solid ground on which computational\nmechanics stands but, most importantly, shows the significant progress achieved\nover three decades and points to the many intriguing and outstanding challenges\nin understanding the computational nature of complex dynamic systems. \n\n"}
{"id": "1710.07009", "contents": "Title: Finite Model Approximations for Partially Observed Markov Decision\n  Processes with Discounted Cost Abstract: We consider finite model approximations of discrete-time partially observed\nMarkov decision processes (POMDPs) under the discounted cost criterion. After\nconverting the original partially observed stochastic control problem to a\nfully observed one on the belief space, the finite models are obtained through\nthe uniform quantization of the state and action spaces of the belief space\nMarkov decision process (MDP). Under mild assumptions on the components of the\noriginal model, it is established that the policies obtained from these finite\nmodels are nearly optimal for the belief space MDP, and so, for the original\npartially observed problem. The assumptions essentially require that the belief\nspace MDP satisfies a mild weak continuity condition. We provide examples and\nintroduce explicit approximation procedures for the quantization of the set of\nprobability measures on the state space of POMDP (i.e., belief space). \n\n"}
{"id": "1710.08005", "contents": "Title: Smart \"Predict, then Optimize\" Abstract: Many real-world analytics problems involve two significant challenges:\nprediction and optimization. Due to the typically complex nature of each\nchallenge, the standard paradigm is predict-then-optimize. By and large,\nmachine learning tools are intended to minimize prediction error and do not\naccount for how the predictions will be used in the downstream optimization\nproblem. In contrast, we propose a new and very general framework, called Smart\n\"Predict, then Optimize\" (SPO), which directly leverages the optimization\nproblem structure, i.e., its objective and constraints, for designing better\nprediction models. A key component of our framework is the SPO loss function\nwhich measures the decision error induced by a prediction.\n  Training a prediction model with respect to the SPO loss is computationally\nchallenging, and thus we derive, using duality theory, a convex surrogate loss\nfunction which we call the SPO+ loss. Most importantly, we prove that the SPO+\nloss is statistically consistent with respect to the SPO loss under mild\nconditions. Our SPO+ loss function can tractably handle any polyhedral, convex,\nor even mixed-integer optimization problem with a linear objective. Numerical\nexperiments on shortest path and portfolio optimization problems show that the\nSPO framework can lead to significant improvement under the\npredict-then-optimize paradigm, in particular when the prediction model being\ntrained is misspecified. We find that linear models trained using SPO+ loss\ntend to dominate random forest algorithms, even when the ground truth is highly\nnonlinear. \n\n"}
{"id": "1710.09047", "contents": "Title: Block Coordinate Descent Only Converge to Minimizers Abstract: Given a non-convex twice continuously differentiable cost function with\nLipschitz continuous gradient, we prove that all of block coordinate gradient\ndescent, block mirror descent and proximal block coordinate descent converge to\na local minimizer, almost surely with random initialization. Furthermore, we\nshow that these results also hold true even for the cost functions with\nnon-isolated critical points. \n\n"}
{"id": "1710.09282", "contents": "Title: A Survey of Model Compression and Acceleration for Deep Neural Networks Abstract: Deep neural networks (DNNs) have recently achieved great success in many\nvisual recognition tasks. However, existing deep neural network models are\ncomputationally expensive and memory intensive, hindering their deployment in\ndevices with low memory resources or in applications with strict latency\nrequirements. Therefore, a natural thought is to perform model compression and\nacceleration in deep networks without significantly decreasing the model\nperformance. During the past five years, tremendous progress has been made in\nthis area. In this paper, we review the recent techniques for compacting and\naccelerating DNN models. In general, these techniques are divided into four\ncategories: parameter pruning and quantization, low-rank factorization,\ntransferred/compact convolutional filters, and knowledge distillation. Methods\nof parameter pruning and quantization are described first, after that the other\ntechniques are introduced. For each category, we also provide insightful\nanalysis about the performance, related applications, advantages, and\ndrawbacks. Then we go through some very recent successful methods, for example,\ndynamic capacity networks and stochastic depths networks. After that, we survey\nthe evaluation matrices, the main datasets used for evaluating the model\nperformance, and recent benchmark efforts. Finally, we conclude this paper,\ndiscuss remaining the challenges and possible directions for future work. \n\n"}
{"id": "1710.09738", "contents": "Title: Chance-Constrained ADMM Approach for Decentralized Control of\n  Distributed Energy Resources Abstract: Distribution systems are undergoing a dramatic transition from a passive\ncircuit that routinely disseminates electric power among downstream nodes to\nthe system with distributed energy resources. The distributed energy resources\ncome in a variety of technologies and typically include photovoltaic (PV)\narrays, thermostatically controlled loads, energy storage units. Often these\nresources are interfaced with the system via inverters that can adjust active\nand reactive power injections, thus supporting the operational performance of\nthe system. This paper designs a control policy for such inverters using the\nlocal power flow measurements. The control actuates active and reactive power\ninjections of the inverter-based distributed energy resources. This strategy is\nthen incorporated into a chance-constrained, decentralized optimal power flow\nformulation to maintain voltage levels and power flows within their limits and\nto mitigate the volatility of (PV) resources. \n\n"}
{"id": "1710.10255", "contents": "Title: Sequential Empirical Coordination Under an Output Entropy Constraint Abstract: This paper considers the problem of sequential empirical coordination, where\nthe objective is to achieve a given value of the expected uniform deviation\nbetween state-action empirical averages and statistical expectations under a\ngiven strategic probability measure, with respect to a given universal\nGlivenko-Cantelli class of test functions. A communication constraint is\nimposed on the Shannon entropy of the resulting action sequence. It is shown\nthat the fundamental limit on the output entropy is given by the minimum of the\nmutual information between the state and the action processes under all\nstrategic measures that have the same marginal state process as the target\nmeasure and approximate the target measure to desired accuracy with respect to\nthe underlying Glivenko--Cantelli seminorm. The fundamental limit is shown to\nbe asymptotically achievable by tree-structured codes. \n\n"}
{"id": "1710.10571", "contents": "Title: Certifying Some Distributional Robustness with Principled Adversarial\n  Training Abstract: Neural networks are vulnerable to adversarial examples and researchers have\nproposed many heuristic attack and defense mechanisms. We address this problem\nthrough the principled lens of distributionally robust optimization, which\nguarantees performance under adversarial input perturbations. By considering a\nLagrangian penalty formulation of perturbing the underlying data distribution\nin a Wasserstein ball, we provide a training procedure that augments model\nparameter updates with worst-case perturbations of training data. For smooth\nlosses, our procedure provably achieves moderate levels of robustness with\nlittle computational or statistical cost relative to empirical risk\nminimization. Furthermore, our statistical guarantees allow us to efficiently\ncertify robustness for the population loss. For imperceptible perturbations,\nour method matches or outperforms heuristic approaches. \n\n"}
{"id": "1710.11029", "contents": "Title: Stochastic gradient descent performs variational inference, converges to\n  limit cycles for deep networks Abstract: Stochastic gradient descent (SGD) is widely believed to perform implicit\nregularization when used to train deep neural networks, but the precise manner\nin which this occurs has thus far been elusive. We prove that SGD minimizes an\naverage potential over the posterior distribution of weights along with an\nentropic regularization term. This potential is however not the original loss\nfunction in general. So SGD does perform variational inference, but for a\ndifferent loss than the one used to compute the gradients. Even more\nsurprisingly, SGD does not even converge in the classical sense: we show that\nthe most likely trajectories of SGD for deep networks do not behave like\nBrownian motion around critical points. Instead, they resemble closed loops\nwith deterministic components. We prove that such \"out-of-equilibrium\" behavior\nis a consequence of highly non-isotropic gradient noise in SGD; the covariance\nmatrix of mini-batch gradients for deep networks has a rank as small as 1% of\nits dimension. We provide extensive empirical validation of these claims,\nproven in the appendix. \n\n"}
{"id": "1710.11040", "contents": "Title: How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in\n  Robotics Abstract: Endowing robots with the capability of assessing risk and making risk-aware\ndecisions is widely considered a key step toward ensuring safety for robots\noperating under uncertainty. But, how should a robot quantify risk? A natural\nand common approach is to consider the framework whereby costs are assigned to\nstochastic outcomes - an assignment captured by a cost random variable.\nQuantifying risk then corresponds to evaluating a risk metric, i.e., a mapping\nfrom the cost random variable to a real number. Yet, the question of what\nconstitutes a \"good\" risk metric has received little attention within the\nrobotics community. The goal of this paper is to explore and partially address\nthis question by advocating axioms that risk metrics in robotics applications\nshould satisfy in order to be employed as rational assessments of risk. We\ndiscuss general representation theorems that precisely characterize the class\nof metrics that satisfy these axioms (referred to as distortion risk metrics),\nand provide instantiations that can be used in applications. We further discuss\npitfalls of commonly used risk metrics in robotics, and discuss additional\nproperties that one must consider in sequential decision making tasks. Our hope\nis that the ideas presented here will lead to a foundational framework for\nquantifying risk (and hence safety) in robotics applications. \n\n"}
{"id": "1710.11548", "contents": "Title: Complex Systems Science meets 5G and IoT Abstract: We propose a new paradigm for telecommunications, and develop a framework\ndrawing on concepts from information (i.e., different metrics of complexity)\nand computational (i.e., agent based modeling) theory, adapted from complex\nsystem science. We proceed in a systematic fashion by dividing network\ncomplexity understanding and analysis into different layers. Modelling layer\nforms the foundation of the proposed framework, supporting analysis and tuning\nlayers. The modelling layer aims at capturing the significant attributes of\nnetworks and the interactions that shape them, through the application of tools\nsuch as agent-based modelling and graph theoretical abstractions, to derive new\nmetrics that holistically describe a network. The analysis phase completes the\ncore functionality of the framework by linking our new metrics to the overall\nnetwork performance. The tuning layer augments this core with algorithms that\naim at automatically guiding networks toward desired conditions. In order to\nmaximize the impact of our ideas, the proposed approach is rooted in relevant,\nnear-future architectures and use cases in 5G networks, i.e., Internet of\nThings (IoT) and self-organizing cellular networks. \n\n"}
{"id": "1711.00313", "contents": "Title: Avoiding Your Teacher's Mistakes: Training Neural Networks with\n  Controlled Weak Supervision Abstract: Training deep neural networks requires massive amounts of training data, but\nfor many tasks only limited labeled data is available. This makes weak\nsupervision attractive, using weak or noisy signals like the output of\nheuristic methods or user click-through data for training. In a semi-supervised\nsetting, we can use a large set of data with weak labels to pretrain a neural\nnetwork and then fine-tune the parameters with a small amount of data with true\nlabels. This feels intuitively sub-optimal as these two independent stages\nleave the model unaware about the varying label quality. What if we could\nsomehow inform the model about the label quality? In this paper, we propose a\nsemi-supervised learning method where we train two neural networks in a\nmulti-task fashion: a \"target network\" and a \"confidence network\". The target\nnetwork is optimized to perform a given task and is trained using a large set\nof unlabeled data that are weakly annotated. We propose to weight the gradient\nupdates to the target network using the scores provided by the second\nconfidence network, which is trained on a small amount of supervised data. Thus\nwe avoid that the weight updates computed from noisy labels harm the quality of\nthe target network model. We evaluate our learning strategy on two different\ntasks: document ranking and sentiment classification. The results demonstrate\nthat our approach not only enhances the performance compared to the baselines\nbut also speeds up the learning process from weak labels. \n\n"}
{"id": "1711.00695", "contents": "Title: A Universal Marginalizer for Amortized Inference in Generative Models Abstract: We consider the problem of inference in a causal generative model where the\nset of available observations differs between data instances. We show how\ncombining samples drawn from the graphical model with an appropriate masking\nfunction makes it possible to train a single neural network to approximate all\nthe corresponding conditional marginal distributions and thus amortize the cost\nof inference. We further demonstrate that the efficiency of importance sampling\nmay be improved by basing proposals on the output of the neural network. We\nalso outline how the same network can be used to generate samples from an\napproximate joint posterior via a chain decomposition of the graph. \n\n"}
{"id": "1711.00726", "contents": "Title: A Comprehensive Low and High-level Feature Analysis for Early Rumor\n  Detection on Twitter Abstract: Recent work have done a good job in modeling rumors and detecting them over\nmicroblog streams. However, the performance of their automatic approaches are\nnot relatively high when looking early in the diffusion. A first intuition is\nthat, at early stage, most of the aggregated rumor features (e.g., propagation\nfeatures) are not mature and distinctive enough. The objective of rumor\ndebunking in microblogs, however, are to detect these misinformation as early\nas possible. In this work, we leverage neural models in learning the hidden\nrepresentations of individual rumor-related tweets at the very beginning of a\nrumor. Our extensive experiments show that the resulting signal improves our\nclassification performance over time, significantly within the first 10 hours.\nTo deepen the understanding of these low and high-level features in\ncontributing to the model performance over time, we conduct an extensive study\non a wide range of high impact rumor features for the 48 hours range. The end\nmodel that engages these features are shown to be competitive, reaches over 90%\naccuracy and out-performs strong baselines in our carefully cured dataset. \n\n"}
{"id": "1711.00946", "contents": "Title: Learning Linear Dynamical Systems via Spectral Filtering Abstract: We present an efficient and practical algorithm for the online prediction of\ndiscrete-time linear dynamical systems with a symmetric transition matrix. We\ncircumvent the non-convex optimization problem using improper learning:\ncarefully overparameterize the class of LDSs by a polylogarithmic factor, in\nexchange for convexity of the loss functions. From this arises a\npolynomial-time algorithm with a near-optimal regret guarantee, with an\nanalogous sample complexity bound for agnostic learning. Our algorithm is based\non a novel filtering technique, which may be of independent interest: we\nconvolve the time series with the eigenvectors of a certain Hankel matrix. \n\n"}
{"id": "1711.01521", "contents": "Title: Stochastic Greedy Algorithms For Multiple Measurement Vectors Abstract: Sparse representation of a single measurement vector (SMV) has been explored\nin a variety of compressive sensing applications. Recently, SMV models have\nbeen extended to solve multiple measurement vectors (MMV) problems, where the\nunderlying signal is assumed to have joint sparse structures. To circumvent the\nNP-hardness of the $\\ell_0$ minimization problem, many deterministic MMV\nalgorithms solve the convex relaxed models with limited efficiency. In this\npaper, we develop stochastic greedy algorithms for solving the joint sparse MMV\nreconstruction problem. In particular, we propose the MMV Stochastic Iterative\nHard Thresholding (MStoIHT) and MMV Stochastic Gradient Matching Pursuit\n(MStoGradMP) algorithms, and we also utilize the mini-batching technique to\nfurther improve their performance. Convergence analysis indicates that the\nproposed algorithms are able to converge faster than their SMV counterparts,\ni.e., concatenated StoIHT and StoGradMP, under certain conditions. Numerical\nexperiments have illustrated the superior effectiveness of the proposed\nalgorithms over their SMV counterparts. \n\n"}
{"id": "1711.01526", "contents": "Title: On Identification of Distribution Grids Abstract: Large-scale integration of distributed energy resources into residential\ndistribution feeders necessitates careful control of their operation through\npower flow analysis. While the knowledge of the distribution system model is\ncrucial for this type of analysis, it is often unavailable or outdated. The\nrecent introduction of synchrophasor technology in low-voltage distribution\ngrids has created an unprecedented opportunity to learn this model from\nhigh-precision, time-synchronized measurements of voltage and current phasors\nat various locations. This paper focuses on joint estimation of model\nparameters (admittance values) and operational structure of a poly-phase\ndistribution network from the available telemetry data via the lasso, a method\nfor regression shrinkage and selection. We propose tractable convex programs\ncapable of tackling the low rank structure of the distribution system and\ndevelop an online algorithm for early detection and localization of critical\nevents that induce a change in the admittance matrix. The efficacy of these\ntechniques is corroborated through power flow studies on four three-phase\nradial distribution systems serving real household demands. \n\n"}
{"id": "1711.01761", "contents": "Title: AdaBatch: Efficient Gradient Aggregation Rules for Sequential and\n  Parallel Stochastic Gradient Methods Abstract: We study a new aggregation operator for gradients coming from a mini-batch\nfor stochastic gradient (SG) methods that allows a significant speed-up in the\ncase of sparse optimization problems. We call this method AdaBatch and it only\nrequires a few lines of code change compared to regular mini-batch SGD\nalgorithms. We provide a theoretical insight to understand how this new class\nof algorithms is performing and show that it is equivalent to an implicit\nper-coordinate rescaling of the gradients, similarly to what Adagrad methods\ncan do. In theory and in practice, this new aggregation allows to keep the same\nsample efficiency of SG methods while increasing the batch size.\nExperimentally, we also show that in the case of smooth convex optimization,\nour procedure can even obtain a better loss when increasing the batch size for\na fixed number of samples. We then apply this new algorithm to obtain a\nparallelizable stochastic gradient method that is synchronous but allows\nspeed-up on par with Hogwild! methods as convergence does not deteriorate with\nthe increase of the batch size. The same approach can be used to make\nmini-batch provably efficient for variance-reduced SG methods such as SVRG. \n\n"}
{"id": "1711.02666", "contents": "Title: Tensor-Generative Adversarial Network with Two-dimensional Sparse\n  Coding: Application to Real-time Indoor Localization Abstract: Localization technology is important for the development of indoor\nlocation-based services (LBS). Global Positioning System (GPS) becomes invalid\nin indoor environments due to the non-line-of-sight issue, so it is urgent to\ndevelop a real-time high-accuracy localization approach for smartphones.\nHowever, accurate localization is challenging due to issues such as real-time\nresponse requirements, limited fingerprint samples and mobile device storage.\nTo address these problems, we propose a novel deep learning architecture:\nTensor-Generative Adversarial Network (TGAN).\n  We first introduce a transform-based 3D tensor to model fingerprint samples.\nInstead of those passive methods that construct a fingerprint database as a\nprior, our model applies artificial neural network with deep learning to train\nnetwork classifiers and then gives out estimations. Then we propose a novel\ntensor-based super-resolution scheme using the generative adversarial network\n(GAN) that adopts sparse coding as the generator network and a residual\nlearning network as the discriminator. Further, we analyze the performance of\ntensor-GAN and implement a trace-based localization experiment, which achieves\nbetter performance. Compared to existing methods for smartphones indoor\npositioning, that are energy-consuming and high demands on devices, TGAN can\ngive out an improved solution in localization accuracy, response time and\nimplementation complexity. \n\n"}
{"id": "1711.02733", "contents": "Title: State Observers for Sensorless Control of Magnetic Levitation Systems Abstract: In this paper we address the problem of state observation for sensorless\ncontrol of nonlinear magnetic levitation systems, that is, the regulation of\nthe position of a levitated object measuring only the voltage and current of\nthe electrical supply. Instrumental for the development of the theory is the\nuse of parameter estimation-based observers, which combined with the dynamic\nregressor extension and mixing parameter estimation technique, allow the\nreconstruction of the magnetic flux. With the knowledge of the latter it is\nshown that the mechanical coordinates can be estimated with suitably tailored\nnonlinear observers. Replacing the observed states, in a certainty equivalent\nmanner, with a full information globally stabilising law completes the\nsensorless controller design. We consider one and two-degrees-of-freedom\nsystems that, interestingly, demand totally different mathematical approaches\nfor their solutions. Simulation results are used to illustrate the performance\nof the proposed schemes. \n\n"}
{"id": "1711.03167", "contents": "Title: Learning Markov Chain in Unordered Dataset Abstract: The assumption that data samples are independently identically distributed is\nthe backbone of many learning algorithms. Nevertheless, datasets often exhibit\nrich structure in practice, and we argue that there exist some unknown order\nwithin the data instances. In this technical report, we introduce OrderNet that\ncan be used to extract the order of data instances in an unsupervised way. By\nassuming that the instances are sampled from a Markov chain, our goal is to\nlearn the transitional operator of the underlying Markov chain, as well as the\norder by maximizing the generation probability under all possible data\npermutations. Specifically, we use neural network as a compact and soft lookup\ntable to approximate the possibly huge, but discrete transition matrix. This\nstrategy allows us to amortize the space complexity with a single model.\nFurthermore, this simple and compact representation also provides a short\ndescription to the dataset and generalizes to unseen instances as well. To\nensure that the learned Markov chain is ergodic, we propose a greedy batch-wise\npermutation scheme that allows fast training. Empirically, we show that\nOrderNet is able to discover an order among data instances. We also extend the\nproposed OrderNet to one-shot recognition task and demonstrate favorable\nresults. \n\n"}
{"id": "1711.03247", "contents": "Title: The nonsmooth landscape of phase retrieval Abstract: We consider a popular nonsmooth formulation of the real phase retrieval\nproblem. We show that under standard statistical assumptions, a simple\nsubgradient method converges linearly when initialized within a constant\nrelative distance of an optimal solution. Seeking to understand the\ndistribution of the stationary points of the problem, we complete the paper by\nproving that as the number of Gaussian measurements increases, the stationary\npoints converge to a codimension two set, at a controlled rate. Experiments on\nimage recovery problems illustrate the developed algorithm and theory. \n\n"}
{"id": "1711.03704", "contents": "Title: A Complete Semidefinite Algorithm for Detecting Copositive Matrices and\n  Tensors Abstract: A real symmetric matrix (resp., tensor) is said to be copositive if the\nassociated quadratic (resp., homogeneous) form is greater than or equal to zero\nover the nonnegative orthant. The problem of detecting their copositivity is\nNP-hard. This paper proposes a complete semidefinite relaxation algorithm for\ndetecting the copositivity of a matrix or tensor. If it is copositive, the\nalgorithm can get a certificate for the copositivity. If it is not, the\nalgorithm can get a point that refutes the copositivity. We show that the\ndetection can be done by solving a finite number of semidefinite relaxations,\nfor all matrices and tensors. \n\n"}
{"id": "1711.04851", "contents": "Title: Learning and Visualizing Localized Geometric Features Using 3D-CNN: An\n  Application to Manufacturability Analysis of Drilled Holes Abstract: 3D Convolutional Neural Networks (3D-CNN) have been used for object\nrecognition based on the voxelized shape of an object. However, interpreting\nthe decision making process of these 3D-CNNs is still an infeasible task. In\nthis paper, we present a unique 3D-CNN based Gradient-weighted Class Activation\nMapping method (3D-GradCAM) for visual explanations of the distinct local\ngeometric features of interest within an object. To enable efficient learning\nof 3D geometries, we augment the voxel data with surface normals of the object\nboundary. We then train a 3D-CNN with this augmented data and identify the\nlocal features critical for decision-making using 3D GradCAM. An application of\nthis feature identification framework is to recognize difficult-to-manufacture\ndrilled hole features in a complex CAD geometry. The framework can be extended\nto identify difficult-to-manufacture features at multiple spatial scales\nleading to a real-time design for manufacturability decision support system. \n\n"}
{"id": "1711.07970", "contents": "Title: Deep Learning for Physical Processes: Incorporating Prior Scientific\n  Knowledge Abstract: We consider the use of Deep Learning methods for modeling complex phenomena\nlike those occurring in natural physical processes. With the large amount of\ndata gathered on these phenomena the data intensive paradigm could begin to\nchallenge more traditional approaches elaborated over the years in fields like\nmaths or physics. However, despite considerable successes in a variety of\napplication domains, the machine learning field is not yet ready to handle the\nlevel of complexity required by such problems. Using an example application,\nnamely Sea Surface Temperature Prediction, we show how general background\nknowledge gained from physics could be used as a guideline for designing\nefficient Deep Learning models. In order to motivate the approach and to assess\nits generality we demonstrate a formal link between the solution of a class of\ndifferential equations underlying a large family of physical phenomena and the\nproposed model. Experiments and comparison with series of baselines including a\nstate of the art numerical approach is then provided. \n\n"}
{"id": "1711.07979", "contents": "Title: Posterior Sampling for Large Scale Reinforcement Learning Abstract: We propose a practical non-episodic PSRL algorithm that unlike recent\nstate-of-the-art PSRL algorithms uses a deterministic, model-independent\nepisode switching schedule. Our algorithm termed deterministic schedule PSRL\n(DS-PSRL) is efficient in terms of time, sample, and space complexity. We prove\na Bayesian regret bound under mild assumptions. Our result is more generally\napplicable to multiple parameters and continuous state action problems. We\ncompare our algorithm with state-of-the-art PSRL algorithms on standard\ndiscrete and continuous problems from the literature. Finally, we show how the\nassumptions of our algorithm satisfy a sensible parametrization for a large\nclass of problems in sequential recommendations. \n\n"}
{"id": "1711.09065", "contents": "Title: Conditions on Shifted Passivity of Port-Hamiltonian Systems Abstract: In this paper, we examine the shifted passivity property of port-Hamiltonian\nsystems. Shifted passivity accounts for the fact that in many applications the\ndesired steady-state values of the input and output variables are nonzero, and\nthus one is interested in passivity with respect to the shifted signals. We\nconsider port-Hamiltonian systems with strictly convex Hamiltonian, and derive\nconditions under which shifted passivity is guaranteed. In case the Hamiltonian\nis quadratic and state dependency appears in an affine manner in the\ndissipation and interconnection matrices, our conditions reduce to negative\nsemidefiniteness of an appropriately constructed constant matrix. Moreover, we\nelaborate on how these conditions can be extended to the case when the shifted\npassivity property can be enforced via output feedback, thus paving the path\nfor controller design. Stability of forced equilibria of the system is analyzed\ninvoking the proposed passivity conditions. The utility and relevance of the\nresults are illustrated with their application to a 6th order synchronous\ngenerator model as well as a controlled rigid body system. \n\n"}
{"id": "1711.09154", "contents": "Title: Observer-Side Parameter Estimation For Adaptive Control Abstract: In adaptive control, a controller is precisely designed for a certain model\nof the system, but that model's parameters are updated online by another\nmechanism called the adaptive update. This allows the controller to aim for the\nbenefits of exact model knowledge while simultaneously remaining robust to\nmodel uncertainty.\n  Like most nonlinear controllers, adaptive controllers are often designed and\nanalyzed under the assumption of deterministic full state feedback. However,\ndoing so inherently decouples the adaptive update mechanism from the\nprobabilistic information provided by modern state observers.\n  The simplest way to reconcile this is to let the observer produce both state\nestimates and model parameter estimates, so that all probabilistic information\nis shared within the framework of the observer. While this technique is\nbecoming increasingly common, it is still not widely accepted due to a lack of\ngeneral closed-loop stability proofs.\n  In this thesis, we will investigate observer-side parameter estimation for\nadaptive control by precisely juxtaposing its mechanics against the current,\nwidely accepted adaptive control designs. Additionally, we will propose a new\ntechnique that increases the robustness of observer-based adaptive control by\nfollowing the same line of reasoning used for the popular concurrent learning\nmethod. \n\n"}
{"id": "1711.09704", "contents": "Title: Multi-scale Transactive Control In Interconnected Bulk Power Systems\n  Under High Renewable Energy Supply and High Demand Response Scenarios Abstract: This thesis presents the design, analysis, and validation of a hierarchical\ntransactive control system that engages demand response resources to enhance\nthe integration of renewable electricity generation resources. This control\nsystem joins energy, capacity and regulation markets together in a unified\nhomeostatic and economically efficient electricity operation that increases\ntotal surplus while improving reliability and decreasing carbon emissions from\nfossil-based generation resources.\n  The work encompasses: (1) the derivation of a short-term demand response\nmodel suitable for transactive control systems and its validation with field\ndemonstration data; (2) an aggregate load model that enables effective control\nof large populations of thermal loads using a new type of thermostat (discrete\ntime with zero deadband); (3) a methodology for optimally controlling response\nto frequency deviations while tracking schedule area exports in areas that have\nhigh penetration of both intermittent renewable resources and fast-acting\ndemand response; and (4) the development of a system-wide (continental\ninterconnection) scale strategy for optimal power trajectory and resource\ndispatch based on a shift from primarily energy cost-based approach to a\nprimarily ramping cost-based one.\n  The results show that multi-layer transactive control systems can be\nconstructed, will enhance renewable resource utilization, and will operate in a\ncoordinated manner with bulk power systems that include both regions with and\nwithout organized power markets. Estimates of Western Electric Coordionating\nCouncil (WECC) system cost savings under target renewable energy generation\nlevels resulting from the proposed system exceed US$150B annually by the year\n2024, when compared to the existing control system. \n\n"}
{"id": "1711.10051", "contents": "Title: Active Regression via Linear-Sample Sparsification Abstract: We present an approach that improves the sample complexity for a variety of\ncurve fitting problems, including active learning for linear regression,\npolynomial regression, and continuous sparse Fourier transforms. In the active\nlinear regression problem, one would like to estimate the least squares\nsolution $\\beta^*$ minimizing $\\|X\\beta - y\\|_2$ given the entire unlabeled\ndataset $X \\in \\mathbb{R}^{n \\times d}$ but only observing a small number of\nlabels $y_i$. We show that $O(d)$ labels suffice to find a constant factor\napproximation $\\tilde{\\beta}$:\n  \\[\n  \\mathbb{E}[\\|X\\tilde{\\beta} - y\\|_2^2] \\leq 2 \\mathbb{E}[\\|X \\beta^* -\ny\\|_2^2].\n  \\] This improves on the best previous result of $O(d \\log d)$ from leverage\nscore sampling. We also present results for the \\emph{inductive} setting,\nshowing when $\\tilde{\\beta}$ will generalize to fresh samples; these apply to\ncontinuous settings such as polynomial regression. Finally, we show how the\ntechniques yield improved results for the non-linear sparse Fourier transform\nsetting. \n\n"}
{"id": "1711.10566", "contents": "Title: Physics Informed Deep Learning (Part II): Data-driven Discovery of\n  Nonlinear Partial Differential Equations Abstract: We introduce physics informed neural networks -- neural networks that are\ntrained to solve supervised learning tasks while respecting any given law of\nphysics described by general nonlinear partial differential equations. In this\nsecond part of our two-part treatise, we focus on the problem of data-driven\ndiscovery of partial differential equations. Depending on whether the available\ndata is scattered in space-time or arranged in fixed temporal snapshots, we\nintroduce two main classes of algorithms, namely continuous time and discrete\ntime models. The effectiveness of our approach is demonstrated using a wide\nrange of benchmark problems in mathematical physics, including conservation\nlaws, incompressible fluid flow, and the propagation of nonlinear shallow-water\nwaves. \n\n"}
{"id": "1711.10589", "contents": "Title: Contextual Outlier Interpretation Abstract: Outlier detection plays an essential role in many data-driven applications to\nidentify isolated instances that are different from the majority. While many\nstatistical learning and data mining techniques have been used for developing\nmore effective outlier detection algorithms, the interpretation of detected\noutliers does not receive much attention. Interpretation is becoming\nincreasingly important to help people trust and evaluate the developed models\nthrough providing intrinsic reasons why the certain outliers are chosen. It is\ndifficult, if not impossible, to simply apply feature selection for explaining\noutliers due to the distinct characteristics of various detection models,\ncomplicated structures of data in certain applications, and imbalanced\ndistribution of outliers and normal instances. In addition, the role of\ncontrastive contexts where outliers locate, as well as the relation between\noutliers and contexts, are usually overlooked in interpretation. To tackle the\nissues above, in this paper, we propose a novel Contextual Outlier\nINterpretation (COIN) method to explain the abnormality of existing outliers\nspotted by detectors. The interpretability for an outlier is achieved from\nthree aspects: outlierness score, attributes that contribute to the\nabnormality, and contextual description of its neighborhoods. Experimental\nresults on various types of datasets demonstrate the flexibility and\neffectiveness of the proposed framework compared with existing interpretation\napproaches. \n\n"}
{"id": "1711.10783", "contents": "Title: Partial Consensus and Conservative Fusion of Gaussian Mixtures for\n  Distributed PHD Fusion Abstract: We propose a novel consensus notion, called \"partial consensus\", for\ndistributed GM-PHD (Gaussian mixture probability hypothesis density) fusion\nbased on a peer-to-peer (P2P) sensor network, in which only highly-weighted\nposterior Gaussian components (GCs) are disseminated in the P2P communication\nfor fusion while the insignificant GCs are not involved. The partial consensus\ndoes not only enjoy high efficiency in both network communication and local\nfusion computation, but also significantly reduces the affect of potential\nfalse data (clutter) to the filter, leading to increased signal-to-noise ratio\nat local sensors. Two \"conservative\" mixture reduction schemes are advocated\nfor fusing the shared GCs in a fully distributed manner. One is given by\npairwise averaging GCs between sensors based on Hungarian assignment and the\nother is merging close GCs based a new GM merging scheme. The proposed\napproaches have a close connection to the conservative fusion approaches known\nas covariance union and arithmetic mean density. In parallel, average consensus\nis sought on the cardinality distribution (namely the GM weight sum) among\nsensors. Simulations for tracking either a single target or multiple targets\nthat simultaneously appear are presented based on a sensor network where each\nsensor operates a GM-PHD filter, in order to compare our approaches with the\nbenchmark generalized covariance intersection approach. The results demonstrate\nthat the partial, arithmetic average, consensus outperforms the complete,\ngeometric average, consensus. \n\n"}
{"id": "1711.11183", "contents": "Title: Strategic Topology Switching for Security-Part I: Consensus & Switching\n  Times Abstract: In this two-part paper, we consider strategic topology switching for the\nsecond-order multi-agent systems under a special class of stealthy attacks,\nnamely the \"zero-dynamics\" attack (ZDA). The main mathematical tool proposed\nhere is to strategically switch the network topology to detect a possible ZDA.\nHowever, it is not clear a priori that such a switching strategy still yields\nconsensus in this switched system, in the normal (un-attacked) operation mode.\nIn Part I, we propose a strategy on the switching times that enables the\ntopology-switching algorithm proposed in Part II to reach the second-order\nconsensus in the absence of a ZDA. Utilizing the theory of stable switched\nlinear systems with unstable subsystems, we characterize sufficient conditions\nfor the dwell time of topology-switching signal to reach consensus. Building on\nthis characterization, we then propose a decentralized time-dependent\ntopology-switching algorithm. The proposed algorithm, used in conjunction with\na simplified control protocol, achieves consensus while providing substantial\nadvantages over other control approaches: it relies only on the relative\nposition measurements (without any requirement for velocity measurements); and\nit does not impose any constraint on the magnitudes of coupling weights. We\nfinally demonstrate our theoretical findings via the numerical simulation\nresults. \n\n"}
{"id": "1712.00673", "contents": "Title: Towards Robust Neural Networks via Random Self-ensemble Abstract: Recent studies have revealed the vulnerability of deep neural networks: A\nsmall adversarial perturbation that is imperceptible to human can easily make a\nwell-trained deep neural network misclassify. This makes it unsafe to apply\nneural networks in security-critical applications. In this paper, we propose a\nnew defense algorithm called Random Self-Ensemble (RSE) by combining two\nimportant concepts: {\\bf randomness} and {\\bf ensemble}. To protect a targeted\nmodel, RSE adds random noise layers to the neural network to prevent the strong\ngradient-based attacks, and ensembles the prediction over random noises to\nstabilize the performance. We show that our algorithm is equivalent to ensemble\nan infinite number of noisy models $f_\\epsilon$ without any additional memory\noverhead, and the proposed training procedure based on noisy stochastic\ngradient descent can ensure the ensemble model has a good predictive\ncapability. Our algorithm significantly outperforms previous defense techniques\non real data sets. For instance, on CIFAR-10 with VGG network (which has 92\\%\naccuracy without any attack), under the strong C\\&W attack within a certain\ndistortion tolerance, the accuracy of unprotected model drops to less than\n10\\%, the best previous defense technique has $48\\%$ accuracy, while our method\nstill has $86\\%$ prediction accuracy under the same level of attack. Finally,\nour method is simple and easy to integrate into any neural network. \n\n"}
{"id": "1712.00828", "contents": "Title: Tensor Train Neighborhood Preserving Embedding Abstract: In this paper, we propose a Tensor Train Neighborhood Preserving Embedding\n(TTNPE) to embed multi-dimensional tensor data into low dimensional tensor\nsubspace. Novel approaches to solve the optimization problem in TTNPE are\nproposed. For this embedding, we evaluate novel trade-off gain among\nclassification, computation, and dimensionality reduction (storage) for\nsupervised learning. It is shown that compared to the state-of-the-arts tensor\nembedding methods, TTNPE achieves superior trade-off in classification,\ncomputation, and dimensionality reduction in MNIST handwritten digits and\nWeizmann face datasets. \n\n"}
{"id": "1712.01033", "contents": "Title: NEON+: Accelerated Gradient Methods for Extracting Negative Curvature\n  for Non-Convex Optimization Abstract: Accelerated gradient (AG) methods are breakthroughs in convex optimization,\nimproving the convergence rate of the gradient descent method for optimization\nwith smooth functions. However, the analysis of AG methods for non-convex\noptimization is still limited. It remains an open question whether AG methods\nfrom convex optimization can accelerate the convergence of the gradient descent\nmethod for finding local minimum of non-convex optimization problems. This\npaper provides an affirmative answer to this question. In particular, we\nanalyze two renowned variants of AG methods (namely Polyak's Heavy Ball method\nand Nesterov's Accelerated Gradient method) for extracting the negative\ncurvature from random noise, which is central to escaping from saddle points.\nBy leveraging the proposed AG methods for extracting the negative curvature, we\npresent a new AG algorithm with double loops for non-convex\noptimization~\\footnote{this is in contrast to a single-loop AG algorithm\nproposed in a recent manuscript~\\citep{AGNON}, which directly analyzed the\nNesterov's AG method for non-convex optimization and appeared online on\nNovember 29, 2017. However, we emphasize that our work is an independent work,\nwhich is inspired by our earlier work~\\citep{NEON17} and is based on a\ndifferent novel analysis.}, which converges to second-order stationary point\n$\\x$ such that $\\|\\nabla f(\\x)\\|\\leq \\epsilon$ and $\\nabla^2 f(\\x)\\geq\n-\\sqrt{\\epsilon} I$ with $\\widetilde O(1/\\epsilon^{1.75})$ iteration\ncomplexity, improving that of gradient descent method by a factor of\n$\\epsilon^{-0.25}$ and matching the best iteration complexity of second-order\nHessian-free methods for non-convex optimization. \n\n"}
{"id": "1712.01232", "contents": "Title: Minimal Input Selection for Robust Control Abstract: This paper studies the problem of selecting a minimum-size set of input nodes\nto guarantee stability of a networked system in the presence of uncertainties\nand time delays. Current approaches to input selection in networked dynamical\nsystems focus on nominal systems with known parameter values in the absence of\ndelays. We derive sufficient conditions for existence of a stabilizing\ncontroller for an uncertain system that are based on a subset of system modes\nlying within the controllability subspace induced by the set of inputs. We then\nformulate the minimum input selection problem and prove that it is equivalent\nto a discrete optimization problem with bounded submodularity ratio, leading to\npolynomial-time algorithms with provable optimality bounds. We show that our\napproach is applicable to different types of uncertainties, including additive\nand multiplicative uncertainties in the system matrices as well as uncertain\ntime delays. We demonstrate our approach in a numerical case study on the IEEE\n39-bus test power system. \n\n"}
{"id": "1712.01536", "contents": "Title: Robust Optimization Approaches for the Design of an Electric Machine Abstract: In this paper two formulations for the robust optimization of the size of the\npermanent magnet in a synchronous machine are discussed. The optimization is\nconstrained by a partial differential equation to describe the electromagnetic\nbehavior of the machine. The need for a robust optimization procedure\noriginates from the fact that optimization parameters have deviations. The\nfirst approach, i.e., \\textcolor{red}{worst-case} optimization, makes use of\nlocal sensitivities. The second approach takes into account expectation values\nand standard deviations. The latter are associated with global sensitivities.\nThe geometry parametrization is elegantly handled thanks to the introduction of\nan affine decomposition. Since the stochastic quantities are determined by\ntools from uncertainty quantification (UQ) and thus require a lot of finite\nelement evaluations, model order reduction is used in order to increase the\nefficiency of the procedure. It is shown that both approaches are equivalent if\na linearization is carried out. \\textcolor{blue}{This finding is supported by\nthe application on an electric machine. The optimization algorithms used are\nsequential quadratic programming, particle swarm optimization and genetic\nalgorithm}. While both formulations reduce the size of the magnets, the UQ\nbased optimization approach is less pessimistic with respect to deviations and\nyields smaller magnets. \n\n"}
{"id": "1712.03010", "contents": "Title: Coordinate Descent with Bandit Sampling Abstract: Coordinate descent methods usually minimize a cost function by updating a\nrandom decision variable (corresponding to one coordinate) at a time. Ideally,\nwe would update the decision variable that yields the largest decrease in the\ncost function. However, finding this coordinate would require checking all of\nthem, which would effectively negate the improvement in computational\ntractability that coordinate descent is intended to afford. To address this, we\npropose a new adaptive method for selecting a coordinate. First, we find a\nlower bound on the amount the cost function decreases when a coordinate is\nupdated. We then use a multi-armed bandit algorithm to learn which coordinates\nresult in the largest lower bound by interleaving this learning with\nconventional coordinate descent updates except that the coordinate is selected\nproportionately to the expected decrease. We show that our approach improves\nthe convergence of coordinate descent methods both theoretically and\nexperimentally. \n\n"}
{"id": "1712.04006", "contents": "Title: Training Ensembles to Detect Adversarial Examples Abstract: We propose a new ensemble method for detecting and classifying adversarial\nexamples generated by state-of-the-art attacks, including DeepFool and C&W. Our\nmethod works by training the members of an ensemble to have low classification\nerror on random benign examples while simultaneously minimizing agreement on\nexamples outside the training distribution. We evaluate on both MNIST and\nCIFAR-10, against oblivious and both white- and black-box adversaries. \n\n"}
{"id": "1712.05245", "contents": "Title: Pointwise Convolutional Neural Networks Abstract: Deep learning with 3D data such as reconstructed point clouds and CAD models\nhas received great research interests recently. However, the capability of\nusing point clouds with convolutional neural network has been so far not fully\nexplored. In this paper, we present a convolutional neural network for semantic\nsegmentation and object recognition with 3D point clouds. At the core of our\nnetwork is pointwise convolution, a new convolution operator that can be\napplied at each point of a point cloud. Our fully convolutional network design,\nwhile being surprisingly simple to implement, can yield competitive accuracy in\nboth semantic segmentation and object recognition task. \n\n"}
{"id": "1712.05273", "contents": "Title: Asymptotic Network Robustness Abstract: This paper examines the dependence of network performance measures on network\nsize and considers scaling results for large networks. We connect two\nperformance measures that are well studied, but appear to be unrelated. The\nfirst measure is concerned with energy metrics, namely the $\\Hcal_2$--norm of a\nnetwork, which arises in control theory applications. The second measure is\nconcerned with the notion of \"tail risk\" which arises in economic and financial\nnetworks. We study the question of why such performance measures may\ndeteriorate at a faster rate than the growth rate of the network. We first\nfocus on the energy metric and its well known connection to controllability\nGramian of the underlying dynamical system. We show that undirected networks\nexhibit the most graceful energy growth rates as network size grows. This rate\nis quantified completely by the proximity of spectral radius to unity or\ndistance to instability. In contrast, we show that the simple characterization\nof energy in terms of network spectrum does not exist for directed networks. We\ndemonstrate that, for any fixed distance to instability, energy of a directed\nnetwork can grow at an exponentially faster rate. We provide general methods\nfor manipulating networks to reduce energy. In particular, we prove that\ncertain operations that increase the symmetry in a network cannot increase\nenergy (in an order sense). Secondly, we focus on tail risk in economic and\nfinancial networks. In contrast to $\\Hcal_2$--norm which arises from computing\nthe expectation of energy in the network, tail risk focuses on tail probability\nbehavior of network variables. Although the two measures differ substantially\nwe show that they are precisely connected through the system Gramian. This\nsurprising result explains why topology considerations rather than specific\nperformance measures dictate the large scale behavior of networks. \n\n"}
{"id": "1712.06787", "contents": "Title: Model Predictive BESS Control for Demand Charge Management and\n  PV-Utilization Improvement Abstract: Adoption of battery energy storage systems for behind-the-meters application\noffers valuable benefits for demand charge management as well as increasing\nPV-utilization. The key point is that while the benefit/cost ratio for a single\napplication may not be favorable for economic benefits of storage systems,\nstacked services can provide multiple revenue streams for the same investment.\nUnder this framework, we propose a model predictive controller to reduce demand\ncharge cost and enhance PV-utilization level simultaneously. Different load\npatterns have been considered in this study and results are compared to the\nconventional rule-based controller. The results verified that the proposed\ncontroller provides satisfactory performance by improving the PV-utilization\nrate between 60% to 80% without significant changes in demand charge (DC)\nsaving. Furthermore, our results suggest that batteries can be used for\nstacking multiple services to improve their benefits. Quantitative analysis for\nPV-utilization as a function of battery size and prediction time window has\nalso been carried out. \n\n"}
{"id": "1712.08226", "contents": "Title: A Primal-Dual Method for Optimal Control and Trajectory Generation in\n  High-Dimensional Systems Abstract: Presented is a method for efficient computation of the Hamilton-Jacobi (HJ)\nequation for time-optimal control problems using the generalized Hopf formula.\nTypically, numerical methods to solve the HJ equation rely on a discrete grid\nof the solution space and exhibit exponential scaling with dimension. The\ngeneralized Hopf formula avoids the use of grids and numerical gradients by\nformulating an unconstrained convex optimization problem. The solution at each\npoint is completely independent, and allows a massively parallel implementation\nif solutions at multiple points are desired. This work presents a primal-dual\nmethod for efficient numeric solution and presents how the resulting optimal\ntrajectory can be generated directly from the solution of the Hopf formula,\nwithout further optimization. Examples presented have execution times on the\norder of milliseconds and experiments show computation scales approximately\npolynomial in dimension with very small high-order coefficients. \n\n"}
{"id": "1712.08447", "contents": "Title: On Reduced Input-Output Dynamic Mode Decomposition Abstract: The identification of reduced-order models from high-dimensional data is a\nchallenging task, and even more so if the identified system should not only be\nsuitable for a certain data set, but generally approximate the input-output\nbehavior of the data source. In this work, we consider the input-output dynamic\nmode decomposition method for system identification. We compare excitation\napproaches for the data-driven identification process and describe an\noptimization-based stabilization strategy for the identified systems. \n\n"}
{"id": "1712.09677", "contents": "Title: Momentum and Stochastic Momentum for Stochastic Gradient, Newton,\n  Proximal Point and Subspace Descent Methods Abstract: In this paper we study several classes of stochastic optimization algorithms\nenriched with heavy ball momentum. Among the methods studied are: stochastic\ngradient descent, stochastic Newton, stochastic proximal point and stochastic\ndual subspace ascent. This is the first time momentum variants of several of\nthese methods are studied. We choose to perform our analysis in a setting in\nwhich all of the above methods are equivalent. We prove global nonassymptotic\nlinear convergence rates for all methods and various measures of success,\nincluding primal function values, primal iterates (in L2 sense), and dual\nfunction values. We also show that the primal iterates converge at an\naccelerated linear rate in the L1 sense. This is the first time a linear rate\nis shown for the stochastic heavy ball method (i.e., stochastic gradient\ndescent method with momentum). Under somewhat weaker conditions, we establish a\nsublinear convergence rate for Cesaro averages of primal iterates. Moreover, we\npropose a novel concept, which we call stochastic momentum, aimed at decreasing\nthe cost of performing the momentum step. We prove linear convergence of\nseveral stochastic methods with stochastic momentum, and show that in some\nsparse data regimes and for sufficiently small momentum parameters, these\nmethods enjoy better overall complexity than methods with deterministic\nmomentum. Finally, we perform extensive numerical testing on artificial and\nreal datasets, including data coming from average consensus problems. \n\n"}
{"id": "1712.09983", "contents": "Title: Random Feature-based Online Multi-kernel Learning in Environments with\n  Unknown Dynamics Abstract: Kernel-based methods exhibit well-documented performance in various nonlinear\nlearning tasks. Most of them rely on a preselected kernel, whose prudent choice\npresumes task-specific prior information. Especially when the latter is not\navailable, multi-kernel learning has gained popularity thanks to its\nflexibility in choosing kernels from a prescribed kernel dictionary. Leveraging\nthe random feature approximation and its recent orthogonality-promoting\nvariant, the present contribution develops a scalable multi-kernel learning\nscheme (termed Raker) to obtain the sought nonlinear learning function `on the\nfly,' first for static environments. To further boost performance in dynamic\nenvironments, an adaptive multi-kernel learning scheme (termed AdaRaker) is\ndeveloped. AdaRaker accounts not only for data-driven learning of kernel\ncombination, but also for the unknown dynamics. Performance is analyzed in\nterms of both static and dynamic regrets. AdaRaker is uniquely capable of\ntracking nonlinear learning functions in environments with unknown dynamics,\nand with with analytic performance guarantees. Tests with synthetic and real\ndatasets are carried out to showcase the effectiveness of the novel algorithms. \n\n"}
{"id": "1801.00283", "contents": "Title: Restricted Boltzmann Machines for Robust and Fast Latent Truth Discovery Abstract: We address the problem of latent truth discovery, LTD for short, where the\ngoal is to discover the underlying true values of entity attributes in the\npresence of noisy, conflicting or incomplete information. Despite a multitude\nof algorithms to address the LTD problem that can be found in literature, only\nlittle is known about their overall performance with respect to effectiveness\n(in terms of truth discovery capabilities), efficiency and robustness. A\npractical LTD approach should satisfy all these characteristics so that it can\nbe applied to heterogeneous datasets of varying quality and degrees of\ncleanliness.\n  We propose a novel algorithm for LTD that satisfies the above requirements.\nThe proposed model is based on Restricted Boltzmann Machines, thus coined\nLTD-RBM. In extensive experiments on various heterogeneous and publicly\navailable datasets, LTD-RBM is superior to state-of-the-art LTD techniques in\nterms of an overall consideration of effectiveness, efficiency and robustness. \n\n"}
{"id": "1801.00329", "contents": "Title: ZOOpt: Toolbox for Derivative-Free Optimization Abstract: Recent advances in derivative-free optimization allow efficient approximation\nof the global-optimal solutions of sophisticated functions, such as functions\nwith many local optima, non-differentiable and non-continuous functions. This\narticle describes the ZOOpt (Zeroth Order Optimization) toolbox that provides\nefficient derivative-free solvers and is designed easy to use. ZOOpt provides\nsingle-machine parallel optimization on the basis of python core and\nmulti-machine distributed optimization for time-consuming tasks by\nincorporating with the Ray framework -- a famous platform for building\ndistributed applications. ZOOpt particularly focuses on optimization problems\nin machine learning, addressing high-dimensional and noisy problems such as\nhyper-parameter tuning and direct policy search. The toolbox is maintained\ntoward a ready-to-use tool in real-world machine learning tasks. \n\n"}
{"id": "1801.00837", "contents": "Title: QuickCast: Fast and Efficient Inter-Datacenter Transfers using\n  Forwarding Tree Cohorts Abstract: Large inter-datacenter transfers are crucial for cloud service efficiency and\nare increasingly used by organizations that have dedicated wide area networks\nbetween datacenters. A recent work uses multicast forwarding trees to reduce\nthe bandwidth needs and improve completion times of point-to-multipoint\ntransfers. Using a single forwarding tree per transfer, however, leads to poor\nperformance because the slowest receiver dictates the completion time for all\nreceivers. Using multiple forwarding trees per transfer alleviates this\nconcern--the average receiver could finish early; however, if done naively,\nbandwidth usage would also increase and it is apriori unclear how best to\npartition receivers, how to construct the multiple trees and how to determine\nthe rate and schedule of flows on these trees. This paper presents QuickCast, a\nfirst solution to these problems. Using simulations on real-world network\ntopologies, we see that QuickCast can speed up the average receiver's\ncompletion time by as much as $10\\times$ while only using $1.04\\times$ more\nbandwidth; further, the completion time for all receivers also improves by as\nmuch as $1.6\\times$ faster at high loads. \n\n"}
{"id": "1801.01221", "contents": "Title: Bounded-Velocity Stochastic Control for Dynamic Resource Allocation Abstract: We consider a general class of dynamic resource allocation problems within a\nstochastic optimal control framework. This class of problems arises in a wide\nvariety of applications, each of which intrinsically involves resources of\ndifferent types and demand with uncertainty and/or variability. The goal\ninvolves dynamically allocating capacity for every resource type in order to\nserve the uncertain/variable demand, modeled as Brownian motion, and maximize\nthe discounted expected net-benefit over an infinite time horizon based on the\nrewards and costs associated with the different resource types, subject to\nflexibility constraints on the rate of change of each type of resource\ncapacity. We derive the optimal control policy within a bounded-velocity\nstochastic control setting, which includes efficient and easily implementable\nalgorithms for governing the dynamic adjustments to resource allocation\ncapacities over time. Computational experiments investigate various issues of\nboth theoretical and practical interest, quantifying the benefits of our\napproach over recent alternative optimization approaches. \n\n"}
{"id": "1801.01780", "contents": "Title: Probabilistic max-plus schemes for solving Hamilton-Jacobi-Bellman\n  equations Abstract: We consider fully nonlinear Hamilton-Jacobi-Bellman equations associated to\ndiffusion control problems involving a finite set-valued (or switching) control\nand possibly a continuum-valued control. In previous works (Akian, Fodjo, 2016\nand 2017), we introduced a lower complexity probabilistic numerical algorithm\nfor such equations by combining max-plus and numerical probabilistic\napproaches. The max-plus approach is in the spirit of the one of McEneaney,\nKaise and Han (2011), and is based on the distributivity of monotone operators\nwith respect to suprema. The numerical probabilistic approach is in the spirit\nof the one proposed by Fahim, Touzi and Warin (2011). A difficulty of the\nlatter algorithm was in the critical constraints imposed on the Hamiltonian to\nensure the monotonicity of the scheme, hence the convergence of the algorithm.\nHere, we present new probabilistic schemes which are monotone under rather weak\nassumptions, and show error estimates for these schemes. These estimates will\nbe used in further works to study the probabilistic max-plus method. \n\n"}
{"id": "1801.03464", "contents": "Title: Stability analysis and state-feedback control of LPV systems with\n  piecewise constant parameters subject to spontaneous Poissonian jumps Abstract: LPV systems with piecewise constant parameters subject to spontaneous\nPoissonian jumps are a class of systems that does not seem to have been\nthoroughly considered in the literature. We partially fill this gap here by\nproviding sufficient stability and performance analysis conditions stated in\nterms of infinite-dimensional LMI problems that can be solved using sum of\nsquares programming. A particularity of the obtained conditions lies in the\npresence of an integral term leading to some technical difficulties when\nattempting to obtain convex conditions for the design of a gain-scheduled\nstate-feedback controller. This difficulty is circumvented by relying on a\nrecent result for time-delay systems analysis and an equivalent integral-free\nLMI condition is obtained. The approach is illustrated through several\nexamples. \n\n"}
{"id": "1801.04378", "contents": "Title: Fairness in Supervised Learning: An Information Theoretic Approach Abstract: Automated decision making systems are increasingly being used in real-world\napplications. In these systems for the most part, the decision rules are\nderived by minimizing the training error on the available historical data.\nTherefore, if there is a bias related to a sensitive attribute such as gender,\nrace, religion, etc. in the data, say, due to cultural/historical\ndiscriminatory practices against a certain demographic, the system could\ncontinue discrimination in decisions by including the said bias in its decision\nrule. We present an information theoretic framework for designing fair\npredictors from data, which aim to prevent discrimination against a specified\nsensitive attribute in a supervised learning setting. We use equalized odds as\nthe criterion for discrimination, which demands that the prediction should be\nindependent of the protected attribute conditioned on the actual label. To\nensure fairness and generalization simultaneously, we compress the data to an\nauxiliary variable, which is used for the prediction task. This auxiliary\nvariable is chosen such that it is decontaminated from the discriminatory\nattribute in the sense of equalized odds. The final predictor is obtained by\napplying a Bayesian decision rule to the auxiliary variable. \n\n"}
{"id": "1801.06077", "contents": "Title: The QLBS Q-Learner Goes NuQLear: Fitted Q Iteration, Inverse RL, and\n  Option Portfolios Abstract: The QLBS model is a discrete-time option hedging and pricing model that is\nbased on Dynamic Programming (DP) and Reinforcement Learning (RL). It combines\nthe famous Q-Learning method for RL with the Black-Scholes (-Merton) model's\nidea of reducing the problem of option pricing and hedging to the problem of\noptimal rebalancing of a dynamic replicating portfolio for the option, which is\nmade of a stock and cash. Here we expand on several NuQLear (Numerical\nQ-Learning) topics with the QLBS model. First, we investigate the performance\nof Fitted Q Iteration for a RL (data-driven) solution to the model, and\nbenchmark it versus a DP (model-based) solution, as well as versus the BSM\nmodel. Second, we develop an Inverse Reinforcement Learning (IRL) setting for\nthe model, where we only observe prices and actions (re-hedges) taken by a\ntrader, but not rewards. Third, we outline how the QLBS model can be used for\npricing portfolios of options, rather than a single option in isolation, thus\nproviding its own, data-driven and model independent solution to the (in)famous\nvolatility smile problem of the Black-Scholes model. \n\n"}
{"id": "1801.06279", "contents": "Title: Robust integral action of port-Hamiltonian systems Abstract: Interconnection and damping assignment, passivity-based control (IDA-PBC) has\nproven to be a successful control technique for the stabilisation of many\nnonlinear systems. In this paper, we propose a method to robustify a system\nwhich has been stabilised using IDA-PBC with respect to constant, matched\ndisturbances via the addition of integral action. The proposed controller\nextends previous work on the topic by being robust against the damping of the\nsystem, a quantity which may not be known in many applications. \n\n"}
{"id": "1801.06954", "contents": "Title: A port-Hamiltonian approach to the control of nonholonomic systems Abstract: In this paper a method of controlling nonholonomic systems within the\nport-Hamiltonian (pH) framework is presented. It is well known that\nnonholonomic systems can be represented as pH systems without Lagrange\nmultipliers by considering a reduced momentum space. Here, we revisit the\nmodelling of these systems for the purpose of identifying the role that\nphysical damping plays. Using this representation, a geometric structure\ngeneralising the well known chained form is identified as \\textit{chained\nstructure}. A discontinuous control law is then proposed for pH systems with\nchained structure such that the configuration of the system asymptotically\napproaches the origin. The proposed control law is robust against the damping\nand inertial of the open-loop system. The results are then demonstrated\nnumerically on a car-like vehicle. \n\n"}
{"id": "1801.07229", "contents": "Title: Combinatorial framework for planning in geological exploration Abstract: The paper describes combinatorial framework for planning of geological\nexploration for oil-gas fields. The suggested scheme of the geological\nexploration involves the following stages: (1) building of special 4-layer\ntree-like model (layer of geological exploration): productive layer, group of\nproductive layers, oil-gas field, oil-gas region (or group of the fields); (2)\ngenerations of local design (exploration) alternatives for each low-layer\ngeological objects: conservation, additional search, independent utilization,\njoint utilization; (3) multicriteria (i.e., multi-attribute) assessment of the\ndesign (exploration) alternatives and their interrelation (compatibility) and\nmapping if the obtained vector estimates into integrated ordinal scale; (4)\nhierarchical design ('bottom-up') of composite exploration plans for each\noil-gas field; (5) integration of the plans into region plans and (6)\naggregation of the region plans into a general exploration plan. Stages 2, 3,\n4, and 5 are based on hierarchical multicriteria morphological design (HMMD)\nmethod (assessment of ranking of alternatives, selection and composition of\nalternatives into composite alternatives). The composition problem is based on\nmorphological clique model. Aggregation of the obtained modular alternatives\n(stage 6) is based on detection of a alternatives 'kernel' and its extension by\naddition of elements (multiple choice model). In addition, the usage of\nmultiset estimates for alternatives is described as well. The alternative\nestimates are based on expert judgment. The suggested combinatorial planning\nmethodology is illustrated by numerical examples for geological exploration of\nYamal peninsula. \n\n"}
{"id": "1801.09197", "contents": "Title: Algorithmic Linearly Constrained Gaussian Processes Abstract: We algorithmically construct multi-output Gaussian process priors which\nsatisfy linear differential equations. Our approach attempts to parametrize all\nsolutions of the equations using Gr\\\"obner bases. If successful, a push forward\nGaussian process along the paramerization is the desired prior. We consider\nseveral examples from physics, geomathematics and control, among them the full\ninhomogeneous system of Maxwell's equations. By bringing together stochastic\nlearning and computer algebra in a novel way, we combine noisy observations\nwith precise algebraic computations. \n\n"}
{"id": "1801.09627", "contents": "Title: Barrier-Certified Adaptive Reinforcement Learning with Applications to\n  Brushbot Navigation Abstract: This paper presents a safe learning framework that employs an adaptive model\nlearning algorithm together with barrier certificates for systems with possibly\nnonstationary agent dynamics. To extract the dynamic structure of the model, we\nuse a sparse optimization technique. We use the learned model in combination\nwith control barrier certificates which constrain policies (feedback\ncontrollers) in order to maintain safety, which refers to avoiding particular\nundesirable regions of the state space. Under certain conditions, recovery of\nsafety in the sense of Lyapunov stability after violations of safety due to the\nnonstationarity is guaranteed. In addition, we reformulate an action-value\nfunction approximation to make any kernel-based nonlinear function estimation\nmethod applicable to our adaptive learning framework. Lastly, solutions to the\nbarrier-certified policy optimization are guaranteed to be globally optimal,\nensuring the greedy policy improvement under mild conditions. The resulting\nframework is validated via simulations of a quadrotor, which has previously\nbeen used under stationarity assumptions in the safe learnings literature, and\nis then tested on a real robot, the brushbot, whose dynamics is unknown, highly\ncomplex and nonstationary. \n\n"}
{"id": "1801.09889", "contents": "Title: Variational and viscosity operators for the evolutive Hamilton-Jacobi\n  equation Abstract: We study the Cauchy problem for the first order evolutive Hamilton-Jacobi\nequation with a Lipschitz initial condition. The Hamiltonian is not necessarily\nconvex in the momentum variable and not a priori compactly supported. We build\nand study an operator giving a variational solution of this problem, and get\nlocal Lipschitz estimates on this operator. Iterating this variational operator\nwe obtain the viscosity operator and extend the estimates to the viscosity\nframework. We also check that the construction of the variational operator\ngives the Lax-Oleinik semigroup if the Hamiltonian is convex or concave in the\nmomentum variable. \n\n"}
{"id": "1801.10505", "contents": "Title: Compositional Construction of Infinite Abstractions for Networks of\n  Stochastic Control Systems Abstract: This paper is concerned with a compositional approach for constructing\ninfinite abstractions of interconnected discrete-time stochastic control\nsystems. The proposed approach uses the interconnection matrix and joint\ndissipativity-type properties of subsystems and their abstractions described by\na new notion of so-called stochastic storage functions. The interconnected\nabstraction framework is based on new notions of so-called stochastic\nsimulation functions, using which one can quantify the distance between\noriginal interconnected stochastic control systems and interconnected\nabstractions in the probabilistic setting. In the first part of the paper, we\nderive dissipativity-type compositional reasoning for the quantification of the\ndistance in probability between the interconnection of stochastic control\nsubsystems and that of their abstractions. Moreover, we focus on a class of\ndiscrete-time nonlinear stochastic control systems with independent noises in\nthe abstract and concrete subsystems, and propose a computational scheme to\nconstruct abstractions together with their corresponding stochastic storage\nfunctions. In the second part of the paper, we consider specifications\nexpressed as syntactically co-safe linear temporal logic formulae and show how\na synthesized policy for the abstract system can be refined to a policy for the\noriginal system while providing guarantee on the probability of satisfaction.\nWe demonstrate the effectiveness of the proposed results by constructing an\nabstraction (totally 3 dimensions) of the interconnection of three\ndiscrete-time nonlinear stochastic control subsystems (together 222 dimensions)\nin a compositional fashion. We also employ the abstraction as a substitute to\nsynthesize a controller enforcing a syntactically co-safe linear temporal logic\nspecification. \n\n"}
{"id": "1802.00212", "contents": "Title: Training Neural Networks by Using Power Linear Units (PoLUs) Abstract: In this paper, we introduce \"Power Linear Unit\" (PoLU) which increases the\nnonlinearity capacity of a neural network and thus helps improving its\nperformance. PoLU adopts several advantages of previously proposed activation\nfunctions. First, the output of PoLU for positive inputs is designed to be\nidentity to avoid the gradient vanishing problem. Second, PoLU has a non-zero\noutput for negative inputs such that the output mean of the units is close to\nzero, hence reducing the bias shift effect. Thirdly, there is a saturation on\nthe negative part of PoLU, which makes it more noise-robust for negative\ninputs. Furthermore, we prove that PoLU is able to map more portions of every\nlayer's input to the same space by using the power function and thus increases\nthe number of response regions of the neural network. We use image\nclassification for comparing our proposed activation function with others. In\nthe experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN)\nand ImageNet are used as benchmark datasets. The neural networks we implemented\ninclude widely-used ELU-Network, ResNet-50, and VGG16, plus a couple of shallow\nnetworks. Experimental results show that our proposed activation function\noutperforms other state-of-the-art models with most networks. \n\n"}
{"id": "1802.01568", "contents": "Title: Selective Sampling and Mixture Models in Generative Adversarial Networks Abstract: In this paper, we propose a multi-generator extension to the adversarial\ntraining framework, in which the objective of each generator is to represent a\nunique component of a target mixture distribution. In the training phase, the\ngenerators cooperate to represent, as a mixture, the target distribution while\nmaintaining distinct manifolds. As opposed to traditional generative models,\ninference from a particular generator after training resembles selective\nsampling from a unique component in the target distribution. We demonstrate the\nfeasibility of the proposed architecture both analytically and with basic\nMulti-Layer Perceptron (MLP) models trained on the MNIST dataset. \n\n"}
{"id": "1802.02302", "contents": "Title: An example showing that A-lower semi-continuity is essential for minimax\n  continuity theorems Abstract: Recently Feinberg et al. [arXiv:1609.03990] established results on continuity\nproperties of minimax values and solution sets for a function of two variables\ndepending on a parameter. Such minimax problems appear in games with perfect\ninformation, when the second player knows the move of the first one, in\nturn-based games, and in robust optimization. Some of the results in\n[arXiv:1609.03990] are proved under the assumption that the multifunction,\ndefining the domains of the second variable, is $A$-lower semi-continuous. The\n$A$-lower semi-continuity property is stronger than lower semi-continuity, but\nin several important cases these properties coincide. This note provides an\nexample demonstrating that in general the $A$-lower semi-continuity assumption\ncannot be relaxed to lower semi-continuity. \n\n"}
{"id": "1802.02483", "contents": "Title: Power-Controlled Hamiltonian Systems: Application to Electrical Systems\n  with Constant Power Loads Abstract: We study a type of port-Hamiltonian system, in which the controller or\ndisturbance is not applied to the flow variables, but to the systems power, a\nscenario that appears in many practical applications. A suitable framework is\nprovided to model these systems and to investigate their shifted passivity\nproperties, based on which, a stability analysis is carried out. The\napplicability of the results is illustrated with the important problem of\nstability analysis of electrical circuits with constant power loads. \n\n"}
{"id": "1802.02618", "contents": "Title: A Diversity-based Substation Cyber Defense Strategy utilizing Coloring\n  Games Abstract: Growing cybersecurity risks in the power grid require that utilities\nimplement a variety of security mechanism (SM) composed mostly of VPNs,\nfirewalls, or other custom security components. While they provide some\nprotection, they might contain software vulnerabilities which can lead to a\ncyber-attack. In this paper, the severity of a cyber-attack has been decreased\nby employing a diverse set of SM that reduce repetition of a single\nvulnerability. This paper focuses on the allocation of diverse SM and tries to\nincrease the security of the cyber assets located within the electronic\nsecurity perimeter(ESP) of a substation. We have used a graph-based coloring\ngame in a distributed manner to allocate diverse SM for protecting the cyber\nassets. The vulnerability assessment for power grid network is also analyzed\nusing this game theoretic method. An improved, diversified SMs for worst-case\nscenario has been demonstrated by reaching the Nash equilibrium of graph\ncoloring game. As a case study, we analyze the IEEE-14 and IEEE-118 bus system,\nobserve the different distributed coloring algorithm for allocating diverse SM\nand calculating the overall network criticality. \n\n"}
{"id": "1802.03099", "contents": "Title: Blockchain-Assisted Crowdsourced Energy Systems Abstract: Crowdsourcing relies on people's contributions to meet product- or\nsystem-level objectives. Crowdsourcing-based methods have been implemented in\nvarious cyber-physical systems and realtime markets. This paper explores a\nframework for Crowdsourced Energy Systems (CES), where small-scale energy\ngeneration or energy trading is crowdsourced from distributed energy resources,\nelectric vehicles, and shapable loads. The merits/pillars of energy\ncrowdsourcing are discussed. Then, an operational model for CESs in\ndistribution networks with different types of crowdsourcees is proposed. The\nmodel yields a market equilibrium depicting traditional and distributed\ngenerator and load setpoints. Given these setpoints, crowdsourcing incentives\nare designed to steer crowdsourcees to the equilibrium. As the number of\ncrowdsourcees and energy trading transactions scales up, a secure energy\ntrading platform is required. To that end, the presented framework is\nintegrated with a lightweight Blockchain implementation and smart contracts.\nNumerical tests are provided to showcase the overall implementation. \n\n"}
{"id": "1802.03628", "contents": "Title: Learning Correlation Space for Time Series Abstract: We propose an approximation algorithm for efficient correlation search in\ntime series data. In our method, we use Fourier transform and neural network to\nembed time series into a low-dimensional Euclidean space. The given space is\nlearned such that time series correlation can be effectively approximated from\nEuclidean distance between corresponding embedded vectors. Therefore, search\nfor correlated time series can be done using an index in the embedding space\nfor efficient nearest neighbor search. Our theoretical analysis illustrates\nthat our method's accuracy can be guaranteed under certain regularity\nconditions. We further conduct experiments on real-world datasets and the\nresults show that our method indeed outperforms the baseline solution. In\nparticular, for approximation of correlation, our method reduces the\napproximation loss by a half in most test cases compared to the baseline\nsolution. For top-$k$ highest correlation search, our method improves the\nprecision from 5\\% to 20\\% while the query time is similar to the baseline\napproach query time. \n\n"}
{"id": "1802.04036", "contents": "Title: Inferring the time-varying functional connectivity of large-scale\n  computer networks from emitted events Abstract: We consider the problem of inferring the functional connectivity of a\nlarge-scale computer network from sparse time series of events emitted by its\nnodes. We do so under the following three domain-specific constraints: (a)\nnon-stationarity of the functional connectivity due to unknown temporal changes\nin the network, (b) sparsity of the time-series of events that limits the\neffectiveness of classical correlation-based analysis, and (c) lack of an\nexplicit model describing how events propagate through the network. Under the\nassumption that the probability of two nodes being functionally connected\ncorrelates with the mean delay between their respective events, we develop an\ninference method whose output is an undirected weighted network where the\nweight of an edge between two nodes denotes the probability of these nodes\nbeing functionally connected. Using a combination of windowing and convolution\nto calculate at each time window a score quantifying the likelihood of a pair\nof nodes emitting events in quick succession, we develop a model of\ntime-varying connectivity whose parameters are determined by maximising the\nmodel's predictive power from one time window to the next. To assess the\neffectiveness of our inference method, we construct synthetic data for which\nground truth is available and use these data to benchmark our approach against\nthree state-of-the-art inference methods. We conclude by discussing its\napplication to data from a real-world large-scale computer network. \n\n"}
{"id": "1802.04162", "contents": "Title: Policy Gradients for Contextual Recommendations Abstract: Decision making is a challenging task in online recommender systems. The\ndecision maker often needs to choose a contextual item at each step from a set\nof candidates. Contextual bandit algorithms have been successfully deployed to\nsuch applications, for the trade-off between exploration and exploitation and\nthe state-of-art performance on minimizing online costs. However, the\napplicability of existing contextual bandit methods is limited by the\nover-simplified assumptions of the problem, such as assuming a simple form of\nthe reward function or assuming a static environment where the states are not\naffected by previous actions. In this work, we put forward Policy Gradients for\nContextual Recommendations (PGCR) to solve the problem without those\nunrealistic assumptions. It optimizes over a restricted class of policies where\nthe marginal probability of choosing an item (in expectation of other items)\nhas a simple closed form, and the gradient of the expected return over the\npolicy in this class is in a succinct form. Moreover, PGCR leverages two useful\nheuristic techniques called Time-Dependent Greed and Actor-Dropout. The former\nensures PGCR to be empirically greedy in the limit, and the latter addresses\nthe trade-off between exploration and exploitation by using the policy network\nwith Dropout as a Bayesian approximation. PGCR can solve the standard\ncontextual bandits as well as its Markov Decision Process generalization.\nTherefore it can be applied to a wide range of realistic settings of\nrecommendations, such as personalized advertising. We evaluate PGCR on toy\ndatasets as well as a real-world dataset of personalized music recommendations.\nExperiments show that PGCR enables fast convergence and low regret, and\noutperforms both classic contextual-bandits and vanilla policy gradient\nmethods. \n\n"}
{"id": "1802.04181", "contents": "Title: State Representation Learning for Control: An Overview Abstract: Representation learning algorithms are designed to learn abstract features\nthat characterize data. State representation learning (SRL) focuses on a\nparticular kind of representation learning where learned features are in low\ndimension, evolve through time, and are influenced by actions of an agent. The\nrepresentation is learned to capture the variation in the environment generated\nby the agent's actions; this kind of representation is particularly suitable\nfor robotics and control scenarios. In particular, the low dimension\ncharacteristic of the representation helps to overcome the curse of\ndimensionality, provides easier interpretation and utilization by humans and\ncan help improve performance and speed in policy learning algorithms such as\nreinforcement learning.\n  This survey aims at covering the state-of-the-art on state representation\nlearning in the most recent years. It reviews different SRL methods that\ninvolve interaction with the environment, their implementations and their\napplications in robotics control tasks (simulated or real). In particular, it\nhighlights how generic learning objectives are differently exploited in the\nreviewed algorithms. Finally, it discusses evaluation methods to assess the\nrepresentation learned and summarizes current and future lines of research. \n\n"}
{"id": "1802.04198", "contents": "Title: client2vec: Towards Systematic Baselines for Banking Applications Abstract: The workflow of data scientists normally involves potentially inefficient\nprocesses such as data mining, feature engineering and model selection. Recent\nresearch has focused on automating this workflow, partly or in its entirety, to\nimprove productivity. We choose the former approach and in this paper share our\nexperience in designing the client2vec: an internal library to rapidly build\nbaselines for banking applications. Client2vec uses marginalized stacked\ndenoising autoencoders on current account transactions data to create vector\nembeddings which represent the behaviors of our clients. These representations\ncan then be used in, and optimized against, a variety of tasks such as client\nsegmentation, profiling and targeting. Here we detail how we selected the\nalgorithmic machinery of client2vec and the data it works on and present\nexperimental results on several business cases. \n\n"}
{"id": "1802.04332", "contents": "Title: Entropy Penalized Semidefinite Programming Abstract: Low-rank methods for semidefinite programming (SDP) have gained a lot of\ninterest recently, especially in machine learning applications. Their analysis\noften involves determinant-based or Schatten-norm penalties, which are hard to\nimplement in practice due to high computational efforts. In this paper, we\npropose Entropy Penalized Semi-definite programming (EP-SDP) which provides a\nunified framework for a wide class of penalty functions used in practice to\npromote a low-rank solution. We show that EP-SDP problems admit efficient\nnumerical algorithm having (almost) linear time complexity of the gradient\niteration which makes it useful for many machine learning and optimization\nproblems. We illustrate the practical efficiency of our approach on several\ncombinatorial optimization and machine learning problems. \n\n"}
{"id": "1802.04431", "contents": "Title: Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic\n  Thresholding Abstract: As spacecraft send back increasing amounts of telemetry data, improved\nanomaly detection systems are needed to lessen the monitoring burden placed on\noperations engineers and reduce operational risk. Current spacecraft monitoring\nsystems only target a subset of anomaly types and often require costly expert\nknowledge to develop and maintain due to challenges involving scale and\ncomplexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs)\nnetworks, a type of Recurrent Neural Network (RNN), in overcoming these issues\nusing expert-labeled telemetry anomaly data from the Soil Moisture Active\nPassive (SMAP) satellite and the Mars Science Laboratory (MSL) rover,\nCuriosity. We also propose a complementary unsupervised and nonparametric\nanomaly thresholding approach developed during a pilot implementation of an\nanomaly detection system for SMAP, and offer false positive mitigation\nstrategies along with other key improvements and lessons learned during\ndevelopment. \n\n"}
{"id": "1802.04456", "contents": "Title: Global Optimal Power Flow over Large-Scale Power Transmission Network Abstract: Optimal power flow (OPF) over power transmission networks poses challenging\nlarge-scale nonlinear optimization problems, which involve a large number of\nquadratic equality and indefinite quadratic inequality constraints. These\ncomputationally intractable constraints are often expressed by linear\nconstraints plus matrix additional rank-one constraints on the outer products\nof the voltage vectors. The existing convex relaxation technique, which drops\nthe difficult rank-one constraints for tractable computation, cannot yield even\na feasible point. We address these computationally difficult problems by an\niterative procedure, which generates a sequence of improved points that\nconverge to a rank-one solution. Each iteration calls a semi-definite program.\nIntensive simulations for the OPF problems over networks with a few thousands\nof buses are provided to demonstrate the efficiency of our approach. The\nsuboptimal values of the OPF problems found by our computational procedure turn\nout to be the global optimal value with computational tolerance less than\n0.01%. \n\n"}
{"id": "1802.04644", "contents": "Title: Price of Anarchy for Mean Field Games Abstract: The price of anarchy, originally introduced to quantify the inefficiency of\nselfish behavior in routing games, is extended to mean field games. The price\nof anarchy is defined as the ratio of a worst case social cost computed for a\nmean field game equilibrium to the optimal social cost as computed by a central\nplanner. We illustrate properties of such a price of anarchy on linear\nquadratic extended mean field games, for which explicit computations are\npossible. Various asymptotic behaviors of the price of anarchy are proved for\nlimiting behaviors of the coefficients in the model and numerics are presented. \n\n"}
{"id": "1802.04712", "contents": "Title: Attention-based Deep Multiple Instance Learning Abstract: Multiple instance learning (MIL) is a variation of supervised learning where\na single class label is assigned to a bag of instances. In this paper, we state\nthe MIL problem as learning the Bernoulli distribution of the bag label where\nthe bag label probability is fully parameterized by neural networks.\nFurthermore, we propose a neural network-based permutation-invariant\naggregation operator that corresponds to the attention mechanism. Notably, an\napplication of the proposed attention-based operator provides insight into the\ncontribution of each instance to the bag label. We show empirically that our\napproach achieves comparable performance to the best MIL methods on benchmark\nMIL datasets and it outperforms other methods on a MNIST-based MIL dataset and\ntwo real-life histopathology datasets without sacrificing interpretability. \n\n"}
{"id": "1802.04927", "contents": "Title: Geometry-Based Data Generation Abstract: Many generative models attempt to replicate the density of their input data.\nHowever, this approach is often undesirable, since data density is highly\naffected by sampling biases, noise, and artifacts. We propose a method called\nSUGAR (Synthesis Using Geometrically Aligned Random-walks) that uses a\ndiffusion process to learn a manifold geometry from the data. Then, it\ngenerates new points evenly along the manifold by pulling randomly generated\npoints into its intrinsic structure using a diffusion kernel. SUGAR equalizes\nthe density along the manifold by selectively generating points in sparse areas\nof the manifold. We demonstrate how the approach corrects sampling biases and\nartifacts, while also revealing intrinsic patterns (e.g. progression) and\nrelations in the data. The method is applicable for correcting missing data,\nfinding hypothetical data points, and learning relationships between data\nfeatures. \n\n"}
{"id": "1802.05315", "contents": "Title: Online Learning for Non-Stationary A/B Tests Abstract: The rollout of new versions of a feature in modern applications is a manual\nmulti-stage process, as the feature is released to ever larger groups of users,\nwhile its performance is carefully monitored. This kind of A/B testing is\nubiquitous, but suboptimal, as the monitoring requires heavy human\nintervention, is not guaranteed to capture consistent, but short-term\nfluctuations in performance, and is inefficient, as better versions take a long\ntime to reach the full population.\n  In this work we formulate this question as that of expert learning, and give\na new algorithm Follow-The-Best-Interval, FTBI, that works in dynamic,\nnon-stationary environments. Our approach is practical, simple, and efficient,\nand has rigorous guarantees on its performance. Finally, we perform a thorough\nevaluation on synthetic and real world datasets and show that our approach\noutperforms current state-of-the-art methods. \n\n"}
{"id": "1802.05394", "contents": "Title: Cost-Effective Training of Deep CNNs with Active Model Adaptation Abstract: Deep convolutional neural networks have achieved great success in various\napplications. However, training an effective DNN model for a specific task is\nrather challenging because it requires a prior knowledge or experience to\ndesign the network architecture, repeated trial-and-error process to tune the\nparameters, and a large set of labeled data to train the model. In this paper,\nwe propose to overcome these challenges by actively adapting a pre-trained\nmodel to a new task with less labeled examples. Specifically, the pre-trained\nmodel is iteratively fine tuned based on the most useful examples. The examples\nare actively selected based on a novel criterion, which jointly estimates the\npotential contribution of an instance on optimizing the feature representation\nas well as improving the classification model for the target task. On one hand,\nthe pre-trained model brings plentiful information from its original task,\navoiding redesign of the network architecture or training from scratch; and on\nthe other hand, the labeling cost can be significantly reduced by active label\nquerying. Experiments on multiple datasets and different pre-trained models\ndemonstrate that the proposed approach can achieve cost-effective training of\nDNNs. \n\n"}
{"id": "1802.05640", "contents": "Title: Gradient Boosting With Piece-Wise Linear Regression Trees Abstract: Gradient Boosted Decision Trees (GBDT) is a very successful ensemble learning\nalgorithm widely used across a variety of applications. Recently, several\nvariants of GBDT training algorithms and implementations have been designed and\nheavily optimized in some very popular open sourced toolkits including XGBoost,\nLightGBM and CatBoost. In this paper, we show that both the accuracy and\nefficiency of GBDT can be further enhanced by using more complex base learners.\nSpecifically, we extend gradient boosting to use piecewise linear regression\ntrees (PL Trees), instead of piecewise constant regression trees, as base\nlearners. We show that PL Trees can accelerate convergence of GBDT and improve\nthe accuracy. We also propose some optimization tricks to substantially reduce\nthe training time of PL Trees, with little sacrifice of accuracy. Moreover, we\npropose several implementation techniques to speedup our algorithm on modern\ncomputer architectures with powerful Single Instruction Multiple Data (SIMD)\nparallelism. The experimental results show that GBDT with PL Trees can provide\nvery competitive testing accuracy with comparable or less training time. \n\n"}
{"id": "1802.06439", "contents": "Title: Local Optimality and Generalization Guarantees for the Langevin\n  Algorithm via Empirical Metastability Abstract: We study the detailed path-wise behavior of the discrete-time Langevin\nalgorithm for non-convex Empirical Risk Minimization (ERM) through the lens of\nmetastability, adopting some techniques from Berglund and Gentz (2003.\n  For a particular local optimum of the empirical risk, with an arbitrary\ninitialization, we show that, with high probability, at least one of the\nfollowing two events will occur: (1) the Langevin trajectory ends up somewhere\noutside the $\\varepsilon$-neighborhood of this particular optimum within a\nshort recurrence time; (2) it enters this $\\varepsilon$-neighborhood by the\nrecurrence time and stays there until a potentially exponentially long escape\ntime. We call this phenomenon empirical metastability.\n  This two-timescale characterization aligns nicely with the existing\nliterature in the following two senses. First, the effective recurrence time\n(i.e., number of iterations multiplied by stepsize) is dimension-independent,\nand resembles the convergence time of continuous-time deterministic Gradient\nDescent (GD). However unlike GD, the Langevin algorithm does not require strong\nconditions on local initialization, and has the possibility of eventually\nvisiting all optima. Second, the scaling of the escape time is consistent with\nthe Eyring-Kramers law, which states that the Langevin scheme will eventually\nvisit all local minima, but it will take an exponentially long time to transit\namong them. We apply this path-wise concentration result in the context of\nstatistical learning to examine local notions of generalization and optimality. \n\n"}
{"id": "1802.06501", "contents": "Title: Recommendations with Negative Feedback via Pairwise Deep Reinforcement\n  Learning Abstract: Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users' personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users' feedback. Users' feedback can be positive and negative and both\ntypes of feedback have great potentials to boost recommendations. However, the\nnumber of negative feedback is much larger than that of positive one; thus\nincorporating them simultaneously is challenging since positive feedback could\nbe buried by negative one. In this paper, we develop a novel approach to\nincorporate them into the proposed deep recommender system (DEERS) framework.\nThe experimental results based on real-world e-commerce data demonstrate the\neffectiveness of the proposed framework. Further experiments have been\nconducted to understand the importance of both positive and negative feedback\nin recommendations. \n\n"}
{"id": "1802.07301", "contents": "Title: On the Connection Between Learning Two-Layers Neural Networks and Tensor\n  Decomposition Abstract: We establish connections between the problem of learning a two-layer neural\nnetwork and tensor decomposition. We consider a model with feature vectors\n$\\boldsymbol x \\in \\mathbb R^d$, $r$ hidden units with weights $\\{\\boldsymbol\nw_i\\}_{1\\le i \\le r}$ and output $y\\in \\mathbb R$, i.e., $y=\\sum_{i=1}^r\n\\sigma( \\boldsymbol w_i^{\\mathsf T}\\boldsymbol x)$, with activation functions\ngiven by low-degree polynomials. In particular, if $\\sigma(x) =\na_0+a_1x+a_3x^3$, we prove that no polynomial-time learning algorithm can\noutperform the trivial predictor that assigns to each example the response\nvariable $\\mathbb E(y)$, when $d^{3/2}\\ll r\\ll d^2$. Our conclusion holds for a\n`natural data distribution', namely standard Gaussian feature vectors\n$\\boldsymbol x$, and output distributed according to a two-layer neural network\nwith random isotropic weights, and under a certain complexity-theoretic\nassumption on tensor decomposition. Roughly speaking, we assume that no\npolynomial-time algorithm can substantially outperform current methods for\ntensor decomposition based on the sum-of-squares hierarchy.\n  We also prove generalizations of this statement for higher degree polynomial\nactivations, and non-random weight vectors. Remarkably, several existing\nalgorithms for learning two-layer networks with rigorous guarantees are based\non tensor decomposition. Our results support the idea that this is indeed the\ncore computational difficulty in learning such networks, under the stated\ngenerative model for the data. As a side result, we show that under this model\nlearning the network requires accurate learning of its weights, a property that\ndoes not hold in a more general setting. \n\n"}
{"id": "1802.07329", "contents": "Title: Bayesian Incremental Learning for Deep Neural Networks Abstract: In industrial machine learning pipelines, data often arrive in parts.\nParticularly in the case of deep neural networks, it may be too expensive to\ntrain the model from scratch each time, so one would rather use a previously\nlearned model and the new data to improve performance. However, deep neural\nnetworks are prone to getting stuck in a suboptimal solution when trained on\nonly new data as compared to the full dataset. Our work focuses on a continuous\nlearning setup where the task is always the same and new parts of data arrive\nsequentially. We apply a Bayesian approach to update the posterior\napproximation with each new piece of data and find this method to outperform\nthe traditional approach in our experiments. \n\n"}
{"id": "1802.07384", "contents": "Title: Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic\n  Corrections Abstract: We present a new algorithm to generate minimal, stable, and symbolic\ncorrections to an input that will cause a neural network with ReLU activations\nto change its output. We argue that such a correction is a useful way to\nprovide feedback to a user when the network's output is different from a\ndesired output. Our algorithm generates such a correction by solving a series\nof linear constraint satisfaction problems. The technique is evaluated on three\nneural network models: one predicting whether an applicant will pay a mortgage,\none predicting whether a first-order theorem can be proved efficiently by a\nsolver using certain heuristics, and the final one judging whether a drawing is\nan accurate rendition of a canonical drawing of a cat. \n\n"}
{"id": "1802.07712", "contents": "Title: Condition numbers of stochastic mean payoff games and what they say\n  about nonarchimedean semidefinite programming Abstract: Semidefinite programming can be considered over any real closed field,\nincluding fields of Puiseux series equipped with their nonarchimedean\nvaluation. Nonarchimedean semidefinite programs encode parametric families of\nclassical semidefinite programs, for sufficiently large values of the\nparameter. Recently, a correspondence has been established between\nnonarchimedean semidefinite programs and stochastic mean payoff games with\nperfect information. This correspondence relies on tropical geometry. It allows\none to solve generic nonarchimedean semidefinite feasibility problems, of large\nscale, by means of stochastic game algorithms. In this paper, we show that the\nmean payoff of these games can be interpreted as a condition number for the\ncorresponding nonarchimedean feasibility problems. This number measures how\nclose a feasible instance is from being infeasible, and vice versa. We show\nthat it coincides with the maximal radius of a ball in Hilbert's projective\nmetric, that is included in the feasible set. The geometric interpretation of\nthe condition number relies in particular on a duality theorem for tropical\nsemidefinite feasibility programs. Then, we bound the complexity of the\nfeasibility problem in terms of the condition number. We finally give explicit\nbounds for this condition number, in terms of the characteristics of the\nstochastic game. As a consequence, we show that the simplest algorithm to\ndecide whether a stochastic mean payoff game is winning, namely value\niteration, has a pseudopolynomial complexity when the number of random\npositions is fixed. \n\n"}
{"id": "1802.08232", "contents": "Title: The Secret Sharer: Evaluating and Testing Unintended Memorization in\n  Neural Networks Abstract: This paper describes a testing methodology for quantitatively assessing the\nrisk that rare or unique training-data sequences are unintentionally memorized\nby generative sequence models---a common type of machine-learning model.\nBecause such models are sometimes trained on sensitive data (e.g., the text of\nusers' private messages), this methodology can benefit privacy by allowing\ndeep-learning practitioners to select means of training that minimize such\nmemorization.\n  In experiments, we show that unintended memorization is a persistent,\nhard-to-avoid issue that can have serious consequences. Specifically, for\nmodels trained without consideration of memorization, we describe new,\nefficient procedures that can extract unique, secret sequences, such as credit\ncard numbers. We show that our testing strategy is a practical and easy-to-use\nfirst line of defense, e.g., by describing its application to quantitatively\nlimit data exposure in Google's Smart Compose, a commercial text-completion\nneural network trained on millions of users' email messages. \n\n"}
{"id": "1802.08603", "contents": "Title: Towards Distributed OPF using ALADIN Abstract: The present paper discusses the application of the recently proposed\nAugmented Lagrangian Alternating Direction Inexact Newton (ALADIN) method to\nnon-convex AC Optimal Power Flow Problems (OPF) in a distributed fashion. In\ncontrast to the often used Alternating Direction of Multipliers Method (ADMM),\nALADIN guarantees locally quadratic convergence for AC OPF. Numerical results\nfor 5 to 300 bus test cases indicate that ALADIN is able to outperform ADMM and\nto reduce the number of iterations by about one order of magnitude. We compare\nALADIN to numerical results for ADMM documented in the literature. The improved\nconvergence speed comes at the cost of increasing the communication effort per\niteration. Therefore, we propose a variant of ALADIN that uses inexact Hessians\nto reduce communication. Additionally, we provide a detailed comparison of\nthese ALADIN variants to ADMM from an algorithmic and communication\nperspective. Moreover, we prove that ALADIN converges locally at quadratic rate\neven for the relevant case of suboptimally solved local NLPs. \n\n"}
{"id": "1802.08953", "contents": "Title: Robust Target-relative Localization with Ultra-Wideband Ranging and\n  Communication Abstract: In this paper we propose a method to achieve relative positioning and\ntracking of a target by a quadcopter using Ultra-wideband (UWB) ranging\nsensors, which are strategically installed to help retrieve both relative\nposition and bearing between the quadcopter and target. To achieve robust\nlocalization for autonomous flight even with uncertainty in the speed of the\ntarget, two main features are developed. First, an estimator based on Extended\nKalman Filter (EKF) is developed to fuse UWB ranging measurements with data\nfrom onboard sensors including inertial measurement unit (IMU), altimeters and\noptical flow. Second, to properly handle the coupling of the target's\norientation with the range measurements, UWB based communication capability is\nutilized to transfer the target's orientation to the quadcopter. Experiment\nresults demonstrate the ability of the quadcopter to control its position\nrelative to the target autonomously in both cases when the target is static and\nmoving. \n\n"}
{"id": "1802.09564", "contents": "Title: Reinforcement and Imitation Learning for Diverse Visuomotor Skills Abstract: We propose a model-free deep reinforcement learning method that leverages a\nsmall amount of demonstration data to assist a reinforcement learning agent. We\napply this approach to robotic manipulation tasks and train end-to-end\nvisuomotor policies that map directly from RGB camera inputs to joint\nvelocities. We demonstrate that our approach can solve a wide variety of\nvisuomotor tasks, for which engineering a scripted controller would be\nlaborious. In experiments, our reinforcement and imitation agent achieves\nsignificantly better performances than agents trained with reinforcement\nlearning or imitation learning alone. We also illustrate that these policies,\ntrained with large visual and dynamics variations, can achieve preliminary\nsuccesses in zero-shot sim2real transfer. A brief visual description of this\nwork can be viewed in https://youtu.be/EDl8SQUNjj0 \n\n"}
{"id": "1802.09816", "contents": "Title: Coarse to fine non-rigid registration: a chain of scale-specific neural\n  networks for multimodal image alignment with application to remote sensing Abstract: We tackle here the problem of multimodal image non-rigid registration, which\nis of prime importance in remote sensing and medical imaging. The difficulties\nencountered by classical registration approaches include feature design and\nslow optimization by gradient descent. By analyzing these methods, we note the\nsignificance of the notion of scale. We design easy-to-train,\nfully-convolutional neural networks able to learn scale-specific features. Once\nchained appropriately, they perform global registration in linear time, getting\nrid of gradient descent schemes by predicting directly the deformation.We show\ntheir performance in terms of quality and speed through various tasks of remote\nsensing multimodal image alignment. In particular, we are able to register\ncorrectly cadastral maps of buildings as well as road polylines onto RGB\nimages, and outperform current keypoint matching methods. \n\n"}
{"id": "1802.10077", "contents": "Title: A Differential Privacy Mechanism Design Under Matrix-Valued Query Abstract: Traditionally, differential privacy mechanism design has been tailored for a\nscalar-valued query function. Although many mechanisms such as the Laplace and\nGaussian mechanisms can be extended to a matrix-valued query function by adding\ni.i.d. noise to each element of the matrix, this method is often sub-optimal as\nit forfeits an opportunity to exploit the structural characteristics typically\nassociated with matrix analysis. In this work, we consider the design of\ndifferential privacy mechanism specifically for a matrix-valued query function.\nThe proposed solution is to utilize a matrix-variate noise, as opposed to the\ntraditional scalar-valued noise. Particularly, we propose a novel differential\nprivacy mechanism called the Matrix-Variate Gaussian (MVG) mechanism, which\nadds a matrix-valued noise drawn from a matrix-variate Gaussian distribution.\nWe prove that the MVG mechanism preserves $(\\epsilon,\\delta)$-differential\nprivacy, and show that it allows the structural characteristics of the\nmatrix-valued query function to naturally be exploited. Furthermore, due to the\nmulti-dimensional nature of the MVG mechanism and the matrix-valued query, we\nintroduce the concept of directional noise, which can be utilized to mitigate\nthe impact the noise has on the utility of the query. Finally, we demonstrate\nthe performance of the MVG mechanism and the advantages of directional noise\nusing three matrix-valued queries on three privacy-sensitive datasets. We find\nthat the MVG mechanism notably outperforms four previous state-of-the-art\napproaches, and provides comparable utility to the non-private baseline. Our\nwork thus presents a promising prospect for both future research and\nimplementation of differential privacy for matrix-valued query functions. \n\n"}
{"id": "1802.10123", "contents": "Title: Latent-space Physics: Towards Learning the Temporal Evolution of Fluid\n  Flow Abstract: We propose a method for the data-driven inference of temporal evolutions of\nphysical functions with deep learning. More specifically, we target fluid\nflows, i.e. Navier-Stokes problems, and we propose a novel LSTM-based approach\nto predict the changes of pressure fields over time. The central challenge in\nthis context is the high dimensionality of Eulerian space-time data sets. We\ndemonstrate for the first time that dense 3D+time functions of physics system\ncan be predicted within the latent spaces of neural networks, and we arrive at\na neural-network based simulation algorithm with significant practical\nspeed-ups. We highlight the capabilities of our method with a series of complex\nliquid simulations, and with a set of single-phase buoyancy simulations. With a\nset of trained networks, our method is more than two orders of magnitudes\nfaster than a traditional pressure solver. Additionally, we present and discuss\na series of detailed evaluations for the different components of our algorithm. \n\n"}
{"id": "1802.10519", "contents": "Title: On the Lie bracket approximation approach to distributed optimization:\n  Extensions and limitations Abstract: We consider the problem of solving a smooth convex optimization problem with\nequality and inequality constraints in a distributed fashion. Assuming that we\nhave a group of agents available capable of communicating over a communication\nnetwork described by a time-invariant directed graph, we derive distributed\ncontinuous-time agent dynamics that ensure convergence to a neighborhood of the\noptimal solution of the optimization problem. Following the ideas introduced in\nour previous work, we combine saddle-point dynamics with Lie bracket\napproximation techniques. While the methodology was previously limited to\nlinear constraints and objective functions given by a sum of strictly convex\nseparable functions, we extend these result here and show that it applies to a\nvery general class of optimization problems under mild assumptions on the\ncommunication topology. \n\n"}
{"id": "1803.00113", "contents": "Title: Approximate Inference for Constructing Astronomical Catalogs from Images Abstract: We present a new, fully generative model for constructing astronomical\ncatalogs from optical telescope image sets. Each pixel intensity is treated as\na random variable with parameters that depend on the latent properties of stars\nand galaxies. These latent properties are themselves modeled as random. We\ncompare two procedures for posterior inference. One procedure is based on\nMarkov chain Monte Carlo (MCMC) while the other is based on variational\ninference (VI). The MCMC procedure excels at quantifying uncertainty, while the\nVI procedure is 1000 times faster. On a supercomputer, the VI procedure\nefficiently uses 665,000 CPU cores to construct an astronomical catalog from 50\nterabytes of images in 14.6 minutes, demonstrating the scaling characteristics\nnecessary to construct catalogs for upcoming astronomical surveys. \n\n"}
{"id": "1803.00367", "contents": "Title: A Benchmark Problem in Transportation Networks Abstract: In this note, we propose a case study of freeway traffic flow modeled as a\nhybrid system. We describe two general classes of networks that model flow\nalong a freeway with merging onramps. The admission rate of traffic flow from\neach onramp is metered via a control input. Both classes of networks are easily\nscaled to accommodate arbitrary state dimension. The model is discrete-time and\npossesses piecewise-affine dynamics. Moreover, we present several control\nobjectives that are especially relevant for traffic flow management. The\nproposed model is flexible and extensible and offers a benchmark for evaluating\ntools and techniques developed for hybrid systems. \n\n"}
{"id": "1803.00744", "contents": "Title: Clinically Meaningful Comparisons Over Time: An Approach to Measuring\n  Patient Similarity based on Subsequence Alignment Abstract: Longitudinal patient data has the potential to improve clinical risk\nstratification models for disease. However, chronic diseases that progress\nslowly over time are often heterogeneous in their clinical presentation.\nPatients may progress through disease stages at varying rates. This leads to\npathophysiological misalignment over time, making it difficult to consistently\ncompare patients in a clinically meaningful way. Furthermore, patients present\nclinically for the first time at different stages of disease. This eliminates\nthe possibility of simply aligning patients based on their initial\npresentation. Finally, patient data may be sampled at different rates due to\ndifferences in schedules or missed visits. To address these challenges, we\npropose a robust measure of patient similarity based on subsequence alignment.\nCompared to global alignment techniques that do not account for\npathophysiological misalignment, focusing on the most relevant subsequences\nallows for an accurate measure of similarity between patients. We demonstrate\nthe utility of our approach in settings where longitudinal data, while useful,\nare limited and lack a clear temporal alignment for comparison. Applied to the\ntask of stratifying patients for risk of progression to probable Alzheimer's\nDisease, our approach outperforms models that use only snapshot data (AUROC of\n0.839 vs. 0.812) and models that use global alignment techniques (AUROC of\n0.822). Our results support the hypothesis that patients' trajectories are\nuseful for quantifying inter-patient similarities and that using subsequence\nmatching and can help account for heterogeneity and misalignment in\nlongitudinal data. \n\n"}
{"id": "1803.00886", "contents": "Title: Deep factorization for speech signal Abstract: Various informative factors mixed in speech signals, leading to great\ndifficulty when decoding any of the factors. An intuitive idea is to factorize\neach speech frame into individual informative factors, though it turns out to\nbe highly difficult. Recently, we found that speaker traits, which were assumed\nto be long-term distributional properties, are actually short-time patterns,\nand can be learned by a carefully designed deep neural network (DNN). This\ndiscovery motivated a cascade deep factorization (CDF) framework that will be\npresented in this paper. The proposed framework infers speech factors in a\nsequential way, where factors previously inferred are used as conditional\nvariables when inferring other factors. We will show that this approach can\neffectively factorize speech signals, and using these factors, the original\nspeech spectrum can be recovered with a high accuracy. This factorization and\nreconstruction approach provides potential values for many speech processing\ntasks, e.g., speaker recognition and emotion recognition, as will be\ndemonstrated in the paper. \n\n"}
{"id": "1803.01451", "contents": "Title: Near-optimal planning using approximate dynamic programming to enhance\n  post-hazard community resilience management Abstract: The lack of a comprehensive decision-making approach at the community level\nis an important problem that warrants immediate attention. Network-level\ndecision-making algorithms need to solve large-scale optimization problems that\npose computational challenges. The complexity of the optimization problems\nincreases when various sources of uncertainty are considered. This research\nintroduces a sequential discrete optimization approach, as a decision-making\nframework at the community level for recovery management. The proposed\nmathematical approach leverages approximate dynamic programming along with\nheuristics for the determination of recovery actions. Our methodology overcomes\nthe curse of dimensionality and manages multi-state, large-scale infrastructure\nsystems following disasters. We also provide computational results showing that\nour methodology not only incorporates recovery policies of responsible public\nand private entities within the community but also substantially enhances the\nperformance of their underlying strategies with limited resources. The\nmethodology can be implemented efficiently to identify near-optimal recovery\ndecisions following a severe earthquake based on multiple objectives for an\nelectrical power network of a testbed community coarsely modeled after Gilroy,\nCalifornia, United States. The proposed optimization method supports\nrisk-informed community decision makers within chaotic post-hazard\ncircumstances. \n\n"}
{"id": "1803.01785", "contents": "Title: Differentiable Submodular Maximization Abstract: We consider learning of submodular functions from data. These functions are\nimportant in machine learning and have a wide range of applications, e.g. data\nsummarization, feature selection and active learning. Despite their\ncombinatorial nature, submodular functions can be maximized approximately with\nstrong theoretical guarantees in polynomial time. Typically, learning the\nsubmodular function and optimization of that function are treated separately,\ni.e. the function is first learned using a proxy objective and subsequently\nmaximized. In contrast, we show how to perform learning and optimization\njointly. By interpreting the output of greedy maximization algorithms as\ndistributions over sequences of items and smoothening these distributions, we\nobtain a differentiable objective. In this way, we can differentiate through\nthe maximization algorithms and optimize the model to work well with the\noptimization algorithm. We theoretically characterize the error made by our\napproach, yielding insights into the tradeoff of smoothness and accuracy. We\ndemonstrate the effectiveness of our approach for jointly learning and\noptimizing on synthetic maximum cut data, and on real world applications such\nas product recommendation and image collection summarization. \n\n"}
{"id": "1803.02348", "contents": "Title: Smoothed Action Value Functions for Learning Gaussian Policies Abstract: State-action value functions (i.e., Q-values) are ubiquitous in reinforcement\nlearning (RL), giving rise to popular algorithms such as SARSA and Q-learning.\nWe propose a new notion of action value defined by a Gaussian smoothed version\nof the expected Q-value. We show that such smoothed Q-values still satisfy a\nBellman equation, making them learnable from experience sampled from an\nenvironment. Moreover, the gradients of expected reward with respect to the\nmean and covariance of a parameterized Gaussian policy can be recovered from\nthe gradient and Hessian of the smoothed Q-value function. Based on these\nrelationships, we develop new algorithms for training a Gaussian policy\ndirectly from a learned smoothed Q-value approximator. The approach is\nadditionally amenable to proximal optimization by augmenting the objective with\na penalty on KL-divergence from a previous policy. We find that the ability to\nlearn both a mean and covariance during training leads to significantly\nimproved results on standard continuous control benchmarks. \n\n"}
{"id": "1803.03607", "contents": "Title: On Generation of Adversarial Examples using Convex Programming Abstract: It has been observed that deep learning architectures tend to make erroneous\ndecisions with high reliability for particularly designed adversarial\ninstances. In this work, we show that the perturbation analysis of these\narchitectures provides a framework for generating adversarial instances by\nconvex programming which, for classification tasks, is able to recover variants\nof existing non-adaptive adversarial methods. The proposed framework can be\nused for the design of adversarial noise under various desirable constraints\nand different types of networks. Moreover, this framework is capable of\nexplaining various existing adversarial methods and can be used to derive new\nalgorithms as well. We make use of these results to obtain novel algorithms.\nThe experiments show the competitive performance of the obtained solutions, in\nterms of fooling ratio, when benchmarked with well-known adversarial methods. \n\n"}
{"id": "1803.04812", "contents": "Title: Learning with End-Users in Distribution Grids: Topology and Parameter\n  Estimation Abstract: Efficient operation of distribution grids in the smart-grid era is hindered\nby the limited presence of real-time nodal and line meters. In particular, this\nprevents the easy estimation of grid topology and associated line parameters\nthat are necessary for control and optimization efforts in the grid. This paper\nstudies the problems of topology and parameter estimation in radial balanced\ndistribution grids where measurements are restricted to only the leaf nodes and\nall intermediate nodes are unobserved/hidden. To this end, we propose two exact\nlearning algorithms that use balanced voltage and injection measured only at\nthe end-users. The first algorithm requires time-stamped voltage samples,\nstatistics of nodal power injections and permissible line impedances to recover\nthe true topology. The second and improved algorithm requires only time-stamped\nvoltage and complex power samples to recover both the true topology and\nimpedances without any additional input (e.g., number of grid nodes, statistics\nof injections at hidden nodes, permissible line impedances). We prove the\ncorrectness of both learning algorithms for grids where unobserved buses/nodes\nhave a degree greater than three and discuss extensions to regimes where that\nassumption doesn't hold. Further, we present computational and, more\nimportantly, the sample complexity of our proposed algorithm for joint topology\nand impedance estimation. We illustrate the performance of the designed\nalgorithms through numerical experiments on the IEEE and custom power\ndistribution models. \n\n"}
{"id": "1803.06076", "contents": "Title: Renewable Energy Integration in Distribution System -- Synchrophasor\n  Sensor based Big Data Analysis, Visualization, and System Operation Abstract: Due to the large volume of heterogeneous data provided by both the customer\nand the grid side, a big data visualization platform is built to discover the\nhidden useful knowledge for smart grid (SG) operation, control and situation\nawareness. An open source cluster computing framework based on Apache Spark is\nconsidering to discover the hidden knowledge of the bag-data. And the data is\ntransmitted with a Open System Interconnection (OSI) model to the data\nvisualization platform in a high-speed communication architecture. Google Earth\nand global geographic information system (GIS) are used to design the\nvisualization platform and realize the result with the test bench.\n  The alternating direction method of multipliers (ADMM) is used to compute the\noptimal power flow in distributed manner. The proposed network reconfiguration\nis solved in parallel manner with the limited switches and the strong computing\ncapability.\n  Further more, a multi-timescale operation approach is modeled with a\nthree-phase distributed system, which consists the hourly scheduling at\nsubstation level and the minutes power flow operation at feeder level. The\nsystem cost with renewable generation is minimized at the substation level. The\ngiven error distribution model of renewable generation is simulated with a\nchance constraint, and the derived deterministic form is modeled with Gaussian\nmixture model (GMM) with genetic algorithm-based expectationmaximization\n(GAEM). The system cost is further reduced with the OPF in real-time (RT)\nscheduling. The semidefinite programming (SDP) is used to relax the\nnonconvexity of the three-phase unbalanced distribution system into a convex\nproblem, which make sure to achieve the global optimal result. In the parallel\nmanner, the ADMM is realizing getting the results in a short time. \n\n"}
{"id": "1803.06377", "contents": "Title: Spread of Information with Confirmation Bias in Cyber-Social Networks Abstract: This paper provides a model to investigate information spreading over\ncyber-social network of agents communicating with each other. The cyber-social\nnetwork considered here comprises individuals and news agencies. Each\nindividual holds a belief represented by a scalar. Individuals receive\ninformation from news agencies that are closer to their belief, confirmation\nbias is explicitly incorporated into the model. The proposed dynamics of\ncyber-social networks is adopted from DeGroot-Friedkin model, where the\nindividual's opinion update mechanism is a convex combination of his innate\nopinion, his neighbors' opinions at the previous time step (obtained from the\nsocial network), and the opinions passed along by news agencies from cyber\nlayer which he follows. The characteristics of the interdependent social and\ncyber networks are radically different here: the social network relies on trust\nand hence static while the news agencies are highly dynamic since they are\nweighted as a function of the distance between an individual state and the\nstate of news agency to account for confirmation bias. The conditions for\nconvergence of the aforementioned dynamics to a unique equilibrium are\ncharacterized. The estimation and exact computation of the steady-state values\nunder non-linear and linear state-dependent weight functions are provided.\nFinally, the impact of polarization in the opinions of news agencies on the\npublic opinion evolution is numerically analyzed in the context of the\nwell-known Krackhardt's advice network. \n\n"}
{"id": "1803.06775", "contents": "Title: Comparing and Integrating Constraint Programming and Temporal Planning\n  for Quantum Circuit Compilation Abstract: Recently, the makespan-minimization problem of compiling a general class of\nquantum algorithms into near-term quantum processors has been introduced to the\nAI community. The research demonstrated that temporal planning is a strong\napproach for a class of quantum circuit compilation (QCC) problems. In this\npaper, we explore the use of constraint programming (CP) as an alternative and\ncomplementary approach to temporal planning. We extend previous work by\nintroducing two new problem variations that incorporate important\ncharacteristics identified by the quantum computing community. We apply\ntemporal planning and CP to the baseline and extended QCC problems as both\nstand-alone and hybrid approaches. Our hybrid methods use solutions found by\ntemporal planning to warm start CP, leveraging the ability of the former to\nfind satisficing solutions to problems with a high degree of task optionality,\nan area that CP typically struggles with. The CP model, benefiting from\ninferred bounds on planning horizon length and task counts provided by the warm\nstart, is then used to find higher quality solutions. Our empirical evaluation\nindicates that while stand-alone CP is only competitive for the smallest\nproblems, CP in our hybridization with temporal planning out-performs\nstand-alone temporal planning in the majority of problem classes. \n\n"}
{"id": "1803.07928", "contents": "Title: Minimal Structural Perturbations for Network Controllability: Complexity\n  Analysis Abstract: Link (edge) addition/deletion or sensor/actuator failures are common\nstructural perturbations for real network systems. This paper is related to the\ncomputation complexity of minimal (cost) link insertion, deletion and vertex\ndeletion with respect to structural controllability of networks. Formally,\ngiven a structured system, we prove that: i) it is NP-hard to add the minimal\ncost of links (including links between state variables and from inputs to state\nvariables) from a given set of links to make the system structurally\ncontrollable, even with identical link costs or a prescribed input topology;\nii) it is NP-hard to determine the minimal cost of links whose deletion\ndeteriorates structural controllability of the system, even with identical link\ncosts or when the removable links are restricted in input links. It is also\nproven that determining the minimal cost of inputs whose deletion causes\nstructural uncontrollability is NP-hard in the strong sense. The reductions in\ntheir proofs are technically independent. These results may serve an answer to\nthe general hardness of optimally designing (modifying) a structurally\ncontrollable network topology and of measuring controllability robustness\nagainst link/actuator failures. Some fundamental approximation results for\nthese related problems are also provided. \n\n"}
{"id": "1803.07984", "contents": "Title: Online data assimilation in distributionally robust optimization Abstract: This paper considers a class of real-time decision making problems to\nminimize the expected value of a function that depends on a random variable\n$\\xi$ under an unknown distribution $\\mathbb{P}$. In this process, samples of\n$\\xi$ are collected sequentially in real time, and the decisions are made,\nusing the real-time data, to guarantee out-of-sample performance. We approach\nthis problem in a distributionally robust optimization framework and propose a\nnovel Online Data Assimilation Algorithm for this purpose. This algorithm\nguarantees the out-of-sample performance in high probability, and gradually\nimproves the quality of the data-driven decisions by incorporating the\nstreaming data. We show that the Online Data Assimilation Algorithm guarantees\nconvergence under the streaming data, and a criteria for termination of the\nalgorithm after certain number of data has been collected. \n\n"}
{"id": "1803.08102", "contents": "Title: Optimized mixing by cutting-and-shuffling Abstract: Mixing by cutting-and-shuffling can be understood and predicted using\ndynamical systems based tools and techniques. In existing studies, mixing is\ngenerated by maps that repeat the same cut-and-shuffle process at every\niteration, in a \"fixed\" manner. However, mixing can be greatly improved by\nvarying the cut-and-shuffle parameters at each step, using a \"variable\"\napproach. To demonstrate this approach, we show how to optimize mixing by\ncutting-and-shuffling on the one-dimensional line interval, known as an\ninterval exchange transformation (IET). Mixing can be significantly improved by\noptimizing variable protocols, especially for initial conditions more complex\nthan just a simple two-color line interval. While we show that optimal variable\nIETs can be found analytically for arbitrary numbers of iterations, for more\ncomplex cutting-and-shuffling systems, computationally expensive numerical\noptimization methods would be required. Furthermore, the number of control\nparameters grows linearly with the number of iterations in variable systems.\nTherefore, optimizing over large numbers of iterations is generally\ncomputationally prohibitive. We demonstrate an ad hoc approach to\ncutting-and-shuffling that is computationally inexpensive and guarantees the\nmixing metric is within a constant factor of the optimum. This ad hoc approach\nyields significantly better mixing than fixed IETs which are known to produce\nweak-mixing, because cut pieces never reconnect. The heuristic principles of\nthis method can be applied to more general cutting-and-shuffling systems. \n\n"}
{"id": "1803.08287", "contents": "Title: Learning-based Model Predictive Control for Safe Exploration Abstract: Learning-based methods have been successful in solving complex control tasks\nwithout significant prior knowledge about the system. However, these methods\ntypically do not provide any safety guarantees, which prevents their use in\nsafety-critical, real-world applications. In this paper, we present a\nlearning-based model predictive control scheme that can provide provable\nhigh-probability safety guarantees. To this end, we exploit regularity\nassumptions on the dynamics in terms of a Gaussian process prior to construct\nprovably accurate confidence intervals on predicted trajectories. Unlike\nprevious approaches, we do not assume that model uncertainties are independent.\nBased on these predictions, we guarantee that trajectories satisfy safety\nconstraints. Moreover, we use a terminal set constraint to recursively\nguarantee the existence of safe control actions at every iteration. In our\nexperiments, we show that the resulting algorithm can be used to safely and\nefficiently explore and learn about dynamic systems. \n\n"}
{"id": "1803.08522", "contents": "Title: Frequency violations from random disturbances: an MCMC approach Abstract: The frequency stability of power systems is increasingly challenged by\nvarious types of disturbances. In particular, the increasing penetration of\nrenewable energy sources is increasing the variability of power generation and\nat the same time reducing system inertia against disturbances. In this paper we\nare particularly interested in understanding how rate of change of frequency\n(RoCoF) violations could arise from unusually large power disturbances. We\ndevise a novel specialization, named ghost sampling, of the Metropolis-Hastings\nMarkov Chain Monte Carlo method that is tailored to efficiently sample rare\npower disturbances leading to nodal frequency violations. Generating a\nrepresentative random sample addresses important statistical questions such as\n\"which generator is most likely to be disconnected due to a RoCoF violation?\"\nor \"what is the probability of having simultaneous RoCoF violations, given that\na violation occurs?\" Our method can perform conditional sampling from any joint\ndistribution of power disturbances including, for instance, correlated and\nnon-Gaussian disturbances, features which have both been recently shown to be\nsignificant in security analyses. \n\n"}
{"id": "1803.08551", "contents": "Title: Failure Localization in Power Systems via Tree Partitions Abstract: Cascading failures in power systems propagate non-locally, making the control\nand mitigation of outages extremely hard. In this work, we use the emerging\nconcept of the tree partition of transmission networks to provide an analytical\ncharacterization of line failure localizability in transmission systems. Our\nresults rigorously establish the well perceived intuition in power community\nthat failures cannot cross bridges, and reveal a finer-grained concept that\nencodes more precise information on failure propagations within tree-partition\nregions. Specifically, when a non-bridge line is tripped, the impact of this\nfailure only propagates within well-defined components, which we refer to as\ncells, of the tree partition defined by the bridges. In contrast, when a bridge\nline is tripped, the impact of this failure propagates globally across the\nnetwork, affecting the power flow on all remaining transmission lines. This\ncharacterization suggests that it is possible to improve the system robustness\nby temporarily switching off certain transmission lines, so as to create more,\nsmaller components in the tree partition; thus spatially localizing line\nfailures and making the grid less vulnerable to large-scale outages. We\nillustrate this approach using the IEEE 118-bus test system and demonstrate\nthat switching off a negligible portion of transmission lines allows the impact\nof line failures to be significantly more localized without substantial changes\nin line congestion. \n\n"}
{"id": "1803.10172", "contents": "Title: Distributed Adaptive Sampling for Kernel Matrix Approximation Abstract: Most kernel-based methods, such as kernel or Gaussian process regression,\nkernel PCA, ICA, or $k$-means clustering, do not scale to large datasets,\nbecause constructing and storing the kernel matrix $\\mathbf{K}_n$ requires at\nleast $\\mathcal{O}(n^2)$ time and space for $n$ samples. Recent works show that\nsampling points with replacement according to their ridge leverage scores (RLS)\ngenerates small dictionaries of relevant points with strong spectral\napproximation guarantees for $\\mathbf{K}_n$. The drawback of RLS-based methods\nis that computing exact RLS requires constructing and storing the whole kernel\nmatrix. In this paper, we introduce SQUEAK, a new algorithm for kernel\napproximation based on RLS sampling that sequentially processes the dataset,\nstoring a dictionary which creates accurate kernel matrix approximations with a\nnumber of points that only depends on the effective dimension $d_{eff}(\\gamma)$\nof the dataset. Moreover since all the RLS estimations are efficiently\nperformed using only the small dictionary, SQUEAK is the first RLS sampling\nalgorithm that never constructs the whole matrix $\\mathbf{K}_n$, runs in linear\ntime $\\widetilde{\\mathcal{O}}(nd_{eff}(\\gamma)^3)$ w.r.t. $n$, and requires\nonly a single pass over the dataset. We also propose a parallel and distributed\nversion of SQUEAK that linearly scales across multiple machines, achieving\nsimilar accuracy in as little as\n$\\widetilde{\\mathcal{O}}(\\log(n)d_{eff}(\\gamma)^3)$ time. \n\n"}
{"id": "1803.10350", "contents": "Title: Sobolev spaces with non-Muckenhoupt weights, fractional elliptic\n  operators, and applications Abstract: We propose a new variational model in weighted Sobolev spaces with\nnon-standard weights and applications to image processing. We show that these\nweights are, in general, not of Muckenhoupt type and therefore the classical\nanalysis tools may not apply. For special cases of the weights, the resulting\nvariational problem is known to be equivalent to the fractional Poisson\nproblem. The trace space for the weighted Sobolev space is identified to be\nembedded in a weighted $L^2$ space. We propose a finite element scheme to solve\nthe Euler-Lagrange equations, and for the image denoising application we\npropose an algorithm to identify the unknown weights. The approach is\nillustrated on several test problems and it yields better results when compared\nto the existing total variation techniques. \n\n"}
{"id": "1803.11060", "contents": "Title: COBRAS: Fast, Iterative, Active Clustering with Pairwise Constraints Abstract: Constraint-based clustering algorithms exploit background knowledge to\nconstruct clusterings that are aligned with the interests of a particular user.\nThis background knowledge is often obtained by allowing the clustering system\nto pose pairwise queries to the user: should these two elements be in the same\ncluster or not? Active clustering methods aim to minimize the number of queries\nneeded to obtain a good clustering by querying the most informative pairs\nfirst. Ideally, a user should be able to answer a couple of these queries,\ninspect the resulting clustering, and repeat these two steps until a\nsatisfactory result is obtained. We present COBRAS, an approach to active\nclustering with pairwise constraints that is suited for such an interactive\nclustering process. A core concept in COBRAS is that of a super-instance: a\nlocal region in the data in which all instances are assumed to belong to the\nsame cluster. COBRAS constructs such super-instances in a top-down manner to\nproduce high-quality results early on in the clustering process, and keeps\nrefining these super-instances as more pairwise queries are given to get more\ndetailed clusterings later on. We experimentally demonstrate that COBRAS\nproduces good clusterings at fast run times, making it an excellent candidate\nfor the iterative clustering scenario outlined above. \n\n"}
{"id": "1804.00130", "contents": "Title: Locally Convex Sparse Learning over Networks Abstract: We consider a distributed learning setup where a sparse signal is estimated\nover a network. Our main interest is to save communication resource for\ninformation exchange over the network and reduce processing time. Each node of\nthe network uses a convex optimization based algorithm that provides a locally\noptimum solution for that node. The nodes exchange their signal estimates over\nthe network in order to refine their local estimates. At a node, the\noptimization algorithm is based on an $\\ell_1$-norm minimization with\nappropriate modifications to promote sparsity as well as to include influence\nof estimates from neighboring nodes. Our expectation is that local estimates in\neach node improve fast and converge, resulting in a limited demand for\ncommunication of estimates between nodes and reducing the processing time. We\nprovide restricted-isometry-property (RIP)-based theoretical analysis on\nestimation quality. In the scenario of clean observation, it is shown that the\nlocal estimates converge to the exact sparse signal under certain technical\nconditions. Simulation results show that the proposed algorithms show\ncompetitive performance compared to a globally optimum distributed LASSO\nalgorithm in the sense of convergence speed and estimation error. \n\n"}
{"id": "1804.00499", "contents": "Title: Semantic Adversarial Examples Abstract: Deep neural networks are known to be vulnerable to adversarial examples,\ni.e., images that are maliciously perturbed to fool the model. Generating\nadversarial examples has been mostly limited to finding small perturbations\nthat maximize the model prediction error. Such images, however, contain\nartificial perturbations that make them somewhat distinguishable from natural\nimages. This property is used by several defense methods to counter adversarial\nexamples by applying denoising filters or training the model to be robust to\nsmall perturbations.\n  In this paper, we introduce a new class of adversarial examples, namely\n\"Semantic Adversarial Examples,\" as images that are arbitrarily perturbed to\nfool the model, but in such a way that the modified image semantically\nrepresents the same object as the original image. We formulate the problem of\ngenerating such images as a constrained optimization problem and develop an\nadversarial transformation based on the shape bias property of human cognitive\nsystem. In our method, we generate adversarial images by first converting the\nRGB image into the HSV (Hue, Saturation and Value) color space and then\nrandomly shifting the Hue and Saturation components, while keeping the Value\ncomponent the same. Our experimental results on CIFAR10 dataset show that the\naccuracy of VGG16 network on adversarial color-shifted images is 5.7%. \n\n"}
{"id": "1804.00823", "contents": "Title: Graph2Seq: Graph to Sequence Learning with Attention-based Neural\n  Networks Abstract: The celebrated Sequence to Sequence learning (Seq2Seq) technique and its\nnumerous variants achieve excellent performance on many tasks. However, many\nmachine learning tasks have inputs naturally represented as graphs; existing\nSeq2Seq models face a significant challenge in achieving accurate conversion\nfrom graph form to the appropriate sequence. To address this challenge, we\nintroduce a novel general end-to-end graph-to-sequence neural encoder-decoder\nmodel that maps an input graph to a sequence of vectors and uses an\nattention-based LSTM method to decode the target sequence from these vectors.\nOur method first generates the node and graph embeddings using an improved\ngraph-based neural network with a novel aggregation strategy to incorporate\nedge direction information in the node embeddings. We further introduce an\nattention mechanism that aligns node embeddings and the decoding sequence to\nbetter cope with large graphs. Experimental results on bAbI, Shortest Path, and\nNatural Language Generation tasks demonstrate that our model achieves\nstate-of-the-art performance and significantly outperforms existing graph\nneural networks, Seq2Seq, and Tree2Seq models; using the proposed\nbi-directional node embedding aggregation strategy, the model can converge\nrapidly to the optimal performance. \n\n"}
{"id": "1804.01002", "contents": "Title: Improving Massive MIMO Belief Propagation Detector with Deep Neural\n  Network Abstract: In this paper, deep neural network (DNN) is utilized to improve the belief\npropagation (BP) detection for massive multiple-input multiple-output (MIMO)\nsystems. A neural network architecture suitable for detection task is firstly\nintroduced by unfolding BP algorithms. DNN MIMO detectors are then proposed\nbased on two modified BP detectors, damped BP and max-sum BP. The correction\nfactors in these algorithms are optimized through deep learning techniques,\naiming at improved detection performance. Numerical results are presented to\ndemonstrate the performance of the DNN detectors in comparison with various BP\nmodifications. The neural network is trained once and can be used for multiple\nonline detections. The results show that, compared to other state-of-the-art\ndetectors, the DNN detectors can achieve lower bit error rate (BER) with\nimproved robustness against various antenna configurations and channel\nconditions at the same level of complexity. \n\n"}
{"id": "1804.02008", "contents": "Title: Deterministic guarantees for Burer-Monteiro factorizations of smooth\n  semidefinite programs Abstract: We consider semidefinite programs (SDPs) with equality constraints. The\nvariable to be optimized is a positive semidefinite matrix $X$ of size $n$.\nFollowing the Burer--Monteiro approach, we optimize a factor $Y$ of size $n\n\\times p$ instead, such that $X = YY^T$. This ensures positive semidefiniteness\nat no cost and can reduce the dimension of the problem if $p$ is small, but\nresults in a non-convex optimization problem with a quadratic cost function and\nquadratic equality constraints in $Y$. In this paper, we show that if the set\nof constraints on $Y$ regularly defines a smooth manifold, then, despite\nnon-convexity, first- and second-order necessary optimality conditions are also\nsufficient, provided $p$ is large enough. For smaller values of $p$, we show a\nsimilar result holds for almost all (linear) cost functions. Under those\nconditions, a global optimum $Y$ maps to a global optimum $X = YY^T$ of the\nSDP. We deduce old and new consequences for SDP relaxations of the generalized\neigenvector problem, the trust-region subproblem and quadratic optimization\nover several spheres, as well as for the Max-Cut and Orthogonal-Cut SDPs which\nare common relaxations in stochastic block modeling and synchronization of\nrotations. \n\n"}
{"id": "1804.03176", "contents": "Title: Frank-Wolfe Splitting via Augmented Lagrangian Method Abstract: Minimizing a function over an intersection of convex sets is an important\ntask in optimization that is often much more challenging than minimizing it\nover each individual constraint set. While traditional methods such as\nFrank-Wolfe (FW) or proximal gradient descent assume access to a linear or\nquadratic oracle on the intersection, splitting techniques take advantage of\nthe structure of each sets, and only require access to the oracle on the\nindividual constraints. In this work, we develop and analyze the Frank-Wolfe\nAugmented Lagrangian (FW-AL) algorithm, a method for minimizing a smooth\nfunction over convex compact sets related by a \"linear consistency\" constraint\nthat only requires access to a linear minimization oracle over the individual\nconstraints. It is based on the Augmented Lagrangian Method (ALM), also known\nas Method of Multipliers, but unlike most existing splitting methods, it only\nrequires access to linear (instead of quadratic) minimization oracles. We use\nrecent advances in the analysis of Frank-Wolfe and the alternating direction\nmethod of multipliers algorithms to prove a sublinear convergence rate for\nFW-AL over general convex compact sets and a linear convergence rate for\npolytopes. \n\n"}
{"id": "1804.04849", "contents": "Title: The unreasonable effectiveness of the forget gate Abstract: Given the success of the gated recurrent unit, a natural question is whether\nall the gates of the long short-term memory (LSTM) network are necessary.\nPrevious research has shown that the forget gate is one of the most important\ngates in the LSTM. Here we show that a forget-gate-only version of the LSTM\nwith chrono-initialized biases, not only provides computational savings but\noutperforms the standard LSTM on multiple benchmark datasets and competes with\nsome of the best contemporary models. Our proposed network, the JANET, achieves\naccuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the\nstandard LSTM which yields accuracies of 98.5% and 91%. \n\n"}
{"id": "1804.05020", "contents": "Title: A Deep Learning Approach to Fast, Format-Agnostic Detection of Malicious\n  Web Content Abstract: Malicious web content is a serious problem on the Internet today. In this\npaper we propose a deep learning approach to detecting malevolent web pages.\nWhile past work on web content detection has relied on syntactic parsing or on\nemulation of HTML and Javascript to extract features, our approach operates\ndirectly on a language-agnostic stream of tokens extracted directly from static\nHTML files with a simple regular expression. This makes it fast enough to\noperate in high-frequency data contexts like firewalls and web proxies, and\nallows it to avoid the attack surface exposure of complex parsing and emulation\ncode. Unlike well-known approaches such as bag-of-words models, which ignore\nspatial information, our neural network examines content at hierarchical\nspatial scales, allowing our model to capture locality and yielding superior\naccuracy compared to bag-of-words baselines. Our proposed architecture achieves\na 97.5% detection rate at a 0.1% false positive rate, and classifies\nsmall-batched web pages at a rate of over 100 per second on commodity hardware.\nThe speed and accuracy of our approach makes it appropriate for deployment to\nendpoints, firewalls, and web proxies. \n\n"}
{"id": "1804.05316", "contents": "Title: From CDF to PDF --- A Density Estimation Method for High Dimensional\n  Data Abstract: CDF2PDF is a method of PDF estimation by approximating CDF. The original idea\nof it was previously proposed in [1] called SIC. However, SIC requires\nadditional hyper-parameter tunning, and no algorithms for computing higher\norder derivative from a trained NN are provided in [1]. CDF2PDF improves SIC by\navoiding the time-consuming hyper-parameter tuning part and enabling higher\norder derivative computation to be done in polynomial time. Experiments of this\nmethod for one-dimensional data shows promising results. \n\n"}
{"id": "1804.07565", "contents": "Title: Moments and convex optimization for analysis and control of nonlinear\n  partial differential equations Abstract: This work presents a convex-optimization-based framework for analysis and\ncontrol of nonlinear partial differential equations. The approach uses a\nparticular weak embedding of the nonlinear PDE, resulting in a linear equation\nin the space of Borel measures. This equation is then used as a constraint of\nan infinite-dimensional linear programming problem (LP). This LP is then\napproximated by a hierarchy of convex, finite-dimensional, semidefinite\nprogramming problems (SDPs). In the case of analysis of uncontrolled PDEs, the\nsolutions to these SDPs provide bounds on a specified, possibly nonlinear,\nfunctional of the solutions to the PDE; in the case of PDE control, the\nsolutions to these SDPs provide bounds on the optimal value of a given optimal\ncontrol problem as well as suboptimal feedback controllers. The entire approach\nis based purely on convex optimization and does not rely on spatio-temporal\ngridding, even though the PDE addressed can be fully nonlinear. The approach is\napplicable to a very broad class nonlinear PDEs with polynomial data.\nComputational complexity is analyzed and several complexity reduction\nprocedures are described. Numerical examples demonstrate the approach. \n\n"}
{"id": "1804.07669", "contents": "Title: Modelling customer online behaviours with neural networks: applications\n  to conversion prediction and advertising retargeting Abstract: In this paper, we apply neural networks into digital marketing world for the\npurpose of better targeting the potential customers. To do so, we model the\ncustomer online behaviours using dedicated neural network architectures.\nStarting from user searched keywords in a search engine to the landing page and\ndifferent following pages, until the user left the site, we model the whole\nvisited journey with a Recurrent Neural Network (RNN), together with\nConvolution Neural Networks (CNN) that can take into account of the semantic\nmeaning of user searched keywords and different visited page names. With such\nmodel, we use Monte Carlo simulation to estimate the conversion rates of each\npotential customer in the future visiting. We believe our concept and the\npreliminary promising results in this paper enable the use of largely available\ncustomer online behaviours data for advanced digital marketing analysis. \n\n"}
{"id": "1804.07884", "contents": "Title: Neural-inspired sensors enable sparse, efficient classification of\n  spatiotemporal data Abstract: Sparse sensor placement is a central challenge in the efficient\ncharacterization of complex systems when the cost of acquiring and processing\ndata is high. Leading sparse sensing methods typically exploit either spatial\nor temporal correlations, but rarely both. This work introduces a new sparse\nsensor optimization that is designed to leverage the rich spatiotemporal\ncoherence exhibited by many systems. Our approach is inspired by the remarkable\nperformance of flying insects, which use a few embedded strain-sensitive\nneurons to achieve rapid and robust flight control despite large gust\ndisturbances. Specifically, we draw on nature to identify targeted\nneural-inspired sensors on a flapping wing to detect body rotation. This task\nis particularly challenging as the rotational twisting mode is three\norders-of-magnitude smaller than the flapping modes. We show that nonlinear\nfiltering in time, built to mimic strain-sensitive neurons, is essential to\ndetect rotation, whereas instantaneous measurements fail. Optimized sparse\nsensor placement results in efficient classification with approximately ten\nsensors, achieving the same accuracy and noise robustness as full measurements\nconsisting of hundreds of sensors. Sparse sensing with neural inspired encoding\nestablishes a new paradigm in hyper-efficient, embodied sensing of\nspatiotemporal data and sheds light on principles of biological sensing for\nagile flight control. \n\n"}
{"id": "1804.10483", "contents": "Title: A Graph-Theoretic Approach to the $\\mathcal{H}_{\\infty}$ Performance of\n  Dynamical Systems on Directed and Undirected Networks Abstract: We study a graph-theoretic approach to the $\\mathcal{H}_{\\infty}$ performance\nof leader following consensus dynamics on directed and undirected graphs. We\nfirst provide graph-theoretic bounds on the system $\\mathcal{H}_{\\infty}$ norm\nof the leader following dynamics and show the tightness of the proposed bounds.\nThen, we discuss the relation between the system $\\mathcal{H}_{\\infty}$ norm\nfor directed and undirected networks for specific classes of graphs, i.e.,\nbalanced digraphs and directed trees. Moreover, we investigate the effects of\nadding directed edges to a directed tree on the resulting system\n$\\mathcal{H}_{\\infty}$ norm. In the end, we apply these theoretical results to\na reference velocity tracking problem in a platoon of connected vehicles and\ndiscuss the effect of the location of the leading vehicle on the overall\n$\\mathcal{H}_{\\infty}$ performance of the system. \n\n"}
{"id": "1804.11154", "contents": "Title: On the Stability of Gradient Based Turbulent Flow Control without\n  Regularization Abstract: In this paper, we discuss selected adjoint approaches for the turbulent flow\ncontrol. In particular, we focus on the application of adjoint solvers for the\nscope of noise reduction, in which flow solutions are obtained by large eddy\nand direct numerical simulations. Optimization results obtained with round and\nplane jet configurations are presented. The results indicate that using large\ncontrol horizons poses a serious problem for the control of turbulent flows due\nto existence of very large sensitivity values with respect to control\nparameters. Typically these sensitivities grow in time and lead to arithmetic\noverflow in the computations. This phenomena is illustrated by a sensitivity\nstudy performed with an exact tangent-linear solver obtained by algorithmic\ndifferentiation techniques. \n\n"}
{"id": "1805.00658", "contents": "Title: Distributed Big-Data Optimization via Block-Iterative Convexification\n  and Averaging Abstract: In this paper, we study distributed big-data nonconvex optimization in\nmulti-agent networks. We consider the (constrained) minimization of the sum of\na smooth (possibly) nonconvex function, i.e., the agents' sum-utility, plus a\nconvex (possibly) nonsmooth regularizer. Our interest is in big-data problems\nwherein there is a large number of variables to optimize. If treated by means\nof standard distributed optimization algorithms, these large-scale problems may\nbe intractable, due to the prohibitive local computation and communication\nburden at each node. We propose a novel distributed solution method whereby at\neach iteration agents optimize and then communicate (in an uncoordinated\nfashion) only a subset of their decision variables. To deal with non-convexity\nof the cost function, the novel scheme hinges on Successive Convex\nApproximation (SCA) techniques coupled with i) a tracking mechanism\ninstrumental to locally estimate gradient averages; and ii) a novel block-wise\nconsensus-based protocol to perform local block-averaging operations and\ngradient tacking. Asymptotic convergence to stationary solutions of the\nnonconvex problem is established. Finally, numerical results show the\neffectiveness of the proposed algorithm and highlight how the block dimension\nimpacts on the communication overhead and practical convergence speed. \n\n"}
{"id": "1805.01648", "contents": "Title: Sharp convergence rates for Langevin dynamics in the nonconvex setting Abstract: We study the problem of sampling from a distribution $p^*(x) \\propto\n\\exp\\left(-U(x)\\right)$, where the function $U$ is $L$-smooth everywhere and\n$m$-strongly convex outside a ball of radius $R$, but potentially nonconvex\ninside this ball. We study both overdamped and underdamped Langevin MCMC and\nestablish upper bounds on the number of steps required to obtain a sample from\na distribution that is within $\\epsilon$ of $p^*$ in $1$-Wasserstein distance.\nFor the first-order method (overdamped Langevin MCMC), the iteration complexity\nis $\\tilde{\\mathcal{O}}\\left(e^{cLR^2}d/\\epsilon^2\\right)$, where $d$ is the\ndimension of the underlying space. For the second-order method (underdamped\nLangevin MCMC), the iteration complexity is\n$\\tilde{\\mathcal{O}}\\left(e^{cLR^2}\\sqrt{d}/\\epsilon\\right)$ for an explicit\npositive constant $c$. Surprisingly, the iteration complexity for both these\nalgorithms is only polynomial in the dimension $d$ and the target accuracy\n$\\epsilon$. It is exponential, however, in the problem parameter $LR^2$, which\nis a measure of non-log-concavity of the target distribution. \n\n"}
{"id": "1805.01889", "contents": "Title: t-PINE: Tensor-based Predictable and Interpretable Node Embeddings Abstract: Graph representations have increasingly grown in popularity during the last\nyears. Existing representation learning approaches explicitly encode network\nstructure. Despite their good performance in downstream processes (e.g., node\nclassification, link prediction), there is still room for improvement in\ndifferent aspects, like efficacy, visualization, and interpretability. In this\npaper, we propose, t-PINE, a method that addresses these limitations. Contrary\nto baseline methods, which generally learn explicit graph representations by\nsolely using an adjacency matrix, t-PINE avails a multi-view information graph,\nthe adjacency matrix represents the first view, and a nearest neighbor\nadjacency, computed over the node features, is the second view, in order to\nlearn explicit and implicit node representations, using the Canonical Polyadic\n(a.k.a. CP) decomposition. We argue that the implicit and the explicit mapping\nfrom a higher-dimensional to a lower-dimensional vector space is the key to\nlearn more useful, highly predictable, and gracefully interpretable\nrepresentations. Having good interpretable representations provides a good\nguidance to understand how each view contributes to the representation learning\nprocess. In addition, it helps us to exclude unrelated dimensions. Extensive\nexperiments show that t-PINE drastically outperforms baseline methods by up to\n158.6% with respect to Micro-F1, in several multi-label classification\nproblems, while it has high visualization and interpretability utility. \n\n"}
{"id": "1805.02349", "contents": "Title: (Nearly) Efficient Algorithms for the Graph Matching Problem on\n  Correlated Random Graphs Abstract: We give a quasipolynomial time algorithm for the graph matching problem (also\nknown as noisy or robust graph isomorphism) on correlated random graphs.\nSpecifically, for every $\\gamma>0$, we give a $n^{O(\\log n)}$ time algorithm\nthat given a pair of $\\gamma$-correlated $G(n,p)$ graphs $G_0,G_1$ with average\ndegree between $n^{\\varepsilon}$ and $n^{1/153}$ for $\\varepsilon = o(1)$,\nrecovers the \"ground truth\" permutation $\\pi\\in S_n$ that matches the vertices\nof $G_0$ to the vertices of $G_n$ in the way that minimizes the number of\nmismatched edges. We also give a recovery algorithm for a denser regime, and a\npolynomial-time algorithm for distinguishing between correlated and\nuncorrelated graphs.\n  Prior work showed that recovery is information-theoretically possible in this\nmodel as long the average degree was at least $\\log n$, but sub-exponential\ntime algorithms were only known in the dense case (i.e., for $p > n^{-o(1)}$).\nMoreover, \"Percolation Graph Matching\", which is the most common heuristic for\nthis problem, has been shown to require knowledge of $n^{\\Omega(1)}$ \"seeds\"\n(i.e., input/output pairs of the permutation $\\pi$) to succeed in this regime.\nIn contrast our algorithms require no seed and succeed for $p$ which is as low\nas $n^{o(1)-1}$. \n\n"}
{"id": "1805.02489", "contents": "Title: Transformer for Emotion Recognition Abstract: This paper describes the UMONS solution for the OMG-Emotion Challenge. We\nexplore a context-dependent architecture where the arousal and valence of an\nutterance are predicted according to its surrounding context (i.e. the\npreceding and following utterances of the video). We report an improvement when\ntaking into account context for both unimodal and multimodal predictions. \n\n"}
{"id": "1805.02508", "contents": "Title: A Generic Self-Evolving Neuro-Fuzzy Controller based High-performance\n  Hexacopter Altitude Control System Abstract: Nowadays, the application of fully autonomous system like rotary wing\nunmanned air vehicles (UAVs) is increasing sharply. Due to the complex\nnonlinear dynamics, a huge research interest is witnessed in developing\nlearning machine based intelligent, self-organizing evolving controller for\nthese vehicles notably to address the system's dynamic characteristics. In this\nwork, such an evolving controller namely Generic-controller (G-controller) is\nproposed to control the altitude of a rotary wing UAV namely hexacopter. This\ncontroller can work with very minor expert domain knowledge. The evolving\narchitecture of this controller is based on an advanced incremental learning\nalgorithm namely Generic Evolving Neuro-Fuzzy Inference System (GENEFIS). The\ncontroller does not require any offline training, since it starts operating\nfrom scratch with an empty set of fuzzy rules, and then add or delete rules on\ndemand. The adaptation laws for the consequent parameters are derived from the\nsliding mode control (SMC) theory. The Lyapunov theory is used to guarantee the\nstability of the proposed controller. In addition, an auxiliary robustifying\ncontrol term is implemented to obtain a uniform asymptotic convergence of\ntracking error to zero. Finally, the G-controller's performance evaluation is\nobserved through the altitude tracking of a UAV namely hexacopter for various\ntrajectories. \n\n"}
{"id": "1805.04276", "contents": "Title: Leveraging Grammar and Reinforcement Learning for Neural Program\n  Synthesis Abstract: Program synthesis is the task of automatically generating a program\nconsistent with a specification. Recent years have seen proposal of a number of\nneural approaches for program synthesis, many of which adopt a sequence\ngeneration paradigm similar to neural machine translation, in which\nsequence-to-sequence models are trained to maximize the likelihood of known\nreference programs. While achieving impressive results, this strategy has two\nkey limitations. First, it ignores Program Aliasing: the fact that many\ndifferent programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many\nsemantically correct programs, which can adversely affect the synthesizer\nperformance. Second, this strategy overlooks the fact that programs have a\nstrict syntax that can be efficiently checked. To address the first limitation,\nwe perform reinforcement learning on top of a supervised model with an\nobjective that explicitly maximizes the likelihood of generating semantically\ncorrect programs. For addressing the second limitation, we introduce a training\nprocedure that directly maximizes the probability of generating syntactically\ncorrect programs that fulfill the specification. We show that our contributions\nlead to improved accuracy of the models, especially in cases where the training\ndata is limited. \n\n"}
{"id": "1805.05189", "contents": "Title: Randomized Smoothing SVRG for Large-scale Nonsmooth Convex Optimization Abstract: In this paper, we consider the problem of minimizing the average of a large\nnumber of nonsmooth and convex functions. Such problems often arise in typical\nmachine learning problems as empirical risk minimization, but are\ncomputationally very challenging. We develop and analyze a new algorithm that\nachieves robust linear convergence rate, and both its time complexity and\ngradient complexity are superior than state-of-art nonsmooth algorithms and\nsubgradient-based schemes. Besides, our algorithm works without any extra error\nbound conditions on the objective function as well as the common\nstrongly-convex condition. We show that our algorithm has wide applications in\noptimization and machine learning problems, and demonstrate experimentally that\nit performs well on a large-scale ranking problem. \n\n"}
{"id": "1805.05935", "contents": "Title: Feedback-Based Tree Search for Reinforcement Learning Abstract: Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of\nartificial intelligence (AI) application domains, we propose a model-based\nreinforcement learning (RL) technique that iteratively applies MCTS on batches\nof small, finite-horizon versions of the original infinite-horizon Markov\ndecision process. The terminal condition of the finite-horizon problems, or the\nleaf-node evaluator of the decision tree generated by MCTS, is specified using\na combination of an estimated value function and an estimated policy function.\nThe recommendations generated by the MCTS procedure are then provided as\nfeedback in order to refine, through classification and regression, the\nleaf-node evaluator for the next iteration. We provide the first sample\ncomplexity bounds for a tree search-based RL algorithm. In addition, we show\nthat a deep neural network implementation of the technique can create a\ncompetitive AI agent for the popular multi-player online battle arena (MOBA)\ngame King of Glory. \n\n"}
{"id": "1805.06498", "contents": "Title: Utility maximization with proportional transaction costs under model\n  uncertainty Abstract: We consider a discrete time financial market with proportional transaction\ncosts under model uncertainty, and study a num\\'eraire-based semi-static\nutility maximization problem with an exponential utility preference. The\nrandomization techniques recently developed in \\cite{BDT17} allow us to\ntransform the original problem into a frictionless counterpart on an enlarged\nspace. By suggesting a different dynamic programming argument than in\n\\cite{bartl2016exponential}, we are able to prove the existence of the optimal\nstrategy and the convex duality theorem in our context with transaction costs.\nIn the frictionless framework, this alternative dynamic programming argument\nalso allows us to generalize the main results in \\cite{bartl2016exponential} to\na weaker market condition. Moreover, as an application of the duality\nrepresentation, some basic features of utility indifference prices are\ninvestigated in our robust setting with transaction costs. \n\n"}
{"id": "1805.07072", "contents": "Title: Optimizing for Generalization in Machine Learning with Cross-Validation\n  Gradients Abstract: Cross-validation is the workhorse of modern applied statistics and machine\nlearning, as it provides a principled framework for selecting the model that\nmaximizes generalization performance. In this paper, we show that the\ncross-validation risk is differentiable with respect to the hyperparameters and\ntraining data for many common machine learning algorithms, including logistic\nregression, elastic-net regression, and support vector machines. Leveraging\nthis property of differentiability, we propose a cross-validation gradient\nmethod (CVGM) for hyperparameter optimization. Our method enables efficient\noptimization in high-dimensional hyperparameter spaces of the cross-validation\nrisk, the best surrogate of the true generalization ability of our learning\nalgorithm. \n\n"}
{"id": "1805.07107", "contents": "Title: Extending Dynamic Bayesian Networks for Anomaly Detection in Complex\n  Logs Abstract: Checking various log files from different processes can be a tedious task as\nthese logs contain lots of events, each with a (possibly large) number of\nattributes. We developed a way to automatically model log files and detect\noutlier traces in the data. For that we extend Dynamic Bayesian Networks to\nmodel the normal behavior found in log files. We introduce a new algorithm that\nis able to learn a model of a log file starting from the data itself. The model\nis capable of scoring traces even when new values or new combinations of values\nappear in the log file. \n\n"}
{"id": "1805.07516", "contents": "Title: Estimation of Non-Normalized Mixture Models and Clustering Using Deep\n  Representation Abstract: We develop a general method for estimating a finite mixture of non-normalized\nmodels. Here, a non-normalized model is defined to be a parametric distribution\nwith an intractable normalization constant. Existing methods for estimating\nnon-normalized models without computing the normalization constant are not\napplicable to mixture models because they contain more than one intractable\nnormalization constant. The proposed method is derived by extending noise\ncontrastive estimation (NCE), which estimates non-normalized models by\ndiscriminating between the observed data and some artificially generated noise.\nWe also propose an extension of NCE with multiple noise distributions. Then,\nbased on the observation that conventional classification learning with neural\nnetworks is implicitly assuming an exponential family as a generative model, we\nintroduce a method for clustering unlabeled data by estimating a finite mixture\nof distributions in an exponential family. Estimation of this mixture model is\nattained by the proposed extensions of NCE where the training data of neural\nnetworks are used as noise. Thus, the proposed method provides a\nprobabilistically principled clustering method that is able to utilize a deep\nrepresentation. Application to image clustering using a deep neural network\ngives promising results. \n\n"}
{"id": "1805.07956", "contents": "Title: Multiple-Step Greedy Policies in Online and Approximate Reinforcement\n  Learning Abstract: Multiple-step lookahead policies have demonstrated high empirical competence\nin Reinforcement Learning, via the use of Monte Carlo Tree Search or Model\nPredictive Control. In a recent work \\cite{efroni2018beyond}, multiple-step\ngreedy policies and their use in vanilla Policy Iteration algorithms were\nproposed and analyzed. In this work, we study multiple-step greedy algorithms\nin more practical setups. We begin by highlighting a counter-intuitive\ndifficulty, arising with soft-policy updates: even in the absence of\napproximations, and contrary to the 1-step-greedy case, monotonic policy\nimprovement is not guaranteed unless the update stepsize is sufficiently large.\nTaking particular care about this difficulty, we formulate and analyze online\nand approximate algorithms that use such a multi-step greedy operator. \n\n"}
{"id": "1805.08306", "contents": "Title: Deep Energy Estimator Networks Abstract: Density estimation is a fundamental problem in statistical learning. This\nproblem is especially challenging for complex high-dimensional data due to the\ncurse of dimensionality. A promising solution to this problem is given here in\nan inference-free hierarchical framework that is built on score matching. We\nrevisit the Bayesian interpretation of the score function and the Parzen score\nmatching, and construct a multilayer perceptron with a scalable objective for\nlearning the energy (i.e. the unnormalized log-density), which is then\noptimized with stochastic gradient descent. In addition, the resulting deep\nenergy estimator network (DEEN) is designed as products of experts. We present\nthe utility of DEEN in learning the energy, the score function, and in\nsingle-step denoising experiments for synthetic and high-dimensional data. We\nalso diagnose stability problems in the direct estimation of the score function\nthat had been observed for denoising autoencoders. \n\n"}
{"id": "1805.09633", "contents": "Title: Decentralized MPC based Obstacle Avoidance for Multi-Robot Target\n  Tracking Scenarios Abstract: In this work, we consider the problem of decentralized multi-robot target\ntracking and obstacle avoidance in dynamic environments. Each robot executes a\nlocal motion planning algorithm which is based on model predictive control\n(MPC). The planner is designed as a quadratic program, subject to constraints\non robot dynamics and obstacle avoidance. Repulsive potential field functions\nare employed to avoid obstacles. The novelty of our approach lies in embedding\nthese non-linear potential field functions as constraints within a convex\noptimization framework. Our method convexifies non-convex constraints and\ndependencies, by replacing them as pre-computed external input forces in robot\ndynamics. The proposed algorithm additionally incorporates different methods to\navoid field local minima problems associated with using potential field\nfunctions in planning. The motion planner does not enforce predefined\ntrajectories or any formation geometry on the robots and is a comprehensive\nsolution for cooperative obstacle avoidance in the context of multi-robot\ntarget tracking. We perform simulation studies in different environmental\nscenarios to showcase the convergence and efficacy of the proposed algorithm.\nVideo of simulation studies: \\url{https://youtu.be/umkdm82Tt0M} \n\n"}
{"id": "1805.09965", "contents": "Title: LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed\n  Learning Abstract: This paper presents a new class of gradient methods for distributed machine\nlearning that adaptively skip the gradient calculations to learn with reduced\ncommunication and computation. Simple rules are designed to detect\nslowly-varying gradients and, therefore, trigger the reuse of outdated\ngradients. The resultant gradient-based algorithms are termed Lazily Aggregated\nGradient --- justifying our acronym LAG used henceforth. Theoretically, the\nmerits of this contribution are: i) the convergence rate is the same as batch\ngradient descent in strongly-convex, convex, and nonconvex smooth cases; and,\nii) if the distributed datasets are heterogeneous (quantified by certain\nmeasurable constants), the communication rounds needed to achieve a targeted\naccuracy are reduced thanks to the adaptive reuse of lagged gradients.\nNumerical experiments on both synthetic and real data corroborate a significant\ncommunication reduction compared to alternatives. \n\n"}
{"id": "1805.10049", "contents": "Title: FLOreS - Fractional order loop shaping MATLAB toolbox Abstract: A novel toolbox named FLOreS is presented for intuitive design of fractional\norder controllers (FOC) using industry standard loop shaping technique. This\nwill allow control engineers to use frequency response data (FRD) of the plant\nto design FOCs by shaping the open loop to meet the necessary specifications of\nstability, robustness, tracking, precision and bandwidth. FLOreS provides a\ngraphical approach using closed-loop sensitivity functions for overall insight\ninto system performance. The main advantage over existing optimization\ntoolboxes for FOC is that the engineer can use prior knowledge and expertise of\nplant during design of FOC. Different approximation methods for fractional\norder filters are also included for greater freedom of final implementation.\nThis combined with the included example plants enables additionally to be used\nas an educational tool. FLOreS has been used for design and implementation of\nboth integer and fractional order controllers on a precision stage to prove\nindustry readiness. \n\n"}
{"id": "1805.10309", "contents": "Title: Learning Self-Imitating Diverse Policies Abstract: The success of popular algorithms for deep reinforcement learning, such as\npolicy-gradients and Q-learning, relies heavily on the availability of an\ninformative reward signal at each timestep of the sequential decision-making\nprocess. When rewards are only sparsely available during an episode, or a\nrewarding feedback is provided only after episode termination, these algorithms\nperform sub-optimally due to the difficultly in credit assignment.\nAlternatively, trajectory-based policy optimization methods, such as\ncross-entropy method and evolution strategies, do not require per-timestep\nrewards, but have been found to suffer from high sample complexity by\ncompleting forgoing the temporal nature of the problem. Improving the\nefficiency of RL algorithms in real-world problems with sparse or episodic\nrewards is therefore a pressing need. In this work, we introduce a\nself-imitation learning algorithm that exploits and explores well in the sparse\nand episodic reward settings. We view each policy as a state-action visitation\ndistribution and formulate policy optimization as a divergence minimization\nproblem. We show that with Jensen-Shannon divergence, this divergence\nminimization problem can be reduced into a policy-gradient algorithm with\nshaped rewards learned from experience replays. Experimental results indicate\nthat our algorithm works comparable to existing algorithms in environments with\ndense rewards, and significantly better in environments with sparse and\nepisodic rewards. We then discuss limitations of self-imitation learning, and\npropose to solve them by using Stein variational policy gradient descent with\nthe Jensen-Shannon kernel to learn multiple diverse policies. We demonstrate\nits effectiveness on a challenging variant of continuous-control MuJoCo\nlocomotion tasks. \n\n"}
{"id": "1805.10477", "contents": "Title: Nonlinear Inductive Matrix Completion based on One-layer Neural Networks Abstract: The goal of a recommendation system is to predict the interest of a user in a\ngiven item by exploiting the existing set of ratings as well as certain\nuser/item features. A standard approach to modeling this problem is Inductive\nMatrix Completion where the predicted rating is modeled as an inner product of\nthe user and the item features projected onto a latent space. In order to learn\nthe parameters effectively from a small number of observed ratings, the latent\nspace is constrained to be low-dimensional which implies that the parameter\nmatrix is constrained to be low-rank. However, such bilinear modeling of the\nratings can be limiting in practice and non-linear prediction functions can\nlead to significant improvements. A natural approach to introducing\nnon-linearity in the prediction function is to apply a non-linear activation\nfunction on top of the projected user/item features. Imposition of\nnon-linearities further complicates an already challenging problem that has two\nsources of non-convexity: a) low-rank structure of the parameter matrix, and b)\nnon-linear activation function. We show that one can still solve the non-linear\nInductive Matrix Completion problem using gradient descent type methods as long\nas the solution is initialized well. That is, close to the optima, the\noptimization function is strongly convex and hence admits standard optimization\ntechniques, at least for certain activation functions, such as Sigmoid and\ntanh. We also highlight the importance of the activation function and show how\nReLU can behave significantly differently than say a sigmoid function. Finally,\nwe apply our proposed technique to recommendation systems and semi-supervised\nclustering, and show that our method can lead to much better performance than\nstandard linear Inductive Matrix Completion methods. \n\n"}
{"id": "1805.10786", "contents": "Title: Phase portrait control for 1D monostable and bistable reaction-diffusion\n  equations Abstract: We consider the problem of controlling parabolic semilinear equations arising\nin population dynamics, either in finite time or infinite time. These are the\nmonostable and bistable equations on $(0,L)$ for a density of individuals $0\n\\leq y(t,x) \\leq 1$, with Dirichlet controls taking their values in $[0,1]$. We\nprove that the system can never be steered to extinction (steady state $0$) or\ninvasion (steady state $1$) in finite time, but is asymptotically controllable\nto $1$ independently of the size $L$, and to $0$ if the length $L$ of the\ninterval domain is less than some threshold value $L^\\star$, which can be\ncomputed from transcendental integrals. In the bistable case, controlling to\nthe other homogeneous steady state $0 <\\theta< 1$ is much more intricate. We\nrely on a staircase control strategy to prove that $\\theta$ can be reached in\nfinite time if and only if $L< L^\\star$. The phase plane analysis of those\nequations is instrumental in the whole process. It allows us to read obstacles\nto controllability, compute the threshold value for domain size as well as\ndesign the path of steady states for the control strategy. \n\n"}
{"id": "1805.10970", "contents": "Title: A Generative Model For Electron Paths Abstract: Chemical reactions can be described as the stepwise redistribution of\nelectrons in molecules. As such, reactions are often depicted using\n`arrow-pushing' diagrams which show this movement as a sequence of arrows. We\npropose an electron path prediction model (ELECTRO) to learn these sequences\ndirectly from raw reaction data. Instead of predicting product molecules\ndirectly from reactant molecules in one shot, learning a model of electron\nmovement has the benefits of (a) being easy for chemists to interpret, (b)\nincorporating constraints of chemistry, such as balanced atom counts before and\nafter the reaction, and (c) naturally encoding the sparsity of chemical\nreactions, which usually involve changes in only a small number of atoms in the\nreactants.We design a method to extract approximate reaction paths from any\ndataset of atom-mapped reaction SMILES strings. Our model achieves excellent\nperformance on an important subset of the USPTO reaction dataset, comparing\nfavorably to the strongest baselines. Furthermore, we show that our model\nrecovers a basic knowledge of chemistry without being explicitly trained to do\nso. \n\n"}
{"id": "1805.11706", "contents": "Title: Supervised Policy Update for Deep Reinforcement Learning Abstract: We propose a new sample-efficient methodology, called Supervised Policy\nUpdate (SPU), for deep reinforcement learning. Starting with data generated by\nthe current policy, SPU formulates and solves a constrained optimization\nproblem in the non-parameterized proximal policy space. Using supervised\nregression, it then converts the optimal non-parameterized policy to a\nparameterized policy, from which it draws new samples. The methodology is\ngeneral in that it applies to both discrete and continuous action spaces, and\ncan handle a wide variety of proximity constraints for the non-parameterized\noptimization problem. We show how the Natural Policy Gradient and Trust Region\nPolicy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization\n(PPO) problem can be addressed by this methodology. The SPU implementation is\nmuch simpler than TRPO. In terms of sample efficiency, our extensive\nexperiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and\noutperforms PPO in Atari video game tasks. \n\n"}
{"id": "1805.12253", "contents": "Title: Sequential Experimental Design for Optimal Structural Intervention in\n  Gene Regulatory Networks Based on the Mean Objective Cost of Uncertainty Abstract: Scientists are attempting to use models of ever increasing complexity,\nespecially in medicine, where gene-based diseases such as cancer require better\nmodeling of cell regulation. Complex models suffer from uncertainty and\nexperiments are needed to reduce this uncertainty. Because experiments can be\ncostly and time-consuming it is desirable to determine experiments providing\nthe most useful information. If a sequence of experiments is to be performed,\nexperimental design is needed to determine the order. A classical approach is\nto maximally reduce the overall uncertainty in the model, meaning maximal\nentropy reduction. A recently proposed method takes into account both model\nuncertainty and the translational objective, for instance, optimal structural\nintervention in gene regulatory networks, where the aim is to alter the\nregulatory logic to maximally reduce the long-run likelihood of being in a\ncancerous state. The mean objective cost of uncertainty (MOCU) quantifies\nuncertainty based on the degree to which model uncertainty affects the\nobjective. Experimental design involves choosing the experiment that yields the\ngreatest reduction in MOCU. This paper introduces finite-horizon dynamic\nprogramming for MOCU-based sequential experimental design and compares it to\nthe greedy approach, which selects one experiment at a time without\nconsideration of the full horizon of experiments. A salient aspect of the paper\nis that it demonstrates the advantage of MOCU-based design over the widely used\nentropy-based design for both greedy and dynamic-programming strategies and\ninvestigates the effect of model conditions on the comparative performances. \n\n"}
{"id": "1806.00458", "contents": "Title: Improved Sample Complexity for Stochastic Compositional Variance Reduced\n  Gradient Abstract: Convex composition optimization is an emerging topic that covers a wide range\nof applications arising from stochastic optimal control, reinforcement learning\nand multi-stage stochastic programming. Existing algorithms suffer from\nunsatisfactory sample complexity and practical issues since they ignore the\nconvexity structure in the algorithmic design. In this paper, we develop a new\nstochastic compositional variance-reduced gradient algorithm with the sample\ncomplexity of $O((m+n)\\log(1/\\epsilon)+1/\\epsilon^3)$ where $m+n$ is the total\nnumber of samples. Our algorithm is near-optimal as the dependence on $m+n$ is\noptimal up to a logarithmic factor. Experimental results on real-world datasets\ndemonstrate the effectiveness and efficiency of the new algorithm. \n\n"}
{"id": "1806.00627", "contents": "Title: Fast Rigid 3D Registration Solution: A Simple Method Free of SVD and\n  Eigen-Decomposition Abstract: A novel solution is obtained to solve the rigid 3D registration problem,\nmotivated by previous eigen-decomposition approaches. Different from existing\nsolvers, the proposed algorithm does not require sophisticated matrix\noperations e.g. singular value decomposition or eigenvalue decomposition.\nInstead, the optimal eigenvector of the point cross-covariance matrix can be\ncomputed within several iterations. It is also proven that the optimal rotation\nmatrix can be directly computed for cases without need of quaternion. The\nsimple framework provides very easy approach of integer-implementation on\nembedded platforms. Simulations on noise-corrupted point clouds have verified\nthe robustness and computation speed of the proposed method. The final results\nindicate that the proposed algorithm is accurate, robust and owns over $60\\%\n\\sim 80\\%$ less computation time than representatives. It has also been applied\nto real-world applications for faster relative robotic navigation. \n\n"}
{"id": "1806.03149", "contents": "Title: Several recent developments in estimation and robust control of quantum\n  systems Abstract: This paper summarizes several recent developments in the area of estimation\nand robust control of quantum systems and outlines several directions for\nfuture research. Quantum state tomography via linear regression estimation and\nadaptive quantum state estimation are introduced and a Hamiltonian\nidentification algorithm is outlined. Two quantum robust control approaches\nincluding sliding mode control and sampling-based learning control are\nillustrated. \n\n"}
{"id": "1806.03475", "contents": "Title: Input Matrix Construction and Approximation Using a Graphic Approach Abstract: Given a state transition matrix (STM), we reinvestigate the problem of\nconstructing the sparest input matrix with a fixed number of inputs to\nguarantee controllability. We give a new and simple graph theoretic\ncharacterization for the sparsity pattern of input matrices to guarantee\ncontrollability for a general STM admitting multiple eigenvalues, and provide a\ndeterministic procedure with polynomial time complexity to construct real\nvalued input matrices with arbi- trarily prescribed sparsity pattern satisfying\ncontrollability. Based on this criterion, some novel results on sparsely\ncontrolling a system are obtained. It is proven that the minimal number of\ninputs to guarantee controllability equals to the maximum geometric\nmultiplicity of the STM under the constraint that some states are\nactuated-forbidden, extending the results of [28]. The minimal sparsity of\ninput matrices with a fixed number of inputs is not necessarily equal to the\nminimal number of actuated states to ensure controllability. Furthermore, a\ngraphic sub- modular function is built, leading to a greedy algorithm to\nefficiently approximate the minimal actuated states to assure controllability\nfor general STMs. For the problem of approximating the sparsest input matrices\nwith a fixed number of inputs, we propose a simple greedy algo- rithm\n(non-submodular) and a two-stage algorithm, and demonstrate that the latter\nalgorithm, inspired from techniques in dynamic coloring, has a provable\napproximation guarantee. Finally, we present numerical results to show the\nefficiency and effectiveness of our approaches. \n\n"}
{"id": "1806.04549", "contents": "Title: Early Seizure Detection with an Energy-Efficient Convolutional Neural\n  Network on an Implantable Microcontroller Abstract: Implantable, closed-loop devices for automated early detection and\nstimulation of epileptic seizures are promising treatment options for patients\nwith severe epilepsy that cannot be treated with traditional means. Most\napproaches for early seizure detection in the literature are, however, not\noptimized for implementation on ultra-low power microcontrollers required for\nlong-term implantation. In this paper we present a convolutional neural network\nfor the early detection of seizures from intracranial EEG signals, designed\nspecifically for this purpose. In addition, we investigate approximations to\ncomply with hardware limits while preserving accuracy. We compare our approach\nto three previously proposed convolutional neural networks and a feature-based\nSVM classifier with respect to detection accuracy, latency and computational\nneeds. Evaluation is based on a comprehensive database with long-term EEG\nrecordings. The proposed method outperforms the other detectors with a median\nsensitivity of 0.96, false detection rate of 10.1 per hour and median detection\ndelay of 3.7 seconds, while being the only approach suited to be realized on a\nlow power microcontroller due to its parsimonious use of computational and\nmemory resources. \n\n"}
{"id": "1806.04808", "contents": "Title: Learning Representations of Ultrahigh-dimensional Data for Random\n  Distance-based Outlier Detection Abstract: Learning expressive low-dimensional representations of ultrahigh-dimensional\ndata, e.g., data with thousands/millions of features, has been a major way to\nenable learning methods to address the curse of dimensionality. However,\nexisting unsupervised representation learning methods mainly focus on\npreserving the data regularity information and learning the representations\nindependently of subsequent outlier detection methods, which can result in\nsuboptimal and unstable performance of detecting irregularities (i.e.,\noutliers).\n  This paper introduces a ranking model-based framework, called RAMODO, to\naddress this issue. RAMODO unifies representation learning and outlier\ndetection to learn low-dimensional representations that are tailored for a\nstate-of-the-art outlier detection approach - the random distance-based\napproach. This customized learning yields more optimal and stable\nrepresentations for the targeted outlier detectors. Additionally, RAMODO can\nleverage little labeled data as prior knowledge to learn more expressive and\napplication-relevant representations. We instantiate RAMODO to an efficient\nmethod called REPEN to demonstrate the performance of RAMODO.\n  Extensive empirical results on eight real-world ultrahigh dimensional data\nsets show that REPEN (i) enables a random distance-based detector to obtain\nsignificantly better AUC performance and two orders of magnitude speedup; (ii)\nperforms substantially better and more stably than four state-of-the-art\nrepresentation learning methods; and (iii) leverages less than 1% labeled data\nto achieve up to 32% AUC improvement. \n\n"}
{"id": "1806.05451", "contents": "Title: The committee machine: Computational to statistical gaps in learning a\n  two-layers neural network Abstract: Heuristic tools from statistical physics have been used in the past to locate\nthe phase transitions and compute the optimal learning and generalization\nerrors in the teacher-student scenario in multi-layer neural networks. In this\ncontribution, we provide a rigorous justification of these approaches for a\ntwo-layers neural network model called the committee machine. We also introduce\na version of the approximate message passing (AMP) algorithm for the committee\nmachine that allows to perform optimal learning in polynomial time for a large\nset of parameters. We find that there are regimes in which a low generalization\nerror is information-theoretically achievable while the AMP algorithm fails to\ndeliver it, strongly suggesting that no efficient algorithm exists for those\ncases, and unveiling a large computational gap. \n\n"}
{"id": "1806.06003", "contents": "Title: On Machine Learning and Structure for Mobile Robots Abstract: Due to recent advances - compute, data, models - the role of learning in\nautonomous systems has expanded significantly, rendering new applications\npossible for the first time. While some of the most significant benefits are\nobtained in the perception modules of the software stack, other aspects\ncontinue to rely on known manual procedures based on prior knowledge on\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\nthese modules when data collection and curation become easier than manual rule\ndesign. Building on this coarse and broad survey of current research, the final\nsections aim to provide insights into future potentials and challenges as well\nas the necessity of structure in current practical applications. \n\n"}
{"id": "1806.06545", "contents": "Title: A Simple Reservoir Model of Working Memory with Real Values Abstract: The prefrontal cortex is known to be involved in many high-level cognitive\nfunctions, in particular, working memory. Here, we study to what extent a group\nof randomly connected units (namely an Echo State Network, ESN) can store and\nmaintain (as output) an arbitrary real value from a streamed input, i.e. can\nact as a sustained working memory unit. Furthermore, we explore to what extent\nsuch an architecture can take advantage of the stored value in order to produce\nnon-linear computations. Comparison between different architectures (with and\nwithout feedback, with and without a working memory unit) shows that an\nexplicit memory improves the performances. \n\n"}
{"id": "1806.07307", "contents": "Title: Estimation from Non-Linear Observations via Convex Programming with\n  Application to Bilinear Regression Abstract: We propose a computationally efficient estimator, formulated as a convex\nprogram, for a broad class of non-linear regression problems that involve\ndifference of convex (DC) non-linearities. The proposed method can be viewed as\na significant extension of the \"anchored regression\" method formulated and\nanalyzed in [10] for regression with convex non-linearities. Our main\nassumption, in addition to other mild statistical and computational\nassumptions, is availability of a certain approximation oracle for the average\nof the gradients of the observation functions at a ground truth. Under this\nassumption and using a PAC-Bayesian analysis we show that the proposed\nestimator produces an accurate estimate with high probability. As a concrete\nexample, we study the proposed framework in the bilinear regression problem\nwith Gaussian factors and quantify a sufficient sample complexity for exact\nrecovery. Furthermore, we describe a computationally tractable scheme that\nprovably produces the required approximation oracle in the considered bilinear\nregression problem. \n\n"}
{"id": "1806.09035", "contents": "Title: Defending Malware Classification Networks Against Adversarial\n  Perturbations with Non-Negative Weight Restrictions Abstract: There is a growing body of literature showing that deep neural networks are\nvulnerable to adversarial input modification. Recently this work has been\nextended from image classification to malware classification over boolean\nfeatures. In this paper we present several new methods for training restricted\nnetworks in this specific domain that are highly effective at preventing\nadversarial perturbations. We start with a fully adversarially resistant neural\nnetwork that has hard non-negative weight restrictions and is equivalent to\nlearning a monotonic boolean function and then attempt to relax the constraints\nto improve classifier accuracy. \n\n"}
{"id": "1806.09211", "contents": "Title: Equalizing Financial Impact in Supervised Learning Abstract: Notions of \"fair classification\" that have arisen in computer science\ngenerally revolve around equalizing certain statistics across protected groups.\nThis approach has been criticized as ignoring societal issues, including how\nerrors can hurt certain groups disproportionately. We pose a modification of\none of the fairness criteria from Hardt, Price, and Srebro [NIPS, 2016] that\nmakes a small step towards addressing this issue in the case of financial\ndecisions like giving loans. We call this new notion \"equalized financial\nimpact.\" \n\n"}
{"id": "1806.09710", "contents": "Title: Why Interpretability in Machine Learning? An Answer Using Distributed\n  Detection and Data Fusion Theory Abstract: As artificial intelligence is increasingly affecting all parts of society and\nlife, there is growing recognition that human interpretability of machine\nlearning models is important. It is often argued that accuracy or other similar\ngeneralization performance metrics must be sacrificed in order to gain\ninterpretability. Such arguments, however, fail to acknowledge that the overall\ndecision-making system is composed of two entities: the learned model and a\nhuman who fuses together model outputs with his or her own information. As\nsuch, the relevant performance criteria should be for the entire system, not\njust for the machine learning component. In this work, we characterize the\nperformance of such two-node tandem data fusion systems using the theory of\ndistributed detection. In doing so, we work in the population setting and model\ninterpretable learned models as multi-level quantizers. We prove that under our\nabstraction, the overall system of a human with an interpretable classifier\noutperforms one with a black box classifier. \n\n"}
{"id": "1806.09846", "contents": "Title: System Design in the Era of IoT --- Meeting the Autonomy Challenge Abstract: The advent of IoT is a great opportunity to reinvigorate Computing by\nfocusing on autonomous system design. This certainly raises technology\nquestions but, more importantly, it requires building new foundation that will\nsystematically integrate the innovative results needed to face increasing\nenvironment and mission complexity.\n  A key idea is to compensate the lack of human intervention by adaptive\ncontrol. This is instrumental for system resilience: it allows both coping with\nuncertainty and managing mixed criticality services. Our proposal for\nknowledge-based design seeks a compromise: preserving rigorousness despite the\nfact that essential properties cannot be guaranteed at design time. It makes\nknowledge generation and application a primary concern and aims to fully and\nseamlessly incorporate the adaptive control paradigm in system architecture. \n\n"}
{"id": "1806.09849", "contents": "Title: SENSE: Abstraction-Based Synthesis of Networked Control Systems Abstract: While many studies and tools target the basic stabilizability problem of\nnetworked control systems (NCS), nowadays modern systems require more\nsophisticated objectives such as those expressed as formulae in linear temporal\nlogic or as automata on infinite strings. One general technique to achieve this\nis based on so-called symbolic models, where complex systems are approximated\nby finite abstractions, and then, correct-by-construction controllers are\nautomatically synthesized for them. We present tool SENSE for the construction\nof finite abstractions for NCS and the automated synthesis of controllers.\nConstructed controllers enforce complex specifications over plants in NCS by\ntaking into account several non-idealities of the communication channels.\n  Given a symbolic model of the plant and network parameters, SENSE can\nefficiently construct a symbolic model of the NCS, by employing operations on\nbinary decision diagrams (BDDs). Then, it synthesizes symbolic controllers\nsatisfying a class of specifications. It has interfaces for the simulation and\nthe visualization of the resulting closed-loop systems using OMNETPP and\nMATLAB. Additionally, SENSE can generate ready-to-implement VHDL/Verilog or\nC/C++ codes from the synthesized controllers. \n\n"}
{"id": "1806.10797", "contents": "Title: $\\mathcal{H}_2(t_f)$ Optimality Conditions for a Finite-time Horizon Abstract: In this paper we establish the interpolatory model reduction framework for\noptimal approximation of MIMO dynamical systems with respect to the\n$\\mathcal{H}_2$ norm over a finite-time horizon, denoted as the\n$\\mathcal{H}_2(t_f)$ norm. Using the underlying inner product space, we derive\nthe interpolatory first-order necessary optimality conditions for approximation\nin the $\\mathcal{H}_2(t_f)$ norm. Then, we develop an algorithm, which yields a\nlocally optimal reduced model that satisfies the established\ninterpolation-based optimality conditions. We test the algorithm on various\nnumerical examples to illustrate its performance. \n\n"}
{"id": "1806.11096", "contents": "Title: Recovering Trees with Convex Clustering Abstract: Convex clustering refers, for given $\\left\\{x_1, \\dots, x_n\\right\\} \\subset\n\\mathbb{R}^p$, to the minimization of \\begin{eqnarray*} u(\\gamma) & = &\n\\underset{u_1, \\dots, u_n }{\\arg\\min}\\;\\sum_{i=1}^{n}{\\lVert x_i - u_i\n\\rVert^2} + \\gamma \\sum_{i,j=1}^{n}{w_{ij} \\lVert u_i - u_j\\rVert},\\\\\n\\end{eqnarray*} where $w_{ij} \\geq 0$ is an affinity that quantifies the\nsimilarity between $x_i$ and $x_j$. We prove that if the affinities $w_{ij}$\nreflect a tree structure in the $\\left\\{x_1, \\dots, x_n\\right\\}$, then the\nconvex clustering solution path reconstructs the tree exactly. The main\ntechnical ingredient implies the following combinatorial byproduct: for every\nset $\\left\\{x_1, \\dots, x_n \\right\\} \\subset \\mathbb{R}^p$ of $n \\geq 2$\ndistinct points, there exist at least $n/6$ points with the property that for\nany of these points $x$ there is a unit vector $v \\in \\mathbb{R}^p$ such that,\nwhen viewed from $x$, `most' points lie in the direction $v$ \\begin{eqnarray*}\n\\frac{1}{n-1}\\sum_{i=1 \\atop x_i \\neq x}^{n}{ \\left\\langle \\frac{x_i -\nx}{\\lVert x_i - x \\rVert}, v \\right\\rangle} & \\geq & \\frac{1}{4}.\n\\end{eqnarray*} \n\n"}
{"id": "1806.11532", "contents": "Title: TextWorld: A Learning Environment for Text-based Games Abstract: We introduce TextWorld, a sandbox learning environment for the training and\nevaluation of RL agents on text-based games. TextWorld is a Python library that\nhandles interactive play-through of text games, as well as backend functions\nlike state tracking and reward assignment. It comes with a curated list of\ngames whose features and challenges we have analyzed. More significantly, it\nenables users to handcraft or automatically generate new games. Its generative\nmechanisms give precise control over the difficulty, scope, and language of\nconstructed games, and can be used to relax challenges inherent to commercial\ntext games like partial observability and sparse rewards. By generating sets of\nvaried but similar games, TextWorld can also be used to study generalization\nand transfer learning. We cast text-based games in the Reinforcement Learning\nformalism, use our framework to develop a set of benchmark games, and evaluate\nseveral baseline agents on this set and the curated list. \n\n"}
{"id": "1807.00649", "contents": "Title: Distributed Ledger Technology, Cyber-Physical Systems, and Social\n  Compliance Abstract: This paper describes how Distributed Ledger Technologies can be used to\ndesign a class of cyber-physical systems, as well as to enforce social\ncontracts and to orchestrate the behaviour of agents trying to access a shared\nresource. The first part of the paper analyses the advantages and disadvantages\nof using Distributed Ledger Technologies architectures to implement certain\ncontrol systems in an Internet of Things (IoT) setting, and then focuses on a\nspecific type of DLT based on a Directed Acyclic Graph. In this setting we\npropose a set of delay differential equations to describe the dynamical\nbehaviour of the Tangle, an IoT-inspired Directed Acyclic Graph designed for\nthe cryptocurrency IOTA. The second part proposes an application of Distributed\nLedger Technologies as a mechanism for dynamic deposit pricing, wherein the\ndeposit of digital currency is used to orchestrate access to a network of\nshared resources. The pricing signal is used as a mechanism to enforce the\ndesired level of compliance according to a predetermined set of rules. After\npresenting an illustrative example, we analyze the control system and provide\nsufficient conditions for the stability of the network. \n\n"}
{"id": "1807.00698", "contents": "Title: A simple proof of the discrete time geometric Pontryagin maximum\n  principle on smooth manifolds Abstract: We establish a geometric Pontryagin maximum principle for discrete time\noptimal control problems on finite dimensional smooth manifolds under the\nfollowing three types of constraints: a) constraints on the states pointwise in\ntime, b) constraints on the control actions pointwise in time, c) constraints\non the frequency spectrum of the optimal control trajectories. Our proof\nfollows, in spirit, the path to establish geometric versions of the Pontryagin\nmaximum principle on smooth manifolds indicated in [Cha11] in the context of\ncontinuous-time optimal control. \n\n"}
{"id": "1807.00787", "contents": "Title: A Unified Approach to Quantifying Algorithmic Unfairness: Measuring\n  Individual & Group Unfairness via Inequality Indices Abstract: Discrimination via algorithmic decision making has received considerable\nattention. Prior work largely focuses on defining conditions for fairness, but\ndoes not define satisfactory measures of algorithmic unfairness. In this paper,\nwe focus on the following question: Given two unfair algorithms, how should we\ndetermine which of the two is more unfair? Our core idea is to use existing\ninequality indices from economics to measure how unequally the outcomes of an\nalgorithm benefit different individuals or groups in a population. Our work\noffers a justified and general framework to compare and contrast the\n(un)fairness of algorithmic predictors. This unifying approach enables us to\nquantify unfairness both at the individual and the group level. Further, our\nwork reveals overlooked tradeoffs between different fairness notions: using our\nproposed measures, the overall individual-level unfairness of an algorithm can\nbe decomposed into a between-group and a within-group component. Earlier\nmethods are typically designed to tackle only between-group unfairness, which\nmay be justified for legal or other reasons. However, we demonstrate that\nminimizing exclusively the between-group component may, in fact, increase the\nwithin-group, and hence the overall unfairness. We characterize and illustrate\nthe tradeoffs between our measures of (un)fairness and the prediction accuracy. \n\n"}
{"id": "1807.00801", "contents": "Title: Deepcode: Feedback Codes via Deep Learning Abstract: The design of codes for communicating reliably over a statistically well\ndefined channel is an important endeavor involving deep mathematical research\nand wide-ranging practical applications. In this work, we present the first\nfamily of codes obtained via deep learning, which significantly beats\nstate-of-the-art codes designed over several decades of research. The\ncommunication channel under consideration is the Gaussian noise channel with\nfeedback, whose study was initiated by Shannon; feedback is known theoretically\nto improve reliability of communication, but no practical codes that do so have\never been successfully constructed.\n  We break this logjam by integrating information theoretic insights\nharmoniously with recurrent-neural-network based encoders and decoders to\ncreate novel codes that outperform known codes by 3 orders of magnitude in\nreliability. We also demonstrate several desirable properties of the codes: (a)\ngeneralization to larger block lengths, (b) composability with known codes, (c)\nadaptation to practical constraints. This result also has broader ramifications\nfor coding theory: even when the channel has a clear mathematical model, deep\nlearning methodologies, when combined with channel-specific\ninformation-theoretic insights, can potentially beat state-of-the-art codes\nconstructed over decades of mathematical research. \n\n"}
{"id": "1807.00937", "contents": "Title: Constrained dynamical optimal transport and its Lagrangian formulation Abstract: We propose dynamical optimal transport (OT) problems constrained in a\nparameterized probability subset. In application problems such as deep\nlearning, the probability distribution is often generated by a parameterized\nmapping function. In this case, we derive a simple formulation for the\nconstrained dynamical OT. \n\n"}
{"id": "1807.01404", "contents": "Title: A Fixed-Point Iteration for Steady-State Analysis of Water Distribution\n  Networks Abstract: This paper develops a fixed-point iteration to solve the steady-state water\nflow equations in an urban water distribution network. The fixed-point\niteration is derived upon the assumption of turbulent flow solutions and the\nvalidity of the Hazen-Williams head loss formula for water flow. Local\nconvergence is ensured if the spectral radius of the Jacobian at the solution\nis smaller than one. The implication is that the solution is at least locally\nunique and that the spectral radius of the Jacobian provides an estimate of the\nconvergence speed. A sample water network is provided to assert the application\nof the proposed method. \n\n"}
{"id": "1807.02581", "contents": "Title: The Goldilocks zone: Towards better understanding of neural network loss\n  landscapes Abstract: We explore the loss landscape of fully-connected and convolutional neural\nnetworks using random, low-dimensional hyperplanes and hyperspheres. Evaluating\nthe Hessian, $H$, of the loss function on these hypersurfaces, we observe 1) an\nunusual excess of the number of positive eigenvalues of $H$, and 2) a large\nvalue of $\\mathrm{Tr}(H) / ||H||$ at a well defined range of configuration\nspace radii, corresponding to a thick, hollow, spherical shell we refer to as\nthe \\textit{Goldilocks zone}. We observe this effect for fully-connected neural\nnetworks over a range of network widths and depths on MNIST and CIFAR-10\ndatasets with the $\\mathrm{ReLU}$ and $\\tanh$ non-linearities, and a similar\neffect for convolutional networks. Using our observations, we demonstrate a\nclose connection between the Goldilocks zone, measures of local\nconvexity/prevalence of positive curvature, and the suitability of a network\ninitialization. We show that the high and stable accuracy reached when\noptimizing on random, low-dimensional hypersurfaces is directly related to the\noverlap between the hypersurface and the Goldilocks zone, and as a corollary\ndemonstrate that the notion of intrinsic dimension is initialization-dependent.\nWe note that common initialization techniques initialize neural networks in\nthis particular region of unusually high convexity/prevalence of positive\ncurvature, and offer a geometric intuition for their success. Furthermore, we\ndemonstrate that initializing a neural network at a number of points and\nselecting for high measures of local convexity such as $\\mathrm{Tr}(H) /\n||H||$, number of positive eigenvalues of $H$, or low initial loss, leads to\nstatistically significantly faster training on MNIST. Based on our\nobservations, we hypothesize that the Goldilocks zone contains an unusually\nhigh density of suitable initialization configurations. \n\n"}
{"id": "1807.02999", "contents": "Title: Decreasing the size of the Restricted Boltzmann machine Abstract: We propose a method to decrease the number of hidden units of the restricted\nBoltzmann machine while avoiding decrease of the performance measured by the\nKullback-Leibler divergence. Then, we demonstrate our algorithm by using\nnumerical simulations. \n\n"}
{"id": "1807.03571", "contents": "Title: A Game-Based Approximate Verification of Deep Neural Networks with\n  Provable Guarantees Abstract: Despite the improved accuracy of deep neural networks, the discovery of\nadversarial examples has raised serious safety concerns. In this paper, we\nstudy two variants of pointwise robustness, the maximum safe radius problem,\nwhich for a given input sample computes the minimum distance to an adversarial\nexample, and the feature robustness problem, which aims to quantify the\nrobustness of individual features to adversarial perturbations. We demonstrate\nthat, under the assumption of Lipschitz continuity, both problems can be\napproximated using finite optimisation by discretising the input space, and the\napproximation has provable guarantees, i.e., the error is bounded. We then show\nthat the resulting optimisation problems can be reduced to the solution of\ntwo-player turn-based games, where the first player selects features and the\nsecond perturbs the image within the feature. While the second player aims to\nminimise the distance to an adversarial example, depending on the optimisation\nobjective the first player can be cooperative or competitive. We employ an\nanytime approach to solve the games, in the sense of approximating the value of\na game by monotonically improving its upper and lower bounds. The Monte Carlo\ntree search algorithm is applied to compute upper bounds for both games, and\nthe Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used\nto compute lower bounds for the maximum safety radius and feature robustness\ngames. When working on the upper bound of the maximum safe radius problem, our\ntool demonstrates competitive performance against existing adversarial example\ncrafting algorithms. Furthermore, we show how our framework can be deployed to\nevaluate pointwise robustness of neural networks in safety-critical\napplications such as traffic sign recognition in self-driving cars. \n\n"}
{"id": "1807.03625", "contents": "Title: Foreign English Accent Adjustment by Learning Phonetic Patterns Abstract: State-of-the-art automatic speech recognition (ASR) systems struggle with the\nlack of data for rare accents. For sufficiently large datasets, neural engines\ntend to outshine statistical models in most natural language processing\nproblems. However, a speech accent remains a challenge for both approaches.\nPhonologists manually create general rules describing a speaker's accent, but\ntheir results remain underutilized. In this paper, we propose a model that\nautomatically retrieves phonological generalizations from a small dataset. This\nmethod leverages the difference in pronunciation between a particular dialect\nand General American English (GAE) and creates new accented samples of words.\nThe proposed model is able to learn all generalizations that previously were\nmanually obtained by phonologists. We use this statistical method to generate a\nmillion phonological variations of words from the CMU Pronouncing Dictionary\nand train a sequence-to-sequence RNN to recognize accented words with 59%\naccuracy. \n\n"}
{"id": "1807.04199", "contents": "Title: Optimal control problems with oscillations, concentrations and\n  discontinuities Abstract: Optimal control problems with oscillations (chattering controls) and\nconcentrations (impulsive controls) can have integral performance criteria such\nthat concentration of the control signal occurs at a discontinuity of the state\nsignal. Techniques from functional analysis (anisotropic parametrized measures)\nare applied to give a precise meaning of the integral cost and to allow for the\nsound application of numerical methods. We show how this can be combined with\nthe Lasserre hierarchy of semidefinite programming relaxations. \n\n"}
{"id": "1807.04237", "contents": "Title: Koopman Performance Analysis of Nonlinear Consensus Networks Abstract: Spectral decomposition of dynamical systems is a popular methodology to\ninvestigate the fundamental qualitative and quantitative properties of these\nsystems and their solutions. In this chapter, we consider a class of nonlinear\ncooperative protocols, which consist of multiple agents that are coupled\ntogether via an undirected state-dependent graph. We develop a representation\nof the system solution by decomposing the nonlinear system utilizing ideas from\nthe Koopman operator theory and its spectral analysis. We use recent results on\nthe extensions of the well-known Hartman theorem for hyperbolic systems to\nestablish a connection between the original nonlinear dynamics and the\nlinearized dynamics in terms of Koopman spectral properties. The expected value\nof the output energy of the nonlinear protocol, which is related to the notions\nof coherence and robustness in dynamical networks, is evaluated and\ncharacterized in terms of Koopman eigenvalues, eigenfunctions, and modes.\nSpectral representation of the performance measure enables us to develop\nalgorithmic methods to assess the performance of this class of nonlinear\ndynamical networks as a function of their graph topology. Finally, we propose a\nscalable computational method for approximation of the components of the\nKoopman mode decomposition, which is necessary to evaluate the systemic\nperformance measure of the nonlinear dynamic network. \n\n"}
{"id": "1807.04261", "contents": "Title: Phase Retrieval Under a Generative Prior Abstract: The phase retrieval problem asks to recover a natural signal $y_0 \\in\n\\mathbb{R}^n$ from $m$ quadratic observations, where $m$ is to be minimized. As\nis common in many imaging problems, natural signals are considered sparse with\nrespect to a known basis, and the generic sparsity prior is enforced via\n$\\ell_1$ regularization. While successful in the realm of linear inverse\nproblems, such $\\ell_1$ methods have encountered possibly fundamental\nlimitations, as no computationally efficient algorithm for phase retrieval of a\n$k$-sparse signal has been proven to succeed with fewer than $O(k^2\\log n)$\ngeneric measurements, exceeding the theoretical optimum of $O(k \\log n)$. In\nthis paper, we propose a novel framework for phase retrieval by 1) modeling\nnatural signals as being in the range of a deep generative neural network $G :\n\\mathbb{R}^k \\rightarrow \\mathbb{R}^n$ and 2) enforcing this prior directly by\noptimizing an empirical risk objective over the domain of the generator. Our\nformulation has provably favorable global geometry for gradient methods, as\nsoon as $m = O(kd^2\\log n)$, where $d$ is the depth of the network.\nSpecifically, when suitable deterministic conditions on the generator and\nmeasurement matrix are met, we construct a descent direction for any point\noutside of a small neighborhood around the unique global minimizer and its\nnegative multiple, and show that such conditions hold with high probability\nunder Gaussian ensembles of multilayer fully-connected generator networks and\nmeasurement matrices. This formulation for structured phase retrieval thus has\ntwo advantages over sparsity based methods: 1) deep generative priors can more\ntightly represent natural signals and 2) information theoretically optimal\nsample complexity. We corroborate these results with experiments showing that\nexploiting generative models in phase retrieval tasks outperforms sparse phase\nretrieval methods. \n\n"}
{"id": "1807.04740", "contents": "Title: Negative Momentum for Improved Game Dynamics Abstract: Games generalize the single-objective optimization paradigm by introducing\ndifferent objective functions for different players. Differentiable games often\nproceed by simultaneous or alternating gradient updates. In machine learning,\ngames are gaining new importance through formulations like generative\nadversarial networks (GANs) and actor-critic systems. However, compared to\nsingle-objective optimization, game dynamics are more complex and less\nunderstood. In this paper, we analyze gradient-based methods with momentum on\nsimple games. We prove that alternating updates are more stable than\nsimultaneous updates. Next, we show both theoretically and empirically that\nalternating gradient updates with a negative momentum term achieves convergence\nin a difficult toy adversarial problem, but also on the notoriously difficult\nto train saturating GANs. \n\n"}
{"id": "1807.05063", "contents": "Title: Quantum Pontryagin Principle under Continuous Measurements and Feedback Abstract: In this note we develop the theory of the quantum Pontryagin principle for\ncontinuous measurements and feedback. The analysis is carried out under the\nassumption of compatible events in the output channel. The plant is a quantum\nsystem, which generally is in a mixed state, coupled to a continuous\nmeasurement channel. The Pontryagin Maximum Principle is derived in both the\nSchr\\\"{o}dinger picture and Heisenberg picture, in particular in statistical\nmoment coordinates. To avoid solving stochastic equations we derive a LQG\nscheme which is more suitable for control purposes. \n\n"}
{"id": "1807.05200", "contents": "Title: Soap films with gravity and almost-minimal surfaces Abstract: Motivated by the study of the equilibrium equations for a soap film hanging\nfrom a wire frame, we prove a compactness theorem for surfaces with\nasymptotically vanishing mean curvature and fixed or converging boundaries. In\nparticular, we obtain sufficient geometric conditions for the minimal surfaces\nspanned by a given boundary to represent all the possible limits of sequences\nof almost-minimal surfaces. Finally, we provide some sharp quantitative\nestimates on the distance of an almost-minimal surface from its limit minimal\nsurface. \n\n"}
{"id": "1807.06172", "contents": "Title: Experimental Resilience Assessment of An Open-Source Driving Agent Abstract: Autonomous vehicles (AV) depend on the sensors like RADAR and camera for the\nperception of the environment, path planning, and control. With the increasing\nautonomy and interactions with the complex environment, there have been growing\nconcerns regarding the safety and reliability of AVs. This paper presents a\nSystems-Theoretic Process Analysis (STPA) based fault injection framework to\nassess the resilience of an open-source driving agent, called openpilot, under\ndifferent environmental conditions and faults affecting sensor data. To\nincrease the coverage of unsafe scenarios during testing, we use a strategic\nsoftware fault-injection approach where the triggers for injecting the faults\nare derived from the unsafe scenarios identified during the high-level hazard\nanalysis of the system. The experimental results show that the proposed\nstrategic fault injection approach increases the hazard coverage compared to\nrandom fault injection and, thus, can help with more effective simulation of\nsafety-critical faults and testing of AVs. In addition, the paper provides\ninsights on the performance of openpilot safety mechanisms and its ability in\ntimely detection and recovery from faulty inputs. \n\n"}
{"id": "1807.07227", "contents": "Title: Convex Relaxations in Power System Optimization: A Brief Introduction Abstract: Convex relaxations of the AC power flow equations have attracted significant\ninterest in the power systems research community in recent years. The following\ncollection of video lectures provides a brief introduction to the mathematics\nof AC power systems, continuous nonlinear optimization, and relaxations of the\npower flow equations. The aim of the videos is to provide the high level ideas\nof convex relaxations and their applications in power system optimization, and\ncould be used as a starting point for researchers who want to study, use or\ndevelop new convex relaxations for use in their own research. The videos do not\naim to provide an in-depth tutorial about specific convex relaxations, but\nrather focus on ideas that are common to all convex relaxations of the AC\noptimal power flow problem. \n\n"}
{"id": "1807.09089", "contents": "Title: Decision Variance in Online Learning Abstract: Online learning has traditionally focused on the expected rewards. In this\npaper, a risk-averse online learning problem under the performance measure of\nthe mean-variance of the rewards is studied. Both the bandit and full\ninformation settings are considered. The performance of several existing\npolicies is analyzed, and new fundamental limitations on risk-averse learning\nis established. In particular, it is shown that although a logarithmic\ndistribution-dependent regret in time $T$ is achievable (similar to the\nrisk-neutral problem), the worst-case (i.e. minimax) regret is lower bounded by\n$\\Omega(T)$ (in contrast to the $\\Omega(\\sqrt{T})$ lower bound in the\nrisk-neutral problem). This sharp difference from the risk-neutral counterpart\nis caused by the the variance in the player's decisions, which, while absent in\nthe regret under the expected reward criterion, contributes to excess\nmean-variance due to the non-linearity of this risk measure. The role of the\ndecision variance in regret performance reflects a risk-averse player's desire\nfor robust decisions and outcomes. \n\n"}
{"id": "1807.09161", "contents": "Title: An argument in favor of strong scaling for deep neural networks with\n  small datasets Abstract: In recent years, with the popularization of deep learning frameworks and\nlarge datasets, researchers have started parallelizing their models in order to\ntrain faster. This is crucially important, because they typically explore many\nhyperparameters in order to find the best ones for their applications. This\nprocess is time consuming and, consequently, speeding up training improves\nproductivity. One approach to parallelize deep learning models followed by many\nresearchers is based on weak scaling. The minibatches increase in size as new\nGPUs are added to the system. In addition, new learning rates schedules have\nbeen proposed to fix optimization issues that occur with large minibatch sizes.\nIn this paper, however, we show that the recommendations provided by recent\nwork do not apply to models that lack large datasets. In fact, we argument in\nfavor of using strong scaling for achieving reliable performance in such cases.\nWe evaluated our approach with up to 32 GPUs and show that weak scaling not\nonly does not have the same accuracy as the sequential model, it also fails to\nconverge most of time. Meanwhile, strong scaling has good scalability while\nhaving exactly the same accuracy of a sequential implementation. \n\n"}
{"id": "1807.09596", "contents": "Title: Contextual Stochastic Block Models Abstract: We provide the first information theoretic tight analysis for inference of\nlatent community structure given a sparse graph along with high dimensional\nnode covariates, correlated with the same latent communities. Our work bridges\nrecent theoretical breakthroughs in the detection of latent community structure\nwithout nodes covariates and a large body of empirical work using diverse\nheuristics for combining node covariates with graphs for inference. The\ntightness of our analysis implies in particular, the information theoretical\nnecessity of combining the different sources of information. Our analysis holds\nfor networks of large degrees as well as for a Gaussian version of the model. \n\n"}
{"id": "1807.09706", "contents": "Title: Remote estimation over a packet-drop channel with Markovian state Abstract: We investigate a remote estimation problem in which a transmitter observes a\nMarkov source and chooses the power level to transmit it over a time-varying\npacket-drop channel. The channel is modeled as a channel with Markovian state\nwhere the packet drop probability depends on the channel state and the transmit\npower. A receiver observes the channel output and the channel state and\nestimates the source realization. The receiver also feeds back the channel\nstate and an acknowledgment for successful reception to the transmitter. We\nconsider two models for the source---finite state Markov chains and first-order\nautoregressive processes. For the first model, using ideas from team theory, we\nestablish the structure of optimal transmission and estimation strategies and\nidentify a dynamic program to determine optimal strategies with that structure.\nFor the second model, we assume that the noise process has unimodal and\nsymmetric distribution. Using ideas from majorization theory, we show that the\noptimal transmission strategy is symmetric and monotonic and the optimal\nestimation strategy is like Kalman filter. Consequently, when there are a\nfinite number of power levels, the optimal transmission strategy may be\ndescribed using thresholds that depend on the channel state. Finally, we\npropose a simulation based approach (Renewal Monte Carlo) to compute the\noptimal thresholds and optimal performance and elucidate the algorithm with an\nexample. \n\n"}
{"id": "1807.10178", "contents": "Title: On Generation of Virtual Outputs via Signal Injection: Application to\n  Observer Design for Electromechanical Systems Abstract: Probing signal injection is a well-established technique to extract\nadditional information from a weakly (or non) observable dynamical system.\nUsing averaging theory, a framework to analyse such schemes for general\nnonlinear systems has been recently proposed in [Combes et. al., 2016], where\nit is shown that the signal injection may be used to generate a new high\nfrequency component of the systems output that can be used for state\nobservation or controller design. A key step for the success of this technique\nis the implementation of a filter to reconstruct this virtual output from the\nmeasurement of the overall systems output. The main contribution of this paper\nis to propose a new filter with guaranteed convergence properties that\noutperforms the classical designs. The method is applied to a general class of\nelectromechanical systems, and its performance is assessed via simulations and\nexperiments on the benchmark example of a 1-dof magnetic levitation system. \n\n"}
{"id": "1807.10335", "contents": "Title: A general metric for identifying adversarial images Abstract: It is well known that a determined adversary can fool a neural network by\nmaking imperceptible adversarial perturbations to an image. Recent studies have\nshown that these perturbations can be detected even without information about\nthe neural network if the strategy taken by the adversary is known beforehand.\nUnfortunately, these studies suffer from the generalization limitation -- the\ndetection method has to be recalibrated every time the adversary changes his\nstrategy. In this study, we attempt to overcome the generalization limitation\nby deriving a metric which reliably identifies adversarial images even when the\napproach taken by the adversary is unknown. Our metric leverages key\ndifferences between the spectra of clean and adversarial images when an image\nis treated as a matrix. Our metric is able to detect adversarial images across\ndifferent datasets and attack strategies without any additional re-calibration.\nIn addition, our approach provides geometric insights into several unanswered\nquestions about adversarial perturbations. \n\n"}
{"id": "1807.10363", "contents": "Title: Message-passing neural networks for high-throughput polymer screening Abstract: Machine learning methods have shown promise in predicting molecular\nproperties, and given sufficient training data machine learning approaches can\nenable rapid high-throughput virtual screening of large libraries of compounds.\nGraph-based neural network architectures have emerged in recent years as the\nmost successful approach for predictions based on molecular structure, and have\nconsistently achieved the best performance on benchmark quantum chemical\ndatasets. However, these models have typically required optimized 3D structural\ninformation for the molecule to achieve the highest accuracy. These 3D\ngeometries are costly to compute for high levels of theory, limiting the\napplicability and practicality of machine learning methods in high-throughput\nscreening applications. In this study, we present a new database of candidate\nmolecules for organic photovoltaic applications, comprising approximately\n91,000 unique chemical structures.Compared to existing datasets, this dataset\ncontains substantially larger molecules (up to 200 atoms) as well as\nextrapolated properties for long polymer chains. We show that message-passing\nneural networks trained with and without 3D structural information for these\nmolecules achieve similar accuracy, comparable to state-of-the-art methods on\nexisting benchmark datasets. These results therefore emphasize that for larger\nmolecules with practical applications, near-optimal prediction results can be\nobtained without using optimized 3D geometry as an input. We further show that\nlearned molecular representations can be leveraged to reduce the training data\nrequired to transfer predictions to a new DFT functional. \n\n"}
{"id": "1807.10536", "contents": "Title: Internal observability of the wave equation in a triangular domain Abstract: We investigate the internal observability of the wave equation with Dirichlet\nboundary conditions in a triangular domain. More precisely, the domain taken\ninto exam is the half of the equilateral triangle. Our approach is based on\nFourier analysis and on tessellation theory: by means of a suitable tiling of\nthe rectangle, we extend earlier observability results in the rectangle to the\ncase of a triangular domain. The paper includes a general result relating\nproblems in general domains to their tiles, and a discussion of the triangular\ncase. As an application, we provide an estimation of the observation time when\nthe observed domain is composed by three strips with a common side to the edges\nof the triangle. \n\n"}
{"id": "1807.10588", "contents": "Title: A Modality-Adaptive Method for Segmenting Brain Tumors and\n  Organs-at-Risk in Radiation Therapy Planning Abstract: In this paper we present a method for simultaneously segmenting brain tumors\nand an extensive set of organs-at-risk for radiation therapy planning of\nglioblastomas. The method combines a contrast-adaptive generative model for\nwhole-brain segmentation with a new spatial regularization model of tumor shape\nusing convolutional restricted Boltzmann machines. We demonstrate\nexperimentally that the method is able to adapt to image acquisitions that\ndiffer substantially from any available training data, ensuring its\napplicability across treatment sites; that its tumor segmentation accuracy is\ncomparable to that of the current state of the art; and that it captures most\norgans-at-risk sufficiently well for radiation therapy planning purposes. The\nproposed method may be a valuable step towards automating the delineation of\nbrain tumors and organs-at-risk in glioblastoma patients undergoing radiation\ntherapy. \n\n"}
{"id": "1807.11274", "contents": "Title: Stochastic Policy Gradient Ascent in Reproducing Kernel Hilbert Spaces Abstract: Reinforcement learning consists of finding policies that maximize an expected\ncumulative long-term reward in a Markov decision process with unknown\ntransition probabilities and instantaneous rewards. In this paper, we consider\nthe problem of finding such optimal policies while assuming they are continuous\nfunctions belonging to a reproducing kernel Hilbert space (RKHS). To learn the\noptimal policy we introduce a stochastic policy gradient ascent algorithm with\nthree unique novel features: (i) The stochastic estimates of policy gradients\nare unbiased. (ii) The variance of stochastic gradients is reduced by drawing\non ideas from numerical differentiation. (iii) Policy complexity is controlled\nusing sparse RKHS representations. Novel feature (i) is instrumental in proving\nconvergence to a stationary point of the expected cumulative reward. Novel\nfeature (ii) facilitates reasonable convergence times. Novel feature (iii) is a\nnecessity in practical implementations which we show can be done in a way that\ndoes not eliminate convergence guarantees. Numerical examples in standard\nproblems illustrate successful learning of policies with low complexity\nrepresentations which are close to stationary points of the expected cumulative\nreward. \n\n"}
{"id": "1807.11624", "contents": "Title: Security against false data injection attack in cyber-physical systems Abstract: In this paper, secure, remote estimation of a linear Gaussian process via\nobservations at multiple sensors is considered. Such a framework is relevant to\nmany cyber-physical systems and internet-of-things applications. Sensors make\nsequential measurements that are shared with a fusion center; the fusion center\napplies a certain filtering algorithm to make its estimates. The challenge is\nthe presence of a few unknown malicious sensors which can inject anomalous\nobservations to skew the estimates at the fusion center. The set of malicious\nsensors may be time-varying. The problems of malicious sensor detection and\nsecure estimation are considered. First, an algorithm for secure estimation is\nproposed. The proposed estimation scheme uses a novel filtering and learning\nalgorithm, where an optimal filter is learnt over time by using the sensor\nobservations in order to filter out malicious sensor observations while\nretaining other sensor measurements. Next, a novel detector to detect injection\nattacks on an unknown sensor subset is developed. Numerical results demonstrate\nup to 3 dB gain in the mean squared error and up to 75% higher attack detection\nprobability under a small false alarm rate constraint, against a competing\nalgorithm that requires additional side information. \n\n"}
{"id": "1807.11697", "contents": "Title: Multimodal Deep Domain Adaptation Abstract: Typically a classifier trained on a given dataset (source domain) does not\nperforms well if it is tested on data acquired in a different setting (target\ndomain). This is the problem that domain adaptation (DA) tries to overcome and,\nwhile it is a well explored topic in computer vision, it is largely ignored in\nrobotic vision where usually visual classification methods are trained and\ntested in the same domain. Robots should be able to deal with unknown\nenvironments, recognize objects and use them in the correct way, so it is\nimportant to explore the domain adaptation scenario also in this context. The\ngoal of the project is to define a benchmark and a protocol for multi-modal\ndomain adaptation that is valuable for the robot vision community. With this\npurpose some of the state-of-the-art DA methods are selected: Deep Adaptation\nNetwork (DAN), Domain Adversarial Training of Neural Network (DANN), Automatic\nDomain Alignment Layers (AutoDIAL) and Adversarial Discriminative Domain\nAdaptation (ADDA). Evaluations have been done using different data types: RGB\nonly, depth only and RGB-D over the following datasets, designed for the\nrobotic community: RGB-D Object Dataset (ROD), Web Object Dataset (WOD),\nAutonomous Robot Indoor Dataset (ARID), Big Berkeley Instance Recognition\nDataset (BigBIRD) and Active Vision Dataset. Although progresses have been made\non the formulation of effective adaptation algorithms and more realistic object\ndatasets are available, the results obtained show that, training a sufficiently\ngood object classifier, especially in the domain adaptation scenario, is still\nan unsolved problem. Also the best way to combine depth with RGB informations\nto improve the performance is a point that needs to be investigated more. \n\n"}
{"id": "1808.00058", "contents": "Title: A Unified Framework for Joint Mobility Prediction and Object Profiling\n  of Drones in UAV Networks Abstract: In recent years, using a network of autonomous and cooperative unmanned\naerial vehicles (UAVs) without command and communication from the ground\nstation has become more imperative, in particular in search-and-rescue\noperations, disaster management, and other applications where human\nintervention is limited. In such scenarios, UAVs can make more efficient\ndecisions if they acquire more information about the mobility, sensing and\nactuation capabilities of their neighbor nodes. In this paper, we develop an\nunsupervised online learning algorithm for joint mobility prediction and object\nprofiling of UAVs to facilitate control and communication protocols. The\nproposed method not only predicts the future locations of the surrounding\nflying objects, but also classifies them into different groups with similar\nlevels of maneuverability (e.g. rotatory, and fixed-wing UAVs) without prior\nknowledge about these classes. This method is flexible in admitting new object\ntypes with unknown mobility profiles, thereby applicable to emerging flying\nAd-hoc networks with heterogeneous nodes. \n\n"}
{"id": "1808.00177", "contents": "Title: Learning Dexterous In-Hand Manipulation Abstract: We use reinforcement learning (RL) to learn dexterous in-hand manipulation\npolicies which can perform vision-based object reorientation on a physical\nShadow Dexterous Hand. The training is performed in a simulated environment in\nwhich we randomize many of the physical properties of the system like friction\ncoefficients and an object's appearance. Our policies transfer to the physical\nrobot despite being trained entirely in simulation. Our method does not rely on\nany human demonstrations, but many behaviors found in human manipulation emerge\nnaturally, including finger gaiting, multi-finger coordination, and the\ncontrolled use of gravity. Our results were obtained using the same distributed\nRL system that was used to train OpenAI Five. We also include a video of our\nresults: https://youtu.be/jwSbzNHGflM \n\n"}
{"id": "1808.02942", "contents": "Title: Distributed heavy-ball: A generalization and acceleration of first-order\n  methods with gradient tracking Abstract: We study distributed optimization to minimize a global objective that is a\nsum of smooth and strongly-convex local cost functions. Recently, several\nalgorithms over undirected and directed graphs have been proposed that use a\ngradient tracking method to achieve linear convergence to the global minimizer.\nHowever, a connection between these different approaches has been unclear. In\nthis paper, we first show that many of the existing first-order algorithms are\nin fact related with a simple state transformation, at the heart of which lies\nthe $\\mathcal{AB}$ algorithm. We then describe \\textit{distributed heavy-ball},\ndenoted as $\\mathcal{AB}m$, i.e., $\\mathcal{AB}$ with momentum, that combines\ngradient tracking with a momentum term and uses nonidentical local step-sizes.\nBy simultaneously implementing both row- and column-stochastic weights,\n$\\mathcal{AB}m$ removes the conservatism in the related work due to\ndoubly-stochastic weights or eigenvector estimation. $\\mathcal{AB}m$ thus\nnaturally leads to optimization and average-consensus over both undirected and\ndirected graphs, casting a unifying framework over several well-known consensus\nalgorithms over arbitrary strongly-connected graphs. We show that\n$\\mathcal{AB}m$ has a global $R$-linear rate when the largest step-size is\npositive and sufficiently small. Following the standard practice in the\nheavy-ball literature, we numerically show that $\\mathcal{AB}m$ achieves\naccelerated convergence especially when the objective function is\nill-conditioned. \n\n"}
{"id": "1808.03601", "contents": "Title: Using Randomness to Improve Robustness of Machine-Learning Models\n  Against Evasion Attacks Abstract: Machine learning models have been widely used in security applications such\nas intrusion detection, spam filtering, and virus or malware detection.\nHowever, it is well-known that adversaries are always trying to adapt their\nattacks to evade detection. For example, an email spammer may guess what\nfeatures spam detection models use and modify or remove those features to avoid\ndetection. There has been some work on making machine learning models more\nrobust to such attacks. However, one simple but promising approach called {\\em\nrandomization} is underexplored. This paper proposes a novel\nrandomization-based approach to improve robustness of machine learning models\nagainst evasion attacks. The proposed approach incorporates randomization into\nboth model training time and model application time (meaning when the model is\nused to detect attacks). We also apply this approach to random forest, an\nexisting ML method which already has some degree of randomness. Experiments on\nintrusion detection and spam filtering data show that our approach further\nimproves robustness of random-forest method. We also discuss how this approach\ncan be applied to other ML models. \n\n"}
{"id": "1808.04291", "contents": "Title: Global Complexity Analysis of Inexact Successive Quadratic Approximation\n  methods for Regularized Optimization under Mild Assumptions Abstract: Successive quadratic approximations (SQA) are numerically efficient for\nminimizing the sum of a smooth function and a convex function. The iteration\ncomplexity of inexact SQA methods has been analyzed recently. In this paper, we\npresent an algorithmic framework of inexact SQA methods with four types of line\nsearches, and analyze its global complexity under milder assumptions. First, we\nshow its well-definedness and some decreasing properties. Second, under the\nquadratic growth condition and a uniform positive lower bound condition on\nstepsizes, we show that the function value sequence and the iterate sequence\nare linearly convergent. Moreover, we obtain a o(1/k) complexity without the\nquadratic growth condition, improving existing O(1/k) complexity results. At\nlast, we show that a local gradient-Lipschitz-continuity condition could\nguarantee a uniform positive lower bound for the stepsizes. \n\n"}
{"id": "1808.04545", "contents": "Title: MT-VAE: Learning Motion Transformations to Generate Multimodal Human\n  Dynamics Abstract: Long-term human motion can be represented as a series of motion\nmodes---motion sequences that capture short-term temporal dynamics---with\ntransitions between them. We leverage this structure and present a novel Motion\nTransformation Variational Auto-Encoders (MT-VAE) for learning motion sequence\ngeneration. Our model jointly learns a feature embedding for motion modes (that\nthe motion sequence can be reconstructed from) and a feature transformation\nthat represents the transition of one motion mode to the next motion mode. Our\nmodel is able to generate multiple diverse and plausible motion sequences in\nthe future from the same input. We apply our approach to both facial and full\nbody motion, and demonstrate applications like analogy-based motion transfer\nand video synthesis. \n\n"}
{"id": "1808.05156", "contents": "Title: An Analysis of Asynchronous Stochastic Accelerated Coordinate Descent Abstract: Gradient descent, and coordinate descent in particular, are core tools in\nmachine learning and elsewhere. Large problem instances are common. To help\nsolve them, two orthogonal approaches are known: acceleration and parallelism.\nIn this work, we ask whether they can be used simultaneously. The answer is\n\"yes\".\n  More specifically, we consider an asynchronous parallel version of the\naccelerated coordinate descent algorithm proposed and analyzed by Lin, Liu and\nXiao (SIOPT'15). We give an analysis based on the efficient implementation of\nthis algorithm. The only constraint is a standard bounded asynchrony\nassumption, namely that each update can overlap with at most q others. (q is at\nmost the number of processors times the ratio in the lengths of the longest and\nshortest updates.) We obtain the following three results:\n  1. A linear speedup for strongly convex functions so long as q is not too\nlarge.\n  2. A substantial, albeit sublinear, speedup for strongly convex functions for\nlarger q.\n  3. A substantial, albeit sublinear, speedup for convex functions. \n\n"}
{"id": "1808.05563", "contents": "Title: Learning Invariances using the Marginal Likelihood Abstract: Generalising well in supervised learning tasks relies on correctly\nextrapolating the training data to a large region of the input space. One way\nto achieve this is to constrain the predictions to be invariant to\ntransformations on the input that are known to be irrelevant (e.g.\ntranslation). Commonly, this is done through data augmentation, where the\ntraining set is enlarged by applying hand-crafted transformations to the\ninputs. We argue that invariances should instead be incorporated in the model\nstructure, and learned using the marginal likelihood, which correctly rewards\nthe reduced complexity of invariant models. We demonstrate this for Gaussian\nprocess models, due to the ease with which their marginal likelihood can be\nestimated. Our main contribution is a variational inference scheme for Gaussian\nprocesses containing invariances described by a sampling procedure. We learn\nthe sampling procedure by back-propagating through it to maximise the marginal\nlikelihood. \n\n"}
{"id": "1808.05965", "contents": "Title: On Geometric Analysis of Affine Sparse Subspace Clustering Abstract: Sparse subspace clustering (SSC) is a state-of-the-art method for segmenting\na set of data points drawn from a union of subspaces into their respective\nsubspaces. It is now well understood that SSC produces subspace-preserving data\naffinity under broad geometric conditions but suffers from a connectivity\nissue. In this paper, we develop a novel geometric analysis for a variant of\nSSC, named affine SSC (ASSC), for the problem of clustering data from a union\nof affine subspaces. Our contributions include a new concept called affine\nindependence for capturing the arrangement of a collection of affine subspaces.\nUnder the affine independence assumption, we show that ASSC is guaranteed to\nproduce subspace-preserving affinity. Moreover, inspired by the phenomenon that\nthe $\\ell_1$ regularization no longer induces sparsity when the solution is\nnonnegative, we further show that subspace-preserving recovery can be achieved\nunder much weaker conditions for all data points other than the extreme points\nof samples from each subspace. In addition, we confirm a curious observation\nthat the affinity produced by ASSC may be subspace-dense---which could\nguarantee the subspace-preserving affinity of ASSC to produce correct\nclustering under rather weak conditions. We validate the theoretical findings\non carefully designed synthetic data and evaluate the performance of ASSC on\nseveral real data sets. \n\n"}
{"id": "1808.06018", "contents": "Title: Optimized Path Planning for Inspection by Unmanned Aerial Vehicles Swarm\n  with Energy Constraints Abstract: Autonomous inspection of large geographical areas is a central requirement\nfor efficient hazard detection and disaster management in future cyber-physical\nsystems such as smart cities. In this regard, exploiting unmanned aerial\nvehicle (UAV) swarms is a promising solution to inspect vast areas efficiently\nand with low cost. In fact, UAVs can easily fly and reach inspection points,\nrecord surveillance data, and send this information to a wireless base station\n(BS). Nonetheless, in many cases, such as operations at remote areas, the UAVs\ncannot be guided directly by the BS in real-time to find their path. Moreover,\nanother key challenge of inspection by UAVs is the limited battery capacity.\nThus, realizing the vision of autonomous inspection via UAVs requires\nenergy-efficient path planning that takes into account the energy constraint of\neach individual UAV. In this paper, a novel path planning algorithm is proposed\nfor performing energy-efficient inspection, under stringent energy availability\nconstraints for each UAV. The developed framework takes into account all\naspects of energy consumption for a UAV swarm during the inspection operations,\nincluding energy required for flying, hovering, and data transmission. It is\nshown that the proposed algorithm can address the path planning problem\nefficiently in polynomial time. Simulation results show that the proposed\nalgorithm can yield substantial performance gains in terms of minimizing the\noverall inspection time and energy. Moreover, the results provide guidelines to\ndetermine parameters such as the number of required UAVs and amount of energy,\nwhile designing an autonomous inspection system. \n\n"}
{"id": "1808.06170", "contents": "Title: Linked Recurrent Neural Networks Abstract: Recurrent Neural Networks (RNNs) have been proven to be effective in modeling\nsequential data and they have been applied to boost a variety of tasks such as\ndocument classification, speech recognition and machine translation. Most of\nexisting RNN models have been designed for sequences assumed to be identically\nand independently distributed (i.i.d). However, in many real-world\napplications, sequences are naturally linked. For example, web documents are\nconnected by hyperlinks; and genes interact with each other. On the one hand,\nlinked sequences are inherently not i.i.d., which poses tremendous challenges\nto existing RNN models. On the other hand, linked sequences offer link\ninformation in addition to the sequential information, which enables\nunprecedented opportunities to build advanced RNN models. In this paper, we\nstudy the problem of RNN for linked sequences. In particular, we introduce a\nprincipled approach to capture link information and propose a linked Recurrent\nNeural Network (LinkedRNN), which models sequential and link information\ncoherently. We conduct experiments on real-world datasets from multiple domains\nand the experimental results validate the effectiveness of the proposed\nframework. \n\n"}
{"id": "1808.07258", "contents": "Title: Escaping from Collapsing Modes in a Constrained Space Abstract: Generative adversarial networks (GANs) often suffer from unpredictable\nmode-collapsing during training. We study the issue of mode collapse of\nBoundary Equilibrium Generative Adversarial Network (BEGAN), which is one of\nthe state-of-the-art generative models. Despite its potential of generating\nhigh-quality images, we find that BEGAN tends to collapse at some modes after a\nperiod of training. We propose a new model, called \\emph{BEGAN with a\nConstrained Space} (BEGAN-CS), which includes a latent-space constraint in the\nloss function. We show that BEGAN-CS can significantly improve training\nstability and suppress mode collapse without either increasing the model\ncomplexity or degrading the image quality. Further, we visualize the\ndistribution of latent vectors to elucidate the effect of latent-space\nconstraint. The experimental results show that our method has additional\nadvantages of being able to train on small datasets and to generate images\nsimilar to a given real image yet with variations of designated attributes\non-the-fly. \n\n"}
{"id": "1808.07945", "contents": "Title: Maximal Jacobian-based Saliency Map Attack Abstract: The Jacobian-based Saliency Map Attack is a family of adversarial attack\nmethods for fooling classification models, such as deep neural networks for\nimage classification tasks. By saturating a few pixels in a given image to\ntheir maximum or minimum values, JSMA can cause the model to misclassify the\nresulting adversarial image as a specified erroneous target class. We propose\ntwo variants of JSMA, one which removes the requirement to specify a target\nclass, and another that additionally does not need to specify whether to only\nincrease or decrease pixel intensities. Our experiments highlight the\ncompetitive speeds and qualities of these variants when applied to datasets of\nhand-written digits and natural scenes. \n\n"}
{"id": "1808.08085", "contents": "Title: A dynamical approach to privacy preserving average consensus Abstract: In this paper we propose a novel method for achieving average consensus in a\ncontinuous-time multiagent network while avoiding to disclose the initial\nstates of the individual agents. In order to achieve privacy protection of the\nstate variables, we introduce maps, called output masks, which alter the value\nof the states before transmitting them. These output masks are local (i.e.,\nimplemented independently by each agent), deterministic, time-varying and\nconverging asymptotically to the true state. The resulting masked system is\nalso time-varying and has the original (unmasked) system as its limit system.\nIt is shown in the paper that the masked system has the original average\nconsensus value as a global attractor. However, in order to preserve privacy,\nit cannot share an equilibrium point with the unmasked system, meaning that in\nthe masked system the global attractor cannot be also stable. \n\n"}
{"id": "1808.08124", "contents": "Title: Insect cyborgs: Bio-mimetic feature generators improve machine learning\n  accuracy on limited data Abstract: Machine learning (ML) classifiers always benefit from more informative input\nfeatures. We seek to auto-generate stronger feature sets in order to address\nthe difficulty that ML methods often experience given limited training data. A\nwide range of biological neural nets (BNNs) excel at fast learning, implying\nthat they are adept at extracting informative features. We can thus look to\nBNNs for tools to improve ML performance in this low-data regime. The insect\nolfactory network learns new odors very rapidly, by means of three key\nelements: A competitive inhibition layer; a high-dimensional sparse plastic\nlayer; and Hebbian updates of synaptic weights.\n  In this work, we deployed MothNet, a computational model of the insect\nolfactory network, as an automatic feature generator: Attached as a front-end\npre-processor, its Readout Neurons provided new features, derived from the\noriginal features, for use by standard ML classifiers. We found that these\n\"insect cyborgs\", i.e. classifiers that are part-insect model and part-ML\nmethod, had significantly better performance than baseline ML methods alone on\na vectorized MNIST dataset. The MothNet feature generator also substantially\nout-performed other feature generating methods such as PCA, PLS, and NNs, as\nwell as pre-training to initialize NN weights. Cyborgs improved relative test\nset accuracy by an average of 6% to 33% depending on baseline ML accuracy,\nwhile relative reduction in test set error exceeded 50% for higher baseline\naccuracy ML models. These results indicate the potential value of BNN-inspired\nfeature generators in the ML context. \n\n"}
{"id": "1808.08469", "contents": "Title: Optimal Nonparametric Inference with Two-Scale Distributional Nearest\n  Neighbors Abstract: The weighted nearest neighbors (WNN) estimator has been popularly used as a\nflexible and easy-to-implement nonparametric tool for mean regression\nestimation. The bagging technique is an elegant way to form WNN estimators with\nweights automatically generated to the nearest neighbors; we name the resulting\nestimator as the distributional nearest neighbors (DNN) for easy reference.\nYet, there is a lack of distributional results for such estimator, limiting its\napplication to statistical inference. Moreover, when the mean regression\nfunction has higher-order smoothness, DNN does not achieve the optimal\nnonparametric convergence rate, mainly because of the bias issue. In this work,\nwe provide an in-depth technical analysis of the DNN, based on which we suggest\na bias reduction approach for the DNN estimator by linearly combining two DNN\nestimators with different subsampling scales, resulting in the novel two-scale\nDNN (TDNN) estimator. The two-scale DNN estimator has an equivalent\nrepresentation of WNN with weights admitting explicit forms and some being\nnegative. We prove that, thanks to the use of negative weights, the two-scale\nDNN estimator enjoys the optimal nonparametric rate of convergence in\nestimating the regression function under the fourth-order smoothness condition.\nWe further go beyond estimation and establish that the DNN and two-scale DNN\nare both asymptotically normal as the subsampling scales and sample size\ndiverge to infinity. For the practical implementation, we also provide variance\nestimators and a distribution estimator using the jackknife and bootstrap\ntechniques for the two-scale DNN. These estimators can be exploited for\nconstructing valid confidence intervals for nonparametric inference of the\nregression function. The theoretical results and appealing finite-sample\nperformance of the suggested two-scale DNN method are illustrated with several\nnumerical examples. \n\n"}
{"id": "1809.01283", "contents": "Title: Routing for Traffic Networks with Mixed Autonomy Abstract: In this work we propose a macroscopic model for studying routing on networks\nshared between human-driven and autonomous vehicles that captures the effects\nof autonomous vehicles forming platoons. We use this to study inefficiency due\nto selfish routing and bound the Price of Anarchy (PoA), the maximum ratio\nbetween total delay experienced by selfish users and the minimum possible total\ndelay. To do so, we establish two road capacity models, each corresponding to\nan assumption regarding the platooning capabilities of autonomous vehicles.\nUsing these we develop a class of road delay functions, parameterized by the\nroad capacity, that are polynomial with respect to vehicle flow. We then bound\nthe PoA and the bicriteria, another measure of the inefficiency due to selfish\nrouting. We find these bounds depend on: 1) the degree of the polynomial in the\nroad cost function and 2) the degree of asymmetry, the difference in how\nhuman-driven and autonomous traffic affect congestion. We demonstrate that\nthese bounds recover the classical bounds when no asymmetry exists. We show the\nbounds are tight in certain cases and that the PoA bound is order-optimal with\nrespect to the degree of asymmetry. \n\n"}
{"id": "1809.01991", "contents": "Title: Evaluation Measures for Quantification: An Axiomatic Approach Abstract: Quantification is the task of estimating, given a set $\\sigma$ of unlabelled\nitems and a set of classes $\\mathcal{C}=\\{c_{1}, \\ldots, c_{|\\mathcal{C}|}\\}$,\nthe prevalence (or `relative frequency') in $\\sigma$ of each class $c_{i}\\in\n\\mathcal{C}$. While quantification may in principle be solved by classifying\neach item in $\\sigma$ and counting how many such items have been labelled with\n$c_{i}$, it has long been shown that this `classify and count' (CC) method\nyields suboptimal quantification accuracy. As a result, quantification is no\nlonger considered a mere byproduct of classification, and has evolved as a task\nof its own. While the scientific community has devoted a lot of attention to\ndevising more accurate quantification methods, it has not devoted much to\ndiscussing what properties an \\emph{evaluation measure for quantification}\n(EMQ) should enjoy, and which EMQs should be adopted as a result. This paper\nlies down a number of interesting properties that an EMQ may or may not enjoy,\ndiscusses if (and when) each of these properties is desirable, surveys the EMQs\nthat have been used so far, and discusses whether they enjoy or not the above\nproperties. As a result of this investigation, some of the EMQs that have been\nused in the literature turn out to be severely unfit, while others emerge as\ncloser to what the quantification community actually needs. However, a\nsignificant result is that no existing EMQ satisfies all the properties\nidentified as desirable, thus indicating that more research is needed in order\nto identify (or synthesize) a truly adequate EMQ. \n\n"}
{"id": "1809.02066", "contents": "Title: Two Dimensional Stochastic Configuration Networks for Image Data\n  Analytics Abstract: Stochastic configuration networks (SCNs) as a class of randomized learner\nmodel have been successfully employed in data analytics due to its universal\napproximation capability and fast modelling property. The technical essence\nlies in stochastically configuring hidden nodes (or basis functions) based on a\nsupervisory mechanism rather than data-independent randomization as usually\nadopted for building randomized neural networks. Given image data modelling\ntasks, the use of one-dimensional SCNs potentially demolishes the spatial\ninformation of images, and may result in undesirable performance. This paper\nextends the original SCNs to two-dimensional version, termed 2DSCNs, for fast\nbuilding randomized learners with matrix-inputs. Some theoretical analyses on\nthe goodness of 2DSCNs against SCNs, including the complexity of the random\nparameter space, and the superiority of generalization, are presented.\nEmpirical results over one regression, four benchmark handwritten digits\nclassification, and two human face recognition datasets demonstrate that the\nproposed 2DSCNs perform favourably and show good potential for image data\nanalytics. \n\n"}
{"id": "1809.02088", "contents": "Title: A Mathematical Model for Vineyard Replacement with Nonlinear Binary\n  Control Optimization Abstract: Vineyard replacement is a common practice in every wine-growing farm since\nthe grapevine production decays over time and requires a new vine to ensure the\nbusiness sustainability. In this paper, we formulate a simple discrete model\nthat captures the vineyard's main dynamics such as production values and grape\nquality. Then, by applying binary non-linear programming methods to find the\nvineyard replacement trigger, we seek the optimal solution concerning different\ngovernmental subsidies to the target producer. \n\n"}
{"id": "1809.02105", "contents": "Title: A Memory-Network Based Solution for Multivariate Time-Series Forecasting Abstract: Multivariate time series forecasting is extensively studied throughout the\nyears with ubiquitous applications in areas such as finance, traffic,\nenvironment, etc. Still, concerns have been raised on traditional methods for\nincapable of modeling complex patterns or dependencies lying in real word data.\nTo address such concerns, various deep learning models, mainly Recurrent Neural\nNetwork (RNN) based methods, are proposed. Nevertheless, capturing extremely\nlong-term patterns while effectively incorporating information from other\nvariables remains a challenge for time-series forecasting. Furthermore,\nlack-of-explainability remains one serious drawback for deep neural network\nmodels. Inspired by Memory Network proposed for solving the question-answering\ntask, we propose a deep learning based model named Memory Time-series network\n(MTNet) for time series forecasting. MTNet consists of a large memory\ncomponent, three separate encoders, and an autoregressive component to train\njointly. Additionally, the attention mechanism designed enable MTNet to be\nhighly interpretable. We can easily tell which part of the historic data is\nreferenced the most. \n\n"}
{"id": "1809.02206", "contents": "Title: Challenges of Context and Time in Reinforcement Learning: Introducing\n  Space Fortress as a Benchmark Abstract: Research in deep reinforcement learning (RL) has coalesced around improving\nperformance on benchmarks like the Arcade Learning Environment. However, these\nbenchmarks conspicuously miss important characteristics like abrupt\ncontext-dependent shifts in strategy and temporal sensitivity that are often\npresent in real-world domains. As a result, RL research has not focused on\nthese challenges, resulting in algorithms which do not understand critical\nchanges in context, and have little notion of real world time. To tackle this\nissue, this paper introduces the game of Space Fortress as a RL benchmark which\nincorporates these characteristics. We show that existing state-of-the-art RL\nalgorithms are unable to learn to play the Space Fortress game. We then confirm\nthat this poor performance is due to the RL algorithms' context insensitivity\nand reward sparsity. We also identify independent axes along which to vary\ncontext and temporal sensitivity, allowing Space Fortress to be used as a\ntestbed for understanding both characteristics in combination and also in\nisolation. We release Space Fortress as an open-source Gym environment. \n\n"}
{"id": "1809.02267", "contents": "Title: Cloud-based Quadratic Optimization with Partially Homomorphic Encryption Abstract: The development of large-scale distributed control systems has led to the\noutsourcing of costly computations to cloud-computing platforms, as well as to\nconcerns about privacy of the collected sensitive data. This paper develops a\ncloud-based protocol for a quadratic optimization problem involving multiple\nparties, each holding information it seeks to maintain private. The protocol is\nbased on the projected gradient ascent on the Lagrange dual problem and\nexploits partially homomorphic encryption and secure multi-party computation\ntechniques. Using formal cryptographic definitions of indistinguishability, the\nprotocol is shown to achieve computational privacy, i.e., there is no\ncomputationally efficient algorithm that any involved party can employ to\nobtain private information beyond what can be inferred from the party's inputs\nand outputs only. In order to reduce the communication complexity of the\nproposed protocol, we introduced a variant that achieves this objective at the\nexpense of weaker privacy guarantees. We discuss in detail the computational\nand communication complexity properties of both algorithms theoretically and\nalso through implementations. We conclude the paper with a discussion on\ncomputational privacy and other notions of privacy such as the non-unique\nretrieval of the private information from the protocol outputs. \n\n"}
{"id": "1809.03062", "contents": "Title: Analysis of the Generalization Error: Empirical Risk Minimization over\n  Deep Artificial Neural Networks Overcomes the Curse of Dimensionality in the\n  Numerical Approximation of Black-Scholes Partial Differential Equations Abstract: The development of new classification and regression algorithms based on\nempirical risk minimization (ERM) over deep neural network hypothesis classes,\ncoined deep learning, revolutionized the area of artificial intelligence,\nmachine learning, and data analysis. In particular, these methods have been\napplied to the numerical solution of high-dimensional partial differential\nequations with great success. Recent simulations indicate that deep\nlearning-based algorithms are capable of overcoming the curse of dimensionality\nfor the numerical solution of Kolmogorov equations, which are widely used in\nmodels from engineering, finance, and the natural sciences. The present paper\nconsiders under which conditions ERM over a deep neural network hypothesis\nclass approximates the solution of a $d$-dimensional Kolmogorov equation with\naffine drift and diffusion coefficients and typical initial values arising from\nproblems in computational finance up to error $\\varepsilon$. We establish that,\nwith high probability over draws of training samples, such an approximation can\nbe achieved with both the size of the hypothesis class and the number of\ntraining samples scaling only polynomially in $d$ and $\\varepsilon^{-1}$. It\ncan be concluded that ERM over deep neural network hypothesis classes overcomes\nthe curse of dimensionality for the numerical solution of linear Kolmogorov\nequations with affine coefficients. \n\n"}
{"id": "1809.03126", "contents": "Title: M-convex Function Minimization Under L1-Distance Constraint Abstract: In this paper we consider a new problem of minimizing an M-convex function\nunder L1-distance constraint (MML1); the constraint is given by an upper bound\nfor L1-distance between a feasible solution and a given \"center.\" This is\nmotivated by a nonlinear integer programming problem for re-allocation of dock\ncapacity in a bike sharing system discussed by Freund et al. (2017). The main\naim of this paper is to better understand the combinatorial structure of the\ndock re-allocation problem through the connection with M-convexity, and show\nits polynomial-time solvability using this connection. For this, we first show\nthat the dock re-allocation problem can be reformulated in the form of (MML1).\nWe then present a pseudo-polynomial-time algorithm for (MML1) based on steepest\ndescent approach. We also propose two polynomial-time algorithms for (MML1) by\nreplacing the L1-distance constraint with a simple linear constraint. Finally,\nwe apply the results for (MML1) to the dock re-allocation problem to obtain a\npseudo-polynomial-time steepest descent algorithm and also polynomial-time\nalgorithms for this problem. The proposed algorithm is based on a\nproximity-scaling algorithm for a relaxation of the dock re-allocation problem,\nwhich is of interest in its own right. \n\n"}
{"id": "1809.03133", "contents": "Title: On Privacy of Quantized Sensor Measurements through Additive Noise Abstract: We study the problem of maximizing privacy of quantized sensor measurements\nby adding random variables. In particular, we consider the setting where\ninformation about the state of a process is obtained using noisy sensor\nmeasurements. This information is quantized and sent to a remote station\nthrough an unsecured communication network. It is desired to keep the state of\nthe process private; however, because the network is not secure, adversaries\nmight have access to sensor information, which could be used to estimate the\nprocess state. To avoid an accurate state estimation, we add random numbers to\nthe quantized sensor measurements and send the sum to the remote station\ninstead. The distribution of these random variables is designed to minimize the\nmutual information between the sum and the quantized sensor measurements for a\ndesired level of distortion -- how different the sum and the quantized sensor\nmeasurements are allowed to be. Simulations are presented to illustrate our\nresults. \n\n"}
{"id": "1809.03142", "contents": "Title: Fast and Efficient Information Transmission with Burst Spikes in Deep\n  Spiking Neural Networks Abstract: The spiking neural networks (SNNs) are considered as one of the most\npromising artificial neural networks due to their energy efficient computing\ncapability. Recently, conversion of a trained deep neural network to an SNN has\nimproved the accuracy of deep SNNs. However, most of the previous studies have\nnot achieved satisfactory results in terms of inference speed and energy\nefficiency. In this paper, we propose a fast and energy-efficient information\ntransmission method with burst spikes and hybrid neural coding scheme in deep\nSNNs. Our experimental results showed the proposed methods can improve\ninference energy efficiency and shorten the latency. \n\n"}
{"id": "1809.03168", "contents": "Title: Dynamic interpolation for obstacle avoidance on Riemannian manifolds Abstract: This work is devoted to studying dynamic interpolation for obstacle\navoidance. This is a problem that consists of minimizing a suitable energy\nfunctional among a set of admissible curves subject to some interpolation\nconditions. The given energy functional depends on velocity, covariant\nacceleration and on artificial potential functions used for avoiding obstacles.\n  We derive first-order necessary conditions for optimality in the proposed\nproblem; that is, given interpolation and boundary conditions we find the set\nof differential equations describing the evolution of a curve that satisfies\nthe prescribed boundary values, interpolates the given points and is an\nextremal for the energy functional.\n  We study the problem in different settings including a general one on a\nRiemannian manifold and a more specific one on a Lie group endowed with a\nleft-invariant metric. We also consider a sub-Riemannian problem. We illustrate\nthe results with examples of rigid bodies, both planar and spatial, and\nunderactuated vehicles including a unicycle and an underactuated unmanned\nvehicle. \n\n"}
{"id": "1809.03314", "contents": "Title: A Robotic Auto-Focus System based on Deep Reinforcement Learning Abstract: Considering its advantages in dealing with high-dimensional visual input and\nlearning control policies in discrete domain, Deep Q Network (DQN) could be an\nalternative method of traditional auto-focus means in the future. In this\npaper, based on Deep Reinforcement Learning, we propose an end-to-end approach\nthat can learn auto-focus policies from visual input and finish at a clear spot\nautomatically. We demonstrate that our method - discretizing the action space\nwith coarse to fine steps and applying DQN is not only a solution to auto-focus\nbut also a general approach towards vision-based control problems. Separate\nphases of training in virtual and real environments are applied to obtain an\neffective model. Virtual experiments, which are carried out after the virtual\ntraining phase, indicates that our method could achieve 100% accuracy on a\ncertain view with different focus range. Further training on real robots could\neliminate the deviation between the simulator and real scenario, leading to\nreliable performances in real applications. \n\n"}
{"id": "1809.04539", "contents": "Title: Frequency-Aware Model Predictive Control Abstract: Transferring solutions found by trajectory optimization to robotic hardware\nremains a challenging task. When the optimization fully exploits the provided\nmodel to perform dynamic tasks, the presence of unmodeled dynamics renders the\nmotion infeasible on the real system. Model errors can be a result of model\nsimplifications, but also naturally arise when deploying the robot in\nunstructured and nondeterministic environments. Predominantly, compliant\ncontacts and actuator dynamics lead to bandwidth limitations. While classical\ncontrol methods provide tools to synthesize controllers that are robust to a\nclass of model errors, such a notion is missing in modern trajectory\noptimization, which is solved in the time domain. We propose frequency-shaped\ncost functions to achieve robust solutions in the context of optimal control\nfor legged robots. Through simulation and hardware experiments we show that\nmotion plans can be made compatible with bandwidth limits set by actuators and\ncontact dynamics. The smoothness of the model predictive solutions can be\ncontinuously tuned without compromising the feasibility of the problem.\nExperiments with the quadrupedal robot ANYmal, which is driven by\nhighly-compliant series elastic actuators, showed significantly improved\ntracking performance of the planned motion, torque, and force trajectories and\nenabled the machine to walk robustly on terrain with unmodeled compliance. \n\n"}
{"id": "1809.05259", "contents": "Title: Random Warping Series: A Random Features Method for Time-Series\n  Embedding Abstract: Time series data analytics has been a problem of substantial interests for\ndecades, and Dynamic Time Warping (DTW) has been the most widely adopted\ntechnique to measure dissimilarity between time series. A number of\nglobal-alignment kernels have since been proposed in the spirit of DTW to\nextend its use to kernel-based estimation method such as support vector\nmachine. However, those kernels suffer from diagonal dominance of the Gram\nmatrix and a quadratic complexity w.r.t. the sample size. In this work, we\nstudy a family of alignment-aware positive definite (p.d.) kernels, with its\nfeature embedding given by a distribution of \\emph{Random Warping Series\n(RWS)}. The proposed kernel does not suffer from the issue of diagonal\ndominance while naturally enjoys a \\emph{Random Features} (RF) approximation,\nwhich reduces the computational complexity of existing DTW-based techniques\nfrom quadratic to linear in terms of both the number and the length of\ntime-series. We also study the convergence of the RF approximation for the\ndomain of time series of unbounded length. Our extensive experiments on 16\nbenchmark datasets demonstrate that RWS outperforms or matches state-of-the-art\nclassification and clustering methods in both accuracy and computational time.\nOur code and data is available at {\n\\url{https://github.com/IBM/RandomWarpingSeries}}. \n\n"}
{"id": "1809.06401", "contents": "Title: Hidden Markov Model Estimation-Based Q-learning for Partially Observable\n  Markov Decision Process Abstract: The objective is to study an on-line Hidden Markov model (HMM)\nestimation-based Q-learning algorithm for partially observable Markov decision\nprocess (POMDP) on finite state and action sets. When the full state\nobservation is available, Q-learning finds the optimal action-value function\ngiven the current action (Q function). However, Q-learning can perform poorly\nwhen the full state observation is not available. In this paper, we formulate\nthe POMDP estimation into a HMM estimation problem and propose a recursive\nalgorithm to estimate both the POMDP parameter and Q function concurrently.\nAlso, we show that the POMDP estimation converges to a set of stationary points\nfor the maximum likelihood estimate, and the Q function estimation converges to\na fixed point that satisfies the Bellman optimality equation weighted on the\ninvariant distribution of the state belief determined by the HMM estimation\nprocess. \n\n"}
{"id": "1809.06646", "contents": "Title: Model-Free Adaptive Optimal Control of Episodic Fixed-Horizon\n  Manufacturing Processes using Reinforcement Learning Abstract: A self-learning optimal control algorithm for episodic fixed-horizon\nmanufacturing processes with time-discrete control actions is proposed and\nevaluated on a simulated deep drawing process. The control model is built\nduring consecutive process executions under optimal control via reinforcement\nlearning, using the measured product quality as reward after each process\nexecution. Prior model formulation, which is required by state-of-the-art\nalgorithms from model predictive control and approximate dynamic programming,\nis therefore obsolete. This avoids several difficulties namely in system\nidentification, accurate modelling, and runtime complexity, that arise when\ndealing with processes subject to nonlinear dynamics and stochastic influences.\nInstead of using pre-created process and observation models, value\nfunction-based reinforcement learning algorithms build functions of expected\nfuture reward, which are used to derive optimal process control decisions. The\nexpectation functions are learned online, by interacting with the process. The\nproposed algorithm takes stochastic variations of the process conditions into\naccount and is able to cope with partial observability. A Q-learning-based\nmethod for adaptive optimal control of partially observable episodic\nfixed-horizon manufacturing processes is developed and studied. The resulting\nalgorithm is instantiated and evaluated by applying it to a simulated\nstochastic optimal control problem in metal sheet deep drawing. \n\n"}
{"id": "1809.06848", "contents": "Title: On the Learning Dynamics of Deep Neural Networks Abstract: While a lot of progress has been made in recent years, the dynamics of\nlearning in deep nonlinear neural networks remain to this day largely\nmisunderstood. In this work, we study the case of binary classification and\nprove various properties of learning in such networks under strong assumptions\nsuch as linear separability of the data. Extending existing results from the\nlinear case, we confirm empirical observations by proving that the\nclassification error also follows a sigmoidal shape in nonlinear architectures.\nWe show that given proper initialization, learning expounds parallel\nindependent modes and that certain regions of parameter space might lead to\nfailed training. We also demonstrate that input norm and features' frequency in\nthe dataset lead to distinct convergence speeds which might shed some light on\nthe generalization capabilities of deep neural networks. We provide a\ncomparison between the dynamics of learning with cross-entropy and hinge\nlosses, which could prove useful to understand recent progress in the training\nof generative adversarial networks. Finally, we identify a phenomenon that we\nbaptize gradient starvation where the most frequent features in a dataset\nprevent the learning of other less frequent but equally informative features. \n\n"}
{"id": "1809.06970", "contents": "Title: FastDeepIoT: Towards Understanding and Optimizing Neural Network\n  Execution Time on Mobile and Embedded Devices Abstract: Deep neural networks show great potential as solutions to many sensing\napplication problems, but their excessive resource demand slows down execution\ntime, pausing a serious impediment to deployment on low-end devices. To address\nthis challenge, recent literature focused on compressing neural network size to\nimprove performance. We show that changing neural network size does not\nproportionally affect performance attributes of interest, such as execution\ntime. Rather, extreme run-time nonlinearities exist over the network\nconfiguration space. Hence, we propose a novel framework, called FastDeepIoT,\nthat uncovers the non-linear relation between neural network structure and\nexecution time, then exploits that understanding to find network configurations\nthat significantly improve the trade-off between execution time and accuracy on\nmobile and embedded devices. FastDeepIoT makes two key contributions. First,\nFastDeepIoT automatically learns an accurate and highly interpretable execution\ntime model for deep neural networks on the target device. This is done without\nprior knowledge of either the hardware specifications or the detailed\nimplementation of the used deep learning library. Second, FastDeepIoT informs a\ncompression algorithm how to minimize execution time on the profiled device\nwithout impacting accuracy. We evaluate FastDeepIoT using three different\nsensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus.\nFastDeepIoT further reduces the neural network execution time by $48\\%$ to\n$78\\%$ and energy consumption by $37\\%$ to $69\\%$ compared with the\nstate-of-the-art compression algorithms. \n\n"}
{"id": "1809.07192", "contents": "Title: Unbalanced Multi-Phase Distribution Grid Topology Estimation and Bus\n  Phase Identification Abstract: There is an increasing need for monitoring and controlling uncertainties\nbrought by distributed energy resources in distribution grids. For such goal,\naccurate multi-phase topology is the basis for correlating measurements in\nunbalanced distribution networks. Unfortunately, such topology knowledge is\noften unavailable due to limited investment, especially for \\revv{low-voltage}\ndistribution grids. Also, the bus phase labeling information is inaccurate due\nto human errors or outdated records. For this challenge, this paper utilizes\nsmart meter data for an information-theoretic approach to learn the topology of\ndistribution grids. Specifically, multi-phase unbalanced systems are converted\ninto symmetrical components, namely positive, negative, and zero sequences.\nThen, this paper proves that the Chow-Liu algorithm finds the topology by\nutilizing power flow equations and the conditional independence relationships\nimplied by the radial multi-phase structure of distribution grids with the\npresence of incorrect bus phase labels. At last, by utilizing Carson's\nequation, this paper proves that the bus phase connection can be correctly\nidentified using voltage measurements. For validation, IEEE systems are\nsimulated using three real data sets. The simulation results demonstrate that\nthe algorithm is highly accurate for finding multi-phase topology even with\nstrong load unbalancing condition and DERs. This ensures close monitoring and\ncontrolling DERs in distribution grids. \n\n"}
{"id": "1809.07196", "contents": "Title: Characterising Across-Stack Optimisations for Deep Convolutional Neural\n  Networks Abstract: Convolutional Neural Networks (CNNs) are extremely computationally demanding,\npresenting a large barrier to their deployment on resource-constrained devices.\nSince such systems are where some of their most useful applications lie (e.g.\nobstacle detection for mobile robots, vision-based medical assistive\ntechnology), significant bodies of work from both machine learning and systems\ncommunities have attempted to provide optimisations that will make CNNs\navailable to edge devices. In this paper we unify the two viewpoints in a Deep\nLearning Inference Stack and take an across-stack approach by implementing and\nevaluating the most common neural network compression techniques (weight\npruning, channel pruning, and quantisation) and optimising their parallel\nexecution with a range of programming approaches (OpenMP, OpenCL) and hardware\narchitectures (CPU, GPU). We provide comprehensive Pareto curves to instruct\ntrade-offs under constraints of accuracy, execution time, and memory space. \n\n"}
{"id": "1809.07276", "contents": "Title: Music Mood Detection Based On Audio And Lyrics With Deep Neural Net Abstract: We consider the task of multimodal music mood prediction based on the audio\nsignal and the lyrics of a track. We reproduce the implementation of\ntraditional feature engineering based approaches and propose a new model based\non deep learning. We compare the performance of both approaches on a database\ncontaining 18,000 tracks with associated valence and arousal values and show\nthat our approach outperforms classical models on the arousal detection task,\nand that both approaches perform equally on the valence prediction task. We\nalso compare the a posteriori fusion with fusion of modalities optimized\nsimultaneously with each unimodal model, and observe a significant improvement\nof valence prediction. We release part of our database for comparison purposes. \n\n"}
{"id": "1809.07874", "contents": "Title: Task-Driven Estimation and Control via Information Bottlenecks Abstract: Our goal is to develop a principled and general algorithmic framework for\ntask-driven estimation and control for robotic systems. State-of-the-art\napproaches for controlling robotic systems typically rely heavily on accurately\nestimating the full state of the robot (e.g., a running robot might estimate\njoint angles and velocities, torso state, and position relative to a goal).\nHowever, full state representations are often excessively rich for the specific\ntask at hand and can lead to significant computational inefficiency and\nbrittleness to errors in state estimation. In contrast, we present an approach\nthat eschews such rich representations and seeks to create task-driven\nrepresentations. The key technical insight is to leverage the theory of\ninformation bottlenecks}to formalize the notion of a \"task-driven\nrepresentation\" in terms of information theoretic quantities that measure the\nminimality of a representation. We propose novel iterative algorithms for\nautomatically synthesizing (offline) a task-driven representation (given in\nterms of a set of task-relevant variables (TRVs)) and a performant control\npolicy that is a function of the TRVs. We present online algorithms for\nestimating the TRVs in order to apply the control policy. We demonstrate that\nour approach results in significant robustness to unmodeled measurement\nuncertainty both theoretically and via thorough simulation experiments\nincluding a spring-loaded inverted pendulum running to a goal location. \n\n"}
{"id": "1809.07892", "contents": "Title: Error Analysis of the Stochastic Linear Feedback Particle Filter Abstract: This paper is concerned with the convergence and long-term stability analysis\nof the feedback particle filter (FPF) algorithm. The FPF is an interacting\nsystem of $N$ particles where the interaction is designed such that the\nempirical distribution of the particles approximates the posterior\ndistribution. It is known that in the mean-field limit ($N=\\infty$), the\ndistribution of the particles is equal to the posterior distribution. However\nlittle is known about the convergence to the mean-field limit. In this paper,\nwe consider the FPF algorithm for the linear Gaussian setting. In this setting,\nthe algorithm is similar to the ensemble Kalman-Bucy filter algorithm. Although\nthese algorithms have been numerically evaluated and widely used in\napplications, their convergence and long-term stability analysis remains an\nactive area of research. In this paper, we show that, (i) the mean-field limit\nis well-defined with a unique strong solution; (ii) the mean-field process is\nstable with respect to the initial condition; (iii) we provide conditions such\nthat the finite-$N$ system is long term stable and we obtain some mean-squared\nerror estimates that are uniform in time. \n\n"}
{"id": "1809.08066", "contents": "Title: Cross-Gramian-Based Dominant Subspaces Abstract: A standard approach for model reduction of linear input-output systems is\nbalanced truncation, which is based on the controllability and observability\nproperties of the underlying system. The related dominant subspace projection\nmodel reduction method similarly utilizes these system properties, yet instead\nof balancing, the associated subspaces are directly conjoined. In this work we\nextend the dominant subspace approach by computation via the cross Gramian for\nlinear systems, and describe an a-priori error indicator for this method.\nFurthermore, efficient computation is discussed alongside numerical examples\nillustrating these findings. \n\n"}
{"id": "1809.08530", "contents": "Title: Provably Correct Automatic Subdifferentiation for Qualified Programs Abstract: The Cheap Gradient Principle (Griewank 2008) --- the computational cost of\ncomputing the gradient of a scalar-valued function is nearly the same (often\nwithin a factor of $5$) as that of simply computing the function itself --- is\nof central importance in optimization; it allows us to quickly obtain (high\ndimensional) gradients of scalar loss functions which are subsequently used in\nblack box gradient-based optimization procedures. The current state of affairs\nis markedly different with regards to computing subderivatives: widely used ML\nlibraries, including TensorFlow and PyTorch, do not correctly compute\n(generalized) subderivatives even on simple examples. This work considers the\nquestion: is there a Cheap Subgradient Principle? Our main result shows that,\nunder certain restrictions on our library of nonsmooth functions (standard in\nnonlinear programming), provably correct generalized subderivatives can be\ncomputed at a computational cost that is within a (dimension-free) factor of\n$6$ of the cost of computing the scalar function itself. \n\n"}
{"id": "1809.08657", "contents": "Title: Accelerated Gossip via Stochastic Heavy Ball Method Abstract: In this paper we show how the stochastic heavy ball method (SHB) -- a popular\nmethod for solving stochastic convex and non-convex optimization problems\n--operates as a randomized gossip algorithm. In particular, we focus on two\nspecial cases of SHB: the Randomized Kaczmarz method with momentum and its\nblock variant. Building upon a recent framework for the design and analysis of\nrandomized gossip algorithms, [Loizou Richtarik, 2016] we interpret the\ndistributed nature of the proposed methods. We present novel protocols for\nsolving the average consensus problem where in each step all nodes of the\nnetwork update their values but only a subset of them exchange their private\nvalues. Numerical experiments on popular wireless sensor networks showing the\nbenefits of our protocols are also presented. \n\n"}
{"id": "1809.09875", "contents": "Title: Active Learning for Deep Object Detection Abstract: The great success that deep models have achieved in the past is mainly owed\nto large amounts of labeled training data. However, the acquisition of labeled\ndata for new tasks aside from existing benchmarks is both challenging and\ncostly. Active learning can make the process of labeling new data more\nefficient by selecting unlabeled samples which, when labeled, are expected to\nimprove the model the most. In this paper, we combine a novel method of active\nlearning for object detection with an incremental learning scheme to enable\ncontinuous exploration of new unlabeled datasets. We propose a set of\nuncertainty-based active learning metrics suitable for most object detectors.\nFurthermore, we present an approach to leverage class imbalances during sample\nselection. All methods are evaluated systematically in a continuous exploration\ncontext on the PASCAL VOC 2012 dataset. \n\n"}
{"id": "1809.09924", "contents": "Title: Hierarchy-based Image Embeddings for Semantic Image Retrieval Abstract: Deep neural networks trained for classification have been found to learn\npowerful image representations, which are also often used for other tasks such\nas comparing images w.r.t. their visual similarity. However, visual similarity\ndoes not imply semantic similarity. In order to learn semantically\ndiscriminative features, we propose to map images onto class embeddings whose\npair-wise dot products correspond to a measure of semantic similarity between\nclasses. Such an embedding does not only improve image retrieval results, but\ncould also facilitate integrating semantics for other tasks, e.g., novelty\ndetection or few-shot learning. We introduce a deterministic algorithm for\ncomputing the class centroids directly based on prior world-knowledge encoded\nin a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,\nand ImageNet show that our learned semantic image embeddings improve the\nsemantic consistency of image retrieval results by a large margin. \n\n"}
{"id": "1809.10388", "contents": "Title: Queue-based Resampling for Online Class Imbalance Learning Abstract: Online class imbalance learning constitutes a new problem and an emerging\nresearch topic that focusses on the challenges of online learning under class\nimbalance and concept drift. Class imbalance deals with data streams that have\nvery skewed distributions while concept drift deals with changes in the class\nimbalance status. Little work exists that addresses these challenges and in\nthis paper we introduce queue-based resampling, a novel algorithm that\nsuccessfully addresses the co-existence of class imbalance and concept drift.\nThe central idea of the proposed resampling algorithm is to selectively include\nin the training set a subset of the examples that appeared in the past. Results\non two popular benchmark datasets demonstrate the effectiveness of queue-based\nresampling over state-of-the-art methods in terms of learning speed and\nquality. \n\n"}
{"id": "1809.10771", "contents": "Title: Double-layered distributed transient frequency control with regional\n  coordination for power networks Abstract: This paper proposes a control strategy for power systems with a two-layer\nstructure that achieves global stabilization and, at the same time, delimits\nthe transient frequencies of targeted buses to a desired safe interval. The\nfirst layer is a model predictive control that, in a receding horizon fashion,\noptimally allocates the power resources while softly respecting transient\nfrequency constraints. As the first layer control requires solving an\noptimization problem online, it only periodically samples the system state and\nupdates its action. The second layer control, however, is implemented in real\ntime, assisting the first layer to achieve frequency invariance and\nattractivity requirements.We show that the controllers designed at both layers\nare Lipschitz in the state. Furthermore, through network partition, they can be\nimplemented in a distributed fashion, only requiring system information from\nneighboring partitions. Simulations on the IEEE 39-bus network illustrate our\nresults. \n\n"}
{"id": "1810.00093", "contents": "Title: Barrier Certificates for Assured Machine Teaching Abstract: Machine teaching can be viewed as optimal control for learning. Given a\nlearner's model, machine teaching aims to determine the optimal training data\nto steer the learner towards a target hypothesis. In this paper, we are\ninterested in providing assurances for machine teaching algorithms using\ncontrol theory. In particular, we study a well-established learner's model in\nthe machine teaching literature that is captured by the local preference over a\nversion space. We interpret the problem of teaching a preference-based learner\nas solving a partially observable Markov decision process (POMDP). We then show\nthat the POMDP formulation can be cast as a special hybrid system, i.e., a\ndiscrete-time switched system. Subsequently, we use barrier certificates to\nverify set-theoric properties of this special hybrid system. We show how the\ncomputation of the barrier certificate can be decomposed and numerically\nimplemented as the solution to a sum-of-squares (SOS) program. For\nillustration, we show how the proposed framework based on control theory can be\nused to verify the teaching performance of two well-known machine teaching\nmethods. \n\n"}
{"id": "1810.00240", "contents": "Title: Reinforcement Learning in R Abstract: Reinforcement learning refers to a group of methods from artificial\nintelligence where an agent performs learning through trial and error. It\ndiffers from supervised learning, since reinforcement learning requires no\nexplicit labels; instead, the agent interacts continuously with its\nenvironment. That is, the agent starts in a specific state and then performs an\naction, based on which it transitions to a new state and, depending on the\noutcome, receives a reward. Different strategies (e.g. Q-learning) have been\nproposed to maximize the overall reward, resulting in a so-called policy, which\ndefines the best possible action in each state. Mathematically, this process\ncan be formalized by a Markov decision process and it has been implemented by\npackages in R; however, there is currently no package available for\nreinforcement learning. As a remedy, this paper demonstrates how to perform\nreinforcement learning in R and, for this purpose, introduces the\nReinforcementLearning package. The package provides a remarkably flexible\nframework and is easily applied to a wide range of different problems. We\ndemonstrate its use by drawing upon common examples from the literature (e.g.\nfinding optimal game strategies). \n\n"}
{"id": "1810.00527", "contents": "Title: Safe Adaptive Switching among Dynamical Movement Primitives: Application\n  to 3D Limit-Cycle Walkers Abstract: Complex motions for robots are frequently generated by switching among a\ncollection of individual movement primitives. We use this approach to formulate\nrobot motion plans as sequences of primitives to be executed one after the\nother. When dealing with dynamical movement primitives, besides accomplishing\nthe high-level objective, planners must also reason about the effect of the\nplan's execution on the safety of the platform. This task becomes more daunting\nin the presence of disturbances, such as external forces. To alleviate this\nissue, we present a framework that builds on rigorous control-theoretic tools\nto generate safely-executable motion plans for externally excited robotic\nsystems. Our framework is illustrated on a 3D limit-cycle gait bipedal robot\nthat adapts its walking pattern to persistent external forcing. \n\n"}
{"id": "1810.00555", "contents": "Title: Probabilistic Meta-Representations Of Neural Networks Abstract: Existing Bayesian treatments of neural networks are typically characterized\nby weak prior and approximate posterior distributions according to which all\nthe weights are drawn independently. Here, we consider a richer prior\ndistribution in which units in the network are represented by latent variables,\nand the weights between units are drawn conditionally on the values of the\ncollection of those variables. This allows rich correlations between related\nweights, and can be seen as realizing a function prior with a Bayesian\ncomplexity regularizer ensuring simple solutions. We illustrate the resulting\nmeta-representations and representations, elucidating the power of this prior. \n\n"}
{"id": "1810.01861", "contents": "Title: Inhibited Softmax for Uncertainty Estimation in Neural Networks Abstract: We present a new method for uncertainty estimation and out-of-distribution\ndetection in neural networks with softmax output. We extend softmax layer with\nan additional constant input. The corresponding additional output is able to\nrepresent the uncertainty of the network. The proposed method requires neither\nadditional parameters nor multiple forward passes nor input preprocessing nor\nout-of-distribution datasets. We show that our method performs comparably to\nmore computationally expensive methods and outperforms baselines on our\nexperiments from image recognition and sentiment analysis domains. \n\n"}
{"id": "1810.02013", "contents": "Title: Probabilistic assessment of the impact of flexible loads under network\n  tariffs in low voltage distribution networks Abstract: Given the historically static nature of low-voltage networks, distribution\nnetwork companies do not possess tools for dealing with an increasingly\nvariable demand due to the high penetration of distributed energy resources\n(DER). Within this context, this paper proposes a probabilistic framework for\ntariff design that minimises the impact of DER on network performance,\nstabilise network company revenue, and improves the equity of network costs\nallocation. To address the issue of the lack of customers' response, we also\nshow how DER-specific tariffs can be complemented with an automated home energy\nmanagement system (HEMS) that reduces peak demand while retaining the desired\ncomfort level. The proposed framework comprises a nonparametric Bayesian model\nwhich statistically generates synthetic load and PV traces, a hot-water-use\nstatistical model, a novel HEMS to schedule customers' controllable devices,\nand a probabilistic power-flow model. Test cases using both energy- and\ndemand-based network tariffs show that flat tariffs with a peak demand\ncomponent reduce the customers' cost, and alleviate network constraints. This\ndemonstrates, first, the efficacy of the proposed tool for the development of\ntariffs that are beneficial for networks with a high DER penetration, and\nsecond, how customers' HEM systems can be part of the solution. \n\n"}
{"id": "1810.02019", "contents": "Title: Seq2Slate: Re-ranking and Slate Optimization with RNNs Abstract: Ranking is a central task in machine learning and information retrieval. In\nthis task, it is especially important to present the user with a slate of items\nthat is appealing as a whole. This in turn requires taking into account\ninteractions between items, since intuitively, placing an item on the slate\naffects the decision of which other items should be placed alongside it. In\nthis work, we propose a sequence-to-sequence model for ranking called\nseq2slate. At each step, the model predicts the next `best' item to place on\nthe slate given the items already selected. The sequential nature of the model\nallows complex dependencies between the items to be captured directly in a\nflexible and scalable way. We show how to learn the model end-to-end from weak\nsupervision in the form of easily obtained click-through data. We further\ndemonstrate the usefulness of our approach in experiments on standard ranking\nbenchmarks as well as in a real-world recommendation system. \n\n"}
{"id": "1810.03105", "contents": "Title: ASVRG: Accelerated Proximal SVRG Abstract: This paper proposes an accelerated proximal stochastic variance reduced\ngradient (ASVRG) method, in which we design a simple and effective momentum\nacceleration trick. Unlike most existing accelerated stochastic variance\nreduction methods such as Katyusha, ASVRG has only one additional variable and\none momentum parameter. Thus, ASVRG is much simpler than those methods, and has\nmuch lower per-iteration complexity. We prove that ASVRG achieves the best\nknown oracle complexities for both strongly convex and non-strongly convex\nobjectives. In addition, we extend ASVRG to mini-batch and non-smooth settings.\nWe also empirically verify our theoretical results and show that the\nperformance of ASVRG is comparable with, and sometimes even better than that of\nthe state-of-the-art stochastic methods. \n\n"}
{"id": "1810.03233", "contents": "Title: Towards Gradient Free and Projection Free Stochastic Optimization Abstract: This paper focuses on the problem of \\emph{constrained} \\emph{stochastic}\noptimization. A zeroth order Frank-Wolfe algorithm is proposed, which in\naddition to the projection-free nature of the vanilla Frank-Wolfe algorithm\nmakes it gradient free. Under convexity and smoothness assumption, we show that\nthe proposed algorithm converges to the optimal objective function at a rate\n$O\\left(1/T^{1/3}\\right)$, where $T$ denotes the iteration count. In\nparticular, the primal sub-optimality gap is shown to have a dimension\ndependence of $O\\left(d^{1/3}\\right)$, which is the best known dimension\ndependence among all zeroth order optimization algorithms with one directional\nderivative per iteration. For non-convex functions, we obtain the\n\\emph{Frank-Wolfe} gap to be $O\\left(d^{1/3}T^{-1/4}\\right)$. Experiments on\nblack-box optimization setups demonstrate the efficacy of the proposed\nalgorithm. \n\n"}
{"id": "1810.03538", "contents": "Title: Combinatorial Attacks on Binarized Neural Networks Abstract: Binarized Neural Networks (BNNs) have recently attracted significant interest\ndue to their computational efficiency. Concurrently, it has been shown that\nneural networks may be overly sensitive to \"attacks\" - tiny adversarial changes\nin the input - which may be detrimental to their use in safety-critical\ndomains. Designing attack algorithms that effectively fool trained models is a\nkey step towards learning robust neural networks. The discrete,\nnon-differentiable nature of BNNs, which distinguishes them from their\nfull-precision counterparts, poses a challenge to gradient-based attacks. In\nthis work, we study the problem of attacking a BNN through the lens of\ncombinatorial and integer optimization. We propose a Mixed Integer Linear\nProgramming (MILP) formulation of the problem. While exact and flexible, the\nMILP quickly becomes intractable as the network and perturbation space grow. To\naddress this issue, we propose IProp, a decomposition-based algorithm that\nsolves a sequence of much smaller MILP problems. Experimentally, we evaluate\nboth proposed methods against the standard gradient-based attack (FGSM) on\nMNIST and Fashion-MNIST, and show that IProp performs favorably compared to\nFGSM, while scaling beyond the limits of the MILP. \n\n"}
{"id": "1810.03979", "contents": "Title: Extended Bit-Plane Compression for Convolutional Neural Network\n  Accelerators Abstract: After the tremendous success of convolutional neural networks in image\nclassification, object detection, speech recognition, etc., there is now rising\ndemand for deployment of these compute-intensive ML models on tightly power\nconstrained embedded and mobile systems at low cost as well as for pushing the\nthroughput in data centers. This has triggered a wave of research towards\nspecialized hardware accelerators. Their performance is often constrained by\nI/O bandwidth and the energy consumption is dominated by I/O transfers to\noff-chip memory. We introduce and evaluate a novel, hardware-friendly\ncompression scheme for the feature maps present within convolutional neural\nnetworks. We show that an average compression ratio of 4.4x relative to\nuncompressed data and a gain of 60% over existing method can be achieved for\nResNet-34 with a compression block requiring <300 bit of sequential cells and\nminimal combinational logic. \n\n"}
{"id": "1810.04301", "contents": "Title: Detection and Mitigation of Biasing Attacks on Distributed Estimation\n  Networks Abstract: The paper considers a problem of detecting and mitigating biasing attacks on\nnetworks of state observers targeting cooperative state estimation algorithms.\nThe problem is cast within the recently developed framework of distributed\nestimation utilizing the vector dissipativity approach. The paper shows that a\nnetwork of distributed observers can be endowed with an additional attack\ndetection layer capable of detecting biasing attacks and correcting their\neffect on estimates produced by the network. An example is provided to\nillustrate the performance of the proposed distributed attack detector. \n\n"}
{"id": "1810.04754", "contents": "Title: Efficient Tensor Decomposition with Boolean Factors Abstract: Tensor decomposition has been extensively used as a tool for exploratory\nanalysis. Motivated by neuroscience applications, we study tensor decomposition\nwith Boolean factors. The resulting optimization problem is challenging due to\nthe non-convex objective and the combinatorial constraints. We propose Binary\nMatching Pursuit (BMP), a novel generalization of the matching pursuit strategy\nto decompose the tensor efficiently. BMP iteratively searches for atoms in a\ngreedy fashion. The greedy atom search step is solved efficiently via a\nMAXCUT-like boolean quadratic program. We prove that BMP is guaranteed to\nconverge sublinearly to the optimal solution and recover the factors under mild\nidentifiability conditions. Experiments demonstrate the superior performance of\nour method over baselines on synthetic and real datasets. We also showcase the\napplication of BMP in quantifying neural interactions underlying\nhigh-resolution spatiotemporal ECoG recordings. \n\n"}
{"id": "1810.05207", "contents": "Title: On Kernel Derivative Approximation with Random Fourier Features Abstract: Random Fourier features (RFF) represent one of the most popular and\nwide-spread techniques in machine learning to scale up kernel algorithms.\nDespite the numerous successful applications of RFFs, unfortunately, quite\nlittle is understood theoretically on their optimality and limitations of their\nperformance. Only recently, precise statistical-computational trade-offs have\nbeen established for RFFs in the approximation of kernel values, kernel ridge\nregression, kernel PCA and SVM classification. Our goal is to spark the\ninvestigation of optimality of RFF-based approximations in tasks involving not\nonly function values but derivatives, which naturally lead to optimization\nproblems with kernel derivatives. Particularly, in this paper, we focus on the\napproximation quality of RFFs for kernel derivatives and prove that the\nexisting finite-sample guarantees can be improved exponentially in terms of the\ndomain where they hold, using recent tools from unbounded empirical process\ntheory. Our result implies that the same approximation guarantee is attainable\nfor kernel derivatives using RFF as achieved for kernel values. \n\n"}
{"id": "1810.06749", "contents": "Title: Optimally rotated coordinate systems for adaptive least-squares\n  regression on sparse grids Abstract: For low-dimensional data sets with a large amount of data points, standard\nkernel methods are usually not feasible for regression anymore. Besides simple\nlinear models or involved heuristic deep learning models, grid-based\ndiscretizations of larger (kernel) model classes lead to algorithms, which\nnaturally scale linearly in the amount of data points. For moderate-dimensional\nor high-dimensional regression tasks, these grid-based discretizations suffer\nfrom the curse of dimensionality. Here, sparse grid methods have proven to\ncircumvent this problem to a large extent. In this context, space- and\ndimension-adaptive sparse grids, which can detect and exploit a given low\neffective dimensionality of nominally high-dimensional data, are particularly\nsuccessful. They nevertheless rely on an axis-aligned structure of the solution\nand exhibit issues for data with predominantly skewed and rotated coordinates.\n  In this paper we propose a preprocessing approach for these adaptive sparse\ngrid algorithms that determines an optimized, problem-dependent coordinate\nsystem and, thus, reduces the effective dimensionality of a given data set in\nthe ANOVA sense. We provide numerical examples on synthetic data as well as\nreal-world data to show how an adaptive sparse grid least squares algorithm\nbenefits from our preprocessing method. \n\n"}
{"id": "1810.06755", "contents": "Title: Discovering Fair Representations in the Data Domain Abstract: Interpretability and fairness are critical in computer vision and machine\nlearning applications, in particular when dealing with human outcomes, e.g.\ninviting or not inviting for a job interview based on application materials\nthat may include photographs. One promising direction to achieve fairness is by\nlearning data representations that remove the semantics of protected\ncharacteristics, and are therefore able to mitigate unfair outcomes. All\navailable models however learn latent embeddings which comes at the cost of\nbeing uninterpretable. We propose to cast this problem as data-to-data\ntranslation, i.e. learning a mapping from an input domain to a fair target\ndomain, where a fairness definition is being enforced. Here the data domain can\nbe images, or any tabular data representation. This task would be\nstraightforward if we had fair target data available, but this is not the case.\nTo overcome this, we learn a highly unconstrained mapping by exploiting\nstatistics of residuals - the difference between input data and its translated\nversion - and the protected characteristics. When applied to the CelebA dataset\nof face images with gender attribute as the protected characteristic, our model\nenforces equality of opportunity by adjusting the eyes and lips regions.\nIntriguingly, on the same dataset we arrive at similar conclusions when using\nsemantic attribute representations of images for translation. On face images of\nthe recent DiF dataset, with the same gender attribute, our method adjusts nose\nregions. In the Adult income dataset, also with protected gender attribute, our\nmodel achieves equality of opportunity by, among others, obfuscating the wife\nand husband relationship. Analyzing those systematic changes will allow us to\nscrutinize the interplay of fairness criterion, chosen protected\ncharacteristics, and prediction performance. \n\n"}
{"id": "1810.09098", "contents": "Title: Stochastic Gradient MCMC for State Space Models Abstract: State space models (SSMs) are a flexible approach to modeling complex time\nseries. However, inference in SSMs is often computationally prohibitive for\nlong time series. Stochastic gradient MCMC (SGMCMC) is a popular method for\nscalable Bayesian inference for large independent data. Unfortunately when\napplied to dependent data, such as in SSMs, SGMCMC's stochastic gradient\nestimates are biased as they break crucial temporal dependencies. To alleviate\nthis, we propose stochastic gradient estimators that control this bias by\nperforming additional computation in a `buffer' to reduce breaking\ndependencies. Furthermore, we derive error bounds for this bias and show a\ngeometric decay under mild conditions. Using these estimators, we develop novel\nSGMCMC samplers for discrete, continuous and mixed-type SSMs with analytic\nmessage passing. Our experiments on real and synthetic data demonstrate the\neffectiveness of our SGMCMC algorithms compared to batch MCMC, allowing us to\nscale inference to long time series with millions of time points. \n\n"}
{"id": "1810.09312", "contents": "Title: Analyzing and Interpreting Convolutional Neural Networks in NLP Abstract: Convolutional neural networks have been successfully applied to various NLP\ntasks. However, it is not obvious whether they model different linguistic\npatterns such as negation, intensification, and clause compositionality to help\nthe decision-making process. In this paper, we apply visualization techniques\nto observe how the model can capture different linguistic features and how\nthese features can affect the performance of the model. Later on, we try to\nidentify the model errors and their sources. We believe that interpreting CNNs\nis the first step to understand the underlying semantic features which can\nraise awareness to further improve the performance and explainability of CNN\nmodels. \n\n"}
{"id": "1810.09440", "contents": "Title: Deep multi-survey classification of variable stars Abstract: During the last decade, a considerable amount of effort has been made to\nclassify variable stars using different machine learning techniques. Typically,\nlight curves are represented as vectors of statistical descriptors or features\nthat are used to train various algorithms. These features demand big\ncomputational powers that can last from hours to days, making impossible to\ncreate scalable and efficient ways of automatically classifying variable stars.\nAlso, light curves from different surveys cannot be integrated and analyzed\ntogether when using features, because of observational differences. For\nexample, having variations in cadence and filters, feature distributions become\nbiased and require expensive data-calibration models. The vast amount of data\nthat will be generated soon make necessary to develop scalable machine learning\narchitectures without expensive integration techniques. Convolutional Neural\nNetworks have shown impressing results in raw image classification and\nrepresentation within the machine learning literature. In this work, we present\na novel Deep Learning model for light curve classification, mainly based on\nconvolutional units. Our architecture receives as input the differences between\ntime and magnitude of light curves. It captures the essential classification\npatterns regardless of cadence and filter. In addition, we introduce a novel\ndata augmentation schema for unevenly sampled time series. We test our method\nusing three different surveys: OGLE-III; Corot; and VVV, which differ in\nfilters, cadence, and area of the sky. We show that besides the benefit of\nscalability, our model obtains state of the art levels accuracy in light curve\nclassification benchmarks. \n\n"}
{"id": "1810.09666", "contents": "Title: Online learning with feedback graphs and switching costs Abstract: We study online learning when partial feedback information is provided\nfollowing every action of the learning process, and the learner incurs\nswitching costs for changing his actions. In this setting, the feedback\ninformation system can be represented by a graph, and previous works studied\nthe expected regret of the learner in the case of a clique (Expert setup), or\ndisconnected single loops (Multi-Armed Bandits (MAB)). This work provides a\nlower bound on the expected regret in the Partial Information (PI) setting,\nnamely for general feedback graphs --excluding the clique. Additionally, it\nshows that all algorithms that are optimal without switching costs are\nnecessarily sub-optimal in the presence of switching costs, which motivates the\nneed to design new algorithms. We propose two new algorithms: Threshold Based\nEXP3 and EXP3. SC. For the two special cases of symmetric PI setting and MAB,\nthe expected regret of both of these algorithms is order optimal in the\nduration of the learning process. Additionally, Threshold Based EXP3 is order\noptimal in the switching cost, whereas EXP3. SC is not. Finally, empirical\nevaluations show that Threshold Based EXP3 outperforms the previously proposed\norder-optimal algorithms EXP3 SET in the presence of switching costs, and Batch\nEXP3 in the MAB setting with switching costs. \n\n"}
{"id": "1810.10358", "contents": "Title: Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent\n  Motion Imaging Abstract: Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in\nvivo perfusion quantification with magnetic resonance imaging (MRI). However,\nits use is limited by typically low accuracy due to low signal-to-noise ratio\n(SNR) at large gradient encoding magnitudes as well as dephasing artefacts\ncaused by subject motion, which is particularly challenging in fetal MRI. To\nmitigate this problem, we propose an implicit IVIM signal acquisition model\nwith which we learn full posterior distribution of perfusion parameters using\nartificial neural networks. This posterior then encapsulates the uncertainty of\nthe inferred parameter estimates, which we validate herein via numerical\nexperiments with rejection-based Bayesian sampling. Compared to\nstate-of-the-art IVIM estimation method of segmented least-squares fitting, our\nproposed approach improves parameter estimation accuracy by 65% on synthetic\nanisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method\nincreases repeatability of parameter estimation in placenta by 46%. \n\n"}
{"id": "1810.11229", "contents": "Title: Null-controllability and control cost estimates for the heat equation on\n  unbounded and large bounded domains Abstract: We survey recent results on the control problem for the heat equation on\nunbounded and large bounded domains. First we formulate new uncertainty\nrelations, respectively spectral inequalities. Then we present an abstract\ncontrol cost estimate which improves upon earlier results. It is particularly\ninteresting when combined with the earlier mentioned spectral inequalities\nsince it yields sharp control cost bounds in several asymptotic regimes. We\nalso show that control problems on unbounded domains can be approximated by\ncorresponding problems on a sequence of bounded domains forming an exhaustion.\nOur results apply also for the generalized heat equation associated with a\nSchr\\\"odinger semigroup. \n\n"}
{"id": "1810.11347", "contents": "Title: Generating equilibrium molecules with deep neural networks Abstract: Discovery of atomistic systems with desirable properties is a major challenge\nin chemistry and material science. Here we introduce a novel, autoregressive,\nconvolutional deep neural network architecture that generates molecular\nequilibrium structures by sequentially placing atoms in three-dimensional\nspace. The model estimates the joint probability over molecular configurations\nwith tractable conditional probabilities which only depend on distances between\natoms and their nuclear charges. It combines concepts from state-of-the-art\natomistic neural networks with auto-regressive generative models for images and\nspeech. We demonstrate that the architecture is capable of generating molecules\nclose to equilibrium for constitutional isomers of C$_7$O$_2$H$_{10}$. \n\n"}
{"id": "1810.11514", "contents": "Title: Learning and Interpreting Multi-Multi-Instance Learning Networks Abstract: We introduce an extension of the multi-instance learning problem where\nexamples are organized as nested bags of instances (e.g., a document could be\nrepresented as a bag of sentences, which in turn are bags of words). This\nframework can be useful in various scenarios, such as text and image\nclassification, but also supervised learning over graphs. As a further\nadvantage, multi-multi instance learning enables a particular way of\ninterpreting predictions and the decision function. Our approach is based on a\nspecial neural network layer, called bag-layer, whose units aggregate bags of\ninputs of arbitrary size. We prove theoretically that the associated class of\nfunctions contains all Boolean functions over sets of sets of instances and we\nprovide empirical evidence that functions of this kind can be actually learned\non semi-synthetic datasets. We finally present experiments on text\nclassification, on citation graphs, and social graph data, which show that our\nmodel obtains competitive results with respect to accuracy when compared to\nother approaches such as convolutional networks on graphs, while at the same\ntime it supports a general approach to interpret the learnt model, as well as\nexplain individual predictions. \n\n"}
{"id": "1810.11520", "contents": "Title: Spectrogram-channels u-net: a source separation model viewing each\n  channel as the spectrogram of each source Abstract: Sound source separation has attracted attention from Music Information\nRetrieval(MIR) researchers, since it is related to many MIR tasks such as\nautomatic lyric transcription, singer identification, and voice conversion. In\nthis paper, we propose an intuitive spectrogram-based model for source\nseparation by adapting U-Net. We call it Spectrogram-Channels U-Net, which\nmeans each channel of the output corresponds to the spectrogram of separated\nsource itself. The proposed model can be used for not only singing voice\nseparation but also multi-instrument separation by changing only the number of\noutput channels. In addition, we propose a loss function that balances volumes\nbetween different sources. Finally, we yield performance that is\nstate-of-the-art on both separation tasks. \n\n"}
{"id": "1810.11562", "contents": "Title: Monitoring the shape of weather, soundscapes, and dynamical systems: a\n  new statistic for dimension-driven data analysis on large data sets Abstract: Dimensionality-reduction methods are a fundamental tool in the analysis of\nlarge data sets. These algorithms work on the assumption that the \"intrinsic\ndimension\" of the data is generally much smaller than the ambient dimension in\nwhich it is collected. Alongside their usual purpose of mapping data into a\nsmaller dimension with minimal information loss, dimensionality-reduction\ntechniques implicitly or explicitly provide information about the dimension of\nthe data set.\n  In this paper, we propose a new statistic that we call the $\\kappa$-profile\nfor analysis of large data sets. The $\\kappa$-profile arises from a\ndimensionality-reduction optimization problem: namely that of finding a\nprojection into $k$-dimensions that optimally preserves the secants between\npoints in the data set. From this optimal projection we extract $\\kappa,$ the\nnorm of the shortest projected secant from among the set of all normalized\nsecants. This $\\kappa$ can be computed for any $k$; thus the tuple of $\\kappa$\nvalues (indexed by dimension) becomes a $\\kappa$-profile. Algorithms such as\nthe Secant-Avoidance Projection algorithm and the Hierarchical Secant-Avoidance\nProjection algorithm, provide a computationally feasible means of estimating\nthe $\\kappa$-profile for large data sets, and thus a method of understanding\nand monitoring their behavior. As we demonstrate in this paper, the\n$\\kappa$-profile serves as a useful statistic in several representative\nsettings: weather data, soundscape data, and dynamical systems data. \n\n"}
{"id": "1810.12442", "contents": "Title: Design and Implementation of Ecological Adaptive Cruise Control for\n  Autonomous Driving with Communication to Traffic Lights Abstract: This paper presents the design and implementation results of an ecological\nadaptive cruise controller (ECO-ACC) which exploits driving automation and\nconnectivity. The controller avoids front collisions and traffic light\nviolations, and is designed to reduce the energy consumption of connected\nautomated vehicles by utilizing historical and real-time signal phase and\ntiming data of traffic lights that adapt to the current traffic conditions. We\npropose an optimization-based generation of a reference velocity, and a\nvelocity-tracking model predictive controller that avoids front collisions and\nviolations. We present an experimental setup encompassing the real vehicle and\ncontroller in the loop, and an environment simulator in which the traffic flow\nand the traffic light patterns are calibrated on real-world data. We present\nand analyze simulation and experimental results, finding a significant\npotential for energy consumption reduction, even in the presence of traffic. \n\n"}
{"id": "1810.13084", "contents": "Title: Provably Accelerated Randomized Gossip Algorithms Abstract: In this work we present novel provably accelerated gossip algorithms for\nsolving the average consensus problem. The proposed protocols are inspired from\nthe recently developed accelerated variants of the randomized Kaczmarz method -\na popular method for solving linear systems. In each gossip iteration all nodes\nof the network update their values but only a pair of them exchange their\nprivate information. Numerical experiments on popular wireless sensor networks\nshowing the benefits of our protocols are also presented. \n\n"}
{"id": "1811.00570", "contents": "Title: On Difficulties of Cross-Lingual Transfer with Order Differences: A Case\n  Study on Dependency Parsing Abstract: Different languages might have different word orders. In this paper, we\ninvestigate cross-lingual transfer and posit that an order-agnostic model will\nperform better when transferring to distant foreign languages. To test our\nhypothesis, we train dependency parsers on an English corpus and evaluate their\ntransfer performance on 30 other languages. Specifically, we compare encoders\nand decoders based on Recurrent Neural Networks (RNNs) and modified\nself-attentive architectures. The former relies on sequential information while\nthe latter is more flexible at modeling word order. Rigorous experiments and\ndetailed analysis shows that RNN-based architectures transfer well to languages\nthat are close to English, while self-attentive models have better overall\ncross-lingual transferability and perform especially well on distant languages. \n\n"}
{"id": "1811.00691", "contents": "Title: Resilient Consensus Through Event-based Communication Abstract: We consider resilient versions of discrete-time multi-agent consensus in the\npresence of faulty or even malicious agents in the network. In particular, we\ndevelop event-triggered update rules which can mitigate the influence of the\nmalicious agents and at the same time reduce the necessary communication. Each\nregular agent updates its state based on a given rule using its neighbors'\ninformation. Only when the triggering condition is satisfied, the regular\nagents send their current states to their neighbors. Otherwise, the neighbors\nwill continue to use the state received the last time. Assuming that a bound on\nthe number of malicious nodes is known, we propose two update rules with\nevent-triggered communication. They follow the so-called mean subsequence\nreduced (MSR) type algorithms and ignore values received from potentially\nmalicious neighbors. We provide full characterizations for the necessary\nconnectivity in the network for the algorithms to perform correctly, which are\nstated in terms of the notion of graph robustness. A numerical example is\nprovided to demonstrate the effectiveness of the proposed approach. \n\n"}
{"id": "1811.01182", "contents": "Title: Stochastic Primal-Dual Method for Empirical Risk Minimization with\n  $\\mathcal{O}(1)$ Per-Iteration Complexity Abstract: Regularized empirical risk minimization problem with linear predictor appears\nfrequently in machine learning. In this paper, we propose a new stochastic\nprimal-dual method to solve this class of problems. Different from existing\nmethods, our proposed methods only require O(1) operations in each iteration.\nWe also develop a variance-reduction variant of the algorithm that converges\nlinearly. Numerical experiments suggest that our methods are faster than\nexisting ones such as proximal SGD, SVRG and SAGA on high-dimensional problems. \n\n"}
{"id": "1811.01303", "contents": "Title: Space-Time Sampling for Network Observability Abstract: Designing sparse sampling strategies is one of the important components in\nhaving resilient estimation and control in networked systems as they make\nnetwork design problems more cost-effective due to their reduced sampling\nrequirements and less fragile to where and when samples are collected. It is\nshown that under what conditions taking coarse samples from a network will\ncontain the same amount of information as a more finer set of samples. Our goal\nis to estimate initial condition of linear time-invariant networks using a set\nof noisy measurements. The observability condition is reformulated as the frame\ncondition, where one can easily trace location and time stamps of each sample.\nWe compare estimation quality of various sampling strategies using estimation\nmeasures, which depend on spectrum of the corresponding frame operators. Using\nproperties of the minimal polynomial of the state matrix, deterministic and\nrandomized methods are suggested to construct observability frames. Intrinsic\ntradeoffs assert that collecting samples from fewer subsystems dictates taking\nmore samples (in average) per subsystem. Three scalable algorithms are\ndeveloped to generate sparse space-time sampling strategies with explicit error\nbounds. \n\n"}
{"id": "1811.01910", "contents": "Title: Evolutionary Data Measures: Understanding the Difficulty of Text\n  Classification Tasks Abstract: Classification tasks are usually analysed and improved through new model\narchitectures or hyperparameter optimisation but the underlying properties of\ndatasets are discovered on an ad-hoc basis as errors occur. However,\nunderstanding the properties of the data is crucial in perfecting models. In\nthis paper we analyse exactly which characteristics of a dataset best determine\nhow difficult that dataset is for the task of text classification. We then\npropose an intuitive measure of difficulty for text classification datasets\nwhich is simple and fast to calculate. We show that this measure generalises to\nunseen data by comparing it to state-of-the-art datasets and results. This\nmeasure can be used to analyse the precise source of errors in a dataset and\nallows fast estimation of how difficult a dataset is to learn. We searched for\nthis measure by training 12 classical and neural network based models on 78\nreal-world datasets, then use a genetic algorithm to discover the best measure\nof difficulty. Our difficulty-calculating code ( https://github.com/Wluper/edm\n) and datasets ( http://data.wluper.com ) are publicly available. \n\n"}
{"id": "1811.02157", "contents": "Title: Solution Refinement at Regular Points of Conic Problems Abstract: Most numerical methods for conic problems use the homogenous primal-dual\nembedding, which yields a primal-dual solution or a certificate establishing\nprimal or dual infeasibility. Following Patrinos (and others, 2018), we express\nthe embedding as the problem of finding a zero of a mapping containing a\nskew-symmetric linear function and projections onto cones and their duals. We\nfocus on the special case when this mapping is regular, i.e., differentiable\nwith nonsingular derivative matrix, at a solution point. While this is not\nalways the case, it is a very common occurrence in practice. We propose a\nsimple method that uses LSQR, a variant of conjugate gradients for least\nsquares problems, and the derivative of the residual mapping to refine an\napproximate solution, i.e., to increase its accuracy. LSQR is a matrix-free\nmethod, i.e., requires only the evaluation of the derivative mapping and its\nadjoint, and so avoids forming or storing large matrices, which makes it\nefficient even for cone problems in which the data matrices are given and\ndense, and also allows the method to extend to cone programs in which the data\nare given as abstract linear operators. Numerical examples show that the method\nalmost always improves an approximate solution of a conic program, and often\ndramatically, at a computational cost that is typically small compared to the\ncost of obtaining the original approximate solution. For completeness we\ndescribe methods for computing the derivative of the projection onto the cones\ncommonly used in practice: nonnegative, second-order, semidefinite, and\nexponential cones. The paper is accompanied by an open source implementation. \n\n"}
{"id": "1811.02171", "contents": "Title: Comments Regarding `On the Identifiability of the Influence Model for\n  Stochastic Spatiotemporal Spread Processes' Abstract: The identifiability analysis of a networked Markov chain model known as the\ninfluence model, as described in a recent contribution to Arxiv, is examined.\nTwo errors in the identifiability analysis -- one related to the\nunidentifiability of the partially-observed influence model, the second related\nto an omission of an additional recurrence criterion for identifiability -- are\nnoted. In addition, some concerns about the formulation of the identifiability\nproblem and the proposed estimation approach are noted. \n\n"}
{"id": "1811.02540", "contents": "Title: Regret Circuits: Composability of Regret Minimizers Abstract: Regret minimization is a powerful tool for solving large-scale problems; it\nwas recently used in breakthrough results for large-scale extensive-form game\nsolving. This was achieved by composing simplex regret minimizers into an\noverall regret-minimization framework for extensive-form game strategy spaces.\nIn this paper we study the general composability of regret minimizers. We\nderive a calculus for constructing regret minimizers for composite convex sets\nthat are obtained from convexity-preserving operations on simpler convex sets.\nWe show that local regret minimizers for the simpler sets can be combined with\nadditional regret minimizers into an aggregate regret minimizer for the\ncomposite set. As one application, we show that the CFR framework can be\nconstructed easily from our framework. We also show ways to include curtailing\n(constraining) operations into our framework. For one, they enables the\nconstruction of CFR generalization for extensive-form games with general convex\nstrategy constraints that can cut across decision points. \n\n"}
{"id": "1811.02579", "contents": "Title: Deep Weighted Averaging Classifiers Abstract: Recent advances in deep learning have achieved impressive gains in\nclassification accuracy on a variety of types of data, including images and\ntext. Despite these gains, however, concerns have been raised about the\ncalibration, robustness, and interpretability of these models. In this paper we\npropose a simple way to modify any conventional deep architecture to\nautomatically provide more transparent explanations for classification\ndecisions, as well as an intuitive notion of the credibility of each\nprediction. Specifically, we draw on ideas from nonparametric kernel\nregression, and propose to predict labels based on a weighted sum of training\ninstances, where the weights are determined by distance in a learned\ninstance-embedding space. Working within the framework of conformal methods, we\npropose a new measure of nonconformity suggested by our model, and\nexperimentally validate the accompanying theoretical expectations,\ndemonstrating improved transparency, controlled error rates, and robustness to\nout-of-domain data, without compromising on accuracy or calibration. \n\n"}
{"id": "1811.02798", "contents": "Title: Multi-Task Graph Autoencoders Abstract: We examine two fundamental tasks associated with graph representation\nlearning: link prediction and node classification. We present a new autoencoder\narchitecture capable of learning a joint representation of local graph\nstructure and available node features for the simultaneous multi-task learning\nof unsupervised link prediction and semi-supervised node classification. Our\nsimple, yet effective and versatile model is efficiently trained end-to-end in\na single stage, whereas previous related deep graph embedding methods require\nmultiple training steps that are difficult to optimize. We provide an empirical\nevaluation of our model on five benchmark relational, graph-structured datasets\nand demonstrate significant improvement over three strong baselines for graph\nrepresentation learning. Reference code and data are available at\nhttps://github.com/vuptran/graph-representation-learning \n\n"}
{"id": "1811.03733", "contents": "Title: Universal Decision-Based Black-Box Perturbations: Breaking\n  Security-Through-Obscurity Defenses Abstract: We study the problem of finding a universal (image-agnostic) perturbation to\nfool machine learning (ML) classifiers (e.g., neural nets, decision tress) in\nthe hard-label black-box setting. Recent work in adversarial ML in the\nwhite-box setting (model parameters are known) has shown that many\nstate-of-the-art image classifiers are vulnerable to universal adversarial\nperturbations: a fixed human-imperceptible perturbation that, when added to any\nimage, causes it to be misclassified with high probability Kurakin et al.\n[2016], Szegedy et al. [2013], Chen et al. [2017a], Carlini and Wagner [2017].\nThis paper considers a more practical and challenging problem of finding such\nuniversal perturbations in an obscure (or black-box) setting. More\nspecifically, we use zeroth order optimization algorithms to find such a\nuniversal adversarial perturbation when no model information is revealed-except\nthat the attacker can make queries to probe the classifier. We further relax\nthe assumption that the output of a query is continuous valued confidence\nscores for all the classes and consider the case where the output is a\nhard-label decision. Surprisingly, we found that even in these extremely\nobscure regimes, state-of-the-art ML classifiers can be fooled with a very high\nprobability just by adding a single human-imperceptible image perturbation to\nany natural image. The surprising existence of universal perturbations in a\nhard-label black-box setting raises serious security concerns with the\nexistence of a universal noise vector that adversaries can possibly exploit to\nbreak a classifier on most natural images. \n\n"}
{"id": "1811.04064", "contents": "Title: Block Belief Propagation for Parameter Learning in Markov Random Fields Abstract: Traditional learning methods for training Markov random fields require doing\ninference over all variables to compute the likelihood gradient. The iteration\ncomplexity for those methods therefore scales with the size of the graphical\nmodels. In this paper, we propose \\emph{block belief propagation learning}\n(BBPL), which uses block-coordinate updates of approximate marginals to compute\napproximate gradients, removing the need to compute inference on the entire\ngraphical model. Thus, the iteration complexity of BBPL does not scale with the\nsize of the graphs. We prove that the method converges to the same solution as\nthat obtained by using full inference per iteration, despite these\napproximations, and we empirically demonstrate its scalability improvements\nover standard training methods. \n\n"}
{"id": "1811.04136", "contents": "Title: The GaussianSketch for Almost Relative Error Kernel Distance Abstract: We introduce two versions of a new sketch for approximately embedding the\nGaussian kernel into Euclidean inner product space. These work by truncating\ninfinite expansions of the Gaussian kernel, and carefully invoking the\nRecursiveTensorSketch [Ahle et al. SODA 2020]. After providing concentration\nand approximation properties of these sketches, we use them to approximate the\nkernel distance between points sets. These sketches yield almost\n$(1+\\varepsilon)$-relative error, but with a small additive $\\alpha$ term. In\nthe first variants the dependence on $1/\\alpha$ is poly-logarithmic, but has\nhigher degree of polynomial dependence on the original dimension $d$. In the\nsecond variant, the dependence on $1/\\alpha$ is still poly-logarithmic, but the\ndependence on $d$ is linear. \n\n"}
{"id": "1811.05157", "contents": "Title: Recurrent Multi-Graph Neural Networks for Travel Cost Prediction Abstract: Origin-destination (OD) matrices are often used in urban planning, where a\ncity is partitioned into regions and an element (i, j) in an OD matrix records\nthe cost (e.g., travel time, fuel consumption, or travel speed) from region i\nto region j. In this paper, we partition a day into multiple intervals, e.g.,\n96 15-min intervals and each interval is associated with an OD matrix which\nrepresents the costs in the interval; and we consider sparse and stochastic OD\nmatrices, where the elements represent stochastic but not deterministic costs\nand some elements are missing due to lack of data between two regions. We solve\nthe sparse, stochastic OD matrix forecasting problem. Given a sequence of\nhistorical OD matrices that are sparse, we aim at predicting future OD matrices\nwith no empty elements. We propose a generic learning framework to solve the\nproblem by dealing with sparse matrices via matrix factorization and two graph\nconvolutional neural networks and capturing temporal dynamics via recurrent\nneural network. Empirical studies using two taxi datasets from different\ncountries verify the effectiveness of the proposed framework. \n\n"}
{"id": "1811.05296", "contents": "Title: SAFE: Self-Attentive Function Embeddings for Binary Similarity Abstract: The binary similarity problem consists in determining if two functions are\nsimilar by only considering their compiled form. Advanced techniques for binary\nsimilarity recently gained momentum as they can be applied in several fields,\nsuch as copyright disputes, malware analysis, vulnerability detection, etc.,\nand thus have an immediate practical impact. Current solutions compare\nfunctions by first transforming their binary code in multi-dimensional vector\nrepresentations (embeddings), and then comparing vectors through simple and\nefficient geometric operations. However, embeddings are usually derived from\nbinary code using manual feature extraction, that may fail in considering\nimportant function characteristics, or may consider features that are not\nimportant for the binary similarity problem. In this paper we propose SAFE, a\nnovel architecture for the embedding of functions based on a self-attentive\nneural network. SAFE works directly on disassembled binary functions, does not\nrequire manual feature extraction, is computationally more efficient than\nexisting solutions (i.e., it does not incur in the computational overhead of\nbuilding or manipulating control flow graphs), and is more general as it works\non stripped binaries and on multiple architectures. We report the results from\na quantitative and qualitative analysis that show how SAFE provides a\nnoticeable performance improvement with respect to previous solutions.\nFurthermore, we show how clusters of our embedding vectors are closely related\nto the semantic of the implemented algorithms, paving the way for further\ninteresting applications (e.g. semantic-based binary function search). \n\n"}
{"id": "1811.05646", "contents": "Title: Fast Distribution Grid Line Outage Identification with $\\mu$PMU Abstract: The growing integration of distributed energy resources (DERs) in urban\ndistribution grids raises various reliability issues due to DER's uncertain and\ncomplex behaviors. With a large-scale DER penetration, traditional outage\ndetection methods, which rely on customers making phone calls and smart meters'\n\"last gasp\" signals, will have limited performance, because the renewable\ngenerators can supply powers after line outages and many urban grids are mesh\nso line outages do not affect power supply. To address these drawbacks, we\npropose a data-driven outage monitoring approach based on the stochastic time\nseries analysis from micro phasor measurement unit ($\\mu$PMU). Specifically, we\nprove via power flow analysis that the dependency of time-series voltage\nmeasurements exhibits significant statistical changes after line outages. This\nmakes the theory on optimal change-point detection suitable to identify line\noutages via $\\mu$PMUs with fast and accurate sampling. However, existing change\npoint detection methods require post-outage voltage distribution unknown in\ndistribution systems. Therefore, we design a maximum likelihood-based method to\ndirectly learn the distribution parameters from $\\mu$PMU data. We prove that\nthe estimated parameters-based detection still achieves the optimal\nperformance, making it extremely useful for distribution grid outage\nidentifications. Simulation results show highly accurate outage identification\nin eight distribution grids with 14 configurations with and without DERs using\n$\\mu$PMU data. \n\n"}
{"id": "1811.05695", "contents": "Title: Efficient and Scalable Multi-task Regression on Massive Number of Tasks Abstract: Many real-world large-scale regression problems can be formulated as\nMulti-task Learning (MTL) problems with a massive number of tasks, as in retail\nand transportation domains. However, existing MTL methods still fail to offer\nboth the generalization performance and the scalability for such problems.\nScaling up MTL methods to problems with a tremendous number of tasks is a big\nchallenge. Here, we propose a novel algorithm, named Convex Clustering\nMulti-Task regression Learning (CCMTL), which integrates with convex clustering\non the k-nearest neighbor graph of the prediction models. Further, CCMTL\nefficiently solves the underlying convex problem with a newly proposed\noptimization method. CCMTL is accurate, efficient to train, and empirically\nscales linearly in the number of tasks. On both synthetic and real-world\ndatasets, the proposed CCMTL outperforms seven state-of-the-art (SoA)\nmulti-task learning methods in terms of prediction accuracy as well as\ncomputational efficiency. On a real-world retail dataset with 23,812 tasks,\nCCMTL requires only around 30 seconds to train on a single thread, while the\nSoA methods need up to hours or even days. \n\n"}
{"id": "1811.06109", "contents": "Title: Predictive Modeling with Delayed Information: a Case Study in E-commerce\n  Transaction Fraud Control Abstract: In Business Intelligence, accurate predictive modeling is the key for\nproviding adaptive decisions. We studied predictive modeling problems in this\nresearch which was motivated by real-world cases that Microsoft data scientists\nencountered while dealing with e-commerce transaction fraud control decisions\nusing transaction streaming data in an uncertain probabilistic decision\nenvironment. The values of most online transactions related features can return\ninstantly, while the true fraud labels only return after a stochastic delay.\nUsing partially mature data directly for predictive modeling in an uncertain\nprobabilistic decision environment would lead to significant inaccuracy on risk\ndecision-making. To improve accurate estimation of the probabilistic prediction\nenvironment, which leads to more accurate predictive modeling, two frameworks,\nCurrent Environment Inference (CEI) and Future Environment Inference (FEI), are\nproposed. These frameworks generated decision environment related features\nusing long-term fully mature and short-term partially mature data, and the\nvalues of those features were estimated using varies of learning methods,\nincluding linear regression, random forest, gradient boosted tree, artificial\nneural network, and recurrent neural network. Performance tests were conducted\nusing some e-commerce transaction data from Microsoft. Testing results\nsuggested that proposed frameworks significantly improved the accuracy of\ndecision environment estimation. \n\n"}
{"id": "1811.07062", "contents": "Title: The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD\n  Training and Sample Size Abstract: We apply state-of-the-art tools in modern high-dimensional numerical linear\nalgebra to approximate efficiently the spectrum of the Hessian of modern\ndeepnets, with tens of millions of parameters, trained on real data. Our\nresults corroborate previous findings, based on small-scale networks, that the\nHessian exhibits \"spiked\" behavior, with several outliers isolated from a\ncontinuous bulk. We decompose the Hessian into different components and study\nthe dynamics with training and sample size of each term individually. \n\n"}
{"id": "1811.07157", "contents": "Title: Recurrent Convolutions for Causal 3D CNNs Abstract: Recently, three dimensional (3D) convolutional neural networks (CNNs) have\nemerged as dominant methods to capture spatiotemporal representations in\nvideos, by adding to pre-existing 2D CNNs a third, temporal dimension. Such 3D\nCNNs, however, are anti-causal (i.e., they exploit information from both the\npast and the future frames to produce feature representations, thus preventing\ntheir use in online settings), constrain the temporal reasoning horizon to the\nsize of the temporal convolution kernel, and are not temporal\nresolution-preserving for video sequence-to-sequence modelling, as, for\ninstance, in action detection. To address these serious limitations, here we\npresent a new 3D CNN architecture for the causal/online processing of videos.\n  Namely, we propose a novel Recurrent Convolutional Network (RCN), which\nrelies on recurrence to capture the temporal context across frames at each\nnetwork level. Our network decomposes 3D convolutions into (1) a 2D spatial\nconvolution component, and (2) an additional hidden state $1\\times 1$\nconvolution, applied across time. The hidden state at any time $t$ is assumed\nto depend on the hidden state at $t-1$ and on the current output of the spatial\nconvolution component. As a result, the proposed network: (i) produces causal\noutputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal\nresolution. Our experiments on the large-scale large Kinetics and MultiThumos\ndatasets show that the proposed method performs comparably to anti-causal 3D\nCNNs, while being causal and using fewer parameters. \n\n"}
{"id": "1811.07465", "contents": "Title: Bayesian Cycle-Consistent Generative Adversarial Networks via\n  Marginalizing Latent Sampling Abstract: Recent techniques built on Generative Adversarial Networks (GANs), such as\nCycle-Consistent GANs, are able to learn mappings among different domains built\nfrom unpaired datasets, through min-max optimization games between generators\nand discriminators. However, it remains challenging to stabilize the training\nprocess and thus cyclic models fall into mode collapse accompanied by the\nsuccess of discriminator. To address this problem, we propose an novel Bayesian\ncyclic model and an integrated cyclic framework for inter-domain mappings. The\nproposed method motivated by Bayesian GAN explores the full posteriors of\ncyclic model via sampling latent variables and optimizes the model with maximum\na posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In\naddition, original CycleGAN cannot generate diversified results. But it is\nfeasible for Bayesian framework to diversify generated images by replacing\nrestricted latent variables in inference process. We evaluate the proposed\nBayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps,\nand Monet2photo. The proposed method improve the per-pixel accuracy by 15% for\nthe Cityscapes semantic segmentation task within origin framework and improve\n20% within the proposed integrated framework, showing better resilience to\nimbalance confrontation. The diversified results of Monet2Photo style transfer\nalso demonstrate its superiority over original cyclic model. We provide codes\nfor all of our experiments in https://github.com/ranery/Bayesian-CycleGAN. \n\n"}
{"id": "1811.07531", "contents": "Title: Feature selection as Monte-Carlo Search in Growing Single Rooted\n  Directed Acyclic Graph by Best Leaf Identification Abstract: Monte Carlo tree search (MCTS) has received considerable interest due to its\nspectacular success in the difficult problem of computer Go and also proved\nbeneficial in a range of other domains. A major issue that has received little\nattention in the MCTS literature is the fact that, in most games, different\nactions can lead to the same state, that may lead to a high degree of\nredundancy in tree representation and unnecessary additional computational\ncost. We extend MCTS to single rooted directed acyclic graph (SR-DAG), and\nconsider the Best Arm Identification (BAI) and the Best Leaf Identification\n(BLI) problem of an expanding SR-DAG of arbitrary depth. We propose algorithms\nthat are (epsilon, delta)-correct in the fixed confidence setting, and prove an\nasymptotic upper bounds of sample complexity for our BAI algorithm. As a major\napplication for our BLI algorithm, a novel approach for Feature Selection is\nproposed by representing the feature set space as a SR-DAG and repeatedly\nevaluating feature subsets until a candidate for the best leaf is returned, a\nproof of concept is shown on benchmark data sets. \n\n"}
{"id": "1811.07569", "contents": "Title: Nash equilibrium seeking in potential games with double-integrator\n  agents Abstract: In this paper, we show the equivalence between a constrained, multi-agent\ncontrol problem, modeled within the port-Hamiltonian framework, and an exact\npotential game. Specifically, critical distance-based constraints determine a\nnetwork of double-integrator agents, which can be represented as a graph.\nVirtual couplings, i.e., pairs of spring-damper, assigned to each edge of the\ngraph, allow to synthesize a distributed, gradient-based control law that\nsteers the network to an invariant set of stable configurations. We\ncharacterize the points belonging to such set as Nash equilibria of the\nassociated potential game, relating the parameters of the virtual couplings\nwith the equilibrium seeking problem, since they are crucial to shape the\ntransient behaviour (i.e., the convergence) and, ideally, the set of reachable\nequilibria. \n\n"}
{"id": "1811.08297", "contents": "Title: Finite Mixture Model of Nonparametric Density Estimation using Sampling\n  Importance Resampling for Persistence Landscape Abstract: Considering the creation of persistence landscape on a parametrized curve and\nstructure of sampling, there exists a random process for which a finite mixture\nmodel of persistence landscape (FMMPL) can provide a better description for a\ngiven dataset. In this paper, a nonparametric approach for computing integrated\nmean of square error (IMSE) in persistence landscape has been presented. As a\nresult, FMMPL is more accurate than the another way. Also, the sampling\nimportance resampling (SIR) has been presented a better description of\nimportant landmark from parametrized curve. The result, provides more accuracy\nand less space complexity than the landmarks selected with simple sampling. \n\n"}
{"id": "1811.09600", "contents": "Title: Decoupling Direction and Norm for Efficient Gradient-Based L2\n  Adversarial Attacks and Defenses Abstract: Research on adversarial examples in computer vision tasks has shown that\nsmall, often imperceptible changes to an image can induce misclassification,\nwhich has security implications for a wide range of image processing systems.\nConsidering $L_2$ norm distortions, the Carlini and Wagner attack is presently\nthe most effective white-box attack in the literature. However, this method is\nslow since it performs a line-search for one of the optimization terms, and\noften requires thousands of iterations. In this paper, an efficient approach is\nproposed to generate gradient-based attacks that induce misclassifications with\nlow $L_2$ norm, by decoupling the direction and the norm of the adversarial\nperturbation that is added to the image. Experiments conducted on the MNIST,\nCIFAR-10 and ImageNet datasets indicate that our attack achieves comparable\nresults to the state-of-the-art (in terms of $L_2$ norm) with considerably\nfewer iterations (as few as 100 iterations), which opens the possibility of\nusing these attacks for adversarial training. Models trained with our attack\nachieve state-of-the-art robustness against white-box gradient-based $L_2$\nattacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense\nwhen the attacks are limited to a maximum norm. \n\n"}
{"id": "1811.09716", "contents": "Title: Robustness via curvature regularization, and vice versa Abstract: State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness. \n\n"}
{"id": "1811.10130", "contents": "Title: Canonical Duality Theory and Algorithm for Solving Bilevel Knapsack\n  Problems with Applications Abstract: A novel canonical duality theory (CDT) is presented for solving general\nbilevel mixed integer nonlinear optimization governed by linear and quadratic\nknapsack problems. It shows that the challenging knapsack problems can be\nsolved analytically in term of their canonical dual solutions. The existence\nand uniqueness of these analytical solutions are proved. NP-Hardness of the\nknapsack problems is discussed. A powerful CDT algorithm combined with an\nalternative iteration and a volume reduction method is proposed for solving the\nNP-hard bilevel knapsack problems. Application is illustrated by a benchmark\nproblem in optimal topology design. The performance and novelty of the proposed\nmethod are compared with the popular commercial codes. \n\n"}
{"id": "1811.10423", "contents": "Title: Dynamic Ecological System Measures Abstract: The system decomposition theory has recently been developed for the dynamic\nanalysis of nonlinear compartmental systems. The application of this theory to\nthe ecosystem analysis has also been introduced in a separate article. Based on\nthis methodology, multiple new dynamic ecological system measures and indices\nof matrix, vector, and scalar types are systematically introduced in the\npresent paper. These mathematical system analysis tools are quantitative\necological indicators that monitor the flow distribution and storage\norganization, quantify the direct, indirect, acyclic, cycling, and transfer\n(diact) effects and utilities of one compartment on another, identify the\nsystem efficiencies and stress, measure the compartmental exposures to system\nflows, determine the residence times and compartmental activity levels, and\nascertain the system resilience and resistance in the case of disturbances. The\nproposed dynamic system measures and indices, thus, extract detailed\ninformation about ecosystems' characteristics, as well as their functions,\nproperties, behaviors, and various other system attributes that are potentially\nhidden in and even obscured by data. A dynamic technique for the quantitative\ncharacterization and classification of main interspecific interactions and the\ndetermination of their strength within food webs is also developed based on the\ndiact effect and utility indices. Moreover, major concepts and quantities in\nthe current static network analyses are also extended to nonlinear dynamic\nsettings and integrated with the proposed dynamic measures and indices in this\nunifying mathematical framework. We consider that the proposed methodology\nbrings a novel complex system theory to the service of urgent and challenging\nenvironmental problems of the day and has the potential to lead the way to a\nmore formalistic ecological science. \n\n"}
{"id": "1811.10564", "contents": "Title: Low-Dose CT via Deep CNN with Skip Connection and Network in Network Abstract: A major challenge in computed tomography (CT) is how to minimize patient\nradiation exposure without compromising image quality and diagnostic\nperformance. The use of deep convolutional (Conv) neural networks for noise\nreduction in Low-Dose CT (LDCT) images has recently shown a great potential in\nthis important application. In this paper, we present a highly efficient and\neffective neural network model for LDCT image noise reduction. Specifically, to\ncapture local anatomical features we integrate Deep Convolutional Neural\nNetworks (CNNs) and Skip connection layers for feature extraction. Also, we\nintroduce parallelized $1\\times 1$ CNN, called Network in Network, to lower the\ndimensionality of the output from the previous layer, achieving faster\ncomputational speed at less feature loss. To optimize the performance of the\nnetwork, we adopt a Wasserstein generative adversarial network (WGAN)\nframework. Quantitative and qualitative comparisons demonstrate that our\nproposed network model can produce images with lower noise and more structural\ndetails than state-of-the-art noise-reduction methods. \n\n"}
{"id": "1811.10649", "contents": "Title: Noisy Computations during Inference: Harmful or Helpful? Abstract: We study two aspects of noisy computations during inference. The first aspect\nis how to mitigate their side effects for naturally trained deep learning\nsystems. One of the motivations for looking into this problem is to reduce the\nhigh power cost of conventional computing of neural networks through the use of\nanalog neuromorphic circuits. Traditional GPU/CPU-centered deep learning\narchitectures exhibit bottlenecks in power-restricted applications (e.g.,\nembedded systems). The use of specialized neuromorphic circuits, where analog\nsignals passed through memory-cell arrays are sensed to accomplish\nmatrix-vector multiplications, promises large power savings and speed gains but\nbrings with it the problems of limited precision of computations and\nunavoidable analog noise. We manage to improve inference accuracy from 21.1% to\n99.5% for MNIST images, from 29.9% to 89.1% for CIFAR10, and from 15.5% to\n89.6% for MNIST stroke sequences with the presence of strong noise (with\nsignal-to-noise power ratio being 0 dB) by noise-injected training and a voting\nmethod. This observation promises neural networks that are insensitive to\ninference noise, which reduces the quality requirements on neuromorphic\ncircuits and is crucial for their practical usage. The second aspect is how to\nutilize the noisy inference as a defensive architecture against black-box\nadversarial attacks. During inference, by injecting proper noise to signals in\nthe neural networks, the robustness of adversarially-trained neural networks\nagainst black-box attacks has been further enhanced by 0.5% and 1.13% for two\nadversarially trained models for MNIST and CIFAR10, respectively. \n\n"}
{"id": "1811.11037", "contents": "Title: A new variational approach to linearization of traction problems in\n  elasticity Abstract: A new energy functional for pure traction problems in elasticity has been\ndeduced in [23] as the variational limit of nonlinear elastic energy functional\nfor a material body subject to an equilibrated force field: a sort of Gamma\nlimit with respect to the weak convergence of strains when a suitable small\nparameter tends to zero. This functional exhibits a gap that makes it different\nfrom the classical linear elasticity functional. Nevertheless a suitable\ncompatibility condition on the force field ensures coincidence of related\nminima and minimizers. Here we show some relevant properties of the new\nfunctional and prove stronger convergence of minimizing sequences for suitable\nchoices of nonlinear elastic energies. \n\n"}
{"id": "1811.11493", "contents": "Title: A randomized gradient-free attack on ReLU networks Abstract: It has recently been shown that neural networks but also other classifiers\nare vulnerable to so called adversarial attacks e.g. in object recognition an\nalmost non-perceivable change of the image changes the decision of the\nclassifier. Relatively fast heuristics have been proposed to produce these\nadversarial inputs but the problem of finding the optimal adversarial input,\nthat is with the minimal change of the input, is NP-hard. While methods based\non mixed-integer optimization which find the optimal adversarial input have\nbeen developed, they do not scale to large networks. Currently, the attack\nscheme proposed by Carlini and Wagner is considered to produce the best\nadversarial inputs. In this paper we propose a new attack scheme for the class\nof ReLU networks based on a direct optimization on the resulting linear\nregions. In our experimental validation we improve in all except one experiment\nout of 18 over the Carlini-Wagner attack with a relative improvement of up to\n9\\%. As our approach is based on the geometrical structure of ReLU networks, it\nis less susceptible to defences targeting their functional properties. \n\n"}
{"id": "1811.12276", "contents": "Title: Improving Hospital Mortality Prediction with Medical Named Entities and\n  Multimodal Learning Abstract: Clinical text provides essential information to estimate the acuity of a\npatient during hospital stays in addition to structured clinical data. In this\nstudy, we explore how clinical text can complement a clinical predictive\nlearning task. We leverage an internal medical natural language processing\nservice to perform named entity extraction and negation detection on clinical\nnotes and compose selected entities into a new text corpus to train document\nrepresentations. We then propose a multimodal neural network to jointly train\ntime series signals and unstructured clinical text representations to predict\nthe in-hospital mortality risk for ICU patients. Our model outperforms the\nbenchmark by 2% AUC. \n\n"}
{"id": "1811.12629", "contents": "Title: LoAdaBoost: loss-based AdaBoost federated machine learning with reduced\n  computational complexity on IID and non-IID intensive care data Abstract: Intensive care data are valuable for improvement of health care, policy\nmaking and many other purposes. Vast amount of such data are stored in\ndifferent locations, on many different devices and in different data silos.\nSharing data among different sources is a big challenge due to regulatory,\noperational and security reasons. One potential solution is federated machine\nlearning, which is a method that sends machine learning algorithms\nsimultaneously to all data sources, trains models in each source and aggregates\nthe learned models. This strategy allows utilization of valuable data without\nmoving them. One challenge in applying federated machine learning is the\npossibly different distributions of data from diverse sources. To tackle this\nproblem, we proposed an adaptive boosting method named LoAdaBoost that\nincreases the efficiency of federated machine learning. Using intensive care\nunit data from hospitals, we investigated the performance of learning in IID\nand non-IID data distribution scenarios, and showed that the proposed\nLoAdaBoost method achieved higher predictive accuracy with lower computational\ncomplexity than the baseline method. \n\n"}
{"id": "1812.00045", "contents": "Title: Using Monte Carlo Tree Search as a Demonstrator within Asynchronous Deep\n  RL Abstract: Deep reinforcement learning (DRL) has achieved great successes in recent\nyears with the help of novel methods and higher compute power. However, there\nare still several challenges to be addressed such as convergence to locally\noptimal policies and long training times. In this paper, firstly, we augment\nAsynchronous Advantage Actor-Critic (A3C) method with a novel self-supervised\nauxiliary task, i.e. \\emph{Terminal Prediction}, measuring temporal closeness\nto terminal states, namely A3C-TP. Secondly, we propose a new framework where\nplanning algorithms such as Monte Carlo tree search or other sources of\n(simulated) demonstrators can be integrated to asynchronous distributed DRL\nmethods. Compared to vanilla A3C, our proposed methods both learn faster and\nconverge to better policies on a two-player mini version of the Pommerman game. \n\n"}
{"id": "1812.01204", "contents": "Title: Distributed Communication-aware Motion Planning for Networked Mobile\n  Robots under Formal Specifications Abstract: Control and communication are often tightly coupled in motion planning of\nnetworked mobile robots, due to the fact that robotic motions will affect the\noverall communication quality, and the quality of service (QoS) of the\ncommunication among the robots will in turn affect their coordination\nperformance. In this paper, we propose a control theoretical motion planning\nframework for a team of networked mobile robots in order to accomplish\nhigh-level spatial and temporal motion objectives while optimizing\ncommunication QoS. Desired motion specifications are formulated as Signal\nTemporal Logic (STL), whereas the communication performances to be optimized\nare captured by recently proposed Spatial Temporal Reach and Escape Logic\n(STREL) formulas. Both the STL and STREL specifications are encoded as mixed\ninteger linear constraints posed on the system and/or environment state\nvariables of the mobile robot network, where satisfactory control strategies\ncan be computed by exploiting a distributed model predictive control (MPC)\napproach. To the best of the authors' knowledge, we are the first to study\ncontroller synthesis for STREL specifications. A two-layer hierarchical MPC\nprocedure is proposed to efficiently solve the problem, whose soundness and\ncompleteness are formally ensured. The effectiveness of the proposed framework\nis validated by simulation examples. \n\n"}
{"id": "1812.01532", "contents": "Title: Control of automated guided vehicles without collision by quantum\n  annealer and digital devices Abstract: We formulate an optimization problem to control a large number of automated\nguided vehicles in a plant without collision. The formulation consists of\nbinary variables. A quadratic cost function over these variables enables us to\nutilize certain solvers on digital computers and recently developed\npurpose-specific hardware such as D-Wave 2000Q and the Fujitsu digital\nannealer. In the present study, we consider an actual plant in Japan, in which\nvehicles run, and assess efficiency of our formulation for optimizing the\nvehicles via several solvers. We confirm that our formulation can be a powerful\napproach for performing smooth control while avoiding collisions between\nvehicles, as compared to a conventional method. In addition, comparative\nexperiments performed using several solvers reveal that D-Wave 2000Q can be\nuseful as a rapid solver for generating a plan for controlling the vehicles in\na short time although it deals only with a small number of vehicles, while a\ndigital computer can rapidly solve the corresponding optimization problem even\nwith a large number of binary variables. \n\n"}
{"id": "1812.02099", "contents": "Title: Unlabeled sample compression schemes and corner peelings for ample and\n  maximum classes Abstract: We examine connections between combinatorial notions that arise in machine\nlearning and topological notions in cubical/simplicial geometry. These\nconnections enable to export results from geometry to machine learning. Our\nfirst main result is based on a geometric construction by Tracy Hall (2004) of\na partial shelling of the cross-polytope which can not be extended. We use it\nto derive a maximum class of VC dimension 3 that has no corners. This refutes\nseveral previous works in machine learning from the past 11 years. In\nparticular, it implies that all previous constructions of optimal unlabeled\nsample compression schemes for maximum classes are erroneous.\n  On the positive side we present a new construction of an unlabeled sample\ncompression scheme for maximum classes. We leave as open whether our unlabeled\nsample compression scheme extends to ample (a.k.a. lopsided or extremal)\nclasses, which represent a natural and far-reaching generalization of maximum\nclasses. Towards resolving this question, we provide a geometric\ncharacterization in terms of unique sink orientations of the 1-skeletons of\nassociated cubical complexes. \n\n"}
{"id": "1812.02464", "contents": "Title: Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without\n  Catastrophic Forgetting Abstract: Neural networks can achieve excellent results in a wide variety of\napplications. However, when they attempt to sequentially learn, they tend to\nlearn the new task while catastrophically forgetting previous ones. We propose\na model that overcomes catastrophic forgetting in sequential reinforcement\nlearning by combining ideas from continual learning in both the image\nclassification domain and the reinforcement learning domain. This model\nfeatures a dual memory system which separates continual learning from\nreinforcement learning and a pseudo-rehearsal system that \"recalls\" items\nrepresentative of previous tasks via a deep generative network. Our model\nsequentially learns Atari 2600 games without demonstrating catastrophic\nforgetting and continues to perform above human level on all three games. This\nresult is achieved without: demanding additional storage requirements as the\nnumber of tasks increases, storing raw data or revisiting past tasks. In\ncomparison, previous state-of-the-art solutions are substantially more\nvulnerable to forgetting on these complex deep reinforcement learning tasks. \n\n"}
{"id": "1812.02711", "contents": "Title: Formal Synthesis of Analytic Controllers for Sampled-Data Systems via\n  Genetic Programming Abstract: This paper presents an automatic formal controller synthesis method for\nnonlinear sampled-data systems with safety and reachability specifications.\nFundamentally, the presented method is not restricted to polynomial systems and\ncontrollers. We consider periodically switched controllers based on a Control\nLyapunov Barrier-like functions. The proposed method utilizes genetic\nprogramming to synthesize these functions as well as the controller modes.\nCorrectness of the controller are subsequently verified by means of a\nSatisfiability Modulo Theories solver. Effectiveness of the proposed\nmethodology is demonstrated on multiple systems. \n\n"}
{"id": "1812.03472", "contents": "Title: Theory of Curriculum Learning, with Convex Loss Functions Abstract: Curriculum Learning - the idea of teaching by gradually exposing the learner\nto examples in a meaningful order, from easy to hard, has been investigated in\nthe context of machine learning long ago. Although methods based on this\nconcept have been empirically shown to improve performance of several learning\nalgorithms, no theoretical analysis has been provided even for simple cases. To\naddress this shortfall, we start by formulating an ideal definition of\ndifficulty score - the loss of the optimal hypothesis at a given datapoint. We\nanalyze the possible contribution of curriculum learning based on this score in\ntwo convex problems - linear regression, and binary classification by hinge\nloss minimization. We show that in both cases, the expected convergence rate\ndecreases monotonically with the ideal difficulty score, in accordance with\nearlier empirical results. We also prove that when the ideal difficulty score\nis fixed, the convergence rate is monotonically increasing with respect to the\nloss of the current hypothesis at each point. We discuss how these results\nbring to term two apparently contradicting heuristics: curriculum learning on\nthe one hand, and hard data mining on the other. \n\n"}
{"id": "1812.03526", "contents": "Title: Path Dependent Optimal Transport and Model Calibration on Exotic\n  Derivatives Abstract: In this paper, we introduce and develop the theory of semimartingale optimal\ntransport in a path dependent setting. Instead of the classical constraints on\nmarginal distributions, we consider a general framework of path dependent\nconstraints. Duality results are established, representing the solution in\nterms of path dependent partial differential equations (PPDEs). Moreover, we\nprovide a dimension reduction result based on the new notion of\n\"semifiltrations\", which identifies appropriate Markovian state variables based\non the constraints and the cost function. Our technique is then applied to the\nexact calibration of volatility models to the prices of general path dependent\nderivatives. \n\n"}
{"id": "1812.03769", "contents": "Title: Generalized Symmetric ADMM for Separable Convex Optimization Abstract: The Alternating Direction Method of Multipliers (ADMM) has been proved to be\neffective for solving separable convex optimization subject to linear\nconstraints. In this paper, we propose a Generalized Symmetric ADMM (GS-ADMM),\nwhich updates the Lagrange multiplier twice with suitable stepsizes, to solve\nthe multi-block separable convex programming. This GS-ADMM partitions the data\ninto two group variables so that one group consists of $p$ block variables\nwhile the other has $q$ block variables, where $p \\ge 1$ and $q \\ge 1$ are two\nintegers. The two grouped variables are updated in a {\\it Gauss-Seidel} scheme,\nwhile the variables within each group are updated in a {\\it Jacobi} scheme,\nwhich would make it very attractive for a big data setting. By adding proper\nproximal terms to the subproblems, we specify the domain of the stepsizes to\nguarantee that GS-ADMM is globally convergent with a worst-case $O(1/t)$\nergodic convergence rate. It turns out that our convergence domain of the\nstepsizes is significantly larger than other convergence domains in the\nliterature. Hence, the GS-ADMM is more flexible and attractive on choosing and\nusing larger stepsizes of the dual variable. Besides, two special cases of\nGS-ADMM, which allows using zero penalty terms, are also discussed and\nanalyzed. Compared with several state-of-the-art methods, preliminary numerical\nexperiments on solving a sparse matrix minimization problem in the statistical\nlearning show that our proposed method is effective and promising. \n\n"}
{"id": "1812.03973", "contents": "Title: Bayesian Layers: A Module for Neural Network Uncertainty Abstract: We describe Bayesian Layers, a module designed for fast experimentation with\nneural network uncertainty. It extends neural network libraries with drop-in\nreplacements for common layers. This enables composition via a unified\nabstraction over deterministic and stochastic functions and allows for\nscalability via the underlying system. These layers capture uncertainty over\nweights (Bayesian neural nets), pre-activation units (dropout), activations\n(\"stochastic output layers\"), or the function itself (Gaussian processes). They\ncan also be reversible to propagate uncertainty from input to output. We\ninclude code examples for common architectures such as Bayesian LSTMs, deep\nGPs, and flow-based models. As demonstration, we fit a 5-billion parameter\n\"Bayesian Transformer\" on 512 TPUv2 cores for uncertainty in machine\ntranslation and a Bayesian dynamics model for model-based planning. Finally, we\nshow how Bayesian Layers can be used within the Edward2 probabilistic\nprogramming language for probabilistic programs with stochastic processes. \n\n"}
{"id": "1812.04048", "contents": "Title: Compressed Distributed Gradient Descent: Communication-Efficient\n  Consensus over Networks Abstract: Network consensus optimization has received increasing attention in recent\nyears and has found important applications in many scientific and engineering\nfields. To solve network consensus optimization problems, one of the most\nwell-known approaches is the distributed gradient descent method (DGD).\nHowever, in networks with slow communication rates, DGD's performance is\nunsatisfactory for solving high-dimensional network consensus problems due to\nthe communication bottleneck. This motivates us to design a\ncommunication-efficient DGD-type algorithm based on compressed information\nexchanges. Our contributions in this paper are three-fold: i) We develop a\ncommunication-efficient algorithm called amplified-differential compression DGD\n(ADC-DGD) and show that it converges under {\\em any} unbiased compression\noperator; ii) We rigorously prove the convergence performances of ADC-DGD and\nshow that they match with those of DGD without compression; iii) We reveal an\ninteresting phase transition phenomenon in the convergence speed of ADC-DGD.\nCollectively, our findings advance the state-of-the-art of network consensus\noptimization theory. \n\n"}
{"id": "1812.04168", "contents": "Title: Secure and Private Implementation of Dynamic Controllers Using\n  Semi-Homomorphic Encryption Abstract: This paper presents a secure and private implementation of linear\ntime-invariant dynamic controllers using Paillier's encryption, a\nsemi-homomorphic encryption method. To avoid overflow or underflow within the\nencryption domain, the state of the controller is reset periodically. A control\ndesign approach is presented to ensure stability and optimize performance of\nthe closed-loop system with encrypted controller. \n\n"}
{"id": "1812.04407", "contents": "Title: Learning Item-Interaction Embeddings for User Recommendations Abstract: Industry-scale recommendation systems have become a cornerstone of the\ne-commerce shopping experience. For Etsy, an online marketplace with over 50\nmillion handmade and vintage items, users come to rely on personalized\nrecommendations to surface relevant items from its massive inventory. One\nhallmark of Etsy's shopping experience is the multitude of ways in which a user\ncan interact with an item they are interested in: they can view it, favorite\nit, add it to a collection, add it to cart, purchase it, etc. We hypothesize\nthat the different ways in which a user interacts with an item indicates\ndifferent kinds of intent. Consequently, a user's recommendations should be\nbased not only on the item from their past activity, but also the way in which\nthey interacted with that item. In this paper, we propose a novel method for\nlearning interaction-based item embeddings that encode the co-occurrence\npatterns of not only the item itself, but also the interaction type. The\nlearned embeddings give us a convenient way of approximating the likelihood\nthat one item-interaction pair would co-occur with another by way of a simple\ninner product. Because of its computational efficiency, our model lends itself\nnaturally as a candidate set selection method, and we evaluate it as such in an\nindustry-scale recommendation system that serves live traffic on Etsy.com. Our\nexperiments reveal that taking interaction type into account shows promising\nresults in improving the accuracy of modeling user shopping behavior. \n\n"}
{"id": "1812.04522", "contents": "Title: Hybrid Strategies using Linear and Piecewise-Linear Decision Rules for\n  Multistage Adaptive Linear Optimization Abstract: Decision rules offer a rich and tractable framework for solving certain\nclasses of multistage adaptive optimization problems. Recent literature has\nshown the promise of using linear and nonlinear decision rules in which\nwait-and-see decisions are represented as functions, whose parameters are\ndecision variables to be optimized, of the underlying uncertain parameters.\nDespite this growing success, solving real-world stochastic optimization\nproblems can become computationally prohibitive when using nonlinear decision\nrules, and in some cases, linear ones. Consequently, decision rules that offer\na competitive trade-off between solution quality and computational time become\nmore attractive. Whereas the extant research has always used homogeneous\ndecision rules, the major contribution of this paper is a computational\nexploration of hybrid decision rules. We first verify empirically that having\nhigher uncertainty resolution or more linear pieces in early stages is more\nsignificant than having it in late stages in terms of solution quality. Then we\nconduct a comprehensive computational study for non-increasing (i.e., higher\nuncertainty resolution in early stages) and non-decreasing (i.e., higher\nuncertainty resolution in late stages) hybrid decision rules to illustrate the\ntrade-off between solution quality and computational cost. We also demonstrate\na case where a linear decision rule is superior to a piecewise-linear decision\nrule within a simulator environment, which supports the need to assess the\nquality of decision rules obtained from a look-ahead model within a simulator\nrather than just using the look-ahead model's objective function value. \n\n"}
{"id": "1812.04616", "contents": "Title: Von Mises-Fisher Loss for Training Sequence to Sequence Models with\n  Continuous Outputs Abstract: The Softmax function is used in the final layer of nearly all existing\nsequence-to-sequence models for language generation. However, it is usually the\nslowest layer to compute which limits the vocabulary size to a subset of most\nfrequent types; and it has a large memory footprint. We propose a general\ntechnique for replacing the softmax layer with a continuous embedding layer.\nOur primary innovations are a novel probabilistic loss, and a training and\ninference procedure in which we generate a probability distribution over\npre-trained word embeddings, instead of a multinomial distribution over the\nvocabulary obtained via softmax. We evaluate this new class of\nsequence-to-sequence models with continuous outputs on the task of neural\nmachine translation. We show that our models obtain upto 2.5x speed-up in\ntraining time while performing on par with the state-of-the-art models in terms\nof translation quality. These models are capable of handling very large\nvocabularies without compromising on translation quality. They also produce\nmore meaningful errors than in the softmax-based models, as these errors\ntypically lie in a subspace of the vector space of the reference translations. \n\n"}
{"id": "1812.05233", "contents": "Title: MetaStyle: Three-Way Trade-Off Among Speed, Flexibility, and Quality in\n  Neural Style Transfer Abstract: An unprecedented booming has been witnessed in the research area of artistic\nstyle transfer ever since Gatys et al. introduced the neural method. One of the\nremaining challenges is to balance a trade-off among three critical\naspects---speed, flexibility, and quality: (i) the vanilla optimization-based\nalgorithm produces impressive results for arbitrary styles, but is\nunsatisfyingly slow due to its iterative nature, (ii) the fast approximation\nmethods based on feed-forward neural networks generate satisfactory artistic\neffects but bound to only a limited number of styles, and (iii)\nfeature-matching methods like AdaIN achieve arbitrary style transfer in a\nreal-time manner but at a cost of the compromised quality. We find it\nconsiderably difficult to balance the trade-off well merely using a single\nfeed-forward step and ask, instead, whether there exists an algorithm that\ncould adapt quickly to any style, while the adapted model maintains high\nefficiency and good image quality. Motivated by this idea, we propose a novel\nmethod, coined MetaStyle, which formulates the neural style transfer as a\nbilevel optimization problem and combines learning with only a few\npost-processing update steps to adapt to a fast approximation model with\nsatisfying artistic effects, comparable to the optimization-based methods for\nan arbitrary style. The qualitative and quantitative analysis in the\nexperiments demonstrates that the proposed approach achieves high-quality\narbitrary artistic style transfer effectively, with a good trade-off among\nspeed, flexibility, and quality. \n\n"}
{"id": "1812.05243", "contents": "Title: A New Homotopy Proximal Variable-Metric Framework for Composite Convex\n  Minimization Abstract: This paper suggests two novel ideas to develop new proximal variable-metric\nmethods for solving a class of composite convex optimization problems. The\nfirst idea is a new parameterization of the optimality condition which allows\nus to develop a class of homotopy proximal variable-metric methods. We show\nthat under appropriate assumptions such as strong convexity-type and\nsmoothness, or self-concordance, our new schemes can achieve finite global\niteration-complexity bounds. Our second idea is a primal-dual-primal framework\nfor proximal-Newton methods which can lead to some useful computational\nfeatures for a subclass of nonsmooth composite convex optimization problems.\nStarting from the primal problem, we formulate its dual problem, and use our\nhomotopy proximal Newton method to solve this dual problem. Instead of solving\nthe subproblem directly in the dual space, we suggest to dualize this\nsubproblem to go back to the primal space. The resulting subproblem shares some\nsimilarity promoted by the regularizer of the original problem and leads to\nsome computational advantages. As a byproduct, we specialize the proposed\nalgorithm to solve covariance estimation problems. Surprisingly, our new\nalgorithm does not require any matrix inversion or Cholesky factorization, and\nfunction evaluation, while it works in the primal space with sparsity\nstructures that are promoted by the regularizer. Numerical examples on several\napplications are given to illustrate our theoretical development and to compare\nwith state-of-the-arts. \n\n"}
{"id": "1812.05990", "contents": "Title: Aggregate Power Flexibility in Unbalanced Distribution Systems Abstract: With a large-scale integration of distributed energy resources (DERs),\ndistribution systems are expected to be capable of providing capacity support\nfor the transmission grid. To effectively harness the collective flexibility\nfrom massive DER devices, this paper studies distribution-level power\naggregation strategies for transmission-distribution interaction. In\nparticular, this paper proposes a method to model and quantify the aggregate\npower flexibility, i.e., the net power injection achievable at the substation,\nin unbalanced distribution systems over time. Incorporating the network\nconstraints and multi-phase unbalanced modeling, the proposed method obtains an\neffective approximate feasible region of the net power injection. For any\naggregate power trajectory within this region, it is proved that there exists a\nfeasible disaggregation solution. In addition, a distributed model predictive\ncontrol (MPC) framework is developed for the practical implementation of the\ntransmission-distribution interaction. At last, we demonstrate the performances\nof the proposed method via numerical tests on a real-world distribution feeder\nwith 126 multi-phase nodes. \n\n"}
{"id": "1812.06562", "contents": "Title: A Robust Deep Learning Approach for Automatic Classification of Seizures\n  Against Non-seizures Abstract: Identifying epileptic seizures through analysis of the electroencephalography\n(EEG) signal becomes a standard method for the diagnosis of epilepsy. Manual\nseizure identification on EEG by trained neurologists is time-consuming,\nlabor-intensive and error-prone, and a reliable automatic seizure/non-seizure\nclassification method is needed. One of the challenges in automatic\nseizure/non-seizure classification is that seizure morphologies exhibit\nconsiderable variabilities. In order to capture essential seizure patterns,\nthis paper leverages an attention mechanism and a bidirectional long short-term\nmemory (BiLSTM) to exploit both spatial and temporal discriminating features\nand overcome seizure variabilities. The attention mechanism is to capture\nspatial features according to the contributions of different brain regions to\nseizures. The BiLSTM is to extract discriminating temporal features in the\nforward and the backward directions. Cross-validation experiments and\ncross-patient experiments over the noisy data of CHB-MIT are performed to\nevaluate our proposed approach. The obtained average sensitivity of 87.00%,\nspecificity of 88.60% and precision of 88.63% in cross-validation experiments\nare higher than using the current state-of-the-art methods, and the standard\ndeviations of our approach are lower. The evaluation results of cross-patient\nexperiments indicate that, our approach has better performance compared with\nthe current state-of-the-art methods and is more robust across patients. \n\n"}
{"id": "1812.07011", "contents": "Title: Protocol for Energy-Efficiency using Robust Control on WSN Abstract: The present work analyzes the feasibility of obtaining a single controller\n(robust), with theoretical guarantees of stability and performance, valid for a\ntotal set of network configurations in designed the controller for an uncertain\nsuccess probability obtain the protocol for Energy-Efficiency in Networked\nControl System NCS. In particular, this work investigates the performance\ndegradation, in terms of the $\\mathcal{H}_{\\infty}$ guaranteed cost, between\noptimal controller design (precisely known probability) and the sub-optimal\ncontroller design (robust to probability uncertainties). The feasibility of the\nproposed methodology is validated by a numerical example. \n\n"}
{"id": "1812.07903", "contents": "Title: An Empirical Evaluation of Sketched SVD and its Application to Leverage\n  Score Ordering Abstract: The power of randomized algorithms in numerical methods have led to fast\nsolutions which use the Singular Value Decomposition (SVD) as a core routine.\nHowever, given the large data size of modern and the modest runtime of SVD,\nmost practical algorithms would require some form of approximation, such as\nsketching, when running SVD. While these approximation methods satisfy many\ntheoretical guarantees, we provide the first algorithmic implementations for\nsketch-and-solve SVD problems on real-world, large-scale datasets. We provide a\ncomprehensive empirical evaluation of these algorithms and provide guidelines\non how to ensure accurate deployment to real-world data. As an application of\nsketched SVD, we present Sketched Leverage Score Ordering, a technique for\ndetermining the ordering of data in the training of neural networks. Our\ntechnique is based on the distributed computation of leverage scores using\nrandom projections. These computed leverage scores provide a flexible and\nefficient method to determine the optimal ordering of training data without\nmanual intervention or annotations. We present empirical results on an\nextensive set of experiments across image classification, language sentiment\nanalysis, and multi-modal sentiment analysis. Our method is faster compared to\nstandard randomized projection algorithms and shows improvements in convergence\nand results. \n\n"}
{"id": "1812.08066", "contents": "Title: Feedback, Dynamics, and Optimal Control in Climate Economics Abstract: For his work in the economics of climate change, Professor William Nordhaus\nwas a co-recipient of the 2018 Nobel Memorial Prize for Economic Sciences. A\ncore component of the work undertaken by Nordhaus is the Dynamic Integrated\nmodel of Climate and Economy, known as the DICE model. The DICE model is a\ndiscrete-time model with two control inputs and is primarily used in\nconjunction with a particular optimal control problem in order to estimate\noptimal pathways for reducing greenhouse gas emissions. In this paper, we\nprovide a tutorial introduction to the DICE model and we indicate challenges\nand open problems of potential interest for the systems and control community. \n\n"}
{"id": "1812.08733", "contents": "Title: Heteroscedastic Gaussian processes for uncertainty modeling in\n  large-scale crowdsourced traffic data Abstract: Accurately modeling traffic speeds is a fundamental part of efficient\nintelligent transportation systems. Nowadays, with the widespread deployment of\nGPS-enabled devices, it has become possible to crowdsource the collection of\nspeed information to road users (e.g. through mobile applications or dedicated\nin-vehicle devices). Despite its rather wide spatial coverage, crowdsourced\nspeed data also brings very important challenges, such as the highly variable\nmeasurement noise in the data due to a variety of driving behaviors and sample\nsizes. When not properly accounted for, this noise can severely compromise any\napplication that relies on accurate traffic data. In this article, we propose\nthe use of heteroscedastic Gaussian processes (HGP) to model the time-varying\nuncertainty in large-scale crowdsourced traffic data. Furthermore, we develop a\nHGP conditioned on sample size and traffic regime (SRC-HGP), which makes use of\nsample size information (probe vehicles per minute) as well as previous\nobserved speeds, in order to more accurately model the uncertainty in observed\nspeeds. Using 6 months of crowdsourced traffic data from Copenhagen, we\nempirically show that the proposed heteroscedastic models produce significantly\nbetter predictive distributions when compared to current state-of-the-art\nmethods for both speed imputation and short-term forecasting tasks. \n\n"}
{"id": "1812.09404", "contents": "Title: Derandomized Distributed Multi-resource Allocation with Little\n  Communication Overhead Abstract: We study a class of distributed optimization problems for multiple shared\nresource allocation in Internet-connected devices. We propose a derandomized\nversion of an existing stochastic additive-increase and multiplicative-decrease\n(AIMD) algorithm. The proposed solution uses one bit feedback signal for each\nresource between the system and the Internet-connected devices and does not\nrequire inter-device communication. Additionally, the Internet-connected\ndevices do not compromise their privacy and the solution does not dependent on\nthe number of participating devices. In the system, each Internet-connected\ndevice has private cost functions which are strictly convex, twice continuously\ndifferentiable and increasing. We show empirically that the long-term average\nallocations of multiple shared resources converge to optimal allocations and\nthe system achieves minimum social cost. Furthermore, we show that the proposed\nderandomized AIMD algorithm converges faster than the stochastic AIMD algorithm\nand both the approaches provide approximately same solutions. \n\n"}
{"id": "1812.10588", "contents": "Title: Synthesizing Robust Domains of Attraction for State-Constrained\n  Perturbed Polynomial Systems Abstract: In this paper we propose a novel semi-definite programming based method to\ncompute robust domains of attraction for state-constrained perturbed polynomial\nsystems. A robust domain of attraction is a set of states such that every\ntrajectory starting from it will approach an equilibrium while never violating\na specified state constraint, regardless of the actual perturbation. The\nsemi-definite program is constructed by relaxing a generalized Zubov's\nequation. The existence of solutions to the constructed semi-definite program\nis guaranteed and there exists a sequence of solutions such that their strict\none sub-level sets inner-approximate the interior of the maximal robust domain\nof attraction in measure under appropriate assumptions. Some illustrative\nexamples demonstrate the performance of our method. \n\n"}
{"id": "1812.11293", "contents": "Title: DeGroot-Friedkin Map in Opinion Dynamics is Mirror Descent Abstract: We provide a variational interpretation of the DeGroot-Friedkin map in\nopinion dynamics. Specifically, we show that the nonlinear dynamics for the\nDeGroot-Friedkin map can be viewed as mirror descent on the standard simplex\nwith the associated Bregman divergence being equal to the generalized\nKullback-Leibler divergence, i.e., an entropic mirror descent. Our results\nreveal that the DeGroot-Friedkin map elicits an individual's social power to be\nclose to her social influence while minimizing the so called \"extropy\" -- the\nentropy of the complimentary opinion. \n\n"}
{"id": "1812.11600", "contents": "Title: Constrained Inverse Optimal Control with Application to a Human\n  Manipulation Task Abstract: This paper presents an inverse optimal control methodology and its\napplication to training a predictive model of human motor control from a\nmanipulation task. It introduces a convex formulation for learning both\nobjective function and constraints of an infinite-horizon constrained optimal\ncontrol problem with nonlinear system dynamics. The inverse approach utilizes\nBellman's principle of optimality to formulate the infinite-horizon optimal\ncontrol problem as a shortest path problem and Lagrange multipliers to identify\nconstraints. We highlight the key benefit of using the shortest path\nformulation, i.e., the possibility of training the predictive model with short\nand selected trajectory segments. The method is applied to training a\npredictive model of movements of a human subject from a manipulation task. The\nstudy indicates that individual human movements can be predicted with low error\nusing an infinite-horizon optimal control problem with constraints on shoulder\nmovement. \n\n"}
{"id": "1812.11796", "contents": "Title: On Positive Duality Gaps in Semidefinite Programming Abstract: We present a novel analysis of semidefinite programs (SDPs) with positive\nduality gaps, i.e. different optimal values in the primal and dual problems.\nThese SDPs are extremely pathological, often unsolvable, and also serve as\nmodels of more general pathological convex programs. However, despite their\nallure, they are not well understood even when they have just two variables.\n  We first completely characterize two variable SDPs with positive gaps; in\nparticular, we transform them into a standard form that makes the positive gap\ntrivial to recognize. The transformation is very simple, as it mostly uses\nelementary row operations coming from Gaussian elimination. We next show that\nthe two variable case sheds light on larger SDPs with positive gaps: we present\nSDPs in any dimension in which the positive gap is caused by the same structure\nas in the two variable case. We analyze a fundamental parameter, the {\\em\nsingularity degree} of the duals of our SDPs, and show that it is the largest\nthat can result in a positive gap.\n  We finally generate a library of difficult SDPs with positive gaps (some of\nthese SDPs have only two variables) and present a computational study. \n\n"}
{"id": "1901.00248", "contents": "Title: A Survey on Multi-output Learning Abstract: Multi-output learning aims to simultaneously predict multiple outputs given\nan input. It is an important learning problem due to the pressing need for\nsophisticated decision making in real-world applications. Inspired by big data,\nthe 4Vs characteristics of multi-output imposes a set of challenges to\nmulti-output learning, in terms of the volume, velocity, variety and veracity\nof the outputs. Increasing number of works in the literature have been devoted\nto the study of multi-output learning and the development of novel approaches\nfor addressing the challenges encountered. However, it lacks a comprehensive\noverview on different types of challenges of multi-output learning brought by\nthe characteristics of the multiple outputs and the techniques proposed to\novercome the challenges. This paper thus attempts to fill in this gap to\nprovide a comprehensive review on this area. We first introduce different\nstages of the life cycle of the output labels. Then we present the paradigm on\nmulti-output learning, including its myriads of output structures, definitions\nof its different sub-problems, model evaluation metrics and popular data\nrepositories used in the study. Subsequently, we review a number of\nstate-of-the-art multi-output learning methods, which are categorized based on\nthe challenges. \n\n"}
{"id": "1901.01590", "contents": "Title: Improving Unsupervised Word-by-Word Translation with Language Model and\n  Denoising Autoencoder Abstract: Unsupervised learning of cross-lingual word embedding offers elegant matching\nof words across languages, but has fundamental limitations in translating\nsentences. In this paper, we propose simple yet effective methods to improve\nword-by-word translation of cross-lingual embeddings, using only monolingual\ncorpora but without any back-translation. We integrate a language model for\ncontext-aware search, and use a novel denoising autoencoder to handle\nreordering. Our system surpasses state-of-the-art unsupervised neural\ntranslation systems without costly iterative training. We also analyze the\neffect of vocabulary size and denoising type on the translation performance,\nwhich provides better understanding of learning the cross-lingual word\nembedding and its usage in translation. \n\n"}
{"id": "1901.02070", "contents": "Title: Convolutional Neural Networks on non-uniform geometrical signals using\n  Euclidean spectral transformation Abstract: Convolutional Neural Networks (CNN) have been successful in processing data\nsignals that are uniformly sampled in the spatial domain (e.g., images).\nHowever, most data signals do not natively exist on a grid, and in the process\nof being sampled onto a uniform physical grid suffer significant aliasing error\nand information loss. Moreover, signals can exist in different topological\nstructures as, for example, points, lines, surfaces and volumes. It has been\nchallenging to analyze signals with mixed topologies (for example, point cloud\nwith surface mesh). To this end, we develop mathematical formulations for\nNon-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample\nnonuniform data signals of different topologies defined on a simplex mesh into\nthe spectral domain with no spatial sampling error. The spectral transform is\nperformed in the Euclidean space, which removes the translation ambiguity from\nworks on the graph spectrum. Our representation has four distinct advantages:\n(1) the process causes no spatial sampling error during the initial sampling,\n(2) the generality of this approach provides a unified framework for using CNNs\nto analyze signals of mixed topologies, (3) it allows us to leverage\nstate-of-the-art backbone CNN architectures for effective learning without\nhaving to design a particular architecture for a particular data structure in\nan ad-hoc fashion, and (4) the representation allows weighted meshes where each\nelement has a different weight (i.e., texture) indicating local properties. We\nachieve results on par with the state-of-the-art for the 3D shape retrieval\ntask, and a new state-of-the-art for the point cloud to surface reconstruction\ntask. \n\n"}
{"id": "1901.02324", "contents": "Title: Learning with Fenchel-Young Losses Abstract: Over the past decades, numerous loss functions have been been proposed for a\nvariety of supervised learning tasks, including regression, classification,\nranking, and more generally structured prediction. Understanding the core\nprinciples and theoretical properties underpinning these losses is key to\nchoose the right loss for the right problem, as well as to create new losses\nwhich combine their strengths. In this paper, we introduce Fenchel-Young\nlosses, a generic way to construct a convex loss function for a regularized\nprediction function. We provide an in-depth study of their properties in a very\nbroad setting, covering all the aforementioned supervised learning tasks, and\nrevealing new connections between sparsity, generalized entropies, and\nseparation margins. We show that Fenchel-Young losses unify many well-known\nloss functions and allow to create useful new ones easily. Finally, we derive\nefficient predictive and training algorithms, making Fenchel-Young losses\nappealing both in theory and practice. \n\n"}
{"id": "1901.02369", "contents": "Title: Learning the optimal state-feedback via supervised imitation learning Abstract: Imitation learning is a control design paradigm that seeks to learn a control\npolicy reproducing demonstrations from expert agents. By substituting expert\ndemonstrations for optimal behaviours, the same paradigm leads to the design of\ncontrol policies closely approximating the optimal state-feedback. This\napproach requires training a machine learning algorithm (in our case deep\nneural networks) directly on state-control pairs originating from optimal\ntrajectories. We have shown in previous work that, when restricted to\nlow-dimensional state and control spaces, this approach is very successful in\nseveral deterministic, non-linear problems in continuous-time. In this work, we\nrefine our previous studies using as a test case a simple quadcopter model with\nquadratic and time-optimal objective functions. We describe in detail the best\nlearning pipeline we have developed, that is able to approximate via deep\nneural networks the state-feedback map to a very high accuracy. We introduce\nthe use of the softplus activation function in the hidden units of neural\nnetworks showing that it results in a smoother control profile whilst retaining\nthe benefits of rectifiers. We show how to evaluate the optimality of the\ntrained state-feedback, and find that already with two layers the objective\nfunction reached and its optimal value differ by less than one percent. We\nlater consider also an additional metric linked to the system asymptotic\nbehaviour - time taken to converge to the policy's fixed point. With respect to\nthese metrics, we show that improvements in the mean absolute error do not\nnecessarily correspond to better policies. \n\n"}
{"id": "1901.02928", "contents": "Title: Beyond the EM Algorithm: Constrained Optimization Methods for Latent\n  Class Model Abstract: Latent class model (LCM), which is a finite mixture of different categorical\ndistributions, is one of the most widely used models in statistics and machine\nlearning fields. Because of its non-continuous nature and the flexibility in\nshape, researchers in practice areas such as marketing and social sciences also\nfrequently use LCM to gain insights from their data. One likelihood-based\nmethod, the Expectation-Maximization (EM) algorithm, is often used to obtain\nthe model estimators. However, the EM algorithm is well-known for its\nnotoriously slow convergence. In this research, we explore alternative\nlikelihood-based methods that can potential remedy the slow convergence of the\nEM algorithm. More specifically, we regard likelihood-based approach as a\nconstrained nonlinear optimization problem, and apply quasi-Newton type methods\nto solve them. We examine two different constrained optimization methods to\nmaximize the log likelihood function. We present simulation study results to\nshow that the proposed methods not only converge in less iterations than the EM\nalgorithm but also produce more accurate model estimators. \n\n"}
{"id": "1901.04420", "contents": "Title: Data Augmentation with Manifold Exploring Geometric Transformations for\n  Increased Performance and Robustness Abstract: In this paper we propose a novel augmentation technique that improves not\nonly the performance of deep neural networks on clean test data, but also\nsignificantly increases their robustness to random transformations, both affine\nand projective. Inspired by ManiFool, the augmentation is performed by a\nline-search manifold-exploration method that learns affine geometric\ntransformations that lead to the misclassification on an image, while ensuring\nthat it remains on the same manifold as the training data.\n  This augmentation method populates any training dataset with images that lie\non the border of the manifolds between two-classes and maximizes the variance\nthe network is exposed to during training. Our method was thoroughly evaluated\non the challenging tasks of fine-grained skin lesion classification from\nlimited data, and breast tumor classification of mammograms. Compared with\ntraditional augmentation methods, and with images synthesized by Generative\nAdversarial Networks our method not only achieves state-of-the-art performance\nbut also significantly improves the network's robustness. \n\n"}
{"id": "1901.04966", "contents": "Title: Identifying and Correcting Label Bias in Machine Learning Abstract: Datasets often contain biases which unfairly disadvantage certain groups, and\nclassifiers trained on such datasets can inherit these biases. In this paper,\nwe provide a mathematical formulation of how this bias can arise. We do so by\nassuming the existence of underlying, unknown, and unbiased labels which are\noverwritten by an agent who intends to provide accurate labels but may have\nbiases against certain groups. Despite the fact that we only observe the biased\nlabels, we are able to show that the bias may nevertheless be corrected by\nre-weighting the data points without changing the labels. We show, with\ntheoretical guarantees, that training on the re-weighted dataset corresponds to\ntraining on the unobserved but unbiased labels, thus leading to an unbiased\nmachine learning classifier. Our procedure is fast and robust and can be used\nwith virtually any learning algorithm. We evaluate on a number of standard\nmachine learning fairness datasets and a variety of fairness notions, finding\nthat our method outperforms standard approaches in achieving fair\nclassification. \n\n"}
{"id": "1901.05850", "contents": "Title: Fast Deep Learning for Automatic Modulation Classification Abstract: In this work, we investigate the feasibility and effectiveness of employing\ndeep learning algorithms for automatic recognition of the modulation type of\nreceived wireless communication signals from subsampled data. Recent work\nconsidered a GNU radio-based data set that mimics the imperfections in a real\nwireless channel and uses 10 different modulation types. A Convolutional Neural\nNetwork (CNN) architecture was then developed and shown to achieve performance\nthat exceeds that of expert-based approaches. Here, we continue this line of\nwork and investigate deep neural network architectures that deliver high\nclassification accuracy. We identify three architectures - namely, a\nConvolutional Long Short-term Deep Neural Network (CLDNN), a Long Short-Term\nMemory neural network (LSTM), and a deep Residual Network (ResNet) - that lead\nto typical classification accuracy values around 90% at high SNR. We then study\nalgorithms to reduce the training time by minimizing the size of the training\ndata set, while incurring a minimal loss in classification accuracy. To this\nend, we demonstrate the performance of Principal Component Analysis in\nsignificantly reducing the training time, while maintaining good performance at\nlow SNR. We also investigate subsampling techniques that further reduce the\ntraining time, and pave the way for online classification at high SNR. Finally,\nwe identify representative SNR values for training each of the candidate\narchitectures, and consequently, realize drastic reductions of the training\ntime, with negligible loss in classification accuracy. \n\n"}
{"id": "1901.06455", "contents": "Title: Lifelong Federated Reinforcement Learning: A Learning Architecture for\n  Navigation in Cloud Robotic Systems Abstract: This paper was motivated by the problem of how to make robots fuse and\ntransfer their experience so that they can effectively use prior knowledge and\nquickly adapt to new environments. To address the problem, we present a\nlearning architecture for navigation in cloud robotic systems: Lifelong\nFederated Reinforcement Learning (LFRL). In the work, We propose a knowledge\nfusion algorithm for upgrading a shared model deployed on the cloud. Then,\neffective transfer learning methods in LFRL are introduced. LFRL is consistent\nwith human cognitive science and fits well in cloud robotic systems.\nExperiments show that LFRL greatly improves the efficiency of reinforcement\nlearning for robot navigation. The cloud robotic system deployment also shows\nthat LFRL is capable of fusing prior knowledge. In addition, we release a cloud\nrobotic navigation-learning website based on LFRL. \n\n"}
{"id": "1901.07377", "contents": "Title: Data assimilation and online optimization with performance guarantees Abstract: This paper considers a class of real-time stochastic optimization problems\ndependent on an unknown probability distribution. In the considered scenario,\ndata is streaming frequently while trying to reach a decision. Thus, we aim to\ndevise a procedure that incorporates samples (data) of the distribution\nsequentially and adjusts decisions accordingly. We approach this problem in a\ndistributionally robust optimization framework and propose a novel Online Data\nAssimilation Algorithm (ONDA Algorithm) for this purpose. This algorithm\nguarantees out-of-sample performance of decisions with high probability, and\ngradually improves the quality of the decisions by incorporating the streaming\ndata. We show that the ONDA Algorithm converges under a sufficiently slow data\nstreaming rate, and provide a criteria for its termination after certain number\nof data have been collected. Simulations illustrate the results. \n\n"}
{"id": "1901.07531", "contents": "Title: Resource-aware IoT Control: Saving Communication through Predictive\n  Triggering Abstract: The Internet of Things (IoT) interconnects multiple physical devices in\nlarge-scale networks. When the 'things' coordinate decisions and act\ncollectively on shared information, feedback is introduced between them.\nMultiple feedback loops are thus closed over a shared, general-purpose network.\nTraditional feedback control is unsuitable for design of IoT control because it\nrelies on high-rate periodic communication and is ignorant of the shared\nnetwork resource. Therefore, recent event-based estimation methods are applied\nherein for resource-aware IoT control allowing agents to decide online whether\ncommunication with other agents is needed, or not. While this can reduce\nnetwork traffic significantly, a severe limitation of typical event-based\napproaches is the need for instantaneous triggering decisions that leave no\ntime to reallocate freed resources (e.g., communication slots), which hence\nremain unused. To address this problem, novel predictive and self triggering\nprotocols are proposed herein. From a unified Bayesian decision framework, two\nschemes are developed: self triggers that predict, at the current triggering\ninstant, the next one; and predictive triggers that check at every time step,\nwhether communication will be needed at a given prediction horizon. The\nsuitability of these triggers for feedback control is demonstrated in hardware\nexperiments on a cart-pole, and scalability is discussed with a multi-vehicle\nsimulation. \n\n"}
{"id": "1901.07821", "contents": "Title: Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff Abstract: Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example. \n\n"}
{"id": "1901.07977", "contents": "Title: Coupling the reduced-order model and the generative model for an\n  importance sampling estimator Abstract: In this work, we develop an importance sampling estimator by coupling the\nreduced-order model and the generative model in a problem setting of\nuncertainty quantification. The target is to estimate the probability that the\nquantity of interest (QoI) in a complex system is beyond a given threshold. To\navoid the prohibitive cost of sampling a large scale system, the reduced-order\nmodel is usually considered for a trade-off between efficiency and accuracy.\nHowever, the Monte Carlo estimator given by the reduced-order model is biased\ndue to the error from dimension reduction. To correct the bias, we still need\nto sample the fine model. An effective technique to reduce the variance\nreduction is importance sampling, where we employ the generative model to\nestimate the distribution of the data from the reduced-order model and use it\nfor the change of measure in the importance sampling estimator. To compensate\nthe approximation errors of the reduced-order model, more data that induce a\nslightly smaller QoI than the threshold need to be included into the training\nset. Although the amount of these data can be controlled by a posterior error\nestimate, redundant data, which may outnumber the effective data, will be kept\ndue to the epistemic uncertainty. To deal with this issue, we introduce a\nweighted empirical distribution to process the data from the reduced-order\nmodel. The generative model is then trained by minimizing the cross entropy\nbetween it and the weighted empirical distribution. We also introduce a penalty\nterm into the objective function to deal with the overfitting for more\nrobustness. Numerical results are presented to demonstrate the effectiveness of\nthe proposed methodology. \n\n"}
{"id": "1901.08022", "contents": "Title: A Universally Optimal Multistage Accelerated Stochastic Gradient Method Abstract: We study the problem of minimizing a strongly convex, smooth function when we\nhave noisy estimates of its gradient. We propose a novel multistage accelerated\nalgorithm that is universally optimal in the sense that it achieves the optimal\nrate both in the deterministic and stochastic case and operates without\nknowledge of noise characteristics. The algorithm consists of stages that use a\nstochastic version of Nesterov's method with a specific restart and parameters\nselected to achieve the fastest reduction in the bias-variance terms in the\nconvergence rate bounds. \n\n"}
{"id": "1901.08087", "contents": "Title: Model Function Based Conditional Gradient Method with Armijo-like Line\n  Search Abstract: The Conditional Gradient Method is generalized to a class of non-smooth\nnon-convex optimization problems with many applications in machine learning.\nThe proposed algorithm iterates by minimizing so-called model functions over\nthe constraint set. Complemented with an Amijo line search procedure, we prove\nthat subsequences converge to a stationary point. The abstract framework of\nmodel functions provides great flexibility for the design of concrete\nalgorithms. As special cases, for example, we develop an algorithm for additive\ncomposite problems and an algorithm for non-linear composite problems which\nleads to a Gauss--Newton-type algorithm. Both instances are novel in non-smooth\nnon-convex optimization and come with numerous applications in machine\nlearning. Moreover, we obtain a hybrid version of Conditional Gradient and\nProximal Minimization schemes for free, which combines advantages of both. Our\nalgorithm is shown to perform favorably on a sparse non-linear robust\nregression problem and we discuss the flexibility of the proposed framework in\nseveral matrix factorization formulations. \n\n"}
{"id": "1901.08508", "contents": "Title: Maximum Entropy Generators for Energy-Based Models Abstract: Maximum likelihood estimation of energy-based models is a challenging problem\ndue to the intractability of the log-likelihood gradient. In this work, we\npropose learning both the energy function and an amortized approximate sampling\nmechanism using a neural generator network, which provides an efficient\napproximation of the log-likelihood gradient. The resulting objective requires\nmaximizing entropy of the generated samples, which we perform using recently\nproposed nonparametric mutual information estimators. Finally, to stabilize the\nresulting adversarial game, we use a zero-centered gradient penalty derived as\na necessary condition from the score matching literature. The proposed\ntechnique can generate sharp images with Inception and FID scores competitive\nwith recent GAN techniques, does not suffer from mode collapse, and is\ncompetitive with state-of-the-art anomaly detection techniques. \n\n"}
{"id": "1901.08583", "contents": "Title: Memory-free dynamics for the TAP equations of Ising models with\n  arbitrary rotation invariant ensembles of random coupling matrices Abstract: We propose an iterative algorithm for solving the Thouless-Anderson-Palmer\n(TAP) equations of Ising models with arbitrary rotation invariant (random)\ncoupling matrices. In the thermodynamic limit, we prove by means of the\ndynamical functional method that the proposed algorithm converges when the\nso-called de Almeida Thouless (AT) criterion is fulfilled. Moreover, we give\nexact analytical expressions for the rate of the convergence. \n\n"}
{"id": "1901.08663", "contents": "Title: New nonasymptotic convergence rates of stochastic proximal\n  pointalgorithm for convex optimization problems Abstract: Large sectors of the recent optimization literature focused in the last\ndecade on the development of optimal stochastic first order schemes for\nconstrained convex models under progressively relaxed assumptions. Stochastic\nproximal point is an iterative scheme born from the adaptation of proximal\npoint algorithm to noisy stochastic optimization, with a resulting iteration\nrelated to stochastic alternating projections. Inspired by the scalability of\nalternating projection methods, we start from the (linear) regularity\nassumption, typically used in convex feasiblity problems to guarantee the\nlinear convergence of stochastic alternating projection methods, and analyze a\ngeneral weak linear regularity condition which facilitates convergence rate\nboosts in stochastic proximal point schemes. Our applications include many\nnon-strongly convex functions classes often used in machine learning and\nstatistics. Moreover, under weak linear regularity assumption we guarantee\n$\\mathcal{O}\\left(\\frac{1}{k}\\right)$ convergence rate for SPP, in terms of the\ndistance to the optimal set, using only projections onto a simple component\nset. Linear convergence is obtained for interpolation setting, when the optimal\nset of the expected cost is included into the optimal sets of each functional\ncomponent. \n\n"}
{"id": "1901.08669", "contents": "Title: SAGA with Arbitrary Sampling Abstract: We study the problem of minimizing the average of a very large number of\nsmooth functions, which is of key importance in training supervised learning\nmodels. One of the most celebrated methods in this context is the SAGA\nalgorithm. Despite years of research on the topic, a general-purpose version of\nSAGA---one that would include arbitrary importance sampling and minibatching\nschemes---does not exist. We remedy this situation and propose a general and\nflexible variant of SAGA following the {\\em arbitrary sampling} paradigm. We\nperform an iteration complexity analysis of the method, largely possible due to\nthe construction of new stochastic Lyapunov functions. We establish linear\nconvergence rates in the smooth and strongly convex regime, and under a\nquadratic functional growth condition (i.e., in a regime not assuming strong\nconvexity). Our rates match those of the primal-dual method Quartz for which an\narbitrary sampling analysis is available, which makes a significant step\ntowards closing the gap in our understanding of complexity of primal and dual\nmethods for finite sum problems. \n\n"}
{"id": "1901.09109", "contents": "Title: DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online\n  Optimization Abstract: Adaptive gradient-based optimization methods such as \\textsc{Adagrad},\n\\textsc{Rmsprop}, and \\textsc{Adam} are widely used in solving large-scale\nmachine learning problems including deep learning. A number of schemes have\nbeen proposed in the literature aiming at parallelizing them, based on\ncommunications of peripheral nodes with a central node, but incur high\ncommunications cost. To address this issue, we develop a novel consensus-based\ndistributed adaptive moment estimation method (\\textsc{Dadam}) for online\noptimization over a decentralized network that enables data parallelization, as\nwell as decentralized computation. The method is particularly useful, since it\ncan accommodate settings where access to local data is allowed. Further, as\nestablished theoretically in this work, it can outperform centralized adaptive\nalgorithms, for certain classes of loss functions used in applications. We\nanalyze the convergence properties of the proposed algorithm and provide a\ndynamic regret bound on the convergence rate of adaptive moment estimation\nmethods in both stochastic and deterministic settings. Empirical results\ndemonstrate that \\textsc{Dadam} works also well in practice and compares\nfavorably to competing online optimization methods. \n\n"}
{"id": "1901.09149", "contents": "Title: Escaping Saddle Points with Adaptive Gradient Methods Abstract: Adaptive methods such as Adam and RMSProp are widely used in deep learning\nbut are not well understood. In this paper, we seek a crisp, clean and precise\ncharacterization of their behavior in nonconvex settings. To this end, we first\nprovide a novel view of adaptive methods as preconditioned SGD, where the\npreconditioner is estimated in an online manner. By studying the preconditioner\non its own, we elucidate its purpose: it rescales the stochastic gradient noise\nto be isotropic near stationary points, which helps escape saddle points.\nFurthermore, we show that adaptive methods can efficiently estimate the\naforementioned preconditioner. By gluing together these two components, we\nprovide the first (to our knowledge) second-order convergence result for any\nadaptive method. The key insight from our analysis is that, compared to SGD,\nadaptive methods escape saddle points faster, and can converge faster overall\nto second-order stationary points. \n\n"}
{"id": "1901.09203", "contents": "Title: ACNN: a Full Resolution DCNN for Medical Image Segmentation Abstract: Deep Convolutional Neural Networks (DCNNs) are used extensively in medical\nimage segmentation and hence 3D navigation for robot-assisted Minimally\nInvasive Surgeries (MISs). However, current DCNNs usually use down sampling\nlayers for increasing the receptive field and gaining abstract semantic\ninformation. These down sampling layers decrease the spatial dimension of\nfeature maps, which can be detrimental to image segmentation. Atrous\nconvolution is an alternative for the down sampling layer. It increases the\nreceptive field whilst maintains the spatial dimension of feature maps. In this\npaper, a method for effective atrous rate setting is proposed to achieve the\nlargest and fully-covered receptive field with a minimum number of atrous\nconvolutional layers. Furthermore, a new and full resolution DCNN - Atrous\nConvolutional Neural Network (ACNN), which incorporates cascaded atrous\nII-blocks, residual learning and Instance Normalization (IN) is proposed.\nApplication results of the proposed ACNN to Magnetic Resonance Imaging (MRI)\nand Computed Tomography (CT) image segmentation demonstrate that the proposed\nACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net\nand Deeplabv3+, but with reduced trainable parameters. \n\n"}
{"id": "1901.09207", "contents": "Title: Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning Abstract: Humans are capable of attributing latent mental contents such as beliefs or\nintentions to others. The social skill is critical in daily life for reasoning\nabout the potential consequences of others' behaviors so as to plan ahead. It\nis known that humans use such reasoning ability recursively by considering what\nothers believe about their own beliefs. In this paper, we start from level-$1$\nrecursion and introduce a probabilistic recursive reasoning (PR2) framework for\nmulti-agent reinforcement learning. Our hypothesis is that it is beneficial for\neach agent to account for how the opponents would react to its future\nbehaviors. Under the PR2 framework, we adopt variational Bayes methods to\napproximate the opponents' conditional policies, to which each agent finds the\nbest response and then improve their own policies. We develop\ndecentralized-training-decentralized-execution algorithms, namely PR2-Q and\nPR2-Actor-Critic, that are proved to converge in the self-play scenarios when\nthere exists one Nash equilibrium. Our methods are tested on both the matrix\ngame and the differential game, which have a non-trivial equilibrium where\ncommon gradient-based methods fail to converge. Our experiments show that it is\ncritical to reason about how the opponents believe about what the agent\nbelieves. We expect our work to contribute a new idea of modeling the opponents\nto the multi-agent reinforcement learning community. \n\n"}
{"id": "1901.09583", "contents": "Title: ML for Flood Forecasting at Scale Abstract: Effective riverine flood forecasting at scale is hindered by a multitude of\nfactors, most notably the need to rely on human calibration in current\nmethodology, the limited amount of data for a specific location, and the\ncomputational difficulty of building continent/global level models that are\nsufficiently accurate. Machine learning (ML) is primed to be useful in this\nscenario: learned models often surpass human experts in complex\nhigh-dimensional scenarios, and the framework of transfer or multitask learning\nis an appealing solution for leveraging local signals to achieve improved\nglobal performance. We propose to build on these strengths and develop ML\nsystems for timely and accurate riverine flood prediction. \n\n"}
{"id": "1901.09674", "contents": "Title: Deep Generative Graph Distribution Learning for Synthetic Power Grids Abstract: Power system studies require the topological structures of real-world power\nnetworks; however, such data is confidential due to important security\nconcerns. Thus, power grid synthesis (PGS), i.e., creating realistic power\ngrids that imitate actual power networks, has gained significant attention. In\nthis letter, we cast PGS into a graph distribution learning (GDL) problem where\nthe probability distribution functions (PDFs) of the nodes (buses) and edges\n(lines) are captured. A novel deep GDL (DeepGDL) model is proposed to learn the\ntopological patterns of buses/lines with their physical features (e.g., power\ninjection and line impedance). Having a deep nonlinear recurrent structure,\nDeepGDL understands complex nonlinear topological properties and captures the\ngraph PDF. Sampling from the obtained PDF, we are able to create a large set of\nrealistic networks that all resemble the original power grid. Simulation\nresults show the significant accuracy of our created synthetic power grids in\nterms of various topological metrics and power flow measurements. \n\n"}
{"id": "1901.10500", "contents": "Title: Discretizing Continuous Action Space for On-Policy Optimization Abstract: In this work, we show that discretizing action space for continuous control\nis a simple yet powerful technique for on-policy optimization. The explosion in\nthe number of discrete actions can be efficiently addressed by a policy with\nfactorized distribution across action dimensions. We show that the discrete\npolicy achieves significant performance gains with state-of-the-art on-policy\noptimization algorithms (PPO, TRPO, ACKTR) especially on high-dimensional tasks\nwith complex dynamics. Additionally, we show that an ordinal parameterization\nof the discrete distribution can introduce the inductive bias that encodes the\nnatural ordering between discrete actions. This ordinal architecture further\nsignificantly improves the performance of PPO/TRPO. \n\n"}
{"id": "1901.10590", "contents": "Title: Throttling Malware Families in 2D Abstract: Malicious software are categorized into families based on their static and\ndynamic characteristics, infection methods, and nature of threat. Visual\nexploration of malware instances and families in a low dimensional space helps\nin giving a first overview about dependencies and relationships among these\ninstances, detecting their groups and isolating outliers. Furthermore, visual\nexploration of different sets of features is useful in assessing the quality of\nthese sets to carry a valid abstract representation, which can be later used in\nclassification and clustering algorithms to achieve a high accuracy. In this\npaper, we investigate one of the best dimensionality reduction techniques known\nas t-SNE to reduce the malware representation from a high dimensional space\nconsisting of thousands of features to a low dimensional space. We experiment\nwith different feature sets and depict malware clusters in 2-D. Surprisingly,\nt-SNE does not only provide nice 2-D drawings, but also dramatically increases\nthe generalization power of SVM classifiers. Moreover, obtained results showed\nthat cross-validation accuracy is much better using the 2-D embedded\nrepresentation of samples than using the original high-dimensional\nrepresentation. \n\n"}
{"id": "1901.10603", "contents": "Title: Numerically Recovering the Critical Points of a Deep Linear Autoencoder Abstract: Numerically locating the critical points of non-convex surfaces is a\nlong-standing problem central to many fields. Recently, the loss surfaces of\ndeep neural networks have been explored to gain insight into outstanding\nquestions in optimization, generalization, and network architecture design.\nHowever, the degree to which recently-proposed methods for numerically\nrecovering critical points actually do so has not been thoroughly evaluated. In\nthis paper, we examine this issue in a case for which the ground truth is\nknown: the deep linear autoencoder. We investigate two sub-problems associated\nwith numerical critical point identification: first, because of large parameter\ncounts, it is infeasible to find all of the critical points for contemporary\nneural networks, necessitating sampling approaches whose characteristics are\npoorly understood; second, the numerical tolerance for accurately identifying a\ncritical point is unknown, and conservative tolerances are difficult to\nsatisfy. We first identify connections between recently-proposed methods and\nwell-understood methods in other fields, including chemical physics, economics,\nand algebraic geometry. We find that several methods work well at recovering\ncertain information about loss surfaces, but fail to take an unbiased sample of\ncritical points. Furthermore, numerical tolerance must be very strict to ensure\nthat numerically-identified critical points have similar properties to true\nanalytical critical points. We also identify a recently-published Newton method\nfor optimization that outperforms previous methods as a critical point-finding\nalgorithm. We expect our results will guide future attempts to numerically\nstudy critical points in large nonlinear neural networks. \n\n"}
{"id": "1901.10618", "contents": "Title: Persuasion-based Robust Sensor Design Against Attackers with Unknown\n  Control Objectives Abstract: In this paper, we introduce a robust sensor design framework to provide\n\"persuasion-based\" defense in stochastic control systems against an unknown\ntype attacker with a control objective exclusive to its type. For effective\ncontrol, such an attacker's actions depend on its belief on the underlying\nstate of the system. We design a robust \"linear-plus-noise\" signaling strategy\nto encode sensor outputs in order to shape the attacker's belief in a strategic\nway and correspondingly to persuade the attacker to take actions that lead to\nminimum damage with respect to the system's objective. The specific model we\nadopt is a Gauss-Markov process driven by a controller with a (partially)\n\"unknown\" malicious/benign control objective. We seek to defend against the\nworst possible distribution over control objectives in a robust way under the\nsolution concept of Stackelberg equilibrium, where the sensor is the leader. We\nshow that a necessary and sufficient condition on the covariance matrix of the\nposterior belief is a certain linear matrix inequality and we provide a\nclosed-form solution for the associated signaling strategy. This enables us to\nformulate an equivalent tractable problem, indeed a semi-definite program, to\ncompute the robust sensor design strategies \"globally\" even though the original\noptimization problem is non-convex and highly nonlinear. We also extend this\nresult to scenarios where the sensor makes noisy or partial measurements.\nFinally, we analyze the ensuing performance numerically for various scenarios. \n\n"}
{"id": "1901.11149", "contents": "Title: Which Factorization Machine Modeling is Better: A Theoretical Answer\n  with Optimal Guarantee Abstract: Factorization machine (FM) is a popular machine learning model to capture the\nsecond order feature interactions. The optimal learning guarantee of FM and its\ngeneralized version is not yet developed. For a rank $k$ generalized FM of $d$\ndimensional input, the previous best known sampling complexity is\n$\\mathcal{O}[k^{3}d\\cdot\\mathrm{polylog}(kd)]$ under Gaussian distribution.\nThis bound is sub-optimal comparing to the information theoretical lower bound\n$\\mathcal{O}(kd)$. In this work, we aim to tighten this bound towards optimal\nand generalize the analysis to sub-gaussian distribution. We prove that when\nthe input data satisfies the so-called $\\tau$-Moment Invertible Property, the\nsampling complexity of generalized FM can be improved to\n$\\mathcal{O}[k^{2}d\\cdot\\mathrm{polylog}(kd)/\\tau^{2}]$. When the second order\nself-interaction terms are excluded in the generalized FM, the bound can be\nimproved to the optimal $\\mathcal{O}[kd\\cdot\\mathrm{polylog}(kd)]$ up to the\nlogarithmic factors. Our analysis also suggests that the positive semi-definite\nconstraint in the conventional FM is redundant as it does not improve the\nsampling complexity while making the model difficult to optimize. We evaluate\nour improved FM model in real-time high precision GPS signal calibration task\nto validate its superiority. \n\n"}
{"id": "math/0408270", "contents": "Title: Solving the Likelihood Equations Abstract: Given a model in algebraic statistics and some data, the likelihood function\nis a rational function on a projective variety. Algebraic algorithms are\npresented for computing all critical points of this function, with the aim of\nidentifying the local maxima in the probability simplex. Applications include\nmodels specified by rank conditions on matrices and the Jukes-Cantor models of\nphylogenetics. The maximum likelihood degree of a generic complete intersection\nis also determined. \n\n"}
{"id": "math/0510333", "contents": "Title: Optimal Bond Portfolios Abstract: We aim to construct a general framework for portfolio management in\ncontinuous time, encompassing both stocks and bonds. In these lecture notes we\ngive an overview of the state of the art of optimal bond portfolios and we\nre-visit main results and mathematical constructions introduced in our previous\npublications (Ann. Appl. Probab. \\textbf{15}, 1260--1305 (2005) and Fin. Stoch.\n{\\bf9}, 429--452 (2005)).\n  A solution of the optimal bond portfolio problem is given for general utility\nfunctions and volatility operator processes, provided that the market price of\nrisk process has certain Malliavin differentiability properties or is finite\ndimensional.\n  The text is essentially self-contained. \n\n"}
{"id": "math/0608381", "contents": "Title: Absolute Extrema of Invariant Optimal Control Problems Abstract: Optimal control problems are usually addressed with the help of the famous\nPontryagin Maximum Principle (PMP) which gives a generalization of the\nclassical Euler-Lagrange and Weierstrass necessary optimality conditions of the\ncalculus of variations. Success in applying the PMP permits to obtain\ncandidates for a local minimum. In 1967 a direct method, which permits to\nobtain global minimizers directly, without using necessary conditions, was\nintroduced by Leitmann. Leitmann's approach is connected, as showed by Carlson\nin 2002, with \"Caratheodory's royal road of the Calculus of variations\". Here\nwe propose a related but different direct approach to problems of the calculus\nof variations and optimal control, which permit to obtain global minima\ndirectly, without recourse to needle variations and necessary conditions. Our\nmethod is inspired by the classical Noether's theorem and its recent extensions\nto optimal control. We make use of the variational symmetries of the problem,\nconsidering parameter-invariance transformations and substituting the original\nproblem by a parameter-family of optimal control problems. Parameters are then\nfixed in order to make the problem trivial, in some sense. Finally, by applying\nthe inverse of the chosen invariance-transformation, we get the global\nminimizer for the original problem. The proposed method is illustrated, by\nsolving concrete problems, and compared with Leitmann's approach. \n\n"}
{"id": "math/0609528", "contents": "Title: Semidefinite Characterization and Computation of Real Radical Ideals Abstract: For an ideal $I\\subseteq\\mathbb{R}[x]$ given by a set of generators, a new\nsemidefinite characterization of its real radical $I(V_\\mathbb{R}(I))$ is\npresented, provided it is zero-dimensional (even if $I$ is not). Moreover we\npropose an algorithm using numerical linear algebra and semidefinite\noptimization techniques, to compute all (finitely many) points of the real\nvariety $V_\\mathbb{R}(I)$ as well as a set of generators of the real radical\nideal. The latter is obtained in the form of a border or Gr\\\"obner basis. The\nalgorithm is based on moment relaxations and, in contrast to other existing\nmethods, it exploits the real algebraic nature of the problem right from the\nbeginning and avoids the computation of complex components. \n\n"}
{"id": "math/0612682", "contents": "Title: Convergence Speed in Distributed Consensus and Control Abstract: We study the convergence speed of distributed iterative algorithms for the\nconsensus and averaging problems, with emphasis on the latter. We first\nconsider the case of a fixed communication topology. We show that a simple\nadaptation of a consensus algorithm leads to an averaging algorithm. We prove\nlower bounds on the worst-case convergence time for various classes of linear,\ntime-invariant, distributed consensus methods, and provide an algorithm that\nessentially matches those lower bounds. We then consider the case of a\ntime-varying topology, and provide a polynomial-time averaging algorithm. \n\n"}
{"id": "math/0612682", "contents": "Title: Convergence Speed in Distributed Consensus and Control Abstract: We study the convergence speed of distributed iterative algorithms for the\nconsensus and averaging problems, with emphasis on the latter. We first\nconsider the case of a fixed communication topology. We show that a simple\nadaptation of a consensus algorithm leads to an averaging algorithm. We prove\nlower bounds on the worst-case convergence time for various classes of linear,\ntime-invariant, distributed consensus methods, and provide an algorithm that\nessentially matches those lower bounds. We then consider the case of a\ntime-varying topology, and provide a polynomial-time averaging algorithm. \n\n"}
{"id": "physics/0009032", "contents": "Title: Information theory and learning: a physical approach Abstract: We try to establish a unified information theoretic approach to learning and\nto explore some of its applications. First, we define {\\em predictive\ninformation} as the mutual information between the past and the future of a\ntime series, discuss its behavior as a function of the length of the series,\nand explain how other quantities of interest studied previously in learning\ntheory - as well as in dynamical systems and statistical mechanics - emerge\nfrom this universally definable concept. We then prove that predictive\ninformation provides the {\\em unique measure for the complexity} of dynamics\nunderlying the time series and show that there are classes of models\ncharacterized by {\\em power-law growth of the predictive information} that are\nqualitatively more complex than any of the systems that have been investigated\nbefore. Further, we investigate numerically the learning of a nonparametric\nprobability density, which is an example of a problem with power-law\ncomplexity, and show that the proper Bayesian formulation of this problem\nprovides for the `Occam' factors that punish overly complex models and thus\nallow one {\\em to learn not only a solution within a specific model class, but\nalso the class itself} using the data only and with very few a priori\nassumptions. We study a possible {\\em information theoretic method} that\nregularizes the learning of an undersampled discrete variable, and show that\nlearning in such a setup goes through stages of very different complexities.\nFinally, we discuss how all of these ideas may be useful in various problems in\nphysics, statistics, and, most importantly, biology. \n\n"}
{"id": "quant-ph/0409156", "contents": "Title: Optimizing linear optics quantum gates Abstract: In this paper, the problem of finding optimal success probabilities of static\nlinear optics quantum gates is linked to the theory of convex optimization. It\nis shown that by exploiting this link, upper bounds for the success probability\nof networks realizing single-mode gates can be derived, which hold in\ngenerality for linear optical networks followed by postselection, i.e., for\nnetworks of arbitrary size, any number of auxiliary modes, and arbitrary photon\nnumbers. As a corollary, the previously formulated conjecture is proven that\nthe optimal success probability of a postselected non-linear sign shift without\nfeed-forward is 1/4, a gate playing the central role in the scheme of\nKnill-Laflamme-Milburn for quantum computation with linear optics. The concept\nof Lagrange duality is shown to be applicable to provide rigorous proofs for\nsuch bounds for elementary gates, although the original problem is a difficult\nnon-convex problem in infinitely many objective variables. The versatility of\nthis approach to identify other optimal linear optical schemes is demonstrated. \n\n"}
