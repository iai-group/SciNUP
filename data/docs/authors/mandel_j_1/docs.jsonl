{"id": "0704.0303", "contents": "Title: Measurement of the Aerosol Phase Function at the Pierre Auger\n  Observatory Abstract: Air fluorescence detectors measure the energy of ultra-high energy cosmic\nrays by collecting fluorescence light emitted from nitrogen molecules along the\nextensive air shower cascade. To ensure a reliable energy determination, the\nlight signal needs to be corrected for atmospheric effects, which not only\nattenuate the signal, but also produce a non-negligible background component\ndue to scattered Cherenkov light and multiple-scattered light. The correction\nrequires regular measurements of the aerosol attenuation length and the aerosol\nphase function, defined as the probability of light scattered in a given\ndirection. At the Pierre Auger Observatory in Malargue, Argentina, the phase\nfunction is measured on an hourly basis using two Aerosol Phase Function (APF)\nlight sources. These sources direct a UV light beam across the field of view of\nthe fluorescence detectors; the phase function can be extracted from the image\nof the shots in the fluorescence detector cameras. This paper describes the\ndesign, current status, standard operation procedure, and performance of the\nAPF system at the Pierre Auger Observatory. \n\n"}
{"id": "0705.2838", "contents": "Title: Coexistence of Weak and Strong Wave Turbulence in a Swell Propagation Abstract: By performing two parallel numerical experiments -- solving the dynamical\nHamiltonian equations and solving the Hasselmann kinetic equation -- we\nexamined the applicability of the theory of weak turbulence to the description\nof the time evolution of an ensemble of free surface waves (a swell) on deep\nwater. We observed qualitative coincidence of the results.\n  To achieve quantitative coincidence, we augmented the kinetic equation by an\nempirical dissipation term modelling the strongly nonlinear process of\nwhite-capping. Fitting the two experiments, we determined the dissipation\nfunction due to wave breaking and found that it depends very sharply on the\nparameter of nonlinearity (the surface steepness). The onset of white-capping\ncan be compared to a second-order phase transition. This result corroborates\nwith experimental observations by Banner, Babanin, Young. \n\n"}
{"id": "0705.4563", "contents": "Title: Kalman Filtering in the Presence of State Space Equality Constraints Abstract: We discuss two separate techniques for Kalman Filtering in the presence of\nstate space equality constraints. We then prove that despite the lack of\nsimilarity in their formulations, under certain conditions, the two methods\nresult in mathematically equivalent constrained estimate structures. We\nconclude that the potential benefits of using equality constraints in Kalman\nFiltering often outweigh the computational costs, and as such, equality\nconstraints, when present, should be enforced by way of one of these two\nmethods. \n\n"}
{"id": "0706.3520", "contents": "Title: On probabilities for separating sets of order statistics Abstract: Consider a set of order statistics that arise from sorting samples from two\ndifferent populations, each with their own, possibly different distribution\nfunction. The probability that these order statistics fall in disjoint, ordered\nintervals, and that of the smallest statistics, a certain number come from the\nfirst populations, are given in terms of the two distribution functions. The\nresult is applied to computing the joint probability of the number of\nrejections and the number of false rejections for the Benjamini-Hochberg false\ndiscovery rate procedure. \n\n"}
{"id": "0706.4128", "contents": "Title: A review of wildland fire spread modelling, 1990-present 2: Empirical\n  and quasi-empirical models Abstract: In recent years, advances in computational power and spatial data analysis\n(GIS, remote sensing, etc) have led to an increase in attempts to model the\nspread and behaviour of wildland fires across the landscape. This series of\nreview papers endeavours to critically and comprehensively review all types of\nsurface fire spread models developed since 1990. This paper reviews models of\nan empirical or quasi-empirical nature. These models are based solely on the\nstatistical analysis of experimentally obtained data with or without some\nphysical framework for the basis of the relations. Other papers in the series\nreview models of a physical or quasi-physical nature, and mathematical\nanalogues and simulation models. The main relations of empirical models are\nthat of wind speed and fuel moisture content with rate of forward spread.\nComparisons are made of the different functional relationships selected by\nvarious authors for these variables. \n\n"}
{"id": "0706.4130", "contents": "Title: A review of wildland fire spread modelling, 1990-present 3: Mathematical\n  analogues and simulation models Abstract: In recent years, advances in computational power and spatial data analysis\n(GIS, remote sensing, etc) have led to an increase in attempts to model the\nspread and behvaiour of wildland fires across the landscape. This series of\nreview papers endeavours to critically and comprehensively review all types of\nsurface fire spread models developed since 1990. This paper reviews models of a\nsimulation or mathematical analogue nature. Most simulation models are\nimplementations of existing empirical or quasi-empirical models and their\nprimary function is to convert these generally one dimensional models to two\ndimensions and then propagate a fire perimeter across a modelled landscape.\nMathematical analogue models are those that are based on some mathematical\nconceit (rather than a physical representation of fire spread) that\ncoincidentally simulates the spread of fire. Other papers in the series review\nmodels of an physical or quasi-physical nature and empirical or quasi-empirical\nnature. Many models are extensions or refinements of models developed before\n1990. Where this is the case, these models are also discussed but much less\ncomprehensively. \n\n"}
{"id": "0707.1161", "contents": "Title: Falsification Of The Atmospheric CO2 Greenhouse Effects Within The Frame\n  Of Physics Abstract: The atmospheric greenhouse effect, an idea that many authors trace back to\nthe traditional works of Fourier (1824), Tyndall (1861), and Arrhenius (1896),\nand which is still supported in global climatology, essentially describes a\nfictitious mechanism, in which a planetary atmosphere acts as a heat pump\ndriven by an environment that is radiatively interacting with but radiatively\nequilibrated to the atmospheric system. According to the second law of\nthermodynamics such a planetary machine can never exist. Nevertheless, in\nalmost all texts of global climatology and in a widespread secondary literature\nit is taken for granted that such mechanism is real and stands on a firm\nscientific foundation. In this paper the popular conjecture is analyzed and the\nunderlying physical principles are clarified. By showing that (a) there are no\ncommon physical laws between the warming phenomenon in glass houses and the\nfictitious atmospheric greenhouse effects, (b) there are no calculations to\ndetermine an average surface temperature of a planet, (c) the frequently\nmentioned difference of 33 degrees Celsius is a meaningless number calculated\nwrongly, (d) the formulas of cavity radiation are used inappropriately, (e) the\nassumption of a radiative balance is unphysical, (f) thermal conductivity and\nfriction must not be set to zero, the atmospheric greenhouse conjecture is\nfalsified. \n\n"}
{"id": "0707.4558", "contents": "Title: Open Problems in Algebraic Statistics Abstract: Algebraic statistics is concerned with the study of probabilistic models and\ntechniques for statistical inference using methods from algebra and geometry.\nThis article presents a list of open mathematical problems in this emerging\nfield, with main emphasis on graphical models with hidden variables, maximum\nlikelihood estimation, and multivariate Gaussian distributions. This article is\nbased on a lecture presented at the IMA in Minneapolis during the 2006/07\nprogram on Applications of Algebraic Geometry. \n\n"}
{"id": "0710.0317", "contents": "Title: A General Strategy for Physics-Based Model Validation Illustrated with\n  Earthquake Phenomenology, Atmospheric Radiative Transfer, and Computational\n  Fluid Dynamics Abstract: Validation is often defined as the process of determining the degree to which\na model is an accurate representation of the real world from the perspective of\nits intended uses. Validation is crucial as industries and governments depend\nincreasingly on predictions by computer models to justify their decisions. In\nthis article, we survey the model validation literature and propose to\nformulate validation as an iterative construction process that mimics the\nprocess occurring implicitly in the minds of scientists. We thus offer a formal\nrepresentation of the progressive build-up of trust in the model, and thereby\nreplace incapacitating claims on the impossibility of validating a given model\nby an adaptive process of constructive approximation. This approach is better\nadapted to the fuzzy, coarse-grained nature of validation. Our procedure\nfactors in the degree of redundancy versus novelty of the experiments used for\nvalidation as well as the degree to which the model predicts the observations.\nWe illustrate the new methodology first with the maturation of Quantum\nMechanics as the arguably best established physics theory and then with several\nconcrete examples drawn from some of our primary scientific interests: a\ncellular automaton model for earthquakes, an anomalous diffusion model for\nsolar radiation transport in the cloudy atmosphere, and a computational fluid\ndynamics code for the Richtmyer-Meshkov instability. This article is an\naugmented version of Sornette et al. [2007] that appeared in Proceedings of the\nNational Academy of Sciences in 2007 (doi: 10.1073/pnas.0611677104), with an\nelectronic supplement at URL\nhttp://www.pnas.org/cgi/content/full/0611677104/DC1. Sornette et al. [2007] is\nalso available in preprint form at physics/0511219. \n\n"}
{"id": "0712.3187", "contents": "Title: On the Korteweg-de Vries approximation for uneven bottoms Abstract: In this paper we focus on the water waves problem for uneven bottoms on a\ntwo-dimensionnal domain. Starting from the symmetric Boussinesq systems derived\nin [Chazel, Influence of topography on long water waves, 2007], we recover the\nuncoupled Korteweg-de Vries (KdV) approximation justified by Schneider and\nWayne for flat bottoms, and by Iguchi in the context of bottoms tending to zero\nat infinity at a substantial rate. The goal of this paper is to investigate the\nvalidity of this approximation for more general bathymetries. We exhibit two\nkinds of topography for which this approximation diverges from the Boussinesq\nsolutions. A topographically modified KdV approximation is then proposed to\ndeal with such bathymetries. Finally, all the models involved are numerically\ncomputed and compared. \n\n"}
{"id": "0712.3920", "contents": "Title: Asymptotic Models for Internal Waves Abstract: We derived here in a systematic way, and for a large class of scaling\nregimes, asymptotic models for the propagation of internal waves at the\ninterface between two layers of immiscible fluids of different densities, under\nthe rigid lid assumption and with a flat bottom. The full (Euler) model for\nthis situation is reduced to a system of evolution equations posed spatially on\n$\\R^d$, $d=1,2$, which involve two nonlocal operators. The different asymptotic\nmodels are obtained by expanding the nonlocal operators with respect to\nsuitable small parameters that depend variously on the amplitude, wave-lengths\nand depth ratio of the two layers. We rigorously derive classical models and\nalso some model systems that appear to be new. Furthermore, the consistency of\nthese asymptotic systems with the full Euler equations is established. \n\n"}
{"id": "0801.3513", "contents": "Title: On some difficulties with a posterior probability approximation\n  technique Abstract: In Scott (2002) and Congdon (2006), a new method is advanced to compute\nposterior probabilities of models under consideration. It is based solely on\nMCMC outputs restricted to single models, i.e., it is bypassing reversible jump\nand other model exploration techniques. While it is indeed possible to\napproximate posterior probabilities based solely on MCMC outputs from single\nmodels, as demonstrated by Gelfand and Dey (1994) and Bartolucci et al. (2006),\nwe show that the proposals of Scott (2002) and Congdon (2006) are biased and\nadvance several arguments towards this thesis, the primary one being the\nconfusion between model-based posteriors and joint pseudo-posteriors. From a\npractical point of view, the bias in Scott's (2002) approximation appears to be\nmuch more severe than the one in Congdon's (2006), the later being often of the\nsame magnitude as the posterior probability it approximates, although we also\nexhibit an example where the divergence from the true posterior probability is\nextreme. \n\n"}
{"id": "0801.3875", "contents": "Title: Towards a Real-Time Data Driven Wildland Fire Model Abstract: A wildland fire model based on semi-empirical relations for the spread rate\nof a surface fire and post-frontal heat release is coupled with the Weather\nResearch and Forecasting atmospheric model (WRF). The propagation of the fire\nfront is implemented by a level set method. Data is assimilated by a morphing\nensemble Kalman filter, which provides amplitude as well as position\ncorrections. Thermal images of a fire will provide the observations and will be\ncompared to a synthetic image from the model state. \n\n"}
{"id": "0802.1615", "contents": "Title: Real-Time Data Driven Wildland Fire Modeling Abstract: We are developing a wildland fire model based on semi-empirical relations\nthat estimate the rate of spread of a surface fire and post-frontal heat\nrelease, coupled with WRF, the Weather Research and Forecasting atmospheric\nmodel. A level set method identifies the fire front. Data are assimilated using\nboth amplitude and position corrections using a morphing ensemble Kalman\nfilter. We will use thermal images of a fire for observations that will be\ncompared to synthetic image based on the model state. \n\n"}
{"id": "0802.3690", "contents": "Title: On variance stabilisation by double Rao-Blackwellisation Abstract: Population Monte Carlo has been introduced as a sequential importance\nsampling technique to overcome poor fit of the importance function. In this\npaper, we compare the performances of the original Population Monte Carlo\nalgorithm with a modified version that eliminates the influence of the\ntransition particle via a double Rao-Blackwellisation. This modification is\nshown to improve the exploration of the modes through an large simulation\nexperiment on posterior distributions of mean mixtures of distributions. \n\n"}
{"id": "0802.4324", "contents": "Title: Proof of the Atmospheric Greenhouse Effect Abstract: A recently advanced argument against the atmospheric greenhouse effect is\nrefuted. A planet without an infrared absorbing atmosphere is mathematically\nconstrained to have an average temperature less than or equal to the effective\nradiating temperature. Observed parameters for Earth prove that without\ninfrared absorption by the atmosphere, the average temperature of Earth's\nsurface would be at least 33 K lower than what is observed. \n\n"}
{"id": "0803.2298", "contents": "Title: Testing the proposed link between cosmic rays and cloud cover Abstract: A decrease in the globally averaged low level cloud cover, deduced from the\nISCCP infra red data, as the cosmic ray intensity decreased during the solar\ncycle 22 was observed by two groups. The groups went on to hypothesise that the\ndecrease in ionization due to cosmic rays causes the decrease in cloud cover,\nthereby explaining a large part of the presently observed global warming. We\nhave examined this hypothesis to look for evidence to corroborate it. None has\nbeen found and so our conclusions are to doubt it. From the absence of\ncorroborative evidence, we estimate that less than 23%, at the 95% confidence\nlevel, of the 11-year cycle change in the globally averaged cloud cover\nobserved in solar cycle 22 is due to the change in the rate of ionization from\nthe solar modulation of cosmic rays. \n\n"}
{"id": "0803.4458", "contents": "Title: Rayleigh-B\\'enard Convection as a Nambu-metriplectic problem Abstract: The traditional Hamiltonian structure of the equations governing conservative\nRayleigh-B\\'enard convection (RBC) is singular, i.e. it's Poisson bracket\npossesses nontrivial Casimir functionals. We show that a special form of one of\nthese Casimirs can be used to extend the bilinear Poisson bracket to a\ntrilinear generalised Nambu bracket. It is further shown that the equations\ngoverning dissipative RBC can be written as the superposition of the\nconservative Nambu bracket with a dissipative symmetric bracket. This leads to\na Nambu-metriplectic system, which completes the geometrical picture of RBC. \n\n"}
{"id": "0804.3152", "contents": "Title: Bayesian computation for statistical models with intractable normalizing\n  constants Abstract: This paper deals with some computational aspects in the Bayesian analysis of\nstatistical models with intractable normalizing constants. In the presence of\nintractable normalizing constants in the likelihood function, traditional MCMC\nmethods cannot be applied. We propose an approach to sample from such posterior\ndistributions. The method can be thought as a Bayesian version of the MCMC-MLE\napproach of Geyer and Thompson (1992). To the best of our knowledge, this is\nthe first general and asymptotically consistent Monte Carlo method for such\nproblems. We illustrate the method with examples from image segmentation and\nsocial network modeling. We study as well the asymptotic behavior of the\nalgorithm and obtain a strong law of large numbers for empirical averages. \n\n"}
{"id": "0804.3207", "contents": "Title: Lookup tables to compute high energy cosmic ray induced atmospheric\n  ionization and changes in atmospheric chemistry Abstract: A variety of events such as gamma-ray bursts and supernovae may expose the\nEarth to an increased flux of high-energy cosmic rays, with potentially\nimportant effects on the biosphere. Existing atmospheric chemistry software\ndoes not have the capability of incorporating the effects of substantial cosmic\nray flux above 10 GeV . An atmospheric code, the NASA-Goddard Space Flight\nCenter two-dimensional (latitude, altitude) time-dependent atmospheric model\n(NGSFC), is used to study atmospheric chemistry changes. Using CORSIKA, we have\ncreated tables that can be used to compute high energy cosmic ray (10 GeV - 1\nPeV) induced atmospheric ionization and also, with the use of the NGSFC code,\ncan be used to simulate the resulting atmospheric chemistry changes. We discuss\nthe tables, their uses, weaknesses, and strengths. \n\n"}
{"id": "0805.0445", "contents": "Title: Simultaneous numerical simulation of direct and inverse cascades in wave\n  turbulence Abstract: Results of direct numerical simulation of isotropic turbulence of surface\ngravity waves in the framework of Hamiltonian equations are presented. For the\nfirst time simultaneous formation of both direct and inverse cascades was\nobserved in the framework of primordial dynamical equations. At the same time,\nstrong long waves background was developed. It was shown, that obtained\nKolmogorov spectra are very sensitive to the presence of this condensate. Such\nsituation has to be typical for experimental wave tanks, flumes, and small\nlakes. \n\n"}
{"id": "0805.3602", "contents": "Title: Marginal Likelihood Integrals for Mixtures of Independence Models Abstract: Inference in Bayesian statistics involves the evaluation of marginal\nlikelihood integrals. We present algebraic algorithms for computing such\nintegrals exactly for discrete data of small sample size. Our methods apply to\nboth uniform priors and Dirichlet priors. The underlying statistical models are\nmixtures of independent distributions, or, in geometric language, secant\nvarieties of Segre-Veronese varieties. \n\n"}
{"id": "0807.4180", "contents": "Title: Habitable Climates: The Influence of Obliquity Abstract: Extrasolar terrestrial planets with the potential to host life might have\nlarge obliquities or be subject to strong obliquity variations. We revisit the\nhabitability of oblique planets with an energy balance climate model (EBM)\nallowing for dynamical transitions to ice-covered snowball states as a result\nof ice-albedo feedback. Despite the great simplicity of our EBM, it captures\nreasonably well the seasonal cycle of global energetic fluxes at Earth's\nsurface. It also performs satisfactorily against a full-physics climate model\nof a highly oblique Earth-like planet, in an unusual regime of circulation\ndominated by heat transport from the poles to the equator. Climates on oblique\nterrestrial planets can violate global radiative balance through much of their\nseasonal cycle, which limits the usefulness of simple radiative equilibrium\narguments. High obliquity planets have severe climates, with large amplitude\nseasonal variations, but they are not necessarily more prone to global snowball\ntransitions than low obliquity planets. We find that terrestrial planets with\nmassive CO2 atmospheres, typically expected in the outer regions of habitable\nzones, can also be subject to such dynamical snowball transitions. Some of the\nsnowball climates investigated for CO2-rich atmospheres experience partial\natmospheric collapse. Since long-term CO2 atmospheric build-up acts as a\nclimatic thermostat for habitable planets, partial CO2 collapse could limit the\nhabitability of such planets. A terrestrial planet's habitability may thus\ndepend sensitively on its short-term climatic stability. \n\n"}
{"id": "0809.0869", "contents": "Title: Helicity cascades in rotating turbulence Abstract: The effect of helicity (velocity-vorticity correlations) is studied in direct\nnumerical simulations of rotating turbulence down to Rossby numbers of 0.02.\nThe results suggest that the presence of net helicity plays an important role\nin the dynamics of the flow. In particular, at small Rossby number, the energy\ncascades to large scales, as expected, but helicity then can dominate the\ncascade to small scales. A phenomenological interpretation in terms of a direct\ncascade of helicity slowed down by wave-eddy interactions leads to the\nprediction of new inertial indices for the small-scale energy and helicity\nspectra. \n\n"}
{"id": "0809.0974", "contents": "Title: Least Squares and Shrinkage Estimation under Bimonotonicity Constraints Abstract: In this paper we describe active set type algorithms for minimization of a\nsmooth function under general order constraints, an important case being\nfunctions on the set of bimonotone r-by-s matrices. These algorithms can be\nused, for instance, to estimate a bimonotone regression function via least\nsquares or (a smooth approximation of) least absolute deviations. Another\napplication is shrinkage estimation in image denoising or, more generally,\nregression problems with two ordinal factors after representing the data in a\nsuitable basis which is indexed by pairs (i,j) in {1,...,r}x{1,...,s}. Various\nnumerical examples illustrate our methods. \n\n"}
{"id": "0811.0878", "contents": "Title: A quantum sensor for atom-surface interactions below 10 $\\mu$m Abstract: We report about the realization of a quantum device for force sensing at\nmicrometric scale. We trap an ultracold $^{88}$Sr atomic cloud with a 1-D\noptical lattice, then we place the atomic sample close to a test surface using\nthe same optical lattice as an elevator. We demonstrate precise positioning of\nthe sample at the $\\mu$m scale. By observing the Bloch oscillations of atoms\ninto the 1-D optical standing wave, we are able to measure the total force on\nthe atoms along the lattice axis, with a spatial resolution of few microns. We\nalso demonstrate a technique for transverse displacement of the atoms, allowing\nto perform measurements near either transparent or reflective test surfaces. In\norder to reduce the minimum distance from the surface, we compress the\nlongitudinal size of the atomic sample by means of an optical tweezer. Such\nsystem is suited for studies of atom-surface interaction at short distance,\nsuch as measurement of Casimir force and search for possible non-Newtonian\ngravity effects. \n\n"}
{"id": "0811.2678", "contents": "Title: The \"north pole problem\" and random orthogonal matrices Abstract: This paper is motivated by the following observation. Take a 3 x 3 random\n(Haar distributed) orthogonal matrix $\\Gamma$, and use it to \"rotate\" the north\npole, $x_0$ say, on the unit sphere in $R^3$. This then gives a point $u=\\Gamma\nx_0$ that is uniformly distributed on the unit sphere. Now use the same\northogonal matrix to transform u, giving $v=\\Gamma u=\\Gamma^2 x_0$. Simulations\nreported in Marzetta et al (2002) suggest that v is more likely to be in the\nnorthern hemisphere than in the southern hemisphere, and, morever, that\n$w=\\Gamma^3 x_0$ has higher probability of being closer to the poles $\\pm x_0$\nthan the uniformly distributed point u. In this paper we prove these results,\nin the general setting of dimension $p\\ge 3$, by deriving the exact\ndistributions of the relevant components of u and v. The essential questions\nanswered are the following. Let x be any fixed point on the unit sphere in\n$R^p$, where $p\\ge 3$. What are the distributions of $U_2=x'\\Gamma^2 x$ and\n$U_3=x'\\Gamma^3 x$? It is clear by orthogonal invariance that these\ndistribution do not depend on x, so that we can, without loss of generality,\ntake x to be $x_0=(1,0,...,0)'\\in R^p$. Call this the \"north pole\". Then\n$x_0'\\Gamma^ k x_0$ is the first component of the vector $\\Gamma^k x_0$. We\nderive stochastic representations for the exact distributions of $U_2$ and\n$U_3$ in terms of random variables with known distributions. \n\n"}
{"id": "0811.4640", "contents": "Title: A new Concept of Ball Lightning Abstract: We suggest that the ball lightning (BL) is a weakly ionized gas, in which the\nelectromagnetic radiation can be accumulated through the Bose-Einstein\ncondensation and/or the photon trapping in the plasma density well. We derive\nthe set of equations describing the stability of BL, and show that the BL\nmoving along charged surface becomes unstable. Eventually the instability leads\nto explosion of BL and release of energy of the trapped photons and/or the\nBose-Einstein condensate. \n\n"}
{"id": "0811.4756", "contents": "Title: Feasibility of free space quantum key distribution with coherent\n  polarization states Abstract: We demonstrate for the first time the feasibility of free space quantum key\ndistribution with continuous variables under real atmospheric conditions. More\nspecifically, we transmit coherent polarization states over a 100m free space\nchannel on the roof of our institute's building. In our scheme, signal and\nlocal oscillator are combined in a single spatial mode which auto-compensates\natmospheric fluctuations and results in an excellent interference. Furthermore,\nthe local oscillator acts as spatial and spectral filter thus allowing\nunrestrained daylight operation. \n\n"}
{"id": "0901.0515", "contents": "Title: Solar activity and the mean global temperature Abstract: The variation with time from 1956-2002 of the globally averaged rate of\nionization produced by cosmic rays in the atmosphere is deduced and shown to\nhave a cyclic component of period roughly twice the 11 year solar cycle period.\nLong term variations in the global average surface temperature as a function of\ntime since 1956 are found to have a similar cyclic component. The cyclic\nvariations are also observed in the solar irradiance and in the mean daily sun\nspot number. The cyclic variation in the cosmic ray rate is observed to be\ndelayed by 2-4 years relative to the temperature, the solar irradiance and\ndaily sun spot variations suggesting that the origin of the correlation is more\nlikely to be direct solar activity than cosmic rays. Assuming that the\ncorrelation is caused by such solar activity, we deduce that the maximum recent\nincrease in the mean surface temperature of the Earth which can be ascribed to\nthis activity is $\\lesssim14%$ of the observed global warming. \n\n"}
{"id": "0901.1461", "contents": "Title: The climate version of the Eta regional forecast model. 2.Evaluation of\n  the Eta CCS model performance against reanalysis data and surface\n  observations Abstract: The climate version Eta CCS, prepared from the NCEP Eta forecast model, was\nintegrated over South America for the period from January 1979 to December\n1983. The model was driven by the two sets of boundary conditions derived from\nthe reanalysis and outputs of HadAM3P atmospheric global model. The mean output\nfields of precipitation, precipitation frequency, and near surface air\ntemperature, simulated by the Eta model, were compared with the observational\ndata of the CRU and GPCP projects. This comparison shows that the Eta model\nreproduces well the main patterns of the summer and winter observed\nprecipitation fields over South America. But the magnitude of precipitation is\nunderestimated by the ETA CCS model in the regions of strong convection\nactivity in summer. This underestimation of observed precipitation is larger\nfor the Eta model driven by HadAM3P than by the reanalysis. The observed number\nof wet days is overestimated by HadAM3P. The number of wet days in both runs of\nthe Eta model is closer to observations. The main summer and winter patterns of\nnear surface air temperature are reproduced well by both HadAM3P and the Eta\nmodel. The Eta model overestimates the observed surface temperature over the\ncentral part of the continent due to the lack of convective cloudiness in this\nregion. The Eta model captures observed annual cycle of precipitation in six\nselected regions over South America. On the whole, these results support the\nconclusion that the Eta model with some improvements can be used for\ndownscaling of the HadAM3P output fields. \n\n"}
{"id": "0901.3725", "contents": "Title: A Brief Tutorial on the Ensemble Kalman Filter Abstract: The ensemble Kalman filter (EnKF) is a recursive filter suitable for problems\nwith a large number of variables, such as discretizations of partial\ndifferential equations in geophysical models. The EnKF originated as a version\nof the Kalman filter for large problems (essentially, the covariance matrix is\nreplaced by the sample covariance), and it is now an important data\nassimilation component of ensemble forecasting. EnKF is related to the particle\nfilter (in this context, a particle is the same thing as an ensemble member)\nbut the EnKF makes the assumption that all probability distributions involved\nare Gaussian. This article briefly describes the derivation and practical\nimplementation of the basic version of EnKF, and reviews several extensions. \n\n"}
{"id": "0902.1568", "contents": "Title: Steady State of a Dissipative Flow-Controlled System and the Maximum\n  Entropy Production Principle Abstract: A theory to predict the steady state position of a dissipative,\nflow-controlled system, as defined by a control volume, is developed based on\nthe Maximum Entropy (MaxEnt) principle of Jaynes, involving minimisation of a\ngeneralised free energy-like potential. The analysis provides a theoretical\njustification of a local, conditional form of the Maximum Entropy Production\n(MEP) principle, which successfully predicts the observable properties of many\nsuch systems. The analysis reveals a very different manifestation of the second\nlaw of thermodynamics in steady state flow systems, which {provides a driving\nforce for} the formation of complex systems, including life. \n\n"}
{"id": "0902.4111", "contents": "Title: Bivariate Instantaneous Frequency and Bandwidth Abstract: The generalizations of instantaneous frequency and instantaneous bandwidth to\na bivariate signal are derived. These are uniquely defined whether the signal\nis represented as a pair of real-valued signals, or as one analytic and one\nanti-analytic signal. A nonstationary but oscillatory bivariate signal has a\nnatural representation as an ellipse whose properties evolve in time, and this\nrepresentation provides a simple geometric interpretation for the bivariate\ninstantaneous moments. The bivariate bandwidth is shown to consist of three\nterms measuring the degree of instability of the time-varying ellipse:\namplitude modulation with fixed eccentricity, eccentricity modulation, and\norientation modulation or precession. An application to the analysis of data\nfrom a free-drifting oceanographic float is presented and discussed. \n\n"}
{"id": "0903.2795", "contents": "Title: Chaotic response of global climate to long-term solar forcing\n  variability Abstract: It is shown that global climate exhibits chaotic response to solar forcing\nvariability in a vast range of timescales: from annual to multi-millennium.\nUnlike linear systems, where periodic forcing leads to periodic response,\nnonlinear chaotic response to periodic forcing can result in exponentially\ndecaying broad-band power spectrum with decay rate T_e equal to the period of\nthe forcing. It is shown that power spectrum of a reconstructed time series of\nNorthern Hemisphere temperature anomaly for the past 2,000 years has an\nexponentially decaying broad-band part with T_e = 11 yr, i.e. the observed\ndecay rate T_e equals the mean period of the solar activity. It is also shown\nthat power spectrum of a reconstruction of atmospheric CO_2 time fluctuations\nfor the past 650,000 years, has an exponentially decaying broad-band part with\nT_e = 41,000 years, i.e. the observed decay rate T_e equals the period of the\nobliquity periodic forcing. A possibility of a chaotic solar forcing of the\nclimate has been also discussed. These results clarify role of solar forcing\nvariability in long-term global climate dynamics (in particular in the unsolved\nproblem of the glaciation cycles) and help in construction of adequate dynamic\nmodels of the global climate. \n\n"}
{"id": "0904.0635", "contents": "Title: Approximate Bayesian Computation: a nonparametric perspective Abstract: Approximate Bayesian Computation is a family of likelihood-free inference\ntechniques that are well-suited to models defined in terms of a stochastic\ngenerating mechanism. In a nutshell, Approximate Bayesian Computation proceeds\nby computing summary statistics s_obs from the data and simulating summary\nstatistics for different values of the parameter theta. The posterior\ndistribution is then approximated by an estimator of the conditional density\ng(theta|s_obs). In this paper, we derive the asymptotic bias and variance of\nthe standard estimators of the posterior distribution which are based on\nrejection sampling and linear adjustment. Additionally, we introduce an\noriginal estimator of the posterior distribution based on quadratic adjustment\nand we show that its bias contains a fewer number of terms than the estimator\nwith linear adjustment. Although we find that the estimators with adjustment\nare not universally superior to the estimator based on rejection sampling, we\nfind that they can achieve better performance when there is a nearly\nhomoscedastic relationship between the summary statistics and the parameter of\ninterest. To make this relationship as homoscedastic as possible, we propose to\nuse transformations of the summary statistics. In different examples borrowed\nfrom the population genetics and epidemiological literature, we show the\npotential of the methods with adjustment and of the transformations of the\nsummary statistics. Supplemental materials containing the details of the proofs\nare available online. \n\n"}
{"id": "0904.0691", "contents": "Title: Convex Optimization Methods for Dimension Reduction and Coefficient\n  Estimation in Multivariate Linear Regression Abstract: In this paper, we study convex optimization methods for computing the trace\nnorm regularized least squares estimate in multivariate linear regression. The\nso-called factor estimation and selection (FES) method, recently proposed by\nYuan et al. [22], conducts parameter estimation and factor selection\nsimultaneously and have been shown to enjoy nice properties in both large and\nfinite samples. To compute the estimates, however, can be very challenging in\npractice because of the high dimensionality and the trace norm constraint. In\nthis paper, we explore a variant of Nesterov's smooth method [20] and interior\npoint methods for computing the penalized least squares estimate. The\nperformance of these methods is then compared using a set of randomly generated\ninstances. We show that the variant of Nesterov's smooth method [20] generally\noutperforms the interior point method implemented in SDPT3 version 4.0 (beta)\n[19] substantially . Moreover, the former method is much more memory efficient. \n\n"}
{"id": "0904.0709", "contents": "Title: Lagrangian views on turbulent mixing of passive scalars Abstract: The Lagrangian view of passive scalar turbulence has recently produced\ninteresting results and interpretations. Innovations in theory, experiments,\nsimulations and data analysis of Lagrangian turbulence are reviewed here in\nbrief. Part of the review is closely related to the so-called Kraichnan model\nfor the advection of the passive scalar in synthetic turbulence. Possible\nimplications for a better understanding of the passive scalar mixing in\nNavier-Stokes turbulence are also discussed. \n\n"}
{"id": "0904.2435", "contents": "Title: Computation of confidence intervals in regression utilizing uncertain\n  prior information Abstract: We consider a linear regression model with regression parameter beta\n=(beta_1, ..., beta_p) and independent and identically N(0, sigma^2)distributed\nerrors. Suppose that the parameter of interest is theta = a^T beta where a is a\nspecified vector. Define the parameter tau = c^T beta - t where the vector c\nand the number t are specified and a and c are linearly independent. Also\nsuppose that we have uncertain prior information that tau = 0. Kabaila and Giri\n(2009c) present a new frequentist 1-alpha confidence interval for theta that\nutilizes this prior information. This interval has expected length that (a) is\nrelatively small when the prior information about tau is correct and (b) has a\nmaximum value that is not too large. It coincides with the standard 1-alpha\nconfidence interval (obtained by fitting the full model to the data) when the\ndata strongly contradicts the prior information. At first sight, the\ncomputation of this new confidence interval seems to be infeasible. However, by\nthe use of the various computational devices that are presented in detail in\nthe present paper, this computation becomes feasible and practicable. \n\n"}
{"id": "0904.3488", "contents": "Title: Critical balance in magnetohydrodynamic, rotating and stratified\n  turbulence: towards a universal scaling conjecture Abstract: It is proposed that critical balance - a scale-by-scale balance between the\nlinear propagation and nonlinear interaction time scales - can be used as a\nuniversal scaling conjecture for determining the spectra of strong turbulence\nin anisotropic wave systems. Magnetohydrodynamic (MHD), rotating and stratified\nturbulence are considered under this assumption and, in particular, a novel and\nexperimentally testable energy cascade scenario and a set of scalings of the\nspectra are proposed for low-Rossby-number rotating turbulence. It is argued\nthat in neutral fluids, the critically balanced anisotropic cascade provides a\nnatural path from strong anisotropy at large scales to isotropic Kolmogorov\nturbulence at very small scales. It is also argued that the kperp^{-2} spectra\nseen in recent numerical simulations of low-Rossby-number rotating turbulence\nmay be analogous to the kperp^{-3/2} spectra of the numerical MHD turbulence in\nthe sense that they could be explained by assuming that fluctuations are\npolarised (aligned) approximately as inertial waves (Alfven waves for MHD). \n\n"}
{"id": "0905.2979", "contents": "Title: Extreme deconvolution: Inferring complete distribution functions from\n  noisy, heterogeneous and incomplete observations Abstract: We generalize the well-known mixtures of Gaussians approach to density\nestimation and the accompanying Expectation--Maximization technique for finding\nthe maximum likelihood parameters of the mixture to the case where each data\npoint carries an individual $d$-dimensional uncertainty covariance and has\nunique missing data properties. This algorithm reconstructs the\nerror-deconvolved or \"underlying\" distribution function common to all samples,\neven when the individual data points are samples from different distributions,\nobtained by convolving the underlying distribution with the heteroskedastic\nuncertainty distribution of the data point and projecting out the missing data\ndirections. We show how this basic algorithm can be extended with conjugate\npriors on all of the model parameters and a \"split-and-merge\" procedure\ndesigned to avoid local maxima of the likelihood. We demonstrate the full\nmethod by applying it to the problem of inferring the three-dimensional\nvelocity distribution of stars near the Sun from noisy two-dimensional,\ntransverse velocity measurements from the Hipparcos satellite. \n\n"}
{"id": "0905.3317", "contents": "Title: Stabilities of Parallel Flow and Horizontal Convection Abstract: In the first part, the stability of two-dimensional parallel flow is\ndiscussed. A more restrictively general stability criterion for inviscid\nparallel flow is obtained analytically.\n  In the second part, we report the numerical simulations of the\npartial-penetrating flow in horizontal convection within a squire cavity tank\nat high Rayleigh numbers $10^7<Ra<10^{10}$. The fast estab but slowly steadied\nflow is simulated, where a shallow and closed circulation cell is obtained\nnumerically as partial-penetrating flow for the first time, which is consistent\nwith the experiment. \n\n"}
{"id": "0906.0198", "contents": "Title: Some Numerical Results on the Rank of Generic Three-Way Arrays over R Abstract: The aim of this paper is the introduction of a new method for the numerical\ncomputation of the rank of a three-way array. We show that the rank of a\nthree-way array over R is intimately related to the real solution set of a\nsystem of polynomial equations. Using this, we present some numerical results\nbased on the computation of Grobner bases.\n  Key words: Tensors; three-way arrays; Candecomp/Parafac; Indscal; generic\nrank; typical rank; Veronese variety; Segre variety; Grobner bases.\n  AMS classification: Primary 15A69; Secondary 15A72, 15A18. \n\n"}
{"id": "0907.1835", "contents": "Title: Information geometry for testing pseudorandom number generators Abstract: The information geometry of the 2-manifold of gamma probability density\nfunctions provides a framework in which pseudorandom number generators may be\nevaluated using a neighbourhood of the curve of exponential density functions.\nThe process is illustrated using the pseudorandom number generator in\nMathematica. This methodology may be useful to add to the current family of\ntest procedures in real applications to finite sampling data. \n\n"}
{"id": "0908.0132", "contents": "Title: Multifractal Detrended Cross-Correlation Analysis of Sunspot Numbers and\n  River Flow Fluctuations Abstract: We use the Detrended Cross-Correlation Analysis (DCCA) to investigate the\ninfluence of sun activity represented by sunspot numbers on one of the climate\nindicators, specifically rivers, represented by river flow fluctuation for\nDaugava, Holston, Nolichucky and French Broad rivers. The Multifractal\nDetrended Cross-Correlation Analysis (MF-DXA) shows that there exist some\ncrossovers in the cross-correlation fluctuation function versus time scale of\nthe river flow and sunspot series. One of these crossovers corresponds to the\nwell-known cycle of solar activity demonstrating a universal property of the\nmentioned rivers. The scaling exponent given by DCCA for original series at\nintermediate time scale, $(12-24)\\leq s\\leq 130$ months, is $\\lambda =\n1.17\\pm0.04$ which is almost similar for all underlying rivers at\n$1\\sigma$confidence interval showing the second universal behavior of river\nrunoffs. To remove the sinusoidal trends embedded in data sets, we apply the\nSingular Value Decomposition (SVD) method. Our results show that there exists a\nlong-range cross-correlation between the sunspot numbers and the underlying\nstreamflow records. The magnitude of the scaling exponent and the corresponding\ncross-correlation exponent are $\\lambda\\in (0.76, 0.85)$ and\n$\\gamma_{\\times}\\in(0.30, 0.48)$, respectively. Different values for scaling\nand cross-correlation exponents may be related to local and external factors\nsuch as topography, drainage network morphology, human activity and so on.\nMultifractal cross-correlation analysis demonstrates that all underlying\nfluctuations have almost weak multifractal nature which is also a universal\nproperty for data series. In addition the empirical relation between scaling\nexponent derived by DCCA and Detrended Fluctuation Analysis (DFA), $\n\\lambda\\approx(h_{\\rm sun} + h_{\\rm river})/2$ is confirmed. \n\n"}
{"id": "0908.1118", "contents": "Title: Drift wave-zonal flow dynamics Abstract: A remarkable phenomenon in turbulent flows is the spontaneous emergence of\ncoherent large spatial scale zonal jets. Geophysical examples of this\nphenomenon include the Jovian banded winds and the Earth's polar front jet. In\nthis work a comprehensive theory for the interaction of jets with turbulence,\nStochastic Structural Stability Theory, is applied to the problem of\nunderstanding the formation and maintenance of the zonal jets that are crucial\nfor enhancing plasma confinement in fusion devices. \n\n"}
{"id": "0908.2098", "contents": "Title: Rigorous confidence bounds for MCMC under a geometric drift condition Abstract: We assume a drift condition towards a small set and bound the mean square\nerror of estimators obtained by taking averages along a single trajectory of a\nMarkov chain Monte Carlo algorithm. We use these bounds to construct\nfixed-width nonasymptotic confidence intervals. For a possibly unbounded\nfunction $f:\\stany \\to R,$ let $I=\\int_{\\stany} f(x) \\pi(x) dx$ be the value of\ninterest and $\\hat{I}_{t,n}=(1/n)\\sum_{i=t}^{t+n-1}f(X_i)$ its MCMC estimate.\nPrecisely, we derive lower bounds for the length of the trajectory $n$ and\nburn-in time $t$ which ensure that $$P(|\\hat{I}_{t,n}-I|\\leq \\varepsilon)\\geq\n1-\\alpha.$$ The bounds depend only and explicitly on drift parameters, on the\n$V-$norm of $f,$ where $V$ is the drift function and on precision and\nconfidence parameters $\\varepsilon, \\alpha.$ Next we analyse an MCMC estimator\nbased on the median of multiple shorter runs that allows for sharper bounds for\nthe required total simulation cost. In particular the methodology can be\napplied for computing Bayesian estimators in practically relevant models. We\nillustrate our bounds numerically in a simple example. \n\n"}
{"id": "0909.1272", "contents": "Title: Rotating helical turbulence. Part I. Global evolution and spectral\n  behavior Abstract: We present results from two 1536^3 direct numerical simulations of rotating\nturbulence where both energy and helicity are injected into the flow by an\nexternal forcing. The dual cascade of energy and helicity towards smaller\nscales observed in isotropic and homogeneous turbulence is broken in the\npresence of rotation, with the development of an inverse cascade of energy now\ncoexisting with direct cascades of energy and helicity. In the direct cascade\nrange, the flux of helicity dominates over that of energy at low Rossby number.\nThese cascades have several consequences for the statistics of the flow. The\nevolution of global quantities and of the energy and helicity spectra is\nstudied, and comparisons with simulations at different Reynolds and Rossby\nnumbers at lower resolution are done to identify scaling laws. \n\n"}
{"id": "0909.1957", "contents": "Title: Minimal atmospheric finite-mode models preserving symmetry and\n  generalized Hamiltonian structures Abstract: A typical problem with the conventional Galerkin approach for the\nconstruction of finite-mode models is to keep structural properties unaffected\nin the process of discretization. We present two examples of finite-mode\napproximations that in some respect preserve the geometric attributes inherited\nfrom their continuous models: a three-component model of the barotropic\nvorticity equation known as Lorenz' maximum simplification equations [Tellus,\n\\textbf{12}, 243--254 (1960)] and a six-component model of the two-dimensional\nRayleigh--B\\'{e}nard convection problem. It is reviewed that the Lorenz--1960\nmodel respects both the maximal set of admitted point symmetries and an\nextension of the noncanonical Hamiltonian form (Nambu form). In a similar\nfashion, it is proved that the famous Lorenz--1963 model violates the\nstructural properties of the Saltzman equations and hence cannot be considered\nas the maximum simplification of the Rayleigh--B\\'{e}nard convection problem.\nUsing a six-component truncation, we show that it is again possible retaining\nboth symmetries and the Nambu representation in the course of discretization.\nThe conservative part of this six-component reduction is related to the\nLagrange top equations. Dissipation is incorporated using a metric tensor. \n\n"}
{"id": "0909.2583", "contents": "Title: Field theoretical approach to the description of the coherent structures\n  in 2D fluids and plasmas Abstract: Evolving from turbulent states the 2D fluids and the plasmas reach states\ncharacterized by a high degree of order, consisting of few vortices. These\nasymptotic states represent a small subset in the space of functions and are\ncharacterised by properties that are difficult to identify in a direct\napproach. The field theoretical approach to the dynamics and to the asymptotic\nstates of fluids and plasmas in 2D provides a considerable extension of the\nusual perspective. The present works discusses a series of consequences of the\nfield theoretical approach, when it is applied to particular problems. The\ndiscussion is developed around known physical problems: the current density\nprofiles in cylindrical plasma, the density pinch in tokamak and the\nconcentration of vorticity. \n\n"}
{"id": "0910.3063", "contents": "Title: Enhanced free space beam capture by improved optical tapers Abstract: In our continuous variable quantum key distribution (QKD) scheme, the\nhomodyne detection set-up requires balancing the intensity of an incident beam\nbetween two photodiodes. Realistic lens systems are insufficient to provide a\nspatially stable focus in the presence of large spatial beam-jitter caused by\natmospheric transmission. We therefore present an improved geometry for optical\ntapers which offer up to four times the angular tolerance of a lens. The\neffective area of a photodiode can thus be increased, without decreasing its\nbandwidth. This makes them suitable for use in our free space QKD experiment\nand in free space optical communication in general. \n\n"}
{"id": "0911.0221", "contents": "Title: Limit theorems for some adaptive MCMC algorithms with subgeometric\n  kernels: Part II Abstract: We prove a central limit theorem for a general class of adaptive Markov Chain\nMonte Carlo algorithms driven by sub-geometrically ergodic Markov kernels. We\ndiscuss in detail the special case of stochastic approximation. We use the\nresult to analyze the asymptotic behavior of an adaptive version of the\nMetropolis Adjusted Langevin algorithm with a heavy tailed target density. \n\n"}
{"id": "0911.0522", "contents": "Title: Can the Adaptive Metropolis Algorithm Collapse Without the Covariance\n  Lower Bound? Abstract: The Adaptive Metropolis (AM) algorithm is based on the symmetric random-walk\nMetropolis algorithm. The proposal distribution has the following\ntime-dependent covariance matrix at step $n+1$ \\[\n  S_n = Cov(X_1,...,X_n) + \\epsilon I, \\] that is, the sample covariance matrix\nof the history of the chain plus a (small) constant $\\epsilon>0$ multiple of\nthe identity matrix $I$. The lower bound on the eigenvalues of $S_n$ induced by\nthe factor $\\epsilon I$ is theoretically convenient, but practically\ncumbersome, as a good value for the parameter $\\epsilon$ may not always be easy\nto choose. This article considers variants of the AM algorithm that do not\nexplicitly bound the eigenvalues of $S_n$ away from zero. The behaviour of\n$S_n$ is studied in detail, indicating that the eigenvalues of $S_n$ do not\ntend to collapse to zero in general. \n\n"}
{"id": "0911.0741", "contents": "Title: Influence of the condensate and inverse cascade on the direct cascade in\n  wave turbulence Abstract: During direct numerical simulation of the isotropic turbulence of surface\ngravity waves in the framework of Hamiltonian equations formation of the long\nwave background or condensate was observed. Exponents of the direct cascade\nspectra at the different levels of an artificial condensate suppression show a\ntendency to become closer to the prediction of the wave turbulence theory at\nlower levels of condensate. A simple qualitative explanation of the mechanism\nof this phenomenon is proposed. \n\n"}
{"id": "0911.1705", "contents": "Title: Simulation-based model selection for dynamical systems in systems and\n  population biology Abstract: Computer simulations have become an important tool across the biomedical\nsciences and beyond. For many important problems several different models or\nhypotheses exist and choosing which one best describes reality or observed data\nis not straightforward. We therefore require suitable statistical tools that\nallow us to choose rationally between different mechanistic models of e.g.\nsignal transduction or gene regulation networks. This is particularly\nchallenging in systems biology where only a small number of molecular species\ncan be assayed at any given time and all measurements are subject to\nmeasurement uncertainty. Here we develop such a model selection framework based\non approximate Bayesian computation and employing sequential Monte Carlo\nsampling. We show that our approach can be applied across a wide range of\nbiological scenarios, and we illustrate its use on real data describing\ninfluenza dynamics and the JAK-STAT signalling pathway. Bayesian model\nselection strikes a balance between the complexity of the simulation models and\ntheir ability to describe observed data. The present approach enables us to\nemploy the whole formal apparatus to any system that can be (efficiently)\nsimulated, even when exact likelihoods are computationally intractable. \n\n"}
{"id": "0911.1904", "contents": "Title: Improving the expected accuracy of forecasts of future climate using a\n  simple bias-variance tradeoff Abstract: We describe a simple method that utilises the standard idea of bias-variance\ntrade-off to improve the expected accuracy of numerical model forecasts of\nfuture climate. The method can be thought of as an optimal multi-model\ncombination between the forecast from a numerical model multi-model ensemble,\non one hand, and a simple statistical forecast, on the other. We apply the\nmethod to predictions for UK temperature and precipitation for the period 2010\nto 2100. The temperature predictions hardly change, while the precipitation\npredictions show large changes. \n\n"}
{"id": "0911.5689", "contents": "Title: Energetics of PCMDI/CMIP3 Climate Models: Energy Budget and Meridional\n  Enthalpy Transport Abstract: We analyze the PCMDI/CMIP3 simulations performed by climate models (CMs)\nusing pre-industrial and SRESA1B scenarios. Relatively large biases are present\nfor most CMs when global energy budgets and when the atmospheric, oceanic, and\nland budgets are considered. Apparently, the biases do not result from\ntransient effects, but depend on the imperfect closure of the energy cycle in\nthe fluid components and on inconsistencies over land. Therefore, the planetary\nemission temperature is underestimated. This may explain the CMs' cold bias. In\nthe pre-industrial scenario, CMs agree on the location in the mid-latitudes of\nthe peaks of the meridional atmospheric enthalpy transport, while large\ndiscrepancies exist on the intensity. Disagreements on the location and\nintensity of the oceanic transport peaks are serious. With increased $CO_2$\nconcentration, a small poleward shift of the peak and an increase in the\nintensity of the atmospheric transport of up to 10% are detected in both\nhemispheres. Instead, most CMs feature a decrease in the oceanic transport\nintensity in the northern hemisphere and an equatorward shift of the peak in\nboth hemispheres. The Bjerkens compensation mechanism is active both on\nclimatological and interannual time scales. The peak of the total meridional\ntransport is typically around $35^\\circ$ in both hemispheres and scenarios,\nwhereas disagreements on the intensity are relevant. With increased $CO_2$\nconcentration, the total transport increases by up to 10%, thus contributing to\npolar amplification. Advances in the representation of physical processes are\ndefinitely needed for providing a self-consistent representation of climate as\na non-equilibrium thermodynamical system. \n\n"}
{"id": "0912.1586", "contents": "Title: Dynamic Trees for Learning and Design Abstract: Dynamic regression trees are an attractive option for automatic regression\nand classification with complicated response surfaces in on-line application\nsettings. We create a sequential tree model whose state changes in time with\nthe accumulation of new data, and provide particle learning algorithms that\nallow for the efficient on-line posterior filtering of tree-states. A major\nadvantage of tree regression is that it allows for the use of very simple\nmodels within each partition. The model also facilitates a natural division of\nlabor in our sequential particle-based inference: tree dynamics are defined\nthrough a few potential changes that are local to each newly arrived\nobservation, while global uncertainty is captured by the ensemble of particles.\nWe consider both constant and linear mean functions at the tree leaves, along\nwith multinomial leaves for classification problems, and propose default prior\nspecifications that allow for prediction to be integrated over all model\nparameters conditional on a given tree. Inference is illustrated in some\nstandard nonparametric regression examples, as well as in the setting of\nsequential experiment design, including both active learning and optimization\napplications, and in on-line classification. We detail implementation\nguidelines and problem specific methodology for each of these motivating\napplications. Throughout, it is demonstrated that our practical approach is\nable to provide better results compared to commonly used methods at a fraction\nof the cost. \n\n"}
{"id": "0912.1772", "contents": "Title: Self-Similarity in Fully Developed Homogeneous Isotropic Turbulence\n  Using the Lyapunov Analysis Abstract: In this work, we calculate the self-similar longitudinal velocity correlation\nfunction and the statistical properties of velocity difference using the\nresults of the Lyapunov analysis of the fully developed isotropic homogeneous\nturbulence just presented by the author in a previous work (arXiv:0911.1463).\nThere, a closure of the von Karman-Howarth equation is proposed and the\nstatistics of velocity difference is determined through a specific analysis of\nthe Fourier-transformed Navier-Stokes equations.\n  The correlation functions correspond to steady-state solutions of the von\nKarman-Howarth equation under the self-similarity hypothesis introduced by von\nKarman. These solutions are numerically determined with the statistics of\nvelocity difference. The obtained results adequately describe the several\nproperties of the fully developed isotropic turbulence. \n\n"}
{"id": "0912.4319", "contents": "Title: Empirical analysis of the solar contribution to global mean air surface\n  temperature change Abstract: The solar contribution to global mean air surface temperature change is\nanalyzed by using an empirical bi-scale climate model characterized by both\nfast and slow characteristic time responses to solar forcing: $\\tau_1 =0.4 \\pm\n0.1$ yr, and $\\tau_2= 8 \\pm 2$ yr or $\\tau_2=12 \\pm 3$ yr. Since 1980 the solar\ncontribution to climate change is uncertain because of the severe uncertainty\nof the total solar irradiance satellite composites. The sun may have caused\nfrom a slight cooling, if PMOD TSI composite is used, to a significant warming\n(up to 65% of the total observed warming) if ACRIM, or other TSI composites are\nused. The model is calibrated only on the empirical 11-year solar cycle\nsignature on the instrumental global surface temperature since 1980. The model\nreconstructs the major temperature patterns covering 400 years of solar induced\ntemperature changes, as shown in recent paleoclimate global temperature\nrecords. \n\n"}
{"id": "1001.0776", "contents": "Title: Atmospheric Variations as observed by IceCube Abstract: We have measured the correlation of rates in IceCube with long and short term\nvariations in the South Pole atmosphere. The yearly temperature variation in\nthe middle stratosphere (30-60 hPa) is highly correlated with the high energy\nmuon rate observed deep in the ice, and causes a +/-10% seasonal modulation in\nthe event rate. The counting rates of the surface detectors, which are due to\nsecondary particles of relatively low energy (muons, electrons and photons),\nhave a negative correlation with temperatures in the lower layers of the\nstratosphere (40-80 hPa), and are modulated at a level of +/-5%. The region of\nthe atmosphere between pressure levels 20-120 hPa, where the first cosmic ray\ninteractions occur and the produced pions/kaons interact or decay to muons, is\nthe Antarctic ozone layer. The anticorrelation between surface and deep ice\ntrigger rates reflects the properties of pion/kaon decay and interaction as the\ndensity of the stratospheric ozone layer changes. Therefore, IceCube closely\nprobes the ozone hole dynamics, and the temporal behavior of the stratospheric\ntemperatures. \n\n"}
{"id": "1001.1304", "contents": "Title: Optical turbulence vertical distribution with standard and high\n  resolution at Mt. Graham Abstract: A characterization of the optical turbulence vertical distribution (Cn2\nprofiles) and all the main integrated astroclimatic parameters derived from the\nCn2 and the wind speed profiles above the site of the Large Binocular Telescope\n(Mt. Graham, Arizona, US) is presented. The statistic includes measurements\nrelated to 43 nights done with a Generalized Scidar (GS) used in standard\nconfiguration with a vertical resolution Delta(H)~1 km on the whole 20 km and\nwith the new technique (HVR-GS) in the first kilometer. The latter achieves a\nresolution Delta(H)~20-30 m in this region of the atmosphere. Measurements done\nin different periods of the year permit us to provide a seasonal variation\nanalysis of the Cn2. A discretized distribution of Cn2 useful for the Ground\nLayer Adaptive Optics (GLAO) simulations is provided and a specific analysis\nfor the LBT Laser Guide Star system ARGOS (running in GLAO configuration) case\nis done including the calculation of the 'gray zones' for J, H and K bands. Mt.\nGraham confirms to be an excellent site with median values of the seeing\nwithout dome contribution epsilon = 0.72\", the isoplanatic angle theta0 = 2.5\"\nand the wavefront coherence time tau0= 4.8 msec. We find that the optical\nturbulence vertical distribution decreases in a much sharper way than what has\nbeen believed so far in proximity of the ground above astronomical sites. We\nfind that 50% of the whole turbulence develops in the first 80+/-15 m from the\nground. We finally prove that the error in the normalization of the\nscintillation that has been recently put in evidence in the principle of the GS\ntechnique, affects these measurements with an absolutely negligible quantity\n(0.04\"). \n\n"}
{"id": "1001.1857", "contents": "Title: The air pressure effect on the homogeneous nucleation of carbon dioxide\n  by molecular simulation Abstract: Vapour-liquid equilibria (VLE) and the influence of an inert carrier gas on\nhomogeneous vapour to liquid nucleation are investigated by molecular\nsimulation for quaternary mixtures of carbon dioxide, nitrogen, oxygen, and\nargon. Canonical ensemble molecular dynamics simulation using the\nYasuoka-Matsumoto method is applied to nucleation in supersaturated vapours\nthat contain more carbon dioxide than in the saturated state at the dew line.\nEstablished molecular models are employed that are known to accurately\nreproduce the VLE of the pure fluids as well as their binary and ternary\nmixtures. On the basis of these models, also the quaternary VLE properties of\nthe bulk fluid are determined with the Grand Equilibrium method.\n  Simulation results for the carrier gas influence on the nucleation rate are\ncompared with the classical nucleation theory (CNT) considering the \"pressure\neffect\" [Phys. Rev. Lett. 101: 125703 (2008)]. It is found that the presence of\nair as a carrier gas decreases the nucleation rate only slightly and, in\nparticular, to a significantly lower extent than predicted by CNT. The\nnucleation rate of carbon dioxide is generally underestimated by CNT, leading\nto a deviation between one and two orders of magnitude for pure carbon dioxide\nin the vicinity of the spinodal line and up to three orders of magnitude in\npresence of air as a carrier gas. Furthermore, CNT predicts a temperature\ndependence of the nucleation rate in the spinodal limit, which cannot be\nconfirmed by molecular simulation. \n\n"}
{"id": "1001.2136", "contents": "Title: An alternative marginal likelihood estimator for phylogenetic models Abstract: Bayesian phylogenetic methods are generating noticeable enthusiasm in the\nfield of molecular systematics. Many phylogenetic models are often at stake and\ndifferent approaches are used to compare them within a Bayesian framework. The\nBayes factor, defined as the ratio of the marginal likelihoods of two competing\nmodels, plays a key role in Bayesian model selection. We focus on an\nalternative estimator of the marginal likelihood whose computation is still a\nchallenging problem. Several computational solutions have been proposed none of\nwhich can be considered outperforming the others simultaneously in terms of\nsimplicity of implementation, computational burden and precision of the\nestimates. Practitioners and researchers, often led by available software, have\nprivileged so far the simplicity of the harmonic mean estimator (HM) and the\narithmetic mean estimator (AM). However it is known that the resulting\nestimates of the Bayesian evidence in favor of one model are biased and often\ninaccurate up to having an infinite variance so that the reliability of the\ncorresponding conclusions is doubtful. Our new implementation of the\ngeneralized harmonic mean (GHM) idea recycles MCMC simulations from the\nposterior, shares the computational simplicity of the original HM estimator,\nbut, unlike it, overcomes the infinite variance issue. The alternative\nestimator is applied to simulated phylogenetic data and produces fully\nsatisfactory results outperforming those simple estimators currently provided\nby most of the publicly available software. \n\n"}
{"id": "1002.0764", "contents": "Title: Formation of bound states of electrons in spherically symmetric\n  oscillations of plasma Abstract: We study spherically symmetric oscillations of electrons in plasma in the\nframe of classical electrodynamics. Firstly, we analyze the electromagnetic\npotentials for the system of radially oscillating charged particles. Secondly,\nwe consider both free and forced spherically symmetric oscillations of\nelectrons. Finally, we discuss the interaction between radially oscillating\nelectrons through the exchange of ion acoustic waves. It is obtained that the\neffective potential of this interaction can be attractive and can transcend the\nDebye-Huckel potential. We suggest that oscillating electrons can form bound\nstates at the initial stages of the spherical plasma structure evolution. The\npossible applications of the obtained results for the theory of natural\nplasmoids are examined. \n\n"}
{"id": "1002.2702", "contents": "Title: Bayesian computational methods Abstract: In this chapter, we will first present the most standard computational\nchallenges met in Bayesian Statistics, focussing primarily on mixture\nestimation and on model choice issues, and then relate these problems with\ncomputational solutions. Of course, this chapter is only a terse introduction\nto the problems and solutions related to Bayesian computations. For more\ncomplete references, see Robert and Casella (2004, 2009), or Marin and Robert\n(2007), among others. We also restrain from providing an introduction to\nBayesian Statistics per se and for comprehensive coverage, address the reader\nto Robert (2007), (again) among others. \n\n"}
{"id": "1002.3784", "contents": "Title: Estimation for High-Dimensional Linear Mixed-Effects Models Using\n  $\\ell_1$-Penalization Abstract: We propose an $\\ell_1$-penalized estimation procedure for high-dimensional\nlinear mixed-effects models. The models are useful whenever there is a grouping\nstructure among high-dimensional observations, i.e. for clustered data. We\nprove a consistency and an oracle optimality result and we develop an algorithm\nwith provable numerical convergence. Furthermore, we demonstrate the\nperformance of the method on simulated and a real high-dimensional data set. \n\n"}
{"id": "1003.0207", "contents": "Title: Sprite discharges on Venus and Jupiter-like planets: a laboratory\n  investigation Abstract: Large sprite discharges at high atmospheric altitudes have been found to be\nphysically similar to small streamer discharges in air at sea level density.\nBased on this understanding, we investigate possible sprite discharges on Venus\nor Jupiter-like planets through laboratory experiments on streamers in\nappropriate CO2-N2 and H2-He mixtures. First, the scaling laws are\nexperimentally confirmed by varying the density of the planetary gasses. Then\nstreamer diameters, velocities and overall morphology are investigated for\nsprites on Venus and Jupiter; they are quite similar to those on earth, but\nlight emissions in the visible range are fainter by two orders of magnitude.\nThe discharge spectra are measured; they are dominated by the minority species\nN2 on Venus, while signatures of both species are found on Jupiter-like\nplanets. The spectrum of a fully developed spark on Venus is measured. We show\nthat this spectrum is significantly different from the expected sprite\nspectrum. \n\n"}
{"id": "1003.0804", "contents": "Title: Branch and Bound Algorithms for Maximizing Expected Improvement\n  Functions Abstract: Deterministic computer simulations are often used as a replacement for\ncomplex physical experiments. Although less expensive than physical\nexperimentation, computer codes can still be time-consuming to run. An\neffective strategy for exploring the response surface of the deterministic\nsimulator is the use of an approximation to the computer code, such as a\nGaussian process (GP) model, coupled with a sequential sampling strategy for\nchoosing design points that can be used to build the GP model. The ultimate\ngoal of such studies is often the estimation of specific features of interest\nof the simulator output, such as the maximum, minimum, or a level set\n(contour). Before approximating such features with the GP model, sufficient\nruns of the computer simulator must be completed.\n  Sequential designs with an expected improvement (EI) function can yield good\nestimates of the features with a minimal number of runs. The challenge is that\nthe expected improvement function itself is often multimodal and difficult to\nmaximize. We develop branch and bound algorithms for efficiently maximizing the\nEI function in specific problems, including the simultaneous estimation of a\nminimum and a maximum, and in the estimation of a contour. These branch and\nbound algorithms outperform other optimization strategies such as genetic\nalgorithms, and over a number of sequential design steps can lead to\ndramatically superior accuracy in estimation of features of interest. \n\n"}
{"id": "1003.1771", "contents": "Title: Data Driven Computing by the Morphing Fast Fourier Transform Ensemble\n  Kalman Filter in Epidemic Spread Simulations Abstract: The FFT EnKF data assimilation method is proposed and applied to a stochastic\ncell simulation of an epidemic, based on the S-I-R spread model. The FFT EnKF\ncombines spatial statistics and ensemble filtering methodologies into a\nlocalized and computationally inexpensive version of EnKF with a very small\nensemble, and it is further combined with the morphing EnKF to assimilate\nchanges in the position of the epidemic. \n\n"}
{"id": "1003.5075", "contents": "Title: Invariant measures of the 2D Euler and Vlasov equations Abstract: We discuss invariant measures of partial differential equations such as the\n2D Euler or Vlasov equations. For the 2D Euler equations, starting from the\nLiouville theorem, valid for N-dimensional approximations of the dynamics, we\ndefine the microcanonical measure as a limit measure where N goes to infinity.\nWhen only the energy and enstrophy invariants are taken into account, we give\nan explicit computation to prove the following result: the microcanonical\nmeasure is actually a Young measure corresponding to the maximization of a\nmean-field entropy. We explain why this result remains true for more general\nmicrocanonical measures, when all the dynamical invariants are taken into\naccount. We give an explicit proof that these microcanonical measures are\ninvariant measures for the dynamics of the 2D Euler equations. We describe a\nmore general set of invariant measures, and discuss briefly their stability and\ntheir consequence for the ergodicity of the 2D Euler equations. The extension\nof these results to the Vlasov equations is also discussed, together with a\nproof of the uniqueness of statistical equilibria, for Vlasov equations with\nrepulsive convex potentials. Even if we consider, in this paper, invariant\nmeasures only for Hamiltonian equations, with no fluxes of conserved\nquantities, we think this work is an important step towards the description of\nnon-equilibrium invariant measures with fluxes. \n\n"}
{"id": "1004.1950", "contents": "Title: Finite volume schemes for dispersive wave propagation and runup Abstract: Finite volume schemes are commonly used to construct approximate solutions to\nconservation laws. In this study we extend the framework of the finite volume\nmethods to dispersive water wave models, in particular to Boussinesq type\nsystems. We focus mainly on the application of the method to bidirectional\nnonlinear, dispersive wave propagation in one space dimension. Special emphasis\nis given to important nonlinear phenomena such as solitary waves interactions,\ndispersive shock wave formation and the runup of breaking and non-breaking long\nwaves. \n\n"}
{"id": "1004.2840", "contents": "Title: Robust Parameter Selection for Parallel Tempering Abstract: This paper describes an algorithm for selecting parameter values (e.g.\ntemperature values) at which to measure equilibrium properties with Parallel\nTempering Monte Carlo simulation. Simple approaches to choosing parameter\nvalues can lead to poor equilibration of the simulation, especially for Ising\nspin systems that undergo $1^st$-order phase transitions. However, starting\nfrom an initial set of parameter values, the careful, iterative respacing of\nthese values based on results with the previous set of values greatly improves\nequilibration. Example spin systems presented here appear in the context of\nQuantum Monte Carlo. \n\n"}
{"id": "1004.2910", "contents": "Title: Conservative Hypothesis Tests and Confidence Intervals using Importance\n  Sampling Abstract: Importance sampling is a common technique for Monte Carlo approximation,\nincluding Monte Carlo approximation of p-values. Here it is shown that a simple\ncorrection of the usual importance sampling p-values creates valid p-values,\nmeaning that a hypothesis test created by rejecting the null when the p-value\nis <= alpha will also have a type I error rate <= alpha. This correction uses\nthe importance weight of the original observation, which gives valuable\ndiagnostic information under the null hypothesis. Using the corrected p-values\ncan be crucial for multiple testing and also in problems where evaluating the\naccuracy of importance sampling approximations is difficult. Inverting the\ncorrected p-values provides a useful way to create Monte Carlo confidence\nintervals that maintain the nominal significance level and use only a single\nMonte Carlo sample. Several applications are described, including accelerated\nmultiple testing for a large neurophysiological dataset and exact conditional\ninference for a logistic regression model with nuisance parameters. \n\n"}
{"id": "1004.3189", "contents": "Title: Short term forecasting of surface layer wind speed using a continuous\n  cascade model Abstract: This paper describes a statistical method for short-term forecasting of\nsurface layer wind velocity amplitude relying on the notion of continuous\ncascades. Inspired by recent empirical findings that suggest the existence of\nsome cascading process in the mesoscale range, we consider that wind speed can\nbe described by a seasonal component and a fluctuating part represented by a\n\"multifractal noise\" associated with a random cascade. Performances of our\nmodel are tested on hourly wind speed series gathered at various locations in\nCorsica (France) and Netherlands. The obtained results show a systematic\nimprovement of the prediction as compared to reference models like persistence\nor Artificial Neural Networks. \n\n"}
{"id": "1004.3457", "contents": "Title: General Temporal Instability Criteria For Stably Stratified Inviscid\n  Flow Abstract: The temporal instability of stably stratified flow was investigated by\nanalyzing the Taylor-Goldstein equation theoretically. According to this\nanalysis, the stable stratification $N^2\\geq0$ has a destabilization mechanism,\nand the flow instability is due to the competition of the kinetic energy with\nthe potential energy, which is dominated by the total Froude number $Fr_t^2$.\nGlobally, $Fr_t^2 \\leq 1$ implies that the total kinetic energy is smaller than\nthe total potential energy. So the potential energy might transfer to the\nkinetic energy after being disturbed, and the flow becomes unstable. On the\nother hand, when the potential energy is smaller than the kinetic energy\n($Fr_t^2>1$), the flow is stable because no potential energy could transfer to\nthe kinetic energy. The flow is more stable with the velocity profile\n$U'/U'''>0$ than that with $U'/U'''<0$. Besides, the unstable perturbation must\nbe long-wave scale. Locally, the flow is unstable as the gradient Richardson\nnumber $Ri>1/4$. These results extend the Rayleigh's, Fj{\\o}rtoft's, Sun's and\nArnol'd's criteria for the inviscid homogenous fluid, but they contradict the\nwell-known Miles-Howard theorem. It is argued here that the transform\n$F=\\phi/(U-c)^n$ is not suitable for temporal stability problem, and that it\nwill lead to contradictions with the results derived from the Taylor-Goldstein\nequation. However, such transform might be useful for the study of the\nOrr-Sommerfeld equation in viscous flows. \n\n"}
{"id": "1004.3480", "contents": "Title: Fully nonlinear weakly dispersive modelling of wave transformation,\n  breaking and runup Abstract: To describe the strongly nonlinear dynamics of waves propagating in the final\nstages of shoaling and in the surf and swash zones, fully nonlinear models are\nrequired. The ability of the Serre or Green Naghdi (S-GN) equations to\nreproduce this nonlinear processes is reviewed. Two high-order methods for\nsolving S-GN equations, based on Finite Volume approaches, are presented. The\nfirst one is based on a quasi-conservative form of the S-GN equations, and the\nsecond on a hybrid Finite Volume/Finite Difference method. We show the ability\nof these two approaches to accurately simulate nonlinear shoaling, breaking and\nrunup processes. \n\n"}
{"id": "1004.4041", "contents": "Title: Pooling Design and Bias Correction in DNA Library Screening Abstract: We study the group test for DNA library screening based on probabilistic\napproach. Group test is a method of detecting a few positive items from among a\nlarge number of items, and has wide range of applications. In DNA library\nscreening, positive item corresponds to the clone having a specified DNA\nsegment, and it is necessary to identify and isolate the positive clones for\ncompiling the libraries. In the group test, a group of items, called pool, is\nassayed in a lump in order to save the cost of testing, and positive items are\ndetected based on the observation from each pool. It is known that the design\nof grouping, that is, pooling design is important to %reduce the estimation\nbias and achieve accurate detection. In the probabilistic approach, positive\nclones are picked up based on the posterior probability. Naive methods of\ncomputing the posterior, however, involves exponentially many sums, and thus we\nneed a device. Loopy belief propagation (loopy BP) algorithm is one of popular\nmethods to obtain approximate posterior probability efficiently. There are some\nworks investigating the relation between the accuracy of the loopy BP and the\npooling design. Based on these works, we develop pooling design with small\nestimation bias of posterior probability, and we show that the balanced\nincomplete block design (BIBD) has nice property for our purpose. Some\nnumerical experiments show that the bias correction under the BIBD is useful to\nimprove the estimation accuracy. \n\n"}
{"id": "1005.1153", "contents": "Title: Transcranial stimulability of phosphenes by long lightning\n  electromagnetic pulses Abstract: The electromagnetic pulses of rare long (order of seconds) repetitive\nlightning discharges near strike point (order of 100m) are analyzed and\ncompared to magnetic fields applied in standard clinical transcranial magnetic\nstimulation (TMS) practice. It is shown that the time-varying lightning\nmagnetic fields and locally induced potentials are in the same order of\nmagnitude and frequency as those established in TMS experiments to study\nstimulated perception phenomena, like magnetophosphenes. Lightning\nelectromagnetic pulse induced transcranial magnetic stimulation of phosphenes\nin the visual cortex is concluded to be a plausible interpretation of a large\nclass of reports on luminous perceptions during thunderstorms. APPENDIX\n(Erratum and Addendum by J. Peer, V. Cooray, G. Cooray and A. Kendl): The\ncomparison of electric fields transcranially induced by lightning discharges\nand by TMS brain stimulators via View E = - dA/dt is shown to be inappropriate.\nCorrected results with respect to evaluation of phosphene stimulability are\npresented. For average lightning parameters the correct induced electric fields\nappear more than an order of magnitude smaller. For typical ranges of stronger\nthan average lightning currents, electric fields above the threshold for\ncortical phosphene stimulation can be induced only for short distances (order\nof meters), or in medium distances (order of 50 m) only for pulses shorter than\nestablished axon excitation periods. Stimulation of retinal phosphene\nperception has much lower threshold and appears most probable for lightning\nelectromagnetic fields. \n\n"}
{"id": "1005.2354", "contents": "Title: Objective Probabilistic Forecasts of Future Climate Based on Jeffreys'\n  Prior: the Case of Correlated Observables Abstract: To include parameter uncertainty into probabilistic climate forecasts one\nmust first specify a prior. We advocate the use of objective priors, and, in\nparticular, the Jeffreys' Prior. In previous work we have derived expressions\nfor the Jeffreys' Prior for the case in which the observations are independent\nand normally distributed. These expressions make the calculation of the prior\nmuch simpler than evaluation directly from the definition. In this paper, we\nnow relax the independence assumption and derive expressions for the Jeffreys'\nPrior for the case in which the observations are distributed with a\nmultivariate normal distribution with constant covariances. Again, these\nexpressions simplify the calculation of the prior: in this case they reduce it\nto the calculation of the differences between the ensemble means of climate\nmodel ensembles based on different parameter settings. These calculations are\nsimple enough to be applied to even the most complex climate models. \n\n"}
{"id": "1005.3907", "contents": "Title: Objective Climate Model Predictions Using Jeffreys' Prior: the General\n  Multivariate Normal Case Abstract: Objective probabilistic forecasts of future climate that include parameter\nuncertainty can be made by using the Bayesian prediction integral with the\nprior set to Jeffreys' Prior. The calculations involved in determining the\nprior can then be simplified by making parametric assumptions about the\ndistribution of the output from the climate model. The most obvious assumption\nto make is that the climate model output is normally distributed, in which case\nevaluating the prior becomes a question of evaluating gradients in the\nparameters of the normal distribution. In previous work we have considered the\nspecial cases of diagonal (but not constant) covariance matrix, and constant\n(but not diagonal) covariance matrix. We now derive expressions for the general\nmultivariate normal distribution, with non-constant non-diagonal covariance\nmatrix. The algebraic manipulation required is more complex than for the\nspecial cases, and involves some slightly esoteric matrix operations including\ntaking the expectation of a vector quadratic form and differentiating the\ndeterminants, traces and inverses of matrices. \n\n"}
{"id": "1005.4565", "contents": "Title: A stability criterion for two-fluid interfaces and applications Abstract: We derive here a new stability criterion for two-fluid interfaces. This\ncriterion ensures the existence of \"stable\" local solutions that do no break\ndown too fast due to Kelvin-Helmholtz instabilities. It can be seen both as a\ntwo-fluid generalization of the Rayleigh-Taylor criterion and as a nonlinear\nversion of the Kelvin stability condition. We show that gravity can control the\ninertial effects of the shear up to frequencies that are high enough for the\nsurface tension to play a relevant role. This explains why surface tension is a\nnecessary condition for well-posedness while the (low frequency) main dynamics\nof interfacial waves is unaffected by it. In order to derive a practical\nversion of this criterion, we work with a nondimensionalized version of the\nequations and allow for the possibility of various asymptotic regimes, such as\nthe shallow water limit. This limit being singular, we have to derive a new\nsymbolic analysis of the Dirichlet-Neumann operator that includes an infinitely\nsmoothing \"tail\" accounting for the contribution of the bottom. We then\nvalidate our criterion by comparison with experimental data in two important\nsettings: air-water interfaces and internal waves. The good agreement we\nobserve allows us to discuss the scenario of wave breaking and the behavior of\nwater-brine interfaces. We also show how to rigorously justify two-fluid\nasymptotic models used for applications and how to relate some of their\nproperties to Kelvin-Helmholtz instabilities. \n\n"}
{"id": "1006.3002", "contents": "Title: Free energy Sequential Monte Carlo, application to mixture modelling Abstract: We introduce a new class of Sequential Monte Carlo (SMC) methods, which we\ncall free energy SMC. This class is inspired by free energy methods, which\noriginate from Physics, and where one samples from a biased distribution such\nthat a given function $\\xi(\\theta)$ of the state $\\theta$ is forced to be\nuniformly distributed over a given interval. From an initial sequence of\ndistributions $(\\pi_t)$ of interest, and a particular choice of $\\xi(\\theta)$,\na free energy SMC sampler computes sequentially a sequence of biased\ndistributions $(\\tilde{\\pi}_{t})$ with the following properties: (a) the\nmarginal distribution of $\\xi(\\theta)$ with respect to $\\tilde{\\pi}_{t}$ is\napproximatively uniform over a specified interval, and (b) $\\tilde{\\pi}_{t}$\nand $\\pi_{t}$ have the same conditional distribution with respect to $\\xi$. We\napply our methodology to mixture posterior distributions, which are highly\nmultimodal. In the mixture context, forcing certain hyper-parameters to higher\nvalues greatly faciliates mode swapping, and makes it possible to recover a\nsymetric output. We illustrate our approach with univariate and bivariate\nGaussian mixtures and two real-world datasets. \n\n"}
{"id": "1007.3622", "contents": "Title: A generalized risk approach to path inference based on hidden Markov\n  models Abstract: Motivated by the unceasing interest in hidden Markov models (HMMs), this\npaper re-examines hidden path inference in these models, using primarily a\nrisk-based framework. While the most common maximum a posteriori (MAP), or\nViterbi, path estimator and the minimum error, or Posterior Decoder (PD), have\nlong been around, other path estimators, or decoders, have been either only\nhinted at or applied more recently and in dedicated applications generally\nunfamiliar to the statistical learning community. Over a decade ago, however, a\nfamily of algorithmically defined decoders aiming to hybridize the two standard\nones was proposed (Brushe et al., 1998). The present paper gives a careful\nanalysis of this hybridization approach, identifies several problems and issues\nwith it and other previously proposed approaches, and proposes practical\nresolutions of those. Furthermore, simple modifications of the classical\ncriteria for hidden path recognition are shown to lead to a new class of\ndecoders. Dynamic programming algorithms to compute these decoders in the usual\nforward-backward manner are presented. A particularly interesting subclass of\nsuch estimators can be also viewed as hybrids of the MAP and PD estimators.\nSimilar to previously proposed MAP-PD hybrids, the new class is parameterized\nby a small number of tunable parameters. Unlike their algorithmic predecessors,\nthe new risk-based decoders are more clearly interpretable, and, most\nimportantly, work \"out of the box\" in practice, which is demonstrated on some\nreal bioinformatics tasks and data. Some further generalizations and\napplications are discussed in conclusion. \n\n"}
{"id": "1008.0149", "contents": "Title: Bayesian Cointegrated Vector Autoregression models incorporating\n  Alpha-stable noise for inter-day price movements via Approximate Bayesian\n  Computation Abstract: We consider a statistical model for pairs of traded assets, based on a\nCointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR\nmodels to incorporate estimation of model parameters in the presence of price\nseries level shifts which are not accurately modeled in the standard Gaussian\nerror correction model (ECM) framework. This involves developing a novel matrix\nvariate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and\nAlpha-stable errors inter-day in the ECM framework. To achieve this we derive a\nnovel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)\nrepresentation of Alpha-stable inter-day innovations. These results are\ngeneralized to asymmetric models for the innovation noise at inter-day\nboundaries allowing for skewed Alpha-stable models.\n  Our proposed model and sampling methodology is general, incorporating the\ncurrent literature on Gaussian models as a special subclass and also allowing\nfor price series level shifts either at random estimated time points or known a\npriori time points. We focus analysis on regularly observed non-Gaussian level\nshifts that can have significant effect on estimation performance in\nstatistical models failing to account for such level shifts, such as at the\nclose and open of markets. We compare the estimation accuracy of our model and\nestimation approach to standard frequentist and Bayesian procedures for CVAR\nmodels when non-Gaussian price series level shifts are present in the\nindividual series, such as inter-day boundaries. We fit a bi-variate\nAlpha-stable model to the inter-day jumps and model the effect of such jumps on\nestimation of matrix-variate CVAR model parameters using the likelihood based\nJohansen procedure and a Bayesian estimation. We illustrate our model and the\ncorresponding estimation procedures we develop on both synthetic and actual\ndata. \n\n"}
{"id": "1008.1355", "contents": "Title: Control Variates for Reversible MCMC Samplers Abstract: A general methodology is introduced for the construction and effective\napplication of control variates to estimation problems involving data from\nreversible MCMC samplers. We propose the use of a specific class of functions\nas control variates, and we introduce a new, consistent estimator for the\nvalues of the coefficients of the optimal linear combination of these\nfunctions. The form and proposed construction of the control variates is\nderived from our solution of the Poisson equation associated with a specific\nMCMC scenario. The new estimator, which can be applied to the same MCMC sample,\nis derived from a novel, finite-dimensional, explicit representation for the\noptimal coefficients. The resulting variance-reduction methodology is primarily\napplicable when the simulated data are generated by a conjugate random-scan\nGibbs sampler. MCMC examples of Bayesian inference problems demonstrate that\nthe corresponding reduction in the estimation variance is significant, and that\nin some cases it can be quite dramatic. Extensions of this methodology in\nseveral directions are given, including certain families of Metropolis-Hastings\nsamplers and hybrid Metropolis-within-Gibbs algorithms. Corresponding\nsimulation examples are presented illustrating the utility of the proposed\nmethods. All methodological and asymptotic arguments are rigorously justified\nunder easily verifiable and essentially minimal conditions. \n\n"}
{"id": "1009.1523", "contents": "Title: Point symmetry group of the barotropic vorticity equation Abstract: The complete point symmetry group of the barotropic vorticity equation on the\n$\\beta$-plane is computed using the direct method supplemented with two\ndifferent techniques. The first technique is based on the preservation of any\nmegaideal of the maximal Lie invariance algebra of a differential equation by\nthe push-forwards of point symmetries of the same equation. The second\ntechnique involves a priori knowledge on normalization properties of a class of\ndifferential equations containing the equation under consideration. Both of\nthese techniques are briefly outlined. \n\n"}
{"id": "1009.2022", "contents": "Title: Estimation of a probability in inverse binomial sampling under\n  normalized linear-linear and inverse-linear loss Abstract: Sequential estimation of the success probability $p$ in inverse binomial\nsampling is considered in this paper. For any estimator $\\hat p$, its quality\nis measured by the risk associated with normalized loss functions of\nlinear-linear or inverse-linear form. These functions are possibly asymmetric,\nwith arbitrary slope parameters $a$ and $b$ for $\\hat p<p$ and $\\hat p>p$\nrespectively. Interest in these functions is motivated by their significance\nand potential uses, which are briefly discussed. Estimators are given for which\nthe risk has an asymptotic value as $p$ tends to $0$, and which guarantee that,\nfor any $p$ in $(0,1)$, the risk is lower than its asymptotic value. This\nallows selecting the required number of successes, $r$, to meet a prescribed\nquality irrespective of the unknown $p$. In addition, the proposed estimators\nare shown to be approximately minimax when $a/b$ does not deviate too much from\n$1$, and asymptotically minimax as $r$ tends to infinity when $a=b$. \n\n"}
{"id": "1009.5544", "contents": "Title: Gamma-Ray Localization of Terrestrial Gamma-Ray Flashes Abstract: Terrestrial Gamma-Ray Flashes (TGFs) are very short bursts of high energy\nphotons and electrons originating in Earth's atmosphere. We present here a\nlocalization study of TGFs carried out at gamma-ray energies above 20 MeV based\non an innovative event selection method. We use the AGILE satellite Silicon\nTracker data that for the first time have been correlated with TGFs detected by\nthe AGILE Mini-Calorimeter. We detect 8 TGFs with gamma-ray photons of energies\nabove 20 MeV localized by the AGILE gamma-ray imager with an accuracy of 5-10\ndegrees at 50 MeV. Remarkably, all TGF-associated gamma rays are compatible\nwith a terrestrial production site closer to the sub-satellite point than 400\nkm. Considering that our gamma rays reach the AGILE satellite at 540 km\naltitude with limited scattering or attenuation, our measurements provide the\nfirst precise direct localization of TGFs from space. \n\n"}
{"id": "1010.0124", "contents": "Title: A model selection approach to genome wide association studies Abstract: For the vast majority of genome wide association studies (GWAS) published so\nfar, statistical analysis was performed by testing markers individually. In\nthis article we present some elementary statistical considerations which\nclearly show that in case of complex traits the approach based on multiple\nregression or generalized linear models is preferable to multiple testing. We\nintroduce a model selection approach to GWAS based on modifications of Bayesian\nInformation Criterion (BIC) and develop some simple search strategies to deal\nwith the huge number of potential models. Comprehensive simulations based on\nreal SNP data confirm that model selection has larger power than multiple\ntesting to detect causal SNPs in complex models. On the other hand multiple\ntesting has substantial problems with proper ranking of causal SNPs and tends\nto detect a certain number of false positive SNPs, which are not linked to any\nof the causal mutations. We show that this behavior is typical in GWAS for\ncomplex traits and can be explained by an aggregated influence of many small\nrandom sample correlations between genotypes of a SNP under investigation and\nother causal SNPs. We believe that our findings at least partially explain\nproblems with low power and nonreplicability of results in many real data GWAS.\nFinally, we discuss the advantages of our model selection approach in the\ncontext of real data analysis, where we consider publicly available gene\nexpression data as traits for individuals from the HapMap project. \n\n"}
{"id": "1010.0701", "contents": "Title: Axially and spherically symmetric solitons in warm plasma Abstract: We study the existence of stable axially and spherically symmetric plasma\nstructures on the basis of the new nonlinear Schrodinger equation (NLSE)\naccounting for nonlocal electron nonlinearities. The numerical solutions of\nNLSE having the form of spatial solitions are obtained and their stability is\nanalyzed. We discuss the possible application of the obtained results to the\ntheoretical description of natural plasmoids in the atmosphere. \n\n"}
{"id": "1010.1542", "contents": "Title: Lie symmetry analysis and exact solutions of the quasi-geostrophic\n  two-layer problem Abstract: The quasi-geostrophic two-layer model is of superior interest in dynamic\nmeteorology since it is one of the easiest ways to study baroclinic processes\nin geophysical fluid dynamics. The complete set of point symmetries of the\ntwo-layer equations is determined. An optimal set of one- and two-dimensional\ninequivalent subalgebras of the maximal Lie invariance algebra is constructed.\nOn the basis of these subalgebras we exhaustively carry out group-invariant\nreduction and compute various classes of exact solutions. Where possible,\nreference to the physical meaning of the exact solutions is given. In\nparticular, the well-known baroclinic Rossby wave solutions in the two-layer\nmodel are rediscovered. \n\n"}
{"id": "1010.3010", "contents": "Title: Symmetry preserving parameterization schemes Abstract: Methods for the design of physical parameterization schemes that possess\ncertain invariance properties are discussed. These methods are based on\ndifferent techniques of group classification and provide means to determine\nexpressions for unclosed terms arising in the course of averaging of nonlinear\ndifferential equations. The demand that the averaged equation is invariant with\nrespect to a subalgebra of the maximal Lie invariance algebra of the unaveraged\nequation leads to a problem of inverse group classification which is solved by\nthe description of differential invariants of the selected subalgebra. Given no\nprescribed symmetry group, the direct group classification problem is relevant.\nWithin this framework, the algebraic method or direct integration of\ndetermining equations for Lie symmetries can be applied. For cumbersome\nparameterizations, a preliminary group classification can be carried out. The\nmethods presented are exemplified by parameterizing the eddy vorticity flux in\nthe averaged vorticity equation. In particular, differential invariants of\n(infinite dimensional) subalgebras of the maximal Lie invariance algebra of the\nunaveraged vorticity equation are computed. A hierarchy of normalized\nsubclasses of generalized vorticity equations is constructed. Invariant\nparameterizations possessing minimal symmetry extensions are described and a\nrestricted class of invariant parameterization is exhaustively classified. The\nphysical importance of the parameterizations designed is discussed. \n\n"}
{"id": "1011.4457", "contents": "Title: Graphical Comparison of MCMC Performance Abstract: This paper presents a graphical method for comparing performance of Markov\nChain Monte Carlo methods. Most researchers present comparisons of MCMC methods\nusing tables of figures of merit; this paper presents a graphical alternative.\nIt first discusses the computation of autocorrelation time, then uses this to\nconstruct a figure of merit, log density function evaluations per independent\nobservation. Then, it demonstrates how one can plot this figure of merit\nagainst a tuning parameter in a grid of plots where columns represent sampling\nmethods and rows represent distributions. This type of visualization makes it\npossible to convey a greater depth of information without overwhelming the user\nwith numbers, allowing researchers to put their contributions into a broader\ncontext than is possible with a textual presentation. \n\n"}
{"id": "1011.4604", "contents": "Title: An Alternating Direction Method for Finding Dantzig Selectors Abstract: In this paper, we study the alternating direction method for finding the\nDantzig selectors, which are first introduced in [8]. In particular, at each\niteration we apply the nonmonotone gradient method proposed in [17] to\napproximately solve one subproblem of this method. We compare our approach with\na first-order method proposed in [3]. The computational results show that our\napproach usually outperforms that method in terms of CPU time while producing\nsolutions of comparable quality. \n\n"}
{"id": "1011.5038", "contents": "Title: Approximate simulation-free Bayesian inference for multiple changepoint\n  models with dependence within segments Abstract: This paper proposes approaches for the analysis of multiple changepoint\nmodels when dependency in the data is modelled through a hierarchical Gaussian\nMarkov random field. Integrated nested Laplace approximations are used to\napproximate data quantities, and an approximate filtering recursions approach\nis proposed for savings in compuational cost when detecting changepoints. All\nof these methods are simulation free. Analysis of real data demonstrates the\nusefulness of the approach in general. The new models which allow for data\ndependence are compared with conventional models where data within segments is\nassumed independent. \n\n"}
{"id": "1012.0421", "contents": "Title: Reply to \"Comment on 'Falsification Of The Atmospheric CO2 Greenhouse\n  Effects Within The Frame Of Physics' by Joshua B. Halpern, Christopher M.\n  Colose, Chris Ho-Stuart, Joel D. Shore, Arthur P. Smith, J\\\"org Zimmermann\" Abstract: It is shown that the notorious claim by Halpern et al. recently repeated in\ntheir comment that the method, logic, and conclusions of our \"Falsification Of\nThe CO2 Greenhouse Effects Within The Frame Of Physics\" would be in error has\nno foundation. Since Halpern et al. communicate our arguments incorrectly,\ntheir comment is scientifcally vacuous. In particular, it is not true that we\nare \"trying to apply the Clausius statement of the Second Law of Thermodynamics\nto only one side of a heat transfer process rather than the entire process\" and\nthat we are \"systematically ignoring most non-radiative heat flows applicable\nto Earth's surface and atmosphere\". Rather, our falsification paper discusses\nthe violation of fundamental physical and mathematical principles in 14\nexamples of common pseudo-derivations of fictitious greenhouse effects that are\nall based on simplistic pictures of radiative transfer and their obscure\nrelation to thermodynamics, including but not limited to those descriptions (a)\nthat define a \"Perpetuum Mobile Of The 2nd Kind\", (b) that rely on incorrectly\ncalculated averages of global temperatures, (c) that refer to incorrectly\nnormalized spectra of electromagnetic radiation. Halpern et al. completely\nmissed an exceptional chance to formulate a scientifically well-founded\nantithesis. They do not even define a greenhouse effect that they wish to\ndefend. We take the opportunity to clarify some misunderstandings, which are\ncommunicated in the current discussion on the non-measurable, i.e. physically\nnon-existing influence of the trace gas CO2 on the climates of the Earth. \n\n"}
{"id": "1012.5390", "contents": "Title: Forward Smoothing using Sequential Monte Carlo Abstract: Sequential Monte Carlo (SMC) methods are a widely used set of computational\ntools for inference in non-linear non-Gaussian state-space models. We propose a\nnew SMC algorithm to compute the expectation of additive functionals\nrecursively. Essentially, it is an online or forward-only implementation of a\nforward filtering backward smoothing SMC algorithm proposed in Doucet .et .al\n(2000). Compared to the standard path space SMC estimator whose asymptotic\nvariance increases quadratically with time even under favourable mixing\nassumptions, the asymptotic variance of the proposed SMC estimator only\nincreases linearly with time. This forward smoothing procedure allows us to\nimplement on-line maximum likelihood parameter estimation algorithms which do\nnot suffer from the particle path degeneracy problem. \n\n"}
{"id": "1101.0253", "contents": "Title: Chaos, storms and climate on Mars Abstract: Channel networks on the plateau adjacent to Juventae Chasma have the highest\ndrainage densities reported on Mars.We model frozen precipitation on the\nJuventae plateau,finding that the trigger for forming these channel networks\ncould have been ephemeral lakeshore precipitation,and that they do not require\npast temperatures higher than today.If short-lived and localized events explain\nsome dendritic channel networks on Mars, this would weaken the link between\ndendritic valley networks and surface climate conditions that could sustain\nlife. Our analysis uses MRAMS simulations and HiRISE DTMs.We model localized\nweather systems driven by water vapor release from ephemeral lakes during\noutflow channel formation.At Juventae Chasma,mean snowfall reaches a maximum of\n0.9mm/hr water equivalent on the SW rim of the chasm.Radiative effects of the\nthick cloud cover raise maximum (minimum, mean) plateau surface temperatures by\nup to 24K(9K, 17K)locally.The key result is that the area of maximum modeled\nprecipitation shows a striking correspondence to the mapped Juventae plateau\nchannel networks.Three independent methods show this fit is unlikely to be due\nto chance.We use a snowpack energy balance model to show that if the snow has\nthe albedo of dust(0.28), and for a solar luminosity of 0.8($\\equiv$3.0Gya),\nthen if the atmospheric greenhouse effect is unchanged from(6K warmer\nthan)today only 0.4%(21%)of lake-induced precipitation events produce snowpack\nthat undergoes melting.However, warming from associated dense cloud cover would\nallow melting over a wider range of conditions.In these localized precipitation\nscenarios, global temperatures need not be higher than today, and the rest of\nthe planet remains dry. \n\n"}
{"id": "1101.1729", "contents": "Title: Dispersive wave runup on non-uniform shores Abstract: Historically the finite volume methods have been developed for the numerical\nintegration of conservation laws. In this study we present some recent results\non the application of such schemes to dispersive PDEs. Namely, we solve\nnumerically a representative of Boussinesq type equations in view of important\napplications to the coastal hydrodynamics. Numerical results of the runup of a\nmoderate wave onto a non-uniform beach are presented along with great lines of\nthe employed numerical method (see D. Dutykh et al. (2011) for more details). \n\n"}
{"id": "1101.1986", "contents": "Title: Reply to Comment on `Formation of bound states of electrons in\n  spherically symmetric oscillations of plasma' Abstract: I reply here to the comment of Dr Shmatov on my recent work and demonstrate\nthe invalidity of his criticism of the classical physics description of the\nformation of bound states of electrons participating in spherically symmetric\noscillations of plasma. \n\n"}
{"id": "1101.4242", "contents": "Title: Efficient Bayesian inference in stochastic chemical kinetic models using\n  graphical processing units Abstract: A goal of systems biology is to understand the dynamics of intracellular\nsystems. Stochastic chemical kinetic models are often utilized to accurately\ncapture the stochastic nature of these systems due to low numbers of molecules.\nCollecting system data allows for estimation of stochastic chemical kinetic\nrate parameters. We describe a well-known, but typically impractical data\naugmentation Markov chain Monte Carlo algorithm for estimating these\nparameters. The impracticality is due to the use of rejection sampling for\nlatent trajectories with fixed initial and final endpoints which can have\ndiminutive acceptance probability. We show how graphical processing units can\nbe efficiently utilized for parameter estimation in systems that hitherto were\ninestimable. For more complex systems, we show the efficiency gain over\ntraditional CPU computing is on the order of 200. Finally, we show a Bayesian\nanalysis of a system based on Michaelis-Menton kinetics. \n\n"}
{"id": "1101.4373", "contents": "Title: Statistical Multiresolution Dantzig Estimation in Imaging: Fundamental\n  Concepts and Algorithmic Framework Abstract: In this paper we are concerned with fully automatic and locally adaptive\nestimation of functions in a \"signal + noise\"-model where the regression\nfunction may additionally be blurred by a linear operator, e.g. by a\nconvolution. To this end, we introduce a general class of statistical\nmultiresolution estimators and develop an algorithmic framework for computing\nthose. By this we mean estimators that are defined as solutions of convex\noptimization problems with supremum-type constraints. We employ a combination\nof the alternating direction method of multipliers with Dykstra's algorithm for\ncomputing orthogonal projections onto intersections of convex sets and prove\nnumerical convergence. The capability of the proposed method is illustrated by\nvarious examples from imaging and signal detection. \n\n"}
{"id": "1101.5091", "contents": "Title: Why approximate Bayesian computational (ABC) methods cannot handle model\n  choice problems Abstract: Approximate Bayesian computation (ABC), also known as likelihood-free\nmethods, have become a favourite tool for the analysis of complex stochastic\nmodels, primarily in population genetics but also in financial analyses. We\nadvocated in Grelaud et al. (2009) the use of ABC for Bayesian model choice in\nthe specific case of Gibbs random fields (GRF), relying on a sufficiency\nproperty mainly enjoyed by GRFs to show that the approach was legitimate.\nDespite having previously suggested the use of ABC for model choice in a wider\nrange of models in the DIY ABC software (Cornuet et al., 2008), we present\ntheoretical evidence that the general use of ABC for model choice is fraught\nwith danger in the sense that no amount of computation, however large, can\nguarantee a proper approximation of the posterior probabilities of the models\nunder comparison. \n\n"}
{"id": "1101.5838", "contents": "Title: Adaptive Gibbs samplers and related MCMC methods Abstract: We consider various versions of adaptive Gibbs and Metropolis-within-Gibbs\nsamplers, which update their selection probabilities (and perhaps also their\nproposal distributions) on the fly during a run by learning as they go in an\nattempt to optimize the algorithm. We present a cautionary example of how even\na simple-seeming adaptive Gibbs sampler may fail to converge. We then present\nvarious positive results guaranteeing convergence of adaptive Gibbs samplers\nunder certain conditions. \n\n"}
{"id": "1102.0944", "contents": "Title: Effective attraction between oscillating electrons in a plasmoid via\n  acoustic waves exchange Abstract: We consider the effective interaction between electrons due to the exchange\nof virtual acoustic waves in a low temperature plasma. Electrons are supposed\nto participate in rapid radial oscillations forming a spherically symmetric\nplasma structure. We show that under certain conditions this effective\ninteraction can result in the attraction between oscillating electrons and can\nbe important for the dynamics of a plasmoid. Some possible applications of the\nobtained results to the theory of natural long-lived plasma structures are also\ndiscussed. \n\n"}
{"id": "1102.0996", "contents": "Title: On statistical uncertainty in nested sampling Abstract: Nested sampling has emerged as a valuable tool for Bayesian analysis, in\nparticular for determining the Bayesian evidence. The method is based on a\nspecific type of random sampling of the likelihood function and prior volume of\nthe parameter space. I study the statistical uncertainty in the evidence\ncomputed with nested sampling. I examine the uncertainty estimator from\nSkilling (2004, 2006) and introduce a new estimator based on a detailed\nanalysis of the statistical properties of nested sampling. Both perform well in\ntest cases and make it possible to obtain the statistical uncertainty in the\nevidence with no additional computational cost. \n\n"}
{"id": "1102.4432", "contents": "Title: Lack of confidence in ABC model choice Abstract: Approximate Bayesian computation (ABC) have become a essential tool for the\nanalysis of complex stochastic models. Earlier, Grelaud et al. (2009) advocated\nthe use of ABC for Bayesian model choice in the specific case of Gibbs random\nfields, relying on a inter-model sufficiency property to show that the\napproximation was legitimate. Having implemented ABC-based model choice in a\nwide range of phylogenetic models in the DIY-ABC software (Cornuet et al.,\n2008), we now present theoretical background as to why a generic use of ABC for\nmodel choice is ungrounded, since it depends on an unknown amount of\ninformation loss induced by the use of insufficient summary statistics. The\napproximation error of the posterior probabilities of the models under\ncomparison may thus be unrelated with the computational effort spent in running\nan ABC algorithm. We then conclude that additional empirical verifications of\nthe performances of the ABC procedure as those available in DIYABC are\nnecessary to conduct model choice. \n\n"}
{"id": "1102.5554", "contents": "Title: Wavelet Ensemble Kalman Filters Abstract: We present a new type of the EnKF for data assimilation in spatial models\nthat uses diagonal approximation of the state covariance in the wavelet space\nto achieve adaptive localization. The efficiency of the new method is\ndemonstrated on an example. \n\n"}
{"id": "1103.0542", "contents": "Title: Optimal scaling and diffusion limits for the Langevin algorithm in high\n  dimensions Abstract: The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm\nwhich makes local moves by incorporating information about the gradient of the\nlogarithm of the target density. In this paper we study the efficiency of MALA\non a natural class of target measures supported on an infinite dimensional\nHilbert space. These natural measures have density with respect to a Gaussian\nrandom field measure and arise in many applications such as Bayesian\nnonparametric statistics and the theory of conditioned diffusions. We prove\nthat, started in stationarity, a suitably interpolated and scaled version of\nthe Markov chain corresponding to MALA converges to an infinite dimensional\ndiffusion process. Our results imply that, in stationarity, the MALA algorithm\napplied to an N-dimensional approximation of the target will take\n$\\mathcal{O}(N^{1/3})$ steps to explore the invariant measure, comparing\nfavorably with the Random Walk Metropolis which was recently shown to require\n$\\mathcal{O}(N)$ steps when applied to the same class of problems. \n\n"}
{"id": "1103.2126", "contents": "Title: Trajectories of Rubber Balloons used in Balloon Releases: Theory and\n  Application Abstract: Balloon releases are one of the main attractions of many fairs. Helium filled\nrubber balloons are released to carry postcards over preferably long distances.\nAlthough such balloons have been considered in atmospheric sciences and air\nsafety analysis, there is only scarce literature available on the subject. This\nwork intends to close this gap by providing a comprehensive theoretical\noverview and a thorough analysis of real-life data. All relevant physical\nproperties of a rubber balloon are carefully modelled and supplemented by\nweather observations to form a self-contained trajectory simulation tool. The\nanalysis of diverse balloon releases provided detailed insight into the flight\ndynamics and potential optimisations. Helium balloons are found to reach\nroutinely altitudes above 10 km. Under optimal conditions, they could stay more\nthan 24 hours airborne while reaching flight distances close to 3000 km.\nHowever, external weather effects reduce the typical lifetime to 2-5 hours. \n\n"}
{"id": "1103.3508", "contents": "Title: Approximating Probability Densities by Iterated Laplace Approximations Abstract: The Laplace approximation is an old, but frequently used method to\napproximate integrals for Bayesian calculations. In this paper we develop an\nextension of the Laplace approximation, by applying it iteratively to the\nresidual, i.e., the difference between the current approximation and the true\nfunction. The final approximation is thus a linear combination of multivariate\nnormal densities, where the coefficients are chosen to achieve a good fit to\nthe target distribution. We illustrate on real and artificial examples that the\nproposed procedure is a computationally efficient alternative to current\napproaches for approximation of multivariate probability densities. The\nR-package iterLap implementing the methods described in this article is\navailable from the CRAN servers. \n\n"}
{"id": "1104.2730", "contents": "Title: Approximate deconvolution large eddy simulation of a barotropic ocean\n  circulation model Abstract: This paper puts forth a new large eddy simulation closure modeling strategy\nfor two-dimensional turbulent geophysical flows. This closure modeling approach\nutilizes approximate deconvolution, which is based solely on mathematical\napproximations and does not employ additional phenomenological arguments to the\nmodel. The new approximate deconvolution model is tested in the numerical\nsimulation of the wind-driven circulation in a shallow ocean basin, a standard\nprototype of more realistic ocean dynamics. The model employs the barotropic\nvorticity equation driven by a symmetric double-gyre wind forcing, which yields\na four-gyre circulation in the time mean. The approximate deconvolution model\nyields the correct four-gyre circulation structure predicted by a direct\nnumerical simulation, on a coarser mesh but at a fraction of the computational\ncost. This first step in the numerical assessment of the new model shows that\napproximate deconvolution could represent a viable tool for under-resolved\ncomputations in the large eddy simulation of more realistic turbulent\ngeophysical flows. \n\n"}
{"id": "1104.3975", "contents": "Title: Random matrix theory for underwater sound propagation Abstract: Ocean acoustic propagation can be formulated as a wave guide with a weakly\nrandom medium generating multiple scattering. Twenty years ago, this was\nrecognized as a quantum chaos problem, and yet random matrix theory, one pillar\nof quantum or wave chaos studies, has never been introduced into the subject.\nThe modes of the wave guide provide a representation for the propagation, which\nin the parabolic approximation is unitary. Scattering induced by the ocean's\ninternal waves leads to a power-law random banded unitary matrix ensemble for\nlong-range deep ocean acoustic propagation. The ensemble has similarities, but\ndiffers, from those introduced for studying the Anderson metal-insulator\ntransition. The resulting long-range propagation ensemble statistics agree well\nwith those of full wave propagation using the parabolic equation. \n\n"}
{"id": "1105.0269", "contents": "Title: Deviance Information Criteria for Model Selection in Approximate\n  Bayesian Computation Abstract: Approximate Bayesian computation (ABC) is a class of algorithmic methods in\nBayesian inference using statistical summaries and computer simulations. ABC\nhas become popular in evolutionary genetics and in other branches of biology.\nHowever model selection under ABC algorithms has been a subject of intense\ndebate during the recent years. Here we propose novel approaches to model\nselection based on posterior predictive distributions and approximations of the\ndeviance. We argue that this framework can settle some contradictions between\nthe computation of model probabilities and posterior predictive checks using\nABC posterior distributions. A simulation study and an analysis of a\nresequencing data set of human DNA show that the deviance criteria lead to\nsensible results in a number of model choice problems of interest to population\ngeneticists. \n\n"}
{"id": "1105.1449", "contents": "Title: A Hybrid (Monte-Carlo/Deterministic) Approach for Multi-Dimensional\n  Radiation Transport Abstract: A novel hybrid Monte Carlo transport scheme is demonstrated in a scene with\nsolar illumination, scattering and absorbing 2D atmosphere, a textured\nreflecting mountain, and a small detector located in the sky (mounted on a\nsatellite or a airplane). It uses a deterministic approximation of an adjoint\ntransport solution to reduce variance, computed quickly by ignoring atmospheric\ninteractions. This allows significant variance and computational cost\nreductions when the atmospheric scattering and absorption coefficient are\nsmall. When combined with an atmospheric photon-redirection scheme, significant\nvariance reduction (equivalently acceleration) is achieved in the presence of\natmospheric interactions. \n\n"}
{"id": "1105.3885", "contents": "Title: On the sixty-year periodicity in climate and astronomical series Abstract: In a recent article by Scafetta, 2010, the author investigates whether or not\nthe decadal and multi-decadal climate oscillations have an astronomical origin.\nIn particular, the author note that several global surface temperature records,\nsince 1850, and records deduced from the orbits of the planets present very\nsimilar power spectra. Among the detected frequencies, large climate\noscillations of about 20 and 60 years, respectively, appear synchronized to the\norbital periods of Jupiter and Saturn. Other investigators have already noted\nthat many climate, geophysical and astromomical data clearly show the\nappearance of a significant, approximately 60-year cycle. Of course, this cycle\nlength is not exactly 60 years and varies by a few years (frequency band)\nbetween various climatic and astronomical phenomena. The main aim of the\npresent research note is to further investigate the above results, considering\ndifferent long-term time series and using a proper continuous wavelet analysis.\nIn particular, we specifically consider the feature and importance of the\nsixty-year periodicity, in order to better build reliable models for climate\npredictions. \n\n"}
{"id": "1105.4065", "contents": "Title: Atmospheric circulation of tidally locked exoplanets: II. Dual-band\n  radiative transfer and convective adjustment Abstract: Improving upon our purely dynamical work, we present three-dimensional\nsimulations of the atmospheric circulation on Earth-like (exo)planets and hot\nJupiters using the GFDL-Princeton Flexible Modeling System (FMS). As the first\nsteps away from the dynamical benchmarks of Heng, Menou & Phillipps (2011), we\nadd dual-band radiative transfer and dry convective adjustment schemes to our\ncomputational setup. Our treatment of radiative transfer assumes stellar\nirradiation to peak at a wavelength shorter than and distinct from that at\nwhich the exoplanet re-emits radiation (\"shortwave\" versus \"longwave\"), and\nalso uses a two-stream approximation. Convection is mimicked by adjusting\nunstable lapse rates to the dry adiabat. The bottom of the atmosphere is\nbounded by a uniform slab with a finite thermal inertia. For our models of hot\nJupiters, we include an analytical formalism for calculating\ntemperature-pressure profiles, in radiative equilibrium, which accounts for the\neffect of collision-induced absorption via a single parameter. We discuss our\nresults within the context of: the predicted temperature-pressure profiles and\nthe absence/presence of a temperature inversion; the possible maintenance, via\natmospheric circulation, of the putative high-altitude, shortwave absorber\nexpected to produce these inversions; the angular/temporal offset of the hot\nspot from the substellar point, its robustness to our ignorance of\nhyperviscosity and hence its utility in distinguishing between different hot\nJovian atmospheres; and various zonal-mean flow quantities. Our work bridges\nthe gap between three-dimensional simulations which are purely dynamical and\nthose which incorporate multi-band radiative transfer, thus contributing to the\nconstruction of a required hierarchy of three-dimensional theoretical models. \n\n"}
{"id": "1106.0322", "contents": "Title: Bayesian Sparsity-Path-Analysis of Genetic Association Signal using\n  Generalized t Priors Abstract: We explore the use of generalized t priors on regression coefficients to help\nunderstand the nature of association signal within \"hit regions\" of genome-wide\nassociation studies. The particular generalized t distribution we adopt is a\nStudent distribution on the absolute value of its argument. For low degrees of\nfreedom we show that the generalized t exhibits 'sparsity-prior' properties\nwith some attractive features over other common forms of sparse priors and\nincludes the well known double-exponential distribution as the degrees of\nfreedom tends to infinity. We pay particular attention to graphical\nrepresentations of posterior statistics obtained from sparsity-path-analysis\n(SPA) where we sweep over the setting of the scale (shrinkage / precision)\nparameter in the prior to explore the space of posterior models obtained over a\nrange of complexities, from very sparse models with all coefficient\ndistributions heavily concentrated around zero, to models with diffuse priors\nand coefficients distributed around their maximum likelihood estimates. The SPA\nplots are akin to LASSO plots of maximum a posteriori (MAP) estimates but they\ncharacterise the complete marginal posterior distributions of the coefficients\nplotted as a function of the precision of the prior. Generating posterior\ndistributions over a range of prior precisions is computationally challenging\nbut naturally amenable to sequential Monte Carlo (SMC) algorithms indexed on\nthe scale parameter. We show how SMC simulation on graphic-processing-units\n(GPUs) provides very efficient inference for SPA. We also present a\nscale-mixture representation of the generalized t prior that leads to an EM\nalgorithm to obtain MAP estimates should only these be required. \n\n"}
{"id": "1106.2525", "contents": "Title: Uniform Stability of a Particle Approximation of the Optimal Filter\n  Derivative Abstract: Sequential Monte Carlo methods, also known as particle methods, are a widely\nused set of computational tools for inference in non-linear non-Gaussian\nstate-space models. In many applications it may be necessary to compute the\nsensitivity, or derivative, of the optimal filter with respect to the static\nparameters of the state-space model; for instance, in order to obtain maximum\nlikelihood model parameters of interest, or to compute the optimal controller\nin an optimal control problem. In Poyiadjis et al. [2011] an original particle\nalgorithm to compute the filter derivative was proposed and it was shown using\nnumerical examples that the particle estimate was numerically stable in the\nsense that it did not deteriorate over time. In this paper we substantiate this\nclaim with a detailed theoretical study. Lp bounds and a central limit theorem\nfor this particle approximation of the filter derivative are presented. It is\nfurther shown that under mixing conditions these Lp bounds and the asymptotic\nvariance characterized by the central limit theorem are uniformly bounded with\nrespect to the time index. We demon- strate the performance predicted by theory\nwith several numerical examples. We also use the particle approximation of the\nfilter derivative to perform online maximum likelihood parameter estimation for\na stochastic volatility model. \n\n"}
{"id": "1106.4432", "contents": "Title: An approximate Bayesian marginal likelihood approach for estimating\n  finite mixtures Abstract: Estimation of finite mixture models when the mixing distribution support is\nunknown is an important problem. This paper gives a new approach based on a\nmarginal likelihood for the unknown support. Motivated by a Bayesian Dirichlet\nprior model, a computationally efficient stochastic approximation version of\nthe marginal likelihood is proposed and large-sample theory is presented. By\nrestricting the support to a finite grid, a simulated annealing method is\nemployed to maximize the marginal likelihood and estimate the support. Real and\nsimulated data examples show that this novel stochastic\napproximation--simulated annealing procedure compares favorably to existing\nmethods. \n\n"}
{"id": "1106.5850", "contents": "Title: Markov Chain Monte Carlo Based on Deterministic Transformations Abstract: In this article we propose a novel MCMC method based on deterministic\ntransformations T: X x D --> X where X is the state-space and D is some set\nwhich may or may not be a subset of X. We refer to our new methodology as\nTransformation-based Markov chain Monte Carlo (TMCMC). One of the remarkable\nadvantages of our proposal is that even if the underlying target distribution\nis very high-dimensional, deterministic transformation of a one-dimensional\nrandom variable is sufficient to generate an appropriate Markov chain that is\nguaranteed to converge to the high-dimensional target distribution. Apart from\nclearly leading to massive computational savings, this idea of\ndeterministically transforming a single random variable very generally leads to\nexcellent acceptance rates, even though all the random variables associated\nwith the high-dimensional target distribution are updated in a single block.\nSince it is well-known that joint updating of many random variables using\nMetropolis-Hastings (MH) algorithm generally leads to poor acceptance rates,\nTMCMC, in this regard, seems to provide a significant advance. We validate our\nproposal theoretically, establishing the convergence properties. Furthermore,\nwe show that TMCMC can be very effectively adopted for simulating from doubly\nintractable distributions.\n  TMCMC is compared with MH using the well-known Challenger data, demonstrating\nthe effectiveness of of the former in the case of highly correlated variables.\nMoreover, we apply our methodology to a challenging posterior simulation\nproblem associated with the geostatistical model of Diggle et al. (1998),\nupdating 160 unknown parameters jointly, using a deterministic transformation\nof a one-dimensional random variable. Remarkable computational savings as well\nas good convergence properties and acceptance rates are the results. \n\n"}
{"id": "1106.6280", "contents": "Title: On optimality of kernels for approximate Bayesian computation using\n  sequential Monte Carlo Abstract: Approximate Bayesian computation (ABC) has gained popularity over the past\nfew years for the analysis of complex models arising in population genetic,\nepidemiology and system biology. Sequential Monte Carlo (SMC) approaches have\nbecome work horses in ABC. Here we discuss how to construct the perturbation\nkernels that are required in ABC SMC approaches, in order to construct a set of\ndistributions that start out from a suitably defined prior and converge towards\nthe unknown posterior. We derive optimality criteria for different kernels,\nwhich are based on the Kullback-Leibler divergence between a distribution and\nthe distribution of the perturbed particles. We will show that for many\ncomplicated posterior distributions, locally adapted kernels tend to show the\nbest performance. In cases where it is possible to estimate the Fisher\ninformation we can construct particularly efficient perturbation kernels. We\nfind that the added moderate cost of adapting kernel functions is easily\nregained in terms of the higher acceptance rate. We demonstrate the\ncomputational efficiency gains in a range of toy-examples which illustrate some\nof the challenges faced in real-world applications of ABC, before turning to\ntwo demanding parameter inference problem in molecular biology, which highlight\nthe huge increases in efficiency that can be gained from choice of optimal\nmodels. We conclude with a general discussion of rational choice of\nperturbation kernels in ABC SMC settings. \n\n"}
{"id": "1107.1390", "contents": "Title: On the effects of clouds and hazes in the atmospheres of hot Jupiters:\n  semi-analytical temperature-pressure profiles Abstract: Motivated by the work of Guillot (2010), we present a semi-analytical\nformalism for calculating the temperature-pressure profiles in hot Jovian\natmospheres which includes the effects of clouds/hazes and collision-induced\nabsorption. Using the dual-band approximation, we assume that stellar\nirradiation and thermal emission from the hot Jupiter occur at distinct\nwavelengths (\"shortwave\" versus \"longwave\"). For a purely absorbing cloud/haze,\nwe demonstrate its dual effect of cooling and warming the upper and lower\natmosphere, respectively, which modifies, in a non-trivial manner, the\ncondition for whether a temperature inversion is present in the upper\natmosphere. The warming effect becomes more pronounced as the cloud/haze deck\nresides at greater depths. If it sits below the shortwave photosphere, the\nwarming effect becomes either more subdued or ceases altogether. If shortwave\nscattering is present, its dual effect is to warm and cool the upper and lower\natmosphere, respectively, thus counteracting the effects of enhanced longwave\nabsorption by the cloud/haze. We make a tentative comparison of a 4-parameter\nmodel to the temperature-pressure data points inferred from the observations of\nHD 189733b and estimate that its Bond albedo is approximately 10%. Besides\ntheir utility in developing physical intuition, our semi-analytical models are\na guide for the parameter space exploration of hot Jovian atmospheres via\nthree-dimensional simulations of atmospheric circulation. \n\n"}
{"id": "1107.1751", "contents": "Title: A diffusion-induced transition in the phase separation of binary fluid\n  mixtures subjected to a temperature ramp Abstract: Demixing of binary fluids subjected to slow temperature ramps shows repeated\nwaves of nucleation which arise as a consequence of the competition between\ngeneration of supersaturation by the temperature ramp and relaxation of\nsupersaturation by diffusive transport and flow. Here, we use an\nadvection-reaction-diffusion model to study the oscillations in the weak- and\nstrong-diffusion regime. There is a sharp transition between the two regimes,\nwhich can only be understood based on the probability distribution function of\nthe composition rather than in terms of the average composition. We argue that\nthis transition might be responsible for some yet unclear features of\nexperiments, like the appearance of secondary oscillations and bimodal droplet\nsize distributions. \n\n"}
{"id": "1107.5289", "contents": "Title: Planetary Atmospheres as Non-Equilibrium Condensed Matter Abstract: Planetary atmospheres, and models of them, are discussed from the viewpoint\nof condensed matter physics. Atmospheres are a form of condensed matter, and\nmany interesting phenomena of condensed matter systems are realized by them.\nThe essential physics of the general circulation is illustrated with idealized\n2-layer and 1-layer models of the atmosphere. Equilibrium and non-equilibrium\nstatistical mechanics are used to directly ascertain the statistics of these\nmodels. \n\n"}
{"id": "1109.0152", "contents": "Title: Stable Graphical Model Estimation with Random Forests for Discrete,\n  Continuous, and Mixed Variables Abstract: A conditional independence graph is a concise representation of pairwise\nconditional independence among many variables. Graphical Random Forests (GRaFo)\nare a novel method for estimating pairwise conditional independence\nrelationships among mixed-type, i.e. continuous and discrete, variables. The\nnumber of edges is a tuning parameter in any graphical model estimator and\nthere is no obvious number that constitutes a good choice. Stability Selection\nhelps choosing this parameter with respect to a bound on the expected number of\nfalse positives (error control).\n  The performance of GRaFo is evaluated and compared with various other methods\nfor p = 50, 100, and 200 possibly mixed-type variables while sample size is n =\n100 (n = 500 for maximum likelihood). Furthermore, GRaFo is applied to data\nfrom the Swiss Health Survey in order to evaluate how well it can reproduce the\ninterconnection of functional health components, personal, and environmental\nfactors, as hypothesized by the World Health Organization's International\nClassification of Functioning, Disability and Health (ICF). Finally, GRaFo is\nused to identify risk factors which may be associated with adverse\nneurodevelopment of children who suffer from trisomy 21 and experienced\nopen-heart surgery.\n  GRaFo performs well with mixed data and thanks to Stability Selection it\nprovides an error control mechanism for false positive selection. \n\n"}
{"id": "1109.2668", "contents": "Title: Climate instability on tidally locked exoplanets Abstract: Feedbacks that can destabilize the climates of synchronously-rotating rocky\nplanets may arise on planets with strong day-night surface temperature\ncontrasts. Earth-like habitable-zone (HZ) planets maintain stable surface\nliquid water over geological time. This requires equilibrium between the\ntemperature-dependent rate of greenhouse-gas consumption by weathering,and\ngreenhouse-gas resupply by other processes. Detected small-radius exoplanets,\nand anticipated M-dwarf HZ rocky planets, are expected to be tidally locked. We\ninvestigate two feedbacks that can destabilize climate on tidally-locked\nplanets. (1) If small changes in pressure alter the temperature distribution\nacross a planet's surface such that the weathering rate increases when the\npressure decreases, a positive feedback occurs involving increasing weathering\nrate near the substellar point, decreasing pressure, and increasing substellar\nsurface temperature. (2) When decreases in pressure increase the surface area\nabove the melting point (through reduced advective cooling of the substellar\npoint), and the corresponding increase in volume of liquid causes net\ndissolution of the atmosphere, a further decrease in pressure occurs. We use an\nidealized energy balance model to map out the conditions under which these\ninstabilities may occur. The weathering runaway can shrink the habitable zone,\nand cause geologically rapid 10^3-fold pressure shifts within the HZ. Mars may\nhave undergone a weathering runaway in the past. Substellar dissolution is\nusually a negative feedback or weak positive feedback on changes in pressure.\nBoth instabilities are suppressed if the atmosphere has a high radiative\nefficiency. Our results are most relevant for atmospheres that are thin and\nhave low greenhouse-gas radiative efficiency. These results identify a new\npathway by which HZ planets can undergo rapid climate shifts and become\nuninhabitable. \n\n"}
{"id": "1109.6090", "contents": "Title: Robust Parametric Classification and Variable Selection by a Minimum\n  Distance Criterion Abstract: We investigate a robust penalized logistic regression algorithm based on a\nminimum distance criterion. Influential outliers are often associated with the\nexplosion of parameter vector estimates, but in the context of standard\nlogistic regression, the bias due to outliers always causes the parameter\nvector to implode, that is shrink towards the zero vector. Thus, using\nLASSO-like penalties to perform variable selection in the presence of outliers\ncan result in missed detections of relevant covariates. We show that by\nchoosing a minimum distance criterion together with an Elastic Net penalty, we\ncan simultaneously find a parsimonious model and avoid estimation implosion\neven in the presence of many outliers in the important small $n$ large $p$\nsituation. Implementation using an MM algorithm is described and performance\nevaluated. \n\n"}
{"id": "1110.1805", "contents": "Title: Energy Release and Particle Acceleration in Flares: Summary and Future\n  Prospects Abstract: RHESSI measurements relevant to the fundamental processes of energy release\nand particle acceleration in flares are summarized. RHESSI's precise\nmeasurements of hard X-ray continuum spectra enable model-independent\ndeconvolution to obtain the parent electron spectrum. Taking into account the\neffects of albedo, these show that the low energy cut-off to the electron\npower-law spectrum is typically below tens of keV, confirming that the\naccelerated electrons contain a large fraction of the energy released in\nflares. RHESSI has detected a high coronal hard X-ray source that is filled\nwith accelerated electrons whose energy density is comparable to the\nmagnetic-field energy density. This suggests an efficient conversion of energy,\npreviously stored in the magnetic field, into the bulk acceleration of\nelectrons. A new, collisionless (Hall) magnetic reconnection process has been\nidentified through theory and simulations, and directly observed in space and\nin the laboratory; it should occur in the solar corona as well, with a\nreconnection rate fast enough for the energy release in flares. The\nreconnection process could result in the formation of multiple elongated\nmagnetic islands, that then collapse to bulk-accelerate the electrons, rapidly\nenough to produce the observed hard X-ray emissions. RHESSI's pioneering\n{\\gamma}-ray line imaging of energetic ions, revealing footpoints straddling a\nflare loop arcade, has provided strong evidence that ion acceleration is also\nrelated to magnetic reconnection. Flare particle acceleration is shown to have\na close relationship to impulsive Solar Energetic Particle (SEP) events\nobserved in the interplanetary medium, and also to both fast coronal mass\nejections and gradual SEP events. \n\n"}
{"id": "1111.1687", "contents": "Title: Discriminant Analysis with Adaptively Pooled Covariance Abstract: Linear and Quadratic Discriminant analysis (LDA/QDA) are common tools for\nclassification problems. For these methods we assume observations are normally\ndistributed within group. We estimate a mean and covariance matrix for each\ngroup and classify using Bayes theorem. With LDA, we estimate a single, pooled\ncovariance matrix, while for QDA we estimate a separate covariance matrix for\neach group. Rarely do we believe in a homogeneous covariance structure between\ngroups, but often there is insufficient data to separately estimate covariance\nmatrices. We propose L1- PDA, a regularized model which adaptively pools\nelements of the precision matrices. Adaptively pooling these matrices decreases\nthe variance of our estimates (as in LDA), without overly biasing them. In this\npaper, we propose and discuss this method, give an efficient algorithm to fit\nit for moderate sized problems, and show its efficacy on real and simulated\ndatasets. \n\n"}
{"id": "1111.2667", "contents": "Title: A note on the lack of symmetry in the graphical lasso Abstract: The graphical lasso (glasso) is a widely-used fast algorithm for estimating\nsparse inverse covariance matrices. The glasso solves an L1 penalized maximum\nlikelihood problem and is available as an R library on CRAN. The output from\nthe glasso, a regularized covariance matrix estimate a sparse inverse\ncovariance matrix estimate, not only identify a graphical model but can also\nserve as intermediate inputs into multivariate procedures such as PCA, LDA,\nMANOVA, and others. The glasso indeed produces a covariance matrix estimate\nwhich solves the L1 penalized optimization problem in a dual sense; however,\nthe method for producing the inverse covariance matrix estimator after this\noptimization is inexact and may produce asymmetric estimates. This problem is\nexacerbated when the amount of L1 regularization that is applied is small,\nwhich in turn is more likely to occur if the true underlying inverse covariance\nmatrix is not sparse. The lack of symmetry can potentially have consequences.\nFirst, it implies that the covariance and inverse covariance estimates are not\nnumerical inverses of one another, and second, asymmetry can possibly lead to\nnegative or complex eigenvalues,rendering many multivariate procedures which\nmay depend on the inverse covariance estimator unusable. We demonstrate this\nproblem, explain its causes, and propose possible remedies. \n\n"}
{"id": "1111.4610", "contents": "Title: A wildland fire modeling and visualization environment Abstract: We present an overview of a modeling environment, consisting of a coupled\natmosphere-wildfire model, utilities for visualization, data processing, and\ndiagnostics, open source software repositories, and a community wiki. The fire\nmodel, called SFIRE, is based on a fire-spread model, implemented by the\nlevel-set method, and it is coupled with the Weather Research Forecasting (WRF)\nmodel. A version with a subset of the features is distributed with WRF 3.3 as\nWRF-Fire. In each time step, the fire module takes the wind as input and\nreturns the latent and sensible heat fluxes. The software architecture uses WRF\nparallel infrastructure for massively parallel computing. Recent features of\nthe code include interpolation from an ideal logarithmic wind profile for\nnonhomogeneous fuels and ignition from a fire perimeter with an atmosphere and\nfire spin-up. Real runs use online sources for fuel maps, fine-scale\ntopography, and meteorological data, and can run faster than real time.\nVisualization pathways allow generating images and animations in many packages,\nincluding VisTrails, VAPOR, MayaVi, and Paraview, as well as output to Google\nEarth. The environment is available from openwfm.org. New diagnostic variables\nwere added to the code recently, including a new kind of fireline intensity,\nwhich takes into account also the speed of burning, unlike Byram's fireline\nintensity. \n\n"}
{"id": "1112.3019", "contents": "Title: Lie reduction and exact solutions of vorticity equation on rotating\n  sphere Abstract: Following our paper [J. Math. Phys. 50 (2009) 123102], we systematically\ncarry out Lie symmetry analysis for the barotropic vorticity equation on the\nrotating sphere. All finite-dimensional subalgebras of the corresponding\nmaximal Lie invariance algebra, which is infinite-dimensional, are classified.\nAppropriate subalgebras are then used to exhaustively determine Lie reductions\nof the equation under consideration. The relevance of the constructed exact\nsolutions for the description of real-world physical processes is discussed. It\nis shown that the results of the above paper are directly related to the\nresults of the recent letter by N. H. Ibragimov and R. N. Ibragimov [Phys.\nLett. A 375 (2011) 3858] in which Lie symmetries and some exact solutions of\nthe nonlinear Euler equations for an atmospheric layer in spherical geometry\nwere determined. \n\n"}
{"id": "1112.3235", "contents": "Title: How can a glacial inception be predicted? Abstract: The Early Anthropogenic Hypothesis considers that greenhouse gas\nconcentrations should have declined during the Holocene in absence of humankind\nactivity, leading to glacial inception around the present. It partly relies on\nthe fact that present levels of northern summer incoming solar radiation are\nclose to those that, in the past, preceded a glacial inception phenomenon,\nassociated to declines in greenhouse gas concentrations. However, experiments\nwith various numerical models of glacial cycles show that next glacial\ninception may still be delayed by several ten thousands of years, even with the\nassumption of greenhouse gas concentration declines during the Holocene.\nFurthermore, as we show here, conceptual models designed to capture the gross\ndynamics of the climate system as a whole suggest also that small disturbances\nmay sometimes cause substantial delays in glacial events, causing a fair level\nof unpredictability on ice age dynamics. This suggests the need of a validated\nmathematical description of the climate system dynamics that allows us to\nquantify uncertainties on predictions. Here, it is proposed to organise our\nknowledge about the physics and dynamics of glacial cycles through a Bayesian\ninference network. Constraints on the physics and dynamics of climate can be\nencapsulated into a stochastic dynamical system. These constraints include, in\nparticular, estimates of the sensitivity of the components of climate to\nexternal forcings, inferred from plans of experiments with large simulators of\nthe atmosphere, oceans and ice sheets. On the other hand, palaeoclimate\nobservations are accounted for through a process of parameter calibration. We\ndiscuss promises and challenges raised by this programme. \n\n"}
{"id": "1112.3760", "contents": "Title: The reduction of plankton biomass induced by mesoscale stirring: a\n  modeling study in the Benguela upwelling Abstract: Recent studies, both based on remote sensed data and coupled models, showed a\nreduction of biological productivity due to vigorous horizontal stirring in\nupwelling areas. In order to better understand this phenomenon, we consider a\nsystem of oceanic flow from the Benguela area coupled with a simple\nbiogeochemical model of Nutrient-Phyto-Zooplankton (NPZ) type. For the flow\nthree different surface velocity fields are considered: one derived from\nsatellite altimetry data, and the other two from a regional numerical model at\ntwo different spatial resolutions. We compute horizontal particle dispersion in\nterms of Lyapunov Exponents, and analyzed their correlations with phytoplankton\nconcentrations. Our modelling approach confirms that in the south Benguela\nthere is a reduction of biological activity when stirring is increased.\nTwo-dimensional offshore advection and latitudinal difference in Primary\nProduction, also mediated by the flow, seem to be the dominant processes\ninvolved. We estimate that mesoscale processes are responsible for 30 to 50% of\nthe offshore fluxes of biological tracers. In the northern area, other factors\nnot taken into account in our simulation are influencing the ecosystem. We\nsuggest explanations for these results in the context of studies performed in\nother eastern boundary upwelling areas. \n\n"}
{"id": "1201.0306", "contents": "Title: Alternating Linearization for Structured Regularization Problems Abstract: We adapt the alternating linearization method for proximal decomposition to\nstructured regularization problems, in particular, to the generalized lasso\nproblems. The method is related to two well-known operator splitting methods,\nthe Douglas--Rachford and the Peaceman--Rachford method, but it has descent\nproperties with respect to the objective function. This is achieved by\nemploying a special update test, which decides whether it is beneficial to make\na Peaceman--Rachford step, any of the two possible Douglas--Rachford steps, or\nnone. The convergence mechanism of the method is related to that of bundle\nmethods of nonsmooth optimization. We also discuss implementation for very\nlarge problems, with the use of specialized algorithms and sparse data\nstructures. Finally, we present numerical results for several synthetic and\nreal-world examples, including a three-dimensional fused lasso problem, which\nillustrate the scalability, efficacy, and accuracy of the method. \n\n"}
{"id": "1201.0498", "contents": "Title: Invariant discretization schemes for the shallow-water equations Abstract: Invariant discretization schemes are derived for the one- and two-dimensional\nshallow-water equations with periodic boundary conditions. While originally\ndesigned for constructing invariant finite difference schemes, we extend the\nusage of difference invariants to allow constructing of invariant finite volume\nmethods as well. It is found that the classical invariant schemes converge to\nthe Lagrangian formulation of the shallow-water equations. These schemes\nrequire to redistribute the grid points according to the physical fluid\nvelocity, i.e., the mesh cannot remain fixed in the course of the numerical\nintegration. Invariant Eulerian discretization schemes are proposed for the\nshallow-water equations in computational coordinates. Instead of using the\nfluid velocity as the grid velocity, an invariant moving mesh generator is\ninvoked in order to determine the location of the grid points at the subsequent\ntime level. The numerical conservation of energy, mass and momentum is\nevaluated for both the invariant and non-invariant schemes. \n\n"}
{"id": "1201.3593", "contents": "Title: Path Following in the Exact Penalty Method of Convex Programming Abstract: Classical penalty methods solve a sequence of unconstrained problems that put\ngreater and greater stress on meeting the constraints. In the limit as the\npenalty constant tends to $\\infty$, one recovers the constrained solution. In\nthe exact penalty method, squared penalties are replaced by absolute value\npenalties, and the solution is recovered for a finite value of the penalty\nconstant. In practice, the kinks in the penalty and the unknown magnitude of\nthe penalty constant prevent wide application of the exact penalty method in\nnonlinear programming. In this article, we examine a strategy of path following\nconsistent with the exact penalty method. Instead of performing optimization at\na single penalty constant, we trace the solution as a continuous function of\nthe penalty constant. Thus, path following starts at the unconstrained solution\nand follows the solution path as the penalty constant increases. In the\nprocess, the solution path hits, slides along, and exits from the various\nconstraints. For quadratic programming, the solution path is piecewise linear\nand takes large jumps from constraint to constraint. For a general convex\nprogram, the solution path is piecewise smooth, and path following operates by\nnumerically solving an ordinary differential equation segment by segment. Our\ndiverse applications to a) projection onto a convex set, b) nonnegative least\nsquares, c) quadratically constrained quadratic programming, d) geometric\nprogramming, and e) semidefinite programming illustrate the mechanics and\npotential of path following. The final detour to image denoising demonstrates\nthe relevance of path following to regularized estimation in inverse problems.\nIn regularized estimation, one follows the solution path as the penalty\nconstant decreases from a large value. \n\n"}
{"id": "1201.4679", "contents": "Title: Nonlinear problems of complex natural systems: Sun and climate dynamics Abstract: Universal role of the nonlinear one-third subharmonic resonance mechanism in\ngeneration of the strong fluctuations in such complex natural dynamical systems\nas global climate and global solar activity is discussed using wavelet\nregression detrended data. Role of the oceanic Rossby waves in the year-scale\nglobal temperature fluctuations and the nonlinear resonance contribution to the\nEl Nino phenomenon have been discussed in detail. The large fluctuations of the\nreconstructed temperature on the millennial time-scales (Antarctic ice cores\ndata for the past 400,000 years) are also shown to be dominated by the\none-third subharmonic resonance, presumably related to Earth precession effect\non the energy that the intertropical regions receive from the Sun. Effects of\nGalactic turbulence on the temperature fluctuations are discussed in this\ncontent. It is also shown that the one-third subharmonic resonance can be\nconsidered as a background for the 11-years solar cycle, and again the global\n(solar) rotation and chaotic propagating waves play significant role in this\nphenomenon. Finally, a multidecadal chaotic coherence between the detrended\nsolar activity and global temperature has been briefly discussed. \n\n"}
{"id": "1202.0709", "contents": "Title: MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster Abstract: Many problems arising in applications result in the need to probe a\nprobability distribution for functions. Examples include Bayesian nonparametric\nstatistics and conditioned diffusion processes. Standard MCMC algorithms\ntypically become arbitrarily slow under the mesh refinement dictated by\nnonparametric description of the unknown function. We describe an approach to\nmodifying a whole range of MCMC methods, applicable whenever the target measure\nhas density with respect to a Gaussian process or Gaussian random field\nreference measure, which ensures that their speed of convergence is robust\nunder mesh refinement. Gaussian processes or random fields are fields whose\nmarginal distributions, when evaluated at any finite set of $N$ points, are\n$\\mathbb{R}^N$-valued Gaussians. The algorithmic approach that we describe is\napplicable not only when the desired probability measure has density with\nrespect to a Gaussian process or Gaussian random field reference measure, but\nalso to some useful non-Gaussian reference measures constructed through random\ntruncation. In the applications of interest the data is often sparse and the\nprior specification is an essential part of the overall modelling strategy.\nThese Gaussian-based reference measures are a very flexible modelling tool,\nfinding wide-ranging application. Examples are shown in density estimation,\ndata assimilation in fluid mechanics, subsurface geophysics and image\nregistration. The key design principle is to formulate the MCMC method so that\nit is, in principle, applicable for functions; this may be achieved by use of\nproposals based on carefully chosen time-discretizations of stochastic\ndynamical systems which exactly preserve the Gaussian reference measure. Taking\nthis approach leads to many new algorithms which can be implemented via minor\nmodification of existing algorithms, yet which show enormous speed-up on a wide\nrange of applied problems. \n\n"}
{"id": "1202.5156", "contents": "Title: Response of Cloud Condensation Nuclei (> 50 nm) to changes in\n  ion-nucleation Abstract: In experiments where ultraviolet light produces aerosols from trace amounts\nof ozone, sulphur dioxide, and water vapour, the number of additional small\nparticles produced by ionization by gamma sources all grow up to diameters\nlarger than 50 nm, appropriate for cloud condensation nuclei. This result\ncontradicts both ion-free control experiments and also theoretical models that\npredict a decline in the response of larger particles due to an insufficiency\nof condensable gases (which leads to slower growth) and to larger losses by\ncoagulation between the particles. This unpredicted experimental finding points\nto a process not included in current theoretical models, possibly an\nion-induced formation of sulphuric acid in small clusters. \n\n"}
{"id": "1203.0258", "contents": "Title: Stable Langmuir solitons in plasma with diatomic ions Abstract: We study stable axially and spherically symmetric spatial solitons in plasma\nwith diatomic ions. The stability of a soliton against the collapse is provided\nby the interaction of induced electric dipole moments of ions with rapidly\noscillating electric field of a plasmoid. We derive the new cubic-quintic\nnonlinear Schrodinger equation which governs the soliton dynamics and\nnumerically solve it. Then we discuss the possibility of implementation of such\nplasmoids in realistic atmospheric plasma. In particular, we suggest that\nspherically symmetric Langmuir solitons, described in the present work, can be\nexcited at the formation stage of long-lived atmospheric plasma structures. The\nimplication of our model for the interpretation of the results of experiments\nfor the plasmoids generation is discussed. \n\n"}
{"id": "1203.1922", "contents": "Title: On the Stability of Super-Earth Atmospheres Abstract: We investigate the stability of super Earth atmospheres around M stars using\na 7-parameter, analytical framework. We construct stability diagrams in the\nparameter space of exoplanetary radius versus semi-major axis and elucidate the\nregions in which the atmospheres are stable against the condensation of their\nmajor constituents, out of the gas phase, on their permanent nightside\nhemispheres. We find that super Earth atmospheres which are nitrogen-dominated\n(\"Earth-like\") occupy a smaller region of allowed parameter space, compared to\nhydrogen-dominated atmospheres, because of the dual effects of diminished\nadvection and enhanced radiative cooling. Furthermore, some super Earths which\nreside within the habitable zones of M stars may not possess stable\natmospheres, depending on the mean molecular weight and infrared photospheric\npressure of their atmospheres. We apply our stability diagrams to GJ 436b and\nGJ 1214b, and demonstrate that atmospheric compositions with high mean\nmolecular weights are disfavoured if these exoplanets possess solid surfaces\nand shallow atmospheres. Finally, we construct stability diagrams tailored to\nthe Kepler dataset, for G and K stars, and predict that about half of the\nexoplanet candidates are expected to habour stable atmospheres if Earth-like\nconditions are assumed. We include 55 Cancri e and CoRoT-7b in our stability\ndiagram for G stars. \n\n"}
{"id": "1203.2230", "contents": "Title: Assimilation of Perimeter Data and Coupling with Fuel Moisture in a\n  Wildland Fire - Atmosphere DDDAS Abstract: We present a methodology to change the state of the Weather Research\nForecasting (WRF) model coupled with the fire spread code SFIRE, based on\nRothermel's formula and the level set method, and with a fuel moisture model.\nThe fire perimeter in the model changes in response to data while the model is\nrunning. However, the atmosphere state takes time to develop in response to the\nforcing by the heat flux from the fire. Therefore, an artificial fire history\nis created from an earlier fire perimeter to the new perimeter, and replayed\nwith the proper heat fluxes to allow the atmosphere state to adjust. The method\nis an extension of an earlier method to start the coupled fire model from a\ndeveloped fire perimeter rather than an ignition point. The level set method is\nalso used to identify parameters of the simulation, such as the spread rate and\nthe fuel moisture. The coupled model is available from openwfm.org, and it\nextends the WRF-Fire code in WRF release. \n\n"}
{"id": "1203.3896", "contents": "Title: Fast and Adaptive Sparse Precision Matrix Estimation in High Dimensions Abstract: This paper proposes a new method for estimating sparse precision matrices in\nthe high dimensional setting. It has been popular to study fast computation and\nadaptive procedures for this problem. We propose a novel approach, called\nSparse Column-wise Inverse Operator, to address these two issues. We analyze an\nadaptive procedure based on cross validation, and establish its convergence\nrate under the Frobenius norm. The convergence rates under other matrix norms\nare also established. This method also enjoys the advantage of fast computation\nfor large-scale problems, via a coordinate descent algorithm. Numerical merits\nare illustrated using both simulated and real datasets. In particular, it\nperforms favorably on an HIV brain tissue dataset and an ADHD resting-state\nfMRI dataset. \n\n"}
{"id": "1204.2889", "contents": "Title: Special solutions to a compact equation for deep-water gravity waves Abstract: Recently, Dyachenko & Zakharov (2011) have derived a compact form of the well\nknown Zakharov integro-differential equation for the third order Hamiltonian\ndynamics of a potential flow of an incompressible, infinitely deep fluid with a\nfree surface. Special traveling wave solutions of this compact equation are\nnumerically constructed using the Petviashvili method. Their stability\nproperties are also investigated. Further, unstable traveling waves with\nwedge-type singularities, viz. peakons, are numerically discovered. To gain\ninsights into the properties of singular traveling waves, we consider the\nacademic case of a perturbed version of the compact equation, for which\nanalytical peakons with exponential shape are derived. Finally, by means of an\naccurate Fourier-type spectral scheme it is found that smooth solitary waves\nappear to collide elastically, suggesting the integrability of the Zakharov\nequation. \n\n"}
{"id": "1204.4148", "contents": "Title: Algorithm for multivariate data standardization up to third moment Abstract: An algorithm for transforming multivariate data to a form with normalized\nfirst, second and third moments is presented. \n\n"}
{"id": "1204.5445", "contents": "Title: A recent tipping point in the Arctic sea-ice cover: abrupt and\n  persistent increase in the seasonal cycle since 2007 Abstract: There is ongoing debate over whether Arctic sea-ice has already passed a\n`tipping point', or whether it will do so in the future. Several recent studies\nargue that the loss of summer sea ice does not involve an irreversible\nbifurcation, because it is highly reversible in models. However, a broader\ndefinition of a `tipping point' also includes other abrupt, non-linear changes\nthat are neither bifurcations nor necessarily irreversible. Examination of\nsatellite data for Arctic sea-ice area reveals an abrupt increase in the\namplitude of seasonal variability in 2007 that has persisted since then. We\nidentified this abrupt transition using recently developed methods that can\ndetect multi-modality in time-series data and sometimes forewarn of\nbifurcations. When removing the mean seasonal cycle (up to 2008) from the\nsatellite data, the residual sea-ice fluctuations switch from uni-modal to\nmulti-modal behaviour around 2007. We originally interpreted this as a\nbifurcation in which a new lower ice cover attractor appears in deseasonalised\nfluctuations and is sampled in every summer-autumn from 2007 onwards. However,\nthis interpretation is clearly sensitive to how the seasonal cycle is removed\nfrom the raw data, and to the presence of continental land masses restricting\nwinter-spring ice fluctuations. Furthermore, there was no robust early warning\nsignal of critical slowing down prior to the hypothesized bifurcation. Early\nwarning indicators do however show destabilization of the summer-autumn sea-ice\ncover since 2007. Thus, the bifurcation hypothesis lacks consistent support,\nbut there was an abrupt and persistent increase in the amplitude of the\nseasonal cycle of Arctic sea-ice cover in 2007, which we describe as a\n(non-bifurcation) `tipping point'. Our statistical methods detect this `tipping\npoint' and its time of onset. \n\n"}
{"id": "1204.5459", "contents": "Title: Inference for SDE models via Approximate Bayesian Computation Abstract: Models defined by stochastic differential equations (SDEs) allow for the\nrepresentation of random variability in dynamical systems. The relevance of\nthis class of models is growing in many applied research areas and is already a\nstandard tool to model e.g. financial, neuronal and population growth dynamics.\nHowever inference for multidimensional SDE models is still very challenging,\nboth computationally and theoretically. Approximate Bayesian computation (ABC)\nallow to perform Bayesian inference for models which are sufficiently complex\nthat the likelihood function is either analytically unavailable or\ncomputationally prohibitive to evaluate. A computationally efficient ABC-MCMC\nalgorithm is proposed, halving the running time in our simulations. Focus is on\nthe case where the SDE describes latent dynamics in state-space models; however\nthe methodology is not limited to the state-space framework. Simulation studies\nfor a pharmacokinetics/pharmacodynamics model and for stochastic chemical\nreactions are considered and a MATLAB package implementing our ABC-MCMC\nalgorithm is provided. \n\n"}
{"id": "1204.5564", "contents": "Title: Segmentor3IsBack: an R package for the fast and exact segmentation of\n  Seq-data Abstract: Genome annotation is an important issue in biology which has long been\naddressed with gene prediction methods and manual experiments requiring\nbiological expertise. The expanding Next Generation Sequencing technologies and\ntheir enhanced precision allow a new approach to the domain: the segmentation\nof RNA-Seq data to determine gene boundaries. Because of its almost linear\ncomplexity, we propose to use the Pruned Dynamic Programming Algorithm, which\nperformances had been acknowledged for CGH arrays, for Seq-experiment outputs.\nThis requires the adaptation of the algorithm to the negative binomial\ndistribution with which we model the data. We show that if the dispersion in\nthe signal is known, the PDP algorithm can be used and we provide an estimator\nfor this dispersion. We then propose to estimate the number of segments, which\ncan be associated to coding or non-coding regions of the genome, using an\noracle penalty. We illustrate the results of our approach on a real data-set\nand show its good performance. Our algorithm is available as an R package on\nthe CRAN repository. \n\n"}
{"id": "1205.0482", "contents": "Title: On the Generalized Ratio of Uniforms as a Combination of Transformed\n  Rejection and Extended Inverse of Density Sampling Abstract: In this work we investigate the relationship among three classical sampling\ntechniques: the inverse of density (Khintchine's theorem), the transformed\nrejection (TR) and the generalized ratio of uniforms (GRoU). Given a monotonic\nprobability density function (PDF), we show that the transformed area obtained\nusing the generalized ratio of uniforms method can be found equivalently by\napplying the transformed rejection sampling approach to the inverse function of\nthe target density. Then we provide an extension of the classical inverse of\ndensity idea, showing that it is completely equivalent to the GRoU method for\nmonotonic densities. Although we concentrate on monotonic probability density\nfunctions (PDFs), we also discuss how the results presented here can be\nextended to any non-monotonic PDF that can be decomposed into a collection of\nintervals where it is monotonically increasing or decreasing. In this general\ncase, we show the connections with transformations of certain random variables\nand the generalized inverse PDF with the GRoU technique. Finally, we also\nintroduce a GRoU technique to handle unbounded target densities. \n\n"}
{"id": "1205.1076", "contents": "Title: Adaptive parallel tempering algorithm Abstract: Parallel tempering is a generic Markov chain Monte Carlo sampling method\nwhich allows good mixing with multimodal target distributions, where\nconventional Metropolis-Hastings algorithms often fail. The mixing properties\nof the sampler depend strongly on the choice of tuning parameters, such as the\ntemperature schedule and the proposal distribution used for local exploration.\nWe propose an adaptive algorithm which tunes both the temperature schedule and\nthe parameters of the random-walk Metropolis kernel automatically. We prove the\nconvergence of the adaptation and a strong law of large numbers for the\nalgorithm. We illustrate the performance of our method with examples. Our\nempirical findings indicate that the algorithm can cope well with different\nkind of scenarios without prior tuning. \n\n"}
{"id": "1205.1245", "contents": "Title: Sparse group lasso and high dimensional multinomial classification Abstract: The sparse group lasso optimization problem is solved using a coordinate\ngradient descent algorithm. The algorithm is applicable to a broad class of\nconvex loss functions. Convergence of the algorithm is established, and the\nalgorithm is used to investigate the performance of the multinomial sparse\ngroup lasso classifier. On three different real data examples the multinomial\ngroup lasso clearly outperforms multinomial lasso in terms of achieved\nclassification error rate and in terms of including fewer features for the\nclassification. The run-time of our sparse group lasso implementation is of the\nsame order of magnitude as the multinomial lasso algorithm implemented in the R\npackage glmnet. Our implementation scales well with the problem size. One of\nthe high dimensional examples considered is a 50 class classification problem\nwith 10k features, which amounts to estimating 500k parameters. The\nimplementation is available as the R package msgl. \n\n"}
{"id": "1205.1880", "contents": "Title: Non-Parametric Methods Applied to the N-Sample Series Comparison Abstract: Anomaly and similarity detection in multidimensional series have a long\nhistory and have found practical usage in many different fields such as\nmedicine, networks, and finance. Anomaly detection is of great appeal for many\ndifferent disciplines; for example, mathematicians searching for a unified\nmathematical formulation based on probability, statisticians searching for\nerror bound estimates, and computer scientists who are trying to design fast\nalgorithms, to name just a few. In summary, we have two contributions: First,\nwe present a self-contained survey of the most promising methods being used in\nthe fields of machine learning, statistics, and bio-informatics today. Included\nwe present discussions about conformal prediction, kernels in the Hilbert\nspace, Kolmogorov's information measure, and non-parametric cumulative\ndistribution function comparison methods (NCDF). Second, building upon this\nfoundation, we provide a powerful NCDF method for series with small\ndimensionality. Through a combination of data organization and statistical\ntests, we describe extensions that scale well with increased dimensionality. \n\n"}
{"id": "1205.4481", "contents": "Title: Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by\n  Exploiting Structure Abstract: In this work we consider the stochastic minimization of nonsmooth convex loss\nfunctions, a central problem in machine learning. We propose a novel algorithm\ncalled Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which\nexploits the structure of common nonsmooth loss functions to achieve optimal\nconvergence rates for a class of problems including SVMs. It is the first\nstochastic algorithm that can achieve the optimal O(1/t) rate for minimizing\nnonsmooth loss functions (with strong convexity). The fast rates are confirmed\nby empirical comparisons, in which ANSGD significantly outperforms previous\nsubgradient descent algorithms including SGD. \n\n"}
{"id": "1205.5658", "contents": "Title: Bayesian computation via empirical likelihood Abstract: Approximate Bayesian computation (ABC) has become an essential tool for the\nanalysis of complex stochastic models when the likelihood function is\nnumerically unavailable. However, the well-established statistical method of\nempirical likelihood provides another route to such settings that bypasses\nsimulations from the model and the choices of the ABC parameters (summary\nstatistics, distance, tolerance), while being convergent in the number of\nobservations. Furthermore, bypassing model simulations may lead to significant\ntime savings in complex models, for instance those found in population\ngenetics. The BCel algorithm we develop in this paper also provides an\nevaluation of its own performance through an associated effective sample size.\nThe method is illustrated using several examples, including estimation of\nstandard distributions, time series, and population genetics models. \n\n"}
{"id": "1206.0039", "contents": "Title: A Multi-Baseline 12 GHz Atmospheric Phase Interferometer with One Micron\n  Path Length Sensitivity Abstract: We have constructed a five station 12 GHz atmospheric phase interferometer\n(API) for the Submillimeter Array (SMA) located near the summit of Mauna Kea,\nHawaii. Operating at the base of unoccupied SMA antenna pads, each station\nemploys a commercial low noise mixing block coupled to a 0.7 m off-axis\nsatellite dish which receives a broadband, white noise-like signal from a\ngeostationary satellite. The signals are processed by an analog correlator to\nproduce the phase delays between all pairs of stations with projected baselines\nranging from 33 to 261 m. Each baseline's amplitude and phase is measured\ncontinuously at a rate of 8 kHz, processed, averaged and output at 10 Hz.\nFurther signal processing and data reduction is accomplished with a Linux\ncomputer, including the removal of the diurnal motion of the target satellite.\nThe placement of the stations below ground level with an environmental shield\ncombined with the use of low temperature coefficient, buried fiber optic cables\nprovides excellent system stability. The sensitivity in terms of rms path\nlength is 1.3 microns which corresponds to phase deviations of about 1 degree\nof phase at the highest operating frequency of the SMA. The two primary data\nproducts are: (1) standard deviations of observed phase over various time\nscales, and (2) phase structure functions. These real-time statistical data\nmeasured by the API in the direction of the satellite provide an estimate of\nthe phase front distortion experienced by the concurrent SMA astronomical\nobservations. The API data also play an important role, along with the local\nopacity measurements and weather predictions, in helping to plan the scheduling\nof science observations on the telescope. \n\n"}
{"id": "1206.0338", "contents": "Title: Poisson noise reduction with non-local PCA Abstract: Photon-limited imaging arises when the number of photons collected by a\nsensor array is small relative to the number of detector elements. Photon\nlimitations are an important concern for many applications such as spectral\nimaging, night vision, nuclear medicine, and astronomy. Typically a Poisson\ndistribution is used to model these observations, and the inherent\nheteroscedasticity of the data combined with standard noise removal methods\nyields significant artifacts. This paper introduces a novel denoising algorithm\nfor photon-limited images which combines elements of dictionary learning and\nsparse patch-based representations of images. The method employs both an\nadaptation of Principal Component Analysis (PCA) for Poisson noise and recently\ndeveloped sparsity-regularized convex optimization algorithms for\nphoton-limited images. A comprehensive empirical evaluation of the proposed\nmethod helps characterize the performance of this approach relative to other\nstate-of-the-art denoising methods. The results reveal that, despite its\nconceptual simplicity, Poisson PCA-based denoising appears to be highly\ncompetitive in very low light regimes. \n\n"}
{"id": "1206.4709", "contents": "Title: Constructing acoustic timefronts using random matrix theory Abstract: In a recent letter [Europhys. Lett. 97, 34002 (2012)], random matrix theory\nis introduced for long-range acoustic propagation in the ocean. The theory is\nexpressed in terms of unitary propagation matrices that represent the\nscattering between acoustic modes due to sound speed fluctuations induced by\nthe ocean's internal waves. The scattering exhibits a power-law decay as a\nfunction of the differences in mode numbers thereby generating a power-law,\nbanded, random unitary matrix ensemble. This work gives a more complete account\nof that approach and extends the methods to the construction of an ensemble of\nacoustic timefronts. The result is a very efficient method for studying the\nstatistical properties of timefronts at various propagation ranges that agrees\nwell with propagation based on the parabolic equation. It helps identify which\ninformation about the ocean environment survives in the timefronts and how to\nconnect features of the data to the surviving environmental information. It\nalso makes direct connections to methods used in other disordered wave guide\ncontexts where the use of random matrix theory has a multi-decade history. \n\n"}
{"id": "1206.5396", "contents": "Title: Markov Chains on Orbits of Permutation Groups Abstract: We present a novel approach to detecting and utilizing symmetries in\nprobabilistic graphical models with two main contributions. First, we present a\nscalable approach to computing generating sets of permutation groups\nrepresenting the symmetries of graphical models. Second, we introduce orbital\nMarkov chains, a novel family of Markov chains leveraging model symmetries to\nreduce mixing times. We establish an insightful connection between model\nsymmetries and rapid mixing of orbital Markov chains. Thus, we present the\nfirst lifted MCMC algorithm for probabilistic graphical models. Both analytical\nand empirical results demonstrate the effectiveness and efficiency of the\napproach. \n\n"}
{"id": "1206.6519", "contents": "Title: A Permutation Approach to Testing Interactions in Many Dimensions Abstract: To date, testing interactions in high dimensions has been a challenging task.\nExisting methods often have issues with sensitivity to modeling assumptions and\nheavily asymptotic nominal p-values. To help alleviate these issues, we propose\na permutation-based method for testing marginal interactions with a binary\nresponse. Our method searches for pairwise correlations which differ between\nclasses. In this manuscript, we compare our method on real and simulated data\nto the standard approach of running many pairwise logistic models. On simulated\ndata our method finds more significant interactions at a lower false discovery\nrate (especially in the presence of main effects). On real genomic data,\nalthough there is no gold standard, our method finds apparent signal and tells\na believable story, while logistic regression does not. We also give asymptotic\nconsistency results under not too restrictive assumptions. \n\n"}
{"id": "1206.6812", "contents": "Title: Stirling's approximations for exchangeable Gibbs weights Abstract: We obtain some approximation results for the weights appearing in the\nexchangeable partition probability function identifying Gibbs partition models\nof parameter $\\alpha \\in (0,1)$, as introduced in Gnedin and Pitman (2006). We\nrely on approximation results for central and non-central generalized Stirling\nnumbers and on known results for conditional and unconditional $\\alpha$\ndiversity. We provide an application to an approximate Bayesian nonparametric\nestimation of discovery probability in species sampling problems under\nnormalized inverse Gaussian priors. \n\n"}
{"id": "1206.6919", "contents": "Title: Complete point symmetry group of the barotropic vorticity equation on a\n  rotating sphere Abstract: The complete point symmetry group of the barotropic vorticity equation on the\nsphere is determined. The method we use relies on the invariance of megaideals\nof the maximal Lie invariance algebra of a system of differential equations\nunder automorphisms generated by the associated group. A convenient set of\nmegaideals is found for the maximal Lie invariance algebra of the spherical\nvorticity equation. We prove that there are only two independent (up to\ncomposition with continuous point symmetry transformations) discrete symmetries\nfor this equation. \n\n"}
{"id": "1206.7051", "contents": "Title: Stochastic Variational Inference Abstract: We develop stochastic variational inference, a scalable algorithm for\napproximating posterior distributions. We develop this technique for a large\nclass of probabilistic models and we demonstrate it with two probabilistic\ntopic models, latent Dirichlet allocation and the hierarchical Dirichlet\nprocess topic model. Using stochastic variational inference, we analyze several\nlarge collections of documents: 300K articles from Nature, 1.8M articles from\nThe New York Times, and 3.8M articles from Wikipedia. Stochastic inference can\neasily handle data sets of this size and outperforms traditional variational\ninference, which can only handle a smaller subset. (We also show that the\nBayesian nonparametric topic model outperforms its parametric counterpart.)\nStochastic variational inference lets us apply complex Bayesian models to\nmassive data sets. \n\n"}
{"id": "1207.0105", "contents": "Title: Optimal inferential models for a Poisson mean Abstract: Statistical inference on the mean of a Poisson distribution is a\nfundamentally important problem with modern applications in, e.g., particle\nphysics. The discreteness of the Poisson distribution makes this problem\nsurprisingly challenging, even in the large-sample case. Here we propose a new\napproach, based on the recently developed framework of inferential models\n(IMs). Specifically, we construct optimal, or at least approximately optimal,\nIMs for two important classes of assertions/hypotheses about the Poisson mean.\nFor point assertions, we develop a novel recursive sorting algorithm to\nconstruct this optimal IM. Numerical comparisons of the proposed method to\nexisting methods are given, for both the mean and the more challenging\nmean-plus-background problem. \n\n"}
{"id": "1207.0520", "contents": "Title: Sparse Vector Autoregressive Modeling Abstract: The vector autoregressive (VAR) model has been widely used for modeling\ntemporal dependence in a multivariate time series. For large (and even\nmoderate) dimensions, the number of AR coefficients can be prohibitively large,\nresulting in noisy estimates, unstable predictions and difficult-to-interpret\ntemporal dependence. To overcome such drawbacks, we propose a 2-stage approach\nfor fitting sparse VAR (sVAR) models in which many of the AR coefficients are\nzero. The first stage selects non-zero AR coefficients based on an estimate of\nthe partial spectral coherence (PSC) together with the use of BIC. The PSC is\nuseful for quantifying the conditional relationship between marginal series in\na multivariate process. A refinement second stage is then applied to further\nreduce the number of parameters. The performance of this 2-stage approach is\nillustrated with simulation results. The 2-stage approach is also applied to\ntwo real data examples: the first is the Google Flu Trends data and the second\nis a time series of concentration levels of air pollutants. \n\n"}
{"id": "1207.3738", "contents": "Title: Ice structures, patterns, and processes: A view across the ice-fields Abstract: We look ahead from the frontiers of research on ice dynamics in its broadest\nsense; on the structures of ice, the patterns or morphologies it may assume,\nand the physical and chemical processes in which it is involved. We highlight\nopen questions in the various fields of ice research in nature; ranging from\nterrestrial and oceanic ice on Earth, to ice in the atmosphere, to ice on other\nsolar system bodies and in interstellar space. \n\n"}
{"id": "1207.4488", "contents": "Title: Microwave emissivity of freshwater ice, Part II: Modelling the Great\n  Bear and Great Slave Lakes Abstract: Lake ice within three Advanced Microwave Scanning Radiometer on EOS (AMSR-E)\npixels over the Great Bear and Great Slave Lakes have been simulated with the\nCanadian Lake Ice Model (CLIMo). The resulting thicknesses and temperatures\nwere fed to a radiative transfer-based ice emissivity model and compared to the\nsatellite measurements at three frequencies---6.925 GHz, 10.65 GHz and 18.7\nGHz. Excluding the melt season, the model was found to have strong predictive\npower, returning a correlation of 0.926 and a residual of 0.78 Kelvin at 18\nGHz, vertical polarization. Discrepencies at melt season are thought to be\ncaused by the presence of dirt in the snow cover which makes the microwave\nsignature more like soil rather than ice. Except at 18 GHz, all results showed\nsignificant bias compared to measured values. Further work needs to be done to\ndetermine the source of this bias. \n\n"}
{"id": "1207.6844", "contents": "Title: Two variants of the MCV3 scheme Abstract: Two variants of the MCV3 scheme are presented based on a flux reconstruction\nformulation. Different from the original multi-moment constrained finite volume\nmethod of third order (MCV3), the multi-moment constraints are imposed at the\ncell center on the point value, the first and second order derivatives. The\ncontinuity of the flux function at cell interfaces are also used as the\nconstraints to ensure the numerical conservation. Compared to the original MCV3\nscheme, both two variants have higher numerical accuracy and less restrictive\nCFL condition for computational stability. Moreover, without the need to solve\nderivative Riemann problem at cell boundaries, the new schemes benefit the\nimplementations in arbitrary quadrilateral in 2D and hexahedron in 3D. \n\n"}
{"id": "1208.1061", "contents": "Title: Data management and analysis with WRF and SFIRE Abstract: We introduce several useful utilities in development for the creation and\nanalysis of real wildland fire simulations using WRF and SFIRE. These utilities\nexist as standalone programs and scripts as well as extensions to other well\nknown software. Python web scrapers automate the process of downloading and\npreprocessing atmospheric and surface data from common sources. Other scripts\nsimplify the domain setup by creating parameter files automatically.\nIntegration with Google Earth allows users to explore the simulation in a 3D\nenvironment along with real surface imagery. Postprocessing scripts provide the\nuser with a number of output data formats compatible with many commonly used\nvisualization suites allowing for the creation of high quality 3D renderings.\nAs a whole, these improvements build toward a unified web application that\nbrings a sophisticated wildland fire modeling environment to scientists and\nusers alike. \n\n"}
{"id": "1208.1720", "contents": "Title: Mixing Coefficients Between Discrete and Real Random Variables:\n  Computation and Properties Abstract: In this paper we study the problem of estimating the alpha-, beta- and\nphi-mixing coefficients between two random variables, that can either assume\nvalues in a finite set or the set of real numbers. In either case, explicit\nclosed-form formulas for the beta-mixing coefficient are already known.\nTherefore for random variables assuming values in a finite set, our\ncontributions are two-fold: (i) In the case of the alpha-mixing coefficient, we\nshow that determining whether or not it exceeds a prespecified threshold is\nNP-complete, and provide efficiently computable upper and lower bounds. (ii) We\nderive an exact closed-form formula for the phi-mixing coefficient. Next, we\nprove analogs of the data-processing inequality from information theory for\neach of the three kinds of mixing coefficients. Then we move on to real-valued\nrandom variables, and show that by using percentile binning and allowing the\nnumber of bins to increase more slowly than the number of samples, we can\ngenerate empirical estimates that are consistent, i.e., converge to the true\nvalues as the number of samples approaches infinity. \n\n"}
{"id": "1208.2208", "contents": "Title: Pairing of charged particles in a quantum plasmoid Abstract: We study a quantum spherically symmetric object which is based on radial\nplasma oscillations. Such a plasmoid is supposed to exist in a dense plasma\ncontaining electrons, ions, and neutral particles. The method of creation and\nannihilation operators is applied to quantize the motion of charged particles\nin a self-consistent potential. We also study the effective interaction between\noscillating particles owing to the exchange of a virtual acoustic wave, which\nis excited in the neutral component of plasma. It is shown that this\ninteraction can be attractive and result in the formation of ion pairs. We\ndiscuss possible applications of this phenomenon in astrophysical and\nterrestrial plasmas. \n\n"}
{"id": "1208.2678", "contents": "Title: Nonequilibrium thermodynamics of circulation regimes in optically-thin,\n  dry atmospheres Abstract: An extensive analysis of an optically-thin, dry atmosphere at different\nvalues of the thermal Rossby number Ro and of the Taylor number Ff is per-\nformed with a general circulation model by varying the rotation rate {\\Omega}\nand the surface drag {\\tau} in a wide parametric range. By using nonequilibrium\nthermodynamics diagnostics such as material entropy production, efficiency,\nmeridional heat transport and kinetic energy dissipation we characterize in a\nnew way the different circulation regimes. Baroclinic circulations feature high\nmechanical dissipation, meridional heat transport, material entropy pro-\nduction and are fairly efficient in converting heat into mechanical work. The\nthermal dissipation associated with the sensible heat flux is found to depend\nmainly on the surface properties, almost independent from the rotation rate and\nvery low for quasi-barotropic circulations and regimes approaching equa- torial\nsuper-rotation. Slowly rotating, axisymmetric circulations have the highest\nmeridional heat transport. At high rotation rates and intermediate- high drag,\natmospheric circulations are zonostrohic with very low mechanical dissipation,\nmeridional heat transport and efficiency. When {\\tau} is interpreted as a\ntunable parameter associated with the turbulent boundary layer trans- fer of\nmomentum and sensible heat, our results confirm the possibility of using the\nMaximum Entropy Production Principle as a tuning guideline in the range of\nvalues of {\\Omega}. This study suggests the effectiveness of using fun-\ndamental nonequilibrium thermodynamics for investigating the properties of\nplanetary atmospheres and extends our knowledge of the thermodynamics of the\natmospheric circulation regimes. \n\n"}
{"id": "1208.3734", "contents": "Title: The key physical parameters governing frictional dissipation in a\n  precipitating atmosphere Abstract: Precipitation generates small-scale turbulent air flows the energy of which\nultimately dissipates to heat. The power of this process has previously been\nestimated to be around 2-4 W m-2 in the tropics: a value comparable in\nmagnitude to the dynamic power of the global circulation. Here we suggest that\nthis previous power estimate is approximately double the true figure. Our\nresult reflects a revised evaluation of the mean precipitation path length Hp.\nWe investigate the dependence of Hp on surface temperature,relative\nhumidity,temperature lapse rate and degree of condensation in the ascending\nair. We find that the degree of condensation,defined as the relative change of\nthe saturated water vapor mixing ratio in the region of condensation, is a\nmajor factor determining Hp. We estimate from theory that the mean large-scale\nrate of frictional dissipation associated with total precipitation in the\ntropics lies between 1 and 2 W m-2 and show that our estimate is supported by\nempirical evidence. We show that under terrestrial conditions frictional\ndissipation constitutes a minor fraction of the dynamic power of\ncondensation-induced atmospheric circulation,which is estimated to be at least\n2.5 times larger. However,because Hp increases with surface temperature Ts, the\nrate of frictional dissipation would exceed that of condensation-induced\ndynamics, and thus block major circulation, at Ts >~320 K in a moist adiabatic\natmosphere. \n\n"}
{"id": "1208.5600", "contents": "Title: A population Monte Carlo scheme with transformed weights and its\n  application to stochastic kinetic models Abstract: This paper addresses the problem of Monte Carlo approximation of posterior\nprobability distributions. In particular, we have considered a recently\nproposed technique known as population Monte Carlo (PMC), which is based on an\niterative importance sampling approach. An important drawback of this\nmethodology is the degeneracy of the importance weights when the dimension of\neither the observations or the variables of interest is high. To alleviate this\ndifficulty, we propose a novel method that performs a nonlinear transformation\non the importance weights. This operation reduces the weight variation, hence\nit avoids their degeneracy and increases the efficiency of the importance\nsampling scheme, specially when drawing from a proposal functions which are\npoorly adapted to the true posterior.\n  For the sake of illustration, we have applied the proposed algorithm to the\nestimation of the parameters of a Gaussian mixture model. This is a very simple\nproblem that enables us to clearly show and discuss the main features of the\nproposed technique. As a practical application, we have also considered the\npopular (and challenging) problem of estimating the rate parameters of\nstochastic kinetic models (SKM). SKMs are highly multivariate systems that\nmodel molecular interactions in biological and chemical problems. We introduce\na particularization of the proposed algorithm to SKMs and present numerical\nresults. \n\n"}
{"id": "1208.5665", "contents": "Title: Emergence and equilibration of jets in beta-plane turbulence:\n  applications of Stochastic Structural Stability Theory Abstract: Stochastic Structural Stability Theory (S3T) provides analytical methods for\nunderstanding the emergence and equilibration of jets from the turbulence in\nplanetary atmospheres based on the dynamics of the statistical mean state of\nthe turbulence closed at second order. Predictions for formation and\nequilibration of turbulent jets made using S3T are critically compared with\nresults of simulations made using the associated quasi-linear and nonlinear\nmodels. S3T predicts the observed bifurcation behavior associated with the\nemergence of jets, their equilibration and their breakdown as a function of\nparameters. Quantitative differences in bifurcation parameter values between\npredictions of S3T and results of nonlinear simulations are traced to\nmodification of the eddy spectrum which results from two processes: nonlinear\neddy-eddy interactions and formation of discrete non-zonal structures.\nRemarkably, these non-zonal structures, which substantially modify the\nturbulence spectrum, are found to arise from S3T instability. Formation as\nlinear instabilities and equilibration at finite amplitude of multiple\nequilibria for identical parameter values in the form of jets with distinct\nmeridional wavenumbers is verified as is the existence at equilibrium of finite\namplitude non-zonal structures in the form of nonlinearly modified Rossby\nwaves. When zonal jets and nonlinearly modified Rossby waves coexist at finite\namplitude the jet structure is generally found to dominate even if it is\nlinearly less unstable. The physical reality of the manifold of S3T jets and\nnon-zonal structures is underscored by the existence in nonlinear simulations\nof jet structure at subcritical S3T parameter values which are identified with\nstable S3T jet modes excited by turbulent fluctuations. \n\n"}
{"id": "1208.6275", "contents": "Title: Atmospheric aerosols at the Pierre Auger Observatory and environmental\n  implications Abstract: The Pierre Auger Observatory detects the highest energy cosmic rays.\nCalorimetric measurements of extensive air showers induced by cosmic rays are\nperformed with a fluorescence detector. Thus, one of the main challenges is the\natmospheric monitoring, especially for aerosols in suspension in the\natmosphere. Several methods are described which have been developed to measure\nthe aerosol optical depth profile and aerosol phase function, using lasers and\nother light sources as recorded by the fluorescence detector. The origin of\natmospheric aerosols traveling through the Auger site is also presented,\nhighlighting the effect of surrounding areas to atmospheric properties. In the\naim to extend the Pierre Auger Observatory to an atmospheric research platform,\na discussion about a collaborative project is presented. \n\n"}
{"id": "1209.1886", "contents": "Title: Evolution of a barotropic shear layer into elliptical vortices Abstract: When a barotropic shear layer becomes unstable, it produces the well known\nKelvin-Helmholtz instability (KH). The non-linear manifestation of KH is\nusually in the form of spiral billows. However, a piecewise linear shear layer\nproduces a different type of KH characterized by elliptical vortices of\nconstant vorticity connected via thin braids. Using direct numerical simulation\nand contour dynamics, we show that the interaction between two\ncounter-propagating vorticity waves is solely responsible for this KH\nformation. We investigate the oscillation of the vorticity wave amplitude, the\nrotation and nutation of the elliptical vortex, and straining of the braids.\nOur analysis also provides possible explanation behind the formation and\nevolution of elliptical vortices appearing in geophysical and astrophysical\nflows, e.g. meddies, Stratospheric polar vortices, Jovian vortices, Neptune's\nGreat Dark Spot and coherent vortices in the wind belts of Uranus. \n\n"}
{"id": "1209.3613", "contents": "Title: Robust seasonal cycle of Arctic sea ice area through tipping point in\n  amplitude Abstract: The variation in the Arctic sea ice is dominated by the seasonal cycle with\nlittle inter-annual correlation. Though the mean sea ice area has decreased\nsteadily in the period of satellite observations, a dramatic transition in the\ndynamics was initiated with the record low September ice area in 2007. The\nchange is much more pronounced in the amplitude of the seasonal cycle than in\nthe annual mean ice area. The shape of the seasonal cycle is surprisingly\nconstant for the whole observational record despite the general decline. A\nsimple explanation, independent of the increased greenhouse warming, for the\nshape of the seasonal cycle is offered. Thus the dramatic climate change in\narctic ice area is seen in the amplitude of the cycle and to a lesser extend\nthe annual mean and the summer ice extend. The reason why the climate change is\nmost pronounced in the amplitude is related to the rapid reduction in perennial\nice and thus a thinning of the ice. The analysis shows that a tipping point for\nthe arctic ice area was crossed in 2007. \n\n"}
{"id": "1209.3862", "contents": "Title: Direct Statistical Simulation of Out-of-Equilibrium Jets Abstract: We present a Direct Statistical Simulation (DSS) of jet formation on a\n\\beta-plane, solving for the statistics of a fluid flow via an expansion in\ncumulants. Here we compare an expansion truncated at second order (CE2) to\nstatistics accumulated by direct numerical simulations (DNS). We show that, for\njets near equilibrium, CE2 is capable of reproducing the jet structure\n(although some differences remain in the second cumulant). However as the\ndegree of departure from equilibrium is increased (as measured by the\nzonostrophy parameter) the jets meander more and CE2 becomes less accurate. We\ndiscuss a possible remedy by inclusion of higher cumulants. \n\n"}
{"id": "1209.4129", "contents": "Title: Comunication-Efficient Algorithms for Statistical Optimization Abstract: We analyze two communication-efficient algorithms for distributed statistical\noptimization on large-scale data sets. The first algorithm is a standard\naveraging method that distributes the $N$ data samples evenly to $\\nummac$\nmachines, performs separate minimization on each subset, and then averages the\nestimates. We provide a sharp analysis of this average mixture algorithm,\nshowing that under a reasonable set of conditions, the combined parameter\nachieves mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-2})$.\nWhenever $m \\le \\sqrt{N}$, this guarantee matches the best possible rate\nachievable by a centralized algorithm having access to all $\\totalnumobs$\nsamples. The second algorithm is a novel method, based on an appropriate form\nof bootstrap subsampling. Requiring only a single round of communication, it\nhas mean-squared error that decays as $\\order(N^{-1} + (N/m)^{-3})$, and so is\nmore robust to the amount of parallelization. In addition, we show that a\nstochastic gradient-based method attains mean-squared error decaying as\n$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties in\nthe rate of convergence. We also provide experimental evaluation of our\nmethods, investigating their performance both on simulated data and on a\nlarge-scale regression problem from the internet search domain. In particular,\nwe show that our methods can be used to efficiently solve an advertisement\nprediction problem from the Chinese SoSo Search Engine, which involves logistic\nregression with $N \\approx 2.4 \\times 10^8$ samples and $d \\approx 740,000$\ncovariates. \n\n"}
{"id": "1209.4279", "contents": "Title: Conservative parameterization schemes Abstract: Parameterization (closure) schemes in numerical weather and climate\nprediction models account for the effects of physical processes that cannot be\nresolved explicitly by these models. Methods for finding physical\nparameterization schemes that preserve conservation laws of systems of\ndifferential equations are introduced. These methods rest on the possibility to\nregard the problem of finding conservative parameterization schemes as a\nconservation law classification problem for classes of differential equations.\nThe relevant classification problems can be solved using the direct or inverse\nclassification procedures. In the direct approach, one starts with a general\nfunctional form of the parameterization scheme. Specific forms are then found\nso that corresponding closed equations admit conservation laws. In the inverse\napproach, one seeks parameterization schemes that preserve one or more\npre-selected conservation laws of the initial model. The physical\ninterpretation of both classification approaches is discussed. Special\nattention is paid to the problem of finding parameterization schemes that\npreserve both conservation laws and symmetries. All methods are illustrated by\nfinding conservative and conservative invariant parameterization schemes for\nsystems of one-dimensional shallow-water equations. \n\n"}
{"id": "1209.5359", "contents": "Title: On Simulations from the Two-Parameter Poisson-Dirichlet Process and the\n  Normalized Inverse-Gaussian Process Abstract: In this paper, we develop simple, yet efficient, procedures for sampling\napproximations of the two-Parameter Poisson-Dirichlet Process and the\nnormalized inverse-Gaussian process. We compare the efficiency of the new\napproximations to the corresponding stick-breaking approximations, in which we\ndemonstrate a substantial improvement. \n\n"}
{"id": "1210.3405", "contents": "Title: Simultaneous adjustment of bias and coverage probabilities for\n  confidence intervals Abstract: A new method is proposed for the correction of confidence intervals when the\noriginal interval does not have the correct nominal coverage probabilities in\nthe frequentist sense. The proposed method is general and does not require any\ndistributional assumptions. It can be applied to both frequentist and Bayesian\ninference where interval estimates are desired. We provide theoretical results\nfor the consistency of the proposed estimator, and give two complex examples,\non confidence interval correction for composite likelihood estimators and in\napproximate Bayesian computation (ABC), to demonstrate the wide applicability\nof the new method. Comparison is made with the double-bootstrap and other\nmethods of improving confidence interval coverage. \n\n"}
{"id": "1210.6703", "contents": "Title: Variance bounding and geometric ergodicity of Markov chain Monte Carlo\n  kernels for approximate Bayesian computation Abstract: Approximate Bayesian computation has emerged as a standard computational tool\nwhen dealing with the increasingly common scenario of completely intractable\nlikelihood functions in Bayesian inference. We show that many common Markov\nchain Monte Carlo kernels used to facilitate inference in this setting can fail\nto be variance bounding, and hence geometrically ergodic, which can have\nconsequences for the reliability of estimates in practice. This phenomenon is\ntypically independent of the choice of tolerance in the approximation. We then\nprove that a recently introduced Markov kernel in this setting can inherit\nvariance bounding and geometric ergodicity from its intractable\nMetropolis--Hastings counterpart, under reasonably weak and manageable\nconditions. We show that the computational cost of this alternative kernel is\nbounded whenever the prior is proper, and present indicative results on an\nexample where spectral gaps and asymptotic variances can be computed, as well\nas an example involving inference for a partially and discretely observed,\ntime-homogeneous, pure jump Markov process. We also supply two general\ntheorems, one of which provides a simple sufficient condition for lack of\nvariance bounding for reversible kernels and the other provides a positive\nresult concerning inheritance of variance bounding and geometric ergodicity for\nmixtures of reversible kernels. \n\n"}
{"id": "1210.7726", "contents": "Title: Performance Analysis of Parameter Estimation Using LASSO Abstract: The Least Absolute Shrinkage and Selection Operator (LASSO) has gained\nattention in a wide class of continuous parametric estimation problems with\npromising results. It has been a subject of research for more than a decade.\nDue to the nature of LASSO, the previous analyses have been non-parametric.\nThis ignores useful information and makes it difficult to compare LASSO to\ntraditional estimators. In particular, the role of the regularization parameter\nand super-resolution properties of LASSO have not been well-understood yet. The\nobjective of this work is to provide a new insight into this context by\nintroducing LASSO as a parametric technique of a varying order. This provides\nus theoretical expressions for the LASSO-based estimation error and false alarm\nrate in the asymptotic case of high SNR and dense grids. For this case, LASSO\nis compared to maximum likelihood and conventional beamforming. It is found\nthat LASSO loses performance due to the regularization term, but the amount of\nloss is practically negligible with a proper choice of the regularization\nparameter. Thus, we provide suggestions on the selection of the regularization\nparameter. Without loss of generality, we present the comparative numerical\nresults in the context of Direction of Arrival (DOA) estimation using a sensor\narray. \n\n"}
{"id": "1211.0878", "contents": "Title: Ramsauer approach for light scattering on non-absorbing spherical\n  particles and application to the Henyey-Greenstein phase function Abstract: We present a new method to study light scattering on non-absorbing spherical\nparticles. This method is based on the Ramsauer approach, a model known in\natomic an nuclear physics. Its main advantage is its intuitive understanding of\nthe underlying physics phenomena. We show that although the approximations are\nnumerous, the Ramsauer analytical solutions describe fairly well the scattering\nphase function and the total cross section. Then this model is applied to the\nHenyey-Greenstein parameterisation of scattering phase function to give a\nrelation between its asymmetry parameter and the mean particle size. \n\n"}
{"id": "1211.3210", "contents": "Title: Fast estimation of the ICL criterion for change-point detection problems\n  with applications to Next-Generation Sequencing data Abstract: In this paper, we consider the Integrated Completed Likelihood (ICL) as a\nuseful criterion for estimating the number of changes in the underlying\ndistribution of data in problems where detecting the precise location of these\nchanges is the main goal. The exact computation of the ICL requires O(Kn2)\noperations (with K the number of segments and n the number of data-points)\nwhich is prohibitive in many practical situations with large sequences of data.\nWe describe a framework to estimate the ICL with O(Kn) complexity. Our approach\nis general in the sense that it can accommodate any given model distribution.\nWe checked the run-time and validity of our approach on simulated data and\ndemonstrate its good performance when analyzing real Next-Generation Sequencing\n(NGS) data using a negative binomial model. \n\n"}
{"id": "1211.3825", "contents": "Title: Generation of two-dimensional water waves by moving bottom disturbances Abstract: We investigate the potential and limitations of the wave generation by\ndisturbances moving at the bottom. More precisely, we assume that the wavemaker\nis composed of an underwater object of a given shape which can be displaced\naccording to a prescribed trajectory. We address the practical question of\ncomputing the wavemaker shape and trajectory generating a wave with prescribed\ncharacteristics. For the sake of simplicity we model the hydrodynamics by a\ngeneralized forced Benjamin-Bona-Mahony (BBM) equation. This practical problem\nis reformulated as a constrained nonlinear optimization problem. Additional\nconstraints are imposed in order to fulfill various practical design\nrequirements. Finally, we present some numerical results in order to\ndemonstrate the feasibility and performance of the proposed methodology. \n\n"}
{"id": "1211.6582", "contents": "Title: Emergence of large scale structure in planetary turbulence Abstract: Planetary and magnetohydrodynamic drift-wave turbulence is observed to\nself-organize into large scale structures such as zonal jets and coherent\nvortices. In this Letter we present a non-equilibrium statistical theory, the\nStochastic Structural Stability theory (SSST), that can make predictions for\nthe formation and finite amplitude equilibration of non-zonal and zonal\nstructures (lattice and stripe patterns) in homogeneous turbulence. This theory\nreveals that the emergence of large scale structure is the result of an\ninstability of the interaction between the coherent flow and the associated\nturbulent field. Comparison of the theory with nonlinear simulations of a\nbarotropic flow in a beta-plane channel with turbulence sustained by isotropic\nrandom stirring, demonstrates that SSST predicts the threshold parameters at\nwhich the coherent structures emerge as well as the characteristics of the\nemerging structures (scale, amplitude, phase speed). It is shown that non-zonal\nstructures (lattice states or zonons) emerge at lower energy input rates of the\nstirring compared to zonal flows (stripe states) and their emergence affects\nthe dynamics of jet formation. \n\n"}
{"id": "1212.0122", "contents": "Title: Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm Abstract: Markov Chain Monte Carlo methods are widely used in signal processing and\ncommunications for statistical inference and stochastic optimization. In this\nwork, we introduce an efficient adaptive Metropolis-Hastings algorithm to draw\nsamples from generic multi-modal and multi-dimensional target distributions.\nThe proposal density is a mixture of Gaussian densities with all parameters\n(weights, mean vectors and covariance matrices) updated using all the\npreviously generated samples applying simple recursive rules. Numerical results\nfor the one and two-dimensional cases are provided. \n\n"}
{"id": "1212.3721", "contents": "Title: Approximate continuous-discrete filters for the estimation of diffusion\n  processes from partial and noisy observations Abstract: In this paper, an alternative approximation to the innovation method is\nintroduced for the parameter estimation of diffusion processes from partial and\nnoisy observations. This is based on a convergent approximation to the first\ntwo conditional moments of the innovation process through approximate\ncontinuous-discrete filters of minimum variance. It is shown that, for finite\nsamples, the resulting approximate estimators converge to the exact one when\nthe error of the approximate filters decreases. For an increasing number of\nobservations, the estimators are asymptotically normal distributed and their\nbias decreases when the above mentioned error does it. A simulation study is\nprovided to illustrate the performance of the new estimators. The results show\nthat, with respect to the conventional approximate estimators, the new ones\nsignificantly enhance the parameter estimation of the test equations. The\nproposed estimators are intended for the recurrent practical situation where a\nnonlinear stochastic system should be identified from a reduced number of\npartial and noisy observations distant in time. \n\n"}
{"id": "1212.5336", "contents": "Title: Percolation transition in the kinematics of nonlinear resonance\n  broadening in Charney-Hasegawa-Mima model of Rossby wave turbulence Abstract: We study the kinematics of nonlinear resonance broadening of interacting\nRossby waves as modelled by the Charney-Hasegawa-Mima equation on a biperiodic\ndomain. We focus on the set of wave modes which can interact quasi-resonantly\nat a particular level of resonance broadening and aim to characterise how the\nstructure of this set changes as the level of resonance broadening is varied.\nThe commonly held view that resonance broadening can be thought of as a\nthickening of the resonant manifold is misleading. We show that in fact the set\nof modes corresponding to a single quasi-resonant triad has a nontrivial\nstructure and that its area in fact diverges for a finite degree of broadening.\nWe also study the connectivity of the network of modes which is generated when\nquasi-resonant triads share common modes. This network has been argued to form\nthe backbone for energy transfer in Rossby wave turbulence. We show that this\nnetwork undergoes a percolation transition when the level of resonance\nbroadening exceeds a critical value. Below this critical value, the largest\nconnected component of the quasi-resonant network contains a negligible\nfraction of the total number of modes in the system whereas above this critical\nvalue a finite fraction of the total number of modes in the system are\ncontained in the largest connected component. We argue that this percolation\ntransition should correspond to the transition to turbulence in the system. \n\n"}
{"id": "1301.2975", "contents": "Title: Fast Approximate Bayesian Computation for discretely observed Markov\n  models using a factorised posterior distribution Abstract: Many modern statistical applications involve inference for complicated\nstochastic models for which the likelihood function is difficult or even\nimpossible to calculate, and hence conventional likelihood-based inferential\nechniques cannot be used. In such settings, Bayesian inference can be performed\nusing Approximate Bayesian Computation (ABC). However, in spite of many recent\ndevelopments to ABC methodology, in many applications the computational cost of\nABC necessitates the choice of summary statistics and tolerances that can\npotentially severely bias the estimate of the posterior.\n  We propose a new \"piecewise\" ABC approach suitable for discretely observed\nMarkov models that involves writing the posterior density of the parameters as\na product of factors, each a function of only a subset of the data, and then\nusing ABC within each factor. The approach has the advantage of side-stepping\nthe need to choose a summary statistic and it enables a stringent tolerance to\nbe set, making the posterior \"less approximate\". We investigate two methods for\nestimating the posterior density based on ABC samples for each of the factors:\nthe first is to use a Gaussian approximation for each factor, and the second is\nto use a kernel density estimate. Both methods have their merits. The Gaussian\napproximation is simple, fast, and probably adequate for many applications. On\nthe other hand, using instead a kernel density estimate has the benefit of\nconsistently estimating the true ABC posterior as the number of ABC samples\ntends to infinity. We illustrate the piecewise ABC approach for three examples;\nin each case, the approach enables \"exact matching\" between simulations and\ndata and offers fast and accurate inference. \n\n"}
{"id": "1301.3690", "contents": "Title: Characterization of the sodium layer at Cerro Pachon, and impact on\n  laser guide star performance Abstract: Detailed knowledge of the mesopheric sodium layer characteristics is crucial\nto estimate and optimize the performance of Laser Guide Star (LGS) assisted\nAdaptive Optics (AO) systems. In this paper, we present an analysis of two sets\nof data on the mesospheric sodium layer. The first set comes from a laser\nexperiment that was carried out at Cerro Tololo to monitor the abundance and\naltitude of the mesospheric sodium in 2001, during six runs covering a period\nof one year. This data is used to derive the mesospheric sodium column density,\nthe sodium layer thickness and the temporal behavior of the sodium layer mean\naltitude. The second set of data was gathered during the first year of the\nGemini MCAO System (GeMS) commissioning and operations. GeMS uses five LGS to\nmeasure and compensate for atmospheric distortions. Analysis of the LGS\nwavefront sensor data provides information about the sodium photon return and\nthe spot elongation seen by the WFS. All these parameters show large variations\non a yearly, nightly and hourly basis, affecting the LGS brightness, shape and\nmean altitude. The sodium photon return varies by a factor of three to four\nover a year, and can change by a factor of two over a night. In addition, the\ncomparison of the photon returns obtained in 2001 with those measured a decade\nlater using GeMS shows a significant difference in laser format efficiencies.\nWe find that the temporal power spectrum of the sodium mean altitude follows a\nlinear trend, in good agreement with the results reported by Pfrommer & Hickson\n(2010). \n\n"}
{"id": "1301.3928", "contents": "Title: Importance sampling for weighted binary random matrices with specified\n  margins Abstract: A sequential importance sampling algorithm is developed for the distribution\nthat results when a matrix of independent, but not identically distributed,\nBernoulli random variables is conditioned on a given sequence of row and column\nsums. This conditional distribution arises in a variety of applications and\nincludes as a special case the uniform distribution over zero-one tables with\nspecified margins. The algorithm uses dynamic programming to combine hard\nmargin constraints, combinatorial approximations, and additional non-uniform\nweighting in a principled way to give state-of-the-art results. \n\n"}
{"id": "1301.4976", "contents": "Title: Supervised Classification Using Sparse Fisher's LDA Abstract: It is well known that in a supervised classification setting when the number\nof features is smaller than the number of observations, Fisher's linear\ndiscriminant rule is asymptotically Bayes. However, there are numerous modern\napplications where classification is needed in the high-dimensional setting.\nNaive implementation of Fisher's rule in this case fails to provide good\nresults because the sample covariance matrix is singular. Moreover, by\nconstructing a classifier that relies on all features the interpretation of the\nresults is challenging. Our goal is to provide robust classification that\nrelies only on a small subset of important features and accounts for the\nunderlying correlation structure. We apply a lasso-type penalty to the\ndiscriminant vector to ensure sparsity of the solution and use a shrinkage type\nestimator for the covariance matrix. The resulting optimization problem is\nsolved using an iterative coordinate ascent algorithm. Furthermore, we analyze\nthe effect of nonconvexity on the sparsity level of the solution and highlight\nthe difference between the penalized and the constrained versions of the\nproblem. The simulation results show that the proposed method performs\nfavorably in comparison to alternatives. The method is used to classify\nleukemia patients based on DNA methylation features. \n\n"}
{"id": "1301.6635", "contents": "Title: Exact sampling and counting for fixed-margin matrices Abstract: The uniform distribution on matrices with specified row and column sums is\noften a natural choice of null model when testing for structure in two-way\ntables (binary or nonnegative integer). Due to the difficulty of sampling from\nthis distribution, many approximate methods have been developed. We will show\nthat by exploiting certain symmetries, exact sampling and counting is in fact\npossible in many nontrivial real-world cases. We illustrate with real datasets\nincluding ecological co-occurrence matrices and contingency tables. \n\n"}
{"id": "1302.0261", "contents": "Title: The Group Square-Root Lasso: Theoretical Properties and Fast Algorithms Abstract: We introduce and study the Group Square-Root Lasso (GSRL) method for\nestimation in high dimensional sparse regression models with group structure.\nThe new estimator minimizes the square root of the residual sum of squares plus\na penalty term proportional to the sum of the Euclidean norms of groups of the\nregression parameter vector. The net advantage of the method over the existing\nGroup Lasso (GL)-type procedures consists in the form of the proportionality\nfactor used in the penalty term, which for GSRL is independent of the variance\nof the error terms. This is of crucial importance in models with more\nparameters than the sample size, when estimating the variance of the noise\nbecomes as difficult as the original problem. We show that the GSRL estimator\nadapts to the unknown sparsity of the regression vector, and has the same\noptimal estimation and prediction accuracy as the GL estimators, under the same\nminimal conditions on the model. This extends the results recently established\nfor the Square-Root Lasso, for sparse regression without group structure.\nMoreover, as a new type of result for Square-Root Lasso methods, with or\nwithout groups, we study correct pattern recovery, and show that it can be\nachieved under conditions similar to those needed by the Lasso or\nGroup-Lasso-type methods, but with a simplified tuning strategy. We implement\nour method via a new algorithm, with proved convergence properties, which,\nunlike existing methods, scales well with the dimension of the problem. Our\nsimulation studies support strongly our theoretical findings. \n\n"}
{"id": "1302.0893", "contents": "Title: Probabilistic Quantitative Precipitation Forecasting Using Ensemble\n  Model Output Statistics Abstract: Statistical post-processing of dynamical forecast ensembles is an essential\ncomponent of weather forecasting. In this article, we present a post-processing\nmethod that generates full predictive probability distributions for\nprecipitation accumulations based on ensemble model output statistics (EMOS).\nWe model precipitation amounts by a generalized extreme value distribution that\nis left-censored at zero. This distribution permits modelling precipitation on\nthe original scale without prior transformation of the data. A closed form\nexpression for its continuous rank probability score can be derived and permits\ncomputationally efficient model fitting. We discuss an extension of our\napproach that incorporates further statistics characterizing the spatial\nvariability of precipitation amounts in the vicinity of the location of\ninterest. The proposed EMOS method is applied to daily 18-h forecasts of 6-h\naccumulated precipitation over Germany in 2011 using the COSMO-DE ensemble\nprediction system operated by the German Meteorological Service. It yields\ncalibrated and sharp predictive distributions and compares favourably with\nextended logistic regression and Bayesian model averaging which are state of\nthe art approaches for precipitation post-processing. The incorporation of\nneighbourhood information further improves predictive performance and turns out\nto be a useful strategy to account for displacement errors of the dynamical\nforecasts in a probabilistic forecasting framework. \n\n"}
{"id": "1302.6989", "contents": "Title: The Bayesian Approach To Inverse Problems Abstract: These lecture notes highlight the mathematical and computational structure\nrelating to the formulation of, and development of algorithms for, the Bayesian\napproach to inverse problems in differential equations. This approach is\nfundamental in the quantification of uncertainty within applications involving\nthe blending of mathematical models with data. \n\n"}
{"id": "1303.2423", "contents": "Title: Discrepancy bounds for uniformly ergodic Markov chain quasi-Monte Carlo Abstract: Markov chains can be used to generate samples whose distribution approximates\na given target distribution. The quality of the samples of such Markov chains\ncan be measured by the discrepancy between the empirical distribution of the\nsamples and the target distribution. We prove upper bounds on this discrepancy\nunder the assumption that the Markov chain is uniformly ergodic and the driver\nsequence is deterministic rather than independent $U(0,1)$ random variables. In\nparticular, we show the existence of driver sequences for which the discrepancy\nof the Markov chain from the target distribution with respect to certain test\nsets converges with (almost) the usual Monte Carlo rate of $n^{-1/2}$. \n\n"}
{"id": "1303.3775", "contents": "Title: Approximation for the Distribution of Three-dimensional Discrete Scan\n  Statistic Abstract: We consider the discrete three dimensional scan statistics. Viewed as the\nmaximum of an 1-dependent stationary r.v.'s sequence, we provide approximations\nand error bounds for the probability distribution of the three dimensional scan\nstatistics. Importance sampling algorithm is used to obtains sharp bounds for\nthe simulation error. Simulation results and comparisons with other\napproximations are presented for the binomial and Poisson models. \n\n"}
{"id": "1303.6223", "contents": "Title: Random Intersection Trees Abstract: Finding interactions between variables in large and high-dimensional datasets\nis often a serious computational challenge. Most approaches build up\ninteraction sets incrementally, adding variables in a greedy fashion. The\ndrawback is that potentially informative high-order interactions may be\noverlooked. Here, we propose at an alternative approach for classification\nproblems with binary predictor variables, called Random Intersection Trees. It\nworks by starting with a maximal interaction that includes all variables, and\nthen gradually removing variables if they fail to appear in randomly chosen\nobservations of a class of interest. We show that informative interactions are\nretained with high probability, and the computational complexity of our\nprocedure is of order $p^\\kappa$ for a value of $\\kappa$ that can reach values\nas low as 1 for very sparse data; in many more general settings, it will still\nbeat the exponent $s$ obtained when using a brute force search constrained to\norder $s$ interactions. In addition, by using some new ideas based on min-wise\nhash schemes, we are able to further reduce the computational cost.\nInteractions found by our algorithm can be used for predictive modelling in\nvarious forms, but they are also often of interest in their own right as useful\ncharacterisations of what distinguishes a certain class from others. \n\n"}
{"id": "1303.6435", "contents": "Title: A theory for the emergence of coherent structures in beta-plane\n  turbulence Abstract: Planetary turbulent flows are observed to self-organize into large scale\nstructures such as zonal jets and coherent vortices. One of the simplest models\nof planetary turbulence is obtained by considering a barotropic flow on a\nbeta-plane channel with turbulence sustained by random stirring. Non-linear\nintegrations of this model show that as the energy input rate of the forcing is\nincreased, the homogeneity of the flow is broken with the emergence of\nnon-zonal, coherent, westward propagating structures and at larger energy input\nrates by the emergence of zonal jets. We study the emergence of non-zonal\ncoherent structures using a non-equilibrium statistical theory, Stochastic\nStructural Stability Theory (S3T, previously referred to as SSST). S3T directly\nmodels a second order approximation to the statistical mean turbulent state and\nallows identification of statistical turbulent equilibria and study of their\nstability. Using S3T, the bifurcation properties of the homogeneous state in\nbarotropic beta-plane turbulence are determined. Analytic expressions for the\nzonal and non-zonal large scale coherent flows that emerge as a result of\nstructural instability are obtained. Through numerical integrations of the S3T\ndynamical system, it is found that the unstable structures equilibrate at\nfinite amplitude. Numerical simulations of the nonlinear equations confirm the\ncharacteristics (scale, amplitude and phase speed) of the structures predicted\nby S3T. \n\n"}
{"id": "1304.0462", "contents": "Title: Gravitational wave parameter estimation with compressed likelihood\n  evaluations Abstract: One of the main bottlenecks in gravitational wave (GW) astronomy is the high\ncost of performing parameter estimation and GW searches on the fly. We propose\na novel technique based on Reduced Order Quadratures (ROQs), an application and\ndata-specific quadrature rule, to perform fast and accurate likelihood\nevaluations. These are the dominant cost in Markov chain Monte Carlo (MCMC)\nalgorithms, which are widely employed in parameter estimation studies, and so\nROQs offer a new way to accelerate GW parameter estimation. We illustrate our\napproach using a four dimensional GW burst model embedded in noise. We build an\nROQ for this model, and perform four dimensional MCMC searches with both the\nstandard and ROQs quadrature rules, showing that, for this model, the ROQ\napproach is around 25 times faster than the standard approach with essentially\nno loss of accuracy. The speed-up from using ROQs is expected to increase for\nmore complex GW signal models and therefore has significant potential to\naccelerate parameter estimation of GW sources such as compact binary\ncoalescences. \n\n"}
{"id": "1304.0697", "contents": "Title: Neutrino Detection, Position Calibration and Marine Science with\n  Acoustic Arrays in the Deep Sea Abstract: Arrays of acoustic receivers are an integral part of present and potential\nfuture Cherenkov neutrino telescopes in the deep sea. They measure the\npositions of individual detector elements which vary with time as an effect of\nundersea currents. At the same time, the acoustic receivers can be employed for\nmarine science purposes, in particular for monitoring the ambient noise\nenvironment and the signals emitted by the fauna of the sea. And last but not\nleast, they can be used for studies towards acoustic detection of\nultra-high-energy neutrinos. Measuring acoustic pressure pulses in huge\nunderwater acoustic arrays with an instrumented volume of the order of 100 km^3\nis a promising approach for the detection of cosmic neutrinos with energies\nexceeding 1 EeV. Pressure signals are produced by the particle cascades that\nevolve when neutrinos interact with nuclei in water, and can be detected over\nlarge distances in the kilometre range. In this article, the status of acoustic\ndetection will be reviewed and plans for the future - most notably in the\ncontext of KM3NeT - will be discussed. The connection between neutrino\ndetection, position calibration and marine science will be illustrated. \n\n"}
{"id": "1304.1354", "contents": "Title: Wave Extremes in the North East Atlantic from Ensemble Forecasts Abstract: A method for estimating return values from ensembles of forecasts at advanced\nlead times is presented. Return values of significant wave height in the\nNorth-East Atlantic, the Norwegian Sea and the North Sea are computed from\narchived +240-h forecasts of the ECMWF ensemble prediction system (EPS) from\n1999 to 2009.\n  We make three assumptions: First, each forecast is representative of a\nsix-hour interval and collectively the data set is then comparable to a time\nperiod of 226 years. Second, the model climate matches the observed\ndistribution, which we confirm by comparing with buoy data. Third, the ensemble\nmembers are sufficiently uncorrelated to be considered independent realizations\nof the model climate. We find anomaly correlations of 0.20, but peak events\n(>P97) are entirely uncorrelated. By comparing return values from individual\nmembers with return values of subsamples of the data set we also find that the\nestimates follow the same distribution and appear unaffected by correlations in\nthe ensemble. The annual mean and variance over the 11-year archived period\nexhibit no significant departures from stationarity compared with a recent\nreforecast, i.e., there is no spurious trend due to model upgrades.\n  EPS yields significantly higher return values than ERA-40 and ERA-Interim and\nis in good agreement with the high-resolution hindcast NORA10, except in the\nlee of unresolved islands where EPS overestimates and in enclosed seas where it\nis biased low. Confidence intervals are half the width of those found for\nERA-Interim due to the magnitude of the data set. \n\n"}
{"id": "1304.1778", "contents": "Title: Sampling from Dirichlet process mixture models with unknown\n  concentration parameter: Mixing issues in large data implementations Abstract: We consider the question of Markov chain Monte Carlo sampling from a general\nstick-breaking Dirichlet process mixture model, with concentration parameter\nalpha. This paper introduces a Gibbs sampling algorithm that combines the slice\nsampling approach of Walker (2007) and the retrospective sampling approach of\nPapaspiliopoulos and Roberts (2008). Our general algorithm is implemented as\nefficient open source C++ software, available as an R package, and is based on\na blocking strategy similar to that suggested by Papaspiliopoulos (2008) and\nimplemented by Yau et al (2011).\n  We discuss the difficulties of achieving good mixing in MCMC samplers of this\nnature and investigate sensitivity to initialisation. We additionally consider\nthe challenges when an additional layer of hierarchy is added such that joint\ninference is to be made on alpha. We introduce a new label switching move and\ncompute the marginal model posterior to help to surmount these difficulties.\nOur work is illustrated using a profile regression (Molitor et al, 2010)\napplication, where we demonstrate good mixing behaviour for both synthetic and\nreal examples. \n\n"}
{"id": "1304.2048", "contents": "Title: Bayesian Computational Tools Abstract: This chapter surveys advances in the field of Bayesian computation over the\npast twenty years, with missing data. It also contains some novel computational\nentries on the double-exponential model that may be of interest per se. \n\n"}
{"id": "1304.4406", "contents": "Title: Cities as nuclei of sustainability? Abstract: We have assembled CO2 emission figures from collections of urban GHG emission\nestimates published in peer reviewed journals or reports from research\ninstitutes and non-governmental organizations. Analyzing the scaling with\npopulation size we find that the exponent is development dependent with a\ntransition from super- to sub-linear scaling. From the climate change\nmitigation point of view, the results suggest that urbanization is desirable in\ndeveloped countries and should be avoided in developing ones. Further, we\ncompare this analysis with a second scaling relation, namely the fundamental\nallometry between city population and area, and propose that density might be\nthe decisive quantity. Last, we derive the theoretical country-wide urban\nemissions by integration and obtain a dependence on the size of the largest\ncity. \n\n"}
{"id": "1305.0759", "contents": "Title: GPfit: An R package for Gaussian Process Model Fitting using a New\n  Optimization Algorithm Abstract: Gaussian process (GP) models are commonly used statistical metamodels for\nemulating expensive computer simulators. Fitting a GP model can be numerically\nunstable if any pair of design points in the input space are close together.\nRanjan, Haynes, and Karsten (2011) proposed a computationally stable approach\nfor fitting GP models to deterministic computer simulators. They used a genetic\nalgorithm based approach that is robust but computationally intensive for\nmaximizing the likelihood. This paper implements a slightly modified version of\nthe model proposed by Ranjan et al. (2011), as the new R package GPfit. A novel\nparameterization of the spatial correlation function and a new multi-start\ngradient based optimization algorithm yield optimization that is robust and\ntypically faster than the genetic algorithm based approach. We present two\nexamples with R codes to illustrate the usage of the main functions in GPfit.\nSeveral test functions are used for performance comparison with a popular R\npackage mlegp. GPfit is a free software and distributed under the general\npublic license, as part of the R software project (R Development Core Team\n2012). \n\n"}
{"id": "1305.5879", "contents": "Title: Statistical Significance of Clustering using Soft Thresholding Abstract: Clustering methods have led to a number of important discoveries in\nbioinformatics and beyond. A major challenge in their use is determining which\nclusters represent important underlying structure, as opposed to spurious\nsampling artifacts. This challenge is especially serious, and very few methods\nare available, when the data are very high in dimension. Statistical\nSignificance of Clustering (SigClust) is a recently developed cluster\nevaluation tool for high dimensional low sample size data. An important\ncomponent of the SigClust approach is the very definition of a single cluster\nas a subset of data sampled from a multivariate Gaussian distribution. The\nimplementation of SigClust requires the estimation of the eigenvalues of the\ncovariance matrix for the null multivariate Gaussian distribution. We show that\nthe original eigenvalue estimation can lead to a test that suffers from severe\ninflation of type-I error, in the important case where there are a few very\nlarge eigenvalues. This paper addresses this critical challenge using a novel\nlikelihood based soft thresholding approach to estimate these eigenvalues,\nwhich leads to a much improved SigClust. Major improvements in SigClust\nperformance are shown by both mathematical analysis, based on the new notion of\nTheoretical Cluster Index, and extensive simulation studies. Applications to\nsome cancer genomic data further demonstrate the usefulness of these\nimprovements. \n\n"}
{"id": "1306.0040", "contents": "Title: Expectation-maximization for logistic regression Abstract: We present a family of expectation-maximization (EM) algorithms for binary\nand negative-binomial logistic regression, drawing a sharp connection with the\nvariational-Bayes algorithm of Jaakkola and Jordan (2000). Indeed, our results\nallow a version of this variational-Bayes approach to be re-interpreted as a\ntrue EM algorithm. We study several interesting features of the algorithm, and\nof this previously unrecognized connection with variational Bayes. We also\ngeneralize the approach to sparsity-promoting priors, and to an online method\nwhose convergence properties are easily established. This latter method\ncompares favorably with stochastic-gradient descent in situations with marked\ncollinearity. \n\n"}
{"id": "1306.0063", "contents": "Title: Wormhole Hamiltonian Monte Carlo Abstract: In machine learning and statistics, probabilistic inference involving\nmultimodal distributions is quite difficult. This is especially true in high\ndimensional problems, where most existing algorithms cannot easily move from\none mode to another. To address this issue, we propose a novel Bayesian\ninference approach based on Markov Chain Monte Carlo. Our method can\neffectively sample from multimodal distributions, especially when the dimension\nis high and the modes are isolated. To this end, it exploits and modifies the\nRiemannian geometric properties of the target distribution to create\n\\emph{wormholes} connecting modes in order to facilitate moving between them.\nFurther, our proposed method uses the regeneration technique in order to adapt\nthe algorithm by identifying new modes and updating the network of wormholes\nwithout affecting the stationary distribution. To find new modes, as opposed to\nrediscovering those previously identified, we employ a novel mode searching\nalgorithm that explores a \\emph{residual energy} function obtained by\nsubtracting an approximate Gaussian mixture density (based on previously\ndiscovered modes) from the target density function. \n\n"}
{"id": "1306.2144", "contents": "Title: Importance Nested Sampling and the MultiNest Algorithm Abstract: Bayesian inference involves two main computational challenges. First, in\nestimating the parameters of some model for the data, the posterior\ndistribution may well be highly multi-modal: a regime in which the convergence\nto stationarity of traditional Markov Chain Monte Carlo (MCMC) techniques\nbecomes incredibly slow. Second, in selecting between a set of competing models\nthe necessary estimation of the Bayesian evidence for each is, by definition, a\n(possibly high-dimensional) integration over the entire parameter space; again\nthis can be a daunting computational task, although new Monte Carlo (MC)\nintegration algorithms offer solutions of ever increasing efficiency. Nested\nsampling (NS) is one such contemporary MC strategy targeted at calculation of\nthe Bayesian evidence, but which also enables posterior inference as a\nby-product, thereby allowing simultaneous parameter estimation and model\nselection. The widely-used MultiNest algorithm presents a particularly\nefficient implementation of the NS technique for multi-modal posteriors. In\nthis paper we discuss importance nested sampling (INS), an alternative\nsummation of the MultiNest draws, which can calculate the Bayesian evidence at\nup to an order of magnitude higher accuracy than `vanilla' NS with no change in\nthe way MultiNest explores the parameter space. This is accomplished by\ntreating as a (pseudo-)importance sample the totality of points collected by\nMultiNest, including those previously discarded under the constrained\nlikelihood sampling of the NS algorithm. We apply this technique to several\nchallenging test problems and compare the accuracy of Bayesian evidences\nobtained with INS against those from vanilla NS. \n\n"}
{"id": "1306.3494", "contents": "Title: Randomized maximum-contrast selection: subagging for large-scale\n  regression Abstract: We introduce a very general method for sparse and large-scale variable\nselection. The large-scale regression settings is such that both the number of\nparameters and the number of samples are extremely large. The proposed method\nis based on careful combination of penalized estimators, each applied to a\nrandom projection of the sample space into a low-dimensional space. In one\nspecial case that we study in detail, the random projections are divided into\nnon-overlapping blocks; each consisting of only a small portion of the original\ndata. Within each block we select the projection yielding the smallest\nout-of-sample error. Our random ensemble estimator then aggregates the results\naccording to new maximal-contrast voting scheme to determine the final selected\nset. Our theoretical results illuminate the effect on performance of increasing\nthe number of non-overlapping blocks. Moreover, we demonstrate that statistical\noptimality is retained along with the computational speedup. The proposed\nmethod achieves minimax rates for approximate recovery over all estimators\nusing the full set of samples. Furthermore, our theoretical results allow the\nnumber of subsamples to grow with the subsample size and do not require\nirrepresentable condition. The estimator is also compared empirically with\nseveral other popular high-dimensional estimators via an extensive simulation\nstudy, which reveals its excellent finite-sample performance. \n\n"}
{"id": "1306.5289", "contents": "Title: Analytic Solutions for D-optimal Factorial Designs under Generalized\n  Linear Models Abstract: We develop two analytic approaches to solve D-optimal approximate designs\nunder generalized linear models. The first approach provides analytic D-optimal\nallocations for generalized linear models with two factors, which include as a\nspecial case the $2^2$ main-effects model considered by Yang, Mandal and\nMajumdar (2012). The second approach leads to explicit solutions for a class of\ngeneralized linear models with more than two factors. With the aid of the\nanalytic solutions, we provide a necessary and sufficient condition under which\na D-optimal design with two quantitative factors could be constructed on the\nboundary points only. It bridges the gap between D-optimal factorial designs\nand D-optimal designs with continuous factors. \n\n"}
{"id": "1306.5824", "contents": "Title: Constrained Optimization for a Subset of the Gaussian Parsimonious\n  Clustering Models Abstract: The expectation-maximization (EM) algorithm is an iterative method for\nfinding maximum likelihood estimates when data are incomplete or are treated as\nbeing incomplete. The EM algorithm and its variants are commonly used for\nparameter estimation in applications of mixture models for clustering and\nclassification. This despite the fact that even the Gaussian mixture model\nlikelihood surface contains many local maxima and is singularity riddled.\nPrevious work has focused on circumventing this problem by constraining the\nsmallest eigenvalue of the component covariance matrices. In this paper, we\nconsider constraining the smallest eigenvalue, the largest eigenvalue, and both\nthe smallest and largest within the family setting. Specifically, a subset of\nthe GPCM family is considered for model-based clustering, where we use a\nre-parameterized version of the famous eigenvalue decomposition of the\ncomponent covariance matrices. Our approach is illustrated using various\nexperiments with simulated and real data. \n\n"}
{"id": "1307.0240", "contents": "Title: Continuous Forest Fire Propagation in a Local Small World Network Model Abstract: This paper presents the development of a new continuous forest fire model\nimplemented as a weighted local small-world network approach. This new approach\nwas designed to simulate fire patterns in real, heterogeneous landscapes. The\nwildland fire spread is simulated on a square lattice in which each cell\nrepresents an area of the land's surface. The interaction between burning and\nnon-burning cells, in the present work induced by flame radiation, may be\nextended well beyond nearest neighbors. It depends on local conditions of\ntopography and vegetation types. An approach based on a solid flame model is\nused to predict the radiative heat flux from the flame generated by the burning\nof each site towards its neighbors. The weighting procedure takes into account\nthe self-degradation of the tree and the ignition processes of a combustible\ncell through time. The model is tested on a field presenting a range of slopes\nand with data collected from a real wildfire scenario. The critical behavior of\nthe spreading process is investigated. \n\n"}
{"id": "1307.0515", "contents": "Title: Stabilizing Cloud Feedback Dramatically Expands the Habitable Zone of\n  Tidally Locked Planets Abstract: The habitable zone (HZ) is the circumstellar region where a planet can\nsustain surface liquid water. Searching for terrestrial planets in the HZ of\nnearby stars is the stated goal of ongoing and planned extrasolar planet\nsurveys. Previous estimates of the inner edge of the HZ were based on\none-dimensional radiative-convective models. The most serious limitation of\nthese models is the inability to predict cloud behavior. Here we use global\nclimate models with sophisticated cloud schemes to show that due to a\nstabilizing cloud feedback, tidally locked planets can be habitable at twice\nthe stellar flux found by previous studies. This dramatically expands the HZ\nand roughly doubles the frequency of habitable planets orbiting red dwarf\nstars. At high stellar flux, strong convection produces thick water clouds near\nthe substellar location that greatly increase the planetary albedo and reduce\nsurface temperatures. Higher insolation produces stronger substellar convection\nand therefore higher albedo, making this phenomenon a stabilizing climate\nfeedback. Substellar clouds also effectively block outgoing radiation from the\nsurface, reducing or even completely reversing the thermal emission contrast\nbetween dayside and nightside. The presence of substellar water clouds and the\nresulting clement surface conditions will therefore be detectable with the\nJames Webb Space Telescope. \n\n"}
{"id": "1307.2741", "contents": "Title: On the choice of ingredients for a theory of the Ice Ages Abstract: \"With five parameters one can fit an elephant\". This provocative statement\nexpresses the fact that when a theory has several adjustable parameters, an\nagreement with empirical data can be of modest value. What about a theory which\ncontains unobserved objects? This is the subject of this paper. It is motivated\nby a model of the Ice Ages of the Pleistocene, which postulates a hot planet in\nan extremely eccentric orbit. This object has many consequences. It is rather\nwell defined by the requirements, that it must not be in conflict with laws of\nnature, nor with empirical data. It must have sufficient mass to produce a\nrapid geographic pole shift on Earth after a close flyby at the end of the\nPleistocene, and also be small enough to disintegrate at this occasion and to\nevaporate during the Holocene. These requirements leave hardly any adaptable\nparameters. In this situation, the agreement with further data, in particular\nthe reverse Dansgaard-Oeschger events of the Holocene, represents a significant\nsupport of this theory. \n\n"}
{"id": "1307.3719", "contents": "Title: Comparison of asymptotic variances of inhomogeneous Markov chains with\n  application to Markov chain Monte Carlo methods Abstract: In this paper, we study the asymptotic variance of sample path averages for\ninhomogeneous Markov chains that evolve alternatingly according to two\ndifferent $\\pi$-reversible Markov transition kernels $P$ and $Q$. More\nspecifically, our main result allows us to compare directly the asymptotic\nvariances of two inhomogeneous Markov chains associated with different kernels\n$P_i$ and $Q_i$, $i\\in\\{0,1\\}$, as soon as the kernels of each pair $(P_0,P_1)$\nand $(Q_0,Q_1)$ can be ordered in the sense of lag-one autocovariance. As an\nimportant application, we use this result for comparing different\ndata-augmentation-type Metropolis-Hastings algorithms. In particular, we\ncompare some pseudo-marginal algorithms and propose a novel exact algorithm,\nreferred to as the random refreshment algorithm, which is more efficient, in\nterms of asymptotic variance, than the Grouped Independence Metropolis-Hastings\nalgorithm and has a computational complexity that does not exceed that of the\nMonte Carlo Within Metropolis algorithm. \n\n"}
{"id": "1307.5580", "contents": "Title: The brightness and spatial distributions of terrestrial radio sources Abstract: Faint undetected sources of radio-frequency interference (RFI) might become\nvisible in long radio observations when they are consistently present over\ntime. Thereby, they might obstruct the detection of the weak astronomical\nsignals of interest. This issue is especially important for Epoch of\nReionisation (EoR) projects that try to detect the faint redshifted HI signals\nfrom the time of the earliest structures in the Universe. We explore the RFI\nsituation at 30-163 MHz by studying brightness histograms of visibility data\nobserved with LOFAR, similar to radio-source-count analyses that are used in\ncosmology. An empirical RFI distribution model is derived that allows the\nsimulation of RFI in radio observations. The brightness histograms show an RFI\ndistribution that follows a power-law distribution with an estimated exponent\naround -1.5. With several assumptions, this can be explained with a uniform\ndistribution of terrestrial radio sources whose radiation follows existing\npropagation models. Extrapolation of the power law implies that the current\nLOFAR EoR observations should be severely RFI limited if the strength of RFI\nsources remains strong after time integration. This is in contrast with actual\nobservations, which almost reach the thermal noise and are thought not to be\nlimited by RFI. Therefore, we conclude that it is unlikely that there are\nundetected RFI sources that will become visible in long observations.\nConsequently, there is no indication that RFI will prevent an EoR detection\nwith LOFAR. \n\n"}
{"id": "1307.5626", "contents": "Title: SSM: Inference for time series analysis with State Space Models Abstract: The main motivation behind the open source library SSM is to reduce the\ntechnical friction that prevents modellers from sharing their work, quickly\niterating in crisis situations, and making their work directly usable by public\nauthorities to serve decision-making. \n\n"}
{"id": "1307.6991", "contents": "Title: Statistical significance of rising and oscillatory trends in global\n  ocean and land temperature in the past 160 years Abstract: Various interpretations of the notion of a trend in the context of global\nwarming are discussed, contrasting the difference between viewing a trend as\nthe deterministic response to an external forcing and viewing it as a slow\nvariation which can be separated from the background spectral continuum of\nlong-range persistent climate noise. The emphasis in this paper is on the\nlatter notion, and a general scheme is presented for testing a multi-parameter\ntrend model against a null hypothesis which models the observed climate record\nas an autocorrelated noise. The scheme is employed to the instrumental global\nsea-surface temperature record and the global land-temperature record. A trend\nmodel comprising a linear plus an oscillatory trend with period of\napproximately 60 yr, and the statistical significance of the trends, are tested\nagainst three different null models: first-order autoregressive process,\nfractional Gaussian noise, and fractional Brownian motion. The linear trend is\nsignificant in all cases, but the oscillatory trend is insignificant for ocean\ndata and barely significant for land data. By means of a Bayesian iteration,\nhowever, using the significance of the linear trend to formulate a sharper null\nhypothesis, the oscillatory trend in the land record appears to be\nstatistically significant. The results suggest that the global land record may\nbe better suited for detection of the global warming signal than the ocean\nrecord. \n\n"}
{"id": "1308.0774", "contents": "Title: Efficient Data Augmentation in Dynamic Models for Binary and Count Data Abstract: Dynamic linear models with Gaussian observations and Gaussian states lead to\nclosed-form formulas for posterior simulation. However, these closed-form\nformulas break down when the response or state evolution ceases to be Gaussian.\nDynamic, generalized linear models exemplify a class of models for which this\nis the case, and include, amongst other models, dynamic binomial logistic\nregression and dynamic negative binomial regression. Finding and appraising\nposterior simulation techniques for these models is important since modeling\ntemporally correlated categories or counts is useful in a variety of\ndisciplines, including ecology, economics, epidemiology, medicine, and\nneuroscience. In this paper, we present one such technique, P\\'olya-Gamma data\naugmentation, and compare it against two competing methods. We find that the\nP\\'olya-Gamma approach works well for dynamic logistic regression and for\ndynamic negative binomial regression when the count sizes are small.\nSupplementary files are provided for replicating the benchmarks. \n\n"}
{"id": "1308.2218", "contents": "Title: Coding for Random Projections Abstract: The method of random projections has become very popular for large-scale\napplications in statistical learning, information retrieval, bio-informatics\nand other applications. Using a well-designed coding scheme for the projected\ndata, which determines the number of bits needed for each projected value and\nhow to allocate these bits, can significantly improve the effectiveness of the\nalgorithm, in storage cost as well as computational speed. In this paper, we\nstudy a number of simple coding schemes, focusing on the task of similarity\nestimation and on an application to training linear classifiers. We demonstrate\nthat uniform quantization outperforms the standard existing influential method\n(Datar et. al. 2004). Indeed, we argue that in many cases coding with just a\nsmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bit\ncoding scheme that generally performs well in practice, as confirmed by our\nexperiments on training linear support vector machines (SVM). \n\n"}
{"id": "1308.3922", "contents": "Title: Lagrangian study of surface transport in the Kuroshio Extension area\n  based on simulation of propagation of Fukushima-derived radionuclides Abstract: Lagrangian approach is applied to study near-surface large-scale transport in\nthe Kuroshio Extension area using a simulation with synthetic particles\nadvected by AVISO altimetric velocity field. A material line technique is\napplied to find the origin of water masses in cold-core cyclonic rings pinched\noff from the jet in summer 2011. Tracking and Lagrangian maps provide the\nevidence of cross-jet transport. Fukushima derived caesium isotopes are used as\nLagrangian tracers to study transport and mixing in the area a few months after\nthe March of 2011 tsunami that caused a heavy damage of the Fukushima nuclear\npower plant (FNPP). Tracking maps are computed to trace the origin of water\nparcels with measured levels of Cs-134 and Cs-137 concentrations collected in\ntwo R/V cruises in June and July 2011 in the large area of the Northwest\nPacific. It is shown that Lagrangian simulation is useful to finding the\nsurface areas that are potentially dangerous due to the risk of radioactive\ncontamination. The results of simulation are supported by tracks of the surface\ndrifters which were deployed in the area. \n\n"}
{"id": "1309.1860", "contents": "Title: Axisymmetrically Tropical Cyclone-like Vortices with Secondary\n  Circulations Abstract: The secondary circulation of the tropical cyclone (TC) is related to its\nformation and intensification, thus becomes very important in the studies. The\nanalytical solutions have both the primary and secondary circulation in a\nthree-dimensionally nonhydrostatic and adiabatic model. We prove that there are\nthree intrinsic radiuses for the axisymmetrically ideal incompressible flow.\nThe first one is the radius of maximum primary circular velocity $r_m$. The\nsecond one is radius of the primary kernel $r_k>r_m$, across which the\nvorticity of the primary circulation changes sign and the vertical velocity\nchanges direction. The last one is the radius of the maximum primary vorticity\n$r_d$, at which the vertical flow of the secondary circulation approaches its\nmaximum, and across which the radius velocity changes sign. The first TC-like\nvortex solution has universal inflow or outflow. The relations between the\nintrinsic length scales are $r_k=\\sqrt{2}r_m$ and $r_d=2r_m$. The second one is\na multi-planar solution, periodically in $z$-coordinate. Within each layer, the\nsolution is a convection vortex. The number of the secondary circulation might\nbe one, two, three, and even more. There are also three intrinsic radiuses\n$r_m$, $r_k$ and $r_d$, but they have different values. It seems that the\nrelative stronger radius velocity could be easily found near boundaries. The\nabove solutions can be applied to study the radial structure of the tornados,\nTCs and mesoscale eddies. \n\n"}
{"id": "1309.1901", "contents": "Title: Variational Bayes Approximations for Clustering via Mixtures of Normal\n  Inverse Gaussian Distributions Abstract: Parameter estimation for model-based clustering using a finite mixture of\nnormal inverse Gaussian (NIG) distributions is achieved through variational\nBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are\nconsidered. The use of variational Bayes approximations here is a substantial\ndeparture from the traditional EM approach and alleviates some of the\nassociated computational complexities and uncertainties. Our variational\nalgorithm is applied to simulated and real data. The paper concludes with\ndiscussion and suggestions for future work. \n\n"}
{"id": "1309.2918", "contents": "Title: On the role of interaction in sequential Monte Carlo algorithms Abstract: We introduce a general form of sequential Monte Carlo algorithm defined in\nterms of a parameterized resampling mechanism. We find that a suitably\ngeneralized notion of the Effective Sample Size (ESS), widely used to monitor\nalgorithm degeneracy, appears naturally in a study of its convergence\nproperties. We are then able to phrase sufficient conditions for time-uniform\nconvergence in terms of algorithmic control of the ESS, in turn achievable by\nadaptively modulating the interaction between particles. This leads us to\nsuggest novel algorithms which are, in senses to be made precise, provably\nstable and yet designed to avoid the degree of interaction which hinders\nparallelization of standard algorithms. As a byproduct, we prove time-uniform\nconvergence of the popular adaptive resampling particle filter. \n\n"}
{"id": "1309.4332", "contents": "Title: Observations on the flow structures and transport in a warm-core ring in\n  the Gulf of Mexico Abstract: This study presents several new observations from the study of a warm-core\nring (WCR) in the Gulf of Mexico based on the ECCO2 global ocean simulation.\nUsing Lagrangian coherent structures (LCS) techniques to investigate this flow\nreveals a pattern of transversely intersecting LCS in the mixed layer of the\nWCR which experiences consistent stretching behavior over a large region of\nspace and time. A detailed analysis of this flow region leads to an analytical\nmodel velocity field which captures the essential elements that generate the\ntransversely intersecting LCS. The model parameters are determined from the WCR\nand the resulting LCS show excellent agreement with those observed in the WCR.\nThe three-dimensional transport behavior which creates these structures relies\non the small radial outflow which is present in the mixed layer and is not seen\nbelow the pycnocline, leading to a sharp change in the character of the LCS at\nthe bottom of the mixed layer. The flow behavior revealed by the LCS limits\nfluid exchange between the WCR and the surrounding ocean, contributing to the\nlong life of WCRs. Further study of these structures and their associated\ntransport behavior may lead to further insights into the development and\npersistence of such geophysical vortices as well as their transport behavior. \n\n"}
{"id": "1309.5808", "contents": "Title: Efficient goodness-of-fit tests in multi-dimensional vine copula models Abstract: We introduce a new goodness-of-fit test for regular vine (R-vine) copula\nmodels, a flexible class of multivariate copulas based on a pair-copula\nconstruction (PCC). The test arises from the information matrix ratio. The\ncorresponding test statistic is derived and its asymptotic normality is proven.\nThe test's power is investigated and compared to 14 other goodness-of-fit\ntests, adapted from the bivariate copula case, in a high dimensional setting.\nThe extensive simulation study shows the excellent performance with respect to\nsize and power as well as the superiority of the information matrix ratio based\ntest against most other goodness-of-fit tests. The best performing tests are\napplied to a portfolio of stock indices and their related volatility indices\nvalidating different R-vine specifications. \n\n"}
{"id": "1309.7098", "contents": "Title: An Explicit Formulation of the Earth Movers Distance with Continuous\n  Road Map Distances Abstract: The Earth movers distance (EMD) is a measure of distance between probability\ndistributions which is at the heart of mass transportation theory. Recent\nresearch has shown that the EMD plays a crucial role in studying the potential\nimpact of Demand-Responsive Transportation (DRT) and Mobility-on-Demand (MoD)\nsystems, which are growing paradigms for one-way vehicle sharing where people\ndrive (or are driven by) shared vehicles from a point of origin to a point of\ndestination. While the ubiquitous physical transportation setting is the road\nnetwork, characterized by systems of roads connected together by interchanges,\nmost analytical works about vehicle sharing represent distances between points\nin a plane using the simple Euclidean metric. Instead, we consider the EMD when\nthe ground metric is taken from a class of one-dimensional, continuous metric\nspaces, reminiscent of road networks. We produce an explicit formulation of the\nEarth movers distance given any finite road network R. The result generalizes\nthe EMD with a Euclidean R1 ground metric, which had remained one of the only\nknown non-discrete cases with an explicit formula. Our formulation casts the\nEMD as the optimal value of a finite-dimensional, real-valued optimization\nproblem, with a convex objective function and linear constraints. In the\nspecial case that the input distributions have piece-wise uniform (constant)\ndensity, the problem reduces to one whose objective function is convex\nquadratic. Both forms are amenable to modern mathematical programming\ntechniques. \n\n"}
{"id": "1309.7209", "contents": "Title: On the efficiency of pseudo-marginal random walk Metropolis algorithms Abstract: We examine the behaviour of the pseudo-marginal random walk Metropolis\nalgorithm, where evaluations of the target density for the accept/reject\nprobability are estimated rather than computed precisely. Under relatively\ngeneral conditions on the target distribution, we obtain limiting formulae for\nthe acceptance rate and for the expected squared jump distance, as the\ndimension of the target approaches infinity, under the assumption that the\nnoise in the estimate of the log-target is additive and is independent of the\nposition. For targets with independent and identically distributed components,\nwe also obtain a limiting diffusion for the first component. We then consider\nthe overall efficiency of the algorithm, in terms of both speed of mixing and\ncomputational time. Assuming the additive noise is Gaussian and is inversely\nproportional to the number of unbiased estimates that are used, we prove that\nthe algorithm is optimally efficient when the variance of the noise is\napproximately 3.283 and the acceptance rate is approximately 7.001%. We also\nfind that the optimal scaling is insensitive to the noise and that the optimal\nvariance of the noise is insensitive to the scaling. The theory is illustrated\nwith a simulation study using the particle marginal random walk Metropolis. \n\n"}
{"id": "1309.7626", "contents": "Title: The dynamics of technology diffusion and the impacts of climate policy\n  instruments in the decarbonisation of the global electricity sector Abstract: This paper presents an analysis of climate policy instruments for the\ndecarbonisation of the global electricity sector in a non-equilibrium economic\nand technology diffusion perspective. Energy markets are driven by innovation,\npath-dependent technology choices and diffusion. However, conventional\noptimisation models lack detail on these aspects and have limited ability to\naddress the effectiveness of policy interventions because they do not represent\ndecision-making. As a result, known effects of technology lock-ins are liable\nto be underestimated. In contrast, our approach places investor decision-making\nat the core of the analysis and investigates how it drives the diffusion of\nlow-carbon technology in a highly disaggregated, hybrid, global\nmacroeconometric model, FTT:Power-E3MG. Ten scenarios to 2050 of the\nelectricity sector in 21 regions exploring combinations of electricity policy\ninstruments are analysed, including their climate impacts. We show that in a\ndiffusion and path-dependent perspective, the impact of combinations of\npolicies does not correspond to the sum of impacts of individual instruments:\nsynergies exist between policy tools. We argue that the carbon price required\nto break the current fossil technology lock-in can be much lower when combined\nwith other policies, and that a 90% decarbonisation of the electricity sector\nby 2050 is affordable without early scrapping. \n\n"}
{"id": "1310.0511", "contents": "Title: Does the problem of global warming exist at all? Insight from the\n  temperature drift induced by inevitable colored noise Abstract: In the present paper we state a problem of the colored noise nonremovability\non the climatic 30-year time scale, which essentially changes the angle of view\non the known problem of global warming. \n\n"}
{"id": "1310.0943", "contents": "Title: Wind and Wave Extremes over the World Oceans From Very Large Forecast\n  Ensembles Abstract: Global return value estimates of significant wave height and 10-m neutral\nwind speed are estimated from very large aggregations of archived ECMWF\nensemble forecasts at +240-h lead time from the period 2003-2012. The upper\npercentiles are found to match ENVISAT wind speed better than ERA-Interim\n(ERA-I), which tends to be biased low. The return estimates are significantly\nhigher for both wind speed and wave height in the extratropics and the\nsubtropics than what is found from ERA-I, but lower than what is reported by\nCaires and Sterl (2005) and Vinoth and Young (2011). The highest discrepancies\nbetween ERA-I and ENS240 are found in the hurricane-prone areas, suggesting\nthat the ensemble comes closer than ERA-I in capturing the intensity of\ntropical cyclones. The width of the confidence intervals are typically reduced\nby 70% due to the size of the data sets. Finally, non-parametric estimates of\nreturn values were computed from the tail of the distribution. These direct\nreturn estimates compare very well with Generalized Pareto estimates. \n\n"}
{"id": "1310.1297", "contents": "Title: Spectral Clustering for Divide-and-Conquer Graph Matching Abstract: We present a parallelized bijective graph matching algorithm that leverages\nseeds and is designed to match very large graphs. Our algorithm combines\nspectral graph embedding with existing state-of-the-art seeded graph matching\nprocedures. We justify our approach by proving that modestly correlated, large\nstochastic block model random graphs are correctly matched utilizing very few\nseeds through our divide-and-conquer procedure. We also demonstrate the\neffectiveness of our approach in matching very large graphs in simulated and\nreal data examples, showing up to a factor of 8 improvement in runtime with\nminimal sacrifice in accuracy. \n\n"}
{"id": "1310.1702", "contents": "Title: Monte Carlo simulation of light scattering in the atmosphere and effect\n  of atmospheric aerosols on the point spread function Abstract: We present a Monte Carlo simulation for the scattering of light in the case\nof an isotropic light source. The scattering phase functions are studied\nparticularly in detail to understand how they can affect the multiple light\nscattering in the atmosphere. We show that although aerosols are usually in\nlower density than molecules in the atmosphere, they can have a non-negligible\neffect on the atmospheric point spread function. This effect is especially\nexpected for ground-based detectors when large aerosols are present in the\natmosphere. \n\n"}
{"id": "1310.1905", "contents": "Title: libcloudph++ 0.2: single-moment bulk, double-moment bulk, and\n  particle-based warm-rain microphysics library in C++ Abstract: This paper introduces a library of algorithms for representing cloud\nmicrophysics in numerical models. The library is written in C++, hence the name\nlibcloudph++. In the current release, the library covers three warm-rain\nschemes: the single- and double-moment bulk schemes, and the particle-based\nscheme with Monte-Carlo coalescence. The three schemes are intended for\nmodelling frameworks of different dimensionality and complexity ranging from\nparcel models to multi-dimensional cloud-resolving (e.g. large-eddy)\nsimulations. A two-dimensional prescribed-flow framework is used in example\nsimulations presented in the paper with the aim of highlighting the library\nfeatures. The libcloudph++ and all its mandatory dependencies are free and\nopen-source software. The Boost.units library is used for zero-overhead\ndimensional analysis of the code at compile time. The particle-based scheme is\nimplemented using the Thrust library that allows to leverage the power of\ngraphics processing units (GPU), retaining the possibility to compile the\nunchanged code for execution on single or multiple standard processors (CPUs).\nThe paper includes complete description of the programming interface (API) of\nthe library and a performance analysis including comparison of GPU and CPU\nsetups. \n\n"}
{"id": "1310.4403", "contents": "Title: Complexity, economic science and possible economic benefits of climate\n  change mitigation policy Abstract: Conventional economic analysis of stringent climate change mitigation policy\ngenerally concludes various levels of economic slowdown as a result of\nsubstantial spending on low carbon technology. Equilibrium economics however\ncould not explain or predict the current economic crisis, which is of financial\nnature. Meanwhile the economic impacts of climate policy find their source\nthrough investments for the diffusion of environmental innovations, in parts a\nfinancial problem. Here, we expose how results of economic analysis of climate\nchange mitigation policy depend entirely on assumptions and theory concerning\nthe finance of the diffusion of innovations, and that in many cases, results\nare simply re-iterations of model assumptions. We show that, while equilibrium\neconomics always predict economic slowdown, methods using non-equilibrium\napproaches suggest the opposite could occur. We show that the solution to\nunderstanding the economic impacts of reducing greenhouse gas emissions lies\nwith research on the dynamics of the financial sector interacting with\ninnovation and technology developments, economic history providing powerful\ninsights through important analogies with previous historical waves of\ninnovation. \n\n"}
{"id": "1310.7349", "contents": "Title: Wave turbulence in the two-layer ocean model Abstract: This paper looks at the two-layer ocean model from a wave turbulence\nperspective. A symmetric form of the two-layer kinetic equation for Rossby\nwaves is derived using canonical variables, allowing the turbulent cascade of\nenergy between the barotropic and baroclinic modes to be studied. It turns out\nthat energy is transferred via local triad interactions from the large-scale\nbaroclinic modes to the baroclinic and barotropic modes at the Rossby\ndeformation scale. From there it is then transferred to the large-scale\nbarotropic modes via a nonlocal inverse transfer. Using scale separation a sys-\ntem of coupled equations were obtained for the small-scale baroclinic component\nand the large-scale barotropic component. Since the total energy of the\nsmall-scale component is not conserved, but the total barotropic plus\nbaroclinic energy is conserved, the baroclinic energy loss at small scales will\nbe compensated by the growth of the barotropic energy at large scales. It is\nfound that this transfer is mostly anisotropic and mostly to the zonal\ncomponent. \n\n"}
{"id": "1311.0317", "contents": "Title: Parsimonious Shifted Asymmetric Laplace Mixtures Abstract: A family of parsimonious shifted asymmetric Laplace mixture models is\nintroduced. We extend the mixture of factor analyzers model to the shifted\nasymmetric Laplace distribution. Imposing constraints on the constitute parts\nof the resulting decomposed component scale matrices leads to a family of\nparsimonious models. An explicit two-stage parameter estimation procedure is\ndescribed, and the Bayesian information criterion and the integrated completed\nlikelihood are compared for model selection. This novel family of models is\napplied to real data, where it is compared to its Gaussian analogue within\nclustering and classification paradigms. \n\n"}
{"id": "1311.0686", "contents": "Title: Particle Metropolis-Hastings using gradient and Hessian information Abstract: Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference in\nnonlinear state space models by combining Markov chain Monte Carlo (MCMC) and\nparticle filtering. The latter is used to estimate the intractable likelihood.\nIn its original formulation, PMH makes use of a marginal MCMC proposal for the\nparameters, typically a Gaussian random walk. However, this can lead to a poor\nexploration of the parameter space and an inefficient use of the generated\nparticles.\n  We propose a number of alternative versions of PMH that incorporate gradient\nand Hessian information about the posterior into the proposal. This information\nis more or less obtained as a byproduct of the likelihood estimation. Indeed,\nwe show how to estimate the required information using a fixed-lag particle\nsmoother, with a computational cost growing linearly in the number of\nparticles. We conclude that the proposed methods can: (i) decrease the length\nof the burn-in phase, (ii) increase the mixing of the Markov chain at the\nstationary phase, and (iii) make the proposal distribution scale invariant\nwhich simplifies tuning. \n\n"}
{"id": "1311.0907", "contents": "Title: Bayesian nonparametric inference on the Stiefel manifold Abstract: The Stiefel manifold $V_{p,d}$ is the space of all $d \\times p$ orthonormal\nmatrices, with the $d-1$ hypersphere and the space of all orthogonal matrices\nconstituting special cases. In modeling data lying on the Stiefel manifold,\nparametric distributions such as the matrix Langevin distribution are often\nused; however, model misspecification is a concern and it is desirable to have\nnonparametric alternatives. Current nonparametric methods are Fr\\'echet mean\nbased. We take a fully generative nonparametric approach, which relies on\nmixing parametric kernels such as the matrix Langevin. The proposed kernel\nmixtures can approximate a large class of distributions on the Stiefel\nmanifold, and we develop theory showing posterior consistency. While there\nexists work developing general posterior consistency results, extending these\nresults to this particular manifold requires substantial new theory. Posterior\ninference is illustrated on a real-world dataset of near-Earth objects. \n\n"}
{"id": "1311.1882", "contents": "Title: The complex singularity of a Stokes wave Abstract: Two-dimensional potential flow of the ideal incompressible fluid with free\nsurface and infinite depth can be described by a conformal map of the fluid\ndomain into the complex lower half-plane. Stokes wave is the fully nonlinear\ngravity wave propagating with the constant velocity. The increase of the scaled\nwave height $H/\\lambda$ from the linear limit $H/\\lambda=0$ to the critical\nvalue $H_{max}/\\lambda$ marks the transition from the limit of almost linear\nwave to a strongly nonlinear limiting Stokes wave. Here $H$ is the wave height\nand $\\lambda$ is the wavelength. We simulated fully nonlinear Euler equations,\nreformulated in terms of conformal variables, to find Stokes waves for\ndifferent wave heights. Analyzing spectra of these solutions we found in\nconformal variables, at each Stokes wave height, the distance $v_c$ from the\nlowest singularity in the upper half-plane to the real line which corresponds\nto the fluid free surface. We also identified that this singularity is the\nsquare-root branch point. The limiting Stokes wave emerges as the singularity\nreaches the fluid surface. From the analysis of data for $v_c\\to 0$ we suggest\na new power law scaling $v_c\\propto (H_{max}-H)^{3/2}$ as well as new estimate\n$H_{max}/\\lambda \\simeq 0.1410633$. \n\n"}
{"id": "1311.2002", "contents": "Title: Multivariate distributions with fixed marginals and correlations Abstract: Consider the problem of drawing random variates $(X_1,\\ldots,X_n)$ from a\ndistribution where the marginal of each $X_i$ is specified, as well as the\ncorrelation between every pair $X_i$ and $X_j$. For given marginals, the\nFr\\'echet-Hoeffding bounds put a lower and upper bound on the correlation\nbetween $X_i$ and $X_j$. Any achievable correlation between $X_i$ and $X_j$ is\na convex combinations of these bounds. The value $\\lambda(X_i,X_j) \\in [0,1]$\nof this convex combination is called here the convexity parameter of\n$(X_i,X_j),$ with $\\lambda(X_i,X_j) = 1$ corresponding to the upper bound and\nmaximal correlation. For given marginal distributions functions\n$F_1,\\ldots,F_n$ of $(X_1,\\ldots,X_n)$ we show that $\\lambda(X_i,X_j) =\n\\lambda_{ij}$ if and only if there exist symmetric Bernoulli random variables\n$(B_1,\\ldots,B_n)$ (that is $\\{0,1\\}$ random variables with mean 1/2) such that\n$\\lambda(B_i,B_j) = \\lambda_{ij}$. In addition, we characterize completely the\nset of convexity parameters for symmetric Bernoulli marginals in two, three and\nfour dimensions. \n\n"}
{"id": "1311.2441", "contents": "Title: On the comparison between MASS and G-SCIDAR techniques Abstract: The Multi Aperture Scintillation Sensor (MASS) and the\nGeneralized-Scintillation Detection and Ranging (Generalized SCIDAR) are two\ninstruments conceived to measure the optical turbulence (OT) vertical\ndistribution on the whole troposphere and low stratosphere (~ 20 km) widely\nused in the astronomical context. In this paper we perform a detailed\nanalysis/comparison of measurements provided by the two instruments and taken\nduring the extended site testing campaign carried out on 2007 at Cerro Paranal\nand promoted by the European Southern Observatory (ESO). The main and final\ngoal of the study is to provide a detailed estimation of the measurements\nreliability i.e dispersion of turbulence measurements done by the two\ninstruments at different heights above the ground. This information is directly\nrelated to our ability in estimating the absolute value of the turbulence\nstratification. To better analyse the uncertainties between the MASS and the GS\nwe took advantage of the availability of measurements taken during the same\ncampaign by a third independent instrument (DIMM - Differential Imaging Motion\nMonitor) measuring the integrated turbulence extended on the whole 20 km. Such\na cross-check comparison permitted us to define the reliability of the\ninstruments and their measurements, their limits and the contexts in which\ntheir use can present some risk. \n\n"}
{"id": "1311.4117", "contents": "Title: Parameter Estimation in Hidden Markov Models with Intractable\n  Likelihoods Using Sequential Monte Carlo Abstract: We propose sequential Monte Carlo based algorithms for maximum likelihood\nestimation of the static parameters in hidden Markov models with an intractable\nlikelihood using ideas from approximate Bayesian computation. The static\nparameter estimation algorithms are gradient based and cover both offline and\nonline estimation. We demonstrate their performance by estimating the\nparameters of three intractable models, namely the alpha-stable distribution,\ng-and-k distribution, and the stochastic volatility model with alpha-stable\nreturns, using both real and synthetic data. \n\n"}
{"id": "1311.4780", "contents": "Title: Asymptotically Exact, Embarrassingly Parallel MCMC Abstract: Communication costs, resulting from synchronization requirements during\nlearning, can greatly slow down many parallel machine learning algorithms. In\nthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in\nwhich subsets of data are processed independently, with very little\ncommunication. First, we arbitrarily partition data onto multiple machines.\nThen, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be\nused to draw samples from a posterior distribution given the data subset.\nFinally, the samples from each machine are combined to form samples from the\nfull posterior. This embarrassingly parallel algorithm allows each machine to\nact independently on a subset of the data (without communication) until the\nfinal combination stage. We prove that our algorithm generates asymptotically\nexact samples and empirically demonstrate its ability to parallelize burn-in\nand sampling in several models. \n\n"}
{"id": "1311.5777", "contents": "Title: Exact simulation of the sample paths of a diffusion with a finite\n  entrance boundary Abstract: Diffusion processes arise in many fields, and so simulating the path of a\ndiffusion is an important problem. It is usually necessary to make some sort of\napproximation via model-discretization, but a recently introduced class of\nalgorithms, known as the exact algorithm and based on retrospective rejection\nsampling ideas, obviate the need for such discretization. In this paper I\nextend the exact algorithm to apply to a class of diffusions with a finite\nentrance boundary. The key innovation is that for these models the Bessel\nprocess is a more suitable candidate process than the more usually chosen\nBrownian motion. The algorithm is illustrated by an application to a general\ndiffusion model of population growth, where it simulates paths efficiently,\nwhile previous algorithms are impracticable. \n\n"}
{"id": "1311.6064", "contents": "Title: Global Regularity for an Inviscid Three-dimensional Slow Limiting Ocean\n  Dynamics Model Abstract: We establish, for smooth enough initial data, the global well-posedness\n(existence, uniqueness and continuous dependence on initial data) of solutions,\nfor an inviscid three-dimensional {\\it slow limiting ocean dynamics} model.\nThis model was derived as a strong rotation limit of the rotating and\nstratified Boussinesg equations with periodic boundary conditions. To establish\nour results we utilize the tools developed for investigating the\ntwo-dimensional incompressible Euler equations and linear transport equations.\nUsing a weaker formulation of the model we also show the global existence and\nuniqueness of solutions, for less regular initial data. \n\n"}
{"id": "1311.7286", "contents": "Title: Approximate Bayesian Computation with composite score functions Abstract: Both Approximate Bayesian Computation (ABC) and composite likelihood methods\nare useful for Bayesian and frequentist inference, respectively, when the\nlikelihood function is intractable. We propose to use composite likelihood\nscore functions as summary statistics in ABC in order to obtain accurate\napproximations to the posterior distribution. This is motivated by the use of\nthe score function of the full likelihood, and extended to general unbiased\nestimating functions in complex models. Moreover, we show that if the composite\nscore is suitably standardised, the resulting ABC procedure is invariant to\nreparameterisations and automatically adjusts the curvature of the composite\nlikelihood, and of the corresponding posterior distribution. The method is\nillustrated through examples with simulated data, and an application to\nmodelling of spatial extreme rainfall data is discussed. \n\n"}
{"id": "1312.2553", "contents": "Title: Hyperviscosity and statistical equilibria of Euler turbulence on the\n  torus and the sphere Abstract: Coherent structures such as jets and vortices appear in two-dimensional (2D)\nturbulence. To gain insight into both numerical simulation and equilibrium\nstatistical mechanical descriptions of 2D Euler flows, the Euler equation with\nadded hyperviscosity is integrated forward in time on the square torus and on\nthe sphere. Coherent structures that form are compared against a hierarchy of\ntruncated Miller-Robert-Sommeria equilibria. The energy-circulation-enstrophy\nMRS-2 description produces a complete condensation of energy to the largest\nscales, and in the absence of rotation correctly predicts the number and\npolarity of coherent vortices. Perturbative imposition of the quartic Casimir\nconstraint improves agreement with numerical simulation by sharpening the cores\nand transferring some energy to smaller-scale modes. MRS-2 cannot explain\nqualitative changes due to rotation, but descriptions that conserve higher\nCasimirs beyond enstrophy have the potential to do so. The result is in\nagreement with the somewhat paradoxical observation that hyperviscosity helps\nto remedy the non-conservation of the third and higher Casimirs in numerical\nsimulation. For a rotating sphere, numerical simulation also demonstrates that\ncoherent structures found at late times depend on initial conditions, limiting\nthe usefulness of statistical mechanics. \n\n"}
{"id": "1312.5638", "contents": "Title: Exploring Multi-Modal Distributions with Nested Sampling Abstract: In performing a Bayesian analysis, two difficult problems often emerge.\nFirst, in estimating the parameters of some model for the data, the resulting\nposterior distribution may be multi-modal or exhibit pronounced (curving)\ndegeneracies. Secondly, in selecting between a set of competing models,\ncalculation of the Bayesian evidence for each model is computationally\nexpensive using existing methods such as thermodynamic integration. Nested\nSampling is a Monte Carlo method targeted at the efficient calculation of the\nevidence, but also produces posterior inferences as a by-product and therefore\nprovides means to carry out parameter estimation as well as model selection.\nThe main challenge in implementing Nested Sampling is to sample from a\nconstrained probability distribution. One possible solution to this problem is\nprovided by the Galilean Monte Carlo (GMC) algorithm. We show results of\napplying Nested Sampling with GMC to some problems which have proven very\ndifficult for standard Markov Chain Monte Carlo (MCMC) and down-hill methods,\ndue to the presence of large number of local minima and/or pronounced (curving)\ndegeneracies between the parameters. We also discuss the use of Nested Sampling\nwith GMC in Bayesian object detection problems, which are inherently\nmulti-modal and require the evaluation of Bayesian evidence for distinguishing\nbetween true and spurious detections. \n\n"}
{"id": "1401.0265", "contents": "Title: Approximate Bayesian Computation for a Class of Time Series Models Abstract: In the following article we consider approximate Bayesian computation (ABC)\nfor certain classes of time series models. In particular, we focus upon\nscenarios where the likelihoods of the observations and parameter are\nintractable, by which we mean that one cannot evaluate the likelihood even\nup-to a positive unbiased estimate. This paper reviews and develops a class of\napproximation procedures based upon the idea of ABC, but, specifically\nmaintains the probabilistic structure of the original statistical model. This\nidea is useful, in that it can facilitate an analysis of the bias of the\napproximation and the adaptation of established computational methods for\nparameter inference. Several existing results in the literature are surveyed\nand novel developments with regards to computation are given. \n\n"}
{"id": "1401.0604", "contents": "Title: Particle Gibbs with Ancestor Sampling Abstract: Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining\nthe two main tools used for Monte Carlo statistical inference: sequential Monte\nCarlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMC\nalgorithm that we refer to as particle Gibbs with ancestor sampling (PGAS).\nPGAS provides the data analyst with an off-the-shelf class of Markov kernels\nthat can be used to simulate the typically high-dimensional and highly\nautocorrelated state trajectory in a state-space model. The ancestor sampling\nprocedure enables fast mixing of the PGAS kernel even when using seemingly few\nparticles in the underlying SMC sampler. This is important as it can\nsignificantly reduce the computational burden that is typically associated with\nusing SMC. PGAS is conceptually similar to the existing PG with backward\nsimulation (PGBS) procedure. Instead of using separate forward and backward\nsweeps as in PGBS, however, we achieve the same effect in a single forward\nsweep. This makes PGAS well suited for addressing inference problems not only\nin state-space models, but also in models with more complex dependencies, such\nas non-Markovian, Bayesian nonparametric, and general probabilistic graphical\nmodels. \n\n"}
{"id": "1401.0616", "contents": "Title: Compatible finite element methods for numerical weather prediction Abstract: This article takes the form of a tutorial on the use of a particular class of\nmixed finite element methods, which can be thought of as the finite element\nextension of the C-grid staggered finite difference method. The class is often\nreferred to as compatible finite elements, mimetic finite elements, discrete\ndifferential forms or finite element exterior calculus. We provide an\nelementary introduction in the case of the one-dimensional wave equation,\nbefore summarising recent results in applications to the rotating shallow water\nequations on the sphere, before taking an outlook towards applications in\nthree-dimensional compressible dynamical cores. \n\n"}
{"id": "1401.1022", "contents": "Title: On Using Control Variates with Stochastic Approximation for Variational\n  Bayes and its Connection to Stochastic Linear Regression Abstract: Recently, we and several other authors have written about the possibilities\nof using stochastic approximation techniques for fitting variational\napproximations to intractable Bayesian posterior distributions. Naive\nimplementations of stochastic approximation suffer from high variance in this\nsetting. Several authors have therefore suggested using control variates to\nreduce this variance, while we have taken a different but analogous approach to\nreducing the variance which we call stochastic linear regression. In this note\nwe take the former perspective and derive the ideal set of control variates for\nstochastic approximation variational Bayes under a certain set of assumptions.\nWe then show that using these control variates is closely related to using the\nstochastic linear regression approximation technique we proposed earlier. A\nsimple example shows that our method for constructing control variates leads to\nstochastic estimators with much lower variance compared to other approaches. \n\n"}
{"id": "1401.1097", "contents": "Title: Definition of a moist-air entropy potential temperature. Application to\n  FIRE-I data flights Abstract: A moist entropy potential temperature -- denoted by ${\\theta}_{s}$ -- is\ndefined analytically in terms of the specific entropy for moist air. The\nexpression for ${\\theta}_{s}$ is valid for a general mixing of dry air, water\nvapour and possible condensed water species. It verifies the same conservative\nproperties as the moist entropy, even for varying dry air or total water\ncontent. The moist formulation for ${\\theta}_{s}$ is equal to the dry\nformulation $\\theta$ if dry air is considered and it verifies new properties\nvalid for the moist air cases, both saturated or under-saturated ones. Exact\nand approximate versions of ${\\theta}_{s}$ are evaluated for several\nStratocumulus cases, in particular by using the aircraft observations FIRE-I\nexperiment data sets. It appears that there is no (or small) jump in\n${\\theta}_{s}$ at the top of the PBL. The mixing in moist entropy is almost\ncomplete in the PBL, with the same values observed in the clear air and the\ncloudy regions, including the very top of the entrainment region. The\nRandall-Deardorff CTEI analysis may be interpreted as a mixing in moist entropy\ncriterion. The iso-${\\theta}_{s}$ lines are plotted on skew $T$-$\\ln(p)$ and\nconserved variable diagrams. All these properties could suggest some hints on\nthe use of moist entropy (or ${\\theta}_{s}$) in cloud modelling or in mixing\nprocesses, with the marine Stratocumulus considered as a paradigm of moist\nturbulence. \n\n"}
{"id": "1401.1234", "contents": "Title: Global Well-posedness of Strong Solutions to the 3D Primitive Equations\n  with Horizontal Eddy Diffusivity Abstract: In this paper, we consider the initial-boundary value problem of the 3D\nprimitive equations for oceanic and atmospheric dynamics with only horizontal\ndiffusion in the temperature equation. Global well-posedness of strong\nsolutions are established with $H^2$ initial data. \n\n"}
{"id": "1401.2006", "contents": "Title: On the definition of a moist-air potential vorticity Abstract: A new potential vorticity is derived by using a specific entropy formulation\nexpressed in terms of a moist-air entropy potential temperature. The new\nformulation is compared with Ertel's version and with others based on virtual\nand equivalent potential temperatures. The new potential vorticity is subject\nto conservative properties ensured by the Second Law applied to the moist-air\nmaterial derivatives. It is shown that the upper tropospheric and stratospheric\n(dry) structures are nearly the same as those obtained with Ertel's component.\nMoreover, new structures are observed in the low troposphere, with negative\nvalues associated with moist frontal regions. The negative values are observed\nin the frontal regions where slantwise convection instabilities may take place,\nbut they are smaller than those observed with the equivalent potential\nvorticity. The main purpose of the article is to diagnose the behaviour of the\nnew potential vorticity from numerical output generated by the ARPEGE NWP\nmodel, with the help of isobaric charts and vertical cross-sections. Two\ninversion methods are suggested. The first method could be based on the\ninvertibility principle verified by the virtual potential vorticity, with a\npossibility to control and modify separately potential vorticity components in\nthe (dry) upper and (moist) lower atmospheric levels. The other method may\nconsist of an inversion process directly applied to the new moist-air entropy\npotential vorticity, because the negative values and the solenoidal term are\nsmaller than those observed with equivalent potential vorticity, as shown by\nnumerical evaluations. \n\n"}
{"id": "1401.2103", "contents": "Title: Restricted Equilibrium and the Energy Cascade in Rotating and Stratified\n  Flows Abstract: Most of the turbulent flows appearing in nature (e.g. geophysical and\nastrophysical flows) are subjected to strong rotation and stratification. These\neffects break the symmetries of classical, homogenous isotropic turbulence. In\ndoing so, they introduce a natural decomposition of phase space in terms of\nwave modes and potential vorticity modes. The appearance of a new time scale\nassociated to the propagation of waves, in addition to the eddy turnover time,\nincreases the complexity of the energy transfers between the various scales;\nnonlinearly interacting waves may dominate at some scales while balanced motion\nmay prevail at others. In the end, it is difficult to predict \\emph{a priori}\nif the energy cascades downscale as in homogeneous isotropic turbulence,\nupscale as expected from balanced dynamics, or follows yet another\nphenomenology.\n  In this paper, we suggest a theoretical approach based on equilibrium\nstatistical mechanics for the ideal system, inspired from the restricted\npartition function formalism introduced in metastability studies. In this\nframework, we show analytically that in the presence of rotation, when the\ndynamics is restricted to the slow modes, the equilibrium energy spectrum\nfeatures an infrared divergence characteristic of an inverse cascade regime,\nwhereas this is not the case for purely stratified flows. \n\n"}
{"id": "1401.2163", "contents": "Title: Estimation of Partially Linear Regression Model under Partial\n  Consistency Property Abstract: In this paper, utilizing recent theoretical results in high dimensional\nstatistical modeling, we propose a model-free yet computationally simple\napproach to estimate the partially linear model $Y=X\\beta+g(Z)+\\varepsilon$.\nMotivated by the partial consistency phenomena, we propose to model $g(Z)$ via\nincidental parameters. Based on partitioning the support of $Z$, a simple local\naverage is used to estimate the response surface. The proposed method seeks to\nstrike a balance between computation burden and efficiency of the estimators\nwhile minimizing model bias. Computationally this approach only involves least\nsquares. We show that given the inconsistent estimator of $g(Z)$, a root $n$\nconsistent estimator of parametric component $\\beta$ of the partially linear\nmodel can be obtained with little cost in efficiency. Moreover, conditional on\nthe $\\beta$ estimates, an optimal estimator of $g(Z)$ can then be obtained\nusing classic nonparametric methods. The statistical inference problem\nregarding $\\beta$ and a two-population nonparametric testing problem regarding\n$g(Z)$ are considered. Our results show that the behavior of test statistics\nare satisfactory. To assess the performance of our method in comparison with\nother methods, three simulation studies are conducted and a real dataset about\nrisk factors of birth weights is analyzed. \n\n"}
{"id": "1401.2379", "contents": "Title: On a general definition of the squared Brunt-V\\\"{a}is\\\"{a}l\\\"{a}\n  Frequency associated with the specific moist entropy potential temperature Abstract: The squared Brunt-V\\\"{a}is\\\"{a}l\\\"{a} Frequency (BVF) is computed in terms of\nthe moist entropy potential temperature recently defined in Marquet (2011).\nBoth homogeneously saturated and non-saturated versions of $N^2$ (the squared\nBVF) are derived. The method employed for computing these special homogeneous\ncases relies on the expression of density written as a function of pressure,\ntotal water content and specific moist entropy only. The associated\nconservative variable diagrams are discussed and compared with existing ones.\nDespite being obtained without any simplification, the formulations for $N^2$\nremain nicely compact and are clearly linked with the squared BVF expressed in\nterms of the adiabatic non-saturated and saturated lapse rates. As in previous\nsimilar expressions, the extreme homogeneous solutions for $N^2$ are of course\ndifferent, but they are not analytically discontinuous. This allows us to\ndefine a simple bridging expression for a single general shape of $N^2$,\ndepending only on the basic mean atmospheric quantities and on a transition\nparameter, to be defined (or parameterized) in connection with the type of\napplication sought. This integrated result remains a linear combination (with\ncomplex but purely local weights) of two terms only, namely the environmental\ngradient of the moist entropy potential temperature and the environmental\ngradient of the total water content. Simplified versions of the various\nequations are also proposed for the case in which the moist entropy potential\ntemperature is approximated by a function of both so-called moist-conservative\nvariables of Betts (1973). \n\n"}
{"id": "1401.3125", "contents": "Title: On the computation of moist-air specific thermal enthalpy Abstract: The specific thermal enthalpy of a moist-air parcel is defined analytically\nfollowing a method in which specific moist entropy is derived from the Third\nLaw of thermodynamics. Specific thermal enthalpy is computed by integrating\nspecific heat content with respect to absolute temperature and including the\nimpacts of various latent heats (i.e., solid condensation, sublimation,\nmelting, and evaporation). It is assumed that thermal enthalpies can be set to\nzero at $0$ K for the solid form of the main chemically inactive components of\nthe atmosphere (solid-$\\alpha$ oxygen and nitrogen, hexagonal ice). The moist\nthermal enthalpy is compared to already existing formulations of moist static\nenergy (MSE). It is shown that the differences between thermal enthalpy and the\nthermal part of MSE may be quite large. This prevents the use of MSE to\nevaluate the enthalpy budget of a moist atmosphere accurately, a situation that\nis particularly true when dry-air and cloud parcels mix because of\nentrainment/detrainment processes along the edges of cloud. Other differences\nare observed when MSE or moist-air thermal enthalpy is plotted on a\npsychrometric diagram or when vertical profiles of surface deficit are plotted. \n\n"}
{"id": "1401.3632", "contents": "Title: Bayesian Conditional Density Filtering Abstract: We propose a Conditional Density Filtering (C-DF) algorithm for efficient\nonline Bayesian inference. C-DF adapts MCMC sampling to the online setting,\nsampling from approximations to conditional posterior distributions obtained by\npropagating surrogate conditional sufficient statistics (a function of data and\nparameter estimates) as new data arrive. These quantities eliminate the need to\nstore or process the entire dataset simultaneously and offer a number of\ndesirable features. Often, these include a reduction in memory requirements and\nruntime and improved mixing, along with state-of-the-art parameter inference\nand prediction. These improvements are demonstrated through several\nillustrative examples including an application to high dimensional compressed\nregression. Finally, we show that C-DF samples converge to the target posterior\ndistribution asymptotically as sampling proceeds and more data arrives. \n\n"}
{"id": "1401.5323", "contents": "Title: Climate of Earth-like planets with high obliquity and eccentric orbits:\n  implications for habitability conditions Abstract: We explore the effects of seasonal variability for the climate of Earth-like\nplanets as determined by the two parameters polar obliquity and orbital\neccentricity using a general circulation model of intermediate complexity. In\nthe first part of the paper we examine the consequences of different values of\nobliquity and eccentricity for the spatio-temporal patterns of radiation and\nsurface temperatures as well as for the main characteristics of the atmospheric\ncirculations. In the second part we analyse the associated implications for the\nhabitability of planets close to the outer edge of the habitable zone (HZ).\nThis part of the paper focuses in particular on the multistability property of\nclimate, i.e. the parallel existence of both an ice-free and an ice-covered\nclimate state. Our results show that seasonal variability affects both the\nexistence of and transitions between the two climate states. Moreover, our\nexperiments reveal that planets with Earth-like atmospheres and high seasonal\nvariability can have ice-free areas at much larger distance from the host star\nthan planets without seasonal variability, which leads to a substantial\nexpansion of the outer edge of the HZ. Sensitivity experiments exploring the\nrole of azimuthal obliquity and surface heat capacity test the robustness of\nour results. On circular orbits, our findings obtained with a general\ncirculation model agree well with previous studies based on one dimensional\nenergy balance models, whereas significant differences are found on eccentric\norbits. \n\n"}
{"id": "1402.1472", "contents": "Title: Parallel inference for massive distributed spatial data using low-rank\n  models Abstract: Due to rapid data growth, statistical analysis of massive datasets often has\nto be carried out in a distributed fashion, either because several datasets\nstored in separate physical locations are all relevant to a given problem, or\nsimply to achieve faster (parallel) computation through a divide-and-conquer\nscheme. In both cases, the challenge is to obtain valid inference that does not\nrequire processing all data at a single central computing node. We show that\nfor a very widely used class of spatial low-rank models, which can be written\nas a linear combination of spatial basis functions plus a fine-scale-variation\ncomponent, parallel spatial inference and prediction for massive distributed\ndata can be carried out exactly, meaning that the results are the same as for a\ntraditional, non-distributed analysis. The communication cost of our\ndistributed algorithms does not depend on the number of data points. After\nextending our results to the spatio-temporal case, we illustrate our\nmethodology by carrying out distributed spatio-temporal particle filtering\ninference on total precipitable water measured by three different satellite\nsensor systems. \n\n"}
{"id": "1402.1510", "contents": "Title: Nonlinear fast growth of water waves under wind forcing Abstract: In the wind-driven wave regime, the Miles mechanism gives an estimate of the\ngrowth rate of the waves under the effect of wind. We consider the case where\nthis growth rate, normalised with respect to the frequency of the carrier wave,\nis of the order of the wave steepness. Using the method of multiple scales, we\ncalculate the terms which appear in the nonlinear Schr\\\"odinger (NLS) equation\nin this regime of fast-growing waves. We define a coordinate transformation\nwhich maps the forced NLS equation into the standard NLS with constant\ncoefficients, that has a number of known analytical soliton solutions. Among\nthese solutions, the Peregrine and the Akhmediev solitons show an enhancement\nof both their lifetime and maximum amplitude which is in qualitative agreement\nwith the results of tank experiments and numerical simulations of dispersive\nfocusing under the action of wind. \n\n"}
{"id": "1402.2676", "contents": "Title: Ranking via Robust Binary Classification and Parallel Parameter\n  Estimation in Large-Scale Data Abstract: We propose RoBiRank, a ranking algorithm that is motivated by observing a\nclose connection between evaluation metrics for learning to rank and loss\nfunctions for robust classification. The algorithm shows a very competitive\nperformance on standard benchmark datasets against other representative\nalgorithms in the literature. On the other hand, in large scale problems where\nexplicit feature vectors and scores are not given, our algorithm can be\nefficiently parallelized across a large number of machines; for a task that\nrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\nour algorithm finds solutions that are of dramatically higher quality than that\ncan be found by a state-of-the-art competitor algorithm, given the same amount\nof wall-clock time for computation. \n\n"}
{"id": "1402.4039", "contents": "Title: Sequential Quasi-Monte Carlo Abstract: We derive and study SQMC (Sequential Quasi-Monte Carlo), a class of\nalgorithms obtained by introducing QMC point sets in particle filtering. SQMC\nis related to, and may be seen as an extension of, the array-RQMC algorithm of\nL'Ecuyer et al. (2006). The complexity of SQMC is $O(N \\log N)$, where $N$ is\nthe number of simulations at each iteration, and its error rate is smaller than\nthe Monte Carlo rate $O_P(N^{-1/2})$. The only requirement to implement SQMC is\nthe ability to write the simulation of particle $x_t^n$ given $x_{t-1}^n$ as a\ndeterministic function of $x_{t-1}^n$ and a fixed number of uniform variates.\nWe show that SQMC is amenable to the same extensions as standard SMC, such as\nforward smoothing, backward smoothing, unbiased likelihood evaluation, and so\non. In particular, SQMC may replace SMC within a PMCMC (particle Markov chain\nMonte Carlo) algorithm. We establish several convergence results. We provide\nnumerical evidence that SQMC may significantly outperform SMC in practical\nscenarios. \n\n"}
{"id": "1402.5637", "contents": "Title: On the concept of pseudo-energy of T. G. Shepherd. A close link with\n  exergy functions Abstract: Shepherd (1993) derived a general expression for the available potential\nenergy for compressible, hydrostatic flow, where the sum of this available\nenergy and the kinetic energy is called pseudo-energy. He demonstrated that for\na special basic state the small-amplitude limit of the generalized available\npotential energy reduces to the well-known approximate form of Lorenz (1955)\nexpressed in a pressure vertical coordinate.\n  Other forms of available energies exist in atmospheric energetics and in\nthermodynamics, where the name exergy has been coined by Rant (1956) to denote\nthe maximum work that can be extracted from any system when it is subject to\nsome constraints (adiabatic transformations or constant total energy for\ninstance).\n  The purpose of this note is to show that the specific available enthalpy\nfunction denoted by $a_h = (h - h_r) - T_r \\: (s - s_r)$ in Marquet (1991) can\nbe obtained from the generalized approach of Shepherd for another basic state.\nThis special form of pseudo-energy also leads to the global hydrostatic\nconcepts of Dutton (1973) and Pichler (1977). The function $a_h$ only depends\non the specific enthalpy $h$ and entropy $s$ at any point, the values $h_r$ and\n$s_r$ refer to the special dead state at temperature $T_r$ and pressure $p_r$.\n  It is explained that for another choice of the basic state, the generalized\nexpression of Shepherd reduces with a good accuracy to the approximate\nfunctions introduced by Pearce (1978) or Blackburn (1983) in meteorology and\nThomson (1853, Lord Kelvin) in thermodynamics. \n\n"}
{"id": "1402.6781", "contents": "Title: Bias Reduction of Long Memory Parameter Estimators via the Pre-filtered\n  Sieve Bootstrap Abstract: This paper investigates the use of bootstrap-based bias correction of\nsemi-parametric estimators of the long memory parameter in fractionally\nintegrated processes. The re-sampling method involves the application of the\nsieve bootstrap to data pre-filtered by a preliminary semi-parametric estimate\nof the long memory parameter. Theoretical justification for using the bootstrap\ntechniques to bias adjust log-periodogram and semi-parametric local Whittle\nestimators of the memory parameter is provided. Simulation evidence comparing\nthe performance of the bootstrap bias correction with analytical bias\ncorrection techniques is also presented. The bootstrap method is shown to\nproduce notable bias reductions, in particular when applied to an estimator for\nwhich analytical adjustments have already been used. The empirical coverage of\nconfidence intervals based on the bias-adjusted estimators is very close to the\nnominal, for a reasonably large sample size, more so than for the comparable\nanalytically adjusted estimators. The precision of inferences (as measured by\ninterval length) is also greater when the bootstrap is used to bias correct\nrather than analytical adjustments. \n\n"}
{"id": "1402.6958", "contents": "Title: Comparison of Ellison and Thorpe scales from Eulerian ocean temperature\n  observations Abstract: Ocean turbulence dissipation rate is estimated either by means of\nmicrostructure shear measurements, or by adiabatically reordering vertical\nprofiles of density. The latter technique leads to the estimate of the Thorpe\nscale, which in turn can be used to obtain average turbulence dissipation rate\nby comparing the Thorpe scale to the Ozmidov scale. In both cases, the\nturbulence dissipation rate can be estimated using single vertical profiles\nfrom shipborne instrumentation. We present here an alternative method to\nestimate the length scale of overturns by using the Ellison length scale. The\nEllison scale is estimated from temperature variance just beyond the internal\nwave band, measured by moored instruments. We apply the method to high\nresolution temperature data from two moorings deployed at different locations\naround the Josephine seamount (North Eastern Atlantic Ocean), in a region of\nbottom-intensified turbulence. The variance of the temperature time series just\nabove the internal wave frequency band is well correlated with the Thorpe\nscale. The method is based on the time--frequency decomposition of variance\ncalled \"maximum overlap discrete wavelet transform\". The results show that the\nEllison length scale can be a viable alternative to the Thorpe scale for\nindirectly estimating turbulence dissipation rate from moored instruments in\nthe ocean if time resolution is sufficiently high. We suggest that fine\nstructure contaminated temperature measurements can provide reliable\ninformation on turbulence intensity. \n\n"}
{"id": "1403.2977", "contents": "Title: Atmospheric effects in astroparticle physics experiments and the\n  challenge of ever greater precision in measurements Abstract: Astroparticle physics and cosmology allow us to scan the universe through\nmultiple messengers. It is the combination of these probes that improves our\nunderstanding of the universe, both in its composition and its dynamics. Unlike\nother areas in science, research in astroparticle physics has a real\noriginality in detection techniques, in infrastructure locations, and in the\nobserved physical phenomenon that is not created directly by humans. It is\nthese features that make the minimisation of statistical and systematic errors\na perpetual challenge. In all these projects, the environment is turned into a\ndetector medium or a target. The atmosphere is probably the environment\ncomponent the most common in astroparticle physics and requires a continuous\nmonitoring of its properties to minimise as much as possible the systematic\nuncertainties associated. This paper introduces the different atmospheric\neffects to take into account in astroparticle physics measurements and provides\na non-exhaustive list of techniques and instruments to monitor the different\nelements composing the atmosphere. A discussion on the close link between\nastroparticle physics and Earth sciences ends this paper. \n\n"}
{"id": "1403.4359", "contents": "Title: Pre-processing for approximate Bayesian computation in image analysis Abstract: Most of the existing algorithms for approximate Bayesian computation (ABC)\nassume that it is feasible to simulate pseudo-data from the model at each\niteration. However, the computational cost of these simulations can be\nprohibitive for high dimensional data. An important example is the Potts model,\nwhich is commonly used in image analysis. Images encountered in real world\napplications can have millions of pixels, therefore scalability is a major\nconcern. We apply ABC with a synthetic likelihood to the hidden Potts model\nwith additive Gaussian noise. Using a pre-processing step, we fit a binding\nfunction to model the relationship between the model parameters and the\nsynthetic likelihood parameters. Our numerical experiments demonstrate that the\nprecomputed binding function dramatically improves the scalability of ABC,\nreducing the average runtime required for model fitting from 71 hours to only 7\nminutes. We also illustrate the method by estimating the smoothing parameter\nfor remotely sensed satellite imagery. Without precomputation, Bayesian\ninference is impractical for datasets of that scale. \n\n"}
{"id": "1403.4431", "contents": "Title: Recovery of ordered periodic orbits with increasing wavelength for sound\n  propagation in a range-dependent waveguide Abstract: We consider sound wave propagation in a range-periodic acoustic waveguide in\nthe deep ocean. It is demonstrated that vertical oscillations of a sound-speed\nperturbation, induced by ocean internal waves, influence near-axial rays in a\nresonant way, producing ray chaos and forming a wide chaotic sea in the\nunderlying phase space. We study interplay between chaotic ray dynamics and\nwave motion with signal frequencies of 50-100 Hz. The Floquet modes of the\nwaveguide are calculated and visualized by means of the Husimi plots. Despite\nof irregular phase space distribution of periodic orbits, the Husimi plots\ndisplay the presence of ordered peaks within the chaotic sea. These peaks, not\nbeing supported by certain periodic orbits, draw the specific \"chainlike\"\npattern, reminiscent of KAM resonance. The link between the peaks and KAM\nresonance is confirmed by ray calculations with lower amplitude of the\nsound-speed perturbation, when the periodic orbits are well-ordered. We\nassociate occurrence of the peaks with the recovery of ordered periodic orbits,\ncorresponding to KAM resonance, due to suppressing of wavefield sensitivity to\nsmall-scale features of the sound-speed profile. \n\n"}
{"id": "1403.4680", "contents": "Title: Likelihood-informed dimension reduction for nonlinear inverse problems Abstract: The intrinsic dimensionality of an inverse problem is affected by prior\ninformation, the accuracy and number of observations, and the smoothing\nproperties of the forward operator. From a Bayesian perspective, changes from\nthe prior to the posterior may, in many problems, be confined to a relatively\nlow-dimensional subspace of the parameter space. We present a dimension\nreduction approach that defines and identifies such a subspace, called the\n\"likelihood-informed subspace\" (LIS), by characterizing the relative influences\nof the prior and the likelihood over the support of the posterior distribution.\nThis identification enables new and more efficient computational methods for\nBayesian inference with nonlinear forward models and Gaussian priors. In\nparticular, we approximate the posterior distribution as the product of a\nlower-dimensional posterior defined on the LIS and the prior distribution\nmarginalized onto the complementary subspace. Markov chain Monte Carlo sampling\ncan then proceed in lower dimensions, with significant gains in computational\nefficiency. We also introduce a Rao-Blackwellization strategy that\nde-randomizes Monte Carlo estimates of posterior expectations for additional\nvariance reduction. We demonstrate the efficiency of our methods using two\nnumerical examples: inference of permeability in a groundwater system governed\nby an elliptic PDE, and an atmospheric remote sensing problem based on Global\nOzone Monitoring System (GOMOS) observations. \n\n"}
{"id": "1403.7137", "contents": "Title: A Sampling Filter for Non-Gaussian Data Assimilation Abstract: Data assimilation combines information from models, measurements, and priors\nto estimate the state of a dynamical system such as the atmosphere. The\nEnsemble Kalman filter (EnKF) is a family of ensemble-based data assimilation\napproaches that has gained wide popularity due its simple formulation, ease of\nimplementation, and good practical results. Most EnKF algorithms assume that\nthe underlying probability distributions are Gaussian. Although this assumption\nis well accepted, it is too restrictive when applied to large nonlinear models,\nnonlinear observation operators, and large levels of uncertainty. Several\napproaches have been proposed in order to avoid the Gaussianity assumption. One\nof the most successful strategies is the maximum likelihood ensemble filter\n(MLEF) which computes a maximum a posteriori estimate of the state assuming the\nposterior distribution is Gaussian. MLEF is designed to work with nonlinear and\neven non-differentiable observation operators, and shows good practical\nperformance. However, there are limits to the degree of nonlinearity that MLEF\ncan handle. This paper proposes a new ensemble-based data assimilation method,\nnamed the \"sampling filter\", which obtains the analysis by sampling directly\nfrom the posterior distribution. The sampling strategy is based on a Hybrid\nMonte Carlo (HMC) approach that can handle non-Gaussian probability\ndistributions. Numerical experiments are carried out using the Lorenz-96 model\nand observation operators with different levels of non-linearity and\ndifferentiability. The proposed filter is also tested with shallow water model\non a sphere with linear observation operator. The results show that the\nsampling filter can perform well even in highly nonlinear situations were EnKF\nand MLEF filters diverge. \n\n"}
{"id": "1403.7676", "contents": "Title: Computation of Maximum Likelihood Estimates for Multiresponse\n  Generalized Linear Mixed Models with Non-nested, Correlated Random Effects Abstract: Estimation of generalized linear mixed models (GLMMs) with non-nested random\neffects structures requires approximation of high-dimensional integrals. Many\nexisting methods are tailored to the low-dimensional integrals produced by\nnested designs. We explore the modifications that are required in order to\nadapt an EM algorithm with first-order and fully exponential Laplace\napproximations to a non-nested, multiple response model. The equations in the\nestimation routine are expressed as functions of the first four derivatives of\nthe conditional likelihood of an arbitrary GLMM, providing a template for\nfuture applications. We apply the method to a joint Poisson-binary model for\nranking sporting teams, and discuss the estimation of a correlated random\neffects model designed to evaluate the sensitivity of value-added models for\nteacher evaluation to assumptions about the missing data process. Source code\nin R is provided in the online supplementary material. \n\n"}
{"id": "1404.2911", "contents": "Title: Inferring structure in bipartite networks using the latent block model\n  and exact ICL Abstract: We consider the task of simultaneous clustering of the two node sets involved\nin a bipartite network. The approach we adopt is based on use of the exact\nintegrated complete likelihood for the latent block model. Using this allows\none to infer the number of clusters as well as cluster memberships using a\ngreedy search. This gives a model-based clustering of the node sets.\nExperiments on simulated bipartite network data show that the greedy search\napproach is vastly more scalable than competing Markov chain Monte Carlo based\nmethods. Application to a number of real observed bipartite networks\ndemonstrate the algorithms discussed. \n\n"}
{"id": "1404.5100", "contents": "Title: Convergence of cyclic coordinatewise l1 minimization Abstract: We consider the general problem of minimizing an objective function which is\nthe sum of a convex function (not strictly convex) and absolute values of a\nsubset of variables (or equivalently the l1-norm of the variables). This\nproblem appears exten- sively in modern statistical applications associated\nwith high-dimensional data or \"big data\", and corresponds to optimizing\nl1-regularized likelihoods in the context of model selection. In such\napplications, cyclic coordinatewise minimization (CCM), where the objective\nfunction is sequentially minimized with respect to each individual coordi-\nnate, is often employed as it offers a computationally cheap and effective\noptimization method. Consequently, it is crucial to obtain theoretical\nguarantees of convergence for the sequence of iterates produced by the cyclic\ncoordinatewise minimization in this setting. Moreover, as the objective\ncorresponds to at l1-regularized likelihoods of many variables, it is important\nto obtain convergence of the iterates themselves, and not just the function\nvalues. Previous results in the literature only establish either, (i) that\nevery limit point of the sequence of iterates is a stationary point of the\nobjective function, or (ii) establish convergence under special assumptions, or\n(iii) establish con- vergence for a different minimization approach (which uses\nquadratic approximation based gradient descent followed by an inexact line\nsearch), (iv) establish convergence of only the function values of the sequence\nof iterates produced by random coordinatewise minimization (a variant of CCM).\nIn this paper, a rigorous general proof of convergence for the cyclic\ncoordinatewise minimization algorithm is provided. We demonstrate the\nusefulness of our general results in contemporary applications. \n\n"}
{"id": "1404.7188", "contents": "Title: Limitations of polynomial chaos expansions in the Bayesian solution of\n  inverse problems Abstract: Polynomial chaos expansions are used to reduce the computational cost in the\nBayesian solutions of inverse problems by creating a surrogate posterior that\ncan be evaluated inexpensively. We show, by analysis and example, that when the\ndata contain significant information beyond what is assumed in the prior, the\nsurrogate posterior can be very different from the posterior, and the resulting\nestimates become inaccurate. One can improve the accuracy by adaptively\nincreasing the order of the polynomial chaos, but the cost may increase too\nfast for this to be cost effective compared to Monte Carlo sampling without a\nsurrogate posterior. \n\n"}
{"id": "1405.0377", "contents": "Title: Hypothesis Testing for Parsimonious Gaussian Mixture Models Abstract: Gaussian mixture models with eigen-decomposed covariance structures make up\nthe most popular family of mixture models for clustering and classification,\ni.e., the Gaussian parsimonious clustering models (GPCM). Although the GPCM\nfamily has been used for almost 20 years, selecting the best member of the\nfamily in a given situation remains a troublesome problem. Likelihood ratio\ntests are developed to tackle this problems. These likelihood ratio tests use\nthe heteroscedastic model under the alternative hypothesis but provide much\nmore flexibility and real-world applicability than previous approaches that\ncompare the homoscedastic Gaussian mixture versus the heteroscedastic one.\nAlong the way, a novel maximum likelihood estimation procedure is developed for\ntwo members of the GPCM family. Simulations show that the $\\chi^2$ reference\ndistribution gives reasonable approximation for the LR statistics only when the\nsample size is considerable and when the mixture components are well separated;\naccordingly, following Lo (2008), a parametric bootstrap is adopted.\nFurthermore, by generalizing the idea of Greselin and Punzo (2013) to the\nclustering context, a closed testing procedure, having the defined likelihood\nratio tests as local tests, is introduced to assess a unique model in the\ngeneral family. The advantages of this likelihood ratio testing procedure are\nillustrated via an application to the well-known Iris data set. \n\n"}
{"id": "1405.0506", "contents": "Title: Sampling Polya-Gamma random variates: alternate and approximate\n  techniques Abstract: Efficiently sampling from the P\\'olya-Gamma distribution, ${PG}(b,z)$, is an\nessential element of P\\'olya-Gamma data augmentation. Polson et. al (2013) show\nhow to efficiently sample from the ${PG}(1,z)$ distribution. We build two new\nsamplers that offer improved performance when sampling from the ${PG}(b,z)$\ndistribution and $b$ is not unity. \n\n"}
{"id": "1405.3034", "contents": "Title: G-AMA: Sparse Gaussian graphical model estimation via alternating\n  minimization Abstract: Several methods have been recently proposed for estimating sparse Gaussian\ngraphical models using $\\ell_{1}$ regularization on the inverse covariance\nmatrix. Despite recent advances, contemporary applications require methods that\nare even faster in order to handle ill-conditioned high dimensional modern day\ndatasets. In this paper, we propose a new method, G-AMA, to solve the sparse\ninverse covariance estimation problem using Alternating Minimization Algorithm\n(AMA), that effectively works as a proximal gradient algorithm on the dual\nproblem. Our approach has several novel advantages over existing methods.\nFirst, we demonstrate that G-AMA is faster than the previous best algorithms by\nmany orders of magnitude and is thus an ideal approach for modern high\nthroughput applications. Second, global linear convergence of G-AMA is\ndemonstrated rigorously, underscoring its good theoretical properties. Third,\nthe dual algorithm operates on the covariance matrix, and thus easily\nfacilitates incorporating additional constraints on pairwise/marginal\nrelationships between feature pairs based on domain specific knowledge. Over\nand above estimating a sparse inverse covariance matrix, we also illustrate how\nto (1) incorporate constraints on the (bivariate) correlations and, (2)\nincorporate equality (equisparsity) or linear constraints between individual\ninverse covariance elements. Fourth, we also show that G-AMA is better adept at\nhandling extremely ill-conditioned problems, as is often the case with real\ndata. The methodology is demonstrated on both simulated and real datasets to\nillustrate its superior performance over recently proposed methods. \n\n"}
{"id": "1405.4141", "contents": "Title: Classification using log Gaussian Cox processes Abstract: McCullagh and Yang (2006) suggest a family of classification algorithms based\non Cox processes. We further investigate the log Gaussian variant which has a\nnumber of appealing properties. Conditioned on the covariates, the distribution\nover labels is given by a type of conditional Markov random field. In the\nsupervised case, computation of the predictive probability of a single test\npoint scales linearly with the number of training points and the multiclass\ngeneralization is straightforward. We show new links between the supervised\nmethod and classical nonparametric methods. We give a detailed analysis of the\npairwise graph representable Markov random field, which we use to extend the\nmodel to semi-supervised learning problems, and propose an inference method\nbased on graph min-cuts. We give the first experimental analysis on supervised\nand semi-supervised datasets and show good empirical performance. \n\n"}
{"id": "1405.7551", "contents": "Title: Origin of atmospheric aerosols at the Pierre Auger Observatory using\n  studies of air mass trajectories in South America Abstract: The Pierre Auger Observatory is making significant contributions towards\nunderstanding the nature and origin of ultra-high energy cosmic rays. One of\nits main challenges is the monitoring of the atmosphere, both in terms of its\nstate variables and its optical properties. The aim of this work is to analyze\naerosol optical depth $\\tau_{\\rm a}(z)$ values measured from 2004 to 2012 at\nthe observatory, which is located in a remote and relatively unstudied area of\nthe Pampa Amarilla, Argentina. The aerosol optical depth is in average quite\nlow - annual mean $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.04$ - and shows a seasonal\ntrend with a winter minimum - $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.03$ -, and a\nsummer maximum - $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.06$ -, and an unexpected\nincrease from August to September - $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.055$). We\ncomputed backward trajectories for the years 2005 to 2012 to interpret the air\nmass origin. Winter nights with low aerosol concentrations show air masses\noriginating from the Pacific Ocean. Average concentrations are affected by\ncontinental sources (wind-blown dust and urban pollution), while the peak\nobserved in September and October could be linked to biomass burning in the\nnorthern part of Argentina or air pollution coming from surrounding urban\nareas. \n\n"}
{"id": "1405.7867", "contents": "Title: Lazy ABC Abstract: Approximate Bayesian computation (ABC) performs statistical inference for\notherwise intractable probability models by accepting parameter proposals when\ncorresponding simulated datasets are sufficiently close to the observations.\nProducing the large quantity of simulations needed requires considerable\ncomputing time. However, it is often clear before a simulation ends that it is\nunpromising: it is likely to produce a poor match or require excessive time.\nThis paper proposes lazy ABC, an ABC importance sampling algorithm which saves\ntime by sometimes abandoning such simulations. This makes ABC more scalable to\napplications where simulation is expensive. By using a random stopping rule and\nappropriate reweighting step, the target distribution is unchanged from that of\nstandard ABC. Theory and practical methods to tune lazy ABC are presented and\nillustrated on a simple epidemic model example. They are also demonstrated on\nthe computationally demanding spatial extremes application of Erhardt and Smith\n(2012), producing efficiency gains, in terms of effective sample size per unit\nCPU time, of roughly 3 times for a 20 location dataset, and 8 times for 35\nlocations. \n\n"}
{"id": "1406.2098", "contents": "Title: Learning directed acyclic graphs via bootstrap aggregating Abstract: Probabilistic graphical models are graphical representations of probability\ndistributions. Graphical models have applications in many fields including\nbiology, social sciences, linguistic, neuroscience. In this paper, we propose\ndirected acyclic graphs (DAGs) learning via bootstrap aggregating. The proposed\nprocedure is named as DAGBag. Specifically, an ensemble of DAGs is first\nlearned based on bootstrap resamples of the data and then an aggregated DAG is\nderived by minimizing the overall distance to the entire ensemble. A family of\nmetrics based on the structural hamming distance is defined for the space of\nDAGs (of a given node set) and is used for aggregation. Under the\nhigh-dimensional-low-sample size setting, the graph learned on one data set\noften has excessive number of false positive edges due to over-fitting of the\nnoise. Aggregation overcomes over-fitting through variance reduction and thus\ngreatly reduces false positives. We also develop an efficient implementation of\nthe hill climbing search algorithm of DAG learning which makes the proposed\nmethod computationally competitive for the high-dimensional regime. The DAGBag\nprocedure is implemented in the R package dagbag. \n\n"}
{"id": "1406.2660", "contents": "Title: Accelerating Metropolis-Hastings algorithms: Delayed acceptance with\n  prefetching Abstract: MCMC algorithms such as Metropolis-Hastings algorithms are slowed down by the\ncomputation of complex target distributions as exemplified by huge datasets. We\noffer in this paper an approach to reduce the computational costs of such\nalgorithms by a simple and universal divide-and-conquer strategy. The idea\nbehind the generic acceleration is to divide the acceptance step into several\nparts, aiming at a major reduction in computing time that outranks the\ncorresponding reduction in acceptance probability. The division decomposes the\n\"prior x likelihood\" term into a product such that some of its components are\nmuch cheaper to compute than others. Each of the components can be sequentially\ncompared with a uniform variate, the first rejection signalling that the\nproposed value is considered no further, This approach can in turn be\naccelerated as part of a prefetching algorithm taking advantage of the parallel\nabilities of the computer at hand. We illustrate those accelerating features on\na series of toy and realistic examples. \n\n"}
{"id": "1406.3183", "contents": "Title: Approximations of the Optimal Importance Density using Gaussian Particle\n  Flow Importance Sampling Abstract: Recently developed particle flow algorithms provide an alternative to\nimportance sampling for drawing particles from a posterior distribution, and a\nnumber of particle filters based on this principle have been proposed. Samples\nare drawn from the prior and then moved according to some dynamics over an\ninterval of pseudo-time such that their final values are distributed according\nto the desired posterior. In practice, implementing a particle flow sampler\nrequires multiple layers of approximation, with the result that the final\nsamples do not in general have the correct posterior distribution. In this\npaper we consider using an approximate Gaussian flow for sampling with a class\nof nonlinear Gaussian models. We use the particle flow within an importance\nsampler, correcting for the discrepancy between the target and actual densities\nwith importance weights. We present a suitable numerical integration procedure\nfor use with this flow and an accompanying step-size control algorithm. In a\nfiltering context, we use the particle flow to sample from the optimal\nimportance density, rather than the filtering density itself, avoiding the need\nto make analytical or numerical approximations of the predictive density.\nSimulations using particle flow importance sampling within a particle filter\ndemonstrate significant improvement over standard approximations of the optimal\nimportance density, and the algorithm falls within the standard sequential\nMonte Carlo framework. \n\n"}
{"id": "1406.4068", "contents": "Title: Functional Regression Abstract: Functional data analysis (FDA) involves the analysis of data whose ideal\nunits of observation are functions defined on some continuous domain, and the\nobserved data consist of a sample of functions taken from some population,\nsampled on a discrete grid. Ramsay and Silverman's 1997 textbook sparked the\ndevelopment of this field, which has accelerated in the past 10 years to become\none of the fastest growing areas of statistics, fueled by the growing number of\napplications yielding this type of data. One unique characteristic of FDA is\nthe need to combine information both across and within functions, which Ramsay\nand Silverman called replication and regularization, respectively. This article\nwill focus on functional regression, the area of FDA that has received the most\nattention in applications and methodological development. First will be an\nintroduction to basis functions, key building blocks for regularization in\nfunctional regression methods, followed by an overview of functional regression\nmethods, split into three types: [1] functional predictor regression\n(scalar-on-function), [2] functional response regression (function-on-scalar)\nand [3] function-on-function regression. For each, the role of replication and\nregularization will be discussed and the methodological development described\nin a roughly chronological manner, at times deviating from the historical\ntimeline to group together similar methods. The primary focus is on modeling\nand methodology, highlighting the modeling structures that have been developed\nand the various regularization approaches employed. At the end is a brief\ndiscussion describing potential areas of future development in this field. \n\n"}
{"id": "1406.4406", "contents": "Title: Posterior concentration rates for empirical Bayes procedures, with\n  applications to Dirichlet Process mixtures Abstract: In this paper we provide general conditions to check on the model and the\nprior to derive posterior concentration rates for data-dependent priors (or\nempirical Bayes approaches). We aim at providing conditions that are close to\nthe conditions provided in the seminal paper by Ghosal and van der Vaart\n(2007a). We then apply the general theorem to two different settings: the\nestimation of a density using Dirichlet process mixtures of Gaussian random\nvariables with base measure depending on some empirical quantities and the\nestimation of the intensity of a counting process under the Aalen model. A\nsimulation study for inhomogeneous Poisson processes also illustrates our\nresults. In the former case we also derive some results on the estimation of\nthe mixing density and on the deconvolution problem. In the latter, we provide\na general theorem on posterior concentration rates for counting processes with\nAalen multiplicative intensity with priors not depending on the data. \n\n"}
{"id": "1406.6010", "contents": "Title: Forest resampling for distributed sequential Monte Carlo Abstract: This paper brings explicit considerations of distributed computing\narchitectures and data structures into the rigorous design of Sequential Monte\nCarlo (SMC) methods. A theoretical result established recently by the authors\nshows that adapting interaction between particles to suitably control the\nEffective Sample Size (ESS) is sufficient to guarantee stability of SMC\nalgorithms. Our objective is to leverage this result and devise algorithms\nwhich are thus guaranteed to work well in a distributed setting. We make three\nmain contributions to achieve this. Firstly, we study mathematical properties\nof the ESS as a function of matrices and graphs that parameterize the\ninteraction amongst particles. Secondly, we show how these graphs can be\ninduced by tree data structures which model the logical network topology of an\nabstract distributed computing environment. Thirdly, we present efficient\ndistributed algorithms that achieve the desired ESS control, perform resampling\nand operate on forests associated with these trees. \n\n"}
{"id": "1406.7536", "contents": "Title: Estimating the distribution of Galaxy Morphologies on a continuous space Abstract: The incredible variety of galaxy shapes cannot be summarized by human defined\ndiscrete classes of shapes without causing a possibly large loss of\ninformation. Dictionary learning and sparse coding allow us to reduce the high\ndimensional space of shapes into a manageable low dimensional continuous vector\nspace. Statistical inference can be done in the reduced space via probability\ndistribution estimation and manifold estimation. \n\n"}
{"id": "1406.7648", "contents": "Title: Bayesian Network Constraint-Based Structure Learning Algorithms:\n  Parallel and Optimised Implementations in the bnlearn R Package Abstract: It is well known in the literature that the problem of learning the structure\nof Bayesian networks is very hard to tackle: its computational complexity is\nsuper-exponential in the number of nodes in the worst case and polynomial in\nmost real-world scenarios.\n  Efficient implementations of score-based structure learning benefit from past\nand current research in optimisation theory, which can be adapted to the task\nby using the network score as the objective function to maximise. This is not\ntrue for approaches based on conditional independence tests, called\nconstraint-based learning algorithms. The only optimisation in widespread use,\nbacktracking, leverages the symmetries implied by the definitions of\nneighbourhood and Markov blanket.\n  In this paper we illustrate how backtracking is implemented in recent\nversions of the bnlearn R package, and how it degrades the stability of\nBayesian network structure learning for little gain in terms of speed. As an\nalternative, we describe a software architecture and framework that can be used\nto parallelise constraint-based structure learning algorithms (also implemented\nin bnlearn) and we demonstrate its performance using four reference networks\nand two real-world data sets from genetics and systems biology. We show that on\nmodern multi-core or multiprocessor hardware parallel implementations are\npreferable over backtracking, which was developed when single-processor\nmachines were the norm. \n\n"}
{"id": "1407.1021", "contents": "Title: Soliton Turbulence in Shallow Water Ocean Surface Waves Abstract: We analyze shallow water wind waves in Currituck Sound, North Carolina and\nexperimentally confirm, for the first time, the presence of $soliton$\n$turbulence$ in ocean waves. Soliton turbulence is an exotic form of nonlinear\nwave motion where low frequency energy may also be viewed as a $dense$\n$soliton$ $gas$, described theoretically by the soliton limit of the\nKorteweg-deVries (KdV) equation, a $completely$ $integrable$ $soliton$\n$system$: Hence the phrase \"soliton turbulence\" is synonymous with \"integrable\nsoliton turbulence.\" For periodic/quasiperiodic boundary conditions the\n$ergodic$ $solutions$ of KdV are exactly solvable by $finite$ $gap$ $theory$\n(FGT), the basis of our data analysis. We find that large amplitude measured\nwave trains near the energetic peak of a storm have low frequency power spectra\nthat behave as $\\sim\\omega^{-1}$. We use the linear Fourier transform to\nestimate this power law from the power spectrum and to filter $densely$\n$packed$ $soliton$ $wave$ $trains$ from the data. We apply FGT to determine the\n$soliton$ $spectrum$ and find that the low frequency $\\sim\\omega^{-1}$ region\nis $soliton$ $dominated$. The solitons have $random$ $FGT$ $phases$, a\n$soliton$ $random$ $phase$ $approximation$, which supports our interpretation\nof the data as soliton turbulence. From the $probability$ $density$ $of$ $the$\n$solitons$ we are able to demonstrate that the solitons are $dense$ $in$ $time$\nand $highly$ $non$ $Gaussian$. \n\n"}
{"id": "1407.3634", "contents": "Title: Stable spatial Langmuir solitons as a model of long-lived atmospheric\n  plasma structures Abstract: I study stable spatial Langmuir solitons in plasma based on nonlinear radial\noscillations of charged particles. I discuss two situations when a Langmuir\nsoliton can be stable. In the former case the stability of solitons against the\ncollapse is due to electron-electron interactions which result in the nonlocal\nterms in the nonlinear Schr\\\"{o}dinger equation. In the latter situation I\nderive the new cubic-quintic nonlinear Schr\\\"{o}dinger equation with accounts\nfor the interaction of induced dipole moments of diatomic ions with a rapidly\noscillating electric field and show that the collapse of Langmuir waves can be\nalso arrested. In both cases I find the numerical solutions of the nonlinear\nSchr\\\"{o}dinger equation and analyze their stability using the\nVakhitov-Kolokolov criterion. I discuss the application of my results for the\ndescription of long-lived atmospheric plasma structures. I show that, using my\nmodel, one can explain the existence of atmospheric plasmoids in the upper\nionosphere. It is also demonstrated that Langmuir solitons described by the\ncubic-quintic nonlinear Schr\\\"{o}dinger equation can describe atmospheric\nplasmoids at the initial stages of their evolution. Note that, besides the\nmodeling of atmospheric plasma structures, my results can be applied for the\nexplanation of the results of experiments where long-lived glowing plasmoids\nweere obtained in electric discharges in liquid nitrogen. \n\n"}
{"id": "1407.4211", "contents": "Title: A marginal sampler for $\\sigma$-Stable Poisson-Kingman mixture models Abstract: We investigate the class of $\\sigma$-stable Poisson-Kingman random\nprobability measures (RPMs) in the context of Bayesian nonparametric mixture\nmodeling. This is a large class of discrete RPMs which encompasses most of the\nthe popular discrete RPMs used in Bayesian nonparametrics, such as the\nDirichlet process, Pitman-Yor process, the normalized inverse Gaussian process\nand the normalized generalized Gamma process. We show how certain sampling\nproperties and marginal characterizations of $\\sigma$-stable Poisson-Kingman\nRPMs can be usefully exploited for devising a Markov chain Monte Carlo (MCMC)\nalgorithm for making inference in Bayesian nonparametric mixture modeling.\nSpecifically, we introduce a novel and efficient MCMC sampling scheme in an\naugmented space that has a fixed number of auxiliary variables per iteration.\nWe apply our sampling scheme for a density estimation and clustering tasks with\nunidimensional and multidimensional datasets, and we compare it against\ncompeting sampling schemes. \n\n"}
{"id": "1407.4543", "contents": "Title: Sparse Quadratic Discriminant Analysis and Community Bayes Abstract: We develop a class of rules spanning the range between quadratic discriminant\nanalysis and naive Bayes, through a path of sparse graphical models. A group\nlasso penalty is used to introduce shrinkage and encourage a similar pattern of\nsparsity across precision matrices. It gives sparse estimates of interactions\nand produces interpretable models. Inspired by the connected-components\nstructure of the estimated precision matrices, we propose the community Bayes\nmodel, which partitions features into several conditional independent\ncommunities and splits the classification problem into separate smaller ones.\nThe community Bayes idea is quite general and can be applied to non-Gaussian\ndata and likelihood-based classifiers. \n\n"}
{"id": "1407.8071", "contents": "Title: A simple scheme for the parallelization of particle filters and its\n  application to the tracking of complex stochastic systems Abstract: We investigate the use of possibly the simplest scheme for the\nparallelisation of the standard particle filter, that consists in splitting the\ncomputational budget into $M$ fully independent particle filters with $N$\nparticles each, and then obtaining the desired estimators by averaging over the\n$M$ independent outcomes of the filters. This approach minimises the\nparallelisation overhead yet displays highly desirable theoretical properties.\nUnder very mild assumptions, we analyse the mean square error (MSE) of the\nestimators of 1-dimensional statistics of the optimal filtering distribution\nand show explicitly the effect of parallelisation scheme on the convergence\nrate. Specifically, we study the decomposition of the MSE into variance and\nbias components, to show that the former decays as $\\frac{1}{MN}$, i.e.,\nlinearly with the total number of particles, while the latter converges towards\n$0$ as $\\frac{1}{N^2}$. Parallelisation, therefore, has the obvious advantage\nof dividing the running times while preserving the (asymptotic) performance of\nthe particle filter. Following this lead, we propose a time-error index to\ncompare schemes with different degrees of parallelisation. Finally, we provide\ntwo numerical examples. The first one deals with the tracking of a Lorenz 63\nchaotic system with dynamical noise and partial (noisy) observations, while the\nsecond example involves a dynamical network of modified FitzHugh-Nagumo (FH-N)\nstochastic nodes. The latter is a large dimensional system ($\\approx3,000$\nstate variables in our computer experiments) designed to numerically reproduce\ntypical electrical phenomena observed in the atria of the human heart. In both\nexamples, we show how the proposed parallelisation scheme attains the same\napproximation accuracy as a centralised particle filter with only a small\nfraction of the running time, using a standard multicore computer. \n\n"}
{"id": "1408.1263", "contents": "Title: Comment on \"A test-tube model for rainfall\" by Wilkinson M., EPL 106\n  (2014) 40001 Abstract: This paper is a comment to M Wilkinson, EPL 106 (2014) 40001, arXiv:1401.4620\n[physics.ao-ph,cond-mat.soft], which draws conclusion from our data that are at\nvariance with our observations. \n\n"}
{"id": "1408.2128", "contents": "Title: High-dimensional unsupervised classification via parsimonious\n  contaminated mixtures Abstract: The contaminated Gaussian distribution represents a simple heavy-tailed\nelliptical generalization of the Gaussian distribution; unlike the\noften-considered t-distribution, it also allows for automatic detection of mild\noutlying or \"bad\" points in the same way that observations are typically\nassigned to the groups in the finite mixture model context. Starting from this\ndistribution, we propose the contaminated factor analysis model as a method for\ndimensionality reduction and detection of bad points in higher dimensions. A\nmixture of contaminated Gaussian factor analyzers (MCGFA) model follows\ntherefrom, and extends the recently proposed mixture of contaminated Gaussian\ndistributions to high-dimensional data. We introduce a family of 32\nparsimonious models formed by introducing constraints on the covariance and\ncontamination structures of the general MCGFA model. We outline a variant of\nthe expectation-maximization algorithm for parameter estimation. Various\nimplementation issues are discussed, and the novel family of models is compared\nto well-established approaches on both simulated and real data. \n\n"}
{"id": "1408.2773", "contents": "Title: On Integration Methods Based on Scrambled Nets of Arbitrary Size Abstract: We consider the problem of evaluating $I(\\varphi):=\\int_{[0,1)^s}\\varphi(x)\ndx$ for a function $\\varphi \\in L^2[0,1)^{s}$. In situations where $I(\\varphi)$\ncan be approximated by an estimate of the form\n$N^{-1}\\sum_{n=0}^{N-1}\\varphi(x^n)$, with $\\{x^n\\}_{n=0}^{N-1}$ a point set in\n$[0,1)^s$, it is now well known that the $O_P(N^{-1/2})$ Monte Carlo\nconvergence rate can be improved by taking for $\\{x^n\\}_{n=0}^{N-1}$ the first\n$N=\\lambda b^m$ points, $\\lambda\\in\\{1,\\dots,b-1\\}$, of a scrambled\n$(t,s)$-sequence in base $b\\geq 2$. In this paper we derive a bound for the\nvariance of scrambled net quadrature rules which is of order $o(N^{-1})$\nwithout any restriction on $N$. As a corollary, this bound allows us to provide\nsimple conditions to get, for any pattern of $N$, an integration error of size\n$o_P(N^{-1/2})$ for functions that depend on the quadrature size $N$. Notably,\nwe establish that sequential quasi-Monte Carlo (M. Gerber and N. Chopin, 2015,\n\\emph{J. R. Statist. Soc. B, to appear.}) reaches the $o_P(N^{-1/2})$\nconvergence rate for any values of $N$. In a numerical study, we show that for\nscrambled net quadrature rules we can relax the constraint on $N$ without any\nloss of efficiency when the integrand $\\varphi$ is a discontinuous function\nwhile, for sequential quasi-Monte Carlo, taking $N=\\lambda b^m$ may only\nprovide moderate gains. \n\n"}
{"id": "1408.3058", "contents": "Title: Surface Shear and Persistent Wave Groups Abstract: We investigate the interaction of waves with surface flows by considering the\nfull set of conserved quantities, subtle but important surface elevation\nchanges induced by wave packets and by directly considering the necessary\nforces to prevent packet spreading in the deep water limit. Narrow surface\nshear flows are shown to exert strong localizing and stabilizing forces on\nwavepackets to maintain their strength and amplify their intensity even in the\nlinear regime. Subtle packet scale nonlinear elevation changes from wave motion\nare crucial here and it suggest that popular notions of wave stress and action\nare naive. Quantitative bounds on the surface shear flow necessary to stabilize\npackets of any wave amplitude are given. One implication of this mechanism is\nthat rogue wave stabilization must be due to a purely nonperturbative process. \n\n"}
{"id": "1408.4344", "contents": "Title: Optimal scaling for the pseudo-marginal random walk Metropolis:\n  insensitivity to the noise generating mechanism Abstract: We examine the optimal scaling and the efficiency of the pseudo-marginal\nrandom walk Metropolis algorithm using a recently-derived result on the\nlimiting efficiency as the dimension, $d\\rightarrow \\infty$. We prove that the\noptimal scaling for a given target varies by less than $20\\%$ across a wide\nrange of distributions for the noise in the estimate of the target, and that\nany scaling that is within $20\\%$ of the optimal one will be at least $70\\%$\nefficient. We demonstrate that this phenomenon occurs even outside the range of\ndistributions for which we rigorously prove it. We then conduct a simulation\nstudy on an example with $d=10$ where importance sampling is used to estimate\nthe target density; we also examine results available from an existing\nsimulations study with $d=5$ and where a particle filter was used. Our key\nconclusions are found to hold in these examples also. \n\n"}
{"id": "1408.4663", "contents": "Title: Exploiting Multi-Core Architectures for Reduced-Variance Estimation with\n  Intractable Likelihoods Abstract: Many popular statistical models for complex phenomena are intractable, in the\nsense that the likelihood function cannot easily be evaluated. Bayesian\nestimation in this setting remains challenging, with a lack of computational\nmethodology to fully exploit modern processing capabilities. In this paper we\nintroduce novel control variates for intractable likelihoods that can\ndramatically reduce the Monte Carlo variance of Bayesian estimators. We prove\nthat our control variates are well-defined and provide a positive variance\nreduction. Furthermore we show how to optimise these control variates for\nvariance reduction. The methodology is highly parallel and offers a route to\nexploit multi-core processing architectures that complements recent research in\nthis direction. Indeed, our work shows that it may not be necessary to\nparallelise the sampling process itself in order to harness the potential of\nmassively multi-core architectures. Simulation results presented on the Ising\nmodel, exponential random graph models and non-linear stochastic differential\nequation models support our theoretical findings. \n\n"}
{"id": "1408.6482", "contents": "Title: Conservation Laws and Bounds on the Efficiency of Wind-Wave Growth Abstract: We examine two means by which wind can impart energy to waves: sheltering and\ndeposition of material upwards from windward surface shear. The shear driven\ndeposition is shown to be the more efficient process. Lengthening of waves to\nmatch the wind speed is shown to be very inefficient and consume a large\nfraction of the energy imparted by the wind. The surface shear provides a low\nenergy sink that absorbs most of the momentum from the wind. These produce\nbounds on the efficiency of wave growth. The results here are computed in a\nmodel independent and perturbation free fashion by a careful consideration of\nconservation laws. By combining these effects we can place bounds on the rates\nwaves can grow in a given fetch and the relative amount of shear flow versus\nthe, relatively small, Stokes drift that must arise. \n\n"}
{"id": "1408.6755", "contents": "Title: Quantile-Based Spectral Analysis in an Object-Oriented Framework and a\n  Reference Implementation in R: The quantspec Package Abstract: Quantile-based approaches to the spectral analysis of time series have\nrecently attracted a lot of attention. Despite a growing literature that\ncontains various estimation proposals, no systematic methods for computing the\nnew estimators are available to date. This paper contains two main\ncontributions. First, an extensible framework for quantile-based spectral\nanalysis of time series is developed and documented using object-oriented\nmodels. A comprehensive, open source, reference implementation of this\nframework, the R package quantspec, was recently contributed to CRAN by the\nauthor of this paper. The second contribution of the present paper is to\nprovide a detailed tutorial, with worked examples, to this R package. A reader\nwho is already familiar with quantile-based spectral analysis and whose primary\ninterest is not the design of the quantspec package, but how to use it, can\nread the tutorial and worked examples (Sections 3 and 4) independently. \n\n"}
{"id": "1409.0423", "contents": "Title: Stochastic Climate Theory and Modelling Abstract: Stochastic methods are a crucial area in contemporary climate research and\nare increasingly being used in comprehensive weather and climate prediction\nmodels as well as reduced order climate models. Stochastic methods are used as\nsubgrid-scale parameterizations as well as for model error representation,\nuncertainty quantification, data assimilation and ensemble prediction. The need\nto use stochastic approaches in weather and climate models arises because we\nstill cannot resolve all necessary processes and scales in comprehensive\nnumerical weather and climate prediction models. In many practical applications\none is mainly interested in the largest and potentially predictable scales and\nnot necessarily in the small and fast scales. For instance, reduced order\nmodels can simulate and predict large scale modes. Statistical mechanics and\ndynamical systems theory suggest that in reduced order models the impact of\nunresolved degrees of freedom can be represented by suitable combinations of\ndeterministic and stochastic components and non-Markovian (memory) terms.\nStochastic approaches in numerical weather and climate prediction models also\nlead to the reduction of model biases. Hence, there is a clear need for\nsystematic stochastic approaches in weather and climate modelling. In this\nreview we present evidence for stochastic effects in laboratory experiments.\nThen we provide an overview of stochastic climate theory from an applied\nmathematics perspectives. We also survey the current use of stochastic methods\nin comprehensive weather and climate prediction models and show that stochastic\nparameterizations have the potential to remedy many of the current biases in\nthese comprehensive models. \n\n"}
{"id": "1409.2817", "contents": "Title: Ribbon Turbulence Abstract: We investigate the non-linear equilibration of a two-layer quasi-geostrophic\nflow in a channel forced by an imposed unstable zonal mean flow, paying\nparticular attention to the role of bottom friction. In the limit of low bottom\nfriction, classical theory of geostrophic turbulence predicts an inverse\ncascade of kinetic energy in the horizontal with condensation at the domain\nscale and barotropization on the vertical. By contrast, in the limit of large\nbottom friction, the flow is dominated by ribbons of high kinetic energy in the\nupper layer. These ribbons correspond to meandering jets separating regions of\nhomogenized potential vorticity. We interpret these result by taking advantage\nof the peculiar conservation laws satisfied by this system: the dynamics can be\nrecast in such a way that the imposed mean flow appears as an initial source of\npotential vorticity levels in the upper layer. The initial baroclinic\ninstability leads to a turbulent flow that stirs this potential vorticity field\nwhile conserving the global distribution of potential vorticity levels.\nStatistical mechanical theory of the 1-1/2 layer quasi-geostrophic model\npredict the formation of two regions of homogenized potential vorticity\nseparated by a minimal interface. We show that the dynamics of the ribbons\nresults from a competition between a tendency to reach this equilibrium state,\nand baroclinic instability that induces meanders of the interface. These\nmeanders intermittently break and induce potential vorticity mixing, but the\ninterface remains sharp throughout the flow evolution. We show that for some\nparameter regimes, the ribbons act as a mixing barrier which prevent relaxation\ntoward equilibrium, favouring the emergence of multiple zonal jets. \n\n"}
{"id": "1409.3353", "contents": "Title: Validation of Danish wind time series from a new global renewable energy\n  atlas for energy system analysis Abstract: We present a new high-resolution global renewable energy atlas ({REatlas})\nthat can be used to calculate customised hourly time series of wind and solar\nPV power generation. In this paper, the atlas is applied to produce\n32-year-long hourly model wind power time series for Denmark for each\nhistorical and future year between 1980 and 2035. These are calibrated and\nvalidated against real production data from the period 2000 to 2010. The high\nnumber of years allows us to discuss how the characteristics of Danish wind\npower generation varies between individual weather years. As an example, the\nannual energy production is found to vary by $\\pm10\\%$ from the average.\nFurthermore, we show how the production pattern change as small onshore\nturbines are gradually replaced by large onshore and offshore turbines.\nFinally, we compare our wind power time series for 2020 to corresponding data\nfrom a handful of Danish energy system models. The aim is to illustrate how\ncurrent differences in model wind may result in significant differences in\ntechnical and economical model predictions. These include up to $15\\%$\ndifferences in installed capacity and $40\\%$ differences in system reserve\nrequirements. \n\n"}
{"id": "1409.3836", "contents": "Title: Hardness of parameter estimation in graphical models Abstract: We consider the problem of learning the canonical parameters specifying an\nundirected graphical model (Markov random field) from the mean parameters. For\ngraphical models representing a minimal exponential family, the canonical\nparameters are uniquely determined by the mean parameters, so the problem is\nfeasible in principle. The goal of this paper is to investigate the\ncomputational feasibility of this statistical task. Our main result shows that\nparameter estimation is in general intractable: no algorithm can learn the\ncanonical parameters of a generic pair-wise binary graphical model from the\nmean parameters in time bounded by a polynomial in the number of variables\n(unless RP = NP). Indeed, such a result has been believed to be true (see the\nmonograph by Wainwright and Jordan (2008)) but no proof was known.\n  Our proof gives a polynomial time reduction from approximating the partition\nfunction of the hard-core model, known to be hard, to learning approximate\nparameters. Our reduction entails showing that the marginal polytope boundary\nhas an inherent repulsive property, which validates an optimization procedure\nover the polytope that does not use any knowledge of its structure (as required\nby the ellipsoid method and others). \n\n"}
{"id": "1409.4735", "contents": "Title: Vertically Driven Waves: Energy Transfer Between Gravity Waves Revisited Abstract: We investigate the energy transfer from large waves to small ones through\nvertical acceleration and demonstrate that this is a much larger effect than\nthat of the potential energy changes of the small waves moving over the larger\nones. Rates of exponential growth for this process are given and limits on the\nstable size of small waves in the horizontal accelerations from the larger ones\nare derived. We discuss the possibility of this being a manifestation of the\nBenjamin-Feir instability. \n\n"}
{"id": "1409.5191", "contents": "Title: Hamiltonian Monte Carlo Without Detailed Balance Abstract: We present a method for performing Hamiltonian Monte Carlo that largely\neliminates sample rejection for typical hyperparameters. In situations that\nwould normally lead to rejection, instead a longer trajectory is computed until\na new state is reached that can be accepted. This is achieved using Markov\nchain transitions that satisfy the fixed point equation, but do not satisfy\ndetailed balance. The resulting algorithm significantly suppresses the random\nwalk behavior and wasted function evaluations that are typically the\nconsequence of update rejection. We demonstrate a greater than factor of two\nimprovement in mixing time on three test problems. We release the source code\nas Python and MATLAB packages. \n\n"}
{"id": "1409.7287", "contents": "Title: Identification of jump Markov linear models using particle filters Abstract: Jump Markov linear models consists of a finite number of linear state space\nmodels and a discrete variable encoding the jumps (or switches) between the\ndifferent linear models. Identifying jump Markov linear models makes for a\nchallenging problem lacking an analytical solution. We derive a new expectation\nmaximization (EM) type algorithm that produce maximum likelihood estimates of\nthe model parameters. Our development hinges upon recent progress in combining\nparticle filters with Markov chain Monte Carlo methods in solving the nonlinear\nstate smoothing problem inherent in the EM formulation. Key to our development\nis that we exploit a conditionally linear Gaussian substructure in the model,\nallowing for an efficient algorithm. \n\n"}
{"id": "1409.7715", "contents": "Title: Parameter inference and model selection in deterministic and stochastic\n  dynamical models via approximate Bayesian computation: modeling a wildlife\n  epidemic Abstract: We consider the problem of selecting deterministic or stochastic models for a\nbiological, ecological, or environmental dynamical process. In most cases, one\nprefers either deterministic or stochastic models as candidate models based on\nexperience or subjective judgment. Due to the complex or intractable likelihood\nin most dynamical models, likelihood-based approaches for model selection are\nnot suitable. We use approximate Bayesian computation for parameter estimation\nand model selection to gain further understanding of the dynamics of two\nepidemics of chronic wasting disease in mule deer. The main novel contribution\nof this work is that under a hierarchical model framework we compare three\ntypes of dynamical models: ordinary differential equation, continuous time\nMarkov chain, and stochastic differential equation models. To our knowledge\nmodel selection between these types of models has not appeared previously.\nSince the practice of incorporating dynamical models into data models is\nbecoming more common, the proposed approach may be very useful in a variety of\napplications. \n\n"}
{"id": "1409.8502", "contents": "Title: Combining Particle MCMC with Rao-Blackwellized Monte Carlo Data\n  Association for Parameter Estimation in Multiple Target Tracking Abstract: We consider state and parameter estimation in multiple target tracking\nproblems with data association uncertainties and unknown number of targets. We\nshow how the problem can be recast into a conditionally linear Gaussian\nstate-space model with unknown parameters and present an algorithm for\ncomputationally efficient inference on the resulting model. The proposed\nalgorithm is based on combining the Rao-Blackwellized Monte Carlo data\nassociation algorithm with particle Markov chain Monte Carlo algorithms to\njointly estimate both parameters and data associations. Both particle marginal\nMetropolis-Hastings and particle Gibbs variants of particle MCMC are\nconsidered. We demonstrate the performance of the method both using simulated\ndata and in a real-data case study of using multiple target tracking to\nestimate the brown bear population in Finland. \n\n"}
{"id": "1410.0524", "contents": "Title: Likelihood free inference for Markov processes: a comparison Abstract: Approaches to Bayesian inference for problems with intractable likelihoods\nhave become increasingly important in recent years. Approximate Bayesian\ncomputation (ABC) and \"likelihood free\" Markov chain Monte Carlo techniques are\npopular methods for tackling inference in these scenarios but such techniques\nare computationally expensive. In this paper we compare the two approaches to\ninference, with a particular focus on parameter inference for stochastic\nkinetic models, widely used in systems biology. Discrete time transition\nkernels for models of this type are intractable for all but the most trivial\nsystems yet forward simulation is usually straightforward. We discuss the\nrelative merits and drawbacks of each approach whilst considering the\ncomputational cost implications and efficiency of these techniques. In order to\nexplore the properties of each approach we examine a range of observation\nregimes using two example models. We use a Lotka--Volterra predator prey model\nto explore the impact of full or partial species observations using various\ntime course observations under the assumption of known and unknown measurement\nerror. Further investigation into the impact of observation error is then made\nusing a Schl\\\"ogl system, a test case which exhibits bi-modal state stability\nin some regions of parameter space. \n\n"}
{"id": "1410.1101", "contents": "Title: Sequential Monte Carlo Samplers for capital allocation under\n  copula-dependent risk models Abstract: In this paper we assume a multivariate risk model has been developed for a\nportfolio and its capital derived as a homogeneous risk measure. The Euler (or\ngradient) principle, then, states that the capital to be allocated to each\ncomponent of the portfolio has to be calculated as an expectation conditional\nto a rare event, which can be challenging to evaluate in practice. We exploit\nthe copula-dependence within the portfolio risks to design a Sequential Monte\nCarlo Samplers based estimate to the marginal conditional expectations involved\nin the problem, showing its efficiency through a series of computational\nexamples. \n\n"}
{"id": "1410.2344", "contents": "Title: Lagrangian analysis of the vertical structure of eddies simulated in the\n  Japan Basin of the Japan/East Sea Abstract: The output from an eddy-resolved multi-layered circulation model is used to\nanalyze the vertical structure of simulated deep-sea eddies in the Japan Basin\nof the Japan/East Sea constrained by bottom topography. We focus on Lagrangian\nanalysis of anticyclonic eddies, generated in the model in a typical year\napproximately at the place of the mooring and the hydrographic sections, where\nsuch eddies have been regularly observed in different years (1993--1997,\n1999--2001). Using a quasi-3D computation of the finite-time Lyapunov exponents\nand displacements for a large number of synthetic tracers in each depth layer,\nwe demonstrate how the simulated feature evolves of the eddy, that does not\nreach the surface in summer, into a one reaching the surface in fall. This\nfinding is confirmed by computing deformation of the model layers across the\nsimulated eddy in zonal and meridional directions and in the corresponding\ntemperature cross sections. Computed Lagrangian tracking maps allow to trace\nthe origin and fate of water masses in different layers of the eddy. The\nresults of simulation are compared with observed temperature zonal and\nmeridional cross sections of a real anticyclonic eddy to be studied at that\nplace during the oceanographic Conductivity, Temperature, and Depth (CTD)\nhydrochemical survey in summer 1999. Both the simulated and observed eddies are\nshown to have the similar eddy core and the relief of layer interfaces and\nisotherms. \n\n"}
{"id": "1410.2707", "contents": "Title: Applying Geospatial Semantic Array Programming for a Reproducible Set of\n  Bioclimatic Indices in Europe Abstract: Bioclimate-driven regression analysis is a widely used approach for modelling\necological niches and zonation. Although the bioclimatic complexity of the\nEuropean continent is high, a particular combination of 12 climatic and\ntopographic covariates was recently found able to reliably reproduce the\necological zoning of the Food and Agriculture Organization of the United\nNations (FAO) for forest resources assessment at pan-European scale, generating\nthe first fuzzy similarity map of FAO ecozones in Europe. The reproducible\nprocedure followed to derive this collection of bioclimatic indices is now\npresented. It required an integration of data-transformation modules (D-TM)\nusing geospatial tools such as Geographic Information System (GIS) software,\nand array-based mathematical implementation such as semantic array programming\n(SemAP). Base variables, intermediate and final covariates are described and\nsemantically defined by providing the workflow of D-TMs and the mathematical\nformulation following the SemAP notation. Source layers to derive base\nvariables were extracted by exclusively relying on global-scale public open\ngeodata in order for the same set of bioclimatic covariates to be reproducible\nin any region worldwide. In particular, two freely available datasets were\nexploited for temperature and precipitation (WorldClim) and elevation (Global\nMulti-resolution Terrain Elevation Data). The working extent covers the\nEuropean continent to the Urals with a resolution of 30 arc-second. The\nproposed set of bioclimatic covariates will be made available as open data in\nthe European Forest Data Centre (EFDAC). The forthcoming complete set of D-TM\ncodelets will enable the 12 covariates to be easily reproduced and expanded\nthrough free software. \n\n"}
{"id": "1410.4070", "contents": "Title: Modulational instability in wind-forced waves Abstract: We consider the wind-forced nonlinear Schroedinger (NLS) equation obtained in\nthe potential flow framework when the Miles growth rate is of the order of the\nwave steepness. In this case, the form of the wind-forcing terms gives rise to\nthe enhancement of the modulational instability and to a band of positive gain\nwith infinite width. This regime is characterised by the fact that the ratio\nbetween wave momentum and norm is not a constant of motion, in contrast to what\nhappens in the standard case where the Miles growth rate is of the order of the\nsteepness squared. \n\n"}
{"id": "1410.4217", "contents": "Title: Sequential Importance Sampling for Two-dimensional Ising Models Abstract: In recent years, sequential importance sampling (SIS) has been well developed\nfor sampling contingency tables with linear constraints. In this paper, we\napply SIS procedure to 2-dimensional Ising models, which give observations of\n0-1 tables and include both linear and quadratic constraints. We show how to\ncompute bounds for specific cells by solving linear programming (LP) problems\nover cut polytopes to reduce rejections. The computational results, which\nincludes both simulations and real data analysis, suggest that our method\nperforms very well for sparse tables and when the 1's are spread out: the\ncomputational times are short, the acceptance rates are high, and if proper\ntests are used then in most cases our conclusions are theoretically reasonable. \n\n"}
{"id": "1410.4231", "contents": "Title: Convergence properties of weighted particle islands with application to\n  the double bootstrap algorithm Abstract: Particle island models (Verg\\'e et al., 2013) provide a means of\nparallelization of sequential Monte Carlo methods, and in this paper we present\nnovel convergence results for algorithms of this sort. In particular we\nestablish a central limit theorem - as the number of islands and the common\nsize of the islands tend jointly to infinity - of the double bootstrap\nalgorithm with possibly adaptive selection on the island level. For this\npurpose we introduce a notion of archipelagos of weighted islands and find\nconditions under which a set of convergence properties are preserved by\ndifferent operations on such archipelagos. This theory allows arbitrary\ncompositions of these operations to be straightforwardly analyzed, providing a\nvery flexible framework covering the double bootstrap algorithm as a special\ncase. Finally, we establish the long-term numerical stability of the double\nbootstrap algorithm by bounding its asymptotic variance under weak and easily\nchecked assumptions satisfied for a wide range of models with possibly\nnon-compact state space. \n\n"}
{"id": "1410.4812", "contents": "Title: Inference and Mixture Modeling with the Elliptical Gamma Distribution Abstract: We study modeling and inference with the Elliptical Gamma Distribution (EGD).\nWe consider maximum likelihood (ML) estimation for EGD scatter matrices, a task\nfor which we develop new fixed-point algorithms. Our algorithms are efficient\nand converge to global optima despite nonconvexity. Moreover, they turn out to\nbe much faster than both a well-known iterative algorithm of Kent & Tyler\n(1991) and sophisticated manifold optimization algorithms. Subsequently, we\ninvoke our ML algorithms as subroutines for estimating parameters of a mixture\nof EGDs. We illustrate our methods by applying them to model natural image\nstatistics---the proposed EGD mixture model yields the most parsimonious model\namong several competing approaches. \n\n"}
{"id": "1410.5392", "contents": "Title: Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models Abstract: Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices. \n\n"}
{"id": "1410.5722", "contents": "Title: Optimisation of an idealised ocean model, stochastic parameterisation of\n  sub-grid eddies Abstract: An optimisation scheme is developed to accurately represent the sub-grid\nscale forcing of a high dimensional chaotic ocean system. Using a simple\nparameterisation scheme, the velocity components of a 30km resolution shallow\nwater ocean model are optimised to have the same climatological mean and\nvariance as that of a less viscous 7.5km resolution model. The 5 day\nlag-covariance is also optimised, leading to a more accurate estimate of the\nhigh resolution response to forcing using the low resolution model.\n  The system considered is an idealised barotropic double gyre that is chaotic\nat both resolutions. Using the optimisation scheme, we find and apply the\nconstant in time, but spatially varying, forcing term that is equal to the time\nintegrated forcing of the sub-mesoscale eddies. A linear stochastic term,\nindependent of the large-scale flow, with no spatial correlation but a\nspatially varying amplitude and time scale is used to represent the transient\neddies. The climatological mean, variance and 5 day lag-covariance of the\nvelocity from a single high resolution integration is used to provide an\noptimisation target. No other high resolution statistics are required.\nAdditional programming effort, for example to build a tangent linear or adjoint\nmodel, is not required either.\n  The focus of this paper is on the optimisation scheme and the accuracy of the\noptimised flow. The method can be applied in future investigations into the\nphysical processes that govern barotropic turbulence and it can perhaps be\napplied to help understand and correct biases in the mean and variance of a\nmore realistic coarse or eddy-permitting ocean model. The method is\ncomplementary to current parameterisations and can be applied at the same time\nwithout modification. \n\n"}
{"id": "1410.6460", "contents": "Title: Markov Chain Monte Carlo and Variational Inference: Bridging the Gap Abstract: Recent advances in stochastic gradient variational inference have made it\npossible to perform variational Bayesian inference with posterior\napproximations containing auxiliary random variables. This enables us to\nexplore a new synthesis of variational inference and Monte Carlo methods where\nwe incorporate one or more steps of MCMC into our variational approximation. By\ndoing so we obtain a rich class of inference algorithms bridging the gap\nbetween variational methods and MCMC, and offering the best of both worlds:\nfast posterior approximation through the maximization of an explicit objective,\nwith the option of trading off additional computation for additional accuracy.\nWe describe the theoretical foundations that make this possible and show some\npromising first results. \n\n"}
{"id": "1410.6466", "contents": "Title: Model Selection for Topic Models via Spectral Decomposition Abstract: Topic models have achieved significant successes in analyzing large-scale\ntext corpus. In practical applications, we are always confronted with the\nchallenge of model selection, i.e., how to appropriately set the number of\ntopics. Following recent advances in topic model inference via tensor\ndecomposition, we make a first attempt to provide theoretical analysis on model\nselection in latent Dirichlet allocation. Under mild conditions, we derive the\nupper bound and lower bound on the number of topics given a text collection of\nfinite size. Experimental results demonstrate that our bounds are accurate and\ntight. Furthermore, using Gaussian mixture model as an example, we show that\nour methodology can be easily generalized to model selection analysis for other\nlatent models. \n\n"}
{"id": "1410.7659", "contents": "Title: Learning graphical models from the Glauber dynamics Abstract: In this paper we consider the problem of learning undirected graphical models\nfrom data generated according to the Glauber dynamics. The Glauber dynamics is\na Markov chain that sequentially updates individual nodes (variables) in a\ngraphical model and it is frequently used to sample from the stationary\ndistribution (to which it converges given sufficient time). Additionally, the\nGlauber dynamics is a natural dynamical model in a variety of settings. This\nwork deviates from the standard formulation of graphical model learning in the\nliterature, where one assumes access to i.i.d. samples from the distribution.\n  Much of the research on graphical model learning has been directed towards\nfinding algorithms with low computational cost. As the main result of this\nwork, we establish that the problem of reconstructing binary pairwise graphical\nmodels is computationally tractable when we observe the Glauber dynamics.\nSpecifically, we show that a binary pairwise graphical model on $p$ nodes with\nmaximum degree $d$ can be learned in time $f(d)p^2\\log p$, for a function\n$f(d)$, using nearly the information-theoretic minimum number of samples. \n\n"}
{"id": "1411.0086", "contents": "Title: High-order Composite Likelihood Inference for Max-Stable Distributions\n  and Processes Abstract: In multivariate or spatial extremes, inference for max-stable processes\nobserved at a large collection of locations is among the most challenging\nproblems in computational statistics, and current approaches typically rely on\nless expensive composite likelihoods constructed from small subsets of data. In\nthis work, we explore the limits of modern state-of-the-art computational\nfacilities to perform full likelihood inference and to efficiently evaluate\nhigh-order composite likelihoods. With extensive simulations, we assess the\nloss of information of composite likelihood estimators with respect to a full\nlikelihood approach for some widely-used multivariate or spatial extreme\nmodels, we discuss how to choose composite likelihood truncation to improve the\nefficiency, and we also provide recommendations for practitioners. \n\n"}
{"id": "1411.0416", "contents": "Title: Spatio-Temporal Analysis of Epidemic Phenomena Using the R Package\n  surveillance Abstract: The availability of geocoded health data and the inherent temporal structure\nof communicable diseases have led to an increased interest in statistical\nmodels and software for spatio-temporal data with epidemic features. The open\nsource R package surveillance can handle various levels of aggregation at which\ninfective events have been recorded: individual-level time-stamped\ngeo-referenced data (case reports) in either continuous space or discrete\nspace, as well as counts aggregated by period and region. For each of these\ndata types, the surveillance package implements tools for visualization,\nlikelihoood inference and simulation from recently developed statistical\nregression frameworks capturing endemic and epidemic dynamics. Altogether, this\npaper is a guide to the spatio-temporal modeling of epidemic phenomena,\nexemplified by analyses of public health surveillance data on measles and\ninvasive meningococcal disease. \n\n"}
{"id": "1411.1314", "contents": "Title: Computation of Gaussian orthant probabilities in high dimension Abstract: We study the computation of Gaussian orthant probabilities, i.e. the\nprobability that a Gaussian falls inside a quadrant. The\nGeweke-Hajivassiliou-Keane (GHK) algorithm [Genz, 1992; Geweke, 1991;\nHajivassiliou et al., 1996; Keane, 1993], is currently used for integrals of\ndimension greater than 10. In this paper we show that for Markovian covariances\nGHK can be interpreted as the estimator of the normalizing constant of a state\nspace model using sequential importance sampling (SIS). We show for an AR(1)\nthe variance of the GHK, properly normalized, diverges exponentially fast with\nthe dimension. As an improvement we propose using a particle filter (PF). We\nthen generalize this idea to arbitrary covariance matrices using Sequential\nMonte Carlo (SMC) with properly tailored MCMC moves. We show empirically that\nthis can lead to drastic improvements on currently used algorithms. We also\nextend the framework to orthants of mixture of Gaussians (Student, Cauchy\netc.), and to the simulation of truncated Gaussians. \n\n"}
{"id": "1411.1451", "contents": "Title: Modelling extremes using approximate Bayesian Computation Abstract: By the nature of their construction, many statistical models for extremes\nresult in likelihood functions that are computationally prohibitive to\nevaluate. This is consequently problematic for the purposes of likelihood-based\ninference. With a focus on the Bayesian framework, this chapter examines the\nuse of approximate Bayesian computation (ABC) techniques for the fitting and\nanalysis of statistical models for extremes. After introducing the ideas behind\nABC algorithms and methods, we demonstrate their application to extremal models\nin stereology and spatial extremes. \n\n"}
{"id": "1411.1702", "contents": "Title: Vectorized and Parallel Particle Filter SMC Parameter Estimation for\n  Stiff ODEs Abstract: Particle filter (PF) sequential Monte Carlo (SMC) methods are very attractive\nfor the estimation of parameters of time dependent systems where the data is\neither not all available at once, or the range of time constants is wide enough\nto create problems in the numerical time propagation of the states. The need to\nevolve a large number of particles makes PF-based methods computationally\nchallenging, the main bottlenecks being the time propagation of each particle\nand the large number of particles. While parallelization is typically advocated\nto speed up the computing time, vectorization of the algorithm on a single\nprocessor may result in even larger speedups for certain problems. In this\npaper we present a formulation of the PF-SMC class of algorithms proposed in\nArnold et al. (2013), which is particularly amenable to a parallel or\nvectorized computing environment, and we illustrate the performance with a few\ncomputed examples in MATLAB. \n\n"}
{"id": "1411.2256", "contents": "Title: Data-Driven Prediction of Thresholded Time Series of Rainfall and SOC\n  models Abstract: We study the occurrence of events, subject to threshold, in a representative\nSOC sandpile model and in high-resolution rainfall data. The predictability in\nboth systems is analyzed by means of a decision variable sensitive to event\nclustering, and the quality of the predictions is evaluated by the receiver\noperating characteristics (ROC) method. In the case of the SOC sandpile model,\nthe scaling of quiet-time distributions with increasing threshold leads to\nincreased predictability of extreme events. A scaling theory allows us to\nunderstand all the details of the prediction procedure and to extrapolate the\nshape of the ROC curves for the most extreme events. For rainfall data, the\nquiet-time distributions do not scale for high thresholds, which means that the\ncorresponding ROC curves cannot be straightforwardly related to those for lower\nthresholds. \n\n"}
{"id": "1411.3688", "contents": "Title: Dimension-independent likelihood-informed MCMC Abstract: Many Bayesian inference problems require exploring the posterior distribution\nof high-dimensional parameters that represent the discretization of an\nunderlying function. This work introduces a family of Markov chain Monte Carlo\n(MCMC) samplers that can adapt to the particular structure of a posterior\ndistribution over functions. Two distinct lines of research intersect in the\nmethods developed here. First, we introduce a general class of\noperator-weighted proposal distributions that are well defined on function\nspace, such that the performance of the resulting MCMC samplers is independent\nof the discretization of the function. Second, by exploiting local Hessian\ninformation and any associated low-dimensional structure in the change from\nprior to posterior distributions, we develop an inhomogeneous discretization\nscheme for the Langevin stochastic differential equation that yields\noperator-weighted proposals adapted to the non-Gaussian structure of the\nposterior. The resulting dimension-independent, likelihood-informed (DILI) MCMC\nsamplers may be useful for a large class of high-dimensional problems where the\ntarget probability measure has a density with respect to a Gaussian reference\nmeasure. Two nonlinear inverse problems are used to demonstrate the efficiency\nof these DILI samplers: an elliptic PDE coefficient inverse problem and path\nreconstruction in a conditioned diffusion. \n\n"}
{"id": "1411.3921", "contents": "Title: Inference for Trans-dimensional Bayesian Models with Diffusive Nested\n  Sampling Abstract: Many inference problems involve inferring the number $N$ of components in\nsome region, along with their properties $\\{\\mathbf{x}_i\\}_{i=1}^N$, from a\ndataset $\\mathcal{D}$. A common statistical example is finite mixture\nmodelling. In the Bayesian framework, these problems are typically solved using\none of the following two methods: i) by executing a Monte Carlo algorithm (such\nas Nested Sampling) once for each possible value of $N$, and calculating the\nmarginal likelihood or evidence as a function of $N$; or ii) by doing a single\nrun that allows the model dimension $N$ to change (such as Markov Chain Monte\nCarlo with birth/death moves), and obtaining the posterior for $N$ directly. In\nthis paper we present a general approach to this problem that uses\ntrans-dimensional MCMC embedded within a Nested Sampling algorithm, allowing us\nto explore the posterior distribution and calculate the marginal likelihood\n(summed over $N$) even if the problem contains a phase transition or other\ndifficult features such as multimodality. We present two example problems,\nfinding sinusoidal signals in noisy data, and finding and measuring galaxies in\na noisy astronomical image. Both of the examples demonstrate phase transitions\nin the relationship between the likelihood and the cumulative prior mass,\nhighlighting the need for Nested Sampling. \n\n"}
{"id": "1411.4784", "contents": "Title: Theory of ball lightning Abstract: We present a comprehensive theory on the formation of ball lightning, a\nluminous sphere sometimes observed after normal lightning. In a ball lightning\nevent, a relativistic electron bunch can be produced by the stepped leader of\nlightning and coherently emit high-power microwave when striking the ground.\nThe intense microwave ionizes the local air and evacuates the resulting plasma\nby its radiation pressure, thereby forming a spherical plasma cavity that traps\nthe microwave. The theory can explain observed properties of ball lightning and\nshould be useful to lightning protection and aviation safety. \n\n"}
{"id": "1411.5519", "contents": "Title: A plethora of generalised solitary gravity-capillary water waves Abstract: The present study describes, first, an efficient algorithm for computing\ncapillary-gravity solitary waves solutions of the irrotational Euler equations\nwith a free surface and, second, provides numerical evidences of the existence\nof an infinite number of generalised solitary waves (solitary waves with\nundamped oscillatory wings). Using conformal mapping, the unknown fluid domain,\nwhich is to be determined, is mapped into a uniform strip of the complex plane.\nIn the transformed domain, a Babenko-like equation is then derived and solved\nnumerically. \n\n"}
{"id": "1411.6370", "contents": "Title: Big Learning with Bayesian Methods Abstract: Explosive growth in data and availability of cheap computing resources have\nsparked increasing interest in Big learning, an emerging subfield that studies\nscalable machine learning algorithms, systems, and applications with Big Data.\nBayesian methods represent one important class of statistic methods for machine\nlearning, with substantial recent developments on adaptive, flexible and\nscalable Bayesian learning. This article provides a survey of the recent\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\nincluding nonparametric Bayesian methods for adaptively inferring model\ncomplexity, regularized Bayesian inference for improving the flexibility via\nposterior regularization, and scalable algorithms and systems based on\nstochastic subsampling and distributed computing for dealing with large-scale\napplications. \n\n"}
{"id": "1411.6902", "contents": "Title: Most probable paths in temporal weighted networks: An application to\n  ocean transport Abstract: We consider paths in weighted and directed temporal networks, introducing\ntools to compute sets of paths of high probability. We quantify the relative\nimportance of the most probable path between two nodes with respect to the\nwhole set of paths, and to a subset of highly probable paths which incorporate\nmost of the connection probability. These concepts are used to provide\nalternative definitions of betweenness centrality. We apply our formalism to a\ntransport network describing surface flow in the Mediterranean sea. Despite the\nfull transport dynamics is described by a very large number of paths we find\nthat, for realistic time scales, only a very small subset of high probability\npaths (or even a single most probable one) is enough to characterize global\nconnectivity properties of the network. \n\n"}
{"id": "1411.7009", "contents": "Title: Additive Gaussian Process Regression Abstract: Additive-interactive regression has recently been shown to offer attractive\nminimax error rates over traditional nonparametric multivariate regression in a\nwide variety of settings, including cases where the predictor count is much\nlarger than the sample size and many of the predictors have important effects\non the response, potentially through complex interactions. We present a\nBayesian implementation of additive-interactive regression using an additive\nGaussian process (AGP) prior and develop an efficient Markov chain sampler that\nextends stochastic search variable selection in this setting. Careful prior and\nhyper-parameter specification are developed in light of performance and\ncomputational considerations, and key innovations address difficulties in\nexploring a joint posterior distribution over multiple subsets of high\ndimensional predictor inclusion vectors. The method offers state-of-the-art\nsupport and interaction recovery while improving dramatically over competitors\nin terms of prediction accuracy on a diverse set of simulated and real data.\nResults from real data studies provide strong evidence that the\nadditive-interactive framework is an attractive modeling platform for\nhigh-dimensional nonparametric regression. \n\n"}
{"id": "1412.0250", "contents": "Title: Discussion on the spectral coherence between planetary, solar and\n  climate oscillations: a reply to some critiques Abstract: During the last few years a number of works have proposed that planetary\nharmonics regulate solar oscillations and the Earth climate. Herein I address\nsome critiques. Detailed analysis of the data do support the planetary theory\nof solar and climate variation. In particular, I show that: (1) high-resolution\ncosmogenic 10Be and 14C solar activity proxy records both during the Holocene\nand during the Marine Interglacial Stage 9.3 (MIS 9.3), 325-336 kyr ago,\npresent four common spectral peaks at about 103, 115, 130 and 150 yrs (this is\nthe frequency band that generates Maunder and Dalton like grand solar minima)\nthat can be deduced from a simple solar model based on a generic non-linear\ncoupling between planetary and solar harmonics; (2) time-frequency analysis and\nadvanced minimum variance distortion-less response (MVDR) magnitude squared\ncoherence analysis confirm the existence of persistent astronomical harmonics\nin the climate records at the decadal and multidecadal scales when used with an\nappropriate window length (110 years) to guarantee a sufficient spectral\nresolution. However, the best coherence test can be currently made only by\ncomparing directly the temperature and astronomical spectra as done in Scafetta\n(J. Atmos. Sol. Terr. Phys. 72(13), 951-970, 2010). The spectral coherence\nbetween planetary, solar and climatic oscillations is confirmed at the\nfollowing periods: 5.2 yr, 5.93 yr, 6.62 yr, 7.42 yr, 9.1 yr (main lunar tidal\ncycle), 10.4 yr (related to the 9.93-10.87-11.86 yr solar cycle harmonics),\n13.8-15.0 yr, 20 yr, 30 yr and 61 yr, 103 yr, 115 yr, 130 yr, 150 yr and about\n1000 year. This work responds to the critiques of Cauquoin et al. (Astron.\nAstrophys. 561, A132, 2014) who ignored alternative planetary theories of solar\nvariations, and of Holm (J. Atmos. Sol. Terr. Phys. 110-111, 23-27, 2014) who\nused inadequate physical and time frequency analysis of the data. \n\n"}
{"id": "1412.2129", "contents": "Title: An iterative step-function estimator for graphons Abstract: Exchangeable graphs arise via a sampling procedure from measurable functions\nknown as graphons. A natural estimation problem is how well we can recover a\ngraphon given a single graph sampled from it. One general framework for\nestimating a graphon uses step-functions obtained by partitioning the nodes of\nthe graph according to some clustering algorithm. We propose an iterative\nstep-function estimator (ISFE) that, given an initial partition, iteratively\nclusters nodes based on their edge densities with respect to the previous\niteration's partition. We analyze ISFE and demonstrate its performance in\ncomparison with other graphon estimation techniques. \n\n"}
{"id": "1412.2325", "contents": "Title: Could the Earth's surface Ultraviolet irradiance be blamed for the\n  global warming? (II) ----Ozone layer depth reconstruction via HEWV effect Abstract: It is suggested by Chen {\\it et al.} that the Earth's surface Ultraviolet\nirradiance ($280-400$ nm) could influence the Earth's surface temperature\nvariation by \"Highly Excited Water Vapor\" (HEWV) effect. In this manuscript, we\nreconstruct the developing history of the ozone layer depth variation from 1860\nto 2011 based on the HEWV effect. It is shown that the reconstructed ozone\nlayer depth variation correlates with the observational variation from 1958 to\n2005 very well ($R=0.8422$, $P>99.9\\%$). From this reconstruction, we may limit\nthe spectra band of the surface Ultraviolet irradiance referred in HEWV effect\nto Ultraviolet B ($280-320$ nm). \n\n"}
{"id": "1412.2844", "contents": "Title: Optimal Reduced Isotonic Regression Abstract: Isotonic regression is a shape-constrained nonparametric regression in which\nthe regression is an increasing step function. For $n$ data points, the number\nof steps in the isotonic regression may be as large as $n$. As a result,\nstandard isotonic regression has been criticized as overfitting the data or\nmaking the representation too complicated. So-called \"reduced\" isotonic\nregression constrains the outcome to be a specified number of steps $b$, $b\n\\leq n$. However, because the previous algorithms for finding the reduced $L_2$\nregression took $\\Theta(n+bm^2)$ time, where $m$ is the number of steps of the\nunconstrained isotonic regression, researchers felt that the algorithms were\ntoo slow and instead used approximations. Other researchers had results that\nwere approximations because they used a greedy top-down approach. Here we give\nan algorithm to find an exact solution in $\\Theta(n+bm)$ time, and a simpler\nalgorithm taking $\\Theta(n+b m \\log m)$ time. These algorithms also determine\noptimal $k$-means clustering of weighted 1-dimensional data. \n\n"}
{"id": "1412.3779", "contents": "Title: Biips: Software for Bayesian Inference with Interacting Particle Systems Abstract: Biips is a software platform for automatic Bayesian inference with\ninteracting particle systems. Biips allows users to define their statistical\nmodel in the probabilistic programming BUGS language, as well as to add custom\nfunctions or samplers within this language. Then it runs sequential Monte Carlo\nbased algorithms (particle filters, particle independent Metropolis-Hastings,\nparticle marginal Metropolis-Hastings) in a black-box manner so that to\napproximate the posterior distribution of interest as well as the marginal\nlikelihood. The software is developed in C++ with interfaces with the softwares\nR, Matlab and Octave. \n\n"}
{"id": "1412.4869", "contents": "Title: Expectation propagation as a way of life: A framework for Bayesian\n  inference on partitioned data Abstract: A common divide-and-conquer approach for Bayesian computation with big data\nis to partition the data, perform local inference for each piece separately,\nand combine the results to obtain a global posterior approximation. While being\nconceptually and computationally appealing, this method involves the\nproblematic need to also split the prior for the local inferences; these\nweakened priors may not provide enough regularization for each separate\ncomputation, thus eliminating one of the key advantages of Bayesian methods. To\nresolve this dilemma while still retaining the generalizability of the\nunderlying local inference method, we apply the idea of expectation propagation\n(EP) as a framework for distributed Bayesian inference. The central idea is to\niteratively update approximations to the local likelihoods given the state of\nthe other approximations and the prior. The present paper has two roles: we\nreview the steps that are needed to keep EP algorithms numerically stable, and\nwe suggest a general approach, inspired by EP, for approaching data\npartitioning problems in a way that achieves the computational benefits of\nparallelism while allowing each local update to make use of relevant\ninformation from the other sites. In addition, we demonstrate how the method\ncan be applied in a hierarchical context to make use of partitioning of both\ndata and parameters. The paper describes a general algorithmic framework,\nrather than a specific algorithm, and presents an example implementation for\nit. \n\n"}
{"id": "1412.5122", "contents": "Title: Fast computation of Tukey trimmed regions and median in dimension $p>2$ Abstract: Given data in $\\mathbb{R}^{p}$, a Tukey $\\kappa$-trimmed region is the set of\nall points that have at least Tukey depth $\\kappa$ w.r.t. the data. As they are\nvisual, affine equivariant and robust, Tukey regions are useful tools in\nnonparametric multivariate analysis. While these regions are easily defined and\ninterpreted, their practical use in applications has been impeded so far by the\nlack of efficient computational procedures in dimension $p > 2$. We construct\ntwo novel algorithms to compute a Tukey $\\kappa$-trimmed region, a na\\\"{i}ve\none and a more sophisticated one that is much faster than known algorithms.\nFurther, a strict bound on the number of facets of a Tukey region is derived.\nIn a large simulation study the novel fast algorithm is compared with the\nna\\\"{i}ve one, which is slower and by construction exact, yielding in every\ncase the same correct results. Finally, the approach is extended to an\nalgorithm that calculates the innermost Tukey region and its barycenter, the\nTukey median. \n\n"}
{"id": "1412.5492", "contents": "Title: Transport map accelerated Markov chain Monte Carlo Abstract: We introduce a new framework for efficient sampling from complex probability\ndistributions, using a combination of optimal transport maps and the\nMetropolis-Hastings rule. The core idea is to use continuous transportation to\ntransform typical Metropolis proposal mechanisms (e.g., random walks, Langevin\nmethods) into non-Gaussian proposal distributions that can more effectively\nexplore the target density. Our approach adaptively constructs a lower\ntriangular transport map-an approximation of the Knothe-Rosenblatt\nrearrangement-using information from previous MCMC states, via the solution of\nan optimization problem. This optimization problem is convex regardless of the\nform of the target distribution. It is solved efficiently using a Newton method\nthat requires no gradient information from the target probability distribution;\nthe target distribution is instead represented via samples. Sequential updates\nenable efficient and parallelizable adaptation of the map even for large\nnumbers of samples. We show that this approach uses inexact or truncated maps\nto produce an adaptive MCMC algorithm that is ergodic for the exact target\ndistribution. Numerical demonstrations on a range of parameter inference\nproblems show order-of-magnitude speedups over standard MCMC techniques,\nmeasured by the number of effectively independent samples produced per target\ndensity evaluation and per unit of wallclock time. \n\n"}
{"id": "1412.5859", "contents": "Title: Investigation of carbon dioxide phase shift possibility under extreme\n  Antarctic winter conditions Abstract: The Antarctic winter atmosphere minimal temperature and pressure series\nreveal that $CO_2$ phase shift (deposition) is possible in some extreme cases,\neven leading to possible $CO_2$ snow phenomenon at Vostok Antarctic station and\nin other near South Pole regions. A hypothesis has been formulated that stable\n$CO_2$ snow cover might have formed in Earth past which may influence\ninterpretation of glacial chronology records. This effect may also manifest in\nother minor gases. Its global climate role is discussed. \n\n"}
{"id": "1412.6890", "contents": "Title: Software for Distributed Computation on Medical Databases: A\n  Demonstration Project Abstract: Bringing together the information latent in distributed medical databases\npromises to personalize medical care by enabling reliable, stable modeling of\noutcomes with rich feature sets (including patient characteristics and\ntreatments received). However, there are barriers to aggregation of medical\ndata, due to lack of standardization of ontologies, privacy concerns,\nproprietary attitudes toward data, and a reluctance to give up control over end\nuse. Aggregation of data is not always necessary for model fitting. In models\nbased on maximizing a likelihood, the computations can be distributed, with\naggregation limited to the intermediate results of calculations on local data,\nrather than raw data. Distributed fitting is also possible for singular value\ndecomposition. There has been work on the technical aspects of shared\ncomputation for particular applications, but little has been published on the\nsoftware needed to support the \"social networking\" aspect of shared computing,\nto reduce the barriers to collaboration. We describe a set of software tools\nthat allow the rapid assembly of a collaborative computational project, based\non the flexible and extensible R statistical software and other open source\npackages, that can work across a heterogeneous collection of database\nenvironments, with full transparency to allow local officials concerned with\nprivacy protections to validate the safety of the method. We describe the\nprinciples, architecture, and successful test results for the site-stratified\nCox model and rank-k Singular Value Decomposition (SVD). \n\n"}
{"id": "1412.8695", "contents": "Title: On Particle Methods for Parameter Estimation in State-Space Models Abstract: Nonlinear non-Gaussian state-space models are ubiquitous in statistics,\neconometrics, information engineering and signal processing. Particle methods,\nalso known as Sequential Monte Carlo (SMC) methods, provide reliable numerical\napproximations to the associated state inference problems. However, in most\napplications, the state-space model of interest also depends on unknown static\nparameters that need to be estimated from the data. In this context, standard\nparticle methods fail and it is necessary to rely on more sophisticated\nalgorithms. The aim of this paper is to present a comprehensive review of\nparticle methods that have been proposed to perform static parameter estimation\nin state-space models. We discuss the advantages and limitations of these\nmethods and illustrate their performance on simple models. \n\n"}
{"id": "1501.00219", "contents": "Title: Spectral diagonal ensemble Kalman filters Abstract: A new type of ensemble Kalman filter is developed, which is based on\nreplacing the sample covariance in the analysis step by its diagonal in a\nspectral basis. It is proved that this technique improves the aproximation of\nthe covariance when the covariance itself is diagonal in the spectral basis, as\nis the case, e.g., for a second-order stationary random field and the Fourier\nbasis. The method is extended by wavelets to the case when the state variables\nare random fields, which are not spatially homogeneous. Efficient\nimplementations by the fast Fourier transform (FFT) and discrete wavelet\ntransform (DWT) are presented for several types of observations, including\nhigh-dimensional data given on a part of the domain, such as radar and\nsatellite images. Computational experiments confirm that the method performs\nwell on the Lorenz 96 problem and the shallow water equations with very small\nensembles and over multiple analysis cycles. \n\n"}
{"id": "1501.06093", "contents": "Title: An Efficient Approach for Optical Radiative Transfer Tomography using\n  the Spherical Harmonics Discrete Ordinates Method Abstract: This paper introduces a method to preform optical tomography, using 3D\nradiative transfer as the forward model. We use an iterative approach\npredicated on the Spherical Harmonics Discrete Ordinates Method (SHDOM) to\nsolve the optimization problem in a scalable manner. We illustrate with an\napplication in remote sensing of a cloudy atmosphere. \n\n"}
{"id": "1502.00348", "contents": "Title: A Novel Statistical Channel Model for Turbulence-Induced Fading in\n  Free-Space Optical Systems Abstract: In this paper, we propose a new probability distribution function which\naccurately describes turbulence-induced fading under a wide range of turbulence\nconditions. The proposed model, termed Double Generalized Gamma (Double GG), is\nbased on a doubly stochastic theory of scintillation and developed via the\nproduct of two Generalized Gamma (GG) distributions. The proposed Double GG\ndistribution generalizes many existing turbulence channel models and provides\nan excellent fit to the published plane and spherical waves simulation data.\nUsing this new statistical channel model, we derive closed form expressions for\nthe outage probability and the average bit error as well as corresponding\nasymptotic expressions of free-space optical communication systems over\nturbulence channels. We demonstrate that our derived expressions cover many\nexisting results in the literature earlier reported for Gamma-Gamma,\nDouble-Weibull and K channels as special cases. \n\n"}
{"id": "1502.00799", "contents": "Title: An early warning indicator for atmospheric blocking events using\n  transfer operators Abstract: The existence of persistent midlatitude atmospheric flow regimes with\ntime-scales larger than 5-10 days and indications of preferred transitions\nbetween them motivates to develop early warning indicators for such regime\ntransitions. In this paper, we use a hemispheric barotropic model together with\nestimates of transfer operators on a reduced phase space to develop an early\nwarning indicator of the zonal to blocked flow transition in this model. It is\nshown that, the spectrum of the transfer operators can be used to study the\nslow dynamics of the flow as well as the non-Markovian character of the\nreduction. The slowest motions are thereby found to have time scales of three\nto six weeks and to be associated with meta-stable regimes (and their\ntransitions) which can be detected as almost-invariant sets of the transfer\noperator. From the energy budget of the model, we are able to explain the\nmeta-stability of the regimes and the existence of preferred transition paths.\nEven though the model is highly simplified, the skill of the early warning\nindicator is promising, suggesting that the transfer operator approach can be\nused in parallel to an operational deterministic model for stochastic\nprediction or to assess forecast uncertainty. \n\n"}
{"id": "1502.00805", "contents": "Title: Factors controlling the time-delay between peak CO2 emissions and\n  concentrations Abstract: Carbon-dioxide (CO2) is the main contributor to anthropogenic global warming,\nand the timing of its peak concentration in the atmosphere is likely to govern\nthe timing of maximum radiative forcing. It is well-known that dynamics of\natmospheric CO2 is governed by multiple time-constants, and here we approximate\nthe solutions to a linear model of atmospheric CO2 dynamics with four\ntime-constants to identify factors governing the time-delay between peaks in\nCO2 emissions and concentrations, and therefore the timing of the concentration\npeak. The main factor affecting this time-delay is the ratio of the rate of\nchange of emissions during its increasing and decreasing phases. If this ratio\nis large in magnitude then the time-delay between peak emissions and\nconcentrations is large. Therefore it is important to limit the magnitude of\nthis ratio through mitigation, in order to achieve an early peak in CO2\nconcentrations. This can be achieved with an early global emissions peak,\ncombined with rapid decarbonization of economic activity, because the delay\nbetween peak emissions and concentrations is affected by the time-scale with\nwhich decarbonization occurs. Of course, for limiting the magnitude of peak\nconcentrations it is also important to limit the magnitude of emissions\nthroughout its trajectory, but that aspect has been studied elsewhere and is\nnot examined here. The carbon cycle parameters affecting the timing of the\nconcentration peak are primarily the long multi-century time-constant of\natmospheric CO2, and the ratio of contributions to the impulse response\nfunction of atmospheric CO2 from the infinite time-constant and the long\ntime-constant respectively. Reducing uncertainties in these parameters can\nreduce uncertainty in forecasts of the radiative forcing peak. \n\n"}
{"id": "1502.01148", "contents": "Title: Bayesian computation: a perspective on the current state, and sampling\n  backwards and forwards Abstract: The past decades have seen enormous improvements in computational inference\nbased on statistical models, with continual enhancement in a wide range of\ncomputational tools, in competition. In Bayesian inference, first and foremost,\nMCMC techniques continue to evolve, moving from random walk proposals to\nLangevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical\nand algorithmic inputs opening wider access to practitioners. However, this\nimpressive evolution in capacity is confronted by an even steeper increase in\nthe complexity of the models and datasets to be addressed. The difficulties of\nmodelling and then handling ever more complex datasets most likely call for a\nnew type of tool for computational inference that dramatically reduce the\ndimension and size of the raw data while capturing its essential aspects.\nApproximate models and algorithms may thus be at the core of the next\ncomputational revolution. \n\n"}
{"id": "1502.02436", "contents": "Title: Exact solutions to Super Resolution on semi-algebraic domains in higher\n  dimensions Abstract: We investigate the multi-dimensional Super Resolution problem on closed\nsemi-algebraic domains for various sampling schemes such as Fourier or moments.\nWe present a new semidefinite programming (SDP) formulation of the 1\n-minimization in the space of Radon measures in the multi-dimensional frame on\nsemi-algebraic sets. While standard approaches have focused on SDP relaxations\nof the dual program (a popular approach is based on Gram matrix\nrepresentations), this paper introduces an exact formulation of the primal 1\n-minimization exact recovery problem of Super Resolution that unleashes\nstandard techniques (such as moment-sum-of-squares hier-archies) to overcome\nintrinsic limitations of previous works in the literature. Notably, we show\nthat one can exactly solve the Super Resolution problem in dimension greater\nthan 2 and for a large family of domains described by semi-algebraic sets. \n\n"}
{"id": "1502.02536", "contents": "Title: Nested Sequential Monte Carlo Methods Abstract: We propose nested sequential Monte Carlo (NSMC), a methodology to sample from\nsequences of probability distributions, even where the random variables are\nhigh-dimensional. NSMC generalises the SMC framework by requiring only\napproximate, properly weighted, samples from the SMC proposal distribution,\nwhile still resulting in a correct SMC algorithm. Furthermore, NSMC can in\nitself be used to produce such properly weighted samples. Consequently, one\nNSMC sampler can be used to construct an efficient high-dimensional proposal\ndistribution for another NSMC sampler, and this nesting of the algorithm can be\ndone to an arbitrary degree. This allows us to consider complex and\nhigh-dimensional models using SMC. We show results that motivate the efficacy\nof our approach on several filtering problems with dimensions in the order of\n100 to 1 000. \n\n"}
{"id": "1502.03042", "contents": "Title: Functional Gaussian Process Model for Bayesian Nonparametric Analysis Abstract: Gaussian process is a theoretically appealing model for nonparametric\nanalysis, but its computational cumbersomeness hinders its use in large scale\nand the existing reduced-rank solutions are usually heuristic. In this work, we\npropose a novel construction of Gaussian process as a projection from fixed\ndiscrete frequencies to any continuous location. This leads to a valid\nstochastic process that has a theoretic support with the reduced rank in the\nspectral density, as well as a high-speed computing algorithm. Our method\nprovides accurate estimates for the covariance parameters and concise form of\npredictive distribution for spatial prediction. For non-stationary data, we\nadopt the mixture framework with a customized spectral dependency structure.\nThis enables clustering based on local stationarity, while maintains the joint\nGaussianness. Our work is directly applicable in solving some of the challenges\nin the spatial data, such as large scale computation, anisotropic covariance,\nspatio-temporal modeling, etc. We illustrate the uses of the model via\nsimulations and an application on a massive dataset. \n\n"}
{"id": "1502.03178", "contents": "Title: Transient coupling relationships of the Holocene Australian monsoon Abstract: The northwest Australian summer monsoon owes a notable degree of its\ninterannual variability to interactions with other regional monsoon systems.\nTherefore, changes in the nature of these relationships may contribute to\nvariability in monsoon strength over longer time scales. Previous attempts to\nevaluate how proxy records from the Indonesian-Australian monsoon region\ncorrespond to other records from the Indian and East Asian monsoon regions, as\nwell as to El Ni\\~no-related proxy records, have been qualitiative, relying on\n`curve-fitting' methods. Here, we seek a quantitative approach for identifying\ncoupling relationships between paleoclimate proxy records, employing\nstatistical techniques to compute the interdependence of two paleoclimate time\nseries. We verify the use of complex networks to identify coupling\nrelationships between modern climate indices. This method is then extended to a\nset of paleoclimate proxy records from the Asian, Australasian and South\nAmerican regions spanning the past 9,000 years. The resulting networks\ndemonstrate the existence of coupling relationships between regional monsoon\nsystems on millennial time scales, but also highlight the transient nature of\nteleconnections during this period. In the context of the northwest Australian\nsummer monsoon, we recognise a shift in coupling relationships from strong\ninterhemispheric links with East Asian and ITCZ-related proxy records in the\nmid-Holocene to significantly weaker coupling in the later Holocene. Although\nthe identified links cannot explain the underlying physical processes leading\nto coupling between regional monsoon systems, this method provides a step\ntowards understanding the role that changes in teleconnections play in\nmillennial- to orbital-scale climate variability. \n\n"}
{"id": "1502.03536", "contents": "Title: Speeding up Permutation Testing in Neuroimaging Abstract: Multiple hypothesis testing is a significant problem in nearly all\nneuroimaging studies. In order to correct for this phenomena, we require a\nreliable estimate of the Family-Wise Error Rate (FWER). The well known\nBonferroni correction method, while simple to implement, is quite conservative,\nand can substantially under-power a study because it ignores dependencies\nbetween test statistics. Permutation testing, on the other hand, is an exact,\nnon-parametric method of estimating the FWER for a given $\\alpha$-threshold,\nbut for acceptably low thresholds the computational burden can be prohibitive.\nIn this paper, we show that permutation testing in fact amounts to populating\nthe columns of a very large matrix ${\\bf P}$. By analyzing the spectrum of this\nmatrix, under certain conditions, we see that ${\\bf P}$ has a low-rank plus a\nlow-variance residual decomposition which makes it suitable for highly\nsub--sampled --- on the order of $0.5\\%$ --- matrix completion methods. Based\non this observation, we propose a novel permutation testing methodology which\noffers a large speedup, without sacrificing the fidelity of the estimated FWER.\nOur evaluations on four different neuroimaging datasets show that a\ncomputational speedup factor of roughly $50\\times$ can be achieved while\nrecovering the FWER distribution up to very high accuracy. Further, we show\nthat the estimated $\\alpha$-threshold is also recovered faithfully, and is\nstable. \n\n"}
{"id": "1502.03697", "contents": "Title: Nonlinear state space smoothing using the conditional particle filter Abstract: To estimate the smoothing distribution in a nonlinear state space model, we\napply the conditional particle filter with ancestor sampling. This gives an\niterative algorithm in a Markov chain Monte Carlo fashion, with asymptotic\nconvergence results. The computational complexity is analyzed, and our proposed\nalgorithm is successfully applied to the challenging problem of sensor fusion\nbetween ultra-wideband and accelerometer/gyroscope measurements for indoor\npositioning. It appears to be a competitive alternative to existing nonlinear\nsmoothing algorithms, in particular the forward filtering-backward simulation\nsmoother. \n\n"}
{"id": "1502.05503", "contents": "Title: Classification and Bayesian Optimization for Likelihood-Free Inference Abstract: Some statistical models are specified via a data generating process for which\nthe likelihood function cannot be computed in closed form. Standard\nlikelihood-based inference is then not feasible but the model parameters can be\ninferred by finding the values which yield simulated data that resemble the\nobserved data. This approach faces at least two major difficulties: The first\ndifficulty is the choice of the discrepancy measure which is used to judge\nwhether the simulated data resemble the observed data. The second difficulty is\nthe computationally efficient identification of regions in the parameter space\nwhere the discrepancy is low. We give here an introduction to our recent work\nwhere we tackle the two difficulties through classification and Bayesian\noptimization. \n\n"}
{"id": "1502.07039", "contents": "Title: Markov Interacting Importance Samplers Abstract: We introduce a new Markov chain Monte Carlo (MCMC) sampler called the Markov\nInteracting Importance Sampler (MIIS). The MIIS sampler uses conditional\nimportance sampling (IS) approximations to jointly sample the current state of\nthe Markov Chain and estimate conditional expectations, possibly by\nincorporating a full range of variance reduction techniques. We compute\nRao-Blackwellized estimates based on the conditional expectations to construct\ncontrol variates for estimating expectations under the target distribution. The\ncontrol variates are particularly efficient when there are substantial\ncorrelations between the variables in the target distribution, a challenging\nsetting for MCMC. An important motivating application of MIIS occurs when the\nexact Gibbs sampler is not available because it is infeasible to directly\nsimulate from the conditional distributions. In this case the MIIS method can\nbe more efficient than a Metropolis-within-Gibbs approach. We also introduce\nthe MIIS random walk algorithm, designed to accelerate convergence and improve\nupon the computational efficiency of standard random walk samplers. Simulated\nand empirical illustrations for Bayesian analysis show that the method\nsignificantly reduces the variance of Monte Carlo estimates compared to\nstandard MCMC approaches, at equivalent implementation and computational\neffort. \n\n"}
{"id": "1502.07458", "contents": "Title: A SAEM Algorithm for Fused Lasso Penalized Non Linear Mixed Effect\n  Models: Application to Group Comparison in Pharmacokinetic Abstract: Non linear mixed effect models are classical tools to analyze non linear\nlongitudinal data in many fields such as population Pharmacokinetic. Groups of\nobservations are usually compared by introducing the group affiliations as\nbinary covariates with a reference group that is stated among the groups. This\napproach is relatively limited as it allows only the comparison of the\nreference group to the others. In this work, we propose to compare the groups\nusing a penalized likelihood approach. Groups are described by the same\nstructural model but with parameters that are group specific. The likelihood is\npenalized with a fused lasso penalty that induces sparsity on the differences\nbetween groups for both fixed effects and variances of random effects. A\npenalized Stochastic Approximation EM algorithm is proposed that is coupled to\nAlternating Direction Method Multipliers to solve the maximization step. An\nextensive simulation study illustrates the performance of this algorithm when\ncomparing more than two groups. Then the approach is applied to real data from\ntwo pharmacokinetic drug-drug interaction trials. \n\n"}
{"id": "1503.00021", "contents": "Title: Mercer kernels and integrated variance experimental design: connections\n  between Gaussian process regression and polynomial approximation Abstract: This paper examines experimental design procedures used to develop surrogates\nof computational models, exploring the interplay between experimental designs\nand approximation algorithms. We focus on two widely used approximation\napproaches, Gaussian process (GP) regression and non-intrusive polynomial\napproximation. First, we introduce algorithms for minimizing a posterior\nintegrated variance (IVAR) design criterion for GP regression. Our formulation\ntreats design as a continuous optimization problem that can be solved with\ngradient-based methods on complex input domains, without resorting to greedy\napproximations. We show that minimizing IVAR in this way yields point sets with\ngood interpolation properties, and that it enables more accurate GP regression\nthan designs based on entropy minimization or mutual information maximization.\nSecond, using a Mercer kernel/eigenfunction perspective on GP regression, we\nidentify conditions under which GP regression coincides with pseudospectral\npolynomial approximation. Departures from these conditions can be understood as\nchanges either to the kernel or to the experimental design itself. We then show\nhow IVAR-optimal designs, while sacrificing discrete orthogonality of the\nkernel eigenfunctions, can yield lower approximation error than orthogonalizing\npoint sets. Finally, we compare the performance of adaptive Gaussian process\nregression and adaptive pseudospectral approximation for several classes of\ntarget functions, identifying features that are favorable to the GP + IVAR\napproach. \n\n"}
{"id": "1503.00847", "contents": "Title: The International Workshop on Wave Hindcasting and Forecasting and the\n  Coastal Hazards Symposium Abstract: Following the 13th International Workshop on Wave Hindcasting and Forecasting\nand 4th Coastal Hazards Symposium in October 2013 in Banff, Canada, a topical\ncollection has appeared in recent issues of Ocean Dynamics. Here we give a\nbrief overview of the history of the conference since its inception in 1986 and\nof the progress made in the fields of wind-generated ocean waves and the\nmodelling of coastal hazards before we summarize the main results of the papers\nthat have appeared in the topical collection. \n\n"}
{"id": "1503.00996", "contents": "Title: Accelerating Metropolis-Hastings algorithms by Delayed Acceptance Abstract: MCMC algorithms such as Metropolis-Hastings algorithms are slowed down by the\ncomputation of complex target distributions as exemplified by huge datasets. We\noffer in this paper a useful generalisation of the Delayed Acceptance approach,\ndevised to reduce the computational costs of such algorithms by a simple and\nuniversal divide-and-conquer strategy. The idea behind the generic acceleration\nis to divide the acceptance step into several parts, aiming at a major\nreduction in computing time that out-ranks the corresponding reduction in\nacceptance probability. Each of the components can be sequentially compared\nwith a uniform variate, the first rejection signalling that the proposed value\nis considered no further. We develop moreover theoretical bounds for the\nvariance of associated estimators with respect to the variance of the standard\nMetropolis-Hastings and detail some results on optimal scaling and general\noptimisation of the procedure. We illustrate those accelerating features on a\nseries of examples \n\n"}
{"id": "1503.01631", "contents": "Title: Application of Sequential Quasi-Monte Carlo to Autonomous Positioning Abstract: Sequential Monte Carlo algorithms (also known as particle filters) are\npopular methods to approximate filtering (and related) distributions of\nstate-space models. However, they converge at the slow $1/\\sqrt{N}$ rate, which\nmay be an issue in real-time data-intensive scenarios. We give a brief outline\nof SQMC (Sequential Quasi-Monte Carlo), a variant of SMC based on\nlow-discrepancy point sets proposed by Gerber and Chopin (2015), which\nconverges at a faster rate, and we illustrate the greater performance of SQMC\non autonomous positioning problems. \n\n"}
{"id": "1503.03806", "contents": "Title: HELIOS-K: An Ultrafast, Open-source Opacity Calculator for Radiative\n  Transfer Abstract: We present an ultrafast opacity calculator that we name HELIOS-K. It takes a\nline list as an input, computes the shape of each spectral line and provides an\noption for grouping an enormous number of lines into a manageable number of\nbins. We implement a combination of Algorithm 916 and Gauss-Hermite quadrature\nto compute the Voigt profile, write the code in CUDA and optimise the\ncomputation for graphics processing units (GPUs). We restate the theory of the\nk-distribution method and use it to reduce $\\sim 10^5$ to $10^8$ lines to $\\sim\n10$ to $10^4$ wavenumber bins, which may then be used for radiative transfer,\natmospheric retrieval and general circulation models. The choice of line-wing\ncutoff for the Voigt profile is a significant source of error and affects the\nvalue of the computed flux by $\\sim 10\\%$. This is an outstanding physical\n(rather than computational) problem, due to our incomplete knowledge of\npressure broadening of spectral lines in the far line wings. We emphasize that\nthis problem remains regardless of whether one performs line-by-line\ncalculations or uses the k-distribution method and affects all calculations of\nexoplanetary atmospheres requiring the use of wavelength-dependent opacities.\nWe elucidate the correlated-k approximation and demonstrate that it applies\nequally to inhomogeneous atmospheres with a single atomic/molecular species or\nhomogeneous atmospheres with multiple species. Using a NVIDIA K20 GPU, HELIOS-K\nis capable of computing an opacity function with $\\sim 10^5$ spectral lines in\n$\\sim 1$ second and is publicly available as part of the Exoclimes Simulation\nPlatform (ESP; www.exoclime.org). \n\n"}
{"id": "1503.04168", "contents": "Title: On the ineffectiveness of constant rotation in the primitive equations\n  and their symmetry analysis Abstract: Modern weather and climate prediction models are based on a system of\nnonlinear partial differential equations called the primitive equations. Lie\nsymmetries of the primitive equations with zero external heating rate are\ncomputed and the structure of its maximal Lie invariance algebra, which is\ninfinite-dimensional, is studied. The maximal Lie invariance algebra for the\ncase of a nonzero constant Coriolis parameter is mapped to the case of\nvanishing Coriolis force. The same mapping allows one to transform the\nconstantly rotating primitive equations to the equations in a resting reference\nframe. This mapping is used to obtain exact solutions for the rotating case\nfrom exact solutions for the nonrotating equations. Another important result of\nthe paper is the computation of the complete point symmetry group of the\nprimitive equations using the algebraic method. \n\n"}
{"id": "1503.05570", "contents": "Title: A Data Science Course for Undergraduates: Thinking with Data Abstract: Data science is an emerging interdisciplinary field that combines elements of\nmathematics, statistics, computer science, and knowledge in a particular\napplication domain for the purpose of extracting meaningful information from\nthe increasingly sophisticated array of data available in many settings. These\ndata tend to be non-traditional, in the sense that they are often live, large,\ncomplex, and/or messy. A first course in statistics at the undergraduate level\ntypically introduces students with a variety of techniques to analyze small,\nneat, and clean data sets. However, whether they pursue more formal training in\nstatistics or not, many of these students will end up working with data that is\nconsiderably more complex, and will need facility with statistical computing\ntechniques. More importantly, these students require a framework for thinking\nstructurally about data. We describe an undergraduate course in a liberal arts\nenvironment that provides students with the tools necessary to apply data\nscience. The course emphasizes modern, practical, and useful skills that cover\nthe full data analysis spectrum, from asking an interesting question to\nacquiring, managing, manipulating, processing, querying, analyzing, and\nvisualizing data, as well communicating findings in written, graphical, and\noral forms. \n\n"}
{"id": "1503.06457", "contents": "Title: Zonal Flows and Turbulence in Fluids and Plasmas Abstract: In geophysical and plasma contexts, zonal flows are well known to arise out\nof turbulence. We elucidate the transition from statistically homogeneous\nturbulence without zonal flows to statistically inhomogeneous turbulence with\nsteady zonal flows. Starting from the Hasegawa--Mima equation, we employ both\nthe quasilinear approximation and a statistical average, which retains a great\ndeal of the qualitative behavior of the full system. Within the resulting\nframework known as CE2, we extend recent understanding of the symmetry-breaking\n`zonostrophic instability'. Zonostrophic instability can be understood in a\nvery general way as the instability of some turbulent background spectrum to a\nzonally symmetric coherent mode. As a special case, the background spectrum can\nconsist of only a single mode. We find that in this case the dispersion\nrelation of zonostrophic instability from the CE2 formalism reduces exactly to\nthat of the 4-mode truncation of generalized modulational instability. We then\nshow that zonal flows constitute pattern formation amid a turbulent bath.\nZonostrophic instability is an example of a Type I$_s$ instability of\npattern-forming systems. The broken symmetry is statistical homogeneity. Near\nthe bifurcation point, the slow dynamics of CE2 are governed by a well-known\namplitude equation, the real Ginzburg-Landau equation. The important features\nof this amplitude equation, and therefore of the CE2 system, are multiple.\nFirst, the zonal flow wavelength is not unique. In an idealized, infinite\nsystem, there is a continuous band of zonal flow wavelengths that allow a\nnonlinear equilibrium. Second, of these wavelengths, only those within a\nsmaller subband are stable. Unstable wavelengths must evolve to reach a stable\nwavelength; this process manifests as merging jets. These behaviors ... \n\n"}
{"id": "1503.07091", "contents": "Title: On nonlinearity implications and wind forcing in Hasselmann equation Abstract: We discuss several experimental and theoretical techniques historically used\nfor Hasselmann equation wind input terms derivation. We show that recently\ndeveloped ZRP technique in conjunction with high-frequency damping without\nspectral peak dissipation allows to reproduce more than a dozen of\nfetch-limited field experiments. Numerical simulation of the same Cauchy\nproblem for different wind input terms has been performed to discuss\nnonlinearity implications as well as correspondence to theoretical predictions. \n\n"}
{"id": "1503.07307", "contents": "Title: Improving the INLA approach for approximate Bayesian inference for\n  latent Gaussian models Abstract: We introduce a new copula-based correction for generalized linear mixed\nmodels (GLMMs) within the integrated nested Laplace approximation (INLA)\napproach for approximate Bayesian inference for latent Gaussian models. While\nINLA is usually very accurate, some (rather extreme) cases of GLMMs with e.g.\nbinomial or Poisson data have been seen to be problematic. Inaccuracies can\noccur when there is a very low degree of smoothing or \"borrowing strength\"\nwithin the model, and we have therefore developed a correction aiming to push\nthe boundaries of the applicability of INLA. Our new correction has been\nimplemented as part of the R-INLA package, and adds only negligible\ncomputational cost. Empirical evaluations on both real and simulated data\nindicate that the method works well. \n\n"}
{"id": "1503.07644", "contents": "Title: Formation of large-scale structures by turbulence in rotating planets Abstract: This thesis presents a newly developed theory for the formation and\nmaintenance of eddy-driven jets in planetary turbulence. The novelty is that\njet formation and maintenance is studied as a dynamics of the statistics of the\nflow rather than a dynamics of individual realizations. This is pursued using\nStochastic Structural Stability Theory (S3T), which studies the closed dynamics\nof the first two cumulants of the full statistical state dynamics of the flow\nafter neglecting or parameterizing third and higher-order cumulants. Using this\nstatistical closure, large-scale structure formation is studied in barotropic\nturbulence on a $\\beta$-plane.\n  It is demonstrated that at analytically predicted critical parameter values\nthe homogeneous turbulent state undergoes a bifurcation becoming inhomogeneous\nwith the emergence of large-scale zonal and/or non-zonal flows. The mechanisms\nby which the turbulent Reynolds stresses organize to reinforce infinitesimal\nmean flow inhomogeneities, thus leading to this statistical state instability,\nare studied for various regimes of parameter values. It is also demonstrated\nthat the S3T instabilities equilibrate to turbulent states with\nfinite-amplitude jets. Moreover, the relation between large-scale structure\nformation through modulational instability and the S3T instability of the\nhomogeneous turbulent state is also investigated and it is shown that the\nmodulational instability results are subsumed by the S3T results. Finally, the\nstudy of the S3T stability of inhomogeneous turbulent jet equilibria is\npresented and the relation with the phenomenon of jet merging is investigated.\nAlthough all these phenomena cannot be predicted by analysis of the dynamics of\nsingle realizations of the flow, it is demonstrated that the predictions of S3T\nare reflected in individual realizations of the flow. \n\n"}
{"id": "1503.07677", "contents": "Title: Surface Wave Effects in the NEMO Ocean Model: Forced and Coupled\n  Experiments Abstract: The NEMO general circulation ocean model is extended to incorporate three\nphysical processes related to ocean surface waves, namely the surface stress\n(modified by growth and dissipation of the oceanic wave field), the turbulent\nkinetic energy flux from breaking waves, and the Stokes-Coriolis force.\nExperiments are done with NEMO in ocean-only (forced) mode and coupled to the\nECMWF atmospheric and wave models. Ocean-only integrations are forced with\nfields from the ERA-Interim reanalysis. All three effects are noticeable in the\nextra-tropics, but the sea-state dependent turbulent kinetic energy flux yields\nby far the largest difference. This is partly because the control run has too\nvigorous deep mixing due to an empirical mixing term in NEMO. We investigate\nthe relation between this ad hoc mixing and Langmuir turbulence and find that\nit is much more effective than the Langmuir parameterization used in NEMO. The\nbiases in sea surface temperature as well as subsurface temperature are\nreduced, and the total ocean heat content exhibits a trend closer to that\nobserved in a recent ocean reanalysis (ORAS4) when wave effects are included.\nSeasonal integrations of the coupled atmosphere-wave-ocean model consisting of\nNEMO, the wave model ECWAM and the atmospheric model of ECMWF similarly show\nthat the sea surface temperature biases are greatly reduced when the mixing is\ncontrolled by the sea state and properly weighted by the thickness of the\nuppermost level of the ocean model. These wave-related physical processes were\nrecently implemented in the operational coupled ensemble forecast system of\nECMWF. \n\n"}
{"id": "1503.07791", "contents": "Title: Sequential Monte Carlo with Adaptive Weights for Approximate Bayesian\n  Computation Abstract: Methods of approximate Bayesian computation (ABC) are increasingly used for\nanalysis of complex models. A major challenge for ABC is over-coming the often\ninherent problem of high rejection rates in the accept/reject methods based on\nprior:predictive sampling. A number of recent developments aim to address this\nwith extensions based on sequential Monte Carlo (SMC) strategies. We build on\nthis here, introducing an ABC SMC method that uses data-based adaptive weights.\nThis easily implemented and computationally trivial extension of ABC SMC can\nvery substantially improve acceptance rates, as is demonstrated in a series of\nexamples with simulated and real data sets, including a currently topical\nexample from dynamic modelling in systems biology applications. \n\n"}
{"id": "1503.08066", "contents": "Title: Scalable Bayesian Inference for the Inverse Temperature of a Hidden\n  Potts Model Abstract: The inverse temperature parameter of the Potts model governs the strength of\nspatial cohesion and therefore has a major influence over the resulting model\nfit. A difficulty arises from the dependence of an intractable normalising\nconstant on the value of this parameter and thus there is no closed-form\nsolution for sampling from the posterior distribution directly. There are a\nvariety of computational approaches for sampling from the posterior without\nevaluating the normalising constant, including the exchange algorithm and\napproximate Bayesian computation (ABC). A serious drawback of these algorithms\nis that they do not scale well for models with a large state space, such as\nimages with a million or more pixels. We introduce a parametric surrogate\nmodel, which approximates the score function using an integral curve. Our\nsurrogate model incorporates known properties of the likelihood, such as\nheteroskedasticity and critical temperature. We demonstrate this method using\nsynthetic data as well as remotely-sensed imagery from the Landsat-8 satellite.\nWe achieve up to a hundredfold improvement in the elapsed runtime, compared to\nthe exchange algorithm or ABC. An open source implementation of our algorithm\nis available in the R package \"bayesImageS.\" \n\n"}
{"id": "1503.08420", "contents": "Title: Line Strengths of Rovibrational and Rotational Transitions in the\n  X$^2\\Pi$ Ground State of OH Abstract: A new line list including positions and absolute intensities (in the form of\nEinstein $A$ values and oscillator strengths) has been produced for the OH\nground X\\DP\\ state rovibrational (Meinel system) and pure rotational\ntransitions. All possible transitions are included with v$\\primed$ and\nv$\\Dprimed$ up to 13, and $J$ up to between 9.5 and 59.5, depending on the\nband. An updated fit to determine molecular constants has been performed, which\nincludes some new rotational data and a simultaneous fitting of all molecular\nconstants. The absolute line intensities are based on a new dipole moment\nfunction, which is a combination of two high level ab initio calculations. The\ncalculations show good agreement with an experimental v=1 lifetime,\nexperimental $\\mu_\\mathrm{v}$ values, and $\\Delta$v=2 line intensity ratios\nfrom an observed spectrum. To achieve this good agreement, an alteration in the\nmethod of converting matrix elements from Hund's case (b) to (a) was made.\nPartitions sums have been calculated using the new energy levels, for the\ntemperature range 5-6000 K, which extends the previously available (in HITRAN)\n70-3000 K range. The resulting absolute intensities have been used to calculate\nO abundances in the Sun, Arcturus, and two red giants in the Galactic open and\nglobular clusters M67 and M71. Literature data based mainly on [O I] lines are\navailable for the Sun and Arcturus, and excellent agreement is found. \n\n"}
{"id": "1504.01418", "contents": "Title: Precomputing Strategy for Hamiltonian Monte Carlo Method Based on\n  Regularity in Parameter Space Abstract: Markov Chain Monte Carlo (MCMC) algorithms play an important role in\nstatistical inference problems dealing with intractable probability\ndistributions. Recently, many MCMC algorithms such as Hamiltonian Monte Carlo\n(HMC) and Riemannian Manifold HMC have been proposed to provide distant\nproposals with high acceptance rate. These algorithms, however, tend to be\ncomputationally intensive which could limit their usefulness, especially for\nbig data problems due to repetitive evaluations of functions and statistical\nquantities that depend on the data. This issue occurs in many statistic\ncomputing problems. In this paper, we propose a novel strategy that exploits\nsmoothness (regularity) of parameter space to improve computational efficiency\nof MCMC algorithms. When evaluation of functions or statistical quantities are\nneeded at a point in parameter space, interpolation from precomputed values or\nprevious computed values is used. More specifically, we focus on Hamiltonian\nMonte Carlo (HMC) algorithms that use geometric information for faster\nexploration of probability distributions. Our proposed method is based on\nprecomputing the required geometric information on a set of grids before\nrunning sampling information at nearby grids at each iteration of HMC. Sparse\ngrid interpolation method is used for high dimensional problems. Tests on\ncomputational examples are shown to illustrate the advantages of our method. \n\n"}
{"id": "1504.04696", "contents": "Title: On estimation of the diagonal elements of a sparse precision matrix Abstract: In this paper, we present several estimators of the diagonal elements of the\ninverse of the covariance matrix, called precision matrix, of a sample of iid\nrandom vectors. The focus is on high dimensional vectors having a sparse\nprecision matrix. It is now well understood that when the underlying\ndistribution is Gaussian, the columns of the precision matrix can be estimated\nindependently form one another by solving linear regression problems under\nsparsity constraints. This approach leads to a computationally efficient\nstrategy for estimating the precision matrix that starts by estimating the\nregression vectors, then estimates the diagonal entries of the precision matrix\nand, in a final step, combines these estimators for getting estimators of the\noff-diagonal entries. While the step of estimating the regression vector has\nbeen intensively studied over the past decade, the problem of deriving\nstatistically accurate estimators of the diagonal entries has received much\nless attention. The goal of the present paper is to fill this gap by presenting\nfour estimators---that seem the most natural ones---of the diagonal entries of\nthe precision matrix and then performing a comprehensive empirical evaluation\nof these estimators. The estimators under consideration are the residual\nvariance, the relaxed maximum likelihood, the symmetry-enforced maximum\nlikelihood and the penalized maximum likelihood. We show, both theoretically\nand empirically, that when the aforementioned regression vectors are estimated\nwithout error, the symmetry-enforced maximum likelihood estimator has the\nsmallest estimation error. However, in a more realistic setting when the\nregression vector is estimated by a sparsity-favoring computationally efficient\nmethod, the qualities of the estimators become relatively comparable with a\nslight advantage for the residual variance estimator. \n\n"}
{"id": "1504.05285", "contents": "Title: Global well-posedness of strong solutions to a tropical climate model Abstract: In this paper, we consider the Cauchy problem to the TROPIC CLIMATE MODEL\nderived by Frierson-Majda-Pauluis in [Comm. Math. Sci, Vol. 2 (2004)] which is\na coupled system of the barotropic and the first baroclinic modes of the\nvelocity and the typical midtropospheric temperature. The system considered in\nthis paper has viscosities in the momentum equations, but no diffusivity in the\ntemperature equation. We establish here the global well-posedness of strong\nsolutions to this model. In proving the global existence of strong solutions,\nto overcome the difficulty caused by the absence of the diffusivity in the\ntemperature equation, we introduce a new velocity $w$ (called the pseudo\nbaroclinic velocity), which has more regularities than the original baroclinic\nmode of the velocity. An auxiliary function $\\phi$, which looks like the\neffective viscous flux for the compressible Navier-Stokes equations, is also\nintroduced to obtain the $L^\\infty$ bound of the temperature. Regarding the\nuniqueness, we use the idea of performing suitable energy estimates at level\none order lower than the natural basic energy estimates for the system. \n\n"}
{"id": "1504.07235", "contents": "Title: Sign Stable Random Projections for Large-Scale Learning Abstract: We study the use of \"sign $\\alpha$-stable random projections\" (where\n$0<\\alpha\\leq 2$) for building basic data processing tools in the context of\nlarge-scale machine learning applications (e.g., classification, regression,\nclustering, and near-neighbor search). After the processing by sign stable\nrandom projections, the inner products of the processed data approximate\nvarious types of nonlinear kernels depending on the value of $\\alpha$. Thus,\nthis approach provides an effective strategy for approximating nonlinear\nlearning algorithms essentially at the cost of linear learning. When $\\alpha\n=2$, it is known that the corresponding nonlinear kernel is the arc-cosine\nkernel. When $\\alpha=1$, the procedure approximates the arc-cos-$\\chi^2$ kernel\n(under certain condition). When $\\alpha\\rightarrow0+$, it corresponds to the\nresemblance kernel.\n  From practitioners' perspective, the method of sign $\\alpha$-stable random\nprojections is ready to be tested for large-scale learning applications, where\n$\\alpha$ can be simply viewed as a tuning parameter. What is missing in the\nliterature is an extensive empirical study to show the effectiveness of sign\nstable random projections, especially for $\\alpha\\neq 2$ or 1. The paper\nsupplies such a study on a wide variety of classification datasets. In\nparticular, we compare shoulder-by-shoulder sign stable random projections with\nthe recently proposed \"0-bit consistent weighted sampling (CWS)\" (Li 2015). \n\n"}
{"id": "1504.07245", "contents": "Title: Approximate Bayesian Computation for Forward Modeling in Cosmology Abstract: Bayesian inference is often used in cosmology and astrophysics to derive\nconstraints on model parameters from observations. This approach relies on the\nability to compute the likelihood of the data given a choice of model\nparameters. In many practical situations, the likelihood function may however\nbe unavailable or intractable due to non-gaussian errors, non-linear\nmeasurements processes, or complex data formats such as catalogs and maps. In\nthese cases, the simulation of mock data sets can often be made through forward\nmodeling. We discuss how Approximate Bayesian Computation (ABC) can be used in\nthese cases to derive an approximation to the posterior constraints using\nsimulated data sets. This technique relies on the sampling of the parameter\nset, a distance metric to quantify the difference between the observation and\nthe simulations and summary statistics to compress the information in the data.\nWe first review the principles of ABC and discuss its implementation using a\nPopulation Monte-Carlo (PMC) algorithm and the Mahalanobis distance metric. We\ntest the performance of the implementation using a Gaussian toy model. We then\napply the ABC technique to the practical case of the calibration of image\nsimulations for wide field cosmological surveys. We find that the ABC analysis\nis able to provide reliable parameter constraints for this problem and is\ntherefore a promising technique for other applications in cosmology and\nastrophysics. Our implementation of the ABC PMC method is made available via a\npublic code release. \n\n"}
{"id": "1505.00965", "contents": "Title: An Introduction to Multilevel Monte Carlo for Option Valuation Abstract: Monte Carlo is a simple and flexible tool that is widely used in\ncomputational finance. In this context, it is common for the quantity of\ninterest to be the expected value of a random variable defined via a stochastic\ndifferential equation. In 2008, Giles proposed a remarkable improvement to the\napproach of discretizing with a numerical method and applying standard Monte\nCarlo. His multilevel Monte Carlo method offers an order of speed up given by\nthe inverse of epsilon, where epsilon is the required accuracy. So computations\ncan run 100 times more quickly when two digits of accuracy are required. The\nmultilevel philosophy has since been adopted by a range of researchers and a\nwealth of practically significant results has arisen, most of which have yet to\nmake their way into the expository literature.\n  In this work, we give a brief, accessible, introduction to multilevel Monte\nCarlo and summarize recent results applicable to the task of option evaluation. \n\n"}
{"id": "1505.01356", "contents": "Title: Equilibrium statistical mechanics and energy partition for the shallow\n  water model Abstract: The aim of this paper is to use large deviation theory in order to compute\nthe entropy of macrostates for the microcanonical measure of the shallow water\nsystem. The main prediction of this full statistical mechanics computation is\nthe energy partition between a large scale vortical flow and small scale\nfluctuations related to inertia-gravity waves. We introduce for that purpose a\ndiscretized model of the continuous shallow water system, and compute the\ncorresponding statistical equilibria. We argue that microcanonical equilibrium\nstates of the discretized model in the continuous limit are equilibrium states\nof the actual shallow water system. We show that the presence of small scale\nfluctuations selects a subclass of equilibria among the states that were\npreviously computed by phenomenological approaches that were neglecting such\nfluctuations. In the limit of weak height fluctuations, the equilibrium state\ncan be interpreted as two subsystems in thermal contact: one subsystem\ncorresponds to the large scale vortical flow, the other subsystem corresponds\nto small scale height and velocity fluctuations. It is shown that either a\nnon-zero circulation or rotation and bottom topography are required to sustain\na non-zero large scale flow at equilibrium. Explicit computation of the\nequilibria and their energy partition is presented in the quasi-geostrophic\nlimit for the energy-enstrophy ensemble. The possible role of small scale\ndissipation and shocks is discussed. A geophysical application to the Zapiola\nanticyclone is presented. \n\n"}
{"id": "1505.02679", "contents": "Title: Heat engines and heat pumps in a hydrostatic atmosphere: How surface\n  pressure and temperature constrain wind power output and circulation cell\n  size Abstract: The kinetic energy budget of the atmosphere's meridional circulation cells is\nanalytically assessed. In the upper atmosphere kinetic energy generation grows\nwith increasing surface temperature difference \\$\\Delta T_s\\$ between the cold\nand warm ends of a circulation cell; in the lower atmosphere it declines. A\nrequirement that kinetic energy generation is positive in the lower atmosphere\nlimits the poleward cell extension \\$L\\$ of Hadley cells via a relationship\nbetween \\$\\Delta T_s\\$ and surface pressure difference \\$\\Delta p_s\\$: an upper\nlimit exists when \\$\\Delta p_s\\$ does not grow with increasing \\$\\Delta T_s\\$.\nThis pattern is demonstrated here using monthly data from MERRA re-analysis.\nKinetic energy generation along air streamlines in the boundary layer does not\nexceed \\$40\\$~J~mol\\$^{-1}\\$; it declines with growing \\$L\\$ and reaches zero\nfor the largest observed \\$L\\$ at 2~km height. The limited meridional cell size\nnecessitates the appearance of heat pumps -- circulation cells with negative\nwork output where the low-level air moves towards colder areas. These cells\nconsume the positive work output of the heat engines -- cells where the\nlow-level air moves towards the warmer areas -- and can in theory drive the\nglobal efficiency of atmospheric circulation down to zero. Relative\ncontributions of \\$\\Delta p_s\\$ and \\$\\Delta T_s\\$ to kinetic energy generation\nare evaluated: \\$\\Delta T_s\\$ dominates in the upper atmosphere, while \\$\\Delta\np_s\\$ dominates in the lower. Analysis and empirical evidence indicate that the\nnet kinetic power output on Earth is dominated by surface pressure gradients,\nwith minor net kinetic energy generation in the upper atmosphere. The role of\ncondensation in generating surface pressure gradients is discussed. \n\n"}
{"id": "1505.02827", "contents": "Title: On Markov chain Monte Carlo methods for tall data Abstract: Markov chain Monte Carlo methods are often deemed too computationally\nintensive to be of any practical use for big data applications, and in\nparticular for inference on datasets containing a large number $n$ of\nindividual data points, also known as tall datasets. In scenarios where data\nare assumed independent, various approaches to scale up the Metropolis-Hastings\nalgorithm in a Bayesian inference context have been recently proposed in\nmachine learning and computational statistics. These approaches can be grouped\ninto two categories: divide-and-conquer approaches and, subsampling-based\nalgorithms. The aims of this article are as follows. First, we present a\ncomprehensive review of the existing literature, commenting on the underlying\nassumptions and theoretical guarantees of each method. Second, by leveraging\nour understanding of these limitations, we propose an original\nsubsampling-based approach which samples from a distribution provably close to\nthe posterior distribution of interest, yet can require less than $O(n)$ data\npoint likelihood evaluations at each iteration for certain statistical models\nin favourable scenarios. Finally, we have only been able so far to propose\nsubsampling-based methods which display good performance in scenarios where the\nBernstein-von Mises approximation of the target posterior distribution is\nexcellent. It remains an open challenge to develop such methods in scenarios\nwhere the Bernstein-von Mises approximation is poor. \n\n"}
{"id": "1505.03001", "contents": "Title: Detecting the large entries of a sparse covariance matrix in\n  sub-quadratic time Abstract: The covariance matrix of a $p$-dimensional random variable is a fundamental\nquantity in data analysis. Given $n$ i.i.d. observations, it is typically\nestimated by the sample covariance matrix, at a computational cost of\n$O(np^{2})$ operations. When $n,p$ are large, this computation may be\nprohibitively slow. Moreover, in several contemporary applications, the\npopulation matrix is approximately sparse, and only its few large entries are\nof interest. This raises the following question, at the focus of our work:\nAssuming approximate sparsity of the covariance matrix, can its large entries\nbe detected much faster, say in sub-quadratic time, without explicitly\ncomputing all its $p^{2}$ entries? In this paper, we present and theoretically\nanalyze two randomized algorithms that detect the large entries of an\napproximately sparse sample covariance matrix using only $O(np\\text{ poly log }\np)$ operations. Furthermore, assuming sparsity of the population matrix, we\nderive sufficient conditions on the underlying random variable and on the\nnumber of samples $n$, for the sample covariance matrix to satisfy our\napproximate sparsity requirements. Finally, we illustrate the performance of\nour algorithms via several simulations. \n\n"}
{"id": "1505.03410", "contents": "Title: Mind the duality gap: safer rules for the Lasso Abstract: Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules. \n\n"}
{"id": "1505.04724", "contents": "Title: A Hybrid Monte-Carlo Sampling Smoother for Four Dimensional Data\n  Assimilation Abstract: This paper constructs an ensemble-based sampling smoother for\nfour-dimensional data assimilation using a Hybrid/Hamiltonian Monte-Carlo\napproach. The smoother samples efficiently from the posterior probability\ndensity of the solution at the initial time. Unlike the well-known ensemble\nKalman smoother, which is optimal only in the linear Gaussian case, the\nproposed methodology naturally accommodates non-Gaussian errors and non-linear\nmodel dynamics and observation operators. Unlike the four-dimensional\nvariational met\\-hod, which only finds a mode of the posterior distribution,\nthe smoother provides an estimate of the posterior uncertainty. One can use the\nensemble mean as the minimum variance estimate of the state, or can use the\nensemble in conjunction with the variational approach to estimate the\nbackground errors for subsequent assimilation windows. Numerical results\ndemonstrate the advantages of the proposed method compared to the traditional\nvariational and ensemble-based smoothing methods. \n\n"}
{"id": "1505.04903", "contents": "Title: The impact of upper tropospheric friction and Gill-type heating on the\n  location and strength of the Tropical Easterly Jet: Idealized physics in a\n  dry Atmospheric General Circulation Model Abstract: An atmospheric general circulation model (AGCM) with idealized and complete\nphysics has been used to evaluate the Tropical Easterly Jet (TEJ) jet. In\nidealized physics, the role of upper tropospheric friction has been found to be\nimportant in getting realistic upper tropospheric zonal wind patterns in\nresponse to heating. In idealized physics, the location and strength of the TEJ\nas a response to Gill heating has been studied. Though the Gill model is\nconsidered to be widely successful in capturing the lower tropospheric\nresponse, it is found to be inadequate in explaining the location and strength\nof the upper level TEJ. Heating from the Gill model and realistic upper\ntropospheric friction does not lead to the formation of a TEJ. \n\n"}
{"id": "1505.05391", "contents": "Title: Efficient Multiple Importance Sampling Estimators Abstract: Multiple importance sampling (MIS) methods use a set of proposal\ndistributions from which samples are drawn. Each sample is then assigned an\nimportance weight that can be obtained according to different strategies. This\nwork is motivated by the trade-off between variance reduction and computational\ncomplexity of the different approaches (classical vs. deterministic mixture)\navailable for the weight calculation. A new method that achieves an efficient\ncompromise between both factors is introduced in this paper. It is based on\nforming a partition of the set of proposal distributions and computing the\nweights accordingly. Computer simulations show the excellent performance of the\nassociated \\mbox{\\emph{partial deterministic mixture} MIS estimator. \n\n"}
{"id": "1505.05770", "contents": "Title: Variational Inference with Normalizing Flows Abstract: The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference. \n\n"}
{"id": "1505.06354", "contents": "Title: Online Updating of Statistical Inference in the Big Data Setting Abstract: We present statistical methods for big data arising from online analytical\nprocessing, where large amounts of data arrive in streams and require fast\nanalysis without storage/access to the historical data. In particular, we\ndevelop iterative estimating algorithms and statistical inferences for linear\nmodels and estimating equations that update as new data arrive. These\nalgorithms are computationally efficient, minimally storage-intensive, and\nallow for possible rank deficiencies in the subset design matrices due to\nrare-event covariates. Within the linear model setting, the proposed\nonline-updating framework leads to predictive residual tests that can be used\nto assess the goodness-of-fit of the hypothesized model. We also propose a new\nonline-updating estimator under the estimating equation setting. Theoretical\nproperties of the goodness-of-fit tests and proposed estimators are examined in\ndetail. In simulation studies and real data applications, our estimator\ncompares favorably with competing approaches under the estimating equation\nsetting. \n\n"}
{"id": "1505.06475", "contents": "Title: A Fast and Flexible Algorithm for the Graph-Fused Lasso Abstract: We propose a new algorithm for solving the graph-fused lasso (GFL), a method\nfor parameter estimation that operates under the assumption that the signal\ntends to be locally constant over a predefined graph structure. Our key insight\nis to decompose the graph into a set of trails which can then each be solved\nefficiently using techniques for the ordinary (1D) fused lasso. We leverage\nthese trails in a proximal algorithm that alternates between closed form primal\nupdates and fast dual trail updates. The resulting techinque is both faster\nthan previous GFL methods and more flexible in the choice of loss function and\ngraph structure. Furthermore, we present two algorithms for constructing trail\nsets and show empirically that they offer a tradeoff between preprocessing time\nand convergence rate. \n\n"}
{"id": "1505.07071", "contents": "Title: The climatological relationships between wind and solar energy supply in\n  Britain Abstract: We use reanalysis data to investigate the daily co-variability of wind and\nsolar irradiance in Britain, and its implications for renewable energy supply\nbalancing. The joint distribution of daily-mean wind speeds and irradiances\nshows that irradiance has a much stronger seasonal cycle than wind, due to the\nrotational tilt of the Earth. Irradiance is weakly anticorrelated with wind\nspeed throughout the year ($-0.4 \\lesssim \\rho \\lesssim -0.2$): there is a weak\ntendency for windy days to be cloudier. This is particularly true in\nAtlantic-facing regions (western Scotland, south-west England). The east coast\nof Britain has the weakest anticorrelation, particularly in winter, primarily\nassociated with a relative increase in the frequency of clear-but-windy days.\nWe also consider the variability in total power output from onshore wind\nturbines and solar photovoltaic panels. In all months, daily variability in\ntotal power is always reduced by incorporating solar capacity. The scenario\nwith the least seasonal variability is approximately 70%-solar to 30%-wind.\nThis work emphasises the importance of considering the full distribution of\ndaily behaviour rather than relying on long-term average relationships or\ncorrelations. In particular, the anticorrelation between wind and solar power\nin Britain cannot solely be relied upon to produce a well-balanced energy\nsupply. \n\n"}
{"id": "1505.07643", "contents": "Title: Cumulant expansions for atmospheric flows Abstract: The equations governing atmospheric flows are nonlinear. Consequently, the\nhierarchy of cumulant equations is not closed. But because atmospheric flows\nare inhomogeneous and anisotropic, the nonlinearity may manifest itself only\nweakly through interactions of mean fields with disturbances such as thermals\nor eddies. In such situations, truncations of the hierarchy of cumulant\nequations hold promise as a closure strategy.\n  We review how truncations at second order can be used to model and elucidate\nthe dynamics of atmospheric flows. Two examples are considered. First, we study\nthe growth of a dry convective boundary layer, which is heated from below,\nleading to turbulent upward energy transport and growth of the boundary layer.\nWe demonstrate that a quasilinear truncation of the equations of motion, in\nwhich interactions of disturbances among each other are neglected but\ninteractions with mean fields are taken into account, can capture the growth of\nthe convective boundary layer even if it does not capture important turbulent\ntransport terms. Second, we study the evolution of two-dimensional large-scale\nwaves representing waves in Earth's upper atmosphere. We demonstrate that a\ncumulant expansion truncated at second order (CE2) can capture the evolution of\nsuch waves and their nonlinear interaction with the mean flow in some\ncircumstances, for example, when the wave amplitude is small enough or the\nplanetary rotation rate is large enough. However, CE2 fails to capture the flow\nevolution when nonlinear eddy--eddy interactions in surf zones become\nimportant. Higher-order closures can capture these missing interactions.\n  The results point to new ways in which the dynamics of turbulent boundary\nlayers may be represented in climate models, and they illustrate different\nclasses of nonlinear processes that can control wave dissipation and momentum\nfluxes in the troposphere. \n\n"}
{"id": "1505.07856", "contents": "Title: Investigations into the impact of astronomical phenomena on the\n  terrestrial biosphere and climate Abstract: This thesis assesses the influence of astronomical phenomena on the Earth's\nbiosphere and climate. I examine in particular the relevance of both the path\nof the Sun through the Galaxy and the evolution of the Earth's orbital\nparameters in modulating non-terrestrial mechanisms. I build models to predict\nthe extinction rate of species, the temporal variation of the impact cratering\nrate and ice sheet deglaciations, and then compare these models with other\nmodels within a Bayesian framework. I find that the temporal distribution of\nmass extinction events over the past 550 Myr can be explained just as well by a\nuniform random distribution as by other models, such as variations in the\nstellar density local to the Sun arising from the Sun's orbit. Given the\nuncertainties in the Galaxy model and the Sun's current phase space\ncoordinates, as well as the errors in the geological data, it is not possible\nto draw a clear connection between terrestrial extinction and the solar motion.\nIn a separate study, I find that the solar motion, which modulates the Galactic\ntidal forces imposed on Oort cloud comets, does not significantly influence\nthis cratering rate. My dynamical models, together with the solar apex motion,\ncan explain the anisotropic perihelia of long period comets without needing to\ninvoke the existence of a Jupiter-mass solar companion. Finally, I find that\nvariations in the Earth's obliquity play a dominant role in triggering\nterrestrial deglaciations over the past 2 Myr. The precession of the equinoxes,\nin contrast, only becomes important in pacing large deglaciations after the\ntransition from the 100-kyr dominant periodicity in the ice coverage to a\n41-kyr dominant periodicity, which occurred 0.7 Myr ago. \n\n"}
{"id": "1506.00138", "contents": "Title: Efficient Computation of Gaussian Likelihoods for Stationary Markov\n  Random Field Models Abstract: Rue and Held (2005) proposed a method for efficiently computing the Gaussian\nlikelihood for stationary Markov random field models, when the data locations\nfall on a complete regular grid, and the model has no additive error term. The\ncalculations rely on the availability of the covariances. We prove a theorem\ngiving the rate of convergence of a spectral method of computing the\ncovariances, establishing that the error decays faster than any polynomial in\nthe size of the computing grid. We extend the exact likelihood calculations to\nthe case of non-rectangular domains and missing values on the interior of the\ngrid and to the case when an additive uncorrelated error term (nugget) is\npresent in the model. We also give an alternative formulation of the likelihood\nthat has a smaller memory burden, parts of which can be computed in parallel.\nWe show in simulations that using the exact likelihood can give far better\nparameter estimates than using standard Markov random field approximations.\nHaving access to the exact likelihood allows for model comparisons via\nlikelihood ratios on large datasets, so as an application of the methods, we\ncompare several state-of-the-art methods for large spatial datasets on an\naerosol optical thickness dataset. We find that simple block independent\nlikelihood and composite likelihood methods outperform stochastic partial\ndifferential equation approximations in terms of computation time and returning\nparameter estimates that nearly maximize the likelihood. \n\n"}
{"id": "1506.00552", "contents": "Title: Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than\n  Random Selection Abstract: There has been significant recent work on the theory and application of\nrandomized coordinate descent algorithms, beginning with the work of Nesterov\n[SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection\nrule achieves the same convergence rate as the Gauss-Southwell selection rule.\nThis result suggests that we should never use the Gauss-Southwell rule, as it\nis typically much more expensive than random selection. However, the empirical\nbehaviours of these algorithms contradict this theoretical result: in\napplications where the computational costs of the selection rules are\ncomparable, the Gauss-Southwell selection rule tends to perform substantially\nbetter than random coordinate selection. We give a simple analysis of the\nGauss-Southwell rule showing that---except in extreme cases---its convergence\nrate is faster than choosing random coordinates. Further, in this work we (i)\nshow that exact coordinate optimization improves the convergence rate for\ncertain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that\ngives an even faster convergence rate given knowledge of the Lipschitz\nconstants of the partial derivatives, (iii) analyze the effect of approximate\nGauss-Southwell rules, and (iv) analyze proximal-gradient variants of the\nGauss-Southwell rule. \n\n"}
{"id": "1506.01015", "contents": "Title: On the Late-Time Behaviour of a Bounded, Inviscid Two-Dimensional Flow Abstract: Using complementary numerical approaches at high resolution, we study the\nlate-time behaviour of an inviscid, incompressible two-dimensional flow on the\nsurface of a sphere. Starting from a random initial vorticity field comprised\nof a small set of intermediate wavenumber spherical harmonics, we find that --\ncontrary to the predictions of equilibrium statistical mechanics -- the flow\ndoes not evolve into a large-scale steady state. Instead, significant\nunsteadiness persists, characterised by a population of persistent small-scale\nvortices interacting with a large-scale oscillating quadrupolar vorticity\nfield. Moreover, the vorticity develops a stepped, staircase distribution,\nconsisting of nearly homogeneous regions separated by sharp gradients. The\npersistence of unsteadiness is explained by a simple point vortex model\ncharacterising the interactions between the four main vortices which emerge. \n\n"}
{"id": "1506.01326", "contents": "Title: Probabilistic Numerics and Uncertainty in Computations Abstract: We deliver a call to arms for probabilistic numerical methods: algorithms for\nnumerical tasks, including linear algebra, integration, optimization and\nsolving differential equations, that return uncertainties in their\ncalculations. Such uncertainties, arising from the loss of precision induced by\nnumerical calculation with limited time or hardware, are important for much\ncontemporary science and industry. Within applications such as climate science\nand astrophysics, the need to make decisions on the basis of computations with\nlarge and complex data has led to a renewed focus on the management of\nnumerical uncertainty. We describe how several seminal classic numerical\nmethods can be interpreted naturally as probabilistic inference. We then show\nthat the probabilistic view suggests new algorithms that can flexibly be\nadapted to suit application specifics, while delivering improved empirical\nperformance. We provide concrete illustrations of the benefits of probabilistic\nnumeric algorithms on real scientific problems from astrometry and astronomical\nimaging, while highlighting open problems with these new algorithms. Finally,\nwe describe how probabilistic numerical methods provide a coherent framework\nfor identifying the uncertainty in calculations performed with a combination of\nnumerical algorithms (e.g. both numerical optimisers and differential equation\nsolvers), potentially allowing the diagnosis (and control) of error sources in\ncomputations. \n\n"}
{"id": "1506.02267", "contents": "Title: Computationally Efficient Bayesian Learning of Gaussian Process State\n  Space Models Abstract: Gaussian processes allow for flexible specification of prior assumptions of\nunknown dynamics in state space models. We present a procedure for efficient\nBayesian learning in Gaussian process state space models, where the\nrepresentation is formed by projecting the problem onto a set of approximate\neigenfunctions derived from the prior covariance structure. Learning under this\nfamily of models can be conducted using a carefully crafted particle MCMC\nalgorithm. This scheme is computationally efficient and yet allows for a fully\nBayesian treatment of the problem. Compared to conventional system\nidentification tools or existing learning methods, we show competitive\nperformance and reliable quantification of uncertainties in the model. \n\n"}
{"id": "1506.03074", "contents": "Title: Variational consensus Monte Carlo Abstract: Practitioners of Bayesian statistics have long depended on Markov chain Monte\nCarlo (MCMC) to obtain samples from intractable posterior distributions.\nUnfortunately, MCMC algorithms are typically serial, and do not scale to the\nlarge datasets typical of modern machine learning. The recently proposed\nconsensus Monte Carlo algorithm removes this limitation by partitioning the\ndata and drawing samples conditional on each partition in parallel (Scott et\nal, 2013). A fixed aggregation function then combines these samples, yielding\napproximate posterior samples. We introduce variational consensus Monte Carlo\n(VCMC), a variational Bayes algorithm that optimizes over aggregation functions\nto obtain samples from a distribution that better approximates the target. The\nresulting objective contains an intractable entropy term; we therefore derive a\nrelaxation of the objective and show that the relaxed problem is blockwise\nconcave under mild conditions. We illustrate the advantages of our algorithm on\nthree inference tasks from the literature, demonstrating both the superior\nquality of the posterior approximation and the moderate overhead of the\noptimization step. Our algorithm achieves a relative error reduction (measured\nagainst serial MCMC) of up to 39% compared to consensus Monte Carlo on the task\nof estimating 300-dimensional probit regression parameter expectations;\nsimilarly, it achieves an error reduction of 92% on the task of estimating\ncluster comembership probabilities in a Gaussian mixture model with 8\ncomponents in 8 dimensions. Furthermore, these gains come at moderate cost\ncompared to the runtime of serial MCMC, achieving near-ideal speedup in some\ninstances. \n\n"}
{"id": "1506.03159", "contents": "Title: Copula variational inference Abstract: We develop a general variational inference method that preserves dependency\namong the latent variables. Our method uses copulas to augment the families of\ndistributions used in mean-field and structured approximations. Copulas model\nthe dependency that is not captured by the original variational distribution,\nand thus the augmented variational family guarantees better approximations to\nthe posterior. With stochastic optimization, inference on the augmented\ndistribution is scalable. Furthermore, our strategy is generic: it can be\napplied to any inference procedure that currently uses the mean-field or\nstructured approach. Copula variational inference has many advantages: it\nreduces bias; it is less sensitive to local optima; it is less sensitive to\nhyperparameters; and it helps characterize and interpret the dependency among\nthe latent variables. \n\n"}
{"id": "1506.03670", "contents": "Title: Spatially adaptive covariance tapering Abstract: Covariance tapering is a popular approach for reducing the computational cost\nof spatial prediction and parameter estimation for Gaussian process models.\nHowever, tapering can have poor performance when the process is sampled at\nspatially irregular locations or when non-stationary covariance models are\nused. This work introduces an adaptive tapering method in order to improve the\nperformance of tapering in these problematic cases. This is achieved by\nintroducing a computationally convenient class of compactly supported\nnon-stationary covariance functions, combined with a new method for choosing\nspatially varying taper ranges. Numerical experiments are used to show that the\nperformance of both kriging prediction and parameter estimation can be improved\nby allowing for spatially varying taper ranges. However, although adaptive\ntapering outperforms regular tapering, simply dividing the data into blocks and\nignoring the dependence between the blocks is often a better method for\nparameter estimation. \n\n"}
{"id": "1506.05555", "contents": "Title: Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with\n  Random Bases Abstract: For big data analysis, high computational cost for Bayesian methods often\nlimits their applications in practice. In recent years, there have been many\nattempts to improve computational efficiency of Bayesian inference. Here we\npropose an efficient and scalable computational technique for a\nstate-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian\nMonte Carlo (HMC). The key idea is to explore and exploit the structure and\nregularity in parameter space for the underlying probabilistic model to\nconstruct an effective approximation of its geometric properties. To this end,\nwe build a surrogate function to approximate the target distribution using\nproperly chosen random bases and an efficient optimization process. The\nresulting method provides a flexible, scalable, and efficient sampling\nalgorithm, which converges to the correct target distribution. We show that by\nchoosing the basis functions and optimization process differently, our method\ncan be related to other approaches for the construction of surrogate functions\nsuch as generalized additive models or Gaussian process models. Experiments\nbased on simulated and real data show that our approach leads to substantially\nmore efficient sampling algorithms compared to existing state-of-the art\nmethods. \n\n"}
{"id": "1506.05863", "contents": "Title: Towards the azimuthal characteristics of ionospheric and seismic effects\n  of \"Chelyabinsk\" meteorite fall according to the data from coherent radar,\n  GPS and seismic networks Abstract: We present the results of a study of the azimuthal characteristics of\nionospheric and seismic effects of the meteorite 'Chelyabinsk', based on the\ndata from the network of GPS receivers, coherent decameter radar EKB SuperDARN\nand network of seismic stations.\n  It is shown, that 6-14 minutes after the bolide explosion, GPS network\nobserved the cone-shaped wavefront of TIDs that is interpreted as a ballistic\nacoustic wave. The typical TIDs propagation velocity were observed\n661+/-256m/s, which corresponds to the expected acoustic wave speed for 240km\nheight. 14 minutes after the bolide explosion, at distances of 200km we\nobserved the emergence and propagation of a TID with spherical wavefront, that\nis interpreted as gravitational mode of internal acoustic waves. The\npropagation velocity of this TID was 337+/-89m/s which corresponds to the\npropagation velocity of these waves in similar situations. At EKB SuperDARN\nradar, we observed TIDs in the sector of azimuthal angles close to the\nperpendicular to the meteorite trajectory. The observed TID velocity (400 m/s)\nand azimuthal properties correlate well with the model of ballistic wave\npropagating at 120-140km altitude.\n  It is shown, that the azimuthal distribution of the amplitude of vertical\nseismic oscillations can be described qualitatively by the model of vertical\nstrike-slip rupture, propagating at 1km/s along the meteorite fall trajectory\nto distance of about 40km. These parameters correspond to the direction and\nvelocity of propagation of the ballistic wave peak by the ground. It is shown,\nthat the model of ballistic wave caused by supersonic motion and burning of the\nmeteorite in the upper atmosphere can satisfactorily explain the various\nazimuthal ionospheric effects, observed by the coherent decameter radar EKB\nSuperDARN, GPS-receivers network, as well as the azimuthal characteristics of\nseismic waves at large distances. \n\n"}
{"id": "1506.05936", "contents": "Title: Sampling constrained probability distributions using Spherical\n  Augmentation Abstract: Statistical models with constrained probability distributions are abundant in\nmachine learning. Some examples include regression models with norm constraints\n(e.g., Lasso), probit, many copula models, and latent Dirichlet allocation\n(LDA). Bayesian inference involving probability distributions confined to\nconstrained domains could be quite challenging for commonly used sampling\nalgorithms. In this paper, we propose a novel augmentation technique that\nhandles a wide range of constraints by mapping the constrained domain to a\nsphere in the augmented space. By moving freely on the surface of this sphere,\nsampling algorithms handle constraints implicitly and generate proposals that\nremain within boundaries when mapped back to the original space. Our proposed\nmethod, called {Spherical Augmentation}, provides a mathematically natural and\ncomputationally efficient framework for sampling from constrained probability\ndistributions. We show the advantages of our method over state-of-the-art\nsampling algorithms, such as exact Hamiltonian Monte Carlo, using several\nexamples including truncated Gaussian distributions, Bayesian Lasso, Bayesian\nbridge regression, reconstruction of quantized stationary Gaussian process, and\nLDA for topic modeling. \n\n"}
{"id": "1506.07044", "contents": "Title: Monte Carlo Methods for the Ferromagnetic Potts Model Using Factor Graph\n  Duality Abstract: Normal factor graph duality offers new possibilities for Monte Carlo\nalgorithms in graphical models. Specifically, we consider the problem of\nestimating the partition function of the ferromagnetic Ising and Potts models\nby Monte Carlo methods, which are known to work well at high temperatures, but\nto fail at low temperatures. We propose Monte Carlo methods (uniform sampling\nand importance sampling) in the dual normal factor graph, and demonstrate that\nthey behave differently: they work particularly well at low temperatures. By\ncomparing the relative error in estimating the partition function, we show that\nthe proposed importance sampling algorithm significantly outperforms the\nstate-of-the-art deterministic and Monte Carlo methods. For the ferromagnetic\nIsing model in an external field, we show the equivalence between the valid\nconfigurations in the dual normal factor graph and the terms that appear in the\nhigh-temperature series expansion of the partition function. Following this\nresult, we discuss connections with Jerrum-Sinclair's polynomial randomized\napproximation scheme (the subgraphs-world process) for evaluating the partition\nfunction of ferromagnetic \n\n"}
{"id": "1506.08640", "contents": "Title: Leave Pima Indians alone: binary regression as a benchmark for Bayesian\n  computation Abstract: Abstract. Whenever a new approach to perform Bayesian computation is\nintroduced, a common practice is to showcase this approach on a binary\nregression model and datasets of moderate size. This paper discusses to which\nextent this practice is sound. It also reviews the current state of the art of\nBayesian computation, using binary regression as a running example. Both\nsampling-based algorithms (importance sampling, MCMC and SMC) and fast\napproximations (Laplace and EP) are covered. Extensive numerical results are\nprovided, some of which might go against conventional wisdom regarding the\neffectiveness of certain algorithms. Implications for other problems (variable\nselection) and other models are also discussed. \n\n"}
{"id": "1507.01571", "contents": "Title: Unified functional network and nonlinear time series analysis for\n  complex systems science: The pyunicorn package Abstract: We introduce the \\texttt{pyunicorn} (Pythonic unified complex network and\nrecurrence analysis toolbox) open source software package for applying and\ncombining modern methods of data analysis and modeling from complex network\ntheory and nonlinear time series analysis. \\texttt{pyunicorn} is a fully\nobject-oriented and easily parallelizable package written in the language\nPython. It allows for the construction of functional networks such as climate\nnetworks in climatology or functional brain networks in neuroscience\nrepresenting the structure of statistical interrelationships in large data sets\nof time series and, subsequently, investigating this structure using advanced\nmethods of complex network theory such as measures and models for spatial\nnetworks, networks of interacting networks, node-weighted statistics or network\nsurrogates. Additionally, \\texttt{pyunicorn} provides insights into the\nnonlinear dynamics of complex systems as recorded in uni- and multivariate time\nseries from a non-traditional perspective by means of recurrence quantification\nanalysis (RQA), recurrence networks, visibility graphs and construction of\nsurrogate time series. The range of possible applications of the library is\noutlined, drawing on several examples mainly from the field of climatology. \n\n"}
{"id": "1507.02646", "contents": "Title: Pareto Smoothed Importance Sampling Abstract: Importance weighting is a general way to adjust Monte Carlo integration to\naccount for draws from the wrong distribution, but the resulting estimate can\nbe highly variable when the importance ratios have a heavy right tail. This\nroutinely occurs when there are aspects of the target distribution that are not\nwell captured by the approximating distribution, in which case more stable\nestimates can be obtained by modifying extreme importance ratios. We present a\nnew method for stabilizing importance weights using a generalized Pareto\ndistribution fit to the upper tail of the distribution of the simulated\nimportance ratios. The method, which empirically performs better than existing\nmethods for stabilizing importance sampling estimates, includes stabilized\neffective sample size estimates, Monte Carlo error estimates, and convergence\ndiagnostics. The presented Pareto $\\hat{k}$ finite sample convergence rate\ndiagnostic is useful for any Monte Carlo estimator. \n\n"}
{"id": "1507.04528", "contents": "Title: A priori truncation method for posterior sampling from homogeneous\n  normalized completely random measure mixture models Abstract: This paper adopts a Bayesian nonparametric mixture model where the mixing\ndistribution belongs to the wide class of normalized homogeneous completely\nrandom measures. We propose a truncation method for the mixing distribution by\ndiscarding the weights of the unnormalized measure smaller than a threshold. We\nprove convergence in law of our approximation, provide some theoretical\nproperties and characterize its posterior distribution so that a blocked Gibbs\nsampler is devised. The versatility of the approximation is illustrated by two\ndifferent applications. In the first the normalized Bessel random measure,\nencompassing the Dirichlet process, is introduced; goodness of fit indexes show\nits good performances as mixing measure for density estimation. The second\ndescribes how to incorporate covariates in the support of the normalized\nmeasure, leading to a linear dependent model for regression and clustering. \n\n"}
{"id": "1507.06244", "contents": "Title: Emulation of Higher-Order Tensors in Manifold Monte Carlo Methods for\n  Bayesian Inverse Problems Abstract: The Bayesian approach to Inverse Problems relies predominantly on Markov\nChain Monte Carlo methods for posterior inference. The typical nonlinear\nconcentration of posterior measure observed in many such Inverse Problems\npresents severe challenges to existing simulation based inference methods.\nMotivated by these challenges the exploitation of local geometric information\nin the form of covariant gradients, metric tensors, Levi-Civita connections,\nand local geodesic flows, have been introduced to more effectively locally\nexplore the configuration space of the posterior measure. However, obtaining\nsuch geometric quantities usually requires extensive computational effort and\ndespite their effectiveness affect the applicability of these\ngeometrically-based Monte Carlo methods. In this paper we explore one way to\naddress this issue by the construction of an emulator of the model from which\nall geometric objects can be obtained in a much more computationally feasible\nmanner. The main concept is to approximate the geometric quantities using a\nGaussian Process emulator which is conditioned on a carefully chosen design set\nof configuration points, which also determines the quality of the emulator. To\nthis end we propose the use of statistical experiment design methods to refine\na potentially arbitrarily initialized design online without destroying the\nconvergence of the resulting Markov chain to the desired invariant measure. The\npractical examples considered in this paper provide a demonstration of the\nsignificant improvement possible in terms of computational loading suggesting\nthis is a promising avenue of further development. \n\n"}
{"id": "1507.06736", "contents": "Title: The sample complexity of weighted sparse approximation Abstract: For Gaussian sampling matrices, we provide bounds on the minimal number of\nmeasurements $m$ required to achieve robust weighted sparse recovery guarantees\nin terms of how well a given prior model for the sparsity support aligns with\nthe true underlying support. Our main contribution is that for a sparse vector\n${\\bf x} \\in \\mathbb{R}^N$ supported on an unknown set $\\mathcal{S} \\subset\n\\{1, \\dots, N\\}$ with $|\\mathcal{S}|\\leq k$, if $\\mathcal{S}$ has\n\\emph{weighted cardinality} $\\omega(\\mathcal{S}) := \\sum_{j \\in \\mathcal{S}}\n\\omega_j^2$, and if the weights on $\\mathcal{S}^c$ exhibit mild growth,\n$\\omega_j^2 \\geq \\gamma \\log(j/\\omega(\\mathcal{S}))$ for $j\\in\\mathcal{S}^c$\nand $\\gamma > 0$, then the sample complexity for sparse recovery via weighted\n$\\ell_1$-minimization using weights $\\omega_j$ is linear in the weighted\nsparsity level, and $m = \\mathcal{O}(\\omega(\\mathcal{S})/\\gamma)$. This main\nresult is a generalization of special cases including a) the standard sparse\nrecovery setting where all weights $\\omega_j \\equiv 1$, and $m =\n\\mathcal{O}\\left(k\\log\\left(N/k\\right)\\right)$; b) the setting where the\nsupport is known a priori, and $m = \\mathcal{O}(k)$; and c) the setting of\nsparse recovery with prior information, and $m$ depends on how well the weights\nare aligned with the support set $\\mathcal{S}$. We further extend the results\nin case c) to the setting of additive noise. Our results are {\\em nonuniform}\nthat is they apply for a fixed support, unknown a priori, and the weights on\n$\\mathcal{S}$ do not all have to be smaller than the weights on $\\mathcal{S}^c$\nfor our recovery results to hold. \n\n"}
{"id": "1507.07445", "contents": "Title: Assessment of petrophysical quantities inspired by joint multifractal\n  approach Abstract: In this paper joint multifractal random walk approach is carried out to\nanalyze some petrophysical quantities for characterizing the petroleum\nreservoir. These quantities include Gamma emission (GR), sonic transient time\n(DT) and Neutron porosity (NPHI) which are collected from four wells of a\nreservoir. To quantify mutual interaction of petrophysical quantities, joint\nmultifractal random walk is implemented. This approach is based on the mutual\nmultiplicative cascade notion in the multifractal formalism and in this\napproach $L_0$ represents a benchmark to describe the nature of\ncross-correlation between two series. The analysis of the petrophysical\nquantities revealed that GR for all wells has strongly multifractal nature due\nto the considerable abundance of large fluctuations in various scales. The\nvariance of probability distribution function, $\\lambda_{\\ell}^2$, at scale\n$\\ell$ and its intercept determine the multifractal properties of the data sets\nsourced by probability density function. The value of $\\lambda_0 ^2$ for NPHI\ndata set is less than GR's, however, DT shows a nearly monofractal behavior,\nnamely $\\lambda_0 ^2\\rightarrow 0$, so we find that $\\lambda_0^2({\\rm\nGR})>\\lambda_0^2({\\rm NPHI})\\gg\\lambda_0^2({\\rm DT})$. While, the value of\nHurst exponents can not discriminate between series GR, NPHI and DT. Joint\nanalysis of the petrophysical quantities for considered wells demonstrates that\n$L_0$ has negative value for GR-NPHI confirming that finding shaly layers is in\ncompetition with finding porous medium while it takes positive value for GR-DT\ndetermining that continuum medium can be detectable by evaluating the\nstatistical properties of GR and its cross-correlation to DT signal. \n\n"}
{"id": "1507.07873", "contents": "Title: Are rogue waves really unexpected? Abstract: An unexpected wave is defined by Gemmrich & Garrett (2008) as a wave that is\nmuch taller than a set of neighboring waves. Their definition of \"unexpected\"\nrefers to a wave that is not anticipated by a casual observer. Clearly,\nunexpected waves defined in this way are predictable in a statistical sense.\nThey can occur relatively often with a small or moderate crest height, but\nlarge unexpected waves that are rogue are rare. Here, this concept is\nelaborated and statistically described based on a third-order nonlinear model.\nIn particular, the conditional return period of an unexpected wave whose crest\nexceeds a given threshold is developed. This definition leads to greater return\nperiods or on average less frequent occurrences of unexpected waves than those\nimplied by the conventional return periods not conditioned on a reference\nthreshold. Ultimately, it appears that a rogue wave that is also unexpected\nwould have a lower occurrence frequency than that of a usual rogue wave. As\nspecific applications, the Andrea and WACSIS rogue wave events are examined in\ndetail. Both waves appeared without warning and their crests were nearly\n$2$-times larger than the surrounding $O(10)$ wave crests, and thus unexpected.\nThe two crest heights are nearly the same as the\nthreshold~$h_{0.3\\cdot10^{6}}\\sim1.6H_{s}$ exceeded on average once\nevery~$0.3\\cdot 10^{6}$ waves, where $H_s$ is the significant wave height. In\ncontrast, the Andrea and WACSIS events, as both rogue and unexpected, would\noccur slightly less often and on average once every~$3\\cdot10^{6}$\nand~$0.6\\cdot10^6$ waves respectively. \n\n"}
{"id": "1508.00793", "contents": "Title: Power-Expected-Posterior Priors for Generalized Linear Models Abstract: The power-expected-posterior (PEP) prior provides an objective, automatic,\nconsistent and parsimonious model selection procedure. At the same time it\nresolves the conceptual and computational problems due to the use of imaginary\ndata. Namely, (i) it dispenses with the need to select and average across all\npossible minimal imaginary samples, and (ii) it diminishes the effect that the\nimaginary data have upon the posterior distribution. These attributes allow for\nlarge sample approximations, when needed, in order to reduce the computational\nburden under more complex models. In this work we generalize the applicability\nof the PEP methodology, focusing on the framework of generalized linear models\n(GLMs), by introducing two new PEP definitions which are in effect applicable\nto any general model setting. Hyper-prior extensions for the power parameter\nthat regulates the contribution of the imaginary data are introduced. We\nfurther study the validity of the predictive matching and of the model\nselection consistency, providing analytical proofs for the former and empirical\nevidence supporting the latter. For estimation of posterior model and inclusion\nprobabilities we introduce a tuning-free Gibbs-based variable selection\nsampler. Several simulation scenarios and one real life example are considered\nin order to evaluate the performance of the proposed methods compared to other\ncommonly used approaches based on mixtures of g-priors. Results indicate that\nthe GLM-PEP priors are more effective in the identification of sparse and\nparsimonious model formulations. \n\n"}
{"id": "1508.02766", "contents": "Title: FFT-Based Fast Computation of Multivariate Kernel Estimators with\n  Unconstrained Bandwidth Matrices Abstract: The problem of fast computation of multivariate kernel density estimation\n(KDE) is still an open research problem. In our view, the existing solutions do\nnot resolve this matter in a satisfactory way. One of the most elegant and\nefficient approach utilizes the fast Fourier transform. Unfortunately, the\nexisting FFT-based solution suffers from a serious limitation, as it can\naccurately operate only with the constrained (i.e., diagonal) multivariate\nbandwidth matrices. In this paper we describe the problem and give a\nsatisfactory solution. The proposed solution may be successfully used also in\nother research problems, for example for the fast computation of the optimal\nbandwidth for KDE. \n\n"}
{"id": "1508.03700", "contents": "Title: Extreme event statistics of daily rainfall: Dynamical systems approach Abstract: We analyse the probability densities of daily rainfall amounts at a variety\nof locations on the Earth. The observed distributions of the amount of rainfall\nfit well to a q-exponential distribution with exponent q close to q=1.3. We\ndiscuss possible reasons for the emergence of this power law. On the contrary,\nthe waiting time distribution between rainy days is observed to follow a\nnear-exponential distribution. A careful investigation shows that a\nq-exponential with q=1.05 yields actually the best fit of the data. A Poisson\nprocess where the rate fluctuates slightly in a superstatistical way is\ndiscussed as a possible model for this. We discuss the extreme value statistics\nfor extreme daily rainfall, which can potentially lead to flooding. This is\ndescribed by Frechet distributions as the corresponding distributions of the\namount of daily rainfall decay with a power law. On the other hand, looking at\nextreme event statistics of waiting times between rainy days (leading to\ndroughts for very long dry periods) we obtain from the observed\nnear-exponential decay of waiting times an extreme event statistics close to\nGumbel distributions. We discuss superstatistical dynamical systems as simple\nmodels in this context. \n\n"}
{"id": "1508.03884", "contents": "Title: A simple sampler for the horseshoe estimator Abstract: In this note we derive a simple Bayesian sampler for linear regression with\nthe horseshoe hierarchy. A new interpretation of the horseshoe model is\npresented, and extensions to logistic regression and alternative hierarchies,\nsuch as horseshoe$+$, are discussed. Due to the conjugacy of the proposed\nhierarchy, Chib's algorithm may be used to easily compute the marginal\nlikelihood of the model. \n\n"}
{"id": "1508.06002", "contents": "Title: On a unified breaking onset threshold for gravity waves in deep and\n  intermediate depth water Abstract: We revisit the classical but as yet unresolved problem of predicting the\nbreaking onset of 2D and 3D irrotational gravity water waves. This study\nfocuses on domains with flat bottom topography and conditions ranging from deep\nto intermediate depth (depth to wavelength ratio from 1 to 0.2). Our\ncalculations based on a fully nonlinear boundary element model investigated\ngeometric, kinematic and energetic differences between maximally recurrent and\nmarginally breaking waves in focusing wave groups. Maximally steep non-breaking\n(maximally recurrent) waves are clearly separated from marginally breaking\nwaves by their normalised energy fluxes localized near the crest region. On the\nsurface, this reduces to the local ratio of the energy flux velocity (here the\nfluid velocity) to the crest point velocity for the tallest wave in the\nevolving group. This provides a robust threshold parameter for breaking onset\nfor 2D and 3D wave packets propagating in uniform water depths from deep to\nintermediate. Warning of imminent breaking onset was found to be detected up to\na fifth of a carrier wave period prior to a breaking event. \n\n"}
{"id": "1508.06235", "contents": "Title: Clustering With Side Information: From a Probabilistic Model to a\n  Deterministic Algorithm Abstract: In this paper, we propose a model-based clustering method (TVClust) that\nrobustly incorporates noisy side information as soft-constraints and aims to\nseek a consensus between side information and the observed data. Our method is\nbased on a nonparametric Bayesian hierarchical model that combines the\nprobabilistic model for the data instance and the one for the side-information.\nAn efficient Gibbs sampling algorithm is proposed for posterior inference.\nUsing the small-variance asymptotics of our probabilistic model, we then derive\na new deterministic clustering algorithm (RDP-means). It can be viewed as an\nextension of K-means that allows for the inclusion of side information and has\nthe additional property that the number of clusters does not need to be\nspecified a priori. Empirical studies have been carried out to compare our work\nwith many constrained clustering algorithms from the literature on both a\nvariety of data sets and under a variety of conditions such as using noisy side\ninformation and erroneous k values. The results of our experiments show strong\nresults for our probabilistic and deterministic approaches under these\nconditions when compared to other algorithms in the literature. \n\n"}
{"id": "1508.06700", "contents": "Title: A surrogate accelerated multicanonical Monte Carlo method for\n  uncertainty quantification Abstract: In this work we consider a class of uncertainty quantification problems where\nthe system performance or reliability is characterized by a scalar parameter\n$y$. The performance parameter $y$ is random due to the presence of various\nsources of uncertainty in the system, and our goal is to estimate the\nprobability density function (PDF) of $y$. We propose to use the multicanonical\nMonte Carlo (MMC) method, a special type of adaptive importance sampling\nalgorithm, to compute the PDF of interest. Moreover, we develop an adaptive\nalgorithm to construct local Gaussian process surrogates to further accelerate\nthe MMC iterations. With numerical examples we demonstrate that the proposed\nmethod can achieve several orders of magnitudes of speedup over the standard\nMonte Carlo method. \n\n"}
{"id": "1509.03521", "contents": "Title: Similarity-based semi-local estimation of EMOS models Abstract: Weather forecasts are typically given in the form of forecast ensembles\nobtained from multiple runs of numerical weather prediction models with varying\ninitial conditions and physics parameterizations. Such ensemble predictions\ntend to be biased and underdispersive and thus require statistical\npostprocessing. In the ensemble model output statistics (EMOS) approach, a\nprobabilistic forecast is given by a single parametric distribution with\nparameters depending on the ensemble members. This article proposes two\nsemi-local methods for estimating the EMOS coefficients where the training data\nfor a specific observation station are augmented with corresponding forecast\ncases from stations with similar characteristics. Similarities between stations\nare determined using either distance functions or clustering based on various\nfeatures of the climatology, forecast errors, ensemble predictions and\nlocations of the observation stations. In a case study on wind speed over\nEurope with forecasts from the Grand Limited Area Model Ensemble Prediction\nSystem, the proposed similarity-based semi-local models show significant\nimprovement in predictive performance compared to standard regional and local\nestimation methods. They further allow for estimating complex models without\nnumerical stability issues and are computationally more efficient than local\nparameter estimation. \n\n"}
{"id": "1509.04752", "contents": "Title: Bayesian inference for spatio-temporal spike-and-slab priors Abstract: In this work, we address the problem of solving a series of underdetermined\nlinear inverse problems subject to a sparsity constraint. We generalize the\nspike-and-slab prior distribution to encode a priori correlation of the support\nof the solution in both space and time by imposing a transformed Gaussian\nprocess on the spike-and-slab probabilities. An expectation propagation (EP)\nalgorithm for posterior inference under the proposed model is derived. For\nlarge scale problems, the standard EP algorithm can be prohibitively slow. We\ntherefore introduce three different approximation schemes to reduce the\ncomputational complexity. Finally, we demonstrate the proposed model using\nnumerical experiments based on both synthetic and real data sets. \n\n"}
{"id": "1509.06326", "contents": "Title: Statistical state dynamics of jet/wave coexistence in barotropic\n  beta-plane turbulence Abstract: Jets coexist with planetary scale waves in the turbulence of planetary\natmospheres. The coherent component of these structures arises from cooperative\ninteraction between the coherent structures and the incoherent small-scale\nturbulence in which they are embedded. It follows that theoretical\nunderstanding of the dynamics of jets and planetary scale waves requires\nadopting the perspective of statistical state dynamics (SSD) which comprises\nthe dynamics of the interaction between coherent and incoherent components in\nthe turbulent state. In this work the S3T implementation of SSD for barotropic\nbeta-plane turbulence is used to develop a theory for the jet/wave coexistence\nregime by separating the coherent motions consisting of the zonal jets together\nwith a selection of large-scale waves from the smaller scale motions which\nconstitute the incoherent component. It is found that mean flow/turbulence\ninteraction gives rise to jets that coexist with large-scale coherent waves in\na synergistic manner. Large-scale waves that would only exist as damped modes\nin the laminar jet are found to be transformed into exponentially growing waves\nby interaction with the incoherent small scale turbulence which results in a\nchange in the mode structure allowing the mode to tap the energy of the mean\njet. This mechanism of destabilization differs fundamentally and serves to\naugment the more familiar S3T instabilities in which jets and waves arise from\nhomogeneous turbulence with energy source exclusively from the incoherent eddy\nfield and provides further insight into the cooperative dynamics of the\njet/waves coexistence regime in planetary turbulence. \n\n"}
{"id": "1509.07138", "contents": "Title: Exact confidence intervals for the average causal effect on a binary\n  outcome Abstract: Based on the physical randomization of completely randomized experiments,\nRigdon and Hudgens (2015) propose two approaches to obtaining exact confidence\nintervals for the average causal effect on a binary outcome. They construct the\nfirst confidence interval by combining, with the Bonferroni adjustment, the\nprediction sets for treatment effects among treatment and control groups, and\nthe second one by inverting a series of randomization tests. With sample size\n$n$, their second approach requires performing $O(n^4)$ randomization tests. We\ndemonstrate that the physical randomization also justifies other ways to\nconstructing exact confidence intervals that are more computationally\nefficient. By exploiting recent advances in hypergeometric confidence intervals\nand the stochastic order information of randomization tests, we propose\napproaches that either do not need to invoke Monte Carlo, or require performing\nat most $O(n^2)$ randomization tests. We provide technical details and R code\nin the Supplementary Material. \n\n"}
{"id": "1509.08526", "contents": "Title: Kinematics of fluid particles on the sea surface. Hamiltonian theory Abstract: We derive the John-Sclavounos equations describing the motion of a fluid\nparticle on the sea surface from first principles using Lagrangian and\nHamiltonian formalisms applied to the motion of a frictionless particle\nconstrained on an unsteady surface. The main result is that vorticity generated\non a stress-free surface vanishes at a wave crest when the horizontal particle\nvelocity equals the crest propagation speed, which is the kinematic criterion\nfor wave breaking. If this holds for the largest crest, then the symplectic\ntwo-form associated with the Hamiltonian dynamics reduces instantaneously to\nthat associated with the motion of a particle in free flight, as if the surface\ndid not exist. Further, exploiting the conservation of the Hamiltonian function\nfor steady surfaces and traveling waves we show that particle velocities remain\nbounded at all times, ruling out the possibility of the finite-time blowup of\nsolutions. \n\n"}
{"id": "1509.09096", "contents": "Title: Comments on \"MSE minus CAPE is the True Conserved Variable for an\n  Adiabatically Lifted Parcel\" Abstract: In a recent paper, Romps (JAS, vol.72, p.3639-3646, 2015, hereafter R15)\nargues that the moist-air static energy (MSE) is only approximately conserved\nfor an adiabatically lifted parcel, and that the quantity \"MSE - CAPE\" could be\nused as a true conserved variable, where CAPE is the convective available\nenergy.\n  It is shown in this comment that the quantity denoted by CAPE in R15 is the\nopposite of the convective available energy. It is explained that the vertical\nadiabatic ascent considered in R15 is not realistic, since it generates\ncondensed water of the order of 10 to 20 g/kg at height above 6 km. Moreover,\nthe thermodynamic equations are written in R15 by making several assumptions,\nnot all of which are explicitly mentioned.\n  This comment aims to clarify the hypotheses made in R15. It will show that\nthese assumptions call into question the validity of the moist-air internal\nenergy, enthalpy and entropy functions in R15. It also demonstrates that it is\npossible to obtain more precise and general formulations for moist-air energy,\nenthalpy and entropy functions, in particular by using the third law of\nthermodynamics. The large differences between the thermodynamics formulas\nderived in R15 and those depending on the third law are illustrated by studying\na realistic pseudo-adiabatic vertical profile.\n  The same notations as in R15 will be used as far as possible in this comment. \n\n"}
{"id": "1510.02958", "contents": "Title: Pseudo-Marginal Slice Sampling Abstract: Markov chain Monte Carlo (MCMC) methods asymptotically sample from complex\nprobability distributions. The pseudo-marginal MCMC framework only requires an\nunbiased estimator of the unnormalized probability distribution function to\nconstruct a Markov chain. However, the resulting chains are harder to tune to a\ntarget distribution than conventional MCMC, and the types of updates available\nare limited. We describe a general way to clamp and update the random numbers\nused in a pseudo-marginal method's unbiased estimator. In this framework we can\nuse slice sampling and other adaptive methods. We obtain more robust Markov\nchains, which often mix more quickly. \n\n"}
{"id": "1510.03105", "contents": "Title: Kernel Sequential Monte Carlo Abstract: We propose kernel sequential Monte Carlo (KSMC), a framework for sampling\nfrom static target densities. KSMC is a family of sequential Monte Carlo\nalgorithms that are based on building emulator models of the current particle\nsystem in a reproducing kernel Hilbert space. We here focus on modelling\nnonlinear covariance structure and gradients of the target. The emulator's\ngeometry is adaptively updated and subsequently used to inform local proposals.\nUnlike in adaptive Markov chain Monte Carlo, continuous adaptation does not\ncompromise convergence of the sampler. KSMC combines the strengths of sequental\nMonte Carlo and kernel methods: superior performance for multimodal targets and\nthe ability to estimate model evidence as compared to Markov chain Monte Carlo,\nand the emulator's ability to represent targets that exhibit high degrees of\nnonlinearity. As KSMC does not require access to target gradients, it is\nparticularly applicable on targets whose gradients are unknown or prohibitively\nexpensive. We describe necessary tuning details and demonstrate the benefits of\nthe the proposed methodology on a series of challenging synthetic and\nreal-world examples. \n\n"}
{"id": "1510.03239", "contents": "Title: Formulations of moist thermodynamics for atmospheric modelling Abstract: Internal energy, enthalpy and entropy are the key quantities to study\nthermodynamic properties of the moist atmosphere, because they correspond to\nthe First (internal energy and enthalpy) and Second (entropy) Laws of\nthermodynamics. The aim of this chapter is to search for analytical formulas\nfor the specific values of enthalpy and entropy and for the moist-air mixture\ncomposing the atmosphere.\n  The Third Law of thermodynamics leads to the definition of absolute reference\nvalues for thermal enthalpies and entropies of all atmospheric species. It is\nshown in this Chapter 22 that it is possible to define and compute a general\nmoist-air entropy potential temperature, which is really an equivalent of the\nmoist-air specific entropy in all circumstances (saturated, or not saturated).\nSimilarly, it is shown that it is possible to define and compute the moist-air\nspecific enthalpy, which is different from the thermal part of what is called\nMoist-Static-Energy in atmospheric studies. \n\n"}
{"id": "1510.04254", "contents": "Title: New singularities for Stokes waves Abstract: In 1880, Stokes famously demonstrated that the singularity that occurs at the\ncrest of the steepest possible water wave in infinite depth must correspond to\na corner of $120^\\circ$. Here, the complex velocity scales like $f^{1/3}$ where\n$f$ is the complex potential. Later in 1973, Grant showed that for any wave\naway from the steepest configuration, the singularity $f = f^*$ moves into the\ncomplex plane, and is of order $(f-f^*)^{1/2}$ (J. Fluid Mech., vol. 59, 1973,\npp. 257-262). Grant conjectured that as the highest wave is approached, other\nsingularities must coalesce at the crest so as to cancel the square-root\nbehaviour. Despite recent advances, the complete singularity structure of the\nStokes wave is still not well understood. In this work, we develop numerical\nmethods for constructing the Riemann surface that represents the extension of\nthe water wave into the complex plane. We show that a countably infinite number\nof distinct singularities exists on other branches of the solution, and that\nthese singularities coalesce as Stokes' highest wave is approached. \n\n"}
{"id": "1511.01609", "contents": "Title: Linear Non-Gaussian Component Analysis via Maximum Likelihood Abstract: Independent component analysis (ICA) is popular in many applications,\nincluding cognitive neuroscience and signal processing. Due to computational\nconstraints, principal component analysis is used for dimension reduction prior\nto ICA (PCA+ICA), which could remove important information. The problem is that\ninteresting independent components (ICs) could be mixed in several principal\ncomponents that are discarded and then these ICs cannot be recovered. We\nformulate a linear non-Gaussian component model with Gaussian noise components.\nTo estimate this model, we propose likelihood component analysis (LCA), in\nwhich dimension reduction and latent variable estimation are achieved\nsimultaneously. Our method orders components by their marginal likelihood\nrather than ordering components by variance as in PCA. We present a parametric\nLCA using the logistic density and a semi-parametric LCA using tilted Gaussians\nwith cubic B-splines. Our algorithm is scalable to datasets common in\napplications (e.g., hundreds of thousands of observations across hundreds of\nvariables with dozens of latent components). In simulations, latent components\nare recovered that are discarded by PCA+ICA methods. We apply our method to\nmultivariate data and demonstrate that LCA is a useful data visualization and\ndimension reduction tool that reveals features not apparent from PCA or\nPCA+ICA. We also apply our method to an fMRI experiment from the Human\nConnectome Project and identify artifacts missed by PCA+ICA. We present\ntheoretical results on identifiability of the linear non-Gaussian component\nmodel and consistency of LCA. \n\n"}
{"id": "1511.03045", "contents": "Title: Influence of Atmospheric Electric Fields on the Radio Emission from\n  Extensive Air Showers Abstract: The atmospheric electric fields in thunderclouds have been shown to\nsignificantly modify the intensity and polarization patterns of the radio\nfootprint of cosmic-ray-induced extensive air showers. Simulations indicated a\nvery non-linear dependence of the signal strength in the frequency window of\n30-80 MHz on the magnitude of the atmospheric electric field. In this work we\npresent an explanation of this dependence based on Monte-Carlo simulations,\nsupported by arguments based on electron dynamics in air showers and expressed\nin terms of a simplified model. We show that by extending the frequency window\nto lower frequencies additional sensitivity to the atmospheric electric field\nis obtained. \n\n"}
{"id": "1511.03095", "contents": "Title: Generalized Multiple Importance Sampling Abstract: Importance Sampling methods are broadly used to approximate posterior\ndistributions or some of their moments. In its standard approach, samples are\ndrawn from a single proposal distribution and weighted properly. However, since\nthe performance depends on the mismatch between the targeted and the proposal\ndistributions, several proposal densities are often employed for the generation\nof samples. Under this Multiple Importance Sampling (MIS) scenario, many works\nhave addressed the selection or adaptation of the proposal distributions,\ninterpreting the sampling and the weighting steps in different ways. In this\npaper, we establish a general framework for sampling and weighing procedures\nwhen more than one proposal are available. The most relevant MIS schemes in the\nliterature are encompassed within the new framework, and, moreover novel valid\nschemes appear naturally. All the MIS schemes are compared and ranked in terms\nof the variance of the associated estimators. Finally, we provide illustrative\nexamples which reveal that, even with a good choice of the proposal densities,\na careful interpretation of the sampling and weighting procedures can make a\nsignificant difference in the performance of the method. \n\n"}
{"id": "1511.05309", "contents": "Title: Optimized Linear Imputation Abstract: Often in real-world datasets, especially in high dimensional data, some\nfeature values are missing. Since most data analysis and statistical methods do\nnot handle gracefully missing values, the first step in the analysis requires\nthe imputation of missing values. Indeed, there has been a long standing\ninterest in methods for the imputation of missing values as a pre-processing\nstep. One recent and effective approach, the IRMI stepwise regression\nimputation method, uses a linear regression model for each real-valued feature\non the basis of all other features in the dataset. However, the proposed\niterative formulation lacks convergence guarantee. Here we propose a closely\nrelated method, stated as a single optimization problem and a block\ncoordinate-descent solution which is guaranteed to converge to a local minimum.\nExperiments show results on both synthetic and benchmark datasets, which are\ncomparable to the results of the IRMI method whenever it converges. However,\nwhile in the set of experiments described here IRMI often does not converge,\nthe performance of our methods is shown to be markedly superior in comparison\nwith other methods. \n\n"}
{"id": "1511.05877", "contents": "Title: Generation of scenarios from calibrated ensemble forecasts with a dual\n  ensemble copula coupling approach Abstract: Probabilistic forecasts in the form of ensemble of scenarios are required for\ncomplex decision making processes. Ensemble forecasting systems provide such\nproducts but the spatio-temporal structures of the forecast uncertainty is lost\nwhen statistical calibration of the ensemble forecasts is applied for each lead\ntime and location independently. Non-parametric approaches allow the\nreconstruction of spatio-temporal joint probability distributions at a low\ncomputational cost. For example, the ensemble copula coupling (ECC) method\nrebuilds the multivariate aspect of the forecast from the original ensemble\nforecasts. Based on the assumption of error stationarity, parametric methods\naim to fully describe the forecast dependence structures. In this study, the\nconcept of ECC is combined with past data statistics in order to account for\nthe autocorrelation of the forecast error. The new approach, called d-ECC, is\napplied to wind forecasts from the high resolution ensemble system COSMO-DE-EPS\nrun operationally at the German weather service. Scenarios generated by ECC and\nd-ECC are compared and assessed in the form of time series by means of\nmultivariate verification tools and in a product oriented framework.\nVerification results over a 3 month period show that the innovative method\nd-ECC outperforms or performs as well as ECC in all investigated aspects. \n\n"}
{"id": "1511.06196", "contents": "Title: Importance Sampling: Intrinsic Dimension and Computational Cost Abstract: The basic idea of importance sampling is to use independent samples from a\nproposal measure in order to approximate expectations with respect to a target\nmeasure. It is key to understand how many samples are required in order to\nguarantee accurate approximations. Intuitively, some notion of distance between\nthe target and the proposal should determine the computational cost of the\nmethod. A major challenge is to quantify this distance in terms of parameters\nor statistics that are pertinent for the practitioner. The subject has\nattracted substantial interest from within a variety of communities. The\nobjective of this paper is to overview and unify the resulting literature by\ncreating an overarching framework. A general theory is presented, with a focus\non the use of importance sampling in Bayesian inverse problems and filtering. \n\n"}
{"id": "1512.01027", "contents": "Title: Discrete Equilibrium Sampling with Arbitrary Nonequilibrium Processes Abstract: We present a novel framework for performing statistical sampling, expectation\nestimation, and partition function approximation using \\emph{arbitrary}\nheuristic stochastic processes defined over discrete state spaces. Using a\nhighly parallel construction we call the \\emph{sequential constraining\nprocess}, we are able to simultaneously generate states with the heuristic\nprocess and accurately estimate their probabilities, even when they are far too\nsmall to be realistically inferred by direct counting. After showing that both\ntheoretically correct importance sampling and Markov chain Monte Carlo are\npossible using the sequential constraining process, we integrate it into a\nmethodology called \\emph{state space sampling}, extending the ideas of state\nspace search from computer science to the sampling context. The methodology\ncomprises a dynamic data structure that constructs a robust Bayesian model of\nthe statistics generated by the heuristic process subject to an accuracy\nconstraint, the posterior Kullback-Leibler divergence. Sampling from the\ndynamic structure will generally yield partial states, which are completed by\nrecursively calling the heuristic to refine the structure and resuming the\nsampling. Our experiments on various Ising models suggest that state space\nsampling enables heuristic state generation with accurate probability\nestimates, demonstrated by illustrating the convergence of a simulated\nannealing process to the Boltzmann distribution with increasing run length.\nConsequently, heretofore unprecedented direct importance sampling using the\n\\emph{final} (marginal) distribution of a generic stochastic process is\nallowed, potentially augmenting the range of algorithms at the Monte Carlo\npractitioner's disposal. \n\n"}
{"id": "1512.01139", "contents": "Title: Kalman-based Stochastic Gradient Method with Stop Condition and\n  Insensitivity to Conditioning Abstract: Modern proximal and stochastic gradient descent (SGD) methods are believed to\nefficiently minimize large composite objective functions, but such methods have\ntwo algorithmic challenges: (1) a lack of fast or justified stop conditions,\nand (2) sensitivity to the objective function's conditioning. In response to\nthe first challenge, modern proximal and SGD methods guarantee convergence only\nafter multiple epochs, but such a guarantee renders proximal and SGD methods\ninfeasible when the number of component functions is very large or infinite. In\nresponse to the second challenge, second order SGD methods have been developed,\nbut they are marred by the complexity of their analysis. In this work, we\naddress these challenges on the limited, but important, linear regression\nproblem by introducing and analyzing a second order proximal/SGD method based\non Kalman Filtering (kSGD). Through our analysis, we show kSGD is\nasymptotically optimal, develop a fast algorithm for very large, infinite or\nstreaming data sources with a justified stop condition, prove that kSGD is\ninsensitive to the problem's conditioning, and develop a unique approach for\nanalyzing the complex second order dynamics. Our theoretical results are\nsupported by numerical experiments on three regression problems (linear,\nnonparametric wavelet, and logistic) using three large publicly available\ndatasets. Moreover, our analysis and experiments lay a foundation for embedding\nkSGD in multiple epoch algorithms, extending kSGD to other problem classes, and\ndeveloping parallel and low memory kSGD implementations. \n\n"}
{"id": "1512.01374", "contents": "Title: Geographic variation of surface energy partitioning in the climatic mean\n  predicted from the maximum power limit Abstract: Convective and radiative cooling are the two principle mechanisms by which\nthe Earth's surface transfers heat into the atmosphere and that shape surface\ntemperature. However, this partitioning is not sufficiently constrained by\nenergy and mass balances alone. We use a simple energy balance model in which\nconvective fluxes and surface temperatures are determined with the additional\nthermodynamic limit of maximum convective power. We then show that the broad\ngeographic variation of heat fluxes and surface temperatures in the\nclimatological mean compare very well with the ERA-Interim reanalysis over land\nand ocean. We also show that the estimates depend considerably on the\nformulation of longwave radiative transfer and that a spatially uniform offset\nis related to the assumed cold temperature sink at which the heat engine\noperates. \n\n"}
{"id": "1512.01955", "contents": "Title: Filter Based Methods For Statistical Linear Inverse Problems Abstract: Ill-posed inverse problems are ubiquitous in applications. Under- standing of\nalgorithms for their solution has been greatly enhanced by a deep understanding\nof the linear inverse problem. In the applied communities ensemble-based\nfiltering methods have recently been used to solve inverse problems by\nintroducing an artificial dynamical sys- tem. This opens up the possibility of\nusing a range of other filtering methods, such as 3DVAR and Kalman based\nmethods, to solve inverse problems, again by introducing an artificial\ndynamical system. The aim of this paper is to analyze such methods in the\ncontext of the ill-posed linear inverse problem. Statistical linear inverse\nproblems are studied in the sense that the observational noise is assumed to be\nderived via realization of a Gaussian random variable. We investigate the\nasymptotic behavior of filter based methods for these inverse problems.\nRigorous convergence rates are established for 3DVAR and for the Kalman\nfilters, including minimax rates in some instances. Blowup of 3DVAR and a\nvariant of its basic form is also presented, and optimality of the Kalman\nfilter is discussed. These analyses reveal a close connection between\n(iterative) regularization schemes in deterministic inverse problems and filter\nbased methods in data assimilation. Numerical experiments are presented to\nillustrate the theory. \n\n"}
{"id": "1512.02713", "contents": "Title: Transformations and Hardy-Krause variation Abstract: Using a multivariable Faa di Bruno formula we give conditions on\ntransformations $\\tau:[0,1]^m\\to\\mathcal{X}$ where $\\mathcal{X}$ is a closed\nand bounded subset of $\\mathbb{R}^d$ such that $f\\circ\\tau$ is of bounded\nvariation in the sense of Hardy and Krause for all $f\\in C^d(\\mathcal{x})$. We\ngive similar conditions for $f\\circ\\tau$ to be smooth enough for scrambled net\nsampling to attain $O(n^{-3/2+\\epsilon})$ accuracy. Some popular symmetric\ntransformations to the simplex and sphere are shown to satisfy neither\ncondition. Some other transformations due to Fang and Wang (1993) satisfy the\nfirst but not the second condition. We provide transformations for the simplex\nthat makes $f\\circ\\tau$ smooth enough to fully benefit from scrambled net\nsampling for all $f$ in a class of generalized polynomials. We also find\nsufficient conditions for the Rosenblatt-Hlawka-M\\\"uck transformation in\n$\\mathbb{R}^2$ and for importance sampling to be of bounded variation in the\nsense of Hardy and Krause. \n\n"}
{"id": "1512.03883", "contents": "Title: Sparse Generalized Principal Component Analysis for Large-scale\n  Applications beyond Gaussianity Abstract: Principal Component Analysis (PCA) is a dimension reduction technique. It\nproduces inconsistent estimators when the dimensionality is moderate to high,\nwhich is often the problem in modern large-scale applications where algorithm\nscalability and model interpretability are difficult to achieve, not to mention\nthe prevalence of missing values. While existing sparse PCA methods alleviate\ninconsistency, they are constrained to the Gaussian assumption of classical PCA\nand fail to address algorithm scalability issues. We generalize sparse PCA to\nthe broad exponential family distributions under high-dimensional setup, with\nbuilt-in treatment for missing values. Meanwhile we propose a family of\niterative sparse generalized PCA (SG-PCA) algorithms such that despite the\nnon-convexity and non-smoothness of the optimization task, the loss function\ndecreases in every iteration. In terms of ease and intuitive parameter tuning,\nour sparsity-inducing regularization is far superior to the popular Lasso.\nFurthermore, to promote overall scalability, accelerated gradient is integrated\nfor fast convergence, while a progressive screening technique gradually\nsqueezes out nuisance dimensions of a large-scale problem for feasible\noptimization. High-dimensional simulation and real data experiments demonstrate\nthe efficiency and efficacy of SG-PCA. \n\n"}
{"id": "1512.03976", "contents": "Title: An iterative importance sampler for Bayesian parameter estimation in\n  stochastic models of multicellular clocks Abstract: We investigate a stochastic version of the synthetic multicellular clock\nmodel proposed by Garcia-Ojalvo, Elowitz and Strogatz. By introducing dynamical\nnoise in the model and assuming that the partial observations of the system can\nbe contaminated by additive noise, we enable a principled mechanism to\nrepresent experimental uncertainties in the synthesis of the multicellular\nsystem and pave the way for the design of probabilistic methods for the\nestimation of any unknowns in the model. Within this setup, we investigate the\nuse of an iterative importance sampling scheme, termed nonlinear population\nMonte Carlo (NPMC), for the Bayesian estimation of the model parameters. The\nalgorithm yields a stochastic approximation of the posterior probability\ndistribution of the unknown parameters given the available data (partial and\npossibly noisy observations). We prove a new theoretical result for this\nalgorithm, which indicates that the approximations converge almost surely to\nthe actual distributions, even when the weights in the importance sampling\nscheme cannot be computed exactly. We also provide a detailed numerical\nassessment of the stochastic multicellular model and the accuracy of the\nproposed NPMC algorithm, including a comparison with the popular particle\nMetropolis-Hastings algorithm of Andrieu {\\em et al.}, 2010, applied to the\nsame model and an approximate Bayesian computation sequential Monte Carlo\nmethod introduced by Mari\\~no {\\em et al.}, 2013. \n\n"}
{"id": "1512.04132", "contents": "Title: The Absence of Stokes Drift in Waves Abstract: Stokes drift has been as central to the history of wave theory as it has been\ndistressingly absent from experiment. Neither wave tanks nor experiments in\nopen bodies detect this without nearly canceling \"eulerian flows.\" Acoustic\nwaves have an analogous problem that is particularly problematic in the\nvorticity production at the edges of beams. Here we demonstrate that the\nexplanation for this arises from subtle end-of-packet and wavetrain gradient\neffects such as microbreaking events and wave-flow decomposition subtleties\nrequired to conserve mass and momentum and avoid fictitious external forces.\nThese losses occur at both ends of packets and can produce a significant\nnonviscous energy loss for translating and spreading surface wave packets and\nwavetrains. In contrast, monochromatic sound wave packets will be shown to\nasymmetrically distort to conserve momentum. This provides an interesting\nanalogy to how such internal forces arise for gradients of electromagnetic\nwavetrains in media. Such examples show that the interactions of waves in media\nare so system dependent as to be completely nonuniversal. These give further\nexamples of how boundary effects must be carefully considered for conservation\nlaws especially when harmonic functions are involved. The induced flows in\nestablishing surface waves are shown to be time changing and dependent on wave\nhistory and suggest that some classical work based on mass flux and wave\ninteractions may need to be reconsidered. \n\n"}
{"id": "1512.04831", "contents": "Title: Coupling stochastic EM and Approximate Bayesian Computation for\n  parameter inference in state-space models Abstract: We study the class of state-space models and perform maximum likelihood\nestimation for the model parameters. We consider a stochastic approximation\nexpectation-maximization (SAEM) algorithm to maximize the likelihood function\nwith the novelty of using approximate Bayesian computation (ABC) within SAEM.\nThe task is to provide each iteration of SAEM with a filtered state of the\nsystem, and this is achieved using an ABC sampler for the hidden state, based\non sequential Monte Carlo (SMC) methodology. It is shown that the resulting\nSAEM-ABC algorithm can be calibrated to return accurate inference, and in some\nsituations it can outperform a version of SAEM incorporating the bootstrap\nfilter. Two simulation studies are presented, first a nonlinear Gaussian\nstate-space model then a state-space model having dynamics expressed by a\nstochastic differential equation. Comparisons with iterated filtering for\nmaximum likelihood inference, and Gibbs sampling and particle marginal methods\nfor Bayesian inference are presented. \n\n"}
{"id": "1512.06286", "contents": "Title: Compressive spectral method for the simulation of the water waves Abstract: In this paper an approach for decreasing the computational effort required\nfor the spectral simulations of the water waves is introduced. Signals with\nmajority of the components zero, are known as the sparse signals. Like majority\nof the signals in the nature it can be realized that water waves are sparse\neither in time or in the frequency domain. Using the sparsity property of the\nwater waves in the time or in the frequency domain, the compressive sampling\nalgorithm can be used as a tool for improving the performance of the spectral\nsimulation of the water waves. The methodology offered in this paper depends on\nthe idea of using a smaller number of spectral components compared to the\nclassical spectral method with a high number of components. After performing\nthe time integration with a smaller number of spectral components and using the\ncompressive sampling technique, it is shown that the water wave field can be\nreconstructed with a significantly better efficiency compared to the classical\nspectral method with a high number of spectral components, especially for long\ntime evolutions.\n  For the sparse water wave model in the time domain the well-known solitary\nwave solutions of the Korteweg-deVries (KdV) equation is considered. For the\nsparse water wave model in the frequency domain the well-known Airy (linear)\nocean waves with Jonswap spectrum is considered. Utilizing a spectral method,\nit is shown that by using a smaller number of spectral components compared to\nthe classical spectral method with a high number of components, it is possible\nto simulate the sparse water waves with negligible error in accuracy and a\ngreat efficiency especially for large time evolutions. \n\n"}
{"id": "1512.06412", "contents": "Title: Latent variable model selection for Gaussian conditional random fields Abstract: We consider the problem of learning a conditional Gaussian graphical model in\nthe presence of latent variables. Building on recent advances in this field, we\nsuggest a method that decomposes the parameters of a conditional Markov random\nfield into the sum of a sparse and a low-rank matrix. We derive convergence\nbounds for this estimator and show that it is well-behaved in the\nhigh-dimensional regime as well as \"sparsistent\" (i.e. capable of recovering\nthe graph structure). We then show how proximal gradient algorithms and\nsemi-definite programming techniques can be employed to fit the model to\nthousands of variables. Through extensive simulations, we illustrate the\nconditions required for identifiability and show that there is a wide range of\nsituations in which this model performs significantly better than its\ncounterparts, for example, by accommodating more latent variables. Finally, the\nsuggested method is applied to two datasets comprising individual level data on\ngenetic variants and metabolites levels. We show our results replicate better\nthan alternative approaches and show enriched biological signal. \n\n"}
{"id": "1512.06564", "contents": "Title: Holonomic gradient method for the probability content of a simplex\n  region with a multivariate normal distribution Abstract: We use the holonomic gradient method to evaluate the probability content of a\nsimplex region under a multivariate normal distribution. This probability\nequals to the integral of the probability density function of the multivariate\nGaussian distribution on the simplex region. For this purpose, we generalize\nthe inclusion--exclusion identity which was given for polyhedra, to the faces\nof a polyhedron. This extended inclusion--exclusion identity enables us to\ncalculate the derivatives of the function associated with the probability\ncontent of a polyhedron in general position. We show that these derivatives can\nbe written as integrals of the faces of the polyhedron. \n\n"}
{"id": "1512.08851", "contents": "Title: Limited fetch revisited: comparison of wind input terms, in surface\n  waves modeling Abstract: Results pertaining to numerical solutions of the Hasselmann kinetic equation\n(HE), for wind driven sea spectra, in the fetch limited geometry, are\npresented. Five versions of source functions, including the recently introduced\nZRP model, have been studied, for the exact expression of Snl and\nhigh-frequency implicit dissipation, due to wave-breaking. Four of the five\nexperiments were done in the absence of spectral peak dissipation for various\nSin terms. They demonstrated the dominance of quadruplet wave-wave interaction,\nin the energy balance, and the formation of self-similar regimes, of unlimited\nwave energy growth, along the fetch. Between them was the ZRP model, which\nstrongly agreed with dozens of field observations performed in the seas and\nlakes, since 1947. The fifth, the WAM3 wind input term experiment, used\nadditional spectral peak dissipation and reproduced the results of a previous,\nsimilar, numerical simulation, but only supported the field experiments for\nmoderate fetches, demonstrating a total energy saturation at half of that of\nthe Pierson-Moscowits limit. The alternative framework for HE numerical\nsimulation is proposed, along with a set of tests, allowing one to select\nphysically-justified source terms. \n\n"}
{"id": "1601.00630", "contents": "Title: Approximating the Distribution of the Median and other Robust Estimators\n  on Uncertain Data Abstract: Robust estimators, like the median of a point set, are important for data\nanalysis in the presence of outliers. We study robust estimators for\nlocationally uncertain points with discrete distributions. That is, each point\nin a data set has a discrete probability distribution describing its location.\nThe probabilistic nature of uncertain data makes it challenging to compute such\nestimators, since the true value of the estimator is now described by a\ndistribution rather than a single point. We show how to construct and estimate\nthe distribution of the median of a point set. Building the approximate support\nof the distribution takes near-linear time, and assigning probability to that\nsupport takes quadratic time. We also develop a general approximation technique\nfor distributions of robust estimators with respect to ranges with bounded VC\ndimension. This includes the geometric median for high dimensions and the\nSiegel estimator for linear regression. \n\n"}
{"id": "1601.01125", "contents": "Title: The Gibbs Sampler with Particle Efficient Importance Sampling for\n  State-Space Models Abstract: We consider Particle Gibbs (PG) as a tool for Bayesian analysis of non-linear\nnon-Gaussian state-space models. PG is a Monte Carlo (MC) approximation of the\nstandard Gibbs procedure which uses sequential MC (SMC) importance sampling\ninside the Gibbs procedure to update the latent and potentially\nhigh-dimensional state trajectories. We propose to combine PG with a generic\nand easily implementable SMC approach known as Particle Efficient Importance\nSampling (PEIS). By using SMC importance sampling densities which are\napproximately fully globally adapted to the targeted density of the states,\nPEIS can substantially improve the mixing and the efficiency of the PG draws\nfrom the posterior of the states and the parameters relative to existing PG\nimplementations. The efficiency gains achieved by PEIS are illustrated in PG\napplications to a univariate stochastic volatility model for asset returns, a\nnon-Gaussian nonlinear local-level model for interest rates, and a multivariate\nstochastic volatility model for the realized covariance matrix of asset\nreturns. \n\n"}
{"id": "1601.01178", "contents": "Title: Weakly informative reparameterisations for location-scale mixtures Abstract: While mixtures of Gaussian distributions have been studied for more than a\ncentury (Pearson, 1894), the construction of a reference Bayesian analysis of\nthose models still remains unsolved, with a general prohibition of the usage of\nimproper priors (Fruwirth-Schnatter, 2006) due to the ill-posed nature of such\nstatistical objects. This difficulty is usually bypassed by an empirical Bayes\nresolution (Richardson and Green, 1997). By creating a new parameterisation\ncantered on the mean and possibly the variance of the mixture distribution\nitself, we manage to develop here a weakly informative prior for a wide class\nof mixtures with an arbitrary number of components. We demonstrate that some\nposterior distributions associated with this prior and a minimal sample size\nare proper. We provide MCMC implementations that exhibit the expected\nexchangeability. We only study here the univariate case, the extension to\nmultivariate location-scale mixtures being currently under study. An R package\ncalled Ultimixt is associated with this paper. \n\n"}
{"id": "1601.02387", "contents": "Title: Bounding errors of Expectation-Propagation Abstract: Expectation Propagation is a very popular algorithm for variational\ninference, but comes with few theoretical guarantees. In this article, we prove\nthat the approximation errors made by EP can be bounded. Our bounds have an\nasymptotic interpretation in the number $n$ of datapoints, which allows us to\nstudy EP's convergence with respect to the true posterior. In particular, we\nshow that EP converges at a rate of $\\mathcal{0}(n^{-2})$ for the mean, up to\nan order of magnitude faster than the traditional Gaussian approximation at the\nmode. We also give similar asymptotic expansions for moments of order 2 to 4,\nas well as excess Kullback-Leibler cost (defined as the additional KL cost\nincurred by using EP rather than the ideal Gaussian approximation). All these\nexpansions highlight the superior convergence properties of EP. Our approach\nfor deriving those results is likely applicable to many similar approximate\ninference methods. In addition, we introduce bounds on the moments of\nlog-concave distributions that may be of independent interest. \n\n"}
{"id": "1601.03050", "contents": "Title: Optical phase curves as diagnostics for aerosol composition in\n  exoplanetary atmospheres Abstract: Optical phase curves have become one of the common probes of exoplanetary\natmospheres, but the information they encode has not been fully elucidated.\nBuilding on a diverse body of work, we upgrade the Flexible Modeling System\n(FMS) to include scattering in the two-stream, dual-band approximation and\ngenerate plausible, three-dimensional structures of irradiated atmospheres to\nstudy the radiative effects of aerosols or condensates. In the optical, we\ntreat the scattering of starlight using a generalisation of Beer's law that\nallows for a finite Bond albedo to be prescribed. In the infrared, we implement\nthe two-stream solutions and include scattering via an infrared scattering\nparameter. We present a suite of four-parameter general circulation models for\nKepler-7b and demonstrate that its climatology is expected to be robust to\nvariations in optical and infrared scattering. The westward and eastward shifts\nof the optical and infrared phase curves, respectively, are shown to be robust\noutcomes of the simulations. Assuming micron-sized particles and a simplified\ntreatment of local brightness, we further show that the peak offset of the\noptical phase curve is sensitive to the composition of the aerosols or\ncondensates. However, to within the measurement uncertainties, we cannot\ndistinguish between aerosols made of silicates (enstatite or forsterite), iron,\ncorundum or titanium oxide, based on a comparison to the measured peak offset\n($41^\\circ \\pm 12^\\circ$) of the optical phase curve of Kepler-7b. Measuring\nhigh-precision optical phase curves will provide important constraints on the\natmospheres of cloudy exoplanets and reduce degeneracies in interpreting their\ninfrared spectra. \n\n"}
{"id": "1601.03704", "contents": "Title: Computationally efficient change point detection for high-dimensional\n  regression Abstract: Large-scale sequential data is often exposed to some degree of inhomogeneity\nin the form of sudden changes in the parameters of the data-generating process.\nWe consider the problem of detecting such structural changes in a\nhigh-dimensional regression setting. We propose a joint estimator of the number\nand the locations of the change points and of the parameters in the\ncorresponding segments. The estimator can be computed using dynamic programming\nor, as we emphasize here, it can be approximated using a binary search\nalgorithm with $O(n \\log(n) \\mathrm{Lasso}(n))$ computational operations while\nstill enjoying essentially the same theoretical properties; here\n$\\mathrm{Lasso}(n)$ denotes the computational cost of computing the Lasso for\nsample size $n$. We establish oracle inequalities for the estimator as well as\nfor its binary search approximation, covering also the case with a large\n(asymptotically growing) number of change points. We evaluate the performance\nof the proposed estimation algorithms on simulated data and apply the\nmethodology to real data. \n\n"}
{"id": "1601.05011", "contents": "Title: Non-smooth Variable Projection Abstract: Variable projection solves structured optimization problems by completely\nminimizing over a subset of the variables while iterating over the remaining\nvariables. Over the last 30 years, the technique has been widely used, with\nempirical and theoretical results demonstrating both greater efficacy and\ngreater stability compared to competing approaches. Classic examples have\nexploited closed-form projections and smoothness of the objective function. We\nextend the approach to problems that include non-smooth terms, and where the\nprojection subproblems can only be solved inexactly by iterative methods. We\npropose an inexact adaptive algonrithm for solving such problems and analyze\nits computational complexity. Finally, we show how the theory can be used to\ndesign methods for selected problems occurring frequently in machine-learning\nand inverse problems. \n\n"}
{"id": "1601.06272", "contents": "Title: Turbulence and fire-spotting effects into wild-land fire simulators Abstract: This paper presents a mathematical approach to model the effects of phenomena\nwith random nature such as turbulence and fire-spotting into the existing\nwildfire simulators. The formulation proposes that the propagation of the\nfire-front is the sum of a drifting component (obtained from an existing\nwildfire simulator without turbulence and fire-spotting) and a random\nfluctuating component. The modelling of the random effects is embodied in a\nprobability density function accounting for the fluctuations around the fire\nperimeter which is given by the drifting component. In past, this formulation\nhas been applied to include these random effects into a wildfire simulator\nbased on an Eulerian moving interface method, namely the Level Set Method\n(LSM), but in this paper the same formulation is adapted for a wildfire\nsimulator based on a Lagrangian front tracking technique, namely the Discrete\nEvent System Specification (DEVS). The main highlight of the present study is\nthe comparison of the performance of a Lagrangian and an Eulerian moving\ninterface method when applied to wild-land fire propagation. Simple idealised\nnumerical experiments are used to investigate the potential applicability of\nthe proposed formulation to DEVS and to compare its behaviour with respect to\nthe LSM. The results show that DEVS based wildfire propagation model\nqualitatively improves its performance (e.g., reproducing flank and back fire,\nincrease in fire spread due to pre-heating of the fuel by hot air and\nfirebrands, fire propagation across no fuel zones, secondary fire generation,\n\\dots). Though the results presented here are devoid of any validation exercise\nand provide only a proof of concept, they show a strong inclination towards an\nintended operational use. The existing LSM or DEVS based operational simulators\nlike WRF-SFIRE and ForeFire respectively can serve as an ideal basis for the\nsame. \n\n"}
{"id": "1601.06486", "contents": "Title: Cliffs Benchmarking Abstract: A numerical model for tsunami simulations Cliffs is exercised with the\ncomplete set of NTHMP-selected benchmark problems focused on inundation, such\nas simulating runup of a non-breaking solitary wave onto a sloping beach, runup\non a conical island, a lab experiment with a scaled model of Monai Valley, and\nthe 1993 Hokkiado tsunami and inundation of the Okushiri Island. \n\n"}
{"id": "1601.06720", "contents": "Title: The Generalized Quasilinear Approximation: Application to Zonal Jets Abstract: Quasilinear theory is often utilized to approximate the dynamics of fluids\nexhibiting significant interactions between mean flows and eddies. In this\npaper we present a generalization of quasilinear theory to include dynamic mode\ninteractions on the large scales. This generalized quasilinear (GQL)\napproximation is achieved by separating the state variables into large and\nsmall zonal scales via a spectral filter rather than by a decomposition into a\nformal mean and fluctuations. Nonlinear interactions involving only small zonal\nscales are then removed. The approximation is conservative and allows for\nscattering of energy between small-scale modes via the large scale (through\nnon-local spectral interactions). We evaluate GQL for the paradigmatic problems\nof the driving of large-scale jets on a spherical surface and on the beta-plane\nand show that it is accurate even for a small number of large-scale modes. As\nthis approximation is formally linear in the small zonal scales it allows for\nthe closure of the system and can be utilized in direct statistical simulation\nschemes that have proved an attractive alternative to direct numerical\nsimulation for many geophysical and astrophysical problems. \n\n"}
{"id": "1602.00207", "contents": "Title: Binomial and Multinomial Proportions: Accurate Estimation and Reliable\n  Assessment of Accuracy Abstract: Misestimates of $\\sigma_{P_o}$, the \\emph{uncertainty} in $P_o$ from a\n2-state Bayes equation used for binary classification, apparently arose from\n$\\hat{\\sigma}_{p_i}$, the uncertainty in underlying pdfs estimated from\nexperimental $b$-bin histograms. To address this, several Bayesian estimator\npairs $(\\hat{p}_i, \\hat{\\sigma}_{p_i})$ were compared for agreement between\nnominal confidence level ($\\xi$) and calculated coverage values ($C$). Large\n$\\xi$-to-$C$ inconsistency for large $b$ and $ p_i \\gg \\frac{1}{b}$ arises for\nall multinomial estimators since priors downweight low likelihood, high $p_i$\nvalues. To improve $\\xi$-to-$C$ matching, $(\\xi-C)^2$ was minimized against\n$\\alpha_0$ in a more general prior pdf\n($\\mathcal{B}[\\alpha_0,(b-1)\\alpha_0;x]$) to obtain\n$(\\hat{p_i})_{\\xi\\leftrightarrow C}$. This improved matching for $b=2$, but for\n$b>2$, $\\xi$-to-$C$ matching by $(\\hat{p_i})_{\\xi\\leftrightarrow C}$ required\nan effective value \"$b=2$\" and renormalization, and this reduced\n$\\hat{p}_i$-to-$p_i$ matching. Better $\\hat{p}_i$-to-$p_i$ matching came from\nthe original multinomial estimators, a new discrete-domain estimator\n$\\hat{p}(n_i,N)$, or an earlier \\emph{joint} estimator, $(\\hat{p_i})_{\\bowtie}$\nthat co-adjusted all estimates $p_i$ for James-Stein shrinkage to a mean\nvector. Best simultaneous $\\xi$-to-$C$ and $\\hat{p}_i$-to-$p_i$ matching came\nby \\emph{de-noising} initial estimates of underlying pdfs. For $b=100$,\n$N<12800$, de-noised $\\hat{p}$ needed $\\approx 10\\times$ fewer observations to\nachieve $\\hat{p}_i$-to-$p_i$ matching equivalent to that found for\n$\\hat{p}(n_i,N)$, $(\\hat{p_i})_{\\bowtie}$ or the original multinomial\n$\\hat{p}_i$. De-noising each different type of initial estimate yielded\nsimilarly high accuracy in Monte-Carlo tests. \n\n"}
{"id": "1602.03658", "contents": "Title: A randomized maximum a posterior method for posterior sampling of high\n  dimensional nonlinear Bayesian inverse problems Abstract: We present a randomized maximum a posteriori (rMAP) method for generating\napproximate samples of posteriors in high dimensional Bayesian inverse problems\ngoverned by large-scale forward problems. We derive the rMAP approach by: 1)\ncasting the problem of computing the MAP point as a stochastic optimization\nproblem; 2) interchanging optimization and expectation; and 3) approximating\nthe expectation with a Monte Carlo method. For a specific randomized data and\nprior mean, rMAP reduces to the maximum likelihood approach (RML). It can also\nbe viewed as an iterative stochastic Newton method. An analysis of the\nconvergence of the rMAP samples is carried out for both linear and nonlinear\ninverse problems. Each rMAP sample requires solution of a PDE-constrained\noptimization problem; to solve these problems, we employ a state-of-the-art\ntrust region inexact Newton conjugate gradient method with sensitivity-based\nwarm starts. An approximate Metropolization approach is presented to reduce the\nbias in rMAP samples. Various numerical methods will be presented to\ndemonstrate the potential of the rMAP approach in posterior sampling of\nnonlinear Bayesian inverse problems in high dimensions. \n\n"}
{"id": "1602.04060", "contents": "Title: Discrete approximation of a mixture distribution via restricted\n  divergence Abstract: Mixture distributions arise in many application areas, for example as\nmarginal distributions or convolutions of distributions. We present a method of\nconstructing an easily tractable discrete mixture distribution as an\napproximation to a mixture distribution with a large to infinite number,\ndiscrete or continuous, of components. The proposed DIRECT (Divergence\nRestricting Conditional Tesselation) algorithm is set up such that a\npre-specified precision, defined in terms of Kullback-Leibler divergence\nbetween true distribution and approximation, is guaranteed. Application of the\nalgorithm is demonstrated in two examples. \n\n"}
{"id": "1602.05339", "contents": "Title: An extended Kundu-Eckhaus equation for modeling dynamics of rogue waves\n  in a chaotic wave-current field Abstract: In this paper we propose an extended Kundu-Eckhaus equation (KEE) for\nmodeling the dynamics of skewed rogue waves emerging in the vicinity of a wave\nblocking point due to opposing current. The equation we propose is a KEE with\nan additional potential term therefore the results presented in this paper can\neasily be generalized to study the quantum tunneling properties of the rogue\nwaves and ultrashort (femtosecond) pulses of the KEE. In the frame of the\nextended KEE, we numerically show that the chaotic perturbations of the ocean\ncurrent trigger the occurrence of the rogue waves on the ocean surface. We\npropose and implement a split-step scheme and show that the extended KEE that\nwe propose is unstable against random chaotic perturbations in the current\nprofile. These perturbations transform the monochromatic wave field into a\nchaotic sea state with many peaks. We numerically show that the shapes of rogue\nwaves due to perturbations in the current profile closely follow the form of\nrational rogue wave solutions, especially for the central peak. We also discuss\nthe effects of magnitude of the chaotic current perturbations on the statistics\nof the rogue wave occurrence. \n\n"}
{"id": "1602.07079", "contents": "Title: The Cessation Threshold of Nonsuspended Sediment Transport Across\n  Aeolian and Fluvial Environments Abstract: Using particle-scale simulations of non-suspended sediment transport for a\nlarge range of Newtonian fluids driving transport, including air and water, we\ndetermine the bulk transport cessation threshold $\\Theta^r_t$ by extrapolating\nthe transport load as a function of the dimensionless fluid shear stress\n(`Shields number') $\\Theta$ to the vanishing transport limit. In this limit,\nthe simulated steady states of continuous transport can be described by simple\nanalytical model equations relating the average transport layer properties to\nthe law of the wall flow velocity profile. We use this model to calculate\n$\\Theta^r_t$ for arbitrary environments and derive a general Shields-like\nthreshold diagram in which a Stokes-like number replaces the particle Reynolds\nnumber. Despite the simplicity of our hydrodynamic description, the predicted\ncessation threshold, both from the simulations and analytical model,\nquantitatively agrees with measurements for transport in air and viscous and\nturbulent liquids despite not being fitted to these measurements. We interpret\nthe analytical model as a description of a continuous rebound motion of\ntransported particles and thus $\\Theta^r_t$ as the minimal fluid shear stress\nneeded to compensate the average energy loss of transported particles during an\naverage rebound at the bed surface. This interpretation, supported by\nsimulations near $\\Theta^r_t$, implies that entrainment mechanisms are needed\nto sustain transport above $\\Theta^r_t$. While entrainment by turbulent events\nsustains intermittent transport, entrainment by particle-bed impacts sustains\ncontinuous transport. Combining our interpretations with the critical energy\ncriterion for incipient motion by Valyrakis and coworkers, we put forward a new\nconceptual picture of sediment transport intermittency. \n\n"}
{"id": "1602.08154", "contents": "Title: Efficient Bayesian Inference for Multivariate Factor Stochastic\n  Volatility Models Abstract: We discuss efficient Bayesian estimation of dynamic covariance matrices in\nmultivariate time series through a factor stochastic volatility model. In\nparticular, we propose two interweaving strategies (Yu and Meng, Journal of\nComputational and Graphical Statistics, 20(3), 531-570, 2011) to substantially\naccelerate convergence and mixing of standard MCMC approaches. Similar to\nmarginal data augmentation techniques, the proposed acceleration procedures\nexploit non-identifiability issues which frequently arise in factor models. Our\nnew interweaving strategies are easy to implement and come at almost no extra\ncomputational cost; nevertheless, they can boost estimation efficiency by\nseveral orders of magnitude as is shown in extensive simulation studies. To\nconclude, the application of our algorithm to a 26-dimensional exchange rate\ndata set illustrates the superior performance of the new approach for\nreal-world data. \n\n"}
{"id": "1603.00351", "contents": "Title: Analyzing Non-proportional Hazards: Use of the MRH Package Abstract: In this manuscript we demonstrate the analysis of right-censored survival\noutcomes using the MRH package in R. The MRH package implements the\nmulti-resolution hazard (MRH) model, which is a Polya-tree based, Bayesian\nsemi-parametric method for flexible estimation of the hazard rate and covariate\neffects. The package allows for covariates to be included under the\nproportional and non-proportional hazards assumption, and for robust estimation\nof the hazard rate in periods of sparsely observed failures via a \"pruning\"\ntool. \n\n"}
{"id": "1603.01136", "contents": "Title: Multilevel Sequential Monte Carlo Samplers for Normalizing Constants Abstract: This article considers the sequential Monte Carlo (SMC) approximation of\nratios of normalizing constants associated to posterior distributions which in\nprinciple rely on continuum models. Therefore, the Monte Carlo estimation error\nand the discrete approximation error must be balanced. A multilevel strategy is\nutilized to substantially reduce the cost to obtain a given error level in the\napproximation as compared to standard estimators. Two estimators are considered\nand relative variance bounds are given. The theoretical results are numerically\nillustrated for the example of identifying a parametrized permeability in an\nelliptic equation given point-wise observations of the pressure. \n\n"}
{"id": "1603.01897", "contents": "Title: Bias Correction of Semiparametric Long Memory Parameter Estimators via\n  the Pre-filtered Sieve Bootstrap Abstract: This paper investigates bootstrap-based bias correction of semiparametric\nestimators of the long memory parameter, $d$, in fractionally integrated\nprocesses. The re-sampling method involves the application of the sieve\nbootstrap to data pre-filtered by a preliminary semiparametric estimate of the\nlong memory parameter. Theoretical justification for using the bootstrap\ntechnique to bias adjust log periodogram and semiparametric local Whittle\nestimators of the memory parameter is provided in the case where the true value\nof $d$ lies in the range $0\\leq d<0.5$. That the bootstrap method provides\nconfidence intervals with the correct asymptotic coverage is also proven, with\nthe intervals shown to adjust explicitly for bias, as estimated via the\nbootstrap. Simulation evidence comparing the performance of the bootstrap bias\ncorrection with analytical bias-correction techniques is presented. The\nbootstrap method is shown to produce notable bias reductions, in particular\nwhen applied to an estimator for which some degree of bias reduction has\nalready been accomplished by analytical means. \n\n"}
{"id": "1603.02532", "contents": "Title: On the inconsistency of $\\ell_1$-penalised sparse precision matrix\n  estimation Abstract: Various $\\ell_1$-penalised estimation methods such as graphical lasso and\nCLIME are widely used for sparse precision matrix estimation. Many of these\nmethods have been shown to be consistent under various quantitative assumptions\nabout the underlying true covariance matrix. Intuitively, these conditions are\nrelated to situations where the penalty term will dominate the optimisation. In\nthis paper, we explore the consistency of $\\ell_1$-based methods for a class of\nsparse latent variable -like models, which are strongly motivated by several\ntypes of applications. We show that all $\\ell_1$-based methods fail\ndramatically for models with nearly linear dependencies between the variables.\nWe also study the consistency on models derived from real gene expression data\nand note that the assumptions needed for consistency never hold even for modest\nsized gene networks and $\\ell_1$-based methods also become unreliable in\npractice for larger networks. \n\n"}
{"id": "1603.02834", "contents": "Title: Inference and rare event simulation for stopped Markov processes via\n  reverse-time sequential Monte Carlo Abstract: We present a sequential Monte Carlo algorithm for Markov chain trajectories\nwith proposals constructed in reverse time, which is advantageous when paths\nare conditioned to end in a rare set. The reverse time proposal distribution is\nconstructed by approximating the ratio of Green's functions in Nagasawa's\nformula. Conditioning arguments can be used to interpret these ratios as\nlow-dimensional conditional sampling distributions of some coordinates of the\nprocess given the others. Hence the difficulty in designing SMC proposals in\nhigh dimension is greatly reduced. We illustrate our method on estimating an\noverflow probability in a queueing model, the probability that a diffusion\nfollows a narrowing corridor, and the initial location of an infection in an\nepidemic model on a network. \n\n"}
{"id": "1603.03510", "contents": "Title: Adaptive Component-wise Multiple-Try Metropolis Sampling Abstract: One of the most widely used samplers in practice is the component-wise\nMetropolis-Hastings (CMH) sampler that updates in turn the components of a\nvector valued Markov chain using accept-reject moves generated from a proposal\ndistribution. When the target distribution of a Markov chain is irregularly\nshaped, a `good' proposal distribution for one part of the state space might be\na `poor' one for another part of the state space. We consider a component-wise\nmultiple-try Metropolis (CMTM) algorithm that can automatically choose from a\nset of candidate moves sampled from different distributions. The computational\nefficiency is increased using an adaptation rule for the CMTM algorithm that\ndynamically builds a better set of proposal distributions as the Markov chain\nruns. The ergodicity of the adaptive chain is demonstrated theoretically. The\nperformance is studied via simulations and real data examples. \n\n"}
{"id": "1603.03706", "contents": "Title: Quantifying the global atmospheric power budget Abstract: The power of atmospheric circulation is a key measure of the Earth's climate\nsystem. The mismatch between predictions and observations under a warming\nclimate calls for a reassessment of how atmospheric power $W$ is defined,\nestimated and constrained. Here we review published formulations for $W$ and\nshow how they differ when applied to a moist atmosphere. Three factors, a\nnon-zero source/sink in the continuity equation, the difference between\nvelocities of gaseous air and condensate, and interaction between the gas and\ncondensate modifying the equations of motion, affect the formulation of $W$.\nStarting from the thermodynamic definition of mechanical work, we derive an\nexpression for $W$ from an explicit consideration of the equations of motion\nand continuity. Our analyses clarify how some past formulations are incomplete\nor invalid. Three caveats are identified. First, $W$ critically depends on the\nboundary condition for gaseous air velocity at the Earth's surface. Second,\nconfusion between gaseous air velocity and mean velocity of air and condensate\nin the expression for $W$ results in gross errors despite the observed\nmagnitudes of these velocities are very close. Third, $W$ expressed in terms of\nmeasurable atmospheric parameters, air pressure and velocity, is\nscale-specific; this must be taken into account when adding contributions to\n$W$ from different processes. We present a formulation of the atmospheric power\nbudget, which distinguishes three components of $W$: the kinetic power\nassociated with horizontal pressure gradients ($W_K$), the gravitational power\nof precipitation ($W_P$) and the condensate loading ($W_c$). We use MERRA and\nNCAR/NCEP re-analyses to evaluate the atmospheric power budget at different\nscales: $W_K$ increases with temporal resolution approaching our theoretical\nestimate for condensation-induced circulation when all convective motion is\nresolved. \n\n"}
{"id": "1603.04022", "contents": "Title: The mineral clouds on HD 209458b and HD189733b Abstract: 3D atmosphere model results are used to comparatively study the kinetic,\nnon-equilibrium cloud formation in the atmospheres of two example planets\nguided by the giant gas planets HD209458b and HD189733b. Rather independently\nof hydrodynamic model differences, our cloud modelling suggests that both\nplanets are covered in mineral clouds throughout the entire modelling domain.\nBoth planets harbour chemically complex clouds that are made of mineral\nparticles that have a height-dependent material composition and size. The\nremaining gas-phase element abundances strongly effects the molecular\nabundances of the atmosphere in the cloud forming regions. Hydrocarbon and\ncyanopolyyne molecules can be rather abundant in the inner, dense part of the\natmospheres of HD189733b and HD209458b. No one value for metallicity and the\nC/O ratio can be used to describe an extrasolar planet. Our results concerning\nthe presence and location of water in relation to the clouds explain some of\nthe observed differences between the two planets. In HD189733b, strong water\nfeatures have been reported while such features appear less strong for\nHD209458b. By considering the location of the clouds in the two atmospheres, we\nsee that obscuring clouds exist high in the atmosphere of HD209458b, but much\ndeeper in HD189733b. We further conclude that the (self-imposed) degeneracy of\ncloud parameters in retrieval methods can only be lifted if the cloud formation\nprocesses are accurately modelled in contrast to prescribing them by\nindependent parameters. \n\n"}
{"id": "1603.05418", "contents": "Title: Analytical Models of Exoplanetary Atmospheres. III. Gaseous C-H-O-N\n  Chemistry with 9 Molecules Abstract: We present novel, analytical, equilibrium-chemistry formulae for the\nabundances of molecules in hot exoplanetary atmospheres that include the\ncarbon, oxygen and nitrogen networks. Our hydrogen-dominated solutions involve\nacetylene (C$_2$H$_2$), ammonia (NH$_3$), carbon dioxide (CO$_2$), carbon\nmonoxide (CO), ethylene (C$_2$H$_4$), hydrogen cyanide (HCN), methane (CH$_4$),\nmolecular nitrogen (N$_2$) and water (H$_2$O). By considering only the gas\nphase, we prove that the mixing ratio of carbon monoxide is governed by a decic\nequation (polynomial equation of degree 10). We validate our solutions against\nnumerical calculations of equilibrium chemistry that perform Gibbs free energy\nminimization and demonstrate that they are accurate at the $\\sim 1\\%$ level for\ntemperatures from 500 to 3000 K. In hydrogen-dominated atmospheres, the ratio\nof abundances of HCN to CH$_4$ is nearly constant across a wide range of\ncarbon-to-oxygen ratios, which makes it a robust diagnostic of the metallicity\nin the gas phase. Our validated formulae allow for the convenient benchmarking\nof chemical kinetics codes and provide an efficient way of enforcing chemical\nequilibrium in atmospheric retrieval calculations. \n\n"}
{"id": "1603.06381", "contents": "Title: Forward and Inverse Uncertainty Quantification using Multilevel Monte\n  Carlo Algorithms for an Elliptic Nonlocal Equation Abstract: This paper considers uncertainty quantification for an elliptic nonlocal\nequation. In particular, it is assumed that the parameters which define the\nkernel in the nonlocal operator are uncertain and a priori distributed\naccording to a probability measure. It is shown that the induced probability\nmeasure on some quantities of interest arising from functionals of the solution\nto the equation with random inputs is well-defined; as is the posterior\ndistribution on parameters given observations. As the elliptic nonlocal\nequation cannot be solved approximate posteriors are constructed. The\nmultilevel Monte Carlo (MLMC) and multilevel sequential Monte Carlo (MLSMC)\nsampling algorithms are used for a priori and a posteriori estimation,\nrespectively, of quantities of interest. These algorithms reduce the amount of\nwork to estimate posterior expectations, for a given level of error, relative\nto Monte Carlo and i.i.d. sampling from the posterior at a given level of\napproximation of the solution of the elliptic nonlocal equation. \n\n"}
{"id": "1603.06532", "contents": "Title: Shear and depth-averaged Stokes drift under a Phillips-type spectrum Abstract: The transport and shear under a Phillips-type spectrum are presented. A\ncombined profile for monochromatic swell and a Phillips-type wind sea spectrum\nwhich can be used to investigate the shear under crossing seas is then\npresented. \n\n"}
{"id": "1603.08815", "contents": "Title: Spectral M-estimation with Applications to Hidden Markov Models Abstract: Method of moment estimators exhibit appealing statistical properties, such as\nasymptotic unbiasedness, for nonconvex problems. However, they typically\nrequire a large number of samples and are extremely sensitive to model\nmisspecification. In this paper, we apply the framework of M-estimation to\ndevelop both a generalized method of moments procedure and a principled method\nfor regularization. Our proposed M-estimator obtains optimal sample efficiency\nrates (in the class of moment-based estimators) and the same well-known rates\non prediction accuracy as other spectral estimators. It also makes it\nstraightforward to incorporate regularization into the sample moment\nconditions. We demonstrate empirically the gains in sample efficiency from our\napproach on hidden Markov models. \n\n"}
{"id": "1603.09272", "contents": "Title: Bayesian inference in hierarchical models by combining independent\n  posteriors Abstract: Hierarchical models are versatile tools for joint modeling of data sets\narising from different, but related, sources. Fully Bayesian inference may,\nhowever, become computationally prohibitive if the source-specific data models\nare complex, or if the number of sources is very large. To facilitate\ncomputation, we propose an approach, where inference is first made\nindependently for the parameters of each data set, whereupon the obtained\nposterior samples are used as observed data in a substitute hierarchical model,\nbased on a scaled likelihood function. Compared to direct inference in a full\nhierarchical model, the approach has the advantage of being able to speed up\nconvergence by breaking down the initial large inference problem into smaller\nindividual subproblems with better convergence properties. Moreover it enables\nparallel processing of the possibly complex inferences of the source-specific\nparameters, which may otherwise create a computational bottleneck if processed\njointly as part of a hierarchical model. The approach is illustrated with both\nsimulated and real data. \n\n"}
{"id": "1604.00872", "contents": "Title: Geometrically Tempered Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo (HMC) has become routinely used for sampling from\nposterior distributions. Its extension Riemann manifold HMC (RMHMC) modifies\nthe proposal kernel through distortion of local distances by a Riemannian\nmetric. The performance depends critically on the choice of metric, with the\nFisher information providing the standard choice. In this article, we propose a\nnew class of metrics aimed at improving HMC's performance on multi-modal target\ndistributions. We refer to the proposed approach as geometrically tempered HMC\n(GTHMC) due to its connection to other tempering methods. We establish a\ngeometric theory behind RMHMC to motivate GTHMC and characterize its\ntheoretical properties. Moreover, we develop a novel variable step size\nintegrator for simulating Hamiltonian dynamics to improve on the usual\nSt\\\"{o}rmer-Verlet integrator which suffers from numerical instability in GTHMC\nsettings. We illustrate GTHMC through simulations, demonstrating generality and\nsubstantial gains over standard HMC implementations in terms of effective\nsample sizes. \n\n"}
{"id": "1604.00971", "contents": "Title: Deep Graphs - a general framework to represent and analyze heterogeneous\n  complex systems across scales Abstract: Network theory has proven to be a powerful tool in describing and analyzing\nsystems by modelling the relations between their constituent objects. In recent\nyears great progress has been made by augmenting `traditional' network theory.\nHowever, existing network representations still lack crucial features in order\nto serve as a general data analysis tool. These include, most importantly, an\nexplicit association of information with possibly heterogeneous types of\nobjects and relations, and a conclusive representation of the properties of\ngroups of nodes as well as the interactions between such groups on different\nscales. In this paper, we introduce a collection of definitions resulting in a\nframework that, on the one hand, entails and unifies existing network\nrepresentations (e.g., network of networks, multilayer networks), and on the\nother hand, generalizes and extends them by incorporating the above features.\nTo implement these features, we first specify the nodes and edges of a finite\ngraph as sets of properties. Second, the mathematical concept of partition\nlattices is transferred to network theory in order to demonstrate how\npartitioning the node and edge set of a graph into supernodes and superedges\nallows to aggregate, compute and allocate information on and between arbitrary\ngroups of nodes. The derived partition lattice of a graph, which we denote by\ndeep graph, constitutes a concise, yet comprehensive representation that\nenables the expression and analysis of heterogeneous properties, relations and\ninteractions on all scales of a complex system in a self-contained manner.\nFurthermore, to be able to utilize existing network-based methods and models,\nwe derive different representations of multilayer networks from our framework\nand demonstrate the advantages of our representation. We exemplify an\napplication of deep graphs using a real world dataset of precipitation\nmeasurements. \n\n"}
{"id": "1604.01877", "contents": "Title: Low frequency modulation of jets in quasigeostrophic turbulence Abstract: Quasigeostrophic turbulence on a beta-plane with a finite deformation radius\nis studied nu- merically, with particular emphasis on frequency and combined\nwavenumber-frequency do- main analyses. Under suitable conditions, simulations\nwith small-scale random forcing and large-scale drag exhibit a spontaneous\nformation of multiple zonal jets. The first hint of wave-like features is seen\nin the distribution of kinetic energy as a function of frequency; specifically,\nfor progressively larger deformation scales there are systematic departures in\nthe form of isolated peaks (at progressively higher frequencies) from a\npower-law scaling. Con- comitantly, there is an inverse flux of kinetic energy\nin frequency space which extends to lower frequencies for smaller deformation\nscales. The identification of these peaks as Rossby waves is made possible by\nexamining the energy spectrum in frequency-zonal wavenumber and\nfrequency-meridional wavenumber diagrams. In fact, the modified Rhines scale\nturns out to be a useful measure of the dominant meridional wavenumber of the\nmodulating Rossby waves; once this is fixed, apart from a spectral peak at the\norigin (the steady jet), almost all the energy is contained in westward\npropagating disturbances that follow the theoretical Rossby dispersion\nrelation. Quite consistently, noting that the zonal scale of the modulating\nwaves is restricted to the first few wavenumbers, the energy spectrum is almost\nentirely contained within the corresponding Rossby dispersion curves on a\nfrequency-meridional wavenumber diagram. Cases when jets do not form are also\nconsidered; once again, there is a hint of Rossby wave activity, though the\nspectral peaks are quite muted. Further, the kinetic energy scaling in\nfrequency domain follows a -5/3 power-law and is distributed much more broadly\nin frequency-wavenumber diagrams \n\n"}
{"id": "1604.02575", "contents": "Title: Well-posed Bayesian Inverse Problems: Priors with Exponential Tails Abstract: We consider the well-posedness of Bayesian inverse problems when the prior\nmeasure has exponential tails. In particular, we consider the class of convex\n(log-concave) probability measures which include the Gaussian and Besov\nmeasures as well as certain classes of hierarchical priors. We identify\nappropriate conditions on the likelihood distribution and the prior measure\nwhich guarantee existence, uniqueness and stability of the posterior measure\nwith respect to perturbations of the data. We also consider consistent\napproximations of the posterior such as discretization by projection. Finally,\nwe present a general recipe for construction of convex priors on Banach spaces\nwhich will be of interest in practical applications where one often works with\nspaces such as $L^2$ or the continuous functions. \n\n"}
{"id": "1604.04432", "contents": "Title: A climate network-based index to discriminate different types of El\n  Ni\\~no and La Ni\\~na Abstract: El Ni\\~no exhibits distinct Eastern Pacific (EP) and Central Pacific (CP)\ntypes which are commonly, but not always consistently, distinguished from each\nother by different signatures in equatorial climate variability. Here, we\npropose an index based on evolving climate networks to objectively discriminate\nbetween both flavors by utilizing a scalar-valued evolving climate network\nmeasure that quantifies spatial localization and dispersion in El Ni\\~no's\nassociated teleconnections. Our index displays a sharp peak (high localization)\nduring EP events, whereas during CP events (larger dispersion) it remains close\nto the baseline observed during normal periods. In contrast to previous\nclassification schemes, our approach specifically account for El Ni\\~no's\nglobal impacts. We confirm recent El Ni\\~no classifications for the years 1951\nto 2014 and assign types to those cases were former works yielded ambiguous\nresults. Ultimately, we study La Ni\\~na episodes and demonstrate that our index\nprovides a similar discrimination into two types. \n\n"}
{"id": "1604.04980", "contents": "Title: Potentially Predictive Variance Reducing Subsample Locations in Local\n  Gaussian Process Regression Abstract: Gaussian process models are commonly used as emulators for computer\nexperiments. However, developing a Gaussian process emulator can be\ncomputationally prohibitive when the number of experimental samples is even\nmoderately large. Local Gaussian process approximation (Gramacy and Apley,\n2015) was proposed as an accurate and computationally feasible emulation\nalternative. However, constructing local sub-designs specific to predictions at\na particular location of interest remains a substantial computational\nbottleneck to the technique. In this paper, two computationally efficient\nneighborhood search limiting techniques are proposed, a maximum distance method\nand a feature approximation method. Two examples demonstrate that the proposed\nmethods indeed save substantial computation while retaining emulation accuracy. \n\n"}
{"id": "1604.05119", "contents": "Title: The water budget of a hurricane as dependent on its movement Abstract: Despite the dangers associated with tropical cyclones and their rainfall, the\norigins of storm moisture remains unclear. Existing studies have focused on the\nregion 40-400 km from the cyclone center. It is known that the rainfall within\nthis area cannot be explained by local processes alone but requires imported\nmoisture. Nonetheless, the dynamics of this imported moisture appears unknown.\nHere, considering a region up to three thousand kilometers from storm center,\nwe analyze precipitation, atmospheric moisture and movement velocities for\nNorth Atlantic hurricanes. Our findings indicate that even over such large\nareas a hurricane's rainfall cannot be accounted for by concurrent evaporation.\nWe propose instead that a hurricane consumes pre-existing atmospheric water\nvapor as it moves. The propagation velocity of the cyclone, i.e. the difference\nbetween its movement velocity and the mean velocity of the surrounding air\n(steering flow), determines the water vapor budget. Water vapor available to\nthe hurricane through its movement makes the hurricane self-sufficient at about\n700 km from the hurricane center obviating the need to concentrate moisture\nfrom greater distances. Such hurricanes leave a dry wake, whereby rainfall is\nsuppressed by up to 40 per cent compared to its long-term mean. The inner\nradius of this dry footprint approximately coincides with the radius of\nhurricane self-sufficiency with respect to water vapor. We discuss how Carnot\nefficiency considerations do not constrain the power of such open systems that\ndeplete the pre-existing moisture. Our findings emphasize the incompletely\nunderstood role and importance of atmospheric moisture supplies, condensation\nand precipitation in hurricane dynamics. \n\n"}
{"id": "1604.07177", "contents": "Title: On the Use of Penalty MCMC for Differential Privacy Abstract: We view the penalty algorithm of Ceperley and Dewing (1999), a Markov chain\nMonte Carlo (MCMC) algorithm for Bayesian inference, in the context of data\nprivacy. Specifically, we study differential privacy of the penalty algorithm\nand advocate its use for data privacy. We show that in the simple model of\nindependent observations the algorithm has desirable convergence and privacy\nproperties that scale with data size. Two special cases are also investigated\nand privacy preserving schemes are proposed for those cases: (i) Data are\ndistributed among several data owners who are interested in the inference of a\ncommon parameter while preserving their data privacy. (ii) The data likelihood\nbelongs to an exponential family. \n\n"}
{"id": "1604.07406", "contents": "Title: Is lightning a possible source of the radio emission on HAT-P-11b? Abstract: Lightning induced radio emission has been observed on Solar system planets.\nThere have been many attempts to observe exoplanets in the radio wavelength,\nhowever, no unequivocal detection has been reported. Lecavelier des Etangs et\nal. carried out radio transit observations of the exoplanet HAT-P-11b, and\nsuggested that a small part of the radio flux can be attributed to the planet.\nHere, we assume that this signal is real, and study if this radio emission\ncould be caused by lightning with similar energetic properties like in the\nSolar system. We find that a lightning storm with 3.8 x $10^6$ times larger\nflash densities than the Earth-storms with the largest lightning activity is\nneeded to produce the observed signal from HAT-P-11b. The optical emission of\nsuch thunderstorm would be comparable to that of the host star. We show that\nHCN produced by lightning chemistry is observable 2-3 yr after the storm, which\nproduces signatures in the L (3.0-4.0 \\mu m) and N (7.5-14.5 \\mu m) infrared\nbands. We conclude that it is unlikely that the observed radio signal was\nproduced by lightning, however, future, combined radio and infrared\nobservations may lead to lightning detection on planets outside the Solar\nsystem. \n\n"}
{"id": "1604.07451", "contents": "Title: Learning Local Dependence In Ordered Data Abstract: In many applications, data come with a natural ordering. This ordering can\noften induce local dependence among nearby variables. However, in complex data,\nthe width of this dependence may vary, making simple assumptions such as a\nconstant neighborhood size unrealistic. We propose a framework for learning\nthis local dependence based on estimating the inverse of the Cholesky factor of\nthe covariance matrix. Penalized maximum likelihood estimation of this matrix\nyields a simple regression interpretation for local dependence in which\nvariables are predicted by their neighbors. Our proposed method involves\nsolving a convex, penalized Gaussian likelihood problem with a hierarchical\ngroup lasso penalty. The problem decomposes into independent subproblems which\ncan be solved efficiently in parallel using first-order methods. Our method\nyields a sparse, symmetric, positive definite estimator of the precision\nmatrix, encoding a Gaussian graphical model. We derive theoretical results not\nfound in existing methods attaining this structure. In particular, our\nconditions for signed support recovery and estimation consistency rates in\nmultiple norms are as mild as those in a regression problem. Empirical results\nshow our method performing favorably compared to existing methods. We apply our\nmethod to genomic data to flexibly model linkage disequilibrium. Our method is\nalso applied to improve the performance of discriminant analysis in sound\nrecording classification. \n\n"}
{"id": "1604.08098", "contents": "Title: Local Uncertainty Sampling for Large-Scale Multi-Class Logistic\n  Regression Abstract: A major challenge for building statistical models in the big data era is that\nthe available data volume far exceeds the computational capability. A common\napproach for solving this problem is to employ a subsampled dataset that can be\nhandled by available computational resources. In this paper, we propose a\ngeneral subsampling scheme for large-scale multi-class logistic regression and\nexamine the variance of the resulting estimator. We show that asymptotically,\nthe proposed method always achieves a smaller variance than that of the uniform\nrandom sampling. Moreover, when the classes are conditionally imbalanced,\nsignificant improvement over uniform sampling can be achieved. Empirical\nperformance of the proposed method is compared to other methods on both\nsimulated and real-world datasets, and these results match and confirm our\ntheoretical analysis. \n\n"}
{"id": "1605.00551", "contents": "Title: Compatible finite element spaces for geophysical fluid dynamics Abstract: Compatible finite elements provide a framework for preserving important\nstructures in equations of geophysical fluid dynamics, and are becoming\nimportant in their use for building atmosphere and ocean models. We survey the\napplication of compatible finite element spaces to geophysical fluid dynamics,\nincluding the application to the nonlinear rotating shallow water equations,\nand the three-dimensional compressible Euler equations. We summarise analytic\nresults about dispersion relations and conservation properties, and present new\nresults on approximation properties in three dimensions on the sphere, and on\nhydrostatic balance properties. \n\n"}
{"id": "1605.01684", "contents": "Title: Fractional Brownian motion, the Matern process, and stochastic modeling\n  of turbulent dispersion Abstract: Stochastic process exhibiting power-law slopes in the frequency domain are\nfrequently well modeled by fractional Brownian motion (fBm). In particular, the\nspectral slope at high frequencies is associated with the degree of small-scale\nroughness or fractal dimension. However, a broad class of real-world signals\nhave a high-frequency slope, like fBm, but a plateau in the vicinity of zero\nfrequency. This low-frequency plateau, it is shown, implies that the temporal\nintegral of the process exhibits diffusive behavior, dispersing from its\ninitial location at a constant rate. Such processes are not well modeled by\nfBm, which has a singularity at zero frequency corresponding to an unbounded\nrate of dispersion. A more appropriate stochastic model is a much lesser-known\nrandom process called the Matern process, which is shown herein to be a damped\nversion of fractional Brownian motion. This article first provides a thorough\nintroduction to fractional Brownian motion, then examines the details of the\nMatern process and its relationship to fBm. An algorithm for the simulation of\nthe Matern process in O(N log N) operations is given. Unlike fBm, the Matern\nprocess is found to provide an excellent match to modeling velocities from\nparticle trajectories in an application to two-dimensional fluid turbulence. \n\n"}
{"id": "1605.02113", "contents": "Title: Likelihood Inflating Sampling Algorithm Abstract: Markov Chain Monte Carlo (MCMC) sampling from a posterior distribution\ncorresponding to a massive data set can be computationally prohibitive since\nproducing one sample requires a number of operations that is linear in the data\nsize. In this paper, we introduce a new communication-free parallel method, the\nLikelihood Inflating Sampling Algorithm (LISA), that significantly reduces\ncomputational costs by randomly splitting the dataset into smaller subsets and\nrunning MCMC methods independently in parallel on each subset using different\nprocessors. Each processor will be used to run an MCMC chain that samples\nsub-posterior distributions which are defined using an \"inflated\" likelihood\nfunction. We develop a strategy for combining the draws from different\nsub-posteriors to study the full posterior of the Bayesian Additive Regression\nTrees (BART) model. The performance of the method is tested using both\nsimulated and real data. \n\n"}
{"id": "1605.03855", "contents": "Title: Edge States in the Climate System: Exploring Global Instabilities and\n  Critical Transitions Abstract: Multistability is a ubiquitous feature in systems of geophysical relevance\nand provides key challenges for our ability to predict a system's response to\nperturbations. Near critical transitions small causes can lead to large effects\nand - for all practical purposes - irreversible changes in the properties of\nthe system. The Earth climate is multistable: present astronomical and\nastrophysical conditions support two stable regimes, the warm climate we live\nin, and a snowball climate, characterized by global glaciation. We first\nprovide an overview of methods and ideas relevant for studying the climate\nresponse to forcings and focus on the properties of critical transitions.\nFollowing an idea developed by Eckhardt and co. for the investigation of\nmultistable turbulent flows, we study the global instability giving rise to the\nsnowball/warm multistability in the climate system by identifying the climatic\nedge state, a saddle embedded in the boundary between the two basins of\nattraction of the stable climates. The edge state attracts initial conditions\nbelonging to such a boundary and is the gate facilitating noise-induced\ntransitions between competing attractors. We use a simplified yet Earth-like\nclimate model constructed by coupling a primitive equations model of the\natmosphere with a simple diffusive ocean. We refer to the climatic edge states\nas Melancholia states. We study their dynamics, their symmetry properties, and\nwe follow a complex set of bifurcations. We find situations where the\nMelancholia state has chaotic dynamics. In these cases, the basin boundary\nbetween the two basins of attraction is a strange geometric set with a nearly\nzero codimension, and relate this feature to the time scale separation between\ninstabilities occurring on weather and climatic time scales. We also discover a\nnew stable climatic state characterized by non-trivial symmetry properties. \n\n"}
{"id": "1605.04029", "contents": "Title: Simple, Scalable and Accurate Posterior Interval Estimation Abstract: There is a lack of simple and scalable algorithms for uncertainty\nquantification. Bayesian methods quantify uncertainty through posterior and\npredictive distributions, but it is difficult to rapidly estimate summaries of\nthese distributions, such as quantiles and intervals. Variational Bayes\napproximations are widely used, but may badly underestimate posterior\ncovariance. Typically, the focus of Bayesian inference is on point and interval\nestimates for one-dimensional functionals of interest. In small scale problems,\nMarkov chain Monte Carlo algorithms remain the gold standard, but such\nalgorithms face major problems in scaling up to big data. Various modifications\nhave been proposed based on parallelization and approximations based on\nsubsamples, but such approaches are either highly complex or lack theoretical\nsupport and/or good performance outside of narrow settings. We propose a very\nsimple and general posterior interval estimation algorithm, which is based on\nrunning Markov chain Monte Carlo in parallel for subsets of the data and\naveraging quantiles estimated from each subset. We provide strong theoretical\nguarantees and illustrate performance in several applications. \n\n"}
{"id": "1605.04281", "contents": "Title: Signal Regression Models for Location, Scale and Shape with an\n  Application to Stock Returns Abstract: We discuss scalar-on-function regression models where all parameters of the\nassumed response distribution can be modeled depending on covariates. We thus\ncombine signal regression models with generalized additive models for location,\nscale and shape (GAMLSS). We compare two fundamentally different methods for\nestimation, a gradient boosting and a penalized likelihood based approach, and\naddress practically important points like identifiability and model choice.\nEstimation by a component-wise gradient boosting algorithm allows for high\ndimensional data settings and variable selection. Estimation by a penalized\nlikelihood based approach has the advantage of directly provided statistical\ninference. The motivating application is a time series of stock returns where\nit is of interest to model both the expectation and the variance depending on\nlagged response values and functional liquidity curves. \n\n"}
{"id": "1605.04382", "contents": "Title: The mixed-phase version of moist-air entropy Abstract: The aim of this note is to derive the mixed-phase version of the moist-air\nentropy potential temperature $\\theta_s$ derived in Marquet (2011).\n  This mixed-phase version is suitable to describe parcels where liquid water\nand ice are allowed to coexist, with possible under- or super-saturations, with\npossible supercooled water and with possible different temperatures for dry air\nand water vapour, on the one hand, condensed water and ice, on the other hand.\n  The impact of this new mixed-phase version for $\\theta_s$ are evaluated by\nusing high latitudes, SHEBA/FIRE-ACE vertical profiles depicted in Figure 7 of\nMorisson et al. (2011). \n\n"}
{"id": "1605.05278", "contents": "Title: Exact Simulation of Noncircular or Improper Complex-Valued Stationary\n  Gaussian Processes using Circulant Embedding Abstract: This paper provides an algorithm for simulating improper (or noncircular)\ncomplex-valued stationary Gaussian processes. The technique utilizes recently\ndeveloped methods for multivariate Gaussian processes from the circulant\nembedding literature. The method can be performed in $\\mathcal{O}(n\\log_2 n)$\noperations, where $n$ is the length of the desired sequence. The method is\nexact, except when eigenvalues of prescribed circulant matrices are negative.\nWe evaluate the performance of the algorithm empirically, and provide a\npractical example where the method is guaranteed to be exact for all $n$, with\nan improper fractional Gaussian noise process. \n\n"}
{"id": "1605.05476", "contents": "Title: Localizing the Ensemble Kalman Particle Filter Abstract: Ensemble methods such as the Ensemble Kalman Filter (EnKF) are widely used\nfor data assimilation in large-scale geophysical applications, as for example\nin numerical weather prediction (NWP). There is a growing interest for physical\nmodels with higher and higher resolution, which brings new challenges for data\nassimilation techniques because of the presence of non-linear and non-Gaussian\nfeatures that are not adequately treated by the EnKF. We propose two new\nlocalized algorithms based on the Ensemble Kalman Particle Filter (EnKPF), a\nhybrid method combining the EnKF and the Particle Filter (PF) in a way that\nmaintains scalability and sample diversity. Localization is a key element of\nthe success of EnKFs in practice, but it is much more challenging to apply to\nPFs. The algorithms that we introduce in the present paper provide a compromise\nbetween the EnKF and the PF while avoiding some of the problems of localization\nfor pure PFs. Numerical experiments with a simplified model of cumulus\nconvection based on a modified shallow water equation show that the proposed\nalgorithms perform better than the local EnKF. In particular, the PF nature of\nthe method allows to capture non-Gaussian characteristics of the estimated\nfields such as the location of wet and dry areas. \n\n"}
{"id": "1605.05798", "contents": "Title: MCMC for Imbalanced Categorical Data Abstract: Many modern applications collect highly imbalanced categorical data, with\nsome categories relatively rare. Bayesian hierarchical models combat data\nsparsity by borrowing information, while also quantifying uncertainty. However,\nposterior computation presents a fundamental barrier to routine use; a single\nclass of algorithms does not work well in all settings and practitioners waste\ntime trying different types of MCMC approaches. This article was motivated by\nan application to quantitative advertising in which we encountered extremely\npoor computational performance for common data augmentation MCMC algorithms but\nobtained excellent performance for adaptive Metropolis. To obtain a deeper\nunderstanding of this behavior, we give strong theory results on computational\ncomplexity in an infinitely imbalanced asymptotic regime. Our results show\ncomputational complexity of Metropolis is logarithmic in sample size, while\ndata augmentation is polynomial in sample size. The root cause of poor\nperformance of data augmentation is a discrepancy between the rates at which\nthe target density and MCMC step sizes concentrate. In general, MCMC algorithms\nthat have a similar discrepancy will fail in large samples - a result with\nsubstantial practical impact. \n\n"}
{"id": "1605.07604", "contents": "Title: Posterior Dispersion Indices Abstract: Probabilistic modeling is cyclical: we specify a model, infer its posterior,\nand evaluate its performance. Evaluation drives the cycle, as we revise our\nmodel based on how it performs. This requires a metric. Traditionally,\npredictive accuracy prevails. Yet, predictive accuracy does not tell the whole\nstory. We propose to evaluate a model through posterior dispersion. The idea is\nto analyze how each datapoint fares in relation to posterior uncertainty around\nthe hidden structure. We propose a family of posterior dispersion indices (PDI)\nthat capture this idea. A PDI identifies rich patterns of model mismatch in\nthree real data examples: voting preferences, supermarket shopping, and\npopulation genetics. \n\n"}
{"id": "1605.09107", "contents": "Title: Analysis of nonstationary modulated time series with applications to\n  oceanographic flow measurements Abstract: We propose a new class of univariate nonstationary time series models, using\nthe framework of modulated time series, which is appropriate for the analysis\nof rapidly-evolving time series as well as time series observations with\nmissing data. We extend our techniques to a class of bivariate time series that\nare isotropic. Exact inference is often not computationally viable for time\nseries analysis, and so we propose an estimation method based on the\nWhittle-likelihood, a commonly adopted pseudo-likelihood. Our inference\nprocedure is shown to be consistent under standard assumptions, as well as\nhaving considerably lower computational cost than exact likelihood in general.\nWe show the utility of this framework for the analysis of drifting instruments,\nan analysis that is key to characterising global ocean circulation and\ntherefore also for decadal to century-scale climate understanding. \n\n"}
{"id": "1605.09454", "contents": "Title: Parallel Markov Chain Monte Carlo via Spectral Clustering Abstract: As it has become common to use many computer cores in routine applications,\nfinding good ways to parallelize popular algorithms has become increasingly\nimportant. In this paper, we present a parallelization scheme for Markov chain\nMonte Carlo (MCMC) methods based on spectral clustering of the underlying state\nspace, generalizing earlier work on parallelization of MCMC methods by state\nspace partitioning. We show empirically that this approach speeds up MCMC\nsampling for multimodal distributions and that it can be usefully applied in\ngreater generality than several related algorithms. Our algorithm converges\nunder reasonable conditions to an `optimal' MCMC algorithm. We also show that\nour approach can be asymptotically far more efficient than naive\nparallelization, even in situations such as completely flat target\ndistributions where no unique optimal algorithm exists. Finally, we combine\ntheoretical and empirical bounds to provide practical guidance on the choice of\ntuning parameters. \n\n"}
{"id": "1606.00566", "contents": "Title: Zonal-mean circulation response to reduced air-sea momentum roughness Abstract: The impact of uncertainties in surface layer physics on the atmospheric\ngeneral circulation is comparatively unexplored. Here the sensitivity of the\nzonal-mean circulation to reduced air-sea momentum roughness ($Z_{0m}$) at low\nflow speed is investigated with the Community Atmosphere Model (CAM3). In an\naquaplanet framework with prescribed sea surface temperatures, the response to\nreduced $Z_{0m}$ resembles the La Ni$\\tilde{\\text{n}}$a minus El\nNi$\\tilde{\\text{n}}$o response to El Ni$\\tilde{\\text{n}}$o Southern Oscillation\nvariability with: i) a poleward shift of the mid-latitude westerlies extending\nall the way to the surface; ii) a weak poleward shift of the subtropical\ndescent region; and iii) a weakening of the Hadley circulation, which is\ngenerally also accompanied by a poleward shift of the inter-tropical\nconvergence zone (ITCZ) and the tropical surface easterlies. Mechanism-denial\nexperiments show this response to be initiated by the reduction of tropical\nlatent and sensible heat fluxes, effected by reducing $Z_{0m}$. The circulation\nresponse is elucidated by considering the effect of the tropical energy fluxes\non the Hadley circulation strength, the upper tropospheric critical layer\nlatitudes, and the lower-tropospheric baroclinic eddy forcing. The ITCZ shift\nis understood via moist static energy budget analysis in the tropics. The\ncirculation response to reduced $Z_{0m}$ carries over to more complex setups\nwith seasonal cycle, full complexity of atmosphere-ice-land-ocean interaction,\nand a slab ocean lower boundary condition. Hence, relatively small changes in\nthe surface parameterization parameters can lead to a significant circulation\nresponse. \n\n"}
{"id": "1606.01156", "contents": "Title: Coupling of Particle Filters Abstract: Particle filters provide Monte Carlo approximations of intractable quantities\nsuch as point-wise evaluations of the likelihood in state space models. In many\nscenarios, the interest lies in the comparison of these quantities as some\nparameter or input varies. To facilitate such comparisons, we introduce and\nstudy methods to couple two particle filters in such a way that the correlation\nbetween the two underlying particle systems is increased. The motivation stems\nfrom the classic variance reduction technique of positively correlating two\nestimators. The key challenge in constructing such a coupling stems from the\ndiscontinuity of the resampling step of the particle filter. As our first\ncontribution, we consider coupled resampling algorithms. Within bootstrap\nparticle filters, they improve the precision of finite-difference estimators of\nthe score vector and boost the performance of particle marginal\nMetropolis--Hastings algorithms for parameter inference. The second\ncontribution arises from the use of these coupled resampling schemes within\nconditional particle filters, allowing for unbiased estimators of smoothing\nfunctionals. The result is a new smoothing strategy that operates by averaging\na number of independent and unbiased estimators, which allows for 1)\nstraightforward parallelization and 2) the construction of accurate error\nestimates. Neither of the above is possible with existing particle smoothers. \n\n"}
{"id": "1606.02275", "contents": "Title: Measuring the reliability of MCMC inference with bidirectional Monte\n  Carlo Abstract: Markov chain Monte Carlo (MCMC) is one of the main workhorses of\nprobabilistic inference, but it is notoriously hard to measure the quality of\napproximate posterior samples. This challenge is particularly salient in black\nbox inference methods, which can hide details and obscure inference failures.\nIn this work, we extend the recently introduced bidirectional Monte Carlo\ntechnique to evaluate MCMC-based posterior inference algorithms. By running\nannealed importance sampling (AIS) chains both from prior to posterior and vice\nversa on simulated data, we upper bound in expectation the symmetrized KL\ndivergence between the true posterior distribution and the distribution of\napproximate samples. We present Bounding Divergences with REverse Annealing\n(BREAD), a protocol for validating the relevance of simulated data experiments\nto real datasets, and integrate it into two probabilistic programming\nlanguages: WebPPL and Stan. As an example of how BREAD can be used to guide the\ndesign of inference algorithms, we apply it to study the effectiveness of\ndifferent model representations in both WebPPL and Stan. \n\n"}
{"id": "1606.03749", "contents": "Title: Scalable Bayesian variable selection and model averaging under block\n  orthogonal design Abstract: We propose a scalable algorithmic framework for exact Bayesian variable\nselection and model averaging in linear models under the assumption that the\nGram matrix is block-diagonal, and as a heuristic for exploring the model space\nfor general designs. In block-diagonal designs our approach returns the most\nprobable model of any given size without resorting to numerical integration.\nThe algorithm also provides a novel and efficient solution to the frequentist\nbest subset selection problem for block-diagonal designs. Posterior\nprobabilities for any number of models are obtained by evaluating a single\none-dimensional integral that can be computed upfront, and other quantities of\ninterest such as variable inclusion probabilities and model averaged regression\nestimates by carrying out an adaptive, deterministic one-dimensional numerical\nintegration. The overall computational cost scales linearly with the number of\nblocks, which can be processed in parallel, and exponentially with the block\nsize, rendering it most adequate in situations where predictors are organized\nin many moderately-sized blocks. For general designs, we approximate the Gram\nmatrix by a block-diagonal using spectral clustering and propose an iterative\nalgorithm that capitalizes on the block-diagonal algorithms to explore\nefficiently the model space. All methods proposed in this article are\nimplemented in the R library mombf. \n\n"}
{"id": "1606.03757", "contents": "Title: DNest4: Diffusive Nested Sampling in C++ and Python Abstract: In probabilistic (Bayesian) inferences, we typically want to compute\nproperties of the posterior distribution, describing knowledge of unknown\nquantities in the context of a particular dataset and the assumed prior\ninformation. The marginal likelihood, also known as the \"evidence\", is a key\nquantity in Bayesian model selection. The Diffusive Nested Sampling algorithm,\na variant of Nested Sampling, is a powerful tool for generating posterior\nsamples and estimating marginal likelihoods. It is effective at solving complex\nproblems including many where the posterior distribution is multimodal or has\nstrong dependencies between variables. DNest4 is an open source (MIT licensed),\nmulti-threaded implementation of this algorithm in C++11, along with associated\nutilities including: i) RJObject, a class template for finite mixture models,\n(ii) A Python package allowing basic use without C++ coding, and iii)\nExperimental support for models implemented in Julia. In this paper we\ndemonstrate DNest4 usage through examples including simple Bayesian data\nanalysis, finite mixture models, and Approximate Bayesian Computation. \n\n"}
{"id": "1606.03766", "contents": "Title: ContaminatedMixt: An R Package for Fitting Parsimonious Mixtures of\n  Multivariate Contaminated Normal Distributions Abstract: We introduce the R package ContaminatedMixt, conceived to disseminate the use\nof mixtures of multivariate contaminated normal distributions as a tool for\nrobust clustering and classification under the common assumption of\nelliptically contoured groups. Thirteen variants of the model are also\nimplemented to introduce parsimony. The expectation-conditional maximization\nalgorithm is adopted to obtain maximum likelihood parameter estimates, and\nlikelihood-based model selection criteria are used to select the model and the\nnumber of groups. Parallel computation can be used on multicore PCs and\ncomputer clusters, when several models have to be fitted. Differently from the\nmore popular mixtures of multivariate normal and t distributions, this approach\nalso allows for automatic detection of mild outliers via the maximum a\nposteriori probabilities procedure. To exemplify the use of the package,\napplications to artificial and real data are presented. \n\n"}
{"id": "1606.05474", "contents": "Title: HELIOS: An Open-source, GPU-accelerated Radiative Transfer Code For\n  Self-consistent Exoplanetary Atmospheres Abstract: We present the open-source radiative transfer code named HELIOS, which is\nconstructed for studying exoplanetary atmospheres. In its initial version, the\nmodel atmospheres of HELIOS are one-dimensional and plane-parallel, and the\nequation of radiative transfer is solved in the two-stream approximation with\nnon-isotropic scattering. A small set of the main infrared absorbers is\nemployed, computed with the opacity calculator HELIOS-K and combined using a\ncorrelated-$k$ approximation. The molecular abundances originate from validated\nanalytical formulae for equilibrium chemistry. We compare HELIOS with the work\nof Miller-Ricci & Fortney using a model of GJ 1214b, and perform several tests,\nwhere we find: model atmospheres with single-temperature layers struggle to\nconverge to radiative equilibrium; $k$-distribution tables constructed with\n$\\gtrsim 0.01$ cm$^{-1}$ resolution in the opacity function ($ \\lesssim 10^3$\npoints per wavenumber bin) may result in errors $\\gtrsim 1$-10 % in the\nsynthetic spectra; and a diffusivity factor of 2 approximates well the exact\nradiative transfer solution in the limit of pure absorption. We construct\n\"null-hypothesis\" models (chemical equilibrium, radiative equilibrium and solar\nelement abundances) for 6 hot Jupiters. We find that the dayside emission\nspectra of HD 189733b and WASP-43b are consistent with the null hypothesis,\nwhile it consistently under-predicts the observed fluxes of WASP-8b, WASP-12b,\nWASP-14b and WASP-33b. We demonstrate that our results are somewhat insensitive\nto the choice of stellar models (blackbody, Kurucz or PHOENIX) and metallicity,\nbut are strongly affected by higher carbon-to-oxygen ratios. The code is\npublicly available as part of the Exoclimes Simulation Platform (ESP;\nexoclime.net). \n\n"}
{"id": "1606.05578", "contents": "Title: Proximity Without Consensus in Online Multi-Agent Optimization Abstract: We consider stochastic optimization problems in multi-agent settings, where a\nnetwork of agents aims to learn parameters which are optimal in terms of a\nglobal objective, while giving preference to locally observed streaming\ninformation. To do so, we depart from the canonical decentralized optimization\nframework where agreement constraints are enforced, and instead formulate a\nproblem where each agent minimizes a global objective while enforcing network\nproximity constraints. This formulation includes online consensus optimization\nas a special case, but allows for the more general hypothesis that there is\ndata heterogeneity across the network. To solve this problem, we propose using\na stochastic saddle point algorithm inspired by Arrow and Hurwicz. This method\nyields a decentralized algorithm for processing observations sequentially\nreceived at each node of the network. Using Lagrange multipliers to penalize\nthe discrepancy between them, only neighboring nodes exchange model\ninformation. We establish that under a constant step-size regime the\ntime-average suboptimality and constraint violation are contained in a\nneighborhood whose radius vanishes with increasing number of iterations. As a\nconsequence, we prove that the time-average primal vectors converge to the\noptimal objective while satisfying the network proximity constraints. We apply\nthis method to the problem of sequentially estimating a correlated random field\nin a sensor network, as well as an online source localization problem, both of\nwhich demonstrate the empirical validity of the aforementioned convergence\nresults. \n\n"}
{"id": "1606.06351", "contents": "Title: Geometric MCMC for Infinite-Dimensional Inverse Problems Abstract: Bayesian inverse problems often involve sampling posterior distributions on\ninfinite-dimensional function spaces. Traditional Markov chain Monte Carlo\n(MCMC) algorithms are characterized by deteriorating mixing times upon\nmesh-refinement, when the finite-dimensional approximations become more\naccurate. Such methods are typically forced to reduce step-sizes as the\ndiscretization gets finer, and thus are expensive as a function of dimension.\nRecently, a new class of MCMC methods with mesh-independent convergence times\nhas emerged. However, few of them take into account the geometry of the\nposterior informed by the data. At the same time, recently developed geometric\nMCMC algorithms have been found to be powerful in exploring complicated\ndistributions that deviate significantly from elliptic Gaussian laws, but are\nin general computationally intractable for models defined in infinite\ndimensions. In this work, we combine geometric methods on a finite-dimensional\nsubspace with mesh-independent infinite-dimensional approaches. Our objective\nis to speed up MCMC mixing times, without significantly increasing the\ncomputational cost per step (for instance, in comparison with the vanilla\npreconditioned Crank-Nicolson (pCN) method). This is achieved by using ideas\nfrom geometric MCMC to probe the complex structure of an intrinsic\nfinite-dimensional subspace where most data information concentrates, while\nretaining robust mixing times as the dimension grows by using pCN-like methods\nin the complementary subspace. The resulting algorithms are demonstrated in the\ncontext of three challenging inverse problems arising in subsurface flow, heat\nconduction and incompressible flow control. The algorithms exhibit up to two\norders of magnitude improvement in sampling efficiency when compared with the\npCN method. \n\n"}
{"id": "1606.07995", "contents": "Title: Efficient data augmentation for fitting stochastic epidemic models to\n  prevalence data Abstract: Stochastic epidemic models describe the dynamics of an epidemic as a disease\nspreads through a population. Typically, only a fraction of cases are observed\nat a set of discrete times. The absence of complete information about the time\nevolution of an epidemic gives rise to a complicated latent variable problem in\nwhich the state space size of the epidemic grows large as the population size\nincreases. This makes analytically integrating over the missing data infeasible\nfor populations of even moderate size. We present a data augmentation Markov\nchain Monte Carlo (MCMC) framework for Bayesian estimation of stochastic\nepidemic model parameters, in which measurements are augmented with\nsubject-level disease histories. In our MCMC algorithm, we propose each new\nsubject-level path, conditional on the data, using a time-inhomogeneous\ncontinuous-time Markov process with rates determined by the infection histories\nof other individuals. The method is general, and may be applied, with minimal\nmodifications, to a broad class of stochastic epidemic models. We present our\nalgorithm in the context of multiple stochastic epidemic models in which the\ndata are binomially sampled prevalence counts, and apply our method to data\nfrom an outbreak of influenza in a British boarding school. \n\n"}
{"id": "1606.08088", "contents": "Title: In Situ and Ex Situ Formation Models of Kepler 11 Planets Abstract: We present formation simulations of the six Kepler 11 planets. Models assume\neither in situ or ex situ assembly, the latter with migration, and are evolved\nto the estimated age of the system, 8 Gyr. Models combine detailed calculations\nof both the gaseous envelope and the condensed core structures, including\naccretion of gas and solids, of the disk's viscous and thermal evolution,\nincluding photo-evaporation and disk-planet interactions, and of the planets'\nevaporative mass loss after disk dispersal. Planet-planet interactions are\nneglected. Both sets of simulations successfully reproduce measured radii,\nmasses, and orbital distances of the planets, except for the radius of Kepler\n11b, which loses its entire gaseous envelope shortly after formation. Gaseous\n(H+He) envelopes account for < 18% of the planet masses, and between 35 and 60%\nof the planet radii. In situ models predict a very massive inner disk, whose\nsolids' surface density (sigma_Z) varies from over 1e4 to 1e3 g/cm2 at\nstellocentric distances 0.1 < r < 0.5 AU. Initial gas densities would be in\nexcess of 1e5 g/cm2 if solids formed locally. Given the high disk temperatures\n(> 1000 K), planetary interiors can only be composed of metals and highly\nrefractory materials. Sequestration of hydrogen by the core and subsequent\noutgassing is required to account for the observed radius of Kepler 11b. Ex\nsitu models predict a relatively low-mass disk, whose initial sigma_Z varies\nfrom 10 to 5 g/cm2 at 0.5 < r < 7 AU and whose initial gas density ranges from\n1e3 to 100 g/cm2. All planetary interiors are expected to be rich in H2O, as\ncore assembly mostly occurs exterior to the ice condensation front. Kepler 11b\nis expected to have a steam atmosphere, and H2O is likely mixed with H+He in\nthe envelopes of the other planets. Results indicate that Kepler 11g may not be\nmore massive than Kepler 11e. \n\n"}
{"id": "1606.08373", "contents": "Title: Which ergodic averages have finite asymptotic variance? Abstract: We show that the class of $L^2$ functions for which ergodic averages of a\nreversible Markov chain have finite asymptotic variance is determined by the\nclass of $L^2$ functions for which ergodic averages of its associated jump\nchain have finite asymptotic variance. This allows us to characterize\ncompletely which ergodic averages have finite asymptotic variance when the\nMarkov chain is an independence sampler. In addition, we obtain a simple\nsufficient condition for all ergodic averages of $L^2$ functions of the primary\nvariable in a pseudo-marginal Markov chain to have finite asymptotic variance. \n\n"}
{"id": "1606.09172", "contents": "Title: Lightning climatology of exoplanets and brown dwarfs guided by Solar\n  System data Abstract: Clouds form on extrasolar planets and brown dwarfs where lightning could\noccur. Lightning is a tracer of atmospheric convection, cloud formation and\nionization processes as known from the Solar System, and may be significant for\nthe formation of prebiotic molecules. We study lightning climatology for the\ndifferent atmospheric environments of Earth, Venus, Jupiter and Saturn. We\npresent lightning distribution maps for Earth, Jupiter and Saturn, and flash\ndensities for these planets and Venus, based on optical and/or radio\nmeasurements from the WWLLN and STARNET radio networks, the LIS/OTD satellite\ninstruments, the Galileo, Cassini, New Horizons and Venus Express spacecraft.\nWe also present flash densities calculated for several phases of two volcano\neruptions, Eyjafjallaj\\\"okull's (2010) and Mt Redoubt's (2009). We estimate\nlightning rates for sample, transiting and directly imaged extrasolar planets\nand brown dwarfs. Based on the large variety of exoplanets, six categories are\nsuggested for which we use the lightning occurrence information from the Solar\nSystem. We examine lightning energy distributions for Earth, Jupiter and\nSaturn. We discuss how strong stellar activity may support lightning activity.\nWe provide a lower limit of the total number of flashes that might occur on\ntransiting planets during their full transit as input for future studies. We\nfind that volcanically very active planets might show the largest lightning\nflash densities. When applying flash densities of the large Saturnian storm\nfrom 2010/11, we find that the exoplanet HD 189733b would produce high\nlightning occurrence even during its short transit. \n\n"}
{"id": "1607.01529", "contents": "Title: Quantitative assessment of drivers of recent climate variability: An\n  information theoretic approach Abstract: Identification and quantification of possible drivers of recent climate\nvariability remain a challenging task. This important issue is addressed\nadopting a non-parametric information theory technique, the Transfer Entropy\nand its normalized variant. It distinctly quantifies actual information\nexchanged along with the directional flow of information between any two\nvariables with no bearing on their common history or inputs, unlike\ncorrelation, mutual information etc. Measurements of greenhouse gases, CO2,\nCH4, and N2O; volcanic aerosols; solar activity: UV radiation, total solar\nirradiance (TSI ) and cosmic ray flux (CR); El Nino Southern Oscillation (ENSO)\nand Global Mean Temperature Anomaly (GMTA) made during 1984-2005 are utilized\nto distinguish driving and responding climate signals. Estimates of their\nrelative contributions reveal that CO 2 (~24%), CH 4 (~19%) and volcanic\naerosols (~23%) are the primary contributors to the observed variations in\nGMTA. While, UV (~9%) and ENSO (~12%) act as secondary drivers of variations in\nthe GMTA, the remaining play a marginal role in the observed recent climate\nvariability. Interestingly, ENSO and GMTA mutually drive each other at varied\ntime lags. This study assists future modelling efforts in climate science. \n\n"}
{"id": "1607.01881", "contents": "Title: Goal-oriented optimal approximations of Bayesian linear inverse problems Abstract: We propose optimal dimensionality reduction techniques for the solution of\ngoal-oriented linear-Gaussian inverse problems, where the quantity of interest\n(QoI) is a function of the inversion parameters. These approximations are\nsuitable for large-scale applications. In particular, we study the\napproximation of the posterior covariance of the QoI as a low-rank negative\nupdate of its prior covariance, and prove optimality of this update with\nrespect to the natural geodesic distance on the manifold of symmetric positive\ndefinite matrices. Assuming exact knowledge of the posterior mean of the QoI,\nthe optimality results extend to optimality in distribution with respect to the\nKullback-Leibler divergence and the Hellinger distance between the associated\ndistributions. We also propose approximation of the posterior mean of the QoI\nas a low-rank linear function of the data, and prove optimality of this\napproximation with respect to a weighted Bayes risk. Both of these optimal\napproximations avoid the explicit computation of the full posterior\ndistribution of the parameters and instead focus on directions that are well\ninformed by the data and relevant to the QoI. These directions stem from a\nbalance among all the components of the goal-oriented inverse problem: prior\ninformation, forward model, measurement noise, and ultimate goals. We\nillustrate the theory using a high-dimensional inverse problem in heat\ntransfer. \n\n"}
{"id": "1607.01904", "contents": "Title: Bayesian inverse problems with $l_1$ priors: a Randomize-then-Optimize\n  approach Abstract: Prior distributions for Bayesian inference that rely on the $l_1$-norm of the\nparameters are of considerable interest, in part because they promote parameter\nfields with less regularity than Gaussian priors (e.g., discontinuities and\nblockiness). These $l_1$-type priors include the total variation (TV) prior and\nthe Besov $B^s_{1,1}$ space prior, and in general yield non-Gaussian posterior\ndistributions. Sampling from these posteriors is challenging, particularly in\nthe inverse problem setting where the parameter space is high-dimensional and\nthe forward problem may be nonlinear. This paper extends the\nrandomize-then-optimize (RTO) method, an optimization-based sampling algorithm\ndeveloped for Bayesian inverse problems with Gaussian priors, to inverse\nproblems with $l_1$-type priors. We use a variable transformation to convert an\n$l_1$-type prior to a standard Gaussian prior, such that the posterior\ndistribution of the transformed parameters is amenable to Metropolized sampling\nvia RTO. We demonstrate this approach on several deconvolution problems and an\nelliptic PDE inverse problem, using TV or Besov $B^s_{1,1}$ space priors. Our\nresults show that the transformed RTO algorithm characterizes the correct\nposterior distribution and can be more efficient than other sampling\nalgorithms. The variable transformation can also be extended to other\nnon-Gaussian priors. \n\n"}
{"id": "1607.02188", "contents": "Title: Whole-brain substitute CT generation using Markov random field mixture\n  models Abstract: Computed tomography (CT) equivalent information is needed for attenuation\ncorrection in PET imaging and for dose planning in radiotherapy. Prior work has\nshown that Gaussian mixture models can be used to generate a substitute CT\n(s-CT) image from a specific set of MRI modalities. This work introduces a more\nflexible class of mixture models for s-CT generation, that incorporates spatial\ndependency in the data through a Markov random field prior on the latent field\nof class memberships associated with a mixture model. Furthermore, the mixture\ndistributions are extended from Gaussian to normal inverse Gaussian (NIG),\nallowing heavier tails and skewness. The amount of data needed to train a model\nfor s-CT generation is of the order of 100 million voxels. The computational\nefficiency of the parameter estimation and prediction methods are hence\nparamount, especially when spatial dependency is included in the models. A\nstochastic Expectation Maximization (EM) gradient algorithm is proposed in\norder to tackle this challenge. The advantages of the spatial model and NIG\ndistributions are evaluated with a cross-validation study based on data from 14\npatients. The study show that the proposed model enhances the predictive\nquality of the s-CT images by reducing the mean absolute error with 17.9%.\nAlso, the distribution of CT values conditioned on the MR images are better\nexplained by the proposed model as evaluated using continuous ranked\nprobability scores. \n\n"}
{"id": "1607.02472", "contents": "Title: Two Iterative Proximal-Point Algorithms for the Calculus of\n  Divergence-based Estimators with Application to Mixture Models Abstract: Estimators derived from an EM algorithm are not robust since they are based\non the maximization of the likelihood function. We propose a proximal-point\nalgorithm based on the EM algorithm which aim to minimize a divergence\ncriterion. Resulting estimators are generally robust against outliers and\nmisspecification. An EM-type proximal-point algorithm is also introduced in\norder to produce robust estimators for mixture models. Convergence properties\nof the two algorithms are treated. We relax an identifiability condition\nimposed on the proximal term in the literature; a condition which is generally\nnot fulfilled by mixture models. The convergence of the introduced algorithms\nis discussed on a two-component Weibull mixture and a two-component Gaussian\nmixture entailing a condition on the initialization of the EM algorithm in\norder for the later to converge. Simulations on mixture models using different\nstatistical divergences are provided to confirm the validity of our work and\nthe robustness of the resulting estimators against outliers in comparison to\nthe EM algorithm. \n\n"}
{"id": "1607.03195", "contents": "Title: Multi-Step Bayesian Optimization for One-Dimensional Feasibility\n  Determination Abstract: Bayesian optimization methods allocate limited sampling budgets to maximize\nexpensive-to-evaluate functions. One-step-lookahead policies are often used,\nbut computing optimal multi-step-lookahead policies remains a challenge. We\nconsider a specialized Bayesian optimization problem: finding the superlevel\nset of an expensive one-dimensional function, with a Markov process prior. We\ncompute the Bayes-optimal sampling policy efficiently, and characterize the\nsuboptimality of one-step lookahead. Our numerical experiments demonstrate that\nthe one-step lookahead policy is close to optimal in this problem, performing\nwithin 98% of optimal in the experimental settings considered. \n\n"}
{"id": "1607.04308", "contents": "Title: Comments on \"Isentropic Analysis of a Simulated Hurricane\" Abstract: This paper describes Comments to the paper of Mrowiec et al. published in the\nJ. Atmos. Sci. in May 2016 (Vol 73, Issue 5, pages 1857-1870) and entitled\n\"Isentropic analysis of a simulated hurricane\".\n  It is explained that the plotting of isentropic surfaces (namely the\nisentropes) requires a precise definition of the specific moist-air entropy,\nand that most of existing \"equivalent potential temperatures\" lead to\ninaccurate definitions of isentropes.\n  It is shown that the use of the third law of thermodynamics leads to a\ndefinition of the specific moist-air entropy (and of a corresponding potential\ntemperature) which allow the plotting of unambigous moist-air isentropes.\n  Numerical applications are shown by using a numerical simulation of the\nhurricane DUMILE. \n\n"}
{"id": "1607.05981", "contents": "Title: Fitting logistic multilevel models with crossed random effects via\n  Bayesian Integrated Nested Laplace Approximations: a simulation study Abstract: Fitting cross-classified multilevel models with binary response is\nchallenging. In this setting a promising method is Bayesian inference through\nIntegrated Nested Laplace Approximations (INLA), which performs well in several\nlatent variable models. Therefore we devise a systematic simulation study to\nassess the performance of INLA with cross-classified logistic data under\ndifferent scenarios defined by the magnitude of the random effects variances,\nthe number of observations, the number of clusters, and the degree of\ncross-classification. In the simulations INLA is systematically compared with\nthe popular method of Maximum Likelihood via Laplace Approximation. By an\napplication to the classical salamander mating data, we compare INLA with the\nbest performing methods. Given the computational speed and the generally good\nperformance, INLA turns out to be a valuable method for fitting the considered\ncross-classified models. \n\n"}
{"id": "1607.06252", "contents": "Title: Strong solutions to the 3D primitive equations with only horizontal\n  dissipation: near $H^1$ initial data Abstract: In this paper, we consider the initial-boundary value problem of the\nthree-dimensional primitive equations for oceanic and atmospheric dynamics with\nonly horizontal viscosity and horizontal diffusivity. We establish the local,\nin time, well-posedness of strong solutions, for any initial data $(v_0,\nT_0)\\in H^1$, by using the local, in space, type energy estimate. We also\nestablish the global well-posedness of strong solutions for this system, with\nany initial data $(v_0, T_0)\\in H^1\\cap L^\\infty$, such that $\\partial_zv_0\\in\nL^m$, for some $m\\in(2,\\infty)$, by using the logarithmic type anisotropic\nSobolev inequality and a logarithmic type Gronwall inequality. This paper\nimproves the previous results obtained in [Cao, C.; Li, J.; Titi, E.S.: Global\nwell-posedness of the 3D primitive equations with only horizontal viscosity and\ndiffusivity, Comm. Pure Appl.Math., Vol. 69 (2016), 1492-1531.], where the\ninitial data $(v_0, T_0)$ was assumed to have $H^2$ regularity. \n\n"}
{"id": "1607.08799", "contents": "Title: Particle Filtering with Invertible Particle Flow Abstract: A key challenge when designing particle filters in high-dimensional state\nspaces is the construction of a proposal distribution that is close to the\nposterior distribution. Recent advances in particle flow filters provide a\npromising avenue to avoid weight degeneracy; particles drawn from the prior\ndistribution are migrated in the state-space to the posterior distribution by\nsolving partial differential equations. Numerous particle flow filters have\nbeen proposed based on different assumptions concerning the flow dynamics.\nApproximations are needed in the implementation of all of these filters; as a\nresult the articles do not exactly match a sample drawn from the desired\nposterior distribution. Past efforts to correct the discrepancies involve\nexpensive calculations of importance weights. In this paper, we present new\nfilters which incorporate deterministic particle flows into an encompassing\nparticle filter framework. The valuable theoretical guarantees concerning\nparticle filter performance still apply, but we can exploit the attractive\nperformance of the particle flow methods. The filters we describe involve a\ncomputationally efficient weight update step, arising because the embedded\nparticle flows we design possess an invertible mapping property. We evaluate\nthe proposed particle flow particle filters' performance through numerical\nsimulations of a challenging multi-target multi-sensor tracking scenario and\ncomplex high-dimensional filtering examples. \n\n"}
{"id": "1607.08845", "contents": "Title: Limit theorems for the Zig-Zag process Abstract: Markov chain Monte Carlo methods provide an essential tool in statistics for\nsampling from complex probability distributions. While the standard approach to\nMCMC involves constructing discrete-time reversible Markov chains whose\ntransition kernel is obtained via the Metropolis- Hastings algorithm, there has\nbeen recent interest in alternative schemes based on piecewise deterministic\nMarkov processes (PDMPs). One such approach is based on the Zig-Zag process,\nintroduced in Bierkens and Roberts (2016), which proved to provide a highly\nscalable sampling scheme for sampling in the big data regime (Bierkens,\nFearnhead and Roberts (2016)). In this paper we study the performance of the\nZig-Zag sampler, focusing on the one-dimensional case. In particular, we\nidentify conditions under which a Central limit theorem (CLT) holds and\ncharacterize the asymptotic variance. Moreover, we study the influence of the\nswitching rate on the diffusivity of the Zig-Zag process by identifying a\ndiffusion limit as the switching rate tends to infinity. Based on our results\nwe compare the performance of the Zig-Zag sampler to existing Monte Carlo\nmethods, both analytically and through simulations. \n\n"}
{"id": "1608.00236", "contents": "Title: Minimizing Sum of Truncated Convex Functions and Its Applications Abstract: In this paper, we study a class of problems where the sum of truncated convex\nfunctions is minimized. In statistical applications, they are commonly\nencountered when $\\ell_0$-penalized models are fitted and usually lead to\nNP-Hard non-convex optimization problems. In this paper, we propose a general\nalgorithm for the global minimizer in low-dimensional settings. We also extend\nthe algorithm to high-dimensional settings, where an approximate solution can\nbe found efficiently. We introduce several applications where the sum of\ntruncated convex functions is used, compare our proposed algorithm with other\nexisting algorithms in simulation studies, and show its utility in\nedge-preserving image restoration on real data. \n\n"}
{"id": "1608.00489", "contents": "Title: Buoyancy driven turbulence and distributed chaos Abstract: It is shown, using results of recent direct numerical simulations, laboratory\nexperiments and atmospheric measurements, that buoyancy driven turbulence\nexhibits a broad diversity of the types of distributed chaos with its stretched\nexponential spectrum $\\exp(-k/k_{\\beta})^{\\beta}$. The distributed chaos with\n$\\beta = 1/3$ (determined by the helicity correlation integral) is the most\ncommon feature of the stably stratified turbulence (due to the strong helical\nwaves presence). These waves mostly dominate spectral properties of the\nvertical component of velocity field, while the horizontal component is\ndominated by the diffusive processes both for the weak and strong stable\nstratification ($\\beta =2/3$). For the last case influence of the low boundary\ncan overcome the wave effects and result in $\\beta =1/2$ for the vertical\ncomponent of the velocity field (the spontaneous breaking of the space\ntranslational symmetry - homogeneity). For the unstably stratified turbulence\nin the Rayleigh-Taylor mixing zone the diffusive processes ($\\beta =2/3$) are\nthe most common dominating processes in the anisotropic chaotic mixing of the\ntwo fluids under buoyancy forces. The distributed chaos in Rayleigh-B\\'{e}nard\nturbulent convection in an upright cell is determined by the strong confinement\nconditions. That is: the spontaneous breaking of the space translational\nsymmetry (homogeneity) by the finite boundaries ($\\beta = 1/2$) or by the\nnon-perfect orientation of the cell along the buoyancy direction ($\\beta\n=4/7$). In all types of turbulence appearance of an inertial range of scales\nresults in deformation of the distributed chaos and $\\beta =3/5$. \n\n"}
{"id": "1608.00990", "contents": "Title: Barrett: out-of-core processing of MultiNest output Abstract: Barrett is a Python package for processing and visualising statistical\ninferences made using the nested sampling algorithm MultiNest. The main\ndifferential feature from competitors are full out-of-core processing allowing\nbarrett to handle arbitrarily large datasets. This is achieved by using the\nHDF5 data format. \n\n"}
{"id": "1608.02148", "contents": "Title: Randomized Matrix Decompositions using R Abstract: Matrix decompositions are fundamental tools in the area of applied\nmathematics, statistical computing, and machine learning. In particular,\nlow-rank matrix decompositions are vital, and widely used for data analysis,\ndimensionality reduction, and data compression. Massive datasets, however, pose\na computational challenge for traditional algorithms, placing significant\nconstraints on both memory and processing power. Recently, the powerful concept\nof randomness has been introduced as a strategy to ease the computational load.\nThe essential idea of probabilistic algorithms is to employ some amount of\nrandomness in order to derive a smaller matrix from a high-dimensional data\nmatrix. The smaller matrix is then used to compute the desired low-rank\napproximation. Such algorithms are shown to be computationally efficient for\napproximating matrices with low-rank structure. We present the \\proglang{R}\npackage rsvd, and provide a tutorial introduction to randomized matrix\ndecompositions. Specifically, randomized routines for the singular value\ndecomposition, (robust) principal component analysis, interpolative\ndecomposition, and CUR decomposition are discussed. Several examples\ndemonstrate the routines, and show the computational advantage over other\nmethods implemented in R. \n\n"}
{"id": "1608.04770", "contents": "Title: On the Charney Conjecture of Data Assimilation Employing Temperature\n  Measurements Alone: The Paradigm of 3D Planetary Geostrophic Model Abstract: Analyzing the validity and success of a data assimilation algorithm when some\nstate variable observations are not available is an important problem in\nmeteorology and engineering. We present an improved data assimilation algorithm\nfor recovering the exact full reference solution (i.e. the velocity and\ntemperature) of the 3D Planetary Geostrophic model, at an exponential rate in\ntime, by employing coarse spatial mesh observations of the temperature alone.\nThis provides, in the case of this paradigm, a rigorous justification to an\nearlier conjecture of Charney which states that temperature history of the\natmosphere, for certain simple atmospheric models, determines all other state\nvariables. \n\n"}
{"id": "1608.05271", "contents": "Title: High-order accurate finite-volume formulations for the pressure gradient\n  force in layered ocean models Abstract: The development of a set of high-order accurate finite-volume formulations\nfor evaluation of the pressure gradient force in layered ocean models is\ndescribed. A pair of new schemes are presented, both based on an integration of\nthe contact pressure force about the perimeter of an associated momentum\ncontrol-volume. The two proposed methods differ in their choice of\ncontrol-volume geometries. High-order accurate numerical integration techniques\nare employed in both schemes to account for non-linearities in the underlying\nequation-of-state definitions and thermodynamic profiles, and details of an\nassociated vertical interpolation and quadrature scheme are discussed in\ndetail. Numerical experiments are used to confirm the consistency of the two\nformulations, and it is demonstrated that the new methods maintain hydrostatic\nand thermobaric equilibrium in the presence of strongly-sloping layer-wise\ngeometry, non-linear equation-of-state definitions and non-uniform vertical\nstratification profiles. Additionally, one scheme is shown to maintain high\nlevels of consistency in the presence of non-linear thermodynamic\nstratification. Use of the new pressure gradient force formulations for hybrid\nvertical coordinate and/or terrain-following general circulation models is\ndiscussed. \n\n"}
{"id": "1608.05373", "contents": "Title: Zonal-flow dynamics from a phase-space perspective Abstract: The wave kinetic equation (WKE) describing drift-wave (DW) turbulence is\nwidely used in studies of zonal flows (ZFs) emerging from DW turbulence.\nHowever, this formulation neglects the exchange of enstrophy between DWs and\nZFs and also ignores effects beyond the geometrical-optics limit. We derive a\nmodified theory that takes both of these effects into account, while still\ntreating DW quanta (\"driftons\") as particles in phase space. The drifton\ndynamics is described by an equation of the Wigner-Moyal type, which is\ncommonly known in the phase-space formulation of quantum mechanics. In the\ngeometrical-optics limit, this formulation features additional terms missing in\nthe traditional WKE that ensure exact conservation of the total enstrophy of\nthe system, in addition to the total energy, which is the only conserved\ninvariant in previous theories based on the WKE. Numerical simulations are\npresented to illustrate the importance of these additional terms. The proposed\nformulation can be considered as a phase-space representation of the\nsecond-order cumulant expansion, or CE2. \n\n"}
{"id": "1608.07263", "contents": "Title: Tutorial models of the climate and habitability of Proxima Centauri b: a\n  thin atmosphere is sufficient to distribute heat given low stellar flux Abstract: Proxima Centauri b, an Earth-size planet in the habitable zone of our nearest\nstellar neighbour, has just been discovered. A theoretical framework of\nsynchronously rotating planets, in which the risk of a runaway greenhouse on\nthe sunlight side and atmospheric collapse on the reverse side are mutually\nameliorated via heat transport is discussed. This is developed via simple\n(tutorial) models of the climate. These show that lower incident stellar flux\nmeans that less heat transport, so less atmospheric mass, is required. The\nincident stellar flux at Proxima Centauri b is indeed low, which may help\nenhance habitability if it has suffered some atmospheric loss or began with a\nlow volatile inventory. \n\n"}
{"id": "1608.08814", "contents": "Title: Importance Sampling and Necessary Sample Size: an Information Theory\n  Approach Abstract: Importance sampling approximates expectations with respect to a target\nmeasure by using samples from a proposal measure. The performance of the method\nover large classes of test functions depends heavily on the closeness between\nboth measures. We derive a general bound that needs to hold for importance\nsampling to be successful, and relates the $f$-divergence between the target\nand the proposal to the sample size. The bound is deduced from a new and simple\ninformation theory paradigm for the study of importance sampling. As examples\nof the general theory we give necessary conditions on the sample size in terms\nof the Kullback-Leibler and $\\chi^2$ divergences, and the total variation and\nHellinger distances. Our approach is non-asymptotic, and its generality allows\nto tell apart the relative merits of these metrics. Unsurprisingly, the\nnon-symmetric divergences give sharper bounds than total variation or\nHellinger. Our results extend existing necessary conditions -and complement\nsufficient ones- on the sample size required for importance sampling. \n\n"}
{"id": "1609.00048", "contents": "Title: Practical sketching algorithms for low-rank matrix approximation Abstract: This paper describes a suite of algorithms for constructing low-rank\napproximations of an input matrix from a random linear image of the matrix,\ncalled a sketch. These methods can preserve structural properties of the input\nmatrix, such as positive-semidefiniteness, and they can produce approximations\nwith a user-specified rank. The algorithms are simple, accurate, numerically\nstable, and provably correct. Moreover, each method is accompanied by an\ninformative error bound that allows users to select parameters a priori to\nachieve a given approximation quality. These claims are supported by numerical\nexperiments with real and synthetic data. \n\n"}
{"id": "1609.03436", "contents": "Title: Quasi-stationary Monte Carlo and the ScaLE Algorithm Abstract: This paper introduces a class of Monte Carlo algorithms which are based upon\nthe simulation of a Markov process whose quasi-stationary distribution\ncoincides with a distribution of interest. This differs fundamentally from,\nsay, current Markov chain Monte Carlo methods which simulate a Markov chain\nwhose stationary distribution is the target. We show how to approximate\ndistributions of interest by carefully combining sequential Monte Carlo methods\nwith methodology for the exact simulation of diffusions. The methodology\nintroduced here is particularly promising in that it is applicable to the same\nclass of problems as gradient based Markov chain Monte Carlo algorithms but\nentirely circumvents the need to conduct Metropolis-Hastings type accept/reject\nsteps whilst retaining exactness: the paper gives theoretical guarantees\nensuring the algorithm has the correct limiting target distribution.\nFurthermore, this methodology is highly amenable to big data problems. By\nemploying a modification to existing na{\\\"\\i}ve sub-sampling and control\nvariate techniques it is possible to obtain an algorithm which is still exact\nbut has sub-linear iterative cost as a function of data size. \n\n"}
{"id": "1609.06549", "contents": "Title: Exploring the Venus global super-rotation using a comprehensive General\n  Circulation Model Abstract: The atmospheric circulation in Venus is well known to exhibit strong\nsuper-rotation. However, the atmospheric mechanisms responsible for the\nformation of this super-rotation are still not fully understood. In this work,\nwe developed a new Venus general circulation model to study the most likely\nmechanisms driving the atmosphere to the current observed circulation. Our\nmodel includes a new radiative transfer, convection and suitably adapted\nboundary layer schemes and a dynamical core that takes into account the\ndependence of the heat capacity at constant pressure with temperature.\n  The new Venus model is able to simulate a super-rotation phenomenon in the\ncloud region quantitatively similar to the one observed. The mechanisms\nmaintaining the strong winds in the cloud region were found in the model\nresults to be a combination of zonal mean circulation, thermal tides and\ntransient waves. In this process, the semi-diurnal tide excited in the upper\nclouds has a key contribution in transporting axial angular momentum mainly\nfrom the upper atmosphere towards the cloud region. The magnitude of the\nsuper-rotation in the cloud region is sensitive to various radiative parameters\nsuch as the amount of solar radiative energy absorbed by the surface, which\ncontrols the static stability near the surface. In this work, we also discuss\nthe main difficulties in representing the flow below the cloud base in Venus\natmospheric models.\n  Our new radiative scheme is more suitable for 3D Venus climate models than\nthose used in previous work due to its easy adaptability to different\natmospheric conditions. This flexibility of the model was crucial to explore\nthe uncertainties in the lower atmospheric conditions and may also be used in\nthe future to explore, for example, dynamical-radiative-microphysical\nfeedbacks. \n\n"}
{"id": "1609.07135", "contents": "Title: Convergence of Regression Adjusted Approximate Bayesian Computation Abstract: We present asymptotic results for the regression-adjusted version of\napproximate Bayesian computation introduced by Beaumont(2002). We show that for\nan appropriate choice of the bandwidth, regression adjustment will lead to a\nposterior that, asymptotically, correctly quantifies uncertainty. Furthermore,\nfor such a choice of bandwidth we can implement an importance sampling\nalgorithm to sample from the posterior whose acceptance probability tends to\nunity as the data sample size increases. This compares favourably to results\nfor standard approximate Bayesian computation, where the only way to obtain a\nposterior that correctly quantifies uncertainty is to choose a much smaller\nbandwidth; one for which the acceptance probability tends to zero and hence for\nwhich Monte Carlo error will dominate. \n\n"}
{"id": "1609.07363", "contents": "Title: Changepoint Detection in the Presence of Outliers Abstract: Many traditional methods for identifying changepoints can struggle in the\npresence of outliers, or when the noise is heavy-tailed. Often they will infer\nadditional changepoints in order to fit the outliers. To overcome this problem,\ndata often needs to be pre-processed to remove outliers, though this is\ndifficult for applications where the data needs to be analysed online. We\npresent an approach to changepoint detection that is robust to the presence of\noutliers. The idea is to adapt existing penalised cost approaches for detecting\nchanges so that they use loss functions that are less sensitive to outliers. We\nargue that loss functions that are bounded, such as the classical biweight\nloss, are particularly suitable -- as we show that only bounded loss functions\nare robust to arbitrarily extreme outliers. We present an efficient dynamic\nprogramming algorithm that can find the optimal segmentation under our\npenalised cost criteria. Importantly, this algorithm can be used in settings\nwhere the data needs to be analysed online. We show that we can consistently\nestimate the number of changepoints, and accurately estimate their locations,\nusing the biweight loss function. We demonstrate the usefulness of our approach\nfor applications such as analysing well-log data, detecting copy number\nvariation, and detecting tampering of wireless devices. \n\n"}
{"id": "1609.07378", "contents": "Title: Multi-Output Artificial Neural Network for Storm Surge Prediction in\n  North Carolina Abstract: During hurricane seasons, emergency managers and other decision makers need\naccurate and `on-time' information on potential storm surge impacts. Fully\ndynamical computer models, such as the ADCIRC tide, storm surge, and wind-wave\nmodel take several hours to complete a forecast when configured at high spatial\nresolution. Additionally, statically meaningful ensembles of high-resolution\nmodels (needed for uncertainty estimation) cannot easily be computed in near\nreal-time. This paper discusses an artificial neural network model for storm\nsurge prediction in North Carolina. The network model provides fast, real-time\nstorm surge estimates at coastal locations in North Carolina. The paper studies\nthe performance of the neural network model vs. other models on synthetic and\nreal hurricane data. \n\n"}
{"id": "1609.07532", "contents": "Title: Well-posed Bayesian Inverse Problems with Infinitely-Divisible and\n  Heavy-Tailed Prior Measures Abstract: We present a new class of prior measures in connection to $\\ell_p$\nregularization techniques when $p \\in(0,1)$ which is based on the generalized\nGamma distribution. We show that the resulting prior measure is heavy-tailed,\nnon-convex and infinitely divisible. Motivated by this observation we discuss\nthe class of infinitely divisible prior measures and draw a connection between\ntheir tail behavior and the tail behavior of their L{\\'evy} measures. Next, we\nuse the laws of pure jump L{\\'e}vy processes in order to define new classes of\nprior measures that are concentrated on the space of functions with bounded\nvariation. These priors serve as an alternative to the classic total variation\nprior and result in well-defined inverse problems. We then study the\nwell-posedness of Bayesian inverse problems in a general enough setting that\nencompasses the above mentioned classes of prior measures. We establish that\nwell-posedness relies on a balance between the growth of the log-likelihood\nfunction and the tail behavior of the prior and apply our results to special\ncases such as additive noise models and linear problems. Finally, we discuss\nsome of the practical aspects of Bayesian inverse problems such as their\nconsistent approximation and present three concrete examples of well-posed\nBayesian inverse problems with heavy-tailed or stochastic process prior\nmeasures. \n\n"}
{"id": "1610.00060", "contents": "Title: Global well-posedness for passively transported nonlinear moisture\n  dynamics with phase changes Abstract: We study a moisture model for warm clouds that has been used by Klein and\nMajda as a basis for multiscale asymptotic expansions for deep convective\nphenomena. These moisture balance equations correspond to a bulk microphysics\nclosure in the spirit of Kessler and of Grabowski and Smolarkiewicz, in which\nwater is present in the gaseous state as water vapor and in the liquid phase as\ncloud water and rain water. It thereby contains closures for the phase changes\ncondensation and evaporation, as well as the processes of autoconversion of\ncloud water into rainwater and the collection of cloud water by the falling\nrain droplets. Phase changes are associated with enormous amounts of latent\nheat and therefore provide a strong coupling to the thermodynamic equation.\n  In this work we assume the velocity field to be given and prove rigorously\nthe global existence and uniqueness of uniformly bounded solutions of the\nmoisture model with viscosity, diffusion and heat conduction. To guarantee\nlocal well-posedness we first need to establish local existence results for\nlinear parabolic equations, subject to the Robin boundary conditions on the\ncylindric type of domains under consideration. We then derive a priori\nestimates, for proving the maximum principle, using the Stampacchia method, as\nwell as the iterative method by Alikakos to obtain uniform boundedness. The\nevaporation term is of power law type, with an exponent in general less or\nequal to one and therefore making the proof of uniqueness more challenging.\nHowever, these difficulties can be circumvented by introducing new unknowns,\nwhich satisfy the required cancellation and monotonicity properties in the\nsource terms. \n\n"}
{"id": "1610.00195", "contents": "Title: Penalized Ensemble Kalman Filters for High Dimensional Non-linear\n  Systems Abstract: The ensemble Kalman filter (EnKF) is a data assimilation technique that uses\nan ensemble of models, updated with data, to track the time evolution of a\nusually non-linear system. It does so by using an empirical approximation to\nthe well-known Kalman filter. However, its performance can suffer when the\nensemble size is smaller than the state space, as is often necessary for\ncomputationally burdensome models. This scenario means that the empirical\nestimate of the state covariance is not full rank and possibly quite noisy. To\nsolve this problem in this high dimensional regime, we propose a\ncomputationally fast and easy to implement algorithm called the penalized\nensemble Kalman filter (PEnKF). Under certain conditions, it can be\ntheoretically proven that the PEnKF will be accurate (the estimation error will\nconverge to zero) despite having fewer ensemble members than state dimensions.\nFurther, as contrasted to localization methods, the proposed approach learns\nthe covariance structure associated with the dynamical system. These\ntheoretical results are supported with simulations of several non-linear and\nhigh dimensional systems. \n\n"}
{"id": "1610.03216", "contents": "Title: HELIOS-Retrieval: An Open-source, Nested Sampling Atmospheric Retrieval\n  Code, Application to the HR 8799 Exoplanets and Inferred Constraints for\n  Planet Formation Abstract: We present an open-source retrieval code named HELIOS-Retrieval (hereafter\nHELIOS-R), designed to obtain chemical abundances and temperature-pressure\nprofiles from inverting the measured spectra of exoplanetary atmospheres. In\nthe current implementation, we use an exact solution of the radiative transfer\nequation, in the pure absorption limit, in our forward model, which allows us\nto analytically integrate over all of the outgoing rays (instead of performing\nGaussian quadrature). Two chemistry models are considered: unconstrained\nchemistry (where the mixing ratios are treated as free parameters) and\nequilibrium chemistry (enforced via analytical formulae, where only the\nelemental abundances are free parameters). The nested sampling algorithm allows\nus to formally implement Occam's Razor based on a comparison of the Bayesian\nevidence between models. We perform a retrieval analysis on the measured\nspectra of the HR 8799b, c, d and e directly imaged exoplanets. Chemical\nequilibrium is disfavored by the Bayesian evidence for HR 8799b, c and d. We\nfind supersolar C/O, C/H and O/H values for the outer HR 8799b and c\nexoplanets, while the inner HR 8799d and e exoplanets have substellar C/O,\nsubstellar C/H and superstellar O/H values. If these retrieved properties are\nrepresentative of the bulk compositions of the exoplanets, then they are\ninconsistent with formation via gravitational instability (without late-time\naccretion) and consistent with a core accretion scenario in which late-time\naccretion of ices occurred differently for the inner and outer exoplanets. For\nHR 8799e, we find that spectroscopy in the K band is crucial for constraining\nC/O and C/H. HELIOS-R is publicly available as part of the Exoclimes Simulation\nPlatform (ESP; www.exoclime.org). \n\n"}
{"id": "1610.03483", "contents": "Title: Learning in Implicit Generative Models Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for\nconstructing generative models with several appealing properties: they do not\nrequire a likelihood function to be specified, only a generating procedure;\nthey provide samples that are sharp and compelling; and they allow us to\nharness our knowledge of building highly accurate neural network classifiers.\nHere, we develop our understanding of GANs with the aim of forming a rich view\nof this growing area of machine learning---to build connections to the diverse\nset of statistical thinking on this topic, of which much can be gained by a\nmutual exchange of ideas. We frame GANs within the wider landscape of\nalgorithms for learning in implicit generative models--models that only specify\na stochastic procedure with which to generate data--and relate these ideas to\nmodelling problems in related fields, such as econometrics and approximate\nBayesian computation. We develop likelihood-free inference methods and\nhighlight hypothesis testing as a principle for learning in implicit generative\nmodels, using which we are able to derive the objective function used by GANs,\nand many other related objectives. The testing viewpoint directs our focus to\nthe general problem of density ratio estimation. There are four approaches for\ndensity ratio estimation, one of which is a solution using classifiers to\ndistinguish real from generated data. Other approaches such as divergence\nminimisation and moment matching have also been explored in the GAN literature,\nand we synthesise these views to form an understanding in terms of the\nrelationships between them and the wider literature, highlighting avenues for\nfuture exploration and cross-pollination. \n\n"}
{"id": "1610.04272", "contents": "Title: Tensor Computation: A New Framework for High-Dimensional Problems in EDA Abstract: Many critical EDA problems suffer from the curse of dimensionality, i.e. the\nvery fast-scaling computational burden produced by large number of parameters\nand/or unknown variables. This phenomenon may be caused by multiple spatial or\ntemporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit\nsimulation), nonlinearity of devices and circuits, large number of design or\noptimization parameters (e.g. full-chip routing/placement and circuit sizing),\nor extensive process variations (e.g. variability/reliability analysis and\ndesign for manufacturability). The computational challenges generated by such\nhigh dimensional problems are generally hard to handle efficiently with\ntraditional EDA core algorithms that are based on matrix and vector\ncomputation. This paper presents \"tensor computation\" as an alternative general\nframework for the development of efficient EDA algorithms and tools. A tensor\nis a high-dimensional generalization of a matrix and a vector, and is a natural\nchoice for both storing and solving efficiently high-dimensional EDA problems.\nThis paper gives a basic tutorial on tensors, demonstrates some recent examples\nof EDA applications (e.g., nonlinear circuit modeling and high-dimensional\nuncertainty quantification), and suggests further open EDA problems where the\nuse of tensor computation could be of advantage. \n\n"}
{"id": "1610.04386", "contents": "Title: Random Feature Expansions for Deep Gaussian Processes Abstract: The composition of multiple Gaussian Processes as a Deep Gaussian Process\n(DGP) enables a deep probabilistic nonparametric approach to flexibly tackle\ncomplex machine learning problems with sound quantification of uncertainty.\nExisting inference approaches for DGP models have limited scalability and are\nnotoriously cumbersome to construct. In this work, we introduce a novel\nformulation of DGPs based on random feature expansions that we train using\nstochastic variational inference. This yields a practical learning framework\nwhich significantly advances the state-of-the-art in inference for DGPs, and\nenables accurate quantification of uncertainty. We extensively showcase the\nscalability and performance of our proposal on several datasets with up to 8\nmillion observations, and various DGP architectures with up to 30 hidden\nlayers. \n\n"}
{"id": "1610.05108", "contents": "Title: The xyz algorithm for fast interaction search in high-dimensional data Abstract: When performing regression on a dataset with $p$ variables, it is often of\ninterest to go beyond using main linear effects and include interactions as\nproducts between individual variables. For small-scale problems, these\ninteractions can be computed explicitly but this leads to a computational\ncomplexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be\nprohibitive if $p$ is very large. We introduce a new randomised algorithm that\nis able to discover interactions with high probability and under mild\nconditions has a runtime that is subquadratic in $p$. We show that strong\ninteractions can be discovered in almost linear time, whilst finding weaker\ninteractions requires $\\mathcal{O}(p^\\alpha)$ operations for $1 < \\alpha < 2$\ndepending on their strength. The underlying idea is to transform interaction\nsearch into a closestpair problem which can be solved efficiently in\nsubquadratic time. The algorithm is called $\\mathit{xyz}$ and is implemented in\nthe language R. We demonstrate its efficiency for application to genome-wide\nassociation studies, where more than $10^{11}$ interactions can be screened in\nunder $280$ seconds with a single-core $1.2$ GHz CPU. \n\n"}
{"id": "1610.08205", "contents": "Title: Wall modeling via function enrichment within a high-order DG method for\n  RANS simulations of incompressible flow Abstract: We present a novel approach to wall modeling for RANS within the\ndiscontinuous Galerkin method. Wall functions are not used to prescribe\nboundary conditions as usual but they are built into the function space of the\nnumerical method as a local enrichment, in addition to the standard polynomial\ncomponent. The Galerkin method then automatically finds the optimal solution\namong all shape functions available. This idea is fully consistent and gives\nthe wall model vast flexibility in separated boundary layers or high adverse\npressure gradients. The wall model is implemented in a high-order discontinuous\nGalerkin solver for incompressible flow complemented by the Spalart-Allmaras\nclosure model. As benchmark examples we present turbulent channel flow starting\nfrom $Re_{\\tau}=180$ and up to $Re_{\\tau}=100{,}000$ as well as flow past\nperiodic hills at Reynolds numbers based on the hill height of $Re_H=10{,}595$\nand $Re_{H}=19{,}000$. \n\n"}
{"id": "1610.09005", "contents": "Title: Fast and Consistent Algorithm for the Latent Block Model Abstract: The latent block model is used to simultaneously rank the rows and columns of\na matrix to reveal a block structure. The algorithms used for estimation are\noften time consuming. However, recent work shows that the log-likelihood ratios\nare equivalent under the complete and observed (with unknown labels) models and\nthe groups posterior distribution to converge as the size of the data increases\nto a Dirac mass located at the actual groups configuration. Based on these\nobservations, the algorithm $Largest$ $Gaps$ is proposed in this paper to\nperform clustering using only the marginals of the matrix, when the number of\nblocks is very small with respect to the size of the whole matrix in the case\nof binary data. In addition, a model selection method is incorporated with a\nproof of its consistency. Thus, this paper shows that studying simplistic\nconfigurations (few blocks compared to the size of the matrix or very\ncontrasting blocks) with complex algorithms is useless since the marginals\nalready give very good parameter and classification estimates. \n\n"}
{"id": "1610.09033", "contents": "Title: Operator Variational Inference Abstract: Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images. \n\n"}
{"id": "1610.09724", "contents": "Title: Likelihood Inference for Large Scale Stochastic Blockmodels with\n  Covariates based on a Divide-and-Conquer Parallelizable Algorithm with\n  Communication Abstract: We consider a stochastic blockmodel equipped with node covariate information,\nthat is helpful in analyzing social network data. The key objective is to\nobtain maximum likelihood estimates of the model parameters. For this task, we\ndevise a fast, scalable Monte Carlo EM type algorithm based on case-control\napproximation of the log-likelihood coupled with a subsampling approach. A key\nfeature of the proposed algorithm is its parallelizability, by processing\nportions of the data on several cores, while leveraging communication of key\nstatistics across the cores during each iteration of the algorithm. The\nperformance of the algorithm is evaluated on synthetic data sets and compared\nwith competing methods for blockmodel parameter estimation. We also illustrate\nthe model on data from a Facebook derived social network enhanced with node\ncovariate information. \n\n"}
{"id": "1610.09788", "contents": "Title: Pseudo-marginal Metropolis--Hastings using averages of unbiased\n  estimators Abstract: We consider a pseudo-marginal Metropolis--Hastings kernel $P_m$ that is\nconstructed using an average of $m$ exchangeable random variables, as well as\nan analogous kernel $P_s$ that averages $s<m$ of these same random variables.\nUsing an embedding technique to facilitate comparisons, we show that the\nasymptotic variances of ergodic averages associated with $P_m$ are lower\nbounded in terms of those associated with $P_s$. We show that the bound\nprovided is tight and disprove a conjecture that when the random variables to\nbe averaged are independent, the asymptotic variance under $P_m$ is never less\nthan $s/m$ times the variance under $P_s$. The conjecture does, however, hold\nwhen considering continuous-time Markov chains. These results imply that if the\ncomputational cost of the algorithm is proportional to $m$, it is often better\nto set $m=1$. We provide intuition as to why these findings differ so markedly\nfrom recent results for pseudo-marginal kernels employing particle filter\napproximations. Our results are exemplified through two simulation studies; in\nthe first the computational cost is effectively proportional to $m$ and in the\nsecond there is a considerable start-up cost at each iteration. \n\n"}
{"id": "1611.01213", "contents": "Title: Bayesian and Variational Bayesian approaches for flows in heterogenous\n  random media Abstract: In this paper, we study porous media flows in heterogeneous stochastic media.\nWe propose an efficient forward simulation technique that is tailored for\nvariational Bayesian inversion. As a starting point, the proposed forward\nsimulation technique decomposes the solution into the sum of separable\nfunctions (with respect to randomness and the space), where each term is\ncalculated based on a variational approach. This is similar to Proper\nGeneralized Decomposition (PGD). Next, we apply a multiscale technique to solve\nfor each term and, further, decompose the random function into 1D fields. As a\nresult, our proposed method provides an approximation hierarchy for the\nsolution as we increase the number of terms in the expansion and, also,\nincrease the spatial resolution of each term. We use the hierarchical solution\ndistributions in a variational Bayesian approximation to perform uncertainty\nquantification in the inverse problem. We conduct a detailed numerical study to\nexplore the performance of the proposed uncertainty quantification technique\nand show the theoretical posterior concentration. \n\n"}
{"id": "1611.01310", "contents": "Title: Achieving Shrinkage in a Time-Varying Parameter Model Framework Abstract: Shrinkage for time-varying parameter (TVP) models is investigated within a\nBayesian framework, with the aim to automatically reduce time-varying\nparameters to static ones, if the model is overfitting. This is achieved\nthrough placing the double gamma shrinkage prior on the process variances. An\nefficient Markov chain Monte Carlo scheme is developed, exploiting boosting\nbased on the ancillarity-sufficiency interweaving strategy. The method is\napplicable both to TVP models for univariate as well as multivariate time\nseries. Applications include a TVP generalized Phillips curve for EU area\ninflation modelling and a multivariate TVP Cholesky stochastic volatility model\nfor joint modelling of the returns from the DAX-30 index. \n\n"}
{"id": "1611.02213", "contents": "Title: A Low-rank Control Variate for Multilevel Monte Carlo Simulation of\n  High-dimensional Uncertain Systems Abstract: Multilevel Monte Carlo (MLMC) is a recently proposed variation of Monte Carlo\n(MC) simulation that achieves variance reduction by simulating the governing\nequations on a series of spatial (or temporal) grids with increasing\nresolution. Instead of directly employing the fine grid solutions, MLMC\nestimates the expectation of the quantity of interest from the coarsest grid\nsolutions as well as differences between each two consecutive grid solutions.\nWhen the differences corresponding to finer grids become smaller, hence less\nvariable, fewer MC realizations of finer grid solutions are needed to compute\nthe difference expectations, thus leading to a reduction in the overall work.\nThis paper presents an extension of MLMC, referred to as multilevel control\nvariates (MLCV), where a low-rank approximation to the solution on each grid,\nobtained primarily based on coarser grid solutions, is used as a control\nvariate for estimating the expectations involved in MLMC. Cost estimates as\nwell as numerical examples are presented to demonstrate the advantage of this\nnew MLCV approach over the standard MLMC when the solution of interest admits a\nlow-rank approximation and the cost of simulating finer grids grows fast. \n\n"}
{"id": "1611.02456", "contents": "Title: Cyclic Coordinate Update Algorithms for Fixed-Point Problems: Analysis\n  and Applications Abstract: Many problems reduce to the fixed-point problem of solving $x=T(x)$. To this\nproblem, we apply the coordinate-update algorithms, which update only one or a\nfew components of $x$ at each step. When each update is cheap, these algorithms\nare faster than the full fixed-point iteration (which updates all the\ncomponents).\n  In this paper, we focus on the coordinate-update algorithms based on the\ncyclic selection rules, where the ordering of coordinates in each cycle is\narbitrary. These algorithms are fast, but their convergence is unknown in the\nfixed-point setting.\n  When $T$ is a nonexpansive operator and has a fixed point, we show that the\nsequence of coordinate-update iterates converges to a fixed point under proper\nstep sizes. This result applies to the primal-dual coordinate-update\nalgorithms, which have applications to optimization problems with nonseparable\nnonsmooth objectives, as well as global linear constraints.\n  Numerically, we apply coordinate-update algorithms with the cyclic, shuffled\ncyclic, and random selection rules to $\\ell_1$ robust least squares, a CT image\nreconstruction problem, as well as nonnegative matrix factorization. They\nconverge much faster than the standard fixed-point iteration. Among the three\nrules, cyclic and shuffled cyclic rules are overall faster than the random\nrule. \n\n"}
{"id": "1611.03112", "contents": "Title: Multiple imputation of multilevel missing data: An introduction to the R\n  package pan Abstract: The treatment of missing data can be difficult in multilevel research because\nstate-of-the-art procedures such as multiple imputation (MI) may require\nadvanced statistical knowledge or a high degree of familiarity with certain\nstatistical software. In the missing data literature, pan has been recommended\nfor MI of multilevel data. In this article, we provide an introduction to MI of\nmultilevel missing data using the R package pan, and we discuss its\npossibilities and limitations in accommodating typical questions in multilevel\nresearch. In order to make pan more accessible to applied researchers, we make\nuse of the mitml package, which provides a user-friendly interface to the pan\npackage and several tools for managing and analyzing multiply imputed data\nsets. We illustrate the use of pan and mitml with two empirical examples that\nrepresent common applications of multilevel models, and we discuss how these\nprocedures may be used in conjunction with other software. \n\n"}
{"id": "1611.06686", "contents": "Title: Scalable Approximations for Generalized Linear Problems Abstract: In stochastic optimization, the population risk is generally approximated by\nthe empirical risk. However, in the large-scale setting, minimization of the\nempirical risk may be computationally restrictive. In this paper, we design an\nefficient algorithm to approximate the population risk minimizer in generalized\nlinear problems such as binary classification with surrogate losses and\ngeneralized linear regression models. We focus on large-scale problems, where\nthe iterative minimization of the empirical risk is computationally\nintractable, i.e., the number of observations $n$ is much larger than the\ndimension of the parameter $p$, i.e. $n \\gg p \\gg 1$. We show that under random\nsub-Gaussian design, the true minimizer of the population risk is approximately\nproportional to the corresponding ordinary least squares (OLS) estimator. Using\nthis relation, we design an algorithm that achieves the same accuracy as the\nempirical risk minimizer through iterations that attain up to a cubic\nconvergence rate, and that are cheaper than any batch optimization algorithm by\nat least a factor of $\\mathcal{O}(p)$. We provide theoretical guarantees for\nour algorithm, and analyze the convergence behavior in terms of data\ndimensions. Finally, we demonstrate the performance of our algorithm on\nwell-known classification and regression problems, through extensive numerical\nstudies on large-scale datasets, and show that it achieves the highest\nperformance compared to several other widely used and specialized optimization\nalgorithms. \n\n"}
{"id": "1611.08996", "contents": "Title: JIGSAW-GEO (1.0): locally orthogonal staggered unstructured grid\n  generation for general circulation modelling on the sphere Abstract: An algorithm for the generation of non-uniform, locally-orthogonal staggered\nunstructured spheroidal grids is described. This technique is designed to\ngenerate very high-quality staggered Voronoi/Delaunay meshes appropriate for\ngeneral circulation modelling on the sphere, including applications to\natmospheric simulation, ocean-modelling and numerical weather prediction. Using\na recently developed Frontal-Delaunay refinement technique, a method for the\nconstruction of high-quality unstructured spheroidal Delaunay triangulations is\nintroduced. A locally-orthogonal polygonal grid, derived from the associated\nVoronoi diagram, is computed as the staggered dual. It is shown that use of the\nFrontal-Delaunay refinement technique allows for the generation of very\nhigh-quality unstructured triangulations, satisfying a-priori bounds on element\nsize and shape. Grid-quality is further improved through the application of\nhill-climbing type optimisation techniques. Overall, the algorithm is shown to\nproduce grids with very high element quality and smooth grading\ncharacteristics, while imposing relatively low computational expense. A\nselection of uniform and non-uniform spheroidal grids appropriate for\nhigh-resolution, multi-scale general circulation modelling are presented. These\ngrids are shown to satisfy the geometric constraints associated with\ncontemporary unstructured C-grid type finite-volume models, including the Model\nfor Prediction Across Scales (MPAS-O). The use of user-defined mesh-spacing\nfunctions to generate smoothly graded, non-uniform grids for multi-resolution\ntype studies is discussed in detail. \n\n"}
{"id": "1611.10242", "contents": "Title: Likelihood-free inference by ratio estimation Abstract: We consider the problem of parametric statistical inference when likelihood\ncomputations are prohibitively expensive but sampling from the model is\npossible. Several so-called likelihood-free methods have been developed to\nperform inference in the absence of a likelihood function. The popular\nsynthetic likelihood approach infers the parameters by modelling summary\nstatistics of the data by a Gaussian probability distribution. In another\npopular approach called approximate Bayesian computation, the inference is\nperformed by identifying parameter values for which the summary statistics of\nthe simulated data are close to those of the observed data. Synthetic\nlikelihood is easier to use as no measure of `closeness' is required but the\nGaussianity assumption is often limiting. Moreover, both approaches require\njudiciously chosen summary statistics. We here present an alternative inference\napproach that is as easy to use as synthetic likelihood but not as restricted\nin its assumptions, and that, in a natural way, enables automatic selection of\nrelevant summary statistic from a large set of candidates. The basic idea is to\nframe the problem of estimating the posterior as a problem of estimating the\nratio between the data generating distribution and the marginal distribution.\nThis problem can be solved by logistic regression, and including regularising\npenalty terms enables automatic selection of the summary statistics relevant to\nthe inference task. We illustrate the general theory on canonical examples and\nemploy it to perform inference for challenging stochastic nonlinear dynamical\nsystems and high-dimensional summary statistics. \n\n"}
{"id": "1612.00762", "contents": "Title: Structured Filtering Abstract: A major challenge facing existing sequential Monte-Carlo methods for\nparameter estimation in physics stems from the inability of existing approaches\nto robustly deal with experiments that have different mechanisms that yield the\nresults with equivalent probability. We address this problem here by proposing\na form of particle filtering that clusters the particles that comprise the\nsequential Monte-Carlo approximation to the posterior before applying a\nresampler. Through a new graphical approach to thinking about such models, we\nare able to devise an artificial-intelligence based strategy that automatically\nlearns the shape and number of the clusters in the support of the posterior. We\ndemonstrate the power of our approach by applying it to randomized gap\nestimation and a form of low circuit-depth phase estimation where existing\nmethods from the physics literature either exhibit much worse performance or\neven fail completely. \n\n"}
{"id": "1612.01719", "contents": "Title: Tropical Vorticity Forcing and Superrotation in the Spherical Shallow\n  Water Equations Abstract: The response of the nonlinear shallow water equations (SWE) on a sphere to\ntropical vorticity forcing is examined with an emphasis on momentum fluxes and\nthe emergence of a superrotating (SR) state. Fixing the radiative damping and\nmomentum drag timescales to be of the order of a few days, a state of SR is\nshown to emerge under steady large-scale and random small-scale vorticity\nforcing. In the first example, the stationary response to a pair of equal and\noppositely signed vortices placed on the equator is considered. Here, the\nequatorial flux budget is dominated by the eddy fluxes and these drive the\nsystem into a state of SR. Eventually, the flux associated with the momentum\ndrag increases to balance the eddy fluxes, resulting in a steady state with a\nSR jet at the equator. The initial value problem with these twin vortices also\nexhibits SR driven by eddy fluxes. Curiously, this transient solution\nspontaneously propagates westward and continually circumnavigates the globe. It\nis worth emphasizing that this SR state does not rely on any particular form of\ndissipation at large scales, and is in fact observed even in the absence of any\nlarge-scale damping mechanism. In the second example, a random small-scale\nvorticity forcing is applied across the tropics. The statistically steady state\nobtained is fairly turbulent in nature, but here too, the eddy fluxes dominate,\nand the system exhibits SR. It is important to note that in both these cases,\nthe direct forcing of the zonal mean zonal flow is zero by construction, and\nthe eddy fluxes at the equator are responsible for its eastward acceleration.\nFurther, in both these examples, the rotational part of the flow dominates the\nmomentum fluxes as well as the stationary zonal mean zonal flow itself.\nArguments based on the nature of potential vorticity and enstrophy are put\nforth to shed some light on these results. \n\n"}
{"id": "1612.01872", "contents": "Title: Simulation from quasi-stationary distributions on reducible state spaces Abstract: Quasi-stationary distributions (QSDs)arise from stochastic processes that\nexhibit transient equilibrium behaviour on the way to absorption QSDs are often\nmathematically intractable and even drawing samples from them is not\nstraightforward. In this paper the framework of Sequential Monte Carlo samplers\nis utilized to simulate QSDs and several novel resampling techniques are\nproposed to accommodate models with reducible state spaces, with particular\nfocus on preserving particle diversity on discrete spaces. Finally an approach\nis considered to estimate eigenvalues associated with QSDs, such as the decay\nparameter. \n\n"}
{"id": "1612.02679", "contents": "Title: Global attractor of the three dimensional primitive equations of\n  large-scale ocean and atmosphere dynamics in an unbounded domain Abstract: This paper is concerned with the long-time behavior of solutions for the\nthree dimensional primitive equations of large-scale ocean and atmosphere\ndynamics in an unbounded domain. Since the Sobolev embedding is no longer\ncompact in an unbounded domain, we cannot obtain the asymptotical compactness\nof the semigroup generated by problem (2.4)-(2.6) by the Sobolev compactness\nembedding theorem even if we obtain the existence of an absorbing set in more\nregular phase space. To overcome this difficulty, we first prove the\nasymptotical compactness in a weak phase space of the semigroup generated by\nproblem (2.4)-(2.6) by combining the tail-estimates and the energy methods.\nThanks to the existence of an absorbing set in more regular phase space, we\nestablish the existence of a global attractor of problem (2.4)-(2.6) by virtue\nof Sobolev interpolation inequality. \n\n"}
{"id": "1612.03233", "contents": "Title: New Tests of Uniformity on the Compact Classical Groups as Diagnostics\n  for Weak-star Mixing of Markov Chains Abstract: This paper introduces two new families of non-parametric tests of\ngoodness-of-fit on the compact classical groups. One of them is a family of\ntests for the eigenvalue distribution induced by the uniform distribution,\nwhich is consistent against all fixed alternatives. The other is a family of\ntests for the uniform distribution on the entire group, which is again\nconsistent against all fixed alternatives. We find the asymptotic distribution\nunder the null and general alternatives. The tests are proved to be\nasymptotically admissible. Local power is derived and the global properties of\nthe power function against local alternatives are explored.\n  The new tests are validated on two random walks for which the mixing-time is\nstudied in the literature. The new tests, and several others, are applied to\nthe Markov chain sampler proposed by \\cite{jones2011randomized}, providing\nstrong evidence supporting the claim that the sampler mixes quickly. \n\n"}
{"id": "1612.03243", "contents": "Title: Statistical State Dynamics of Vertically Sheared Horizontal Flows in\n  Two-Dimensional Stratified Turbulence Abstract: Simulations of strongly stratified turbulence often exhibit coherent\nlarge-scale structures called vertically sheared horizontal flows (VSHFs).\nVSHFs emerge in both two-dimensional (2D) and three-dimensional (3D) stratified\nturbulence with similar vertical structure. The mechanism responsible for VSHF\nformation is not fully understood. In this work, the formation and\nequilibration of VSHFs in a 2D Boussinesq model of stratified turbulence is\nstudied using statistical state dynamics (SSD). In SSD, equations of motion are\nexpressed directly in the statistical variables of the turbulent state.\nRestriction to 2D turbulence makes available an analytically and\ncomputationally attractive implementation of SSD referred to as S3T, in which\nthe SSD is expressed by coupling the equation for the horizontal mean structure\nwith the equation for the ensemble mean perturbation covariance. This second\norder SSD produces accurate statistics, through second order, when compared\nwith fully nonlinear simulations. In particular, S3T captures the spontaneous\nemergence of the VSHF and associated density layers seen in simulations of\nturbulence maintained by homogeneous large-scale stochastic excitation. An\nadvantage of the S3T system is that the VSHF formation mechanism, which is\nwave-mean flow interaction between the emergent VSHF and the stochastically\nexcited large-scale gravity waves, is analytically understood in the S3T\nsystem. Comparison with fully nonlinear simulations verifies that S3T solutions\naccurately predict the scale selection, dependence on stochastic excitation\nstrength, and nonlinear equilibrium structure of the VSHF. These results\nfacilitate relating VSHF theory and geophysical examples of turbulent jets such\nas the ocean's equatorial deep jets. \n\n"}
{"id": "1612.03301", "contents": "Title: Gradient Coding Abstract: We propose a novel coding theoretic framework for mitigating stragglers in\ndistributed learning. We show how carefully replicating data blocks and coding\nacross gradients can provide tolerance to failures and stragglers for\nSynchronous Gradient Descent. We implement our schemes in python (using MPI) to\nrun on Amazon EC2, and show how we compare against baseline approaches in\nrunning time and generalization error. \n\n"}
{"id": "1612.03319", "contents": "Title: Anytime Monte Carlo Abstract: Monte Carlo algorithms simulate some prescribed number of samples, taking\nsome random real time to complete the computations necessary. This work\nconsiders the converse: to impose a real-time budget on the computation, which\nresults in the number of samples simulated being random. To complicate matters,\nthe real time taken for each simulation may depend on the sample produced, so\nthat the samples themselves are not independent of their number, and a length\nbias with respect to compute time is apparent. This is especially problematic\nwhen a Markov chain Monte Carlo (MCMC) algorithm is used and the final state of\nthe Markov chain -- rather than an average over all states -- is required,\nwhich is the case in parallel tempering implementations of MCMC. The length\nbias does not diminish with the compute budget in this case. It also occurs in\nsequential Monte Carlo (SMC) algorithms, which is the focus of this paper. We\npropose an anytime framework to address the concern, using a continuous-time\nMarkov jump process to study the progress of the computation in real time. We\nfirst show that for any MCMC algorithm, the length bias of the final state's\ndistribution due to the imposed real-time computing budget can be eliminated by\nusing a multiple chain construction. The utility of this construction is then\ndemonstrated on a large-scale SMC^2 implementation, using four billion\nparticles distributed across a cluster of 128 graphics processing units on the\nAmazon EC2 service. The anytime framework imposes a real-time budget on the\nMCMC move steps within the SMC$^{2}$ algorithm, ensuring that all processors\nare simultaneously ready for the resampling step, demonstrably reducing\nidleness to due waiting times and providing substantial control over the total\ncompute budget. \n\n"}
{"id": "1612.03374", "contents": "Title: Beta-plane turbulence above monoscale topography Abstract: Using a one-layer QG model, we study the effect of random monoscale\ntopography on forced beta-plane turbulence. The forcing is a uniform steady\nwind stress that produces both a uniform large-scale zonal flow $U(t)$ and\nsmaller-scale macroturbulence (both standing and transient eddies). The flow\n$U(t)$ is retarded by Ekman drag and by the domain-averaged topographic form\nstress produced by the eddies. The topographic form stress typically balances\nmost of the applied wind stress, while the Ekman drag provides all of the\nenergy dissipation required to balance the wind work. A collection of\nstatistically equilibrated solutions delineates the main flow regimes and the\ndependence of the time-mean $U$ on the problem parameters and the statistical\nproperties of the topography.\n  If $\\beta$ is smaller than the topographic PV gradient then the flow consists\nof stagnant pools attached to pockets of closed geostrophic contours. The\nstagnant dead zones are bordered by jets and the flow through the domain is\nconcentrated into a narrow channel of open geostrophic contours.\n  If $\\beta$ is comparable to, or larger than, the topographic PV gradient then\nall geostrophic contours are open and the flow is uniformly distributed\nthroughout the domain. In this case there is an \"eddy saturation\" regime in\nwhich $U$ is insensitive to changes in the wind stress. We show that eddy\nsaturation requires strong transient eddies that act as PV diffusion. This PV\ndiffusion does not alter the energy of the standing eddies, but it does\nincrease the topographic form stress by enhancing the correlation between\ntopographic slope and the standing-eddy pressure field. Last, using bounds\nbased on the energy and enstrophy we show that as the wind stress increases the\nflow transitions from a regime in which form stress balances the wind stress to\na regime in which the form stress is very small and large transport ensues. \n\n"}
{"id": "1612.04271", "contents": "Title: BayesBD: An R Package for Bayesian Inference on Image Boundaries Abstract: We present the BayesBD package providing Bayesian inference for boundaries of\nnoisy images. The BayesBD package implements flexible Gaussian process priors\nindexed by the circle to recover the boundary in a binary or Gaussian noised\nimage, with the benefits of guaranteed geometric restrictions on the estimated\nboundary, (nearly) minimax optimal and smoothness adaptive convergence rates,\nand convenient joint inferences under certain assumptions. The core sampling\ntasks for our model have linear complexity, and our implementation in c++ using\npackages Rcpp and RcppArmadillo is computationally efficient. Users can access\nthe full functionality of the package in both Rgui and the corresponding shiny\napplication. Additionally, the package includes numerous utility functions to\naid users in data preparation and analysis of results. We compare BayesBD with\nselected existing packages using both simulations and real data applications,\nand demonstrate the excellent performance and flexibility of BayesBD even when\nthe observation contains complicated structural information that may violate\nits assumptions. \n\n"}
{"id": "1612.06998", "contents": "Title: Postprocessing Galerkin method applied to a data assimilation algorithm:\n  a uniform in time error estimate Abstract: We apply the Postprocessing Galerkin method to a recently introduced\ncontinuous data assimilation (downscaling) algorithm for obtaining a numerical\napproximation of the solution of the two-dimensional Navier-Stokes equations\ncorresponding to given measurements from a coarse spatial mesh. Under suitable\nconditions on the relaxation (nudging) parameter, the resolution of the coarse\nspatial mesh and the resolution of the numerical scheme, we obtain uniform in\ntime estimates for the error between the numerical approximation given by the\nPostprocessing Galerkin method and the reference solution corresponding to the\nmeasurements. Our results are valid for a large class of interpolant operators,\nincluding low Fourier modes and local averages over finite volume elements.\nNotably, we use here the 2D Navier-Stokes equations as a paradigm, but our\nresults apply equally to other evolution equations, such as the Boussinesq\nsystem of Benard convection and other oceanic and atmospheric circulation\nmodels. \n\n"}
{"id": "1612.07010", "contents": "Title: Permutation in genetic association studies with covariates: controlling\n  the familywise error rate with score tests in generalized linear models Abstract: In genome-wide association (GWA) studies the goal is to detect associations\nbetween genetic markers and a given phenotype. The number of genetic markers\ncan be large and effective methods for control of the overall error rate is a\ncentral topic when analyzing GWA data. The Bonferroni method is known to be\nconservative when the tests are dependent. Permutation methods give exact\ncontrol of the overall error rate when the assumption of exchangeability is\nsatisfied, but are computationally intensive for large datasets. For regression\nmodels the exchangeability assumption is in general not satisfied and there is\nno standard solution on how to do permutation testing, except some approximate\nmethods. In this paper we will discuss permutation methods for control of the\nfamilywise error rate in genetic association studies and present an approximate\nsolution. These methods will be compared using simulated data. \n\n"}
{"id": "1612.07223", "contents": "Title: A proof of concept for scale-adaptive parameterizations: the case of the\n  Lorenz '96 model Abstract: Constructing efficient and accurate parameterizations of sub-grid scale\nprocesses is a central area of interest in the numerical modelling of\ngeophysical fluids. Using a modified version of the two-level Lorenz '96 model,\nwe present here a proof of concept of a scale-adaptive parameterization\nconstructed using statistical mechanical arguments. By a suitable use of the\nRuelle response theory and of the Mori-Zwanzig projection method, it is\npossible to derive explicitly a parameterization for the fast variables that\ntranslates into deterministic, stochastic and non-markovian contributions to\nthe equations of motion of the variables of interest. We show that our approach\nis computationally parsimonious, has great flexibility, as it is explicitly\nscale-adaptive, and we prove that it is competitive compared to empirical\nad-hoc approaches. While the parameterization proposed here is universal and\ncan be easily analytically adapted to changes in the parameters' values by a\nsimple rescaling procedure, the parameterization constructed with the ad-hoc\napproach needs to be recomputed each time the parameters of the systems are\nchanged. The price of the higher flexibility of the method proposed here is\nhaving a lower accuracy in each individual case. \n\n"}
{"id": "1612.07717", "contents": "Title: Multilevel Monte Carlo and Improved Timestepping Methods in Atmospheric\n  Dispersion Modelling Abstract: A common way to simulate the transport and spread of pollutants in the\natmosphere is via stochastic Lagrangian dispersion models. Mathematically,\nthese models describe turbulent transport processes with stochastic\ndifferential equations (SDEs). The computational bottleneck is the Monte Carlo\nalgorithm, which simulates the motion of a large number of model particles in a\nturbulent velocity field; for each particle, a trajectory is calculated with a\nnumerical timestepping method. Choosing an efficient numerical method is\nparticularly important in operational emergency-response applications, such as\ntracking radioactive clouds from nuclear accidents or predicting the impact of\nvolcanic ash clouds on international aviation, where accurate and timely\npredictions are essential. In this paper, we investigate the application of the\nMultilevel Monte Carlo (MLMC) method to simulate the propagation of particles\nin a representative one-dimensional dispersion scenario in the atmospheric\nboundary layer. MLMC can be shown to result in asymptotically superior\ncomputational complexity and reduced computational cost when compared to the\nStandard Monte Carlo (StMC) method, which is currently used in atmospheric\ndispersion modelling. To reduce the absolute cost of the method also in the\nnon-asymptotic regime, it is equally important to choose the best possible\nnumerical timestepping method on each level. To investigate this, we also\ncompare the standard symplectic Euler method, which is used in many operational\nmodels, with two improved timestepping algorithms based on SDE splitting\nmethods. \n\n"}
{"id": "1612.08141", "contents": "Title: PLMIX: An R package for modeling and clustering partially ranked data Abstract: Ranking data represent a peculiar form of multivariate ordinal data taking\nvalues in the set of permutations. Despite the numerous methodological\ncontributions to increase the flexibility of ranked data modeling, the\napplication of more sophisticated models is limited by the related\ncomputational issues. The PLMIX package offers a comprehensive framework aimed\nat endowing the R statistical environment with some recent methodological\nadvances in modeling and clustering partially ranked data. The usefulness of\nthe novel PLMIX package can be motivated from several perspectives: (i) it\ncontributes to fill the gap concerning Bayesian estimation of ranking models in\nR, by focusing on the Plackett-Luce model and its extension within the finite\nmixture approach as the generative sampling distribution; (ii) it addresses\ncomputational complexity by combining the flexibility of R routines and the\nspeed of compiled C++ code, with possible parallel execution; (iii) it covers\nthe fundamental phases of ranking data analysis allowing for a more careful and\ncritical application of ranking models in real experiments; (iv) it provides\neffective tools for clustering heterogeneous partially ranked data. The\nfunctionality of the novel package is illustrated with several applications to\nsimulated and real datasets. \n\n"}
{"id": "1612.09123", "contents": "Title: Population and trends in the global mean temperature Abstract: The Fisher Ideal index, developed to measure price inflation, is applied to\ndefine a population-weighted temperature trend. This method has the advantages\nthat the trend is representative for the population distribution throughout the\nsample but without conflating the trend in the population distribution and the\ntrend in the temperature. I show that the trend in the global area-weighted\naverage surface air temperature is different in key details from the\npopulation-weighted trend. I extend the index to include urbanization and the\nurban heat island effect. This substantially changes the trend again. I further\nextend the index to include international migration, but this has a minor\nimpact on the trend. \n\n"}
{"id": "1701.01672", "contents": "Title: Detecting changes in slope with an $L_0$ penalty Abstract: Whilst there are many approaches to detecting changes in mean for a\nunivariate time-series, the problem of detecting multiple changes in slope has\ncomparatively been ignored. Part of the reason for this is that detecting\nchanges in slope is much more challenging. For example, simple binary\nsegmentation procedures do not work for this problem, whilst efficient dynamic\nprogramming methods that work well for the change in mean problem cannot be\ndirectly used for detecting changes in slope. We present a novel dynamic\nprogramming approach, CPOP, for finding the \"best\" continuous piecewise-linear\nfit to data. We define best based on a criterion that measures fit to data\nusing the residual sum of squares, but penalises complexity based on an $L_0$\npenalty on changes in slope. We show that using such a criterion is more\nreliable at estimating changepoint locations than approaches that penalise\ncomplexity using an $L_1$ penalty. Empirically CPOP has good computational\nproperties, and can analyse a time-series with over 10,000 observations and\nover 100 changes in a few minutes. Our method is used to analyse data on the\nmotion of bacteria, and provides fits to the data that both have substantially\nsmaller residual sum of squares and are more parsimonious than two competing\napproaches. \n\n"}
{"id": "1701.02002", "contents": "Title: Smoothing with Couplings of Conditional Particle Filters Abstract: In state space models, smoothing refers to the task of estimating a latent\nstochastic process given noisy measurements related to the process. We propose\nan unbiased estimator of smoothing expectations. The lack-of-bias property has\nmethodological benefits: independent estimators can be generated in parallel,\nand confidence intervals can be constructed from the central limit theorem to\nquantify the approximation error. To design unbiased estimators, we combine a\ngeneric debiasing technique for Markov chains with a Markov chain Monte Carlo\nalgorithm for smoothing. The resulting procedure is widely applicable and we\nshow in numerical experiments that the removal of the bias comes at a\nmanageable increase in variance. We establish the validity of the proposed\nestimators under mild assumptions. Numerical experiments are provided on toy\nmodels, including a setting of highly-informative observations, and a realistic\nLotka-Volterra model with an intractable transition density. \n\n"}
{"id": "1701.02522", "contents": "Title: Magnus expansions and pseudospectra of Master Equations Abstract: New directions in research on master equations are showcased by example.\nMagnus expansions, time-varying rates, and pseudospectra are highlighted. Exact\neigenvalues are found and contrasted with the large errors produced by standard\nnumerical methods in some cases. Isomerisation provides a running example and\nan illustrative application to chemical kinetics. We also give a brief example\nof the totally asymmetric exclusion process. \n\n"}
{"id": "1701.03757", "contents": "Title: Deep Probabilistic Programming Abstract: We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow. \n\n"}
{"id": "1701.04244", "contents": "Title: Piecewise Deterministic Markov Processes for Scalable Monte Carlo on\n  Restricted Domains Abstract: Piecewise Deterministic Monte Carlo algorithms enable simulation from a\nposterior distribution, whilst only needing to access a sub-sample of data at\neach iteration. We show how they can be implemented in settings where the\nparameters live on a restricted domain. \n\n"}
{"id": "1701.04742", "contents": "Title: Stochastic parameterization of subgrid-scale processes: A review of\n  recent physically-based approaches Abstract: We review some recent methods of subgrid-scale parameterization used in the\ncontext of climate modeling. These methods are developed to take into account\n(subgrid) processes playing an important role in the correct representation of\nthe atmospheric and climate variability. We illustrate these methods on a\nsimple stochastic triad system relevant for the atmospheric and climate\ndynamics, and we show in particular that the stability properties of the\nunderlying dynamics of the subgrid processes has a considerable impact on their\nperformances. \n\n"}
{"id": "1701.05128", "contents": "Title: A Constructive Approach to High-dimensional Regression Abstract: We develop a constructive approach to estimating sparse, high-dimensional\nlinear regression models. The approach is a computational algorithm motivated\nfrom the KKT conditions for the $\\ell_0$-penalized least squares solutions. It\ngenerates a sequence of solutions iteratively, based on support detection using\nprimal and dual information and root finding. We refer to the algorithm as SDAR\nfor brevity. Under a sparse Rieze condition on the design matrix and certain\nother conditions, we show that with high probability, the $\\ell_2$ estimation\nerror of the solution sequence decays exponentially to the minimax error bound\nin $O(\\sqrt{J}\\log(R))$ steps; and under a mutual coherence condition and\ncertain other conditions, the $\\ell_{\\infty}$ estimation error decays to the\noptimal error bound in $O(\\log(R))$ steps, where $J$ is the number of important\npredictors, $R$ is the relative magnitude of the nonzero target coefficients.\nComputational complexity analysis shows that the cost of SDAR is $O(np)$ per\niteration. Moreover the oracle least squares estimator can be exactly recovered\nwith high probability at the same cost if we know the sparsity level. We also\nconsider an adaptive version of SDAR to make it more practical in applications.\nNumerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR\nis competitive with or outperforms them in accuracy and efficiency. \n\n"}
{"id": "1701.05146", "contents": "Title: On parameter estimation with the Wasserstein distance Abstract: Statistical inference can be performed by minimizing, over the parameter\nspace, the Wasserstein distance between model distributions and the empirical\ndistribution of the data. We study asymptotic properties of such minimum\nWasserstein distance estimators, complementing results derived by Bassetti,\nBodini and Regazzini in 2006. In particular, our results cover the misspecified\nsetting, in which the data-generating process is not assumed to be part of the\nfamily of distributions described by the model. Our results are motivated by\nrecent applications of minimum Wasserstein estimators to complex generative\nmodels. We discuss some difficulties arising in the approximation of these\nestimators and illustrate their behavior in several numerical experiments. Two\nof our examples are taken from the literature on approximate Bayesian\ncomputation and have likelihood functions that are not analytically tractable.\nTwo other examples involve misspecified models. \n\n"}
{"id": "1701.05892", "contents": "Title: Bayesian Static Parameter Estimation for Partially Observed Diffusions\n  via Multilevel Monte Carlo Abstract: In this article we consider static Bayesian parameter estimation for\npartially observed diffusions that are discretely observed. We work under the\nassumption that one must resort to discretizing the underlying diffusion\nprocess, for instance using the Euler-Maruyama method. Given this assumption,\nwe show how one can use Markov chain Monte Carlo (MCMC) and particularly\nparticle MCMC [Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle\nMarkov chain Monte Carlo methods (with discussion). J. R. Statist. Soc. Ser. B,\n72, 269--342] to implement a new approximation of the multilevel (ML) Monte\nCarlo (MC) collapsing sum identity. Our approach comprises constructing an\napproximate coupling of the posterior density of the joint distribution over\nparameter and hidden variables at two different discretization levels and then\ncorrecting by an importance sampling method. The variance of the weights are\nindependent of the length of the observed data set. The utility of such a\nmethod is that, for a prescribed level of mean square error, the cost of this\nMLMC method is provably less than i.i.d. sampling from the posterior associated\nto the most precise discretization. However the method here comprises using\nonly known and efficient simulation methodologies. The theoretical results are\nillustrated by inference of the parameters of two prototypical processes given\nnoisy partial observations of the process: the first is an Ornstein Uhlenbeck\nprocess and the second is a more general Langevin equation. \n\n"}
{"id": "1701.08431", "contents": "Title: Source localization in an ocean waveguide using supervised machine\n  learning Abstract: Source localization in ocean acoustics is posed as a machine learning problem\nin which data-driven methods learn source ranges directly from observed\nacoustic data. The pressure received by a vertical linear array is preprocessed\nby constructing a normalized sample covariance matrix (SCM) and used as the\ninput. Three machine learning methods (feed-forward neural networks (FNN),\nsupport vector machines (SVM) and random forests (RF)) are investigated in this\npaper, with focus on the FNN. The range estimation problem is solved both as a\nclassification problem and as a regression problem by these three machine\nlearning algorithms. The results of range estimation for the Noise09 experiment\nare compared for FNN, SVM, RF and conventional matched-field processing and\ndemonstrate the potential of machine learning for underwater source\nlocalization.. \n\n"}
{"id": "1702.00317", "contents": "Title: On SGD's Failure in Practice: Characterizing and Overcoming Stalling Abstract: Stochastic Gradient Descent (SGD) is widely used in machine learning problems\nto efficiently perform empirical risk minimization, yet, in practice, SGD is\nknown to stall before reaching the actual minimizer of the empirical risk. SGD\nstalling has often been attributed to its sensitivity to the conditioning of\nthe problem; however, as we demonstrate, SGD will stall even when applied to a\nsimple linear regression problem with unity condition number for standard\nlearning rates. Thus, in this work, we numerically demonstrate and\nmathematically argue that stalling is a crippling and generic limitation of SGD\nand its variants in practice. Once we have established the problem of stalling,\nwe generalize an existing framework for hedging against its effects, which (1)\ndeters SGD and its variants from stalling, (2) still provides convergence\nguarantees, and (3) makes SGD and its variants more practical methods for\nminimization. \n\n"}
{"id": "1702.00428", "contents": "Title: Malliavin-based Multilevel Monte Carlo Estimators for Densities of\n  Max-stable Processes Abstract: We introduce a class of unbiased Monte Carlo estimators for the multivariate\ndensity of max-stable fields generated by Gaussian processes. Our estimators\ntake advantage of recent results on exact simulation of max-stable fields\ncombined with identities studied in the Malliavin calculus literature and ideas\ndeveloped in the multilevel Monte Carlo literature. Our approach allows\nestimating multivariate densities of max-stable fields with precision\n$\\varepsilon $ at a computational cost of order $O\\left( \\varepsilon ^{-2}\\log\n\\log \\log \\left( 1/\\varepsilon \\right) \\right) $. \n\n"}
{"id": "1702.00434", "contents": "Title: Efficient algorithms for Bayesian Nearest Neighbor Gaussian Processes Abstract: We consider alternate formulations of recently proposed hierarchical Nearest\nNeighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved\nconvergence, faster computing time, and more robust and reproducible Bayesian\ninference. Algorithms are defined that improve CPU memory management and\nexploit existing high-performance numerical linear algebra libraries.\nComputational and inferential benefits are assessed for alternate NNGP\nspecifications using simulated datasets and remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska. The resulting data product is the\nfirst statistically robust map of forest canopy for the TIU. \n\n"}
{"id": "1702.01185", "contents": "Title: Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC) Abstract: For a large class of orthogonal basis functions, there has been a recent\nidentification of expansion methods for computing accurate, stable\napproximations of a quantity of interest. This paper presents, within the\ncontext of uncertainty quantification, a practical implementation using basis\nadaptation, and coherence motivated sampling, which under assumptions has\nsatisfying guarantees. This implementation is referred to as Basis Adaptive\nSample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use\nof anisotropic polynomial order which admits evolving global bases for\napproximation in an efficient manner, leading to consistently stable\napproximation for a practical class of smooth functionals. This fully adaptive,\nnon-intrusive method, requires no a priori information of the solution, and has\nsatisfying theoretical guarantees of recovery. A key contribution to stability\nis the use of a presented correction sampling for coherence-optimal sampling in\norder to improve stability and accuracy within the adaptive basis scheme.\nTheoretically, the method may dramatically reduce the impact of dimensionality\nin function approximation, and numerically the method is demonstrated to\nperform well on problems with dimension up to 1000. \n\n"}
{"id": "1702.01506", "contents": "Title: A data assimilation algorithm: the paradigm of the 3D Leray-alpha model\n  of turbulence Abstract: In this paper we survey the various implementations of a new data\nassimilation (downscaling) algorithm based on spatial coarse mesh measurements.\nAs a paradigm, we demonstrate the application of this algorithm to the 3D\nLeray-$\\alpha$ subgrid scale turbulence model. Most importantly, we use this\nparadigm to show that it is not always necessary that one has to collect coarse\nmesh measurements of all the state variables, that are involved in the\nunderlying evolutionary system, in order to recover the corresponding exact\nreference solution. Specifically, we show that in the case of the 3D\nLeray$-\\alpha$ model of turbulence the solutions of the algorithm, constructed\nusing only coarse mesh observations of any two components of the\nthree-dimensional velocity field, and without any information of the third\ncomponent, converge, at an exponential rate in time, to the corresponding exact\nreference solution of the 3D Leray$-\\alpha$ model. This study serves as an\naddendum to our recent work on abridged continuous data assimilation for the 2D\nNavier-Stokes equations. Notably, similar results have also been recently\nestablished for the 3D viscous Planetary Geostrophic circulation model in which\nwe show that coarse mesh measurements of the temperature alone are sufficient\nfor recovering, through our data assimilation algorithm, the full solution;\nviz. the three components of velocity vector field and the temperature.\nConsequently, this proves the Charney conjecture for the 3D Planetary\nGeostrophic model; namely, that the history of the large spatial scales of\ntemperature is sufficient for determining all the other quantities (state\nvariables) of the model. \n\n"}
{"id": "1702.02993", "contents": "Title: Short term unpredictability of high Reynolds number turbulence --- rough\n  dependence on initial data Abstract: Short term unpredictability is discovered numerically for high Reynolds\nnumber fluid flows under periodic boundary conditions. Furthermore, the\nabundance of the short term unpredictability is also discovered. These\ndiscoveries support our theory that fully developed turbulence is constantly\ndriven by such short term unpredictability. \n\n"}
{"id": "1702.03146", "contents": "Title: Analysis of a nonlinear importance sampling scheme for Bayesian\n  parameter estimation in state-space models Abstract: The Bayesian estimation of the unknown parameters of state-space (dynamical)\nsystems has received considerable attention over the past decade, with a\nhandful of powerful algorithms being introduced. In this paper we tackle the\ntheoretical analysis of the recently proposed {\\it nonlinear} population Monte\nCarlo (NPMC). This is an iterative importance sampling scheme whose key\nfeatures, compared to conventional importance samplers, are (i) the approximate\ncomputation of the importance weights (IWs) assigned to the Monte Carlo samples\nand (ii) the nonlinear transformation of these IWs in order to prevent the\ndegeneracy problem that flaws the performance of conventional importance\nsamplers. The contribution of the present paper is a rigorous proof of\nconvergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo\nsamples, $M$, increases. Our analysis reveals that the NIS approximation errors\nconverge to 0 almost surely and with the optimal Monte Carlo rate of\n$M^{-\\frac{1}{2}}$. Moreover, we prove that this is achieved even when the mean\nestimation error of the IWs remains constant, a property that has been termed\n{\\it exact approximation} in the Markov chain Monte Carlo literature. We\nillustrate these theoretical results by means of a computer simulation example\ninvolving the estimation of the parameters of a state-space model typically\nused for target tracking. \n\n"}
{"id": "1702.03716", "contents": "Title: A New Interpretation of Vortex-Split Sudden Stratospheric Warmings in\n  Terms of Equilibrium Statistical Mechanics Abstract: Vortex-split sudden stratospheric warmings (S-SSWs) are investigated by using\nthe Japanese 55-year Reanalysis (JRA-55), a spherical barotropic\nquasi-geostrophic (QG) model, and equilibrium statistical mechanics. The QG\nmodel reproduces well the evolution of the composite potential vorticity (PV)\nfield obtained from JRA-55 by considering a time-dependent effective topography\ngiven by the composite height field of the 550 K potential temperature surface.\nThe zonal-wavenumber-2 component of the effective topography is the most\nessential feature required to observe the vortex splitting. The\nstatistical-mechanics theory predicts a large-scale steady state as the most\nprobable outcome of turbulent stirring, and such a state can be computed\nwithout solving the QG dynamics. The theory is applied to a disk domain, which\nis modeled on the north polar cap in the stratosphere. The equilibrium state is\nobtained by computing the maximum of an entropy functional. In the range of\nparameters relevant to the winter stratosphere, this state is anticyclonic. By\ncontrast, cyclonic states are quasi-stationary states corresponding to saddle\npoints of the entropy functional. The theoretical calculations are compared\nwith the results of the quasi-static experiment in which the wavenumber-2\ntopographic amplitude is increased linearly and slowly with time. The results\nsuggest that S-SSWs can be qualitatively interpreted as the transition from the\ncyclonic quasi-stationary state toward the anticyclonic equilibrium state. The\npolar vortex splits during the transition toward the equilibrium state. Without\nany forcing such as radiative cooling, the anticyclonic equilibrium state would\nbe realized sufficiently after an S-SSW. \n\n"}
{"id": "1702.05518", "contents": "Title: Sampling strategies for fast updating of Gaussian Markov random fields Abstract: Gaussian Markov random fields (GMRFs) are popular for modeling dependence in\nlarge areal datasets due to their ease of interpretation and computational\nconvenience afforded by the sparse precision matrices needed for random\nvariable generation. Typically in Bayesian computation, GMRFs are updated\njointly in a block Gibbs sampler or componentwise in a single-site sampler via\nthe full conditional distributions. The former approach can speed convergence\nby updating correlated variables all at once, while the latter avoids solving\nlarge matrices. We consider a sampling approach in which the underlying graph\ncan be cut so that conditionally independent sites are updated simultaneously.\nThis algorithm allows a practitioner to parallelize updates of subsets of\nlocations or to take advantage of `vectorized' calculations in a high-level\nlanguage such as R. Through both simulated and real data, we demonstrate\ncomputational savings that can be achieved versus both single-site and block\nupdating, regardless of whether the data are on a regular or an irregular\nlattice. The approach provides a good compromise between statistical and\ncomputational efficiency and is accessible to statisticians without expertise\nin numerical analysis or advanced computing. \n\n"}
{"id": "1702.05593", "contents": "Title: Uncovering the Edge of the Polar Vortex Abstract: The polar vortices play a crucial role in the formation of the ozone hole and\ncan cause severe weather anomalies. Their boundaries, known as the vortex\n`edges', are typically identified via methods that are either frame-dependent\nor return non-material structures, and hence are unsuitable for assessing\nmaterial transport barriers. Using two-dimensional velocity data on isentropic\nsurfaces in the northern hemisphere, we show that elliptic Lagrangian Coherent\nStructures (LCSs) identify the correct outermost material surface dividing the\ncoherent vortex core from the surrounding incoherent surf zone. Despite the\npurely kinematic construction of LCSs, we find a remarkable contrast in\ntemperature and ozone concentration across the identified vortex boundary. We\nalso show that potential vorticity-based methods, despite their simplicity,\nmisidentify the correct extent of the vortex edge. Finally, exploiting the\nshrinkage of the vortex at various isentropic levels, we observe a trend in the\nmagnitude of vertical motion inside the vortex which is consistent with\nprevious results. \n\n"}
{"id": "1702.05698", "contents": "Title: Online Robust Principal Component Analysis with Change Point Detection Abstract: Robust PCA methods are typically batch algorithms which requires loading all\nobservations into memory before processing. This makes them inefficient to\nprocess big data. In this paper, we develop an efficient online robust\nprincipal component methods, namely online moving window robust principal\ncomponent analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can\nsuccessfully track not only slowly changing subspace but also abruptly changed\nsubspace. By embedding hypothesis testing into the algorithm, OMWRPCA can\ndetect change points of the underlying subspaces. Extensive simulation studies\ndemonstrate the superior performance of OMWRPCA compared with other\nstate-of-art approaches. We also apply the algorithm for real-time background\nsubtraction of surveillance video. \n\n"}
{"id": "1702.07007", "contents": "Title: Detecting causal associations in large nonlinear time series datasets Abstract: Identifying causal relationships from observational time series data is a key\nproblem in disciplines such as climate science or neuroscience, where\nexperiments are often not possible. Data-driven causal inference is challenging\nsince datasets are often high-dimensional and nonlinear with limited sample\nsizes. Here we introduce a novel method that flexibly combines linear or\nnonlinear conditional independence tests with a causal discovery algorithm that\nallows to reconstruct causal networks from large-scale time series datasets. We\nvalidate the method on a well-established climatic teleconnection connecting\nthe tropical Pacific with extra-tropical temperatures and using large-scale\nsynthetic datasets mimicking the typical properties of real data. The\nexperiments demonstrate that our method outperforms alternative techniques in\ndetection power from small to large-scale datasets and opens up entirely new\npossibilities to discover causal networks from time series across a range of\nresearch fields. \n\n"}
{"id": "1702.07094", "contents": "Title: BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time\n  Series Abstract: The R package BigVAR allows for the simultaneous estimation of\nhigh-dimensional time series by applying structured penalties to the\nconventional vector autoregression (VAR) and vector autoregression with\nexogenous variables (VARX) frameworks. Our methods can be utilized in many\nforecasting applications that make use of time-dependent data such as\nmacroeconomics, finance, and internet traffic. Our package extends solution\nalgorithms from the machine learning and signal processing literatures to a\ntime dependent setting: selecting the regularization parameter by sequential\ncross validation and provides substantial improvements in forecasting\nperformance over conventional methods. We offer a user-friendly interface that\nutilizes R's s4 object class structure which makes our methodology easily\naccessible to practicioners.\n  In this paper, we present an overview of our notation, the models that\ncomprise BigVAR, and the functionality of our package with a detailed example\nusing publicly available macroeconomic data. In addition, we present a\nsimulation study comparing the performance of several procedures that refit the\nsupport selected by a BigVAR procedure according to several variants of least\nsquares and conclude that refitting generally degrades forecast performance. \n\n"}
{"id": "1702.07400", "contents": "Title: Horseshoe Regularization for Feature Subset Selection Abstract: Feature subset selection arises in many high-dimensional applications of\nstatistics, such as compressed sensing and genomics. The $\\ell_0$ penalty is\nideal for this task, the caveat being it requires the NP-hard combinatorial\nevaluation of all models. A recent area of considerable interest is to develop\nefficient algorithms to fit models with a non-convex $\\ell_\\gamma$ penalty for\n$\\gamma\\in (0,1)$, which results in sparser models than the convex $\\ell_1$ or\nlasso penalty, but is harder to fit. We propose an alternative, termed the\nhorseshoe regularization penalty for feature subset selection, and demonstrate\nits theoretical and computational advantages. The distinguishing feature from\nexisting non-convex optimization approaches is a full probabilistic\nrepresentation of the penalty as the negative of the logarithm of a suitable\nprior, which in turn enables efficient expectation-maximization and local\nlinear approximation algorithms for optimization and MCMC for uncertainty\nquantification. In synthetic and real data, the resulting algorithms provide\nbetter statistical performance, and the computation requires a fraction of time\nof state-of-the-art non-convex solvers. \n\n"}
{"id": "1702.07583", "contents": "Title: Topological Origin of Equatorial Waves Abstract: Topology sheds new light on the emergence of unidirectional edge waves in a\nvariety of physical systems, from condensed matter to artificial lattices.\nWaves observed in geophysical flows are also robust to perturbations, which\nsuggests a role for topology. We show a topological origin for two celebrated\nequatorially trapped waves known as Kelvin and Yanai modes, due to the Earth's\nrotation that breaks time-reversal symmetry. The non-trivial structure of the\nbulk Poincar\\'e wave modes encoded through the first Chern number of value $2$\nguarantees existence for these waves. This invariant demonstrates that ocean\nand atmospheric waves share fundamental properties with topological insulators,\nand that topology plays an unexpected role in the Earth climate system. \n\n"}
{"id": "1702.08446", "contents": "Title: Monte Carlo on manifolds: sampling densities and integrating functions Abstract: We describe and analyze some Monte Carlo methods for manifolds in Euclidean\nspace defined by equality and inequality constraints. First, we give an MCMC\nsampler for probability distributions defined by un-normalized densities on\nsuch manifolds. The sampler uses a specific orthogonal projection to the\nsurface that requires only information about the tangent space to the manifold,\nobtainable from first derivatives of the constraint functions, hence avoiding\nthe need for curvature information or second derivatives. Second, we use the\nsampler to develop a multi-stage algorithm to compute integrals over such\nmanifolds. We provide single-run error estimates that avoid the need for\nmultiple independent runs. Computational experiments on various test problems\nshow that the algorithms and error estimates work in practice. The method is\napplied to compute the entropies of different sticky hard sphere systems. These\npredict the temperature or interaction energy at which loops of hard sticky\nspheres become preferable to chains. \n\n"}
{"id": "1702.08896", "contents": "Title: Hierarchical Implicit Models and Likelihood-Free Variational Inference Abstract: Implicit probabilistic models are a flexible class of models defined by a\nsimulation process for data. They form the basis for theories which encompass\nour understanding of the physical world. Despite this fundamental nature, the\nuse of implicit models remains limited due to challenges in specifying complex\nlatent structure in them, and in performing inferences in such models with\nlarge data sets. In this paper, we first introduce hierarchical implicit models\n(HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian\nmodeling, thereby defining models via simulators of data with rich hidden\nstructure. Next, we develop likelihood-free variational inference (LFVI), a\nscalable variational inference algorithm for HIMs. Key to LFVI is specifying a\nvariational family that is also implicit. This matches the model's flexibility\nand allows for accurate approximation of the posterior. We demonstrate diverse\napplications: a large-scale physical simulator for predator-prey populations in\necology; a Bayesian generative adversarial network for discrete data; and a\ndeep implicit model for text generation. \n\n"}
{"id": "1703.00368", "contents": "Title: Approximate Computational Approaches for Bayesian Sensor Placement in\n  High Dimensions Abstract: Since the cost of installing and maintaining sensors is usually high, sensor\nlocations are always strategically selected. For those aiming at inferring\ncertain quantities of interest (QoI), it is desirable to explore the dependency\nbetween sensor measurements and QoI. One of the most popular metric for the\ndependency is mutual information which naturally measures how much information\nabout one variable can be obtained given the other. However, computing mutual\ninformation is always challenging, and the result is unreliable in high\ndimension. In this paper, we propose an approach to find an approximate lower\nbound of mutual information and compute it in a lower dimension. Then, sensors\nare placed where highest mutual information (lower bound) is achieved and QoI\nis inferred via Bayes rule given sensor measurements. In addition, Bayesian\noptimization is introduced to provide a continuous mutual information surface\nover the domain and thus reduce the number of evaluations. A chemical release\naccident is simulated where multiple sensors are placed to locate the source of\nthe release. The result shows that the proposed approach is both effective and\nefficient in inferring QoI. \n\n"}
{"id": "1703.00864", "contents": "Title: The Unreasonable Effectiveness of Structured Random Orthogonal\n  Embeddings Abstract: We examine a class of embeddings based on structured random matrices with\northogonal rows which can be applied in many machine learning applications\nincluding dimensionality reduction and kernel approximation. For both the\nJohnson-Lindenstrauss transform and the angular kernel, we show that we can\nselect matrices yielding guaranteed improved performance in accuracy and/or\nspeed compared to earlier methods. We introduce matrices with complex entries\nwhich give significant further accuracy improvement. We provide geometric and\nMarkov chain-based perspectives to help understand the benefits, and empirical\nresults which suggest that the approach is helpful in a wider range of\napplications. \n\n"}
{"id": "1703.01106", "contents": "Title: Differentially Private Bayesian Learning on Distributed Data Abstract: Many applications of machine learning, for example in health care, would\nbenefit from methods that can guarantee privacy of data subjects. Differential\nprivacy (DP) has become established as a standard for protecting learning\nresults. The standard DP algorithms require a single trusted party to have\naccess to the entire data, which is a clear weakness. We consider DP Bayesian\nlearning in a distributed setting, where each party only holds a single sample\nor a few samples of the data. We propose a learning strategy based on a secure\nmulti-party sum function for aggregating summaries from data holders and the\nGaussian mechanism for DP. Our method builds on an asymptotically optimal and\npractically efficient DP Bayesian inference with rapidly diminishing extra\ncost. \n\n"}
{"id": "1703.01421", "contents": "Title: Approximate $l_0$-penalized estimation of piecewise-constant signals on\n  graphs Abstract: We study recovery of piecewise-constant signals on graphs by the estimator\nminimizing an $l_0$-edge-penalized objective. Although exact minimization of\nthis objective may be computationally intractable, we show that the same\nstatistical risk guarantees are achieved by the $\\alpha$-expansion algorithm\nwhich computes an approximate minimizer in polynomial time. We establish that\nfor graphs with small average vertex degree, these guarantees are minimax\nrate-optimal over classes of edge-sparse signals. For spatially inhomogeneous\ngraphs, we propose minimization of an edge-weighted objective where each edge\nis weighted by its effective resistance or another measure of its contribution\nto the graph's connectivity. We establish minimax optimality of the resulting\nestimators over corresponding edge-weighted sparsity classes. We show\ntheoretically that these risk guarantees are not always achieved by the\nestimator minimizing the $l_1$/total-variation relaxation, and empirically that\nthe $l_0$-based estimates are more accurate in high signal-to-noise settings. \n\n"}
{"id": "1703.02293", "contents": "Title: Variable selection for mixed data clustering: a model-based approach Abstract: We propose two approaches for selecting variables in latent class analysis\n(i.e.,mixture model assuming within component independence), which is the\ncommon model-based clustering method for mixed data. The first approach\nconsists in optimizing the BIC with a modified version of the EM algorithm.\nThis approach simultaneously performs both model selection and parameter\ninference. The second approach consists in maximizing the MICL, which considers\nthe clustering task, with an algorithm of alternate optimization. This approach\nperforms model selection without requiring the maximum likelihood estimates for\nmodel comparison, then parameter inference is done for the unique selected\nmodel. Thus, the benefits of both approaches is to avoid the computation of the\nmaximum likelihood estimates for each model comparison. Moreover, they also\navoid the use of the standard algorithms for variable selection which are often\nsuboptimal (e.g. stepwise method) and computationally expensive. The case of\ndata with missing values is also discussed. The interest of both proposed\ncriteria is shown on simulated and real data. \n\n"}
{"id": "1703.02998", "contents": "Title: A note on quickly sampling a sparse matrix with low rank expectation Abstract: Given matrices $X,Y \\in R^{n \\times K}$ and $S \\in R^{K \\times K}$ with\npositive elements, this paper proposes an algorithm fastRG to sample a sparse\nmatrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson\nelements. This allows for quickly sampling from a broad class of stochastic\nblockmodel graphs (degree-corrected, mixed membership, overlapping) all of\nwhich are specific parameterizations of the generalized random product graph\nmodel defined in Section 2.2. The basic idea of fastRG is to first sample the\nnumber of edges $m$ and then sample each edge. The key insight is that because\nof the the low rank expectation, it is easy to sample individual edges. The\nnaive \"element-wise\" algorithm requires $O(n^2)$ operations to generate the\n$n\\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring\nlog terms, fastRG runs in time $O(n)$. An implementation in fastRG is available\non github. A computational experiment in Section 2.4 simulates graphs up to\n$n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with\n$n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5\nGHz Intel i5. \n\n"}
{"id": "1703.03352", "contents": "Title: A log-linear time algorithm for constrained changepoint detection Abstract: Changepoint detection is a central problem in time series and genomic data.\nFor some applications, it is natural to impose constraints on the directions of\nchanges. One example is ChIP-seq data, for which adding an up-down constraint\nimproves peak detection accuracy, but makes the optimization problem more\ncomplicated. We show how a recently proposed functional pruning technique can\nbe adapted to solve such constrained changepoint detection problems. This leads\nto a new algorithm which can solve problems with arbitrary affine constraints\non adjacent segment means, and which has empirical time complexity that is\nlog-linear in the amount of data. This algorithm achieves state-of-the-art\naccuracy in a benchmark of several genomic data sets, and is orders of\nmagnitude faster than existing algorithms that have similar accuracy. Our\nimplementation is available as the PeakSegPDPA function in the coseg R package,\nhttps://github.com/tdhock/coseg \n\n"}
{"id": "1703.05471", "contents": "Title: Model selection and parameter inference in phylogenetics using Nested\n  Sampling Abstract: Bayesian inference methods rely on numerical algorithms for both model\nselection and parameter inference. In general, these algorithms require a high\ncomputational effort to yield reliable estimates. One of the major challenges\nin phylogenetics is the estimation of the marginal likelihood. This quantity is\ncommonly used for comparing different evolutionary models, but its calculation,\neven for simple models, incurs high computational cost. Another interesting\nchallenge relates to the estimation of the posterior distribution. Often, long\nMarkov chains are required to get sufficient samples to carry out parameter\ninference, especially for tree distributions. In general, these problems are\naddressed separately by using different procedures. Nested sampling (NS) is a\nBayesian computation algorithm which provides the means to estimate marginal\nlikelihoods together with their uncertainties, and to sample from the posterior\ndistribution at no extra cost. The methods currently used in phylogenetics for\nmarginal likelihood estimation lack in practicality due to their dependence on\nmany tuning parameters and the inability of most implementations to provide a\ndirect way to calculate the uncertainties associated with the estimates. To\naddress these issues, we introduce NS to phylogenetics. Its performance is\nassessed under different scenarios and compared to established methods. We\nconclude that NS is a competitive and attractive algorithm for phylogenetic\ninference. An implementation is available as a package for BEAST 2 under the\nLGPL licence, accessible at https://github.com/BEAST2-Dev/nested-sampling. \n\n"}
{"id": "1703.05984", "contents": "Title: A Tutorial on Bridge Sampling Abstract: The marginal likelihood plays an important role in many areas of Bayesian\nstatistics such as parameter estimation, model comparison, and model averaging.\nIn most applications, however, the marginal likelihood is not analytically\ntractable and must be approximated using numerical methods. Here we provide a\ntutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and\nrelatively straightforward sampling method that allows researchers to obtain\nthe marginal likelihood for models of varying complexity. First, we introduce\nbridge sampling and three related sampling methods using the beta-binomial\nmodel as a running example. We then apply bridge sampling to estimate the\nmarginal likelihood for the Expectancy Valence (EV) model---a popular model for\nreinforcement learning. Our results indicate that bridge sampling provides\naccurate estimates for both a single participant and a hierarchical version of\nthe EV model. We conclude that bridge sampling is an attractive method for\nmathematical psychologists who typically aim to approximate the marginal\nlikelihood for a limited set of possibly high-dimensional models. \n\n"}
{"id": "1703.06098", "contents": "Title: Multilevel linear models, Gibbs samplers and multigrid decompositions Abstract: We study the convergence properties of the Gibbs Sampler in the context of\nposterior distributions arising from Bayesian analysis of conditionally\nGaussian hierarchical models. We develop a multigrid approach to derive\nanalytic expressions for the convergence rates of the algorithm for various\nwidely used model structures, including nested and crossed random effects. Our\nresults apply to multilevel models with an arbitrary number of layers in the\nhierarchy, while most previous work was limited to the two-level nested case.\nThe theoretical results provide explicit and easy-to-implement guidelines to\noptimize practical implementations of the Gibbs Sampler, such as indications on\nwhich parametrization to choose (e.g. centred and non-centred), which\nconstraint to impose to guarantee statistical identifiability, and which\nparameters to monitor in the diagnostic process. Simulations suggest that the\nresults are informative also in the context of non-Gaussian distributions and\nmore general MCMC schemes, such as gradient-based ones.implementation of Gibbs\nsamplers on conditionally Gaussian hierarchical models. \n\n"}
{"id": "1703.06206", "contents": "Title: Sequential Monte Carlo Methods in the nimble R Package Abstract: nimble is an R package for constructing algorithms and conducting inference\non hierarchical models. The nimble package provides a unique combination of\nflexible model specification and the ability to program model-generic\nalgorithms. Specifically, the package allows users to code models in the BUGS\nlanguage, and it allows users to write algorithms that can be applied to any\nappropriate model. In this paper, we introduce nimble's capabilities for\nstate-space model analysis using sequential Monte Carlo (SMC) techniques. We\nfirst provide an overview of state-space models and commonly-used SMC\nalgorithms. We then describe how to build a state-space model and conduct\ninference using existing SMC algorithms within nimble. SMC algorithms within\nnimble currently include the bootstrap filter, auxiliary particle filter,\nensemble Kalman filter, IF2 method of iterated filtering, and a particle MCMC\nsampler. These algorithms can be run in R or compiled into C++ for more\nefficient execution. Examples of applying SMC algorithms to linear\nautoregressive models and a stochastic volatility model are provided. Finally,\nwe give an overview of how model-generic algorithms are coded within nimble by\nproviding code for a simple SMC algorithm. This illustrates how users can\neasily extend nimble's SMC methods in high-level code. \n\n"}
{"id": "1703.06359", "contents": "Title: Fully symmetric kernel quadrature Abstract: Kernel quadratures and other kernel-based approximation methods typically\nsuffer from prohibitive cubic time and quadratic space complexity in the number\nof function evaluations. The problem arises because a system of linear\nequations needs to be solved. In this article we show that the weights of a\nkernel quadrature rule can be computed efficiently and exactly for up to tens\nof millions of nodes if the kernel, integration domain, and measure are fully\nsymmetric and the node set is a union of fully symmetric sets. This is based on\nthe observations that in such a setting there are only as many distinct weights\nas there are fully symmetric sets and that these weights can be solved from a\nlinear system of equations constructed out of row sums of certain submatrices\nof the full kernel matrix. We present several numerical examples that show\nfeasibility, both for a large number of nodes and in high dimensions, of the\ndeveloped fully symmetric kernel quadrature rules. Most prominent of the fully\nsymmetric kernel quadrature rules we propose are those that use sparse grids. \n\n"}
{"id": "1703.06594", "contents": "Title: A barotropic model of eddy saturation Abstract: \"Eddy saturation\" refers to a regime in which the total volume transport of\nan oceanic current is insensitive to the wind stress strength. Baroclinicity is\ncurrently believed to be key to the development of an eddy-saturated state. In\nthis paper, it is shown that eddy saturation can also occur in a purely\nbarotropic flow over topography, without baroclinicity. Thus, eddy saturation\nis a fundamental property of barotropic dynamics above topography. It is\ndemonstrated that the main factor controlling the appearance or not of\neddy-saturated states in the barotropic setting is the structure of geostrophic\ncontours, that is the contours of $f/H$ of the ratio of the Coriolis parameter\nto the ocean's depth. Eddy-saturated states occur when the geostrophic contours\nare open, that is when the geostrophic contours span the whole zonal extent of\nthe domain. This minimal requirement for eddy-saturated states is demonstrated\nusing numerical integrations of a single-layer quasi-geostrophic flow over two\ndifferent topographies characterized by either open or closed geostrophic\ncontours with parameter values loosely inspired by the Southern Ocean. In this\nsetting, transient eddies are produced through a barotropic-topographic\ninstability that occurs due tot the interaction of the large-scale zonal flow\nwith the topography. Through the study of this barotropic-topographic\ninstability insight is gained on how eddy-saturated states are established. \n\n"}
{"id": "1703.07039", "contents": "Title: A Simple Online Parameter Estimation Technique with Asymptotic\n  Guarantees Abstract: In many modern settings, data are acquired iteratively over time, rather than\nall at once. Such settings are known as online, as opposed to offline or batch.\nWe introduce a simple technique for online parameter estimation, which can\noperate in low memory settings, settings where data are correlated, and only\nrequires a single inspection of the available data at each time period. We show\nthat the estimators---constructed via the technique---are asymptotically normal\nunder generous assumptions, and present a technique for the online computation\nof the covariance matrices for such estimators. A set of numerical studies\ndemonstrates that our estimators can be as efficient as their offline\ncounterparts, and that our technique generates estimates and confidence\nintervals that match their offline counterparts in various parameter estimation\nsettings. \n\n"}
{"id": "1703.07285", "contents": "Title: From safe screening rules to working sets for faster Lasso-type solvers Abstract: Convex sparsity-promoting regularizations are ubiquitous in modern\nstatistical learning. By construction, they yield solutions with few non-zero\ncoefficients, which correspond to saturated constraints in the dual\noptimization formulation. Working set (WS) strategies are generic optimization\ntechniques that consist in solving simpler problems that only consider a subset\nof constraints, whose indices form the WS. Working set methods therefore\ninvolve two nested iterations: the outer loop corresponds to the definition of\nthe WS and the inner loop calls a solver for the subproblems. For the Lasso\nestimator a WS is a set of features, while for a Group Lasso it refers to a set\nof groups. In practice, WS are generally small in this context so the\nassociated feature Gram matrix can fit in memory. Here we show that the\nGauss-Southwell rule (a greedy strategy for block coordinate descent\ntechniques) leads to fast solvers in this case. Combined with a working set\nstrategy based on an aggressive use of so-called Gap Safe screening rules, we\npropose a solver achieving state-of-the-art performance on sparse learning\nproblems. Results are presented on Lasso and multi-task Lasso estimators. \n\n"}
{"id": "1703.08504", "contents": "Title: Shingle 2.0: generalising self-consistent and automated domain\n  discretisation for multi-scale geophysical models Abstract: The approaches taken to describe and develop spatial discretisations of the\ndomains required for geophysical simulation models are commonly ad hoc, model\nor application specific and under-documented. This is particularly acute for\nsimulation models that are flexible in their use of multi-scale, anisotropic,\nfully unstructured meshes where a relatively large number of heterogeneous\nparameters are required to constrain their full description. As a consequence,\nit can be difficult to reproduce simulations, ensure a provenance in model data\nhandling and initialisation, and a challenge to conduct model intercomparisons\nrigorously. This paper takes a novel approach to spatial discretisation,\nconsidering it much like a numerical simulation model problem of its own. It\nintroduces a generalised, extensible, self-documenting approach to carefully\ndescribe, and necessarily fully, the constraints over the heterogeneous\nparameter space that determine how a domain is spatially discretised. This\nadditionally provides a method to accurately record these constraints, using\nhigh-level natural language based abstractions, that enables full accounts of\nprovenance, sharing and distribution. Together with this description, a\ngeneralised consistent approach to unstructured mesh generation for geophysical\nmodels is developed, that is automated, robust and repeatable, quick-to-draft,\nrigorously verified and consistent to the source data throughout. This\ninterprets the description above to execute a self-consistent spatial\ndiscretisation process, which is automatically validated to expected discrete\ncharacteristics and metrics. \n\n"}
{"id": "1703.09281", "contents": "Title: Global surface temperature trends and the effect of World War II Abstract: Using parametric analysis (curve fitting) we find a persistent temperature\nbump, coincident with World War II (WW2), in eight independent time series,\nfour land- and four ocean-based. We fit the data with a Gaussian on a quadratic\nbackground. Six parameters (constant, linear and quadratic background terms and\nthe amplitude, position and width of the Gaussian) are free to vary. The mean\nfitted Gaussian amplitude is 0.339$\\pm$ 0.065$\\,^\\circ$C, non-zero by\n5.2$\\sigma$ and therefore not accidental. The area is 2.0$\\pm$0.5$\\,^\\circ$C\nyr. Temperature recovered to baseline rather quickly. Rather than coincidence,\nor systematic measuring error synchronized with WW2, we conjecture the bump is\ndue to human activity, including the greatly increased combustion (relative to\nthat era) of fossil and other fuels. Background surface temperature behavior, a\nbyproduct of our study but largely independent of the WW2 bump, is far more\nconsequential nowadays. The linear term, 0.747$\\pm$0.023$\\,^\\circ$C/century,\nagrees well with other findings, but the present-day rate of increase,\n$\\approx$2.5$\\,^\\circ$C/century, is far greater because of the quadratic term. \n\n"}
{"id": "1703.09301", "contents": "Title: Large-scale estimation of random graph models with local dependence Abstract: A class of random graph models is considered, combining features of\nexponential-family models and latent structure models, with the goal of\nretaining the strengths of both of them while reducing the weaknesses of each\nof them. An open problem is how to estimate such models from large networks. A\nnovel approach to large-scale estimation is proposed, taking advantage of the\nlocal structure of such models for the purpose of local computing. The main\nidea is that random graphs with local dependence can be decomposed into\nsubgraphs, which enables parallel computing on subgraphs and suggests a\ntwo-step estimation approach. The first step estimates the local structure\nunderlying random graphs. The second step estimates parameters given the\nestimated local structure of random graphs. Both steps can be implemented in\nparallel, which enables large-scale estimation. The advantages of the two-step\nestimation approach are demonstrated by simulation studies with up to 10,000\nnodes and an application to a large Amazon product recommendation network with\nmore than 10,000 products. \n\n"}
{"id": "1704.00117", "contents": "Title: A Multi-Index Markov Chain Monte Carlo Method Abstract: In this article we consider computing expectations w.r.t.~probability laws\nassociated to a certain class of stochastic systems. In order to achieve such a\ntask, one must not only resort to numerical approximation of the expectation,\nbut also to a biased discretization of the associated probability. We are\nconcerned with the situation for which the discretization is required in\nmultiple dimensions, for instance in space and time. In such contexts, it is\nknown that the multi-index Monte Carlo (MIMC) method can improve upon\ni.i.d.~sampling from the most accurate approximation of the probability law.\nIndeed by a non-trivial modification of the multilevel Monte Carlo (MLMC)\nmethod and it can reduce the work to obtain a given level of error, relative to\nthe afore mentioned i.i.d.~sampling and relative even to MLMC. In this article\nwe consider the case when such probability laws are too complex to sampled\nindependently. We develop a modification of the MIMC method which allows one to\nuse standard Markov chain Monte Carlo (MCMC) algorithms to replace independent\nand coupled sampling, in certain contexts. We prove a variance theorem which\nshows that using our MIMCMC method is preferable, in the sense above, to\ni.i.d.~sampling from the most accurate approximation, under assumptions. The\nmethod is numerically illustrated on a problem associated to a stochastic\npartial differential equation (SPDE). \n\n"}
{"id": "1704.02389", "contents": "Title: Extracting quasi-steady Lagrangian transport patterns from the ocean\n  circulation: An application to the Gulf of Mexico Abstract: We construct a climatology of Lagrangian coherent structures (LCSs), the\nconcealed skeleton that shapes transport, with a twelve-year-long\ndata-assimilative simulation of the sea-surface circulation in the Gulf of\nMexico (GoM). Computed as time-mean Cauchy-Green strain tensorlines of the\nclimatological velocity, the climatological LCSs (cLCSs) unveil recurrent\nLagrangian circulation patterns. cLCSs strongly constrain the ensemble-mean\nLagrangian circulation of the instantaneous model velocity, thus we show that a\nclimatological velocity may preserve meaningful transport information. Also,\nthe climatological transport patterns we report agree well with GoM kinematics\nand dynamics, as described in several previous observational and numerical\nstudies. For example, cLCSs identify regions of persistent isolation, and\nsuggest that coastal regions previously identified as high-risk for pollution\nimpact, are regions of maximal attraction. Also, we show examples where cLCSs\nare remarkably similar to transport patterns observed during the Deepwater\nHorizon and Ixtoc oil spills, and during the Grand LAgrangian Deployment (GLAD)\nexperiment. Thus, it is shown that cLCSs are an efficient way of synthesizing\nvast amounts of Lagrangian information. The cLCS method confirms previous GoM\nstudies, and contributes to our understanding by revealing the persistent\nnature of the dynamics and kinematics treated therein. \n\n"}
{"id": "1704.02719", "contents": "Title: Mechanism of mean flow generation in rotating turbulence through\n  inhomogeneous helicity Abstract: Recent numerical simulations showed that the mean flow is generated in\ninhomogeneous turbulence of an incompressible fluid accompanied with helicity\nand system rotation. In order to investigate the mechanism of this phenomenon,\nwe carry out a numerical simulation of inhomogeneous turbulence in a rotating\nsystem. In the simulation, an external force is applied to inject inhomogeneous\nturbulent helicity and the rotation axis is taken to be perpendicular to the\ninhomogeneous direction. No mean velocity is set in the initial condition of\nthe simulation. The simulation results show that only in the case with both the\nhelical forcing and the system rotation, the mean flow directed to the rotation\naxis is generated and sustained. We investigate the physical origin of this\nflow-generation phenomenon by considering the budget of the Reynolds-stress\ntransport equation. It is found that the pressure diffusion term has a large\ncontribution in the Reynolds stress equation and supports the generated mean\nflow. It is shown that a model expression for the pressure diffusion can be\nexpressed by the turbulent helicity gradient coupled with the angular velocity\nof the system rotation. This implies that inhomogeneous helicity can play a\nsignificant role for the generation of the large-scale velocity distribution in\nincompressible turbulent flows. \n\n"}
{"id": "1704.02791", "contents": "Title: Efficient SMC$^2$ schemes for stochastic kinetic models Abstract: Fitting stochastic kinetic models represented by Markov jump processes within\nthe Bayesian paradigm is complicated by the intractability of the observed data\nlikelihood. There has therefore been considerable attention given to the design\nof pseudo-marginal Markov chain Monte Carlo algorithms for such models.\nHowever, these methods are typically computationally intensive, often require\ncareful tuning and must be restarted from scratch upon receipt of new\nobservations. Sequential Monte Carlo (SMC) methods on the other hand aim to\nefficiently reuse posterior samples at each time point. Despite their appeal,\napplying SMC schemes in scenarios with both dynamic states and static\nparameters is made difficult by the problem of particle degeneracy. A\nprincipled approach for overcoming this problem is to move each parameter\nparticle through a Metropolis-Hastings kernel that leaves the target invariant.\nThis rejuvenation step is key to a recently proposed SMC$^2$ algorithm, which\ncan be seen as the pseudo-marginal analogue of an idealised scheme known as\niterated batch importance sampling. Computing the parameter weights in SMC$^2$\nrequires running a particle filter over dynamic states to unbiasedly estimate\nthe intractable observed data likelihood contributions at each time point. In\nthis paper, we propose to use an auxiliary particle filter inside the SMC$^2$\nscheme. Our method uses two recently proposed constructs for sampling\nconditioned jump processes and we find that the resulting inference schemes\ntypically require fewer state particles than when using a simple bootstrap\nfilter. Using two applications, we compare the performance of the proposed\napproach with various competing methods, including two global MCMC schemes. \n\n"}
{"id": "1704.03459", "contents": "Title: Dynamic nested sampling: an improved algorithm for parameter estimation\n  and evidence calculation Abstract: We introduce dynamic nested sampling: a generalisation of the nested sampling\nalgorithm in which the number of \"live points\" varies to allocate samples more\nefficiently. In empirical tests the new method significantly improves\ncalculation accuracy compared to standard nested sampling with the same number\nof samples; this increase in accuracy is equivalent to speeding up the\ncomputation by factors of up to ~72 for parameter estimation and ~7 for\nevidence calculations. We also show that the accuracy of both parameter\nestimation and evidence calculations can be improved simultaneously. In\naddition, unlike in standard nested sampling, more accurate results can be\nobtained by continuing the calculation for longer. Popular standard nested\nsampling implementations can be easily adapted to perform dynamic nested\nsampling, and several dynamic nested sampling software packages are now\npublicly available. \n\n"}
{"id": "1704.03581", "contents": "Title: P\\'olya Urn Latent Dirichlet Allocation: a doubly sparse massively\n  parallel sampler Abstract: Latent Dirichlet Allocation (LDA) is a topic model widely used in natural\nlanguage processing and machine learning. Most approaches to training the model\nrely on iterative algorithms, which makes it difficult to run LDA on big\ncorpora that are best analyzed in parallel and distributed computational\nenvironments. Indeed, current approaches to parallel inference either don't\nconverge to the correct posterior or require storage of large dense matrices in\nmemory. We present a novel sampler that overcomes both problems, and we show\nthat this sampler is faster, both empirically and theoretically, than previous\nGibbs samplers for LDA. We do so by employing a novel P\\'olya-urn-based\napproximation in the sparse partially collapsed sampler for LDA. We prove that\nthe approximation error vanishes with data size, making our algorithm\nasymptotically exact, a property of importance for large-scale topic models. In\naddition, we show, via an explicit example, that - contrary to popular belief\nin the topic modeling literature - partially collapsed samplers can be more\nefficient than fully collapsed samplers. We conclude by comparing the\nperformance of our algorithm with that of other approaches on well-known\ncorpora. \n\n"}
{"id": "1704.05098", "contents": "Title: Statistical inference for high dimensional regression via Constrained\n  Lasso Abstract: In this paper, we propose a new method for estimation and constructing\nconfidence intervals for low-dimensional components in a high-dimensional\nmodel. The proposed estimator, called Constrained Lasso (CLasso) estimator, is\nobtained by simultaneously solving two estimating equations---one imposing a\nzero-bias constraint for the low-dimensional parameter and the other forming an\n$\\ell_1$-penalized procedure for the high-dimensional nuisance parameter. By\ncarefully choosing the zero-bias constraint, the resulting estimator of the low\ndimensional parameter is shown to admit an asymptotically normal limit\nattaining the Cram\\'{e}r-Rao lower bound in a semiparametric sense. We propose\na tuning-free iterative algorithm for implementing the CLasso. We show that\nwhen the algorithm is initialized at the Lasso estimator, the de-sparsified\nestimator proposed in van de Geer et al. [\\emph{Ann. Statist.} {\\bf 42} (2014)\n1166--1202] is asymptotically equivalent to the first iterate of the algorithm.\nWe analyse the asymptotic properties of the CLasso estimator and show the\nglobally linear convergence of the algorithm. We also demonstrate encouraging\nempirical performance of the CLasso through numerical studies. \n\n"}
{"id": "1704.06017", "contents": "Title: PAFit: an R Package for the Non-Parametric Estimation of Preferential\n  Attachment and Node Fitness in Temporal Complex Networks Abstract: Many real-world systems are profitably described as complex networks that\ngrow over time. Preferential attachment and node fitness are two simple growth\nmechanisms that not only explain certain structural properties commonly\nobserved in real-world systems, but are also tied to a number of applications\nin modeling and inference. While there are statistical packages for estimating\nvarious parametric forms of the preferential attachment function, there is no\nsuch package implementing non-parametric estimation procedures. The\nnon-parametric approach to the estimation of the preferential attachment\nfunction allows for comparatively finer-grained investigations of the\n`rich-get-richer' phenomenon that could lead to novel insights in the search to\nexplain certain nonstandard structural properties observed in real-world\nnetworks. This paper introduces the R package PAFit, which implements\nnon-parametric procedures for estimating the preferential attachment function\nand node fitnesses in a growing network, as well as a number of functions for\ngenerating complex networks from these two mechanisms. The main computational\npart of the package is implemented in C++ with OpenMP to ensure scalability to\nlarge-scale networks. We first introduce the main functionalities of PAFit\nthrough simulated examples, and then use the package to analyze a collaboration\nnetwork between scientists in the field of complex networks. The results\nindicate the joint presence of `rich-get-richer' and `fit-get-richer' phenomena\nin the collaboration network. The estimated attachment function is observed to\nbe near-linear, which we interpret as meaning that the chance an author gets a\nnew collaborator is proportional to their current number of collaborators.\nFurthermore, the estimated author fitnesses reveal a host of familiar faces\nfrom the complex networks community among the field's topmost fittest network\nscientists. \n\n"}
{"id": "1704.06186", "contents": "Title: Enduring Lagrangian coherence of a Loop Current ring assessed using\n  independent observations Abstract: Ocean flows are routinely inferred from low-resolution satellite altimetry\nmeasurements of sea surface height assuming a geostrophic balance. Recent\nnonlinear dynamical systems techniques have revealed that surface currents\nderived from altimetry can support mesoscale eddies with material boundaries\nthat do not filament for many months, thereby representing effective transport\nmechanisms. However, the long-range Lagrangian coherence assessed for mesoscale\neddy boundaries detected from altimetry is constrained by the impossibility of\ncurrent altimeters to resolve ageostrophic submesoscale motions. These may act\nto prevent Lagrangian coherence from manifesting in the rigorous form described\nby the nonlinear dynamical systems theories. Here we use a combination of\nsatellite ocean color and surface drifter trajectory data, rarely available\nsimultaneously over an extended period of time, to provide observational\nevidence for the enduring Lagrangian coherence of a Loop Current ring detected\nfrom altimetry. We also seek indications of this behavior in the flow produced\nby a data-assimilative system which demonstrated ability to reproduce observed\nrelative dispersion statistics down into the marginally submesoscale range.\nHowever, the simulated flow, total surface and subsurface or subsampled\nemulating altimetry, is not found to support the long-lasting Lagrangian\ncoherence that characterizes the observed ring. This highlights the importance\nof the Lagrangian metrics produced by the nonlinear dynamical systems tools\nemployed here in assessing model performance. \n\n"}
{"id": "1704.06988", "contents": "Title: Ensemble Kalman methods for high-dimensional hierarchical dynamic\n  space-time models Abstract: We propose a new class of filtering and smoothing methods for inference in\nhigh-dimensional, nonlinear, non-Gaussian, spatio-temporal state-space models.\nThe main idea is to combine the ensemble Kalman filter and smoother, developed\nin the geophysics literature, with state-space algorithms from the statistics\nliterature. Our algorithms address a variety of estimation scenarios, including\non-line and off-line state and parameter estimation. We take a Bayesian\nperspective, for which the goal is to generate samples from the joint posterior\ndistribution of states and parameters. The key benefit of our approach is the\nuse of ensemble Kalman methods for dimension reduction, which allows inference\nfor high-dimensional state vectors. We compare our methods to existing ones,\nincluding ensemble Kalman filters, particle filters, and particle MCMC. Using a\nreal data example of cloud motion and data simulated under a number of\nnonlinear and non-Gaussian scenarios, we show that our approaches outperform\nthese existing methods. \n\n"}
{"id": "1705.01024", "contents": "Title: A projection pursuit framework for testing general high-dimensional\n  hypothesis Abstract: This article develops a framework for testing general hypothesis in\nhigh-dimensional models where the number of variables may far exceed the number\nof observations. Existing literature has considered less than a handful of\nhypotheses, such as testing individual coordinates of the model parameter.\nHowever, the problem of testing general and complex hypotheses remains widely\nopen. We propose a new inference method developed around the hypothesis\nadaptive projection pursuit framework, which solves the testing problems in the\nmost general case. The proposed inference is centered around a new class of\nestimators defined as $l_1$ projection of the initial guess of the unknown onto\nthe space defined by the null. This projection automatically takes into account\nthe structure of the null hypothesis and allows us to study formal inference\nfor a number of long-standing problems. For example, we can directly conduct\ninference on the sparsity level of the model parameters and the minimum signal\nstrength. This is especially significant given the fact that the former is a\nfundamental condition underlying most of the theoretical development in\nhigh-dimensional statistics, while the latter is a key condition used to\nestablish variable selection properties. Moreover, the proposed method is\nasymptotically exact and has satisfactory power properties for testing very\ngeneral functionals of the high-dimensional parameters. The simulation studies\nlend further support to our theoretical claims and additionally show excellent\nfinite-sample size and power properties of the proposed test. \n\n"}
{"id": "1705.02013", "contents": "Title: FEniCS Application to a Finite Element Quasi-Geostrophic Model. Linear\n  and non-linear analysis of Munk-like Solutions and Data Assimilation\n  Implementation Through Adjoint Method Abstract: The first aim of this work is to construct a Finite Elements model for\nquasi-geostrophic equation. This model is analyzed through FEniCS interface. As\na second purpose, it shows the potential of the control theory to treat Data\nAssimilation large scale problems. In section one the finite element\napproximation of the quasi-geostrophic model is described. In section two some\nnumerical results on different domains are shown. In section three a comparison\nbetween Navier Stokes model and quasi-geostrophic one is discussed. Last\nsection is dedicated to a more complex data assimilation/control problem based\non quasi-geostrophic equations, to test possible developments and improvements. \n\n"}
{"id": "1705.02786", "contents": "Title: A local ensemble transform Kalman particle filter for convective scale\n  data assimilation Abstract: Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)\nare a key component of probabilistic weather forecasting. They represent the\nuncertainty in the initial conditions by an ensemble which incorporates\ninformation coming from the physical model with the latest observations.\nHigh-resolution numerical weather prediction models ran at operational centers\nare able to resolve non-linear and non-Gaussian physical phenomena such as\nconvection. There is therefore a growing need to develop ensemble assimilation\nalgorithms able to deal with non-Gaussianity while staying computationally\nfeasible. In the present paper we address some of these needs by proposing a\nnew hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully\nformulated in ensemble space and uses a deterministic scheme such that it has\nthe ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a\nlimiting case. A new criterion for choosing the proportion of particle filter\nand ETKF update is also proposed. The new algorithm is implemented in the COSMO\nframework and numerical experiments in a quasi-operational convective-scale\nsetup are conducted. The results show the feasibility of the new algorithm in\npractice and indicate a strong potential for such local hybrid methods, in\nparticular for forecasting non-Gaussian variables such as wind and hourly\nprecipitation. \n\n"}
{"id": "1705.02964", "contents": "Title: Nonsmooth Invariant Manifolds in a Conceptual Climate Model Abstract: There is widespread agreement that ice sheets flowed into the ocean in\ntropical latitudes at sea level during the Earth's past. Whether these extreme\nice ages were snowball Earth events, with the entire surface covered in ice, or\nwhether ocean water remained ice free in regions about the equator, continues\nto be controversial. For the latter situation to occur, the effect of positive\nice albedo feedback would have to be damped to stabilize an advancing ice sheet\nshy of the equator. In this paper we analyze a conceptual model comprised of a\nzonally averaged surface temperature equation coupled to a dynamic ice line\nequation. This difference equation model is aligned with the cold world of\nthese great glacial episodes through an appropriately chosen albedo function.\nUsing the spectral method, the analysis leads to a nonsmooth singular\nperturbation problem. The Hadamard graph transform method is applied to prove\nthe persistence of an invariant manifold, thereby providing insight into model\nbehavior. A stable climate state with the ice line resting in tropical\nlatitudes, but with open water about the equator, is shown to exist. Also\npresented are local smooth and nonsmooth bifurcations as parameters related to\natmospheric CO$_2$ concentrations and the efficiency of meridional heat\ntransport, respectively, are varied. \n\n"}
{"id": "1705.03196", "contents": "Title: Accurate Computation of the Distribution of Sums of Dependent\n  Log-Normals with Applications to the Black-Scholes Model Abstract: We present a new Monte Carlo methodology for the accurate estimation of the\ndistribution of the sum of dependent log-normal random variables. The\nmethodology delivers statistically unbiased estimators for three distributional\nquantities of significant interest in finance and risk management: the left\ntail, or cumulative distribution function, the probability density function,\nand the right tail, or complementary distribution function of the sum of\ndependent log-normal factors. In all of these three cases our methodology\ndelivers fast and highly accurate estimators in settings for which existing\nmethodology delivers estimators with large variance that tend to underestimate\nthe true quantity of interest. We provide insight into the computational\nchallenges using theory and numerical experiments, and explain their much wider\nimplications for Monte Carlo statistical estimators of rare-event\nprobabilities. In particular, we find that theoretically strongly-efficient\nestimators should be used with great caution in practice, because they may\nyield inaccurate results in the pre-limit. Further, this inaccuracy may not be\ndetectable from the output of the Monte Carlo simulation, because the\nsimulation output may severely underestimate the true variance of the\nestimator. \n\n"}
{"id": "1705.04584", "contents": "Title: spBayesSurv: Fitting Bayesian Spatial Survival Models Using R Abstract: Spatial survival analysis has received a great deal of attention over the\nlast 20 years due to the important role that geographical information can play\nin predicting survival. This paper provides an introduction to a set of\nprograms for implementing some Bayesian spatial survival models in R using the\npackage spBayesSurv. The function survregbayes includes the three most\ncommonly-used semiparametric models: proportional hazards, proportional odds,\nand accelerated failure time. All manner of censored survival times are\nsimultaneously accommodated including uncensored, interval censored,\ncurrent-status, left and right censored, and mixtures of these. Left-truncated\ndata are also accommodated. Time-dependent covariates are allowed under the\npiecewise constant assumption. Both georeferenced and areally observed spatial\nlocations are handled via frailties. Model fit is assessed with conditional\nCox-Snell residual plots, and model choice is carried out via the log pseudo\nmarginal likelihood, the deviance information criterion and the Watanabe-Akaike\ninformation criterion. The accelerated failure time frailty model with a\ncovariate-dependent baseline is included in the function frailtyGAFT. In\naddition, the package also provides two marginal survival models: proportional\nhazards and linear dependent Dirichlet process mixture, where the spatial\ndependence is modeled via spatial copulas. Note that the package can also\nhandle non-spatial data using non-spatial versions of aforementioned models. \n\n"}
{"id": "1705.05955", "contents": "Title: ExoMol Line List XXI: Nitric Oxide (NO) Abstract: Line lists for the ground electronic ground state for six major isotopologues\nof nitric oxide are presented. The line lists are constructed using empirical\nenergy levels (and line positions) and high-level {\\it ab inito} intensities.\nThe energy levels were obtained using a combination of two approaches, from an\neffective Hamiltonian and from solving the rovibronic Schr\\\"{o}dinger equation\nvariationally. The effective hamiltonian model was obtained through a fit to\nthe experimental line positions of NO available in the literature for all six\nisotopologues using the programs SPFIT and SPCAT. The variational model was\nbuilt through a least squares fit of the \\textit{ab inito} potential and\nspin-orbit curves to the experimentally derived energies and experimental line\npositions of the main isotopologue only using the Duo program. The \\textit{ab\ninito} potential energy, spin-orbit and dipole moment curves (PEC, SOC and DMC)\nare computed using high-level {\\it ab inito} methods and the MARVEL method is\nused to obtain energies of NO from experimental transition frequencies. Each\nline list covers a wavenumber range from 0 - 40,000 \\cm with approximately\n22,000 rovibronic states and 2.3-2.6 million transitions extending to $J_{max}\n= 184.5$ and $v_{max} = 51$. Partition functions are also calculated up to a\ntemperature of 5000 K. The calculated absorption line intensities at 296 K\nusing these line lists show excellent agreement with those included in the\nHITRAN and HITEMP databases. The computed NO line lists are the most\ncomprehensive to date, covering a wider wavenumber and temperature range\ncompared to both the HITRAN and HITEMP databases. These line lists are also\nmore accurate than those used in HITEMP. The full line lists are available from\nthe CDS and ExoMol databases; data will also be available from CDMS. \n\n"}
{"id": "1705.07598", "contents": "Title: Rao-Blackwellized Particle Smoothing as Message Passing Abstract: In this manuscript the fixed-lag smoothing problem for conditionally linear\nGaussian state-space models is investigated from a factor graph perspective.\nMore specifically, after formulating Bayesian smoothing for an arbitrary\nstate-space model as forward-backward message passing over a factor graph, we\nfocus on the above mentioned class of models and derive a novel\nRao-Blackwellized particle smoother for it. Then, we show how our technique can\nbe modified to estimate a point mass approximation of the so called joint\nsmoothing distribution. Finally, the estimation accuracy and the computational\nrequirements of our smoothing algorithms are analysed for a specific\nstate-space model. \n\n"}
{"id": "1705.07646", "contents": "Title: An approximate empirical Bayesian method for large-scale linear-Gaussian\n  inverse problems Abstract: We study Bayesian inference methods for solving linear inverse problems,\nfocusing on hierarchical formulations where the prior or the likelihood\nfunction depend on unspecified hyperparameters. In practice, these\nhyperparameters are often determined via an empirical Bayesian method that\nmaximizes the marginal likelihood function, i.e., the probability density of\nthe data conditional on the hyperparameters. Evaluating the marginal\nlikelihood, however, is computationally challenging for large-scale problems.\nIn this work, we present a method to approximately evaluate marginal likelihood\nfunctions, based on a low-rank approximation of the update from the prior\ncovariance to the posterior covariance. We show that this approximation is\noptimal in a minimax sense. Moreover, we provide an efficient algorithm to\nimplement the proposed method, based on a combination of the randomized SVD and\na spectral approximation method to compute square roots of the prior covariance\nmatrix. Several numerical examples demonstrate good performance of the proposed\nmethod. \n\n"}
{"id": "1705.07880", "contents": "Title: Reducing Reparameterization Gradient Variance Abstract: Optimization with noisy gradients has become ubiquitous in statistics and\nmachine learning. Reparameterization gradients, or gradient estimates computed\nvia the \"reparameterization trick,\" represent a class of noisy gradients often\nused in Monte Carlo variational inference (MCVI). However, when these gradient\nestimators are too noisy, the optimization procedure can be slow or fail to\nconverge. One way to reduce noise is to use more samples for the gradient\nestimate, but this can be computationally expensive. Instead, we view the noisy\ngradient as a random variable, and form an inexpensive approximation of the\ngenerating procedure for the gradient sample. This approximation has high\ncorrelation with the noisy gradient by construction, making it a useful control\nvariate for variance reduction. We demonstrate our approach on non-conjugate\nmulti-level hierarchical models and a Bayesian neural net where we observed\ngradient variance reductions of multiple orders of magnitude (20-2,000x). \n\n"}
{"id": "1705.08931", "contents": "Title: Proximity Variational Inference Abstract: Variational inference is a powerful approach for approximate posterior\ninference. However, it is sensitive to initialization and can be subject to\npoor local optima. In this paper, we develop proximity variational inference\n(PVI). PVI is a new method for optimizing the variational objective that\nconstrains subsequent iterates of the variational parameters to robustify the\noptimization path. Consequently, PVI is less sensitive to initialization and\noptimization quirks and finds better local optima. We demonstrate our method on\nthree proximity statistics. We study PVI on a Bernoulli factor model and\nsigmoid belief network with both real and synthetic data and compare to\ndeterministic annealing (Katahira et al., 2008). We highlight the flexibility\nof PVI by designing a proximity statistic for Bayesian deep learning models\nsuch as the variational autoencoder (Kingma and Welling, 2014; Rezende et al.,\n2014). Empirically, we show that PVI consistently finds better local optima and\ngives better predictive performance. \n\n"}
{"id": "1705.09395", "contents": "Title: Optimal Experimental Design Using A Consistent Bayesian Approach Abstract: We consider the utilization of a computational model to guide the optimal\nacquisition of experimental data to inform the stochastic description of model\ninput parameters. Our formulation is based on the recently developed consistent\nBayesian approach for solving stochastic inverse problems which seeks a\nposterior probability density that is consistent with the model and the data in\nthe sense that the push-forward of the posterior (through the computational\nmodel) matches the observed density on the observations almost everywhere.\nGiven a set a potential observations, our optimal experimental design (OED)\nseeks the observation, or set of observations, that maximizes the expected\ninformation gain from the prior probability density on the model parameters. We\ndiscuss the characterization of the space of observed densities and a\ncomputationally efficient approach for rescaling observed densities to satisfy\nthe fundamental assumptions of the consistent Bayesian approach. Numerical\nresults are presented to compare our approach with existing OED methodologies\nusing the classical/statistical Bayesian approach and to demonstrate our OED on\na set of representative PDE-based models. \n\n"}
{"id": "1705.09831", "contents": "Title: A well-balanced meshless tsunami propagation and inundation model Abstract: We present a novel meshless tsunami propagation and inundation model. We\ndiscretize the nonlinear shallow-water equations using a well-balanced scheme\nrelying on radial basis function based finite differences. The inundation model\nrelies on radial basis function generated extrapolation from the wet points\nclosest to the wet-dry interface into the dry region. Numerical results against\nstandard one- and two-dimensional benchmarks are presented. \n\n"}
{"id": "1705.09874", "contents": "Title: Targeted Learning with Daily EHR Data Abstract: Electronic health records (EHR) data provide a cost and time-effective\nopportunity to conduct cohort studies of the effects of multiple time-point\ninterventions in the diverse patient population found in real-world clinical\nsettings. Because the computational cost of analyzing EHR data at daily (or\nmore granular) scale can be quite high, a pragmatic approach has been to\npartition the follow-up into coarser intervals of pre-specified length. Current\nguidelines suggest employing a 'small' interval, but the feasibility and\npractical impact of this recommendation has not been evaluated and no formal\nmethodology to inform this choice has been developed. We start filling these\ngaps by leveraging large-scale EHR data from a diabetes study to develop and\nillustrate a fast and scalable targeted learning approach that allows to follow\nthe current recommendation and study its practical impact on inference. More\nspecifically, we map daily EHR data into four analytic datasets using 90, 30,\n15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation\napproach, the longitudinal TMLE, to estimate the causal effects of four dynamic\ntreatment rules with each dataset, and compare the resulting inferences. To\novercome the computational challenges presented by the size of these data, we\npropose a novel TMLE implementation, the 'long-format TMLE', and rely on the\nlatest advances in scalable data-adaptive machine-learning software, xgboost\nand h2o, for estimation of the TMLE nuisance parameters. \n\n"}
{"id": "1705.10347", "contents": "Title: An effective likelihood-free approximate computing method with\n  statistical inferential guarantees Abstract: Approximate Bayesian computing is a powerful likelihood-free method that has\ngrown increasingly popular since early applications in population genetics.\nHowever, complications arise in the theoretical justification for Bayesian\ninference conducted from this method with a non-sufficient summary statistic.\nIn this paper, we seek to re-frame approximate Bayesian computing within a\nfrequentist context and justify its performance by standards set on the\nfrequency coverage rate. In doing so, we develop a new computational technique\ncalled approximate confidence distribution computing, yielding theoretical\nsupport for the use of non-sufficient summary statistics in likelihood-free\nmethods. Furthermore, we demonstrate that approximate confidence distribution\ncomputing extends the scope of approximate Bayesian computing to include\ndata-dependent priors without damaging the inferential integrity. This\ndata-dependent prior can be viewed as an initial `distribution estimate' of the\ntarget parameter which is updated with the results of the approximate\nconfidence distribution computing method. A general strategy for constructing\nan appropriate data-dependent prior is also discussed and is shown to often\nincrease the computing speed while maintaining statistical inferential\nguarantees. We supplement the theory with simulation studies illustrating the\nbenefits of the proposed method, namely the potential for broader applications\nand the increased computing speed compared to the standard approximate Bayesian\ncomputing methods. \n\n"}
{"id": "1705.10376", "contents": "Title: Conducting Simulations in Causal Inference with Networks-Based\n  Structural Equation Models Abstract: The past decade has seen an increasing body of literature devoted to the\nestimation of causal effects in network-dependent data. However, the validity\nof many classical statistical methods in such data is often questioned. There\nis an emerging need for objective and practical ways to assess which causal\nmethodologies might be applicable and valid in network-dependent data. This\npaper describes a set of tools implemented in the simcausal R package that\nallow simulating data based on user-specified structural equation model for\nconnected units. Specification and simulation of counterfactual data is\nimplemented for static, dynamic and stochastic interventions. A new interface\naims to simplify the specification of network-based functional relationships\nbetween connected units. A set of examples illustrates how these simulations\nmay be applied to evaluation of different statistical methods for estimation of\ncausal effects in network-dependent data. \n\n"}
{"id": "1705.11140", "contents": "Title: Variational Sequential Monte Carlo Abstract: Many recent advances in large scale probabilistic inference rely on\nvariational methods. The success of variational approaches depends on (i)\nformulating a flexible parametric family of distributions, and (ii) optimizing\nthe parameters to find the member of this family that most closely approximates\nthe exact posterior. In this paper we present a new approximating family of\ndistributions, the variational sequential Monte Carlo (VSMC) family, and show\nhow to optimize it in variational inference. VSMC melds variational inference\n(VI) and sequential Monte Carlo (SMC), providing practitioners with flexible,\naccurate, and powerful Bayesian inference. The VSMC family is a variational\nfamily that can approximate the posterior arbitrarily well, while still\nallowing for efficient optimization of its parameters. We demonstrate its\nutility on state space models, stochastic volatility models for financial data,\nand deep Markov models of brain neural circuits. \n\n"}
{"id": "1706.00098", "contents": "Title: Bayesian $l_0$-regularized Least Squares Abstract: Bayesian $l_0$-regularized least squares is a variable selection technique\nfor high dimensional predictors. The challenge is optimizing a non-convex\nobjective function via search over model space consisting of all possible\npredictor combinations. Spike-and-slab (a.k.a. Bernoulli-Gaussian) priors are\nthe gold standard for Bayesian variable selection, with a caveat of\ncomputational speed and scalability. Single Best Replacement (SBR) provides a\nfast scalable alternative. We provide a link between Bayesian regularization\nand proximal updating, which provides an equivalence between finding a\nposterior mode and a posterior mean with a different regularization prior. This\nallows us to use SBR to find the spike-and-slab estimator. To illustrate our\nmethodology, we provide simulation evidence and a real data example on the\nstatistical properties and computational efficiency of SBR versus direct\nposterior sampling using spike-and-slab priors. Finally, we conclude with\ndirections for future research. \n\n"}
{"id": "1706.01260", "contents": "Title: The Classical Complexity of Boson Sampling Abstract: We study the classical complexity of the exact Boson Sampling problem where\nthe objective is to produce provably correct random samples from a particular\nquantum mechanical distribution. The computational framework was proposed by\nAaronson and Arkhipov in 2011 as an attainable demonstration of `quantum\nsupremacy', that is a practical quantum computing experiment able to produce\noutput at a speed beyond the reach of classical (that is non-quantum) computer\nhardware. Since its introduction Boson Sampling has been the subject of intense\ninternational research in the world of quantum computing. On the face of it,\nthe problem is challenging for classical computation. Aaronson and Arkhipov\nshow that exact Boson Sampling is not efficiently solvable by a classical\ncomputer unless $P^{\\#P} = BPP^{NP}$ and the polynomial hierarchy collapses to\nthe third level.\n  The fastest known exact classical algorithm for the standard Boson Sampling\nproblem takes $O({m + n -1 \\choose n} n 2^n )$ time to produce samples for a\nsystem with input size $n$ and $m$ output modes, making it infeasible for\nanything but the smallest values of $n$ and $m$. We give an algorithm that is\nmuch faster, running in $O(n 2^n + \\operatorname{poly}(m,n))$ time and $O(m)$\nadditional space. The algorithm is simple to implement and has low constant\nfactor overheads. As a consequence our classical algorithm is able to solve the\nexact Boson Sampling problem for system sizes far beyond current photonic\nquantum computing experimentation, thereby significantly reducing the\nlikelihood of achieving near-term quantum supremacy in the context of Boson\nSampling. \n\n"}
{"id": "1706.01478", "contents": "Title: An optimal $(\\epsilon,\\delta)$-approximation scheme for the mean of\n  random variables with bounded relative variance Abstract: Randomized approximation algorithms for many #P-complete problems (such as\nthe partition function of a Gibbs distribution, the volume of a convex body,\nthe permanent of a $\\{0,1\\}$-matrix, and many others) reduce to creating random\nvariables $X_1,X_2,\\ldots$ with finite mean $\\mu$ and standard\ndeviation$\\sigma$ such that $\\mu$ is the solution for the problem input, and\nthe relative standard deviation $|\\sigma/\\mu| \\leq c$ for known $c$. Under\nthese circumstances, it is known that the number of samples from the $\\{X_i\\}$\nneeded to form an $(\\epsilon,\\delta)$-approximation $\\hat \\mu$ that satisfies\n$\\mathbb{P}(|\\hat \\mu - \\mu| > \\epsilon \\mu) \\leq \\delta$ is at least\n$(2-o(1))\\epsilon^{-2} c^2\\ln(1/\\delta)$. We present here an easy to implement\n$(\\epsilon,\\delta)$-approximation $\\hat \\mu$ that uses\n$(2+o(1))c^2\\epsilon^{-2}\\ln(1/\\delta)$ samples. This achieves the same optimal\nrunning time as other estimators, but without the need for extra conditions\nsuch as bounds on third or fourth moments. \n\n"}
{"id": "1706.01724", "contents": "Title: Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic\n  Gradient Riemannian MCMC Abstract: It is challenging to develop stochastic gradient based scalable inference for\ndeep discrete latent variable models (LVMs), due to the difficulties in not\nonly computing the gradients, but also adapting the step sizes to different\nlatent factors and hidden layers. For the Poisson gamma belief network (PGBN),\na recently proposed deep discrete LVM, we derive an alternative representation\nthat is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data\naugmentation and marginalization techniques, we derive a block-diagonal Fisher\ninformation matrix and its inverse for the simplex-constrained global model\nparameters of DLDA. Exploiting that Fisher information matrix with stochastic\ngradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian\n(TLASGR) MCMC that jointly learns simplex-constrained global parameters across\nall layers and topics, with topic and layer specific learning rates.\nState-of-the-art results are demonstrated on big data sets. \n\n"}
{"id": "1706.02380", "contents": "Title: Multi-sample Estimation of Bacterial Composition Matrix in Metagenomics\n  Data Abstract: Metagenomics sequencing is routinely applied to quantify bacterial abundances\nin microbiome studies, where the bacterial composition is estimated based on\nthe sequencing read counts. Due to limited sequencing depth and DNA dropouts,\nmany rare bacterial taxa might not be captured in the final sequencing reads,\nwhich results in many zero counts. Naive composition estimation using count\nnormalization leads to many zero proportions, which tend to result in\ninaccurate estimates of bacterial abundance and diversity. This paper takes a\nmulti-sample approach to the estimation of bacterial abundances in order to\nborrow information across samples and across species. Empirical results from\nreal data sets suggest that the composition matrix over multiple samples is\napproximately low rank, which motivates a regularized maximum likelihood\nestimation with a nuclear norm penalty. An efficient optimization algorithm\nusing the generalized accelerated proximal gradient and Euclidean projection\nonto simplex space is developed. The theoretical upper bounds and the minimax\nlower bounds of the estimation errors, measured by the Kullback-Leibler\ndivergence and the Frobenius norm, are established. Simulation studies\ndemonstrate that the proposed estimator outperforms the naive estimators. The\nmethod is applied to an analysis of a human gut microbiome dataset. \n\n"}
{"id": "1706.02808", "contents": "Title: A randomized Halton algorithm in R Abstract: Randomized quasi-Monte Carlo (RQMC) sampling can bring orders of magnitude\nreduction in variance compared to plain Monte Carlo (MC) sampling. The extent\nof the efficiency gain varies from problem to problem and can be hard to\npredict. This article presents an R function rhalton that produces scrambled\nversions of Halton sequences. On some problems it brings efficiency gains of\nseveral thousand fold. On other problems, the efficiency gain is minor. The\ncode is designed to make it easy to determine whether a given integrand will\nbenefit from RQMC sampling. An RQMC sample of n points in $[0,1]^d$ can be\nextended later to a larger n and/or d. \n\n"}
{"id": "1706.02952", "contents": "Title: TIP: Typifying the Interpretability of Procedures Abstract: We provide a novel notion of what it means to be interpretable, looking past\nthe usual association with human understanding. Our key insight is that\ninterpretability is not an absolute concept and so we define it relative to a\ntarget model, which may or may not be a human. We define a framework that\nallows for comparing interpretable procedures by linking them to important\npractical aspects such as accuracy and robustness. We characterize many of the\ncurrent state-of-the-art interpretable methods in our framework portraying its\ngeneral applicability. Finally, principled interpretable strategies are\nproposed and empirically evaluated on synthetic data, as well as on the largest\npublic olfaction dataset that was made recently available \\cite{olfs}. We also\nexperiment on MNIST with a simple target model and different oracle models of\nvarying complexity. This leads to the insight that the improvement in the\ntarget model is not only a function of the oracle model's performance, but also\nits relative complexity with respect to the target model. Further experiments\non CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit\nof our methods over Knowledge Distillation when the target models are simple\nand the complex model is a neural network. \n\n"}
{"id": "1706.03188", "contents": "Title: Radiative Transfer for Exoplanet Atmospheres Abstract: Remote sensing of the atmospheres of distant worlds motivates a firm\nunderstanding of radiative transfer. In this review, we provide a pedagogical\ncookbook that describes the principal ingredients needed to perform a radiative\ntransfer calculation and predict the spectrum of an exoplanet atmosphere,\nincluding solving the radiative transfer equation, calculating opacities (and\nchemistry), iterating for radiative equilibrium (or not), and adapting the\noutput of the calculations to the astronomical observations. A review of the\nstate of the art is performed, focusing on selected milestone papers.\nOutstanding issues, including the need to understand aerosols or clouds and\nelucidating the assumptions and caveats behind inversion methods, are\ndiscussed. A checklist is provided to assist referees/reviewers in their\nscrutiny of works involving radiative transfer. A table summarizing the\nmethodology employed by past studies is provided. \n\n"}
{"id": "1706.04606", "contents": "Title: Nudged elastic band calculations accelerated with Gaussian process\n  regression Abstract: Minimum energy paths for transitions such as atomic and/or spin\nrearrangements in thermalized systems are the transition paths of largest\nstatistical weight. Such paths are frequently calculated using the nudged\nelastic band method, where an initial path is iteratively shifted to the\nnearest minimum energy path. The computational effort can be large, especially\nwhen ab initio or electron density functional calculations are used to evaluate\nthe energy and atomic forces. Here, we show how the number of such evaluations\ncan be reduced by an order of magnitude using a Gaussian process regression\napproach where an approximate energy surface is generated and refined in each\niteration. When the goal is to evaluate the transition rate within harmonic\ntransition state theory, the evaluation of the Hessian matrix at the initial\nand final state minima can be carried out beforehand and used as input in the\nminimum energy path calculation, thereby improving stability and reducing the\nnumber of iterations needed for convergence. A Gaussian process model also\nprovides an uncertainty estimate for the approximate energy surface, and this\ncan be used to focus the calculations on the lesser-known part of the path,\nthereby reducing the number of needed energy and force evaluations to a half in\nthe present calculations. The methodology is illustrated using the\ntwo-dimensional M\\\"uller-Brown potential surface and performance assessed on an\nestablished benchmark involving 13 rearrangement transitions of a heptamer\nisland on a solid surface. \n\n"}
{"id": "1706.04834", "contents": "Title: A coherent structure approach for parameter estimation in Lagrangian\n  Data Assimilation Abstract: We introduce a data assimilation method to estimate model parameters with\nobservations of passive tracers by directly assimilating Lagrangian Coherent\nStructures. Our approach differs from the usual Lagrangian Data Assimilation\napproach, where parameters are estimated based on tracer trajectories. We\nemploy the Approximate Bayesian Computation (ABC) framework to avoid computing\nthe likelihood function of the coherent structure, which is usually\nunavailable. We solve the ABC by a Sequential Monte Carlo (SMC) method, and use\nPrincipal Component Analysis (PCA) to identify the coherent patterns from\ntracer trajectory data. Our new method shows remarkably improved results\ncompared to the bootstrap particle filter when the physical model exhibits\nchaotic advection. \n\n"}
{"id": "1706.05305", "contents": "Title: Sequential quasi-Monte Carlo: Introduction for Non-Experts, Dimension\n  Reduction, Application to Partly Observed Diffusion Processes Abstract: SMC (Sequential Monte Carlo) is a class of Monte Carlo algorithms for\nfiltering and related sequential problems. Gerber and Chopin (2015) introduced\nSQMC (Sequential quasi-Monte Carlo), a QMC version of SMC. This paper has two\nobjectives: (a) to introduce Sequential Monte Carlo to the QMC community, whose\nmembers are usually less familiar with state-space models and particle\nfiltering; (b) to extend SQMC to the filtering of continuous-time state-space\nmodels, where the latent process is a diffusion. A recurring point in the paper\nwill be the notion of dimension reduction, that is how to implement SQMC in\nsuch a way that it provides good performance despite the high dimension of the\nproblem. \n\n"}
{"id": "1706.07564", "contents": "Title: Least Squares Polynomial Chaos Expansion: A Review of Sampling\n  Strategies Abstract: As non-institutive polynomial chaos expansion (PCE) techniques have gained\ngrowing popularity among researchers, we here provide a comprehensive review of\nmajor sampling strategies for the least squares based PCE. Traditional sampling\nmethods, such as Monte Carlo, Latin hypercube, quasi-Monte Carlo, optimal\ndesign of experiments (ODE), Gaussian quadratures, as well as more recent\ntechniques, such as coherence-optimal and randomized quadratures are discussed.\nWe also propose a hybrid sampling method, dubbed alphabetic-coherence-optimal,\nthat employs the so-called alphabetic optimality criteria used in the context\nof ODE in conjunction with coherence-optimal samples. A comparison between the\nempirical performance of the selected sampling methods applied to three\nnumerical examples, including high-order PCE's, high-dimensional problems, and\nlow oversampling ratios, is presented to provide a road map for practitioners\nseeking the most suitable sampling technique for a problem at hand. We observed\nthat the alphabetic-coherence-optimal technique outperforms other sampling\nmethods, specially when high-order ODE are employed and/or the oversampling\nratio is low. \n\n"}
{"id": "1706.09693", "contents": "Title: Image classification using local tensor singular value decompositions Abstract: From linear classifiers to neural networks, image classification has been a\nwidely explored topic in mathematics, and many algorithms have proven to be\neffective classifiers. However, the most accurate classifiers typically have\nsignificantly high storage costs, or require complicated procedures that may be\ncomputationally expensive. We present a novel (nonlinear) classification\napproach using truncation of local tensor singular value decompositions (tSVD)\nthat robustly offers accurate results, while maintaining manageable storage\ncosts. Our approach takes advantage of the optimality of the representation\nunder the tensor algebra described to determine to which class an image\nbelongs. We extend our approach to a method that can determine specific\npairwise match scores, which could be useful in, for example, object\nrecognition problems where pose/position are different. We demonstrate the\npromise of our new techniques on the MNIST data set. \n\n"}
{"id": "1706.09873", "contents": "Title: Importance sampling correction versus standard averages of reversible\n  MCMCs in terms of the asymptotic variance Abstract: We establish an ordering criterion for the asymptotic variances of two\nconsistent Markov chain Monte Carlo (MCMC) estimators: an importance sampling\n(IS) estimator, based on an approximate reversible chain and subsequent IS\nweighting, and a standard MCMC estimator, based on an exact reversible chain.\nEssentially, we relax the criterion of the Peskun type covariance ordering by\nconsidering two different invariant probabilities, and obtain, in place of a\nstrict ordering of asymptotic variances, a bound of the asymptotic variance of\nIS by that of the direct MCMC. Simple examples show that IS can have\narbitrarily better or worse asymptotic variance than Metropolis-Hastings and\ndelayed-acceptance (DA) MCMC. Our ordering implies that IS is guaranteed to be\ncompetitive up to a factor depending on the supremum of the (marginal) IS\nweight. We elaborate upon the criterion in case of unbiased estimators as part\nof an auxiliary variable framework. We show how the criterion implies\nasymptotic variance guarantees for IS in terms of pseudo-marginal (PM) and DA\ncorrections, essentially if the ratio of exact and approximate likelihoods is\nbounded. We also show that convergence of the IS chain can be less affected by\nunbounded high-variance unbiased estimators than PM and DA chains. \n\n"}
{"id": "1706.09888", "contents": "Title: Fast model-fitting of Bayesian variable selection regression using the\n  iterative complex factorization algorithm Abstract: Bayesian variable selection regression (BVSR) is able to jointly analyze\ngenome-wide genetic datasets, but the slow computation via Markov chain Monte\nCarlo (MCMC) hampered its wide-spread usage. Here we present a novel iterative\nmethod to solve a special class of linear systems, which can increase the speed\nof the BVSR model-fitting tenfold. The iterative method hinges on the complex\nfactorization of the sum of two matrices and the solution path resides in the\ncomplex domain (instead of the real domain). Compared to the Gauss-Seidel\nmethod, the complex factorization converges almost instantaneously and its\nerror is several magnitude smaller than that of the Gauss-Seidel method. More\nimportantly, the error is always within the pre-specified precision while the\nGauss-Seidel method is not. For large problems with thousands of covariates,\nthe complex factorization is 10 -- 100 times faster than either the\nGauss-Seidel method or the direct method via the Cholesky decomposition. In\nBVSR, one needs to repetitively solve large penalized regression systems whose\ndesign matrices only change slightly between adjacent MCMC steps. This slight\nchange in design matrix enables the adaptation of the iterative complex\nfactorization method. The computational innovation will facilitate the\nwide-spread use of BVSR in reanalyzing genome-wide association datasets. \n\n"}
{"id": "1707.03307", "contents": "Title: Fast calibrated additive quantile regression Abstract: We propose a novel framework for fitting additive quantile regression models,\nwhich provides well calibrated inference about the conditional quantiles and\nfast automatic estimation of the smoothing parameters, for model structures as\ndiverse as those usable with distributional GAMs, while maintaining equivalent\nnumerical efficiency and stability. The proposed methods are at once\nstatistically rigorous and computationally efficient, because they are based on\nthe general belief updating framework of Bissiri et al. (2016) to loss based\ninference, but compute by adapting the stable fitting methods of Wood et al.\n(2016). We show how the pinball loss is statistically suboptimal relative to a\nnovel smooth generalisation, which also gives access to fast estimation\nmethods. Further, we provide a novel calibration method for efficiently\nselecting the 'learning rate' balancing the loss with the smoothing priors\nduring inference, thereby obtaining reliable quantile uncertainty estimates.\nOur work was motivated by a probabilistic electricity load forecasting\napplication, used here to demonstrate the proposed approach. The methods\ndescribed here are implemented by the qgam R package, available on the\nComprehensive R Archive Network (CRAN). \n\n"}
{"id": "1707.04878", "contents": "Title: Bayesian nonparametric spectral density estimation using B-spline priors Abstract: We present a new Bayesian nonparametric approach to estimating the spectral\ndensity of a stationary time series. A nonparametric prior based on a mixture\nof B-spline distributions is specified and can be regarded as a generalization\nof the Bernstein polynomial prior of Petrone (1999a,b) and Choudhuri et al.\n(2004). Whittle's likelihood approximation is used to obtain the\npseudo-posterior distribution. This method allows for a data-driven choice of\nthe number of mixture components and the location of knots. Posterior samples\nare obtained using a Metropolis-within-Gibbs Markov chain Monte Carlo\nalgorithm, and mixing is improved using parallel tempering. We conduct a\nsimulation study to demonstrate that for complicated spectral densities, the\nB-spline prior provides more accurate Monte Carlo estimates in terms of\n$L_1$-error and uniform coverage probabilities than the Bernstein polynomial\nprior. We apply the algorithm to annual mean sunspot data to estimate the solar\ncycle. Finally, we demonstrate the algorithm's ability to estimate a spectral\ndensity with sharp features, using real gravitational wave detector data from\nLIGO's sixth science run, recoloured to match the Advanced LIGO target\nsensitivity. \n\n"}
{"id": "1707.05200", "contents": "Title: A Discrete Bouncy Particle Sampler Abstract: Most Markov chain Monte Carlo methods operate in discrete time and are\nreversible with respect to the target probability. Nevertheless, it is now\nunderstood that the use of non-reversible Markov chains can be beneficial in\nmany contexts. In particular, the recently-proposed Bouncy Particle Sampler\nleverages a continuous-time and non-reversible Markov process and empirically\nshows state-of-the-art performances when used to explore certain probability\ndensities; however, its implementation typically requires the computation of\nlocal upper bounds on the gradient of the log target density.\n  We present the Discrete Bouncy Particle Sampler, a general algorithm based\nupon a guided random walk, a partial refreshment of direction, and a\ndelayed-rejection step. We show that the Bouncy Particle Sampler can be\nunderstood as a scaling limit of a special case of our algorithm. In contrast\nto the Bouncy Particle Sampler, implementing the Discrete Bouncy Particle\nSampler only requires point-wise evaluation of the target density and its\ngradient. We propose extensions of the basic algorithm for situations when the\nexact gradient of the target density is not available. In a Gaussian setting,\nwe establish a scaling limit for the radial process as dimension increases to\ninfinity. We leverage this result to obtain the theoretical efficiency of the\nDiscrete Bouncy Particle Sampler as a function of the partial-refreshment\nparameter, which leads to a simple and robust tuning criterion. A further\nanalysis in a more general setting suggests that this tuning criterion applies\nmore generally. Theoretical and empirical efficiency curves are then compared\nfor different targets and algorithm variations. \n\n"}
{"id": "1707.05659", "contents": "Title: The finite gap method and the analytic description of the exact rogue\n  wave recurrence in the periodic NLS Cauchy problem. 1 Abstract: The focusing NLS equation is the simplest universal model describing the\nmodulation instability (MI) of quasi monochromatic waves in weakly nonlinear\nmedia, considered the main physical mechanism for the appearance of rogue\n(anomalous) waves (RWs) in Nature. In this paper we study, using the finite gap\nmethod, the NLS Cauchy problem for periodic initial perturbations of the\nunstable background solution of NLS exciting just one of the unstable modes. We\ndistinguish two cases. In the case in which only the corresponding unstable gap\nis theoretically open, the solution describes an exact deterministic alternate\nrecurrence of linear and nonlinear stages of MI, and the nonlinear RW stages\nare described by the 1-breather Akhmediev solution, whose parameters, different\nat each RW appearance, are always given in terms of the initial data through\nelementary functions. If the number of unstable modes is >1, this uniform in t\ndynamics is sensibly affected by perturbations due to numerics and/or real\nexperiments, provoking O(1) corrections to the result. In the second case in\nwhich more than one unstable gap is open, a detailed investigation of all these\ngaps is necessary to get a uniform in $t$ dynamics, and this study is postponed\nto a subsequent paper. It is however possible to obtain the elementary\ndescription of the first nonlinear stage of MI, given again by the Akhmediev\n1-breather solution, and how perturbations due to numerics and/or real\nexperiments can affect this result. \n\n"}
{"id": "1707.05861", "contents": "Title: On Adaptive Propensity Score Truncation in Causal Inference Abstract: The positivity assumption, or the experimental treatment assignment (ETA)\nassumption, is important for identifiability in causal inference. Even if the\npositivity assumption holds, practical violations of this assumption may\njeopardize the finite sample performance of the causal estimator. One of the\nconsequences of practical violations of the positivity assumption is extreme\nvalues in the estimated propensity score (PS). A common practice to address\nthis issue is truncating the PS estimate when constructing PS-based estimators.\nIn this study, we propose a novel adaptive truncation method,\nPositivity-C-TMLE, based on the collaborative targeted maximum likelihood\nestimation (C-TMLE) methodology. We demonstrate the outstanding performance of\nour novel approach in a variety of simulations by comparing it with other\ncommonly studied estimators. Results show that by adaptively truncating the\nestimated PS with a more targeted objective function, the Positivity-C-TMLE\nestimator achieves the best performance for both point estimation and\nconfidence interval coverage among all estimators considered. \n\n"}
{"id": "1707.06360", "contents": "Title: Common and Individual Structure of Brain Networks Abstract: This article focuses on the problem of studying shared- and\nindividual-specific structure in replicated networks or graph-valued data. In\nparticular, the observed data consist of $n$ graphs, $G_i, i=1,\\ldots,n$, with\neach graph consisting of a collection of edges between $V$ nodes. In brain\nconnectomics, the graph for an individual corresponds to a set of\ninterconnections among brain regions. Such data can be organized as a $V \\times\nV$ binary adjacency matrix $A_i$ for each $i$, with ones indicating an edge\nbetween a pair of nodes and zeros indicating no edge. When nodes have a shared\nmeaning across replicates $i=1,\\ldots,n$, it becomes of substantial interest to\nstudy similarities and differences in the adjacency matrices. To address this\nproblem, we propose a method to estimate a common structure and low-dimensional\nindividual-specific deviations from replicated networks. The proposed Multiple\nGRAph Factorization (M-GRAF) model relies on a logistic regression mapping\ncombined with a hierarchical eigenvalue decomposition. We develop an efficient\nalgorithm for estimation and study basic properties of our approach. Simulation\nstudies show excellent operating characteristics and we apply the method to\nhuman brain connectomics data. \n\n"}
{"id": "1707.08220", "contents": "Title: Bayesian hierarchical weighting adjustment and survey inference Abstract: We combine Bayesian prediction and weighted inference as a unified approach\nto survey inference. The general principles of Bayesian analysis imply that\nmodels for survey outcomes should be conditional on all variables that affect\nthe probability of inclusion. We incorporate the weighting variables under the\nframework of multilevel regression and poststratification, as a byproduct\ngenerating model-based weights after smoothing. We investigate deep\ninteractions and introduce structured prior distributions for smoothing and\nstability of estimates. The computation is done via Stan and implemented in the\nopen source R package \"rstanarm\" ready for public use. Simulation studies\nillustrate that model-based prediction and weighting inference outperform\nclassical weighting. We apply the proposal to the New York Longitudinal Study\nof Wellbeing. The new approach generates robust weights and increases\nefficiency for finite population inference, especially for subsets of the\npopulation. \n\n"}
{"id": "1707.08513", "contents": "Title: Markov Chain Monte Carlo sampling for conditional tests: A link between\n  permutation tests and algebraic statistics Abstract: We consider conditional tests for non-negative discrete exponential families.\nWe develop two Markov Chain Monte Carlo (MCMC) algorithms which allow us to\nsample from the conditional space and to perform approximated tests. The first\nalgorithm is based on the MCMC sampling described by Sturmfels. The second MCMC\nsampling consists in a more efficient algorithm which exploits the optimal\npartition of the conditional space into orbits of permutations. We thus\nestablish a link between standard permutation and algebraic-statistics-based\nsampling. Through a simulation study we compare the exact cumulative\ndistribution function (cdf) with the approximated cdfs which are obtained with\nthe two MCMC samplings and the standard permutation sampling. We conclude that\nthe MCMC sampling which exploits the partition of the conditional space into\norbits of permutations gives an estimated cdf, under $H_0$, which is more\nreliable and converges to the exact cdf with the least steps. This sampling\ntechnique can also be used to build an approximation of the exact cdf when its\nexact computation is computationally infeasible. \n\n"}
{"id": "1707.09705", "contents": "Title: Mini-batch Tempered MCMC Abstract: In this paper we propose a general framework of performing MCMC with only a\nmini-batch of data. We show by estimating the Metropolis-Hasting ratio with\nonly a mini-batch of data, one is essentially sampling from the true posterior\nraised to a known temperature. We show by experiments that our method,\nMini-batch Tempered MCMC (MINT-MCMC), can efficiently explore multiple modes of\na posterior distribution. Based on the Equi-Energy sampler (Kou et al. 2006),\nwe developed a new parallel MCMC algorithm based on the Equi-Energy sampler,\nwhich enables efficient sampling from high-dimensional multi-modal posteriors\nwith well separated modes. \n\n"}
{"id": "1708.00257", "contents": "Title: Robust PCA by Manifold Optimization Abstract: Robust PCA is a widely used statistical procedure to recover a underlying\nlow-rank matrix with grossly corrupted observations. This work considers the\nproblem of robust PCA as a nonconvex optimization problem on the manifold of\nlow-rank matrices, and proposes two algorithms (for two versions of\nretractions) based on manifold optimization. It is shown that, with a proper\ndesigned initialization, the proposed algorithms are guaranteed to converge to\nthe underlying low-rank matrix linearly. Compared with a previous work based on\nthe Burer-Monterio decomposition of low-rank matrices, the proposed algorithms\nreduce the dependence on the conditional number of the underlying low-rank\nmatrix theoretically. Simulations and real data examples confirm the\ncompetitive performance of our method. \n\n"}
{"id": "1708.00427", "contents": "Title: Fast Exact Conformalization of Lasso using Piecewise Linear Homotopy Abstract: Conformal prediction is a general method that converts almost any point\npredictor to a prediction set. The resulting set keeps good statistical\nproperties of the original estimator under standard assumptions, and guarantees\nvalid average coverage even when the model is misspecified. A main challenge in\napplying conformal prediction in modern applications is efficient computation,\nas it generally requires an exhaustive search over the entire output space. In\nthis paper we develop an exact and computationally efficient conformalization\nof the Lasso and elastic net. The method makes use of a novel piecewise linear\nhomotopy of the Lasso solution under perturbation of a single input sample\npoint. As a by-product, we provide a simpler and better justified online Lasso\nalgorithm, which may be of independent interest. Our derivation also reveals an\ninteresting accuracy-stability trade-off in conformal inference, which is\nanalogous to the bias-variance trade-off in traditional parameter estimation.\nThe practical performance of the new algorithm is demonstrated using both\nsynthetic and real data examples. \n\n"}
{"id": "1708.00762", "contents": "Title: Numerical instability of the Akhmediev breather and a finite-gap model\n  of it Abstract: In this paper we study the numerical instabilities of the NLS Akhmediev\nbreather, the simplest space periodic, one-mode perturbation of the unstable\nbackground, limiting our considerations to the simplest case of one unstable\nmode. In agreement with recent theoretical findings of the authors, in the\nsituation in which the round-off errors are negligible with respect to the\nperturbations due to the discrete scheme used in the numerical experiments, the\nsplit-step Fourier method (SSFM), the numerical output is well-described by a\nsuitable genus 2 finite-gap solution of NLS. This solution can be written in\nterms of different elementary functions in different time regions and,\nultimately, it shows an exact recurrence of rogue waves described, at each\nappearance, by the Akhmediev breather. We discover a remarkable empirical\nformula connecting the recurrence time with the number of time steps used in\nthe SSFM and, via our recent theoretical findings, we establish that the SSFM\nopens up a vertical unstable gap whose length can be computed with high\naccuracy, and is proportional to the inverse of the square of the number of\ntime steps used in the SSFM. This neat picture essentially changes when the\nround-off error is sufficiently large. Indeed experiments in standard double\nprecision show serious instabilities in both the periods and phases of the\nrecurrence. In contrast with it, as predicted by the theory, replacing the\nexact Akhmediev Cauchy datum by its first harmonic approximation, we only\nslightly modify the numerical output. Let us also remark, that the first rogue\nwave appearance is completely stable in all experiments and is in perfect\nagreement with the Akhmediev formula and with the theoretical prediction in\nterms of the Cauchy data. \n\n"}
{"id": "1708.00829", "contents": "Title: Complexity Results for MCMC derived from Quantitative Bounds Abstract: This paper considers how to obtain MCMC quantitative convergence bounds which\ncan be translated into tight complexity bounds in high-dimensional {settings}.\nWe propose a modified drift-and-minorization approach, which establishes\ngeneralized drift conditions defined in subsets of the state space. The subsets\nare called the \"large sets\", and are chosen to rule out some \"bad\" states which\nhave poor drift property when the dimension of the state space gets large.\nUsing the \"large sets\" together with a \"fitted family of drift functions\", a\nquantitative bound can be obtained which can be translated into a tight\ncomplexity bound. As a demonstration, we analyze several Gibbs samplers and\nobtain complexity upper bounds for the mixing time. In particular, for one\nexample of Gibbs sampler which is related to the James--Stein estimator, we\nshow that the number of iterations required for the Gibbs sampler to converge\nis constant under certain conditions on the observed data and the initial\nstate. It is our hope that this modified drift-and-minorization approach can be\nemployed in many other specific examples to obtain complexity bounds for\nhigh-dimensional Markov chains. \n\n"}
{"id": "1708.00842", "contents": "Title: Latent Parameter Estimation in Fusion Networks Using Separable\n  Likelihoods Abstract: Multi-sensor state space models underpin fusion applications in networks of\nsensors. Estimation of latent parameters in these models has the potential to\nprovide highly desirable capabilities such as network self-calibration.\nConventional solutions to the problem pose difficulties in scaling with the\nnumber of sensors due to the joint multi-sensor filtering involved when\nevaluating the parameter likelihood. In this article, we propose a separable\npseudo-likelihood which is a more accurate approximation compared to a\npreviously proposed alternative under typical operating conditions. In\naddition, we consider using separable likelihoods in the presence of many\nobjects and ambiguity in associating measurements with objects that originated\nthem. To this end, we use a state space model with a hypothesis based\nparameterisation, and, develop an empirical Bayesian perspective in order to\nevaluate separable likelihoods on this model using local filtering. Bayesian\ninference with this likelihood is carried out using belief propagation on the\nassociated pairwise Markov random field. We specify a particle algorithm for\nlatent parameter estimation in a linear Gaussian state space model and\ndemonstrate its efficacy for network self-calibration using measurements from\nnon-cooperative targets in comparison with alternatives. \n\n"}
{"id": "1708.00886", "contents": "Title: Application of a Second-order Stochastic Optimization Algorithm for\n  Fitting Stochastic Epidemiological Models Abstract: Epidemiological models have tremendous potential to forecast disease burden\nand quantify the impact of interventions. Detailed models are increasingly\npopular, however these models tend to be stochastic and very costly to\nevaluate. Fortunately, readily available high-performance cloud computing now\nmeans that these models can be evaluated many times in parallel. Here, we\nbriefly describe PSPO, an extension to Spall's second-order stochastic\noptimization algorithm, Simultaneous Perturbation Stochastic Approximation\n(SPSA), that takes full advantage of parallel computing environments. The main\nfocus of this work is on the use of PSPO to maximize the pseudo-likelihood of a\nstochastic epidemiological model to data from a 1861 measles outbreak in\nHagelloch, Germany. Results indicate that PSPO far outperforms gradient ascent\nand SPSA on this challenging likelihood maximization problem. \n\n"}
{"id": "1708.00955", "contents": "Title: Hamiltonian Monte Carlo with Energy Conserving Subsampling Abstract: Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional\nposterior distributions with proposed parameter draws obtained by iterating on\na discretized version of the Hamiltonian dynamics. The iterations make HMC\ncomputationally costly, especially in problems with large datasets, since it is\nnecessary to compute posterior densities and their derivatives with respect to\nthe parameters. Naively computing the Hamiltonian dynamics on a subset of the\ndata causes HMC to lose its key ability to generate distant parameter proposals\nwith high acceptance probability. The key insight in our article is that\nefficient subsampling HMC for the parameters is possible if both the dynamics\nand the acceptance probability are computed from the same data subsample in\neach complete HMC iteration. We show that this is possible to do in a\nprincipled way in a HMC-within-Gibbs framework where the subsample is updated\nusing a pseudo marginal MH step and the parameters are then updated using an\nHMC step, based on the current subsample. We show that our subsampling methods\nare fast and compare favorably to two popular sampling algorithms that utilize\ngradient estimates from data subsampling. We also explore the current\nlimitations of subsampling HMC algorithms by varying the quality of the\nvariance reducing control variates used in the estimators of the posterior\ndensity and its gradients. \n\n"}
{"id": "1708.02365", "contents": "Title: Indirect Inference with a Non-Smooth Criterion Function Abstract: Indirect inference requires simulating realisations of endogenous variables\nfrom the model under study. When the endogenous variables are discontinuous\nfunctions of the model parameters, the resulting indirect inference criterion\nfunction is discontinuous and does not permit the use of derivative-based\noptimisation routines. Using a change of variables technique, we propose a\nnovel simulation algorithm that alleviates the discontinuities inherent in such\nindirect inference criterion functions, and permits the application of\nderivative-based optimisation routines to estimate the unknown model\nparameters. Unlike competing approaches, this approach does not rely on kernel\nsmoothing or bandwidth parameters. Several Monte Carlo examples that have\nfeatured in the literature on indirect inference with discontinuous outcomes\nillustrate the approach, and demonstrate the superior performance of this\napproach over existing alternatives. \n\n"}
{"id": "1708.03031", "contents": "Title: Statistical state dynamics of weak jets in barotropic beta-plane\n  turbulence Abstract: Zonal jets in a barotropic setup emerge out of homogeneous turbulence through\na flow-forming instability of the homogeneous turbulent state (`zonostrophic\ninstability') which occurs as the turbulence intensity increases. This has been\ndemonstrated using the statistical state dynamics (SSD) framework with a\nclosure at second order. Furthermore, it was shown that for small\nsupercriticality the flow-forming instability follows Ginzburg-Landau (G-L)\ndynamics. Here, the SSD framework is used to study the equilibration of this\nflow-forming instability for small supercriticality. First, we compare the\npredictions of the weakly nonlinear G-L dynamics to the fully nonlinear SSD\ndynamics closed at second order for a wide ranges of parameters. A new branch\nof jet equilibria is revealed that is not contiguously connected with the G-L\nbranch. This new branch at weak supercriticalities involves jets with larger\namplitude compared to the ones of the G-L branch. Furthermore, this new branch\ncontinues even for subcritical values with respect to the linear flow-forming\ninstability. Thus, a new nonlinear flow-forming instability out of homogeneous\nturbulence is revealed. Second, we investigate how both the linear flow-forming\ninstability and the novel nonlinear flow-forming instability are equilibrated.\nWe identify the physical processes underlying the jet equilibration as well as\nthe types of eddies that contribute in each process. Third, we propose a\nmodification of the diffusion coefficient of the G-L dynamics that is able to\ncapture the asymmetric evolution for weak jets at scales other than the\nmarginal scale (side-band instabilities) for the linear flow-forming\ninstability. \n\n"}
{"id": "1708.03288", "contents": "Title: Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is\n  low Abstract: We study a seemingly unexpected and relatively less understood overfitting\naspect of a fundamental tool in sparse linear modeling - best subset selection,\nwhich minimizes the residual sum of squares subject to a constraint on the\nnumber of nonzero coefficients. While the best subset selection procedure is\noften perceived as the \"gold standard\" in sparse learning when the signal to\nnoise ratio (SNR) is high, its predictive performance deteriorates when the SNR\nis low. In particular, it is outperformed by continuous shrinkage methods, such\nas ridge regression and the Lasso. We investigate the behavior of best subset\nselection in the high-noise regimes and propose an alternative approach based\non a regularized version of the least-squares criterion. Our proposed\nestimators (a) mitigate, to a large extent, the poor predictive performance of\nbest subset selection in the high-noise regimes; and (b) perform favorably,\nwhile generally delivering substantially sparser models, relative to the best\npredictive models available via ridge regression and the Lasso. We conduct an\nextensive theoretical analysis of the predictive properties of the proposed\napproach and provide justification for its superior predictive performance\nrelative to best subset selection when the noise-level is high. Our estimators\ncan be expressed as solutions to mixed integer second order conic optimization\nproblems and, hence, are amenable to modern computational tools from\nmathematical optimization. \n\n"}
{"id": "1708.04144", "contents": "Title: An efficient SPDE approach for El Ni\\~no Abstract: We consider the numerical approximation of stochastic partial differential\nequations (SPDEs) based models for a quasi-periodic climate pattern in the\ntropical Pacific Ocean known as El Ni\\~no phenomenon. We show that for these\nmodels the mean and the covariance are given by a deterministic partial\ndifferential equation and by an operator differential equation, respectively.\nIn this context we provide a numerical framework to approximate these\nparameters directly. We compare this method to stochastic differential\nequations and SPDEs based models from the literature solved by Taylor methods\nand stochastic Galerkin methods, respectively. Numerical results for different\nscenarios taking as a reference measured data of the years 2014 and 2015 (last\nNi\\~no event) validate the efficiency of our approach. \n\n"}
{"id": "1708.04490", "contents": "Title: Sparse Inverse Covariance Estimation for High-throughput microRNA\n  Sequencing Data in the Poisson Log-Normal Graphical Model Abstract: We introduce the Poisson Log-Normal Graphical Model for count data, and\npresent a normality transformation for data arising from this distribution. The\nmodel and transformation are feasible for high-throughput microRNA (miRNA)\nsequencing data and directly account for known overdispersion relationships\npresent in this data set. The model allows for network dependencies to be\nmodeled, and we provide an algorithm which utilizes a one-step EM based result\nin order to allow for a provable increase in performance in determining the\nnetwork structure. The model is shown to provide an increase in performance in\nsimulation settings over a range of network structures. The model is applied to\nhigh-throughput miRNA sequencing data from patients with breast cancer from The\nCancer Genome Atlas (TCGA). By selecting the most highly connected miRNA\nmolecules in the fitted network we find that nearly all of them are known to be\ninvolved in the regulation of breast cancer. \n\n"}
{"id": "1708.04527", "contents": "Title: The Trimmed Lasso: Sparsity and Robustness Abstract: Nonconvex penalty methods for sparse modeling in linear regression have been\na topic of fervent interest in recent years. Herein, we study a family of\nnonconvex penalty functions that we call the trimmed Lasso and that offers\nexact control over the desired level of sparsity of estimators. We analyze its\nstructural properties and in doing so show the following:\n  1) Drawing parallels between robust statistics and robust optimization, we\nshow that the trimmed-Lasso-regularized least squares problem can be viewed as\na generalized form of total least squares under a specific model of\nuncertainty. In contrast, this same model of uncertainty, viewed instead\nthrough a robust optimization lens, leads to the convex SLOPE (or OWL) penalty.\n  2) Further, in relating the trimmed Lasso to commonly used sparsity-inducing\npenalty functions, we provide a succinct characterization of the connection\nbetween trimmed-Lasso- like approaches and penalty functions that are\ncoordinate-wise separable, showing that the trimmed penalties subsume existing\ncoordinate-wise separable penalties, with strict containment in general.\n  3) Finally, we describe a variety of exact and heuristic algorithms, both\nexisting and new, for trimmed Lasso regularized estimation problems. We include\na comparison between the different approaches and an accompanying\nimplementation of the algorithms. \n\n"}
{"id": "1708.04535", "contents": "Title: The exact rogue wave recurrence in the NLS periodic setting via matched\n  asymptotic expansions, for 1 and 2 unstable modes Abstract: The focusing Nonlinear Schr\\\"odinger (NLS) equation is the simplest universal\nmodel describing the modulation instability (MI) of quasi monochromatic waves\nin weakly nonlinear media, the main physical mechanism for the generation of\nrogue (anomalous) waves (RWs) in Nature. In this paper we investigate the\n$x$-periodic Cauchy problem for NLS for a generic periodic initial perturbation\nof the unstable constant background solution, in the case of $N=1,2$ unstable\nmodes. We use matched asymptotic expansion techniques to show that the solution\nof this problem describes an exact deterministic alternate recurrence of linear\nand nonlinear stages of MI, and that the nonlinear RW stages are described by\nthe N-breather solution of Akhmediev type, whose parameters, different at each\nRW appearence, are always given in terms of the initial data through elementary\nfunctions. This paper is motivated by a preceeding work of the authors in which\na different approach, the finite gap method, was used to investigate periodic\nCauchy problems giving rise to RW recurrence. \n\n"}
{"id": "1708.06448", "contents": "Title: The p-convolution forest: a method for solving graphical models with\n  additive probabilistic equations Abstract: Convolution trees, loopy belief propagation, and fast numerical p-convolution\nare combined for the first time to efficiently solve networks with several\nadditive constraints between random variables. An implementation of this\n\"convolution forest\" approach is constructed from scratch, including an\nimproved trimmed convolution tree algorithm and engineering details that permit\nfast inference in practice, and improve the ability of scientists to prototype\nmodels with additive relationships between discrete variables. The utility of\nthis approach is demonstrated using several examples: these include\nillustrations on special cases of some classic NP-complete problems (subset sum\nand knapsack), identification of GC-rich genomic regions with a large hidden\nMarkov model, inference of molecular composition from summary statistics of the\nintact molecule, and estimation of elemental abundance in the presence of\noverlapping isotope peaks. \n\n"}
{"id": "1708.07481", "contents": "Title: Preconditioned Spectral Clustering for Stochastic Block Partition\n  Streaming Graph Challenge Abstract: Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is\ndemonstrated to efficiently solve eigenvalue problems for graph Laplacians that\nappear in spectral clustering. For static graph partitioning, 10-20 iterations\nof LOBPCG without preconditioning result in ~10x error reduction, enough to\nachieve 100% correctness for all Challenge datasets with known truth\npartitions, e.g., for graphs with 5K/.1M (50K/1M) Vertices/Edges in 2 (7)\nseconds, compared to over 5,000 (30,000) seconds needed by the baseline Python\ncode. Our Python code 100% correctly determines 98 (160) clusters from the\nChallenge static graphs with 0.5M (2M) vertices in 270 (1,700) seconds using\n10GB (50GB) of memory. Our single-precision MATLAB code calculates the same\nclusters at half time and memory. For streaming graph partitioning, LOBPCG is\ninitiated with approximate eigenvectors of the graph Laplacian already computed\nfor the previous graph, in many cases reducing 2-3 times the number of required\nLOBPCG iterations, compared to the static case. Our spectral clustering is\ngeneric, i.e. assuming nothing specific of the block model or streaming, used\nto generate the graphs for the Challenge, in contrast to the base code.\nNevertheless, in 10-stage streaming comparison with the base code for the 5K\ngraph, the quality of our clusters is similar or better starting at stage 4 (7)\nfor emerging edging (snowballing) streaming, while the computations are over\n100-1000 faster. \n\n"}
{"id": "1708.07804", "contents": "Title: BAMBI: An R package for Fitting Bivariate Angular Mixture Models Abstract: Statistical analyses of directional or angular data have applications in a\nvariety of fields, such as geology, meteorology and bioinformatics. There is\nsubstantial literature on descriptive and inferential techniques for univariate\nangular data, with the bivariate (or more generally, multivariate) cases\nreceiving more attention in recent years. More specifically, the bivariate\nwrapped normal, von Mises sine and von Mises cosine distributions, and mixtures\nthereof, have been proposed for practical use. However, there is a lack of\nsoftware implementing these distributions and the associated inferential\ntechniques. In this article, we introduce BAMBI, an R package for analyzing\nbivariate (and univariate) angular data. We implement random data generation,\ndensity evaluation, and computation of theoretical summary measures (variances\nand correlation coefficients) for the three aforementioned bivariate angular\ndistributions, as well as two univariate angular distributions: the univariate\nwrapped normal and the univariate von Mises distribution. The major\ncontribution of BAMBI to statistical computing is in providing Bayesian methods\nfor modeling angular data using finite mixtures of these distributions. We also\nprovide functions for visual and numerical diagnostics and Bayesian inference\nfor the fitted models. In this article, we first provide a brief review of the\ndistributions and techniques used in BAMBI, then describe the capabilities of\nthe package, and finally conclude with demonstrations of mixture model fitting\nusing BAMBI on the two real datasets included in the package, one univariate\nand one bivariate. \n\n"}
{"id": "1708.08396", "contents": "Title: Controlled Sequential Monte Carlo Abstract: Sequential Monte Carlo methods, also known as particle methods, are a popular\nset of techniques for approximating high-dimensional probability distributions\nand their normalizing constants. These methods have found numerous applications\nin statistics and related fields; e.g. for inference in non-linear non-Gaussian\nstate space models, and in complex static models. Like many Monte Carlo\nsampling schemes, they rely on proposal distributions which crucially impact\ntheir performance. We introduce here a class of controlled sequential Monte\nCarlo algorithms, where the proposal distributions are determined by\napproximating the solution to an associated optimal control problem using an\niterative scheme. This method builds upon a number of existing algorithms in\neconometrics, physics, and statistics for inference in state space models, and\ngeneralizes these methods so as to accommodate complex static models. We\nprovide a theoretical analysis concerning the fluctuation and stability of this\nmethodology that also provides insight into the properties of related\nalgorithms. We demonstrate significant gains over state-of-the-art methods at a\nfixed computational complexity on a variety of applications. \n\n"}
{"id": "1709.00151", "contents": "Title: Approximately Optimal Subset Selection for Statistical Design and\n  Modelling Abstract: We study the problem of optimal subset selection from a set of correlated\nrandom variables. In particular, we consider the associated combinatorial\noptimization problem of maximizing the determinant of a symmetric positive\ndefinite matrix that characterizes the chosen subset. This problem arises in\nmany domains, such as experimental designs, regression modeling, and\nenvironmental statistics. We establish an efficient polynomial-time algorithm\nusing Determinantal Point Process for approximating the optimal solution to the\nproblem. We demonstrate the advantages of our methods by presenting\ncomputational results for both synthetic and real data sets. \n\n"}
{"id": "1709.00404", "contents": "Title: Unbiased Hamiltonian Monte Carlo with couplings Abstract: We propose a methodology to parallelize Hamiltonian Monte Carlo estimators.\nOur approach constructs a pair of Hamiltonian Monte Carlo chains that are\ncoupled in such a way that they meet exactly after some random number of\niterations. These chains can then be combined so that resulting estimators are\nunbiased. This allows us to produce independent replicates in parallel and\naverage them to obtain estimators that are consistent in the limit of the\nnumber of replicates, instead of the usual limit of the number of Markov chain\niterations. We investigate the scalability of our coupling in high dimensions\non a toy example. The choice of algorithmic parameters and the efficiency of\nour proposed methodology are then illustrated on a logistic regression with 300\ncovariates, and a log-Gaussian Cox point processes model with low to fine\ngrained discretizations. \n\n"}
{"id": "1709.00814", "contents": "Title: How vortices and shocks provide for a flux loop in two-dimensional\n  compressible turbulence Abstract: Large-scale turbulence in fluid layers and other quasi-two-dimensional\ncompressible systems consists of planar vortices and waves. Separately, wave\nturbulence usually produces a direct energy cascade, while solenoidal planar\nturbulence transports energy to large scales by an inverse cascade. Here, we\nconsider turbulence at finite Mach numbers when the interaction between\nacoustic waves and vortices is substantial. We employ solenoidal pumping at\nintermediate scales and show how both direct and inverse energy cascades are\nformed starting from the pumping scale. We show that there is an inverse\ncascade of kinetic energy up to a scale $\\ell$, where a typical velocity\nreaches the speed of sound; this creates shock waves, which provide for a\ncompensating direct cascade. When the system size is less than $\\ell$, the\nsteady state contains a system-size pair of long-living condensate vortices\nconnected by a system of shocks. Thus turbulence in fluid layers processes\nenergy via a loop: Most energy first goes to large scales via vortices and is\nthen transported by waves to small-scale dissipation. \n\n"}
{"id": "1709.00872", "contents": "Title: On synthetic data with predetermined subject partitioning and cluster\n  profiling, and pre-specified categorical variable marginal dependence\n  structure Abstract: A standard approach for assessing the performance of partition or mixture\nmodels is to create synthetic data sets with a pre-specified clustering\nstructure, and assess how well the model reveals this structure. A common\nformat is that subjects are assigned to different clusters, with variable\nobservations simulated so that subjects within the same cluster have similar\nprofiles, allowing for some variability. In this manuscript, we consider\nobservations from nominal, ordinal and interval categorical variables.\nTheoretical and empirical results are utilized to explore the dependence\nstructure between the variables, in relation to the clustering structure for\nthe subjects. A novel approach is proposed that allows to control the marginal\nassociation or correlation structure of the variables, and to specify exact\ncorrelation values. Practical examples are shown and additional theoretical\nresults are derived for interval data, commonly observed in cohort studies,\nincluding observations that emulate Single Nucleotide Polymorphisms. We compare\na synthetic dataset to a real one, to demonstrate similarities and differences. \n\n"}
{"id": "1709.02069", "contents": "Title: An Alternative Approach to Functional Linear Partial Quantile Regression Abstract: Functional data such as curves and surfaces have become more and more common\nwith modern technological advancements. The use of functional predictors\nremains challenging due to its inherent infinite-dimensionality. The common\npractice is to project functional data into a finite dimensional space. The\npopular partial least square (PLS) method has been well studied for the\nfunctional linear model [1]. As an alternative, quantile regression provides a\nrobust and more comprehensive picture of the conditional distribution of a\nresponse when it is non-normal, heavy-tailed, or contaminated by outliers.\nWhile partial quantile regression (PQR) was proposed in [2], no theoretical\nguarantees were provided due to the iterative nature of the algorithm and the\nnon-smoothness of quantile loss function. To address these issues, we propose\nan alternative PQR (APQR) formulation with guaranteed convergence. This novel\nformulation motivates new theories and allows us to establish asymptotic\nproperties. Numerical studies on a benchmark dataset show the superiority of\nour new approach. We also apply our novel method to a functional magnetic\nresonance imaging (fMRI) data to predict attention deficit hyperactivity\ndisorder (ADHD) and a diffusion tensor imaging (DTI) dataset to predict\nAlzheimer's disease (AD). \n\n"}
{"id": "1709.03162", "contents": "Title: Bayesian bandits: balancing the exploration-exploitation tradeoff via\n  double sampling Abstract: Reinforcement learning studies how to balance exploration and exploitation in\nreal-world systems, optimizing interactions with the world while simultaneously\nlearning how the world operates. One general class of algorithms for such\nlearning is the multi-armed bandit setting. Randomized probability matching,\nbased upon the Thompson sampling approach introduced in the 1930s, has recently\nbeen shown to perform well and to enjoy provable optimality properties. It\npermits generative, interpretable modeling in a Bayesian setting, where prior\nknowledge is incorporated, and the computed posteriors naturally capture the\nfull state of knowledge. In this work, we harness the information contained in\nthe Bayesian posterior and estimate its sufficient statistics via sampling. In\nseveral application domains, for example in health and medicine, each\ninteraction with the world can be expensive and invasive, whereas drawing\nsamples from the model is relatively inexpensive. Exploiting this viewpoint, we\ndevelop a double sampling technique driven by the uncertainty in the learning\nprocess: it favors exploitation when certain about the properties of each arm,\nexploring otherwise. The proposed algorithm does not make any distributional\nassumption and it is applicable to complex reward distributions, as long as\nBayesian posterior updates are computable. Utilizing the estimated posterior\nsufficient statistics, double sampling autonomously balances the\nexploration-exploitation tradeoff to make better informed decisions. We\nempirically show its reduced cumulative regret when compared to\nstate-of-the-art alternatives in representative bandit settings. \n\n"}
{"id": "1709.03163", "contents": "Title: Variational inference for the multi-armed contextual bandit Abstract: In many biomedical, science, and engineering problems, one must sequentially\ndecide which action to take next so as to maximize rewards. One general class\nof algorithms for optimizing interactions with the world, while simultaneously\nlearning how the world operates, is the multi-armed bandit setting and, in\nparticular, the contextual bandit case. In this setting, for each executed\naction, one observes rewards that are dependent on a given 'context', available\nat each interaction with the world. The Thompson sampling algorithm has\nrecently been shown to enjoy provable optimality properties for this set of\nproblems, and to perform well in real-world settings. It facilitates generative\nand interpretable modeling of the problem at hand. Nevertheless, the design and\ncomplexity of the model limit its application, since one must both sample from\nthe distributions modeled and calculate their expected rewards. We here show\nhow these limitations can be overcome using variational inference to\napproximate complex models, applying to the reinforcement learning case\nadvances developed for the inference case in the machine learning community\nover the past two decades. We consider contextual multi-armed bandit\napplications where the true reward distribution is unknown and complex, which\nwe approximate with a mixture model whose parameters are inferred via\nvariational inference. We show how the proposed variational Thompson sampling\napproach is accurate in approximating the true distribution, and attains\nreduced regrets even with complex reward distributions. The proposed algorithm\nis valuable for practical scenarios where restrictive modeling assumptions are\nundesirable. \n\n"}
{"id": "1709.03184", "contents": "Title: A Physics-Based Approach to Unsupervised Discovery of Coherent\n  Structures in Spatiotemporal Systems Abstract: Given that observational and numerical climate data are being produced at\never more prodigious rates, increasingly sophisticated and automated analysis\ntechniques have become essential. Deep learning is quickly becoming a standard\napproach for such analyses and, while great progress is being made, major\nchallenges remain. Unlike commercial applications in which deep learning has\nled to surprising successes, scientific data is highly complex and typically\nunlabeled. Moreover, interpretability and detecting new mechanisms are key to\nscientific discovery. To enhance discovery we present a complementary\nphysics-based, data-driven approach that exploits the causal nature of\nspatiotemporal data sets generated by local dynamics (e.g. hydrodynamic flows).\nWe illustrate how novel patterns and coherent structures can be discovered in\ncellular automata and outline the path from them to climate data. \n\n"}
{"id": "1709.03312", "contents": "Title: A determinant-free method to simulate the parameters of large Gaussian\n  fields Abstract: We propose a determinant-free approach for simulation-based Bayesian\ninference in high-dimensional Gaussian models. We introduce auxiliary variables\nwith covariance equal to the inverse covariance of the model. The joint\nprobability of the auxiliary model can be computed without evaluating\ndeterminants, which are often hard to compute in high dimensions. We develop a\nMarkov chain Monte Carlo sampling scheme for the auxiliary model that requires\nno more than the application of inverse-matrix-square-roots and the solution of\nlinear systems. These operations can be performed at large scales with rational\napproximations. We provide an empirical study on both synthetic and real-world\ndata for sparse Gaussian processes and for large-scale Gaussian Markov random\nfields. \n\n"}
{"id": "1709.03471", "contents": "Title: Bayesian inference, model selection and likelihood estimation using fast\n  rejection sampling: the Conway-Maxwell-Poisson distribution Abstract: Bayesian inference for models with intractable likelihood functions\nrepresents a challenging suite of problems in modern statistics. In this work\nwe analyse the Conway-Maxwell-Poisson (COM-Poisson) distribution, a two\nparameter generalisation of the Poisson distribution. COM-Poisson regression\nmodelling allows the flexibility to model dispersed count data as part of a\ngeneralised linear model (GLM) with a COM-Poisson response, where exogenous\ncovariates control the mean and dispersion level of the response. The major\ndifficulty with COM-Poisson regression is that the likelihood function contains\nmultiple intractable normalising constants and is not amenable to standard\ninference and MCMC techniques. Recent work by Chanialidis et al. (2017) has\nseen the development of a sampler to draw random variates from the COM-Poisson\nlikelihood using a rejection sampling algorithm. We provide a new rejection\nsampler for the COM-Poisson distribution which significantly reduces the CPU\ntime required to perform inference for COM-Poisson regression models. A novel\nextension of this work shows that for any intractable likelihood function with\nan associated rejection sampler it is possible to construct unbiased estimators\nof the intractable likelihood which proves useful for model selection or for\nuse within pseudo-marginal MCMC algorithms (Andrieu and Roberts, 2009). We\ndemonstrate all of these methods on a real-world dataset of takeover bids. \n\n"}
{"id": "1709.04196", "contents": "Title: Particle Filters and Data Assimilation Abstract: State-space models can be used to incorporate subject knowledge on the\nunderlying dynamics of a time series by the introduction of a latent Markov\nstate-process. A user can specify the dynamics of this process together with\nhow the state relates to partial and noisy observations that have been made.\nInference and prediction then involves solving a challenging inverse problem:\ncalculating the conditional distribution of quantities of interest given the\nobservations. This article reviews Monte Carlo algorithms for solving this\ninverse problem, covering methods based on the particle filter and the ensemble\nKalman filter. We discuss the challenges posed by models with high-dimensional\nstates, joint estimation of parameters and the state, and inference for the\nhistory of the state process. We also point out some potential new developments\nwhich will be important for tackling cutting-edge filtering applications. \n\n"}
{"id": "1709.05885", "contents": "Title: Variational Gaussian Approximation for Poisson Data Abstract: The Poisson model is frequently employed to describe count data, but in a\nBayesian context it leads to an analytically intractable posterior probability\ndistribution. In this work, we analyze a variational Gaussian approximation to\nthe posterior distribution arising from the Poisson model with a Gaussian\nprior. This is achieved by seeking an optimal Gaussian distribution minimizing\nthe Kullback-Leibler divergence from the posterior distribution to the\napproximation, or equivalently maximizing the lower bound for the model\nevidence. We derive an explicit expression for the lower bound, and show the\nexistence and uniqueness of the optimal Gaussian approximation. The lower bound\nfunctional can be viewed as a variant of classical Tikhonov regularization that\npenalizes also the covariance. Then we develop an efficient alternating\ndirection maximization algorithm for solving the optimization problem, and\nanalyze its convergence. We discuss strategies for reducing the computational\ncomplexity via low rank structure of the forward operator and the sparsity of\nthe covariance. Further, as an application of the lower bound, we discuss\nhierarchical Bayesian modeling for selecting the hyperparameter in the prior\ndistribution, and propose a monotonically convergent algorithm for determining\nthe hyperparameter. We present extensive numerical experiments to illustrate\nthe Gaussian approximation and the algorithms. \n\n"}
{"id": "1709.06597", "contents": "Title: varbvs: Fast Variable Selection for Large-scale Regression Abstract: We introduce varbvs, a suite of functions written in R and MATLAB for\nregression analysis of large-scale data sets using Bayesian variable selection\nmethods. We have developed numerical optimization algorithms based on\nvariational approximation methods that make it feasible to apply Bayesian\nvariable selection to very large data sets. With a focus on examples from\ngenome-wide association studies, we demonstrate that varbvs scales well to data\nsets with hundreds of thousands of variables and thousands of samples, and has\nfeatures that facilitate rapid data analyses. Moreover, varbvs allows for\nextensive model customization, which can be used to incorporate external\ninformation into the analysis. We expect that the combination of an easy-to-use\ninterface and robust, scalable algorithms for posterior computation will\nencourage broader use of Bayesian variable selection in areas of applied\nstatistics and computational biology. The most recent R and MATLAB source code\nis available for download at Github (https://github.com/pcarbo/varbvs), and the\nR package can be installed from CRAN\n(https://cran.r-project.org/package=varbvs). \n\n"}
{"id": "1709.07710", "contents": "Title: Barker's algorithm for Bayesian inference with intractable likelihoods Abstract: In this expository paper we abstract and describe a simple MCMC scheme for\nsampling from intractable target densities. The approach has been introduced in\nGon\\c{c}alves et al. (2017a) in the specific context of jump-diffusions, and is\nbased on the Barker's algorithm paired with a simple Bernoulli factory type\nscheme, the so called 2-coin algorithm. In many settings it is an alternative\nto standard Metropolis-Hastings pseudo-marginal method for simulating from\nintractable target densities. Although Barker's is well-known to be slightly\nless efficient than Metropolis-Hastings, the key advantage of our approach is\nthat it allows to implement the \"marginal Barker's\" instead of the extended\nstate space pseudo-marginal Metropolis-Hastings, owing to the special form of\nthe accept/reject probability. We shall illustrate our methodology in the\ncontext of Bayesian inference for discretely observed Wright-Fisher family of\ndiffusions. \n\n"}
{"id": "1709.08238", "contents": "Title: Counterparty Credit Limits: The Impact of a Risk-Mitigation Measure on\n  Everyday Trading Abstract: A counterparty credit limit (CCL) is a limit that is imposed by a financial\ninstitution to cap its maximum possible exposure to a specified counterparty.\nCCLs help institutions to mitigate counterparty credit risk via selective\ndiversification of their exposures. In this paper, we analyze how CCLs impact\nthe prices that institutions pay for their trades during everyday trading. We\nstudy a high-quality data set from a large electronic trading platform in the\nforeign exchange spot market, which enables institutions to apply CCLs. We find\nempirically that CCLs had little impact on the vast majority of trades in this\ndata. We also study the impact of CCLs using a new model of trading. By\nsimulating our model with different underlying CCL networks, we highlight that\nCCLs can have a major impact in some situations. \n\n"}
{"id": "1709.08266", "contents": "Title: On the wave turbulence theory for stratified flows in the ocean Abstract: After the pioneering work of Garrett and Munk, the statistics of oceanic\ninternal gravity waves has become a central subject of research in\noceanography. The time evolution of the spectral energy of internal waves in\nthe ocean can be described by a near-resonance wave turbulence equation, of\nquantum Boltzmann type. In this work, we provide the first rigorous\nmathematical study for the equation by showing the global existence and\nuniqueness of strong solutions. \n\n"}
{"id": "1709.08625", "contents": "Title: HLIBCov: Parallel Hierarchical Matrix Approximation of Large Covariance\n  Matrices and Likelihoods with Applications in Parameter Identification Abstract: We provide more technical details about the HLIBCov package, which is using\nparallel hierarchical ($\\H$-) matrices to identify unknown parameters of the\ncovariance function (variance, smoothness, and covariance length). These\nparameters are estimated by maximizing the joint Gaussian log-likelihood\nfunction. The HLIBCov package approximates large dense inhomogeneous covariance\nmatrices with a log-linear computational cost and storage requirement. We\nexplain how to compute the Cholesky factorization, determinant, inverse and\nquadratic form in the H-matrix format. To demonstrate the numerical\nperformance, we identify three unknown parameters in an example with 2,000,000\nlocations on a PC-desktop. \n\n"}
{"id": "1709.08720", "contents": "Title: On Extreme Value Index Estimation under Random Censoring Abstract: Extreme value analysis in the presence of censoring is receiving much\nattention as it has applications in many disciplines, including survival and\nreliability studies. Estimation of extreme value index (EVI) is of primary\nimportance as it is a critical parameter needed in estimating extreme events\nsuch as quantiles and exceedance probabilities. In this paper, we review\nseveral estimators of the extreme value index when data is subject to random\ncensoring. In addition, four estimators are proposed, one based on the\nexponential regression approximation of log spacings, one based on a Zipf\nestimator and two based on variants of the moment estimator. The proposed\nestimators and the existing ones are compared under the same simulation\nconditions. The performance measures for the estimators include confidence\ninterval length and coverage probability. The simulation results show that no\nestimator is universally the best as the estimators depend on the size of the\nEVI parameter, percentage of censoring in the right tail and the underlying\ndistribution. However, certain estimators such as the proposed reduced-bias\nestimator and the adapted moment estimator are found to perform well across\nmost scenarios. Moreover, we present a bootstrap algorithm for obtaining\nsamples for extreme value analysis in the context of censoring. Some of the\nestimators that performed well in the simulation study are illustrated using a\npractical dataset from medical research \n\n"}
{"id": "1709.08725", "contents": "Title: A Machine Learning Framework to Forecast Wave Conditions Abstract: A~machine learning framework is developed to estimate ocean-wave conditions.\nBy supervised training of machine learning models on many thousands of\niterations of a physics-based wave model, accurate representations of\nsignificant wave heights and period can be used to predict ocean conditions. A\nmodel of Monterey Bay was used as the example test site; it was forced by\nmeasured wave conditions, ocean-current nowcasts, and reported winds. These\ninput data along with model outputs of spatially variable wave heights and\ncharacteristic period were aggregated into supervised learning training and\ntest data sets, which were supplied to machine learning models. These machine\nlearning models replicated wave heights with a root-mean-squared error of 9cm\nand correctly identify over 90% of the characteristic periods for the test-data\nsets. Impressively, transforming model inputs to outputs through matrix\noperations requires only a fraction (<1/1,000) of the computation time compared\nto forecasting with the physics-based model. \n\n"}
{"id": "1709.08860", "contents": "Title: A Data Driven, Zero-Dimensional Time Delay Model with Radiative Forcing\n  for Simulating Global Climate Abstract: Several complicated non-linear models exist which simulate the physical\nprocesses leading to fluctuations in global climate. Some of these more\nadvanced models use observations to constrain various parameters involved.\nHowever, they tend to be very computationally expensive. Also, the exact\nphysical processes that affect the climate variations have not been completely\ncomprehended. Therefore, to obtain an insight into global climate, we have\ndeveloped a physically motivated reduced climate model. The model utilizes a\nnovel mathematical formulation involving a non-linear delay differential\nequation to study temperature fluctuations when subjected to imposed radiative\nforcing. We have further incorporated simplified equations to test the effect\nof speculated mechanisms of climate forcing and evaluated the extent of their\ninfluence. The findings are significant in our efforts to predict climate\nchange and help in policy framing necessary to tackle it. \n\n"}
{"id": "1709.09280", "contents": "Title: Particle rolling MCMC with double-block sampling Abstract: An efficient simulation-based methodology is proposed for the rolling window\nestimation of state space models, called particle rolling Markov chain Monte\nCarlo (MCMC) with double block sampling. In our method, which is based on\nSequential Monte Carlo (SMC), particles are sequentially updated to approximate\nthe posterior distribution for each window by learning new information and\ndiscarding old information from observations. Th particles are refreshed with\nan MCMC algorithm when the importance weights degenerate. To avoid degeneracy,\nwhich is crucial for reducing the computation time, we introduce a block\nsampling scheme and generate multiple candidates by the algorithm based on the\nconditional SMC. The theoretical discussion shows that the proposed methodology\nwith a nested structure is expressed as SMC sampling for the augmented space to\nprovide the justification. The computational performance is evaluated in\nillustrative examples, showing that the posterior distributions of the model\nparameters are accurately estimated. The proofs and additional discussions\n(algorithms and experimental results) are provided in the Supplementary\nMaterial. \n\n"}
{"id": "1710.01227", "contents": "Title: Keep It Real: Tail Probabilities of Compound Heavy-Tailed Distributions Abstract: We propose an analytical approach to the computation of tail probabilities of\ncompound distributions whose individual components have heavy tails. Our\napproach is based on the contour integration method, and gives rise to a\nrepresentation of the tail probability of a compound distribution in the form\nof a rapidly convergent one-dimensional integral involving a discontinuity of\nthe imaginary part of its moment generating function across a branch cut. The\nlatter integral can be evaluated in quadratures, or alternatively represented\nas an asymptotic expansion. Our approach thus offers a viable (especially at\nhigh percentile levels) alternative to more standard methods such as Monte\nCarlo or the Fast Fourier Transform, traditionally used for such problems. As a\npractical application, we use our method to compute the operational Value at\nRisk (VAR) of a financial institution, where individual losses are modeled as\nspliced distributions whose large loss components are given by power-law or\nlognormal distributions. Finally, we briefly discuss extensions of the present\nformalism for calculation of tail probabilities of compound distributions made\nof compound distributions with heavy tails. \n\n"}
{"id": "1710.04465", "contents": "Title: Markerless visual servoing on unknown objects for humanoid robot\n  platforms Abstract: To precisely reach for an object with a humanoid robot, it is of central\nimportance to have good knowledge of both end-effector, object pose and shape.\nIn this work we propose a framework for markerless visual servoing on unknown\nobjects, which is divided in four main parts: I) a least-squares minimization\nproblem is formulated to find the volume of the object graspable by the robot's\nhand using its stereo vision; II) a recursive Bayesian filtering technique,\nbased on Sequential Monte Carlo (SMC) filtering, estimates the 6D pose\n(position and orientation) of the robot's end-effector without the use of\nmarkers; III) a nonlinear constrained optimization problem is formulated to\ncompute the desired graspable pose about the object; IV) an image-based visual\nservo control commands the robot's end-effector toward the desired pose. We\ndemonstrate effectiveness and robustness of our approach with extensive\nexperiments on the iCub humanoid robot platform, achieving real-time\ncomputation, smooth trajectories and sub-pixel precisions. \n\n"}
{"id": "1710.04586", "contents": "Title: Particle Filtering for Stochastic Navier-Stokes Signal Observed with\n  Linear Additive Noise Abstract: We consider a non-linear filtering problem, whereby the signal obeys the\nstochastic Navier-Stokes equations and is observed through a linear mapping\nwith additive noise. The setup is relevant to data assimilation for numerical\nweather prediction and climate modelling, where similar models are used for\nunknown ocean or wind velocities. We present a particle filtering methodology\nthat uses likelihood informed importance proposals, adaptive tempering, and a\nsmall number of appropriate Markov Chain Monte Carlo steps. We provide a\ndetailed design for each of these steps and show in our numerical examples that\nthey are all crucial in terms of achieving good performance and efficiency. \n\n"}
{"id": "1710.06382", "contents": "Title: Convergence diagnostics for stochastic gradient descent with constant\n  step size Abstract: Many iterative procedures in stochastic optimization exhibit a transient\nphase followed by a stationary phase. During the transient phase the procedure\nconverges towards a region of interest, and during the stationary phase the\nprocedure oscillates in that region, commonly around a single point. In this\npaper, we develop a statistical diagnostic test to detect such phase transition\nin the context of stochastic gradient descent with constant learning rate. We\npresent theory and experiments suggesting that the region where the proposed\ndiagnostic is activated coincides with the convergence region. For a class of\nloss functions, we derive a closed-form solution describing such region.\nFinally, we suggest an application to speed up convergence of stochastic\ngradient descent by halving the learning rate each time stationarity is\ndetected. This leads to a new variant of stochastic gradient descent, which in\nmany settings is comparable to state-of-art. \n\n"}
{"id": "1710.06642", "contents": "Title: Is the bump significant? An axion-search example Abstract: Many experiments in physics involve searching for a localized excess over\nbackground expectations in an observed spectrum. If the background is known and\nthere is Gaussian noise, the amount of excess of successive observations can be\nquantified by the runs statistic taking care of the look-elsewhere effect. The\ndistribution of the runs statistic under the background model is known\nanalytically but the computation becomes too expensive for more than about a\nhundred observations. This work demonstrates a principled high-precision\nextrapolation from a few dozen up to millions of data points. It is most\nprecise in the interesting regime when an excess is present. The method is\nverified for benchmark cases and successfully applied to real data from an\naxion search. The code that implements our method is available at\nhttps://github.com/fredRos/runs . \n\n"}
{"id": "1710.06965", "contents": "Title: Importance sampling the union of rare events with an application to\n  power systems analysis Abstract: We consider importance sampling to estimate the probability $\\mu$ of a union\nof $J$ rare events $H_j$ defined by a random variable $\\boldsymbol{x}$. The\nsampler we study has been used in spatial statistics, genomics and\ncombinatorics going back at least to Karp and Luby (1983). It works by sampling\none event at random, then sampling $\\boldsymbol{x}$ conditionally on that event\nhappening and it constructs an unbiased estimate of $\\mu$ by multiplying an\ninverse moment of the number of occuring events by the union bound. We prove\nsome variance bounds for this sampler. For a sample size of $n$, it has a\nvariance no larger than $\\mu(\\bar\\mu-\\mu)/n$ where $\\bar\\mu$ is the union\nbound. It also has a coefficient of variation no larger than\n$\\sqrt{(J+J^{-1}-2)/(4n)}$ regardless of the overlap pattern among the $J$\nevents. Our motivating problem comes from power system reliability, where the\nphase differences between connected nodes have a joint Gaussian distribution\nand the $J$ rare events arise from unacceptably large phase differences. In the\ngrid reliability problems even some events defined by $5772$ constraints in\n$326$ dimensions, with probability below $10^{-22}$, are estimated with a\ncoefficient of variation of about $0.0024$ with only $n=10{,}000$ sample\nvalues. \n\n"}
{"id": "1710.07457", "contents": "Title: Learning Wasserstein Embeddings Abstract: The Wasserstein distance received a lot of attention recently in the\ncommunity of machine learning, especially for its principled way of comparing\ndistributions. It has found numerous applications in several hard problems,\nsuch as domain adaptation, dimensionality reduction or generative models.\nHowever, its use is still limited by a heavy computational cost. Our goal is to\nalleviate this problem by providing an approximation mechanism that allows to\nbreak its inherent complexity. It relies on the search of an embedding where\nthe Euclidean distance mimics the Wasserstein distance. We show that such an\nembedding can be found with a siamese architecture associated with a decoder\nnetwork that allows to move from the embedding space back to the original input\nspace. Once this embedding has been found, computing optimization problems in\nthe Wasserstein space (e.g. barycenters, principal directions or even\narchetypes) can be conducted extremely fast. Numerical experiments supporting\nthis idea are conducted on image datasets, and show the wide potential benefits\nof our method. \n\n"}
{"id": "1710.07702", "contents": "Title: On the Consistency of Graph-based Bayesian Learning and the Scalability\n  of Sampling Algorithms Abstract: A popular approach to semi-supervised learning proceeds by endowing the input\ndata with a graph structure in order to extract geometric information and\nincorporate it into a Bayesian framework. We introduce new theory that gives\nappropriate scalings of graph parameters that provably lead to a well-defined\nlimiting posterior as the size of the unlabeled data set grows. Furthermore, we\nshow that these consistency results have profound algorithmic implications.\nWhen consistency holds, carefully designed graph-based Markov chain Monte Carlo\nalgorithms are proved to have a uniform spectral gap, independent of the number\nof unlabeled inputs. Several numerical experiments corroborate both the\nstatistical consistency and the algorithmic scalability established by the\ntheory. \n\n"}
{"id": "1710.08297", "contents": "Title: Coastal flood implications of 1.5 {\\deg}C, 2.0 {\\deg}C, and 2.5 {\\deg}C\n  temperature stabilization targets in the 21st and 22nd century Abstract: Sea-level rise (SLR) is magnifying the frequency and severity of coastal\nflooding. The rate and amount of global mean sea-level (GMSL) rise is a\nfunction of the trajectory of global mean surface temperature (GMST).\nTherefore, temperature stabilization targets (e.g., 1.5 {\\deg}C and 2.0 {\\deg}C\nof warming above pre-industrial levels, as from the Paris Agreement) have\nimportant implications for coastal flood risk. Here, we assess differences in\nthe return periods of coastal floods at a global network of tide gauges between\nscenarios that stabilize GMST warming at 1.5 {\\deg}C, 2.0 {\\deg}C, and 2.5\n{\\deg}C above pre-industrial levels. We employ probabilistic, localized SLR\nprojections and long-term hourly tide gauge records to construct estimates of\nthe return levels of current and future flood heights for the 21st and 22nd\ncenturies. By 2100, under 1.5 {\\deg}C, 2.0 {\\deg}C, and 2.5 {\\deg}C GMST\nstabilization, median GMSL is projected to rise 47 cm with a very likely range\nof 28-82 cm (90% probability), 55 cm (very likely 30-94 cm), and 58 cm (very\nlikely 36-93 cm), respectively. As an independent comparison, a semi-empirical\nsea level model calibrated to temperature and GMSL over the past two millennia\nestimates median GMSL will rise within < 13% of these projections. By 2150,\nrelative to the 2.0 {\\deg}C scenario, GMST stabilization of 1.5 {\\deg}C\ninundates roughly 5 million fewer inhabitants that currently occupy lands,\nincluding 40,000 fewer individuals currently residing in Small Island\nDeveloping States. Relative to a 2.0 {\\deg}C scenario, the reduction in the\namplification of the frequency of the 100-yr flood arising from a 1.5 {\\deg}C\nGMST stabilization is greatest in the eastern United States and in Europe, with\nflood frequency amplification being reduced by about half. \n\n"}
{"id": "1710.08976", "contents": "Title: A class of multi-resolution approximations for large spatial datasets Abstract: Gaussian processes are popular and flexible models for spatial, temporal, and\nfunctional data, but they are computationally infeasible for large datasets. We\ndiscuss Gaussian-process approximations that use basis functions at multiple\nresolutions to achieve fast inference and that can (approximately) represent\nany spatial covariance structure. We consider two special cases of this\nmulti-resolution-approximation framework, a taper version and a\ndomain-partitioning (block) version. We describe theoretical properties and\ninference procedures, and study the computational complexity of the methods.\nNumerical comparisons and an application to satellite data are also provided. \n\n"}
{"id": "1710.09707", "contents": "Title: Calibrated Projection in MATLAB: Users' Manual Abstract: We present the calibrated-projection MATLAB package implementing the method\nto construct confidence intervals proposed by Kaido, Molinari and Stoye (2017).\nThis manual provides details on how to use the package for inference on\nprojections of partially identified parameters. It also explains how to use the\nMATLAB functions we developed to compute confidence intervals on solutions of\nnonlinear optimization problems with estimated constraints. \n\n"}
{"id": "1710.10951", "contents": "Title: SGDLibrary: A MATLAB library for stochastic gradient descent algorithms Abstract: We consider the problem of finding the minimizer of a function $f:\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the finite-sum form $\\min f(w) =\n1/n\\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent\nyears in the field of machine learning (ML). One promising approach for\nlarge-scale data is to use a stochastic optimization algorithm to solve the\nproblem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library\nof a collection of stochastic optimization algorithms. The purpose of the\nlibrary is to provide researchers and implementers a comprehensive evaluation\nenvironment for the use of these algorithms on various ML problems. \n\n"}
{"id": "1711.00484", "contents": "Title: Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments Abstract: Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons. \n\n"}
{"id": "1711.00505", "contents": "Title: Novel discoveries on the mathematical foundation of linear hydrodynamic\n  stability theory Abstract: We present some new discoveries on the mathematical foundation of linear\nhydrodynamic stability theory. The new discoveries are: 1. Linearized Euler\nequations fail to provide a linear approximation on inviscid hydrodynamic\nstability. 2. Eigenvalue instability predicted by high Reynolds number\nlinearized Navier-Stokes equations cannot capture the dominant instability of\nsuper fast growth. 3. As equations for directional differentials, Rayleigh\nequation and Orr-Sommerfeld equation cannot capture the nature of the full\ndifferentials. \n\n"}
{"id": "1711.02621", "contents": "Title: Convex Optimization with Unbounded Nonconvex Oracles using Simulated\n  Annealing Abstract: We consider the problem of minimizing a convex objective function $F$ when\none can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes\nsome structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function,\nmaking the task of minimizing $F$ intractable. To overcome this, prior work has\noften focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this\npaper we study the more general case when the noise has magnitude $\\alpha F(x)\n+ \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm\nthat finds an approximate minimizer of $F$ for this noise model. Previously,\nMarkov chains, such as the stochastic gradient Langevin dynamics, have been\nused to arrive at approximate solutions to these optimization problems.\nHowever, for the noise model considered in this paper, no single temperature\nallows such a Markov chain to both mix quickly and concentrate near the global\nminimizer. We bypass this by combining \"simulated annealing\" with the\nstochastic gradient Langevin dynamics, and gradually decreasing the temperature\nof the chain in order to approach the global minimizer. As a corollary one can\napproximately minimize a nonconvex function that is close to a convex function;\nhowever, the closeness can deteriorate as one moves away from the optimum. \n\n"}
{"id": "1711.02691", "contents": "Title: Bayesian Inference of Selection in the Wright-Fisher Diffusion Model Abstract: The increasing availability of population-level allele frequency data across\none or more related populations necessitates the development of methods that\ncan efficiently estimate population genetics parameters, such as the strength\nof selection acting on the population(s), from such data. Existing methods for\nthis problem in the setting of the Wright-Fisher diffusion model are primarily\nlikelihood-based, and rely on numerical approximation for likelihood\ncomputation and on bootstrapping for assessment of variability in the resulting\nestimates, requiring extensive computation. Recent work (Jenkins and Spano,\n2015) has provided a method for obtaining exact samples from general\nWright-Fisher diffusion processes, enabling the development of methods for\nBayesian estimation in this setting. We develop and implement a Bayesian method\nfor estimating the strength of selection based on the Wright-Fisher diffusion\nfor data sampled at a single time point. The method utilizes the work of\nJenkins and Spano (2015) to develop a Markov chain Monte Carlo algorithm to\ndraw samples from the joint posterior distribution of the selection coefficient\nand the allele frequencies. We demonstrate that when assumptions about the\ninitial allele frequencies are accurate the method performs well for both\nsimulated data and for an empirical data set on hypoxia in flies (Zhou et al.\n2011), where we find evidence for strong positive selection in a region of\nchromosome 2L previously identified by Ronen et al. (2013). We discuss possible\nextensions of our method to the more general settings commonly encountered in\npractice, highlighting the advantages of Bayesian approaches to inference in\nthis setting. \n\n"}
{"id": "1711.03668", "contents": "Title: Vortex axisymmetrization, inviscid damping, and vorticity depletion in\n  the linearized 2D Euler equations Abstract: Coherent vortices are often observed to persist for long times in turbulent\n2D flows even at very high Reynolds numbers and are observed in experiments and\ncomputer simulations to potentially be asymptotically stable in a weak sense\nfor the 2D Euler equations. We consider the incompressible 2D Euler equations\nlinearized around a radially symmetric, strictly monotone decreasing vorticity\ndistribution. For sufficiently regular data, we prove the inviscid damping of\nthe $\\theta$-dependent radial and angular velocity fields with the optimal\nrates $\\|u^r(t)\\| \\lesssim \\langle t \\rangle^{-1}$ and $\\|u^\\theta(t)\\|\n\\lesssim \\langle t \\rangle^{-2}$ in the appropriate radially weighted $L^2$\nspaces. We moreover prove that the vorticity weakly converges back to radial\nsymmetry as $t \\rightarrow \\infty$, a phenomenon known as vortex\naxisymmetrization in the physics literature, and characterize the dynamics in\nhigher Sobolev spaces. Furthermore, we prove that the $\\theta$-dependent\nangular Fourier modes in the vorticity are ejected from the origin as $t \\to\n\\infty$, resulting in faster inviscid damping rates than those possible with\npassive scalar evolution. This non-local effect is called vorticity depletion.\nOur work appears to be the first to find vorticity depletion relevant for the\ndynamics of vortices. \n\n"}
{"id": "1711.03954", "contents": "Title: EddyNet: A Deep Neural Network For Pixel-Wise Classification of Oceanic\n  Eddies Abstract: This work presents EddyNet, a deep learning based architecture for automated\neddy detection and classification from Sea Surface Height (SSH) maps provided\nby the Copernicus Marine and Environment Monitoring Service (CMEMS). EddyNet is\na U-Net like network that consists of a convolutional encoder-decoder followed\nby a pixel-wise classification layer. The output is a map with the same size of\nthe input where pixels have the following labels \\{'0': Non eddy, '1':\nanticyclonic eddy, '2': cyclonic eddy\\}. We investigate the use of SELU\nactivation function instead of the classical ReLU+BN and we use an overlap\nbased loss function instead of the cross entropy loss. Keras Python code, the\ntraining datasets and EddyNet weights files are open-source and freely\navailable on https://github.com/redouanelg/EddyNet. \n\n"}
{"id": "1711.04702", "contents": "Title: wTO: an R package for computing weighted topological overlap and\n  consensus networks with an integrated visualization tool Abstract: Network analyses, such as of gene co-expression networks, metabolic networks\nand ecological networks have become a central approach for the systems-level\nstudy of biological data. Several software packages exist for generating and\nanalyzing such networks, either from correlation scores or the absolute value\nof a transformed score called weighted topological overlap (wTO). However,\nsince gene regulatory processes can up- or down-regulate genes, it is of great\ninterest to explicitly consider both positive and negative correlations when\nconstructing a gene co-expression network. Here, we present an R package for\ncalculating the wTO, that, in contrast to existing packages, explicitly\naddresses the sign of the wTO values, and is thus especially valuable for the\nanalysis of gene regulatory networks. The package includes the calculation of\np-values (raw and adjusted) for each pairwise gene score. Our package also\nallows the calculation of networks from time series (without replicates). Since\nnetworks from independent datasets (biological repeats or related studies) are\nnot the same due to technical and biological noise in the data, we\nadditionally, incorporated a novel method for calculating a consensus network\n(CN) from two or more networks into our R package. We compare our new wTO\npackage to state of art packages and demonstrate the application of the wTO and\nCN functions using 3 independently derived datasets from healthy human\npre-frontal cortex samples. To showcase an example for the time series\napplication we utilized a metagenomics data set. In this work, we developed a\nsoftware package that allows the computation of wTO networks, CNs and a\nvisualization tool in the R statistical environment. It is publicly available\non CRAN repositories under the GPL-2 Open Source License\n(https://cran.r-project.org/web/packages/wTO/). \n\n"}
{"id": "1711.05174", "contents": "Title: Near-Optimal Discrete Optimization for Experimental Design: A Regret\n  Minimization Approach Abstract: The experimental design problem concerns the selection of k points from a\npotentially large design pool of p-dimensional vectors, so as to maximize the\nstatistical efficiency regressed on the selected k design points. Statistical\nefficiency is measured by optimality criteria, including A(verage),\nD(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the\nT-optimality, exact optimization is NP-hard.\n  We propose a polynomial-time regret minimization framework to achieve a\n$(1+\\varepsilon)$ approximation with only $O(p/\\varepsilon^2)$ design points,\nfor all the optimality criteria above.\n  In contrast, to the best of our knowledge, before our work, no\npolynomial-time algorithm achieves $(1+\\varepsilon)$ approximations for\nD/E/G-optimality, and the best poly-time algorithm achieving\n$(1+\\varepsilon)$-approximation for A/V-optimality requires $k =\n\\Omega(p^2/\\varepsilon)$ design points. \n\n"}
{"id": "1711.07424", "contents": "Title: Informed proposals for local MCMC in discrete spaces Abstract: There is a lack of methodological results to design efficient Markov chain\nMonte Carlo (MCMC) algorithms for statistical models with discrete-valued\nhigh-dimensional parameters. Motivated by this consideration, we propose a\nsimple framework for the design of informed MCMC proposals (i.e.\nMetropolis-Hastings proposal distributions that appropriately incorporate local\ninformation about the target) which is naturally applicable to both discrete\nand continuous spaces. We explicitly characterize the class of optimal proposal\ndistributions under this framework, which we refer to as locally-balanced\nproposals, and prove their Peskun-optimality in high-dimensional regimes. The\nresulting algorithms are straightforward to implement in discrete spaces and\nprovide orders of magnitude improvements in efficiency compared to alternative\nMCMC schemes, including discrete versions of Hamiltonian Monte Carlo.\nSimulations are performed with both simulated and real datasets, including a\ndetailed application to Bayesian record linkage. A direct connection with\ngradient-based MCMC suggests that locally-balanced proposals may be seen as a\nnatural way to extend the latter to discrete spaces. \n\n"}
{"id": "1711.07748", "contents": "Title: Model-based Clustering with Sparse Covariance Matrices Abstract: Finite Gaussian mixture models are widely used for model-based clustering of\ncontinuous data. Nevertheless, since the number of model parameters scales\nquadratically with the number of variables, these models can be easily\nover-parameterized. For this reason, parsimonious models have been developed\nvia covariance matrix decompositions or assuming local independence. However,\nthese remedies do not allow for direct estimation of sparse covariance matrices\nnor do they take into account that the structure of association among the\nvariables can vary from one cluster to the other. To this end, we introduce\nmixtures of Gaussian covariance graph models for model-based clustering with\nsparse covariance matrices. A penalized likelihood approach is employed for\nestimation and a general penalty term on the graph configurations can be used\nto induce different levels of sparsity and incorporate prior knowledge. Model\nestimation is carried out using a structural-EM algorithm for parameters and\ngraph structure estimation, where two alternative strategies based on a genetic\nalgorithm and an efficient stepwise search are proposed for inference. With\nthis approach, sparse component covariance matrices are directly obtained. The\nframework results in a parsimonious model-based clustering of the data via a\nflexible model for the within-group joint distribution of the variables.\nExtensive simulated data experiments and application to illustrative datasets\nshow that the method attains good classification performance and model quality. \n\n"}
{"id": "1711.08044", "contents": "Title: Photonuclear Reactions in Lightning Discovered from Detection of\n  Positrons and Neutrons Abstract: Lightning and thundercloud are the most dramatic natural particle\naccelerators on the Earth. Relativistic electrons accelerated by electric\nfields therein emit bremsstrahlung gamma rays, which have been detected at\nground observations, by airborne detectors, and as terrestrial gamma-ray\nflashes (TGFs) from space. The energy of the gamma rays is sufficiently high to\npotentially invoke atmospheric photonuclear reactions 14N(gamma, n)13N, which\nwould produce neutrons and eventually positrons via beta-plus decay of\ngenerated unstable radioactive isotopes, especially 13N. However, no clear\nobservational evidence for the reaction has been reported to date. Here we\nreport the first detection of neutron and positron signals from lightning with\na ground observation. During a thunderstorm on 6 February 2017 in Japan, a\nTGF-like intense flash (within 1 ms) was detected at our monitoring sites\n0.5-1.7 km away from the lightning. The subsequent initial burst quickly\nsubsided with an exponential decay constant of 40-60 ms, followed by a\nprolonged line emission at about 0.511 megaelectronvolt (MeV), lasting for a\nminute. The observed decay timescale and spectral cutoff at about 10 MeV of the\ninitial emission are well explained with de-excitation gamma rays from the\nnuclei excited by neutron capture. The centre energy of the prolonged line\nemission corresponds to the electron-positron annihilation, and hence is the\nconclusive indication of positrons produced after the lightning. Our detection\nof neutrons and positrons is unequivocal evidence that natural lightning\ntriggers photonuclear reactions. No other natural event on the Earth is known\nto trigger photonuclear reactions. This discovery places lightning as only the\nsecond known natural channel on the Earth after the atmospheric cosmic-ray\ninteraction, in which isotopes, such as 13C, 14C, and 15N, are produced. \n\n"}
{"id": "1711.09365", "contents": "Title: Ensemble-marginalized Kalman filter for linear time-dependent PDEs with\n  noisy boundary conditions: Application to heat transfer in building walls Abstract: In this work, we present the ensemble-marginalized Kalman filter (EnMKF), a\nsequential algorithm analogous to our previously proposed approach [1,2], for\nestimating the state and parameters of linear parabolic partial differential\nequations in initial-boundary value problems when the boundary data are noisy.\nWe apply EnMKF to infer the thermal properties of building walls and to\nestimate the corresponding heat flux from real and synthetic data. Compared\nwith a modified Ensemble Kalman Filter (EnKF) that is not marginalized, EnMKF\nreduces the bias error, avoids the collapse of the ensemble without needing to\nadd inflation, and converges to the mean field posterior using $50\\%$ or less\nof the ensemble size required by EnKF. According to our results, the\nmarginalization technique in EnMKF is key to performance improvement with\nsmaller ensembles at any fixed time. \n\n"}
{"id": "1711.11446", "contents": "Title: Stratosphere circulation on tidally locked ExoEarths Abstract: Stratosphere circulation is important to interpret abundances of\nphoto-chemically produced compounds like ozone that we aim to observe to assess\nhabitability of exoplanets. We thus investigate a tidally locked ExoEarth\nscenario for TRAPPIST-1b, TRAPPIST-1d, Proxima Centauri~b and GJ 667 C~f with a\nsimplified 3D atmosphere model and for different stratospheric wind breaking\nassumptions.\n  These planets are representatives for different circulation regimes for\norbital periods: $P_{orb}=1-100$~days. The circulation of exoplanets with\n$P_{orb} \\leq $ 25~days can be dominated by the standing tropical Rossby wave\nin the troposphere and also in the stratosphere: It leads to a strong\nequatorial eastward wind jet and to 'Anti-Brewer-Dobson'-circulation that\nconfines air masses to the stratospheric equatorial region. Thus, the\ndistribution of photo-chemically produced species and aerosols may be limited\nto an 'equatorial transport belt'. In contrast, planets with $P_{orb}>25$~days,\nlike GJ~667~C~f, exhibit efficient thermally driven circulation in the\nstratosphere that allows for a day side-wide distribution of air masses.\n  The influence of the standing tropical Rossby waves on tidally locked\nExoEarths with $P_{orb} \\leq 25$~days can, however, be circumvented with deep\nstratospheric wind breaking alone - allowing for equator-to-pole transport like\non Earth. For planets with $3 \\leq P_{orb} \\leq 6$~days, the extratropical\nRossby wave acts as an additional safe-guard against the tropical Rossby wave\nin case of shallow wind breaking. Therefore, TRAPPIST-1d is less prone to have\nan equatorial transport belt in the stratosphere than Proxima~Centauri~b.\n  Even our Earth model shows an equatorial wind jet, if stratosphere wind\nbreaking is inefficient. \n\n"}
{"id": "1712.00716", "contents": "Title: Convolutional Phase Retrieval via Gradient Descent Abstract: We study the convolutional phase retrieval problem, of recovering an unknown\nsignal $\\mathbf x \\in \\mathbb C^n $ from $m$ measurements consisting of the\nmagnitude of its cyclic convolution with a given kernel $\\mathbf a \\in \\mathbb\nC^m $. This model is motivated by applications such as channel estimation,\noptics, and underwater acoustic communication, where the signal of interest is\nacted on by a given channel/filter, and phase information is difficult or\nimpossible to acquire. We show that when $\\mathbf a$ is random and the number\nof observations $m$ is sufficiently large, with high probability $\\mathbf x$\ncan be efficiently recovered up to a global phase shift using a combination of\nspectral initialization and generalized gradient descent. The main challenge is\ncoping with dependencies in the measurement operator. We overcome this\nchallenge by using ideas from decoupling theory, suprema of chaos processes and\nthe restricted isometry property of random circulant matrices, and recent\nanalysis of alternating minimization methods. \n\n"}
{"id": "1712.00849", "contents": "Title: Comment: A brief survey of the current state of play for Bayesian\n  computation in data science at Big-Data scale Abstract: We wish to contribute to the discussion of \"Comparing Consensus Monte Carlo\nStrategies for Distributed Bayesian Computation\" by offering our views on the\ncurrent best methods for Bayesian computation, both at big-data scale and with\nsmaller data sets, as summarized in Table 1. This table is certainly an\nover-simplification of a highly complicated area of research in constant\n(present and likely future) flux, but we believe that constructing summaries of\nthis type is worthwhile despite their drawbacks, if only to facilitate further\ndiscussion. \n\n"}
{"id": "1712.01521", "contents": "Title: An Online Algorithm for Nonparametric Correlations Abstract: Nonparametric correlations such as Spearman's rank correlation and Kendall's\ntau correlation are widely applied in scientific and engineering fields. This\npaper investigates the problem of computing nonparametric correlations on the\nfly for streaming data. Standard batch algorithms are generally too slow to\nhandle real-world big data applications. They also require too much memory\nbecause all the data need to be stored in the memory before processing. This\npaper proposes a novel online algorithm for computing nonparametric\ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and\nis quite suitable for edge devices, where only limited memory and processing\npower are available. You can seek a balance between speed and accuracy by\nchanging the number of cutpoints specified in the algorithm. The online\nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster\nthan the corresponding batch algorithm, and it can compute them based either on\nall past observations or on fixed-size sliding windows. \n\n"}
{"id": "1712.02009", "contents": "Title: On the nonparametric maximum likelihood estimator for Gaussian location\n  mixture densities with application to Gaussian denoising Abstract: We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\nestimating Gaussian location mixture densities in $d$-dimensions from\nindependent observations. Unlike usual likelihood-based methods for fitting\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\nmultiplicative factors) when the true density is a discrete Gaussian mixture\nwithout any prior information on the number of mixture components. NPMLEs can\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\nestimator. Here our results imply that the empirical Bayes estimator performs\nat nearly the optimal level (up to logarithmic multiplicative factors) for\ndenoising in clustering situations without any prior knowledge of the number of\nclusters. \n\n"}
{"id": "1712.05460", "contents": "Title: Honey from the Hives: A Theoretical and Computational Exploration of\n  Combinatorial Hives Abstract: In the first half of this manuscript, we begin with a brief review of\ncombinatorial hives as introduced by Knutson and Tao, and focus on a conjecture\nby Danilov and Koshevoy for generating such a hive from Hermitian matrix pairs\nthrough an optimization scheme. We examine a proposal by Appleby and Whitehead\nin the spirit of this conjecture and analytically elucidate an obstruction in\ntheir construction for guaranteeing hive generation, while detailing stronger\nconditions under which we can produce hives with almost certain probability. We\nprovide the first mapping of this prescription onto a practical algorithmic\nspace that enables us to produce affirming computational results and open a new\narea of research into the analysis of the random geometries and curvatures of\nhive surfaces from select matrix ensembles.\n  The second part of this manuscript concerns Littlewood-Richardson\ncoefficients and methods of estimating them from the hive construction. We\nillustrate experimental confirmation of two numerical algorithms that we\nprovide as tools for the community: one as a rounded estimator on the\ncontinuous hive polytope volume following a proposal by Narayanan, and the\nother as a novel construction using a coordinate hit-and-run on the hive\nlattice itself. We compare the advantages of each, and include numerical\nresults on their accuracies for some tested cases. \n\n"}
{"id": "1712.08242", "contents": "Title: Exploring the Lyapunov instability properties of high-dimensional\n  atmospheric and climate models Abstract: The stability properties of intermediate-order climate models are\ninvestigated by computing their Lyapunov exponents (LEs). The two models\nconsidered are PUMA (Portable University Model of the Atmosphere), a\nprimitive-equation simple general circulation model, and MAOOAM (Modular\nArbitrary-Order Ocean-Atmosphere Model), a quasi-geostrophic coupled\nocean-atmosphere model on a beta-plane. We wish to investigate the effect of\nthe different levels of filtering on the instabilities and dynamics of the\natmospheric flows. Moreover, we assess the impact of the oceanic coupling, the\ndissipation scheme and the resolution on the spectra of LEs.\n  The PUMA Lyapunov spectrum is computed for two different values of the\nmeridional temperature gradient defining the Newtonian forcing. The increase of\nthe gradient gives rise to a higher baroclinicity and stronger instabilities,\ncorresponding to a larger dimension of the unstable manifold and a larger first\nLE. The convergence rate of the rate functional for the large deviation law of\nthe finite-time Lyapunov exponents (FTLEs) is fast for all exponents, which can\nbe interpreted as resulting from the absence of a clear-cut atmospheric\ntime-scale separation in such a model.\n  The MAOOAM spectra show that the dominant atmospheric instability is\ncorrectly represented even at low resolutions. However, the dynamics of the\ncentral manifold, which is mostly associated to the ocean dynamics, is not\nfully resolved because of its associated long time scales, even at intermediate\norders.\n  This paper highlights the need to investigate the natural variability of the\natmosphere-ocean coupled dynamics by associating rate of growth and decay of\nperturbations to the physical modes described using the formalism of the\ncovariant Lyapunov vectors and to consider long integrations in order to\ndisentangle the dynamical processes occurring at all time scales. \n\n"}
{"id": "1712.08364", "contents": "Title: Differential geometry and stochastic dynamics with deep learning\n  numerics Abstract: In this paper, we demonstrate how deterministic and stochastic dynamics on\nmanifolds, as well as differential geometric constructions can be implemented\nconcisely and efficiently using modern computational frameworks that mix\nsymbolic expressions with efficient numerical computations. In particular, we\nuse the symbolic expression and automatic differentiation features of the\npython library Theano, originally developed for high-performance computations\nin deep learning. We show how various aspects of differential geometry and Lie\ngroup theory, connections, metrics, curvature, left/right invariance, geodesics\nand parallel transport can be formulated with Theano using the automatic\ncomputation of derivatives of any order. We will also show how symbolic\nstochastic integrators and concepts from non-linear statistics can be\nformulated and optimized with only a few lines of code. We will then give\nexplicit examples on low-dimensional classical manifolds for visualization and\ndemonstrate how this approach allows both a concise implementation and\nefficient scaling to high dimensional problems. \n\n"}
{"id": "1712.08837", "contents": "Title: Optimization and Testing in Linear Non-Gaussian Component Analysis Abstract: Independent component analysis (ICA) decomposes multivariate data into\nmutually independent components (ICs). The ICA model is subject to a constraint\nthat at most one of these components is Gaussian, which is required for model\nidentifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the\nICA model to a linear latent factor model with any number of both non-Gaussian\ncomponents (signals) and Gaussian components (noise), where observations are\nlinear combinations of independent components. Although the individual Gaussian\ncomponents are not identifiable, the Gaussian subspace is identifiable. We\nintroduce an estimator along with its optimization approach in which\nnon-Gaussian and Gaussian components are estimated simultaneously, maximizing\nthe discrepancy of each non-Gaussian component from Gaussianity while\nminimizing the discrepancy of each Gaussian component from Gaussianity. When\nthe number of non-Gaussian components is unknown, we develop a statistical test\nto determine it based on resampling and the discrepancy of estimated\ncomponents. Through a variety of simulation studies, we demonstrate the\nimprovements of our estimator over competing estimators, and we illustrate the\neffectiveness of the test to determine the number of non-Gaussian components.\nFurther, we apply our method to real data examples and demonstrate its\npractical value. \n\n"}
{"id": "1712.10131", "contents": "Title: Sparse Polynomial Chaos Expansions via Compressed Sensing and D-optimal\n  Design Abstract: In the field of uncertainty quantification, sparse polynomial chaos (PC)\nexpansions are commonly used by researchers for a variety of purposes, such as\nsurrogate modeling. Ideas from compressed sensing may be employed to exploit\nthis sparsity in order to reduce computational costs. A class of greedy\ncompressed sensing algorithms use least squares minimization to approximate PC\ncoefficients. This least squares problem lends itself to the theory of optimal\ndesign of experiments (ODE). Our work focuses on selecting an experimental\ndesign that improves the accuracy of sparse PC approximations for a fixed\ncomputational budget. We propose DSP, a novel sequential design, greedy\nalgorithm for sparse PC approximation. The algorithm sequentially augments an\nexperimental design according to a set of the basis polynomials deemed\nimportant by the magnitude of their coefficients, at each iteration. Our\nalgorithm incorporates topics from ODE to estimate the PC coefficients. A\nvariety of numerical simulations are performed on three physical models and\nmanufactured sparse PC expansions to provide a comparative study between our\nproposed algorithm and other non-adaptive methods. Further, we examine the\nimportance of sampling by comparing different strategies in terms of their\nability to generate a candidate pool from which an optimal experimental design\nis chosen. It is demonstrated that the most accurate PC coefficient\napproximations, with the least variability, are produced with our\ndesign-adaptive greedy algorithm and the use of a studied importance sampling\nstrategy. We provide theoretical and numerical results which show that using an\noptimal sampling strategy for the candidate pool is key, both in terms of\naccuracy in the approximation, but also in terms of constructing an optimal\ndesign. \n\n"}
{"id": "1712.10204", "contents": "Title: Large-scale vorticity generation and kinetic energy budget along the\n  U.S. West Coast Abstract: We attempt to evaluate energy budget over a restricted but extremely well\nstudied oceanic region along the shorelines of Oregon and California. The\nanalysis is based on a recently updated geostrophic flow field data set\ncovering 22 years with daily resolution on a grid of\n0.25$^\\circ\\times$0.25$^\\circ$, and turbulent wind stress data from the\nERA-Interim reanalysis over the same geographic region with the same temporal\nand spatial resolutions. Integrated 2D kinetic energy, enstrophy, wind stress\nwork and { kinetic energy tendency} are determined separately for the shore-\nand open water regions. The empirical analysis is supported by 2D lattice\nBoltzmann simulations of freely decaying vortices along a rough solid wall,\nwhich permits to separate the pure shoreline effects and dissipation properties\nof surface flow fields. Comparisons clearly demonstrate that kinetic energy and\nvorticity { of the geostrophic flow field} are mostly generated along the\nshorelines and advected to the open water regions, where the net wind stress\nwork is almost negligible. Our results support that the geostrophic flow field\nis quasistationary on the timescale of a couple of days, thus total forcing is\npractically equal to total dissipation. Estimates of unknown terms in the\nequation of oceanic kinetic energy budget are based on other studies,\nnevertheless our results suggest that { an effective} eddy kinematic viscosity\nis in the order of magnitude $10^{-2}$ m$^2$/s along the shorelines, and it is\nlower { by a factor of two} in the open water region. \n\n"}
{"id": "1801.00658", "contents": "Title: Comparison of stochastic parameterizations in the framework of a coupled\n  ocean-atmosphere model Abstract: A new framework is proposed for the evaluation of stochastic subgrid-scale\nparameterizations in the context of MAOOAM, a coupled ocean-atmosphere model of\nintermediate complexity. Two physically-based parameterizations are\ninvestigated, the first one based on the singular perturbation of Markov\noperator, also known as homogenization. The second one is a recently proposed\nparameterization based on the Ruelle's response theory. The two\nparameterization are implemented in a rigorous way, assuming however that the\nunresolved scale relevant statistics are Gaussian. They are extensively tested\nfor a low-order version known to exhibit low-frequency variability, and some\npreliminary results are obtained for an intermediate-order version. Several\ndifferent configurations of the resolved-unresolved scale separations are then\nconsidered. Both parameterizations show remarkable performances in correcting\nthe impact of model errors, being even able to change the modality of the\nprobability distributions. Their respective limitations are also discussed. \n\n"}
{"id": "1801.00718", "contents": "Title: Selective review of offline change point detection methods Abstract: This article presents a selective survey of algorithms for the offline\ndetection of multiple change points in multivariate time series. A general yet\nstructuring methodological strategy is adopted to organize this vast body of\nwork. More precisely, detection algorithms considered in this review are\ncharacterized by three elements: a cost function, a search method and a\nconstraint on the number of changes. Each of those elements is described,\nreviewed and discussed separately. Implementations of the main algorithms\ndescribed in this article are provided within a Python package called ruptures. \n\n"}
{"id": "1801.02106", "contents": "Title: Bayesian Lasso Posterior Sampling via Parallelized Measure Transport Abstract: It is well known that the Lasso can be interpreted as a Bayesian posterior\nmode estimate with a Laplacian prior. Obtaining samples from the full posterior\ndistribution, the Bayesian Lasso, confers major advantages in performance as\ncompared to having only the Lasso point estimate. Traditionally, the Bayesian\nLasso is implemented via Gibbs sampling methods which suffer from lack of\nscalability, unknown convergence rates, and generation of samples that are\nnecessarily correlated. We provide a measure transport approach to generate\ni.i.d samples from the posterior by constructing a transport map that\ntransforms a sample from the Laplacian prior into a sample from the posterior.\nWe show how the construction of this transport map can be parallelized into\nmodules that iteratively solve Lasso problems and perform closed-form linear\nalgebra updates. With this posterior sampling method, we perform maximum\nlikelihood estimation of the Lasso regularization parameter via the EM\nalgorithm. We provide comparisons to traditional Gibbs samplers using the\ndiabetes dataset of Efron et al. Lastly, we give an example implementation on a\ncomputing system that leverages parallelization, a graphics processing unit,\nwhose execution time has much less dependence on dimension as compared to a\nstandard implementation. \n\n"}
{"id": "1801.03612", "contents": "Title: Using probabilistic programs as proposals Abstract: Monte Carlo inference has asymptotic guarantees, but can be slow when using\ngeneric proposals. Handcrafted proposals that rely on user knowledge about the\nposterior distribution can be efficient, but are difficult to derive and\nimplement. This paper proposes to let users express their posterior knowledge\nin the form of proposal programs, which are samplers written in probabilistic\nprogramming languages. One strategy for writing good proposal programs is to\ncombine domain-specific heuristic algorithms with neural network models. The\nheuristics identify high probability regions, and the neural networks model the\nposterior uncertainty around the outputs of the algorithm. Proposal programs\ncan be used as proposal distributions in importance sampling and\nMetropolis-Hastings samplers without sacrificing asymptotic consistency, and\ncan be optimized offline using inference compilation. Support for optimizing\nand using proposal programs is easily implemented in a sampling-based\nprobabilistic programming runtime. The paper illustrates the proposed technique\nwith a proposal program that combines RANSAC and neural networks to accelerate\ninference in a Bayesian linear regression with outliers model. \n\n"}
{"id": "1801.05661", "contents": "Title: A Randomized Exchange Algorithm for Computing Optimal Approximate\n  Designs of Experiments Abstract: We propose a class of subspace ascent methods for computing optimal\napproximate designs that covers both existing as well as new and more efficient\nalgorithms. Within this class of methods, we construct a simple, randomized\nexchange algorithm (REX). Numerical comparisons suggest that the performance of\nREX is comparable or superior to the performance of state-of-the-art methods\nacross a broad range of problem structures and sizes. We focus on the most\ncommonly used criterion of D-optimality that also has applications beyond\nexperimental design, such as the construction of the minimum volume ellipsoid\ncontaining a given set of data-points. For D-optimality, we prove that the\nproposed algorithm converges to the optimum. We also provide formulas for the\noptimal exchange of weights in the case of the criterion of A-optimality. These\nformulas enable one to use REX for computing A-optimal and I-optimal designs. \n\n"}
{"id": "1801.05935", "contents": "Title: Computation of the Maximum Likelihood estimator in low-rank Factor\n  Analysis Abstract: Factor analysis, a classical multivariate statistical technique is popularly\nused as a fundamental tool for dimensionality reduction in statistics,\neconometrics and data science. Estimation is often carried out via the Maximum\nLikelihood (ML) principle, which seeks to maximize the likelihood under the\nassumption that the positive definite covariance matrix can be decomposed as\nthe sum of a low rank positive semidefinite matrix and a diagonal matrix with\nnonnegative entries. This leads to a challenging rank constrained nonconvex\noptimization problem. We reformulate the low rank ML Factor Analysis problem as\na nonlinear nonsmooth semidefinite optimization problem, study various\nstructural properties of this reformulation and propose fast and scalable\nalgorithms based on difference of convex (DC) optimization. Our approach has\ncomputational guarantees, gracefully scales to large problems, is applicable to\nsituations where the sample covariance matrix is rank deficient and adapts to\nvariants of the ML problem with additional constraints on the problem\nparameters. Our numerical experiments demonstrate the significant usefulness of\nour approach over existing state-of-the-art approaches. \n\n"}
{"id": "1801.07856", "contents": "Title: Random matrix theory for an adiabatically-varying oceanic acoustic\n  waveguide Abstract: Problem of sound propagation in the ocean is considered. A novel approach of\nK. Hegewisch and S. Tomsovic for statistical modelling of acoustic wavefields\nin the random ocean is examined. The approach is based on construction of a\nwavefield propagator by means of random matrix theory. It is shown that this\napproach can be generalized onto acoustic waveguides with adiabatic\nlongitudinal variations. Efficient generalization is obtained by means of\nstepwise approximation of the propagator. Accuracy of the generalized approach\nis confirmed numerically for a model of an underwater sound channel crossing a\ncold synoptic eddy. It is found that the eddy leads to substantial suppression\nof sound scattering. \n\n"}
{"id": "1801.07873", "contents": "Title: Gaussian variational approximation for high-dimensional state space\n  models Abstract: Our article considers a Gaussian variational approximation of the posterior\ndensity in a high-dimensional state space model. The variational parameters to\nbe optimized are the mean vector and the covariance matrix of the\napproximation. The number of parameters in the covariance matrix grows as the\nsquare of the number of model parameters, so it is necessary to find simple yet\neffective parameterizations of the covariance structure when the number of\nmodel parameters is large. We approximate the joint posterior distribution over\nthe high-dimensional state vectors by a dynamic factor model, having Markovian\ntime dependence and a factor covariance structure for the states. This gives a\nreduced description of the dependence structure for the states, as well as a\ntemporal conditional independence structure similar to that in the true\nposterior. The usefulness of the approach is illustrated for prediction in two\nhigh-dimensional applications that are challenging for Markov chain Monte Carlo\nsampling. The first is a spatio-temporal model for the spread of the Eurasian\nCollared-Dove across North America; the second is a Wishart-based multivariate\nstochastic volatility model for financial returns. \n\n"}
{"id": "1801.08238", "contents": "Title: Bernhard Haurwitz Memorial Lecture (2017): Potential Vorticity Aspects\n  of Tropical Dynamics Abstract: This paper is the textual material accompanying the 2017 Bernhard Haurwitz\nMemorial Lecture, delivered by the author on 28 June 2017, at a joint session\nof the American Meteorological Society's 21st Conference on Atmospheric and\nOceanic Fluid Dynamics and 19th Conference on Middle Atmosphere (26-30 June\n2017, Portland, OR). \n\n"}
{"id": "1802.02695", "contents": "Title: The physics of climate change: simple models in climate science Abstract: There is a perception that climate science can only be approached with\ncomplex computer simulations. But working climate scientists often use simple\nmodels to understand their simulations and make order-of-magnitude estimates.\nThis article presents some of these simple models with the goal of making\nclimate science more accessible and comprehensible. \n\n"}
{"id": "1802.04686", "contents": "Title: Remote sensing of geomagnetic fields and atomic collisions in the\n  mesosphere Abstract: Magnetic-field sensing has contributed to the formulation of the\nplate-tectonics theory, the discovery and mapping of underground structures on\nEarth, and the study of magnetism in other planets. Filling the gap between\nspace-based and near-Earth observation, we demonstrate a novel method for\nremote measurement of the geomagnetic field at an altitude of 85-100 km. The\nmethod consists of optical pumping of atomic sodium in the upper mesosphere\nwith an intensity-modulated laser beam, and simultaneous ground-based\nobservation of the resultant magneto-optical resonance when driving the\natomic-sodium spins at the Larmor precession frequency. The experiment was\ncarried out at the Roque de Los Muchachos Observatory in La Palma (Canary\nIslands) where we validated this technique and remotely measured the Larmor\nprecession frequency of sodium as 260.4(1) kHz, corresponding to a mesospheric\nmagnetic field of 0.3720(1) G. We demonstrate a magnetometry accuracy level of\n0.28 mG/$\\sqrt{\\text{Hz}}$ in good atmospheric conditions. In addition, these\nobservations allow us to characterize various atomic-collision processes in the\nmesosphere. Remote detection of mesospheric magnetic fields has potential\napplications such as mapping of large-scale magnetic structures in the\nlithosphere and the study of electric-current fluctuations in the ionosphere. \n\n"}
{"id": "1802.04791", "contents": "Title: Stochastic Variance-Reduced Hamilton Monte Carlo Methods Abstract: We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling\nfrom a smooth and strongly log-concave distribution. At the core of our\nproposed method is a variance reduction technique inspired by the recent\nadvance in stochastic optimization. We show that, to achieve $\\epsilon$\naccuracy in 2-Wasserstein distance, our algorithm achieves $\\tilde\nO(n+\\kappa^{2}d^{1/2}/\\epsilon+\\kappa^{4/3}d^{1/3}n^{2/3}/\\epsilon^{2/3})$\ngradient complexity (i.e., number of component gradient evaluations), which\noutperforms the state-of-the-art HMC and stochastic gradient HMC methods in a\nwide regime. We also extend our algorithm for sampling from smooth and general\nlog-concave distributions, and prove the corresponding gradient complexity as\nwell. Experiments on both synthetic and real data demonstrate the superior\nperformance of our algorithm. \n\n"}
{"id": "1802.06151", "contents": "Title: Scalable Inference for Space-Time Gaussian Cox Processes Abstract: The log-Gaussian Cox process is a flexible and popular class of point pattern\nmodels for capturing spatial and space-time dependence for point patterns.\nModel fitting requires approximation of stochastic integrals which is\nimplemented through discretization over the domain of interest. With fine scale\ndiscretization, inference based on Markov chain Monte Carlo is computationally\nburdensome because of the cost of matrix decompositions and storage, such as\nthe Cholesky, for high dimensional covariance matrices associated with latent\nGaussian variables. This article addresses these computational bottlenecks by\ncombining two recent developments: (i) a data augmentation strategy that has\nbeen proposed for space-time Gaussian Cox processes that is based on exact\nBayesian inference and does not require fine grid approximations for infinite\ndimensional integrals, and (ii) a recently developed family of\nsparsity-inducing Gaussian processes, called nearest-neighbor Gaussian\nprocesses, to avoid expensive matrix computations. Our inference is delivered\nwithin the fully model-based Bayesian paradigm and does not sacrifice the\nrichness of traditional log-Gaussian Cox processes. We apply our method to\ncrime event data in San Francisco and investigate the recovery of the intensity\nsurface. \n\n"}
{"id": "1802.07643", "contents": "Title: Floating structures in shallow water: local well-posedness in the\n  axisymmetric case Abstract: The floating structure problem describes the interaction between surface\nwater waves and a floating body, generally a boat or a wave energy converter.\nAs shown by Lannes in [18] the equations for the fluid motion can be reduced to\na set of two evolution equations on the surface elevation and the horizontal\ndischarge. The presence of the object is accounted for by a constraint on the\ndischarge under the object; the pressure exerted by the fluid on this object is\nthen the Lagrange multiplier associated to this constraint. Our goal in this\npaper is to prove the well-posedness of this fluid-structure interaction\nproblem in the shallow water approximation under the assumption that the flow\nis axisymmetric without swirl. We write the fluid equations as a quasilinear\nhyperbolic mixed initial boundary value problem and the solid equation as a\nsecond order ODE coupled to the fluid equations. Finally we prove the local in\ntime well-posedness for this coupled problem, provided some compatibility\nconditions on the initial data are satisfied. \n\n"}
{"id": "1802.08055", "contents": "Title: A Learning Based Approach for Uncertainty Analysis in Numerical Weather\n  Prediction Models Abstract: Complex numerical weather prediction models incorporate a variety of physical\nprocesses, each described by multiple alternative physical schemes with\nspecific parameters. The selection of the physical schemes and the choice of\nthe corresponding physical parameters during model configuration can\nsignificantly impact the accuracy of model forecasts. There is no combination\nof physical schemes that works best for all times, at all locations, and under\nall conditions. It is therefore of considerable interest to understand the\ninterplay between the choice of physics and the accuracy of the resulting\nforecasts under different conditions. This paper demonstrates the use of\nmachine learning techniques to study the uncertainty in numerical weather\nprediction models due to the interaction of multiple physical processes. The\nfirst problem addressed herein is the estimation of systematic model errors in\noutput quantities of interest at future times, and the use of this information\nto improve the model forecasts. The second problem considered is the\nidentification of those specific physical processes that contribute most to the\nforecast uncertainty in the quantity of interest under specified meteorological\nconditions.\n  The discrepancies between model results and observations at past times are\nused to learn the relationships between the choice of physical processes and\nthe resulting forecast errors. Numerical experiments are carried out with the\nWeather Research and Forecasting (WRF) model. The output quantity of interest\nis the model precipitation, a variable that is both extremely important and\nvery challenging to forecast. The physical processes under consideration\ninclude various micro-physics schemes, cumulus parameterizations, short wave,\nand long wave radiation schemes. The experiments demonstrate the strong\npotential of machine learning approaches to aid the study of model errors. \n\n"}
{"id": "1802.08318", "contents": "Title: Proportional Volume Sampling and Approximation Algorithms for A-Optimal\n  Design Abstract: We study the optimal design problems where the goal is to choose a set of\nlinear measurements to obtain the most accurate estimate of an unknown vector\nin $d$ dimensions. We study the $A$-optimal design variant where the objective\nis to minimize the average variance of the error in the maximum likelihood\nestimate of the vector being measured. The problem also finds applications in\nsensor placement in wireless networks, sparse least squares regression, feature\nselection for $k$-means clustering, and matrix approximation. In this paper, we\nintroduce proportional volume sampling to obtain improved approximation\nalgorithms for $A$-optimal design. Our main result is to obtain improved\napproximation algorithms for the $A$-optimal design problem by introducing the\nproportional volume sampling algorithm. Our results nearly optimal bounds in\nthe asymptotic regime when the number of measurements done, $k$, is\nsignificantly more than the dimension $d$. We also give first approximation\nalgorithms when $k$ is small including when $k=d$. The proportional\nvolume-sampling algorithm also gives approximation algorithms for other optimal\ndesign objectives such as $D$-optimal design and generalized ratio objective\nmatching or improving previous best known results. Interestingly, we show that\na similar guarantee cannot be obtained for the $E$-optimal design problem. We\nalso show that the $A$-optimal design problem is NP-hard to approximate within\na fixed constant when $k=d$. \n\n"}
{"id": "1802.08671", "contents": "Title: Langevin Monte Carlo and JKO splitting Abstract: Algorithms based on discretizing Langevin diffusion are popular tools for\nsampling from high-dimensional distributions. We develop novel connections\nbetween such Monte Carlo algorithms, the theory of Wasserstein gradient flow,\nand the operator splitting approach to solving PDEs. In particular, we show\nthat a proximal version of the Unadjusted Langevin Algorithm corresponds to a\nscheme that alternates between solving the gradient flows of two specific\nfunctionals on the space of probability measures. Using this perspective, we\nderive some new non-asymptotic results on the convergence properties of this\nalgorithm. \n\n"}
{"id": "1802.08798", "contents": "Title: Automatic adaptation of MCMC algorithms Abstract: Markov chain Monte Carlo (MCMC) methods are ubiquitous tools for\nsimulation-based inference in many fields but designing and identifying good\nMCMC samplers is still an open question. This paper introduces a novel MCMC\nalgorithm, namely, Auto Adapt MCMC. For sampling variables or blocks of\nvariables, we use two levels of adaptation where the inner adaptation optimizes\nthe MCMC performance within each sampler, while the outer adaptation explores\nthe valid space of kernels to find the optimal samplers. We provide a\ntheoretical foundation for our approach. To show the generality and usefulness\nof the approach, we describe a framework using only standard MCMC samplers as\ncandidate samplers and some adaptation schemes for both inner and outer\niterations. In several benchmark problems, we show that our proposed approach\nsubstantially outperforms other approaches, including an automatic blocking\nalgorithm, in terms of MCMC efficiency and computational time. \n\n"}
{"id": "1802.08898", "contents": "Title: Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from\nhigh-dimensional distributions in Statistics and Machine learning. HMC is known\nto run very efficiently in practice and its popular second-order \"leapfrog\"\nimplementation has long been conjectured to run in $d^{1/4}$ gradient\nevaluations. Here we show that this conjecture is true when sampling from\nstrongly log-concave target distributions that satisfy a weak third-order\nregularity property associated with the input data. Our regularity condition is\nweaker than the Lipschitz Hessian property and allows us to show faster\nconvergence bounds for a much larger class of distributions than would be\npossible with the usual Lipschitz Hessian constant alone. Important\ndistributions that satisfy our regularity condition include posterior\ndistributions used in Bayesian logistic regression for which the data satisfies\nan \"incoherence\" property. Our result compares favorably with the best\navailable bounds for the class of strongly log-concave distributions, which\ngrow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our\nsimulations on synthetic data suggest that, when our regularity condition is\nsatisfied, leapfrog HMC performs better than its competitors -- both in terms\nof accuracy and in terms of the number of gradient evaluations it requires. \n\n"}
{"id": "1802.08901", "contents": "Title: A quasi-physical dynamic reduced order model for thermospheric mass\n  density via Hermitian Space Dynamic Mode Decomposition Abstract: Thermospheric mass density is a major driver of satellite drag, the largest\nsource of uncertainty in accurately predicting the orbit of satellites in low\nEarth orbit (LEO) pertinent to space situational awareness. Most existing\nmodels for thermosphere are either physics-based or empirical. Physics-based\nmodels offer the potential for good predictive/forecast capabilities but\nrequire dedicated parallel resources for real-time evaluation and data\nassimilative capabilities that have yet to be developed. Empirical models are\nfast to evaluate, but offer very limited forecasting abilities. This paper\npresents a methodology of developing a reduced-order dynamic model from\nhigh-dimensional physics-based models by capturing the underlying dynamical\nbehavior. This work develops a quasi-physical reduced order model (ROM) for\nthermospheric mass density using simulated output from NCAR's\nThermosphere-Ionosphere-Electrodynamics General Circular Model (TIE-GCM). The\nROM is derived using a dynamic system formulation from a large dataset of\nTIE-GCM simulations spanning 12 years and covering a complete solar cycle.\nTowards this end, a new reduced order modeling approach, based on Dynamic Mode\nDecomposition with control (DMDc), that uses the Hermitian space of the problem\nto derive the dynamics and input matrices in a tractable manner is developed.\nResults show that the ROM performs well in serving as a reduced order surrogate\nfor TIE-GCM while almost always maintaining the forecast error to within 5\\% of\nthe simulated densities after 24 hours. \n\n"}
{"id": "1803.00847", "contents": "Title: Vertically Sheared Horizontal Flow-Forming Instability in Stratified\n  Turbulence: Analytical Linear Stability Analysis of Statistical State\n  Dynamics Equilibria Abstract: Vertically banded zonal jets are frequently observed in weakly or\nnon-rotating stratified turbulence, with the quasi-biennial oscillation in the\nequatorial stratosphere and the ocean's equatorial deep jets being two\nexamples. Explaining the formation of jets in stratified turbulence is a\nfundamental problem in geophysical fluid dynamics. Statistical state dynamics\n(SSD) provides powerful methods for analyzing turbulent systems exhibiting\nemergent organization, such as banded jets. In SSD, dynamical equations are\nwritten directly for the evolution of the turbulence statistics, enabling\ndirect analysis of the statistical interactions between the incoherent\ncomponent of the turbulence and the coherent large-scale structure component\nthat underlie jet formation. A second-order closure of SSD, known as S3T, has\npreviously been applied to show that meridionally banded jets emerge in\nbarotropic beta-plane turbulence via a statistical instability referred to as\nthe zonostrophic instability. Two-dimensional Boussinesq turbulence provides a\nsimple model of non-rotating stratified turbulence analogous to the beta-plane\nmodel of planetary turbulence. Jets known as vertically sheared horizontal\nflows (VSHFs) often emerge in simulations of Boussinesq turbulence, but their\ndynamics is not yet clearly understood. In this work S3T analysis of the\nzonostrophic instability is extended to study VSHF emergence in two-dimensional\nBoussinesq turbulence using an analytical formulation of S3T amenable to\nperturbation stability analysis. VSHFs are shown to form via an instability\nthat is analogous in stratified turbulence to the zonostrophic instability in\nbeta-plane turbulence. This instability is shown to be strikingly similar to\nthe zonostrophic instability, suggesting that jet emergence in both geostrophic\nand non-rotating stratified turbulence may be understood as instances of the\nsame generic phenomenon. \n\n"}
{"id": "1803.01328", "contents": "Title: WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling Abstract: To train an inference network jointly with a deep generative topic model,\nmaking it both scalable to big corpora and fast in out-of-sample prediction, we\ndevelop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet\nallocation, which infers posterior samples via a hybrid of stochastic-gradient\nMCMC and autoencoding variational Bayes. The generative network of WHAI has a\nhierarchy of gamma distributions, while the inference network of WHAI is a\nWeibull upward-downward variational autoencoder, which integrates a\ndeterministic-upward deep neural network, and a stochastic-downward deep\ngenerative model based on a hierarchy of Weibull distributions. The Weibull\ndistribution can be used to well approximate a gamma distribution with an\nanalytic Kullback-Leibler divergence, and has a simple reparameterization via\nthe uniform noise, which help efficiently compute the gradients of the evidence\nlower bound with respect to the parameters of the inference network. The\neffectiveness and efficiency of WHAI are illustrated with experiments on big\ncorpora. \n\n"}
{"id": "1803.02032", "contents": "Title: John's Walk Abstract: We present an affine-invariant random walk for drawing uniform random samples\nfrom a convex body $\\mathcal{K} \\subset \\mathbb{R}^n$ that uses maximum volume\ninscribed ellipsoids, known as John's ellipsoids, for the proposal\ndistribution. Our algorithm makes steps using uniform sampling from the John's\nellipsoid of the symmetrization of $\\mathcal{K}$ at the current point. We show\nthat from a warm start, the random walk mixes in $\\widetilde{O}(n^7)$ steps\nwhere the log factors depend only on constants associated with the warm start\nand desired total variation distance to uniformity. We also prove polynomial\nmixing bounds starting from any fixed point $x$ such that for any chord $pq$ of\n$\\mathcal{K}$ containing $x$, $\\left|\\log \\frac{|p-x|}{|q-x|}\\right|$ is\nbounded above by a polynomial in $n$. \n\n"}
{"id": "1803.02272", "contents": "Title: A geometric view of Biodiversity: scaling to metagenomics Abstract: We have designed a new efficient dimensionality reduction algorithm in order\nto investigate new ways of accurately characterizing the biodiversity, namely\nfrom a geometric point of view, scaling with large environmental sets produced\nby NGS ($\\sim 10^5$ sequences). The approach is based on Multidimensional\nScaling (MDS) that allows for mapping items on a set of $n$ points into a low\ndimensional euclidean space given the set of pairwise distances. We compute all\npairwise distances between reads in a given sample, run MDS on the distance\nmatrix, and analyze the projection on first axis, by visualization tools. We\nhave circumvented the quadratic complexity of computing pairwise distances by\nimplementing it on a hyperparallel computer (Turing, a Blue Gene Q), and the\ncubic complexity of the spectral decomposition by implementing a dense random\nprojection based algorithm. We have applied this data analysis scheme on a set\nof $10^5$ reads, which are amplicons of a diatom environmental sample from Lake\nGeneva. Analyzing the shape of the point cloud paves the way for a geometric\nanalysis of biodiversity, and for accurately building OTUs (Operational\nTaxonomic Units), when the data set is too large for implementing unsupervised,\nhierarchical, high-dimensional clustering. \n\n"}
{"id": "1803.03858", "contents": "Title: Testing One Hypothesis Multiple Times: The Multidimensional Case Abstract: The identification of new rare signals in data, the detection of a sudden\nchange in a trend, and the selection of competing models, are among the most\nchallenging problems in statistical practice. These challenges can be tackled\nusing a test of hypothesis where a nuisance parameter is present only under the\nalternative, and a computationally efficient solution can be obtained by the\n\"Testing One Hypothesis Multiple times\" (TOHM) method. In the one-dimensional\nsetting, a fine discretization of the space of the non-identifiable parameter\nis specified, and a global p-value is obtained by approximating the\ndistribution of the supremum of the resulting stochastic process. In this\npaper, we propose a computationally efficient inferential tool to perform TOHM\nin the multidimensional setting. Here, the approximations of interest typically\ninvolve the expected Euler Characteristics (EC) of the excursion set of the\nunderlying random field. We introduce a simple algorithm to compute the EC in\nmultiple dimensions and for arbitrary large significance levels. This leads to\nan highly generalizable computational tool to perform inference under\nnon-standard regularity conditions. \n\n"}
{"id": "1803.04084", "contents": "Title: Link prediction for egocentrically sampled networks Abstract: Link prediction in networks is typically accomplished by estimating or\nranking the probabilities of edges for all pairs of nodes. In practice,\nespecially for social networks, the data are often collected by egocentric\nsampling, which means selecting a subset of nodes and recording all of their\nedges. This sampling mechanism requires different prediction tools than the\ntypical assumption of links missing at random. We propose a new computationally\nefficient link prediction algorithm for egocentrically sampled networks, which\nestimates the underlying probability matrix by estimating its row space. For\nnetworks created by sampling rows, our method outperforms many popular link\nprediction and graphon estimation techniques. \n\n"}
{"id": "1803.05554", "contents": "Title: Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models Abstract: Learning a Bayesian network (BN) from data can be useful for decision-making\nor discovering causal relationships. However, traditional methods often fail in\nmodern applications, which exhibit a larger number of observed variables than\ndata points. The resulting uncertainty about the underlying network as well as\nthe desire to incorporate prior information recommend a Bayesian approach to\nlearning the BN, but the highly combinatorial structure of BNs poses a striking\nchallenge for inference. The current state-of-the-art methods such as order\nMCMC are faster than previous methods but prevent the use of many natural\nstructural priors and still have running time exponential in the maximum\nindegree of the true directed acyclic graph (DAG) of the BN. We here propose an\nalternative posterior approximation based on the observation that, if we\nincorporate empirical conditional independence tests, we can focus on a\nhigh-probability DAG associated with each order of the vertices. We show that\nour method allows the desired flexibility in prior specification, removes\ntiming dependence on the maximum indegree and yields provably good posterior\napproximations; in addition, we show that it achieves superior accuracy,\nscalability, and sampler mixing on several datasets. \n\n"}
{"id": "1803.06010", "contents": "Title: Ridge Regression and Provable Deterministic Ridge Leverage Score\n  Sampling Abstract: Ridge leverage scores provide a balance between low-rank approximation and\nregularization, and are ubiquitous in randomized linear algebra and machine\nlearning. Deterministic algorithms are also of interest in the moderately big\ndata regime, because deterministic algorithms provide interpretability to the\npractitioner by having no failure probability and always returning the same\nresults.\n  We provide provable guarantees for deterministic column sampling using ridge\nleverage scores. The matrix sketch returned by our algorithm is a column subset\nof the original matrix, yielding additional interpretability. Like the\nrandomized counterparts, the deterministic algorithm provides (1 + {\\epsilon})\nerror column subset selection, (1 + {\\epsilon}) error projection-cost\npreservation, and an additive-multiplicative spectral bound. We also show that\nunder the assumption of power-law decay of ridge leverage scores, this\ndeterministic algorithm is provably as accurate as randomized algorithms.\n  Lastly, ridge regression is frequently used to regularize ill-posed linear\nleast-squares problems. While ridge regression provides shrinkage for the\nregression coefficients, many of the coefficients remain small but non-zero.\nPerforming ridge regression with the matrix sketch returned by our algorithm\nand a particular regularization parameter forces coefficients to zero and has a\nprovable (1 + {\\epsilon}) bound on the statistical risk. As such, it is an\ninteresting alternative to elastic net regularization. \n\n"}
{"id": "1803.06277", "contents": "Title: Extreme Events: Mechanisms and Prediction Abstract: Extreme events, such as rogue waves, earthquakes and stock market crashes,\noccur spontaneously in many dynamical systems. Because of their usually adverse\nconsequences, quantification, prediction and mitigation of extreme events are\nhighly desirable. Here, we review several aspects of extreme events in\nphenomena described by high-dimensional, chaotic dynamical systems. We\nspecially focus on two pressing aspects of the problem: (i) Mechanisms\nunderlying the formation of extreme events and (ii) Real-time prediction of\nextreme events. For each aspect, we explore methods relying on models, data or\nboth. We discuss the strengths and limitations of each approach as well as\npossible future research directions. \n\n"}
{"id": "1803.06328", "contents": "Title: Nesting Probabilistic Programs Abstract: We formalize the notion of nesting probabilistic programming queries and\ninvestigate the resulting statistical implications. We demonstrate that while\nquery nesting allows the definition of models which could not otherwise be\nexpressed, such as those involving agents reasoning about other agents,\nexisting systems take approaches which lead to inconsistent estimates. We show\nhow to correct this by delineating possible ways one might want to nest queries\nand asserting the respective conditions required for convergence. We further\nintroduce a new online nested Monte Carlo estimator that makes it substantially\neasier to ensure these conditions are met, thereby providing a simple framework\nfor designing statistically correct inference engines. We prove the correctness\nof this online estimator and show that, when using the recommended setup, its\nasymptotic variance is always better than that of the equivalent fixed\nestimator, while its bias is always within a factor of two. \n\n"}
{"id": "1803.07418", "contents": "Title: Large-Scale Model Selection with Misspecification Abstract: Model selection is crucial to high-dimensional learning and inference for\ncontemporary big data applications in pinpointing the best set of covariates\namong a sequence of candidate interpretable models. Most existing work assumes\nimplicitly that the models are correctly specified or have fixed\ndimensionality. Yet both features of model misspecification and high\ndimensionality are prevalent in practice. In this paper, we exploit the\nframework of model selection principles in misspecified models originated in Lv\nand Liu (2014) and investigate the asymptotic expansion of Bayesian principle\nof model selection in the setting of high-dimensional misspecified models. With\na natural choice of prior probabilities that encourages interpretability and\nincorporates Kullback-Leibler divergence, we suggest the high-dimensional\ngeneralized Bayesian information criterion with prior probability (HGBIC_p) for\nlarge-scale model selection with misspecification. Our new information\ncriterion characterizes the impacts of both model misspecification and high\ndimensionality on model selection. We further establish the consistency of\ncovariance contrast matrix estimation and the model selection consistency of\nHGBIC_p in ultra-high dimensions under some mild regularity conditions. The\nadvantages of our new method are supported by numerical studies. \n\n"}
{"id": "1803.07859", "contents": "Title: Efficient Sampling and Structure Learning of Bayesian Networks Abstract: Bayesian networks are probabilistic graphical models widely employed to\nunderstand dependencies in high dimensional data, and even to facilitate causal\ndiscovery. Learning the underlying network structure, which is encoded as a\ndirected acyclic graph (DAG) is highly challenging mainly due to the vast\nnumber of possible networks in combination with the acyclicity constraint.\nEfforts have focussed on two fronts: constraint-based methods that perform\nconditional independence tests to exclude edges and score and search approaches\nwhich explore the DAG space with greedy or MCMC schemes. Here we synthesise\nthese two fields in a novel hybrid method which reduces the complexity of MCMC\napproaches to that of a constraint-based method. Individual steps in the MCMC\nscheme only require simple table lookups so that very long chains can be\nefficiently obtained. Furthermore, the scheme includes an iterative procedure\nto correct for errors from the conditional independence tests. The algorithm\noffers markedly superior performance to alternatives, particularly because DAGs\ncan also be sampled from the posterior distribution, enabling full Bayesian\nmodel averaging for much larger Bayesian networks. \n\n"}
{"id": "1803.09365", "contents": "Title: Finite Sample Complexity of Sequential Monte Carlo Estimators Abstract: We present bounds for the finite sample error of sequential Monte Carlo\nsamplers on static spaces. Our approach explicitly relates the performance of\nthe algorithm to properties of the chosen sequence of distributions and mixing\nproperties of the associated Markov kernels. This allows us to give the first\nfinite sample comparison to other Monte Carlo schemes. We obtain bounds for the\ncomplexity of sequential Monte Carlo approximations for a variety of target\ndistributions including finite spaces, product measures, and log-concave\ndistributions including Bayesian logistic regression. The bounds obtained are\nwithin a logarithmic factor of similar bounds obtainable for Markov chain Monte\nCarlo. \n\n"}
{"id": "1803.09606", "contents": "Title: Can we use linear response theory to assess geoengineering strategies? Abstract: Geoengineering can control only some climatic variables but not others,\nresulting in side-effects. We investigate in an intermediate-complexity climate\nmodel the applicability of linear response theory (LRT) to the assessment of a\ngeoengineering method. This application of LRT is twofold. First, our objective\n(O1) is to assess only the best possible geoengineering scenario by looking for\na suitable modulation of solar forcing that can cancel out or otherwise\nmodulate a climate change signal resulting from a rise in CO2 alone. Here we\nconsider only the cancellation of the expected global mean surface air\ntemperature. It is a straightforward inverse problem for this solar forcing,\nand, considering an infinite time period, we use LRT to provide the solution in\nthe frequency domain in closed form. We provide procedures suitable for\nnumerical implementation that apply to finite time periods too. Second, to be\nable to use LRT to quantify side-effects, the response with respect to\nuncontrolled observables, such as regional must be approximately linear. Our\nobjective (O2) here is to assess the linearity of the response. We find that\nunder geoengineering in the sense of (O1) the asymptotic response of the\nglobally averaged temperature is actually not zero. This is due to an\ninaccurate determination of the linear susceptibilities. The error is due to a\nsignificant quadratic nonlinearity of the response. This nonlinear contribution\ncan be easily removed, which results in much better estimates of the linear\nsusceptibility, and, in turn, in a fivefold reduction in the global average\nsurface temperature under geoengineering. This correction dramatically improves\nalso the agreement of the spatial patterns of the predicted and of the true\nresponse. However, such an agreement is not perfect and is worse in the case of\nthe precipitation patterns, as a result of greater degree of nonlinearity. \n\n"}
{"id": "1803.10817", "contents": "Title: Wave kinetic equation in a nonstationary and inhomogeneous medium with a\n  weak quadratic nonlinearity Abstract: We present a systematic derivation of the wave kinetic equation describing\nthe dynamics of a statistically inhomogeneous incoherent wave field in a medium\nwith a weak quadratic nonlinearity. The medium can be nonstationary and\ninhomogeneous. Primarily based on the Weyl phase-space representation, our\nderivation assumes the standard geometrical-optics ordering and the quasinormal\napproximation for the statistical closure. The resulting wave kinetic equation\nsimultaneously captures the effects of the medium inhomogeneity (both in time\nand space) and of the nonlinear wave scattering. This general formalism can\nserve as a stepping stone for future studies of weak wave turbulence\ninteracting with mean fields in nonstationary and inhomogeneous media. \n\n"}
{"id": "1804.00735", "contents": "Title: A Fast Divide-and-Conquer Sparse Cox Regression Abstract: We propose a computationally and statistically efficient divide-and-conquer\n(DAC) algorithm to fit sparse Cox regression to massive datasets where the\nsample size $n_0$ is exceedingly large and the covariate dimension $p$ is not\nsmall but $n_0\\gg p$. The proposed algorithm achieves computational efficiency\nthrough a one-step linear approximation followed by a least square\napproximation to the partial likelihood (PL). These sequences of linearization\nenable us to maximize the PL with only a small subset and perform penalized\nestimation via a fast approximation to the PL. The algorithm is applicable for\nthe analysis of both time-independent and time-dependent survival data.\nSimulations suggest that the proposed DAC algorithm substantially outperforms\nthe full sample-based estimators and the existing DAC algorithm with respect to\nthe computational speed, while it achieves similar statistical efficiency as\nthe full sample-based estimators. The proposed algorithm was applied to an\nextraordinarily large time-independent survival dataset and an extraordinarily\nlarge time-dependent survival dataset for the prediction of heart\nfailure-specific readmission within 30 days among Medicare heart failure\npatients. \n\n"}
{"id": "1804.01431", "contents": "Title: Posterior Inference for Sparse Hierarchical Non-stationary Models Abstract: Gaussian processes are valuable tools for non-parametric modelling, where\ntypically an assumption of stationarity is employed. While removing this\nassumption can improve prediction, fitting such models is challenging. In this\nwork, hierarchical models are constructed based on Gaussian Markov random\nfields with stochastic spatially varying parameters. Importantly, this allows\nfor non-stationarity while also addressing the computational burden through a\nsparse banded representation of the precision matrix. In this setting,\nefficient Markov chain Monte Carlo (MCMC) sampling is challenging due to the\nstrong coupling a posteriori of the parameters and hyperparameters. We develop\nand compare three adaptive MCMC schemes and make use of banded matrix\noperations for faster inference. Furthermore, a novel extension to\nmulti-dimensional settings is proposed through an additive structure that\nretains the flexibility and scalability of the model, while also inheriting\ninterpretability from the additive approach. A thorough assessment of the\nefficiency and accuracy of the methods in nonstationary settings is presented\nfor both simulated experiments and a computer emulation problem. \n\n"}
{"id": "1804.01811", "contents": "Title: Asymptotic genealogies of interacting particle systems with an\n  application to sequential Monte Carlo Abstract: We study weighted particle systems in which new generations are resampled\nfrom current particles with probabilities proportional to their weights. This\ncovers a broad class of sequential Monte Carlo (SMC) methods, widely-used in\napplied statistics and cognate disciplines. We consider the genealogical tree\nembedded into such particle systems, and identify conditions, as well as an\nappropriate time-scaling, under which they converge to the Kingman n-coalescent\nin the infinite system size limit in the sense of finite-dimensional\ndistributions. Thus, the tractable n-coalescent can be used to predict the\nshape and size of SMC genealogies, as we illustrate by characterising the\nlimiting mean and variance of the tree height. SMC genealogies are known to be\nconnected to algorithm performance, so that our results are likely to have\napplications in the design of new methods as well. Our conditions for\nconvergence are strong, but we show by simulation that they do not appear to be\nnecessary. \n\n"}
{"id": "1804.02274", "contents": "Title: Computationally efficient inference for latent position network models Abstract: Latent position models are widely used for the analysis of networks in a\nvariety of research fields. In fact, these models possess a number of desirable\ntheoretical properties, and are particularly easy to interpret. However,\nstatistical methodologies to fit these models generally incur a computational\ncost which grows with the square of the number of nodes in the graph. This\nmakes the analysis of large social networks impractical. In this paper, we\npropose a new method characterised by a much reduced computational complexity,\nwhich can be used to fit latent position models on networks of several tens of\nthousands nodes. Our approach relies on an approximation of the likelihood\nfunction, where the amount of noise introduced by the approximation can be\narbitrarily reduced at the expense of computational efficiency. We establish\nseveral theoretical results that show how the likelihood error propagates to\nthe invariant distribution of the Markov chain Monte Carlo sampler. In\nparticular, we demonstrate that one can achieve a substantial reduction in\ncomputing time and still obtain a good estimate of the latent structure.\nFinally, we propose applications of our method to simulated networks and to a\nlarge coauthorships network, highlighting the usefulness of our approach. \n\n"}
{"id": "1804.02502", "contents": "Title: Principal Component Analysis: A Natural Approach to Data Exploration Abstract: Principal component analysis (PCA) is often used for analyzing data in the\nmost diverse areas. In this work, we report an integrated approach to several\ntheoretical and practical aspects of PCA. We start by providing, in an\nintuitive and accessible manner, the basic principles underlying PCA and its\napplications. Next, we present a systematic, though no exclusive, survey of\nsome representative works illustrating the potential of PCA applications to a\nwide range of areas. An experimental investigation of the ability of PCA for\nvariance explanation and dimensionality reduction is also developed, which\nconfirms the efficacy of PCA and also shows that standardizing or not the\noriginal data can have important effects on the obtained results. Overall, we\nbelieve the several covered issues can assist researchers from the most diverse\nareas in using and interpreting PCA. \n\n"}
{"id": "1804.02655", "contents": "Title: Efficient Computational Algorithm for Optimal Continuous Experimental\n  Designs Abstract: A simple yet efficient computational algorithm for computing the continuous\noptimal experimental design for linear models is proposed. An alternative proof\nthe monotonic convergence for $D$-optimal criterion on continuous design spaces\nare provided. We further show that the proposed algorithm converges to the\n$D$-optimal design. We also provide an algorithm for the $A$-optimality and\nconjecture that the algorithm convergence monotonically on continuous design\nspaces. Different numerical examples are used to demonstrated the usefulness\nand performance of the proposed algorithms. \n\n"}
{"id": "1804.02719", "contents": "Title: Accelerating MCMC Algorithms Abstract: Markov chain Monte Carlo algorithms are used to simulate from complex\nstatistical distributions by way of a local exploration of these distributions.\nThis local feature avoids heavy requests on understanding the nature of the\ntarget, but it also potentially induces a lengthy exploration of this target,\nwith a requirement on the number of simulations that grows with the dimension\nof the problem and with the complexity of the data behind it. Several\ntechniques are available towards accelerating the convergence of these Monte\nCarlo algorithms, either at the exploration level (as in tempering, Hamiltonian\nMonte Carlo and partly deterministic methods) or at the exploitation level\n(with Rao-Blackwellisation and scalable methods). \n\n"}
{"id": "1804.03674", "contents": "Title: Moment Inequalities in the Context of Simulated and Predicted Variables Abstract: This paper explores the effects of simulated moments on the performance of\ninference methods based on moment inequalities. Commonly used confidence sets\nfor parameters are level sets of criterion functions whose boundary points may\ndepend on sample moments in an irregular manner. Due to this feature,\nsimulation errors can affect the performance of inference in non-standard ways.\nIn particular, a (first-order) bias due to the simulation errors may remain in\nthe estimated boundary of the confidence set. We demonstrate, through Monte\nCarlo experiments, that simulation errors can significantly reduce the coverage\nprobabilities of confidence sets in small samples. The size distortion is\nparticularly severe when the number of inequality restrictions is large. These\nresults highlight the danger of ignoring the sampling variations due to the\nsimulation errors in moment inequality models. Similar issues arise when using\npredicted variables in moment inequalities models. We propose a method for\nproperly correcting for these variations based on regularizing the intersection\nof moments in parameter space, and we show that our proposed method performs\nwell theoretically and in practice. \n\n"}
{"id": "1804.04299", "contents": "Title: Model identification for ARMA time series through convolutional neural\n  networks Abstract: In this paper, we use convolutional neural networks to address the problem of\nmodel identification for autoregressive moving average time series models. We\ncompare the performance of several neural network architectures, trained on\nsimulated time series, with likelihood based methods, in particular the Akaike\nand Bayesian information criteria. We find that our neural networks can\nsignificantly outperform these likelihood based methods in terms of accuracy\nand, by orders of magnitude, in terms of speed. \n\n"}
{"id": "1804.04754", "contents": "Title: Simple rules govern the patterns of Arctic sea ice melt ponds Abstract: Climate change, amplified in the far north, has led to rapid sea ice decline\nin recent years. In the summer, melt ponds form on the surface of Arctic sea\nice, significantly lowering the ice reflectivity (albedo) and thereby\naccelerating ice melt. Pond geometry controls the details of this crucial\nfeedback; however, a reliable model of pond geometry does not currently exist.\nHere we show that a simple model of voids surrounding randomly sized and placed\noverlapping circles reproduces the essential features of pond patterns. The\nonly two model parameters, characteristic circle radius and coverage fraction,\nare chosen by comparing, between the model and the aerial photographs of the\nponds, two correlation functions which determine the typical pond size and\ntheir connectedness. Using these parameters, the void model robustly reproduces\nthe ponds' area-perimeter and area-abundance relationships over more than 6\norders of magnitude. By analyzing the correlation functions of ponds on several\ndates, we also find that the pond scale and the connectedness are surprisingly\nconstant across different years and ice types. Moreover, we find that ponds\nresemble percolation clusters near the percolation threshold. These results\ndemonstrate that the geometry and abundance of Arctic melt ponds can be simply\ndescribed, which can be exploited in future models of Arctic melt ponds that\nwould improve predictions of the response of sea ice to Arctic warming. \n\n"}
{"id": "1804.06742", "contents": "Title: An efficient open-source implementation to compute the Jacobian matrix\n  for the Newton-Raphson power flow algorithm Abstract: Power flow calculations for systems with a large number of buses, e.g. grids\nwith multiple voltage levels, or time series based calculations result in a\nhigh computational effort. A common power flow solver for the efficient\nanalysis of power systems is the Newton-Raphson algorithm. The main\ncomputational effort of this method results from the linearization of the\nnonlinear power flow problem and solving the resulting linear equation. This\npaper presents an algorithm for the fast linearization of the power flow\nproblem by creating the Jacobian matrix directly in CRS format. The increase in\nspeed is achieved by reducing the number of iterations over the nonzero\nelements of the sparse Jacobian matrix. This allows to efficiently create the\nJacobian matrix without having to approximate the problem. A comparison of the\ncalculation time of three power grids shows that comparable open-source\nimplementations need 3-14x the time to create the Jacobian matrix. \n\n"}
{"id": "1804.08536", "contents": "Title: Hamiltonian distributed chaos in Arctic and Antarctic Oscillations Abstract: The Arctic and Antarctic Oscillations (AO and AAO indices) are studied using\nthe Hamiltonian distributed chaos approach. Using the daily data (AO since\n1950y and AAO since 1979y) it is shown that the power spectra of the both AO\nand AAO indices exhibit the stretched exponential behaviour $E(f) \\propto\n\\exp-(f/f_0)^{3/4}$ corresponding to the Hamiltonian distributed chaos. The\ncharacteristic time scale for the both indices $T_0=1/f_0\\simeq 41$ day\ncorresponds to the well known from the numerous extratropic observations near\n40 day period. \n\n"}
{"id": "1805.00397", "contents": "Title: Hamiltonian distributed chaos in the Asian-Australian Monsoons and in\n  the ENSO Abstract: Two subsystems of the Asian Monsoon: the Indian Summer Monsoon and the\nWestern North Pacific Monsoon, have been analysed using their daily indices\nISMI and WNPMI. It is shown that on the intraseasonal time scales the ISMI and\nWNPMI are dominated by the Hamiltonian distributed chaos with the stretched\nexponential spectrum: $E(f) \\propto \\exp-(f/f_0)^{\\beta}$ and analytical values\nof the parameter $\\beta =3/4$ and $\\beta =1/2$ correspondingly. The relevant\ndaily indices Ni\\~no 3 and Ni\\~no 4 (with $\\beta =1/2$) of the El\nNi\\~no-Southern Oscillation (ENSO) and the Australian Monsoon (AUSM index with\n$\\beta = 1/2$) have been also discussed in this context. \n\n"}
{"id": "1805.00834", "contents": "Title: Reply to the Comments of Olivier Pauluis to the paper \"A Third-Law\n  Isentropic Analysis of a Simulated Hurricane\" Abstract: A careful reading of old articles puts Olivier Pauluis' criticisms concerning\nthe definition of isentropic processes in terms of a potential temperature\nclosely associated with the entropy of moist air, together with the third\nprinciple of thermodynamics, into perspective. \n\n"}
{"id": "1805.01596", "contents": "Title: Reduced-order modeling of fully turbulent buoyancy-driven flows using\n  the Green's function method Abstract: A One-Dimensional (1D) Reduced-Order Model (ROM) has been developed for a 3D\nRayleigh-B\\'enard convection system in the turbulent regime with Rayleigh\nnumber $\\mathrm{Ra}=10^6$. The state vector of the 1D ROM is horizontally\naveraged temperature. Using the Green's Function (GRF) method, which involves\napplying many localized, weak forcings to the system one at a time and\ncalculating the responses using long-time averaged Direct Numerical Simulations\n(DNS), the system's Linear Response Function (LRF) has been computed. Another\nmatrix, called the Eddy Flux Matrix (EFM), that relates changes in the\ndivergence of vertical eddy heat fluxes to changes in the state vector, has\nalso been calculated. Using various tests, it is shown that the LRF and EFM can\naccurately predict the time-mean responses of temperature and eddy heat flux to\nexternal forcings, and that the LRF can well predict the forcing needed to\nchange the mean flow in a specified way (inverse problem). The non-normality of\nthe LRF is discussed and its eigen/singular vectors are compared with the\nleading Proper Orthogonal Decomposition (POD) modes of the DNS data.\nFurthermore, it is shown that if the LRF and EFM are simply scaled by the\nsquare-root of Rayleigh number, they perform equally well for flows at other\n$\\mathrm{Ra}$, at least in the investigated range of $ 5 \\times 10^5 \\le\n\\mathrm{Ra} \\le 1.25 \\times 10^6$. The GRF method can be applied to develop 1D\nor 3D ROMs for any turbulent flow, and the calculated LRF and EFM can help with\nbetter analyzing and controlling the nonlinear system. \n\n"}
{"id": "1805.01852", "contents": "Title: Inference for $L_2$-Boosting Abstract: We propose a statistical inference framework for the component-wise\nfunctional gradient descent algorithm (CFGD) under normality assumption for\nmodel errors, also known as $L_2$-Boosting. The CFGD is one of the most\nversatile tools to analyze data, because it scales well to high-dimensional\ndata sets, allows for a very flexible definition of additive regression models\nand incorporates inbuilt variable selection. Due to the variable selection, we\nbuild on recent proposals for post-selection inference. However, the iterative\nnature of component-wise boosting, which can repeatedly select the same\ncomponent to update, necessitates adaptations and extensions to existing\napproaches. We propose tests and confidence intervals for linear, grouped and\npenalized additive model components selected by $L_2$-Boosting. Our concepts\nalso transfer to slow-learning algorithms more generally, and to other\nselection techniques which restrict the response space to more complex sets\nthan polyhedra. We apply our framework to an additive model for sales prices of\nresidential apartments and investigate the properties of our concepts in\nsimulation studies. \n\n"}
{"id": "1805.01870", "contents": "Title: Hedging parameter selection for basis pursuit Abstract: In Compressed Sensing and high dimensional estimation, signal recovery often\nrelies on sparsity assumptions and estimation is performed via\n$\\ell_1$-penalized least-squares optimization, a.k.a. LASSO. The $\\ell_1$\npenalisation is usually controlled by a weight, also called \"relaxation\nparameter\", denoted by $\\lambda$. It is commonly thought that the practical\nefficiency of the LASSO for prediction crucially relies on accurate selection\nof $\\lambda$. In this short note, we propose to consider the hyper-parameter\nselection problem from a new perspective which combines the Hedge online\nlearning method by Freund and Shapire, with the stochastic Frank-Wolfe method\nfor the LASSO. Using the Hedge algorithm, we show that a our simple selection\nrule can achieve prediction results comparable to Cross Validation at a\npotentially much lower computational cost. \n\n"}
{"id": "1805.02990", "contents": "Title: Almost sure error bounds for data assimilation in dissipative systems\n  with unbounded observation noise Abstract: Data assimilation is uniquely challenging in weather forecasting due to the\nhigh dimensionality of the employed models and the nonlinearity of the\ngoverning equations. Although current operational schemes are used\nsuccessfully, our understanding of their long-term error behaviour is still\nincomplete. In this work, we study the error of some simple data assimilation\nschemes in the presence of unbounded (e.g. Gaussian) noise on a wide class of\ndissipative dynamical systems with certain properties, including the Lorenz\nmodels and the 2D incompressible Navier-Stokes equations. We exploit the\nproperties of the dynamics to derive analytic bounds on the long-term error for\nindividual realisations of the noise in time. These bounds are proportional to\nthe amplitude of the noise. Furthermore, we find that the error exhibits a form\nof stationary behaviour, and in particular an accumulation of error does not\noccur. This improves on previous results in which either the noise was bounded\nor the error was considered in expectation only. \n\n"}
{"id": "1805.03288", "contents": "Title: Fused Density Estimation: Theory and Methods Abstract: In this paper we introduce a method for nonparametric density estimation on\ngeometric networks. We define fused density estimators as solutions to a total\nvariation regularized maximum-likelihood density estimation problem. We provide\ntheoretical support for fused density estimation by proving that the squared\nHellinger rate of convergence for the estimator achieves the minimax bound over\nunivariate densities of log-bounded variation. We reduce the original\nvariational formulation in order to transform it into a tractable,\nfinite-dimensional quadratic program. Because random variables on geometric\nnetworks are simple generalizations of the univariate case, this method also\nprovides a useful tool for univariate density estimation. Lastly, we apply this\nmethod and assess its performance on examples in the univariate and geometric\nnetwork setting. We compare the performance of different optimization\ntechniques to solve the problem, and use these results to inform\nrecommendations for the computation of fused density estimators. \n\n"}
{"id": "1805.03317", "contents": "Title: Subsampling Sequential Monte Carlo for Static Bayesian Models Abstract: We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference\nin large data problems by data subsampling. SMC sequentially updates a cloud of\nparticles through a sequence of distributions, beginning with a distribution\nthat is easy to sample from such as the prior and ending with the posterior\ndistribution. Each update of the particle cloud consists of three steps:\nreweighting, resampling, and moving. In the move step, each particle is moved\nusing a Markov kernel; this is typically the most computationally expensive\npart, particularly when the dataset is large. It is crucial to have an\nefficient move step to ensure particle diversity. Our article makes two\nimportant contributions. First, in order to speed up the SMC computation, we\nuse an approximately unbiased and efficient annealed likelihood estimator based\non data subsampling. The subsampling approach is more memory efficient than the\ncorresponding full data SMC, which is an advantage for parallel computation.\nSecond, we use a Metropolis within Gibbs kernel with two conditional updates. A\nHamiltonian Monte Carlo update makes distant moves for the model parameters,\nand a block pseudo-marginal proposal is used for the particles corresponding to\nthe auxiliary variables for the data subsampling. We demonstrate both the\nusefulness and limitations of the methodology for estimating four generalized\nlinear models and a generalized additive model with large datasets. \n\n"}
{"id": "1805.04907", "contents": "Title: A Computational Framework for Modelling and Analyzing Ice Storms Abstract: Ice storms are extreme weather events that can have devastating implications\nfor the sustainability of natural ecosystems as well as man made\ninfrastructure. Ice storms are caused by a complex mix of atmospheric\nconditions and are among the least understood of severe weather events. Our\nability to model ice storms and characterize storm features will go a long way\ntowards both enabling support systems that offset storm impacts and increasing\nour understanding of ice storms. In this paper, we present a holistic\ncomputational framework to answer key questions of interest about ice storms.\nWe model ice storms as a function of relevant surface and atmospheric\nvariables. We learn these models by adapting and applying supervised and\nunsupervised machine learning algorithms on data with missing or incorrect\nlabels. We also include a knowledge representation module that reasons with\ndomain knowledge to revise the output of the learned models. Our models are\ntrained using reanalysis data and historical records of storm events. We\nevaluate these models on reanalyis data as well as Global Climate Model (GCM)\ndata for historical and future climate change scenarios. Furthermore, we\ndiscuss the use of appropriate bias correction approaches to run such modeling\nframeworks with GCM data. \n\n"}
{"id": "1805.06328", "contents": "Title: The degree profile and Gini index of random caterpillar trees Abstract: In this paper, we investigate the degree profile and Gini index of random\ncaterpillar trees (RCTs). We consider RCTs which evolve in two different\nmanners: uniform and nonuniform. The degrees of the vertices on the central\npath (i.e., the degree profile) of a uniform RCT follow a multinomial\ndistribution. For nonuniform RCTs, we focus on those growing in the fashion of\npreferential attachment. We develop methods based on stochastic recurrences to\ncompute the exact expectations and the dispersion matrix of the degree\nvariables. A generalized P\\'{o}lya urn model is exploited to determine the\nexact joint distribution of these degree variables. We apply the methods from\ncombinatorics to prove that the asymptotic distribution is Dirichlet. In\naddition, we propose a new type of Gini index to quantitatively distinguish the\nevolutionary characteristics of the two classes of RCTs. We present the results\nvia several numerical experiments. \n\n"}
{"id": "1805.07011", "contents": "Title: A shadowing-based inflation scheme for ensemble data assimilation Abstract: Artificial ensemble inflation is a common technique in ensemble data\nassimilation, whereby the ensemble covariance is periodically increased in\norder to prevent deviation of the ensemble from the observations and possible\nensemble collapse. This manuscript introduces a new form of covariance\ninflation for ensemble data assimilation based upon shadowing ideas from\ndynamical systems theory. We present results from a low order nonlinear chaotic\nsystem that supports using shadowing inflation, demonstrating that shadowing\ninflation is more robust to parameter tuning than standard multiplicative\ncovariance inflation, outperforming in observation-sparse scenarios and often\nleading to longer forecast shadowing times. \n\n"}
{"id": "1805.07415", "contents": "Title: Effects of dissociation/recombination on the day-night temperature\n  contrasts of ultra-hot Jupiters Abstract: Secondary eclipse observations of ultra-hot Jupiters have found evidence that\nhydrogen is dissociated on their daysides. Additionally, full-phase light curve\nobservations of ultra-hot Jupiters show a smaller day-night emitted flux\ncontrast than that expected from previous theory. Recently, it was proposed by\nBell & Cowan (2018) that the heat intake to dissociate hydrogen and heat\nrelease due to recombination of dissociated hydrogen can affect the atmospheric\ncirculation of ultra-hot Jupiters. In this work, we add cooling/heating due to\ndissociation/recombination into the analytic theory of Komacek & Showman (2016)\nand Zhang & Showman (2017) for the dayside-nightside temperature contrasts of\nhot Jupiters. We find that at high values of incident stellar flux, the\nday-night temperature contrast of ultra-hot Jupiters may decrease with\nincreasing incident stellar flux due to dissociation/recombination, the\nopposite of that expected without including the effects of\ndissociation/recombination. We propose that a combination of a greater number\nof full-phase light curve observations of ultra-hot Jupiters and future General\nCirculation Models that include the effects of dissociation/recombination could\ndetermine in detail how the atmospheric circulation of ultra-hot Jupiters\ndiffers from that of cooler planets. \n\n"}
{"id": "1805.09108", "contents": "Title: Deep Learning Estimation of Absorbed Dose for Nuclear Medicine\n  Diagnostics Abstract: The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated\nby convolving an image of a time-integrated activity distribution with a dose\nvoxel kernel (DVK) consisting of different types of tissues. This fast and\ninacurate approximation is inappropriate for personalized dosimetry as it\nneglects tissue heterogenity. The latter can be calculated using different\nimaging techniques such as CT and SPECT combined with a time consuming\nmonte-carlo simulation. The aim of this study is, for the first time, an\nestimation of DVKs from CT-derived density kernels (DK) via deep learning in\nconvolutional neural networks (CNNs). The proposed CNN achieved, on the test\nset, a mean intersection over union (IOU) of $= 0.86$ after $308$ epochs and a\ncorresponding mean squared error (MSE) $= 1.24 \\cdot 10^{-4}$. This\ngeneralization ability shows that the trained CNN can indeed learn the\ndifficult transfer function from DK to DVK. Future work will evaluate DVKs\nestimated by CNNs with full monte-carlo simulations of a whole body CT to\npredict patient specific voxel dose maps. \n\n"}
{"id": "1805.10038", "contents": "Title: A Multi-Scan Labeled Random Finite Set Model for Multi-object State\n  Estimation Abstract: State space models in which the system state is a finite set--called the\nmulti-object state--have generated considerable interest in recent years.\nSmoothing for state space models provides better estimation performance than\nfiltering by using the full posterior rather than the filtering density. In\nmulti-object state estimation, the Bayes multi-object filtering recursion\nadmits an analytic solution known as the Generalized Labeled Multi-Bernoulli\n(GLMB) filter. In this work, we extend the analytic GLMB recursion to propagate\nthe multi-object posterior. We also propose an implementation of this so-called\nmulti-scan GLMB posterior recursion using a similar approach to the GLMB filter\nimplementation. \n\n"}
{"id": "1805.10127", "contents": "Title: Collapse of generalized Euler and surface quasi-geostrophic\n  point-vortices Abstract: Point vortex models are presented for the generalized Euler equations, which\nare characterized by a fractional Laplacian relation between the active scalar\nand the streamfunction. Special focus is given to the case of the surface\nquasi-geostrophic (SQG) equations, for which the existence of finite-time\nsingularities is still a matter of debate. Point vortex trajectories are\nexpressed using Nambu dynamics. The formulation is based on a noncanonical\nbracket and allows for a geometrical interpretation of trajectories as\nintersections of level sets of the Hamiltonian and Casimir. Within this\nsetting, we focus on the collapse of solutions for the three point vortex\nmodel. In particular, we show that for SQG the collapse can be either\nself-similar or non-self-similar. Self-similarity occurs only when the\nHamiltonian is zero, while non-self-similarity appears for non-zero values of\nthe same. For both cases, collapse is allowed for any choice of circulations\nwithin a permitted interval. These results differ strikingly from the classical\npoint vortex model, where collapse is self-similar for any value of the\nHamiltonian, but the vortex circulations must satisfy a strict relationship.\nResults may also shed a light on the formation of singularities in the SQG\npartial differential equations, where the singularity is thought to be reached\nonly in a self-similar way. \n\n"}
{"id": "1806.00225", "contents": "Title: Model-based clustering for populations of networks Abstract: Until recently obtaining data on populations of networks was typically rare.\nHowever, with the advancement of automatic monitoring devices and the growing\nsocial and scientific interest in networks, such data has become more widely\navailable. From sociological experiments involving cognitive social structures\nto fMRI scans revealing large-scale brain networks of groups of patients, there\nis a growing awareness that we urgently need tools to analyse populations of\nnetworks and particularly to model the variation between networks due to\ncovariates. We propose a model-based clustering method based on mixtures of\ngeneralized linear (mixed) models that can be employed to describe the joint\ndistribution of a populations of networks in a parsimonious manner and to\nidentify subpopulations of networks that share certain topological properties\nof interest (degree distribution, community structure, effect of covariates on\nthe presence of an edge, etc.). Maximum likelihood estimation for the proposed\nmodel can be efficiently carried out with an implementation of the EM\nalgorithm. We assess the performance of this method on simulated data and\nconclude with an example application on advice networks in a small business. \n\n"}
{"id": "1806.00237", "contents": "Title: Accounting for model errors in iterative ensemble smoothers Abstract: In the strong-constraint formulation of the history-matching problem, we\nassume that all the model errors relate to a selection of uncertain model input\nparameters. One does not account for additional model errors that could result\nfrom, e.g., excluded uncertain parameters, neglected physics in the model\nformulation, the use of an approximate model forcing, or discretization errors\nresulting from numerical approximations. If parameters with significant\nuncertainties are unaccounted for, there is a risk for an unphysical update, of\nsome uncertain parameters, that compensates for errors in the omitted\nparameters. This paper gives the theoretical foundation for introducing model\nerrors in ensemble methods for history matching. In particular, we explain\nprocedures for practically including model errors in iterative ensemble\nsmoothers like ESMDA and IES. Also, we demonstrate the impact of adding (or\nneglecting) model errors in the parameter-estimation problem. \n\n"}
{"id": "1806.00989", "contents": "Title: Asymptotic optimality of adaptive importance sampling Abstract: Adaptive importance sampling (AIS) uses past samples to update the\n\\textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with\ntwo steps : (i) to explore the space with $n_t$ points according to $q_t$ and\n(ii) to exploit the current amount of information to update the sampling\npolicy. The very fundamental question raised in this paper concerns the\nbehavior of empirical sums based on AIS. Without making any assumption on the\nallocation policy $n_t$, the theory developed involves no restriction on the\nsplit of computational resources between the explore (i) and the exploit (ii)\nstep. It is shown that AIS is asymptotically optimal : the asymptotic behavior\nof AIS is the same as some \"oracle\" strategy that knows the targeted sampling\npolicy from the beginning. From a practical perspective, weighted AIS is\nintroduced, a new method that allows to forget poor samples from early stages. \n\n"}
{"id": "1806.01750", "contents": "Title: Chaotic Hamiltonian dynamics of surface air temperature on daily to\n  intraseasonal time scales Abstract: The surface air temperature daily records at the land-based locations with\ndifferent climate conditions (from Arctic to Patagonia) have been studied on\nthe daily to intraseasonal time scales (low frequency annual and seasonal\nvariations have been removed by subtracting a wavelet regression from the daily\nrecords). It is shown that the power spectra of the daily time series exhibit a\nuniversal behaviour corresponding to the Hamiltonian distributed chaos. Global\naverage temperature fluctuations (land-based data) and the tropical Pacific sea\nsurface temperature fluctuations (El Ni\\~no/La Ni\\~na phenomenon) have been\nalso considered in this context. It is shown that the practical smooth\npredictability for the surface air temperature dynamics is possible at least up\nto the fundamental (pumping) period of the distributed chaos. \n\n"}
{"id": "1806.02068", "contents": "Title: Dynamically rescaled Hamiltonian Monte Carlo for Bayesian Hierarchical\n  Models Abstract: Dynamically rescaled Hamiltonian Monte Carlo (DRHMC) is introduced as a\ncomputationally fast and easily implemented method for performing full Bayesian\nanalysis in hierarchical statistical models. The method relies on introducing a\nmodified parameterisation so that the re-parameterised target distribution has\nclose to constant scaling properties, and thus is easily sampled using standard\n(Euclidian metric) Hamiltonian Monte Carlo. Provided that the parameterisations\nof the conditional distributions specifying the hierarchical model are\n\"constant information parameterisations\" (CIP), the relation between the\nmodified- and original parameterisation is bijective, explicitly computed and\nadmit exploitation of sparsity in the numerical linear algebra involved. CIPs\nfor a large catalogue of statistical models are presented, and from the\ncatalogue, it is clear that many CIPs are currently routinely used in\nstatistical computing. A relation between the proposed methodology and a class\nof explicitly integrated Riemann manifold Hamiltonian Monte Carlo methods is\ndiscussed. The methodology is illustrated on several example models, including\na model for inflation rates with multiple levels of non-linearly dependent\nlatent variables. \n\n"}
{"id": "1806.02670", "contents": "Title: Scalable Bayesian Nonparametric Clustering and Classification Abstract: We develop a scalable multi-step Monte Carlo algorithm for inference under a\nlarge class of nonparametric Bayesian models for clustering and classification.\nEach step is \"embarrassingly parallel\" and can be implemented using the same\nMarkov chain Monte Carlo sampler. The simplicity and generality of our approach\nmakes inference for a wide range of Bayesian nonparametric mixture models\napplicable to large datasets. Specifically, we apply the approach to inference\nunder a product partition model with regression on covariates. We show results\nfor inference with two motivating data sets: a large set of electronic health\nrecords (EHR) and a bank telemarketing dataset. We find interesting clusters\nand favorable classification performance relative to other widely used\ncompeting classifiers. \n\n"}
{"id": "1806.03791", "contents": "Title: The Effect of Network Width on the Performance of Large-batch Training Abstract: Distributed implementations of mini-batch stochastic gradient descent (SGD)\nsuffer from communication overheads, attributed to the high frequency of\ngradient updates inherent in small-batch training. Training with large batches\ncan reduce these overheads; however, large batches can affect the convergence\nproperties and generalization performance of SGD. In this work, we take a first\nstep towards analyzing how the structure (width and depth) of a neural network\naffects the performance of large-batch training. We present new theoretical\nresults which suggest that--for a fixed number of parameters--wider networks\nare more amenable to fast large-batch training compared to deeper ones. We\nprovide extensive experiments on residual and fully-connected neural networks\nwhich suggest that wider networks can be trained using larger batches without\nincurring a convergence slow-down, unlike their deeper variants. \n\n"}
{"id": "1806.03948", "contents": "Title: Hadamard Matrices, Quaternions, and the Pearson Chi-square Statistic Abstract: We present a symbolic decomposition of the Pearson chi-square statistic with\nunequal cell probabilities, by presenting Hadamard-type matrices whose columns\nare eigenvectors of the variance-covariance matrix of the cell counts. All of\nthe eigenvectors have non-zero values so each component test uses all cell\nprobabilities in a way that makes it intuitively interpretable. When all cell\nprobabilities are distinct and unrelated we establish that such decomposition\nis only possible when the number of multinomial cells is a small power of 2.\nFor higher powers of 2, we show, using the theory of orthogonal designs, that\nthe targeted decomposition is possible when appropriate relations are imposed\non the cell probabilities, the simplest of which is when the probabilities are\nequal and the decomposition is reduced to the one obtained by Hadamard\nmatrices. Simulations are given to illustrate the sensitivity of various\ncomponents to changes in location, scale skewness and tail probability, as well\nas to illustrate the potential improvement in power when the cell probabilities\nare changed. \n\n"}
{"id": "1806.04505", "contents": "Title: Possible Implications of Self-Similarity for Tornadogenesis and\n  Maintenance Abstract: Self-similarity in tornadic and some non-tornadic supercell flows is studied\nand power laws relating various quantities in such flows are demonstrated.\nMagnitudes of the exponents in these power laws are related to the intensity of\nthe corresponding flow and thus the severity of the supercell storm. The\nfeatures studied in this paper include the vertical vorticity and\npseudovorticity, both obtained from radar observations and from numerical\nsimulations, the tangential velocity, and the energy spectrum as a function of\nthe wave number. Connections to fractals are highlighted and discussed. \n\n"}
{"id": "1806.04731", "contents": "Title: Deep learning to represent sub-grid processes in climate models Abstract: The representation of nonlinear sub-grid processes, especially clouds, has\nbeen a major source of uncertainty in climate models for decades.\nCloud-resolving models better represent many of these processes and can now be\nrun globally but only for short-term simulations of at most a few years because\nof computational limitations. Here we demonstrate that deep learning can be\nused to capture many advantages of cloud-resolving modeling at a fraction of\nthe computational cost. We train a deep neural network to represent all\natmospheric sub-grid processes in a climate model by learning from a\nmulti-scale model in which convection is treated explicitly. The trained neural\nnetwork then replaces the traditional sub-grid parameterizations in a global\ngeneral circulation model in which it freely interacts with the resolved\ndynamics and the surface-flux scheme. The prognostic multi-year simulations are\nstable and closely reproduce not only the mean climate of the cloud-resolving\nsimulation but also key aspects of variability, including precipitation\nextremes and the equatorial wave spectrum. Furthermore, the neural network\napproximately conserves energy despite not being explicitly instructed to.\nFinally, we show that the neural network parameterization generalizes to new\nsurface forcing patterns but struggles to cope with temperatures far outside\nits training manifold. Our results show the feasibility of using deep learning\nfor climate model parameterization. In a broader context, we anticipate that\ndata-driven Earth System Model development could play a key role in reducing\nclimate prediction uncertainty in the coming decade. \n\n"}
{"id": "1806.04854", "contents": "Title: Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam Abstract: Uncertainty computation in deep learning is essential to design robust and\nreliable systems. Variational inference (VI) is a promising approach for such\ncomputation, but requires more effort to implement and execute compared to\nmaximum-likelihood methods. In this paper, we propose new natural-gradient\nalgorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms\ncan be implemented within the Adam optimizer by perturbing the network weights\nduring gradient evaluations, and uncertainty estimates can be cheaply obtained\nby using the vector that adapts the learning rate. This requires lower memory,\ncomputation, and implementation effort than existing VI methods, while\nobtaining uncertainty estimates of comparable quality. Our empirical results\nconfirm this and further suggest that the weight-perturbation in our algorithm\ncould be useful for exploration in reinforcement learning and stochastic\noptimization. \n\n"}
{"id": "1806.05852", "contents": "Title: Coupled conditional backward sampling particle filter Abstract: The conditional particle filter (CPF) is a promising algorithm for general\nhidden Markov model smoothing. Empirical evidence suggests that the variant of\nCPF with backward sampling (CBPF) performs well even with long time series.\nPrevious theoretical results have not been able to demonstrate the improvement\nbrought by backward sampling, whereas we provide rates showing that CBPF can\nremain effective with a fixed number of particles independent of the time\nhorizon. Our result is based on analysis of a new coupling of two CBPFs, the\ncoupled conditional backward sampling particle filter (CCBPF). We show that\nCCBPF has good stability properties in the sense that with fixed number of\nparticles, the coupling time in terms of iterations increases only linearly\nwith respect to the time horizon under a general (strong mixing) condition. The\nCCBPF is useful not only as a theoretical tool, but also as a practical method\nthat allows for unbiased estimation of smoothing expectations, following the\nrecent developments by Jacob et al. (to appear). Unbiased estimation has many\nadvantages, such as enabling the construction of asymptotically exact\nconfidence intervals and straightforward parallelisation. \n\n"}
{"id": "1806.06784", "contents": "Title: Robust inference on the average treatment effect using the outcome\n  highly adaptive lasso Abstract: Many estimators of the average effect of a treatment on an outcome require\nestimation of the propensity score, the outcome regression, or both. It is\noften beneficial to utilize flexible techniques such as semiparametric\nregression or machine learning to estimate these quantities. However, optimal\nestimation of these regressions does not necessarily lead to optimal estimation\nof the average treatment effect, particularly in settings with strong\ninstrumental variables. A recent proposal addressed these issues via the\noutcome-adaptive lasso, a penalized regression technique for estimating the\npropensity score that seeks to minimize the impact of instrumental variables on\ntreatment effect estimators. However, a notable limitation of this approach is\nthat its application is restricted to parametric models. We propose a more\nflexible alternative that we call the outcome highly adaptive lasso. We discuss\nlarge sample theory for this estimator and propose closed form confidence\nintervals based on the proposed estimator. We show via simulation that our\nmethod offers benefits over several popular approaches. \n\n"}
{"id": "1806.06822", "contents": "Title: Exploring exomoon atmospheres with an idealized general circulation\n  model Abstract: Recent studies have shown that large exomoons can form in the accretion disks\naround super-Jovian extrasolar planets. These planets are abundant at about 1\nAU from Sun-like stars, which makes their putative moons interesting for\nstudies of habitability. Technological advances could soon make an exomoon\ndiscovery with Kepler or the upcoming CHEOPS and PLATO space missions possible.\nExomoon climates might be substantially different from exoplanet climates\nbecause the day-night cycles on moons are determined by the moon's synchronous\nrotation with its host planet. Moreover, planetary illumination at the top of\nthe moon's atmosphere and tidal heating at the moon's surface can be\nsubstantial, which can affect the redistribution of energy on exomoons. Using\nan idealized general circulation model with simplified hydrologic, radiative,\nand convective processes, we calculate surface temperature, wind speed, mean\nmeridional circulation, and energy transport on a 2.5 Mars-mass moon orbiting a\n10-Jupiter-mass at 1 AU from a Sun-like star. The strong thermal irradiation\nfrom a young giant planet causes the satellite's polar regions to warm, which\nremains consistent with the dynamically-driven polar amplification seen in\nEarth models that lack ice-albedo feedback. Thermal irradiation from young,\nluminous giant planets onto water-rich exomoons can be strong enough to induce\nwater loss on a planet, which could lead to a runaway greenhouse. Moons that\nare in synchronous rotation with their host planet and do not experience a\nrunaway greenhouse could experience substantial polar melting induced by the\npolar amplification of planetary illumination and geothermal heating from tidal\neffects. \n\n"}
{"id": "1806.07274", "contents": "Title: Efficient data augmentation for multivariate probit models with panel\n  data: An application to general practitioner decision-making about\n  contraceptives Abstract: This article considers the problem of estimating a multivariate probit model\nin a panel data setting with emphasis on sampling a high-dimensional\ncorrelation matrix and improving the overall efficiency of the data\naugmentation approach. We reparameterise the correlation matrix in a principled\nway and then carry out efficient Bayesian inference using Hamiltonian Monte\nCarlo. We also propose a novel antithetic variable method to generate samples\nfrom the posterior distribution of the random effects and regression\ncoefficients, resulting in significant gains in efficiency. We apply the\nmethodology by analysing stated preference data obtained from Australian\ngeneral practitioners evaluating alternative contraceptive products. Our\nanalysis suggests that the joint probability of discussing combinations of\ncontraceptive products with a patient shows medical practice variation among\nthe general practitioners, which indicates some resistance to even discuss\nthese products, let alone recommend them. \n\n"}
{"id": "1806.07533", "contents": "Title: An Asynchronous Distributed Expectation Maximization Algorithm For\n  Massive Data: The DEM Algorithm Abstract: The family of Expectation-Maximization (EM) algorithms provides a general\napproach to fitting flexible models for large and complex data. The expectation\n(E) step of EM-type algorithms is time-consuming in massive data applications\nbecause it requires multiple passes through the full data. We address this\nproblem by proposing an asynchronous and distributed generalization of the EM\ncalled the Distributed EM (DEM). Using DEM, existing EM-type algorithms are\neasily extended to massive data settings by exploiting the divide-and-conquer\ntechnique and widely available computing power, such as grid computing. The DEM\nalgorithm reserves two groups of computing processes called \\emph{workers} and\n\\emph{managers} for performing the E step and the maximization step (M step),\nrespectively. The samples are randomly partitioned into a large number of\ndisjoint subsets and are stored on the worker processes. The E step of DEM\nalgorithm is performed in parallel on all the workers, and every worker\ncommunicates its results to the managers at the end of local E step. The\nmanagers perform the M step after they have received results from a\n$\\gamma$-fraction of the workers, where $\\gamma$ is a fixed constant in $(0,\n1]$. The sequence of parameter estimates generated by the DEM algorithm retains\nthe attractive properties of EM: convergence of the sequence of parameter\nestimates to a local mode and linear global rate of convergence. Across diverse\nsimulations focused on linear mixed-effects models, the DEM algorithm is\nsignificantly faster than competing EM-type algorithms while having a similar\naccuracy. The DEM algorithm maintains its superior empirical performance on a\nmovie ratings database consisting of 10 million ratings. \n\n"}
{"id": "1806.08813", "contents": "Title: An Annealed Sequential Monte Carlo Method for Bayesian Phylogenetics Abstract: We describe an \"embarrassingly parallel\" method for Bayesian phylogenetic\ninference, annealed Sequential Monte Carlo, based on recent advances in the\nSequential Monte Carlo literature such as adaptive determination of annealing\nparameters. The algorithm provides an approximate posterior distribution over\ntrees and evolutionary parameters as well as an unbiased estimator for the\nmarginal likelihood. This unbiasedness property can be used for the purpose of\ntesting the correctness of posterior simulation software. We evaluate the\nperformance of phylogenetic annealed Sequential Monte Carlo by reviewing and\ncomparing with other computational Bayesian phylogenetic methods, in\nparticular, different marginal likelihood estimation methods. Unlike previous\nSequential Monte Carlo methods in phylogenetics, our annealed method can\nutilize standard Markov chain Monte Carlo tree moves and hence benefit from the\nlarge inventory of such moves available in the literature. Consequently, the\nannealed Sequential Monte Carlo method should be relatively easy to incorporate\ninto existing phylogenetic software packages based on Markov chain Monte Carlo\nalgorithms. We illustrate our method using simulation studies and real data\nanalysis. \n\n"}
{"id": "1806.09861", "contents": "Title: The importance of ensemble techniques for operational space weather\n  forecasting Abstract: The space weather community has begun to use frontier methods such as data\nassimilation, machine learning, and ensemble modeling to advance current\noperational forecasting efforts. This was highlighted by a multi-disciplinary\nsession at the 2017 American Geophysical Union Meeting, 'Frontier\nSolar-Terrestrial Science Enabled by the Combination of Data-Driven Techniques\nand Physics-Based Understanding', with considerable discussion surrounding\nensemble techniques. Here ensemble methods are described in detail; using a set\nof predictions to improve on a single-model output, for example taking a simple\naverage of multiple models, or using more complex techniques for data\nassimilation. They have been used extensively in fields such as numerical\nweather prediction and data science, for both improving model accuracy and\nproviding a measure of model uncertainty. Researchers in the space weather\ncommunity have found them to be similarly useful, and some examples of success\nstories are highlighted in this commentary. Future developments are also\nencouraged to transition these basic research efforts to operational\nforecasting as well as providing prediction errors to aid end-user\nunderstanding. \n\n"}
{"id": "1806.10548", "contents": "Title: Hamiltonian distributed chaos in the north-south dipole and quadrupole\n  teleconnections Abstract: It is shown that the north-south teleconnections in the northern hemisphere:\nNorth Atlantic (NAO), East Pacific (EPO), Western Pacific (WPO) dipole\noscillations and Pacific/North American quadrupole pattern (PNA), are dominated\nby the Hamiltonian distributed chaos on the daily to intraseasonal time scales.\nDifferences of the chaotic properties of the dipole and quadrupole oscillations\nas well as their relation to the surface air temperature have been briefly\ndiscussed. A chaotic spectral affinity of the PNA quadrupole pattern to the\nArctic Oscillation and the Greenland blocking phenomenon have been considered\nin this context. \n\n"}
{"id": "1807.00420", "contents": "Title: A Piecewise Deterministic Markov Process via $(r,\\theta)$ swaps in\n  hyperspherical coordinates Abstract: Recently, a class of stochastic processes known as piecewise deterministic\nMarkov processes has been used to define continuous-time Markov chain Monte\nCarlo algorithms with a number of attractive properties, including\ncompatibility with stochastic gradients like those typically found in\noptimization and variational inference, and high efficiency on certain big data\nproblems. Not many processes in this class that are capable of targeting\narbitrary invariant distributions are currently known, and within one subclass\nall previously known processes utilize linear transition functions. In this\nwork, we derive a process whose transition function is nonlinear through\nsolving its Fokker-Planck equation in hyperspherical coordinates. We explore\nits behavior on Gaussian targets, as well as a Bayesian logistic regression\nmodel with synthetic data. We discuss implications to both the theory of\npiecewise deterministic Markov processes, and to Bayesian statisticians as well\nas physicists seeking to use them for simulation-based computation. \n\n"}
{"id": "1807.01006", "contents": "Title: Local-in-time Physical Solutions of the Incompressible Semi-Geostrophic\n  Equations in Eulerian Coordinates Abstract: We prove the existence of local-in-time smooth solutions of the\nincompressible semi-geostrophic equations expressed in Eulerian co-ordinates in\n3-dimensional smooth bounded simply-connected domains. Our solutions adhere to\nCullen's Stability Principle in that the geopotential is guaranteed to be a\nconvex map for all times of its existence. We achieve our results by appealing\nto the theory of so-called div-curl systems (or Hodge systems), making use of\nrecent results of Wang, which yield useful estimates on the ageostrophic\nvelocity field. To our knowledge, this work constitutes the first time that any\nnotion of bounded solution of the semi-geostrophic equations in Eulerian\nco-ordinates has been constructed on a bounded domain. Indeed, our work solves\nan open problem as highlighted by, among others, A. Figalli in his CIME\nlectures on the semi-geostrophic equations. Our methods are largely elementary.\nWe discuss the application of the novel ideas in this work to the case of\nvariable Coriolis force in the final section of the article. \n\n"}
{"id": "1807.01914", "contents": "Title: A multiple-try Metropolis-Hastings algorithm with tailored proposals Abstract: We present a new multiple-try Metropolis-Hastings algorithm designed to be\nespecially beneficial when a tailored proposal distribution is available. The\nalgorithm is based on a given acyclic graph $G$, where one of the nodes in $G$,\n$k$ say, contains the current state of the Markov chain and the remaining nodes\ncontain proposed states generated by applying the tailored proposal\ndistribution. The Metropolis-Hastings algorithm alternates between two types of\nupdates. The first update type is using the tailored proposal distribution to\ngenerate new states in all nodes in $G$ except in node $k$. The second update\ntype is generating a new value for $k$, thereby changing the value of the\ncurrent state. We evaluate the effectiveness of the proposed scheme in an\nexample with previously defined target and proposal distributions. \n\n"}
{"id": "1807.05507", "contents": "Title: Adaptive Dimension Reduction to Accelerate Infinite-Dimensional\n  Geometric Markov Chain Monte Carlo Abstract: Bayesian inverse problems highly rely on efficient and effective inference\nmethods for uncertainty quantification (UQ). Infinite-dimensional MCMC\nalgorithms, directly defined on function spaces, are robust under refinement of\nphysical models. Recent development of this class of algorithms has started to\nincorporate the geometry of the posterior informed by data so that they are\ncapable of exploring complex probability structures. However, the required\ngeometric quantities are usually expensive to obtain in high dimensions. On the\nother hand, most geometric information of the unknown parameter space in this\nsetting is concentrated in an \\emph{intrinsic} finite-dimensional subspace. To\nmitigate the computational intensity and scale up the applications of\ninfinite-dimensional geometric MCMC ($\\infty$-GMC), we apply geometry-informed\nalgorithms to the intrinsic subspace to probe its complex structure, and\nsimpler methods like preconditioned Crank-Nicolson (pCN) to its geometry-flat\ncomplementary subspace. In this work, we take advantage of dimension reduction\ntechniques to accelerate the original $\\infty$-GMC algorithms. More\nspecifically, partial spectral decomposition of the (prior or posterior)\ncovariance operator is used to identify certain number of principal\neigen-directions as a basis for the intrinsic subspace. The combination of\ndimension-independent algorithms, geometric information, and dimension\nreduction yields more efficient implementation, \\emph{(adaptive)\ndimension-reduced infinite-dimensional geometric MCMC}. With a small amount of\ncomputational overhead, we can achieve over 70 times speed-up compared to pCN\nusing a simulated elliptic inverse problem and an inverse problem involving\nturbulent combustion. A number of error bounds comparing various MCMC proposals\nare presented to predict the asymptotic behavior of the proposed\ndimension-reduced algorithms. \n\n"}
{"id": "1807.06240", "contents": "Title: Definition of the moist-air exergy norm: a comparison with existing\n  \"moist energy norms\" Abstract: This study presents a new formulation for the norms and scalar products used\nin tangent linear or adjoint models to determine forecast errors and\nsensitivity to observations and to calculate singular vectors. The new norm is\nderived from the concept of moist-air available enthalpy, which is one of the\navailability functions referred to as exergy in general thermodynamics. It is\nshown that the sum of the kinetic energy and the moist-air available enthalpy\ncan be used to define a new moist-air squared norm which is quadratic in: 1)\nwind components; 2) temperature; 3) surface pressure; and 4) water vapor\ncontent. Preliminary numerical applications are performed to show that the new\nweighting factors for temperature and water vapor are significantly different\nfrom those used in observation impact studies, and are in better agreement with\nobserved analysis increments. These numerical applications confirm that the\nweighting factors for water vapor and temperature exhibit a large increase with\nheight (by several orders of magnitude) and a minimum in the middle\ntroposphere, respectively. \n\n"}
{"id": "1807.08735", "contents": "Title: Uniform in time error estimates for a finite element method applied to a\n  downscaling data assimilation algorithm for the Navier-Stokes equations Abstract: In this paper we analyze a finite element method applied to a continuous\ndownscaling data assimilation algorithm for the numerical approximation of the\ntwo and three dimensional Navier-Stokes equations corresponding to given\nmeasurements on a coarse spatial scale. For representing the coarse mesh\nmeasurements we consider different types of interpolation operators including a\nLagrange interpolant. We obtain uniform-in-time estimates for the error between\na finite element approximation and the reference solution corresponding to the\ncoarse mesh measurements. We consider both the case of a plain Galerkin method\nand a Galerkin method with grad-div stabilization. For the stabilized method we\nprove error bounds in which the constants do not depend on inverse powers of\nthe viscosity. Some numerical experiments illustrate the theoretical results. \n\n"}
{"id": "1807.09737", "contents": "Title: Convergence Rates of Gaussian ODE Filters Abstract: A recently-introduced class of probabilistic (uncertainty-aware) solvers for\nordinary differential equations (ODEs) applies Gaussian (Kalman) filtering to\ninitial value problems. These methods model the true solution $x$ and its first\n$q$ derivatives \\emph{a priori} as a Gauss--Markov process $\\boldsymbol{X}$,\nwhich is then iteratively conditioned on information about $\\dot{x}$. This\narticle establishes worst-case local convergence rates of order $q+1$ for a\nwide range of versions of this Gaussian ODE filter, as well as global\nconvergence rates of order $q$ in the case of $q=1$ and an integrated Brownian\nmotion prior, and analyses how inaccurate information on $\\dot{x}$ coming from\napproximate evaluations of $f$ affects these rates. Moreover, we show that, in\nthe globally convergent case, the posterior credible intervals are well\ncalibrated in the sense that they globally contract at the same rate as the\ntruncation error. We illustrate these theoretical results by numerical\nexperiments which might indicate their generalizability to $q \\in\n\\{2,3,\\dots\\}$. \n\n"}
{"id": "1807.11143", "contents": "Title: ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks Abstract: To backpropagate the gradients through stochastic binary layers, we propose\nthe augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low\nvariance, and has low computational complexity. Exploiting variable\naugmentation, REINFORCE, and reparameterization, the ARM estimator achieves\nadaptive variance reduction for Monte Carlo integration by merging two\nexpectations via common random numbers. The variance-reduction mechanism of the\nARM estimator can also be attributed to either antithetic sampling in an\naugmented space, or the use of an optimal anti-symmetric \"self-control\"\nbaseline function together with the REINFORCE estimator in that augmented\nspace. Experimental results show the ARM estimator provides state-of-the-art\nperformance in auto-encoding variational inference and maximum likelihood\nestimation, for discrete latent variable models with one or multiple stochastic\nbinary layers. Python code for reproducible research is publicly available. \n\n"}
{"id": "1807.11859", "contents": "Title: Condensational and collisional growth of cloud droplets in a turbulent\n  environment Abstract: We investigate the effect of turbulence on the combined condensational and\ncollisional growth of cloud droplets by means of high resolution direct\nnumerical simulations of turbulence and a superparticle approximation for\ndroplet dynamics and collisions. The droplets are subject to turbulence as well\nas gravity, and their collision and coalescence efficiencies are taken to be\nunity. We solve the thermodynamic equations governing temperature, water-vapor\nmixing ratio, and the resulting supersaturation fields together with the\nNavier-Stokes equation. We find that the droplet-size distribution broadens\nwith increasing Reynolds number and/or mean energy dissipation rate. Turbulence\naffects the condensational growth directly through supersaturation\nfluctuations, and it influences collisional growth indirectly through\ncondensation. Our simulations show for the first time that, in the absence of\nthe mean updraft cooling, supersaturation fluctuation-induced broadening of\ndroplet-size distributions enhances the collisional growth. This is contrary to\nclassical (non-turbulent) condensational growth, which leads to a growing mean\ndroplet size, but a narrower droplet-size distribution. Our findings, instead,\nshow that condensational growth facilitates collisional growth by broadening\nthe size distribution in the tails at an early stage of rain formation. With\nincreasing Reynolds numbers, evaporation becomes stronger. This counteracts the\nbroadening effect due to condensation at late stages of rain formation. Our\nconclusions are consistent with results of laboratory experiments and field\nobservations, and show that supersaturation fluctuations are important for\nprecipitation. \n\n"}
{"id": "1808.01770", "contents": "Title: Spline Regression with Automatic Knot Selection Abstract: In this paper we introduce a new method for automatically selecting knots in\nspline regression. The approach consists in setting a large number of initial\nknots and fitting the spline regression through a penalized likelihood\nprocedure called adaptive ridge. The proposed method is similar to penalized\nspline regression methods (e.g. P-splines), with the noticeable difference that\nthe output is a sparse spline regression with a small number of knots. We show\nthat our method called A-spline, for adaptive splines yields sparse regression\nmodels with high interpretability, while having similar predictive performance\nsimilar to penalized spline regression methods. A-spline is applied both to\nsimulated and real dataset. A fast and publicly available implementation in R\nis provided along with this paper. \n\n"}
{"id": "1808.01814", "contents": "Title: Anisotropic turbulent transport in stably stratified rotating stellar\n  radiation zones Abstract: Rotation is one of the key physical mechanisms that deeply impact the\nevolution of stars. Helio- and asteroseismology reveal a strong extraction of\nangular momentum from stellar radiation zones over the whole\nHertzsprung-Russell diagram. Turbulent transport in differentially rotating\nstably stratified stellar radiation zones should be carefully modeled and its\nstrength evaluated. Stratification and rotation imply that this turbulent\ntransport is anisotropic. Only phenomenological prescriptions have been\nproposed for the transport in the horizontal direction, which however\nconstitutes a cornerstone in current theoretical formalisms for stellar\nhydrodynamics in evolution codes. We derive a new theoretical prescription for\nthe anisotropy of the turbulent transport in radiation zones using a spectral\nformalism for turbulence that takes simultaneously stable stratification,\nrotation, and a radial shear into account. Then, the horizontal turbulent\ntransport resulting from 3D turbulent motions sustained by the instability of\nthe radial differential rotation is derived. We implement this framework in the\nstellar evolution code STAREVOL and quantify its impact on the rotational and\nstructural evolution of low-mass stars from the pre-main-sequence to the red\ngiant branch. The anisotropy of the turbulent transport scales as\n$N^4\\tau^2/\\left(2\\Omega^2\\right)$, $N$ and $\\Omega$ being the buoyancy and\nrotation frequencies respectively and $\\tau$ a time characterizing the source\nof turbulence. This leads to a horizontal turbulent transport of similar\nstrength in average that those obtained with previously proposed prescriptions\neven if it can be locally larger below the convective envelope. As a\nconsequence, a complementary transport mechanism like internal gravity waves or\nmagnetic fields is still needed to explain the observed strong transport of\nangular momentum along stellar evolution. \n\n"}
{"id": "1808.01932", "contents": "Title: CaliCo: a R package for Bayesian calibration Abstract: In this article, we present a recently released R package for Bayesian\ncalibration. Many industrial fields are facing unfeasible or costly field\nexperiments. These experiments are replaced with numerical/computer experiments\nwhich are realized by running a numerical code. Bayesian calibration intends to\nestimate, through a posterior distribution, input parameters of the code in\norder to make the code outputs close to the available experimental data. The\ncode can be time consuming while the Bayesian calibration implies a lot of code\ncalls which makes studies too burdensome. A discrepancy might also appear\nbetween the numerical code and the physical system when facing incompatibility\nbetween experimental data and numerical code outputs. The package CaliCo deals\nwith these issues through four statistical models which deal with a time\nconsuming code or not and with discrepancy or not. A guideline for users is\nprovided in order to illustrate the main functions and their arguments.\nEventually, a toy example is detailed using CaliCo. This example (based on a\nreal physical system) is in five dimensions and uses simulated data. \n\n"}
{"id": "1808.02310", "contents": "Title: Effects of periodic forcing on a Paleoclimate delay model Abstract: We present a study of a delay differential equation (DDE) model for the\nMid-Pleistocene Transition (MPT). We investigate the behavior of the model when\nsubjected to periodic forcing. The unforced model has a bistable region\nconsisting of a stable equilibrium along with a large amplitude stable periodic\norbit. We study how forcing affects solutions in this region. Forcing based on\nastronomical data causes a sudden transition in time and under increase of the\nforcing amplitude, moving the model response from a non-MPT regime to an MPT\nregime. Similar transition behavior is found for periodic forcing. A\nbifurcation analysis shows that the transition is not due to a bifurcation but\ninstead to a shifting basin of attraction. While determining the basin boundary\nwe demonstrate how one can accurately compute the intersection of a stable\nmanifold of a saddle with a slow manifold in a DDE by embedding the algorithm\nfor planar maps proposed by England et al. (SIADS 2004(3)) into the\nequation-free framework by Kevrekidis et al. (Rev. Phys. Chem. 2009 (60)). \n\n"}
{"id": "1808.03975", "contents": "Title: Global Existence of Weak Solutions to the Compressible Primitive\n  Equations of Atmospheric Dynamics with Degenerate Viscosities Abstract: We show the existence of global weak solutions to the three-dimensional\ncompressible primitive equations of atmospheric dynamics with degenerate\nviscosities. In analogy with the case of the compressible Navier-Stokes\nequations, the weak solutions satisfy the basic energy inequality, the\nBresh-Desjardins entropy inequality and the Mellet-Vasseur estimate. These\nestimates play an important role in establishing the compactness of the\nvertical velocity of the approximating solutions, and therefore are essential\nto recover the vertical velocity in the weak solutions. \n\n"}
{"id": "1808.05414", "contents": "Title: Functional Outlier Detection and Taxonomy by Sequential Transformations Abstract: Functional data analysis can be seriously impaired by abnormal observations,\nwhich can be classified as either magnitude or shape outliers based on their\nway of deviating from the bulk of data. Identifying magnitude outliers is\nrelatively easy, while detecting shape outliers is much more challenging. We\npropose turning the shape outliers into magnitude outliers through data\ntransformation and detecting them using the functional boxplot. Besides easing\nthe detection procedure, applying several transformations sequentially provides\na reasonable taxonomy for the flagged outliers. A joint functional ranking,\nwhich consists of several transformations, is also defined here. Simulation\nstudies are carried out to evaluate the performance of the proposed method\nusing different functional depth notions. Interesting results are obtained in\nseveral practical applications. \n\n"}
{"id": "1808.05653", "contents": "Title: Atomic iron and titanium in the atmosphere of the exoplanet KELT-9b Abstract: The chemical composition of an exoplanet is a key ingredient in constraining\nits formation history. Iron is the most abundant transition metal, but has\nnever been directly detected in an exoplanet due to its highly refractory\nnature. KELT-9b (HD 195689b) is the archetype of the class of ultra-hot\nJupiters that straddle the transition between stars and gas-giant exoplanets\nand serve as distinctive laboratories for studying atmospheric chemistry,\nbecause of its high equilibrium temperature of 4050 +/- 180 K. These properties\nimply that its atmosphere is a tightly constrained chemical system that is\nexpected to be nearly in chemical equilibrium and cloud-free. It was previously\npredicted that the spectral lines of iron will be detectable in the visible\nrange of wavelengths. At these high temperatures, iron and several other\ntransition metals are not sequestered in molecules or cloud particles and exist\nsolely in their atomic forms. Here, we report the direct detection of atomic\nneutral and singly-ionized iron (Fe and Fe+), and singly-ionized titanium (Ti+)\nin KELT-9b via the cross-correlation technique applied to high-resolution\nspectra obtained during the primary transit of the exoplanet. \n\n"}
{"id": "1808.05884", "contents": "Title: An Empirical Evaluation of the Approximation of Subjective Logic\n  Operators Using Monte Carlo Simulations Abstract: In this paper we analyze the use of subjective logic as a framework for\nperforming approximate transformations over probability distribution functions.\nAs for any approximation, we evaluate subjective logic in terms of\ncomputational efficiency and bias. However, while the computational cost may be\neasily estimated, the bias of subjective logic operators have not yet been\ninvestigated. In order to evaluate this bias, we propose an experimental\nprotocol that exploits Monte Carlo simulations and their properties to assess\nthe distance between the result produced by subjective logic operators and the\ntrue result of the corresponding transformation over probability distributions.\nThis protocol allows a modeler to get an estimate of the degree of\napproximation she must be ready to accept as a trade-off for the computational\nefficiency and the interpretability of the subjective logic framework.\nConcretely, we apply our method to the relevant case study of the subjective\nlogic operator for binomial multiplication and fusion, and we study empirically\ntheir degree of approximation. \n\n"}
{"id": "1808.08618", "contents": "Title: Deep Learning: Computational Aspects Abstract: In this article we review computational aspects of Deep Learning (DL). Deep\nlearning uses network architectures consisting of hierarchical layers of latent\nvariables to construct predictors for high-dimensional input-output models.\nTraining a deep learning architecture is computationally intensive, and\nefficient linear algebra libraries is the key for training and inference.\nStochastic gradient descent (SGD) optimization and batch sampling are used to\nlearn from massive data sets. \n\n"}
{"id": "1808.09489", "contents": "Title: Convergence Rate of Krasulina Estimator Abstract: Principal component analysis (PCA) is one of the most commonly used\nstatistical procedures with a wide range of applications. Consider the points\n$X_1, X_2,..., X_n$ are vectors drawn i.i.d. from a distribution with mean zero\nand covariance $\\Sigma$, where $\\Sigma$ is unknown. Let $A_n = X_nX_n^T$, then\n$E[A_n] = \\Sigma$. This paper consider the problem of finding the least\neigenvalue and eigenvector of matrix $\\Sigma$. A classical such estimator are\ndue to Krasulina\\cite{krasulina_method_1969}. We are going to state the\nconvergence proof of Krasulina for the least eigenvalue and corresponding\neigenvector, and then find their convergence rate. \n\n"}
{"id": "1808.10507", "contents": "Title: Variational integrator for the rotating shallow-water equations on the\n  sphere Abstract: We develop a variational integrator for the shallow-water equations on a\nrotating sphere. The variational integrator is built around a discretization of\nthe continuous Euler-Poincar\\'{e} reduction framework for Eulerian\nhydrodynamics. We describe the discretization of the continuous\nEuler-Poincar\\'{e} equations on arbitrary simplicial meshes. Standard numerical\ntests are carried out to verify the accuracy and the excellent conservational\nproperties of the discrete variational integrator. \n\n"}
{"id": "1809.00027", "contents": "Title: Dimensionality-Reduction of Climate Data using Deep Autoencoders Abstract: We explore the use of deep neural networks for nonlinear dimensionality\nreduction in climate applications. We train convolutional autoencoders (CAEs)\nto encode two temperature field datasets from pre-industrial control runs in\nthe CMIP5 first ensemble, obtained with the CCSM4 model and the IPSL-CM5A-LR\nmodel, respectively. With the later dataset, consisting of 36500 96$\\times$96\nsurface temperature fields, the CAE out-performs PCA in terms of mean squared\nerror of the reconstruction from a 40 dimensional encoding. Moreover, the noise\nin the filters of the convolutional layers in the autoencoders suggests that\nthe CAE can be trained to produce better results. Our results indicate that\nconvolutional autoencoders may provide an effective platform for the\nconstruction of surrogate climate models. \n\n"}
{"id": "1809.00256", "contents": "Title: Percolation Framework of the Earth's Topography Abstract: Self-similarity and long-range correlations are the remarkable features of\nthe Earth's surface topography. Here we develop an approach based on\npercolation theory to study the geometrical features of Earth. Our analysis is\nbased on high-resolution, 1 arc min, ETOPO1 global relief records.We find some\nevidence for abrupt transitions that occurred during the evolution of the\nEarth's relief network, indicative of a continental/cluster aggregation. We\napply finite-size-scaling analysis based on a coarse-graining procedure to show\nthat the observed transition is most likely discontinuous. Furthermore, we\nstudy the percolation on two-dimensional fractional Brownian motion surfaces\nwith Hurst exponent $H$ as a model of long-range correlated topography, which\nsuggests that the long-range correlations may play a key role in the observed\ndiscontinuity on Earth. Our framework presented here provides a theoretical\nmodel to better understand the geometrical phase transition on Earth, and it\nalso identifies the critical nodes that will be more exposed to global climate\nchange in the Earth's relief network. \n\n"}
{"id": "1809.00987", "contents": "Title: Two tsunamis generated near the Kuril Islands and analytical long wave\n  theory Abstract: This paper addresses long waves on the water surface. It is assumed that\ninitially the water surface has not yet been displaced from its mean level, but\nthe velocity field has already become different from zero. This means that the\nmotion of a body of water is triggered by a sudden change in the velocity\nfield.\n  The long wave is modeled mathematically as a specific wave packet. The model\nis used to estimate duration of the wave origin formation, size of the origin,\nwater elevation in the origin, energy supplied to the water by the quake,\ndistribution of wave heights in the wave packet. Relationship between group\nvelocity of the packet and phase velocity of the packet's wave of maximum\nheight is revealed. These results are applied to Kuril tsunamis of November\n2006 and January 2007. \n\n"}
{"id": "1809.02385", "contents": "Title: Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers Abstract: In recent years, data have become increasingly higher dimensional and,\ntherefore, an increased need has arisen for dimension reduction techniques for\nclustering. Although such techniques are firmly established in the literature\nfor multivariate data, there is a relative paucity in the area of matrix\nvariate, or three-way, data. Furthermore, the few methods that are available\nall assume matrix variate normality, which is not always sensible if cluster\nskewness or excess kurtosis is present. Mixtures of bilinear factor analyzers\nusing skewed matrix variate distributions are proposed. In all, four such\nmixture models are presented, based on matrix variate skew-t, generalized\nhyperbolic, variance-gamma, and normal inverse Gaussian distributions,\nrespectively. \n\n"}
{"id": "1809.04000", "contents": "Title: Statistical post-processing of hydrological forecasts using Bayesian\n  model averaging Abstract: Accurate and reliable probabilistic forecasts of hydrological quantities like\nrunoff or water level are beneficial to various areas of society. Probabilistic\nstate-of-the-art hydrological ensemble prediction models are usually driven\nwith meteorological ensemble forecasts. Hence, biases and dispersion errors of\nthe meteorological forecasts cascade down to the hydrological predictions and\nadd to the errors of the hydrological models. The systematic parts of these\nerrors can be reduced by applying statistical post-processing. For a sound\nestimation of predictive uncertainty and an optimal correction of systematic\nerrors, statistical post-processing methods should be tailored to the\nparticular forecast variable at hand. Former studies have shown that it can\nmake sense to treat hydrological quantities as bounded variables. In this\npaper, a doubly truncated Bayesian model averaging (BMA) method, which allows\nfor flexible post-processing of (multi-model) ensemble forecasts of water\nlevel, is introduced. A case study based on water level for a gauge of river\nRhine, reveals a good predictive skill of doubly truncated BMA compared both to\nthe raw ensemble and the reference ensemble model output statistics approach. \n\n"}
{"id": "1809.04129", "contents": "Title: Rethinking the Effective Sample Size Abstract: The effective sample size (ESS) is widely used in sample-based simulation\nmethods for assessing the quality of a Monte Carlo approximation of a given\ndistribution and of related integrals. In this paper, we revisit the\napproximation of the ESS in the specific context of importance sampling (IS).\nThe derivation of this approximation, that we will denote as\n$\\widehat{\\text{ESS}}$, is partially available in Kong (1992). This\napproximation has been widely used in the last 25 years due to its simplicity\nas a practical rule of thumb in a wide variety of importance sampling methods.\nHowever, we show that the multiple assumptions and approximations in the\nderivation of $\\widehat{\\text{ESS}}$, makes it difficult to be considered even\nas a reasonable approximation of the ESS. We extend the discussion of the\n$\\widehat{\\text{ESS}}$ in the multiple importance sampling (MIS) setting, we\ndisplay numerical examples, and we discuss several avenues for developing\nalternative metrics. This paper does not cover the use of ESS for MCMC\nalgorithms. \n\n"}
{"id": "1809.05065", "contents": "Title: Lyapunov analysis of multiscale dynamics: The slow bundle of the\n  two-scale Lorenz 96 model Abstract: We investigate the geometrical structure of instabilities in the two-scales\nLorenz 96 model through the prism of Lyapunov analysis. Our detailed study of\nthe full spectrum of covariant Lyapunov vectors reveals the presence of a slow\nbundle in tangent space, composed by a set of vectors with a significant\nprojection on the slow degrees of freedom; they correspond to the smallest (in\nabsolute value) Lyapunov exponents and thereby to the longer time scales. We\nshow that the dimension of the slow bundle is extensive in the number of both\nslow and fast degrees of freedom, and discuss its relationship with the results\nof a finite-size analysis of instabilities, supporting the conjecture that the\nslow-variable behavior is effectively determined by a non-trivial subset of\ndegrees of freedom. More precisely, we show that the slow bundle corresponds to\nthe Lyapunov spectrum region where fast and slow instability rates overlap,\n\"mixing\" their evolution into a set of vectors which simultaneously carry\ninformation on both scales. We suggest these results may pave the way for\nfuture applications to ensemble forecasting and data assimilations in weather\nand climate models. \n\n"}
{"id": "1809.05430", "contents": "Title: Surrogate-based global sensitivity analysis for turbulence and\n  fire-spotting effects in regional-scale wildland fire modeling Abstract: In presence of strong winds, wildfires feature nonlinear behavior, possibly\ninducing fire-spotting. We present a global sensitivity analysis of a new\nsub-model for turbulence and fire-spotting included in a wildfire spread model\nbased on a stochastic representation of the fireline. To limit the number of\nmodel evaluations, fast surrogate models based on generalized Polynomial Chaos\n(gPC) and Gaussian Process are used to identify the key parameters affecting\ntopology and size of burnt area. This study investigates the application of\nthese surrogates to compute Sobol' sensitivity indices in an idealized test\ncase. The wind is known to drive the fire propagation. The results show that it\nis a more general leading factor that governs the generation of secondary\nfires. This study also compares the performance of the surrogates for varying\nsize and type of training sets as well as for varying parameterization and\nchoice of algorithms. The best performance was achieved using a gPC strategy\nbased on a sparse least-angle regression (LAR) and a low-discrepancy Halton's\nsequence. Still, the LAR-based gPC surrogate tends to filter out the\ninformation coming from parameters with large length-scale, which is not the\ncase of the cleaning-based gPC surrogate. For both algorithms, sparsity ensures\na surrogate can be built using an affordable number of forward model\nevaluations, while the model response is highly multi-scale and nonlinear.\nUsing a sparse surrogate is thus a promising strategy to analyze new models and\nits dependency on input parameters in wildfire applications. \n\n"}
{"id": "1809.06405", "contents": "Title: Bayesian analysis of absolute continuous Marshall-Olkin bivariate Pareto\n  distribution with location and scale parameters Abstract: This paper provides two different novel approaches of slice sampling to\nestimate the parameters of absolute continuous Marshall-Olkin bivariate Pareto\ndistribution with location and scale parameters. We carry out the bayesian\nanalysis taking gamma prior for shape and scale parameters and truncated normal\nfor location parameters. Credible intervals and coverage probabilities are also\nprovided for all methods. A real-life data analysis is shown for illustrative\npurpose. \n\n"}
{"id": "1809.06467", "contents": "Title: Atmospheric variability driven by radiative cloud feedback in brown\n  dwarfs and directly imaged extrasolar giant planets Abstract: Growing observational evidence has suggested active meteorology in\natmospheres of brown dwarfs (BDs) and directly imaged extrasolar giant planets\n(EGPs). In particular, a number of surveys have shown that near-IR brightness\nvariability is common among L and T dwarfs. Despite initial understandings of\natmospheric dynamics which is the major cause of the variability by previous\nstudies, the detailed mechanism of variability remains elusive, and we need to\nseek a natural, self-consistent mechanism. Clouds are important in shaping the\nthermal structure and spectral properties of these atmospheres via large\nopacity, and we expect the same for inducing atmospheric variability. In this\nwork, using a time-dependent one-dimensional model that incorporates a\nself-consistent coupling between the thermal structure, convective mixing,\ncloud radiative heating/cooling and condensation/evaporation of clouds, we show\nthat radiative cloud feedback can drive spontaneous atmospheric variability in\nboth temperature and cloud structure in conditions appropriate for BDs and\ndirectly imaged EGPs. The typical periods of variability are one to tens of\nhours with typical amplitude of the variability up to hundreds of Kelvins in\neffective temperature. The existence of variability is robust over a wide range\nof parameter space, but the detailed evolution of variability is sensitive to\nmodel parameters. Our novel, self-consistent mechanism has important\nimplications for the observed flux variability of BDs and directly imaged EGPs,\nespecially those evolve in short timescales. It is also a promising mechanism\nfor cloud breaking, which has been proposed to explain the L/T transition of\nbrown dwarfs. \n\n"}
{"id": "1809.06924", "contents": "Title: Modelling sea ice and melt ponds evolution: sensitivity to microscale\n  heat transfer mechanisms Abstract: We present a mathematical model describing the evolution of sea ice and\nmeltwater during summer. The system is described by two coupled partial\ndifferential equations for the ice thickness $h$ and pond depth $w$ fields. We\ntest the sensitivity of the model to variations of parameters controlling\nfluid-dynamic processes at the pond level, namely the variation of turbulent\nheat flux with pond depth and the lateral melting of ice enclosing a pond. We\nobserve that different heat flux scalings determine different rates of total\nsurface ablations, while the system is relatively robust in terms of\nprobability distributions of pond surface areas. Finally, we study pond\nmorphology in terms of fractal dimensions, showing that the role of lateral\nmelting is minor, whereas there is evidence of an impact from the initial sea\nice topography. \n\n"}
{"id": "1809.08671", "contents": "Title: Jovian vortices and jets Abstract: We explore the conditions required for isolated vortices to exist in sheared\nzonal flows and the stability of the underlying zonal winds. This is done using\nthe standard 2-layer quasigeostrophic model with the lower layer depth becoming\ninfinite; however, this model differs from the usual layer model because the\nlower layer is not assumed to be motionless but has a steady configuration of\nalternating zonal flows [1]. Steady state vortices are obtained by a simulated\nannealing computational method introduced in [2], generalized and applied in\n[3] in fluid flow, and used in the context of magnetohydrodynamics in [4-6].\nVarious cases of vortices with a constant potential vorticity anomaly atop\nzonal winds and the stability of the underlying winds are considered using a\nmix of computational and analytical techniques. \n\n"}
{"id": "1809.08672", "contents": "Title: Buoyant Motion of a Turbulent Thermal Abstract: By introducing an equivalence between magnetostatics and the equations\ngoverning buoyant motion, we derive analytical expressions for the acceleration\nof isolated density anomalies, a.k.a. thermals. In particular, we investigate\nbuoyant acceleration, defined as the sum of the Archimedean buoyancy $B$ and an\nassociated perturbation pressure gradient. For the case of a uniform spherical\nthermal, the anomaly fluid accelerates at $2B/3$, extending the textbook result\nfor the induced mass of a solid sphere to the case of a fluid sphere. For a\nmore general ellipsoidal thermal, we show that the buoyant acceleration is a\nsimple analytical function of the ellipsoid's aspect ratio. The relevance of\nthese idealized uniform-density results to turbulent thermals is explored by\nanalyzing direct numerical simulations of thermals at $\\textit{Re}=6300$. We\nfind that our results fully characterize a thermal's initial motion over a\ndistance comparable to its length. Beyond this buoyancy-dominated regime, a\nthermal develops an ellipsoidal vortex circulation and begins to entrain\nenvironmental fluid. Our analytical expressions do not describe the total\nacceleration of this mature thermal, but still accurately relate the buoyant\nacceleration to the thermal's mean Archimedean buoyancy and aspect ratio. Thus,\nour analytical formulae provide a simple and direct means of estimating the\nbuoyant acceleration of turbulent thermals. \n\n"}
{"id": "1809.10227", "contents": "Title: Symmetry Exploits for Bayesian Cubature Methods Abstract: Bayesian cubature provides a flexible framework for numerical integration, in\nwhich a priori knowledge on the integrand can be encoded and exploited. This\nadditional flexibility, compared to many classical cubature methods, comes at a\ncomputational cost which is cubic in the number of evaluations of the\nintegrand. It has been recently observed that fully symmetric point sets can be\nexploited in order to reduce - in some cases substantially - the computational\ncost of the standard Bayesian cubature method. This work identifies several\nadditional symmetry exploits within the Bayesian cubature framework. In\nparticular, we go beyond earlier work in considering non-symmetric measures\nand, in addition to the standard Bayesian cubature method, present exploits for\nthe Bayes-Sard cubature method and the multi-output Bayesian cubature method. \n\n"}
{"id": "1810.00810", "contents": "Title: Multilevel Adaptive Sparse Grid Quadrature for Monte Carlo models Abstract: Many problems require to approximate an expected value by some kind of Monte\nCarlo (MC) sampling, e.g. molecular dynamics (MD) or simulation of stochastic\nreaction models (also termed kinetic Monte Carlo (kMC)). Often, we are\nfurthermore interested in some integral of the MC model's output over the input\nparameters. We present a Multilevel Adaptive Sparse Grid strategy for the\nnumerical integration of such problems where the integrand is implicitly\ndefined by a Monte Carlo model. In this approach, we exploit different levels\nof sampling accuracy in the Monte Carlo model to reduce the overall\ncomputational costs compared to a single level approach. Unlike existing\napproaches for Multilevel Numerical Quadrature, our approach is not based on a\ntelescoping sum, but we rather utilize the intrinsic multilevel structure of\nthe sparse grids and the employed locally supported, piecewise linear basis\nfunctions. Besides illustrative toy models, we demonstrate the methodology on a\nrealistic kMC model for CO oxidation. We find significant savings compared to\nthe single level approach - often orders of magnitude. \n\n"}
{"id": "1810.02030", "contents": "Title: Robust Estimation and Generative Adversarial Nets Abstract: Robust estimation under Huber's $\\epsilon$-contamination model has become an\nimportant topic in statistics and theoretical computer science. Statistically\noptimal procedures such as Tukey's median and other estimators based on depth\nfunctions are impractical because of their computational intractability. In\nthis paper, we establish an intriguing connection between $f$-GANs and various\ndepth functions through the lens of $f$-Learning. Similar to the derivation of\n$f$-GANs, we show that these depth functions that lead to statistically optimal\nrobust estimators can all be viewed as variational lower bounds of the total\nvariation distance in the framework of $f$-Learning. This connection opens the\ndoor of computing robust estimators using tools developed for training GANs. In\nparticular, we show in both theory and experiments that some appropriate\nstructures of discriminator networks with hidden layers in GANs lead to\nstatistically optimal robust location estimators for both Gaussian distribution\nand general elliptical distributions where first moment may not exist. \n\n"}
{"id": "1810.03328", "contents": "Title: Topological transition in stratified fluids Abstract: Lamb waves are trapped acoustic-gravity waves that propagate energy over\ngreat distances along a solid boundary in density stratified, compressible\nfluids. They constitute useful indicators of explosions in planetary\natmospheres. When the density stratification exceeds a threshold, or when the\nimpermeability condition at the boundary is relaxed, atmospheric Lamb waves\nsuddenly disappear. Here we use topological arguments to predict the possible\nexistence of new trapped Lamb-like waves in the absence of a solid boundary,\ndepending on the stratification profile. The topological origin of the\nLamb-like waves is emphasized by relating their existence to two-band crossing\npoints carrying opposite Chern numbers. The existence of these band crossings\ncoincides with a restoration of the vertical mirror symmetry that is in general\nbroken by gravity. From this perspective, Lamb-like waves also bear strong\nsimilarities with boundary modes encountered in quantum valley Hall effect. Our\nstudy shows that the presence of Lamb-like waves encode essential information\non the underlying stratification profile in astrophysical and geophysical\nflows, which is often poorly constrained by observations. \n\n"}
{"id": "1810.03688", "contents": "Title: Practical Bayesian Optimization for Transportation Simulators Abstract: We provide a method to solve optimization problem when objective function is\na complex stochastic simulator of an urban transportation system. To reach this\ngoal, a Bayesian optimization framework is introduced. We show how the choice\nof prior and inference algorithm effect the outcome of our optimization\nprocedure. We develop dimensionality reduction techniques that allow for our\noptimization techniques to be applicable for real-life problems. We develop a\ndistributed, Gaussian Process Bayesian regression and active learning models\nthat allow parallel execution of our algorithms and enable usage of high\nperformance computing. We present a fully Bayesian approach that is more sample\nefficient and reduces computational budget. Our framework is supported by\ntheoretical analysis and an empirical study. We demonstrate our framework on\nthe problem of calibrating a multi-modal transportation network of city of\nBloomington, Illinois. Finally, we discuss directions for further research. \n\n"}
{"id": "1810.04200", "contents": "Title: Multi-resolution filters for massive spatio-temporal data Abstract: Spatio-temporal data sets are rapidly growing in size. For example,\nenvironmental variables are measured with ever-higher resolution by increasing\nnumbers of automated sensors mounted on satellites and aircraft. Using such\ndata, which are typically noisy and incomplete, the goal is to obtain complete\nmaps of the spatio-temporal process, together with proper uncertainty\nquantification. We focus here on real-time filtering inference in linear\nGaussian state-space models. At each time point, the state is a spatial field\nevaluated on a very large spatial grid, making exact inference using the Kalman\nfilter computationally infeasible. Instead, we propose a multi-resolution\nfilter (MRF), a highly scalable and fully probabilistic filtering method that\nresolves spatial features at all scales. We prove that the MRF matrices exhibit\na particular block-sparse multi-resolution structure that is preserved under\nfiltering operations through time. We also discuss inference on time-varying\nparameters using an approximate Rao-Blackwellized particle filter, in which the\nintegrated likelihood is computed using the MRF. We compare the MRF to existing\napproaches in a simulation study and a real satellite-data application. \n\n"}
{"id": "1810.04443", "contents": "Title: On the Properties of Simulation-based Estimators in High Dimensions Abstract: Considering the increasing size of available data, the need for statistical\nmethods that control the finite sample bias is growing. This is mainly due to\nthe frequent settings where the number of variables is large and allowed to\nincrease with the sample size bringing standard inferential procedures to incur\nsignificant loss in terms of performance. Moreover, the complexity of\nstatistical models is also increasing thereby entailing important computational\nchallenges in constructing new estimators or in implementing classical ones. A\ntrade-off between numerical complexity and statistical properties is often\naccepted. However, numerically efficient estimators that are altogether\nunbiased, consistent and asymptotically normal in high dimensional problems\nwould generally be ideal. In this paper, we set a general framework from which\nsuch estimators can easily be derived for wide classes of models. This\nframework is based on the concepts that underlie simulation-based estimation\nmethods such as indirect inference. The approach allows various extensions\ncompared to previous results as it is adapted to possibly inconsistent\nestimators and is applicable to discrete models and/or models with a large\nnumber of parameters. We consider an algorithm, namely the Iterative Bootstrap\n(IB), to efficiently compute simulation-based estimators by showing its\nconvergence properties. Within this framework we also prove the properties of\nsimulation-based estimators, more specifically the unbiasedness, consistency\nand asymptotic normality when the number of parameters is allowed to increase\nwith the sample size. Therefore, an important implication of the proposed\napproach is that it allows to obtain unbiased estimators in finite samples.\nFinally, we study this approach when applied to three common models, namely\nlogistic regression, negative binomial regression and lasso regression. \n\n"}
{"id": "1810.05374", "contents": "Title: Limitations of \"Limitations of Bayesian leave-one-out cross-validation\n  for model selection\" Abstract: This article is an invited discussion of the article by Gronau and\nWagenmakers (2018) that can be found at\nhttps://dx.doi.org/10.1007/s42113-018-0011-7. \n\n"}
{"id": "1810.05620", "contents": "Title: Computing Elimination Ideals and Discriminants of Likelihood Equations Abstract: We develop a probabilistic algorithm for computing elimination ideals of\nlikelihood equations, which is for larger models by far more efficient than\ndirectly computing Groebner bases or the interpolation method proposed in the\nfirst author's previous work. The efficiency is improved by a theoretical\nresult showing that the sum of data variables appears in most coefficients of\nthe generator polynomial of elimination ideal. Furthermore, applying the known\nstructures of Newton polytopes of discriminants, we can also efficiently deduce\ndiscriminants of the elimination ideals. For instance, the discriminants of 3\nby 3 matrix model and one Jukes-Cantor model in phylogenetics (with sizes over\n30 GB and 8 GB text files, respectively) can be computed by our methods. \n\n"}
{"id": "1810.07128", "contents": "Title: High-dimensional Varying Index Coefficient Models via Stein's Identity Abstract: We study the parameter estimation problem for a varying index coefficient\nmodel in high dimensions. Unlike the most existing works that iteratively\nestimate the parameters and link functions, based on the generalized Stein's\nidentity, we propose computationally efficient estimators for the\nhigh-dimensional parameters without estimating the link functions. We consider\ntwo different setups where we either estimate each sparse parameter vector\nindividually or estimate the parameters simultaneously as a sparse or low-rank\nmatrix. For all these cases, our estimators are shown to achieve optimal\nstatistical rates of convergence (up to logarithmic terms in the low-rank\nsetting). Moreover, throughout our analysis, we only require the covariate to\nsatisfy certain moment conditions, which is significantly weaker than the\nGaussian or elliptically symmetric assumptions that are commonly made in the\nexisting literature. Finally, we conduct extensive numerical experiments to\ncorroborate the theoretical results. \n\n"}
{"id": "1810.08727", "contents": "Title: Condition Number Analysis of Logistic Regression, and its Implications\n  for Standard First-Order Solution Methods Abstract: Logistic regression is one of the most popular methods in binary\nclassification, wherein estimation of model parameters is carried out by\nsolving the maximum likelihood (ML) optimization problem, and the ML estimator\nis defined to be the optimal solution of this problem. It is well known that\nthe ML estimator exists when the data is non-separable, but fails to exist when\nthe data is separable. First-order methods are the algorithms of choice for\nsolving large-scale instances of the logistic regression problem. In this\npaper, we introduce a pair of condition numbers that measure the degree of\nnon-separability or separability of a given dataset in the setting of binary\nclassification, and we study how these condition numbers relate to and inform\nthe properties and the convergence guarantees of first-order methods. When the\ntraining data is non-separable, we show that the degree of non-separability\nnaturally enters the analysis and informs the properties and convergence\nguarantees of two standard first-order methods: steepest descent (for any given\nnorm) and stochastic gradient descent. Expanding on the work of Bach, we also\nshow how the degree of non-separability enters into the analysis of linear\nconvergence of steepest descent (without needing strong convexity), as well as\nthe adaptive convergence of stochastic gradient descent. When the training data\nis separable, first-order methods rather curiously have good empirical success,\nwhich is not well understood in theory. In the case of separable data, we\ndemonstrate how the degree of separability enters into the analysis of $\\ell_2$\nsteepest descent and stochastic gradient descent for delivering\napproximate-maximum-margin solutions with associated computational guarantees\nas well. This suggests that first-order methods can lead to statistically\nmeaningful solutions in the separable case, even though the ML solution does\nnot exist. \n\n"}
{"id": "1810.09899", "contents": "Title: Dynamic Likelihood-free Inference via Ratio Estimation (DIRE) Abstract: Parametric statistical models that are implicitly defined in terms of a\nstochastic data generating process are used in a wide range of scientific\ndisciplines because they enable accurate modeling. However, learning the\nparameters from observed data is generally very difficult because their\nlikelihood function is typically intractable. Likelihood-free Bayesian\ninference methods have been proposed which include the frameworks of\napproximate Bayesian computation (ABC), synthetic likelihood, and its recent\ngeneralization that performs likelihood-free inference by ratio estimation\n(LFIRE). A major difficulty in all these methods is choosing summary statistics\nthat reduce the dimensionality of the data to facilitate inference. While\nseveral methods for choosing summary statistics have been proposed for ABC, the\nliterature for synthetic likelihood and LFIRE is very thin to date. We here\naddress this gap in the literature, focusing on the important special case of\ntime-series models. We show that convolutional neural networks trained to\npredict the input parameters from the data provide suitable summary statistics\nfor LFIRE. On a wide range of time-series models, a single neural network\narchitecture produced equally or more accurate posteriors than alternative\nmethods. \n\n"}
{"id": "1810.09920", "contents": "Title: Clustering Time Series with Nonlinear Dynamics: A Bayesian\n  Non-Parametric and Particle-Based Approach Abstract: We propose a general statistical framework for clustering multiple time\nseries that exhibit nonlinear dynamics into an a-priori-unknown number of\nsub-groups. Our motivation comes from neuroscience, where an important problem\nis to identify, within a large assembly of neurons, subsets that respond\nsimilarly to a stimulus or contingency. Upon modeling the multiple time series\nas the output of a Dirichlet process mixture of nonlinear state-space models,\nwe derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that\nalternates between sampling cluster assignments and sampling parameter values\nthat form the basis of the clustering. The Metropolis step employs recent\ninnovations in particle-based methods. We apply the framework to clustering\ntime series acquired from the prefrontal cortex of mice in an experiment\ndesigned to characterize the neural underpinnings of fear. \n\n"}
{"id": "1810.10285", "contents": "Title: Flow structures govern particle collisions in turbulence Abstract: The role of the spatial structure of a turbulent flow in enhancing particle\ncollision rates in suspensions is an open question. We show and quantify, as a\nfunction of particle inertia, the correlation between the multiscale structures\nof turbulence and particle collisions: Straining zones contribute predominantly\nto rapid head-on collisions compared to vortical regions. We also discover the\nimportance of vortex-strain worm-rolls, which goes beyond ideas of preferential\nconcentration and may explain the rapid growth of aggregates in natural\nprocesses, such as the initiation of rain in warm clouds. \n\n"}
{"id": "1810.11130", "contents": "Title: Robust Importance Sampling with Adaptive Winsorization Abstract: Importance sampling is a widely used technique to estimate properties of a\ndistribution. This paper investigates trading-off some bias for variance by\nadaptively winsorizing the importance sampling estimator. The novel winsorizing\nprocedure, based on the Balancing Principle (or Lepskii's Method), chooses a\nthreshold level among a pre-defined set by roughly balancing the bias and\nvariance of the estimator when winsorized at different levels. As a\nconsequence, it provides a principled way to perform winsorization with\nfinite-sample optimality guarantees under minimal assumptions. In various\nexamples, the proposed estimator is shown to have smaller mean squared error\nand mean absolute deviation than leading alternatives. \n\n"}
{"id": "1810.11332", "contents": "Title: A fast algorithm for computing distance correlation Abstract: Classical dependence measures such as Pearson correlation, Spearman's $\\rho$,\nand Kendall's $\\tau$ can detect only monotonic or linear dependence. To\novercome these limitations, Szekely et al.(2007) proposed distance covariance\nas a weighted $L_2$ distance between the joint characteristic function and the\nproduct of marginal distributions. The distance covariance is $0$ if and only\nif two random vectors ${X}$ and ${Y}$ are independent. This measure has the\npower to detect the presence of a dependence structure when the sample size is\nlarge enough. They further showed that the sample distance covariance can be\ncalculated simply from modified Euclidean distances, which typically requires\n$\\mathcal{O}(n^2)$ cost. The quadratic computing time greatly limits the\napplication of distance covariance to large data. In this paper, we present a\nsimple exact $\\mathcal{O}(n\\log(n))$ algorithm to calculate the sample distance\ncovariance between two univariate random variables. The proposed method\nessentially consists of two sorting steps, so it is easy to implement.\nEmpirical results show that the proposed algorithm is significantly faster than\nstate-of-the-art methods. The algorithm's speed will enable researchers to\nexplore complicated dependence structures in large datasets. \n\n"}
{"id": "1810.12184", "contents": "Title: Multivariate Analysis and Visualization using R Package muvis Abstract: Increased application of multivariate data in many scientific areas has\nconsiderably raised the complexity of analysis and interpretation. Although\nquite a few approaches have been put forward to address this issue, there is\nstill a gap between the most efficient proposed methods and available software.\nmuvis is an R package (core team (2017)) which is a toolkit for analyzing\nmultivariate datasets. Several tools are implemented for common analyses of\nmultivariate datasets, including preprocessing, dimensionality reduction,\nstatistical analysis, Probabilistic Graphical Modeling, hypothesis testing, and\nvisualization. Furthermore, we have implemented two novel\nmethods--Variable-wise Kullback-Leibler Divergence (VKL) and Violating\nVariable-wise Kullback-Leibler Divergence (VVKL)--which are proposed to find\nthe features with most different probability distributions between two specific\ngroups of samples. The main aim of the package is to provide a wide range of\nusers with different levels of expertise in R with a set of tools for\ncomprehensive analysis of multivariate datasets. We exploited the NHANES\ndataset to declare the functionality of muvis in practice. \n\n"}
{"id": "1810.12451", "contents": "Title: Hurricane's maximum potential intensity and surface heat fluxes Abstract: Emanuel's concept of Maximum Potential Intensity (E-PI) relates the maximum\nvelocity $V_{\\rm max}$ of tropical storms, assumed to be in gradient wind\nbalance, to environmental parameters. Several studies suggested that the\nunbalanced flow is responsible for E-PI sometimes significantly underpredicting\n$V_{\\rm max}$. Additionally, two major modifications generated a considerable\nrange of E-PI predictions: the dissipative heating and the power expended to\nlift water were respectively suggested to increase and reduce E-PI $V_{\\rm\nmax}$ by about 20%. Here we re-derive the E-PI concept separating its dynamic\nand thermodynamic assumptions and lifting the gradient wind balance limitation.\nOur analysis reveals that E-PI formulations for a balanced and a radially\nunbalanced flow are similar, while the systematic underestimate of $V_{\\rm\nmax}$ reflects instead an incompatibility between several E-PI assumptions. We\ndiscuss how these assumptions can be modified. We further show that\nirrespective of whether dissipative heating occurs or not, E-PI uniquely\nrelates $V_{\\rm max}$ to the latent heat flux (not to the total oceanic heat\nflux as originally proposed). We clarify that, in contrast to previous\nsuggestions, lifting water has little impact on E-PI. We demonstrate that in\nE-PI the negative work of the pressure gradient in the upper atmosphere\nconsumes all the kinetic energy generated in the boundary layer. This key\ndynamic constraint is independent of other E-PI assumptions and thus can apply\nto diverse circulation patterns. Finally, we show that the E-PI maximum kinetic\nenergy per unit volume equals the local partial pressure of water vapor and\ndiscuss the implications of this finding for predicting $V_{\\rm max}$. \n\n"}
{"id": "1811.00890", "contents": "Title: Probabilistic Programming with Densities in SlicStan: Efficient,\n  Flexible and Deterministic Abstract: Stan is a probabilistic programming language that has been increasingly used\nfor real-world scalable projects. However, to make practical inference\npossible, the language sacrifices some of its usability by adopting a block\nsyntax, which lacks compositionality and flexible user-defined functions.\nMoreover, the semantics of the language has been mainly given in terms of\nintuition about implementation, and has not been formalised.\n  This paper provides a formal treatment of the Stan language, and introduces\nthe probabilistic programming language SlicStan --- a compositional,\nself-optimising version of Stan. Our main contributions are: (1) the\nformalisation of a core subset of Stan through an operational density-based\nsemantics; (2) the design and semantics of the Stan-like language SlicStan,\nwhich facilities better code reuse and abstraction through its compositional\nsyntax, more flexible functions, and information-flow type system; and (3) a\nformal, semantic-preserving procedure for translating SlicStan to Stan. \n\n"}
{"id": "1811.01034", "contents": "Title: Regime Transition in the Energy Cascade of Rotating Turbulence Abstract: Transition from a split to a forward kinetic energy cascade system is\nexplored in the context of rotating turbulence using direct numerical\nsimulations with a three-dimensional isotropic random force uncorrelated with\nthe velocity field. Our parametric study covers confinement effects in large\naspect ratio domains and a broad range of rotation rates. Results indicate that\nfor fixed geometrical dimensions the Rossby number acts as a control parameter,\nwhereas for a fixed Rossby number the product of the domain size along the\nrotation axis and forcing wavenumber governs the amount of energy that cascades\ninversely. The regime transition criterion hence depends on both control\nparameters. \n\n"}
{"id": "1811.02449", "contents": "Title: Predictability problem in dynamical systems and in chaotic climate\n  dynamics Abstract: Predictability horizon properties of chaotic dynamical systems can be related\nto their spectral properties. It is shown, using this relationship, that the\nspectral properties of the leading large-scale climate daily indices indicate a\nsmooth predictability: i.e. their intrinsic predictability horizons can be\nindefinitely extended by reducing the initial error. Special properties of the\nMadden-Julian Oscillation and some models (including ensemble weather\nforecasting) have been also discussed in this context. \n\n"}
{"id": "1811.02963", "contents": "Title: Simulation-based inference methods for partially observed Markov model\n  via the R package is2 Abstract: Partially observed Markov process (POMP) models are powerful tools for time\nseries modeling and analysis. Inherited the flexible framework of R package\npomp, the is2 package extends some useful Monte Carlo statistical methodologies\nto improve on convergence rates. A variety of efficient statistical methods for\nPOMP models have been developed including fixed lag smoothing, second-order\niterated smoothing, momentum iterated filtering, average iterated filtering,\naccelerate iterated filtering and particle iterated filtering. In this paper,\nwe show the utility of these methodologies based on two toy problems. We also\ndemonstrate the potential of some methods in a more complex model, employing a\nnonlinear epidemiological model with a discrete population, seasonality, and\nextra-demographic stochasticity. We discuss the extension beyond POMP models\nand the development of additional methods within the framework provided by is2. \n\n"}
{"id": "1811.04249", "contents": "Title: Bayesian variational inference for exponential random graph models Abstract: Deriving Bayesian inference for exponential random graph models (ERGMs) is a\nchallenging \"doubly intractable\" problem as the normalizing constants of the\nlikelihood and posterior density are both intractable. Markov chain Monte Carlo\n(MCMC) methods which yield Bayesian inference for ERGMs, such as the exchange\nalgorithm, are asymptotically exact but computationally intensive, as a network\nhas to be drawn from the likelihood at every step using, for instance, a \"tie\nno tie\" sampler. In this article, we develop a variety of variational methods\nfor Gaussian approximation of the posterior density and model selection. These\ninclude nonconjugate variational message passing based on an adjusted\npseudolikelihood and stochastic variational inference. To overcome the\ncomputational hurdle of drawing a network from the likelihood at each\niteration, we propose stochastic gradient ascent with biased but consistent\ngradient estimates computed using adaptive self-normalized importance sampling.\nThese methods provide attractive fast alternatives to MCMC for posterior\napproximation. We illustrate the variational methods using real networks and\ncompare their accuracy with results obtained via MCMC and Laplace\napproximation. \n\n"}
{"id": "1811.04817", "contents": "Title: A test case for application of convolutional neural networks to\n  spatio-temporal climate data: Re-identifying clustered weather patterns Abstract: Convolutional neural networks (CNNs) can potentially provide powerful tools\nfor classifying and identifying patterns in climate and environmental data.\nHowever, because of the inherent complexities of such data, which are often\nspatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\ndesigned/evaluated for each specific dataset and application. Yet to start,\nCNN, a supervised technique, requires a large labeled dataset. Labeling demands\n(human) expert time, which combined with the limited number of relevant\nexamples in this area, can discourage using CNNs for new problems. To address\nthese challenges, here we (1) Propose an effective auto-labeling strategy based\non using an unsupervised clustering algorithm and evaluating the performance of\nCNNs in re-identifying these clusters; (2) Use this approach to label thousands\nof daily large-scale weather patterns over North America in the outputs of a\nfully-coupled climate model and show the capabilities of CNNs in re-identifying\nthe 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\ncluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\nnonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\ntraining samples per cluster. Effects of architecture and hyperparameters on\nthe performance of CNNs are examined and discussed. \n\n"}
{"id": "1811.06289", "contents": "Title: On a new class of score functions to estimate tail probabilities of some\n  stochastic processes with Adaptive Multilevel Splitting Abstract: We investigate the application of the Adaptive Multilevel Splitting algorithm\nfor the estimation of tail probabilities of solutions of Stochastic\nDifferential Equations evaluated at a given time, and of associated temporal\naverages.\n  We introduce a new, very general and effective family of score functions\nwhich is designed for these problems. We illustrate its behavior on a series of\nnumerical experiments. In particular, we demonstrate how it can be used to\nestimate large deviation rate functionals for the longtime limit of temporal\naverages. \n\n"}
{"id": "1811.06601", "contents": "Title: Estimating the Mean and Variance of a High-dimensional Normal\n  Distribution Using a Mixture Prior Abstract: This paper provides a framework for estimating the mean and variance of a\nhigh-dimensional normal density. The main setting considered is a fixed number\nof vector following a high-dimensional normal distribution with unknown mean\nand diagonal covariance matrix. The diagonal covariance matrix can be known or\nunknown. If the covariance matrix is unknown, the sample size can be as small\nas $2$. The proposed estimator is based on the idea that the unobserved pairs\nof mean and variance for each dimension are drawn from an unknown bivariate\ndistribution, which we model as a mixture of normal-inverse gammas. The mixture\nof normal-inverse gamma distributions provides advantages over more traditional\nempirical Bayes methods, which are based on a normal-normal model. When fitting\na mixture model, we are essentially clustering the unobserved mean and variance\npairs for each dimension into different groups, with each group having a\ndifferent normal-inverse gamma distribution. The proposed estimator of each\nmean is the posterior mean of shrinkage estimates, each of which shrinks a\nsample mean towards a different component of the mixture distribution.\nSimilarly, the proposed estimator of variance has an analogous interpretation\nin terms of sample variances and components of the mixture distribution. If\ndiagonal covariance matrix is known, then the sample size can be as small as\n$1$, and we treat the pairs of known variance and unknown mean for each\ndimension as random observations coming from a flexible mixture of\nnormal-inverse gamma distributions. \n\n"}
{"id": "1811.09469", "contents": "Title: Parallel sequential Monte Carlo for stochastic gradient-free nonconvex\n  optimization Abstract: We introduce and analyze a parallel sequential Monte Carlo methodology for\nthe numerical solution of optimization problems that involve the minimization\nof a cost function that consists of the sum of many individual components. The\nproposed scheme is a stochastic zeroth order optimization algorithm which\ndemands only the capability to evaluate small subsets of components of the cost\nfunction. It can be depicted as a bank of samplers that generate particle\napproximations of several sequences of probability measures. These measures are\nconstructed in such a way that they have associated probability density\nfunctions whose global maxima coincide with the global minima of the original\ncost function. The algorithm selects the best performing sampler and uses it to\napproximate a global minimum of the cost function. We prove analytically that\nthe resulting estimator converges to a global minimum of the cost function\nalmost surely and provide explicit convergence rates in terms of the number of\ngenerated Monte Carlo samples and the dimension of the search space. We show,\nby way of numerical examples, that the algorithm can tackle cost functions with\nmultiple minima or with broad \"flat\" regions which are hard to minimize using\ngradient-based techniques. \n\n"}
{"id": "1811.09747", "contents": "Title: Amortized Bayesian inference for clustering models Abstract: We develop methods for efficient amortized approximate Bayesian inference\nover posterior distributions of probabilistic clustering models, such as\nDirichlet process mixture models. The approach is based on mapping distributed,\nsymmetry-invariant representations of cluster arrangements into conditional\nprobabilities. The method parallelizes easily, yields iid samples from the\napproximate posterior of cluster assignments with the same computational cost\nof a single Gibbs sampler sweep, and can easily be applied to both conjugate\nand non-conjugate models, as training only requires samples from the generative\nmodel. \n\n"}
{"id": "1811.11733", "contents": "Title: orthoDr: Semiparametric Dimension Reduction via Orthogonality\n  Constrained Optimization Abstract: orthoDr is a package in R that solves dimension reduction problems using\northogonality constrained optimization approach. The package serves as a\nunified framework for many regression and survival analysis dimension reduction\nmodels that utilize semiparametric estimating equations. The main computational\nmachinery of orthoDr is a first-order algorithm developed by\n\\cite{wen2013feasible} for optimization within the Stiefel manifold. We\nimplement the algorithm through Rcpp and OpenMP for fast computation. In\naddition, we developed a general-purpose solver for such constrained problems\nwith user-specified objective functions, which works as a drop-in version of\noptim(). The package also serves as a platform for future methodology\ndevelopments along this line of work. \n\n"}
{"id": "1812.00375", "contents": "Title: Ensemble-based implicit sampling for Bayesian inverse problems with\n  non-Gaussian priors Abstract: In the paper, we develop an ensemble-based implicit sampling method for\nBayesian inverse problems. For Bayesian inference, the iterative ensemble\nsmoother (IES) and implicit sampling are integrated to obtain importance\nensemble samples, which build an importance density. The proposed method shares\na similar idea to importance sampling. IES is used to approximate mean and\ncovariance of a posterior distribution. This provides the MAP point and the\ninverse of Hessian matrix, which are necessary to construct the implicit map in\nimplicit sampling. The importance samples are generated by the implicit map and\nthe corresponding weights are the ratio between the importance density and\nposterior density. In the proposed method, we use the ensemble samples of IES\nto find the optimization solution of likelihood function and the inverse of\nHessian matrix. This approach avoids the explicit computation for Jacobian\nmatrix and Hessian matrix, which are very computationally expensive in high\ndimension spaces. To treat non-Gaussian models, discrete cosine transform and\nGaussian mixture model are used to characterize the non-Gaussian priors. The\nensemble-based implicit sampling method is extended to the non-Gaussian priors\nfor exploring the posterior of unknowns in inverse problems. The proposed\nmethod is used for each individual Gaussian model in the Gaussian mixture\nmodel. The proposed approach substantially improves the applicability of\nimplicit sampling method. A few numerical examples are presented to demonstrate\nthe efficacy of the proposed method with applications of inverse problems for\nsubsurface flow problems and anomalous diffusion models in porous media. \n\n"}
{"id": "1812.01380", "contents": "Title: The Lagrange approach in the monotone single index model Abstract: The finite-dimensional parameters of the monotone single index model are\noften estimated by minimization of a least squares criterion and\nreparametrization to deal with the non-unicity. We avoid the reparametrization\nby using a Lagrange-type method and replace the minimization over the\nfinite-dimensional parameter alpha by a `crossing of zero' criterion at the\nderivative level. In particular, we consider a simple score estimator (SSE), an\nefficient score estimator (ESE), and a penalized least squares estimator (PLSE)\nfor which we can apply this method. The SSE and ESE were discussed in\nBalabdaoui, Groeneboom and Hendrickx (2018}, but the proofs still used\nreparametrization. Another version of the PLSE was discussed in Kuchibhotla and\nPatra (2017), where also reparametrization was used. The estimators are\ncompared with the profile least squares estimator (LSE), Han's maximum rank\nestimator (MRE), the effective dimension reduction estimator (EDR) and a linear\nleast squares estimator, which can be used if the covariates have an\nelliptically symmetric distribution. We also investigate the effects of random\nstarting values in the search algorithms. \n\n"}
{"id": "1812.01502", "contents": "Title: Parallelising Particle Filters with Butterfly Interactions Abstract: Bootstrap particle filter (BPF) is the corner stone of many popular\nalgorithms used for solving inference problems involving time series that are\nobserved through noisy measurements in a non-linear and non-Gaussian context.\nThe long term stability of BPF arises from particle interactions which in the\ncontext of modern parallel computing systems typically means that particle\ninformation needs to be communicated between processing elements, which makes\nparallel implementation of BPF nontrivial.\n  In this paper we show that it is possible to constrain the interactions in a\nway which, under some assumptions, enables the reduction of the cost of\ncommunicating the particle information while still preserving the consistency\nand the long term stability of the BPF. Numerical experiments demonstrate that\nalthough the imposed constraints introduce additional error, the proposed\nmethod shows potential to be the method of choice in certain settings. \n\n"}
{"id": "1812.01943", "contents": "Title: Prediction of typhoon tracks using a generative adversarial network with\n  observational and meteorological data Abstract: Tracks of typhoons are predicted using a generative adversarial network (GAN)\nwith observational data in form of satellite images and meteorological data\nfrom a reanalysis database. Time series of images of typhoons which occurred in\nthe Korean Peninsula in the past are used to train the neural network. The\ntrained GAN is employed to produce a 6-hour-advance track of a typhoon for\nwhich the GAN was not trained. The predicted image favorably identifies the\nfuture location of the typhoon center as well as the deformed cloud structures.\nThe errors between predicted and real typhoon centers are measured\nquantitatively in kilometers. 65.5 % of all typhoon center predictions have an\nerror of less than 80 km, 31.5 % lie within a range of 80 - 120 km and the\nremaining 3.0 % are above 120 km. The overall error is 67.2 km, compared to\n95.6 km when only observational data are used as input. The cloud structure\nprediction is evaluated qualitatively. It is shown that the GAN is able to\npredict trends in cloud motion. It is found that adding physically meaningful\nmeteorological data to satellite images improves the sharpness of predicted\nimages. \n\n"}
{"id": "1812.05488", "contents": "Title: A bulk-interface correspondence for equatorial waves Abstract: Topology is bringing new tools for the study of fluid waves. The existence of\nunidirectional Yanai and Kelvin equatorial waves has been related to a\ntopological invariant, the Chern number, that describes the winding of\n$f$-plane shallow water eigenmodes around band crossing points in parameter\nspace. In this previous study, the topological invariant was a property of the\ninterface between two hemispheres. Here we ask whether a topological index can\nbe assigned to each hemisphere. We show that this can be done if the shallow\nwater model in $f$-plane geometry is regularized by an additional odd-viscous\nterm. We then compute the spectrum of a shallow water model with a sharp\nequator separating two flat hemispheres, and recover the Kelvin and Yanai waves\nas two exponentially trapped waves along the equator, with all the other modes\ndelocalized into the bulk. This model provides an exactly solvable example of\nbulk-interface correspondence in a flow with a sharp interface, and offers a\ntopological interpretation for some of the transition modes described by [Iga,\nJournal of Fluid Mechanics 1995]. It also paves the way towards a topological\ninterpretation of coastal Kelvin waves along a boundary, and more generally, to\nan understanding of bulk-boundary correspondence in continuous media. \n\n"}
{"id": "1812.07929", "contents": "Title: Importance Sampling-based Transport Map Hamiltonian Monte Carlo for\n  Bayesian Hierarchical Models Abstract: We propose an importance sampling (IS)-based transport map Hamiltonian Monte\nCarlo procedure for performing full Bayesian analysis in general nonlinear\nhigh-dimensional hierarchical models. Using IS techniques to construct a\ntransport map, the proposed method transforms the typically highly challenging\ntarget distribution of a hierarchical model into a target which is easily\nsampled using standard Hamiltonian Monte Carlo. Conventional applications of\nhigh-dimensional IS, where infinite variance of IS weights can be a serious\nproblem, require computationally costly high-fidelity IS distributions. An\nappealing property of our method is that the IS distributions employed can be\nof rather low fidelity, making it computationally cheap. We illustrate our\nalgorithm in applications to challenging dynamic state-space models, where it\nexhibits very high simulation efficiency compared to relevant benchmarks, even\nfor variants of the proposed method implemented using a few dozen lines of code\nin the Stan statistical software. \n\n"}
{"id": "1812.09063", "contents": "Title: Efficient Calculation of the Joint Distribution of Order Statistics Abstract: We consider the problem of computing the joint distribution of order\nstatistics of stochastically independent random variables in one- and two-group\nmodels. While recursive formulas for evaluating the joint cumulative\ndistribution function of such order statistics exist in the literature for a\nlonger time, their numerical implementation remains a challenging task. We\ntackle this task by presenting novel generalizations of known recursions which\nwe utilize to obtain exact results (calculated in rational arithmetic) as well\nas faithfully rounded results. Finally, some applications in stepwise multiple\nhypothesis testing are discussed. \n\n"}
{"id": "1812.11689", "contents": "Title: K-nearest Neighbor Search by Random Projection Forests Abstract: K-nearest neighbor (kNN) search has wide applications in many areas,\nincluding data mining, machine learning, statistics and many applied domains.\nInspired by the success of ensemble methods and the flexibility of tree-based\nmethodology, we propose random projection forests (rpForests), for kNN search.\nrpForests finds kNNs by aggregating results from an ensemble of random\nprojection trees with each constructed recursively through a series of\ncarefully chosen random projections. rpForests achieves a remarkable accuracy\nin terms of fast decay in the missing rate of kNNs and that of discrepancy in\nthe kNN distances. rpForests has a very low computational complexity. The\nensemble nature of rpForests makes it easily run in parallel on multicore or\nclustered computers; the running time is expected to be nearly inversely\nproportional to the number of cores or machines. We give theoretical insights\nby showing the exponential decay of the probability that neighboring points\nwould be separated by ensemble random projection trees when the ensemble size\nincreases. Our theory can be used to refine the choice of random projections in\nthe growth of trees, and experiments show that the effect is remarkable. \n\n"}
{"id": "1901.01477", "contents": "Title: Dynamic Visualization and Fast Computation for Convex Clustering via\n  Algorithmic Regularization Abstract: Convex clustering is a promising new approach to the classical problem of\nclustering, combining strong performance in empirical studies with rigorous\ntheoretical foundations. Despite these advantages, convex clustering has not\nbeen widely adopted, due to its computationally intensive nature and its lack\nof compelling visualizations. To address these impediments, we introduce\nAlgorithmic Regularization, an innovative technique for obtaining high-quality\nestimates of regularization paths using an iterative one-step approximation\nscheme. We justify our approach with a novel theoretical result, guaranteeing\nglobal convergence of the approximate path to the exact solution under\neasily-checked non-data-dependent assumptions. The application of algorithmic\nregularization to convex clustering yields the Convex Clustering via\nAlgorithmic Regularization Paths (CARP) algorithm for computing the clustering\nsolution path. On example data sets from genomics and text analysis, CARP\ndelivers over a 100-fold speed-up over existing methods, while attaining a\nfiner approximation grid than standard methods. Furthermore, CARP enables\nimproved visualization of clustering solutions: the fine solution grid returned\nby CARP can be used to construct a convex clustering-based dendrogram, as well\nas forming the basis of a dynamic path-wise visualization based on modern web\ntechnologies. Our methods are implemented in the open-source R package\nclustRviz, available at https://github.com/DataSlingers/clustRviz. \n\n"}
{"id": "1901.02976", "contents": "Title: The square root rule for adaptive importance sampling Abstract: In adaptive importance sampling, and other contexts, we have $K>1$ unbiased\nand uncorrelated estimates $\\hat\\mu_k$ of a common quantity $\\mu$. The optimal\nunbiased linear combination weights them inversely to their variances but those\nweights are unknown and hard to estimate. A simple deterministic square root\nrule based on a working model that $\\mathrm{Var}(\\hat\\mu_k)\\propto k^{-1/2}$\ngives an unbisaed estimate of $\\mu$ that is nearly optimal under a wide range\nof alternative variance patterns. We show that if\n$\\mathrm{Var}(\\hat\\mu_k)\\propto k^{-y}$ for an unknown rate parameter $y\\in\n[0,1]$ then the square root rule yields the optimal variance rate with a\nconstant that is too large by at most $9/8$ for any $0\\le y\\le 1$ and any\nnumber $K$ of estimates. Numerical work shows that rule is similarly robust to\nsome other patterns with mildly decreasing variance as $k$ increases. \n\n"}
{"id": "1901.03958", "contents": "Title: On the Convergence of the Laplace Approximation and\n  Noise-Level-Robustness of Laplace-based Monte Carlo Methods for Bayesian\n  Inverse Problems Abstract: The Bayesian approach to inverse problems provides a rigorous framework for\nthe incorporation and quantification of uncertainties in measurements,\nparameters and models. We are interested in designing numerical methods which\nare robust w.r.t. the size of the observational noise, i.e., methods which\nbehave well in case of concentrated posterior measures. The concentration of\nthe posterior is a highly desirable situation in practice, since it relates to\ninformative or large data. However, it can pose a computational challenge for\nnumerical methods based on the prior or reference measure. We propose to employ\nthe Laplace approximation of the posterior as the base measure for numerical\nintegration in this context. The Laplace approximation is a Gaussian measure\ncentered at the maximum a-posteriori estimate and with covariance matrix\ndepending on the logposterior density. We discuss convergence results of the\nLaplace approximation in terms of the Hellinger distance and analyze the\nefficiency of Monte Carlo methods based on it. In particular, we show that\nLaplace-based importance sampling and Laplace-based quasi-Monte-Carlo methods\nare robust w.r.t. the concentration of the posterior for large classes of\nposterior distributions and integrands whereas prior-based importance sampling\nand plain quasi-Monte Carlo are not. Numerical experiments are presented to\nillustrate the theoretical findings. \n\n"}
{"id": "1901.04816", "contents": "Title: Learning Direct and Inverse Transmission Matrices Abstract: Linear problems appear in a variety of disciplines and their application for\nthe transmission matrix recovery is one of the most stimulating challenges in\nbiomedical imaging. Its knowledge turns any random media into an optical tool\nthat can focus or transmit an image through disorder. Here, converting an\ninput-output problem into a statistical mechanical formulation, we investigate\nhow inference protocols can learn the transmission couplings by\npseudolikelihood maximization. Bridging linear regression and thermodynamics\nlet us propose an innovative framework to pursue the solution of the\nscattering-riddle. \n\n"}
{"id": "1901.08115", "contents": "Title: A weighted Discrepancy Bound of quasi-Monte Carlo Importance Sampling Abstract: Importance sampling Monte-Carlo methods are widely used for the approximation\nof expectations with respect to partially known probability measures. In this\npaper we study a deterministic version of such an estimator based on\nquasi-Monte Carlo. We obtain an explicit error bound in terms of the\nstar-discrepancy for this method. \n\n"}
{"id": "1901.08606", "contents": "Title: Fast Markov Chain Monte Carlo Algorithms via Lie Groups Abstract: From basic considerations of the Lie group that preserves a target\nprobability measure, we derive the Barker, Metropolis, and ensemble Markov\nchain Monte Carlo (MCMC) algorithms, as well as variants of waste-recycling\nMetropolis-Hastings and an altogether new MCMC algorithm. We illustrate these\nconstructions with explicit numerical computations, and we empirically\ndemonstrate on a spin glass that the new algorithm converges more quickly than\nits siblings. \n\n"}
{"id": "1901.10543", "contents": "Title: A High-Dimensional Particle Filter Algorithm Abstract: Online data assimilation in time series models over a large spatial extent is\nan important problem in both geosciences and robotics. Such models are\nintrinsically high-dimensional, rendering traditional particle filter\nalgorithms ineffective. Though methods that begin to address this problem\nexist, they either rely on additional assumptions or lead to error that is\nspatially inhomogeneous. I present a novel particle-based algorithm for online\napproximation of the filtering problem on such models, using the fact that each\nlocus affects only nearby loci at the next time step. The algorithm is based on\na Metropolis-Hastings-like MCMC for creating hybrid particles at each step. I\nshow simulation results that suggest the error of this algorithm is uniform in\nboth space and time, with a lower bias, though higher variance, as compared to\na previously-proposed algorithm. \n\n"}
{"id": "1901.10568", "contents": "Title: Stochastic Gradient MCMC for Nonlinear State Space Models Abstract: State space models (SSMs) provide a flexible framework for modeling complex\ntime series via a latent stochastic process. Inference for nonlinear,\nnon-Gaussian SSMs is often tackled with particle methods that do not scale well\nto long time series. The challenge is two-fold: not only do computations scale\nlinearly with time, as in the linear case, but particle filters additionally\nsuffer from increasing particle degeneracy with longer series. Stochastic\ngradient MCMC methods have been developed to scale Bayesian inference for\nfinite-state hidden Markov models and linear SSMs using buffered stochastic\ngradient estimates to account for temporal dependencies. We extend these\nstochastic gradient estimators to nonlinear SSMs using particle methods. We\npresent error bounds that account for both buffering error and particle error\nin the case of nonlinear SSMs that are log-concave in the latent process. We\nevaluate our proposed particle buffered stochastic gradient using stochastic\ngradient MCMC for inference on both long sequential synthetic and\nminute-resolution financial returns data, demonstrating the importance of this\nclass of methods. \n\n"}
{"id": "1901.11426", "contents": "Title: The Effects of Gravity on the Climate and Circulation of a Terrestrial\n  Planet Abstract: The climate and circulation of a terrestrial planet are governed by, among\nother things, the distance to its host star, its size, rotation rate,\nobliquity, atmospheric composition and gravity. Here we explore the effects of\nthe last of these, the Newtonian gravitational acceleration, on its atmosphere\nand climate. We first demonstrate that if the atmosphere obeys the hydrostatic\nprimitive equations, which are a very good approximation for most terrestrial\natmospheres, and if the radiative forcing is unaltered, changes in gravity have\nno effect at all on the circulation except for a vertical rescaling. That is to\nsay, the effects of gravity may be completely scaled away and the circulation\nis unaltered. However, if the atmosphere contains a dilute condensible that is\nradiatively active, such as water or methane, then an increase in gravity will\ngenerally lead to a cooling of the planet because the total path length of the\ncondensible will be reduced as gravity increases, leading to a reduction in the\ngreenhouse effect. Furthermore, the specific humidity will decrease, leading to\nchanges in the moist adiabatic lapse rate, in the equator-to-pole heat\ntransport, and in the surface energy balance because of changes in the sensible\nand latent fluxes. These effects are all demonstrated both by theoretical\narguments and by numerical simulations with moist and dry general circulation\nmodels. \n\n"}
{"id": "astro-ph/0201018", "contents": "Title: Evidence for Nearby Supernova Explosions Abstract: Supernova explosions are one of the most energetic--and potentially\nlethal--phenomena in the Universe. Scientists have speculated for decades about\nthe possible consequences for life on Earth of a nearby supernova, but\nplausible candidates for such an event were lacking. Here we show that the\nScorpius-Centaurus OB association, a group of young stars currently located\nat~130 parsecs from the Sun, has generated 20 SN explosions during the last 11\nMyr, some of them probably as close as 40 pc to our planet. We find that the\ndeposition on Earth of 60Fe atoms produced by these explosions can explain the\nrecent measurements of an excess of this isotope in deep ocean crust samples.\nWe propose that ~2 Myr ago, one of the SNe exploded close enough to Earth to\nseriously damage the ozone layer, provoking or contributing to the\nPliocene-Pleistocene boundary marine extinction. \n\n"}
{"id": "astro-ph/0307485", "contents": "Title: The Rise of High Energy Neutrino Astronomy at Horizon Abstract: High Energy Neutrino Astronomy energy windows may be ruled by a new Upward\nand Horizontal Tau Air-Showers (UpTaus, HorTaus) detectors (scintillators and\noptical arrays) located a few kilometers beyond facing High Mountains Chains,\nor located above their top or flying on Planes, on Balloons and on Satellites\nfacing downward to the Earth Crust Horizon. While looking downward to the Earth\nseeking for Upward and Horizontal Tau Air-Showers the same optical detectors\nmay often capture cosmic rays Cherenkov lights reflected or diffused by\ndownward Shower cosmic rays hitting the Earth soil (a sea,a lake, ice lands,\ndesert or ground). These new detectors may be built up as a circular hybrid\ncrown array at high quota facing to horizon in correlation with present largest\ntelescopes, like Magic or Shalon or ASHRA ones, if able to look below the Earth\nedge. The UpTaus and HorTau Astronomy at 10^15 eV up to 10^19 eV energy windows\nmay test the largest primary neutrino fluxes in Z-Burst neutrino model needed\nto solve the GZK puzzle or even the smaller but inevitable GZK neutrino flux\nsecondary of the same GZK cut-off. These HorTus signals might well be observed\nin future EUSO experiment. \n\n"}
{"id": "astro-ph/0411284", "contents": "Title: Terrestrial Ozone Depletion Due to a Milky Way Gamma-Ray Burst Abstract: Based on cosmological rates, it is probable that at least once in the last Gy\nthe Earth has been irradiated by a gamma-ray burst in our Galaxy from within 2\nkpc. Using a two-dimensional atmospheric model we have performed the first\ncomputation of the effects upon the Earth's atmosphere of one such impulsive\nevent. A ten second burst delivering 100 kJ/m^2 to the Earth penetrates to the\nstratosphere and results in globally averaged ozone depletion of 35%, with\ndepletion reaching 55% at some latitudes. Significant global depletion persists\nfor over 5 years after the burst. This depletion would have dramatic\nimplications for life since a 50% decrease in ozone column density results in\napproximately three times the normal UVB flux. Widespread extinctions are\nlikely, based on extrapolation from UVB sensitivity of modern organisms.\nAdditional effects include a shot of nitrate fertilizer and NO2 opacity in the\nvisible providing a cooling perturbation to the climate over a similar\ntimescale. These results lend support to the hypothesis that a GRB may have\ninitiated the late Ordovician mass extinction (Melott et al. 2004). \n\n"}
{"id": "astro-ph/0505472", "contents": "Title: Gamma-Ray Bursts and the Earth: Exploration of Atmospheric, Biological,\n  Climatic and Biogeochemical Effects Abstract: Gamma-Ray Bursts (GRBs) are likely to have made a number of significant\nimpacts on the Earth during the last billion years. We have used a\ntwo-dimensional atmospheric model to investigate the effects on the Earth's\natmosphere of GRBs delivering a range of fluences, at various latitudes, at the\nequinoxes and solstices, and at different times of day. We have estimated DNA\ndamage levels caused by increased solar UVB radiation, reduction in solar\nvisible light due to $\\mathrm{NO_2}$ opacity, and deposition of nitrates\nthrough rainout of $\\mathrm{HNO_3}$. For the ``typical'' nearest burst in the\nlast billion years, we find globally averaged ozone depletion up to 38%.\nLocalized depletion reaches as much as 74%. Significant global depletion (at\nleast 10%) persists up to about 7 years after the burst. Our results depend\nstrongly on time of year and latitude over which the burst occurs. We find DNA\ndamage of up to 16 times the normal annual global average, well above lethal\nlevels for simple life forms such as phytoplankton. The greatest damage occurs\nat low to mid latitudes. We find reductions in visible sunlight of a few\npercent, primarily in the polar regions. Nitrate deposition similar to or\nslightly greater than that currently caused by lightning is also observed,\nlasting several years. We discuss how these results support the hypothesis that\nthe Late Ordovician mass extinction may have been initiated by a GRB. \n\n"}
{"id": "astro-ph/0602092", "contents": "Title: Do extragalactic cosmic rays induce cycles in fossil diversity? Abstract: Recent work has revealed a 62 (+/-) 3-million-year cycle in the fossil\ndiversity in the past 542 My, however no plausible mechanism has been found. We\npropose that the cycle may be caused by modulation of cosmic ray (CR) flux by\nthe Solar system vertical oscillation (64 My period) in the galaxy, the\ngalactic north-south anisotropy of CR production in the galactic\nhalo/wind/termination shock (due to the galactic motion toward the Virgo\ncluster), and the shielding by galactic magnetic fields. We revisit the\nmechanism of CR propagation and show that CR flux can vary by a factor of about\n4.6 and reach a maximum at north-most displacement of the Sun. The very high\nstatistical significance of (i) the phase agreement between Solar north-ward\nexcursions and the diversity minima and (ii) the correlation of the magnitude\nof diversity drops with CR amplitudes through all cycles provide solid support\nfor our model. Various observational predictions which can be used to confirm\nor falsify our hypothesis are presented. \n\n"}
{"id": "astro-ph/0606226", "contents": "Title: Anti-Neutrino Imprint in Solar Neutrino Flare Abstract: Future neutrino detector at Megaton mass might enlarge the neutrino telescope\nthresholds revealing cosmic supernova background and largest solar flares\nneutrino. Indeed the solar energetic flare particles while scattering among\nthemselves on Solar corona atmosphere must produce prompt charged pions, whose\nchain decays are source of solar (electron-muon) neutrino \"flare\" (at tens or\nhundreds MeV energy). These brief (minutes) neutrino \"burst\" at largest flare\npeak may overcome by three to five order of magnitude the steady atmospheric\nneutrino noise on the Earth, possibly leading to their detection above\ndetection. Moreover the birth of anti-neutrinos at a few tens MeVs is well\nloudly flaring above a null thermal \"hep\" anti-neutrino solar background and\nalso above a tiny supernova relic and atmospheric noise. The largest prompt\nsolar anti-neutrino \"burst\" may be well detected in future SuperKamikande\n(Gadolinium implemented) by anti-neutrino signatures mostly in inverse Beta\ndecay. Our estimate for the recent and exceptional October - November 2003\nsolar flares and January 20th 2005 exceptional eruption might lead to a few\nevents above or near unity for existing Super-Kamiokande and above unity for\nMegaton detectors. The neutrino spectra may reflect in a subtle way the\nneutrino flavor oscillations and mixing in flight. A comparison of the solar\nneutrino flare (at their birth place on Sun and after oscillation on the\narrival on the Earth) with other neutrino foreground is estimated: it offers an\nindependent track to disentangle the neutrino flavor puzzles and its most\nsecret mixing angles. The sharpest noise-free anti-neutrino imprint maybe its\nfirst clean voice. \n\n"}
{"id": "astro-ph/0610725", "contents": "Title: UHE Cosmic Rays and Neutrinos Showering on Planet Edges Abstract: Ultra High Energy (UHE) Cosmic Rays, UHECR, may graze high altitude\natmosphere leading to horizontal upward air-showers. Also PeVs electron\nantineutrino hitting electron in atmosphere may air-shower at W boson resonant\nmass. On the other side ultra high energy muon and electron neutrinos may also\nlead, by UHE neutrinos mass state mixing, to the rise of a corresponding UHE\nTau neutrino flavor; the consequent UHE tau neutrinos, via charge current\ninteractions in matter, may create UHE taus at horizons (Earth skimming\nneutrinos or Hor-taus) whose escape in atmosphere and whose consequent decay in\nflight, may be later amplified by upward showering on terrestrial, planetary\natmospheres. Indeed because of the finite terrestrial radius, its thin\natmosphere size its dense crust, the UHE tau cannot extend much more than 360\nkilometers in air, corresponding to an energy of about 7.2 EeV, near but below\nGZK cut-off ones; on the contrary Jupiter (or even Saturn) may offer a wider,\nless dense and thicker gaseous layer at the horizons where Tau may loose little\nenergy, travel longer before decay and rise and shower at 4-6 10^{19} eV or ZeV\nextreme energy. Titan atmosphere may open a rare window of opportunity for\nUp-ward Taus at PeVs. Also solar atmosphere may play a role, but unfortunately\ntau-showers secondaries maybe are too noisy to be disentangled, while Jupiter\natmosphere, or better, Saturn one, may offer a clearer imprint for GZK (and\nhigher Z-Burst) Tau showering, well below the horizons edges. \n\n"}
{"id": "astro-ph/0610954", "contents": "Title: Updated Z-Burst Neutrinos at Horizons Abstract: Recent homogeneous and isotropic maps of UHECR, suggest an isotropic cosmic\norigin almost uncorrelated to nearby Local Universe prescribed by GZK (tens\nMpc) cut-off. Z-Burst model based on UHE neutrino resonant scattering on light\nrelic ones in nearby Hot neutrino Dark Halo, may overcome the absence of such a\nlocal imprint and explain the recent correlation with BL Lac at distances of a\nfew hundred Mpc. Z-Burst multiple imprint, due to very possible lightest\nnon-degenerated neutrino masses, may inject energy and modulate UHECR ZeV edge\nspectra. The Z-burst (and GZK) ultra high energy neutrinos (ZeV and EeV band)\nmay also shine, by UHE neutrinos mass state mixing, and rise in corresponding\nUHE Tau neutrino flavor, whose charged current tau production and its decay in\nflight, maybe the source of UHE showering on Earth. The Radius and the\natmosphere size of our planet constrains the tau maximal distance and energy to\nmake a shower. These terrestrial tau energies are near GZK energy limit. Higher\ndistances and energies are available in bigger planets; eventual solar\natmosphere horizons may amplify the UHE tau flight allowing tau showering at\nZeV energies offering a novel way to reveal the expected Z-Burst extreme\nneutrino fluxes. \n\n"}
{"id": "astro-ph/0612745", "contents": "Title: Splitting neutrino masses and showering into Sky Abstract: Neutrino masses might be as light as a few time the atmospheric neutrino mass\nsplitting. High Energy ZeV cosmic neutrinos (in Z-Showering model) might hit\nrelic ones at each mass in different resonance energies in our nearby Universe.\nThis non-degenerated density and energy must split UHE Z-boson secondaries (in\nZ-Burst model) leading to multi injection of UHECR nucleons within future\nextreme AUGER energy. Secondaries of Z-Burst as neutral gamma, below a few tens\nEeV are better surviving local GZK cut-off and they might explain recent Hires\nBL-Lac UHECR correlations at small angles. A different high energy resonance\nmust lead to Glashow's anti-neutrino showers while hitting electrons in matter.\nIn air, Glashow's anti-neutrino showers lead to collimated and directional\nair-showers offering a new Neutrino Astronomy. At greater energy around PeV,\nTau escaping mountains and Earth and decaying in flight are effectively\nshowering in air sky. These Horizontal showering is splitting by geomagnetic\nfield in forked shapes. Such air-showers secondaries release amplified and\nbeamed gamma bursts (like observed TGF), made also by muon and electron pair\nbundles, with their accompanying rich Cherenkov flashes. Also planet' s largest\n(Saturn, Jupiter) atmosphere limbs offer an ideal screen for UHE GZK and\nZ-burst tau neutrino, because their largest sizes. Titan thick atmosphere and\nsmall radius are optimal for discovering up-going resonant Glashow resonant\nshowers. Earth detection of Neutrino showering by twin Magic Telescopes on top\nmountains, or by balloons and satellites arrays facing the limbs are the\nsimplest and cheapest way toward UHE Neutrino Astronomy . \n\n"}
{"id": "astro-ph/9805317", "contents": "Title: Anoxia duirng the Late Permian Binary Mass Extinction and Dark Matter Abstract: Recent evidence quite convincingly indicates that the Late Permian biotic\ncrisis was in fact a binary extinction with a distinct end-Guadalupian\nextinction pulse preceding the major terminal end-Permian Tartarian event by 5\nmillion years. In addition anoxia appears to be closely associated with each of\nthese end-Paleozoic binary extinctions. Most leading models cannot explain both\nanoxia and the binary characteristic of this crisis. In this paper we show that\nthe recently proposed volcanogenic dark matter scenario succeeds in doing this. \n\n"}
{"id": "chao-dyn/9802003", "contents": "Title: Noise rectification in quasigeostrophic forced turbulence Abstract: We study the appearance of large scale mean motion sustained by stochastic\nforcing on a rotating fluid (in the quasigeostrophic approximation) flowing\nover topography. As in other noise rectification phenomena, the effect requires\nnonlinearity and absence of detailed balance to occur. By application of an\nanalytical coarse graining procedure we identify the physical mechanism\nproducing such effect: It is a forcing coming from the small scales that\nmanifests in a change in the effective viscosity operator and in the effective\nnoise statistical properties. \n\n"}
{"id": "nlin/0306056", "contents": "Title: Ray chaos and ray clustering in an ocean waveguide Abstract: We consider ray propagation in a waveguide with a designed sound-speed\nprofile perturbed by a range-dependent perturbation caused by internal waves in\ndeep ocean environments. The Hamiltonian formalism in terms of the action and\nangle variables is applied to study nonlinear ray dynamics with two\nsound-channel models and three perturbation models: a single-mode perturbation,\na random-like sound-speed fluctuations, and a mixed perturbation. In the\nintegrable limit without any perturbation, we derive analytical expressions for\nray arrival times and timefronts at a given range, the main measurable\ncharacteristics in field experiments in the ocean. In the presence of a\nsingle-mode perturbation, ray chaos is shown to arise as a result of\noverlapping nonlinear ray-medium resonances. Poincar\\'e maps, plots of\nvariations of the action per a ray cycle length, and plots with rays escaping\nthe channel reveal inhomogeneous structure of the underlying phase space with\nremarkable zones of stability where stable coherent ray clusters may be formed.\nWe demonstrate the possibility of determining the wavelength of the\nperturbation mode from the arrival time distribution under conditions of ray\nchaos. It is surprising that coherent ray clusters, consisting of fans of rays\nwhich propagate over long ranges with close dynamical characteristics, can\nsurvive under a random-like multiplicative perturbation modelling sound-speed\nfluctuations caused by a wide spectrum of internal waves. \n\n"}
{"id": "nlin/0408005", "contents": "Title: Small and Large Scale Fluctuations in Atmospheric Wind Speeds Abstract: Atmospheric wind speeds and their fluctuations at different locations\n(onshore and offshore) are examined. One of the most striking features is the\nmarked intermittency of probability density functions (PDF) of velocity\ndifferences -- no matter what location is considered. The shape of these PDFs\nis found to be robust over a wide range of scales which seems to contradict the\nmathematical concept of stability where a Gaussian distribution should be the\nlimiting one. Motivated by the instationarity of atmospheric winds it is shown\nthat the intermittent distributions can be understood as a superposition of\ndifferent subsets of isotropic turbulence. Thus we suggest a simple stochastic\nmodel to reproduce the measured statistics of wind speed fluctuations. \n\n"}
{"id": "nlin/0605003", "contents": "Title: Differential models for 2D turbulence Abstract: We present two phenomenological models for 2D turbulence in which the energy\nspectrum obeys a nonlinear fourth-order and a second-order differential\nequations respectively. Both equations respect the scaling properties of the\noriginal Navier-Stokes equations and it has both the -5/3 inverse-cascade and t\n-3 direct-cascade spectra. In addition, the fourth order equation has\nRaleigh-Jeans thermodynamic distributions, as exact steady state solutions. We\nuse the fourth-order model to derive a relation between the direct-cascade and\nthe inverse-cascade Kolmogorov constants which is in a good qualitative\nagreement with the laboratory and numerical experiments. We obtain a steady\nstate solution where both the enstrophy and the energy cascades are present\nsimultaneously and we discuss it in context of the Nastrom-Gage spectrum\nobserved in atmospheric turbulence. We also consider the effect of the bottom\nfriction onto the cascade solutions, and show that it leads to an additional\ndecrease and finite-wavenumber cutoffs of the respective cascade spectra. \n\n"}
{"id": "nlin/0605040", "contents": "Title: Sling effect in collisions of water droplets in turbulent clouds Abstract: We describe and evaluate the contribution of sling effect into the collision\nrate of the same-size water droplets in turbulent clouds. We show that already\nfor Stokes numbers exceeding 0.2 the sling effect gives a contribution\ncomparable to Saffman-Turner contribution, which may explain why the latter\nconsistently underestimates collision rate (even with the account of\npreferential concentration). \n\n"}
{"id": "nlin/0606058", "contents": "Title: A Model of Intra-seasonal Oscillations in the Earth atmosphere Abstract: We suggest a way of rationalizing an intra-seasonal oscillations (IOs) of the\nEarth atmospheric flow as four meteorological relevant triads of interacting\nplanetary waves, isolated from the system of all the rest planetary waves.\n  Our model is independent of the topography (mountains, etc.) and gives a\nnatural explanation of IOs both in the North and South Hemispheres. Spherical\nplanetary waves are an example of a wave mesoscopic system obeying discrete\nresonances that also appears in other areas of physics. \n\n"}
{"id": "nlin/0610056", "contents": "Title: Energy Conservation and Second-Order Statistics in Stably Stratified\n  Turbulent Boundary Layers Abstract: We address the dynamical and statistical description of stably stratified\nturbulent boundary layers with the important example of the atmospheric\nboundary layer with a stable temperature stratification in mind. Traditional\napproaches to this problem, based on the profiles of mean quantities, velocity\nsecond-order correlations, and dimensional estimates of the turbulent thermal\nflux run into a well known difficulty, predicting the suppression of turbulence\nat a small critical value of the Richardson number, in contradiction with\nobservations. Phenomenological attempts to overcome this problem suffer from\nvarious theoretical inconsistencies. Here we present an approach taking into\nfull account all the second-order statistics, which allows us to respect the\nconservation of total mechanical energy. The analysis culminates in an analytic\nsolution of the profiles of all mean quantities and all second-order\ncorrelations removing the unphysical predictions of previous theories. We\npropose that the approach taken here is sufficient to describe the lower parts\nof the atmospheric boundary layer, as long as the Richardson number does not\nexceed an order of unity. For much higher Richardson numbers the physics may\nchange qualitatively, requiring careful consideration of the potential\nKelvin-Helmoholtz waves and their interaction with the vortical turbulence. \n\n"}
{"id": "physics/0105023", "contents": "Title: Forces at the Sea Bed using a Finite Element Solution of the Mild Slope\n  Wave Equation Abstract: An algorithm to compute forces at the sea bed from a finite element solution\nto the mild slope wave equation is devised in this work. The algorithm is best\nconsidered as consisting of two logical parts: The first is concerned with the\ncomputation of the derivatives to a finite element solution, given the\nassociated mesh; the second is a bi-quadratic least squares fit which serves to\nmodel the sea bed locally in the vicinity of a node. The force at the sea bed\ncan be quantified in terms of either lift and drag, the likes of Stokes'\nformula or traction. While the latter quantity is the most desireable, the\ndirect computation of tractions at the sea bed is controversial in the context\nof the mild slope wave equation as a result of the irrotationality implied by\nthe use of potentials. This work ultimately envisages a ``Monte Carlo''\napproach using wave induced forces to elucidate presently known heavy mineral\nplacer deposits and, consequently, to predict the existance of other deposits\nwhich remain as yet undiscovered. \n\n"}
{"id": "physics/0111110", "contents": "Title: Metastability in simple climate models: Pathwise analysis of slowly\n  driven Langevin equations Abstract: We consider simple stochastic climate models, described by slowly\ntime-dependent Langevin equations. We show that when the noise intensity is not\ntoo large, these systems can spend substantial amounts of time in metastable\nequilibrium, instead of adiabatically following the stationary distribution of\nthe frozen system. This behaviour can be characterized by describing the\nlocation of typical paths, and bounding the probability of atypical paths. We\nillustrate this approach by giving a quantitative description of phenomena\nassociated with bistability, for three famous examples of simple climate\nmodels: Stochastic resonance in an energy balance model describing Ice Ages;\nhysteresis in a box model for the Atlantic thermohaline circulation; and\nbifurcation delay in the case of the Lorenz model for Rayleigh-B'enard\nconvection. \n\n"}
{"id": "physics/0205071", "contents": "Title: Problem of water vapor absorption continuum in atmospheric windows.\n  Return of dimer hypothesis Abstract: Two alternative hypotheses try to explain the origin of the continuum. The\npresent communication gives new argument in favor of the dimer hypothesis. This\nargument is based on the existence of the wide component of line in absorption\nspectrum of polyatomic molecules. \n\n"}
{"id": "physics/0209047", "contents": "Title: Vortex line representation for flows of ideal and viscous fluids Abstract: It is shown that the Euler hydrodynamics for vortical flows of an ideal fluid\ncoincides with the equations of motion of a charged {\\it compressible} fluid\nmoving due to a self-consistent electromagnetic field. Transition to the\nLagrangian description in a new hydrodynamics is equivalent for the original\nEuler equations to the mixed Lagrangian-Eulerian description - the vortex line\nrepresentation (VLR). Due to compressibility of a \"new\" fluid the collapse of\nvortex lines can happen as the result of breaking (or overturning) of vortex\nlines. It is found that the Navier-Stokes equation in the vortex line\nrepresentation can be reduced to the equation of the diffusive type for the\nCauchy invariant with the diffusion tensor given by the metric of the VLR. \n\n"}
{"id": "physics/0212055", "contents": "Title: PARIS Altimetry with L1 Frequency Data from the Bridge 2 Experiment Abstract: A portion of 20 minutes of the GPS signals collected during the Bridge 2\nexperimental campaign, performed by ESA, have been processed. An innovative\nalgorithm called Parfait, developed by Starlab and implemented within Starlab's\nGNSS-R Software package STARLIGHT (STARLab Interferometric Gnss Toolkit), has\nbeen successfully used with this set of data. A comparison with tide values\nindependently collected and with differential GPS processed data has been\nperformed. We report a successful PARIS phase altimetric measure of the Zeeland\nBrug over the sea surface with a rapidly changing tide, with a precision better\nthan 2 cm. \n\n"}
{"id": "physics/0304035", "contents": "Title: A model for rapid stochastic distortions of small-scale turbulence Abstract: We present a model describing evolution of the small-scale Navier-Stokes\nturbulence due to its stochastic distortions by much larger turbulent scales.\nThis study is motivated by numerical findings (laval, 2001) that such\ninteractions of separated scales play important role in turbulence\nintermittency. We introduce description of turbulence in terms of the moments\nof the k-space quantities using a method previously developed for the kinematic\ndynamo problem (Nazarenko, 2003). Working with the $k$-space moments allows to\nintroduce new useful measures of intermittency such as the mean polarization\nand the spectral flatness. Our study of the 2D turbulence shows that the energy\ncascade is scale invariant and Gaussian whereas the enstrophy cascade is\nintermittent. In 3D, we show that the statistics of turbulence wavepackets\ndeviates from gaussianity toward dominance of the plane polarizations. Such\nturbulence is formed by ellipsoids in the $k$-space centered at its origin and\nhaving one large, one neutral and one small axes with the velocity field\npointing parallel to the smallest axis. \n\n"}
{"id": "physics/0306043", "contents": "Title: Characteristics of Aerosol Spectral Optical Depths over Manora Peak,\n  Nainital $-$ A High Altitude Station in the Central Himalayas Abstract: We present, for the first time, spectral behaviour of aerosol optical depths\n(AODs) over Manora Peak, Nainital located at an altitude of $\\sim$ 2 km in the\nShivalik ranges of central Himalayas. The observations were carried out using a\nMulti-Wavelength solar Radiometer during January to December 2002. The main\nresults of the study are extremely low AODs during winter, a remarkable\nincrease to high values in summer and a distinct change in the spectral\ndependencies of AODs from a relatively steeper spectra during winter to a\nshallower one in summer. During transparent days, the AOD values lie usually\nbelow 0.08 while during dusty (turbid) days, it lies between 0.08 to 0.69 at\n0.5 $\\mu$m. The average AOD value at 0.5 $\\mu$m during winters, particularly in\nJanuary and February, is $\\sim 0.03\\pm0.01$. The mean aerosol extinction law at\nManora Peak during 2002 is best represented by $0.10 \\lambda^{-0.61}$. However\nduring transparent days, which almost covers 40% of the time, it is represented\nby $0.02 \\lambda^{-0.97}$. This value of wavelength exponent, representing\nreduced coarse concentration and presence of fine aerosols, indicates that the\nstation measures aerosol in the free troposphere at least during part of the\nyear. \n\n"}
{"id": "physics/0306157", "contents": "Title: Electron gas oscillations in plasma. Theory and applications Abstract: We analyze the obtained solutions of the non-linear Shroedinger equation for\nspherically and axially symmetrical electrons density oscillations in plasma.\nThe conditions of the oscillations process existence are examined. It is shown\nthat in the center or on the axis of symmetry of the systems the static density\nof electrons enhances. This process results in the increasing of density and\npressure of the ion gas. We suggest that this mechanism could occur in nature\nas rare phenomenon called the fireball and could be used in carrying out the\nresearch concerning controlled fusion. The description of the experiments,\ncarried out for the purpose to generate long-lived spherical plasma structures,\nis presented. \n\n"}
{"id": "physics/0307039", "contents": "Title: Passive tracer patchiness and particle trajectory stability in\n  incompressible two-dimensional flows Abstract: Particle motion is considered in incompressible two-dimensional flows\nconsisting of a steady background gyre on which an unsteady wave-like\nperturbation is superimposed. A dynamical systems point of view that exploits\nthe action--angle formalism is adopted. It is argued and demonstrated\nnumerically that for a large class of problems one expects to observe a mixed\nphase space, i.e., the occurrence of ``regular islands'' in an otherwise\n``chaotic sea.'' This leads to patchiness in the evolution of passive tracer\ndistributions. Also, it is argued and demonstrated numerically that particle\ntrajectory stability is largely controlled by the background flow: trajectory\ninstability, quantified by various measures of the ``degree of chaos,''\nincreases on average with increasing $|\\mathrm{d}\\omega/\\mathrm{d}I|$, where\n$\\omega (I)$ is the angular frequency of the trajectory in the background flow\nand $I$ is the action. \n\n"}
{"id": "physics/0307144", "contents": "Title: Light Propagation in Turbulent Media Abstract: First, we make a revision of the up-to-date Passive Scalar Fields properties:\nalso, the refractive index is among them. Afterwards, we formulated the\nproperties that make the family of `isotropic' fractional Brownian motion (with\nparameter H) a good candidate to simulate the turbulent refractive index.\nMoreover, we obtained its fractal dimension which matches the estimated by\nConstantin for passive scalar, and thus the parameter H determines the state of\nthe turbulence.\n  Next, using a path integral velocity representation, with the Markovian\nmodel, to calculate the effects of the turbulence over a system of grids.\n  Finally, with the tools of Stochastic Calculus for fractional Brownian\nmotions we studied the ray-equation coming from the Geometric Optics in the\nturbulent case. Our analysis covers those cases where average temperature\ngradients are relevant. \n\n"}
{"id": "physics/0310027", "contents": "Title: Exact vortex solution of the Jacobs-Rebbi equation for ideal fluids Abstract: The Jacobs-Rebbi equation arises in many contexts where vortical motion in\ntwo-dimensional ideal media is investigated. Alternatively, it can be derived\nin the Abelian Higgs field theory. It is considered non-integrable and\nnumerical solutions have been found, consisting of localised, robust vortices.\nWe show in this work that the equation is integrable and provide the Lax pair.\nThe exact solution is obtained in terms of Riemann \\emph{theta} functions. \n\n"}
{"id": "physics/0310079", "contents": "Title: Do medium range ensemble forecasts give useful predictions of temporal\n  correlations? Abstract: Medium range ensemble forecasts are typically used to derive predictions of\nthe conditional marginal distributions of future events on individual days. We\nassess whether they can also be used to predict the conditional correlations\nbetween different days. \n\n"}
{"id": "physics/0311052", "contents": "Title: PARFAIT: GNSS-R coastal altimetry Abstract: GNSS-R signals contain a coherent and an incoherent component. A new\nalgorithm for coherent phase altimetry over rough ocean surfaces, called\nPARFAIT, has been developed and implemented in Starlab's STARLIGHT GNSS-R\nsoftware package. In this paper we report our extraction and analysis of the\ncoherent component of L1 GPS-R signals collected during the ESTEC Bridge 2\nexperimental campaign using this technique. The altimetric results have been\ncompared with a GPS-buoy calibrated tide model with a resulting precision of\nthe order 1 cm. \n\n"}
{"id": "physics/0312083", "contents": "Title: Multilayer shallow-water model with stratification and shear Abstract: The purpose of this paper is to present a shallow-water-type model with\nmultiple inhomogeneous layers featuring variable linear velocity vertical shear\nand startificaion in horizontal space and time. This is achieved by writing the\nlayer velocity and buoyancy fields as linear functions of depth, with\ncoefficients that depend arbitrarily on horizontal position and time. The model\nis a generalization of Ripa's (1995) single-layer model to an arbitrary number\nof layers. Unlike models with homogeneous layers the present model is able to\nrepresent thermodynamics processes driven by heat and freshwater fluxes through\nthe surface or mixing processes resulting from fluid exchanges across\ncontiguous layers. A model configuration with only one layer has been\npreviously shown to provide: a very good representation of the exact vertical\nnormal modes up to the first internal mode; an exact representation of\nlong-perturbation (free boundary) baroclinic instability; and a very reasonable\nrepresentation of short-perturbation (classical Eady) baroclinic instability.\nHere it is shown that substantially more accurate overall results with respect\nto single-layer calculations can be achieved by considering a stack of only a\nfew layers. A similar behavior is found in ageostrophic (classical Stone)\nbaroclinic instability by describing accurately the dependence of the solutions\non the Richardson number with only two layers. \n\n"}
{"id": "physics/0401046", "contents": "Title: The problem with the Brier score Abstract: The Brier score is frequently used by meteorologists to measure the skill of\nbinary probabilistic forecasts. We show, however, that in simple idealised\ncases it gives counterintuitive results. We advocate the use of an alternative\nmeasure that has a more compelling intuitive justification. \n\n"}
{"id": "physics/0401078", "contents": "Title: Linear waves and baroclinic instability in an inhomogeneous-density\n  layered primitive-equation ocean model Abstract: We consider a multilayer generalization of Ripa's inhomogeneous-density\nsingle-layer primitive-equation model. In addition to vary arbitrarily in\nhorizontal position and time, the horizontal velocity and buoyancy fields are\nallowed to vary linearly with depth within each layer of the model. Preliminary\nresults on linear waves and baroclinic instability suggest that a configuration\ninvolving a few layers may set the basis for a quite accurate and numerically\nefficient ocean model. \n\n"}
{"id": "physics/0402026", "contents": "Title: Improving probabilistic weather forecasts using seasonally varying\n  calibration parameters Abstract: We show that probabilistic weather forecasts of site specific temperatures\ncan be dramatically improved by using seasonally varying rather than constant\ncalibration parameters. \n\n"}
{"id": "physics/0402027", "contents": "Title: Singular vector ensemble forecasting systems and the prediction of flow\n  dependent uncertainty Abstract: The ECMWF ensemble weather forecasts are generated by perturbing the initial\nconditions of the forecast using a subset of the singular vectors of the\nlinearised propagator. Previous results show that when creating probabilistic\nforecasts from this ensemble better forecasts are obtained if the mean of the\nspread and the variability of the spread are calibrated separately. We show\nresults from a simple linear model that suggest that this may be a generic\nproperty for all singular vector based ensemble forecasting systems based on\nonly a subset of the full set of singular vectors. \n\n"}
{"id": "physics/0403005", "contents": "Title: Modeling turbulent wave-front phase as a fractional Brownian motion: a\n  new approach Abstract: This paper introduces a general and new formalism to model the turbulent\nwave-front phase using fractional Brownian motion processes. Moreover, it\nextends results to non-Kolmogorov turbulence. In particular, generalized\nexpressions for the Strehl ratio and the angle-of-arrival variance are\nobtained. These are dependent on the dynamic state of the turbulence. \n\n"}
{"id": "physics/0406029", "contents": "Title: Sea state monitoring using coastal GNSS-R Abstract: We report on a coastal experiment to study GPS L1 reflections. The campaign\nwas carried out at the Barcelona Port breaker and dedicated to the development\nof sea-state retrieval algorithms. An experimental system built for this\npurpose collected and processed GPS data to automatically generate a times\nseries of the interferometric complex field (ICF). The ICF was analyzed off\nline and compared to a simple developed model that relates ICF coherence time\nto the ratio of significant wave height (SWH) and mean wave period (MWP). The\nanalysis using this model showed good consistency between the ICF coherence\ntime and nearby oceanographic buoy data. Based on this result, preliminary\nconclusions are drawn on the potential of coastal GNSS-R for sea state\nmonitoring using semi-empirical modeling to relate GNSS-R ICF coherence time to\nSWH. \n\n"}
{"id": "physics/0407037", "contents": "Title: The Eddy Experiment: GNSS-R speculometry for directional sea-roughness\n  retrieval from low altitude aircraft Abstract: We report on the retrieval of directional sea surface roughness, in terms of\nits full directional mean square slope (including direction and isotropy), from\nGlobal Navigation Satellite System Reflections (GNSS-R) Delay-Doppler-Map (DDM)\ndata collected during an experimental flight at 1 km altitude. This study\nemphasizes the utilization of the entire DDM to more precisely infer ocean\nroughness directional parameters. In particular, we argue that the DDM exhibits\nthe impact of both roughness and scatterer velocity. Obtained estimates are\nanalyzed and compared to co-located Jason-1 measurements, ECMWF numerical\nweather model outputs and optical data. \n\n"}
{"id": "physics/0407082", "contents": "Title: A link between an ice age era and a rapid polar shift Abstract: The striking asymmetry of the ice cover during the Last Global Maximum\nsuggests that the North Pole was in Greenland and then rapidly shifted to its\npresent position in the Arctic See. A scenario which causes such a rapid\ngeographic polar shift is physically possible. It involves an additional\nplanet, which disappeared by evaporation within the Holocene. This is only\npossible within such a short period, if the planet was in an extremely\neccentric orbit and hot. Then, since this produced an interplanetary gas cloud,\nthe polar shift had to be preceded by a cold period with large global\ntemperature variations during several million years. \n\n"}
{"id": "physics/0409096", "contents": "Title: Probabilistic temperature forecasting: a summary of our recent research\n  results Abstract: We summarise the main results from a number of our recent articles on the\nsubject of probabilistic temperature forecasting. \n\n"}
{"id": "physics/0409127", "contents": "Title: Probabilistic forecasting of temperature: comments on the Bayesian Model\n  Averaging approach Abstract: A specific implementation of Bayesian model averaging has recently been\nsuggested as a method for the calibration of ensemble temperature forecasts. We\npoint out the similarities between this new approach and an earlier method\nknown as kernel regression. We also argue that the Bayesian model averaging\nmethod (as applied) has a number of flaws that would result in forecasts with\nsuboptimally calibrated mean and uncertainty. \n\n"}
{"id": "physics/0410053", "contents": "Title: Probabilistic temperature forecasting: a comparison of four\n  spread-regression models Abstract: Spread regression is an extension of linear regression that allows for the\ninclusion of a predictor that contains information about the variance. It can\nbe used to take the information from a weather forecast ensemble and produce a\nprobabilistic prediction of future temperatures. There are a number of ways\nthat spread regression can be formulated in detail. We perform an empirical\ncomparison of four of the most obvious methods applied to the calibration of a\nyear of ECMWF temperature forecasts for London Heathrow. \n\n"}
{"id": "physics/0411011", "contents": "Title: Water waves over a time-dependent bottom: Exact description for 2D\n  potential flows Abstract: Two-dimensional potential flows of an ideal fluid with a free surface are\nconsidered in situations when shape of the bottom depends on time due to\nexternal reasons. Exact nonlinear equations describing surface waves in terms\nof the so called conformal variables are derived for an arbitrary time-evolving\nbottom parameterized by an analytical function. An efficient numerical method\nfor the obtained equations is suggested. \n\n"}
{"id": "physics/0501087", "contents": "Title: A Bayesian Estimator for Linear Calibration Error Effects in Thermal\n  Remote Sensing Abstract: The Bayesian Land Surface Temperature estimator previously developed has been\nextended to include the effects of imperfectly known gain and offset\ncalibration errors. It is possible to treat both gain and offset as nuisance\nparameters and, by integrating over an uninformative range for their\nmagnitudes, eliminate the dependence of surface temperature and emissivity\nestimates upon the exact calibration error. \n\n"}
{"id": "physics/0502146", "contents": "Title: Quasi-planar steep water waves Abstract: A new description for highly nonlinear potential water waves is suggested,\nwhere weak 3D effects are included as small corrections to exact 2D equations\nwritten in conformal variables. Contrary to the traditional approach, a small\nparameter in this theory is not the surface slope, but it is the ratio of a\ntypical wave length to a large transversal scale along the second horizontal\ncoordinate. A first-order correction for the Hamiltonian functional is\ncalculated, and the corresponding equations of motion are derived for steep\nwater waves over an arbitrary inhomogeneous quasi-1D bottom profile. \n\n"}
{"id": "physics/0503119", "contents": "Title: Global Warming: some back-of-the-envelope calculations Abstract: We do several simple calculations and measurements in an effort to gain\nunderstanding of global warming and the carbon cycle. Some conclusions are\ninteresting: (i) There has been global warming since the end of the \"little ice\nage\" around 1700. There is no statistically significant evidence of\nacceleration of global warming since 1940. (ii) The increase of CO_2 in the\natmosphere, beginning around 1940, accurately tracks the burning of fossil\nfuels. Burning all of the remaining economically viable reserves of oil, gas\nand coal over the next 150 years or so will approximately double the\npre-industrial atmospheric concentration of CO_2. The corresponding increase in\nthe average temperature, due to the greenhouse effect, is quite uncertain:\nbetween 1.3 and 4.8K. This increase of temperature is (partially?) offset by\nthe increase of aerosols and deforestation. (iii) Ice core samples indicate\nthat the pre-historic CO_2 concentration and temperature are well correlated.\nWe conclude that changes in the temperatures of the oceans are probably the\ncause of the changes of pre-historic atmospheric CO_2 concentration. (iv) Data\nsuggests that large volcanic explosions can trigger transitions from glacial to\ninterglacial climates. (v) Most of the carbon fixed by photosynthesis in the\nAmazon basin returns to the atmosphere due to aerobic decay. \n\n"}
{"id": "physics/0503155", "contents": "Title: Describing two-dimensional vortical flows : the typhoon case Abstract: We present results of a numerical study of the differential equation\ngoverning the stationary states of the two-dimensional planetary atmosphere and\nmagnetized plasma (within the Charney Hasegawa Mima model). The most strinking\nresult is that the equation appears to be able to reproduce the main features\nof the flow structure of a typhoon. \n\n"}
{"id": "physics/0505103", "contents": "Title: Statistical modelling of tropical cyclone tracks: a comparison of models\n  for the variance of trajectories Abstract: We describe results from the second stage of a project to build a statistical\nmodel for hurricane tracks. In the first stage we modelled the unconditional\nmean track. We now attempt to model the unconditional variance of fluctuations\naround the mean. The variance models we describe use a semi-parametric nearest\nneighbours approach in which the optimal averaging length-scale is estimated\nusing a jack-knife out-of-sample fitting procedure. We test three different\nmodels. These models consider the variance structure of the deviations from the\nunconditional mean track to be isotropic, anisotropic but uncorrelated, and\nanisotropic and correlated, respectively. The results show that, of these\nmodels, the anisotropic correlated model gives the best predictions of the\ndistribution of future positions of hurricanes. \n\n"}
{"id": "physics/0506055", "contents": "Title: Improving on the empirical covariance matrix using truncated PCA with\n  white noise residuals Abstract: The empirical covariance matrix is not necessarily the best estimator for the\npopulation covariance matrix: we describe a simple method which gives better\nestimates in two examples. The method models the covariance matrix using\ntruncated PCA with white noise residuals. Jack-knife cross-validation is used\nto find the truncation that maximises the out-of-sample likelihood score. \n\n"}
{"id": "physics/0508226", "contents": "Title: Sensitivity of ray dynamics in an underwater sound channel to vertical\n  scale of longitudinal sound-speed variations Abstract: We investigate sound ray propagation in a range-dependent underwater acoustic\nwaveguide. Our attention is focused on sensitivity of ray dynamics to the\nvertical structure of a sound-speed perturbation induced by ocean internal\nwaves. Two models of longitudinal sound-speed variations are considered: a\nperiodic inhomogeneity and a stochastic one. It is found that vertical\noscillations of a sound-speed perturbation can affect rays in a resonant\nmanner. Such resonances give rise to chaos in certain regions of phase space.\nIt is shown that stability of steep rays, being observed in experiments, is\nconnected with suppression of resonances in the case of small-scale vertical\nsound-speed oscillations. \n\n"}
{"id": "physics/0509024", "contents": "Title: Statistical modelling of tropical cyclone tracks: modelling the\n  autocorrelation in track shape Abstract: We describe results from the third stage of a project to build a statistical\nmodel for hurricane tracks. In the first stage we modelled the unconditional\nmean track. In the second stage we modelled the unconditional variance of\nfluctuations around the mean. Now we address the question of how to model the\nautocorrelations in the standardised fluctuations. We perform a thorough\ndiagnostic analysis of these fluctuations, and fit a type of AR(1) model. We\nthen assess the goodness of fit of this model in a number of ways, including an\nout-of-sample comparison with a simpler model, an in-sample residual analysis,\nand a comparison of simulated tracks from the model with the observed tracks.\nBroadly speaking, the model captures the behaviour of observed hurricane\ntracks. In detail, however, there are a number of systematic errors. \n\n"}
{"id": "physics/0510150", "contents": "Title: Current effects on scattering of surface gravity waves by bottom\n  topography Abstract: Scattering of random surface gravity waves by small amplitude topography in\nthe presence of a uniform current is investigated theoretically. This problem\nis relevant to ocean waves propagation on shallow continental shelves where\ntidal currents are often significant. A perturbation expansion of the wave\naction to second order in powers of the bottom amplitude yields an evolution\nequation for the wave action spectrum. A scattering source term gives the rate\nof exchange of the wave action spectrum between wave components, with\nconservation of the total action at each absolute frequency. With and without\ncurrent, the scattering term yields reflection coefficients for the amplitudes\nof waves that converge, to the results of previous theories for monochromatic\nwaves propagating in one dimension over sinusoidal bars. Over sandy continental\nshelves, tidal currents are known to generate sandwaves with scales comparable\nto those of surface waves. Application of the theory to such a real topography\nsuggests that scattering mainly results in a broadening of the directional wave\nspectrum, due to forward scattering, while the back-scattering is generally\nweaker. The current may strongly influence surface gravity wave scattering by\nselecting different bottom scales with widely different spectral densities due\nthe sharp bottom spectrum roll-off. \n\n"}
{"id": "physics/0510203", "contents": "Title: Statistical modelling of tropical cyclone genesis: a non-parametric\n  model for the annual distribution Abstract: As part of a project to develop more accurate estimates of the risks due to\ntropical cyclones, we describe a non-parametric method for the statistical\nsimulation of the location of tropical cyclone genesis. The method avoids the\nuse of arbitrary grid boxes, and the spatial smoothing of the historical data\nis constructed optimally according to a clearly defined merit function. \n\n"}
{"id": "physics/0511236", "contents": "Title: Efficient Data Assimilation for Spatiotemporal Chaos: a Local Ensemble\n  Transform Kalman Filter Abstract: Data assimilation is an iterative approach to the problem of estimating the\nstate of a dynamical system using both current and past observations of the\nsystem together with a model for the system's time evolution. Rather than\nsolving the problem from scratch each time new observations become available,\none uses the model to ``forecast'' the current state, using a prior state\nestimate (which incorporates information from past data) as the initial\ncondition, then uses current data to correct the prior forecast to a current\nstate estimate. This Bayesian approach is most effective when the uncertainty\nin both the observations and in the state estimate, as it evolves over time,\nare accurately quantified. In this article, we describe a practical method for\ndata assimilation in large, spatiotemporally chaotic systems. The method is a\ntype of ``ensemble Kalman filter'', in which the state estimate and its\napproximate uncertainty are represented at any given time by an ensemble of\nsystem states. We discuss both the mathematical basis of this approach and its\nimplementation; our primary emphasis is on ease of use and computational speed\nrather than improving accuracy over previously published approaches to ensemble\nKalman filtering. We include some numerical results demonstrating the\nefficiency and accuracy of our implementation for assimilating real atmospheric\ndata with the global forecast model used by the U.S. National Weather Service. \n\n"}
{"id": "physics/0512091", "contents": "Title: Statistical modelling of tropical cyclone tracks: modelling cyclone\n  lysis Abstract: We describe results from the fifth stage of a project to build a statistical\nmodel of tropical cyclone tracks. The previous stages considered genesis and\nthe shape of tracks. We now consider in more detail how to represent the lysis\n(death) of tropical cyclones. Improving the lysis model turns out to bring a\nsignificant improvement to the track model overall. \n\n"}
{"id": "physics/0512092", "contents": "Title: Year-ahead prediction of US landfalling hurricane numbers: intense\n  hurricanes Abstract: We continue with our program to derive simple practical methods that can be\nused to predict the number of US landfalling hurricanes a year in advance. We\nrepeat an earlier study, but for a slightly different definition landfalling\nhurricanes, and for intense hurricanes only. We find that the averaging lengths\nneeded for optimal predictions of numbers of intense hurricanes are longer than\nthose needed for optimal predictions of numbers of hurricanes of all strengths. \n\n"}
{"id": "physics/0512113", "contents": "Title: Year ahead prediction of US landfalling hurricane numbers: the optimal\n  combination of long and short baselines Abstract: Annual levels of US landfalling hurricane activity averaged over the last 11\nyears (1995-2005) are higher than those averaged over the previous 95 years\n(1900-1994). How, then, should we best predict hurricane activity rates for\nnext year? Based on the assumption that the higher rates will continue we use\nan optimal combination of averages over the long and short time-periods to\nproduce a prediction that minimises MSE. \n\n"}
{"id": "physics/0512135", "contents": "Title: Statistical modelling of tropical cyclone tracks: non-normal innovations Abstract: We present results from the sixth stage of a project to build a statistical\nhurricane model. Previous papers have described our modelling of the tracks,\ngenesis, and lysis of hurricanes. In our track model we have so far employed a\nnormal distribution for the residuals when computing innovations, even though\nwe have demonstrated that their distribution is not normal. Here, we test to\nsee if the track model can be improved by including more realistic non-normal\ninnovations. The results are mixed. Some features of the model improve, but\nothers slightly worsen. \n\n"}
{"id": "physics/0602024", "contents": "Title: Piecewise continuous distribution function method and ultrasound at half\n  space Abstract: The system of hydrodynamic-type equations, derived by two-side distribution\nfunction for a stratified gas in gravity field is applied to a problem of\nultrasound propagation and attenuation. The background state and linearized\nversion of the obtained system is studied and compared with the Navier-Stokes\none at arbitrary Knudsen numbers. The WKB solutions for ultrasound in a\nstratified medium are constructed in explicit form. The problem of a generation\nby a moving plane in a rarefied gas is explored and used as a test while\ncompared with experiment. \n\n"}
{"id": "physics/0606192", "contents": "Title: Year ahead prediction of US landfalling hurricane numbers: the optimal\n  combination of long and short baselines for intense hurricanes Abstract: In previous work, we have shown how to combine long and short historical\nbaselines to make predictions of future hurricane numbers. We now ask: how\nshould such combinations change if we are interested in predicting the future\nnumber of intense hurricanes? \n\n"}
{"id": "physics/0611070", "contents": "Title: Year ahead prediction of US landfalling hurricane numbers: the optimal\n  combination of multiple levels of activity since 1900 Abstract: In earlier work we considered methods for predicting future levels of\nhurricane activity based on the assumption that historical mean activity was at\none constant level from 1900 to 1994, and has been at another constant level\nsince then. We now make this model a little more subtle, and account for the\npossibility of four different levels of mean hurricane activity since 1900. \n\n"}
{"id": "physics/0611086", "contents": "Title: An objective change point analysis of landfalling historical Atlantic\n  hurricane numbers Abstract: In previous work we have analysed the Atlantic basin hurricane number\ntime-series to identify decadal time-scale change points. We now repeat the\nanalysis but for US landfalling hurricanes. The results are very different. \n\n"}
{"id": "physics/0611107", "contents": "Title: Change-point detection in the historical hurricane number time-series:\n  why can't we detect change-points at US landfall? Abstract: The time series of the number of hurricanes per year in the Atlantic basin\nshows a clear change of level between 1994 and 1995. The time series of the\nnumber of hurricanes that make landfall in the US, however, does not show the\nsame obvious change of level. Prima-facie this seems rather surprising, given\nthat the landfalling hurricanes are a subset of the basin hurricanes. We\ninvestigate whether it really should be considered surprising or whether there\nis a simple statistical explanation for the disappearance of this change-point\nat landfall. \n\n"}
{"id": "physics/0612042", "contents": "Title: Extreme Value Statistics of the Total Energy in an Intermediate\n  Complexity Model of the Mid-latitude Atmospheric Jet. Part II: trend\n  detection and assessment Abstract: A baroclinic model for the atmospheric jet at middle-latitudes is used as\nstochastic generator of non-stationary time series of the total energy of the\nsystem. A linear time trend is imposed on the parameter $T_E$, descriptive of\nthe forced equator-to-pole temperature gradient and responsible for setting the\naverage baroclinicity in the model. The focus lies on establishing a\ntheoretically sound framework for the detection and assessment of trend at\nextreme values of the generated time series. This problem is dealt with by\nfitting time-dependent Generalized Extreme Value (GEV) models to sequences of\nyearly maxima of the total energy. A family of GEV models is used in which the\nlocation $\\mu$ and scale parameters $\\sigma$ depend quadratically and linearly\non time, respectively, while the shape parameter $\\xi$ is kept constant. From\nthis family, a model is selected by using diagnostic graphical tools, such as\nprobability and quantile plots, and by means of the likelihood ratio test. The\ninferred location and scale parameters are found to depend in a rather smooth\nway on time and, therefore, on $T_E$. In particular, power-law dependences of\n$\\mu$ and $\\sigma$ on $T_E$ are obtained, in analogy with the results of a\nprevious work where the same baroclinic model was run with fixed values of\n$T_E$ spanning the same range as in this case. It is emphasized under which\nconditions the adopted approach is valid. \n\n"}
{"id": "physics/0701022", "contents": "Title: Humidity contribution to C_n^2 over a 600m pathlength in a tropical\n  marine environment Abstract: We present new optical turbulence structure parameter measurements, C_n^2,\nover sea water between La Parguera and Magueyes Island (17.6N 67W) on the\nsouthwest coast of Puerto Rico. The 600 meter horizontal paths were located\napproximately 1.5 m and 10 m above sea level. No data of this type has ever\nbeen made available in the literature. Based on the data, we show that the\nC_n^2 measurements are about 7 times less compared to equivalent land data.\nThis strong evidence reinforces our previous argument that humidity must be\naccounted for to better ascertain the near surface atmospheric turbulence\neffects, which current visible / near infrared C_n^2 bulk models fail to do. We\nalso explore the generalised fractal dimension of this littoral data and\ncompare it to our reference land data. We find cases that exhibit monofractal\ncharacteristics, that is to say, the effect of rising temperatures during the\ndaylight hours upon turbulence are counterbalanced by humidity, leading to a\nsingle characteristic scale for the measurements. In other words, significant\nmoisture changes in the measurement volume cancels optical turbulence increases\ndue to temperature rises. Figures available as JPG only. \n\n"}
{"id": "physics/0701162", "contents": "Title: Five year prediction of Sea Surface Temperature in the Tropical\n  Atlantic: a comparison of simple statistical methods Abstract: We are developing schemes that predict future hurricane numbers by first\npredicting future sea surface temperatures (SSTs), and then apply the observed\nstatistical relationship between SST and hurricane numbers. As part of this\noverall goal, in this study we compare the historical performance of three\nsimple statistical methods for making five-year SST forecasts. We also present\nSST forecasts for 2006-2010 using these methods and compare them to forecasts\nmade from two structural time series models. \n\n"}
{"id": "physics/0701167", "contents": "Title: Predicting hurricane numbers from Sea Surface Temperature: closed form\n  expressions for the mean, variance and standard error of the number of\n  hurricanes Abstract: One way to predict hurricane numbers would be to predict sea surface\ntemperature, and then predict hurricane numbers as a function of the predicted\nsea surface temperature. For certain parametric models for sea surface\ntemperature and the relationship between sea surface temperature and hurricane\nnumbers, closed-form solutions exist for the mean and the variance of the\nnumber of predicted hurricanes, and for the standard error on the mean. We\nderive a number of such expressions. \n\n"}
{"id": "physics/0701170", "contents": "Title: Predicting basin and landfalling hurricane numbers from sea surface\n  temperature Abstract: We are building a hurricane number prediction scheme based on first\npredicting main development region sea surface temperature (SST), then\npredicting the number of hurricanes in the Atlantic basin given the SST\nprediction, and finally predicting the number of US landfalling hurricanes\nbased on the prediction of the number of basin hurricanes. We have described a\nnumber of SST prediction methods in previous work. We now investigate the\nempirical relationship between SST and basin hurricane numbers, and put this\ntogether with the SST predictions to make predictions of both basin and\nlandfalling hurricane numbers. \n\n"}
{"id": "physics/0701175", "contents": "Title: Correlations between hurricane numbers and sea surface temperature: why\n  does the correlation disappear at landfall? Abstract: There is significant correlation between main development region sea surface\ntemperature and the number of hurricanes that form in the Atlantic basin. The\ncorrelation between the same sea surface temperatures and the number of\n\\emph{landfalling} hurricanes is much lower, however. Why is this? Do we need\nto consider complex physical hypotheses, or is there a simple statistical\nexplanation? \n\n"}
{"id": "physics/0701176", "contents": "Title: Predicting landfalling hurricane numbers from sea surface temperature:\n  theoretical comparisons of direct and indirect approaches Abstract: We consider two ways that one might convert a prediction of sea surface\ntemperature (SST) into a prediction of landfalling hurricane numbers. First,\none might regress historical numbers of landfalling hurricanes onto historical\nSSTs, and use the fitted regression relation to predict future landfalling\nhurricane numbers given predicted SSTs. We call this the direct approach.\nSecond, one might regress \\emph{basin} hurricane numbers onto historical SSTs,\nestimate the proportion of basin hurricanes that make landfall, and use the\nfitted regression relation and estimated proportion to predict future\nlandfalling hurricane numbers. We call this the \\emph{indirect} approach. Which\nof these two methods is likely to work better? We answer this question for two\nsimple models. The first model is reasonably realistic, but we have to resort\nto using simulations to answer the question in the context of this model. The\nsecond model is less realistic, but allows us to derive a general analytical\nresult. \n\n"}
{"id": "physics/0702034", "contents": "Title: In a book \"Tsunami and Nonlinear Waves\": Numerical Verification of the\n  Hasselmann equation Abstract: The purpose of this article is numerical verification of the thory of weak\nturbulence. We performed numerical simulation of an ensemble of nonlinearly\ninteracting free gravity waves (swell) by two different methods: solution of\nprimordial dynamical equations describing potential flow of the ideal fluid\nwith a free surface and, solution of the kinetic Hasselmann equation,\ndescribing the wave ensemble in the framework of the theory of weak turbulence.\nComparison of the results demonstrates pretty good applicability of the weak\nturbulent approach. \n\n"}
{"id": "physics/0702067", "contents": "Title: Explicit wave-averaged primitive equations using a Generalized\n  Lagrangian Mean Abstract: The generalized Langrangian mean theory provides exact equations for general\nwave-turbulence-mean flow interactions in three dimensions. For practical\napplications, these equations must be closed by specifying the wave forcing\nterms. Here an approximate closure is obtained under the hypotheses of small\nsurface slope, weak horizontal gradients of the water depth and mean current,\nand weak curvature of the mean current profile. These assumptions yield\nanalytical expressions for the mean momentum and pressure forcing terms that\ncan be expressed in terms of the wave spectrum. A vertical change of coordinate\nis then applied to obtain glm2z-RANS equations (55) and (57) with non-divergent\nmass transport in cartesian coordinates. To lowest order, agreement is found\nwith Eulerian-mean theories, and the present approximation provides an explicit\nextension of known wave-averaged equations to short-scale variations of the\nwave field, and vertically varying currents only limited to weak or localized\nprofile curvatures. Further, the underlying exact equations provide a natural\nframework for extensions to finite wave amplitudes and any realistic situation.\nThe accuracy of the approximations is discussed using comparisons with exact\nnumerical solutions for linear waves over arbitrary bottom slopes, for which\nthe equations are still exact when properly accounting for partial standing\nwaves. For finite amplitude waves it is found that the approximate solutions\nare probably accurate for ocean mixed layer modelling and shoaling waves,\nprovided that an adequate turbulent closure is designed. However, for surf zone\napplications the approximations are expected to give only qualitative results\ndue to the large influence of wave nonlinearity on the vertical profiles of\nwave forcing terms. \n\n"}
{"id": "physics/0702145", "contents": "Title: Numerical Verification of the Weak Turbulent Model for Swell Evolution Abstract: The purpose of this article is numerical verification of the theory of weak\nturbulence. We performed numerical simulation of an ensemble of nonlinearly\ninteracting free gravity waves (swell) by two different methods: solution of\nprimordial dynamical equations describing potential flow of the ideal fluid\nwith a free surface and, solution of the kinetic Hasselmann equation,\ndescribing the wave ensemble in the framework of the theory of weak turbulence.\n  In both cases we observed effects predicted by this theory: frequency\ndownshift, angular spreading and formation of Zakharov-Filonenko spectrum\n$I_{\\omega} \\sim \\omega^{-4}$. To achieve quantitative coincidence of the\nresults obtained by different methods, one has to supply the Hasselmann kinetic\nequation by an empirical dissipation term $S_{diss}$ modeling the coherent\neffects of white-capping. Using of the standard dissipation terms from\noperational wave predicting model ({\\it WAM}) leads to significant improvement\non short times, but not resolve the discrepancy completely, leaving the\nquestion about optimal choice of $S_{diss}$ open. In a long run {\\it WAM}\ndissipative terms overestimate dissipation essentially. \n\n"}
{"id": "physics/9708037", "contents": "Title: Tracking down the ENSO delayed oscillator with an adjoint OGCM Abstract: The adjoint of an ocean general circulation model is used as a tool for\ninvestigating the causes of changes in ENSO SST indices. We identify adjoint\nKelvin and Rossby waves in the sensitivities to sea level and wind stress at\nearlier times, which can be traced back for more than a year through western\nand weak eastern boundary reflections. Depending on the thermocline depth the\nfirst and second baroclinic modes are excited. The sensitivities to the heat\nflux and SST are local and decay in about a month. The sensitivities to the\nfluxes are converted into the influence of SST using the adjoint of a\nstatistical atmosphere model. Focusing on SST perturbations in the index region\nitself, we recover, up to a scale factor, the delayed oscillator concept. \n\n"}
{"id": "physics/9904002", "contents": "Title: What caused the onset of the 1997-1998 El Nino? Abstract: There has been intense debate about the causes of the 1997-1998 El Nino. One\nside sees the obvious intense westerly wind events as the main cause for the\nexceptional heating in summer 1997, the other emphasizes slower oceanic\nprocesses. We present a quantitative analysis of all factors contributing to\nthe onset of this El Nino. At six months' lead time the initial state\ncontributes about 40% of the heating compared with an average year, and the\nwind about 50%. Compared with 1996, these contributions are 30% and 90%\nrespectively. As westerly wind events are difficult to predict, this limited\nthe predictability of the onset of this El Nino. \n\n"}
