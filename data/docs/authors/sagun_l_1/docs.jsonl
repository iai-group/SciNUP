{"id": "0707.0895", "contents": "Title: Segmentation and Context of Literary and Musical Sequences Abstract: We test a segmentation algorithm, based on the calculation of the\nJensen-Shannon divergence between probability distributions, to two symbolic\nsequences of literary and musical origin. The first sequence represents the\nsuccessive appearance of characters in a theatrical play, and the second\nrepresents the succession of tones from the twelve-tone scale in a keyboard\nsonata. The algorithm divides the sequences into segments of maximal\ncompositional divergence between them. For the play, these segments are related\nto changes in the frequency of appearance of different characters and in the\ngeographical setting of the action. For the sonata, the segments correspond to\ntonal domains and reveal in detail the characteristic tonal progression of such\nkind of musical composition. \n\n"}
{"id": "0711.2270", "contents": "Title: Can a Computer Laugh ? Abstract: A computer model of \"a sense of humour\" suggested previously\n[arXiv:0711.2058,0711.2061], relating the humorous effect with a specific\nmalfunction in information processing, is given in somewhat different\nexposition. Psychological aspects of humour are elaborated more thoroughly. The\nmechanism of laughter is formulated on the more general level. Detailed\ndiscussion is presented for the higher levels of information processing, which\nare responsible for a perception of complex samples of humour. Development of a\nsense of humour in the process of evolution is discussed. \n\n"}
{"id": "0903.4217", "contents": "Title: Conditional Probability Tree Estimation Analysis and Algorithms Abstract: We consider the problem of estimating the conditional probability of a label\nin time $O(\\log n)$, where $n$ is the number of possible labels. We analyze a\nnatural reduction of this problem to a set of binary regression problems\norganized in a tree structure, proving a regret bound that scales with the\ndepth of the tree. Motivated by this analysis, we propose the first online\nalgorithm which provably constructs a logarithmic depth tree on the set of\nlabels to solve this problem. We test the algorithm empirically, showing that\nit works succesfully on a dataset with roughly $10^6$ labels. \n\n"}
{"id": "1002.0709", "contents": "Title: Aggregating Algorithm competing with Banach lattices Abstract: The paper deals with on-line regression settings with signals belonging to a\nBanach lattice. Our algorithms work in a semi-online setting where all the\ninputs are known in advance and outcomes are unknown and given step by step. We\napply the Aggregating Algorithm to construct a prediction method whose\ncumulative loss over all the input vectors is comparable with the cumulative\nloss of any linear functional on the Banach lattice. As a by-product we get an\nalgorithm that takes signals from an arbitrary domain. Its cumulative loss is\ncomparable with the cumulative loss of any predictor function from Besov and\nTriebel-Lizorkin spaces. We describe several applications of our setting. \n\n"}
{"id": "1009.0861", "contents": "Title: On the Estimation of Coherence Abstract: Low-rank matrix approximations are often used to help scale standard machine\nlearning algorithms to large-scale problems. Recently, matrix coherence has\nbeen used to characterize the ability to extract global information from a\nsubset of matrix entries in the context of these low-rank approximations and\nother sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Since\ncoherence is defined in terms of the singular vectors of a matrix and is\nexpensive to compute, the practical significance of these results largely\nhinges on the following question: Can we efficiently and accurately estimate\nthe coherence of a matrix? In this paper we address this question. We propose a\nnovel algorithm for estimating coherence from a small number of columns,\nformally analyze its behavior, and derive a new coherence-based matrix\napproximation bound based on this analysis. We then present extensive\nexperimental results on synthetic and real datasets that corroborate our\nworst-case theoretical analysis, yet provide strong support for the use of our\nproposed algorithm whenever low-rank approximation is being considered. Our\nalgorithm efficiently and accurately estimates matrix coherence across a wide\nrange of datasets, and these coherence estimates are excellent predictors of\nthe effectiveness of sampling-based matrix approximation on a case-by-case\nbasis. \n\n"}
{"id": "1010.0056", "contents": "Title: Online Learning in Opportunistic Spectrum Access: A Restless Bandit\n  Approach Abstract: We consider an opportunistic spectrum access (OSA) problem where the\ntime-varying condition of each channel (e.g., as a result of random fading or\ncertain primary users' activities) is modeled as an arbitrary finite-state\nMarkov chain. At each instance of time, a (secondary) user probes a channel and\ncollects a certain reward as a function of the state of the channel (e.g., good\nchannel condition results in higher data rate for the user). Each channel has\npotentially different state space and statistics, both unknown to the user, who\ntries to learn which one is the best as it goes and maximizes its usage of the\nbest channel. The objective is to construct a good online learning algorithm so\nas to minimize the difference between the user's performance in total rewards\nand that of using the best channel (on average) had it known which one is the\nbest from a priori knowledge of the channel statistics (also known as the\nregret). This is a classic exploration and exploitation problem and results\nabound when the reward processes are assumed to be iid. Compared to prior work,\nthe biggest difference is that in our case the reward process is assumed to be\nMarkovian, of which iid is a special case. In addition, the reward processes\nare restless in that the channel conditions will continue to evolve independent\nof the user's actions. This leads to a restless bandit problem, for which there\nexists little result on either algorithms or performance bounds in this\nlearning context to the best of our knowledge. In this paper we introduce an\nalgorithm that utilizes regenerative cycles of a Markov chain and computes a\nsample-mean based index policy, and show that under mild conditions on the\nstate transition probabilities of the Markov chains this algorithm achieves\nlogarithmic regret uniformly over time, and that this regret bound is also\noptimal. \n\n"}
{"id": "1101.5785", "contents": "Title: Statistical Compressed Sensing of Gaussian Mixture Models Abstract: A novel framework of compressed sensing, namely statistical compressed\nsensing (SCS), that aims at efficiently sampling a collection of signals that\nfollow a statistical distribution, and achieving accurate reconstruction on\naverage, is introduced. SCS based on Gaussian models is investigated in depth.\nFor signals that follow a single Gaussian model, with Gaussian or Bernoulli\nsensing matrices of O(k) measurements, considerably smaller than the O(k\nlog(N/k)) required by conventional CS based on sparse models, where N is the\nsignal dimension, and with an optimal decoder implemented via linear filtering,\nsignificantly faster than the pursuit decoders applied in conventional CS, the\nerror of SCS is shown tightly upper bounded by a constant times the best k-term\napproximation error, with overwhelming probability. The failure probability is\nalso significantly smaller than that of conventional sparsity-oriented CS.\nStronger yet simpler results further show that for any sensing matrix, the\nerror of Gaussian SCS is upper bounded by a constant times the best k-term\napproximation with probability one, and the bound constant can be efficiently\ncalculated. For Gaussian mixture models (GMMs), that assume multiple Gaussian\ndistributions and that each signal follows one of them with an unknown index, a\npiecewise linear estimator is introduced to decode SCS. The accuracy of model\nselection, at the heart of the piecewise linear decoder, is analyzed in terms\nof the properties of the Gaussian distributions and the number of sensing\nmeasurements. A maximum a posteriori expectation-maximization algorithm that\niteratively estimates the Gaussian models parameters, the signals model\nselection, and decodes the signals, is presented for GMM-based SCS. In real\nimage sensing applications, GMM-based SCS is shown to lead to improved results\ncompared to conventional CS, at a considerably lower computational cost. \n\n"}
{"id": "1107.2021", "contents": "Title: Multi-Instance Learning with Any Hypothesis Class Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size. \n\n"}
{"id": "1109.5664", "contents": "Title: Deterministic Feature Selection for $k$-means Clustering Abstract: We study feature selection for $k$-means clustering. Although the literature\ncontains many methods with good empirical performance, algorithms with provable\ntheoretical behavior have only recently been developed. Unfortunately, these\nalgorithms are randomized and fail with, say, a constant probability. We\naddress this issue by presenting a deterministic feature selection algorithm\nfor k-means with theoretical guarantees. At the heart of our algorithm lies a\ndeterministic method for decompositions of the identity. \n\n"}
{"id": "1110.3109", "contents": "Title: Robust Image Analysis by L1-Norm Semi-supervised Learning Abstract: This paper presents a novel L1-norm semi-supervised learning algorithm for\nrobust image analysis by giving new L1-norm formulation of Laplacian\nregularization which is the key step of graph-based semi-supervised learning.\nSince our L1-norm Laplacian regularization is defined directly over the\neigenvectors of the normalized Laplacian matrix, we successfully formulate\nsemi-supervised learning as an L1-norm linear reconstruction problem which can\nbe effectively solved with sparse coding. By working with only a small subset\nof eigenvectors, we further develop a fast sparse coding algorithm for our\nL1-norm semi-supervised learning. Due to the sparsity induced by sparse coding,\nthe proposed algorithm can deal with the noise in the data to some extent and\nthus has important applications to robust image analysis, such as noise-robust\nimage classification and noise reduction for visual and textual bag-of-words\n(BOW) models. In particular, this paper is the first attempt to obtain robust\nimage representation by sparse co-refinement of visual and textual BOW models.\nThe experimental results have shown the promising performance of the proposed\nalgorithm. \n\n"}
{"id": "1112.6384", "contents": "Title: Proof nets for the Lambek-Grishin calculus Abstract: Grishin's generalization of Lambek's Syntactic Calculus combines a\nnon-commutative multiplicative conjunction and its residuals (product, left and\nright division) with a dual family: multiplicative disjunction, right and left\ndifference. Interaction between these two families takes the form of linear\ndistributivity principles. We study proof nets for the Lambek-Grishin calculus\nand the correspondence between these nets and unfocused and focused versions of\nits sequent calculus. \n\n"}
{"id": "1203.2507", "contents": "Title: Deviation optimal learning using greedy Q-aggregation Abstract: Given a finite family of functions, the goal of model selection aggregation\nis to construct a procedure that mimics the function from this family that is\nthe closest to an unknown regression function. More precisely, we consider a\ngeneral regression model with fixed design and measure the distance between\nfunctions by the mean squared error at the design points. While procedures\nbased on exponential weights are known to solve the problem of model selection\naggregation in expectation, they are, surprisingly, sub-optimal in deviation.\nWe propose a new formulation called Q-aggregation that addresses this\nlimitation; namely, its solution leads to sharp oracle inequalities that are\noptimal in a minimax sense. Moreover, based on the new formulation, we design\ngreedy Q-aggregation procedures that produce sparse aggregation models\nachieving the optimal rate. The convergence and performance of these greedy\nprocedures are illustrated and compared with other standard methods on\nsimulated examples. \n\n"}
{"id": "1203.5181", "contents": "Title: $k$-MLE: A fast algorithm for learning statistical mixture models Abstract: We describe $k$-MLE, a fast and efficient local search algorithm for learning\nfinite statistical mixtures of exponential families such as Gaussian mixture\nmodels. Mixture models are traditionally learned using the\nexpectation-maximization (EM) soft clustering technique that monotonically\nincreases the incomplete (expected complete) likelihood. Given prescribed\nmixture weights, the hard clustering $k$-MLE algorithm iteratively assigns data\nto the most likely weighted component and update the component models using\nMaximum Likelihood Estimators (MLEs). Using the duality between exponential\nfamilies and Bregman divergences, we prove that the local convergence of the\ncomplete likelihood of $k$-MLE follows directly from the convergence of a dual\nadditively weighted Bregman hard clustering. The inner loop of $k$-MLE can be\nimplemented using any $k$-means heuristic like the celebrated Lloyd's batched\nor Hartigan's greedy swap updates. We then show how to update the mixture\nweights by minimizing a cross-entropy criterion that implies to update weights\nby taking the relative proportion of cluster points, and reiterate the mixture\nparameter update and mixture weight update processes until convergence. Hard EM\nis interpreted as a special case of $k$-MLE when both the component update and\nthe weight update are performed successively in the inner loop. To initialize\n$k$-MLE, we propose $k$-MLE++, a careful initialization of $k$-MLE guaranteeing\nprobabilistically a global bound on the best possible complete likelihood. \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1205.6849", "contents": "Title: Beyond $\\ell_1$-norm minimization for sparse signal recovery Abstract: Sparse signal recovery has been dominated by the basis pursuit denoise (BPDN)\nproblem formulation for over a decade. In this paper, we propose an algorithm\nthat outperforms BPDN in finding sparse solutions to underdetermined linear\nsystems of equations at no additional computational cost. Our algorithm, called\nWSPGL1, is a modification of the spectral projected gradient for $\\ell_1$\nminimization (SPGL1) algorithm in which the sequence of LASSO subproblems are\nreplaced by a sequence of weighted LASSO subproblems with constant weights\napplied to a support estimate. The support estimate is derived from the data\nand is updated at every iteration. The algorithm also modifies the Pareto curve\nat every iteration to reflect the new weighted $\\ell_1$ minimization problem\nthat is being solved. We demonstrate through extensive simulations that the\nsparse recovery performance of our algorithm is superior to that of $\\ell_1$\nminimization and approaches the recovery performance of iterative re-weighted\n$\\ell_1$ (IRWL1) minimization of Cand{\\`e}s, Wakin, and Boyd, although it does\nnot match it in general. Moreover, our algorithm has the computational cost of\na single BPDN problem. \n\n"}
{"id": "1206.6381", "contents": "Title: Shortest path distance in random k-nearest neighbor graphs Abstract: Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs. \n\n"}
{"id": "1208.1860", "contents": "Title: Scaling Multiple-Source Entity Resolution using Statistically Efficient\n  Transfer Learning Abstract: We consider a serious, previously-unexplored challenge facing almost all\napproaches to scaling up entity resolution (ER) to multiple data sources: the\nprohibitive cost of labeling training data for supervised learning of\nsimilarity scores for each pair of sources. While there exists a rich\nliterature describing almost all aspects of pairwise ER, this new challenge is\narising now due to the unprecedented ability to acquire and store data from\nonline sources, features driven by ER such as enriched search verticals, and\nthe uniqueness of noisy and missing data characteristics for each source. We\nshow on real-world and synthetic data that for state-of-the-art techniques, the\nreality of heterogeneous sources means that the number of labeled training data\nmust scale quadratically in the number of sources, just to maintain constant\nprecision/recall. We address this challenge with a brand new transfer learning\nalgorithm which requires far less training data (or equivalently, achieves\nsuperior accuracy with the same data) and is trained using fast convex\noptimization. The intuition behind our approach is to adaptively share\nstructure learned about one scoring problem with all other scoring problems\nsharing a data source in common. We demonstrate that our theoretically\nmotivated approach incurs no runtime cost while it can maintain constant\nprecision/recall with the cost of labeling increasing only linearly with the\nnumber of sources. \n\n"}
{"id": "1208.3719", "contents": "Title: Auto-WEKA: Combined Selection and Hyperparameter Optimization of\n  Classification Algorithms Abstract: Many different machine learning algorithms exist; taking into account each\nalgorithm's hyperparameters, there is a staggeringly large number of possible\nalternatives overall. We consider the problem of simultaneously selecting a\nlearning algorithm and setting its hyperparameters, going beyond previous work\nthat addresses these issues in isolation. We show that this problem can be\naddressed by a fully automated approach, leveraging recent innovations in\nBayesian optimization. Specifically, we consider a wide range of feature\nselection techniques (combining 3 search and 8 evaluator methods) and all\nclassification approaches implemented in WEKA, spanning 2 ensemble methods, 10\nmeta-methods, 27 base classifiers, and hyperparameter settings for each\nclassifier. On each of 21 popular datasets from the UCI repository, the KDD Cup\n09, variants of the MNIST dataset and CIFAR-10, we show classification\nperformance often much better than using standard selection/hyperparameter\noptimization methods. We hope that our approach will help non-expert users to\nmore effectively identify machine learning algorithms and hyperparameter\nsettings appropriate to their applications, and hence to achieve improved\nperformance. \n\n"}
{"id": "1209.1688", "contents": "Title: Rank Centrality: Ranking from Pair-wise Comparisons Abstract: The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal. \n\n"}
{"id": "1211.1043", "contents": "Title: Soft (Gaussian CDE) regression models and loss functions Abstract: Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution. \n\n"}
{"id": "1211.5063", "contents": "Title: On the difficulty of training Recurrent Neural Networks Abstract: There are two widely known issues with properly training Recurrent Neural\nNetworks, the vanishing and the exploding gradient problems detailed in Bengio\net al. (1994). In this paper we attempt to improve the understanding of the\nunderlying issues by exploring these problems from an analytical, a geometric\nand a dynamical systems perspective. Our analysis is used to justify a simple\nyet effective solution. We propose a gradient norm clipping strategy to deal\nwith exploding gradients and a soft constraint for the vanishing gradients\nproblem. We validate empirically our hypothesis and proposed solutions in the\nexperimental section. \n\n"}
{"id": "1211.6687", "contents": "Title: Robustness Analysis of Hottopixx, a Linear Programming Model for\n  Factoring Nonnegative Matrices Abstract: Although nonnegative matrix factorization (NMF) is NP-hard in general, it has\nbeen shown very recently that it is tractable under the assumption that the\ninput nonnegative data matrix is close to being separable (separability\nrequires that all columns of the input matrix belongs to the cone spanned by a\nsmall subset of these columns). Since then, several algorithms have been\ndesigned to handle this subclass of NMF problems. In particular, Bittorf,\nRecht, R\\'e and Tropp (`Factoring nonnegative matrices with linear programs',\nNIPS 2012) proposed a linear programming model, referred to as Hottopixx. In\nthis paper, we provide a new and more general robustness analysis of their\nmethod. In particular, we design a provably more robust variant using a\npost-processing strategy which allows us to deal with duplicates and near\nduplicates in the dataset. \n\n"}
{"id": "1212.2002", "contents": "Title: A simpler approach to obtaining an O(1/t) convergence rate for the\n  projected stochastic subgradient method Abstract: In this note, we present a new averaging technique for the projected\nstochastic subgradient method. By using a weighted average with a weight of t+1\nfor each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)\nwith both an easy proof and an easy implementation. The new scheme is compared\nempirically to existing techniques, with similar performance behavior. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1212.5701", "contents": "Title: ADADELTA: An Adaptive Learning Rate Method Abstract: We present a novel per-dimension learning rate method for gradient descent\ncalled ADADELTA. The method dynamically adapts over time using only first order\ninformation and has minimal computational overhead beyond vanilla stochastic\ngradient descent. The method requires no manual tuning of a learning rate and\nappears robust to noisy gradient information, different model architecture\nchoices, various data modalities and selection of hyperparameters. We show\npromising results compared to other methods on the MNIST digit classification\ntask using a single machine and on a large scale voice dataset in a distributed\ncluster environment. \n\n"}
{"id": "1302.2576", "contents": "Title: The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking Abstract: We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models. \n\n"}
{"id": "1302.3283", "contents": "Title: StructBoost: Boosting Methods for Predicting Structured Output Variables Abstract: Boosting is a method for learning a single accurate predictor by linearly\ncombining a set of less accurate weak learners. Recently, structured learning\nhas found many applications in computer vision. Inspired by structured support\nvector machines (SSVM), here we propose a new boosting algorithm for structured\noutput prediction, which we refer to as StructBoost. StructBoost supports\nnonlinear structured learning by combining a set of weak structured learners.\nAs SSVM generalizes SVM, our StructBoost generalizes standard boosting\napproaches such as AdaBoost, or LPBoost to structured learning. The resulting\noptimization problem of StructBoost is more challenging than SSVM in the sense\nthat it may involve exponentially many variables and constraints. In contrast,\nfor SSVM one usually has an exponential number of constraints and a\ncutting-plane method is used. In order to efficiently solve StructBoost, we\nformulate an equivalent $ 1 $-slack formulation and solve it using a\ncombination of cutting planes and column generation. We show the versatility\nand usefulness of StructBoost on a range of problems such as optimizing the\ntree loss for hierarchical multi-class classification, optimizing the Pascal\noverlap criterion for robust visual tracking and learning conditional random\nfield parameters for image segmentation. \n\n"}
{"id": "1303.5145", "contents": "Title: Node-Based Learning of Multiple Gaussian Graphical Models Abstract: We consider the problem of estimating high-dimensional Gaussian graphical\nmodels corresponding to a single set of variables under several distinct\nconditions. This problem is motivated by the task of recovering transcriptional\nregulatory networks on the basis of gene expression data {containing\nheterogeneous samples, such as different disease states, multiple species, or\ndifferent developmental stages}. We assume that most aspects of the conditional\ndependence networks are shared, but that there are some structured differences\nbetween them. Rather than assuming that similarities and differences between\nnetworks are driven by individual edges, we take a node-based approach, which\nin many cases provides a more intuitive interpretation of the network\ndifferences. We consider estimation under two distinct assumptions: (1)\ndifferences between the K networks are due to individual nodes that are\nperturbed across conditions, or (2) similarities among the K networks are due\nto the presence of common hub nodes that are shared across all K networks.\nUsing a row-column overlap norm penalty function, we formulate two convex\noptimization problems that correspond to these two assumptions. We solve these\nproblems using an alternating direction method of multipliers algorithm, and we\nderive a set of necessary and sufficient conditions that allows us to decompose\nthe problem into independent subproblems so that our algorithm can be scaled to\nhigh-dimensional settings. Our proposal is illustrated on synthetic data, a\nwebpage data set, and a brain cancer gene expression data set. \n\n"}
{"id": "1304.0104", "contents": "Title: Meaning-focused and Quantum-inspired Information Retrieval Abstract: In recent years, quantum-based methods have promisingly integrated the\ntraditional procedures in information retrieval (IR) and natural language\nprocessing (NLP). Inspired by our research on the identification and\napplication of quantum structures in cognition, more specifically our work on\nthe representation of concepts and their combinations, we put forward a\n'quantum meaning based' framework for structured query retrieval in text\ncorpora and standardized testing corpora. This scheme for IR rests on\nconsidering as basic notions, (i) 'entities of meaning', e.g., concepts and\ntheir combinations and (ii) traces of such entities of meaning, which is how\ndocuments are considered in this approach. The meaning content of these\n'entities of meaning' is reconstructed by solving an 'inverse problem' in the\nquantum formalism, consisting of reconstructing the full states of the entities\nof meaning from their collapsed states identified as traces in relevant\ndocuments. The advantages with respect to traditional approaches, such as\nLatent Semantic Analysis (LSA), are discussed by means of concrete examples. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1306.2665", "contents": "Title: Precisely Verifying the Null Space Conditions in Compressed Sensing: A\n  Sandwiching Algorithm Abstract: In this paper, we propose new efficient algorithms to verify the null space\ncondition in compressed sensing (CS). Given an $(n-m) \\times n$ ($m>0$) CS\nmatrix $A$ and a positive $k$, we are interested in computing $\\displaystyle\n\\alpha_k = \\max_{\\{z: Az=0,z\\neq 0\\}}\\max_{\\{K: |K|\\leq k\\}}$ ${\\|z_K\n\\|_{1}}{\\|z\\|_{1}}$, where $K$ represents subsets of $\\{1,2,...,n\\}$, and $|K|$\nis the cardinality of $K$. In particular, we are interested in finding the\nmaximum $k$ such that $\\alpha_k < {1}{2}$. However, computing $\\alpha_k$ is\nknown to be extremely challenging. In this paper, we first propose a series of\nnew polynomial-time algorithms to compute upper bounds on $\\alpha_k$. Based on\nthese new polynomial-time algorithms, we further design a new sandwiching\nalgorithm, to compute the \\emph{exact} $\\alpha_k$ with greatly reduced\ncomplexity. When needed, this new sandwiching algorithm also achieves a smooth\ntradeoff between computational complexity and result accuracy. Empirical\nresults show the performance improvements of our algorithm over existing known\nmethods; and our algorithm outputs precise values of $\\alpha_k$, with much\nlower complexity than exhaustive search. \n\n"}
{"id": "1307.1493", "contents": "Title: Dropout Training as Adaptive Regularization Abstract: Dropout and other feature noising schemes control overfitting by artificially\ncorrupting the training data. For generalized linear models, dropout performs a\nform of adaptive regularization. Using this viewpoint, we show that the dropout\nregularizer is first-order equivalent to an L2 regularizer applied after\nscaling the features by an estimate of the inverse diagonal Fisher information\nmatrix. We also establish a connection to AdaGrad, an online learning\nalgorithm, and find that a close relative of AdaGrad operates by repeatedly\nsolving linear dropout-regularized problems. By casting dropout as\nregularization, we develop a natural semi-supervised algorithm that uses\nunlabeled data to create a better adaptive regularizer. We apply this idea to\ndocument classification tasks, and show that it consistently boosts the\nperformance of dropout training, improving on state-of-the-art results on the\nIMDB reviews dataset. \n\n"}
{"id": "1308.1006", "contents": "Title: Fast Semidifferential-based Submodular Function Optimization Abstract: We present a practical and powerful new framework for both unconstrained and\nconstrained submodular function optimization based on discrete\nsemidifferentials (sub- and super-differentials). The resulting algorithms,\nwhich repeatedly compute and then efficiently optimize submodular\nsemigradients, offer new and generalize many old methods for submodular\noptimization. Our approach, moreover, takes steps towards providing a unifying\nparadigm applicable to both submodular min- imization and maximization,\nproblems that historically have been treated quite distinctly. The practicality\nof our algorithms is important since interest in submodularity, owing to its\nnatural and wide applicability, has recently been in ascendance within machine\nlearning. We analyze theoretical properties of our algorithms for minimization\nand maximization, and show that many state-of-the-art maximization algorithms\nare special cases. Lastly, we complement our theoretical analyses with\nsupporting empirical experiments. \n\n"}
{"id": "1308.1507", "contents": "Title: Logical analysis of natural language semantics to solve the problem of\n  computer understanding Abstract: An object--oriented approach to create a natural language understanding\nsystem is considered. The understanding program is a formal system built on the\nbase of predicative calculus. Horn's clauses are used as well--formed formulas.\nAn inference is based on the principle of resolution. Sentences of natural\nlanguage are represented in the view of typical predicate set. These predicates\ndescribe physical objects and processes, abstract objects, categories and\nsemantic relations between objects. Predicates for concrete assertions are\nsaved in a database. To describe the semantics of classes for physical objects,\nabstract concepts and processes, a knowledge base is applied. The proposed\nrepresentation of natural language sentences is a semantic net. Nodes of such\nnet are typical predicates. This approach is perspective as, firstly, such\ntypification of nodes facilitates essentially forming of processing algorithms\nand object descriptions, secondly, the effectiveness of algorithms is increased\n(particularly for the great number of nodes), thirdly, to describe the\nsemantics of words, encyclopedic knowledge is used, and this permits\nessentially to extend the class of solved problems. \n\n"}
{"id": "1309.0671", "contents": "Title: BayesOpt: A Library for Bayesian optimization with Robotics Applications Abstract: The purpose of this paper is twofold. On one side, we present a general\nframework for Bayesian optimization and we compare it with some related fields\nin active learning and Bayesian numerical analysis. On the other hand, Bayesian\noptimization and related problems (bandits, sequential experimental design) are\nhighly dependent on the surrogate model that is selected. However, there is no\nclear standard in the literature. Thus, we present a fast and flexible toolbox\nthat allows to test and combine different models and criteria with little\neffort. It includes most of the state-of-the-art contributions, algorithms and\nmodels. Its speed also removes part of the stigma that Bayesian optimization\nmethods are only good for \"expensive functions\". The software is free and it\ncan be used in many operating systems and computer languages. \n\n"}
{"id": "1309.3533", "contents": "Title: Mixed Membership Models for Time Series Abstract: In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework. \n\n"}
{"id": "1310.1533", "contents": "Title: CAM: Causal additive models, high-dimensional order search and penalized\n  regression Abstract: We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data. \n\n"}
{"id": "1310.1949", "contents": "Title: Least Squares Revisited: Scalable Approaches for Multi-class Prediction Abstract: This work provides simple algorithms for multi-class (and multi-label)\nprediction in settings where both the number of examples n and the data\ndimension d are relatively large. These robust and parameter free algorithms\nare essentially iterative least-squares updates and very versatile both in\ntheory and in practice. On the theoretical front, we present several variants\nwith convergence guarantees. Owing to their effective use of second-order\nstructure, these algorithms are substantially better than first-order methods\nin many practical scenarios. On the empirical side, we present a scalable\nstagewise variant of our approach, which achieves dramatic computational\nspeedups over popular optimization packages such as Liblinear and Vowpal Wabbit\non standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art\naccuracies. \n\n"}
{"id": "1311.1194", "contents": "Title: Identifying Purpose Behind Electoral Tweets Abstract: Tweets pertaining to a single event, such as a national election, can number\nin the hundreds of millions. Automatically analyzing them is beneficial in many\ndownstream natural language applications such as question answering and\nsummarization. In this paper, we propose a new task: identifying the purpose\nbehind electoral tweets--why do people post election-oriented tweets? We show\nthat identifying purpose is correlated with the related phenomenon of sentiment\nand emotion detection, but yet significantly different. Detecting purpose has a\nnumber of applications including detecting the mood of the electorate,\nestimating the popularity of policies, identifying key issues of contention,\nand predicting the course of events. We create a large dataset of electoral\ntweets and annotate a few thousand tweets for purpose. We develop a system that\nautomatically classifies electoral tweets as per their purpose, obtaining an\naccuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class\ntask (both accuracies well above the most-frequent-class baseline). Finally, we\nshow that resources developed for emotion detection are also helpful for\ndetecting purpose. \n\n"}
{"id": "1311.1780", "contents": "Title: Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks Abstract: In this paper we propose and investigate a novel nonlinear unit, called $L_p$\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\nseveral projections of a subset of units in the layer below and computes a\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\nunit. First, the proposed unit can be understood as a generalization of a\nnumber of conventional pooling operators such as average, root-mean-square and\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\nwhich achieved the state-of-the-art object recognition results on a number of\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\nactivation function based on which we argue that the $L_p$ unit is more\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\nunit defines a superelliptic boundary, with its exact shape defined by the\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\ncurved boundaries more efficiently by combining a few $L_p$ units of different\norders. This insight justifies the need for learning different orders for each\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\nunits achieve the state-of-the-art results on a number of benchmark datasets.\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\nrecurrent neural networks (RNN). \n\n"}
{"id": "1311.6107", "contents": "Title: Off-policy reinforcement learning for $ H_\\infty $ control design Abstract: The $H_\\infty$ control design problem is considered for nonlinear systems\nwith unknown internal system model. It is known that the nonlinear $ H_\\infty $\ncontrol problem can be transformed into solving the so-called\nHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial\ndifferential equation that is generally impossible to be solved analytically.\nEven worse, model-based approaches cannot be used for approximately solving HJI\nequation, when the accurate system model is unavailable or costly to obtain in\npractice. To overcome these difficulties, an off-policy reinforcement leaning\n(RL) method is introduced to learn the solution of HJI equation from real\nsystem data instead of mathematical system model, and its convergence is\nproved. In the off-policy RL method, the system data can be generated with\narbitrary policies rather than the evaluating policy, which is extremely\nimportant and promising for practical systems. For implementation purpose, a\nneural network (NN) based actor-critic structure is employed and a least-square\nNN weight update algorithm is derived based on the method of weighted\nresiduals. Finally, the developed NN-based off-policy RL method is tested on a\nlinear F16 aircraft plant, and further applied to a rotational/translational\nactuator system. \n\n"}
{"id": "1312.0493", "contents": "Title: Bidirectional Recursive Neural Networks for Token-Level Labeling with\n  Structure Abstract: Recently, deep architectures, such as recurrent and recursive neural networks\nhave been successfully applied to various natural language processing tasks.\nInspired by bidirectional recurrent neural networks which use representations\nthat summarize the past and future around an instance, we propose a novel\narchitecture that aims to capture the structural information around an input,\nand use it to label instances. We apply our method to the task of opinion\nexpression extraction, where we employ the binary parse tree of a sentence as\nthe structure, and word vector representations as the initial representation of\na single token. We conduct preliminary experiments to investigate its\nperformance and compare it to the sequential approach. \n\n"}
{"id": "1312.4314", "contents": "Title: Learning Factored Representations in a Deep Mixture of Experts Abstract: Mixtures of Experts combine the outputs of several \"expert\" networks, each of\nwhich specializes in a different part of the input space. This is achieved by\ntraining a \"gating\" network that maps each input to a distribution over the\nexperts. Such models show promise for building larger networks that are still\ncheap to compute at test time, and more parallelizable at training time. In\nthis this work, we extend the Mixture of Experts to a stacked model, the Deep\nMixture of Experts, with multiple sets of gating and experts. This\nexponentially increases the number of effective experts by associating each\ninput with a combination of experts at each layer, yet maintains a modest model\nsize. On a randomly translated version of the MNIST dataset, we find that the\nDeep Mixture of Experts automatically learns to develop location-dependent\n(\"where\") experts at the first layer, and class-specific (\"what\") experts at\nthe second layer. In addition, we see that the different combinations are in\nuse when the model is applied to a dataset of speech monophones. These\ndemonstrate effective use of all expert combinations. \n\n"}
{"id": "1312.5542", "contents": "Title: Word Emdeddings through Hellinger PCA Abstract: Word embeddings resulting from neural language models have been shown to be\nsuccessful for a large variety of NLP tasks. However, such architecture might\nbe difficult to train and time-consuming. Instead, we propose to drastically\nsimplify the word embeddings computation through a Hellinger PCA of the word\nco-occurence matrix. We compare those new word embeddings with some well-known\nembeddings on NER and movie review tasks and show that we can reach similar or\neven better performance. Although deep learning is not really necessary for\ngenerating good word embeddings, we show that it can provide an easy way to\nadapt embeddings to specific tasks. \n\n"}
{"id": "1312.5604", "contents": "Title: Learning Transformations for Classification Forests Abstract: This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework. \n\n"}
{"id": "1312.5921", "contents": "Title: Group-sparse Embeddings in Collective Matrix Factorization Abstract: CMF is a technique for simultaneously learning low-rank representations based\non a collection of matrices with shared entities. A typical example is the\njoint modeling of user-item, item-property, and user-feature matrices in a\nrecommender system. The key idea in CMF is that the embeddings are shared\nacross the matrices, which enables transferring information between them. The\nexisting solutions, however, break down when the individual matrices have\nlow-rank structure not shared with others. In this work we present a novel CMF\nsolution that allows each of the matrices to have a separate low-rank structure\nthat is independent of the other matrices, as well as structures that are\nshared only by a subset of them. We compare MAP and variational Bayesian\nsolutions based on alternating optimization algorithms and show that the model\nautomatically infers the nature of each factor using group-wise sparsity. Our\napproach supports in a principled way continuous, binary and count observations\nand is efficient for sparse matrices involving missing data. We illustrate the\nsolution on a number of examples, focusing in particular on an interesting\nuse-case of augmented multi-view learning. \n\n"}
{"id": "1312.7006", "contents": "Title: A Convex Formulation for Mixed Regression with Two Components: Minimax\n  Optimal Rates Abstract: We consider the mixed regression problem with two components, under\nadversarial and stochastic noise. We give a convex optimization formulation\nthat provably recovers the true solution, and provide upper bounds on the\nrecovery errors for both arbitrary noise and stochastic noise settings. We also\ngive matching minimax lower bounds (up to log factors), showing that under\ncertain assumptions, our algorithm is information-theoretically optimal. Our\nresults represent the first tractable algorithm guaranteeing successful\nrecovery with tight bounds on recovery errors and sample complexity. \n\n"}
{"id": "1401.0579", "contents": "Title: More Algorithms for Provable Dictionary Learning Abstract: In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration. \n\n"}
{"id": "1402.3371", "contents": "Title: An evaluative baseline for geo-semantic relatedness and similarity Abstract: In geographic information science and semantics, the computation of semantic\nsimilarity is widely recognised as key to supporting a vast number of tasks in\ninformation integration and retrieval. By contrast, the role of geo-semantic\nrelatedness has been largely ignored. In natural language processing, semantic\nrelatedness is often confused with the more specific semantic similarity. In\nthis article, we discuss a notion of geo-semantic relatedness based on Lehrer's\nsemantic fields, and we compare it with geo-semantic similarity. We then\ndescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a\nnew open dataset designed to evaluate computational measures of geo-semantic\nrelatedness and similarity. This dataset is larger than existing datasets of\nthis kind, and includes 97 geographic terms combined into 50 term pairs rated\nby 203 human subjects. GeReSiD is available online and can be used as an\nevaluation baseline to determine empirically to what degree a given\ncomputational model approximates geo-semantic relatedness and similarity. \n\n"}
{"id": "1402.3511", "contents": "Title: A Clockwork RNN Abstract: Sequence prediction and classification are ubiquitous and challenging\nproblems in machine learning that can require identifying complex dependencies\nbetween temporally distant inputs. Recurrent Neural Networks (RNNs) have the\nability, in theory, to cope with these temporal dependencies by virtue of the\nshort-term memory implemented by their recurrent (feedback) connections.\nHowever, in practice they are difficult to train successfully when the\nlong-term memory is required. This paper introduces a simple, yet powerful\nmodification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in\nwhich the hidden layer is partitioned into separate modules, each processing\ninputs at its own temporal granularity, making computations only at its\nprescribed clock rate. Rather than making the standard RNN models more complex,\nCW-RNN reduces the number of RNN parameters, improves the performance\nsignificantly in the tasks tested, and speeds up the network evaluation. The\nnetwork is demonstrated in preliminary experiments involving two tasks: audio\nsignal generation and TIMIT spoken word classification, where it outperforms\nboth RNN and LSTM networks. \n\n"}
{"id": "1402.4306", "contents": "Title: Student-t Processes as Alternatives to Gaussian Processes Abstract: We investigate the Student-t process as an alternative to the Gaussian\nprocess as a nonparametric prior over functions. We derive closed form\nexpressions for the marginal likelihood and predictive distribution of a\nStudent-t process, by integrating away an inverse Wishart process prior over\nthe covariance kernel of a Gaussian process model. We show surprising\nequivalences between different hierarchical Gaussian process models leading to\nStudent-t processes, and derive a new sampling scheme for the inverse Wishart\nprocess, which helps elucidate these equivalences. Overall, we show that a\nStudent-t process can retain the attractive properties of a Gaussian process --\na nonparametric representation, analytic marginal and predictive distributions,\nand easy model selection through covariance kernels -- but has enhanced\nflexibility, and predictive covariances that, unlike a Gaussian process,\nexplicitly depend on the values of training observations. We verify empirically\nthat a Student-t process is especially useful in situations where there are\nchanges in covariance structure, or in applications like Bayesian optimization,\nwhere accurate predictive covariances are critical for good performance. These\nadvantages come at no additional computational cost over Gaussian processes. \n\n"}
{"id": "1403.1600", "contents": "Title: Collaborative Filtering with Information-Rich and Information-Sparse\n  Entities Abstract: In this paper, we consider a popular model for collaborative filtering in\nrecommender systems where some users of a website rate some items, such as\nmovies, and the goal is to recover the ratings of some or all of the unrated\nitems of each user. In particular, we consider both the clustering model, where\nonly users (or items) are clustered, and the co-clustering model, where both\nusers and items are clustered, and further, we assume that some users rate many\nitems (information-rich users) and some users rate only a few items\n(information-sparse users). When users (or items) are clustered, our algorithm\ncan recover the rating matrix with $\\omega(MK \\log M)$ noisy entries while $MK$\nentries are necessary, where $K$ is the number of clusters and $M$ is the\nnumber of items. In the case of co-clustering, we prove that $K^2$ entries are\nnecessary for recovering the rating matrix, and our algorithm achieves this\nlower bound within a logarithmic factor when $K$ is sufficiently large. We\ncompare our algorithms with a well-known algorithms called alternating\nminimization (AM), and a similarity score-based algorithm known as the\npopularity-among-friends (PAF) algorithm by applying all three to the MovieLens\nand Netflix data sets. Our co-clustering algorithm and AM have similar overall\nerror rates when recovering the rating matrix, both of which are lower than the\nerror rate under PAF. But more importantly, the error rate of our co-clustering\nalgorithm is significantly lower than AM and PAF in the scenarios of interest\nin recommender systems: when recommending a few items to each user or when\nrecommending items to users who only rated a few items (these users are the\nmajority of the total user population). The performance difference increases\neven more when noise is added to the datasets. \n\n"}
{"id": "1403.3460", "contents": "Title: Scalable and Robust Construction of Topical Hierarchies Abstract: Automated generation of high-quality topical hierarchies for a text\ncollection is a dream problem in knowledge engineering with many valuable\napplications. In this paper a scalable and robust algorithm is proposed for\nconstructing a hierarchy of topics from a text collection. We divide and\nconquer the problem using a top-down recursive framework, based on a tensor\northogonal decomposition technique. We solve a critical challenge to perform\nscalable inference for our newly designed hierarchical topic model. Experiments\nwith various real-world datasets illustrate its ability to generate robust,\nhigh-quality hierarchies efficiently. Our method reduces the time of\nconstruction by several orders of magnitude, and its robust feature renders it\npossible for users to interactively revise the hierarchy. \n\n"}
{"id": "1404.3184", "contents": "Title: Decreasing Weighted Sorted $\\ell_1$ Regularization Abstract: We consider a new family of regularizers, termed {\\it weighted sorted\n$\\ell_1$ norms} (WSL1), which generalizes the recently introduced {\\it\noctagonal shrinkage and clustering algorithm for regression} (OSCAR) and also\ncontains the $\\ell_1$ and $\\ell_{\\infty}$ norms as particular instances. We\nfocus on a special case of the WSL1, the {\\sl decreasing WSL1} (DWSL1), where\nthe elements of the argument vector are sorted in non-increasing order and the\nweights are also non-increasing. In this paper, after showing that the DWSL1 is\nindeed a norm, we derive two key tools for its use as a regularizer: the dual\nnorm and the Moreau proximity operator. \n\n"}
{"id": "1404.4714", "contents": "Title: Radical-Enhanced Chinese Character Embedding Abstract: We present a method to leverage radical for learning Chinese character\nembedding. Radical is a semantic and phonetic component of Chinese character.\nIt plays an important role as characters with the same radical usually have\nsimilar semantic meaning and grammatical usage. However, existing Chinese\nprocessing algorithms typically regard word or character as the basic unit but\nignore the crucial radical information. In this paper, we fill this gap by\nleveraging radical for learning continuous representation of Chinese character.\nWe develop a dedicated neural architecture to effectively learn character\nembedding and apply it on Chinese character similarity judgement and Chinese\nword segmentation. Experiment results show that our radical-enhanced method\noutperforms existing embedding learning algorithms on both tasks. \n\n"}
{"id": "1404.7296", "contents": "Title: A Deep Architecture for Semantic Parsing Abstract: Many successful approaches to semantic parsing build on top of the syntactic\nanalysis of text, and make use of distributional representations or statistical\nmodels to match parses to ontology-specific queries. This paper presents a\nnovel deep learning architecture which provides a semantic parsing system\nthrough the union of two neural models of language semantics. It allows for the\ngeneration of ontology-specific queries from natural language statements and\nquestions without the need for parsing, which makes it especially suitable to\ngrammatically malformed or syntactically atypical text, such as tweets, as well\nas permitting the development of semantic parsers for resource-poor languages. \n\n"}
{"id": "1405.3396", "contents": "Title: Reducing Dueling Bandits to Cardinal Bandits Abstract: We present algorithms for reducing the Dueling Bandits problem to the\nconventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits\nproblem is an online model of learning with ordinal feedback of the form \"A is\npreferred to B\" (as opposed to cardinal feedback like \"A has value 2.5\"),\ngiving it wide applicability in learning from implicit user feedback and\nrevealed and stated preferences. In contrast to existing algorithms for the\nDueling Bandits problem, our reductions -- named $\\Doubler$, $\\MultiSbm$ and\n$\\DoubleSbm$ -- provide a generic schema for translating the extensive body of\nknown results about conventional Multi-Armed Bandit algorithms to the Dueling\nBandits setting. For $\\Doubler$ and $\\MultiSbm$ we prove regret upper bounds in\nboth finite and infinite settings, and conjecture about the performance of\n$\\DoubleSbm$ which empirically outperforms the other two as well as previous\nalgorithms in our experiments. In addition, we provide the first almost optimal\nregret bound in terms of second order terms, such as the differences between\nthe values of the arms. \n\n"}
{"id": "1405.3726", "contents": "Title: Topic words analysis based on LDA model Abstract: Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected. \n\n"}
{"id": "1405.7910", "contents": "Title: Optimal CUR Matrix Decompositions Abstract: The CUR decomposition of an $m \\times n$ matrix $A$ finds an $m \\times c$\nmatrix $C$ with a subset of $c < n$ columns of $A,$ together with an $r \\times\nn$ matrix $R$ with a subset of $r < m$ rows of $A,$ as well as a $c \\times r$\nlow-rank matrix $U$ such that the matrix $C U R$ approximates the matrix $A,$\nthat is, $ || A - CUR ||_F^2 \\le (1+\\epsilon) || A - A_k||_F^2$, where\n$||.||_F$ denotes the Frobenius norm and $A_k$ is the best $m \\times n$ matrix\nof rank $k$ constructed via the SVD. We present input-sparsity-time and\ndeterministic algorithms for constructing such a CUR decomposition where\n$c=O(k/\\epsilon)$ and $r=O(k/\\epsilon)$ and rank$(U) = k$. Up to constant\nfactors, our algorithms are simultaneously optimal in $c, r,$ and rank$(U)$. \n\n"}
{"id": "1406.3884", "contents": "Title: Learning An Invariant Speech Representation Abstract: Recognition of speech, and in particular the ability to generalize and learn\nfrom small sets of labelled examples like humans do, depends on an appropriate\nrepresentation of the acoustic input. We formulate the problem of finding\nrobust speech features for supervised learning with small sample complexity as\na problem of learning representations of the signal that are maximally\ninvariant to intraclass transformations and deformations. We propose an\nextension of a theory for unsupervised learning of invariant visual\nrepresentations to the auditory domain and empirically evaluate its validity\nfor voiced speech sound classification. Our version of the theory requires the\nmemory-based, unsupervised storage of acoustic templates -- such as specific\nphones or words -- together with all the transformations of each that normally\noccur. A quasi-invariant representation for a speech segment can be obtained by\nprojecting it to each template orbit, i.e., the set of transformed signals, and\ncomputing the associated one-dimensional empirical probability distributions.\nThe computations can be performed by modules of filtering and pooling, and\nextended to hierarchical architectures. In this paper, we apply a single-layer,\nmulticomponent representation for phonemes and demonstrate improved accuracy\nand decreased sample complexity for vowel classification compared to standard\nspectral, cepstral and perceptual features. \n\n"}
{"id": "1406.3926", "contents": "Title: Bayesian Optimal Control of Smoothly Parameterized Systems: The Lazy\n  Posterior Sampling Algorithm Abstract: We study Bayesian optimal control of a general class of smoothly\nparameterized Markov decision problems. Since computing the optimal control is\ncomputationally expensive, we design an algorithm that trades off performance\nfor computational efficiency. The algorithm is a lazy posterior sampling method\nthat maintains a distribution over the unknown parameter. The algorithm changes\nits policy only when the variance of the distribution is reduced sufficiently.\nImportantly, we analyze the algorithm and show the precise nature of the\nperformance vs. computation tradeoff. Finally, we show the effectiveness of the\nmethod on a web server control application. \n\n"}
{"id": "1406.4444", "contents": "Title: PRISM: Person Re-Identification via Structured Matching Abstract: Person re-identification (re-id), an emerging problem in visual surveillance,\ndeals with maintaining entities of individuals whilst they traverse various\nlocations surveilled by a camera network. From a visual perspective re-id is\nchallenging due to significant changes in visual appearance of individuals in\ncameras with different pose, illumination and calibration. Globally the\nchallenge arises from the need to maintain structurally consistent matches\namong all the individual entities across different camera views. We propose\nPRISM, a structured matching method to jointly account for these challenges. We\nview the global problem as a weighted graph matching problem and estimate edge\nweights by learning to predict them based on the co-occurrences of visual\npatterns in the training examples. These co-occurrence based scores in turn\naccount for appearance changes by inferring likely and unlikely visual\nco-occurrences appearing in training instances. We implement PRISM on single\nshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in\nterms of matching rate while being computationally efficient. \n\n"}
{"id": "1406.5429", "contents": "Title: Playing with Duality: An Overview of Recent Primal-Dual Approaches for\n  Solving Large-Scale Optimization Problems Abstract: Optimization methods are at the core of many problems in signal/image\nprocessing, computer vision, and machine learning. For a long time, it has been\nrecognized that looking at the dual of an optimization problem may drastically\nsimplify its solution. Deriving efficient strategies which jointly brings into\nplay the primal and the dual problems is however a more recent idea which has\ngenerated many important new contributions in the last years. These novel\ndevelopments are grounded on recent advances in convex analysis, discrete\noptimization, parallel processing, and non-smooth optimization with emphasis on\nsparsity issues. In this paper, we aim at presenting the principles of\nprimal-dual approaches, while giving an overview of numerical methods which\nhave been proposed in different contexts. We show the benefits which can be\ndrawn from primal-dual algorithms both for solving large-scale convex\noptimization problems and discrete ones, and we provide various application\nexamples to illustrate their usefulness. \n\n"}
{"id": "1407.0753", "contents": "Title: Global convergence of splitting methods for nonconvex composite\n  optimization Abstract: We consider the problem of minimizing the sum of a smooth function $h$ with a\nbounded Hessian, and a nonsmooth function. We assume that the latter function\nis a composition of a proper closed function $P$ and a surjective linear map\n$\\cal M$, with the proximal mappings of $\\tau P$, $\\tau > 0$, simple to\ncompute. This problem is nonconvex in general and encompasses many important\napplications in engineering and machine learning. In this paper, we examined\ntwo types of splitting methods for solving this nonconvex optimization problem:\nalternating direction method of multipliers and proximal gradient algorithm.\nFor the direct adaptation of the alternating direction method of multipliers,\nwe show that, if the penalty parameter is chosen sufficiently large and the\nsequence generated has a cluster point, then it gives a stationary point of the\nnonconvex problem. We also establish convergence of the whole sequence under an\nadditional assumption that the functions $h$ and $P$ are semi-algebraic.\nFurthermore, we give simple sufficient conditions to guarantee boundedness of\nthe sequence generated. These conditions can be satisfied for a wide range of\napplications including the least squares problem with the $\\ell_{1/2}$\nregularization. Finally, when $\\cal M$ is the identity so that the proximal\ngradient algorithm can be efficiently applied, we show that any cluster point\nis stationary under a slightly more flexible constant step-size rule than what\nis known in the literature for a nonconvex $h$. \n\n"}
{"id": "1407.4070", "contents": "Title: Fast matrix completion without the condition number Abstract: We give the first algorithm for Matrix Completion whose running time and\nsample complexity is polynomial in the rank of the unknown target matrix,\nlinear in the dimension of the matrix, and logarithmic in the condition number\nof the matrix. To the best of our knowledge, all previous algorithms either\nincurred a quadratic dependence on the condition number of the unknown matrix\nor a quadratic dependence on the dimension of the matrix in the running time.\nOur algorithm is based on a novel extension of Alternating Minimization which\nwe show has theoretical guarantees under standard assumptions even in the\npresence of noise. \n\n"}
{"id": "1407.6267", "contents": "Title: Learning in games via reinforcement and regularization Abstract: We investigate a class of reinforcement learning dynamics where players\nadjust their strategies based on their actions' cumulative payoffs over time -\nspecifically, by playing mixed strategies that maximize their expected\ncumulative payoff minus a regularization term. A widely studied example is\nexponential reinforcement learning, a process induced by an entropic\nregularization term which leads mixed strategies to evolve according to the\nreplicator dynamics. However, in contrast to the class of regularization\nfunctions used to define smooth best responses in models of stochastic\nfictitious play, the functions used in this paper need not be infinitely steep\nat the boundary of the simplex; in fact, dropping this requirement gives rise\nto an important dichotomy between steep and nonsteep cases. In this general\nframework, we extend several properties of exponential learning, including the\nelimination of dominated strategies, the asymptotic stability of strict Nash\nequilibria, and the convergence of time-averaged trajectories in zero-sum games\nwith an interior Nash equilibrium. \n\n"}
{"id": "1408.0017", "contents": "Title: Learning Nash Equilibria in Congestion Games Abstract: We study the repeated congestion game, in which multiple populations of\nplayers share resources, and make, at each iteration, a decentralized decision\non which resources to utilize. We investigate the following question: given a\nmodel of how individual players update their strategies, does the resulting\ndynamics of strategy profiles converge to the set of Nash equilibria of the\none-shot game? We consider in particular a model in which players update their\nstrategies using algorithms with sublinear discounted regret. We show that the\nresulting sequence of strategy profiles converges to the set of Nash equilibria\nin the sense of Ces\\`aro means. However, strong convergence is not guaranteed\nin general. We show that strong convergence can be guaranteed for a class of\nalgorithms with a vanishing upper bound on discounted regret, and which satisfy\nan additional condition. We call such algorithms AREP algorithms, for\nApproximate REPlicator, as they can be interpreted as a discrete-time\napproximation of the replicator equation, which models the continuous-time\nevolution of population strategies, and which is known to converge for the\nclass of congestion games. In particular, we show that the discounted Hedge\nalgorithm belongs to the AREP class, which guarantees its strong convergence. \n\n"}
{"id": "1409.3821", "contents": "Title: Computational Implications of Reducing Data to Sufficient Statistics Abstract: Given a large dataset and an estimation task, it is common to pre-process the\ndata by reducing them to a set of sufficient statistics. This step is often\nregarded as straightforward and advantageous (in that it simplifies statistical\nanalysis). I show that -on the contrary- reducing data to sufficient statistics\ncan change a computationally tractable estimation problem into an intractable\none. I discuss connections with recent work in theoretical computer science,\nand implications for some techniques to estimate graphical models. \n\n"}
{"id": "1410.3351", "contents": "Title: Ricci Curvature and the Manifold Learning Problem Abstract: Consider a sample of $n$ points taken i.i.d from a submanifold $\\Sigma$ of\nEuclidean space. We show that there is a way to estimate the Ricci curvature of\n$\\Sigma$ with respect to the induced metric from the sample. Our method is\ngrounded in the notions of Carr\\'e du Champ for diffusion semi-groups, the\ntheory of Empirical processes and local Principal Component Analysis. \n\n"}
{"id": "1411.0972", "contents": "Title: Convex Optimization for Big Data Abstract: This article reviews recent advances in convex optimization algorithms for\nBig Data, which aim to reduce the computational, storage, and communications\nbottlenecks. We provide an overview of this emerging field, describe\ncontemporary approximation techniques like first-order methods and\nrandomization for scalability, and survey the important role of parallel and\ndistributed computation. The new Big Data algorithms are based on surprisingly\nsimple principles and attain staggering accelerations even on classical\nproblems. \n\n"}
{"id": "1411.3698", "contents": "Title: Minimal Realization Problems for Hidden Markov Models Abstract: Consider a stationary discrete random process with alphabet size d, which is\nassumed to be the output process of an unknown stationary Hidden Markov Model\n(HMM). Given the joint probabilities of finite length strings of the process,\nwe are interested in finding a finite state generative model to describe the\nentire process. In particular, we focus on two classes of models: HMMs and\nquasi-HMMs, which is a strictly larger class of models containing HMMs. In the\nmain theorem, we show that if the random process is generated by an HMM of\norder less or equal than k, and whose transition and observation probability\nmatrix are in general position, namely almost everywhere on the parameter\nspace, both the minimal quasi-HMM realization and the minimal HMM realization\ncan be efficiently computed based on the joint probabilities of all the length\nN strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to\ncompare and connect the two lines of literature: realization theory of HMMs,\nand the recent development in learning latent variable models with tensor\ndecomposition techniques. \n\n"}
{"id": "1412.2106", "contents": "Title: Consistent optimization of AMS by logistic loss minimization Abstract: In this paper, we theoretically justify an approach popular among\nparticipants of the Higgs Boson Machine Learning Challenge to optimize\napproximate median significance (AMS). The approach is based on the following\ntwo-stage procedure. First, a real-valued function is learned by minimizing a\nsurrogate loss for binary classification, such as logistic loss, on the\ntraining sample. Then, a threshold is tuned on a separate validation sample, by\ndirect optimization of AMS. We show that the regret of the resulting\n(thresholded) classifier measured with respect to the squared AMS, is\nupperbounded by the regret of the underlying real-valued function measured with\nrespect to the logistic loss. Hence, we prove that minimizing logistic\nsurrogate is a consistent method of optimizing AMS. \n\n"}
{"id": "1412.3489", "contents": "Title: Quantum Deep Learning Abstract: In recent years, deep learning has had a profound impact on machine learning\nand artificial intelligence. At the same time, algorithms for quantum computers\nhave been shown to efficiently solve some problems that are intractable on\nconventional, classical computers. We show that quantum computing not only\nreduces the time required to train a deep restricted Boltzmann machine, but\nalso provides a richer and more comprehensive framework for deep learning than\nclassical computing and leads to significant improvements in the optimization\nof the underlying objective function. Our quantum methods also permit efficient\ntraining of full Boltzmann machines and multi-layer, fully connected models and\ndo not have well known classical counterparts. \n\n"}
{"id": "1412.3714", "contents": "Title: Feature Weight Tuning for Recursive Neural Networks Abstract: This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models. \n\n"}
{"id": "1412.4182", "contents": "Title: The Statistics of Streaming Sparse Regression Abstract: We present a sparse analogue to stochastic gradient descent that is\nguaranteed to perform well under similar conditions to the lasso. In the linear\nregression setup with irrepresentable noise features, our algorithm recovers\nthe support set of the optimal parameter vector with high probability, and\nachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),\nwhere k is the sparsity of the solution, d is the number of features, and T is\nthe number of training examples. Meanwhile, our algorithm does not require any\nmore computational resources than stochastic gradient descent. In our\nexperiments, we find that our method substantially out-performs existing\nstreaming algorithms on both real and simulated data. \n\n"}
{"id": "1412.6418", "contents": "Title: Inducing Semantic Representation from Text by Jointly Predicting and\n  Factorizing Relations Abstract: In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage. \n\n"}
{"id": "1412.6558", "contents": "Title: Random Walk Initialization for Training Very Deep Feedforward Networks Abstract: Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases. \n\n"}
{"id": "1412.6568", "contents": "Title: Improving zero-shot learning by mitigating the hubness problem Abstract: The zero-shot paradigm exploits vector-based word representations extracted\nfrom text corpora with unsupervised methods to learn general mapping functions\nfrom other feature spaces onto word space, where the words associated to the\nnearest neighbours of the mapped vectors are used as their linguistic labels.\nWe show that the neighbourhoods of the mapped elements are strongly polluted by\nhubs, vectors that tend to be near a high proportion of items, pushing their\ncorrect labels down the neighbour list. After illustrating the problem\nempirically, we propose a simple method to correct it by taking the proximity\ndistribution of potential neighbours across many mapped vectors into account.\nWe show that this correction leads to consistent improvements in realistic\nzero-shot experiments in the cross-lingual, image labeling and image retrieval\ndomains. \n\n"}
{"id": "1412.6821", "contents": "Title: A Stable Multi-Scale Kernel for Topological Machine Learning Abstract: Topological data analysis offers a rich source of valuable information to\nstudy vision problems. Yet, so far we lack a theoretically sound connection to\npopular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In\nthis work, we establish such a connection by designing a multi-scale kernel for\npersistence diagrams, a stable summary representation of topological features\nin data. We show that this kernel is positive definite and prove its stability\nwith respect to the 1-Wasserstein distance. Experiments on two benchmark\ndatasets for 3D shape classification/retrieval and texture recognition show\nconsiderable performance gains of the proposed method compared to an\nalternative approach that is based on the recently introduced persistence\nlandscapes. \n\n"}
{"id": "1412.7449", "contents": "Title: Grammar as a Foreign Language Abstract: Syntactic constituency parsing is a fundamental problem in natural language\nprocessing and has been the subject of intensive research and engineering for\ndecades. As a result, the most accurate parsers are domain specific, complex,\nand inefficient. In this paper we show that the domain agnostic\nattention-enhanced sequence-to-sequence model achieves state-of-the-art results\non the most widely used syntactic constituency parsing dataset, when trained on\na large synthetic corpus that was annotated using existing parsers. It also\nmatches the performance of standard parsers when trained only on a small\nhuman-annotated dataset, which shows that this model is highly data-efficient,\nin contrast to sequence-to-sequence models without the attention mechanism. Our\nparser is also fast, processing over a hundred sentences per second with an\nunoptimized CPU implementation. \n\n"}
{"id": "1501.03227", "contents": "Title: Using Riemannian geometry for SSVEP-based Brain Computer Interface Abstract: Riemannian geometry has been applied to Brain Computer Interface (BCI) for\nbrain signals classification yielding promising results. Studying\nelectroencephalographic (EEG) signals from their associated covariance matrices\nallows a mitigation of common sources of variability (electronic, electrical,\nbiological) by constructing a representation which is invariant to these\nperturbations. While working in Euclidean space with covariance matrices is\nknown to be error-prone, one might take advantage of algorithmic advances in\ninformation geometry and matrix manifold to implement methods for Symmetric\nPositive-Definite (SPD) matrices. This paper proposes a comprehensive review of\nthe actual tools of information geometry and how they could be applied on\ncovariance matrices of EEG. In practice, covariance matrices should be\nestimated, thus a thorough study of all estimators is conducted on real EEG\ndataset. As a main contribution, this paper proposes an online implementation\nof a classifier in the Riemannian space and its subsequent assessment in\nSteady-State Visually Evoked Potential (SSVEP) experimentations. \n\n"}
{"id": "1501.06241", "contents": "Title: Sequential Sensing with Model Mismatch Abstract: We characterize the performance of sequential information guided sensing,\nInfo-Greedy Sensing, when there is a mismatch between the true signal model and\nthe assumed model, which may be a sample estimate. In particular, we consider a\nsetup where the signal is low-rank Gaussian and the measurements are taken in\nthe directions of eigenvectors of the covariance matrix in a decreasing order\nof eigenvalues. We establish a set of performance bounds when a mismatched\ncovariance matrix is used, in terms of the gap of signal posterior entropy, as\nwell as the additional amount of power required to achieve the same signal\nrecovery precision. Based on this, we further study how to choose an\ninitialization for Info-Greedy Sensing using the sample covariance matrix, or\nusing an efficient covariance sketching scheme. \n\n"}
{"id": "1501.06243", "contents": "Title: Poisson Matrix Completion Abstract: We extend the theory of matrix completion to the case where we make Poisson\nobservations for a subset of entries of a low-rank matrix. We consider the\n(now) usual matrix recovery formulation through maximum likelihood with proper\nconstraints on the matrix $M$, and establish theoretical upper and lower bounds\non the recovery error. Our bounds are nearly optimal up to a factor on the\norder of $\\mathcal{O}(\\log(d_1 d_2))$. These bounds are obtained by adapting\nthe arguments used for one-bit matrix completion \\cite{davenport20121}\n(although these two problems are different in nature) and the adaptation\nrequires new techniques exploiting properties of the Poisson likelihood\nfunction and tackling the difficulties posed by the locally sub-Gaussian\ncharacteristic of the Poisson distribution. Our results highlight a few\nimportant distinctions of Poisson matrix completion compared to the prior work\nin matrix completion including having to impose a minimum signal-to-noise\nrequirement on each observed entry. We also develop an efficient iterative\nalgorithm and demonstrate its good performance in recovering solar flare\nimages. \n\n"}
{"id": "1502.03296", "contents": "Title: Statistical laws in linguistics Abstract: Zipf's law is just one out of many universal laws proposed to describe\nstatistical regularities in language. Here we review and critically discuss how\nthese laws can be statistically interpreted, fitted, and tested (falsified).\nThe modern availability of large databases of written text allows for tests\nwith an unprecedent statistical accuracy and also a characterization of the\nfluctuations around the typical behavior. We find that fluctuations are usually\nmuch larger than expected based on simplifying statistical assumptions (e.g.,\nindependence and lack of correlations between observations).These\nsimplifications appear also in usual statistical tests so that the large\nfluctuations can be erroneously interpreted as a falsification of the law.\nInstead, here we argue that linguistic laws are only meaningful (falsifiable)\nif accompanied by a model for which the fluctuations can be computed (e.g., a\ngenerative model of the text). The large fluctuations we report show that the\nconstraints imposed by linguistic laws on the creativity process of text\ngeneration are not as tight as one could expect. \n\n"}
{"id": "1502.06398", "contents": "Title: Bandit Convex Optimization: sqrt{T} Regret in One Dimension Abstract: We analyze the minimax regret of the adversarial bandit convex optimization\nproblem. Focusing on the one-dimensional case, we prove that the minimax regret\nis $\\widetilde\\Theta(\\sqrt{T})$ and partially resolve a decade-old open\nproblem. Our analysis is non-constructive, as we do not present a concrete\nalgorithm that attains this regret rate. Instead, we use minimax duality to\nreduce the problem to a Bayesian setting, where the convex loss functions are\ndrawn from a worst-case distribution, and then we solve the Bayesian version of\nthe problem with a variant of Thompson Sampling. Our analysis features a novel\nuse of convexity, formalized as a \"local-to-global\" property of convex\nfunctions, that may be of independent interest. \n\n"}
{"id": "1503.01212", "contents": "Title: Hierarchies of Relaxations for Online Prediction Problems with Evolving\n  Constraints Abstract: We study online prediction where regret of the algorithm is measured against\na benchmark defined via evolving constraints. This framework captures online\nprediction on graphs, as well as other prediction problems with combinatorial\nstructure. A key aspect here is that finding the optimal benchmark predictor\n(even in hindsight, given all the data) might be computationally hard due to\nthe combinatorial nature of the constraints. Despite this, we provide\npolynomial-time \\emph{prediction} algorithms that achieve low regret against\ncombinatorial benchmark sets. We do so by building improper learning algorithms\nbased on two ideas that work together. The first is to alleviate part of the\ncomputational burden through random playout, and the second is to employ\nLasserre semidefinite hierarchies to approximate the resulting integer program.\nInterestingly, for our prediction algorithms, we only need to compute the\nvalues of the semidefinite programs and not the rounded solutions. However, the\nintegrality gap for Lasserre hierarchy \\emph{does} enter the generic regret\nbound in terms of Rademacher complexity of the benchmark set. This establishes\na trade-off between the computation time and the regret bound of the algorithm. \n\n"}
{"id": "1503.06733", "contents": "Title: Yara Parser: A Fast and Accurate Dependency Parser Abstract: Dependency parsers are among the most crucial tools in natural language\nprocessing as they have many important applications in downstream tasks such as\ninformation retrieval, machine translation and knowledge acquisition. We\nintroduce the Yara Parser, a fast and accurate open-source dependency parser\nbased on the arc-eager algorithm and beam search. It achieves an unlabeled\naccuracy of 93.32 on the standard WSJ test set which ranks it among the top\ndependency parsers. At its fastest, Yara can parse about 4000 sentences per\nsecond when in greedy mode (1 beam). When optimizing for accuracy (using 64\nbeams and Brown cluster features), Yara can parse 45 sentences per second. The\nparser can be trained on any syntactic dependency treebank and different\noptions are provided in order to make it more flexible and tunable for specific\ntasks. It is released with the Apache version 2.0 license and can be used for\nboth commercial and academic purposes. The parser can be found at\nhttps://github.com/yahoo/YaraParser. \n\n"}
{"id": "1503.07613", "contents": "Title: Unsupervised authorship attribution Abstract: We describe a technique for attributing parts of a written text to a set of\nunknown authors. Nothing is assumed to be known a priori about the writing\nstyles of potential authors. We use multiple independent clusterings of an\ninput text to identify parts that are similar and dissimilar to one another. We\ndescribe algorithms necessary to combine the multiple clusterings into a\nmeaningful output. We show results of the application of the technique on texts\nhaving multiple writing styles. \n\n"}
{"id": "1503.07790", "contents": "Title: Transductive Multi-label Zero-shot Learning Abstract: Zero-shot learning has received increasing interest as a means to alleviate\nthe often prohibitive expense of annotating training data for large scale\nrecognition problems. These methods have achieved great success via learning\nintermediate semantic representations in the form of attributes and more\nrecently, semantic word vectors. However, they have thus far been constrained\nto the single-label case, in contrast to the growing popularity and importance\nof more realistic multi-label data. In this paper, for the first time, we\ninvestigate and formalise a general framework for multi-label zero-shot\nlearning, addressing the unique challenge therein: how to exploit multi-label\ncorrelation at test time with no training data for those classes? In\nparticular, we propose (1) a multi-output deep regression model to project an\nimage into a semantic word space, which explicitly exploits the correlations in\nthe intermediate semantic layer of word vectors; (2) a novel zero-shot learning\nalgorithm for multi-label data that exploits the unique compositionality\nproperty of semantic word vector representations; and (3) a transductive\nlearning strategy to enable the regression model learned from seen classes to\ngeneralise well to unseen classes. Our zero-shot learning experiments on a\nnumber of standard multi-label datasets demonstrate that our method outperforms\na variety of baselines. \n\n"}
{"id": "1504.01683", "contents": "Title: Jointly Embedding Relations and Mentions for Knowledge Population Abstract: This paper contributes a joint embedding model for predicting relations\nbetween a pair of entities in the scenario of relation inference. It differs\nfrom most stand-alone approaches which separately operate on either knowledge\nbases or free texts. The proposed model simultaneously learns low-dimensional\nvector representations for both triplets in knowledge repositories and the\nmentions of relations in free texts, so that we can leverage the evidence both\nresources to make more accurate predictions. We use NELL to evaluate the\nperformance of our approach, compared with cutting-edge methods. Results of\nextensive experiments show that our model achieves significant improvement on\nrelation extraction. \n\n"}
{"id": "1504.04343", "contents": "Title: Caffe con Troll: Shallow Ideas to Speed Up Deep Learning Abstract: We present Caffe con Troll (CcT), a fully compatible end-to-end version of\nthe popular framework Caffe with rebuilt internals. We built CcT to examine the\nperformance characteristics of training and deploying general-purpose\nconvolutional neural networks across different hardware architectures. We find\nthat, by employing standard batching optimizations for CPU training, we achieve\na 4.5x throughput improvement over Caffe on popular networks like CaffeNet.\nMoreover, with these improvements, the end-to-end training time for CNNs is\ndirectly proportional to the FLOPS delivered by the CPU, which enables us to\nefficiently train hybrid CPU-GPU systems for CNNs. \n\n"}
{"id": "1505.00384", "contents": "Title: Making Sense of Hidden Layer Information in Deep Networks by Learning\n  Hierarchical Targets Abstract: This paper proposes an architecture for deep neural networks with hidden\nlayer branches that learn targets of lower hierarchy than final layer targets.\nThe branches provide a channel for enforcing useful information in hidden layer\nwhich helps in attaining better accuracy, both for the final layer and hidden\nlayers. The shared layers modify their weights using the gradients of all cost\nfunctions higher than the branching layer. This model provides a flexible\ninference system with many levels of targets which is modular and can be used\nefficiently in situations requiring different levels of results according to\ncomplexity. This paper applies the idea to a text classification task on 20\nNewsgroups data set with two level of hierarchical targets and a comparison is\nmade with training without the use of hidden layer branches. \n\n"}
{"id": "1505.00468", "contents": "Title: VQA: Visual Question Answering Abstract: We propose the task of free-form and open-ended Visual Question Answering\n(VQA). Given an image and a natural language question about the image, the task\nis to provide an accurate natural language answer. Mirroring real-world\nscenarios, such as helping the visually impaired, both the questions and\nanswers are open-ended. Visual questions selectively target different areas of\nan image, including background details and underlying context. As a result, a\nsystem that succeeds at VQA typically needs a more detailed understanding of\nthe image and complex reasoning than a system producing generic image captions.\nMoreover, VQA is amenable to automatic evaluation, since many open-ended\nanswers contain only a few words or a closed set of answers that can be\nprovided in a multiple-choice format. We provide a dataset containing ~0.25M\nimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the\ninformation it provides. Numerous baselines and methods for VQA are provided\nand compared with human performance. Our VQA demo is available on CloudCV\n(http://cloudcv.org/vqa). \n\n"}
{"id": "1505.00477", "contents": "Title: Kernel Spectral Clustering and applications Abstract: In this chapter we review the main literature related to kernel spectral\nclustering (KSC), an approach to clustering cast within a kernel-based\noptimization setting. KSC represents a least-squares support vector machine\nbased formulation of spectral clustering described by a weighted kernel PCA\nobjective. Just as in the classifier case, the binary clustering model is\nexpressed by a hyperplane in a high dimensional space induced by a kernel. In\naddition, the multi-way clustering can be obtained by combining a set of binary\ndecision functions via an Error Correcting Output Codes (ECOC) encoding scheme.\nBecause of its model-based nature, the KSC method encompasses three main steps:\ntraining, validation, testing. In the validation stage model selection is\nperformed to obtain tuning parameters, like the number of clusters present in\nthe data. This is a major advantage compared to classical spectral clustering\nwhere the determination of the clustering parameters is unclear and relies on\nheuristics. Once a KSC model is trained on a small subset of the entire data,\nit is able to generalize well to unseen test points. Beyond the basic\nformulation, sparse KSC algorithms based on the Incomplete Cholesky\nDecomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are\nreviewed. In that respect, we show how it is possible to handle large scale\ndata. Also, two possible ways to perform hierarchical clustering and a soft\nclustering method are presented. Finally, real-world applications such as image\nsegmentation, power load time-series clustering, document clustering and big\ndata learning are considered. \n\n"}
{"id": "1505.04343", "contents": "Title: Provably Correct Algorithms for Matrix Column Subset Selection with\n  Selectively Sampled Data Abstract: We consider the problem of matrix column subset selection, which selects a\nsubset of columns from an input matrix such that the input can be well\napproximated by the span of the selected columns. Column subset selection has\nbeen applied to numerous real-world data applications such as population\ngenetics summarization, electronic circuits testing and recommendation systems.\nIn many applications the complete data matrix is unavailable and one needs to\nselect representative columns by inspecting only a small portion of the input\nmatrix. In this paper we propose the first provably correct column subset\nselection algorithms for partially observed data matrices. Our proposed\nalgorithms exhibit different merits and limitations in terms of statistical\naccuracy, computational efficiency, sample complexity and sampling schemes,\nwhich provides a nice exploration of the tradeoff between these desired\nproperties for column subset selection. The proposed methods employ the idea of\nfeedback driven sampling and are inspired by several sampling schemes\npreviously introduced for low-rank matrix approximation tasks (Drineas et al.,\n2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and\nSingh, 2014). Our analysis shows that, under the assumption that the input data\nmatrix has incoherent rows but possibly coherent columns, all algorithms\nprovably converge to the best low-rank approximation of the original data as\nnumber of selected columns increases. Furthermore, two of the proposed\nalgorithms enjoy a relative error bound, which is preferred for column subset\nselection and matrix approximation purposes. We also demonstrate through both\ntheoretical and empirical analysis the power of feedback driven sampling\ncompared to uniform random sampling on input matrices with highly correlated\ncolumns. \n\n"}
{"id": "1505.04657", "contents": "Title: Mining User Opinions in Mobile App Reviews: A Keyword-based Approach Abstract: User reviews of mobile apps often contain complaints or suggestions which are\nvaluable for app developers to improve user experience and satisfaction.\nHowever, due to the large volume and noisy-nature of those reviews, manually\nanalyzing them for useful opinions is inherently challenging. To address this\nproblem, we propose MARK, a keyword-based framework for semi-automated review\nanalysis. MARK allows an analyst describing his interests in one or some mobile\napps by a set of keywords. It then finds and lists the reviews most relevant to\nthose keywords for further analysis. It can also draw the trends over time of\nthose keywords and detect their sudden changes, which might indicate the\noccurrences of serious issues. To help analysts describe their interests more\neffectively, MARK can automatically extract keywords from raw reviews and rank\nthem by their associations with negative reviews. In addition, based on a\nvector-based semantic representation of keywords, MARK can divide a large set\nof keywords into more cohesive subsets, or suggest keywords similar to the\nselected ones. \n\n"}
{"id": "1505.04870", "contents": "Title: Flickr30k Entities: Collecting Region-to-Phrase Correspondences for\n  Richer Image-to-Sentence Models Abstract: The Flickr30k dataset has become a standard benchmark for sentence-based\nimage description. This paper presents Flickr30k Entities, which augments the\n158k captions from Flickr30k with 244k coreference chains, linking mentions of\nthe same entities across different captions for the same image, and associating\nthem with 276k manually annotated bounding boxes. Such annotations are\nessential for continued progress in automatic image description and grounded\nlanguage understanding. They enable us to define a new benchmark for\nlocalization of textual entity mentions in an image. We present a strong\nbaseline for this task that combines an image-text embedding, detectors for\ncommon objects, a color classifier, and a bias towards selecting larger\nobjects. While our baseline rivals in accuracy more complex state-of-the-art\nmodels, we show that its gains cannot be easily parlayed into improvements on\nsuch tasks as image-sentence retrieval, thus underlining the limitations of\ncurrent methods and the need for further research. \n\n"}
{"id": "1505.05663", "contents": "Title: Inferring Graphs from Cascades: A Sparse Recovery Framework Abstract: In the Network Inference problem, one seeks to recover the edges of an\nunknown graph from the observations of cascades propagating over this graph. In\nthis paper, we approach this problem from the sparse recovery perspective. We\nintroduce a general model of cascades, including the voter model and the\nindependent cascade model, for which we provide the first algorithm which\nrecovers the graph's edges with high probability and $O(s\\log m)$ measurements\nwhere $s$ is the maximum degree of the graph and $m$ is the number of nodes.\nFurthermore, we show that our algorithm also recovers the edge weights (the\nparameters of the diffusion process) and is robust in the context of\napproximate sparsity. Finally we prove an almost matching lower bound of\n$\\Omega(s\\log\\frac{m}{s})$ and validate our approach empirically on synthetic\ngraphs. \n\n"}
{"id": "1505.05667", "contents": "Title: A Re-ranking Model for Dependency Parser with Recursive Convolutional\n  Neural Network Abstract: In this work, we address the problem to model all the nodes (words or\nphrases) in a dependency tree with the dense representations. We propose a\nrecursive convolutional neural network (RCNN) architecture to capture syntactic\nand compositional-semantic representations of phrases and words in a dependency\ntree. Different with the original recursive neural network, we introduce the\nconvolution and pooling layers, which can model a variety of compositions by\nthe feature maps and choose the most informative compositions by the pooling\nlayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list\nof candidate dependency parsing trees. The experiments show that RCNN is very\neffective to improve the state-of-the-art dependency parsing on both English\nand Chinese datasets. \n\n"}
{"id": "1506.02312", "contents": "Title: A Framework for Constrained and Adaptive Behavior-Based Agents Abstract: Behavior Trees are commonly used to model agents for robotics and games,\nwhere constrained behaviors must be designed by human experts in order to\nguarantee that these agents will execute a specific chain of actions given a\nspecific set of perceptions. In such application areas, learning is a desirable\nfeature to provide agents with the ability to adapt and improve interactions\nwith humans and environment, but often discarded due to its unreliability. In\nthis paper, we propose a framework that uses Reinforcement Learning nodes as\npart of Behavior Trees to address the problem of adding learning capabilities\nin constrained agents. We show how this framework relates to Options in\nHierarchical Reinforcement Learning, ensuring convergence of nested learning\nnodes, and we empirically show that the learning nodes do not affect the\nexecution of other nodes in the tree. \n\n"}
{"id": "1506.02544", "contents": "Title: Learning with Group Invariant Features: A Kernel Perspective Abstract: We analyze in this paper a random feature map based on a theory of invariance\nI-theory introduced recently. More specifically, a group invariant signal\nsignature is obtained through cumulative distributions of group transformed\nrandom projections. Our analysis bridges invariant feature learning with kernel\nmethods, as we show that this feature map defines an expected Haar integration\nkernel that is invariant to the specified group action. We show how this\nnon-linear random feature map approximates this group invariant kernel\nuniformly on a set of $N$ points. Moreover, we show that it defines a function\nspace that is dense in the equivalent Invariant Reproducing Kernel Hilbert\nSpace. Finally, we quantify error rates of the convergence of the empirical\nrisk minimization, as well as the reduction in the sample complexity of a\nlearning algorithm using such an invariant representation for signal\nclassification, in a classical supervised learning setting. \n\n"}
{"id": "1506.02903", "contents": "Title: Mixing Time Estimation in Reversible Markov Chains from a Single Sample\n  Path Abstract: This article provides the first procedure for computing a fully\ndata-dependent interval that traps the mixing time $t_{\\text{mix}}$ of a finite\nreversible ergodic Markov chain at a prescribed confidence level. The interval\nis computed from a single finite-length sample path from the Markov chain, and\ndoes not require the knowledge of any parameters of the chain. This stands in\ncontrast to previous approaches, which either only provide point estimates, or\nrequire a reset mechanism, or additional prior knowledge. The interval is\nconstructed around the relaxation time $t_{\\text{relax}}$, which is strongly\nrelated to the mixing time, and the width of the interval converges to zero\nroughly at a $\\sqrt{n}$ rate, where $n$ is the length of the sample path. Upper\nand lower bounds are given on the number of samples required to achieve\nconstant-factor multiplicative accuracy. The lower bounds indicate that, unless\nfurther restrictions are placed on the chain, no procedure can achieve this\naccuracy level before seeing each state at least $\\Omega(t_{\\text{relax}})$\ntimes on the average. Finally, future directions of research are identified. \n\n"}
{"id": "1506.03137", "contents": "Title: Symmetric Tensor Completion from Multilinear Entries and Learning\n  Product Mixtures over the Hypercube Abstract: We give an algorithm for completing an order-$m$ symmetric low-rank tensor\nfrom its multilinear entries in time roughly proportional to the number of\ntensor entries. We apply our tensor completion algorithm to the problem of\nlearning mixtures of product distributions over the hypercube, obtaining new\nalgorithmic results. If the centers of the product distribution are linearly\nindependent, then we recover distributions with as many as $\\Omega(n)$ centers\nin polynomial time and sample complexity. In the general case, we recover\ndistributions with as many as $\\tilde\\Omega(n)$ centers in quasi-polynomial\ntime, answering an open problem of Feldman et al. (SIAM J. Comp.) for the\nspecial case of distributions with incoherent bias vectors.\n  Our main algorithmic tool is the iterated application of a low-rank matrix\ncompletion algorithm for matrices with adversarially missing entries. \n\n"}
{"id": "1506.03425", "contents": "Title: Fast Online Clustering with Randomized Skeleton Sets Abstract: We present a new fast online clustering algorithm that reliably recovers\narbitrary-shaped data clusters in high throughout data streams. Unlike the\nexisting state-of-the-art online clustering methods based on k-means or\nk-medoid, it does not make any restrictive generative assumptions. In addition,\nin contrast to existing nonparametric clustering techniques such as DBScan or\nDenStream, it gives provable theoretical guarantees. To achieve fast\nclustering, we propose to represent each cluster by a skeleton set which is\nupdated continuously as new data is seen. A skeleton set consists of weighted\nsamples from the data where weights encode local densities. The size of each\nskeleton set is adapted according to the cluster geometry. The proposed\ntechnique automatically detects the number of clusters and is robust to\noutliers. The algorithm works for the infinite data stream where more than one\npass over the data is not feasible. We provide theoretical guarantees on the\nquality of the clustering and also demonstrate its advantage over the existing\nstate-of-the-art on several datasets. \n\n"}
{"id": "1506.05561", "contents": "Title: Comparing and evaluating extended Lambek calculi Abstract: Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, was\ninnovative in many ways, notably as a precursor of linear logic. But it also\nshowed that we could treat our grammatical framework as a logic (as opposed to\na logical theory). However, though it was successful in giving at least a basic\ntreatment of many linguistic phenomena, it was also clear that a slightly more\nexpressive logical calculus was needed for many other cases. Therefore, many\nextensions and variants of the Lambek calculus have been proposed, since the\neighties and up until the present day. As a result, there is now a large class\nof calculi, each with its own empirical successes and theoretical results, but\nalso each with its own logical primitives. This raises the question: how do we\ncompare and evaluate these different logical formalisms? To answer this\nquestion, I present two unifying frameworks for these extended Lambek calculi.\nBoth are proof net calculi with graph contraction criteria. The first calculus\nis a very general system: you specify the structure of your sequents and it\ngives you the connectives and contractions which correspond to it. The calculus\ncan be extended with structural rules, which translate directly into graph\nrewrite rules. The second calculus is first-order (multiplicative\nintuitionistic) linear logic, which turns out to have several other,\nindependently proposed extensions of the Lambek calculus as fragments. I will\nillustrate the use of each calculus in building bridges between analyses\nproposed in different frameworks, in highlighting differences and in helping to\nidentify problems. \n\n"}
{"id": "1506.08105", "contents": "Title: Modelling of directional data using Kent distributions Abstract: The modelling of data on a spherical surface requires the consideration of\ndirectional probability distributions. To model asymmetrically distributed data\non a three-dimensional sphere, Kent distributions are often used. The moment\nestimates of the parameters are typically used in modelling tasks involving\nKent distributions. However, these lack a rigorous statistical treatment. The\nfocus of the paper is to introduce a Bayesian estimation of the parameters of\nthe Kent distribution which has not been carried out in the literature, partly\nbecause of its complex mathematical form. We employ the Bayesian\ninformation-theoretic paradigm of Minimum Message Length (MML) to bridge this\ngap and derive reliable estimators. The inferred parameters are subsequently\nused in mixture modelling of Kent distributions. The problem of inferring the\nsuitable number of mixture components is also addressed using the MML\ncriterion. We demonstrate the superior performance of the derived MML-based\nparameter estimates against the traditional estimators. We apply the MML\nprinciple to infer mixtures of Kent distributions to model empirical data\ncorresponding to protein conformations. We demonstrate the effectiveness of\nKent models to act as improved descriptors of protein structural data as\ncompared to commonly used von Mises-Fisher distributions. \n\n"}
{"id": "1506.08180", "contents": "Title: An Empirical Study of Stochastic Variational Algorithms for the Beta\n  Bernoulli Process Abstract: Stochastic variational inference (SVI) is emerging as the most promising\ncandidate for scaling inference in Bayesian probabilistic models to large\ndatasets. However, the performance of these methods has been assessed primarily\nin the context of Bayesian topic models, particularly latent Dirichlet\nallocation (LDA). Deriving several new algorithms, and using synthetic, image\nand genomic datasets, we investigate whether the understanding gleaned from LDA\napplies in the setting of sparse latent factor models, specifically beta\nprocess factor analysis (BPFA). We demonstrate that the big picture is\nconsistent: using Gibbs sampling within SVI to maintain certain posterior\ndependencies is extremely effective. However, we find that different posterior\ndependencies are important in BPFA relative to LDA. Particularly,\napproximations able to model intra-local variable dependence perform best. \n\n"}
{"id": "1506.08230", "contents": "Title: Convolutional networks and learning invariant to homogeneous\n  multiplicative scalings Abstract: The conventional classification schemes -- notably multinomial logistic\nregression -- used in conjunction with convolutional networks (convnets) are\nclassical in statistics, designed without consideration for the usual coupling\nwith convnets, stochastic gradient descent, and backpropagation. In the\nspecific application to supervised learning for convnets, a simple\nscale-invariant classification stage turns out to be more robust than\nmultinomial logistic regression, appears to result in slightly lower errors on\nseveral standard test sets, has similar computational costs, and features\nprecise control over the actual rate of learning. \"Scale-invariant\" means that\nmultiplying the input values by any nonzero scalar leaves the output unchanged. \n\n"}
{"id": "1507.00473", "contents": "Title: The Optimal Sample Complexity of PAC Learning Abstract: This work establishes a new upper bound on the number of samples sufficient\nfor PAC learning in the realizable case. The bound matches known lower bounds\nup to numerical constant factors. This solves a long-standing open problem on\nthe sample complexity of PAC learning. The technique and analysis build on a\nrecent breakthrough by Hans Simon. \n\n"}
{"id": "1507.00825", "contents": "Title: Ridge Regression, Hubness, and Zero-Shot Learning Abstract: This paper discusses the effect of hubness in zero-shot learning, when ridge\nregression is used to find a mapping between the example space to the label\nspace. Contrary to the existing approach, which attempts to find a mapping from\nthe example space to the label space, we show that mapping labels into the\nexample space is desirable to suppress the emergence of hubs in the subsequent\nnearest neighbor search step. Assuming a simple data model, we prove that the\nproposed approach indeed reduces hubness. This was verified empirically on the\ntasks of bilingual lexicon extraction and image labeling: hubness was reduced\nwith both of these tasks and the accuracy was improved accordingly. \n\n"}
{"id": "1507.01526", "contents": "Title: Grid Long Short-Term Memory Abstract: This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task. \n\n"}
{"id": "1507.03292", "contents": "Title: Cluster-Aided Mobility Predictions Abstract: Predicting the future location of users in wireless net- works has numerous\napplications, and can help service providers to improve the quality of service\nperceived by their clients. The location predictors proposed so far estimate\nthe next location of a specific user by inspecting the past individual\ntrajectories of this user. As a consequence, when the training data collected\nfor a given user is limited, the resulting prediction is inaccurate. In this\npaper, we develop cluster-aided predictors that exploit past trajectories\ncollected from all users to predict the next location of a given user. These\npredictors rely on clustering techniques and extract from the training data\nsimilarities among the mobility patterns of the various users to improve the\nprediction accuracy. Specifically, we present CAMP (Cluster-Aided Mobility\nPredictor), a cluster-aided predictor whose design is based on recent\nnon-parametric bayesian statistical tools. CAMP is robust and adaptive in the\nsense that it exploits similarities in users' mobility only if such\nsimilarities are really present in the training data. We analytically prove the\nconsistency of the predictions provided by CAMP, and investigate its\nperformance using two large-scale datasets. CAMP significantly outperforms\nexisting predictors, and in particular those that only exploit individual past\ntrajectories. \n\n"}
{"id": "1507.05498", "contents": "Title: On the Minimax Risk of Dictionary Learning Abstract: We consider the problem of learning a dictionary matrix from a number of\nobserved signals, which are assumed to be generated via a linear model with a\ncommon underlying dictionary. In particular, we derive lower bounds on the\nminimum achievable worst case mean squared error (MSE), regardless of\ncomputational complexity of the dictionary learning (DL) schemes. By casting DL\nas a classical (or frequentist) estimation problem, the lower bounds on the\nworst case MSE are derived by following an established information-theoretic\napproach to minimax estimation. The main conceptual contribution of this paper\nis the adaption of the information-theoretic approach to minimax estimation for\nthe DL problem in order to derive lower bounds on the worst case MSE of any DL\nscheme. We derive three different lower bounds applying to different generative\nmodels for the observed signals. The first bound applies to a wide range of\nmodels, it only requires the existence of a covariance matrix of the (unknown)\nunderlying coefficient vector. By specializing this bound to the case of sparse\ncoefficient distributions, and assuming the true dictionary satisfies the\nrestricted isometry property, we obtain a lower bound on the worst case MSE of\nDL schemes in terms of a signal to noise ratio (SNR). The third bound applies\nto a more restrictive subclass of coefficient distributions by requiring the\nnon-zero coefficients to be Gaussian. While, compared with the previous two\nbounds, the applicability of this final bound is the most limited it is the\ntightest of the three bounds in the low SNR regime. \n\n"}
{"id": "1507.08396", "contents": "Title: Tag-Weighted Topic Model For Large-scale Semi-Structured Documents Abstract: To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling. \n\n"}
{"id": "1508.00317", "contents": "Title: Time-series modeling with undecimated fully convolutional neural\n  networks Abstract: We present a new convolutional neural network-based time-series model.\nTypical convolutional neural network (CNN) architectures rely on the use of\nmax-pooling operators in between layers, which leads to reduced resolution at\nthe top layers. Instead, in this work we consider a fully convolutional network\n(FCN) architecture that uses causal filtering operations, and allows for the\nrate of the output signal to be the same as that of the input signal. We\nfurthermore propose an undecimated version of the FCN, which we refer to as the\nundecimated fully convolutional neural network (UFCNN), and is motivated by the\nundecimated wavelet transform. Our experimental results verify that using the\nundecimated version of the FCN is necessary in order to allow for effective\ntime-series modeling. The UFCNN has several advantages compared to other\ntime-series models such as the recurrent neural network (RNN) and long\nshort-term memory (LSTM), since it does not suffer from either the vanishing or\nexploding gradients problems, and is therefore easier to train. Convolution\noperations can also be implemented more efficiently compared to the recursion\nthat is involved in RNN-based models. We evaluate the performance of our model\nin a synthetic target tracking task using bearing only measurements generated\nfrom a state-space model, a probabilistic modeling of polyphonic music\nsequences problem, and a high frequency trading task using a time-series of\nask/bid quotes and their corresponding volumes. Our experimental results using\nsynthetic and real datasets verify the significant advantages of the UFCNN\ncompared to the RNN and LSTM baselines. \n\n"}
{"id": "1508.02354", "contents": "Title: Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models\n  of Meaning Abstract: Deep compositional models of meaning acting on distributional representations\nof words in order to produce vectors of larger text constituents are evolving\nto a popular area of NLP research. We detail a compositional distributional\nframework based on a rich form of word embeddings that aims at facilitating the\ninteractions between words in the context of a sentence. Embeddings and\ncomposition layers are jointly learned against a generic objective that\nenhances the vectors with syntactic information from the surrounding context.\nFurthermore, each word is associated with a number of senses, the most\nplausible of which is selected dynamically during the composition process. We\nevaluate the produced vectors qualitatively and quantitatively with positive\nresults. At the sentence level, the effectiveness of the framework is\ndemonstrated on the MSRPar task, for which we report results within the\nstate-of-the-art range. \n\n"}
{"id": "1508.03712", "contents": "Title: Towards an Axiomatic Approach to Hierarchical Clustering of Measures Abstract: We propose some axioms for hierarchical clustering of probability measures\nand investigate their ramifications. The basic idea is to let the user\nstipulate the clusters for some elementary measures. This is done without the\nneed of any notion of metric, similarity or dissimilarity. Our main results\nthen show that for each suitable choice of user-defined clustering on\nelementary measures we obtain a unique notion of clustering on a large set of\ndistributions satisfying a set of additivity and continuity axioms. We\nillustrate the developed theory by numerous examples including some with and\nsome without a density. \n\n"}
{"id": "1508.06615", "contents": "Title: Character-Aware Neural Language Models Abstract: We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information. \n\n"}
{"id": "1509.00685", "contents": "Title: A Neural Attention Model for Abstractive Sentence Summarization Abstract: Summarization based on text extraction is inherently limited, but\ngeneration-style abstractive methods have proven challenging to build. In this\nwork, we propose a fully data-driven approach to abstractive sentence\nsummarization. Our method utilizes a local attention-based model that generates\neach word of the summary conditioned on the input sentence. While the model is\nstructurally simple, it can easily be trained end-to-end and scales to a large\namount of training data. The model shows significant performance gains on the\nDUC-2004 shared task compared with several strong baselines. \n\n"}
{"id": "1509.04355", "contents": "Title: Towards Making High Dimensional Distance Metric Learning Practical Abstract: In this work, we study distance metric learning (DML) for high dimensional\ndata. A typical approach for DML with high dimensional data is to perform the\ndimensionality reduction first before learning the distance metric. The main\nshortcoming of this approach is that it may result in a suboptimal solution due\nto the subspace removed by the dimensionality reduction method. In this work,\nwe present a dual random projection frame for DML with high dimensional data\nthat explicitly addresses the limitation of dimensionality reduction for DML.\nThe key idea is to first project all the data points into a low dimensional\nspace by random projection, and compute the dual variables using the projected\nvectors. It then reconstructs the distance metric in the original space using\nthe estimated dual variables. The proposed method, on one hand, enjoys the\nlight computation of random projection, and on the other hand, alleviates the\nlimitation of most dimensionality reduction methods. We verify both empirically\nand theoretically the effectiveness of the proposed algorithm for high\ndimensional DML. \n\n"}
{"id": "1509.06585", "contents": "Title: A Review of Features for the Discrimination of Twitter Users:\n  Application to the Prediction of Offline Influence Abstract: Many works related to Twitter aim at characterizing its users in some way:\nrole on the service (spammers, bots, organizations, etc.), nature of the user\n(socio-professional category, age, etc.), topics of interest , and others.\nHowever, for a given user classification problem, it is very difficult to\nselect a set of appropriate features, because the many features described in\nthe literature are very heterogeneous, with name overlaps and collisions, and\nnumerous very close variants. In this article, we review a wide range of such\nfeatures. In order to present a clear state-of-the-art description, we unify\ntheir names, definitions and relationships, and we propose a new, neutral,\ntypology. We then illustrate the interest of our review by applying a selection\nof these features to the offline influence detection problem. This task\nconsists in identifying users which are influential in real-life, based on\ntheir Twitter account and related data. We show that most features deemed\nefficient to predict online influence, such as the numbers of retweets and\nfollowers, are not relevant to this problem. However, We propose several\ncontent-based approaches to label Twitter users as Influencers or not. We also\nrank them according to a predicted influence level. Our proposals are evaluated\nover the CLEF RepLab 2014 dataset, and outmatch state-of-the-art methods. \n\n"}
{"id": "1509.06807", "contents": "Title: Bandit Label Inference for Weakly Supervised Learning Abstract: The scarcity of data annotated at the desired level of granularity is a\nrecurring issue in many applications. Significant amounts of effort have been\ndevoted to developing weakly supervised methods tailored to each individual\nsetting, which are often carefully designed to take advantage of the particular\nproperties of weak supervision regimes, form of available data and prior\nknowledge of the task at hand. Unfortunately, it is difficult to adapt these\nmethods to new tasks and/or forms of data, which often require different weak\nsupervision regimes or models. We present a general-purpose method that can\nsolve any weakly supervised learning problem irrespective of the weak\nsupervision regime or the model. The proposed method turns any off-the-shelf\nstrongly supervised classifier into a weakly supervised classifier and allows\nthe user to specify any arbitrary weakly supervision regime via a loss\nfunction. We apply the method to several different weak supervision regimes and\ndemonstrate competitive results compared to methods specifically engineered for\nthose settings. \n\n"}
{"id": "1509.07943", "contents": "Title: Super-Resolution Off the Grid Abstract: Super-resolution is the problem of recovering a superposition of point\nsources using bandlimited measurements, which may be corrupted with noise. This\nsignal processing problem arises in numerous imaging problems, ranging from\nastronomy to biology to spectroscopy, where it is common to take (coarse)\nFourier measurements of an object. Of particular interest is in obtaining\nestimation procedures which are robust to noise, with the following desirable\nstatistical and computational properties: we seek to use coarse Fourier\nmeasurements (bounded by some cutoff frequency); we hope to take a\n(quantifiably) small number of measurements; we desire our algorithm to run\nquickly.\n  Suppose we have k point sources in d dimensions, where the points are\nseparated by at least \\Delta from each other (in Euclidean distance). This work\nprovides an algorithm with the following favorable guarantees: - The algorithm\nuses Fourier measurements, whose frequencies are bounded by O(1/\\Delta) (up to\nlog factors). Previous algorithms require a cutoff frequency which may be as\nlarge as {\\Omega}( d/\\Delta). - The number of measurements taken by and the\ncomputational complexity of our algorithm are bounded by a polynomial in both\nthe number of points k and the dimension d, with no dependence on the\nseparation \\Delta. In contrast, previous algorithms depended inverse\npolynomially on the minimal separation and exponentially on the dimension for\nboth of these quantities.\n  Our estimation procedure itself is simple: we take random bandlimited\nmeasurements (as opposed to taking an exponential number of measurements on the\nhyper-grid). Furthermore, our analysis and algorithm are elementary (based on\nconcentration bounds for sampling and the singular value decomposition). \n\n"}
{"id": "1510.00012", "contents": "Title: Fast Discrete Distribution Clustering Using Wasserstein Barycenter with\n  Sparse Support Abstract: In a variety of research areas, the weighted bag of vectors and the histogram\nare widely used descriptors for complex objects. Both can be expressed as\ndiscrete distributions. D2-clustering pursues the minimum total within-cluster\nvariation for a set of discrete distributions subject to the\nKantorovich-Wasserstein metric. D2-clustering has a severe scalability issue,\nthe bottleneck being the computation of a centroid distribution, called\nWasserstein barycenter, that minimizes its sum of squared distances to the\ncluster members. In this paper, we develop a modified Bregman ADMM approach for\ncomputing the approximate discrete Wasserstein barycenter of large clusters. In\nthe case when the support points of the barycenters are unknown and have low\ncardinality, our method achieves high accuracy empirically at a much reduced\ncomputational cost. The strengths and weaknesses of our method and its\nalternatives are examined through experiments, and we recommend scenarios for\ntheir respective usage. Moreover, we develop both serial and parallelized\nversions of the algorithm. By experimenting with large-scale data, we\ndemonstrate the computational efficiency of the new methods and investigate\ntheir convergence properties and numerical stability. The clustering results\nobtained on several datasets in different domains are highly competitive in\ncomparison with some widely used methods in the corresponding areas. \n\n"}
{"id": "1510.00756", "contents": "Title: Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using\n  Hierarchy Width Abstract: Gibbs sampling on factor graphs is a widely used inference technique, which\noften produces good empirical results. Theoretical guarantees for its\nperformance are weak: even for tree structured graphs, the mixing time of Gibbs\nmay be exponential in the number of variables. To help understand the behavior\nof Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy\nwidth. We show that under suitable conditions on the weights, bounded hierarchy\nwidth ensures polynomial mixing time. Our study of hierarchy width is in part\nmotivated by a class of factor graph templates, hierarchical templates, which\nhave bounded hierarchy width---regardless of the data used to instantiate them.\nWe demonstrate a rich application from natural language processing in which\nGibbs sampling provably mixes rapidly and achieves accuracy that exceeds human\nvolunteers. \n\n"}
{"id": "1510.02879", "contents": "Title: Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive\n  Transfer from multiple sources in the same domain Abstract: Transferring knowledge from prior source tasks in solving a new target task\ncan be useful in several learning applications. The application of transfer\nposes two serious challenges which have not been adequately addressed. First,\nthe agent should be able to avoid negative transfer, which happens when the\ntransfer hampers or slows down the learning instead of helping it. Second, the\nagent should be able to selectively transfer, which is the ability to select\nand transfer from different and multiple source tasks for different parts of\nthe state space of the target task. We propose A2T (Attend, Adapt and\nTransfer), an attentive deep architecture which adapts and transfers from these\nsource tasks. Our model is generic enough to effect transfer of either policies\nor value functions. Empirical evaluations on different learning algorithms show\nthat A2T is an effective architecture for transfer by being able to avoid\nnegative transfer while transferring selectively from multiple source tasks in\nthe same domain. \n\n"}
{"id": "1510.03519", "contents": "Title: Bridge Correlational Neural Networks for Multilingual Multimodal\n  Representation Learning Abstract: Recently there has been a lot of interest in learning common representations\nfor multiple views of data. Typically, such common representations are learned\nusing a parallel corpus between the two views (say, 1M images and their English\ncaptions). In this work, we address a real-world scenario where no direct\nparallel data is available between two views of interest (say, $V_1$ and $V_2$)\nbut parallel data is available between each of these views and a pivot view\n($V_3$). We propose a model for learning a common representation for $V_1$,\n$V_2$ and $V_3$ using only the parallel data available between $V_1V_3$ and\n$V_2V_3$. The proposed model is generic and even works when there are $n$ views\nof interest and only one pivot view which acts as a bridge between them. There\nare two specific downstream applications that we focus on (i) transfer learning\nbetween languages $L_1$,$L_2$,...,$L_n$ using a pivot language $L$ and (ii)\ncross modal access between images and a language $L_1$ using a pivot language\n$L_2$. Our model achieves state-of-the-art performance in multilingual document\nclassification on the publicly available multilingual TED corpus and promising\nresults in multilingual multimodal retrieval on a new dataset created and\nreleased as a part of this work. \n\n"}
{"id": "1510.04822", "contents": "Title: SGD with Variance Reduction beyond Empirical Risk Minimization Abstract: We introduce a doubly stochastic proximal gradient algorithm for optimizing a\nfinite average of smooth convex functions, whose gradients depend on\nnumerically expensive expectations. Our main motivation is the acceleration of\nthe optimization of the regularized Cox partial-likelihood (the core model used\nin survival analysis), but our algorithm can be used in different settings as\nwell. The proposed algorithm is doubly stochastic in the sense that gradient\nsteps are done using stochastic gradient descent (SGD) with variance reduction,\nwhere the inner expectations are approximated by a Monte-Carlo Markov-Chain\n(MCMC) algorithm. We derive conditions on the MCMC number of iterations\nguaranteeing convergence, and obtain a linear rate of convergence under strong\nconvexity and a sublinear rate without this assumption. We illustrate the fact\nthat our algorithm improves the state-of-the-art solver for regularized Cox\npartial-likelihood on several datasets from survival analysis. \n\n"}
{"id": "1511.02580", "contents": "Title: How far can we go without convolution: Improving fully-connected\n  networks Abstract: We propose ways to improve the performance of fully connected networks. We\nfound that two approaches in particular have a strong effect on performance:\nlinear bottleneck layers and unsupervised pre-training using autoencoders\nwithout hidden unit biases. We show how both approaches can be related to\nimproving gradient flow and reducing sparsity in the network. We show that a\nfully connected network can yield approximately 70% classification accuracy on\nthe permutation-invariant CIFAR-10 task, which is much higher than the current\nstate-of-the-art. By adding deformations to the training data, the fully\nconnected network achieves 78% accuracy, which is just 10% short of a decent\nconvolutional network. \n\n"}
{"id": "1511.04024", "contents": "Title: Multimodal Skip-gram Using Convolutional Pseudowords Abstract: This work studies the representational mapping across multimodal data such\nthat given a piece of the raw data in one modality the corresponding semantic\ndescription in terms of the raw data in another modality is immediately\nobtained. Such a representational mapping can be found in a wide spectrum of\nreal-world applications including image/video retrieval, object recognition,\naction/behavior recognition, and event understanding and prediction. To that\nend, we introduce a simplified training objective for learning multimodal\nembeddings using the skip-gram architecture by introducing convolutional\n\"pseudowords:\" embeddings composed of the additive combination of distributed\nword representations and image features from convolutional neural networks\nprojected into the multimodal space. We present extensive results of the\nrepresentational properties of these embeddings on various word similarity\nbenchmarks to show the promise of this approach. \n\n"}
{"id": "1511.04587", "contents": "Title: Accurate Image Super-Resolution Using Very Deep Convolutional Networks Abstract: We present a highly accurate single-image super-resolution (SR) method. Our\nmethod uses a very deep convolutional network inspired by VGG-net used for\nImageNet classification \\cite{simonyan2015very}. We find increasing our network\ndepth shows a significant improvement in accuracy. Our final model uses 20\nweight layers. By cascading small filters many times in a deep network\nstructure, contextual information over large image regions is exploited in an\nefficient way. With very deep networks, however, convergence speed becomes a\ncritical issue during training. We propose a simple yet effective training\nprocedure. We learn residuals only and use extremely high learning rates\n($10^4$ times higher than SRCNN \\cite{dong2015image}) enabled by adjustable\ngradient clipping. Our proposed method performs better than existing methods in\naccuracy and visual improvements in our results are easily noticeable. \n\n"}
{"id": "1511.04773", "contents": "Title: Large-Scale Approximate Kernel Canonical Correlation Analysis Abstract: Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view\nrepresentation learning technique with broad applicability in statistics and\nmachine learning. Although there is a closed-form solution for the KCCA\nobjective, it involves solving an $N\\times N$ eigenvalue system where $N$ is\nthe training set size, making its computational requirements in both memory and\ntime prohibitive for large-scale problems. Various approximation techniques\nhave been developed for KCCA. A commonly used approach is to first transform\nthe original inputs to an $M$-dimensional random feature space so that inner\nproducts in the feature space approximate kernel evaluations, and then apply\nlinear CCA to the transformed inputs. In many applications, however, the\ndimensionality $M$ of the random feature space may need to be very large in\norder to obtain a sufficiently good approximation; it then becomes challenging\nto perform the linear CCA step on the resulting very high-dimensional data\nmatrices. We show how to use a stochastic optimization algorithm, recently\nproposed for linear CCA and its neural-network extension, to further alleviate\nthe computation requirements of approximate KCCA. This approach allows us to\nrun approximate KCCA on a speech dataset with $1.4$ million training samples\nand a random feature space of dimensionality $M=100000$ on a typical\nworkstation. \n\n"}
{"id": "1511.05616", "contents": "Title: Learning Structured Inference Neural Networks with Label Relations Abstract: Images of scenes have various objects as well as abundant attributes, and\ndiverse levels of visual categorization are possible. A natural image could be\nassigned with fine-grained labels that describe major components,\ncoarse-grained labels that depict high level abstraction or a set of labels\nthat reveal attributes. Such categorization at different concept layers can be\nmodeled with label graphs encoding label information. In this paper, we exploit\nthis rich information with a state-of-art deep learning framework, and propose\na generic structured model that leverages diverse label relations to improve\nimage classification performance. Our approach employs a novel stacked label\nprediction neural network, capturing both inter-level and intra-level label\nsemantics. We evaluate our method on benchmark image datasets, and empirical\nresults illustrate the efficacy of our model. \n\n"}
{"id": "1511.06241", "contents": "Title: Convolutional Clustering for Unsupervised Learning Abstract: The task of labeling data for training deep neural networks is daunting and\ntedious, requiring millions of labels to achieve the current state-of-the-art\nresults. Such reliance on large amounts of labeled data can be relaxed by\nexploiting hierarchical features via unsupervised learning techniques. In this\nwork, we propose to train a deep convolutional network based on an enhanced\nversion of the k-means clustering algorithm, which reduces the number of\ncorrelated parameters in the form of similar filters, and thus increases test\ncategorization accuracy. We call our algorithm convolutional k-means\nclustering. We further show that learning the connection between the layers of\na deep convolutional neural network improves its ability to be trained on a\nsmaller amount of labeled data. Our experiments show that the proposed\nalgorithm outperforms other techniques that learn filters unsupervised.\nSpecifically, we obtained a test accuracy of 74.1% on STL-10 and a test error\nof 0.5% on MNIST. \n\n"}
{"id": "1511.07263", "contents": "Title: Input Sparsity Time Low-Rank Approximation via Ridge Leverage Score\n  Sampling Abstract: We present a new algorithm for finding a near optimal low-rank approximation\nof a matrix $A$ in $O(nnz(A))$ time. Our method is based on a recursive\nsampling scheme for computing a representative subset of $A$'s columns, which\nis then used to find a low-rank approximation.\n  This approach differs substantially from prior $O(nnz(A))$ time algorithms,\nwhich are all based on fast Johnson-Lindenstrauss random projections. It\nmatches the guarantees of these methods while offering a number of advantages.\n  Not only are sampling algorithms faster for sparse and structured data, but\nthey can also be applied in settings where random projections cannot. For\nexample, we give new single-pass streaming algorithms for the column subset\nselection and projection-cost preserving sample problems. Our method has also\nbeen used to give the fastest algorithms for provably approximating kernel\nmatrices [MM16]. \n\n"}
{"id": "1512.00728", "contents": "Title: Annotating Character Relationships in Literary Texts Abstract: We present a dataset of manually annotated relationships between characters\nin literary texts, in order to support the training and evaluation of automatic\nmethods for relation type prediction in this domain (Makazhanov et al., 2014;\nKokkinakis, 2013) and the broader computational analysis of literary character\n(Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and\nGurevych, 2015). In this work, we solicit annotations from workers on Amazon\nMechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_\non four dimensions of interest: for a given pair of characters, we collect\njudgments as to the coarse-grained category (professional, social, familial),\nfine-grained category (friend, lover, parent, rival, employer), and affinity\n(positive, negative, neutral) that describes their primary relationship in a\ntext. We do not assume that this relationship is static; we also collect\njudgments as to whether it changes at any point in the course of the text. \n\n"}
{"id": "1512.02181", "contents": "Title: The Teaching Dimension of Linear Learners Abstract: Teaching dimension is a learning theoretic quantity that specifies the\nminimum training set size to teach a target model to a learner. Previous\nstudies on teaching dimension focused on version-space learners which maintain\nall hypotheses consistent with the training data, and cannot be applied to\nmodern machine learners which select a specific hypothesis via optimization.\nThis paper presents the first known teaching dimension for ridge regression,\nsupport vector machines, and logistic regression. We also exhibit optimal\ntraining sets that match these teaching dimensions. Our approach generalizes to\nother linear learners. \n\n"}
{"id": "1512.03219", "contents": "Title: Norm-Free Radon-Nikodym Approach to Machine Learning Abstract: For Machine Learning (ML) classification problem, where a vector of\n$\\mathbf{x}$--observations (values of attributes) is mapped to a single $y$\nvalue (class label), a generalized Radon--Nikodym type of solution is proposed.\nQuantum--mechanics --like probability states $\\psi^2(\\mathbf{x})$ are\nconsidered and \"Cluster Centers\", corresponding to the extremums of\n$<y\\psi^2(\\mathbf{x})>/<\\psi^2(\\mathbf{x})>$, are found from generalized\neigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes and\ncorresponding to them eigenvectors $\\psi^{[i]}(\\mathbf{x})$ define \"Cluster\nCenters\". The projection of a $\\psi$ state, localized at given $\\mathbf{x}$ to\nclassify, on these eigenvectors define the probability of $y^{[i]}$ outcome,\nthus avoiding using a norm ($L^2$ or other types), required for \"quality\ncriteria\" in a typical Machine Learning technique. A coverage of each `Cluster\nCenter\" is calculated, what potentially allows to separate system properties\n(described by $y^{[i]}$ outcomes) and system testing conditions (described by\n$C^{[i]}$ coverage). As an example of such application $y$ distribution\nestimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can be\nconsidered as Gauss quadratures generalization. This estimator allows to\nperform $y$ probability distribution estimation in a strongly non--Gaussian\ncase. \n\n"}
{"id": "1512.07108", "contents": "Title: Recent Advances in Convolutional Neural Networks Abstract: In the last few years, deep learning has led to very good performance on a\nvariety of problems, such as visual recognition, speech recognition and natural\nlanguage processing. Among different types of deep neural networks,\nconvolutional neural networks have been most extensively studied. Leveraging on\nthe rapid growth in the amount of the annotated data and the great improvements\nin the strengths of graphics processor units, the research on convolutional\nneural networks has been emerged swiftly and achieved state-of-the-art results\non various tasks. In this paper, we provide a broad survey of the recent\nadvances in convolutional neural networks. We detailize the improvements of CNN\non different aspects, including layer design, activation function, loss\nfunction, regularization, optimization and fast computation. Besides, we also\nintroduce various applications of convolutional neural networks in computer\nvision, speech and natural language processing. \n\n"}
{"id": "1512.09080", "contents": "Title: Detection in the stochastic block model with multiple clusters: proof of\n  the achievability conjectures, acyclic BP, and the information-computation\n  gap Abstract: In a paper that initiated the modern study of the stochastic block model,\nDecelle et al., backed by Mossel et al., made the following conjecture: Denote\nby $k$ the number of balanced communities, $a/n$ the probability of connecting\ninside communities and $b/n$ across, and set\n$\\mathrm{SNR}=(a-b)^2/(k(a+(k-1)b)$; for any $k \\geq 2$, it is possible to\ndetect communities efficiently whenever $\\mathrm{SNR}>1$ (the KS threshold),\nwhereas for $k\\geq 4$, it is possible to detect communities\ninformation-theoretically for some $\\mathrm{SNR}<1$. Massouli\\'e, Mossel et\nal.\\ and Bordenave et al.\\ succeeded in proving that the KS threshold is\nefficiently achievable for $k=2$, while Mossel et al.\\ proved that it cannot be\ncrossed information-theoretically for $k=2$. The above conjecture remained open\nfor $k \\geq 3$.\n  This paper proves this conjecture, further extending the efficient detection\nto non-symmetrical SBMs with a generalized notion of detection and KS\nthreshold. For the efficient part, a linearized acyclic belief propagation\n(ABP) algorithm is developed and proved to detect communities for any $k$ down\nto the KS threshold in time $O(n \\log n)$. Achieving this requires showing\noptimality of ABP in the presence of cycles, a challenge for message passing\nalgorithms. The paper further connects ABP to a power iteration method with a\nnonbacktracking operator of generalized order, formalizing the interplay\nbetween message passing and spectral methods. For the information-theoretic\n(IT) part, a non-efficient algorithm sampling a typical clustering is shown to\nbreak down the KS threshold at $k=4$. The emerging gap is shown to be large in\nsome cases; if $a=0$, the KS threshold reads $b \\gtrsim k^2$ whereas the IT\nbound reads $b \\gtrsim k \\ln(k)$, making the SBM a good study-case for\ninformation-computation gaps. \n\n"}
{"id": "1601.00770", "contents": "Title: End-to-End Relation Extraction using LSTMs on Sequences and Tree\n  Structures Abstract: We present a novel end-to-end neural model to extract entities and relations\nbetween them. Our recurrent neural network based model captures both word\nsequence and dependency tree substructure information by stacking bidirectional\ntree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows\nour model to jointly represent both entities and relations with shared\nparameters in a single model. We further encourage detection of entities during\ntraining and use of entity information in relation extraction via entity\npretraining and scheduled sampling. Our model improves over the\nstate-of-the-art feature-based model on end-to-end relation extraction,\nachieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and\nACE2004, respectively. We also show that our LSTM-RNN based model compares\nfavorably to the state-of-the-art CNN based model (in F1-score) on nominal\nrelation classification (SemEval-2010 Task 8). Finally, we present an extensive\nablation analysis of several model components. \n\n"}
{"id": "1601.05647", "contents": "Title: On Structured Sparsity of Phonological Posteriors for Linguistic Parsing Abstract: The speech signal conveys information on different time scales from short\ntime scale or segmental, associated to phonological and phonetic information to\nlong time scale or supra segmental, associated to syllabic and prosodic\ninformation. Linguistic and neurocognitive studies recognize the phonological\nclasses at segmental level as the essential and invariant representations used\nin speech temporal organization. In the context of speech processing, a deep\nneural network (DNN) is an effective computational method to infer the\nprobability of individual phonological classes from a short segment of speech\nsignal. A vector of all phonological class probabilities is referred to as\nphonological posterior. There are only very few classes comprising a short term\nspeech signal; hence, the phonological posterior is a sparse vector. Although\nthe phonological posteriors are estimated at segmental level, we claim that\nthey convey supra-segmental information. Specifically, we demonstrate that\nphonological posteriors are indicative of syllabic and prosodic events.\nBuilding on findings from converging linguistic evidence on the gestural model\nof Articulatory Phonology as well as the neural basis of speech perception, we\nhypothesize that phonological posteriors convey properties of linguistic\nclasses at multiple time scales, and this information is embedded in their\nsupport (index) of active coefficients. To verify this hypothesis, we obtain a\nbinary representation of phonological posteriors at the segmental level which\nis referred to as first-order sparsity structure; the high-order structures are\nobtained by the concatenation of first-order binary vectors. It is then\nconfirmed that the classification of supra-segmental linguistic events, the\nproblem known as linguistic parsing, can be achieved with high accuracy using\nasimple binary pattern matching of first-order or high-order structures. \n\n"}
{"id": "1601.07621", "contents": "Title: Revealing Fundamental Physics from the Daya Bay Neutrino Experiment\n  using Deep Neural Networks Abstract: Experiments in particle physics produce enormous quantities of data that must\nbe analyzed and interpreted by teams of physicists. This analysis is often\nexploratory, where scientists are unable to enumerate the possible types of\nsignal prior to performing the experiment. Thus, tools for summarizing,\nclustering, visualizing and classifying high-dimensional data are essential. In\nthis work, we show that meaningful physical content can be revealed by\ntransforming the raw data into a learned high-level representation using deep\nneural networks, with measurements taken at the Daya Bay Neutrino Experiment as\na case study. We further show how convolutional deep neural networks can\nprovide an effective classification filter with greater than 97% accuracy\nacross different classes of physics events, significantly better than other\nmachine learning approaches. \n\n"}
{"id": "1602.00991", "contents": "Title: Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks Abstract: This paper presents to the best of our knowledge the first end-to-end object\ntracking approach which directly maps from raw sensor input to object tracks in\nsensor space without requiring any feature engineering or system identification\nin the form of plant or sensor models. Specifically, our system accepts a\nstream of raw sensor data at one end and, in real-time, produces an estimate of\nthe entire environment state at the output including even occluded objects. We\nachieve this by framing the problem as a deep learning task and exploit\nsequence models in the form of recurrent neural networks to learn a mapping\nfrom sensor measurements to object tracks. In particular, we propose a learning\nmethod based on a form of input dropout which allows learning in an\nunsupervised manner, only based on raw, occluded sensor data without access to\nground-truth annotations. We demonstrate our approach using a synthetic dataset\ndesigned to mimic the task of tracking objects in 2D laser data -- as commonly\nencountered in robotics applications -- and show that it learns to track many\ndynamic objects despite occlusions and the presence of sensor noise. \n\n"}
{"id": "1602.02123", "contents": "Title: Sequence Classification with Neural Conditional Random Fields Abstract: The proliferation of sensor devices monitoring human activity generates\nvoluminous amount of temporal sequences needing to be interpreted and\ncategorized. Moreover, complex behavior detection requires the personalization\nof multi-sensor fusion algorithms. Conditional random fields (CRFs) are\ncommonly used in structured prediction tasks such as part-of-speech tagging in\nnatural language processing. Conditional probabilities guide the choice of each\ntag/label in the sequence conflating the structured prediction task with the\nsequence classification task where different models provide different\ncategorization of the same sequence. The claim of this paper is that CRF models\nalso provide discriminative models to distinguish between types of sequence\nregardless of the accuracy of the labels obtained if we calibrate the class\nmembership estimate of the sequence. We introduce and compare different neural\nnetwork based linear-chain CRFs and we present experiments on two complex\nsequence classification and structured prediction tasks to support this claim. \n\n"}
{"id": "1602.02285", "contents": "Title: A Deep Learning Approach to Unsupervised Ensemble Learning Abstract: We show how deep learning methods can be applied in the context of\ncrowdsourcing and unsupervised ensemble learning. First, we prove that the\npopular model of Dawid and Skene, which assumes that all classifiers are\nconditionally independent, is {\\em equivalent} to a Restricted Boltzmann\nMachine (RBM) with a single hidden node. Hence, under this model, the posterior\nprobabilities of the true labels can be instead estimated via a trained RBM.\nNext, to address the more general case, where classifiers may strongly violate\nthe conditional independence assumption, we propose to apply RBM-based Deep\nNeural Net (DNN). Experimental results on various simulated and real-world\ndatasets demonstrate that our proposed DNN approach outperforms other\nstate-of-the-art methods, in particular when the data violates the conditional\nindependence assumption. \n\n"}
{"id": "1602.02389", "contents": "Title: Ensemble Robustness and Generalization of Stochastic Deep Learning\n  Algorithms Abstract: The question why deep learning algorithms generalize so well has attracted\nincreasing research interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided\ncomplete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this\nwork, we focus on the robustness approach (Xu & Mannor, 2012), i.e., if the\nerror of a hypothesis will not change much due to perturbations of its training\nexamples, then it will also generalize well. As most deep learning algorithms\nare stochastic (e.g., Stochastic Gradient Descent, Dropout, and\nBayes-by-backprop), we revisit the robustness arguments of Xu & Mannor, and\nintroduce a new approach, ensemble robustness, that concerns the robustness of\na population of hypotheses. Through the lens of ensemble robustness, we reveal\nthat a stochastic learning algorithm can generalize well as long as its\nsensitiveness to adversarial perturbations is bounded in average over training\nexamples. Moreover, an algorithm may be sensitive to some adversarial examples\n(Goodfellow et al., 2015) but still generalize well. To support our claims, we\nprovide extensive simulations for different deep learning algorithms and\ndifferent network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize. \n\n"}
{"id": "1602.07320", "contents": "Title: Stuck in a What? Adventures in Weight Space Abstract: Deep learning researchers commonly suggest that converged models are stuck in\nlocal minima. More recently, some researchers observed that under reasonable\nassumptions, the vast majority of critical points are saddle points, not true\nminima. Both descriptions suggest that weights converge around a point in\nweight space, be it a local optima or merely a critical point. However, it's\npossible that neither interpretation is accurate. As neural networks are\ntypically over-complete, it's easy to show the existence of vast continuous\nregions through weight space with equal loss. In this paper, we build on recent\nwork empirically characterizing the error surfaces of neural networks. We\nanalyze training paths through weight space, presenting evidence that apparent\nconvergence of loss does not correspond to weights arriving at critical points,\nbut instead to large movements through flat regions of weight space. While it's\ntrivial to show that neural network error surfaces are globally non-convex, we\nshow that error surfaces are also locally non-convex, even after breaking\nsymmetry with a random initialization and also after partial training. \n\n"}
{"id": "1602.07373", "contents": "Title: On Study of the Binarized Deep Neural Network for Image Classification Abstract: Recently, the deep neural network (derived from the artificial neural\nnetwork) has attracted many researchers' attention by its outstanding\nperformance. However, since this network requires high-performance GPUs and\nlarge storage, it is very hard to use it on individual devices. In order to\nimprove the deep neural network, many trials have been made by refining the\nnetwork structure or training strategy. Unlike those trials, in this paper, we\nfocused on the basic propagation function of the artificial neural network and\nproposed the binarized deep neural network. This network is a pure binary\nsystem, in which all the values and calculations are binarized. As a result,\nour network can save a lot of computational resource and storage. Therefore, it\nis possible to use it on various devices. Moreover, the experimental results\nproved the feasibility of the proposed network. \n\n"}
{"id": "1602.09118", "contents": "Title: Easy Monotonic Policy Iteration Abstract: A key problem in reinforcement learning for control with general function\napproximators (such as deep neural networks and other nonlinear functions) is\nthat, for many algorithms employed in practice, updates to the policy or\n$Q$-function may fail to improve performance---or worse, actually cause the\npolicy performance to degrade. Prior work has addressed this for policy\niteration by deriving tight policy improvement bounds; by optimizing the lower\nbound on policy improvement, a better policy is guaranteed. However, existing\napproaches suffer from bounds that are hard to optimize in practice because\nthey include sup norm terms which cannot be efficiently estimated or\ndifferentiated. In this work, we derive a better policy improvement bound where\nthe sup norm of the policy divergence has been replaced with an average\ndivergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that\ngenerates sequences of policies with guaranteed non-decreasing returns and is\neasy to implement in a sample-based framework. \n\n"}
{"id": "1603.00106", "contents": "Title: Characterizing Diseases from Unstructured Text: A Vocabulary Driven\n  Word2vec Approach Abstract: Traditional disease surveillance can be augmented with a wide variety of\nreal-time sources such as, news and social media. However, these sources are in\ngeneral unstructured and, construction of surveillance tools such as\ntaxonomical correlations and trace mapping involves considerable human\nsupervision. In this paper, we motivate a disease vocabulary driven word2vec\nmodel (Dis2Vec) to model diseases and constituent attributes as word embeddings\nfrom the HealthMap news corpus. We use these word embeddings to automatically\ncreate disease taxonomies and evaluate our model against corresponding human\nannotated taxonomies. We compare our model accuracies against several\nstate-of-the art word2vec methods. Our results demonstrate that Dis2Vec\noutperforms traditional distributed vector representations in its ability to\nfaithfully capture taxonomical attributes across different class of diseases\nsuch as endemic, emerging and rare. \n\n"}
{"id": "1603.01670", "contents": "Title: Network Morphism Abstract: We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme. \n\n"}
{"id": "1603.02412", "contents": "Title: Stochastic dual averaging methods using variance reduction techniques\n  for regularized empirical risk minimization problems Abstract: We consider a composite convex minimization problem associated with\nregularized empirical risk minimization, which often arises in machine\nlearning. We propose two new stochastic gradient methods that are based on\nstochastic dual averaging method with variance reduction. Our methods generate\na sparser solution than the existing methods because we do not need to take the\naverage of the history of the solutions. This is favorable in terms of both\ninterpretability and generalization. Moreover, our methods have theoretical\nsupport for both a strongly and a non-strongly convex regularizer and achieve\nthe best known convergence rates among existing nonaccelerated stochastic\ngradient methods. \n\n"}
{"id": "1603.02839", "contents": "Title: Starting Small -- Learning with Adaptive Sample Sizes Abstract: For many machine learning problems, data is abundant and it may be\nprohibitive to make multiple passes through the full training set. In this\ncontext, we investigate strategies for dynamically increasing the effective\nsample size, when using iterative methods such as stochastic gradient descent.\nOur interest is motivated by the rise of variance-reduced methods, which\nachieve linear convergence rates that scale favorably for smaller sample sizes.\nExploiting this feature, we show -- theoretically and empirically -- how to\nobtain significant speed-ups with a novel algorithm that reaches statistical\naccuracy on an $n$-sample in $2n$, instead of $n \\log n$ steps. \n\n"}
{"id": "1603.02845", "contents": "Title: Unsupervised word segmentation and lexicon discovery using acoustic word\n  embeddings Abstract: In settings where only unlabelled speech data is available, speech technology\nneeds to be developed without transcriptions, pronunciation dictionaries, or\nlanguage modelling text. A similar problem is faced when modelling infant\nlanguage acquisition. In these cases, categorical linguistic structure needs to\nbe discovered directly from speech audio. We present a novel unsupervised\nBayesian model that segments unlabelled speech and clusters the segments into\nhypothesized word groupings. The result is a complete unsupervised tokenization\nof the input speech in terms of discovered word types. In our approach, a\npotential word segment (of arbitrary length) is embedded in a fixed-dimensional\nacoustic vector space. The model, implemented as a Gibbs sampler, then builds a\nwhole-word acoustic model in this space while jointly performing segmentation.\nWe report word error rates in a small-vocabulary connected digit recognition\ntask by mapping the unsupervised decoded output to ground truth transcriptions.\nThe model achieves around 20% error rate, outperforming a previous HMM-based\nsystem by about 10% absolute. Moreover, in contrast to the baseline, our model\ndoes not require a pre-specified vocabulary size. \n\n"}
{"id": "1603.05145", "contents": "Title: Suppressing the Unusual: towards Robust CNNs using Symmetric Activation\n  Functions Abstract: Many deep Convolutional Neural Networks (CNN) make incorrect predictions on\nadversarial samples obtained by imperceptible perturbations of clean samples.\nWe hypothesize that this is caused by a failure to suppress unusual signals\nwithin network layers. As remedy we propose the use of Symmetric Activation\nFunctions (SAF) in non-linear signal transducer units. These units suppress\nsignals of exceptional magnitude. We prove that SAF networks can perform\nclassification tasks to arbitrary precision in a simplified situation. In\npractice, rather than use SAFs alone, we add them into CNNs to improve their\nrobustness. The modified CNNs can be easily trained using popular strategies\nwith the moderate training load. Our experiments on MNIST and CIFAR-10 show\nthat the modified CNNs perform similarly to plain ones on clean samples, and\nare remarkably more robust against adversarial and nonsense samples. \n\n"}
{"id": "1603.05570", "contents": "Title: Predicate Gradual Logic and Linguistics Abstract: There are several major proposals for treating donkey anaphora such as\ndiscourse representation theory and the likes, or E-Type theories and the\nlikes. Every one of them works well for a set of specific examples that they\nuse to demonstrate validity of their approaches. As I show in this paper,\nhowever, they are not very generalisable and do not account for essentially the\nsame problem that they remedy when it manifests in other examples. I propose\nanother logical approach. I develoop logic that extends a recent, propositional\ngradual logic, and show that it can treat donkey anaphora generally. I also\nidentify and address a problem around the modern convention on existential\nimport. Furthermore, I show that Aristotle's syllogisms and conversion are\nrealisable in this logic. \n\n"}
{"id": "1603.07235", "contents": "Title: Global-Local Face Upsampling Network Abstract: Face hallucination, which is the task of generating a high-resolution face\nimage from a low-resolution input image, is a well-studied problem that is\nuseful in widespread application areas. Face hallucination is particularly\nchallenging when the input face resolution is very low (e.g., 10 x 12 pixels)\nand/or the image is captured in an uncontrolled setting with large pose and\nillumination variations. In this paper, we revisit the algorithm introduced in\n[1] and present a deep interpretation of this framework that achieves\nstate-of-the-art under such challenging scenarios. In our deep network\narchitecture the global and local constraints that define a face can be\nefficiently modeled and learned end-to-end using training data. Conceptually\nour network design can be partitioned into two sub-networks: the first one\nimplements the holistic face reconstruction according to global constraints,\nand the second one enhances face-specific details and enforces local patch\nstatistics. We optimize the deep network using a new loss function for\nsuper-resolution that combines reconstruction error with a learned face quality\nmeasure in adversarial setting, producing improved visual results. We conduct\nextensive experiments in both controlled and uncontrolled setups and show that\nour algorithm improves the state of the art both numerically and visually. \n\n"}
{"id": "1603.07834", "contents": "Title: An end-to-end convolutional selective autoencoder approach to Soybean\n  Cyst Nematode eggs detection Abstract: This paper proposes a novel selective autoencoder approach within the\nframework of deep convolutional networks. The crux of the idea is to train a\ndeep convolutional autoencoder to suppress undesired parts of an image frame\nwhile allowing the desired parts resulting in efficient object detection. The\nefficacy of the framework is demonstrated on a critical plant science problem.\nIn the United States, approximately $1 billion is lost per annum due to a\nnematode infection on soybean plants. Currently, plant-pathologists rely on\nlabor-intensive and time-consuming identification of Soybean Cyst Nematode\n(SCN) eggs in soil samples via manual microscopy. The proposed framework\nattempts to significantly expedite the process by using a series of manually\nlabeled microscopic images for training followed by automated high-throughput\negg detection. The problem is particularly difficult due to the presence of a\nlarge population of non-egg particles (disturbances) in the image frames that\nare very similar to SCN eggs in shape, pose and illumination. Therefore, the\nselective autoencoder is trained to learn unique features related to the\ninvariant shapes and sizes of the SCN eggs without handcrafting. After that, a\ncomposite non-maximum suppression and differencing is applied at the\npost-processing stage. \n\n"}
{"id": "1603.08016", "contents": "Title: Classifying Syntactic Regularities for Hundreds of Languages Abstract: This paper presents a comparison of classification methods for linguistic\ntypology for the purpose of expanding an extensive, but sparse language\nresource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath,\n2013). We experimented with a variety of regression and nearest-neighbor\nmethods for use in classification over a set of 325 languages and six syntactic\nrules drawn from WALS. To classify each rule, we consider the typological\nfeatures of the other five rules; linguistic features extracted from a\nword-aligned Bible in each language; and genealogical features (genus and\nfamily) of each language. In general, we find that propagating the majority\nlabel among all languages of the same genus achieves the best accuracy in label\npre- diction. Following this, a logistic regression model that combines\ntypological and linguistic features offers the next best performance.\nInterestingly, this model actually outperforms the majority labels among all\nlanguages of the same family. \n\n"}
{"id": "1603.08367", "contents": "Title: Sparse Activity and Sparse Connectivity in Supervised Learning Abstract: Sparseness is a useful regularizer for learning in a wide range of\napplications, in particular in neural networks. This paper proposes a model\ntargeted at classification tasks, where sparse activity and sparse connectivity\nare used to enhance classification capabilities. The tool for achieving this is\na sparseness-enforcing projection operator which finds the closest vector with\na pre-defined sparseness for any given vector. In the theoretical part of this\npaper, a comprehensive theory for such a projection is developed. In\nconclusion, it is shown that the projection is differentiable almost everywhere\nand can thus be implemented as a smooth neuronal transfer function. The entire\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\non the MNIST database of handwritten digits show that classification\nperformance can be boosted by sparse activity or sparse connectivity. With a\ncombination of both, performance can be significantly better compared to\nclassical non-sparse approaches. \n\n"}
{"id": "1603.09381", "contents": "Title: Clinical Information Extraction via Convolutional Neural Network Abstract: We report an implementation of a clinical information extraction tool that\nleverages deep neural network to annotate event spans and their attributes from\nraw clinical notes and pathology reports. Our approach uses context words and\ntheir part-of-speech tags and shape information as features. Then we hire\ntemporal (1D) convolutional neural network to learn hidden feature\nrepresentations. Finally, we use Multilayer Perceptron (MLP) to predict event\nspans. The empirical evaluation demonstrates that our approach significantly\noutperforms baselines. \n\n"}
{"id": "1604.00772", "contents": "Title: The CMA Evolution Strategy: A Tutorial Abstract: This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands\nfor Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized,\nmethod for real-parameter (continuous domain) optimization of non-linear,\nnon-convex functions. We try to motivate and derive the algorithm from\nintuitive concepts and from requirements of non-linear, non-convex search in\ncontinuous domain. \n\n"}
{"id": "1604.01792", "contents": "Title: Advances in Very Deep Convolutional Neural Networks for LVCSR Abstract: Very deep CNNs with small 3x3 kernels have recently been shown to achieve\nvery strong performance as acoustic models in hybrid NN-HMM speech recognition\nsystems. In this paper we investigate how to efficiently scale these models to\nlarger datasets. Specifically, we address the design choice of pooling and\npadding along the time dimension which renders convolutional evaluation of\nsequences highly inefficient. We propose a new CNN design without timepadding\nand without timepooling, which is slightly suboptimal for accuracy, but has two\nsignificant advantages: it enables sequence training and deployment by allowing\nefficient convolutional evaluation of full utterances, and, it allows for batch\nnormalization to be straightforwardly adopted to CNNs on sequence data. Through\nbatch normalization, we recover the lost peformance from removing the\ntime-pooling, while keeping the benefit of efficient convolutional evaluation.\nWe demonstrate the performance of our models both on larger scale data than\nbefore, and after sequence training. Our very deep CNN model sequence trained\non the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5\ntest-set, matching with a single model the performance of the 2015 IBM system\ncombination, which was the previous best published result. \n\n"}
{"id": "1604.01904", "contents": "Title: Neural Headline Generation with Sentence-wise Optimization Abstract: Recently, neural models have been proposed for headline generation by\nlearning to map documents to headlines with recurrent neural networks.\nNevertheless, as traditional neural network utilizes maximum likelihood\nestimation for parameter optimization, it essentially constrains the expected\ntraining objective within word level rather than sentence level. Moreover, the\nperformance of model prediction significantly relies on training data\ndistribution. To overcome these drawbacks, we employ minimum risk training\nstrategy in this paper, which directly optimizes model parameters in sentence\nlevel with respect to evaluation metrics and leads to significant improvements\nfor headline generation. Experiment results show that our models outperforms\nstate-of-the-art systems on both English and Chinese headline generation tasks. \n\n"}
{"id": "1604.02646", "contents": "Title: Visualization Regularizers for Neural Network based Image Recognition Abstract: The success of deep neural networks is mostly due their ability to learn\nmeaningful features from the data. Features learned in the hidden layers of\ndeep neural networks trained in computer vision tasks have been shown to be\nsimilar to mid-level vision features. We leverage this fact in this work and\npropose the visualization regularizer for image tasks. The proposed\nregularization technique enforces smoothness of the features learned by hidden\nnodes and turns out to be a special case of Tikhonov regularization. We achieve\nhigher classification accuracy as compared to existing regularizers such as the\nL2 norm regularizer and dropout, on benchmark datasets without changing the\ntraining computational complexity. \n\n"}
{"id": "1604.04383", "contents": "Title: Composition of Deep and Spiking Neural Networks for Very Low Bit Rate\n  Speech Coding Abstract: Most current very low bit rate (VLBR) speech coding systems use hidden Markov\nmodel (HMM) based speech recognition/synthesis techniques. This allows\ntransmission of information (such as phonemes) segment by segment that\ndecreases the bit rate. However, the encoder based on a phoneme speech\nrecognition may create bursts of segmental errors. Segmental errors are further\npropagated to optional suprasegmental (such as syllable) information coding.\nTogether with the errors of voicing detection in pitch parametrization,\nHMM-based speech coding creates speech discontinuities and unnatural speech\nsound artefacts.\n  In this paper, we propose a novel VLBR speech coding framework based on\nneural networks (NNs) for end-to-end speech analysis and synthesis without\nHMMs. The speech coding framework relies on phonological (sub-phonetic)\nrepresentation of speech, and it is designed as a composition of deep and\nspiking NNs: a bank of phonological analysers at the transmitter, and a\nphonological synthesizer at the receiver, both realised as deep NNs, and a\nspiking NN as an incremental and robust encoder of syllable boundaries for\ncoding of continuous fundamental frequency (F0). A combination of phonological\nfeatures defines much more sound patterns than phonetic features defined by\nHMM-based speech coders, and the finer analysis/synthesis code contributes into\nsmoother encoded speech. Listeners significantly prefer the NN-based approach\ndue to fewer discontinuities and speech artefacts of the encoded speech. A\nsingle forward pass is required during the speech encoding and decoding. The\nproposed VLBR speech coding operates at a bit rate of approximately 360 bits/s. \n\n"}
{"id": "1604.05499", "contents": "Title: Exploring Segment Representations for Neural Segmentation Models Abstract: Many natural language processing (NLP) tasks can be generalized into\nsegmentation problem. In this paper, we combine semi-CRF with neural network to\nsolve NLP segmentation tasks. Our model represents a segment both by composing\nthe input units and embedding the entire segment. We thoroughly study different\ncomposition functions and different segment embeddings. We conduct extensive\nexperiments on two typical segmentation tasks: named entity recognition (NER)\nand Chinese word segmentation (CWS). Experimental results show that our neural\nsemi-CRF model benefits from representing the entire segment and achieves the\nstate-of-the-art performance on CWS benchmark dataset and competitive results\non the CoNLL03 dataset. \n\n"}
{"id": "1605.02105", "contents": "Title: Distributed Learning with Infinitely Many Hypotheses Abstract: We consider a distributed learning setup where a network of agents\nsequentially access realizations of a set of random variables with unknown\ndistributions. The network objective is to find a parametrized distribution\nthat best describes their joint observations in the sense of the\nKullback-Leibler divergence. Apart from recent efforts in the literature, we\nanalyze the case of countably many hypotheses and the case of a continuum of\nhypotheses. We provide non-asymptotic bounds for the concentration rate of the\nagents' beliefs around the correct hypothesis in terms of the number of agents,\nthe network parameters, and the learning abilities of the agents. Additionally,\nwe provide a novel motivation for a general set of distributed Non-Bayesian\nupdate rules as instances of the distributed stochastic mirror descent\nalgorithm. \n\n"}
{"id": "1605.02216", "contents": "Title: Distributed stochastic optimization for deep learning (thesis) Abstract: We study the problem of how to distribute the training of large-scale deep\nlearning models in the parallel computing environment. We propose a new\ndistributed stochastic optimization method called Elastic Averaging SGD\n(EASGD). We analyze the convergence rate of the EASGD method in the synchronous\nscenario and compare its stability condition with the existing ADMM method in\nthe round-robin scheme. An asynchronous and momentum variant of the EASGD\nmethod is applied to train deep convolutional neural networks for image\nclassification on the CIFAR and ImageNet datasets. Our approach accelerates the\ntraining and furthermore achieves better test accuracy. It also requires a much\nsmaller amount of communication than other common baseline approaches such as\nthe DOWNPOUR method.\n  We then investigate the limit in speedup of the initial and the asymptotic\nphase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We find\nthat the spread of the input data distribution has a big impact on their\ninitial convergence rate and stability region. We also find a surprising\nconnection between the momentum SGD and the EASGD method with a negative moving\naverage rate. A non-convex case is also studied to understand when EASGD can\nget trapped by a saddle point.\n  Finally, we scale up the EASGD method by using a tree structured network\ntopology. We show empirically its advantage and challenge. We also establish a\nconnection between the EASGD and the DOWNPOUR method with the classical Jacobi\nand the Gauss-Seidel method, thus unifying a class of distributed stochastic\noptimization methods. \n\n"}
{"id": "1605.03391", "contents": "Title: Unbiased split variable selection for random survival forests using\n  maximally selected rank statistics Abstract: The most popular approach for analyzing survival data is the Cox regression\nmodel. The Cox model may, however, be misspecified, and its proportionality\nassumption may not always be fulfilled. An alternative approach for survival\nprediction is random forests for survival outcomes. The standard split\ncriterion for random survival forests is the log-rank test statistics, which\nfavors splitting variables with many possible split points. Conditional\ninference forests avoid this split variable selection bias. However, linear\nrank statistics are utilized by default in conditional inference forests to\nselect the optimal splitting variable, which cannot detect non-linear effects\nin the independent variables. An alternative is to use maximally selected rank\nstatistics for the split point selection. As in conditional inference forests,\nsplitting variables are compared on the p-value scale. However, instead of the\nconditional Monte-Carlo approach used in conditional inference forests, p-value\napproximations are employed. We describe several p-value approximations and the\nimplementation of the proposed random forest approach. A simulation study\ndemonstrates that unbiased split variable selection is possible. However, there\nis a trade-off between unbiased split variable selection and runtime. In\nbenchmark studies of prediction performance on simulated and real datasets the\nnew method performs better than random survival forests if informative\ndichotomous variables are combined with uninformative variables with more\ncategories and better than conditional inference forests if non-linear\ncovariate effects are included. In a runtime comparison the method proves to be\ncomputationally faster than both alternatives, if a simple p-value\napproximation is used. \n\n"}
{"id": "1605.03832", "contents": "Title: Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic\n  Representation Learning Abstract: We introduce polyglot language models, recurrent neural network models\ntrained to predict symbol sequences in many different languages using shared\nrepresentations of symbols and conditioning on typological information about\nthe language to be predicted. We apply these to the problem of modeling phone\nsequences---a domain in which universal symbol inventories and\ncross-linguistically shared feature representations are a natural fit.\nIntrinsic evaluation on held-out perplexity, qualitative analysis of the\nlearned representations, and extrinsic evaluation in two downstream\napplications that make use of phonetic features show (i) that polyglot models\nbetter generalize to held-out data than comparable monolingual models and (ii)\nthat polyglot phonetic feature representations are of higher quality than those\nlearned monolingually. \n\n"}
{"id": "1605.03956", "contents": "Title: On the Convergent Properties of Word Embedding Methods Abstract: Do word embeddings converge to learn similar things over different\ninitializations? How repeatable are experiments with word embeddings? Are all\nword embedding techniques equally reliable? In this paper we propose evaluating\nmethods for learning word representations by their consistency across\ninitializations. We propose a measure to quantify the similarity of the learned\nword representations under this setting (where they are subject to different\nrandom initializations). Our preliminary results illustrate that our metric not\nonly measures a intrinsic property of word embedding methods but also\ncorrelates well with other evaluation metrics on downstream tasks. We believe\nour methods are is useful in characterizing robustness -- an important property\nto consider when developing new word embedding methods. \n\n"}
{"id": "1605.04553", "contents": "Title: A Proposal for Linguistic Similarity Datasets Based on Commonality Lists Abstract: Similarity is a core notion that is used in psychology and two branches of\nlinguistics: theoretical and computational. The similarity datasets that come\nfrom the two fields differ in design: psychological datasets are focused around\na certain topic such as fruit names, while linguistic datasets contain words\nfrom various categories. The later makes humans assign low similarity scores to\nthe words that have nothing in common and to the words that have contrast in\nmeaning, making similarity scores ambiguous. In this work we discuss the\nsimilarity collection procedure for a multi-category dataset that avoids score\nambiguity and suggest changes to the evaluation procedure to reflect the\ninsights of psychological literature for word, phrase and sentence similarity.\nWe suggest to ask humans to provide a list of commonalities and differences\ninstead of numerical similarity scores and employ the structure of human\njudgements beyond pairwise similarity for model evaluation. We believe that the\nproposed approach will give rise to datasets that test meaning representation\nmodels more thoroughly with respect to the human treatment of similarity. \n\n"}
{"id": "1605.06855", "contents": "Title: Smart broadcasting: Do you want to be seen? Abstract: Many users in online social networks are constantly trying to gain attention\nfrom their followers by broadcasting posts to them. These broadcasters are\nlikely to gain greater attention if their posts can remain visible for a longer\nperiod of time among their followers' most recent feeds. Then when to post? In\nthis paper, we study the problem of smart broadcasting using the framework of\ntemporal point processes, where we model users feeds and posts as discrete\nevents occurring in continuous time. Based on such continuous-time model, then\nchoosing a broadcasting strategy for a user becomes a problem of designing the\nconditional intensity of her posting events. We derive a novel formula which\nlinks this conditional intensity with the visibility of the user in her\nfollowers' feeds. Furthermore, by exploiting this formula, we develop an\nefficient convex optimization framework for the when-to-post problem. Our\nmethod can find broadcasting strategies that reach a desired visibility level\nwith provable guarantees. We experimented with data gathered from Twitter, and\nshow that our framework can consistently make broadcasters' post more visible\nthan alternatives. \n\n"}
{"id": "1605.08491", "contents": "Title: Provable Algorithms for Inference in Topic Models Abstract: Recently, there has been considerable progress on designing algorithms with\nprovable guarantees -- typically using linear algebraic methods -- for\nparameter learning in latent variable models. But designing provable algorithms\nfor inference has proven to be more challenging. Here we take a first step\ntowards provable inference in topic models. We leverage a property of topic\nmodels that enables us to construct simple linear estimators for the unknown\ntopic proportions that have small variance, and consequently can work with\nshort documents. Our estimators also correspond to finding an estimate around\nwhich the posterior is well-concentrated. We show lower bounds that for shorter\ndocuments it can be information theoretically impossible to find the hidden\ntopics. Finally, we give empirical results that demonstrate that our algorithm\nworks on realistic topic models. It yields good solutions on synthetic data and\nruns in time comparable to a {\\em single} iteration of Gibbs sampling. \n\n"}
{"id": "1605.09128", "contents": "Title: Control of Memory, Active Perception, and Action in Minecraft Abstract: In this paper, we introduce a new set of reinforcement learning (RL) tasks in\nMinecraft (a flexible 3D world). We then use these tasks to systematically\ncompare and contrast existing deep reinforcement learning (DRL) architectures\nwith our new memory-based DRL architectures. These tasks are designed to\nemphasize, in a controllable manner, issues that pose challenges for RL methods\nincluding partial observability (due to first-person visual observations),\ndelayed rewards, high-dimensional visual observations, and the need to use\nactive perception in a correct manner so as to perform well in the tasks. While\nthese tasks are conceptually simple to describe, by virtue of having all of\nthese challenges simultaneously they are difficult for current DRL\narchitectures. Additionally, we evaluate the generalization performance of the\narchitectures on environments not used during training. The experimental\nresults show that our new architectures generalize to unseen environments\nbetter than existing DRL architectures. \n\n"}
{"id": "1605.09477", "contents": "Title: A Neural Autoregressive Approach to Collaborative Filtering Abstract: This paper proposes CF-NADE, a neural autoregressive architecture for\ncollaborative filtering (CF) tasks, which is inspired by the Restricted\nBoltzmann Machine (RBM) based CF model and the Neural Autoregressive\nDistribution Estimator (NADE). We first describe the basic CF-NADE model for CF\ntasks. Then we propose to improve the model by sharing parameters between\ndifferent ratings. A factored version of CF-NADE is also proposed for better\nscalability. Furthermore, we take the ordinal nature of the preferences into\nconsideration and propose an ordinal cost to optimize CF-NADE, which shows\nsuperior performance. Finally, CF-NADE can be extended to a deep model, with\nonly moderately increased computational complexity. Experimental results show\nthat CF-NADE with a single hidden layer beats all previous state-of-the-art\nmethods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more\nhidden layers can further improve the performance. \n\n"}
{"id": "1605.09533", "contents": "Title: Robust Deep-Learning-Based Road-Prediction for Augmented Reality\n  Navigation Systems Abstract: This paper proposes an approach that predicts the road course from camera\nsensors leveraging deep learning techniques. Road pixels are identified by\ntraining a multi-scale convolutional neural network on a large number of\nfull-scene-labeled night-time road images including adverse weather conditions.\nA framework is presented that applies the proposed approach to longer distance\nroad course estimation, which is the basis for an augmented reality navigation\napplication. In this framework long range sensor data (radar) and data from a\nmap database are fused with short range sensor data (camera) to produce a\nprecise longitudinal and lateral localization and road course estimation. The\nproposed approach reliably detects roads with and without lane markings and\nthus increases the robustness and availability of road course estimations and\naugmented reality navigation. Evaluations on an extensive set of high precision\nground truth data taken from a differential GPS and an inertial measurement\nunit show that the proposed approach reaches state-of-the-art performance\nwithout the limitation of requiring existing lane markings. \n\n"}
{"id": "1605.09646", "contents": "Title: Average-case Hardness of RIP Certification Abstract: The restricted isometry property (RIP) for design matrices gives guarantees\nfor optimal recovery in sparse linear models. It is of high interest in\ncompressed sensing and statistical learning. This property is particularly\nimportant for computationally efficient recovery methods. As a consequence,\neven though it is in general NP-hard to check that RIP holds, there have been\nsubstantial efforts to find tractable proxies for it. These would allow the\nconstruction of RIP matrices and the polynomial-time verification of RIP given\nan arbitrary matrix. We consider the framework of average-case certifiers, that\nnever wrongly declare that a matrix is RIP, while being often correct for\nrandom instances. While there are such functions which are tractable in a\nsuboptimal parameter regime, we show that this is a computationally hard task\nin any better regime. Our results are based on a new, weaker assumption on the\nproblem of detecting dense subgraphs. \n\n"}
{"id": "1606.02718", "contents": "Title: Learning Thermodynamics with Boltzmann Machines Abstract: A Boltzmann machine is a stochastic neural network that has been extensively\nused in the layers of deep architectures for modern machine learning\napplications. In this paper, we develop a Boltzmann machine that is capable of\nmodelling thermodynamic observables for physical systems in thermal\nequilibrium. Through unsupervised learning, we train the Boltzmann machine on\ndata sets constructed with spin configurations importance-sampled from the\npartition function of an Ising Hamiltonian at different temperatures using\nMonte Carlo (MC) methods. The trained Boltzmann machine is then used to\ngenerate spin states, for which we compare thermodynamic observables to those\ncomputed by direct MC sampling. We demonstrate that the Boltzmann machine can\nfaithfully reproduce the observables of the physical system. Further, we\nobserve that the number of neurons required to obtain accurate results\nincreases as the system is brought close to criticality. \n\n"}
{"id": "1606.03077", "contents": "Title: Efficient Robust Proper Learning of Log-concave Distributions Abstract: We study the {\\em robust proper learning} of univariate log-concave\ndistributions (over continuous and discrete domains). Given a set of samples\ndrawn from an unknown target distribution, we want to compute a log-concave\nhypothesis distribution that is as close as possible to the target, in total\nvariation distance. In this work, we give the first computationally efficient\nalgorithm for this learning problem. Our algorithm achieves the\ninformation-theoretically optimal sample size (up to a constant factor), runs\nin polynomial time, and is robust to model misspecification with nearly-optimal\nerror guarantees.\n  Specifically, we give an algorithm that, on input $n=O(1/\\eps^{5/2})$ samples\nfrom an unknown distribution $f$, runs in time $\\widetilde{O}(n^{8/5})$, and\noutputs a log-concave hypothesis $h$ that (with high probability) satisfies\n$\\dtv(h, f) = O(\\opt)+\\eps$, where $\\opt$ is the minimum total variation\ndistance between $f$ and the class of log-concave distributions. Our approach\nto the robust proper learning problem is quite flexible and may be applicable\nto many other univariate distribution families. \n\n"}
{"id": "1606.04217", "contents": "Title: Word Representation Models for Morphologically Rich Languages in Neural\n  Machine Translation Abstract: Dealing with the complex word forms in morphologically rich languages is an\nopen problem in language processing, and is particularly important in\ntranslation. In contrast to most modern neural systems of translation, which\ndiscard the identity for rare words, in this paper we propose several\narchitectures for learning word representations from character and morpheme\nlevel word decompositions. We incorporate these representations in a novel\nmachine translation model which jointly learns word alignments and translations\nvia a hard attention mechanism. Evaluating on translating from several\nmorphologically rich languages into English, we show consistent improvements\nover strong baseline methods, of between 1 and 1.5 BLEU points. \n\n"}
{"id": "1606.04754", "contents": "Title: A Correlational Encoder Decoder Architecture for Pivot Based Sequence\n  Generation Abstract: Interlingua based Machine Translation (MT) aims to encode multiple languages\ninto a common linguistic representation and then decode sentences in multiple\ntarget languages from this representation. In this work we explore this idea in\nthe context of neural encoder decoder architectures, albeit on a smaller scale\nand without MT as the end goal. Specifically, we consider the case of three\nlanguages or modalities X, Z and Y wherein we are interested in generating\nsequences in Y starting from information available in X. However, there is no\nparallel training data available between X and Y but, training data is\navailable between X & Z and Z & Y (as is often the case in many real world\napplications). Z thus acts as a pivot/bridge. An obvious solution, which is\nperhaps less elegant but works very well in practice is to train a two stage\nmodel which first converts from X to Z and then from Z to Y. Instead we explore\nan interlingua inspired solution which jointly learns to do the following (i)\nencode X and Z to a common representation and (ii) decode Y from this common\nrepresentation. We evaluate our model on two tasks: (i) bridge transliteration\nand (ii) bridge captioning. We report promising results in both these\napplications and believe that this is a right step towards truly interlingua\ninspired encoder decoder architectures. \n\n"}
{"id": "1606.05336", "contents": "Title: On the Expressive Power of Deep Neural Networks Abstract: We propose a new approach to the problem of neural network expressivity,\nwhich seeks to characterize how structural properties of a neural network\nfamily affect the functions it is able to compute. Our approach is based on an\ninterrelated set of measures of expressivity, unified by the novel notion of\ntrajectory length, which measures how the output of a network changes as the\ninput sweeps along a one-dimensional path. Our findings can be summarized as\nfollows:\n  (1) The complexity of the computed function grows exponentially with depth.\n  (2) All weights are not equal: trained networks are more sensitive to their\nlower (initial) layer weights.\n  (3) Regularizing on trajectory length (trajectory regularization) is a\nsimpler alternative to batch normalization, with the same performance. \n\n"}
{"id": "1606.06121", "contents": "Title: Quantifying and Reducing Stereotypes in Word Embeddings Abstract: Machine learning algorithms are optimized to model statistical properties of\nthe training data. If the input data reflects stereotypes and biases of the\nbroader society, then the output of the learning algorithm also captures these\nstereotypes. In this paper, we initiate the study of gender stereotypes in {\\em\nword embedding}, a popular framework to represent text data. As their use\nbecomes increasingly common, applications can inadvertently amplify unwanted\nstereotypes. We show across multiple datasets that the embeddings contain\nsignificant gender stereotypes, especially with regard to professions. We\ncreated a novel gender analogy task and combined it with crowdsourcing to\nsystematically quantify the gender bias in a given embedding. We developed an\nefficient algorithm that reduces gender stereotype using just a handful of\ntraining examples while preserving the useful geometric properties of the\nembedding. We evaluated our algorithm on several metrics. While we focus on\nmale/female stereotypes, our framework may be applicable to other types of\nembedding biases. \n\n"}
{"id": "1606.06237", "contents": "Title: Online and Differentially-Private Tensor Decomposition Abstract: In this paper, we resolve many of the key algorithmic questions regarding\nrobustness, memory efficiency, and differential privacy of tensor\ndecomposition. We propose simple variants of the tensor power method which\nenjoy these strong properties. We present the first guarantees for online\ntensor power method which has a linear memory requirement. Moreover, we present\na noise calibrated tensor power method with efficient privacy guarantees. At\nthe heart of all these guarantees lies a careful perturbation analysis derived\nin this paper which improves up on the existing results significantly. \n\n"}
{"id": "1606.06361", "contents": "Title: A Probabilistic Generative Grammar for Semantic Parsing Abstract: Domain-general semantic parsing is a long-standing goal in natural language\nprocessing, where the semantic parser is capable of robustly parsing sentences\nfrom domains outside of which it was trained. Current approaches largely rely\non additional supervision from new domains in order to generalize to those\ndomains. We present a generative model of natural language utterances and\nlogical forms and demonstrate its application to semantic parsing. Our approach\nrelies on domain-independent supervision to generalize to new domains. We\nderive and implement efficient algorithms for training, parsing, and sentence\ngeneration. The work relies on a novel application of hierarchical Dirichlet\nprocesses (HDPs) for structured prediction, which we also present in this\nmanuscript.\n  This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov\n(2022), where the model plays a central role in a larger natural language\nunderstanding system.\n  This manuscript provides a new simplified and more complete presentation of\nthe work first introduced in Saparov, Saraswat, and Mitchell (2017). The\ndescription and proofs of correctness of the training algorithm, parsing\nalgorithm, and sentence generation algorithm are much simplified in this new\npresentation. We also describe the novel application of hierarchical Dirichlet\nprocesses for structured prediction. In addition, we extend the earlier work\nwith a new model of word morphology, which utilizes the comprehensive\nmorphological data from Wiktionary. \n\n"}
{"id": "1606.06640", "contents": "Title: Neural Morphological Tagging from Characters for Morphologically Rich\n  Languages Abstract: This paper investigates neural character-based morphological tagging for\nlanguages with complex morphology and large tag sets. We systematically explore\na variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain\ncharacter-based word vectors combined with bidirectional LSTMs to model\nacross-word context in an end-to-end setting. We explore supplementary use of\nword-based vectors trained on large amounts of unlabeled data. Our experiments\nfor morphological tagging suggest that for \"simple\" model configurations, the\nchoice of the network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or\nthe augmentation with pre-trained word embeddings can be important and clearly\nimpact the accuracy. Increasing the model capacity by adding depth, for\nexample, and carefully optimizing the neural networks can lead to substantial\nimprovements, and the differences in accuracy (but not training time) become\nmuch smaller or even negligible. Overall, our best morphological taggers for\nGerman and Czech outperform the best results reported in the literature by a\nlarge margin. \n\n"}
{"id": "1606.06812", "contents": "Title: Link Prediction via Matrix Completion Abstract: Inspired by practical importance of social networks, economic networks,\nbiological networks and so on, studies on large and complex networks have\nattracted a surge of attentions in the recent years. Link prediction is a\nfundamental issue to understand the mechanisms by which new links are added to\nthe networks. We introduce the method of robust principal component analysis\n(robust PCA) into link prediction, and estimate the missing entries of the\nadjacency matrix. On one hand, our algorithm is based on the sparsity and low\nrank property of the matrix, on the other hand, it also performs very well when\nthe network is dense. This is because a relatively dense real network is also\nsparse in comparison to the complete graph. According to extensive experiments\non real networks from disparate fields, when the target network is connected\nand sufficiently dense, whatever it is weighted or unweighted, our method is\ndemonstrated to be very effective and with prediction accuracy being\nconsiderably improved comparing with many state-of-the-art algorithms. \n\n"}
{"id": "1606.07470", "contents": "Title: NN-grams: Unifying neural network and n-gram language models for Speech\n  Recognition Abstract: We present NN-grams, a novel, hybrid language model integrating n-grams and\nneural networks (NN) for speech recognition. The model takes as input both word\nhistories as well as n-gram counts. Thus, it combines the memorization capacity\nand scalability of an n-gram model with the generalization ability of neural\nnetworks. We report experiments where the model is trained on 26B words.\nNN-grams are efficient at run-time since they do not include an output soft-max\nlayer. The model is trained using noise contrastive estimation (NCE), an\napproach that transforms the estimation problem of neural networks into one of\nbinary classification between data samples and noise samples. We present\nresults with noise samples derived from either an n-gram distribution or from\nspeech recognition lattices. NN-grams outperforms an n-gram model on an Italian\nspeech recognition dictation task. \n\n"}
{"id": "1606.07487", "contents": "Title: The VGLC: The Video Game Level Corpus Abstract: Levels are a key component of many different video games, and a large body of\nwork has been produced on how to procedurally generate game levels. Recently,\nMachine Learning techniques have been applied to video game level generation\ntowards the purpose of automatically generating levels that have the properties\nof the training corpus. Towards that end we have made available a corpora of\nvideo game levels in an easy to parse format ideal for different machine\nlearning and other game AI research purposes. \n\n"}
{"id": "1606.07493", "contents": "Title: Sort Story: Sorting Jumbled Images and Captions into Stories Abstract: Temporal common sense has applications in AI tasks such as QA, multi-document\nsummarization, and human-AI communication. We propose the task of sequencing --\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\ntask is to sort them such that the output sequence forms a coherent story. We\npresent multiple approaches, via unary (position) and pairwise (order)\npredictions, and their ensemble-based combinations, achieving strong results on\nthis task. We use both text-based and image-based features, which depict\ncomplementary improvements. Using qualitative examples, we demonstrate that our\nmodels have learnt interesting aspects of temporal common sense. \n\n"}
{"id": "1606.08571", "contents": "Title: Alternating Back-Propagation for Generator Network Abstract: This paper proposes an alternating back-propagation algorithm for learning\nthe generator network model. The model is a non-linear generalization of factor\nanalysis. In this model, the mapping from the continuous latent factors to the\nobserved signal is parametrized by a convolutional neural network. The\nalternating back-propagation algorithm iterates the following two steps: (1)\nInferential back-propagation, which infers the latent factors by Langevin\ndynamics or gradient descent. (2) Learning back-propagation, which updates the\nparameters given the inferred latent factors by gradient descent. The gradient\ncomputations in both steps are powered by back-propagation, and they share most\nof their code in common. We show that the alternating back-propagation\nalgorithm can learn realistic generator models of natural images, video\nsequences, and sounds. Moreover, it can also be used to learn from incomplete\nor indirect training data. \n\n"}
{"id": "1607.00076", "contents": "Title: Multi-class classification: mirror descent approach Abstract: We consider the problem of multi-class classification and a stochastic opti-\nmization approach to it. We derive risk bounds for stochastic mirror descent\nalgorithm and provide examples of set geometries that make the use of the\nalgorithm efficient in terms of error in k. \n\n"}
{"id": "1607.00534", "contents": "Title: Text comparison using word vector representations and dimensionality\n  reduction Abstract: This paper describes a technique to compare large text sources using word\nvector representations (word2vec) and dimensionality reduction (t-SNE) and how\nit can be implemented using Python. The technique provides a bird's-eye view of\ntext sources, e.g. text summaries and their source material, and enables users\nto explore text sources like a geographical map. Word vector representations\ncapture many linguistic properties such as gender, tense, plurality and even\nsemantic concepts like \"capital city of\". Using dimensionality reduction, a 2D\nmap can be computed where semantically similar words are close to each other.\nThe technique uses the word2vec model from the gensim Python library and t-SNE\nfrom scikit-learn. \n\n"}
{"id": "1607.01668", "contents": "Title: Tensor Decomposition for Signal Processing and Machine Learning Abstract: Tensors or {\\em multi-way arrays} are functions of three or more indices\n$(i,j,k,\\cdots)$ -- similar to matrices (two-way arrays), which are functions\nof two indices $(r,c)$ for (row,column). Tensors have a rich history,\nstretching over almost a century, and touching upon numerous disciplines; but\nthey have only recently become ubiquitous in signal and data analytics at the\nconfluence of signal processing, statistics, data mining and machine learning.\nThis overview article aims to provide a good starting point for researchers and\npractitioners interested in learning about and working with tensors. As such,\nit focuses on fundamentals and motivation (using various application examples),\naiming to strike an appropriate balance of breadth {\\em and depth} that will\nenable someone having taken first graduate courses in matrix algebra and\nprobability to get started doing research and/or developing tensor algorithms\nand software. Some background in applied optimization is useful but not\nstrictly required. The material covered includes tensor rank and rank\ndecomposition; basic tensor factorization models and their relationships and\nproperties (including fairly good coverage of identifiability); broad coverage\nof algorithms ranging from alternating optimization to stochastic gradient;\nstatistical performance analysis; and applications ranging from source\nseparation to collaborative filtering, mixture and topic modeling,\nclassification, and multilinear subspace learning. \n\n"}
{"id": "1607.02061", "contents": "Title: Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity Abstract: Several studies on sentence processing suggest that the mental lexicon keeps\ntrack of the mutual expectations between words. Current DSMs, however,\nrepresent context words as separate features, thereby loosing important\ninformation for word expectations, such as word interrelations. In this paper,\nwe present a DSM that addresses this issue by defining verb contexts as joint\nsyntactic dependencies. We test our representation in a verb similarity task on\ntwo datasets, showing that joint contexts achieve performances comparable to\nsingle dependencies or even better. Moreover, they are able to overcome the\ndata sparsity problem of joint feature spaces, in spite of the limited size of\nour training corpus. \n\n"}
{"id": "1607.04606", "contents": "Title: Enriching Word Vectors with Subword Information Abstract: Continuous word representations, trained on large unlabeled corpora are\nuseful for many natural language processing tasks. Popular models that learn\nsuch representations ignore the morphology of words, by assigning a distinct\nvector to each word. This is a limitation, especially for languages with large\nvocabularies and many rare words. In this paper, we propose a new approach\nbased on the skipgram model, where each word is represented as a bag of\ncharacter $n$-grams. A vector representation is associated to each character\n$n$-gram; words being represented as the sum of these representations. Our\nmethod is fast, allowing to train models on large corpora quickly and allows us\nto compute word representations for words that did not appear in the training\ndata. We evaluate our word representations on nine different languages, both on\nword similarity and analogy tasks. By comparing to recently proposed\nmorphological word representations, we show that our vectors achieve\nstate-of-the-art performance on these tasks. \n\n"}
{"id": "1607.05014", "contents": "Title: Language classification from bilingual word embedding graphs Abstract: We study the role of the second language in bilingual word embeddings in\nmonolingual semantic evaluation tasks. We find strongly and weakly positive\ncorrelations between down-stream task performance and second language\nsimilarity to the target language. Additionally, we show how bilingual word\nembeddings can be employed for the task of semantic language classification and\nthat joint semantic spaces vary in meaningful ways across second languages. Our\nresults support the hypothesis that semantic language similarity is influenced\nby both structural similarity as well as geography/contact. \n\n"}
{"id": "1608.00647", "contents": "Title: Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests Abstract: Disparate areas of machine learning have benefited from models that can take\nraw data with little preprocessing as input and learn rich representations of\nthat raw data in order to perform well on a given prediction task. We evaluate\nthis approach in healthcare by using longitudinal measurements of lab tests,\none of the more raw signals of a patient's health state widely available in\nclinical data, to predict disease onsets. In particular, we train a Long\nShort-Term Memory (LSTM) recurrent neural network and two novel convolutional\nneural networks for multi-task prediction of disease onset for 133 conditions\nbased on 18 common lab tests measured over time in a cohort of 298K patients\nderived from 8 years of administrative claims data. We compare the neural\nnetworks to a logistic regression with several hand-engineered, clinically\nrelevant features. We find that the representation-based learning approaches\nsignificantly outperform this baseline. We believe that our work suggests a new\navenue for patient risk stratification based solely on lab results. \n\n"}
{"id": "1608.01405", "contents": "Title: Entailment Relations on Distributions Abstract: In this paper we give an overview of partial orders on the space of\nprobability distributions that carry a notion of information content and serve\nas a generalisation of the Bayesian order given in (Coecke and Martin, 2011).\nWe investigate what constraints are necessary in order to get a unique notion\nof information content. These partial orders can be used to give an ordering on\nwords in vector space models of natural language meaning relating to the\ncontexts in which words are used, which is useful for a notion of entailment\nand word disambiguation. The construction used also points towards a way to\ncreate orderings on the space of density operators which allow a more\nfine-grained study of entailment. The partial orders in this paper are directed\ncomplete and form domains in the sense of domain theory. \n\n"}
{"id": "1608.02153", "contents": "Title: OCR of historical printings with an application to building diachronic\n  corpora: A case study using the RIDGES herbal corpus Abstract: This article describes the results of a case study that applies Neural\nNetwork-based Optical Character Recognition (OCR) to scanned images of books\nprinted between 1487 and 1870 by training the OCR engine OCRopus\n[@breuel2013high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted].\nTraining specific OCR models was possible because the necessary *ground truth*\nis available as error-corrected diplomatic transcriptions. The OCR results have\nbeen evaluated for accuracy against the ground truth of unseen test sets.\nCharacter and word accuracies (percentage of correctly recognized items) for\nthe resulting machine-readable texts of individual documents range from 94% to\nmore than 99% (character level) and from 76% to 97% (word level). This includes\nthe earliest printed books, which were thought to be inaccessible by OCR\nmethods until recently. Furthermore, OCR models trained on one part of the\ncorpus consisting of books with different printing dates and different typesets\n*(mixed models)* have been tested for their predictive power on the books from\nthe other part containing yet other fonts, mostly yielding character accuracies\nwell above 90%. It therefore seems possible to construct generalized models\ntrained on a range of fonts that can be applied to a wide variety of historical\nprintings still giving good results. A moderate postcorrection effort of some\npages will then enable the training of individual models with even better\naccuracies. Using this method, diachronic corpora including early printings can\nbe constructed much faster and cheaper than by manual transcription. The OCR\nmethods reported here open up the possibility of transforming our printed\ntextual cultural heritage into electronic text by largely automatic means,\nwhich is a prerequisite for the mass conversion of scanned books. \n\n"}
{"id": "1608.02717", "contents": "Title: Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task Abstract: We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs. \n\n"}
{"id": "1608.03339", "contents": "Title: Distributed learning with regularized least squares Abstract: We study distributed learning with the least squares regularization scheme in\na reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach,\nthe algorithm partitions a data set into disjoint data subsets, applies the\nleast squares regularization scheme to each data subset to produce an output\nfunction, and then takes an average of the individual output functions as a\nfinal global estimator or predictor. We show with error bounds in expectation\nin both the $L^2$-metric and RKHS-metric that the global output function of\nthis distributed learning is a good approximation to the algorithm processing\nthe whole data in one single machine. Our error bounds are sharp and stated in\na general setting without any eigenfunction assumption. The analysis is\nachieved by a novel second order decomposition of operator differences in our\nintegral operator approach. Even for the classical least squares regularization\nscheme in the RKHS associated with a general kernel, we give the best learning\nrate in the literature. \n\n"}
{"id": "1608.04689", "contents": "Title: A Shallow High-Order Parametric Approach to Data Visualization and\n  Compression Abstract: Explicit high-order feature interactions efficiently capture essential\nstructural knowledge about the data of interest and have been used for\nconstructing generative models. We present a supervised discriminative\nHigh-Order Parametric Embedding (HOPE) approach to data visualization and\ncompression. Compared to deep embedding models with complicated deep\narchitectures, HOPE generates more effective high-order feature mapping through\nan embarrassingly simple shallow model. Furthermore, two approaches to\ngenerating a small number of exemplars conveying high-order interactions to\nrepresent large-scale data sets are proposed. These exemplars in combination\nwith the feature mapping learned by HOPE effectively capture essential data\nvariations. Moreover, through HOPE, these exemplars are employed to increase\nthe computational efficiency of kNN classification for fast information\nretrieval by thousands of times. For classification in two-dimensional\nembedding space on MNIST and USPS datasets, our shallow method HOPE with simple\nSigmoid transformations significantly outperforms state-of-the-art supervised\ndeep embedding models based on deep neural networks, and even achieved\nhistorically low test error rate of 0.65% in two-dimensional space on MNIST,\nwhich demonstrates the representational efficiency and power of supervised\nshallow models with high-order feature interactions. \n\n"}
{"id": "1608.04783", "contents": "Title: Application of multiview techniques to NHANES dataset Abstract: Disease prediction or classification using health datasets involve using\nwell-known predictors associated with the disease as features for the models.\nThis study considers multiple data components of an individual's health, using\nthe relationship between variables to generate features that may improve the\nperformance of disease classification models. In order to capture information\nfrom different aspects of the data, this project uses a multiview learning\napproach, using Canonical Correlation Analysis (CCA), a technique that finds\nprojections with maximum correlations between two data views. Data categories\ncollected from the NHANES survey (1999-2014) are used as views to learn the\nmultiview representations. The usefulness of the representations is\ndemonstrated by applying them as features in a Diabetes classification task. \n\n"}
{"id": "1608.06010", "contents": "Title: Feedback-Controlled Sequential Lasso Screening Abstract: One way to solve lasso problems when the dictionary does not fit into\navailable memory is to first screen the dictionary to remove unneeded features.\nPrior research has shown that sequential screening methods offer the greatest\npromise in this endeavor. Most existing work on sequential screening targets\nthe context of tuning parameter selection, where one screens and solves a\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\nregularization parameters. In contrast, we focus on the scenario where a target\nregularization parameter has already been chosen via cross-validated model\nselection, and we then need to solve many lasso instances using this fixed\nvalue. In this context, we propose and explore a feedback controlled sequential\nscreening scheme. Feedback is used at each iteration to select the next problem\nto be solved. This allows the sequence of problems to be adapted to the\ninstance presented and the number of intermediate problems to be automatically\nselected. We demonstrate our feedback scheme using several datasets including a\ndictionary of approximate size 100,000 by 300,000. \n\n"}
{"id": "1608.06253", "contents": "Title: Multi-Dueling Bandits and Their Application to Online Ranker Evaluation Abstract: New ranking algorithms are continually being developed and refined,\nnecessitating the development of efficient methods for evaluating these\nrankers. Online ranker evaluation focuses on the challenge of efficiently\ndetermining, from implicit user feedback, which ranker out of a finite set of\nrankers is the best. Online ranker evaluation can be modeled by dueling ban-\ndits, a mathematical model for online learning under limited feedback from\npairwise comparisons. Comparisons of pairs of rankers is performed by\ninterleaving their result sets and examining which documents users click on.\nThe dueling bandits model addresses the key issue of which pair of rankers to\ncompare at each iteration, thereby providing a solution to the\nexploration-exploitation trade-off. Recently, methods for simultaneously\ncomparing more than two rankers have been developed. However, the question of\nwhich rankers to compare at each iteration was left open. We address this\nquestion by proposing a generalization of the dueling bandits model that uses\nsimultaneous comparisons of an unrestricted number of rankers. We evaluate our\nalgorithm on synthetic data and several standard large-scale online ranker\nevaluation datasets. Our experimental results show that the algorithm yields\norders of magnitude improvement in performance compared to stateof- the-art\ndueling bandit algorithms. \n\n"}
{"id": "1608.06879", "contents": "Title: AIDE: Fast and Communication Efficient Distributed Optimization Abstract: In this paper, we present two new communication-efficient methods for\ndistributed minimization of an average of functions. The first algorithm is an\ninexact variant of the DANE algorithm that allows any local algorithm to return\nan approximate solution to a local subproblem. We show that such a strategy\ndoes not affect the theoretical guarantees of DANE significantly. In fact, our\napproach can be viewed as a robustification strategy since the method is\nsubstantially better behaved than DANE on data partition arising in practice.\nIt is well known that DANE algorithm does not match the communication\ncomplexity lower bounds. To bridge this gap, we propose an accelerated variant\nof the first method, called AIDE, that not only matches the communication lower\nbounds but can also be implemented using a purely first-order oracle. Our\nempirical results show that AIDE is superior to other communication efficient\nalgorithms in settings that naturally arise in machine learning applications. \n\n"}
{"id": "1608.07019", "contents": "Title: Comparison among dimensionality reduction techniques based on Random\n  Projection for cancer classification Abstract: Random Projection (RP) technique has been widely applied in many scenarios\nbecause it can reduce high-dimensional features into low-dimensional space\nwithin short time and meet the need of real-time analysis of massive data.\nThere is an urgent need of dimensionality reduction with fast increase of big\ngenomics data. However, the performance of RP is usually lower. We attempt to\nimprove classification accuracy of RP through combining other reduction\ndimension methods such as Principle Component Analysis (PCA), Linear\nDiscriminant Analysis (LDA), and Feature Selection (FS). We compared\nclassification accuracy and running time of different combination methods on\nthree microarray datasets and a simulation dataset. Experimental results show a\nremarkable improvement of 14.77% in classification accuracy of FS followed by\nRP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield\na more discriminative subspace with an increase of 13.65% on classification\naccuracy on the same dataset. FS followed by RP outperforms other combination\nmethods in classification accuracy on most of the datasets. \n\n"}
{"id": "1608.07739", "contents": "Title: Bayesian selection for the l2-Potts model regularization parameter: 1D\n  piecewise constant signal denoising Abstract: Piecewise constant denoising can be solved either by deterministic\noptimization approaches, based on the Potts model, or by stochastic Bayesian\nprocedures. The former lead to low computational time but require the selection\nof a regularization parameter, whose value significantly impacts the achieved\nsolution, and whose automated selection remains an involved and challenging\nproblem. Conversely, fully Bayesian formalisms encapsulate the regularization\nparameter selection into hierarchical models, at the price of high\ncomputational costs. This contribution proposes an operational strategy that\ncombines hierarchical Bayesian and Potts model formulations, with the double\naim of automatically tuning the regularization parameter and of maintaining\ncomputational effciency. The proposed procedure relies on formally connecting a\nBayesian framework to a l2-Potts functional. Behaviors and performance for the\nproposed piecewise constant denoising and regularization parameter tuning\ntechniques are studied qualitatively and assessed quantitatively, and shown to\ncompare favorably against those of a fully Bayesian hierarchical procedure,\nboth in accuracy and in computational load. \n\n"}
{"id": "1608.07949", "contents": "Title: Learning-Based Resource Allocation Scheme for TDD-Based CRAN System Abstract: Explosive growth in the use of smart wireless devices has necessitated the\nprovision of higher data rates and always-on connectivity, which are the main\nmotivators for designing the fifth generation (5G) systems. To achieve higher\nsystem efficiency, massive antenna deployment with tight coordination is one\npotential strategy for designing 5G systems, but has two types of associated\nsystem overhead. First is the synchronization overhead, which can be reduced by\nimplementing a cloud radio access network (CRAN)-based architecture design,\nthat separates the baseband processing and radio access functionality to\nachieve better system synchronization. Second is the overhead for acquiring\nchannel state information (CSI) of the users present in the system, which,\nhowever, increases tremendously when instantaneous CSI is used to serve\nhigh-mobility users. To serve a large number of users, a CRAN system with a\ndense deployment of remote radio heads (RRHs) is considered, such that each\nuser has a line-of-sight (LOS) link with the corresponding RRH. Since, the\ntrajectory of movement for high-mobility users is predictable; therefore,\nfairly accurate position estimates for those users can be obtained, and can be\nused for resource allocation to serve the considered users. The resource\nallocation is dependent upon various correlated system parameters, and these\ncorrelations can be learned using well-known \\emph{machine learning}\nalgorithms. This paper proposes a novel \\emph{learning-based resource\nallocation scheme} for time division duplex (TDD) based 5G CRAN systems with\ndense RRH deployment, by using only the users' position estimates for resource\nallocation, thus avoiding the need for CSI acquisition. This reduces the\noverall system overhead significantly, while still achieving near-optimal\nsystem performance; thus, better (effective) system efficiency is achieved.\n(See the paper for full abstract) \n\n"}
{"id": "1609.00070", "contents": "Title: How Much is 131 Million Dollars? Putting Numbers in Perspective with\n  Compositional Descriptions Abstract: How much is 131 million US dollars? To help readers put such numbers in\ncontext, we propose a new task of automatically generating short descriptions\nknown as perspectives, e.g. \"$131 million is about the cost to employ everyone\nin Texas over a lunch period\". First, we collect a dataset of numeric mentions\nin news articles, where each mention is labeled with a set of rated\nperspectives. We then propose a system to generate these descriptions\nconsisting of two steps: formula construction and description generation. In\nconstruction, we compose formulae from numeric facts in a knowledge base and\nrank the resulting formulas based on familiarity, numeric proximity and\nsemantic compatibility. In generation, we convert a formula into natural\nlanguage using a sequence-to-sequence recurrent neural network. Our system\nobtains a 15.2% F1 improvement over a non-compositional baseline at formula\nconstruction and a 12.5 BLEU point improvement over a baseline description\ngeneration. \n\n"}
{"id": "1609.02727", "contents": "Title: Detecting Singleton Review Spammers Using Semantic Similarity Abstract: Online reviews have increasingly become a very important resource for\nconsumers when making purchases. Though it is becoming more and more difficult\nfor people to make well-informed buying decisions without being deceived by\nfake reviews. Prior works on the opinion spam problem mostly considered\nclassifying fake reviews using behavioral user patterns. They focused on\nprolific users who write more than a couple of reviews, discarding one-time\nreviewers. The number of singleton reviewers however is expected to be high for\nmany review websites. While behavioral patterns are effective when dealing with\nelite users, for one-time reviewers, the review text needs to be exploited. In\nthis paper we tackle the problem of detecting fake reviews written by the same\nperson using multiple names, posting each review under a different name. We\npropose two methods to detect similar reviews and show the results generally\noutperform the vectorial similarity measures used in prior works. The first\nmethod extends the semantic similarity between words to the reviews level. The\nsecond method is based on topic modeling and exploits the similarity of the\nreviews topic distributions using two models: bag-of-words and\nbag-of-opinion-phrases. The experiments were conducted on reviews from three\ndifferent datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset\n(800 reviews). \n\n"}
{"id": "1609.03663", "contents": "Title: An Experimental Study of LSTM Encoder-Decoder Model for Text\n  Simplification Abstract: Text simplification (TS) aims to reduce the lexical and structural complexity\nof a text, while still retaining the semantic meaning. Current automatic TS\ntechniques are limited to either lexical-level applications or manually\ndefining a large amount of rules. Since deep neural networks are powerful\nmodels that have achieved excellent performance over many difficult tasks, in\nthis paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder\nmodel for sentence level TS, which makes minimal assumptions about word\nsequence. We conduct preliminary experiments to find that the model is able to\nlearn operation rules such as reversing, sorting and replacing from sequence\npairs, which shows that the model may potentially discover and apply rules such\nas modifying sentence structure, substituting words, and removing words for TS. \n\n"}
{"id": "1609.04309", "contents": "Title: Efficient softmax approximation for GPUs Abstract: We propose an approximate strategy to efficiently train neural network based\nlanguage models over very large vocabularies. Our approach, called adaptive\nsoftmax, circumvents the linear dependency on the vocabulary size by exploiting\nthe unbalanced word distribution to form clusters that explicitly minimize the\nexpectation of computation time. Our approach further reduces the computational\ntime by exploiting the specificities of modern architectures and matrix-matrix\nvector operations, making it particularly suited for graphical processing\nunits. Our experiments carried out on standard benchmarks, such as EuroParl and\nOne Billion Word, show that our approach brings a large gain in efficiency over\nstandard approximations while achieving an accuracy close to that of the full\nsoftmax. The code of our method is available at\nhttps://github.com/facebookresearch/adaptive-softmax. \n\n"}
{"id": "1609.06694", "contents": "Title: PixelNet: Towards a General Pixel-level Architecture Abstract: We explore architectures for general pixel-level prediction problems, from\nlow-level edge detection to mid-level surface normal estimation to high-level\nsemantic segmentation. Convolutional predictors, such as the\nfully-convolutional network (FCN), have achieved remarkable success by\nexploiting the spatial redundancy of neighboring pixels through convolutional\nprocessing. Though computationally efficient, we point out that such approaches\nare not statistically efficient during learning precisely because spatial\nredundancy limits the information learned from neighboring pixels. We\ndemonstrate that (1) stratified sampling allows us to add diversity during\nbatch updates and (2) sampled multi-scale features allow us to explore more\nnonlinear predictors (multiple fully-connected layers followed by ReLU) that\nimprove overall accuracy. Finally, our objective is to show how a architecture\ncan get performance better than (or comparable to) the architectures designed\nfor a particular task. Interestingly, our single architecture produces\nstate-of-the-art results for semantic segmentation on PASCAL-Context, surface\nnormal estimation on NYUDv2 dataset, and edge detection on BSDS without\ncontextual post-processing. \n\n"}
{"id": "1609.06783", "contents": "Title: Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor\n  Processes Abstract: The Dirichlet process and its extension, the Pitman-Yor process, are\nstochastic processes that take probability distributions as a parameter. These\nprocesses can be stacked up to form a hierarchical nonparametric Bayesian\nmodel. In this article, we present efficient methods for the use of these\nprocesses in this hierarchical context, and apply them to latent variable\nmodels for text analytics. In particular, we propose a general framework for\ndesigning these Bayesian models, which are called topic models in the computer\nscience community. We then propose a specific nonparametric Bayesian topic\nmodel for modelling text from social media. We focus on tweets (posts on\nTwitter) in this article due to their ease of access. We find that our\nnonparametric model performs better than existing parametric models in both\ngoodness of fit and real world applications. \n\n"}
{"id": "1609.06831", "contents": "Title: Hawkes Processes with Stochastic Excitations Abstract: We propose an extension to Hawkes processes by treating the levels of\nself-excitation as a stochastic differential equation. Our new point process\nallows better approximation in application domains where events and intensities\naccelerate each other with correlated levels of contagion. We generalize a\nrecent algorithm for simulating draws from Hawkes processes whose levels of\nexcitation are stochastic processes, and propose a hybrid Markov chain Monte\nCarlo approach for model fitting. Our sampling procedure scales linearly with\nthe number of required events and does not require stationarity of the point\nprocess. A modular inference procedure consisting of a combination between\nGibbs and Metropolis Hastings steps is put forward. We recover expectation\nmaximization as a special case. Our general approach is illustrated for\ncontagion following geometric Brownian motion and exponential Langevin\ndynamics. \n\n"}
{"id": "1609.07479", "contents": "Title: Incorporating Relation Paths in Neural Relation Extraction Abstract: Distantly supervised relation extraction has been widely used to find novel\nrelational facts from plain text. To predict the relation between a pair of two\ntarget entities, existing methods solely rely on those direct sentences\ncontaining both entities. In fact, there are also many sentences containing\nonly one of the target entities, which provide rich and useful information for\nrelation extraction. To address this issue, we build inference chains between\ntwo target entities via intermediate entities, and propose a path-based neural\nrelation extraction model to encode the relational semantics from both direct\nsentences and inference chains. Experimental results on real-world datasets\nshow that, our model can make full use of those sentences containing only one\ntarget entity, and achieves significant and consistent improvements on relation\nextraction as compared with baselines. The source code of this paper can be\nobtained from https: //github.com/thunlp/PathNRE. \n\n"}
{"id": "1609.07540", "contents": "Title: Derivative Delay Embedding: Online Modeling of Streaming Time Series Abstract: The staggering amount of streaming time series coming from the real world\ncalls for more efficient and effective online modeling solution. For time\nseries modeling, most existing works make some unrealistic assumptions such as\nthe input data is of fixed length or well aligned, which requires extra effort\non segmentation or normalization of the raw streaming data. Although some\nliterature claim their approaches to be invariant to data length and\nmisalignment, they are too time-consuming to model a streaming time series in\nan online manner. We propose a novel and more practical online modeling and\nclassification scheme, DDE-MGM, which does not make any assumptions on the time\nseries while maintaining high efficiency and state-of-the-art performance. The\nderivative delay embedding (DDE) is developed to incrementally transform time\nseries to the embedding space, where the intrinsic characteristics of data is\npreserved as recursive patterns regardless of the stream length and\nmisalignment. Then, a non-parametric Markov geographic model (MGM) is proposed\nto both model and classify the pattern in an online manner. Experimental\nresults demonstrate the effectiveness and superior classification accuracy of\nthe proposed DDE-MGM in an online setting as compared to the state-of-the-art. \n\n"}
{"id": "1609.08144", "contents": "Title: Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation Abstract: Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system. \n\n"}
{"id": "1609.08496", "contents": "Title: Topic Modeling over Short Texts by Incorporating Word Embeddings Abstract: Inferring topics from the overwhelming amount of short texts becomes a\ncritical but challenging task for many content analysis tasks, such as content\ncharactering, user interest profiling, and emerging topic detecting. Existing\nmethods such as probabilistic latent semantic analysis (PLSA) and latent\nDirichlet allocation (LDA) cannot solve this prob- lem very well since only\nvery limited word co-occurrence information is available in short texts. This\npaper studies how to incorporate the external word correlation knowledge into\nshort texts to improve the coherence of topic modeling. Based on recent results\nin word embeddings that learn se- mantically representations for words from a\nlarge corpus, we introduce a novel method, Embedding-based Topic Model (ETM),\nto learn latent topics from short texts. ETM not only solves the problem of\nvery limited word co-occurrence information by aggregating short texts into\nlong pseudo- texts, but also utilizes a Markov Random Field regularized model\nthat gives correlated words a better chance to be put into the same topic. The\nexperiments on real-world datasets validate the effectiveness of our model\ncomparing with the state-of-the-art models. \n\n"}
{"id": "1609.09823", "contents": "Title: On the Worst-case Communication Overhead for Distributed Data Shuffling Abstract: Distributed learning platforms for processing large scale data-sets are\nbecoming increasingly prevalent. In typical distributed implementations, a\ncentralized master node breaks the data-set into smaller batches for parallel\nprocessing across distributed workers to achieve speed-up and efficiency.\nSeveral computational tasks are of sequential nature, and involve multiple\npasses over the data. At each iteration over the data, it is common practice to\nrandomly re-shuffle the data at the master node, assigning different batches\nfor each worker to process. This random re-shuffling operation comes at the\ncost of extra communication overhead, since at each shuffle, new data points\nneed to be delivered to the distributed workers.\n  In this paper, we focus on characterizing the information theoretically\noptimal communication overhead for the distributed data shuffling problem. We\npropose a novel coded data delivery scheme for the case of no excess storage,\nwhere every worker can only store the assigned data batches under processing.\nOur scheme exploits a new type of coding opportunity and is applicable to any\narbitrary shuffle, and for any number of workers. We also present an\ninformation theoretic lower bound on the minimum communication overhead for\ndata shuffling, and show that the proposed scheme matches this lower bound for\nthe worst-case communication overhead. \n\n"}
{"id": "1610.01101", "contents": "Title: A SMART Stochastic Algorithm for Nonconvex Optimization with\n  Applications to Robust Machine Learning Abstract: In this paper, we show how to transform any optimization problem that arises\nfrom fitting a machine learning model into one that (1) detects and removes\ncontaminated data from the training set while (2) simultaneously fitting the\ntrimmed model on the uncontaminated data that remains. To solve the resulting\nnonconvex optimization problem, we introduce a fast stochastic\nproximal-gradient algorithm that incorporates prior knowledge through nonsmooth\nregularization. For datasets of size $n$, our approach requires\n$O(n^{2/3}/\\varepsilon)$ gradient evaluations to reach $\\varepsilon$-accuracy\nand, when a certain error bound holds, the complexity improves to $O(\\kappa\nn^{2/3}\\log(1/\\varepsilon))$. These rates are $n^{1/3}$ times better than those\nachieved by typical, full gradient methods. \n\n"}
{"id": "1610.02273", "contents": "Title: Near-Data Processing for Differentiable Machine Learning Models Abstract: Near-data processing (NDP) refers to augmenting memory or storage with\nprocessing power. Despite its potential for acceleration computing and reducing\npower requirements, only limited progress has been made in popularizing NDP for\nvarious reasons. Recently, two major changes have occurred that have ignited\nrenewed interest and caused a resurgence of NDP. The first is the success of\nmachine learning (ML), which often demands a great deal of computation for\ntraining, requiring frequent transfers of big data. The second is the\npopularity of NAND flash-based solid-state drives (SSDs) containing multicore\nprocessors that can accommodate extra computation for data processing. In this\npaper, we evaluate the potential of NDP for ML using a new SSD platform that\nallows us to simulate instorage processing (ISP) of ML workloads. Our platform\n(named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD\nthat can execute various ML algorithms using data stored in the SSD. To conduct\na thorough performance analysis and an in-depth comparison with alternative\ntechniques, we focus on a specific algorithm: stochastic gradient descent\n(SGD), which is the de facto standard for training differentiable models such\nas logistic regression and neural networks. We implement and compare three SGD\nvariants (synchronous, Downpour, and elastic averaging) using ISP-ML,\nexploiting the multiple NAND channels to parallelize SGD. In addition, we\ncompare the performance of ISP and that of conventional in-host processing,\nrevealing the advantages of ISP. Based on the advantages and limitations\nidentified through our experiments, we further discuss directions for future\nresearch on ISP for accelerating ML. \n\n"}
{"id": "1610.02683", "contents": "Title: Interpreting Neural Networks to Improve Politeness Comprehension Abstract: We present an interpretable neural network approach to predicting and\nunderstanding politeness in natural language requests. Our models are based on\nsimple convolutional neural networks directly on raw text, avoiding any manual\nidentification of complex sentiment or syntactic features, while performing\nbetter than such feature-based models from previous work. More importantly, we\nuse the challenging task of politeness prediction as a testbed to next present\na much-needed understanding of what these successful networks are actually\nlearning. For this, we present several network visualizations based on\nactivation clusters, first derivative saliency, and embedding space\ntransformations, helping us automatically identify several subtle linguistics\nmarkers of politeness theories. Further, this analysis reveals multiple novel,\nhigh-scoring politeness strategies which, when added back as new features,\nreduce the accuracy gap between the original featurized system and the neural\nmodel, thus providing a clear quantitative interpretation of the success of\nthese neural networks. \n\n"}
{"id": "1610.03009", "contents": "Title: Investigation of Synthetic Speech Detection Using Frame- and\n  Segment-Specific Importance Weighting Abstract: Speaker verification systems are vulnerable to spoofing attacks which\npresents a major problem in their real-life deployment. To date, most of the\nproposed synthetic speech detectors (SSDs) have weighted the importance of\ndifferent segments of speech equally. However, different attack methods have\ndifferent strengths and weaknesses and the traces that they leave may be short\nor long term acoustic artifacts. Moreover, those may occur for only particular\nphonemes or sounds. Here, we propose three algorithms that weigh\nlikelihood-ratio scores of individual frames, phonemes, and sound-classes\ndepending on their importance for the SSD. Significant improvement over the\nbaseline system has been obtained for known attack methods that were used in\ntraining the SSDs. However, improvement with unknown attack types was not\nsubstantial. Thus, the type of distortions that were caused by the unknown\nsystems were different and could not be captured better with the proposed SSD\ncompared to the baseline SSD. \n\n"}
{"id": "1610.03112", "contents": "Title: Leveraging Recurrent Neural Networks for Multimodal Recognition of\n  Social Norm Violation in Dialog Abstract: Social norms are shared rules that govern and facilitate social interaction.\nViolating such social norms via teasing and insults may serve to upend power\nimbalances or, on the contrary reinforce solidarity and rapport in\nconversation, rapport which is highly situated and context-dependent. In this\nwork, we investigate the task of automatically identifying the phenomena of\nsocial norm violation in discourse. Towards this goal, we leverage the power of\nrecurrent neural networks and multimodal information present in the\ninteraction, and propose a predictive model to recognize social norm violation.\nUsing long-term temporal and contextual information, our model achieves an F1\nscore of 0.705. Implications of our work regarding developing a social-aware\nagent are discussed. \n\n"}
{"id": "1610.04167", "contents": "Title: Tensorial Mixture Models Abstract: Casting neural networks in generative frameworks is a highly sought-after\nendeavor these days. Contemporary methods, such as Generative Adversarial\nNetworks, capture some of the generative capabilities, but not all. In\nparticular, they lack the ability of tractable marginalization, and thus are\nnot suitable for many tasks. Other methods, based on arithmetic circuits and\nsum-product networks, do allow tractable marginalization, but their performance\nis challenged by the need to learn the structure of a circuit. Building on the\ntractability of arithmetic circuits, we leverage concepts from tensor analysis,\nand derive a family of generative models we call Tensorial Mixture Models\n(TMMs). TMMs assume a simple convolutional network structure, and in addition,\nlend themselves to theoretical analyses that allow comprehensive understanding\nof the relation between their structure and their expressive properties. We\nthus obtain a generative model that is tractable on one hand, and on the other\nhand, allows effective representation of rich distributions in an easily\ncontrolled manner. These two capabilities are brought together in the task of\nclassification under missing data, where TMMs deliver state of the art\naccuracies with seamless implementation and design. \n\n"}
{"id": "1610.04795", "contents": "Title: Sample Efficient Optimization for Learning Controllers for Bipedal\n  Locomotion Abstract: Learning policies for bipedal locomotion can be difficult, as experiments are\nexpensive and simulation does not usually transfer well to hardware. To counter\nthis, we need al- gorithms that are sample efficient and inherently safe.\nBayesian Optimization is a powerful sample-efficient tool for optimizing\nnon-convex black-box functions. However, its performance can degrade in higher\ndimensions. We develop a distance metric for bipedal locomotion that enhances\nthe sample-efficiency of Bayesian Optimization and use it to train a 16\ndimensional neuromuscular model for planar walking. This distance metric\nreflects some basic gait features of healthy walking and helps us quickly\neliminate a majority of unstable controllers. With our approach we can learn\npolicies for walking in less than 100 trials for a range of challenging\nsettings. In simulation, we show results on two different costs and on various\nterrains including rough ground and ramps, sloping upwards and downwards. We\nalso perturb our models with unknown inertial disturbances analogous with\ndifferences between simulation and hardware. These results are promising, as\nthey indicate that this method can potentially be used to learn control\npolicies on hardware. \n\n"}
{"id": "1610.05688", "contents": "Title: Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models Abstract: Conventional deep neural networks (DNN) for speech acoustic modeling rely on\nGaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary\nclass labels as the targets for DNN training. Subword classes in speech\nrecognition systems correspond to context-dependent tied states or senones. The\npresent work addresses some limitations of GMM-HMM senone alignments for DNN\ntraining. We hypothesize that the senone probabilities obtained from a DNN\ntrained with binary labels can provide more accurate targets to learn better\nacoustic models. However, DNN outputs bear inaccuracies which are exhibited as\nhigh dimensional unstructured noise, whereas the informative components are\nstructured and low-dimensional. We exploit principle component analysis (PCA)\nand sparse coding to characterize the senone subspaces. Enhanced probabilities\nobtained from low-rank and sparse reconstructions are used as soft-targets for\nDNN acoustic modeling, that also enables training with untranscribed data.\nExperiments conducted on AMI corpus shows 4.6% relative reduction in word error\nrate. \n\n"}
{"id": "1610.06283", "contents": "Title: Deep Neural Networks for Improved, Impromptu Trajectory Tracking of\n  Quadrotors Abstract: Trajectory tracking control for quadrotors is important for applications\nranging from surveying and inspection, to film making. However, designing and\ntuning classical controllers, such as proportional-integral-derivative (PID)\ncontrollers, to achieve high tracking precision can be time-consuming and\ndifficult, due to hidden dynamics and other non-idealities. The Deep Neural\nNetwork (DNN), with its superior capability of approximating abstract,\nnonlinear functions, proposes a novel approach for enhancing trajectory\ntracking control. This paper presents a DNN-based algorithm as an add-on module\nthat improves the tracking performance of a classical feedback controller.\nGiven a desired trajectory, the DNNs provide a tailored reference input to the\ncontroller based on their gained experience. The input aims to achieve a unity\nmap between the desired and the output trajectory. The motivation for this work\nis an interactive \"fly-as-you-draw\" application, in which a user draws a\ntrajectory on a mobile device, and a quadrotor instantly flies that trajectory\nwith the DNN-enhanced control system. Experimental results demonstrate that the\nproposed approach improves the tracking precision for user-drawn trajectories\nafter the DNNs are trained on selected periodic trajectories, suggesting the\nmethod's potential in real-world applications. Tracking errors are reduced by\naround 40-50% for both training and testing trajectories from users,\nhighlighting the DNNs' capability of generalizing knowledge. \n\n"}
{"id": "1610.06940", "contents": "Title: Safety Verification of Deep Neural Networks Abstract: Deep neural networks have achieved impressive experimental results in image\nclassification, but can surprisingly be unstable with respect to adversarial\nperturbations, that is, minimal changes to the input image that cause the\nnetwork to misclassify it. With potential applications including perception\nmodules and end-to-end controllers for self-driving cars, this raises concerns\nabout their safety. We develop a novel automated verification framework for\nfeed-forward multi-layer neural networks based on Satisfiability Modulo Theory\n(SMT). We focus on safety of image classification decisions with respect to\nimage manipulations, such as scratches or changes to camera angle or lighting\nconditions that would result in the same class being assigned by a human, and\ndefine safety for an individual decision in terms of invariance of the\nclassification within a small neighbourhood of the original image. We enable\nexhaustive search of the region by employing discretisation, and propagate the\nanalysis layer by layer. Our method works directly with the network code and,\nin contrast to existing methods, can guarantee that adversarial examples, if\nthey exist, are found for the given region and family of manipulations. If\nfound, adversarial examples can be shown to human testers and/or used to\nfine-tune the network. We implement the techniques using Z3 and evaluate them\non state-of-the-art networks, including regularised and deep learning networks.\nWe also compare against existing techniques to search for adversarial examples\nand estimate network robustness. \n\n"}
{"id": "1610.09027", "contents": "Title: Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes Abstract: Neural networks augmented with external memory have the ability to learn\nalgorithmic solutions to complex tasks. These models appear promising for\napplications such as language modeling and machine translation. However, they\nscale poorly in both space and time as the amount of memory grows --- limiting\ntheir applicability to real-world domains. Here, we present an end-to-end\ndifferentiable memory access scheme, which we call Sparse Access Memory (SAM),\nthat retains the representational power of the original approaches whilst\ntraining efficiently with very large memories. We show that SAM achieves\nasymptotic lower bounds in space and time complexity, and find that an\nimplementation runs $1,\\!000\\times$ faster and with $3,\\!000\\times$ less\nphysical memory than non-sparse models. SAM learns with comparable data\nefficiency to existing models on a range of synthetic tasks and one-shot\nOmniglot character recognition, and can scale to tasks requiring $100,\\!000$s\nof time steps and memories. As well, we show how our approach can be adapted\nfor models that maintain temporal associations between memories, as with the\nrecently introduced Differentiable Neural Computer. \n\n"}
{"id": "1610.09722", "contents": "Title: Represent, Aggregate, and Constrain: A Novel Architecture for Machine\n  Reading from Noisy Sources Abstract: In order to extract event information from text, a machine reading model must\nlearn to accurately read and interpret the ways in which that information is\nexpressed. But it must also, as the human reader must, aggregate numerous\nindividual value hypotheses into a single coherent global analysis, applying\nglobal constraints which reflect prior knowledge of the domain.\n  In this work we focus on the task of extracting plane crash event information\nfrom clusters of related news articles whose labels are derived via distant\nsupervision. Unlike previous machine reading work, we assume that while most\ntarget values will occur frequently in most clusters, they may also be missing\nor incorrect.\n  We introduce a novel neural architecture to explicitly model the noisy nature\nof the data and to deal with these aforementioned learning issues. Our models\nare trained end-to-end and achieve an improvement of more than 12.1 F$_1$ over\nprevious work, despite using far less linguistic annotation. We apply factor\ngraph constraints to promote more coherent event analyses, with belief\npropagation inference formulated within the transitions of a recurrent neural\nnetwork. We show this technique additionally improves maximum F$_1$ by up to\n2.8 points, resulting in a relative improvement of $50\\%$ over the previous\nstate-of-the-art. \n\n"}
{"id": "1611.01186", "contents": "Title: Demystifying ResNet Abstract: The Residual Network (ResNet), proposed in He et al. (2015), utilized\nshortcut connections to significantly reduce the difficulty of training, which\nresulted in great performance boosts in terms of both training and\ngeneralization error.\n  It was empirically observed in He et al. (2015) that stacking more layers of\nresidual blocks with shortcut 2 results in smaller training error, while it is\nnot true for shortcut of length 1 or 3. We provide a theoretical explanation\nfor the uniqueness of shortcut 2.\n  We show that with or without nonlinearities, by adding shortcuts that have\ndepth two, the condition number of the Hessian of the loss function at the zero\ninitial point is depth-invariant, which makes training very deep models no more\ndifficult than shallow ones. Shortcuts of higher depth result in an extremely\nflat (high-order) stationary point initially, from which the optimization\nalgorithm is hard to escape. The shortcut 1, however, is essentially equivalent\nto no shortcuts, which has a condition number exploding to infinity as the\nnumber of layers grows. We further argue that as the number of layers tends to\ninfinity, it suffices to only look at the loss function at the zero initial\npoint.\n  Extensive experiments are provided accompanying our theoretical results. We\nshow that initializing the network to small weights with shortcut 2 achieves\nsignificantly better results than random Gaussian (Xavier) initialization,\northogonal initialization, and shortcuts of deeper depth, from various\nperspectives ranging from final loss, learning dynamics and stability, to the\nbehavior of the Hessian along the learning process. \n\n"}
{"id": "1611.01600", "contents": "Title: Loss-aware Binarization of Deep Networks Abstract: Deep neural network models, though very powerful and highly successful, are\ncomputationally expensive in terms of space and time. Recently, there have been\na number of attempts on binarizing the network weights and activations. This\ngreatly reduces the network size, and replaces the underlying multiplications\nto additions or even XNOR bit operations. However, existing binarization\nschemes are based on simple matrix approximation and ignore the effect of\nbinarization on the loss. In this paper, we propose a proximal Newton algorithm\nwith diagonal Hessian approximation that directly minimizes the loss w.r.t. the\nbinarized weights. The underlying proximal step has an efficient closed-form\nsolution, and the second-order information can be efficiently obtained from the\nsecond moments already computed by the Adam optimizer. Experiments on both\nfeedforward and recurrent networks show that the proposed loss-aware\nbinarization algorithm outperforms existing binarization schemes, and is also\nmore robust for wide and deep networks. \n\n"}
{"id": "1611.01726", "contents": "Title: LSTM-Based System-Call Language Modeling and Robust Ensemble Method for\n  Designing Host-Based Intrusion Detection Systems Abstract: In computer security, designing a robust intrusion detection system is one of\nthe most fundamental and important problems. In this paper, we propose a\nsystem-call language-modeling approach for designing anomaly-based host\nintrusion detection systems. To remedy the issue of high false-alarm rates\ncommonly arising in conventional methods, we employ a novel ensemble method\nthat blends multiple thresholding classifiers into a single one, making it\npossible to accumulate 'highly normal' sequences. The proposed system-call\nlanguage model has various advantages leveraged by the fact that it can learn\nthe semantic meaning and interactions of each system call that existing methods\ncannot effectively consider. Through diverse experiments on public benchmark\ndatasets, we demonstrate the validity and effectiveness of the proposed method.\nMoreover, we show that our model possesses high portability, which is one of\nthe key aspects of realizing successful intrusion detection systems. \n\n"}
{"id": "1611.03279", "contents": "Title: Tracing metaphors in time through self-distance in vector spaces Abstract: From a diachronic corpus of Italian, we build consecutive vector spaces in\ntime and use them to compare a term's cosine similarity to itself in different\ntime spans. We assume that a drop in similarity might be related to the\nemergence of a metaphorical sense at a given time. Similarity-based\nobservations are matched to the actual year when a figurative meaning was\ndocumented in a reference dictionary and through manual inspection of corpus\noccurrences. \n\n"}
{"id": "1611.03777", "contents": "Title: Tricks from Deep Learning Abstract: The deep learning community has devised a diverse set of methods to make\ngradient optimization, using large datasets, of large and highly complex models\nwith deeply cascaded nonlinearities, practical. Taken as a whole, these methods\nconstitute a breakthrough, allowing computational structures which are quite\nwide, very deep, and with an enormous number and variety of free parameters to\nbe effectively optimized. The result now dominates much of practical machine\nlearning, with applications in machine translation, computer vision, and speech\nrecognition. Many of these methods, viewed through the lens of algorithmic\ndifferentiation (AD), can be seen as either addressing issues with the gradient\nitself, or finding ways of achieving increased efficiency using tricks that are\nAD-related, but not provided by current AD systems.\n  The goal of this paper is to explain not just those methods of most relevance\nto AD, but also the technical constraints and mindset which led to their\ndiscovery. After explaining this context, we present a \"laundry list\" of\nmethods developed by the deep learning community. Two of these are discussed in\nfurther mathematical detail: a way to dramatically reduce the size of the tape\nwhen performing reverse-mode AD on a (theoretically) time-reversible process\nlike an ODE integrator; and a new mathematical insight that allows for the\nimplementation of a stochastic Newton's method. \n\n"}
{"id": "1611.04149", "contents": "Title: Accelerated Variance Reduced Block Coordinate Descent Abstract: Algorithms with fast convergence, small number of data access, and low\nper-iteration complexity are particularly favorable in the big data era, due to\nthe demand for obtaining \\emph{highly accurate solutions} to problems with\n\\emph{a large number of samples} in \\emph{ultra-high} dimensional space.\nExisting algorithms lack at least one of these qualities, and thus are\ninefficient in handling such big data challenge. In this paper, we propose a\nmethod enjoying all these merits with an accelerated convergence rate\n$O(\\frac{1}{k^2})$. Empirical studies on large scale datasets with more than\none million features are conducted to show the effectiveness of our methods in\npractice. \n\n"}
{"id": "1611.04416", "contents": "Title: On numerical approximation schemes for expectation propagation Abstract: Several numerical approximation strategies for the expectation-propagation\nalgorithm are studied in the context of large-scale learning: the Laplace\nmethod, a faster variant of it, Gaussian quadrature, and a deterministic\nversion of variational sampling (i.e., combining quadrature with variational\napproximation). Experiments in training linear binary classifiers show that the\nexpectation-propagation algorithm converges best using variational sampling,\nwhile it also converges well using Laplace-style methods with smooth factors\nbut tends to be unstable with non-differentiable ones. Gaussian quadrature\nyields unstable behavior or convergence to a sub-optimal solution in most\nexperiments. \n\n"}
{"id": "1611.04835", "contents": "Title: Multilinear Low-Rank Tensors on Graphs & Applications Abstract: We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework. \n\n"}
{"id": "1611.04847", "contents": "Title: The Power of Side-information in Subgraph Detection Abstract: In this work, we tackle the problem of hidden community detection. We\nconsider Belief Propagation (BP) applied to the problem of detecting a hidden\nErd\\H{o}s-R\\'enyi (ER) graph embedded in a larger and sparser ER graph, in the\npresence of side-information. We derive two related algorithms based on BP to\nperform subgraph detection in the presence of two kinds of side-information.\nThe first variant of side-information consists of a set of nodes, called cues,\nknown to be from the subgraph. The second variant of side-information consists\nof a set of nodes that are cues with a given probability. It was shown in past\nworks that BP without side-information fails to detect the subgraph correctly\nwhen an effective signal-to-noise ratio (SNR) parameter falls below a\nthreshold. In contrast, in the presence of non-trivial side-information, we\nshow that the BP algorithm achieves asymptotically zero error for any value of\nthe SNR parameter. We validate our results through simulations on synthetic\ndatasets as well as on a few real world networks. \n\n"}
{"id": "1611.04871", "contents": "Title: Audio Event and Scene Recognition: A Unified Approach using Strongly and\n  Weakly Labeled Data Abstract: In this paper we propose a novel learning framework called Supervised and\nWeakly Supervised Learning where the goal is to learn simultaneously from\nweakly and strongly labeled data. Strongly labeled data can be simply\nunderstood as fully supervised data where all labeled instances are available.\nIn weakly supervised learning only data is weakly labeled which prevents one\nfrom directly applying supervised learning methods. Our proposed framework is\nmotivated by the fact that a small amount of strongly labeled data can give\nconsiderable improvement over only weakly supervised learning. The primary\nproblem domain focus of this paper is acoustic event and scene detection in\naudio recordings. We first propose a naive formulation for leveraging labeled\ndata in both forms. We then propose a more general framework for Supervised and\nWeakly Supervised Learning (SWSL). Based on this general framework, we propose\na graph based approach for SWSL. Our main method is based on manifold\nregularization on graphs in which we show that the unified learning can be\nformulated as a constraint optimization problem which can be solved by\niterative concave-convex procedure (CCCP). Our experiments show that our\nproposed framework can address several concerns of audio content analysis using\nweakly labeled data. \n\n"}
{"id": "1611.05013", "contents": "Title: PixelVAE: A Latent Variable Model for Natural Images Abstract: Natural image modeling is a landmark challenge of unsupervised learning.\nVariational Autoencoders (VAEs) learn a useful latent representation and model\nglobal structure well but have difficulty capturing small details. PixelCNN\nmodels details very well, but lacks a latent code and is difficult to scale for\ncapturing large structures. We present PixelVAE, a VAE model with an\nautoregressive decoder based on PixelCNN. Our model requires very few expensive\nautoregressive layers compared to PixelCNN and learns latent codes that are\nmore compressed than a standard VAE while still capturing most non-trivial\nstructure. Finally, we extend our model to a hierarchy of latent variables at\ndifferent scales. Our model achieves state-of-the-art performance on binarized\nMNIST, competitive performance on 64x64 ImageNet, and high-quality samples on\nthe LSUN bedrooms dataset. \n\n"}
{"id": "1611.06310", "contents": "Title: Local minima in training of neural networks Abstract: There has been a lot of recent interest in trying to characterize the error\nsurface of deep models. This stems from a long standing question. Given that\ndeep networks are highly nonlinear systems optimized by local gradient methods,\nwhy do they not seem to be affected by bad local minima? It is widely believed\nthat training of deep models using gradient methods works so well because the\nerror surface either has no local minima, or if they exist they need to be\nclose in value to the global minimum. It is known that such results hold under\nvery strong assumptions which are not satisfied by real models. In this paper\nwe present examples showing that for such theorem to be true additional\nassumptions on the data, initialization schemes and/or the model classes have\nto be made. We look at the particular case of finite size datasets. We\ndemonstrate that in this scenario one can construct counter-examples (datasets\nor initialization schemes) when the network does become susceptible to bad\nlocal minima over the weight space. \n\n"}
{"id": "1611.07429", "contents": "Title: TreeView: Peeking into Deep Neural Networks Via Feature-Space\n  Partitioning Abstract: With the advent of highly predictive but opaque deep learning models, it has\nbecome more important than ever to understand and explain the predictions of\nsuch models. Existing approaches define interpretability as the inverse of\ncomplexity and achieve interpretability at the cost of accuracy. This\nintroduces a risk of producing interpretable but misleading explanations. As\nhumans, we are prone to engage in this kind of behavior \\cite{mythos}. In this\npaper, we take a step in the direction of tackling the problem of\ninterpretability without compromising the model accuracy. We propose to build a\nTreeview representation of the complex model via hierarchical partitioning of\nthe feature space, which reveals the iterative rejection of unlikely class\nlabels until the correct association is predicted. \n\n"}
{"id": "1611.08459", "contents": "Title: Neural Machine Translation with Latent Semantic of Image and Text Abstract: Although attention-based Neural Machine Translation have achieved great\nsuccess, attention-mechanism cannot capture the entire meaning of the source\nsentence because the attention mechanism generates a target word depending\nheavily on the relevant parts of the source sentence. The report of earlier\nstudies has introduced a latent variable to capture the entire meaning of\nsentence and achieved improvement on attention-based Neural Machine\nTranslation. We follow this approach and we believe that the capturing meaning\nof sentence benefits from image information because human beings understand the\nmeaning of language not only from textual information but also from perceptual\ninformation such as that gained from vision. As described herein, we propose a\nneural machine translation model that introduces a continuous latent variable\ncontaining an underlying semantic extracted from texts and images. Our model,\nwhich can be trained end-to-end, requires image information only when training.\nExperiments conducted with an English--German translation task show that our\nmodel outperforms over the baseline. \n\n"}
{"id": "1611.08655", "contents": "Title: A Deep Neural Network to identify foreshocks in real time Abstract: Foreshock events provide valuable insight to predict imminent major\nearthquakes. However, it is difficult to identify them in real time. In this\npaper, I propose an algorithm based on deep learning to instantaneously\nclassify a seismic waveform as a foreshock, mainshock or an aftershock event\nachieving a high accuracy of 99% in classification. As a result, this is by far\nthe most reliable method to predict major earthquakes that are preceded by\nforeshocks. In addition, I discuss methods to create an earthquake dataset that\nis compatible with deep networks. \n\n"}
{"id": "1611.09328", "contents": "Title: Accelerated Gradient Temporal Difference Learning Abstract: The family of temporal difference (TD) methods span a spectrum from\ncomputationally frugal linear methods like TD({\\lambda}) to data efficient\nleast squares methods. Least square methods make the best use of available data\ndirectly computing the TD solution and thus do not require tuning a typically\nhighly sensitive learning rate parameter, but require quadratic computation and\nstorage. Recent algorithmic developments have yielded several sub-quadratic\nmethods that use an approximation to the least squares TD solution, but incur\nbias. In this paper, we propose a new family of accelerated gradient TD (ATD)\nmethods that (1) provide similar data efficiency benefits to least-squares\nmethods, at a fraction of the computation and storage (2) significantly reduce\nparameter sensitivity compared to linear TD methods, and (3) are asymptotically\nunbiased. We illustrate these claims with a proof of convergence in expectation\nand experiments on several benchmark domains and a large-scale industrial\nenergy allocation domain. \n\n"}
{"id": "1611.09340", "contents": "Title: Diet Networks: Thin Parameters for Fat Genomics Abstract: Learning tasks such as those involving genomic data often poses a serious\nchallenge: the number of input features can be orders of magnitude larger than\nthe number of training examples, making it difficult to avoid overfitting, even\nwhen using the known regularization techniques. We focus here on tasks in which\nthe input is a description of the genetic variation specific to a patient, the\nsingle nucleotide polymorphisms (SNPs), yielding millions of ternary inputs.\nImproving the ability of deep learning to handle such datasets could have an\nimportant impact in precision medicine, where high-dimensional data regarding a\nparticular patient is used to make predictions of interest. Even though the\namount of data for such tasks is increasing, this mismatch between the number\nof examples and the number of inputs remains a concern. Naive implementations\nof classifier neural networks involve a huge number of free parameters in their\nfirst layer: each input feature is associated with as many parameters as there\nare hidden units. We propose a novel neural network parametrization which\nconsiderably reduces the number of free parameters. It is based on the idea\nthat we can first learn or provide a distributed representation for each input\nfeature (e.g. for each position in the genome where variations are observed),\nand then learn (with another neural network called the parameter prediction\nnetwork) how to map a feature's distributed representation to the vector of\nparameters specific to that feature in the classifier neural network (the\nweights which link the value of the feature to each of the hidden units). We\nshow experimentally on a population stratification task of interest to medical\nstudies that the proposed approach can significantly reduce both the number of\nparameters and the error rate of the classifier. \n\n"}
{"id": "1611.09434", "contents": "Title: Input Switched Affine Networks: An RNN Architecture Designed for\n  Interpretability Abstract: There exist many problem domains where the interpretability of neural network\nmodels is essential for deployment. Here we introduce a recurrent architecture\ncomposed of input-switched affine transformations - in other words an RNN\nwithout any explicit nonlinearities, but with input-dependent recurrent\nweights. This simple form allows the RNN to be analyzed via straightforward\nlinear methods: we can exactly characterize the linear contribution of each\ninput to the model predictions; we can use a change-of-basis to disentangle\ninput, output, and computational hidden unit subspaces; we can fully\nreverse-engineer the architecture's solution to a simple task. Despite this\nease of interpretation, the input switched affine network achieves reasonable\nperformance on a text modeling tasks, and allows greater computational\nefficiency than networks with standard nonlinearities. \n\n"}
{"id": "1612.00212", "contents": "Title: Training Bit Fully Convolutional Network for Fast Semantic Segmentation Abstract: Fully convolutional neural networks give accurate, per-pixel prediction for\ninput images and have applications like semantic segmentation. However, a\ntypical FCN usually requires lots of floating point computation and large\nrun-time memory, which effectively limits its usability. We propose a method to\ntrain Bit Fully Convolution Network (BFCN), a fully convolutional neural\nnetwork that has low bit-width weights and activations. Because most of its\ncomputation-intensive convolutions are accomplished between low bit-width\nnumbers, a BFCN can be accelerated by an efficient bit-convolution\nimplementation. On CPU, the dot product operation between two bit vectors can\nbe reduced to bitwise operations and popcounts, which can offer much higher\nthroughput than 32-bit multiplications and additions.\n  To validate the effectiveness of BFCN, we conduct experiments on the PASCAL\nVOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights\nand 2-bit activations, which runs 7.8x faster on CPU or requires less than 1\\%\nresources on FPGA, can achieve comparable performance as the 32-bit\ncounterpart. \n\n"}
{"id": "1612.00394", "contents": "Title: Definition Modeling: Learning to define word embeddings in natural\n  language Abstract: Distributed representations of words have been shown to capture lexical\nsemantics, as demonstrated by their effectiveness in word similarity and\nanalogical relation tasks. But, these tasks only evaluate lexical semantics\nindirectly. In this paper, we study whether it is possible to utilize\ndistributed representations to generate dictionary definitions of words, as a\nmore direct and transparent representation of the embeddings' semantics. We\nintroduce definition modeling, the task of generating a definition for a given\nword and its embedding. We present several definition model architectures based\non recurrent neural networks, and experiment with the models over multiple data\nsets. Our results show that a model that controls dependencies between the word\nbeing defined and the definition words performs significantly better, and that\na character-level convolution layer designed to leverage morphology can\ncomplement word-level embeddings. Finally, an error analysis suggests that the\nerrors made by a definition model may provide insight into the shortcomings of\nword embeddings. \n\n"}
{"id": "1612.00542", "contents": "Title: Breast Mass Classification from Mammograms using Deep Convolutional\n  Neural Networks Abstract: Mammography is the most widely used method to screen breast cancer. Because\nof its mostly manual nature, variability in mass appearance, and low\nsignal-to-noise ratio, a significant number of breast masses are missed or\nmisdiagnosed. In this work, we present how Convolutional Neural Networks can be\nused to directly classify pre-segmented breast masses in mammograms as benign\nor malignant, using a combination of transfer learning, careful pre-processing\nand data augmentation to overcome limited training data. We achieve\nstate-of-the-art results on the DDSM dataset, surpassing human performance, and\nshow interpretability of our model. \n\n"}
{"id": "1612.01425", "contents": "Title: Zeroth-order Asynchronous Doubly Stochastic Algorithm with Variance\n  Reduction Abstract: Zeroth-order (derivative-free) optimization attracts a lot of attention in\nmachine learning, because explicit gradient calculations may be computationally\nexpensive or infeasible. To handle large scale problems both in volume and\ndimension, recently asynchronous doubly stochastic zeroth-order algorithms were\nproposed. The convergence rate of existing asynchronous doubly stochastic\nzeroth order algorithms is $O(\\frac{1}{\\sqrt{T}})$ (also for the sequential\nstochastic zeroth-order optimization algorithms). In this paper, we focus on\nthe finite sums of smooth but not necessarily convex functions, and propose an\nasynchronous doubly stochastic zeroth-order optimization algorithm using the\naccelerated technology of variance reduction (AsyDSZOVR). Rigorous theoretical\nanalysis show that the convergence rate can be improved from\n$O(\\frac{1}{\\sqrt{T}})$ the best result of existing algorithms to\n$O(\\frac{1}{T})$. Also our theoretical results is an improvement to the ones of\nthe sequential stochastic zeroth-order optimization algorithms. \n\n"}
{"id": "1612.01600", "contents": "Title: Distributed Gaussian Learning over Time-varying Directed Graphs Abstract: We present a distributed (non-Bayesian) learning algorithm for the problem of\nparameter estimation with Gaussian noise. The algorithm is expressed as\nexplicit updates on the parameters of the Gaussian beliefs (i.e. means and\nprecision). We show a convergence rate of $O(1/k)$ with the constant term\ndepending on the number of agents and the topology of the network. Moreover, we\nshow almost sure convergence to the optimal solution of the estimation problem\nfor the general case of time-varying directed graphs. \n\n"}
{"id": "1612.01663", "contents": "Title: Efficient Non-oblivious Randomized Reduction for Risk Minimization with\n  Improved Excess Risk Guarantee Abstract: In this paper, we address learning problems for high dimensional data.\nPreviously, oblivious random projection based approaches that project high\ndimensional features onto a random subspace have been used in practice for\ntackling high-dimensionality challenge in machine learning. Recently, various\nnon-oblivious randomized reduction methods have been developed and deployed for\nsolving many numerical problems such as matrix product approximation, low-rank\nmatrix approximation, etc. However, they are less explored for the machine\nlearning tasks, e.g., classification. More seriously, the theoretical analysis\nof excess risk bounds for risk minimization, an important measure of\ngeneralization performance, has not been established for non-oblivious\nrandomized reduction methods. It therefore remains an open problem what is the\nbenefit of using them over previous oblivious random projection based\napproaches. To tackle these challenges, we propose an algorithmic framework for\nemploying non-oblivious randomized reduction method for general empirical risk\nminimizing in machine learning tasks, where the original high-dimensional\nfeatures are projected onto a random subspace that is derived from the data\nwith a small matrix approximation error. We then derive the first excess risk\nbound for the proposed non-oblivious randomized reduction approach without\nrequiring strong assumptions on the training data. The established excess risk\nbound exhibits that the proposed approach provides much better generalization\nperformance and it also sheds more insights about different randomized\nreduction approaches. Finally, we conduct extensive experiments on both\nsynthetic and real-world benchmark datasets, whose dimension scales to\n$O(10^7)$, to demonstrate the efficacy of our proposed approach. \n\n"}
{"id": "1612.02741", "contents": "Title: Coupling Distributed and Symbolic Execution for Natural Language Queries Abstract: Building neural networks to query a knowledge base (a table) with natural\nlanguage is an emerging research topic in deep learning. An executor for table\nquerying typically requires multiple steps of execution because queries may\nhave complicated structures. In previous studies, researchers have developed\neither fully distributed executors or symbolic executors for table querying. A\ndistributed executor can be trained in an end-to-end fashion, but is weak in\nterms of execution efficiency and explicit interpretability. A symbolic\nexecutor is efficient in execution, but is very difficult to train especially\nat initial stages. In this paper, we propose to couple distributed and symbolic\nexecution for natural language queries, where the symbolic executor is\npretrained with the distributed executor's intermediate execution results in a\nstep-by-step fashion. Experiments show that our approach significantly\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\nhigh learning efficiency, high execution efficiency, and high interpretability. \n\n"}
{"id": "1612.03663", "contents": "Title: Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification Abstract: Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest. \n\n"}
{"id": "1612.03809", "contents": "Title: Generalizable Features From Unsupervised Learning Abstract: Humans learn a predictive model of the world and use this model to reason\nabout future events and the consequences of actions. In contrast to most\nmachine predictors, we exhibit an impressive ability to generalize to unseen\nscenarios and reason intelligently in these settings. One important aspect of\nthis ability is physical intuition(Lake et al., 2016). In this work, we explore\nthe potential of unsupervised learning to find features that promote better\ngeneralization to settings outside the supervised training distribution. Our\ntask is predicting the stability of towers of square blocks. We demonstrate\nthat an unsupervised model, trained to predict future frames of a video\nsequence of stable and unstable block configurations, can yield features that\nsupport extrapolating stability prediction to blocks configurations outside the\ntraining set distribution \n\n"}
{"id": "1612.03839", "contents": "Title: Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD) Abstract: Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases. \n\n"}
{"id": "1612.04629", "contents": "Title: How Grammatical is Character-level Neural Machine Translation? Assessing\n  MT Quality with Contrastive Translation Pairs Abstract: Analysing translation quality in regards to specific linguistic phenomena has\nhistorically been difficult and time-consuming. Neural machine translation has\nthe attractive property that it can produce scores for arbitrary translations,\nand we propose a novel method to assess how well NMT systems model specific\nlinguistic phenomena such as agreement over long distances, the production of\nnovel words, and the faithful translation of polarity. The core idea is that we\nmeasure whether a reference translation is more probable under a NMT model than\na contrastive translation which introduces a specific type of error. We present\nLingEval97, a large-scale data set of 97000 contrastive translation pairs based\non the WMT English->German translation task, with errors automatically created\nwith simple rules. We report results for a number of systems, and find that\nrecently introduced character-level NMT systems perform better at\ntransliteration than models with byte-pair encoding (BPE) segmentation, but\nperform more poorly at morphosyntactic agreement, and translating discontiguous\nunits of meaning. \n\n"}
{"id": "1612.04642", "contents": "Title: Harmonic Networks: Deep Translation and Rotation Equivariance Abstract: Translating or rotating an input image should not affect the results of many\ncomputer vision tasks. Convolutional neural networks (CNNs) are already\ntranslation equivariant: input image translations produce proportionate feature\nmap translations. This is not the case for rotations. Global rotation\nequivariance is typically sought through data augmentation, but patch-wise\nequivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN\nexhibiting equivariance to patch-wise translation and 360-rotation. We achieve\nthis by replacing regular CNN filters with circular harmonics, returning a\nmaximal response and orientation for every receptive field patch.\n  H-Nets use a rich, parameter-efficient and low computational complexity\nrepresentation, and we show that deep feature maps within the network encode\ncomplicated rotational invariants. We demonstrate that our layers are general\nenough to be used in conjunction with the latest architectures and techniques,\nsuch as deep supervision and batch normalization. We also achieve\nstate-of-the-art classification on rotated-MNIST, and competitive results on\nother benchmark challenges. \n\n"}
{"id": "1612.05968", "contents": "Title: Deep Multi-instance Networks with Sparse Label Assignment for Whole\n  Mammogram Classification Abstract: Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods requires great effort to annotate the\ntraining data by costly manual labeling and specialized computational models to\ndetect these annotations during test. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\nfor labeling a set of instances/patches, we propose end-to-end trained deep\nmulti-instance networks for mass classification based on whole mammogram\nwithout the aforementioned costly need to annotate the training data. We\nexplore three different schemes to construct deep multi-instance networks for\nwhole mammogram classification. Experimental results on the INbreast dataset\ndemonstrate the robustness of proposed deep networks compared to previous work\nusing segmentation and detection annotations in the training. \n\n"}
{"id": "1612.06000", "contents": "Title: Sample-efficient Deep Reinforcement Learning for Dialog Control Abstract: Representing a dialog policy as a recurrent neural network (RNN) is\nattractive because it handles partial observability, infers a latent\nrepresentation of state, and can be optimized with supervised learning (SL) or\nreinforcement learning (RL). For RL, a policy gradient approach is natural, but\nis sample inefficient. In this paper, we present 3 methods for reducing the\nnumber of dialogs required to optimize an RNN-based dialog policy with RL. The\nkey idea is to maintain a second RNN which predicts the value of the current\npolicy, and to apply experience replay to both networks. On two tasks, these\nmethods reduce the number of dialogs/episodes required by about a third, vs.\nstandard policy gradient methods. \n\n"}
{"id": "1612.06549", "contents": "Title: Exploring Different Dimensions of Attention for Uncertainty Detection Abstract: Neural networks with attention have proven effective for many natural\nlanguage processing tasks. In this paper, we develop attention mechanisms for\nuncertainty detection. In particular, we generalize standardly used attention\nmechanisms by introducing external attention and sequence-preserving attention.\nThese novel architectures differ from standard approaches in that they use\nexternal resources to compute attention weights and preserve sequence\ninformation. We compare them to other configurations along different dimensions\nof attention. Our novel architectures set the new state of the art on a\nWikipedia benchmark dataset and perform similar to the state-of-the-art model\non a biomedical benchmark which uses a large set of linguistic features. \n\n"}
{"id": "1612.06704", "contents": "Title: Action-Driven Object Detection with Top-Down Visual Attentions Abstract: A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7. \n\n"}
{"id": "1701.00757", "contents": "Title: Clustering Signed Networks with the Geometric Mean of Laplacians Abstract: Signed networks allow to model positive and negative relationships. We\nanalyze existing extensions of spectral clustering to signed networks. It turns\nout that existing approaches do not recover the ground truth clustering in\nseveral situations where either the positive or the negative network structures\ncontain no noise. Our analysis shows that these problems arise as existing\napproaches take some form of arithmetic mean of the Laplacians of the positive\nand negative part. As a solution we propose to use the geometric mean of the\nLaplacians of positive and negative part and show that it outperforms the\nexisting approaches. While the geometric mean of matrices is computationally\nexpensive, we show that eigenvectors of the geometric mean can be computed\nefficiently, leading to a numerical scheme for sparse matrices which is of\nindependent interest. \n\n"}
{"id": "1701.01896", "contents": "Title: Universality for eigenvalue algorithms on sample covariance matrices Abstract: We prove a universal limit theorem for the halting time, or iteration count,\nof the power/inverse power methods and the QR eigenvalue algorithm.\nSpecifically, we analyze the required number of iterations to compute extreme\neigenvalues of random, positive-definite sample covariance matrices to within a\nprescribed tolerance. The universality theorem provides a complexity estimate\nfor the algorithms which, in this random setting, holds with high probability.\nThe method of proof relies on recent results on the statistics of the\neigenvalues and eigenvectors of random sample covariance matrices (i.e.,\ndelocalization, rigidity and edge universality). \n\n"}
{"id": "1701.02185", "contents": "Title: Crowdsourcing Ground Truth for Medical Relation Extraction Abstract: Cognitive computing systems require human labeled data for evaluation, and\noften for training. The standard practice used in gathering this data minimizes\ndisagreement between annotators, and we have found this results in data that\nfails to account for the ambiguity inherent in language. We have proposed the\nCrowdTruth method for collecting ground truth through crowdsourcing, that\nreconsiders the role of people in machine learning based on the observation\nthat disagreement between annotators provides a useful signal for phenomena\nsuch as ambiguity in the text. We report on using this method to build an\nannotated data set for medical relation extraction for the $cause$ and $treat$\nrelations, and how this data performed in a supervised training experiment. We\ndemonstrate that by modeling ambiguity, labeled data gathered from crowd\nworkers can (1) reach the level of quality of domain experts for this task\nwhile reducing the cost, and (2) provide better training data at scale than\ndistant supervision. We further propose and validate new weighted measures for\nprecision, recall, and F-measure, that account for ambiguity in both human and\nmachine performance on this task. \n\n"}
{"id": "1701.03866", "contents": "Title: Long Timescale Credit Assignment in NeuralNetworks with External Memory Abstract: Credit assignment in traditional recurrent neural networks usually involves\nback-propagating through a long chain of tied weight matrices. The length of\nthis chain scales linearly with the number of time-steps as the same network is\nrun at each time-step. This creates many problems, such as vanishing gradients,\nthat have been well studied. In contrast, a NNEM's architecture recurrent\nactivity doesn't involve a long chain of activity (though some architectures\nsuch as the NTM do utilize a traditional recurrent architecture as a\ncontroller). Rather, the externally stored embedding vectors are used at each\ntime-step, but no messages are passed from previous time-steps. This means that\nvanishing gradients aren't a problem, as all of the necessary gradient paths\nare short. However, these paths are extremely numerous (one per embedding\nvector in memory) and reused for a very long time (until it leaves the memory).\nThus, the forward-pass information of each memory must be stored for the entire\nduration of the memory. This is problematic as this additional storage far\nsurpasses that of the actual memories, to the extent that large memories on\ninfeasible to back-propagate through in high dimensional settings. One way to\nget around the need to hold onto forward-pass information is to recalculate the\nforward-pass whenever gradient information is available. However, if the\nobservations are too large to store in the domain of interest, direct\nreinstatement of a forward pass cannot occur. Instead, we rely on a learned\nautoencoder to reinstate the observation, and then use the embedding network to\nrecalculate the forward-pass. Since the recalculated embedding vector is\nunlikely to perfectly match the one stored in memory, we try out 2\napproximations to utilize error gradient w.r.t. the vector in memory. \n\n"}
{"id": "1701.03924", "contents": "Title: QCRI Machine Translation Systems for IWSLT 16 Abstract: This paper describes QCRI's machine translation systems for the IWSLT 2016\nevaluation campaign. We participated in the Arabic->English and English->Arabic\ntracks. We built both Phrase-based and Neural machine translation models, in an\neffort to probe whether the newly emerged NMT framework surpasses the\ntraditional phrase-based systems in Arabic-English language pairs. We trained a\nvery strong phrase-based system including, a big language model, the Operation\nSequence Model, Neural Network Joint Model and Class-based models along with\ndifferent domain adaptation techniques such as MML filtering, mixture modeling\nand using fine tuning over NNJM model. However, a Neural MT system, trained by\nstacking data from different genres through fine-tuning, and applying ensemble\nover 8 models, beat our very strong phrase-based system by a significant 2 BLEU\npoints margin in Arabic->English direction. We did not obtain similar gains in\nthe other direction but were still able to outperform the phrase-based system.\nWe also applied system combination on phrase-based and NMT outputs. \n\n"}
{"id": "1701.08528", "contents": "Title: Self-Adaptation of Activity Recognition Systems to New Sensors Abstract: Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach. \n\n"}
{"id": "1702.01466", "contents": "Title: Prepositions in Context Abstract: Prepositions are highly polysemous, and their variegated senses encode\nsignificant semantic information. In this paper we match each preposition's\ncomplement and attachment and their interplay crucially to the geometry of the\nword vectors to the left and right of the preposition. Extracting such features\nfrom the vast number of instances of each preposition and clustering them makes\nfor an efficient preposition sense disambigution (PSD) algorithm, which is\ncomparable to and better than state-of-the-art on two benchmark datasets. Our\nreliance on no external linguistic resource allows us to scale the PSD\nalgorithm to a large WikiCorpus and learn sense-specific preposition\nrepresentations -- which we show to encode semantic relations and paraphrasing\nof verb particle compounds, via simple vector operations. \n\n"}
{"id": "1702.02170", "contents": "Title: How to evaluate word embeddings? On importance of data efficiency and\n  simple supervised tasks Abstract: Maybe the single most important goal of representation learning is making\nsubsequent learning faster. Surprisingly, this fact is not well reflected in\nthe way embeddings are evaluated. In addition, recent practice in word\nembeddings points towards importance of learning specialized representations.\nWe argue that focus of word representation evaluation should reflect those\ntrends and shift towards evaluating what useful information is easily\naccessible. Specifically, we propose that evaluation should focus on data\nefficiency and simple supervised tasks, where the amount of available data is\nvaried and scores of a supervised model are reported for each subset (as\ncommonly done in transfer learning).\n  In order to illustrate significance of such analysis, a comprehensive\nevaluation of selected word embeddings is presented. Proposed approach yields a\nmore complete picture and brings new insight into performance characteristics,\nfor instance information about word similarity or analogy tends to be\nnon--linearly encoded in the embedding space, which questions the cosine-based,\nunsupervised, evaluation methods. All results and analysis scripts are\navailable online. \n\n"}
{"id": "1702.02212", "contents": "Title: MORSE: Semantic-ally Drive-n MORpheme SEgment-er Abstract: We present in this paper a novel framework for morpheme segmentation which\nuses the morpho-syntactic regularities preserved by word representations, in\naddition to orthographic features, to segment words into morphemes. This\nframework is the first to consider vocabulary-wide syntactico-semantic\ninformation for this task. We also analyze the deficiencies of available\nbenchmarking datasets and introduce our own dataset that was created on the\nbasis of compositionality. We validate our algorithm across datasets and\npresent state-of-the-art results. \n\n"}
{"id": "1702.05270", "contents": "Title: Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers\n  from Vision Abstract: People can refer to quantities in a visual scene by using either exact\ncardinals (e.g. one, two, three) or natural language quantifiers (e.g. few,\nmost, all). In humans, these two processes underlie fairly different cognitive\nand neural mechanisms. Inspired by this evidence, the present study proposes\ntwo models for learning the objective meaning of cardinals and quantifiers from\nvisual scenes containing multiple objects. We show that a model capitalizing on\na 'fuzzy' measure of similarity is effective for learning quantifiers, whereas\nthe learning of exact cardinals is better accomplished when information about\nnumber is provided. \n\n"}
{"id": "1702.05639", "contents": "Title: Deep Stochastic Configuration Networks with Universal Approximation\n  Property Abstract: This paper develops a randomized approach for incrementally building deep\nneural networks, where a supervisory mechanism is proposed to constrain the\nrandom assignment of the weights and biases, and all the hidden layers have\ndirect links to the output layer. A fundamental result on the universal\napproximation property is established for such a class of randomized leaner\nmodels, namely deep stochastic configuration networks (DeepSCNs). A learning\nalgorithm is presented to implement DeepSCNs with either specific architecture\nor self-organization. The read-out weights attached with all direct links from\neach hidden layer to the output layer are evaluated by the least squares\nmethod. Given a set of training examples, DeepSCNs can speedily produce a\nlearning representation, that is, a collection of random basis functions with\nthe cascaded inputs together with the read-out weights. An empirical study on a\nfunction approximation is carried out to demonstrate some properties of the\nproposed deep learner model. \n\n"}
{"id": "1702.05815", "contents": "Title: Compressive Embedding and Visualization using Graphs Abstract: Visualizing high-dimensional data has been a focus in data analysis\ncommunities for decades, which has led to the design of many algorithms, some\nof which are now considered references (such as t-SNE for example). In our era\nof overwhelming data volumes, the scalability of such methods have become more\nand more important. In this work, we present a method which allows to apply any\nvisualization or embedding algorithm on very large datasets by considering only\na fraction of the data as input and then extending the information to all data\npoints using a graph encoding its global similarity. We show that in most\ncases, using only $\\mathcal{O}(\\log(N))$ samples is sufficient to diffuse the\ninformation to all $N$ data points. In addition, we propose quantitative\nmethods to measure the quality of embeddings and demonstrate the validity of\nour technique on both synthetic and real-world datasets. \n\n"}
{"id": "1702.05882", "contents": "Title: Phase Diagram of Restricted Boltzmann Machines and Generalised Hopfield\n  Networks with Arbitrary Priors Abstract: Restricted Boltzmann Machines are described by the Gibbs measure of a\nbipartite spin glass, which in turn corresponds to the one of a generalised\nHopfield network. This equivalence allows us to characterise the state of these\nsystems in terms of retrieval capabilities, both at low and high load. We study\nthe paramagnetic-spin glass and the spin glass-retrieval phase transitions, as\nthe pattern (i.e. weight) distribution and spin (i.e. unit) priors vary\nsmoothly from Gaussian real variables to Boolean discrete variables. Our\nanalysis shows that the presence of a retrieval phase is robust and not\npeculiar to the standard Hopfield model with Boolean patterns. The retrieval\nregion is larger when the pattern entries and retrieval units get more peaked\nand, conversely, when the hidden units acquire a broader prior and therefore\nhave a stronger response to high fields. Moreover, at low load retrieval always\nexists below some critical temperature, for every pattern distribution ranging\nfrom the Boolean to the Gaussian case. \n\n"}
{"id": "1702.06832", "contents": "Title: Adversarial examples for generative models Abstract: We explore methods of producing adversarial examples on deep generative\nmodels such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning\narchitectures are known to be vulnerable to adversarial examples, but previous\nwork has focused on the application of adversarial examples to classification\ntasks. Deep generative models have recently become popular due to their ability\nto model input data distributions and generate realistic examples from those\ndistributions. We present three classes of attacks on the VAE and VAE-GAN\narchitectures and demonstrate them against networks trained on MNIST, SVHN and\nCelebA. Our first attack leverages classification-based adversaries by\nattaching a classifier to the trained encoder of the target generative model,\nwhich can then be used to indirectly manipulate the latent representation. Our\nsecond attack directly uses the VAE loss function to generate a target\nreconstruction image from the adversarial example. Our third attack moves\nbeyond relying on classification or the standard loss for the gradient and\ndirectly optimizes against differences in source and target latent\nrepresentations. We also motivate why an attacker might be interested in\ndeploying such techniques against a target generative network. \n\n"}
{"id": "1702.06943", "contents": "Title: Tuple-oriented Compression for Large-scale Mini-batch Stochastic\n  Gradient Descent Abstract: Data compression is a popular technique for improving the efficiency of data\nprocessing workloads such as SQL queries and more recently, machine learning\n(ML) with classical batch gradient methods. But the efficacy of such ideas for\nmini-batch stochastic gradient descent (MGD), arguably the workhorse algorithm\nof modern ML, is an open question. MGD's unique data access pattern renders\nprior art, including those designed for batch gradient methods, less effective.\nWe fill this crucial research gap by proposing a new lossless compression\nscheme we call tuple-oriented compression (TOC) that is inspired by an unlikely\nsource, the string/text compression scheme Lempel-Ziv-Welch, but tailored to\nMGD in a way that preserves tuple boundaries within mini-batches. We then\npresent a suite of novel compressed matrix operation execution techniques\ntailored to the TOC compression scheme that operate directly over the\ncompressed data representation and avoid decompression overheads. An extensive\nempirical evaluation with real-world datasets shows that TOC consistently\nachieves substantial compression ratios by up to 51x and reduces runtimes for\nMGD workloads by up to 10.2x in popular ML systems. \n\n"}
{"id": "1702.07028", "contents": "Title: On the ability of neural nets to express distributions Abstract: Deep neural nets have caused a revolution in many classification tasks. A\nrelated ongoing revolution -- also theoretically not understood -- concerns\ntheir ability to serve as generative models for complicated types of data such\nas images and texts. These models are trained using ideas like variational\nautoencoders and Generative Adversarial Networks.\n  We take a first cut at explaining the expressivity of multilayer nets by\ngiving a sufficient criterion for a function to be approximable by a neural\nnetwork with $n$ hidden layers. A key ingredient is Barron's Theorem\n\\cite{Barron1993}, which gives a Fourier criterion for approximability of a\nfunction by a neural network with 1 hidden layer. We show that a composition of\n$n$ functions which satisfy certain Fourier conditions (\"Barron functions\") can\nbe approximated by a $n+1$-layer neural network.\n  For probability distributions, this translates into a criterion for a\nprobability distribution to be approximable in Wasserstein distance -- a\nnatural metric on probability distributions -- by a neural network applied to a\nfixed base distribution (e.g., multivariate gaussian).\n  Building up recent lower bound work, we also give an example function that\nshows that composition of Barron functions is more expressive than Barron\nfunctions alone. \n\n"}
{"id": "1702.07463", "contents": "Title: Sequence Modeling via Segmentations Abstract: Segmental structure is a common pattern in many types of sequences such as\nphrases in human languages. In this paper, we present a probabilistic model for\nsequences via their segmentations. The probability of a segmented sequence is\ncalculated as the product of the probabilities of all its segments, where each\nsegment is modeled using existing tools such as recurrent neural networks.\nSince the segmentation of a sequence is usually unknown in advance, we sum over\nall valid segmentations to obtain the final probability for the sequence. An\nefficient dynamic programming algorithm is developed for forward and backward\ncomputations without resorting to any approximation. We demonstrate our\napproach on text segmentation and speech recognition tasks. In addition to\nquantitative results, we also show that our approach can discover meaningful\nsegments in their respective application contexts. \n\n"}
{"id": "1702.07908", "contents": "Title: CHAOS: A Parallelization Scheme for Training Convolutional Neural\n  Networks on Intel Xeon Phi Abstract: Deep learning is an important component of big-data analytic tools and\nintelligent applications, such as, self-driving cars, computer vision, speech\nrecognition, or precision medicine. However, the training process is\ncomputationally intensive, and often requires a large amount of time if\nperformed sequentially. Modern parallel computing systems provide the\ncapability to reduce the required training time of deep neural networks. In\nthis paper, we present our parallelization scheme for training convolutional\nneural networks (CNN) named Controlled Hogwild with Arbitrary Order of\nSynchronization (CHAOS). Major features of CHAOS include the support for thread\nand vector parallelism, non-instant updates of weight parameters during\nback-propagation without a significant delay, and implicit synchronization in\narbitrary order. CHAOS is tailored for parallel computing systems that are\naccelerated with the Intel Xeon Phi. We evaluate our parallelization approach\nempirically using measurement techniques and performance modeling for various\nnumbers of threads and CNN architectures. Experimental results for the MNIST\ndataset of handwritten digits using the total number of threads on the Xeon Phi\nshow speedups of up to 103x compared to the execution on one thread of the Xeon\nPhi, 14x compared to the sequential execution on Intel Xeon E5, and 58x\ncompared to the sequential execution on Intel Core i5. \n\n"}
{"id": "1702.08489", "contents": "Title: Depth Separation for Neural Networks Abstract: Let $f:\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}\\to\\mathbb{S}$ be a function of\nthe form $f(\\mathbf{x},\\mathbf{x}') = g(\\langle\\mathbf{x},\\mathbf{x}'\\rangle)$\nfor $g:[-1,1]\\to \\mathbb{R}$. We give a simple proof that shows that poly-size\ndepth two neural networks with (exponentially) bounded weights cannot\napproximate $f$ whenever $g$ cannot be approximated by a low degree polynomial.\nMoreover, for many $g$'s, such as $g(x)=\\sin(\\pi d^3x)$, the number of neurons\nmust be $2^{\\Omega\\left(d\\log(d)\\right)}$. Furthermore, the result holds\nw.r.t.\\ the uniform distribution on $\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$.\nAs many functions of the above form can be well approximated by poly-size depth\nthree networks with poly-bounded weights, this establishes a separation between\ndepth two and depth three networks w.r.t.\\ the uniform distribution on\n$\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$. \n\n"}
{"id": "1702.08513", "contents": "Title: Learning Deep Visual Object Models From Noisy Web Data: How to Make it\n  Work Abstract: Deep networks thrive when trained on large scale data collections. This has\ngiven ImageNet a central role in the development of deep architectures for\nvisual object classification. However, ImageNet was created during a specific\nperiod in time, and as such it is prone to aging, as well as dataset bias\nissues. Moving beyond fixed training datasets will lead to more robust visual\nsystems, especially when deployed on robots in new environments which must\ntrain on the objects they encounter there. To make this possible, it is\nimportant to break free from the need for manual annotators. Recent work has\nbegun to investigate how to use the massive amount of images available on the\nWeb in place of manual image annotations. We contribute to this research thread\nwith two findings: (1) a study correlating a given level of noisily labels to\nthe expected drop in accuracy, for two deep architectures, on two different\ntypes of noise, that clearly identifies GoogLeNet as a suitable architecture\nfor learning from Web data; (2) a recipe for the creation of Web datasets with\nminimal noise and maximum visual variability, based on a visual and natural\nlanguage processing concept expansion strategy. By combining these two results,\nwe obtain a method for learning powerful deep object models automatically from\nthe Web. We confirm the effectiveness of our approach through object\ncategorization experiments using our Web-derived version of ImageNet on a\npopular robot vision benchmark database, and on a lifelong object discovery\ntask on a mobile robot. \n\n"}
{"id": "1702.08536", "contents": "Title: Fast Threshold Tests for Detecting Discrimination Abstract: Threshold tests have recently been proposed as a useful method for detecting\nbias in lending, hiring, and policing decisions. For example, in the case of\ncredit extensions, these tests aim to estimate the bar for granting loans to\nwhite and minority applicants, with a higher inferred threshold for minorities\nindicative of discrimination. This technique, however, requires fitting a\ncomplex Bayesian latent variable model for which inference is often\ncomputationally challenging. Here we develop a method for fitting threshold\ntests that is two orders of magnitude faster than the existing approach,\nreducing computation from hours to minutes. To achieve these performance gains,\nwe introduce and analyze a flexible family of probability distributions on the\ninterval [0, 1] -- which we call discriminant distributions -- that is\ncomputationally efficient to work with. We demonstrate our technique by\nanalyzing 2.7 million police stops of pedestrians in New York City. \n\n"}
{"id": "1702.08653", "contents": "Title: Scaffolding Networks: Incremental Learning and Teaching Through\n  Questioning Abstract: We introduce a new paradigm of learning for reasoning, understanding, and\nprediction, as well as the scaffolding network to implement this paradigm. The\nscaffolding network embodies an incremental learning approach that is\nformulated as a teacher-student network architecture to teach machines how to\nunderstand text and do reasoning. The key to our computational scaffolding\napproach is the interactions between the teacher and the student through\nsequential questioning. The student observes each sentence in the text\nincrementally, and it uses an attention-based neural net to discover and\nregister the key information in relation to its current memory. Meanwhile, the\nteacher asks questions about the observed text, and the student network gets\nrewarded by correctly answering these questions. The entire network is updated\ncontinually using reinforcement learning. Our experimental results on synthetic\nand real datasets show that the scaffolding network not only outperforms\nstate-of-the-art methods but also learns to do reasoning in a scalable way even\nwith little human generated input. \n\n"}
{"id": "1703.00144", "contents": "Title: Theoretical Properties for Neural Networks with Weight Matrices of Low\n  Displacement Rank Abstract: Recently low displacement rank (LDR) matrices, or so-called structured\nmatrices, have been proposed to compress large-scale neural networks. Empirical\nresults have shown that neural networks with weight matrices of LDR matrices,\nreferred as LDR neural networks, can achieve significant reduction in space and\ncomputational complexity while retaining high accuracy. We formally study LDR\nmatrices in deep learning. First, we prove the universal approximation property\nof LDR neural networks with a mild condition on the displacement operators. We\nthen show that the error bounds of LDR neural networks are as efficient as\ngeneral neural networks with both single-layer and multiple-layer structure.\nFinally, we propose back-propagation based training algorithm for general LDR\nneural networks. \n\n"}
{"id": "1703.00439", "contents": "Title: Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for\n  Regularized Empirical Risk Minimization Abstract: In this paper, we develop a new accelerated stochastic gradient method for\nefficiently solving the convex regularized empirical risk minimization problem\nin mini-batch settings. The use of mini-batches is becoming a golden standard\nin the machine learning community, because mini-batch settings stabilize the\ngradient estimate and can easily make good use of parallel computing. The core\nof our proposed method is the incorporation of our new \"double acceleration\"\ntechnique and variance reduction technique. We theoretically analyze our\nproposed method and show that our method much improves the mini-batch\nefficiencies of previous accelerated stochastic methods, and essentially only\nneeds size $\\sqrt{n}$ mini-batches for achieving the optimal iteration\ncomplexities for both non-strongly and strongly convex objectives, where $n$ is\nthe training set size. Further, we show that even in non-mini-batch settings,\nour method achieves the best known convergence rate for both non-strongly and\nstrongly convex objectives. \n\n"}
{"id": "1703.00593", "contents": "Title: Positive-Unlabeled Learning with Non-Negative Risk Estimator Abstract: From only positive (P) and unlabeled (U) data, a binary classifier could be\ntrained with PU learning, in which the state of the art is unbiased PU\nlearning. However, if its model is very flexible, empirical risks on training\ndata will go negative, and we will suffer from serious overfitting. In this\npaper, we propose a non-negative risk estimator for PU learning: when getting\nminimized, it is more robust against overfitting, and thus we are able to use\nvery flexible models (such as deep neural networks) given limited P data.\nMoreover, we analyze the bias, consistency, and mean-squared-error reduction of\nthe proposed risk estimator, and bound the estimation error of the resulting\nempirical risk minimizer. Experiments demonstrate that our risk estimator fixes\nthe overfitting problem of its unbiased counterparts. \n\n"}
{"id": "1703.01461", "contents": "Title: Addressing Appearance Change in Outdoor Robotics with Adversarial Domain\n  Adaptation Abstract: Appearance changes due to weather and seasonal conditions represent a strong\nimpediment to the robust implementation of machine learning systems in outdoor\nrobotics. While supervised learning optimises a model for the training domain,\nit will deliver degraded performance in application domains that underlie\ndistributional shifts caused by these changes. Traditionally, this problem has\nbeen addressed via the collection of labelled data in multiple domains or by\nimposing priors on the type of shift between both domains. We frame the problem\nin the context of unsupervised domain adaptation and develop a framework for\napplying adversarial techniques to adapt popular, state-of-the-art network\narchitectures with the additional objective to align features across domains.\nMoreover, as adversarial training is notoriously unstable, we first perform an\nextensive ablation study, adapting many techniques known to stabilise\ngenerative adversarial networks, and evaluate on a surrogate classification\ntask with the same appearance change. The distilled insights are applied to the\nproblem of free-space segmentation for motion planning in autonomous driving. \n\n"}
{"id": "1703.01913", "contents": "Title: Near-Optimal Closeness Testing of Discrete Histogram Distributions Abstract: We investigate the problem of testing the equivalence between two discrete\nhistograms. A {\\em $k$-histogram} over $[n]$ is a probability distribution that\nis piecewise constant over some set of $k$ intervals over $[n]$. Histograms\nhave been extensively studied in computer science and statistics. Given a set\nof samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to\ndistinguish (with high probability) between the cases that $p = q$ and\n$\\|p-q\\|_1 \\geq \\epsilon$. The main contribution of this paper is a new\nalgorithm for this testing problem and a nearly matching information-theoretic\nlower bound. Specifically, the sample complexity of our algorithm matches our\nlower bound up to a logarithmic factor, improving on previous work by\npolynomial factors in the relevant parameters. Our algorithmic approach applies\nin a more general setting and yields improved sample upper bounds for testing\ncloseness of other structured distributions as well. \n\n"}
{"id": "1703.03478", "contents": "Title: Online Learning with Abstention Abstract: We present an extensive study of the key problem of online learning where\nalgorithms are allowed to abstain from making predictions. In the adversarial\nsetting, we show how existing online algorithms and guarantees can be adapted\nto this problem. In the stochastic setting, we first point out a bias problem\nthat limits the straightforward extension of algorithms such as UCB-N to\ntime-varying feedback graphs, as needed in this context. Next, we give a new\nalgorithm, UCB-GT, that exploits historical data and is adapted to time-varying\nfeedback graphs. We show that this algorithm benefits from more favorable\nregret guarantees than a possible, but limited, extension of UCB-N. We further\nreport the results of a series of experiments demonstrating that UCB-GT largely\noutperforms that extension of UCB-N, as well as more standard baselines. \n\n"}
{"id": "1703.04816", "contents": "Title: Making Neural QA as Simple as Possible but not Simpler Abstract: Recent development of large-scale question answering (QA) datasets triggered\na substantial amount of research into end-to-end neural architectures for QA.\nIncreasingly complex systems have been conceived without comparison to simpler\nneural baseline systems that would justify their complexity. In this work, we\npropose a simple heuristic that guides the development of neural baseline\nsystems for the extractive QA task. We find that there are two ingredients\nnecessary for building a high-performing neural QA system: first, the awareness\nof question words while processing the context and second, a composition\nfunction that goes beyond simple bag-of-words modeling, such as recurrent\nneural networks. Our results show that FastQA, a system that meets these two\nrequirements, can achieve very competitive performance compared with existing\nmodels. We argue that this surprising finding puts results of previous systems\nand the complexity of recent QA datasets into perspective. \n\n"}
{"id": "1703.05698", "contents": "Title: Neural Sketch Learning for Conditional Program Generation Abstract: We study the problem of generating source code in a strongly typed, Java-like\nprogramming language, given a label (for example a set of API calls or types)\ncarrying a small amount of information about the code that is desired. The\ngenerated programs are expected to respect a \"realistic\" relationship between\nprograms and labels, as exemplified by a corpus of labeled programs available\nduring training.\n  Two challenges in such conditional program generation are that the generated\nprograms must satisfy a rich set of syntactic and semantic constraints, and\nthat source code contains many low-level features that impede learning. We\naddress these problems by training a neural generator not on code but on\nprogram sketches, or models of program syntax that abstract out names and\noperations that do not generalize across programs. During generation, we infer\na posterior distribution over sketches, then concretize samples from this\ndistribution into type-safe programs using combinatorial techniques. We\nimplement our ideas in a system for generating API-heavy Java code, and show\nthat it can often predict the entire body of a method given just a few API\ncalls or data types that appear in the method. \n\n"}
{"id": "1703.06229", "contents": "Title: Curriculum Dropout Abstract: Dropout is a very effective way of regularizing neural networks.\nStochastically \"dropping out\" units with a certain probability discourages\nover-specific co-adaptations of feature detectors, preventing overfitting and\nimproving network generalization. Besides, Dropout can be interpreted as an\napproximate model aggregation technique, where an exponential number of smaller\nnetworks are averaged in order to get a more powerful ensemble. In this paper,\nwe show that using a fixed dropout probability during training is a suboptimal\nchoice. We thus propose a time scheduling for the probability of retaining\nneurons in the network. This induces an adaptive regularization scheme that\nsmoothly increases the difficulty of the optimization problem. This idea of\n\"starting easy\" and adaptively increasing the difficulty of the learning\nproblem has its roots in curriculum learning and allows one to train better\nmodels. Indeed, we prove that our optimization strategy implements a very\ngeneral curriculum scheme, by gradually adding noise to both the input and\nintermediate feature representations within the network architecture.\nExperiments on seven image classification datasets and different network\narchitectures show that our method, named Curriculum Dropout, frequently yields\nto better generalization and, at worst, performs just as well as the standard\nDropout method. \n\n"}
{"id": "1703.06513", "contents": "Title: Bernoulli Rank-$1$ Bandits for Click Feedback Abstract: The probability that a user will click a search result depends both on its\nrelevance and its position on the results page. The position based model\nexplains this behavior by ascribing to every item an attraction probability,\nand to every position an examination probability. To be clicked, a result must\nbe both attractive and examined. The probabilities of an item-position pair\nbeing clicked thus form the entries of a rank-$1$ matrix. We propose the\nlearning problem of a Bernoulli rank-$1$ bandit where at each step, the\nlearning agent chooses a pair of row and column arms, and receives the product\nof their Bernoulli-distributed values as a reward. This is a special case of\nthe stochastic rank-$1$ bandit problem considered in recent work that proposed\nan elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret\nscales linearly with the number of rows and columns on \"benign\" instances.\nThese are the instances where the minimum of the average row and column rewards\n$\\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to\nbe competitive with straightforward bandit strategies as $\\mu \\rightarrow 0$.\nIn this paper we propose Rank1ElimKL which simply replaces the (crude)\nconfidence intervals of Rank1Elim with confidence intervals based on\nKullback-Leibler (KL) divergences, and with the help of a novel result\nconcerning the scaling of KL divergences we prove that with this change, our\nalgorithm will be competitive no matter the value of $\\mu$. Experiments with\nsynthetic data confirm that on benign instances the performance of Rank1ElimKL\nis significantly better than that of even Rank1Elim, while experiments with\nmodels derived from real data confirm that the improvements are significant\nacross the board, regardless of whether the data is benign or not. \n\n"}
{"id": "1703.06925", "contents": "Title: Black-Box Optimization in Machine Learning with Trust Region Based\n  Derivative Free Algorithm Abstract: In this work, we utilize a Trust Region based Derivative Free Optimization\n(DFO-TR) method to directly maximize the Area Under Receiver Operating\nCharacteristic Curve (AUC), which is a nonsmooth, noisy function. We show that\nAUC is a smooth function, in expectation, if the distributions of the positive\nand negative data points obey a jointly normal distribution. The practical\nperformance of this algorithm is compared to three prominent Bayesian\noptimization methods and random search. The presented numerical results show\nthat DFO-TR surpasses Bayesian optimization and random search on various\nblack-box optimization problem, such as maximizing AUC and hyperparameter\ntuning. \n\n"}
{"id": "1703.07476", "contents": "Title: Topic Identification for Speech without ASR Abstract: Modern topic identification (topic ID) systems for speech use automatic\nspeech recognition (ASR) to produce speech transcripts, and perform supervised\nclassification on such ASR outputs. However, under resource-limited conditions,\nthe manually transcribed speech required to develop standard ASR systems can be\nseverely limited or unavailable. In this paper, we investigate alternative\nunsupervised solutions to obtaining tokenizations of speech in terms of a\nvocabulary of automatically discovered word-like or phoneme-like units, without\ndepending on the supervised training of ASR systems. Moreover, using automatic\nphoneme-like tokenizations, we demonstrate that a convolutional neural network\nbased framework for learning spoken document representations provides\ncompetitive performance compared to a standard bag-of-words representation, as\nevidenced by comprehensive topic ID evaluations on both single-label and\nmulti-label classification tasks. \n\n"}
{"id": "1703.07915", "contents": "Title: Perspective: Energy Landscapes for Machine Learning Abstract: Machine learning techniques are being increasingly used as flexible\nnon-linear fitting and prediction tools in the physical sciences. Fitting\nfunctions that exhibit multiple solutions as local minima can be analysed in\nterms of the corresponding machine learning landscape. Methods to explore and\nvisualise molecular potential energy landscapes can be applied to these machine\nlearning landscapes to gain new insight into the solution space involved in\ntraining and the nature of the corresponding predictions. In particular, we can\ndefine quantities analogous to molecular structure, thermodynamics, and\nkinetics, and relate these emergent properties to the structure of the\nunderlying landscape. This Perspective aims to describe these analogies with\nexamples from recent applications, and suggest avenues for new\ninterdisciplinary research. \n\n"}
{"id": "1703.08403", "contents": "Title: Asymmetric Learning Vector Quantization for Efficient Nearest Neighbor\n  Classification in Dynamic Time Warping Spaces Abstract: The nearest neighbor method together with the dynamic time warping (DTW)\ndistance is one of the most popular approaches in time series classification.\nThis method suffers from high storage and computation requirements for large\ntraining sets. As a solution to both drawbacks, this article extends learning\nvector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed LVQ\nscheme uses asymmetric weighted averaging as update rule. Empirical results\nexhibited superior performance of asymmetric generalized LVQ (GLVQ) over other\nstate-of-the-art prototype generation methods for nearest neighbor\nclassification. \n\n"}
{"id": "1704.00389", "contents": "Title: Hidden Two-Stream Convolutional Networks for Action Recognition Abstract: Analyzing videos of human actions involves understanding the temporal\nrelationships among video frames. State-of-the-art action recognition\napproaches rely on traditional optical flow estimation methods to pre-compute\nmotion information for CNNs. Such a two-stage approach is computationally\nexpensive, storage demanding, and not end-to-end trainable. In this paper, we\npresent a novel CNN architecture that implicitly captures motion information\nbetween adjacent frames. We name our approach hidden two-stream CNNs because it\nonly takes raw video frames as input and directly predicts action classes\nwithout explicitly computing optical flow. Our end-to-end approach is 10x\nfaster than its two-stage baseline. Experimental results on four challenging\naction recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show\nthat our approach significantly outperforms the previous best real-time\napproaches. \n\n"}
{"id": "1704.00774", "contents": "Title: Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency\n  and Compositionality Abstract: Increasing the capacity of recurrent neural networks (RNN) usually involves\naugmenting the size of the hidden layer, with significant increase of\ncomputational cost. Recurrent neural tensor networks (RNTN) increase capacity\nusing distinct hidden layer weights for each word, but with greater costs in\nmemory usage. In this paper, we introduce restricted recurrent neural tensor\nnetworks (r-RNTN) which reserve distinct hidden layer weights for frequent\nvocabulary words while sharing a single set of weights for infrequent words.\nPerplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve\nlanguage model performance over RNNs using only a small fraction of the\nparameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated\nRecurrent Units and Long Short-Term Memory. \n\n"}
{"id": "1704.01041", "contents": "Title: Polynomial Time and Sample Complexity for Non-Gaussian Component\n  Analysis: Spectral Methods Abstract: The problem of Non-Gaussian Component Analysis (NGCA) is about finding a\nmaximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points\nprojected onto $E$ follow a non-gaussian distribution. Although this is an\nappropriate model for some real world data analysis problems, there has been\nlittle progress on this problem over the last decade.\n  In this paper, we attempt to address this state of affairs in two ways.\nFirst, we give a new characterization of standard gaussian distributions in\nhigh-dimensions, which lead to effective tests for non-gaussianness. Second, we\npropose a simple algorithm, \\emph{Reweighted PCA}, as a method for solving the\nNGCA problem. We prove that for a general unknown non-gaussian distribution,\nthis algorithm recovers at least one direction in $E$, with sample and time\ncomplexity depending polynomially on the dimension of the ambient space. We\nconjecture that the algorithm actually recovers the entire $E$. \n\n"}
{"id": "1704.01255", "contents": "Title: Linear Additive Markov Processes Abstract: We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP\nmay be influenced by states visited in the distant history of the process, but\nunlike higher-order Markov processes, LAMP retains an efficient\nparametrization. LAMP also allows the specific dependence on history to be\nlearned efficiently from data. We characterize some theoretical properties of\nLAMP, including its steady-state and mixing time. We then give an algorithm\nbased on alternating minimization to learn LAMP models from data. Finally, we\nperform a series of real-world experiments to show that LAMP is more powerful\nthan first-order Markov processes, and even holds its own against deep\nsequential models (LSTMs) with a negligible increase in parameter complexity. \n\n"}
{"id": "1704.01419", "contents": "Title: Linear Ensembles of Word Embedding Models Abstract: This paper explores linear methods for combining several word embedding\nmodels into an ensemble. We construct the combined models using an iterative\nmethod based on either ordinary least squares regression or the solution to the\northogonal Procrustes problem.\n  We evaluate the proposed approaches on Estonian---a morphologically complex\nlanguage, for which the available corpora for training word embeddings are\nrelatively small. We compare both combined models with each other and with the\ninput word embedding models using synonym and analogy tests. The results show\nthat while using the ordinary least squares regression performs poorly in our\nexperiments, using orthogonal Procrustes to combine several word embedding\nmodels into an ensemble model leads to 7-10% relative improvements over the\nmean result of the initial models in synonym tests and 19-47% in analogy tests. \n\n"}
{"id": "1704.01427", "contents": "Title: AMIDST: a Java Toolbox for Scalable Probabilistic Machine Learning Abstract: The AMIDST Toolbox is a software for scalable probabilistic machine learning\nwith a spe- cial focus on (massive) streaming data. The toolbox supports a\nflexible modeling language based on probabilistic graphical models with latent\nvariables and temporal dependencies. The specified models can be learnt from\nlarge data sets using parallel or distributed implementa- tions of Bayesian\nlearning algorithms for either streaming or batch data. These algorithms are\nbased on a flexible variational message passing scheme, which supports discrete\nand continu- ous variables from a wide range of probability distributions.\nAMIDST also leverages existing functionality and algorithms by interfacing to\nsoftware tools such as Flink, Spark, MOA, Weka, R and HUGIN. AMIDST is an open\nsource toolbox written in Java and available at http://www.amidsttoolbox.com\nunder the Apache Software License version 2.0. \n\n"}
{"id": "1704.02227", "contents": "Title: Training Triplet Networks with GAN Abstract: Triplet networks are widely used models that are characterized by good\nperformance in classification and retrieval tasks. In this work we propose to\ntrain a triplet network by putting it as the discriminator in Generative\nAdversarial Nets (GANs). We make use of the good capability of representation\nlearning of the discriminator to increase the predictive quality of the model.\nWe evaluated our approach on Cifar10 and MNIST datasets and observed\nsignificant improvement on the classification performance using the simple k-nn\nmethod. \n\n"}
{"id": "1704.02598", "contents": "Title: A Sample Complexity Measure with Applications to Learning Optimal\n  Auctions Abstract: We introduce a new sample complexity measure, which we refer to as\nsplit-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size\n$m$, the split-sample growth rate $\\hat{\\tau}_H(m)$ counts how many different\nhypotheses can empirical risk minimization output on any sub-sample of $S$ of\nsize $m/2$. We show that the expected generalization error is upper bounded by\n$O\\left(\\sqrt{\\frac{\\log(\\hat{\\tau}_H(2m))}{m}}\\right)$. Our result is enabled\nby a strengthening of the Rademacher complexity analysis of the expected\ngeneralization error. We show that this sample complexity measure, greatly\nsimplifies the analysis of the sample complexity of optimal auction design, for\nmany auction classes studied in the literature. Their sample complexity can be\nderived solely by noticing that in these auction classes, ERM on any sample or\nsub-sample will pick parameters that are equal to one of the points in the\nsample. \n\n"}
{"id": "1704.02813", "contents": "Title: Character-Word LSTM Language Models Abstract: We present a Character-Word Long Short-Term Memory Language Model which both\nreduces the perplexity with respect to a baseline word-level language model and\nreduces the number of parameters of the model. Character information can reveal\nstructural (dis)similarities between words and can even be used when a word is\nout-of-vocabulary, thus improving the modeling of infrequent and unknown words.\nBy concatenating word and character embeddings, we achieve up to 2.77% relative\nimprovement on English compared to a baseline model with a similar amount of\nparameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level\nmodels with a larger number of parameters. \n\n"}
{"id": "1704.03809", "contents": "Title: A Neural Parametric Singing Synthesizer Abstract: We present a new model for singing synthesis based on a modified version of\nthe WaveNet architecture. Instead of modeling raw waveform, we model features\nproduced by a parametric vocoder that separates the influence of pitch and\ntimbre. This allows conveniently modifying pitch to match any target melody,\nfacilitates training on more modest dataset sizes, and significantly reduces\ntraining and generation times. Our model makes frame-wise predictions using\nmixture density outputs rather than categorical outputs in order to reduce the\nrequired parameter count. As we found overfitting to be an issue with the\nrelatively small datasets used in our experiments, we propose a method to\nregularize the model and make the autoregressive generation process more robust\nto prediction errors. Using a simple multi-stream architecture, harmonic,\naperiodic and voiced/unvoiced components can all be predicted in a coherent\nmanner. We compare our method to existing parametric statistical and\nstate-of-the-art concatenative methods using quantitative metrics and a\nlistening test. While naive implementations of the autoregressive generation\nalgorithm tend to be inefficient, using a smart algorithm we can greatly speed\nup the process and obtain a system that's competitive in both speed and\nquality. \n\n"}
{"id": "1704.04866", "contents": "Title: Effective Warm Start for the Online Actor-Critic Reinforcement Learning\n  based mHealth Intervention Abstract: Online reinforcement learning (RL) is increasingly popular for the\npersonalized mobile health (mHealth) intervention. It is able to personalize\nthe type and dose of interventions according to user's ongoing statuses and\nchanging needs. However, at the beginning of online learning, there are usually\ntoo few samples to support the RL updating, which leads to poor performances. A\ndelay in good performance of the online learning algorithms can be especially\ndetrimental in the mHealth, where users tend to quickly disengage with the\nmHealth app. To address this problem, we propose a new online RL methodology\nthat focuses on an effective warm start. The main idea is to make full use of\nthe data accumulated and the decision rule achieved in a former study. As a\nresult, we can greatly enrich the data size at the beginning of online learning\nin our method. Such case accelerates the online learning process for new users\nto achieve good performances not only at the beginning of online learning but\nalso through the whole online learning process. Besides, we use the decision\nrules achieved in a previous study to initialize the parameter in our online RL\nmodel for new users. It provides a good initialization for the proposed online\nRL algorithm. Experiment results show that promising improvements have been\nachieved by our method compared with the state-of-the-art method. \n\n"}
{"id": "1704.05409", "contents": "Title: Ranking to Learn: Feature Ranking and Selection via Eigenvector\n  Centrality Abstract: In an era where accumulating data is easy and storing it inexpensive, feature\nselection plays a central role in helping to reduce the high-dimensionality of\nhuge amounts of otherwise meaningless data. In this paper, we propose a\ngraph-based method for feature selection that ranks features by identifying the\nmost important ones into arbitrary set of cues. Mapping the problem on an\naffinity graph-where features are the nodes-the solution is given by assessing\nthe importance of nodes through some indicators of centrality, in particular,\nthe Eigen-vector Centrality (EC). The gist of EC is to estimate the importance\nof a feature as a function of the importance of its neighbors. Ranking central\nnodes individuates candidate features, which turn out to be effective from a\nclassification point of view, as proved by a thoroughly experimental section.\nOur approach has been tested on 7 diverse datasets from recent literature\n(e.g., biological data and object recognition, among others), and compared\nagainst filter, embedded and wrappers methods. The results are remarkable in\nterms of accuracy, stability and low execution time. \n\n"}
{"id": "1704.06933", "contents": "Title: Adversarial Neural Machine Translation Abstract: In this paper, we study a new learning paradigm for Neural Machine\nTranslation (NMT). Instead of maximizing the likelihood of the human\ntranslation as in previous works, we minimize the distinction between human\ntranslation and the translation given by an NMT model. To achieve this goal,\ninspired by the recent success of generative adversarial networks (GANs), we\nemploy an adversarial training architecture and name it as Adversarial-NMT. In\nAdversarial-NMT, the training of the NMT model is assisted by an adversary,\nwhich is an elaborately designed Convolutional Neural Network (CNN). The goal\nof the adversary is to differentiate the translation result generated by the\nNMT model from that by human. The goal of the NMT model is to produce high\nquality translations so as to cheat the adversary. A policy gradient method is\nleveraged to co-train the NMT model and the adversary. Experimental results on\nEnglish$\\rightarrow$French and German$\\rightarrow$English translation tasks\nshow that Adversarial-NMT can achieve significantly better translation quality\nthan several strong baselines. \n\n"}
{"id": "1704.07329", "contents": "Title: A Trie-Structured Bayesian Model for Unsupervised Morphological\n  Segmentation Abstract: In this paper, we introduce a trie-structured Bayesian model for unsupervised\nmorphological segmentation. We adopt prior information from different sources\nin the model. We use neural word embeddings to discover words that are\nmorphologically derived from each other and thereby that are semantically\nsimilar. We use letter successor variety counts obtained from tries that are\nbuilt by neural word embeddings. Our results show that using different\ninformation sources such as neural word embeddings and letter successor variety\nas prior information improves morphological segmentation in a Bayesian model.\nOur model outperforms other unsupervised morphological segmentation models on\nTurkish and gives promising results on English and German for scarce resources. \n\n"}
{"id": "1704.07943", "contents": "Title: Reward Maximization Under Uncertainty: Leveraging Side-Observations on\n  Networks Abstract: We study the stochastic multi-armed bandit (MAB) problem in the presence of\nside-observations across actions that occur as a result of an underlying\nnetwork structure. In our model, a bipartite graph captures the relationship\nbetween actions and a common set of unknowns such that choosing an action\nreveals observations for the unknowns that it is connected to. This models a\ncommon scenario in online social networks where users respond to their friends'\nactivity, thus providing side information about each other's preferences. Our\ncontributions are as follows: 1) We derive an asymptotic lower bound (with\nrespect to time) as a function of the bi-partite network structure on the\nregret of any uniformly good policy that achieves the maximum long-term average\nreward. 2) We propose two policies - a randomized policy; and a policy based on\nthe well-known upper confidence bound (UCB) policies - both of which explore\neach action at a rate that is a function of its network position. We show,\nunder mild assumptions, that these policies achieve the asymptotic lower bound\non the regret up to a multiplicative factor, independent of the network\nstructure. Finally, we use numerical examples on a real-world social network\nand a routing example network to demonstrate the benefits obtained by our\npolicies over other existing policies. \n\n"}
{"id": "1704.08432", "contents": "Title: DeepCCI: End-to-end Deep Learning for Chemical-Chemical Interaction\n  Prediction Abstract: Chemical-chemical interaction (CCI) plays a key role in predicting candidate\ndrugs, toxicity, therapeutic effects, and biological functions. In various\ntypes of chemical analyses, computational approaches are often required due to\nthe amount of data that needs to be handled. The recent remarkable growth and\noutstanding performance of deep learning have attracted considerable research\nattention. However,even in state-of-the-art drug analysis methods, deep\nlearning continues to be used only as a classifier, although deep learning is\ncapable of not only simple classification but also automated feature\nextraction. In this paper, we propose the first end-to-end learning method for\nCCI, named DeepCCI. Hidden features are derived from a simplified molecular\ninput line entry system (SMILES), which is a string notation representing the\nchemical structure, instead of learning from crafted features. To discover\nhidden representations for the SMILES strings, we use convolutional neural\nnetworks (CNNs). To guarantee the commutative property for homogeneous\ninteraction, we apply model sharing and hidden representation merging\ntechniques. The performance of DeepCCI was compared with a plain deep\nclassifier and conventional machine learning methods. The proposed DeepCCI\nshowed the best performance in all seven evaluation metrics used. In addition,\nthe commutative property was experimentally validated. The automatically\nextracted features through end-to-end SMILES learning alleviates the\nsignificant efforts required for manual feature engineering. It is expected to\nimprove prediction performance, in drug analyses. \n\n"}
{"id": "1704.08829", "contents": "Title: Deep Feature Learning for Graphs Abstract: This paper presents a general graph representation learning framework called\nDeepGL for learning deep node and edge representations from large (attributed)\ngraphs. In particular, DeepGL begins by deriving a set of base features (e.g.,\ngraphlet features) and automatically learns a multi-layered hierarchical graph\nrepresentation where each successive layer leverages the output from the\nprevious layer to learn features of a higher-order. Contrary to previous work,\nDeepGL learns relational functions (each representing a feature) that\ngeneralize across-networks and therefore useful for graph-based transfer\nlearning tasks. Moreover, DeepGL naturally supports attributed graphs, learns\ninterpretable features, and is space-efficient (by learning sparse feature\nvectors). In addition, DeepGL is expressive, flexible with many interchangeable\ncomponents, efficient with a time complexity of $\\mathcal{O}(|E|)$, and\nscalable for large networks via an efficient parallel implementation. Compared\nwith the state-of-the-art method, DeepGL is (1) effective for across-network\ntransfer learning tasks and attributed graph representation learning, (2)\nspace-efficient requiring up to 6x less memory, (3) fast with up to 182x\nspeedup in runtime performance, and (4) accurate with an average improvement of\n20% or more on many learning tasks. \n\n"}
{"id": "1705.02210", "contents": "Title: SLDR-DL: A Framework for SLD-Resolution with Deep Learning Abstract: This paper introduces an SLD-resolution technique based on deep learning.\nThis technique enables neural networks to learn from old and successful\nresolution processes and to use learnt experiences to guide new resolution\nprocesses. An implementation of this technique is named SLDR-DL. It includes a\nProlog library of deep feedforward neural networks and some essential functions\nof resolution. In the SLDR-DL framework, users can define logical rules in the\nform of definite clauses and teach neural networks to use the rules in\nreasoning processes. \n\n"}
{"id": "1705.03998", "contents": "Title: Mining Functional Modules by Multiview-NMF of Phenome-Genome Association Abstract: Background: Mining gene modules from genomic data is an important step to\ndetect gene members of pathways or other relations such as protein-protein\ninteractions. In this work, we explore the plausibility of detecting gene\nmodules by factorizing gene-phenotype associations from a phenotype ontology\nrather than the conventionally used gene expression data. In particular, the\nhierarchical structure of ontology has not been sufficiently utilized in\nclustering genes while functionally related genes are consistently associated\nwith phenotypes on the same path in the phenotype ontology. Results: We propose\na hierarchal Nonnegative Matrix Factorization (NMF)-based method, called\nConsistent Multiple Nonnegative Matrix Factorization (CMNMF), to factorize\ngenome-phenome association matrix at two levels of the hierarchical structure\nin phenotype ontology for mining gene functional modules. CMNMF constrains the\ngene clusters from the association matrices at two consecutive levels to be\nconsistent since the genes are annotated with both the child phenotype and the\nparent phenotype in the consecutive levels. CMNMF also restricts the identified\nphenotype clusters to be densely connected in the phenotype ontology hierarchy.\nIn the experiments on mining functionally related genes from mouse phenotype\nontology and human phenotype ontology, CMNMF effectively improved clustering\nperformance over the baseline methods. Gene ontology enrichment analysis was\nalso conducted to reveal interesting gene modules. Conclusions: Utilizing the\ninformation in the hierarchical structure of phenotype ontology, CMNMF can\nidentify functional gene modules with more biological significance than the\nconventional methods. CMNMF could also be a better tool for predicting members\nof gene pathways and protein-protein interactions. Availability:\nhttps://github.com/nkiip/CMNMF \n\n"}
{"id": "1705.04243", "contents": "Title: Spectral gap estimates in mean field spin glasses Abstract: We show that mixing for local, reversible dynamics of mean field spin glasses\nis exponentially slow in the low temperature regime. We introduce a notion of\nfree energy barriers for the overlap, and prove that their existence imply that\nthe spectral gap is exponentially small, and thus that mixing is exponentially\nslow. We then exhibit sufficient conditions on the equilibrium Gibbs measure\nwhich guarantee the existence of these barriers, using the notion of replicon\neigenvalue and 2D Guerra Talagrand bounds. We show how these sufficient\nconditions cover large classes of Ising spin models for reversible\nnearest-neighbor dynamics and spherical models for Langevin dynamics. Finally,\nin the case of Ising spins, Panchenko's recent rigorous calculation [79] of the\nfree energy for a system of \"two real replica\" enables us to prove a quenched\nLDP for the overlap distribution, which gives us a wider criterion for slow\nmixing directly related to the Franz-Parisi-Virasoro approach [43,60]. This\ncondition holds in a wider range of temperatures. \n\n"}
{"id": "1705.04282", "contents": "Title: Learning to see people like people Abstract: Humans make complex inferences on faces, ranging from objective properties\n(gender, ethnicity, expression, age, identity, etc) to subjective judgments\n(facial attractiveness, trustworthiness, sociability, friendliness, etc). While\nthe objective aspects of face perception have been extensively studied,\nrelatively fewer computational models have been developed for the social\nimpressions of faces. Bridging this gap, we develop a method to predict human\nimpressions of faces in 40 subjective social dimensions, using deep\nrepresentations from state-of-the-art neural networks. We find that model\nperformance grows as the human consensus on a face trait increases, and that\nmodel predictions outperform human groups in correlation with human averages.\nThis illustrates the learnability of subjective social perception of faces,\nespecially when there is high human consensus. Our system can be used to decide\nwhich photographs from a personal collection will make the best impression. The\nresults are significant for the field of social robotics, demonstrating that\nrobots can learn the subjective judgments defining the underlying fabric of\nhuman interaction. \n\n"}
{"id": "1705.06273", "contents": "Title: Transfer Learning for Named-Entity Recognition with Neural Networks Abstract: Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for named-entity recognition (NER). In order to achieve high\nperformances, ANNs need to be trained on a large labeled dataset. However,\nlabels might be difficult to obtain for the dataset on which the user wants to\nperform NER: label scarcity is particularly pronounced for patient note\nde-identification, which is an instance of NER. In this work, we analyze to\nwhat extent transfer learning may address this issue. In particular, we\ndemonstrate that transferring an ANN model trained on a large labeled dataset\nto another dataset with a limited number of labels improves upon the\nstate-of-the-art results on two different datasets for patient note\nde-identification. \n\n"}
{"id": "1705.07371", "contents": "Title: Spelling Correction as a Foreign Language Abstract: In this paper, we reformulated the spell correction problem as a machine\ntranslation task under the encoder-decoder framework. This reformulation\nenabled us to use a single model for solving the problem that is traditionally\nformulated as learning a language model and an error model. This model employs\nmulti-layer recurrent neural networks as an encoder and a decoder. We\ndemonstrate the effectiveness of this model using an internal dataset, where\nthe training data is automatically obtained from user logs. The model offers\ncompetitive performance as compared to the state of the art methods but does\nnot require any feature engineering nor hand tuning between models. \n\n"}
{"id": "1705.07485", "contents": "Title: Shake-Shake regularization Abstract: The method introduced in this paper aims at helping deep learning\npractitioners faced with an overfit problem. The idea is to replace, in a\nmulti-branch network, the standard summation of parallel branches with a\nstochastic affine combination. Applied to 3-branch residual networks,\nshake-shake regularization improves on the best single shot published results\non CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%.\nExperiments on architectures without skip connections or Batch Normalization\nshow encouraging results and open the door to a large set of applications. Code\nis available at https://github.com/xgastaldi/shake-shake \n\n"}
{"id": "1705.07562", "contents": "Title: On the diffusion approximation of nonconvex stochastic gradient descent Abstract: We study the Stochastic Gradient Descent (SGD) method in nonconvex\noptimization problems from the point of view of approximating diffusion\nprocesses. We prove rigorously that the diffusion process can approximate the\nSGD algorithm weakly using the weak form of master equation for probability\nevolution. In the small step size regime and the presence of omnidirectional\nnoise, our weak approximating diffusion process suggests the following dynamics\nfor the SGD iteration starting from a local minimizer (resp.~saddle point): it\nescapes in a number of iterations exponentially (resp.~almost linearly)\ndependent on the inverse stepsize. The results are obtained using the theory\nfor random perturbations of dynamical systems (theory of large deviations for\nlocal minimizers and theory of exiting for unstable stationary points). In\naddition, we discuss the effects of batch size for the deep neural networks,\nand we find that small batch size is helpful for SGD algorithms to escape\nunstable stationary points and sharp minimizers. Our theory indicates that one\nshould increase the batch size at later stage for the SGD to be trapped in flat\nminimizers for better generalization. \n\n"}
{"id": "1705.07815", "contents": "Title: Minimax Statistical Learning with Wasserstein Distances Abstract: As opposed to standard empirical risk minimization (ERM), distributionally\nrobust optimization aims to minimize the worst-case risk over a larger\nambiguity set containing the original empirical distribution of the training\ndata. In this work, we describe a minimax framework for statistical learning\nwith ambiguity sets given by balls in Wasserstein space. In particular, we\nprove generalization bounds that involve the covering number properties of the\noriginal ERM problem. As an illustrative example, we provide generalization\nguarantees for transport-based domain adaptation problems where the Wasserstein\ndistance between the source and target domain distributions can be reliably\nestimated from unlabeled samples. \n\n"}
{"id": "1705.08378", "contents": "Title: Detecting Adversarial Image Examples in Deep Networks with Adaptive\n  Noise Reduction Abstract: Recently, many studies have demonstrated deep neural network (DNN)\nclassifiers can be fooled by the adversarial example, which is crafted via\nintroducing some perturbations into an original sample. Accordingly, some\npowerful defense techniques were proposed. However, existing defense techniques\noften require modifying the target model or depend on the prior knowledge of\nattacks. In this paper, we propose a straightforward method for detecting\nadversarial image examples, which can be directly deployed into unmodified\noff-the-shelf DNN models. We consider the perturbation to images as a kind of\nnoise and introduce two classic image processing techniques, scalar\nquantization and smoothing spatial filter, to reduce its effect. The image\nentropy is employed as a metric to implement an adaptive noise reduction for\ndifferent kinds of images. Consequently, the adversarial example can be\neffectively detected by comparing the classification results of a given sample\nand its denoised version, without referring to any prior knowledge of attacks.\nMore than 20,000 adversarial examples against some state-of-the-art DNN models\nare used to evaluate the proposed method, which are crafted with different\nattack techniques. The experiments show that our detection method can achieve a\nhigh overall F1 score of 96.39% and certainly raises the bar for defense-aware\nattacks. \n\n"}
{"id": "1705.08922", "contents": "Title: Exploring the Regularity of Sparse Structure in Convolutional Neural\n  Networks Abstract: Sparsity helps reduce the computational complexity of deep neural networks by\nskipping zeros. Taking advantage of sparsity is listed as a high priority in\nnext generation DNN accelerators such as TPU. The structure of sparsity, i.e.,\nthe granularity of pruning, affects the efficiency of hardware accelerator\ndesign as well as the prediction accuracy. Coarse-grained pruning creates\nregular sparsity patterns, making it more amenable for hardware acceleration\nbut more challenging to maintain the same accuracy. In this paper we\nquantitatively measure the trade-off between sparsity regularity and prediction\naccuracy, providing insights in how to maintain accuracy while having more a\nmore structured sparsity pattern. Our experimental results show that\ncoarse-grained pruning can achieve a sparsity ratio similar to unstructured\npruning without loss of accuracy. Moreover, due to the index saving effect,\ncoarse-grained pruning is able to obtain a better compression ratio than\nfine-grained sparsity at the same accuracy threshold. Based on the recent\nsparse convolutional neural network accelerator (SCNN), our experiments further\ndemonstrate that coarse-grained sparsity saves about 2x the memory references\ncompared to fine-grained sparsity. Since memory reference is more than two\norders of magnitude more expensive than arithmetic operations, the regularity\nof sparse structure leads to more efficient hardware design. \n\n"}
{"id": "1705.09886", "contents": "Title: Convergence Analysis of Two-layer Neural Networks with ReLU Activation Abstract: In recent years, stochastic gradient descent (SGD) based techniques has\nbecome the standard tools for training neural networks. However, formal\ntheoretical understanding of why SGD can train neural networks in practice is\nlargely missing.\n  In this paper, we make progress on understanding this mystery by providing a\nconvergence analysis for SGD on a rich subset of two-layer feedforward networks\nwith ReLU activations. This subset is characterized by a special structure\ncalled \"identity mapping\". We prove that, if input follows from Gaussian\ndistribution, with standard $O(1/\\sqrt{d})$ initialization of the weights, SGD\nconverges to the global minimum in polynomial number of steps. Unlike normal\nvanilla networks, the \"identity mapping\" makes our network asymmetric and thus\nthe global minimum is unique. To complement our theory, we are also able to\nshow experimentally that multi-layer networks with this mapping have better\nperformance compared with normal vanilla networks.\n  Our convergence theorem differs from traditional non-convex optimization\ntechniques. We show that SGD converges to optimal in \"two phases\": In phase I,\nthe gradient points to the wrong direction, however, a potential function $g$\ngradually decreases. Then in phase II, SGD enters a nice one point convex\nregion and converges. We also show that the identity mapping is necessary for\nconvergence, as it moves the initial point to a better place for optimization.\nExperiment verifies our claims. \n\n"}
{"id": "1705.10229", "contents": "Title: Latent Intention Dialogue Models Abstract: Developing a dialogue agent that is capable of making autonomous decisions\nand communicating by natural language is one of the long-term goals of machine\nlearning research. Traditional approaches either rely on hand-crafting a small\nstate-action set for applying reinforcement learning that is not scalable or\nconstructing deterministic models for learning dialogue sentences that fail to\ncapture natural conversational variability. In this paper, we propose a Latent\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\nlearn underlying dialogue intentions in the framework of neural variational\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\ninterpreted as actions guiding the generation of machine responses, which can\nbe further refined autonomously by reinforcement learning. The experimental\nevaluation of LIDM shows that the model out-performs published benchmarks for\nboth corpus-based and human evaluation, demonstrating the effectiveness of\ndiscrete latent variable models for learning goal-oriented dialogues. \n\n"}
{"id": "1705.10723", "contents": "Title: Fast Regression with an $\\ell_\\infty$ Guarantee Abstract: Sketching has emerged as a powerful technique for speeding up problems in\nnumerical linear algebra, such as regression. In the overconstrained regression\nproblem, one is given an $n \\times d$ matrix $A$, with $n \\gg d$, as well as an\n$n \\times 1$ vector $b$, and one wants to find a vector $\\hat{x}$ so as to\nminimize the residual error $\\|Ax-b\\|_2$. Using the sketch and solve paradigm,\none first computes $S \\cdot A$ and $S \\cdot b$ for a randomly chosen matrix\n$S$, then outputs $x' = (SA)^{\\dagger} Sb$ so as to minimize $\\|SAx' - Sb\\|_2$.\n  The sketch-and-solve paradigm gives a bound on $\\|x'-x^*\\|_2$ when $A$ is\nwell-conditioned. Our main result is that, when $S$ is the subsampled\nrandomized Fourier/Hadamard transform, the error $x' - x^*$ behaves as if it\nlies in a \"random\" direction within this bound: for any fixed direction $a\\in\n\\mathbb{R}^d$, we have with $1 - d^{-c}$ probability that\n  \\[\n  \\langle a, x'-x^*\\rangle \\lesssim\n\\frac{\\|a\\|_2\\|x'-x^*\\|_2}{d^{\\frac{1}{2}-\\gamma}}, \\quad (1)\n  \\]\n  where $c, \\gamma > 0$ are arbitrary constants.\n  This implies $\\|x'-x^*\\|_{\\infty}$ is a factor $d^{\\frac{1}{2}-\\gamma}$\nsmaller than $\\|x'-x^*\\|_2$. It also gives a better bound on the generalization\nof $x'$ to new examples: if rows of $A$ correspond to examples and columns to\nfeatures, then our result gives a better bound for the error introduced by\nsketch-and-solve when classifying fresh examples. We show that not all\noblivious subspace embeddings $S$ satisfy these properties. In particular, we\ngive counterexamples showing that matrices based on Count-Sketch or leverage\nscore sampling do not satisfy these properties.\n  We also provide lower bounds, both on how small $\\|x'-x^*\\|_2$ can be, and\nfor our new guarantee (1), showing that the subsampled randomized\nFourier/Hadamard transform is nearly optimal. \n\n"}
{"id": "1706.00878", "contents": "Title: MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU Abstract: In this paper, we explore optimizations to run Recurrent Neural Network (RNN)\nmodels locally on mobile devices. RNN models are widely used for Natural\nLanguage Processing, Machine Translation, and other tasks. However, existing\nmobile applications that use RNN models do so on the cloud. To address privacy\nand efficiency concerns, we show how RNN models can be run locally on mobile\ndevices. Existing work on porting deep learning models to mobile devices focus\non Convolution Neural Networks (CNNs) and cannot be applied directly to RNN\nmodels. In response, we present MobiRNN, a mobile-specific optimization\nframework that implements GPU offloading specifically for mobile GPUs.\nEvaluations using an RNN model for activity recognition shows that MobiRNN does\nsignificantly decrease the latency of running RNN models on phones. \n\n"}
{"id": "1706.01214", "contents": "Title: Inconsistent Node Flattening for Improving Top-down Hierarchical\n  Classification Abstract: Large-scale classification of data where classes are structurally organized\nin a hierarchy is an important area of research. Top-down approaches that\nexploit the hierarchy during the learning and prediction phase are efficient\nfor large scale hierarchical classification. However, accuracy of top-down\napproaches is poor due to error propagation i.e., prediction errors made at\nhigher levels in the hierarchy cannot be corrected at lower levels. One of the\nmain reason behind errors at the higher levels is the presence of inconsistent\nnodes that are introduced due to the arbitrary process of creating these\nhierarchies by domain experts. In this paper, we propose two different\ndata-driven approaches (local and global) for hierarchical structure\nmodification that identifies and flattens inconsistent nodes present within the\nhierarchy. Our extensive empirical evaluation of the proposed approaches on\nseveral image and text datasets with varying distribution of features, classes\nand training instances per class shows improved classification performance over\ncompeting hierarchical modification approaches. Specifically, we see an\nimprovement upto 7% in Macro-F1 score with our approach over best TD baseline.\nSOURCE CODE: http://www.cs.gmu.edu/~mlbio/InconsistentNodeFlattening \n\n"}
{"id": "1706.01750", "contents": "Title: Multi-View Kernels for Low-Dimensional Modeling of Seismic Events Abstract: The problem of learning from seismic recordings has been studied for years.\nThere is a growing interest in developing automatic mechanisms for identifying\nthe properties of a seismic event. One main motivation is the ability have a\nreliable identification of man-made explosions. The availability of multiple\nhigh-dimensional observations has increased the use of machine learning\ntechniques in a variety of fields. In this work, we propose to use a\nkernel-fusion based dimensionality reduction framework for generating\nmeaningful seismic representations from raw data. The proposed method is tested\non 2023 events that were recorded in Israel and in Jordan. The method achieves\npromising results in classification of event type as well as in estimating the\nlocation of the event. The proposed fusion and dimensionality reduction tools\nmay be applied to other types of geophysical data. \n\n"}
{"id": "1706.02416", "contents": "Title: Generalized Value Iteration Networks: Life Beyond Lattices Abstract: In this paper, we introduce a generalized value iteration network (GVIN),\nwhich is an end-to-end neural network planning module. GVIN emulates the value\niteration algorithm by using a novel graph convolution operator, which enables\nGVIN to learn and plan on irregular spatial graphs. We propose three novel\ndifferentiable kernels as graph convolution operators and show that the\nembedding based kernel achieves the best performance. We further propose\nepisodic Q-learning, an improvement upon traditional n-step Q-learning that\nstabilizes training for networks that contain a planning module. Lastly, we\nevaluate GVIN on planning problems in 2D mazes, irregular graphs, and\nreal-world street networks, showing that GVIN generalizes well for both\narbitrary graphs and unseen graphs of larger scale and outperforms a naive\ngeneralization of VIN (discretizing a spatial graph into a 2D image). \n\n"}
{"id": "1706.02633", "contents": "Title: Real-valued (Medical) Time Series Generation with Recurrent Conditional\n  GANs Abstract: Generative Adversarial Networks (GANs) have shown remarkable success as a\nframework for training models to produce realistic-looking data. In this work,\nwe propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to\nproduce realistic real-valued multi-dimensional time series, with an emphasis\non their application to medical data. RGANs make use of recurrent neural\nnetworks in the generator and the discriminator. In the case of RCGANs, both of\nthese RNNs are conditioned on auxiliary information. We demonstrate our models\nin a set of toy datasets, where we show visually and quantitatively (using\nsample likelihood and maximum mean discrepancy) that they can successfully\ngenerate realistic time-series. We also describe novel evaluation methods for\nGANs, where we generate a synthetic labelled training dataset, and evaluate on\na real test set the performance of a model trained on the synthetic data, and\nvice-versa. We illustrate with these metrics that RCGANs can generate\ntime-series data useful for supervised training, with only minor degradation in\nperformance on real test data. This is demonstrated on digit classification\nfrom 'serialised' MNIST and by training an early warning system on a medical\ndataset of 17,000 patients from an intensive care unit. We further discuss and\nanalyse the privacy concerns that may arise when using RCGANs to generate\nrealistic synthetic medical time series data. \n\n"}
{"id": "1706.03283", "contents": "Title: Deep Recurrent Neural Networks for seizure detection and early seizure\n  detection systems Abstract: Epilepsy is common neurological diseases, affecting about 0.6-0.8 % of world\npopulation. Epileptic patients suffer from chronic unprovoked seizures, which\ncan result in broad spectrum of debilitating medical and social consequences.\nSince seizures, in general, occur infrequently and are unpredictable, automated\nseizure detection systems are recommended to screen for seizures during\nlong-term electroencephalogram (EEG) recordings. In addition, systems for early\nseizure detection can lead to the development of new types of intervention\nsystems that are designed to control or shorten the duration of seizure events.\nIn this article, we investigate the utility of recurrent neural networks (RNNs)\nin designing seizure detection and early seizure detection systems. We propose\na deep learning framework via the use of Gated Recurrent Unit (GRU) RNNs for\nseizure detection. We use publicly available data in order to evaluate our\nmethod and demonstrate very promising evaluation results with overall accuracy\nclose to 100 %. We also systematically investigate the application of our\nmethod for early seizure warning systems. Our method can detect about 98% of\nseizure events within the first 5 seconds of the overall epileptic seizure\nduration. \n\n"}
{"id": "1706.03301", "contents": "Title: Neural networks and rational functions Abstract: Neural networks and rational functions efficiently approximate each other. In\nmore detail, it is shown here that for any ReLU network, there exists a\nrational function of degree $O(\\text{polylog}(1/\\epsilon))$ which is\n$\\epsilon$-close, and similarly for any rational function there exists a ReLU\nnetwork of size $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close. By\ncontrast, polynomials need degree $\\Omega(\\text{poly}(1/\\epsilon))$ to\napproximate even a single ReLU. When converting a ReLU network to a rational\nfunction as above, the hidden constants depend exponentially on the number of\nlayers, which is shown to be tight; in other words, a compositional\nrepresentation can be beneficial even for rational functions. \n\n"}
{"id": "1706.03815", "contents": "Title: Encoding of phonology in a recurrent neural model of grounded speech Abstract: We study the representation and encoding of phonemes in a recurrent neural\nnetwork model of grounded speech. We use a model which processes images and\ntheir spoken descriptions, and projects the visual and auditory representations\ninto the same semantic space. We perform a number of analyses on how\ninformation about individual phonemes is encoded in the MFCC features extracted\nfrom the speech signal, and the activations of the layers of the model. Via\nexperiments with phoneme decoding and phoneme discrimination we show that\nphoneme representations are most salient in the lower layers of the model,\nwhere low-level signals are processed at a fine-grained level, although a large\namount of phonological information is retain at the top recurrent layer. We\nfurther find out that the attention mechanism following the top recurrent layer\nsignificantly attenuates encoding of phonology and makes the utterance\nembeddings much more invariant to synonymy. Moreover, a hierarchical clustering\nof phoneme representations learned by the network shows an organizational\nstructure of phonemes similar to those proposed in linguistics. \n\n"}
{"id": "1706.04148", "contents": "Title: Personalizing Session-based Recommendations with Hierarchical Recurrent\n  Neural Networks Abstract: Session-based recommendations are highly relevant in many modern on-line\nservices (e.g. e-commerce, video streaming) and recommendation settings.\nRecently, Recurrent Neural Networks have been shown to perform very well in\nsession-based settings. While in many session-based recommendation domains user\nidentifiers are hard to come by, there are also domains in which user profiles\nare readily available. We propose a seamless way to personalize RNN models with\ncross-session information transfer and devise a Hierarchical RNN model that\nrelays end evolves latent hidden states of the RNNs across user sessions.\nResults on two industry datasets show large improvements over the session-only\nRNNs. \n\n"}
{"id": "1706.04454", "contents": "Title: Empirical Analysis of the Hessian of Over-Parametrized Neural Networks Abstract: We study the properties of common loss surfaces through their Hessian matrix.\nIn particular, in the context of deep learning, we empirically show that the\nspectrum of the Hessian is composed of two parts: (1) the bulk centered near\nzero, (2) and outliers away from the bulk. We present numerical evidence and\nmathematical justifications to the following conjectures laid out by Sagun et\nal. (2016): Fixing data, increasing the number of parameters merely scales the\nbulk of the spectrum; fixing the dimension and changing the data (for instance\nadding more clusters or making the data less separable) only affects the\noutliers. We believe that our observations have striking implications for\nnon-convex optimization in high dimensions. First, the flatness of such\nlandscapes (which can be measured by the singularity of the Hessian) implies\nthat classical notions of basins of attraction may be quite misleading. And\nthat the discussion of wide/narrow basins may be in need of a new perspective\naround over-parametrization and redundancy that are able to create large\nconnected components at the bottom of the landscape. Second, the dependence of\nsmall number of large eigenvalues to the data distribution can be linked to the\nspectrum of the covariance matrix of gradients of model outputs. With this in\nmind, we may reevaluate the connections within the data-architecture-algorithm\nframework of a model, hoping that it would shed light into the geometry of\nhigh-dimensional and non-convex spaces in modern applications. In particular,\nwe present a case that links the two observations: small and large batch\ngradient descent appear to converge to different basins of attraction but we\nshow that they are in fact connected through their flat region and so belong to\nthe same basin. \n\n"}
{"id": "1706.04872", "contents": "Title: Towards a theory of word order. Comment on \"Dependency distance: a new\n  perspective on syntactic patterns in natural language\" by Haitao Liu et al Abstract: Comment on \"Dependency distance: a new perspective on syntactic patterns in\nnatural language\" by Haitao Liu et al \n\n"}
{"id": "1706.07642", "contents": "Title: A Variance Maximization Criterion for Active Learning Abstract: Active learning aims to train a classifier as fast as possible with as few\nlabels as possible. The core element in virtually any active learning strategy\nis the criterion that measures the usefulness of the unlabeled data based on\nwhich new points to be labeled are picked. We propose a novel approach which we\nrefer to as maximizing variance for active learning or MVAL for short. MVAL\nmeasures the value of unlabeled instances by evaluating the rate of change of\noutput variables caused by changes in the next sample to be queried and its\npotential labelling. In a sense, this criterion measures how unstable the\nclassifier's output is for the unlabeled data points under perturbations of the\ntraining data. MVAL maintains, what we refer to as, retraining information\nmatrices to keep track of these output scores and exploits two kinds of\nvariance to measure the informativeness and representativeness, respectively.\nBy fusing these variances, MVAL is able to select the instances which are both\ninformative and representative. We employ our technique both in combination\nwith logistic regression and support vector machines and demonstrate that MVAL\nachieves state-of-the-art performance in experiments on a large number of\nstandard benchmark datasets. \n\n"}
{"id": "1707.00061", "contents": "Title: Racial Disparity in Natural Language Processing: A Case Study of Social\n  Media African-American English Abstract: We highlight an important frontier in algorithmic fairness: disparity in the\nquality of natural language processing algorithms when applied to language from\nauthors of different social groups. For example, current systems sometimes\nanalyze the language of females and minorities more poorly than they do of\nwhites and males. We conduct an empirical analysis of racial disparity in\nlanguage identification for tweets written in African-American English, and\ndiscuss implications of disparity in NLP. \n\n"}
{"id": "1707.00143", "contents": "Title: Fast Approximate Nearest Neighbor Search With The Navigating\n  Spreading-out Graph Abstract: Approximate nearest neighbor search (ANNS) is a fundamental problem in\ndatabases and data mining. A scalable ANNS algorithm should be both\nmemory-efficient and fast. Some early graph-based approaches have shown\nattractive theoretical guarantees on search time complexity, but they all\nsuffer from the problem of high indexing time complexity. Recently, some\ngraph-based methods have been proposed to reduce indexing complexity by\napproximating the traditional graphs; these methods have achieved revolutionary\nperformance on million-scale datasets. Yet, they still can not scale to\nbillion-node databases. In this paper, to further improve the search-efficiency\nand scalability of graph-based methods, we start by introducing four aspects:\n(1) ensuring the connectivity of the graph; (2) lowering the average out-degree\nof the graph for fast traversal; (3) shortening the search path; and (4)\nreducing the index size. Then, we propose a novel graph structure called\nMonotonic Relative Neighborhood Graph (MRNG) which guarantees very low search\ncomplexity (close to logarithmic time). To further lower the indexing\ncomplexity and make it practical for billion-node ANNS problems, we propose a\nnovel graph structure named Navigating Spreading-out Graph (NSG) by\napproximating the MRNG. The NSG takes the four aspects into account\nsimultaneously. Extensive experiments show that NSG outperforms all the\nexisting algorithms significantly. In addition, NSG shows superior performance\nin the E-commercial search scenario of Taobao (Alibaba Group) and has been\nintegrated into their search engine at billion-node scale. \n\n"}
{"id": "1707.00391", "contents": "Title: Fair Pipelines Abstract: This work facilitates ensuring fairness of machine learning in the real world\nby decoupling fairness considerations in compound decisions. In particular,\nthis work studies how fairness propagates through a compound decision-making\nprocesses, which we call a pipeline. Prior work in algorithmic fairness only\nfocuses on fairness with respect to one decision. However, many decision-making\nprocesses require more than one decision. For instance, hiring is at least a\ntwo stage model: deciding who to interview from the applicant pool and then\ndeciding who to hire from the interview pool. Perhaps surprisingly, we show\nthat the composition of fair components may not guarantee a fair pipeline under\na $(1+\\varepsilon)$-equal opportunity definition of fair. However, we identify\ncircumstances that do provide that guarantee. We also propose numerous\ndirections for future work on more general compound machine learning decisions. \n\n"}
{"id": "1707.00684", "contents": "Title: Deep-learning-based data page classification for holographic memory Abstract: We propose a deep-learning-based classification of data pages used in\nholographic memory. We numerically investigated the classification performance\nof a conventional multi-layer perceptron (MLP) and a deep neural network, under\nthe condition that reconstructed page data are contaminated by some noise and\nare randomly laterally shifted. The MLP was found to have a classification\naccuracy of 91.58%, whereas the deep neural network was able to classify data\npages at an accuracy of 99.98%. The accuracy of the deep neural network is two\norders of magnitude better than the MLP. \n\n"}
{"id": "1707.01213", "contents": "Title: Data-Driven Sparse Structure Selection for Deep Neural Networks Abstract: Deep convolutional neural networks have liberated its extraordinary power on\nvarious tasks. However, it is still very challenging to deploy state-of-the-art\nmodels into real-world applications due to their high computational complexity.\nHow can we design a compact and effective network without massive experiments\nand expert knowledge? In this paper, we propose a simple and effective\nframework to learn and prune deep models in an end-to-end manner. In our\nframework, a new type of parameter -- scaling factor is first introduced to\nscale the outputs of specific structures, such as neurons, groups or residual\nblocks. Then we add sparsity regularizations on these factors, and solve this\noptimization problem by a modified stochastic Accelerated Proximal Gradient\n(APG) method. By forcing some of the factors to zero, we can safely remove the\ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing\nwith other structure selection methods that may need thousands of trials or\niterative fine-tuning, our method is trained fully end-to-end in one training\npass without bells and whistles. We evaluate our method, Sparse Structure\nSelection with several state-of-the-art CNNs, and demonstrate very promising\nresults with adaptive depth and width selection. \n\n"}
{"id": "1707.02392", "contents": "Title: Learning Representations and Generative Models for 3D Point Clouds Abstract: Three-dimensional geometric data offer an excellent domain for studying\nrepresentation learning and generative modeling. In this paper, we look at\ngeometric data represented as point clouds. We introduce a deep AutoEncoder\n(AE) network with state-of-the-art reconstruction quality and generalization\nability. The learned representations outperform existing methods on 3D\nrecognition tasks and enable shape editing via simple algebraic manipulations,\nsuch as semantic part editing, shape analogies and shape interpolation, as well\nas shape completion. We perform a thorough study of different generative models\nincluding GANs operating on the raw point clouds, significantly improved GANs\ntrained in the fixed latent space of our AEs, and Gaussian Mixture Models\n(GMMs). To quantitatively evaluate generative models we introduce measures of\nsample fidelity and diversity based on matchings between sets of point clouds.\nInterestingly, our evaluation of generalization, fidelity and diversity reveals\nthat GMMs trained in the latent space of our AEs yield the best results\noverall. \n\n"}
{"id": "1707.02702", "contents": "Title: Composition Properties of Inferential Privacy for Time-Series Data Abstract: With the proliferation of mobile devices and the internet of things,\ndeveloping principled solutions for privacy in time series applications has\nbecome increasingly important. While differential privacy is the gold standard\nfor database privacy, many time series applications require a different kind of\nguarantee, and a number of recent works have used some form of inferential\nprivacy to address these situations.\n  However, a major barrier to using inferential privacy in practice is its lack\nof graceful composition -- even if the same or related sensitive data is used\nin multiple releases that are safe individually, the combined release may have\npoor privacy properties. In this paper, we study composition properties of a\nform of inferential privacy called Pufferfish when applied to time-series data.\nWe show that while general Pufferfish mechanisms may not compose gracefully, a\nspecific Pufferfish mechanism, called the Markov Quilt Mechanism, which was\nrecently introduced, has strong composition properties comparable to that of\npure differential privacy when applied to time series data. \n\n"}
{"id": "1707.04412", "contents": "Title: Evaluating Semantic Parsing against a Simple Web-based Question\n  Answering Model Abstract: Semantic parsing shines at analyzing complex natural language that involves\ncomposition and computation over multiple pieces of evidence. However, datasets\nfor semantic parsing contain many factoid questions that can be answered from a\nsingle web document. In this paper, we propose to evaluate semantic\nparsing-based question answering models by comparing them to a question\nanswering baseline that queries the web and extracts the answer only from web\nsnippets, without access to the target knowledge-base. We investigate this\napproach on COMPLEXQUESTIONS, a dataset designed to focus on compositional\nlanguage, and find that our model obtains reasonable performance (35 F1\ncompared to 41 F1 of state-of-the-art). We find in our analysis that our model\nperforms well on complex questions involving conjunctions, but struggles on\nquestions that involve relation composition and superlatives. \n\n"}
{"id": "1707.05878", "contents": "Title: On-line Building Energy Optimization using Deep Reinforcement Learning Abstract: Unprecedented high volumes of data are becoming available with the growth of\nthe advanced metering infrastructure. These are expected to benefit planning\nand operation of the future power system, and to help the customers transition\nfrom a passive to an active role. In this paper, we explore for the first time\nin the smart grid context the benefits of using Deep Reinforcement Learning, a\nhybrid type of methods that combines Reinforcement Learning with Deep Learning,\nto perform on-line optimization of schedules for building energy management\nsystems. The learning procedure was explored using two methods, Deep Q-learning\nand Deep Policy Gradient, both of them being extended to perform multiple\nactions simultaneously. The proposed approach was validated on the large-scale\nPecan Street Inc. database. This highly-dimensional database includes\ninformation about photovoltaic power generation, electric vehicles as well as\nbuildings appliances. Moreover, these on-line energy scheduling strategies\ncould be used to provide real-time feedback to consumers to encourage more\nefficient use of electricity. \n\n"}
{"id": "1707.08139", "contents": "Title: Analogs of Linguistic Structure in Deep Representations Abstract: We investigate the compositional structure of message vectors computed by a\ndeep network trained on a communication game. By comparing truth-conditional\nrepresentations of encoder-produced message vectors to human-produced referring\nexpressions, we are able to identify aligned (vector, utterance) pairs with the\nsame meaning. We then search for structured relationships among these aligned\npairs to discover simple vector space transformations corresponding to\nnegation, conjunction, and disjunction. Our results suggest that neural\nrepresentations are capable of spontaneously developing a \"syntax\" with\nfunctional analogues to qualitative properties of natural language. \n\n"}
{"id": "1707.08308", "contents": "Title: Tensor Regression Networks Abstract: Convolutional neural networks typically consist of many convolutional layers\nfollowed by one or more fully connected layers. While convolutional layers map\nbetween high-order activation tensors, the fully connected layers operate on\nflattened activation vectors. Despite empirical success, this approach has\nnotable drawbacks. Flattening followed by fully connected layers discards\nmultilinear structure in the activations and requires many parameters. We\naddress these problems by incorporating tensor algebraic operations that\npreserve multilinear structure at every layer. First, we introduce Tensor\nContraction Layers (TCLs) that reduce the dimensionality of their input while\npreserving their multilinear structure using tensor contraction. Next, we\nintroduce Tensor Regression Layers (TRLs), which express outputs through a\nlow-rank multilinear mapping from a high-order activation tensor to an output\ntensor of arbitrary order. We learn the contraction and regression factors\nend-to-end, and produce accurate nets with fewer parameters. Additionally, our\nlayers regularize networks by imposing low-rank constraints on the activations\n(TCL) and regression weights (TRL). Experiments on ImageNet show that, applied\nto VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters\ncompared to fully connected layers by more than 65% while maintaining or\nincreasing accuracy. In addition to the space savings, our approach's ability\nto leverage topological structure can be crucial for structured data such as\nMRI. In particular, we demonstrate significant performance improvements over\ncomparable architectures on three tasks associated with the UK Biobank dataset. \n\n"}
{"id": "1707.08552", "contents": "Title: A Robust Multi-Batch L-BFGS Method for Machine Learning Abstract: This paper describes an implementation of the L-BFGS method designed to deal\nwith two adversarial situations. The first occurs in distributed computing\nenvironments where some of the computational nodes devoted to the evaluation of\nthe function and gradient are unable to return results on time. A similar\nchallenge occurs in a multi-batch approach in which the data points used to\ncompute function and gradients are purposely changed at each iteration to\naccelerate the learning process. Difficulties arise because L-BFGS employs\ngradient differences to update the Hessian approximations, and when these\ngradients are computed using different data points the updating process can be\nunstable. This paper shows how to perform stable quasi-Newton updating in the\nmulti-batch setting, studies the convergence properties for both convex and\nnonconvex functions, and illustrates the behavior of the algorithm in a\ndistributed computing platform on binary classification logistic regression and\nneural network training problems that arise in machine learning. \n\n"}
{"id": "1707.09168", "contents": "Title: Learning to Predict Charges for Criminal Cases with Legal Basis Abstract: The charge prediction task is to determine appropriate charges for a given\ncase, which is helpful for legal assistant systems where the user input is fact\ndescription. We argue that relevant law articles play an important role in this\ntask, and therefore propose an attention-based neural network method to jointly\nmodel the charge prediction task and the relevant article extraction task in a\nunified framework. The experimental results show that, besides providing legal\nbasis, the relevant articles can also clearly improve the charge prediction\nresults, and our full model can effectively predict appropriate charges for\ncases with different expression styles. \n\n"}
{"id": "1707.09872", "contents": "Title: Full-Network Embedding in a Multimodal Embedding Pipeline Abstract: The current state-of-the-art for image annotation and image retrieval tasks\nis obtained through deep neural networks, which combine an image representation\nand a text representation into a shared embedding space. In this paper we\nevaluate the impact of using the Full-Network embedding in this setting,\nreplacing the original image representation in a competitive multimodal\nembedding generation scheme. Unlike the one-layer image embeddings typically\nused by most approaches, the Full-Network embedding provides a multi-scale\nrepresentation of images, which results in richer characterizations. To measure\nthe influence of the Full-Network embedding, we evaluate its performance on\nthree different datasets, and compare the results with the original multimodal\nembedding generation scheme when using a one-layer image embedding, and with\nthe rest of the state-of-the-art. Results for image annotation and image\nretrieval tasks indicate that the Full-Network embedding is consistently\nsuperior to the one-layer embedding. These results motivate the integration of\nthe Full-Network embedding on any multimodal embedding generation scheme,\nsomething feasible thanks to the flexibility of the approach. \n\n"}
{"id": "1708.00102", "contents": "Title: Advantages and Limitations of using Successor Features for Transfer in\n  Reinforcement Learning Abstract: One question central to Reinforcement Learning is how to learn a feature\nrepresentation that supports algorithm scaling and re-use of learned\ninformation from different tasks. Successor Features approach this problem by\nlearning a feature representation that satisfies a temporal constraint. We\npresent an implementation of an approach that decouples the feature\nrepresentation from the reward function, making it suitable for transferring\nknowledge between domains. We then assess the advantages and limitations of\nusing Successor Features for transfer. \n\n"}
{"id": "1708.00117", "contents": "Title: Compiling Deep Learning Models for Custom Hardware Accelerators Abstract: Convolutional neural networks (CNNs) are the core of most state-of-the-art\ndeep learning algorithms specialized for object detection and classification.\nCNNs are both computationally complex and embarrassingly parallel. Two\nproperties that leave room for potential software and hardware optimizations\nfor embedded systems. Given a programmable hardware accelerator with a CNN\noriented custom instructions set, the compiler's task is to exploit the\nhardware's full potential, while abiding with the hardware constraints and\nmaintaining generality to run different CNN models with varying workload\nproperties. Snowflake is an efficient and scalable hardware accelerator\nimplemented on programmable logic devices. It implements a control pipeline for\na custom instruction set. The goal of this paper is to present Snowflake's\ncompiler that generates machine level instructions from Torch7 model\ndescription files. The main software design points explored in this work are:\nmodel structure parsing, CNN workload breakdown, loop rearrangement for memory\nbandwidth optimizations and memory access balancing. The performance achieved\nby compiler generated instructions matches against hand optimized code for\nconvolution layers. Generated instructions also efficiently execute AlexNet and\nResNet18 inference on Snowflake. Snowflake with $256$ processing units was\nsynthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in\n$93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$\nframes/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W. \n\n"}
{"id": "1708.00185", "contents": "Title: Tensorial Recurrent Neural Networks for Longitudinal Data Analysis Abstract: Traditional Recurrent Neural Networks assume vectorized data as inputs.\nHowever many data from modern science and technology come in certain structures\nsuch as tensorial time series data. To apply the recurrent neural networks for\nthis type of data, a vectorisation process is necessary, while such a\nvectorisation leads to the loss of the precise information of the spatial or\nlongitudinal dimensions. In addition, such a vectorized data is not an optimum\nsolution for learning the representation of the longitudinal data. In this\npaper, we propose a new variant of tensorial neural networks which directly\ntake tensorial time series data as inputs. We call this new variant as\nTensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor\nTucker decomposition. \n\n"}
{"id": "1708.01464", "contents": "Title: Massively Multilingual Neural Grapheme-to-Phoneme Conversion Abstract: Grapheme-to-phoneme conversion (g2p) is necessary for text-to-speech and\nautomatic speech recognition systems. Most g2p systems are monolingual: they\nrequire language-specific data or handcrafting of rules. Such systems are\ndifficult to extend to low resource languages, for which data and handcrafted\nrules are not available. As an alternative, we present a neural\nsequence-to-sequence approach to g2p which is trained on\nspelling--pronunciation pairs in hundreds of languages. The system shares a\nsingle encoder and decoder across all languages, allowing it to utilize the\nintrinsic similarities between different writing systems. We show an 11%\nimprovement in phoneme error rate over an approach based on adapting\nhigh-resource monolingual g2p models to low-resource languages. Our model is\nalso much more compact relative to previous approaches. \n\n"}
{"id": "1708.01565", "contents": "Title: Improving Speaker-Independent Lipreading with Domain-Adversarial\n  Training Abstract: We present a Lipreading system, i.e. a speech recognition system using only\nvisual features, which uses domain-adversarial training for speaker\nindependence. Domain-adversarial training is integrated into the optimization\nof a lipreader based on a stack of feedforward and LSTM (Long Short-Term\nMemory) recurrent neural networks, yielding an end-to-end trainable system\nwhich only requires a very small number of frames of untranscribed target data\nto substantially improve the recognition accuracy on the target speaker. On\npairs of different source and target speakers, we achieve a relative accuracy\nimprovement of around 40% with only 15 to 20 seconds of untranscribed target\nspeech data. On multi-speaker training setups, the accuracy improvements are\nsmaller but still substantial. \n\n"}
{"id": "1708.01713", "contents": "Title: Automatic Question-Answering Using A Deep Similarity Neural Network Abstract: Automatic question-answering is a classical problem in natural language\nprocessing, which aims at designing systems that can automatically answer a\nquestion, in the same way as human does. In this work, we propose a deep\nlearning based model for automatic question-answering. First the questions and\nanswers are embedded using neural probabilistic modeling. Then a deep\nsimilarity neural network is trained to find the similarity score of a pair of\nanswer and question. Then for each question, the best answer is found as the\none with the highest similarity score. We first train this model on a\nlarge-scale public question-answering database, and then fine-tune it to\ntransfer to the customer-care chat data. We have also tested our framework on a\npublic question-answering database and achieved very good performance. \n\n"}
{"id": "1708.03246", "contents": "Title: SESA: Supervised Explicit Semantic Analysis Abstract: In recent years supervised representation learning has provided state of the\nart or close to the state of the art results in semantic analysis tasks\nincluding ranking and information retrieval. The core idea is to learn how to\nembed items into a latent space such that they optimize a supervised objective\nin that latent space. The dimensions of the latent space have no clear\nsemantics, and this reduces the interpretability of the system. For example, in\npersonalization models, it is hard to explain why a particular item is ranked\nhigh for a given user profile. We propose a novel model of representation\nlearning called Supervised Explicit Semantic Analysis (SESA) that is trained in\na supervised fashion to embed items to a set of dimensions with explicit\nsemantics. The model learns to compare two objects by representing them in this\nexplicit space, where each dimension corresponds to a concept from a knowledge\nbase. This work extends Explicit Semantic Analysis (ESA) with a supervised\nmodel for ranking problems. We apply this model to the task of Job-Profile\nrelevance in LinkedIn in which a set of skills defines our explicit dimensions\nof the space. Every profile and job are encoded to this set of skills their\nsimilarity is calculated in this space. We use RNNs to embed text input into\nthis space. In addition to interpretability, our model makes use of the\nweb-scale collaborative skills data that is provided by users for each LinkedIn\nprofile. Our model provides state of the art result while it remains\ninterpretable. \n\n"}
{"id": "1708.04622", "contents": "Title: Deep Learning the Ising Model Near Criticality Abstract: It is well established that neural networks with deep architectures perform\nbetter than shallow networks for many tasks in machine learning. In statistical\nphysics, while there has been recent interest in representing physical data\nwith generative modelling, the focus has been on shallow neural networks. A\nnatural question to ask is whether deep neural networks hold any advantage over\nshallow networks in representing such data. We investigate this question by\nusing unsupervised, generative graphical models to learn the probability\ndistribution of a two-dimensional Ising system. Deep Boltzmann machines, deep\nbelief networks, and deep restricted Boltzmann networks are trained on thermal\nspin configurations from this system, and compared to the shallow architecture\nof the restricted Boltzmann machine. We benchmark the models, focussing on the\naccuracy of generating energetic observables near the phase transition, where\nthese quantities are most difficult to approximate. Interestingly, after\ntraining the generative networks, we observe that the accuracy essentially\ndepends only on the number of neurons in the first hidden layer of the network,\nand not on other model details such as network depth or model type. This is\nevidence that shallow networks are more efficient than deep networks at\nrepresenting physical probability distributions associated with Ising systems\nnear criticality. \n\n"}
{"id": "1708.04968", "contents": "Title: Fault in your stars: An Analysis of Android App Reviews Abstract: Mobile app distribution platforms such as Google Play Store allow users to\nshare their feedback about downloaded apps in the form of a review comment and\na corresponding star rating. Typically, the star rating ranges from one to five\nstars, with one star denoting a high sense of dissatisfaction with the app and\nfive stars denoting a high sense of satisfaction.\n  Unfortunately, due to a variety of reasons, often the star rating provided by\na user is inconsistent with the opinion expressed in the review. For example,\nconsider the following review for the Facebook App on Android; \"Awesome App\".\nOne would reasonably expect the rating for this review to be five stars, but\nthe actual rating is one star!\n  Such inconsistent ratings can lead to a deflated (or inflated) overall\naverage rating of an app which can affect user downloads, as typically users\nlook at the average star ratings while making a decision on downloading an app.\nAlso, the app developers receive a biased feedback about the application that\ndoes not represent ground reality. This is especially significant for small\napps with a few thousand downloads as even a small number of mismatched reviews\ncan bring down the average rating drastically.\n  In this paper, we conducted a study on this review-rating mismatch problem.\nWe manually examined 8600 reviews from 10 popular Android apps and found that\n20% of the ratings in our dataset were inconsistent with the review. Further,\nwe developed three systems; two of which were based on traditional machine\nlearning and one on deep learning to automatically identify reviews whose\nrating did not match with the opinion expressed in the review. Our deep\nlearning system performed the best and had an accuracy of 92% in identifying\nthe correct star rating to be associated with a given review. \n\n"}
{"id": "1708.05797", "contents": "Title: CLaC @ QATS: Quality Assessment for Text Simplification Abstract: This paper describes our approach to the 2016 QATS quality assessment shared\ntask. We trained three independent Random Forest classifiers in order to assess\nthe quality of the simplified texts in terms of grammaticality, meaning\npreservation and simplicity. We used the language model of Google-Ngram as\nfeature to predict the grammaticality. Meaning preservation is predicted using\ntwo complementary approaches based on word embedding and WordNet synonyms. A\nwider range of features including TF-IDF, sentence length and frequency of cue\nphrases are used to evaluate the simplicity aspect. Overall, the accuracy of\nthe system ranges from 33.33% for the overall aspect to 58.73% for\ngrammaticality. \n\n"}
{"id": "1708.07827", "contents": "Title: Second-Order Optimization for Non-Convex Machine Learning: An Empirical\n  Study Abstract: While first-order optimization methods such as stochastic gradient descent\n(SGD) are popular in machine learning (ML), they come with well-known\ndeficiencies, including relatively-slow convergence, sensitivity to the\nsettings of hyper-parameters such as learning rate, stagnation at high training\nerrors, and difficulty in escaping flat regions and saddle points. These issues\nare particularly acute in highly non-convex settings such as those arising in\nneural networks. Motivated by this, there has been recent interest in\nsecond-order methods that aim to alleviate these shortcomings by capturing\ncurvature information. In this paper, we report detailed empirical evaluations\nof a class of Newton-type methods, namely sub-sampled variants of trust region\n(TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex\nML problems. In doing so, we demonstrate that these methods not only can be\ncomputationally competitive with hand-tuned SGD with momentum, obtaining\ncomparable or better generalization performance, but also they are highly\nrobust to hyper-parameter settings. Further, in contrast to SGD with momentum,\nwe show that the manner in which these Newton-type methods employ curvature\ninformation allows them to seamlessly escape flat regions and saddle points. \n\n"}
{"id": "1708.09151", "contents": "Title: Paradigm Completion for Derivational Morphology Abstract: The generation of complex derived word forms has been an overlooked problem\nin NLP; we fill this gap by applying neural sequence-to-sequence models to the\ntask. We overview the theoretical motivation for a paradigmatic treatment of\nderivational morphology, and introduce the task of derivational paradigm\ncompletion as a parallel to inflectional paradigm completion. State-of-the-art\nneural models, adapted from the inflection task, are able to learn a range of\nderivation patterns, and outperform a non-neural baseline by 16.4%. However,\ndue to semantic, historical, and lexical considerations involved in\nderivational morphology, future work will be needed to achieve performance\nparity with inflection-generating systems. \n\n"}
{"id": "1709.01643", "contents": "Title: Learning to Compose Domain-Specific Transformations for Data\n  Augmentation Abstract: Data augmentation is a ubiquitous technique for increasing the size of\nlabeled training sets by leveraging task-specific data transformations that\npreserve class labels. While it is often easy for domain experts to specify\nindividual transformations, constructing and tuning the more sophisticated\ncompositions typically needed to achieve state-of-the-art results is a\ntime-consuming manual task in practice. We propose a method for automating this\nprocess by learning a generative sequence model over user-specified\ntransformation functions using a generative adversarial approach. Our method\ncan make use of arbitrary, non-deterministic transformation functions, is\nrobust to misspecified user input, and is trained on unlabeled data. The\nlearned transformation model can then be used to perform data augmentation for\nany end discriminative model. In our experiments, we show the efficacy of our\napproach on both image and text datasets, achieving improvements of 4.0\naccuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task,\nand 3.4 accuracy points when using domain-specific transformation operations on\na medical imaging dataset as compared to standard heuristic augmentation\napproaches. \n\n"}
{"id": "1709.02271", "contents": "Title: Leveraging Discourse Information Effectively for Authorship Attribution Abstract: We explore techniques to maximize the effectiveness of discourse information\nin the task of authorship attribution. We present a novel method to embed\ndiscourse features in a Convolutional Neural Network text classifier, which\nachieves a state-of-the-art result by a substantial margin. We empirically\ninvestigate several featurization methods to understand the conditions under\nwhich discourse features contribute non-trivial performance gains, and analyze\ndiscourse embeddings. \n\n"}
{"id": "1709.02800", "contents": "Title: GOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for\n  Evolving Data Streams Abstract: Designing adaptive classifiers for an evolving data stream is a challenging\ntask due to the data size and its dynamically changing nature. Combining\nindividual classifiers in an online setting, the ensemble approach, is a\nwell-known solution. It is possible that a subset of classifiers in the\nensemble outperforms others in a time-varying fashion. However, optimum weight\nassignment for component classifiers is a problem which is not yet fully\naddressed in online evolving environments. We propose a novel data stream\nensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble\n(GOOWE), which assigns optimum weights to the component classifiers using a\nsliding window containing the most recent data instances. We map vote scores of\nindividual classifiers and true class labels into a spatial environment. Based\non the Euclidean distance between vote scores and ideal-points, and using the\nlinear least squares (LSQ) solution, we present a novel, dynamic, and online\nweighting approach. While LSQ is used for batch mode ensemble classifiers, it\nis the first time that we adapt and use it for online environments by providing\na spatial modeling of online ensembles. In order to show the robustness of the\nproposed algorithm, we use real-world datasets and synthetic data generators\nusing the MOA libraries. First, we analyze the impact of our weighting system\non prediction accuracy through two scenarios. Second, we compare GOOWE with 8\nstate-of-the-art ensemble classifiers in a comprehensive experimental\nenvironment. Our experiments show that GOOWE provides improved reactions to\ndifferent types of concept drift compared to our baselines. The statistical\ntests indicate a significant improvement in accuracy, with conservative time\nand memory requirements. \n\n"}
{"id": "1709.02911", "contents": "Title: Semi-Supervised Instance Population of an Ontology using Word Vector\n  Embeddings Abstract: In many modern day systems such as information extraction and knowledge\nmanagement agents, ontologies play a vital role in maintaining the concept\nhierarchies of the selected domain. However, ontology population has become a\nproblematic process due to its nature of heavy coupling with manual human\nintervention. With the use of word embeddings in the field of natural language\nprocessing, it became a popular topic due to its ability to cope up with\nsemantic sensitivity. Hence, in this study, we propose a novel way of\nsemi-supervised ontology population through word embeddings as the basis. We\nbuilt several models including traditional benchmark models and new types of\nmodels which are based on word embeddings. Finally, we ensemble them together\nto come up with a synergistic model with better accuracy. We demonstrate that\nour ensemble model can outperform the individual models. \n\n"}
{"id": "1709.03159", "contents": "Title: R2N2: Residual Recurrent Neural Networks for Multivariate Time Series\n  Forecasting Abstract: Multivariate time-series modeling and forecasting is an important problem\nwith numerous applications. Traditional approaches such as VAR (vector\nauto-regressive) models and more recent approaches such as RNNs (recurrent\nneural networks) are indispensable tools in modeling time-series data. In many\nmultivariate time series modeling problems, there is usually a significant\nlinear dependency component, for which VARs are suitable, and a nonlinear\ncomponent, for which RNNs are suitable. Modeling such times series with only\nVAR or only RNNs can lead to poor predictive performance or complex models with\nlarge training times. In this work, we propose a hybrid model called R2N2\n(Residual RNN), which first models the time series with a simple linear model\n(like VAR) and then models its residual errors using RNNs. R2N2s can be trained\nusing existing algorithms for VARs and RNNs. Through an extensive empirical\nevaluation on two real world datasets (aviation and climate domains), we show\nthat R2N2 is competitive, usually better than VAR or RNN, used alone. We also\nshow that R2N2 is faster to train as compared to an RNN, while requiring less\nnumber of hidden units. \n\n"}
{"id": "1709.03871", "contents": "Title: Agnostic Learning by Refuting Abstract: The sample complexity of learning a Boolean-valued function class is\nprecisely characterized by its Rademacher complexity. This has little bearing,\nhowever, on the sample complexity of \\emph{efficient} agnostic learning.\n  We introduce \\emph{refutation complexity}, a natural computational analog of\nRademacher complexity of a Boolean concept class and show that it exactly\ncharacterizes the sample complexity of \\emph{efficient} agnostic learning.\nInformally, refutation complexity of a class $\\mathcal{C}$ is the minimum\nnumber of example-label pairs required to efficiently distinguish between the\ncase that the labels correlate with the evaluation of some member of\n$\\mathcal{C}$ (\\emph{structure}) and the case where the labels are i.i.d.\nRademacher random variables (\\emph{noise}). The easy direction of this\nrelationship was implicitly used in the recent framework for improper PAC\nlearning lower bounds of Daniely and co-authors via connections to the hardness\nof refuting random constraint satisfaction problems. Our work can be seen as\nmaking the relationship between agnostic learning and refutation implicit in\ntheir work into an explicit equivalence. In a recent, independent work, Salil\nVadhan discovered a similar relationship between refutation and PAC-learning in\nthe realizable (i.e. noiseless) case. \n\n"}
{"id": "1709.05254", "contents": "Title: Detection of Anomalies in Large Scale Accounting Data using Deep\n  Autoencoder Networks Abstract: Learning to detect fraud in large-scale accounting data is one of the\nlong-standing challenges in financial statement audits or fraud investigations.\nNowadays, the majority of applied techniques refer to handcrafted rules derived\nfrom known fraud scenarios. While fairly successful, these rules exhibit the\ndrawback that they often fail to generalize beyond known fraud scenarios and\nfraudsters gradually find ways to circumvent them. To overcome this\ndisadvantage and inspired by the recent success of deep learning we propose the\napplication of deep autoencoder neural networks to detect anomalous journal\nentries. We demonstrate that the trained network's reconstruction error\nobtainable for a journal entry and regularized by the entry's individual\nattribute probabilities can be interpreted as a highly adaptive anomaly\nassessment. Experiments on two real-world datasets of journal entries, show the\neffectiveness of the approach resulting in high f1-scores of 32.93 (dataset A)\nand 16.95 (dataset B) and less false positive alerts compared to state of the\nart baseline methods. Initial feedback received by chartered accountants and\nfraud examiners underpinned the quality of the approach in capturing highly\nrelevant accounting anomalies. \n\n"}
{"id": "1709.05914", "contents": "Title: Limitations of Cross-Lingual Learning from Image Search Abstract: Cross-lingual representation learning is an important step in making NLP\nscale to all the world's languages. Recent work on bilingual lexicon induction\nsuggests that it is possible to learn cross-lingual representations of words\nbased on similarities between images associated with these words. However, that\nwork focused on the translation of selected nouns only. In our work, we\ninvestigate whether the meaning of other parts-of-speech, in particular\nadjectives and verbs, can be learned in the same way. We also experiment with\ncombining the representations learned from visual data with embeddings learned\nfrom textual data. Our experiments across five language pairs indicate that\nprevious work does not scale to the problem of learning cross-lingual\nrepresentations beyond simple nouns. \n\n"}
{"id": "1709.06080", "contents": "Title: Feedforward and Recurrent Neural Networks Backward Propagation and\n  Hessian in Matrix Form Abstract: In this paper we focus on the linear algebra theory behind feedforward (FNN)\nand recurrent (RNN) neural networks. We review backward propagation, including\nbackward propagation through time (BPTT). Also, we obtain a new exact\nexpression for Hessian, which represents second order effects. We show that for\n$t$ time steps the weight gradient can be expressed as a rank-$t$ matrix, while\nthe weight Hessian is as a sum of $t^{2}$ Kronecker products of rank-$1$ and\n$W^{T}AW$ matrices, for some matrix $A$ and weight matrix $W$. Also, we show\nthat for a mini-batch of size $r$, the weight update can be expressed as a\nrank-$rt$ matrix. Finally, we briefly comment on the eigenvalues of the Hessian\nmatrix. \n\n"}
{"id": "1709.06201", "contents": "Title: Human Understandable Explanation Extraction for Black-box Classification\n  Models Based on Matrix Factorization Abstract: In recent years, a number of artificial intelligent services have been\ndeveloped such as defect detection system or diagnosis system for customer\nservices. Unfortunately, the core in these services is a black-box in which\nhuman cannot understand the underlying decision making logic, even though the\ninspection of the logic is crucial before launching a commercial service. Our\ngoal in this paper is to propose an analytic method of a model explanation that\nis applicable to general classification models. To this end, we introduce the\nconcept of a contribution matrix and an explanation embedding in a constraint\nspace by using a matrix factorization. We extract a rule-like model explanation\nfrom the contribution matrix with the help of the nonnegative matrix\nfactorization. To validate our method, the experiment results provide with open\ndatasets as well as an industry dataset of a LTE network diagnosis and the\nresults show our method extracts reasonable explanations. \n\n"}
{"id": "1709.06404", "contents": "Title: Interactive Music Generation with Positional Constraints using\n  Anticipation-RNNs Abstract: Recurrent Neural Networks (RNNS) are now widely used on sequence generation\ntasks due to their ability to learn long-range dependencies and to generate\nsequences of arbitrary length. However, their left-to-right generation\nprocedure only allows a limited control from a potential user which makes them\nunsuitable for interactive and creative usages such as interactive music\ngeneration. This paper introduces a novel architecture called Anticipation-RNN\nwhich possesses the assets of the RNN-based generative models while allowing to\nenforce user-defined positional constraints. We demonstrate its efficiency on\nthe task of generating melodies satisfying positional constraints in the style\nof the soprano parts of the J.S. Bach chorale harmonizations. Sampling using\nthe Anticipation-RNN is of the same order of complexity than sampling from the\ntraditional RNN model. This fast and interactive generation of musical\nsequences opens ways to devise real-time systems that could be used for\ncreative purposes. \n\n"}
{"id": "1709.06429", "contents": "Title: Neural Networks for Text Correction and Completion in Keyboard Decoding Abstract: Despite the ubiquity of mobile and wearable text messaging applications, the\nproblem of keyboard text decoding is not tackled sufficiently in the light of\nthe enormous success of the deep learning Recurrent Neural Network (RNN) and\nConvolutional Neural Networks (CNN) for natural language understanding. In\nparticular, considering that the keyboard decoders should operate on devices\nwith memory and processor resource constraints, makes it challenging to deploy\nindustrial scale deep neural network (DNN) models. This paper proposes a\nsequence-to-sequence neural attention network system for automatic text\ncorrection and completion. Given an erroneous sequence, our model encodes\ncharacter level hidden representations and then decodes the revised sequence\nthus enabling auto-correction and completion. We achieve this by a combination\nof character level CNN and gated recurrent unit (GRU) encoder along with and a\nword level gated recurrent unit (GRU) attention decoder. Unlike traditional\nlanguage models that learn from billions of words, our corpus size is only 12\nmillion words; an order of magnitude smaller. The memory footprint of our\nlearnt model for inference and prediction is also an order of magnitude smaller\nthan the conventional language model based text decoders. We report baseline\nperformance for neural keyboard decoders in such limited domain. Our models\nachieve a word level accuracy of $90\\%$ and a character error rate CER of\n$2.4\\%$ over the Twitter typo dataset. We present a novel dataset of noisy to\ncorrected mappings by inducing the noise distribution from the Twitter data\nover the OpenSubtitles 2009 dataset; on which our model predicts with a word\nlevel accuracy of $98\\%$ and sequence accuracy of $68.9\\%$. In our user study,\nour model achieved an average CER of $2.6\\%$ with the state-of-the-art\nnon-neural touch-screen keyboard decoder at CER of $1.6\\%$. \n\n"}
{"id": "1709.08055", "contents": "Title: Feature-based time-series analysis Abstract: This work presents an introduction to feature-based time-series analysis. The\ntime series as a data type is first described, along with an overview of the\ninterdisciplinary time-series analysis literature. I then summarize the range\nof feature-based representations for time series that have been developed to\naid interpretable insights into time-series structure. Particular emphasis is\ngiven to emerging research that facilitates wide comparison of feature-based\nrepresentations that allow us to understand the properties of a time-series\ndataset that make it suited to a particular feature-based representation or\nanalysis algorithm. The future of time-series analysis is likely to embrace\napproaches that exploit machine learning methods to partially automate human\nlearning to aid understanding of the complex dynamical patterns in the time\nseries we measure from the world. \n\n"}
{"id": "1709.10486", "contents": "Title: Symbol, Conversational, and Societal Grounding with a Toy Robot Abstract: Essential to meaningful interaction is grounding at the symbolic,\nconversational, and societal levels. We present ongoing work with Anki's Cozmo\ntoy robot as a research platform where we leverage the recent\nwords-as-classifiers model of lexical semantics in interactive reference\nresolution tasks for language grounding. \n\n"}
{"id": "1710.02277", "contents": "Title: Efficient K-Shot Learning with Regularized Deep Networks Abstract: Feature representations from pre-trained deep neural networks have been known\nto exhibit excellent generalization and utility across a variety of related\ntasks. Fine-tuning is by far the simplest and most widely used approach that\nseeks to exploit and adapt these feature representations to novel tasks with\nlimited data. Despite the effectiveness of fine-tuning, itis often sub-optimal\nand requires very careful optimization to prevent severe over-fitting to small\ndatasets. The problem of sub-optimality and over-fitting, is due in part to the\nlarge number of parameters used in a typical deep convolutional neural network.\nTo address these problems, we propose a simple yet effective regularization\nmethod for fine-tuning pre-trained deep networks for the task of k-shot\nlearning. To prevent overfitting, our key strategy is to cluster the model\nparameters while ensuring intra-cluster similarity and inter-cluster diversity\nof the parameters, effectively regularizing the dimensionality of the parameter\nsearch space. In particular, we identify groups of neurons within each layer of\na deep network that shares similar activation patterns. When the network is to\nbe fine-tuned for a classification task using only k examples, we propagate a\nsingle gradient to all of the neuron parameters that belong to the same group.\nThe grouping of neurons is non-trivial as neuron activations depend on the\ndistribution of the input data. To efficiently search for optimal groupings\nconditioned on the input data, we propose a reinforcement learning search\nstrategy using recurrent networks to learn the optimal group assignments for\neach network layer. Experimental results show that our method can be easily\napplied to several popular convolutional neural networks and improve upon other\nstate-of-the-art fine-tuning based k-shot learning strategies by more than10% \n\n"}
{"id": "1710.02437", "contents": "Title: Learning Word Embeddings for Hyponymy with Entailment-Based\n  Distributional Semantics Abstract: Lexical entailment, such as hyponymy, is a fundamental issue in the semantics\nof natural language. This paper proposes distributional semantic models which\nefficiently learn word embeddings for entailment, using a recently-proposed\nframework for modelling entailment in a vector-space. These models postulate a\nlatent vector for a pseudo-phrase containing two neighbouring word vectors. We\ninvestigate both modelling words as the evidence they contribute about this\nphrase vector, or as the posterior distribution of a one-word phrase vector,\nand find that the posterior vectors perform better. The resulting word\nembeddings outperform the best previous results on predicting hyponymy between\nwords, in unsupervised and semi-supervised experiments. \n\n"}
{"id": "1710.03641", "contents": "Title: Continuous Adaptation via Meta-Learning in Nonstationary and Competitive\n  Environments Abstract: Ability to continuously learn and adapt from limited experience in\nnonstationary environments is an important milestone on the path towards\ngeneral intelligence. In this paper, we cast the problem of continuous\nadaptation into the learning-to-learn framework. We develop a simple\ngradient-based meta-learning algorithm suitable for adaptation in dynamically\nchanging and adversarial scenarios. Additionally, we design a new multi-agent\ncompetitive environment, RoboSumo, and define iterated adaptation games for\ntesting various aspects of continuous adaptation strategies. We demonstrate\nthat meta-learning enables significantly more efficient adaptation than\nreactive baselines in the few-shot regime. Our experiments with a population of\nagents that learn and compete suggest that meta-learners are the fittest. \n\n"}
{"id": "1710.03667", "contents": "Title: High-dimensional dynamics of generalization error in neural networks Abstract: We perform an average case analysis of the generalization dynamics of large\nneural networks trained using gradient descent. We study the\npractically-relevant \"high-dimensional\" regime where the number of free\nparameters in the network is on the order of or even larger than the number of\nexamples in the dataset. Using random matrix theory and exact solutions in\nlinear models, we derive the generalization error and training error dynamics\nof learning and analyze how they depend on the dimensionality of data and\nsignal to noise ratio of the learning problem. We find that the dynamics of\ngradient descent learning naturally protect against overtraining and\noverfitting in large networks. Overtraining is worst at intermediate network\nsizes, when the effective number of free parameters equals the number of\nsamples, and thus can be reduced by making a network smaller or larger.\nAdditionally, in the high-dimensional regime, low generalization error requires\nstarting with small initial weights. We then turn to non-linear neural\nnetworks, and show that making networks very large does not harm their\ngeneralization performance. On the contrary, it can in fact reduce\novertraining, even without early stopping or regularization of any sort. We\nidentify two novel phenomena underlying this behavior in overcomplete models:\nfirst, there is a frozen subspace of the weights in which no learning occurs\nunder gradient descent; and second, the statistical properties of the\nhigh-dimensional regime yield better-conditioned input correlations which\nprotect against overtraining. We demonstrate that naive application of\nworst-case theories such as Rademacher complexity are inaccurate in predicting\nthe generalization performance of deep neural networks, and derive an\nalternative bound which incorporates the frozen subspace and conditioning\neffects and qualitatively matches the behavior observed in simulation. \n\n"}
{"id": "1710.04344", "contents": "Title: Using Context Events in Neural Network Models for Event Temporal Status\n  Identification Abstract: Focusing on the task of identifying event temporal status, we find that\nevents directly or indirectly governing the target event in a dependency tree\nare most important contexts. Therefore, we extract dependency chains containing\ncontext events and use them as input in neural network models, which\nconsistently outperform previous models using local context words as input.\nVisualization verifies that the dependency chain representation can effectively\ncapture the context events which are closely related to the target event and\nplay key roles in predicting event temporal status. \n\n"}
{"id": "1710.06081", "contents": "Title: Boosting Adversarial Attacks with Momentum Abstract: Deep neural networks are vulnerable to adversarial examples, which poses\nsecurity concerns on these algorithms due to the potentially severe\nconsequences. Adversarial attacks serve as an important surrogate to evaluate\nthe robustness of deep learning models before they are deployed. However, most\nof existing adversarial attacks can only fool a black-box model with a low\nsuccess rate. To address this issue, we propose a broad class of momentum-based\niterative algorithms to boost adversarial attacks. By integrating the momentum\nterm into the iterative process for attacks, our methods can stabilize update\ndirections and escape from poor local maxima during the iterations, resulting\nin more transferable adversarial examples. To further improve the success rates\nfor black-box attacks, we apply momentum iterative algorithms to an ensemble of\nmodels, and show that the adversarially trained models with a strong defense\nability are also vulnerable to our black-box attacks. We hope that the proposed\nmethods will serve as a benchmark for evaluating the robustness of various deep\nmodels and defense methods. With this method, we won the first places in NIPS\n2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack\ncompetitions. \n\n"}
{"id": "1710.06273", "contents": "Title: Combinatorial Penalties: Which structures are preserved by convex\n  relaxations? Abstract: We consider the homogeneous and the non-homogeneous convex relaxations for\ncombinatorial penalty functions defined on support sets. Our study identifies\nkey differences in the tightness of the resulting relaxations through the\nnotion of the lower combinatorial envelope of a set-function along with new\nnecessary conditions for support identification. We then propose a general\nadaptive estimator for convex monotone regularizers, and derive new sufficient\nconditions for support recovery in the asymptotic setting. \n\n"}
{"id": "1710.06393", "contents": "Title: RETUYT in TASS 2017: Sentiment Analysis for Spanish Tweets using SVM and\n  CNN Abstract: This article presents classifiers based on SVM and Convolutional Neural\nNetworks (CNN) for the TASS 2017 challenge on tweets sentiment analysis. The\nclassifier with the best performance in general uses a combination of SVM and\nCNN. The use of word embeddings was particularly useful for improving the\nclassifiers performance. \n\n"}
{"id": "1710.06832", "contents": "Title: The Origins of Computational Mechanics: A Brief Intellectual History and\n  Several Clarifications Abstract: The principle goal of computational mechanics is to define pattern and\nstructure so that the organization of complex systems can be detected and\nquantified. Computational mechanics developed from efforts in the 1970s and\nearly 1980s to identify strange attractors as the mechanism driving weak fluid\nturbulence via the method of reconstructing attractor geometry from measurement\ntime series and in the mid-1980s to estimate equations of motion directly from\ncomplex time series. In providing a mathematical and operational definition of\nstructure it addressed weaknesses of these early approaches to discovering\npatterns in natural systems.\n  Since then, computational mechanics has led to a range of results from\ntheoretical physics and nonlinear mathematics to diverse applications---from\nclosed-form analysis of Markov and non-Markov stochastic processes that are\nergodic or nonergodic and their measures of information and intrinsic\ncomputation to complex materials and deterministic chaos and intelligence in\nMaxwellian demons to quantum compression of classical processes and the\nevolution of computation and language.\n  This brief review clarifies several misunderstandings and addresses concerns\nrecently raised regarding early works in the field (1980s). We show that\nmisguided evaluations of the contributions of computational mechanics are\ngroundless and stem from a lack of familiarity with its basic goals and from a\nfailure to consider its historical context. For all practical purposes, its\nmodern methods and results largely supersede the early works. This not only\nrenders recent criticism moot and shows the solid ground on which computational\nmechanics stands but, most importantly, shows the significant progress achieved\nover three decades and points to the many intriguing and outstanding challenges\nin understanding the computational nature of complex dynamic systems. \n\n"}
{"id": "1710.06940", "contents": "Title: Concept Drift Learning with Alternating Learners Abstract: Data-driven predictive analytics are in use today across a number of\nindustrial applications, but further integration is hindered by the requirement\nof similarity among model training and test data distributions. This paper\naddresses the need of learning from possibly nonstationary data streams, or\nunder concept drift, a commonly seen phenomenon in practical applications. A\nsimple dual-learner ensemble strategy, alternating learners framework, is\nproposed. A long-memory model learns stable concepts from a long relevant time\nwindow, while a short-memory model learns transient concepts from a small\nrecent window. The difference in prediction performance of these two models is\nmonitored and induces an alternating policy to select, update and reset the two\nmodels. The method features an online updating mechanism to maintain the\nensemble accuracy, and a concept-dependent trigger to focus on relevant data.\nThrough empirical studies the method demonstrates effective tracking and\nprediction when the steaming data carry abrupt and/or gradual changes. \n\n"}
{"id": "1710.07328", "contents": "Title: Online Monotone Games Abstract: Algorithmic game theory (AGT) focuses on the design and analysis of\nalgorithms for interacting agents, with interactions rigorously formalized\nwithin the framework of games. Results from AGT find applications in domains\nsuch as online bidding auctions for web advertisements and network routing\nprotocols. Monotone games are games where agent strategies naturally converge\nto an equilibrium state. Previous results in AGT have been obtained for convex,\nsocially-convex, or smooth games, but not monotone games. Our primary\ntheoretical contributions are defining the monotone game setting and its\nextension to the online setting, a new notion of regret for this setting, and\naccompanying algorithms that achieve sub-linear regret. We demonstrate the\nutility of online monotone game theory on a variety of problem domains\nincluding variational inequalities, reinforcement learning, and generative\nadversarial networks. \n\n"}
{"id": "1710.08893", "contents": "Title: Fast Model Identification via Physics Engines for Data-Efficient Policy\n  Search Abstract: This paper presents a method for identifying mechanical parameters of robots\nor objects, such as their mass and friction coefficients. Key features are the\nuse of off-the-shelf physics engines and the adaptation of a Bayesian\noptimization technique towards minimizing the number of real-world experiments\nneeded for model-based reinforcement learning. The proposed framework\nreproduces in a physics engine experiments performed on a real robot and\noptimizes the model's mechanical parameters so as to match real-world\ntrajectories. The optimized model is then used for learning a policy in\nsimulation, before real-world deployment. It is well understood, however, that\nit is hard to exactly reproduce real trajectories in simulation. Moreover, a\nnear-optimal policy can be frequently found with an imperfect model. Therefore,\nthis work proposes a strategy for identifying a model that is just good enough\nto approximate the value of a locally optimal policy with a certain confidence,\ninstead of wasting effort on identifying the most accurate model. Evaluations,\nperformed both in simulation and on a real robotic manipulation task, indicate\nthat the proposed strategy results in an overall time-efficient, integrated\nmodel identification and learning solution, which significantly improves the\ndata-efficiency of existing policy search algorithms. \n\n"}
{"id": "1710.09553", "contents": "Title: Rethinking generalization requires revisiting old ideas: statistical\n  mechanics approaches and complex learning behavior Abstract: We describe an approach to understand the peculiar and counterintuitive\ngeneralization properties of deep neural networks. The approach involves going\nbeyond worst-case theoretical capacity control frameworks that have been\npopular in machine learning in recent years to revisit old ideas in the\nstatistical mechanics of neural networks. Within this approach, we present a\nprototypical Very Simple Deep Learning (VSDL) model, whose behavior is\ncontrolled by two control parameters, one describing an effective amount of\ndata, or load, on the network (that decreases when noise is added to the\ninput), and one with an effective temperature interpretation (that increases\nwhen algorithms are early stopped). Using this model, we describe how a very\nsimple application of ideas from the statistical mechanics theory of\ngeneralization provides a strong qualitative description of recently-observed\nempirical results regarding the inability of deep neural networks not to\noverfit training data, discontinuous learning and sharp transitions in the\ngeneralization properties of learning algorithms, etc. \n\n"}
{"id": "1710.09813", "contents": "Title: Sparse Diffusion-Convolutional Neural Networks Abstract: The predictive power and overall computational efficiency of\nDiffusion-convolutional neural networks make them an attractive choice for node\nclassification tasks. However, a naive dense-tensor-based implementation of\nDCNNs leads to $\\mathcal{O}(N^2)$ memory complexity which is prohibitive for\nlarge graphs. In this paper, we introduce a simple method for thresholding\ninput graphs that provably reduces memory requirements of DCNNs to O(N) (i.e.\nlinear in the number of nodes in the input) without significantly affecting\npredictive performance. \n\n"}
{"id": "1710.10328", "contents": "Title: Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU\n  with Generalized Hamming Network Abstract: We revisit fuzzy neural network with a cornerstone notion of generalized\nhamming distance, which provides a novel and theoretically justified framework\nto re-interpret many useful neural network techniques in terms of fuzzy logic.\nIn particular, we conjecture and empirically illustrate that, the celebrated\nbatch normalization (BN) technique actually adapts the normalized bias such\nthat it approximates the rightful bias induced by the generalized hamming\ndistance. Once the due bias is enforced analytically, neither the optimization\nof bias terms nor the sophisticated batch normalization is needed. Also in the\nlight of generalized hamming distance, the popular rectified linear units\n(ReLU) can be treated as setting a minimal hamming distance threshold between\nnetwork inputs and weights. This thresholding scheme, on the one hand, can be\nimproved by introducing double thresholding on both extremes of neuron outputs.\nOn the other hand, ReLUs turn out to be non-essential and can be removed from\nnetworks trained for simple tasks like MNIST classification. The proposed\ngeneralized hamming network (GHN) as such not only lends itself to rigorous\nanalysis and interpretation within the fuzzy logic theory but also demonstrates\nfast learning speed, well-controlled behaviour and state-of-the-art\nperformances on a variety of learning tasks. \n\n"}
{"id": "1710.10370", "contents": "Title: Topology Adaptive Graph Convolutional Networks Abstract: Spectral graph convolutional neural networks (CNNs) require approximation to\nthe convolution to alleviate the computational complexity, resulting in\nperformance loss. This paper proposes the topology adaptive graph convolutional\nnetwork (TAGCN), a novel graph convolutional network defined in the vertex\ndomain. We provide a systematic way to design a set of fixed-size learnable\nfilters to perform convolutions on graphs. The topologies of these filters are\nadaptive to the topology of the graph when they scan the graph to perform\nconvolution. The TAGCN not only inherits the properties of convolutions in CNN\nfor grid-structured data, but it is also consistent with convolution as defined\nin graph signal processing. Since no approximation to the convolution is\nneeded, TAGCN exhibits better performance than existing spectral CNNs on a\nnumber of data sets and is also computationally simpler than other recent\nmethods. \n\n"}
{"id": "1710.10733", "contents": "Title: Attacking the Madry Defense Model with $L_1$-based Adversarial Examples Abstract: The Madry Lab recently hosted a competition designed to test the robustness\nof their adversarially trained MNIST model. Attacks were constrained to perturb\neach pixel of the input image by a scaled maximal $L_\\infty$ distortion\n$\\epsilon$ = 0.3. This discourages the use of attacks which are not optimized\non the $L_\\infty$ distortion metric. Our experimental results demonstrate that\nby relaxing the $L_\\infty$ constraint of the competition, the elastic-net\nattack to deep neural networks (EAD) can generate transferable adversarial\nexamples which, despite their high average $L_\\infty$ distortion, have minimal\nvisual distortion. These results call into question the use of $L_\\infty$ as a\nsole measure for visual distortion, and further demonstrate the power of EAD at\ngenerating robust adversarial examples. \n\n"}
{"id": "1710.10784", "contents": "Title: How deep learning works --The geometry of deep learning Abstract: Why and how that deep learning works well on different tasks remains a\nmystery from a theoretical perspective. In this paper we draw a geometric\npicture of the deep learning system by finding its analogies with two existing\ngeometric structures, the geometry of quantum computations and the geometry of\nthe diffeomorphic template matching. In this framework, we give the geometric\nstructures of different deep learning systems including convolutional neural\nnetworks, residual networks, recursive neural networks, recurrent neural\nnetworks and the equilibrium prapagation framework. We can also analysis the\nrelationship between the geometrical structures and their performance of\ndifferent networks in an algorithmic level so that the geometric framework may\nguide the design of the structures and algorithms of deep learning systems. \n\n"}
{"id": "1710.11004", "contents": "Title: Denoising random forests Abstract: This paper proposes a novel type of random forests called a denoising random\nforests that are robust against noises contained in test samples. Such\nnoise-corrupted samples cause serious damage to the estimation performances of\nrandom forests, since unexpected child nodes are often selected and the leaf\nnodes that the input sample reaches are sometimes far from those for a clean\nsample. Our main idea for tackling this problem originates from a binary\nindicator vector that encodes a traversal path of a sample in the forest. Our\nproposed method effectively employs this vector by introducing denoising\nautoencoders into random forests. A denoising autoencoder can be trained with\nindicator vectors produced from clean and noisy input samples, and non-leaf\nnodes where incorrect decisions are made can be identified by comparing the\ninput and output of the trained denoising autoencoder. Multiple traversal paths\nwith respect to the nodes with incorrect decisions caused by the noises can\nthen be considered for the estimation. \n\n"}
{"id": "1710.11248", "contents": "Title: Learning Robust Rewards with Adversarial Inverse Reinforcement Learning Abstract: Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the\nneed for extensive feature and reward engineering. Deep reinforcement learning\nmethods can remove the need for explicit engineering of policy or value\nfeatures, but still require a manually specified reward function. Inverse\nreinforcement learning holds the promise of automatic reward acquisition, but\nhas proven exceptionally difficult to apply to large, high-dimensional problems\nwith unknown dynamics. In this work, we propose adverserial inverse\nreinforcement learning (AIRL), a practical and scalable inverse reinforcement\nlearning algorithm based on an adversarial reward learning formulation. We\ndemonstrate that AIRL is able to recover reward functions that are robust to\nchanges in dynamics, enabling us to learn policies even under significant\nvariation in the environment seen during training. Our experiments show that\nAIRL greatly outperforms prior methods in these transfer settings. \n\n"}
{"id": "1711.00073", "contents": "Title: Long-term Forecasting using Higher Order Tensor RNNs Abstract: We present Higher-Order Tensor RNN (HOT-RNN), a novel family of neural\nsequence architectures for multivariate forecasting in environments with\nnonlinear dynamics. Long-term forecasting in such systems is highly\nchallenging, since there exist long-term temporal dependencies, higher-order\ncorrelations and sensitivity to error propagation. Our proposed recurrent\narchitecture addresses these issues by learning the nonlinear dynamics directly\nusing higher-order moments and higher-order state transition functions.\nFurthermore, we decompose the higher-order structure using the tensor-train\ndecomposition to reduce the number of parameters while preserving the model\nperformance. We theoretically establish the approximation guarantees and the\nvariance bound for HOT-RNN for general sequence inputs. We also demonstrate 5%\n~ 12% improvements for long-term prediction over general RNN and LSTM\narchitectures on a range of simulated environments with nonlinear dynamics, as\nwell on real-world time series data. \n\n"}
{"id": "1711.00244", "contents": "Title: Efficient Inferencing of Compressed Deep Neural Networks Abstract: Large number of weights in deep neural networks makes the models difficult to\nbe deployed in low memory environments such as, mobile phones, IOT edge devices\nas well as \"inferencing as a service\" environments on cloud. Prior work has\nconsidered reduction in the size of the models, through compression techniques\nlike pruning, quantization, Huffman encoding etc. However, efficient\ninferencing using the compressed models has received little attention,\nspecially with the Huffman encoding in place. In this paper, we propose\nefficient parallel algorithms for inferencing of single image and batches,\nunder various memory constraints. Our experimental results show that our\napproach of using variable batch size for inferencing achieves 15-25\\%\nperformance improvement in the inference throughput for AlexNet, while\nmaintaining memory and latency constraints. \n\n"}
{"id": "1711.00489", "contents": "Title: Don't Decay the Learning Rate, Increase the Batch Size Abstract: It is common practice to decay the learning rate. Here we show one can\nusually obtain the same learning curve on both training and test sets by\ninstead increasing the batch size during training. This procedure is successful\nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,\nand Adam. It reaches equivalent test accuracies after the same number of\ntraining epochs, but with fewer parameter updates, leading to greater\nparallelism and shorter training times. We can further reduce the number of\nparameter updates by increasing the learning rate $\\epsilon$ and scaling the\nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum\ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly\nreduce the test accuracy. Crucially, our techniques allow us to repurpose\nexisting training schedules for large batch training with no hyper-parameter\ntuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under\n30 minutes. \n\n"}
{"id": "1711.00768", "contents": "Title: SRL4ORL: Improving Opinion Role Labeling using Multi-task Learning with\n  Semantic Role Labeling Abstract: For over a decade, machine learning has been used to extract\nopinion-holder-target structures from text to answer the question \"Who\nexpressed what kind of sentiment towards what?\". Recent neural approaches do\nnot outperform the state-of-the-art feature-based models for Opinion Role\nLabeling (ORL). We suspect this is due to the scarcity of labeled training data\nand address this issue using different multi-task learning (MTL) techniques\nwith a related task which has substantially more data, i.e. Semantic Role\nLabeling (SRL). We show that two MTL models improve significantly over the\nsingle-task model for labeling of both holders and targets, on the development\nand the test sets. We found that the vanilla MTL model which makes predictions\nusing only shared ORL and SRL features, performs the best. With deeper analysis\nwe determine what works and what might be done to make further improvements for\nORL. \n\n"}
{"id": "1711.00991", "contents": "Title: The neighborhood lattice for encoding partial correlations in a Hilbert\n  space Abstract: Neighborhood regression has been a successful approach in graphical and\nstructural equation modeling, with applications to learning undirected and\ndirected graphical models. We extend these ideas by defining and studying an\nalgebraic structure called the neighborhood lattice based on a generalized\nnotion of neighborhood regression. We show that this algebraic structure has\nthe potential to provide an economic encoding of all conditional independence\nstatements in a Gaussian distribution (or conditional uncorrelatedness in\ngeneral), even in the cases where no graphical model exists that could\n\"perfectly\" encode all such statements. We study the computational complexity\nof computing these structures and show that under a sparsity assumption, they\ncan be computed in polynomial time, even in the absence of the assumption of\nperfectness to a graph. On the other hand, assuming perfectness, we show how\nthese neighborhood lattices may be \"graphically\" computed using the separation\nproperties of the so-called partial correlation graph. We also draw connections\nwith directed acyclic graphical models and Bayesian networks. We derive these\nresults using an abstract generalization of partial uncorrelatedness, called\npartial orthogonality, which allows us to use algebraic properties of\nprojection operators on Hilbert spaces to significantly simplify and extend\nexisting ideas and arguments. Consequently, our results apply to a wide range\nof random objects and data structures, such as random vectors, data matrices,\nand functions. \n\n"}
{"id": "1711.01416", "contents": "Title: Language as a matrix product state Abstract: We propose a statistical model for natural language that begins by\nconsidering language as a monoid, then representing it in complex matrices with\na compatible translation invariant probability measure. We interpret the\nprobability measure as arising via the Born rule from a translation invariant\nmatrix product state. \n\n"}
{"id": "1711.01567", "contents": "Title: Robust Speech Recognition Using Generative Adversarial Networks Abstract: This paper describes a general, scalable, end-to-end framework that uses the\ngenerative adversarial network (GAN) objective to enable robust speech\nrecognition. Encoders trained with the proposed approach enjoy improved\ninvariance by learning to map noisy audio to the same embedding space as that\nof clean audio. Unlike previous methods, the new framework does not rely on\ndomain expertise or simplifying assumptions as are often needed in signal\nprocessing, and directly encourages robustness in a data-driven way. We show\nthe new approach improves simulated far-field speech recognition of vanilla\nsequence-to-sequence models without specialized front-ends or preprocessing. \n\n"}
{"id": "1711.01596", "contents": "Title: Is Input Sparsity Time Possible for Kernel Low-Rank Approximation? Abstract: Low-rank approximation is a common tool used to accelerate kernel methods:\nthe $n \\times n$ kernel matrix $K$ is approximated via a rank-$k$ matrix\n$\\tilde K$ which can be stored in much less space and processed more quickly.\nIn this work we study the limits of computationally efficient low-rank kernel\napproximation. We show that for a broad class of kernels, including the popular\nGaussian and polynomial kernels, computing a relative error $k$-rank\napproximation to $K$ is at least as difficult as multiplying the input data\nmatrix $A \\in \\mathbb{R}^{n \\times d}$ by an arbitrary matrix $C \\in\n\\mathbb{R}^{d \\times k}$. Barring a breakthrough in fast matrix multiplication,\nwhen $k$ is not too large, this requires $\\Omega(nnz(A)k)$ time where $nnz(A)$\nis the number of non-zeros in $A$. This lower bound matches, in many parameter\nregimes, recent work on subquadratic time algorithms for low-rank approximation\nof general kernels [MM16,MW17], demonstrating that these algorithms are\nunlikely to be significantly improved, in particular to $O(nnz(A))$ input\nsparsity runtimes. At the same time there is hope: we show for the first time\nthat $O(nnz(A))$ time approximation is possible for general radial basis\nfunction kernels (e.g., the Gaussian kernel) for the closely related problem of\nlow-rank approximation of the kernelized dataset. \n\n"}
{"id": "1711.01921", "contents": "Title: $A^{4}NT$: Author Attribute Anonymity by Adversarial Training of Neural\n  Machine Translation Abstract: Text-based analysis methods allow to reveal privacy relevant author\nattributes such as gender, age and identify of the text's author. Such methods\ncan compromise the privacy of an anonymous author even when the author tries to\nremove privacy sensitive content. In this paper, we propose an automatic\nmethod, called Adversarial Author Attribute Anonymity Neural Translation\n($A^4NT$), to combat such text-based adversaries. We combine\nsequence-to-sequence language models used in machine translation and generative\nadversarial networks to obfuscate author attributes. Unlike machine translation\ntechniques which need paired data, our method can be trained on unpaired\ncorpora of text containing different authors. Importantly, we propose and\nevaluate techniques to impose constraints on our $A^4NT$ to preserve the\nsemantics of the input text. $A^4NT$ learns to make minimal changes to the\ninput text to successfully fool author attribute classifiers, while aiming to\nmaintain the meaning of the input. We show through experiments on two different\ndatasets and three settings that our proposed method is effective in fooling\nthe author attribute classifiers and thereby improving the anonymity of\nauthors. \n\n"}
{"id": "1711.02088", "contents": "Title: Computer activity learning from system call time series Abstract: Using a previously introduced similarity function for the stream of system\ncalls generated by a computer, we engineer a program-in-execution classifier\nusing deep learning methods. Tested on malware classification, it significantly\noutperforms current state of the art. We provide a series of performance\nmeasures and tests to demonstrate the capabilities, including measurements from\nproduction use. We show how the system scales linearly with the number of\nendpoints. With the system we estimate the total number of malware families\ncreated over the last 10 years as 3450, in line with reasonable economic\nconstraints. The more limited rate for new malware families than previously\nacknowledged implies that machine learning malware classifiers risk being\ntested on their training set; we achieve F1 = 0.995 in a test carefully\ndesigned to mitigate this risk. \n\n"}
{"id": "1711.02114", "contents": "Title: Bounding and Counting Linear Regions of Deep Neural Networks Abstract: We investigate the complexity of deep neural networks (DNN) that represent\npiecewise linear (PWL) functions. In particular, we study the number of linear\nregions, i.e. pieces, that a PWL function represented by a DNN can attain, both\ntheoretically and empirically. We present (i) tighter upper and lower bounds\nfor the maximum number of linear regions on rectifier networks, which are exact\nfor inputs of dimension one; (ii) a first upper bound for multi-layer maxout\nnetworks; and (iii) a first method to perform exact enumeration or counting of\nthe number of regions by modeling the DNN with a mixed-integer linear\nformulation. These bounds come from leveraging the dimension of the space\ndefining each linear region. The results also indicate that a deep rectifier\nnetwork can only have more linear regions than every shallow counterpart with\nsame number of neurons if that number exceeds the dimension of the input. \n\n"}
{"id": "1711.02132", "contents": "Title: Weighted Transformer Network for Machine Translation Abstract: State-of-the-art results on neural machine translation often use attentional\nsequence-to-sequence models with some form of convolution or recursion. Vaswani\net al. (2017) propose a new architecture that avoids recurrence and convolution\ncompletely. Instead, it uses only self-attention and feed-forward layers. While\nthe proposed architecture achieves state-of-the-art results on several machine\ntranslation tasks, it requires a large number of parameters and training\niterations to converge. We propose Weighted Transformer, a Transformer with\nmodified attention layers, that not only outperforms the baseline network in\nBLEU score but also converges 15-40% faster. Specifically, we replace the\nmulti-head attention by multiple self-attention branches that the model learns\nto combine during the training process. Our model improves the state-of-the-art\nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation\ntask and by 0.4 on the English-to-French translation task. \n\n"}
{"id": "1711.02295", "contents": "Title: Quality-Efficiency Trade-offs in Machine Learning for Text Processing Abstract: Data mining, machine learning, and natural language processing are powerful\ntechniques that can be used together to extract information from large texts.\nDepending on the task or problem at hand, there are many different approaches\nthat can be used. The methods available are continuously being optimized, but\nnot all these methods have been tested and compared in a set of problems that\ncan be solved using supervised machine learning algorithms. The question is\nwhat happens to the quality of the methods if we increase the training data\nsize from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when\nthe rate of data processing diminishes? Can we trade quality for time\nefficiency and recover the quality loss by just being able to process more\ndata? We attempt to answer these questions in a general way for text processing\ntasks, considering the trade-offs involving training data size, learning time,\nand quality obtained. We propose a performance trade-off framework and apply it\nto three important text processing problems: Named Entity Recognition,\nSentiment Analysis and Document Classification. These problems were also chosen\nbecause they have different levels of object granularity: words, paragraphs,\nand documents. For each problem, we selected several supervised machine\nlearning algorithms and we evaluated the trade-offs of them on large publicly\navailable data sets (news, reviews, patents). To explore these trade-offs, we\nuse different data subsets of increasing size ranging from 50 MB to several GB.\nWe also consider the impact of the data set and the evaluation technique. We\nfind that the results do not change significantly and that most of the time the\nbest algorithms is the fastest. However, we also show that the results for\nsmall data (say less than 100 MB) are different from the results for big data\nand in those cases the best algorithm is much harder to determine. \n\n"}
{"id": "1711.02487", "contents": "Title: Deep density networks and uncertainty in recommender systems Abstract: Building robust online content recommendation systems requires learning\ncomplex interactions between user preferences and content features. The field\nhas evolved rapidly in recent years from traditional multi-arm bandit and\ncollaborative filtering techniques, with new methods employing Deep Learning\nmodels to capture non-linearities. Despite progress, the dynamic nature of\nonline recommendations still poses great challenges, such as finding the\ndelicate balance between exploration and exploitation. In this paper we show\nhow uncertainty estimations can be incorporated by employing them in an\noptimistic exploitation/exploration strategy for more efficient exploration of\nnew recommendations. We provide a novel hybrid deep neural network model, Deep\nDensity Networks (DDN), which integrates content-based deep learning models\nwith a collaborative scheme that is able to robustly model and estimate\nuncertainty. Finally, we present online and offline results after incorporating\nDNN into a real world content recommendation system that serves billions of\nrecommendations per day, and show the benefit of using DDN in practice. \n\n"}
{"id": "1711.02651", "contents": "Title: Theoretical limitations of Encoder-Decoder GAN architectures Abstract: Encoder-decoder GANs architectures (e.g., BiGAN and ALI) seek to add an\ninference mechanism to the GANs setup, consisting of a small encoder deep net\nthat maps data-points to their succinct encodings. The intuition is that being\nforced to train an encoder alongside the usual generator forces the system to\nlearn meaningful mappings from the code to the data-point and vice-versa, which\nshould improve the learning of the target distribution and ameliorate\nmode-collapse. It should also yield meaningful codes that are useful as\nfeatures for downstream tasks. The current paper shows rigorously that even on\nreal-life distributions of images, the encode-decoder GAN training objectives\n(a) cannot prevent mode collapse; i.e. the objective can be near-optimal even\nwhen the generated distribution has low and finite support (b) cannot prevent\nlearning meaningless codes for data -- essentially white noise. Thus if\nencoder-decoder GANs do indeed work then it must be due to reasons as yet not\nunderstood, since the training objective can be low even for meaningless\nsolutions. \n\n"}
{"id": "1711.02653", "contents": "Title: Neural system identification for large populations separating \"what\" and\n  \"where\" Abstract: Neuroscientists classify neurons into different types that perform similar\ncomputations at different locations in the visual field. Traditional methods\nfor neural system identification do not capitalize on this separation of 'what'\nand 'where'. Learning deep convolutional feature spaces that are shared among\nmany neurons provides an exciting path forward, but the architectural design\nneeds to account for data limitations: While new experimental techniques enable\nrecordings from thousands of neurons, experimental time is limited so that one\ncan sample only a small fraction of each neuron's response space. Here, we show\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\nneural data is the estimation of the individual receptive field locations, a\nproblem that has been scratched only at the surface thus far. We propose a CNN\narchitecture with a sparse readout layer factorizing the spatial (where) and\nfeature (what) dimensions. Our network scales well to thousands of neurons and\nshort recordings and can be trained end-to-end. We evaluate this architecture\non ground-truth data to explore the challenges and limitations of CNN-based\nsystem identification. Moreover, we show that our network model outperforms\ncurrent state-of-the art system identification models of mouse primary visual\ncortex. \n\n"}
{"id": "1711.02666", "contents": "Title: Tensor-Generative Adversarial Network with Two-dimensional Sparse\n  Coding: Application to Real-time Indoor Localization Abstract: Localization technology is important for the development of indoor\nlocation-based services (LBS). Global Positioning System (GPS) becomes invalid\nin indoor environments due to the non-line-of-sight issue, so it is urgent to\ndevelop a real-time high-accuracy localization approach for smartphones.\nHowever, accurate localization is challenging due to issues such as real-time\nresponse requirements, limited fingerprint samples and mobile device storage.\nTo address these problems, we propose a novel deep learning architecture:\nTensor-Generative Adversarial Network (TGAN).\n  We first introduce a transform-based 3D tensor to model fingerprint samples.\nInstead of those passive methods that construct a fingerprint database as a\nprior, our model applies artificial neural network with deep learning to train\nnetwork classifiers and then gives out estimations. Then we propose a novel\ntensor-based super-resolution scheme using the generative adversarial network\n(GAN) that adopts sparse coding as the generator network and a residual\nlearning network as the discriminator. Further, we analyze the performance of\ntensor-GAN and implement a trace-based localization experiment, which achieves\nbetter performance. Compared to existing methods for smartphones indoor\npositioning, that are energy-consuming and high demands on devices, TGAN can\ngive out an improved solution in localization accuracy, response time and\nimplementation complexity. \n\n"}
{"id": "1711.02771", "contents": "Title: On the Discrimination-Generalization Tradeoff in GANs Abstract: Generative adversarial training can be generally understood as minimizing\ncertain moment matching loss defined by a set of discriminator functions,\ntypically neural networks. The discriminator set should be large enough to be\nable to uniquely identify the true distribution (discriminative), and also be\nsmall enough to go beyond memorizing samples (generalizable). In this paper, we\nshow that a discriminator set is guaranteed to be discriminative whenever its\nlinear span is dense in the set of bounded continuous functions. This is a very\nmild condition satisfied even by neural networks with a single neuron. Further,\nwe develop generalization bounds between the learned distribution and true\ndistribution under different evaluation metrics. When evaluated with neural\ndistance, our bounds show that generalization is guaranteed as long as the\ndiscriminator set is small enough, regardless of the size of the generator or\nhypothesis set. When evaluated with KL divergence, our bound provides an\nexplanation on the counter-intuitive behaviors of testing likelihood in GAN\ntraining. Our analysis sheds lights on understanding the practical performance\nof GANs. \n\n"}
{"id": "1711.03577", "contents": "Title: What Really is Deep Learning Doing? Abstract: Deep learning has achieved a great success in many areas, from computer\nvision to natural language processing, to game playing, and much more. Yet,\nwhat deep learning is really doing is still an open question. There are a lot\nof works in this direction. For example, [5] tried to explain deep learning by\ngroup renormalization, and [6] tried to explain deep learning from the view of\nfunctional approximation. In order to address this very crucial question, here\nwe see deep learning from perspective of mechanical learning and learning\nmachine (see [1], [2]). From this particular angle, we can see deep learning\nmuch better and answer with confidence: What deep learning is really doing? why\nit works well, how it works, and how much data is necessary for learning. We\nalso will discuss advantages and disadvantages of deep learning at the end of\nthis work. \n\n"}
{"id": "1711.03602", "contents": "Title: The Lifted Matrix-Space Model for Semantic Composition Abstract: Tree-structured neural network architectures for sentence encoding draw\ninspiration from the approach to semantic composition generally seen in formal\nlinguistics, and have shown empirical improvements over comparable sequence\nmodels by doing so. Moreover, adding multiplicative interaction terms to the\ncomposition functions in these models can yield significant further\nimprovements. However, existing compositional approaches that adopt such a\npowerful composition function scale poorly, with parameter counts exploding as\nmodel dimension or vocabulary size grows. We introduce the Lifted Matrix-Space\nmodel, which uses a global transformation to map vector word embeddings to\nmatrices, which can then be composed via an operation based on matrix-matrix\nmultiplication. Its composition function effectively transmits a larger number\nof activations across layers with relatively few model parameters. We evaluate\nour model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the\nStanford Sentiment Treebank and find that it consistently outperforms TreeLSTM\n(Tai et al., 2015), the previous best known composition function for\ntree-structured models. \n\n"}
{"id": "1711.03712", "contents": "Title: Quantized Memory-Augmented Neural Networks Abstract: Memory-augmented neural networks (MANNs) refer to a class of neural network\nmodels equipped with external memory (such as neural Turing machines and memory\nnetworks). These neural networks outperform conventional recurrent neural\nnetworks (RNNs) in terms of learning long-term dependency, allowing them to\nsolve intriguing AI tasks that would otherwise be hard to address. This paper\nconcerns the problem of quantizing MANNs. Quantization is known to be effective\nwhen we deploy deep models on embedded systems with limited resources.\nFurthermore, quantization can substantially reduce the energy consumption of\nthe inference procedure. These benefits justify recent developments of\nquantized multi layer perceptrons, convolutional networks, and RNNs. However,\nno prior work has reported the successful quantization of MANNs. The in-depth\nanalysis presented here reveals various challenges that do not appear in the\nquantization of the other networks. Without addressing them properly, quantized\nMANNs would normally suffer from excessive quantization error which leads to\ndegraded performance. In this paper, we identify memory addressing\n(specifically, content-based addressing) as the main reason for the performance\ndegradation and propose a robust quantization method for MANNs to address the\nchallenge. In our experiments, we achieved a computation-energy gain of 22x\nwith 8-bit fixed-point and binary quantization compared to the floating-point\nimplementation. Measured on the bAbI dataset, the resulting model, named the\nquantized MANN (Q-MANN), improved the error rate by 46% and 30% with 8-bit\nfixed-point and binary quantization, respectively, compared to the MANN\nquantized using conventional techniques. \n\n"}
{"id": "1711.03800", "contents": "Title: Object Referring in Visual Scene with Spoken Language Abstract: Object referring has important applications, especially for human-machine\ninteraction. While having received great attention, the task is mainly attacked\nwith written language (text) as input rather than spoken language (speech),\nwhich is more natural. This paper investigates Object Referring with Spoken\nLanguage (ORSpoken) by presenting two datasets and one novel approach. Objects\nare annotated with their locations in images, text descriptions and speech\ndescriptions. This makes the datasets ideal for multi-modality learning. The\napproach is developed by carefully taking down ORSpoken problem into three\nsub-problems and introducing task-specific vision-language interactions at the\ncorresponding levels. Experiments show that our method outperforms competing\nmethods consistently and significantly. The approach is also evaluated in the\npresence of audio noise, showing the efficacy of the proposed vision-language\ninteraction methods in counteracting background noise. \n\n"}
{"id": "1711.04150", "contents": "Title: STWalk: Learning Trajectory Representations in Temporal Graphs Abstract: Analyzing the temporal behavior of nodes in time-varying graphs is useful for\nmany applications such as targeted advertising, community evolution and outlier\ndetection. In this paper, we present a novel approach, STWalk, for learning\ntrajectory representations of nodes in temporal graphs. The proposed framework\nmakes use of structural properties of graphs at current and previous time-steps\nto learn effective node trajectory representations. STWalk performs random\nwalks on a graph at a given time step (called space-walk) as well as on graphs\nfrom past time-steps (called time-walk) to capture the spatio-temporal behavior\nof nodes. We propose two variants of STWalk to learn trajectory\nrepresentations. In one algorithm, we perform space-walk and time-walk as part\nof a single step. In the other variant, we perform space-walk and time-walk\nseparately and combine the learned representations to get the final trajectory\nembedding. Extensive experiments on three real-world temporal graph datasets\nvalidate the effectiveness of the learned representations when compared to\nthree baseline methods. We also show the goodness of the learned trajectory\nembeddings for change point detection, as well as demonstrate that arithmetic\noperations on these trajectory representations yield interesting and\ninterpretable results. \n\n"}
{"id": "1711.04810", "contents": "Title: \"Found in Translation\": Predicting Outcomes of Complex Organic Chemistry\n  Reactions using Neural Sequence-to-Sequence Models Abstract: There is an intuitive analogy of an organic chemist's understanding of a\ncompound and a language speaker's understanding of a word. Consequently, it is\npossible to introduce the basic concepts and analyze potential impacts of\nlinguistic analysis to the world of organic chemistry. In this work, we cast\nthe reaction prediction task as a translation problem by introducing a\ntemplate-free sequence-to-sequence model, trained end-to-end and fully\ndata-driven. We propose a novel way of tokenization, which is arbitrarily\nextensible with reaction information. With this approach, we demonstrate\nresults superior to the state-of-the-art solution by a significant margin on\nthe top-1 accuracy. Specifically, our approach achieves an accuracy of 80.1%\nwithout relying on auxiliary knowledge such as reaction templates. Also, 66.4%\naccuracy is reached on a larger and noisier dataset. \n\n"}
{"id": "1711.06006", "contents": "Title: Hindsight policy gradients Abstract: A reinforcement learning agent that needs to pursue different goals across\nepisodes requires a goal-conditional policy. In addition to their potential to\ngeneralize desirable behavior to unseen goals, such policies may also enable\nhigher-level planning based on subgoals. In sparse-reward environments, the\ncapacity to exploit information about the degree to which an arbitrary goal has\nbeen achieved while another goal was intended appears crucial to enable sample\nefficient learning. However, reinforcement learning agents have only recently\nbeen endowed with such capacity for hindsight. In this paper, we demonstrate\nhow hindsight can be introduced to policy gradient methods, generalizing this\nidea to a broad class of successful algorithms. Our experiments on a diverse\nselection of sparse-reward environments show that hindsight leads to a\nremarkable increase in sample efficiency. \n\n"}
{"id": "1711.06652", "contents": "Title: Hardening Quantum Machine Learning Against Adversaries Abstract: Security for machine learning has begun to become a serious issue for present\nday applications. An important question remaining is whether emerging quantum\ntechnologies will help or hinder the security of machine learning. Here we\ndiscuss a number of ways that quantum information can be used to help make\nquantum classifiers more secure or private. In particular, we demonstrate a\nform of robust principal component analysis that, under some circumstances, can\nprovide an exponential speedup relative to robust methods used at present. To\ndemonstrate this approach we introduce a linear combinations of unitaries\nHamiltonian simulation method that we show functions when given an imprecise\nHamiltonian oracle, which may be of independent interest. We also introduce a\nnew quantum approach for bagging and boosting that can use quantum\nsuperposition over the classifiers or splits of the training set to aggregate\nover many more models than would be possible classically. Finally, we provide a\nprivate form of $k$--means clustering that can be used to prevent an all\npowerful adversary from learning more than a small fraction of a bit from any\nuser. These examples show the role that quantum technologies can play in the\nsecurity of ML and vice versa. This illustrates that quantum computing can\nprovide useful advantages to machine learning apart from speedups. \n\n"}
{"id": "1711.07356", "contents": "Title: Evaluating Robustness of Neural Networks with Mixed Integer Programming Abstract: Neural networks have demonstrated considerable success on a wide variety of\nreal-world problems. However, networks trained only to optimize for training\naccuracy can often be fooled by adversarial examples - slightly perturbed\ninputs that are misclassified with high confidence. Verification of networks\nenables us to gauge their vulnerability to such adversarial examples. We\nformulate verification of piecewise-linear neural networks as a mixed integer\nprogram. On a representative task of finding minimum adversarial distortions,\nour verifier is two to three orders of magnitude quicker than the\nstate-of-the-art. We achieve this computational speedup via tight formulations\nfor non-linearities, as well as a novel presolve algorithm that makes full use\nof all information available. The computational speedup allows us to verify\nproperties on convolutional networks with an order of magnitude more ReLUs than\nnetworks previously verified by any complete verifier. In particular, we\ndetermine for the first time the exact adversarial accuracy of an MNIST\nclassifier to perturbations with bounded $l_\\infty$ norm $\\epsilon=0.1$: for\nthis classifier, we find an adversarial example for 4.38% of samples, and a\ncertificate of robustness (to perturbations with bounded norm) for the\nremainder. Across all robust training procedures and network architectures\nconsidered, we are able to certify more samples than the state-of-the-art and\nfind more adversarial examples than a strong first-order attack. \n\n"}
{"id": "1711.07632", "contents": "Title: Generating Thematic Chinese Poetry using Conditional Variational\n  Autoencoders with Hybrid Decoders Abstract: Computer poetry generation is our first step towards computer writing.\nWriting must have a theme. The current approaches of using sequence-to-sequence\nmodels with attention often produce non-thematic poems. We present a novel\nconditional variational autoencoder with a hybrid decoder adding the\ndeconvolutional neural networks to the general recurrent neural networks to\nfully learn topic information via latent variables. This approach significantly\nimproves the relevance of the generated poems by representing each line of the\npoem not only in a context-sensitive manner but also in a holistic way that is\nhighly related to the given keyword and the learned topic. A proposed augmented\nword2vec model further improves the rhythm and symmetry. Tests show that the\ngenerated poems by our approach are mostly satisfying with regulated rules and\nconsistent themes, and 73.42% of them receive an Overall score no less than 3\n(the highest score is 5). \n\n"}
{"id": "1711.07798", "contents": "Title: Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional\n  Neural Networks Abstract: Sentiment analysis is attracting more and more attentions and has become a\nvery hot research topic due to its potential applications in personalized\nrecommendation, opinion mining, etc. Most of the existing methods are based on\neither textual or visual data and can not achieve satisfactory results, as it\nis very hard to extract sufficient information from only one single modality\ndata. Inspired by the observation that there exists strong semantic correlation\nbetween visual and textual data in social medias, we propose an end-to-end deep\nfusion convolutional neural network to jointly learn textual and visual\nsentiment representations from training examples. The two modality information\nare fused together in a pooling layer and fed into fully-connected layers to\npredict the sentiment polarity. We evaluate the proposed approach on two widely\nused data sets. Results show that our method achieves promising result compared\nwith the state-of-the-art methods which clearly demonstrate its competency. \n\n"}
{"id": "1711.08172", "contents": "Title: Run-and-Inspect Method for Nonconvex Optimization and Global Optimality\n  Bounds for R-Local Minimizers Abstract: Many optimization algorithms converge to stationary points. When the\nunderlying problem is nonconvex, they may get trapped at local minimizers and\noccasionally stagnate near saddle points. We propose the Run-and-Inspect\nMethod, which adds an \"inspect\" phase to existing algorithms that helps escape\nfrom non-global stationary points. The inspection samples a set of points in a\nradius $R$ around the current point. When a sample point yields a sufficient\ndecrease in the objective, we move there and resume an existing algorithm. If\nno sufficient decrease is found, the current point is called an approximate\n$R$-local minimizer. We show that an $R$-local minimizer is globally optimal,\nup to a specific error depending on $R$, if the objective function can be\nimplicitly decomposed into a smooth convex function plus a restricted function\nthat is possibly nonconvex, nonsmooth. For high-dimensional problems, we\nintroduce blockwise inspections to overcome the curse of dimensionality while\nstill maintaining optimality bounds up to a factor equal to the number of\nblocks. Our method performs well on a set of artificial and realistic nonconvex\nproblems by coupling with gradient descent, coordinate descent, EM, and\nprox-linear algorithms. \n\n"}
{"id": "1711.08231", "contents": "Title: Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling\n  Sequence Data? Abstract: Existing neural models usually predict the tag of the current token\nindependent of the neighboring tags. The popular LSTM-CRF model considers the\ntag dependencies between every two consecutive tags. However, it is hard for\nexisting neural models to take longer distance dependencies of tags into\nconsideration. The scalability is mainly limited by the complex model\nstructures and the cost of dynamic programming during training. In our work, we\nfirst design a new model called \"high order LSTM\" to predict multiple tags for\nthe current token which contains not only the current tag but also the previous\nseveral tags. We call the number of tags in one prediction as \"order\". Then we\npropose a new method called Multi-Order BiLSTM (MO-BiLSTM) which combines low\norder and high order LSTMs together. MO-BiLSTM keeps the scalability to high\norder models with a pruning technique. We evaluate MO-BiLSTM on all-phrase\nchunking and NER datasets. Experiment results show that MO-BiLSTM achieves the\nstate-of-the-art result in chunking and highly competitive results in two NER\ndatasets. \n\n"}
{"id": "1711.08442", "contents": "Title: From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine\n  Training Through Stopping Sets Abstract: We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC)\nestimators of Restricted Boltzmann Machines (RBMs). We denote our approach\nMarkov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange\nfor random running times. MCLV uses a stopping set built from the training data\nand has maximum number of Markov chain steps K (referred as MCLV-K). We present\na MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and\ndifferences between LVS-K and Contrastive Divergence (CD-K), with LVS-K\nsignificantly outperforming CD-K training RBMs over the MNIST dataset,\nindicating MCLV to be a promising direction in learning generative models. \n\n"}
{"id": "1711.10122", "contents": "Title: End-to-end Adversarial Learning for Generative Conversational Agents Abstract: This paper presents a new adversarial learning method for generative\nconversational agents (GCA) besides a new model of GCA. Similar to previous\nworks on adversarial learning for dialogue generation, our method assumes the\nGCA as a generator that aims at fooling a discriminator that labels dialogues\nas human-generated or machine-generated; however, in our approach, the\ndiscriminator performs token-level classification, i.e. it indicates whether\nthe current token was generated by humans or machines. To do so, the\ndiscriminator also receives the context utterances (the dialogue history) and\nthe incomplete answer up to the current token as input. This new approach makes\npossible the end-to-end training by backpropagation. A self-conversation\nprocess enables to produce a set of generated data with more diversity for the\nadversarial training. This approach improves the performance on questions not\nrelated to the training data. Experimental results with human and adversarial\nevaluations show that the adversarial method yields significant performance\ngains over the usual teacher forcing training. \n\n"}
{"id": "1711.10791", "contents": "Title: Reinforcement Learning To Adapt Speech Enhancement to Instantaneous\n  Input Signal Quality Abstract: Today, the optimal performance of existing noise-suppression algorithms, both\ndata-driven and those based on classic statistical methods, is range bound to\nspecific levels of instantaneous input signal-to-noise ratios. In this paper,\nwe present a new approach to improve the adaptivity of such algorithms enabling\nthem to perform robustly across a wide range of input signal and noise types.\nOur methodology is based on the dynamic control of algorithmic parameters via\nreinforcement learning. Specifically, we model the noise-suppression module as\na black box, requiring no knowledge of the algorithmic mechanics except a\nsimple feedback from the output. We utilize this feedback as the reward signal\nfor a reinforcement-learning agent that learns a policy to adapt the\nalgorithmic parameters for every incoming audio frame (16 ms of data). Our\npreliminary results show that such a control mechanism can substantially\nincrease the overall performance of the underlying noise-suppression algorithm;\n42% and 16% improvements in output SNR and MSE, respectively, when compared to\nno adaptivity. \n\n"}
{"id": "1711.10856", "contents": "Title: Semi-Supervised and Active Few-Shot Learning with Prototypical Networks Abstract: We consider the problem of semi-supervised few-shot classification where a\nclassifier needs to adapt to new tasks using a few labeled examples and\n(potentially many) unlabeled examples. We propose a clustering approach to the\nproblem. The features extracted with Prototypical Networks are clustered using\n$K$-means with the few labeled examples guiding the clustering process. We note\nthat in many real-world applications the adaptation performance can be\nsignificantly improved by requesting the few labels through user feedback. We\ndemonstrate good performance of the active adaptation strategy using image\ndata. \n\n"}
{"id": "1711.10967", "contents": "Title: The Block Point Process Model for Continuous-Time Event-Based Dynamic\n  Networks Abstract: We consider the problem of analyzing timestamped relational events between a\nset of entities, such as messages between users of an on-line social network.\nSuch data are often analyzed using static or discrete-time network models,\nwhich discard a significant amount of information by aggregating events over\ntime to form network snapshots. In this paper, we introduce a block point\nprocess model (BPPM) for continuous-time event-based dynamic networks. The BPPM\nis inspired by the well-known stochastic block model (SBM) for static networks.\nWe show that networks generated by the BPPM follow an SBM in the limit of a\ngrowing number of nodes. We use this property to develop principled and\nefficient local search and variational inference procedures initialized by\nregularized spectral clustering. We fit BPPMs with exponential Hawkes processes\nto analyze several real network data sets, including a Facebook wall post\nnetwork with over 3,500 nodes and 130,000 events. \n\n"}
{"id": "1712.00481", "contents": "Title: Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes Abstract: In order to submit a claim to insurance companies, a doctor needs to code a\npatient encounter with both the diagnosis (ICDs) and procedures performed\n(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant\nprocedures code is a cumbersome and time-consuming task as a doctor has to\nchoose from around 13,000 procedure codes with no predefined one-to-one\nmapping. In this paper, we propose a state-of-the-art deep learning method for\nautomatic and intelligent coding of procedures (CPTs) from the diagnosis codes\n(ICDs) entered by the doctor. Precisely, we cast the learning problem as a\nmulti-label classification problem and use distributed representation to learn\nthe input mapping of high-dimensional sparse ICDs codes. Our final model\ntrained on 2.3 million claims is able to outperform existing rule-based\nprobabilistic and association-rule mining based methods and has a recall of\n90@3. \n\n"}
{"id": "1712.01807", "contents": "Title: Improving the Performance of Online Neural Transducer Models Abstract: Having a sequence-to-sequence model which can operate in an online fashion is\nimportant for streaming applications such as Voice Search. Neural transducer is\na streaming sequence-to-sequence model, but has shown a significant degradation\nin performance compared to non-streaming models such as Listen, Attend and\nSpell (LAS). In this paper, we present various improvements to NT.\nSpecifically, we look at increasing the window over which NT computes\nattention, mainly by looking backwards in time so the model still remains\nonline. In addition, we explore initializing a NT model from a LAS-trained\nmodel so that it is guided with a better alignment. Finally, we explore\nincluding stronger language models such as using wordpiece models, and applying\nan external LM during the beam search. On a Voice Search task, we find with\nthese improvements we can get NT to match the performance of LAS. \n\n"}
{"id": "1712.01821", "contents": "Title: Neural Machine Translation by Generating Multiple Linguistic Factors Abstract: Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported. \n\n"}
{"id": "1712.02743", "contents": "Title: End-to-end Learning of Deterministic Decision Trees Abstract: Conventional decision trees have a number of favorable properties, including\ninterpretability, a small computational footprint and the ability to learn from\nlittle training data. However, they lack a key quality that has helped fuel the\ndeep learning revolution: that of being end-to-end trainable, and to learn from\nscratch those features that best allow to solve a given supervised learning\nproblem. Recent work (Kontschieder 2015) has addressed this deficit, but at the\ncost of losing a main attractive trait of decision trees: the fact that each\nsample is routed along a small subset of tree nodes only. We here propose a\nmodel and Expectation-Maximization training scheme for decision trees that are\nfully probabilistic at train time, but after a deterministic annealing process\nbecome deterministic at test time. We also analyze the learned oblique split\nparameters on image datasets and show that Neural Networks can be trained at\neach split node. In summary, we present the first end-to-end learning scheme\nfor deterministic decision trees and present results on par with or superior to\npublished standard oblique decision tree algorithms. \n\n"}
{"id": "1712.03337", "contents": "Title: Bayesian Joint Matrix Decomposition for Data Integration with\n  Heterogeneous Noise Abstract: Matrix decomposition is a popular and fundamental approach in machine\nlearning and data mining. It has been successfully applied into various fields.\nMost matrix decomposition methods focus on decomposing a data matrix from one\nsingle source. However, it is common that data are from different sources with\nheterogeneous noise. A few of matrix decomposition methods have been extended\nfor such multi-view data integration and pattern discovery. While only few\nmethods were designed to consider the heterogeneity of noise in such multi-view\ndata for data integration explicitly. To this end, we propose a joint matrix\ndecomposition framework (BJMD), which models the heterogeneity of noise by\nGaussian distribution in a Bayesian framework. We develop two algorithms to\nsolve this model: one is a variational Bayesian inference algorithm, which\nmakes full use of the posterior distribution; and another is a maximum a\nposterior algorithm, which is more scalable and can be easily paralleled.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nBJMD considering the heterogeneity of noise is superior or competitive to the\nstate-of-the-art methods. \n\n"}
{"id": "1712.03942", "contents": "Title: StrassenNets: Deep Learning with a Multiplication Budget Abstract: A large fraction of the arithmetic operations required to evaluate deep\nneural networks (DNNs) consists of matrix multiplications, in both convolution\nand fully connected layers. We perform end-to-end learning of low-cost\napproximations of matrix multiplications in DNN layers by casting matrix\nmultiplications as 2-layer sum-product networks (SPNs) (arithmetic circuits)\nand learning their (ternary) edge weights from data. The SPNs disentangle\nmultiplication and addition operations and enable us to impose a budget on the\nnumber of multiplication operations. Combining our method with knowledge\ndistillation and applying it to image classification DNNs (trained on ImageNet)\nand language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction\nin number of multiplications (over 99.5%) while maintaining the predictive\nperformance of the full-precision models. Finally, we demonstrate that the\nproposed framework is able to rediscover Strassen's matrix multiplication\nalgorithm, learning to multiply $2 \\times 2$ matrices using only 7\nmultiplications instead of 8. \n\n"}
{"id": "1712.04116", "contents": "Title: A Novel Document Generation Process for Topic Detection based on\n  Hierarchical Latent Tree Models Abstract: We propose a novel document generation process based on hierarchical latent\ntree models (HLTMs) learned from data. An HLTM has a layer of observed word\nvariables at the bottom and multiple layers of latent variables on top. For\neach document, we first sample values for the latent variables layer by layer\nvia logic sampling, then draw relative frequencies for the words conditioned on\nthe values of the latent variables, and finally generate words for the document\nusing the relative word frequencies. The motivation for the work is to take\nword counts into consideration with HLTMs. In comparison with LDA-based\nhierarchical document generation processes, the new process achieves\ndrastically better model fit with much fewer parameters. It also yields more\nmeaningful topics and topic hierarchies. It is the new state-of-the-art for the\nhierarchical topic detection. \n\n"}
{"id": "1712.04432", "contents": "Title: Integrated Model, Batch and Domain Parallelism in Training Neural\n  Networks Abstract: We propose a new integrated method of exploiting model, batch and domain\nparallelism for the training of deep neural networks (DNNs) on large\ndistributed-memory computers using minibatch stochastic gradient descent (SGD).\nOur goal is to find an efficient parallelization strategy for a fixed batch\nsize using $P$ processes. Our method is inspired by the communication-avoiding\nalgorithms in numerical linear algebra. We see $P$ processes as logically\ndivided into a $P_r \\times P_c$ grid where the $P_r$ dimension is implicitly\nresponsible for model/domain parallelism and the $P_c$ dimension is implicitly\nresponsible for batch parallelism. In practice, the integrated matrix-based\nparallel algorithm encapsulates these types of parallelism automatically. We\nanalyze the communication complexity and analytically demonstrate that the\nlowest communication costs are often achieved neither with pure model nor with\npure data parallelism. We also show how the domain parallel approach can help\nin extending the theoretical scaling limit of the typical batch parallel\nmethod. \n\n"}
{"id": "1712.05134", "contents": "Title: Learning Compact Recurrent Neural Networks with Block-Term Tensor\n  Decomposition Abstract: Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.\nHowever, when dealing with high dimensional inputs, the training of RNNs\nbecomes computational expensive due to the large number of model parameters.\nThis hinders RNNs from solving many important computer vision tasks, such as\nAction Recognition in Videos and Image Captioning. To overcome this problem, we\npropose a compact and flexible structure, namely Block-Term tensor\ndecomposition, which greatly reduces the parameters of RNNs and improves their\ntraining efficiency. Compared with alternative low-rank approximations, such as\ntensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only\nmore concise (when using the same rank), but also able to attain a better\napproximation to the original RNNs with much fewer parameters. On three\nchallenging tasks, including Action Recognition in Videos, Image Captioning and\nImage Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of\nboth prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes\n17,388 times fewer parameters than the standard LSTM to achieve an accuracy\nimprovement over 15.6\\% in the Action Recognition task on the UCF11 dataset. \n\n"}
{"id": "1712.06751", "contents": "Title: HotFlip: White-Box Adversarial Examples for Text Classification Abstract: We propose an efficient method to generate white-box adversarial examples to\ntrick a character-level neural classifier. We find that only a few\nmanipulations are needed to greatly decrease the accuracy. Our method relies on\nan atomic flip operation, which swaps one token for another, based on the\ngradients of the one-hot input vectors. Due to efficiency of our method, we can\nperform adversarial training which makes the model more robust to attacks at\ntest time. With the use of a few semantics-preserving constraints, we\ndemonstrate that HotFlip can be adapted to attack a word-level classifier as\nwell. \n\n"}
{"id": "1712.07316", "contents": "Title: A Flexible Approach to Automated RNN Architecture Generation Abstract: The process of designing neural architectures requires expert knowledge and\nextensive trial and error. While automated architecture search may simplify\nthese requirements, the recurrent neural network (RNN) architectures generated\nby existing methods are limited in both flexibility and components. We propose\na domain-specific language (DSL) for use in automated architecture search which\ncan produce novel RNNs of arbitrary depth and width. The DSL is flexible enough\nto define standard architectures such as the Gated Recurrent Unit and Long\nShort Term Memory and allows the introduction of non-standard RNN components\nsuch as trigonometric curves and layer normalization. Using two different\ncandidate generation techniques, random search with a ranking function and\nreinforcement learning, we explore the novel architectures produced by the RNN\nDSL for language modeling and machine translation domains. The resulting\narchitectures do not follow human intuition yet perform well on their targeted\ntasks, suggesting the space of usable RNN architectures is far larger than\npreviously assumed. \n\n"}
{"id": "1712.07557", "contents": "Title: Differentially Private Federated Learning: A Client Level Perspective Abstract: Federated learning is a recent advance in privacy protection. In this\ncontext, a trusted curator aggregates parameters optimized in decentralized\nfashion by multiple clients. The resulting model is then distributed back to\nall clients, ultimately converging to a joint representative model without\nexplicitly having to share the data. However, the protocol is vulnerable to\ndifferential attacks, which could originate from any party contributing during\nfederated optimization. In such an attack, a client's contribution during\ntraining and information about their data set is revealed through analyzing the\ndistributed model. We tackle this problem and propose an algorithm for client\nsided differential privacy preserving federated optimization. The aim is to\nhide clients' contributions during training, balancing the trade-off between\nprivacy loss and model performance. Empirical studies suggest that given a\nsufficiently large number of participating clients, our proposed procedure can\nmaintain client-level differential privacy at only a minor cost in model\nperformance. \n\n"}
{"id": "1712.08992", "contents": "Title: Leveraging Native Language Speech for Accent Identification using Deep\n  Siamese Networks Abstract: The problem of automatic accent identification is important for several\napplications like speaker profiling and recognition as well as for improving\nspeech recognition systems. The accented nature of speech can be primarily\nattributed to the influence of the speaker's native language on the given\nspeech recording. In this paper, we propose a novel accent identification\nsystem whose training exploits speech in native languages along with the\naccented speech. Specifically, we develop a deep Siamese network-based model\nwhich learns the association between accented speech recordings and the native\nlanguage speech recordings. The Siamese networks are trained with i-vector\nfeatures extracted from the speech recordings using either an unsupervised\nGaussian mixture model (GMM) or a supervised deep neural network (DNN) model.\nWe perform several accent identification experiments using the CSLU Foreign\nAccented English (FAE) corpus. In these experiments, our proposed approach\nusing deep Siamese networks yield significant relative performance improvements\nof 15.4 percent on a 10-class accent identification task, over a baseline\nDNN-based classification system that uses GMM i-vectors. Furthermore, we\npresent a detailed error analysis of the proposed accent identification system. \n\n"}
{"id": "1712.09926", "contents": "Title: Rapid Adaptation with Conditionally Shifted Neurons Abstract: We describe a mechanism by which artificial neural networks can learn rapid\nadaptation - the ability to adapt on the fly, with little data, to new tasks -\nthat we call conditionally shifted neurons. We apply this mechanism in the\nframework of metalearning, where the aim is to replicate some of the\nflexibility of human learning in machines. Conditionally shifted neurons modify\ntheir activation values with task-specific shifts retrieved from a memory\nmodule, which is populated rapidly based on limited task experience. On\nmetalearning benchmarks from the vision and language domains, models augmented\nwith conditionally shifted neurons achieve state-of-the-art results. \n\n"}
{"id": "1712.09983", "contents": "Title: Random Feature-based Online Multi-kernel Learning in Environments with\n  Unknown Dynamics Abstract: Kernel-based methods exhibit well-documented performance in various nonlinear\nlearning tasks. Most of them rely on a preselected kernel, whose prudent choice\npresumes task-specific prior information. Especially when the latter is not\navailable, multi-kernel learning has gained popularity thanks to its\nflexibility in choosing kernels from a prescribed kernel dictionary. Leveraging\nthe random feature approximation and its recent orthogonality-promoting\nvariant, the present contribution develops a scalable multi-kernel learning\nscheme (termed Raker) to obtain the sought nonlinear learning function `on the\nfly,' first for static environments. To further boost performance in dynamic\nenvironments, an adaptive multi-kernel learning scheme (termed AdaRaker) is\ndeveloped. AdaRaker accounts not only for data-driven learning of kernel\ncombination, but also for the unknown dynamics. Performance is analyzed in\nterms of both static and dynamic regrets. AdaRaker is uniquely capable of\ntracking nonlinear learning functions in environments with unknown dynamics,\nand with with analytic performance guarantees. Tests with synthetic and real\ndatasets are carried out to showcase the effectiveness of the novel algorithms. \n\n"}
{"id": "1801.02622", "contents": "Title: Graph Memory Networks for Molecular Activity Prediction Abstract: Molecular activity prediction is critical in drug design. Machine learning\ntechniques such as kernel methods and random forests have been successful for\nthis task. These models require fixed-size feature vectors as input while the\nmolecules are variable in size and structure. As a result, fixed-size\nfingerprint representation is poor in handling substructures for large\nmolecules. In addition, molecular activity tests, or a so-called BioAssays, are\nrelatively small in the number of tested molecules due to its complexity. Here\nwe approach the problem through deep neural networks as they are flexible in\nmodeling structured data such as grids, sequences and graphs. We train multiple\nBioAssays using a multi-task learning framework, which combines information\nfrom multiple sources to improve the performance of prediction, especially on\nsmall datasets. We propose Graph Memory Network (GraphMem), a memory-augmented\nneural network to model the graph structure in molecules. GraphMem consists of\na recurrent controller coupled with an external memory whose cells dynamically\ninteract and change through a multi-hop reasoning process. Applied to the\nmolecules, the dynamic interactions enable an iterative refinement of the\nrepresentation of molecular graphs with multiple bond types. GraphMem is\ncapable of jointly training on multiple datasets by using a specific-task query\nfed to the controller as an input. We demonstrate the effectiveness of the\nproposed model for separately and jointly training on more than 100K\nmeasurements, spanning across 9 BioAssay activity tests. \n\n"}
{"id": "1801.02710", "contents": "Title: Modeling urbanization patterns with generative adversarial networks Abstract: In this study we propose a new method to simulate hyper-realistic urban\npatterns using Generative Adversarial Networks trained with a global urban\nland-use inventory. We generated a synthetic urban \"universe\" that\nqualitatively reproduces the complex spatial organization observed in global\nurban patterns, while being able to quantitatively recover certain key\nhigh-level urban spatial metrics. \n\n"}
{"id": "1801.02764", "contents": "Title: Compressing Deep Neural Networks: A New Hashing Pipeline Using Kac's\n  Random Walk Matrices Abstract: The popularity of deep learning is increasing by the day. However, despite\nthe recent advancements in hardware, deep neural networks remain\ncomputationally intensive. Recent work has shown that by preserving the angular\ndistance between vectors, random feature maps are able to reduce dimensionality\nwithout introducing bias to the estimator. We test a variety of established\nhashing pipelines as well as a new approach using Kac's random walk matrices.\nWe demonstrate that this method achieves similar accuracy to existing\npipelines. \n\n"}
{"id": "1801.03558", "contents": "Title: Inference Suboptimality in Variational Autoencoders Abstract: Amortized inference allows latent-variable models trained via variational\nlearning to scale to large datasets. The quality of approximate inference is\ndetermined by two factors: a) the capacity of the variational distribution to\nmatch the true posterior and b) the ability of the recognition network to\nproduce good variational parameters for each datapoint. We examine approximate\ninference in variational autoencoders in terms of these factors. We find that\ndivergence from the true posterior is often due to imperfect recognition\nnetworks, rather than the limited complexity of the approximating distribution.\nWe show that this is due partly to the generator learning to accommodate the\nchoice of approximation. Furthermore, we show that the parameters used to\nincrease the expressiveness of the approximation play a role in generalizing\ninference rather than simply improving the complexity of the approximation. \n\n"}
{"id": "1801.03622", "contents": "Title: Topic-based Evaluation for Conversational Bots Abstract: Dialog evaluation is a challenging problem, especially for non task-oriented\ndialogs where conversational success is not well-defined. We propose to\nevaluate dialog quality using topic-based metrics that describe the ability of\na conversational bot to sustain coherent and engaging conversations on a topic,\nand the diversity of topics that a bot can handle. To detect conversation\ntopics per utterance, we adopt Deep Average Networks (DAN) and train a topic\nclassifier on a variety of question and query data categorized into multiple\ntopics. We propose a novel extension to DAN by adding a topic-word attention\ntable that allows the system to jointly capture topic keywords in an utterance\nand perform topic classification. We compare our proposed topic based metrics\nwith the ratings provided by users and show that our metrics both correlate\nwith and complement human judgment. Our analysis is performed on tens of\nthousands of real human-bot dialogs from the Alexa Prize competition and\nhighlights user expectations for conversational bots. \n\n"}
{"id": "1801.03796", "contents": "Title: Theory for Swap Acceleration near the Glass and Jamming Transitions Abstract: Swap algorithms can shift the glass transition to lower temperatures, a\nrecent unexplained observation constraining the nature of this phenomenon. Here\nwe show that swap dynamic is governed by an effective potential describing both\nparticle interactions as well as their ability to change size. Requiring its\nstability is more demanding than for the potential energy alone. This result\nimplies that stable configurations appear at lower energies with swap dynamics,\nand thus at lower temperatures when the liquid is cooled. \\maa{ The magnitude\nof this effect is proportional to the width of the radii distribution, and\ndecreases with compression for finite-range purely repulsive interaction\npotentials.} We test these predictions numerically and discuss the implications\nof these findings for the glass transition.We extend these results to the case\nof hard spheres where swap is argued to destroy meta-stable states of the free\nenergy coarse-grained on vibrational time scales. Our analysis unravels the\nsoft elastic modes responsible for the speed up swap induces, and allows us to\npredict the structure and the vibrational properties of glass configurations\nreachable with swap. In particular for continuously poly-disperse systems we\npredict the jamming transition to be dramatically altered, as we confirm\nnumerically. A surprising practical outcome of our analysis is new algorithm\nthat generates ultra-stable glasses by simple descent in an appropriate\neffective potential. \n\n"}
{"id": "1801.08058", "contents": "Title: Intel nGraph: An Intermediate Representation, Compiler, and Executor for\n  Deep Learning Abstract: The Deep Learning (DL) community sees many novel topologies published each\nyear. Achieving high performance on each new topology remains challenging, as\neach requires some level of manual effort. This issue is compounded by the\nproliferation of frameworks and hardware platforms. The current approach, which\nwe call \"direct optimization\", requires deep changes within each framework to\nimprove the training performance for each hardware backend (CPUs, GPUs, FPGAs,\nASICs) and requires $\\mathcal{O}(fp)$ effort; where $f$ is the number of\nframeworks and $p$ is the number of platforms. While optimized kernels for\ndeep-learning primitives are provided via libraries like Intel Math Kernel\nLibrary for Deep Neural Networks (MKL-DNN), there are several compiler-inspired\nways in which performance can be further optimized. Building on our experience\ncreating neon (a fast deep learning library on GPUs), we developed Intel\nnGraph, a soon to be open-sourced C++ library to simplify the realization of\noptimized deep learning performance across frameworks and hardware platforms.\nInitially-supported frameworks include TensorFlow, MXNet, and Intel neon\nframework. Initial backends are Intel Architecture CPUs (CPU), the Intel(R)\nNervana Neural Network Processor(R) (NNP), and NVIDIA GPUs. Currently supported\ncompiler optimizations include efficient memory management and data layout\nabstraction. In this paper, we describe our overall architecture and its core\ncomponents. In the future, we envision extending nGraph API support to a wider\nrange of frameworks, hardware (including FPGAs and ASICs), and compiler\noptimizations (training versus inference optimizations, multi-node and\nmulti-device scaling via efficient sub-graph partitioning, and HW-specific\ncompounding of operations). \n\n"}
{"id": "1801.08676", "contents": "Title: Neural Algebra of Classifiers Abstract: The world is fundamentally compositional, so it is natural to think of visual\nrecognition as the recognition of basic visually primitives that are composed\naccording to well-defined rules. This strategy allows us to recognize unseen\ncomplex concepts from simple visual primitives. However, the current trend in\nvisual recognition follows a data greedy approach where huge amounts of data\nare required to learn models for any desired visual concept. In this paper, we\nbuild on the compositionality principle and develop an \"algebra\" to compose\nclassifiers for complex visual concepts. To this end, we learn neural network\nmodules to perform boolean algebra operations on simple visual classifiers.\nSince these modules form a complete functional set, a classifier for any\ncomplex visual concept defined as a boolean expression of primitives can be\nobtained by recursively applying the learned modules, even if we do not have a\nsingle training sample. As our experiments show, using such a framework, we can\ncompose classifiers for complex visual concepts outperforming standard\nbaselines on two well-known visual recognition benchmarks. Finally, we present\na qualitative analysis of our method and its properties. \n\n"}
{"id": "1801.08702", "contents": "Title: Improving Bi-directional Generation between Different Modalities with\n  Variational Autoencoders Abstract: We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. A major approach to achieve this objective is to train a model that\nintegrates all the information of different modalities into a joint\nrepresentation and then to generate one modality from the corresponding other\nmodality via this joint representation. We simply applied this approach to\nvariational autoencoders (VAEs), which we call a joint multimodal variational\nautoencoder (JMVAE). However, we found that when this model attempts to\ngenerate a large dimensional modality missing at the input, the joint\nrepresentation collapses and this modality cannot be generated successfully.\nFurthermore, we confirmed that this difficulty cannot be resolved even using a\nknown solution. Therefore, in this study, we propose two models to prevent this\ndifficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that\nthese methods can prevent the difficulty above and that they generate\nmodalities bi-directionally with equal or higher likelihood than conventional\nVAE methods, which generate in only one direction. Moreover, we confirm that\nthese methods can obtain the joint representation appropriately, so that they\ncan generate various variations of modality by moving over the joint\nrepresentation or changing the value of another modality. \n\n"}
{"id": "1801.09746", "contents": "Title: A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts Abstract: Motivated by a project to create a system for people who are deaf or\nhard-of-hearing that would use automatic speech recognition (ASR) to produce\nreal-time text captions of spoken English during in-person meetings with\nhearing individuals, we have augmented a transcript of the Switchboard\nconversational dialogue corpus with an overlay of word-importance annotations,\nwith a numeric score for each word, to indicate its importance to the meaning\nof each dialogue turn. Further, we demonstrate the utility of this corpus by\ntraining an automatic word importance labeling model; our best performing model\nhas an F-score of 0.60 in an ordinal 6-class word-importance classification\ntask with an agreement (concordance correlation coefficient) of 0.839 with the\nhuman annotators (agreement score between annotators is 0.89). Finally, we\ndiscuss our intended future applications of this resource, particularly for the\ntask of evaluating ASR performance, i.e. creating metrics that predict\nASR-output caption text usability for DHH users better thanWord Error Rate\n(WER). \n\n"}
{"id": "1801.09797", "contents": "Title: Discrete Autoencoders for Sequence Models Abstract: Recurrent models for sequences have been recently successful at many tasks,\nespecially for language modeling and machine translation. Nevertheless, it\nremains challenging to extract good representations from these models. For\ninstance, even though language has a clear hierarchical structure going from\ncharacters through words to sentences, it is not apparent in current language\nmodels. We propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to\npropagate gradients though this discrete representation we introduce an\nimproved semantic hashing technique. We show that this technique performs well\non a newly proposed quantitative efficiency measure. We also analyze latent\ncodes produced by the model showing how they correspond to words and phrases.\nFinally, we present an application of the autoencoder-augmented model to\ngenerating diverse translations. \n\n"}
{"id": "1802.01021", "contents": "Title: DeepType: Multilingual Entity Linking by Neural Type System Evolution Abstract: The wealth of structured (e.g. Wikidata) and unstructured data about the\nworld available today presents an incredible opportunity for tomorrow's\nArtificial Intelligence. So far, integration of these two different modalities\nis a difficult process, involving many decisions concerning how best to\nrepresent the information so that it will be captured or useful, and\nhand-labeling large amounts of data. DeepType overcomes this challenge by\nexplicitly integrating symbolic information into the reasoning process of a\nneural network with a type system. First we construct a type system, and\nsecond, we use it to constrain the outputs of a neural network to respect the\nsymbolic structure. We achieve this by reformulating the design problem into a\nmixed integer problem: create a type system and subsequently train a neural\nnetwork with it. In this reformulation discrete variables select which\nparent-child relations from an ontology are types within the type system, while\ncontinuous variables control a classifier fit to the type system. The original\nproblem cannot be solved exactly, so we propose a 2-step algorithm: 1)\nheuristic search or stochastic optimization over discrete variables that define\na type system informed by an Oracle and a Learnability heuristic, 2) gradient\ndescent to fit classifier parameters. We apply DeepType to the problem of\nEntity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC\nKBP 2010) and find that it outperforms all existing solutions by a wide margin,\nincluding approaches that rely on a human-designed type system or recent deep\nlearning-based entity embeddings, while explicitly using symbolic information\nlets it integrate new entities without retraining. \n\n"}
{"id": "1802.01059", "contents": "Title: Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain\n  Features Abstract: Unsupervised learning of time series data, also known as temporal clustering,\nis a challenging problem in machine learning. Here we propose a novel\nalgorithm, Deep Temporal Clustering (DTC), to naturally integrate\ndimensionality reduction and temporal clustering into a single end-to-end\nlearning framework, fully unsupervised. The algorithm utilizes an autoencoder\nfor temporal dimensionality reduction and a novel temporal clustering layer for\ncluster assignment. Then it jointly optimizes the clustering objective and the\ndimensionality reduction objec tive. Based on requirement and application, the\ntemporal clustering layer can be customized with any temporal similarity\nmetric. Several similarity metrics and state-of-the-art algorithms are\nconsidered and compared. To gain insight into temporal features that the\nnetwork has learned for its clustering, we apply a visualization method that\ngenerates a region of interest heatmap for the time series. The viability of\nthe algorithm is demonstrated using time series data from diverse domains,\nranging from earthquakes to spacecraft sensor data. In each case, we show that\nthe proposed algorithm outperforms traditional methods. The superior\nperformance is attributed to the fully integrated temporal dimensionality\nreduction and clustering criterion. \n\n"}
{"id": "1802.02046", "contents": "Title: Neural Network Detection of Data Sequences in Communication Systems Abstract: We consider detection based on deep learning, and show it is possible to\ntrain detectors that perform well without any knowledge of the underlying\nchannel models. Moreover, when the channel model is known, we demonstrate that\nit is possible to train detectors that do not require channel state information\n(CSI). In particular, a technique we call a sliding bidirectional recurrent\nneural network (SBRNN) is proposed for detection where, after training, the\ndetector estimates the data in real-time as the signal stream arrives at the\nreceiver. We evaluate this algorithm, as well as other neural network (NN)\narchitectures, using the Poisson channel model, which is applicable to both\noptical and molecular communication systems. In addition, we also evaluate the\nperformance of this detection method applied to data sent over a molecular\ncommunication platform, where the channel model is difficult to model\nanalytically. We show that SBRNN is computationally efficient, and can perform\ndetection under various channel conditions without knowing the underlying\nchannel model. We also demonstrate that the bit error rate (BER) performance of\nthe proposed SBRNN detector is better than that of a Viterbi detector with\nimperfect CSI as well as that of other NN detectors that have been previously\nproposed. Finally, we show that the SBRNN can perform well in rapidly changing\nchannels, where the coherence time is on the order of a single symbol duration. \n\n"}
{"id": "1802.02607", "contents": "Title: Learning from Past Mistakes: Improving Automatic Speech Recognition\n  Output via Noisy-Clean Phrase Context Modeling Abstract: Automatic speech recognition (ASR) systems often make unrecoverable errors\ndue to subsystem pruning (acoustic, language and pronunciation models); for\nexample pruning words due to acoustics using short-term context, prior to\nrescoring with long-term context based on linguistics. In this work we model\nASR as a phrase-based noisy transformation channel and propose an error\ncorrection system that can learn from the aggregate errors of all the\nindependent modules constituting the ASR and attempt to invert those. The\nproposed system can exploit long-term context using a neural network language\nmodel and can better choose between existing ASR output possibilities as well\nas re-introduce previously pruned or unseen (out-of-vocabulary) phrases. It\nprovides corrections under poorly performing ASR conditions without degrading\nany accurate transcriptions; such corrections are greater on top of\nout-of-domain and mismatched data ASR. Our system consistently provides\nimprovements over the baseline ASR, even when baseline is further optimized\nthrough recurrent neural network language model rescoring. This demonstrates\nthat any ASR improvements can be exploited independently and that our proposed\nsystem can potentially still provide benefits on highly optimized ASR. Finally,\nwe present an extensive analysis of the type of errors corrected by our system. \n\n"}
{"id": "1802.03063", "contents": "Title: Learning Latent Representations in Neural Networks for Clustering\n  through Pseudo Supervision and Graph-based Activity Regularization Abstract: In this paper, we propose a novel unsupervised clustering approach exploiting\nthe hidden information that is indirectly introduced through a pseudo\nclassification objective. Specifically, we randomly assign a pseudo\nparent-class label to each observation which is then modified by applying the\ndomain specific transformation associated with the assigned label. Generated\npseudo observation-label pairs are subsequently used to train a neural network\nwith Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes\nfor each pseudo parent-class. Due to the unsupervised objective based on\nGraph-based Activity Regularization (GAR) terms, softmax duplicates of each\nparent-class are specialized as the hidden information captured through the\nhelp of domain specific transformations is propagated during training.\nUltimately we obtain a k-means friendly latent representation. Furthermore, we\ndemonstrate how the chosen transformation type impacts performance and helps\npropagate the latent information that is useful in revealing unknown clusters.\nOur results show state-of-the-art performance for unsupervised clustering tasks\non MNIST, SVHN and USPS datasets, with the highest accuracies reported to date\nin the literature. \n\n"}
{"id": "1802.03236", "contents": "Title: Learning Robust Options Abstract: Robust reinforcement learning aims to produce policies that have strong\nguarantees even in the face of environments/transition models whose parameters\nhave strong uncertainty. Existing work uses value-based methods and the usual\nprimitive action setting. In this paper, we propose robust methods for learning\ntemporally abstract actions, in the framework of options. We present a Robust\nOptions Policy Iteration (ROPI) algorithm with convergence guarantees, which\nlearns options that are robust to model uncertainty. We utilize ROPI to learn\nrobust options with the Robust Options Deep Q Network (RO-DQN) that solves\nmultiple tasks and mitigates model misspecification due to model uncertainty.\nWe present experimental results which suggest that policy iteration with linear\nfeatures may have an inherent form of robustness when using coarse feature\nrepresentations. In addition, we present experimental results which demonstrate\nthat robustness helps policy iteration implemented on top of deep neural\nnetworks to generalize over a much broader range of dynamics than non-robust\npolicy iteration. \n\n"}
{"id": "1802.03560", "contents": "Title: The Importance of Norm Regularization in Linear Graph Embedding:\n  Theoretical Analysis and Empirical Demonstration Abstract: Learning distributed representations for nodes in graphs is a crucial\nprimitive in network analysis with a wide spectrum of applications. Linear\ngraph embedding methods learn such representations by optimizing the likelihood\nof both positive and negative edges while constraining the dimension of the\nembedding vectors. We argue that the generalization performance of these\nmethods is not due to the dimensionality constraint as commonly believed, but\nrather the small norm of embedding vectors. Both theoretical and empirical\nevidence are provided to support this argument: (a) we prove that the\ngeneralization error of these methods can be bounded by limiting the norm of\nvectors, regardless of the embedding dimension; (b) we show that the\ngeneralization performance of linear graph embedding methods is correlated with\nthe norm of embedding vectors, which is small due to the early stopping of SGD\nand the vanishing gradients. We performed extensive experiments to validate our\nanalysis and showcased the importance of proper norm regularization in\npractice. \n\n"}
{"id": "1802.03594", "contents": "Title: Online Learning for Effort Reduction in Interactive Neural Machine\n  Translation Abstract: Neural machine translation systems require large amounts of training data and\nresources. Even with this, the quality of the translations may be insufficient\nfor some users or domains. In such cases, the output of the system must be\nrevised by a human agent. This can be done in a post-editing stage or following\nan interactive machine translation protocol.\n  We explore the incremental update of neural machine translation systems\nduring the post-editing or interactive translation processes. Such\nmodifications aim to incorporate the new knowledge, from the edited sentences,\ninto the translation system. Updates to the model are performed on-the-fly, as\nsentences are corrected, via online learning techniques. In addition, we\nimplement a novel interactive, adaptive system, able to react to\nsingle-character interactions. This system greatly reduces the human effort\nrequired for obtaining high-quality translations.\n  In order to stress our proposals, we conduct exhaustive experiments varying\nthe amount and type of data available for training. Results show that online\nlearning effectively achieves the objective of reducing the human effort\nrequired during the post-editing or the interactive machine translation stages.\nMoreover, these adaptive systems also perform well in scenarios with scarce\nresources. We show that a neural machine translation system can be rapidly\nadapted to a specific domain, exclusively by means of online learning\ntechniques. \n\n"}
{"id": "1802.03689", "contents": "Title: Dual Control Memory Augmented Neural Networks for Treatment\n  Recommendations Abstract: Machine-assisted treatment recommendations hold a promise to reduce physician\ntime and decision errors. We formulate the task as a sequence-to-sequence\nprediction model that takes the entire time-ordered medical history as input,\nand predicts a sequence of future clinical procedures and medications. It is\nbuilt on the premise that an effective treatment plan may have long-term\ndependencies from previous medical history. We approach the problem by using a\nmemory-augmented neural network, in particular, by leveraging the recent\ndifferentiable neural computer that consists of a neural controller and an\nexternal memory module. But differing from the original model, we use dual\ncontrollers, one for encoding the history followed by another for decoding the\ntreatment sequences. In the encoding phase, the memory is updated as new input\nis read; at the end of this phase, the memory holds not only the medical\nhistory but also the information about the current illness. During the decoding\nphase, the memory is write-protected. The decoding controller generates a\ntreatment sequence, one treatment option at a time. The resulting dual\ncontroller write-protected memory-augmented neural network is demonstrated on\nthe MIMIC-III dataset on two tasks: procedure prediction and medication\nprescription. The results show improved performance over both traditional\nbag-of-words and sequence-to-sequence methods. \n\n"}
{"id": "1802.03881", "contents": "Title: Answerer in Questioner's Mind: Information Theoretic Approach to\n  Goal-Oriented Visual Dialog Abstract: Goal-oriented dialog has been given attention due to its numerous\napplications in artificial intelligence. Goal-oriented dialogue tasks occur\nwhen a questioner asks an action-oriented question and an answerer responds\nwith the intent of letting the questioner know a correct action to take. To ask\nthe adequate question, deep learning and reinforcement learning have been\nrecently applied. However, these approaches struggle to find a competent\nrecurrent neural questioner, owing to the complexity of learning a series of\nsentences. Motivated by theory of mind, we propose \"Answerer in Questioner's\nMind\" (AQM), a novel information theoretic algorithm for goal-oriented dialog.\nWith AQM, a questioner asks and infers based on an approximated probabilistic\nmodel of the answerer. The questioner figures out the answerer's intention via\nselecting a plausible question by explicitly calculating the information gain\nof the candidate intentions and possible answers to each question. We test our\nframework on two goal-oriented visual dialog tasks: \"MNIST Counting Dialog\" and\n\"GuessWhat?!\". In our experiments, AQM outperforms comparative algorithms by a\nlarge margin. \n\n"}
{"id": "1802.04036", "contents": "Title: Inferring the time-varying functional connectivity of large-scale\n  computer networks from emitted events Abstract: We consider the problem of inferring the functional connectivity of a\nlarge-scale computer network from sparse time series of events emitted by its\nnodes. We do so under the following three domain-specific constraints: (a)\nnon-stationarity of the functional connectivity due to unknown temporal changes\nin the network, (b) sparsity of the time-series of events that limits the\neffectiveness of classical correlation-based analysis, and (c) lack of an\nexplicit model describing how events propagate through the network. Under the\nassumption that the probability of two nodes being functionally connected\ncorrelates with the mean delay between their respective events, we develop an\ninference method whose output is an undirected weighted network where the\nweight of an edge between two nodes denotes the probability of these nodes\nbeing functionally connected. Using a combination of windowing and convolution\nto calculate at each time window a score quantifying the likelihood of a pair\nof nodes emitting events in quick succession, we develop a model of\ntime-varying connectivity whose parameters are determined by maximising the\nmodel's predictive power from one time window to the next. To assess the\neffectiveness of our inference method, we construct synthetic data for which\nground truth is available and use these data to benchmark our approach against\nthree state-of-the-art inference methods. We conclude by discussing its\napplication to data from a real-world large-scale computer network. \n\n"}
{"id": "1802.04220", "contents": "Title: Augment and Reduce: Stochastic Inference for Large Categorical\n  Distributions Abstract: Categorical distributions are ubiquitous in machine learning, e.g., in\nclassification, language models, and recommendation systems. However, when the\nnumber of possible outcomes is very large, using categorical distributions\nbecomes computationally expensive, as the complexity scales linearly with the\nnumber of outcomes. To address this problem, we propose augment and reduce\n(A&R), a method to alleviate the computational complexity. A&R uses two ideas:\nlatent variable augmentation and stochastic variational inference. It maximizes\na lower bound on the marginal likelihood of the data. Unlike existing methods\nwhich are specific to softmax, A&R is more general and is amenable to other\ncategorical models, such as multinomial probit. On several large-scale\nclassification problems, we show that A&R provides a tighter bound on the\nmarginal likelihood and has better predictive performance than existing\napproaches. \n\n"}
{"id": "1802.04420", "contents": "Title: Towards Understanding the Generalization Bias of Two Layer Convolutional\n  Linear Classifiers with Gradient Descent Abstract: A major challenge in understanding the generalization of deep learning is to\nexplain why (stochastic) gradient descent can exploit the network architecture\nto find solutions that have good generalization performance when using high\ncapacity models. We find simple but realistic examples showing that this\nphenomenon exists even when learning linear classifiers --- between two linear\nnetworks with the same capacity, the one with a convolutional layer can\ngeneralize better than the other when the data distribution has some underlying\nspatial structure. We argue that this difference results from a combination of\nthe convolution architecture, data distribution and gradient descent, all of\nwhich are necessary to be included in a meaningful analysis. We provide a\ngeneral analysis of the generalization performance as a function of data\ndistribution and convolutional filter size, given gradient descent as the\noptimization algorithm, then interpret the results using concrete examples.\nExperimental results show that our analysis is able to explain what happens in\nour introduced examples. \n\n"}
{"id": "1802.04431", "contents": "Title: Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic\n  Thresholding Abstract: As spacecraft send back increasing amounts of telemetry data, improved\nanomaly detection systems are needed to lessen the monitoring burden placed on\noperations engineers and reduce operational risk. Current spacecraft monitoring\nsystems only target a subset of anomaly types and often require costly expert\nknowledge to develop and maintain due to challenges involving scale and\ncomplexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs)\nnetworks, a type of Recurrent Neural Network (RNN), in overcoming these issues\nusing expert-labeled telemetry anomaly data from the Soil Moisture Active\nPassive (SMAP) satellite and the Mars Science Laboratory (MSL) rover,\nCuriosity. We also propose a complementary unsupervised and nonparametric\nanomaly thresholding approach developed during a pilot implementation of an\nanomaly detection system for SMAP, and offer false positive mitigation\nstrategies along with other key improvements and lessons learned during\ndevelopment. \n\n"}
{"id": "1802.04675", "contents": "Title: Attention based Sentence Extraction from Scientific Articles using\n  Pseudo-Labeled data Abstract: In this work, we present a weakly supervised sentence extraction technique\nfor identifying important sentences in scientific papers that are worthy of\ninclusion in the abstract. We propose a new attention based deep learning\narchitecture that jointly learns to identify important content, as well as the\ncue phrases that are indicative of summary worthy sentences. We propose a new\ncontext embedding technique for determining the focus of a given paper using\ntopic models and use it jointly with an LSTM based sequence encoder to learn\nattention weights across the sentence words. We use a collection of articles\npublicly available through ACL anthology for our experiments. Our system\nachieves a performance that is better, in terms of several ROUGE metrics, as\ncompared to several state of art extractive techniques. It also generates more\ncoherent summaries and preserves the overall structure of the document. \n\n"}
{"id": "1802.05368", "contents": "Title: Universal Neural Machine Translation for Extremely Low Resource\n  Languages Abstract: In this paper, we propose a new universal machine translation approach\nfocusing on languages with a limited amount of parallel data. Our proposed\napproach utilizes a transfer-learning approach to share lexical and sentence\nlevel representations across multiple source languages into one target\nlanguage. The lexical part is shared through a Universal Lexical Representation\nto support multilingual word-level sharing. The sentence-level sharing is\nrepresented by a model of experts from all source languages that share the\nsource encoders with all other languages. This enables the low-resource\nlanguage to utilize the lexical and sentence representations of the higher\nresource languages. Our approach is able to achieve 23 BLEU on Romanian-English\nWMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU\nof strong baseline system which uses multilingual training and\nback-translation. Furthermore, we show that the proposed approach can achieve\nalmost 20 BLEU on the same dataset through fine-tuning a pre-trained\nmulti-lingual system in a zero-shot setting. \n\n"}
{"id": "1802.05373", "contents": "Title: Improving Retrieval Modeling Using Cross Convolution Networks And Multi\n  Frequency Word Embedding Abstract: To build a satisfying chatbot that has the ability of managing a\ngoal-oriented multi-turn dialogue, accurate modeling of human conversation is\ncrucial. In this paper we concentrate on the task of response selection for\nmulti-turn human-computer conversation with a given context. Previous\napproaches show weakness in capturing information of rare keywords that appear\nin either or both context and correct response, and struggle with long input\nsequences. We propose Cross Convolution Network (CCN) and Multi Frequency word\nembedding to address both problems. We train several models using the Ubuntu\nDialogue dataset which is the largest freely available multi-turn based\ndialogue corpus. We further build an ensemble model by averaging predictions of\nmultiple models. We achieve a new state-of-the-art on this dataset with\nconsiderable improvements compared to previous best results. \n\n"}
{"id": "1802.05411", "contents": "Title: Selecting the Best in GANs Family: a Post Selection Inference Framework Abstract: \"Which Generative Adversarial Networks (GANs) generates the most plausible\nimages?\" has been a frequently asked question among researchers. To address\nthis problem, we first propose an \\emph{incomplete} U-statistics estimate of\nmaximum mean discrepancy $\\mathrm{MMD}_{inc}$ to measure the distribution\ndiscrepancy between generated and real images. $\\mathrm{MMD}_{inc}$ enjoys the\nadvantages of asymptotic normality, computation efficiency, and model\nagnosticity. We then propose a GANs analysis framework to select and test the\n\"best\" member in GANs family using the Post Selection Inference (PSI) with\n$\\mathrm{MMD}_{inc}$. In the experiments, we adopt the proposed framework on 7\nGANs variants and compare their $\\mathrm{MMD}_{inc}$ scores. \n\n"}
{"id": "1802.05584", "contents": "Title: Convolutional Analysis Operator Learning: Acceleration and Convergence Abstract: Convolutional operator learning is gaining attention in many signal\nprocessing and computer vision applications. Learning kernels has mostly relied\non so-called patch-domain approaches that extract and store many overlapping\npatches across training signals. Due to memory demands, patch-domain methods\nhave limitations when learning kernels from large datasets -- particularly with\nmulti-layered structures, e.g., convolutional neural networks -- or when\napplying the learned kernels to high-dimensional signal recovery problems. The\nso-called convolution approach does not store many overlapping patches, and\nthus overcomes the memory problems particularly with careful algorithmic\ndesigns; it has been studied within the \"synthesis\" signal model, e.g.,\nconvolutional dictionary learning. This paper proposes a new convolutional\nanalysis operator learning (CAOL) framework that learns an analysis sparsifying\nregularizer with the convolution perspective, and develops a new convergent\nBlock Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve\nthe corresponding block multi-nonconvex problems. To learn diverse filters\nwithin the CAOL framework, this paper introduces an orthogonality constraint\nthat enforces a tight-frame filter condition, and a regularizer that promotes\ndiversity between filters. Numerical experiments show that, with sharp\nmajorizers, BPEG-M significantly accelerates the CAOL convergence rate compared\nto the state-of-the-art block proximal gradient (BPG) method. Numerical\nexperiments for sparse-view computational tomography show that a convolutional\nsparsifying regularizer learned via CAOL significantly improves reconstruction\nquality compared to a conventional edge-preserving regularizer. Using more and\nwider kernels in a learned regularizer better preserves edges in reconstructed\nimages. \n\n"}
{"id": "1802.06384", "contents": "Title: Spurious Valleys in Two-layer Neural Network Optimization Landscapes Abstract: Neural networks provide a rich class of high-dimensional, non-convex\noptimization problems. Despite their non-convexity, gradient-descent methods\noften successfully optimize these models. This has motivated a recent spur in\nresearch attempting to characterize properties of their loss surface that may\nexplain such success.\n  In this paper, we address this phenomenon by studying a key topological\nproperty of the loss: the presence or absence of spurious valleys, defined as\nconnected components of sub-level sets that do not include a global minimum.\nFocusing on a class of two-layer neural networks defined by smooth (but\ngenerally non-linear) activation functions, we identify a notion of intrinsic\ndimension and show that it provides necessary and sufficient conditions for the\nabsence of spurious valleys. More concretely, finite intrinsic dimension\nguarantees that for sufficiently overparametrised models no spurious valleys\nexist, independently of the data distribution. Conversely, infinite intrinsic\ndimension implies that spurious valleys do exist for certain data\ndistributions, independently of model overparametrisation. Besides these\npositive and negative results, we show that, although spurious valleys may\nexist in general, they are confined to low risk levels and avoided with high\nprobability on overparametrised models. \n\n"}
{"id": "1802.06463", "contents": "Title: Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross\n  Entropy Abstract: We study model recovery for data classification, where the training labels\nare generated from a one-hidden-layer neural network with sigmoid activations,\nalso known as a single-layer feedforward network, and the goal is to recover\nthe weights of the neural network. We consider two network models, the\nfully-connected network (FCN) and the non-overlapping convolutional neural\nnetwork (CNN). We prove that with Gaussian inputs, the empirical risk based on\ncross entropy exhibits strong convexity and smoothness {\\em uniformly} in a\nlocal neighborhood of the ground truth, as soon as the sample complexity is\nsufficiently large. This implies that if initialized in this neighborhood,\ngradient descent converges linearly to a critical point that is provably close\nto the ground truth. Furthermore, we show such an initialization can be\nobtained via the tensor method. This establishes the global convergence\nguarantee for empirical risk minimization using cross entropy via gradient\ndescent for learning one-hidden-layer neural networks, at the near-optimal\nsample and computational complexity with respect to the network input dimension\nwithout unrealistic assumptions such as requiring a fresh set of samples at\neach iteration. \n\n"}
{"id": "1802.06552", "contents": "Title: Are Generative Classifiers More Robust to Adversarial Attacks? Abstract: There is a rising interest in studying the robustness of deep neural network\nclassifiers against adversaries, with both advanced attack and defence\ntechniques being actively developed. However, most recent work focuses on\ndiscriminative classifiers, which only model the conditional distribution of\nthe labels given the inputs. In this paper, we propose and investigate the deep\nBayes classifier, which improves classical naive Bayes with conditional deep\ngenerative models. We further develop detection methods for adversarial\nexamples, which reject inputs with low likelihood under the generative model.\nExperimental results suggest that deep Bayes classifiers are more robust than\ndeep discriminative classifiers, and that the proposed detection methods are\neffective against many recently proposed attacks. \n\n"}
{"id": "1802.06640", "contents": "Title: Finding Influential Training Samples for Gradient Boosted Decision Trees Abstract: We address the problem of finding influential training samples for a\nparticular case of tree ensemble-based models, e.g., Random Forest (RF) or\nGradient Boosted Decision Trees (GBDT). A natural way of formalizing this\nproblem is studying how the model's predictions change upon leave-one-out\nretraining, leaving out each individual training sample. Recent work has shown\nthat, for parametric models, this analysis can be conducted in a\ncomputationally efficient way. We propose several ways of extending this\nframework to non-parametric GBDT ensembles under the assumption that tree\nstructures remain fixed. Furthermore, we introduce a general scheme of\nobtaining further approximations to our method that balance the trade-off\nbetween performance and computational complexity. We evaluate our approaches on\nvarious experimental setups and use-case scenarios and demonstrate both the\nquality of our approach to finding influential training samples in comparison\nto the baselines and its computational efficiency. \n\n"}
{"id": "1802.06765", "contents": "Title: Interpretable VAEs for nonlinear group factor analysis Abstract: Deep generative models have recently yielded encouraging results in producing\nsubjectively realistic samples of complex data. Far less attention has been\npaid to making these generative models interpretable. In many scenarios,\nranging from scientific applications to finance, the observed variables have a\nnatural grouping. It is often of interest to understand systems of interaction\namongst these groups, and latent factor models (LFMs) are an attractive\napproach. However, traditional LFMs are limited by assuming a linear\ncorrelation structure. We present an output interpretable VAE (oi-VAE) for\ngrouped data that models complex, nonlinear latent-to-observed relationships.\nWe combine a structured VAE comprised of group-specific generators with a\nsparsity-inducing prior. We demonstrate that oi-VAE yields meaningful notions\nof interpretability in the analysis of motion capture and MEG data. We further\nshow that in these situations, the regularization inherent to oi-VAE can\nactually lead to improved generalization and learned generative processes. \n\n"}
{"id": "1802.07008", "contents": "Title: Segmentation hi\\'erarchique faiblement supervis\\'ee Abstract: Image segmentation is the process of partitioning an image into a set of\nmeaningful regions according to some criteria. Hierarchical segmentation has\nemerged as a major trend in this regard as it favors the emergence of important\nregions at different scales. On the other hand, many methods allow us to have\nprior information on the position of structures of interest in the images. In\nthis paper, we present a versatile hierarchical segmentation method that takes\ninto account any prior spatial information and outputs a hierarchical\nsegmentation that emphasizes the contours or regions of interest while\npreserving the important structures in the image. An application of this method\nto the weakly-supervised segmentation problem is presented. \n\n"}
{"id": "1802.07044", "contents": "Title: The Description Length of Deep Learning Models Abstract: Solomonoff's general theory of inference and the Minimum Description Length\nprinciple formalize Occam's razor, and hold that a good model of data is a\nmodel that is good at losslessly compressing the data, including the cost of\ndescribing the model itself. Deep neural networks might seem to go against this\nprinciple given the large number of parameters to be encoded.\n  We demonstrate experimentally the ability of deep neural networks to compress\nthe training data even when accounting for parameter encoding. The compression\nviewpoint originally motivated the use of variational methods in neural\nnetworks. Unexpectedly, we found that these variational methods provide\nsurprisingly poor compression bounds, despite being explicitly built to\nminimize such bounds. This might explain the relatively poor practical\nperformance of variational methods in deep learning. On the other hand, simple\nincremental encoding methods yield excellent compression values on deep\nnetworks, vindicating Solomonoff's approach. \n\n"}
{"id": "1802.07417", "contents": "Title: Breaking the gridlock in Mixture-of-Experts: Consistent and Efficient\n  Algorithms Abstract: Mixture-of-Experts (MoE) is a widely popular model for ensemble learning and\nis a basic building block of highly successful modern neural networks as well\nas a component in Gated Recurrent Units (GRU) and Attention networks. However,\npresent algorithms for learning MoE including the EM algorithm, and gradient\ndescent are known to get stuck in local optima. From a theoretical viewpoint,\nfinding an efficient and provably consistent algorithm to learn the parameters\nremains a long standing open problem for more than two decades. In this paper,\nwe introduce the first algorithm that learns the true parameters of a MoE model\nfor a wide class of non-linearities with global consistency guarantees. While\nexisting algorithms jointly or iteratively estimate the expert parameters and\nthe gating paramters in the MoE, we propose a novel algorithm that breaks the\ndeadlock and can directly estimate the expert parameters by sensing its echo in\na carefully designed cross-moment tensor between the inputs and the output.\nOnce the experts are known, the recovery of gating parameters still requires an\nEM algorithm; however, we show that the EM algorithm for this simplified\nproblem, unlike the joint EM algorithm, converges to the true parameters. We\nempirically validate our algorithm on both the synthetic and real data sets in\na variety of settings, and show superior performance to standard baselines. \n\n"}
{"id": "1802.08735", "contents": "Title: A DIRT-T Approach to Unsupervised Domain Adaptation Abstract: Domain adaptation refers to the problem of leveraging labeled data in a\nsource domain to learn an accurate model in a target domain where labels are\nscarce or unavailable. A recent approach for finding a common representation of\nthe two domains is via domain adversarial training (Ganin & Lempitsky, 2015),\nwhich attempts to induce a feature extractor that matches the source and target\nfeature distributions in some feature space. However, domain adversarial\ntraining faces two critical limitations: 1) if the feature extraction function\nhas high-capacity, then feature distribution matching is a weak constraint, 2)\nin non-conservative domain adaptation (where no single classifier can perform\nwell in both the source and target domains), training the model to do well on\nthe source domain hurts performance on the target domain. In this paper, we\naddress these issues through the lens of the cluster assumption, i.e., decision\nboundaries should not cross high-density data regions. We propose two novel and\nrelated models: 1) the Virtual Adversarial Domain Adaptation (VADA) model,\nwhich combines domain adversarial training with a penalty term that punishes\nthe violation the cluster assumption; 2) the Decision-boundary Iterative\nRefinement Training with a Teacher (DIRT-T) model, which takes the VADA model\nas initialization and employs natural gradient steps to further minimize the\ncluster assumption violation. Extensive empirical results demonstrate that the\ncombination of these two models significantly improve the state-of-the-art\nperformance on the digit, traffic sign, and Wi-Fi recognition domain adaptation\nbenchmarks. \n\n"}
{"id": "1802.08770", "contents": "Title: A Walk with SGD Abstract: We present novel empirical observations regarding how stochastic gradient\ndescent (SGD) navigates the loss landscape of over-parametrized deep neural\nnetworks (DNNs). These observations expose the qualitatively different roles of\nlearning rate and batch-size in DNN optimization and generalization.\nSpecifically we study the DNN loss surface along the trajectory of SGD by\ninterpolating the loss surface between parameters from consecutive\n\\textit{iterations} and tracking various metrics during training. We find that\nthe loss interpolation between parameters before and after each training\niteration's update is roughly convex with a minimum (\\textit{valley floor}) in\nbetween for most of the training. Based on this and other metrics, we deduce\nthat for most of the training update steps, SGD moves in valley like regions of\nthe loss surface by jumping from one valley wall to another at a height above\nthe valley floor. This 'bouncing between walls at a height' mechanism helps SGD\ntraverse larger distance for small batch sizes and large learning rates which\nwe find play qualitatively different roles in the dynamics. While a large\nlearning rate maintains a large height from the valley floor, a small batch\nsize injects noise facilitating exploration. We find this mechanism is crucial\nfor generalization because the valley floor has barriers and this exploration\nabove the valley floor allows SGD to quickly travel far away from the\ninitialization point (without being affected by barriers) and find flatter\nregions, corresponding to better generalization. \n\n"}
{"id": "1802.08880", "contents": "Title: Asynchronous Stochastic Proximal Methods for Nonconvex Nonsmooth\n  Optimization Abstract: We study stochastic algorithms for solving nonconvex optimization problems\nwith a convex yet possibly nonsmooth regularizer, which find wide applications\nin many practical machine learning applications. However, compared to\nasynchronous parallel stochastic gradient descent (AsynSGD), an algorithm\ntargeting smooth optimization, the understanding of the behavior of stochastic\nalgorithms for nonsmooth regularized optimization problems is limited,\nespecially when the objective function is nonconvex. To fill this theoretical\ngap, in this paper, we propose and analyze asynchronous parallel stochastic\nproximal gradient (Asyn-ProxSGD) methods for nonconvex problems. We establish\nan ergodic convergence rate of $O(1/\\sqrt{K})$ for the proposed Asyn-ProxSGD,\nwhere $K$ is the number of updates made on the model, matching the convergence\nrate currently known for AsynSGD (for smooth problems). To our knowledge, this\nis the first work that provides convergence rates of asynchronous parallel\nProxSGD algorithms for nonconvex problems. Furthermore, our results are also\nthe first to show the convergence of any stochastic proximal methods without\nassuming an increasing batch size or the use of additional variance reduction\ntechniques. We implement the proposed algorithms on Parameter Server and\ndemonstrate its convergence behavior and near-linear speedup, as the number of\nworkers increases, on two real-world datasets. \n\n"}
{"id": "1802.09129", "contents": "Title: Multi-Evidence Filtering and Fusion for Multi-Label Classification,\n  Object Detection and Semantic Segmentation Based on Weakly Supervised\n  Learning Abstract: Supervised object detection and semantic segmentation require object or even\npixel level annotations. When there exist image level labels only, it is\nchallenging for weakly supervised algorithms to achieve accurate predictions.\nThe accuracy achieved by top weakly supervised algorithms is still\nsignificantly lower than their fully supervised counterparts. In this paper, we\npropose a novel weakly supervised curriculum learning pipeline for multi-label\nobject recognition, detection and semantic segmentation. In this pipeline, we\nfirst obtain intermediate object localization and pixel labeling results for\nthe training images, and then use such results to train task-specific deep\nnetworks in a fully supervised manner. The entire process consists of four\nstages, including object localization in the training images, filtering and\nfusing object instances, pixel labeling for the training images, and\ntask-specific network training. To obtain clean object instances in the\ntraining images, we propose a novel algorithm for filtering, fusing and\nclassifying object instances collected from multiple solution mechanisms. In\nthis algorithm, we incorporate both metric learning and density-based\nclustering to filter detected object instances. Experiments show that our\nweakly supervised pipeline achieves state-of-the-art results in multi-label\nimage classification as well as weakly supervised object detection and very\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\nPASCAL VOC 2007 and PASCAL VOC 2012. \n\n"}
{"id": "1802.09564", "contents": "Title: Reinforcement and Imitation Learning for Diverse Visuomotor Skills Abstract: We propose a model-free deep reinforcement learning method that leverages a\nsmall amount of demonstration data to assist a reinforcement learning agent. We\napply this approach to robotic manipulation tasks and train end-to-end\nvisuomotor policies that map directly from RGB camera inputs to joint\nvelocities. We demonstrate that our approach can solve a wide variety of\nvisuomotor tasks, for which engineering a scripted controller would be\nlaborious. In experiments, our reinforcement and imitation agent achieves\nsignificantly better performances than agents trained with reinforcement\nlearning or imitation learning alone. We also illustrate that these policies,\ntrained with large visual and dynamics variations, can achieve preliminary\nsuccesses in zero-shot sim2real transfer. A brief visual description of this\nwork can be viewed in https://youtu.be/EDl8SQUNjj0 \n\n"}
{"id": "1802.09714", "contents": "Title: Robust Actor-Critic Contextual Bandit for Mobile Health (mHealth)\n  Interventions Abstract: We consider the actor-critic contextual bandit for the mobile health\n(mHealth) intervention. State-of-the-art decision-making algorithms generally\nignore the outliers in the dataset. In this paper, we propose a novel robust\ncontextual bandit method for the mHealth. It can achieve the conflicting goal\nof reducing the influence of outliers while seeking for a similar solution\ncompared with the state-of-the-art contextual bandit methods on the datasets\nwithout outliers. Such performance relies on two technologies: (1) the\ncapped-$\\ell_{2}$ norm; (2) a reliable method to set the thresholding\nhyper-parameter, which is inspired by one of the most fundamental techniques in\nthe statistics. Although the model is non-convex and non-differentiable, we\npropose an effective reweighted algorithm and provide solid theoretical\nanalyses. We prove that the proposed algorithm can find sufficiently decreasing\npoints after each iteration and finally converges after a finite number of\niterations. Extensive experiment results on two datasets demonstrate that our\nmethod can achieve almost identical results compared with state-of-the-art\ncontextual bandit methods on the dataset without outliers, and significantly\noutperform those state-of-the-art methods on the badly noised dataset with\noutliers in a variety of parameter settings. \n\n"}
{"id": "1802.09756", "contents": "Title: Real-Time Bidding with Multi-Agent Reinforcement Learning in Display\n  Advertising Abstract: Real-time advertising allows advertisers to bid for each impression for a\nvisiting user. To optimize specific goals such as maximizing revenue and return\non investment (ROI) led by ad placements, advertisers not only need to estimate\nthe relevance between the ads and user's interests, but most importantly\nrequire a strategic response with respect to other advertisers bidding in the\nmarket. In this paper, we formulate bidding optimization with multi-agent\nreinforcement learning. To deal with a large number of advertisers, we propose\na clustering method and assign each cluster with a strategic bidding agent. A\npractical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed\nand implemented to balance the tradeoff between the competition and cooperation\namong advertisers. The empirical study on our industry-scaled real-world data\nhas demonstrated the effectiveness of our methods. Our results show\ncluster-based bidding would largely outperform single-agent and bandit\napproaches, and the coordinated bidding achieves better overall objectives than\npurely self-interested bidding agents. \n\n"}
{"id": "1802.09850", "contents": "Title: Solving Inverse Computational Imaging Problems using Deep Pixel-level\n  Prior Abstract: Signal reconstruction is a challenging aspect of computational imaging as it\noften involves solving ill-posed inverse problems. Recently, deep feed-forward\nneural networks have led to state-of-the-art results in solving various inverse\nimaging problems. However, being task specific, these networks have to be\nlearned for each inverse problem. On the other hand, a more flexible approach\nwould be to learn a deep generative model once and then use it as a signal\nprior for solving various inverse problems. We show that among the various\nstate of the art deep generative models, autoregressive models are especially\nsuitable for our purpose for the following reasons. First, they explicitly\nmodel the pixel level dependencies and hence are capable of reconstructing\nlow-level details such as texture patterns and edges better. Second, they\nprovide an explicit expression for the image prior which can then be used for\nMAP based inference along with the forward model. Third, they can model long\nrange dependencies in images which make them ideal for handling global\nmultiplexing as encountered in various compressive imaging systems. We\ndemonstrate the efficacy of our proposed approach in solving three\ncomputational imaging problems: Single Pixel Camera (SPC), LiSens and FlatCam.\nFor both real and simulated cases, we obtain better reconstructions than the\nstate-of-the-art methods in terms of perceptual and quantitative metrics. \n\n"}
{"id": "1802.10172", "contents": "Title: Semi-Supervised Learning Enabled by Multiscale Deep Neural Network\n  Inversion Abstract: Deep Neural Networks (DNNs) provide state-of-the-art solutions in several\ndifficult machine perceptual tasks. However, their performance relies on the\navailability of a large set of labeled training data, which limits the breadth\nof their applicability. Hence, there is a need for new {\\em semi-supervised\nlearning} methods for DNNs that can leverage both (a small amount of) labeled\nand unlabeled training data. In this paper, we develop a general loss function\nenabling DNNs of any topology to be trained in a semi-supervised manner without\nextra hyper-parameters. As opposed to current semi-supervised techniques based\non topology-specific or unstable approaches, ours is both robust and general.\nWe demonstrate that our approach reaches state-of-the-art performance on the\nSVHN ($9.82\\%$ test error, with $500$ labels and wide Resnet) and CIFAR10\n(16.38% test error, with 8000 labels and sigmoid convolutional neural network)\ndata sets. \n\n"}
{"id": "1802.10592", "contents": "Title: Model-Ensemble Trust-Region Policy Optimization Abstract: Model-free reinforcement learning (RL) methods are succeeding in a growing\nnumber of tasks, aided by recent advances in deep learning. However, they tend\nto suffer from high sample complexity, which hinders their use in real-world\ndomains. Alternatively, model-based reinforcement learning promises to reduce\nsample complexity, but tends to require careful tuning and to date have\nsucceeded mainly in restrictive domains where simple models are sufficient for\nlearning. In this paper, we analyze the behavior of vanilla model-based\nreinforcement learning methods when deep neural networks are used to learn both\nthe model and the policy, and show that the learned policy tends to exploit\nregions where insufficient data is available for the model to be learned,\ncausing instability in training. To overcome this issue, we propose to use an\nensemble of models to maintain the model uncertainty and regularize the\nlearning process. We further show that the use of likelihood ratio derivatives\nyields much more stable learning than backpropagation through time. Altogether,\nour approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO)\nsignificantly reduces the sample complexity compared to model-free deep RL\nmethods on challenging continuous control benchmark tasks. \n\n"}
{"id": "1803.00186", "contents": "Title: Smoothed analysis for low-rank solutions to semidefinite programs in\n  quadratic penalty form Abstract: Semidefinite programs (SDP) are important in learning and combinatorial\noptimization with numerous applications. In pursuit of low-rank solutions and\nlow complexity algorithms, we consider the Burer--Monteiro factorization\napproach for solving SDPs. We show that all approximate local optima are global\noptima for the penalty formulation of appropriately rank-constrained SDPs as\nlong as the number of constraints scales sub-quadratically with the desired\nrank of the optimal solution. Our result is based on a simple penalty function\nformulation of the rank-constrained SDP along with a smoothed analysis to avoid\nworst-case cost matrices. We particularize our results to two applications,\nnamely, Max-Cut and matrix completion. \n\n"}
{"id": "1803.00195", "contents": "Title: The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of\n  Escaping from Sharp Minima and Regularization Effects Abstract: Understanding the behavior of stochastic gradient descent (SGD) in the\ncontext of deep neural networks has raised lots of concerns recently. Along\nthis line, we study a general form of gradient based optimization dynamics with\nunbiased noise, which unifies SGD and standard Langevin dynamics. Through\ninvestigating this general optimization dynamics, we analyze the behavior of\nSGD on escaping from minima and its regularization effects. A novel indicator\nis derived to characterize the efficiency of escaping from minima through\nmeasuring the alignment of noise covariance and the curvature of loss function.\nBased on this indicator, two conditions are established to show which type of\nnoise structure is superior to isotropic noise in term of escaping efficiency.\nWe further show that the anisotropic noise in SGD satisfies the two conditions,\nand thus helps to escape from sharp and poor minima effectively, towards more\nstable and flat minima that typically generalize well. We systematically design\nvarious experiments to verify the benefits of the anisotropic noise, compared\nwith full gradient descent plus isotropic diffusion (i.e. Langevin dynamics). \n\n"}
{"id": "1803.00204", "contents": "Title: Scalar Quantization as Sparse Least Square Optimization Abstract: Quantization can be used to form new vectors/matrices with shared values\nclose to the original. In recent years, the popularity of scalar quantization\nfor value-sharing applications has been soaring as it has been found huge\nutilities in reducing the complexity of neural networks. Existing\nclustering-based quantization techniques, while being well-developed, have\nmultiple drawbacks including the dependency of the random seed, empty or\nout-of-the-range clusters, and high time complexity for a large number of\nclusters. To overcome these problems, in this paper, the problem of scalar\nquantization is examined from a new perspective, namely sparse least square\noptimization. Specifically, inspired by the property of sparse least square\nregression, several quantization algorithms based on $l_1$ least square are\nproposed. In addition, similar schemes with $l_1 + l_2$ and $l_0$\nregularization are proposed. Furthermore, to compute quantization results with\na given amount of values/clusters, this paper designed an iterative method and\na clustering-based method, and both of them are built on sparse least square.\nThe paper shows that the latter method is mathematically equivalent to an\nimproved version of k-means clustering-based quantization algorithm, although\nthe two algorithms originated from different intuitions. The algorithms\nproposed were tested with three types of data and their computational\nperformances, including information loss, time consumption, and the\ndistribution of the values of the sparse vectors, were compared and analyzed.\nThe paper offers a new perspective to probe the area of quantization, and the\nalgorithms proposed can outperform existing methods especially under some\nbit-width reduction scenarios, when the required post-quantization resolution\n(number of values) is not significantly lower than the original number. \n\n"}
{"id": "1803.00838", "contents": "Title: A multi-instance deep neural network classifier: application to Higgs\n  boson CP measurement Abstract: We investigate properties of a classifier applied to the measurements of the\nCP state of the Higgs boson in $H\\rightarrow\\tau\\tau$ decays. The problem is\nframed as binary classifier applied to individual instances. Then the prior\nknowledge that the instances belong to the same class is used to define the\nmulti-instance classifier. Its final score is calculated as multiplication of\nsingle instance scores for a given series of instances. In the paper we discuss\nproperties of such classifier, notably its dependence on the number of\ninstances in the series. This classifier exhibits very strong random dependence\non the number of epochs used for training and requires careful tuning of the\nclassification threshold. We derive formula for this optimal threshold. \n\n"}
{"id": "1803.01128", "contents": "Title: Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with\n  Adversarial Examples Abstract: Crafting adversarial examples has become an important technique to evaluate\nthe robustness of deep neural networks (DNNs). However, most existing works\nfocus on attacking the image classification problem since its input space is\ncontinuous and output space is finite.\n  In this paper, we study the much more challenging problem of crafting\nadversarial examples for sequence-to-sequence (seq2seq) models, whose inputs\nare discrete text strings and outputs have an almost infinite number of\npossibilities. To address the challenges caused by the discrete input space, we\npropose a projected gradient method combined with group lasso and gradient\nregularization. To handle the almost infinite output space, we design some\nnovel loss functions to conduct non-overlapping attack and targeted keyword\nattack. We apply our algorithm to machine translation and text summarization\ntasks, and verify the effectiveness of the proposed algorithm: by changing less\nthan 3 words, we can make seq2seq model to produce desired outputs with high\nsuccess rates. On the other hand, we recognize that, compared with the\nwell-evaluated CNN-based classifiers, seq2seq models are intrinsically more\nrobust to adversarial attacks. \n\n"}
{"id": "1803.01216", "contents": "Title: Deep Bayesian Active Semi-Supervised Learning Abstract: In many applications the process of generating label information is expensive\nand time consuming. We present a new method that combines active and\nsemi-supervised deep learning to achieve high generalization performance from a\ndeep convolutional neural network with as few known labels as possible. In a\nsetting where a small amount of labeled data as well as a large amount of\nunlabeled data is available, our method first learns the labeled data set. This\ninitialization is followed by an expectation maximization algorithm, where\nfurther training reduces classification entropy on the unlabeled data by\ntargeting a low entropy fit which is consistent with the labeled data. In\naddition the algorithm asks at a specified frequency an oracle for labels of\ndata with entropy above a certain entropy quantile. Using this active learning\ncomponent we obtain an agile labeling process that achieves high accuracy, but\nrequires only a small amount of known labels. For the MNIST dataset we report\nan error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These\nresults are obtained without employing any special network architecture or data\naugmentation. \n\n"}
{"id": "1803.02279", "contents": "Title: An End-to-End Goal-Oriented Dialog System with a Generative Natural\n  Language Response Generation Abstract: Recently advancements in deep learning allowed the development of end-to-end\ntrained goal-oriented dialog systems. Although these systems already achieve\ngood performance, some simplifications limit their usage in real-life\nscenarios.\n  In this work, we address two of these limitations: ignoring positional\ninformation and a fixed number of possible response candidates. We propose to\nuse positional encodings in the input to model the word order of the user\nutterances. Furthermore, by using a feedforward neural network, we are able to\ngenerate the output word by word and are no longer restricted to a fixed number\nof possible response candidates. Using the positional encoding, we were able to\nachieve better accuracies in the Dialog bAbI Tasks and using the feedforward\nneural network for generating the response, we were able to save computation\ntime and space consumption. \n\n"}
{"id": "1803.02815", "contents": "Title: Sever: A Robust Meta-Algorithm for Stochastic Optimization Abstract: In high dimensions, most machine learning methods are brittle to even a small\nfraction of structured outliers. To address this, we introduce a new\nmeta-algorithm that can take in a base learner such as least squares or\nstochastic gradient descent, and harden the learner to be resistant to\noutliers. Our method, Sever, possesses strong theoretical guarantees yet is\nalso highly scalable -- beyond running the base learner itself, it only\nrequires computing the top singular vector of a certain $n \\times d$ matrix. We\napply Sever on a drug design dataset and a spam classification dataset, and\nfind that in both cases it has substantially greater robustness than several\nbaselines. On the spam dataset, with $1\\%$ corruptions, we achieved $7.4\\%$\ntest error, compared to $13.4\\%-20.5\\%$ for the baselines, and $3\\%$ error on\nthe uncorrupted dataset. Similarly, on the drug design dataset, with $10\\%$\ncorruptions, we achieved $1.42$ mean-squared error test error, compared to\n$1.51$-$2.33$ for the baselines, and $1.23$ error on the uncorrupted dataset. \n\n"}
{"id": "1803.03254", "contents": "Title: GONet: A Semi-Supervised Deep Learning Approach For Traversability\n  Estimation Abstract: We present semi-supervised deep learning approaches for traversability\nestimation from fisheye images. Our method, GONet, and the proposed extensions\nleverage Generative Adversarial Networks (GANs) to effectively predict whether\nthe area seen in the input image(s) is safe for a robot to traverse. These\nmethods are trained with many positive images of traversable places, but just a\nsmall set of negative images depicting blocked and unsafe areas. This makes the\nproposed methods practical. Positive examples can be collected easily by simply\noperating a robot through traversable spaces, while obtaining negative examples\nis time consuming, costly, and potentially dangerous. Through extensive\nexperiments and several demonstrations, we show that the proposed\ntraversability estimation approaches are robust and can generalize to unseen\nscenarios. Further, we demonstrate that our methods are memory efficient and\nfast, allowing for real-time operation on a mobile robot with single or stereo\nfisheye cameras. As part of our contributions, we open-source two new datasets\nfor traversability estimation. These datasets are composed of approximately 24h\nof videos from more than 25 indoor environments. Our methods outperform\nbaseline approaches for traversability estimation on these new datasets. \n\n"}
{"id": "1803.03370", "contents": "Title: Expert Finding in Heterogeneous Bibliographic Networks with\n  Locally-trained Embeddings Abstract: Expert finding is an important task in both industry and academia. It is\nchallenging to rank candidates with appropriate expertise for various queries.\nIn addition, different types of objects interact with one another, which\nnaturally forms heterogeneous information networks. We study the task of expert\nfinding in heterogeneous bibliographical networks based on two aspects: textual\ncontent analysis and authority ranking. Regarding the textual content analysis,\nwe propose a new method for query expansion via locally-trained embedding\nlearning with concept hierarchy as guidance, which is particularly tailored for\nspecific queries with narrow semantic meanings. Compared with global embedding\nlearning, locally-trained embedding learning projects the terms into a latent\nsemantic space constrained on relevant topics, therefore it preserves more\nprecise and subtle information for specific queries. Considering the candidate\nranking, the heterogeneous information network structure, while being largely\nignored in the previous studies of expert finding, provides additional\ninformation. Specifically, different types of interactions among objects play\ndifferent roles. We propose a ranking algorithm to estimate the authority of\nobjects in the network, treating each strongly-typed edge type individually. To\ndemonstrate the effectiveness of the proposed framework, we apply the proposed\nmethod to a large-scale bibliographical dataset with over two million entries\nand one million researcher candidates. The experiment results show that the\nproposed framework outperforms existing methods for both general and specific\nqueries. \n\n"}
{"id": "1803.03585", "contents": "Title: The Importance of Being Recurrent for Modeling Hierarchical Structure Abstract: Recent work has shown that recurrent neural networks (RNNs) can implicitly\ncapture and exploit hierarchical information when trained to solve common\nnatural language processing tasks such as language modeling (Linzen et al.,\n2016) and neural machine translation (Shi et al., 2016). In contrast, the\nability to model structured data with non-recurrent neural networks has\nreceived little attention despite their success in many NLP tasks (Gehring et\nal., 2017; Vaswani et al., 2017). In this work, we compare the two\narchitectures---recurrent versus non-recurrent---with respect to their ability\nto model hierarchical structure and find that recurrency is indeed important\nfor this purpose. \n\n"}
{"id": "1803.03607", "contents": "Title: On Generation of Adversarial Examples using Convex Programming Abstract: It has been observed that deep learning architectures tend to make erroneous\ndecisions with high reliability for particularly designed adversarial\ninstances. In this work, we show that the perturbation analysis of these\narchitectures provides a framework for generating adversarial instances by\nconvex programming which, for classification tasks, is able to recover variants\nof existing non-adaptive adversarial methods. The proposed framework can be\nused for the design of adversarial noise under various desirable constraints\nand different types of networks. Moreover, this framework is capable of\nexplaining various existing adversarial methods and can be used to derive new\nalgorithms as well. We make use of these results to obtain novel algorithms.\nThe experiments show the competitive performance of the obtained solutions, in\nterms of fooling ratio, when benchmarked with well-known adversarial methods. \n\n"}
{"id": "1803.04742", "contents": "Title: VERSE: Versatile Graph Embeddings from Similarity Measures Abstract: Embedding a web-scale information network into a low-dimensional vector space\nfacilitates tasks such as link prediction, classification, and visualization.\nPast research has addressed the problem of extracting such embeddings by\nadopting methods from words to graphs, without defining a clearly\ncomprehensible graph-related objective. Yet, as we show, the objectives used in\npast works implicitly utilize similarity measures among graph nodes.\n  In this paper, we carry the similarity orientation of previous works to its\nlogical conclusion; we propose VERtex Similarity Embeddings (VERSE), a simple,\nversatile, and memory-efficient method that derives graph embeddings explicitly\ncalibrated to preserve the distributions of a selected vertex-to-vertex\nsimilarity measure. VERSE learns such embeddings by training a single-layer\nneural network. While its default, scalable version does so via sampling\nsimilarity information, we also develop a variant using the full information\nper vertex. Our experimental study on standard benchmarks and real-world\ndatasets demonstrates that VERSE, instantiated with diverse similarity\nmeasures, outperforms state-of-the-art methods in terms of precision and recall\nin major data mining tasks and supersedes them in time and space efficiency,\nwhile the scalable sampling-based variant achieves equally good results as the\nnon-scalable full variant. \n\n"}
{"id": "1803.05928", "contents": "Title: RankME: Reliable Human Ratings for Natural Language Generation Abstract: Human evaluation for natural language generation (NLG) often suffers from\ninconsistent user ratings. While previous research tends to attribute this\nproblem to individual user preferences, we show that the quality of human\njudgements can also be improved by experimental design. We present a novel\nrank-based magnitude estimation method (RankME), which combines the use of\ncontinuous scales and relative assessments. We show that RankME significantly\nimproves the reliability and consistency of human ratings compared to\ntraditional evaluation methods. In addition, we show that it is possible to\nevaluate NLG systems according to multiple, distinct criteria, which is\nimportant for error analysis. Finally, we demonstrate that RankME, in\ncombination with Bayesian estimation of system quality, is a cost-effective\nalternative for ranking multiple NLG systems. \n\n"}
{"id": "1803.06969", "contents": "Title: Comparing Dynamics: Deep Neural Networks versus Glassy Systems Abstract: We analyze numerically the training dynamics of deep neural networks (DNN) by\nusing methods developed in statistical physics of glassy systems. The two main\nissues we address are (1) the complexity of the loss landscape and of the\ndynamics within it, and (2) to what extent DNNs share similarities with glassy\nsystems. Our findings, obtained for different architectures and datasets,\nsuggest that during the training process the dynamics slows down because of an\nincreasingly large number of flat directions. At large times, when the loss is\napproaching zero, the system diffuses at the bottom of the landscape. Despite\nsome similarities with the dynamics of mean-field glassy systems, in\nparticular, the absence of barrier crossing, we find distinctive dynamical\nbehaviors in the two cases, showing that the statistical properties of the\ncorresponding loss and energy landscapes are different. In contrast, when the\nnetwork is under-parametrized we observe a typical glassy behavior, thus\nsuggesting the existence of different phases depending on whether the network\nis under-parametrized or over-parametrized. \n\n"}
{"id": "1803.07055", "contents": "Title: Simple random search provides a competitive approach to reinforcement\n  learning Abstract: A common belief in model-free reinforcement learning is that methods based on\nrandom search in the parameter space of policies exhibit significantly worse\nsample complexity than those that explore the space of actions. We dispel such\nbeliefs by introducing a random search method for training static, linear\npolicies for continuous control problems, matching state-of-the-art sample\nefficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a\nnearly optimal controller for a challenging instance of the Linear Quadratic\nRegulator, a classical problem in control theory, when the dynamics are not\nknown. Computationally, our random search algorithm is at least 15 times more\nefficient than the fastest competing model-free methods on these benchmarks. We\ntake advantage of this computational efficiency to evaluate the performance of\nour method over hundreds of random seeds and many different hyperparameter\nconfigurations for each benchmark task. Our simulations highlight a high\nvariability in performance in these benchmark tasks, suggesting that commonly\nused estimations of sample efficiency do not adequately evaluate the\nperformance of RL algorithms. \n\n"}
{"id": "1803.07225", "contents": "Title: Monte Carlo Information Geometry: The dually flat case Abstract: Exponential families and mixture families are parametric probability models\nthat can be geometrically studied as smooth statistical manifolds with respect\nto any statistical divergence like the Kullback-Leibler (KL) divergence or the\nHellinger divergence. When equipping a statistical manifold with the KL\ndivergence, the induced manifold structure is dually flat, and the KL\ndivergence between distributions amounts to an equivalent Bregman divergence on\ntheir corresponding parameters. In practice, the corresponding Bregman\ngenerators of mixture/exponential families require to perform definite integral\ncalculus that can either be too time-consuming (for exponentially large\ndiscrete support case) or even do not admit closed-form formula (for continuous\nsupport case). In these cases, the dually flat construction remains theoretical\nand cannot be used by information-geometric algorithms. To bypass this problem,\nwe consider performing stochastic Monte Carlo (MC) estimation of those\nintegral-based mixture/exponential family Bregman generators. We show that,\nunder natural assumptions, these MC generators are almost surely Bregman\ngenerators. We define a series of dually flat information geometries, termed\nMonte Carlo Information Geometries, that increasingly-finely approximate the\nuntractable geometry. The advantage of this MCIG is that it allows a practical\nuse of the Bregman algorithmic toolbox on a wide range of probability\ndistribution families. We demonstrate our approach with a clustering task on a\nmixture family manifold. \n\n"}
{"id": "1803.07534", "contents": "Title: Stacked Neural Networks for end-to-end ciliary motion analysis Abstract: Cilia are hairlike structures protruding from nearly every cell in the body.\nDiseases known as ciliopathies, where cilia function is disrupted, can result\nin a wide spectrum of disorders. However, most techniques for assessing ciliary\nmotion rely on manual identification and tracking of cilia; this process is\nlaborious and error-prone, and does not scale well. Even where automated\nciliary motion analysis tools exist, their applicability is limited. Here, we\npropose an end-to-end computational machine learning pipeline that\nautomatically identifies regions of cilia from videos, extracts patches of\ncilia, and classifies patients as exhibiting normal or abnormal ciliary motion.\nIn particular, we demonstrate how convolutional LSTM are able to encode complex\nfeatures while remaining sensitive enough to differentiate between a variety of\nmotion patterns. Our framework achieves 90% with only a few hundred training\nepochs. We find that the combination of segmentation and classification\nnetworks in a single pipeline yields performance comparable to existing\ncomputational pipelines, while providing the additional benefit of an\nend-to-end, fully-automated analysis toolbox for ciliary motion. \n\n"}
{"id": "1803.07679", "contents": "Title: Product Characterisation towards Personalisation: Learning Attributes\n  from Unstructured Data to Recommend Fashion Products Abstract: In this paper, we describe a solution to tackle a common set of challenges in\ne-commerce, which arise from the fact that new products are continually being\nadded to the catalogue. The challenges involve properly personalising the\ncustomer experience, forecasting demand and planning the product range. We\nargue that the foundational piece to solve all of these problems is having\nconsistent and detailed information about each product, information that is\nrarely available or consistent given the multitude of suppliers and types of\nproducts. We describe in detail the architecture and methodology implemented at\nASOS, one of the world's largest fashion e-commerce retailers, to tackle this\nproblem. We then show how this quantitative understanding of the products can\nbe leveraged to improve recommendations in a hybrid recommender system\napproach. \n\n"}
{"id": "1803.07771", "contents": "Title: $\\rho$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis Abstract: Sentiment analysis is a key component in various text mining applications.\nNumerous sentiment classification techniques, including conventional and deep\nlearning-based methods, have been proposed in the literature. In most existing\nmethods, a high-quality training set is assumed to be given. Nevertheless,\nconstructing a high-quality training set that consists of highly accurate\nlabels is challenging in real applications. This difficulty stems from the fact\nthat text samples usually contain complex sentiment representations, and their\nannotation is subjective. We address this challenge in this study by leveraging\na new labeling strategy and utilizing a two-level long short-term memory\nnetwork to construct a sentiment classifier. Lexical cues are useful for\nsentiment analysis, and they have been utilized in conventional studies. For\nexample, polar and privative words play important roles in sentiment analysis.\nA new encoding strategy, that is, $\\rho$-hot encoding, is proposed to alleviate\nthe drawbacks of one-hot encoding and thus effectively incorporate useful\nlexical cues. We compile three Chinese data sets on the basis of our label\nstrategy and proposed methodology. Experiments on the three data sets\ndemonstrate that the proposed method outperforms state-of-the-art algorithms. \n\n"}
{"id": "1803.09080", "contents": "Title: AAANE: Attention-based Adversarial Autoencoder for Multi-scale Network\n  Embedding Abstract: Network embedding represents nodes in a continuous vector space and preserves\nstructure information from the Network. Existing methods usually adopt a\n\"one-size-fits-all\" approach when concerning multi-scale structure information,\nsuch as first- and second-order proximity of nodes, ignoring the fact that\ndifferent scales play different roles in the embedding learning. In this paper,\nwe propose an Attention-based Adversarial Autoencoder Network Embedding(AAANE)\nframework, which promotes the collaboration of different scales and lets them\nvote for robust representations. The proposed AAANE consists of two components:\n1) Attention-based autoencoder effectively capture the highly non-linear\nnetwork structure, which can de-emphasize irrelevant scales during training. 2)\nAn adversarial regularization guides the autoencoder learn robust\nrepresentations by matching the posterior distribution of the latent embeddings\nto given prior distribution. This is the first attempt to introduce attention\nmechanisms to multi-scale network embedding. Experimental results on real-world\nnetworks show that our learned attention parameters are different for every\nnetwork and the proposed approach outperforms existing state-of-the-art\napproaches for network embedding. \n\n"}
{"id": "1803.09327", "contents": "Title: Stabilizing Gradients for Deep Neural Networks via Efficient SVD\n  Parameterization Abstract: Vanishing and exploding gradients are two of the main obstacles in training\ndeep neural networks, especially in capturing long range dependencies in\nrecurrent neural networks~(RNNs). In this paper, we present an efficient\nparametrization of the transition matrix of an RNN that allows us to stabilize\nthe gradients that arise in its training. Specifically, we parameterize the\ntransition matrix by its singular value decomposition(SVD), which allows us to\nexplicitly track and control its singular values. We attain efficiency by using\ntools that are common in numerical linear algebra, namely Householder\nreflectors for representing the orthogonal matrices that arise in the SVD. By\nexplicitly controlling the singular values, our proposed Spectral-RNN method\nallows us to easily solve the exploding gradient problem and we observe that it\nempirically solves the vanishing gradient issue to a large extent. We note that\nthe SVD parameterization can be used for any rectangular weight matrix, hence\nit can be easily extended to any deep neural network, such as a multi-layer\nperceptron. Theoretically, we demonstrate that our parameterization does not\nlose any expressive power, and show how it controls generalization of RNN for\nthe classification task. %, and show how it potentially makes the optimization\nprocess easier. Our extensive experimental results also demonstrate that the\nproposed framework converges faster, and has good generalization, especially in\ncapturing long range dependencies, as shown on the synthetic addition and copy\ntasks, as well as on MNIST and Penn Tree Bank data sets. \n\n"}
{"id": "1803.10172", "contents": "Title: Distributed Adaptive Sampling for Kernel Matrix Approximation Abstract: Most kernel-based methods, such as kernel or Gaussian process regression,\nkernel PCA, ICA, or $k$-means clustering, do not scale to large datasets,\nbecause constructing and storing the kernel matrix $\\mathbf{K}_n$ requires at\nleast $\\mathcal{O}(n^2)$ time and space for $n$ samples. Recent works show that\nsampling points with replacement according to their ridge leverage scores (RLS)\ngenerates small dictionaries of relevant points with strong spectral\napproximation guarantees for $\\mathbf{K}_n$. The drawback of RLS-based methods\nis that computing exact RLS requires constructing and storing the whole kernel\nmatrix. In this paper, we introduce SQUEAK, a new algorithm for kernel\napproximation based on RLS sampling that sequentially processes the dataset,\nstoring a dictionary which creates accurate kernel matrix approximations with a\nnumber of points that only depends on the effective dimension $d_{eff}(\\gamma)$\nof the dataset. Moreover since all the RLS estimations are efficiently\nperformed using only the small dictionary, SQUEAK is the first RLS sampling\nalgorithm that never constructs the whole matrix $\\mathbf{K}_n$, runs in linear\ntime $\\widetilde{\\mathcal{O}}(nd_{eff}(\\gamma)^3)$ w.r.t. $n$, and requires\nonly a single pass over the dataset. We also propose a parallel and distributed\nversion of SQUEAK that linearly scales across multiple machines, achieving\nsimilar accuracy in as little as\n$\\widetilde{\\mathcal{O}}(\\log(n)d_{eff}(\\gamma)^3)$ time. \n\n"}
{"id": "1803.10274", "contents": "Title: A Study of Clustering Techniques and Hierarchical Matrix Formats for\n  Kernel Ridge Regression Abstract: We present memory-efficient and scalable algorithms for kernel methods used\nin machine learning. Using hierarchical matrix approximations for the kernel\nmatrix the memory requirements, the number of floating point operations, and\nthe execution time are drastically reduced compared to standard dense linear\nalgebra routines. We consider both the general $\\mathcal{H}$ matrix\nhierarchical format as well as Hierarchically Semi-Separable (HSS) matrices.\nFurthermore, we investigate the impact of several preprocessing and clustering\ntechniques on the hierarchical matrix compression. Effective clustering of the\ninput leads to a ten-fold increase in efficiency of the compression. The\nalgorithms are implemented using the STRUMPACK solver library. These results\nconfirm that --- with correct tuning of the hyperparameters --- classification\nusing kernel ridge regression with the compressed matrix does not lose\nprediction accuracy compared to the exact --- not compressed --- kernel matrix\nand that our approach can be extended to $\\mathcal{O}(1M)$ datasets, for which\ncomputation with the full kernel matrix becomes prohibitively expensive. We\npresent numerical experiments in a distributed memory environment up to 1,024\nprocessors of the NERSC's Cori supercomputer using well-known datasets to the\nmachine learning community that range from dimension 8 up to 784. \n\n"}
{"id": "1803.11112", "contents": "Title: Identifying Semantic Divergences in Parallel Text without Annotations Abstract: Recognizing that even correct translations are not always semantically\nequivalent, we automatically detect meaning divergences in parallel sentence\npairs with a deep neural model of bilingual semantic similarity which can be\ntrained for any parallel corpus without any manual annotation. We show that our\nsemantic model detects divergences more accurately than models based on surface\nfeatures derived from word alignments, and that these divergences matter for\nneural machine translation. \n\n"}
{"id": "1804.00316", "contents": "Title: Completely Unsupervised Phoneme Recognition by Adversarially Learning\n  Mapping Relationships from Audio Embeddings Abstract: Unsupervised discovery of acoustic tokens from audio corpora without\nannotation and learning vector representations for these tokens have been\nwidely studied. Although these techniques have been shown successful in some\napplications such as query-by-example Spoken Term Detection (STD), the lack of\nmapping relationships between these discovered tokens and real phonemes have\nlimited the down-stream applications. This paper represents probably the first\nattempt towards the goal of completely unsupervised phoneme recognition, or\nmapping audio signals to phoneme sequences without phoneme-labeled audio data.\nThe basic idea is to cluster the embedded acoustic tokens and learn the mapping\nbetween the cluster sequences and the unknown phoneme sequences with a\nGenerative Adversarial Network (GAN). An unsupervised phoneme recognition\naccuracy of 36% was achieved in the preliminary experiments. \n\n"}
{"id": "1804.00709", "contents": "Title: Generative Adversarial Learning for Spectrum Sensing Abstract: A novel approach of training data augmentation and domain adaptation is\npresented to support machine learning applications for cognitive radio. Machine\nlearning provides effective tools to automate cognitive radio functionalities\nby reliably extracting and learning intrinsic spectrum dynamics. However, there\nare two important challenges to overcome, in order to fully utilize the machine\nlearning benefits with cognitive radios. First, machine learning requires\nsignificant amount of truthed data to capture complex channel and emitter\ncharacteristics, and train the underlying algorithm (e.g., a classifier).\nSecond, the training data that has been identified for one spectrum environment\ncannot be used for another one (e.g., after channel and emitter conditions\nchange). To address these challenges, a generative adversarial network (GAN)\nwith deep learning structures is used to 1)~generate additional synthetic\ntraining data to improve classifier accuracy, and 2) adapt training data to\nspectrum dynamics. This approach is applied to spectrum sensing by assuming\nonly limited training data without knowledge of spectrum statistics. Machine\nlearning classifiers are trained with limited, augmented and adapted training\ndata to detect signals. Results show that training data augmentation increases\nthe classifier accuracy significantly and this increase is sustained with\ndomain adaptation as spectrum conditions change. \n\n"}
{"id": "1804.01650", "contents": "Title: Jointly Detecting and Separating Singing Voice: A Multi-Task Approach Abstract: A main challenge in applying deep learning to music processing is the\navailability of training data. One potential solution is Multi-task Learning,\nin which the model also learns to solve related auxiliary tasks on additional\ndatasets to exploit their correlation. While intuitive in principle, it can be\nchallenging to identify related tasks and construct the model to optimally\nshare information between tasks. In this paper, we explore vocal activity\ndetection as an additional task to stabilise and improve the performance of\nvocal separation. Further, we identify problematic biases specific to each\ndataset that could limit the generalisation capability of separation and\ndetection models, to which our proposed approach is robust. Experiments show\nimproved performance in separation as well as vocal detection compared to\nsingle-task baselines. However, we find that the commonly used\nSignal-to-Distortion Ratio (SDR) metrics did not capture the improvement on\nnon-vocal sections, indicating the need for improved evaluation methodologies. \n\n"}
{"id": "1804.02528", "contents": "Title: ANNETT-O: An Ontology for Describing Artificial Neural Network\n  Evaluation, Topology and Training Abstract: Deep learning models, while effective and versatile, are becoming\nincreasingly complex, often including multiple overlapping networks of\narbitrary depths, multiple objectives and non-intuitive training methodologies.\nThis makes it increasingly difficult for researchers and practitioners to\ndesign, train and understand them. In this paper we present ANNETT-O, a\nmuch-needed, generic and computer-actionable vocabulary for researchers and\npractitioners to describe their deep learning configurations, training\nprocedures and experiments. The proposed ontology focuses on topological,\ntraining and evaluation aspects of complex deep neural configurations, while\nkeeping peripheral entities more succinct. Knowledge bases implementing\nANNETT-O can support a wide variety of queries, providing relevant insights to\nusers. In addition to a detailed description of the ontology, we demonstrate\nits suitability to the task via a number of hypothetical use-cases of\nincreasing complexity. \n\n"}
{"id": "1804.02763", "contents": "Title: Comparison of non-linear activation functions for deep neural networks\n  on MNIST classification task Abstract: Activation functions play a key role in neural networks so it becomes\nfundamental to understand their advantages and disadvantages in order to\nachieve better performances. This paper will first introduce common types of\nnon linear activation functions that are alternative to the well known sigmoid\nfunction and then evaluate their characteristics. Moreover deeper neural\nnetworks will be analysed because they positively influence the final\nperformances compared to shallower networks. They also strictly depend on the\nweight initialisation hence the effect of drawing weights from Gaussian and\nuniform distribution will be analysed making particular attention on how the\nnumber of incoming and outgoing connection to a node influence the whole\nnetwork. \n\n"}
{"id": "1804.03346", "contents": "Title: Learning Latent Events from Network Message Logs Abstract: We consider the problem of separating error messages generated in large\ndistributed data center networks into error events. In such networks, each\nerror event leads to a stream of messages generated by hardware and software\ncomponents affected by the event. These messages are stored in a giant message\nlog. We consider the unsupervised learning problem of identifying the\nsignatures of events that generated these messages; here, the signature of an\nerror event refers to the mixture of messages generated by the event. One of\nthe main contributions of the paper is a novel mapping of our problem which\ntransforms it into a problem of topic discovery in documents. Events in our\nproblem correspond to topics and messages in our problem correspond to words in\nthe topic discovery problem. However, there is no direct analog of documents.\nTherefore, we use a non-parametric change-point detection algorithm, which has\nlinear computational complexity in the number of messages, to divide the\nmessage log into smaller subsets called episodes, which serve as the\nequivalents of documents. After this mapping has been done, we use a well-known\nalgorithm for topic discovery, called LDA, to solve our problem. We\ntheoretically analyze the change-point detection algorithm, and show that it is\nconsistent and has low sample complexity. We also demonstrate the scalability\nof our algorithm on a real data set consisting of $97$ million messages\ncollected over a period of $15$ days, from a distributed data center network\nwhich supports the operations of a large wireless service provider. \n\n"}
{"id": "1804.03629", "contents": "Title: Probabilistic Prediction of Vehicle Semantic Intention and Motion Abstract: Accurately predicting the possible behaviors of traffic participants is an\nessential capability for future autonomous vehicles. The majority of current\nresearches fix the number of driving intentions by considering only a specific\nscenario. However, distinct driving environments usually contain various\npossible driving maneuvers. Therefore, a intention prediction method that can\nadapt to different traffic scenarios is needed. To further improve the overall\nvehicle prediction performance, motion information is usually incorporated with\nclassified intentions. As suggested in some literature, the methods that\ndirectly predict possible goal locations can achieve better performance for\nlong-term motion prediction than other approaches due to their automatic\nincorporation of environment constraints. Moreover, by obtaining the temporal\ninformation of the predicted destinations, the optimal trajectories for\npredicted vehicles as well as the desirable path for ego autonomous vehicle\ncould be easily generated. In this paper, we propose a Semantic-based Intention\nand Motion Prediction (SIMP) method, which can be adapted to any driving\nscenarios by using semantic-defined vehicle behaviors. It utilizes a\nprobabilistic framework based on deep neural network to estimate the\nintentions, final locations, and the corresponding time information for\nsurrounding vehicles. An exemplar real-world scenario was used to implement and\nexamine the proposed method. \n\n"}
{"id": "1804.04211", "contents": "Title: Evaluating Word Embedding Hyper-Parameters for Similarity and Analogy\n  Tasks Abstract: The versatility of word embeddings for various applications is attracting\nresearchers from various fields. However, the impact of hyper-parameters when\ntraining embedding model is often poorly understood. How much do\nhyper-parameters such as vector dimensions and corpus size affect the quality\nof embeddings, and how do these results translate to downstream applications?\nUsing standard embedding evaluation metrics and datasets, we conduct a study to\nempirically measure the impact of these hyper-parameters. \n\n"}
{"id": "1804.05260", "contents": "Title: ClassiNet -- Predicting Missing Features for Short-Text Classification Abstract: The fundamental problem in short-text classification is \\emph{feature\nsparseness} -- the lack of feature overlap between a trained model and a test\ninstance to be classified. We propose \\emph{ClassiNet} -- a network of\nclassifiers trained for predicting missing features in a given instance, to\novercome the feature sparseness problem. Using a set of unlabeled training\ninstances, we first learn binary classifiers as feature predictors for\npredicting whether a particular feature occurs in a given instance. Next, each\nfeature predictor is represented as a vertex $v_i$ in the ClassiNet where a\none-to-one correspondence exists between feature predictors and vertices. The\nweight of the directed edge $e_{ij}$ connecting a vertex $v_i$ to a vertex\n$v_j$ represents the conditional probability that given $v_i$ exists in an\ninstance, $v_j$ also exists in the same instance. We show that ClassiNets\ngeneralize word co-occurrence graphs by considering implicit co-occurrences\nbetween features. We extract numerous features from the trained ClassiNet to\novercome feature sparseness. In particular, for a given instance $\\vec{x}$, we\nfind similar features from ClassiNet that did not appear in $\\vec{x}$, and\nappend those features in the representation of $\\vec{x}$. Moreover, we propose\na method based on graph propagation to find features that are indirectly\nrelated to a given short-text. We evaluate ClassiNets on several benchmark\ndatasets for short-text classification. Our experimental results show that by\nusing ClassiNet, we can statistically significantly improve the accuracy in\nshort-text classification tasks, without having to use any external resources\nsuch as thesauri for finding related features. \n\n"}
{"id": "1804.05345", "contents": "Title: Data-Dependent Coresets for Compressing Neural Networks with\n  Applications to Generalization Bounds Abstract: We present an efficient coresets-based neural network compression algorithm\nthat sparsifies the parameters of a trained fully-connected neural network in a\nmanner that provably approximates the network's output. Our approach is based\non an importance sampling scheme that judiciously defines a sampling\ndistribution over the neural network parameters, and as a result, retains\nparameters of high importance while discarding redundant ones. We leverage a\nnovel, empirical notion of sensitivity and extend traditional coreset\nconstructions to the application of compressing parameters. Our theoretical\nanalysis establishes guarantees on the size and accuracy of the resulting\ncompressed network and gives rise to generalization bounds that may provide new\ninsights into the generalization properties of neural networks. We demonstrate\nthe practical effectiveness of our algorithm on a variety of neural network\nconfigurations and real-world data sets. \n\n"}
{"id": "1804.06137", "contents": "Title: SeerNet at SemEval-2018 Task 1: Domain Adaptation for Affect in Tweets Abstract: The paper describes the best performing system for the SemEval-2018 Affect in\nTweets (English) sub-tasks. The system focuses on the ordinal classification\nand regression sub-tasks for valence and emotion. For ordinal classification\nvalence is classified into 7 different classes ranging from -3 to 3 whereas\nemotion is classified into 4 different classes 0 to 3 separately for each\nemotion namely anger, fear, joy and sadness. The regression sub-tasks estimate\nthe intensity of valence and each emotion. The system performs domain\nadaptation of 4 different models and creates an ensemble to give the final\nprediction. The proposed system achieved 1st position out of 75 teams which\nparticipated in the fore-mentioned sub-tasks. We outperform the baseline model\nby margins ranging from 49.2% to 76.4%, thus, pushing the state-of-the-art\nsignificantly. \n\n"}
{"id": "1804.06188", "contents": "Title: VC-Dimension Based Generalization Bounds for Relational Learning Abstract: In many applications of relational learning, the available data can be seen\nas a sample from a larger relational structure (e.g. we may be given a small\nfragment from some social network). In this paper we are particularly concerned\nwith scenarios in which we can assume that (i) the domain elements appearing in\nthe given sample have been uniformly sampled without replacement from the\n(unknown) full domain and (ii) the sample is complete for these domain elements\n(i.e. it is the full substructure induced by these elements). Within this\nsetting, we study bounds on the error of sufficient statistics of relational\nmodels that are estimated on the available data. As our main result, we prove a\nbound based on a variant of the Vapnik-Chervonenkis dimension which is suitable\nfor relational data. \n\n"}
{"id": "1804.06546", "contents": "Title: Deep Generative Networks For Sequence Prediction Abstract: This thesis investigates unsupervised time series representation learning for\nsequence prediction problems, i.e. generating nice-looking input samples given\na previous history, for high dimensional input sequences by decoupling the\nstatic input representation from the recurrent sequence representation. We\nintroduce three models based on Generative Stochastic Networks (GSN) for\nunsupervised sequence learning and prediction. Experimental results for these\nthree models are presented on pixels of sequential handwritten digit (MNIST)\ndata, videos of low-resolution bouncing balls, and motion capture data. The\nmain contribution of this thesis is to provide evidence that GSNs are a viable\nframework to learn useful representations of complex sequential input data, and\nto suggest a new framework for deep generative models to learn complex\nsequences by decoupling static input representations from dynamic time\ndependency representations. \n\n"}
{"id": "1804.06561", "contents": "Title: A Mean Field View of the Landscape of Two-Layers Neural Networks Abstract: Multi-layer neural networks are among the most powerful models in machine\nlearning, yet the fundamental reasons for this success defy mathematical\nunderstanding. Learning a neural network requires to optimize a non-convex\nhigh-dimensional objective (risk function), a problem which is usually attacked\nusing stochastic gradient descent (SGD). Does SGD converge to a global optimum\nof the risk or only to a local optimum? In the first case, does this happen\nbecause local minima are absent, or because SGD somehow avoids them? In the\nsecond, why do local minima reached by SGD have good generalization properties?\n  In this paper we consider a simple case, namely two-layers neural networks,\nand prove that -in a suitable scaling limit- SGD dynamics is captured by a\ncertain non-linear partial differential equation (PDE) that we call\ndistributional dynamics (DD). We then consider several specific examples, and\nshow how DD can be used to prove convergence of SGD to networks with nearly\nideal generalization error. This description allows to 'average-out' some of\nthe complexities of the landscape of neural networks, and can be used to prove\na general convergence result for noisy SGD. \n\n"}
{"id": "1804.06659", "contents": "Title: NTUA-SLP at SemEval-2018 Task 3: Tracking Ironic Tweets using Ensembles\n  of Word and Character Level Attentive RNNs Abstract: In this paper we present two deep-learning systems that competed at\nSemEval-2018 Task 3 \"Irony detection in English tweets\". We design and ensemble\ntwo independent models, based on recurrent neural networks (Bi-LSTM), which\noperate at the word and character level, in order to capture both the semantic\nand syntactic information in tweets. Our models are augmented with a\nself-attention mechanism, in order to identify the most informative words. The\nembedding layer of our word-level model is initialized with word2vec word\nembeddings, pretrained on a collection of 550 million English tweets. We did\nnot utilize any handcrafted features, lexicons or external datasets as prior\ninformation and our models are trained end-to-end using back propagation on\nconstrained data. Furthermore, we provide visualizations of tweets with\nannotations for the salient tokens of the attention layer that can help to\ninterpret the inner workings of the proposed models. We ranked 2nd out of 42\nteams in Subtask A and 2nd out of 31 teams in Subtask B. However,\npost-task-completion enhancements of our models achieve state-of-the-art\nresults ranking 1st for both subtasks. \n\n"}
{"id": "1804.06776", "contents": "Title: Improving Long-Horizon Forecasts with Expectation-Biased LSTM Networks Abstract: State-of-the-art forecasting methods using Recurrent Neural Net- works (RNN)\nbased on Long-Short Term Memory (LSTM) cells have shown exceptional performance\ntargeting short-horizon forecasts, e.g given a set of predictor features,\nforecast a target value for the next few time steps in the future. However, in\nmany applica- tions, the performance of these methods decays as the forecasting\nhorizon extends beyond these few time steps. This paper aims to explore the\nchallenges of long-horizon forecasting using LSTM networks. Here, we illustrate\nthe long-horizon forecasting problem in datasets from neuroscience and energy\nsupply management. We then propose expectation-biasing, an approach motivated\nby the literature of Dynamic Belief Networks, as a solution to improve\nlong-horizon forecasting using LSTMs. We propose two LSTM ar- chitectures along\nwith two methods for expectation biasing that significantly outperforms\nstandard practice. \n\n"}
{"id": "1804.07612", "contents": "Title: Revisiting Small Batch Training for Deep Neural Networks Abstract: Modern deep neural network training is typically based on mini-batch\nstochastic gradient optimization. While the use of large mini-batches increases\nthe available computational parallelism, small batch training has been shown to\nprovide improved generalization performance and allows a significantly smaller\nmemory footprint, which might also be exploited to improve machine throughput.\n  In this paper, we review common assumptions on learning rate scaling and\ntraining duration, as a basis for an experimental comparison of test\nperformance for different mini-batch sizes. We adopt a learning rate that\ncorresponds to a constant average weight update per gradient calculation (i.e.,\nper unit cost of computation), and point out that this results in a variance of\nthe weight updates that increases linearly with the mini-batch size $m$.\n  The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet\ndatasets show that increasing the mini-batch size progressively reduces the\nrange of learning rates that provide stable convergence and acceptable test\nperformance. On the other hand, small mini-batch sizes provide more up-to-date\ngradient calculations, which yields more stable and reliable training. The best\nperformance has been consistently obtained for mini-batch sizes between $m = 2$\nand $m = 32$, which contrasts with recent work advocating the use of mini-batch\nsizes in the thousands. \n\n"}
{"id": "1804.07726", "contents": "Title: Phrase-Indexed Question Answering: A New Challenge for Scalable Document\n  Comprehension Abstract: We formalize a new modular variant of current question answering tasks by\nenforcing complete independence of the document encoder from the question\nencoder. This formulation addresses a key challenge in machine comprehension by\nrequiring a standalone representation of the document discourse. It\nadditionally leads to a significant scalability advantage since the encoding of\nthe answer candidate phrases in the document can be pre-computed and indexed\noffline for efficient retrieval. We experiment with baseline models for the new\ntask, which achieve a reasonable accuracy but significantly underperform\nunconstrained QA models. We invite the QA research community to engage in\nPhrase-Indexed Question Answering (PIQA, pika) for closing the gap. The\nleaderboard is at: nlp.cs.washington.edu/piqa \n\n"}
{"id": "1804.08053", "contents": "Title: Learning Sentence Embeddings for Coherence Modelling and Beyond Abstract: We present a novel and effective technique for performing text coherence\ntasks while facilitating deeper insights into the data. Despite obtaining\never-increasing task performance, modern deep-learning approaches to NLP tasks\noften only provide users with the final network decision and no additional\nunderstanding of the data. In this work, we show that a new type of sentence\nembedding learned through self-supervision can be applied effectively to text\ncoherence tasks while serving as a window through which deeper understanding of\nthe data can be obtained. To produce these sentence embeddings, we train a\nrecurrent neural network to take individual sentences and predict their\nlocation in a document in the form of a distribution over locations. We\ndemonstrate that these embeddings, combined with simple visual heuristics, can\nbe used to achieve performance competitive with state-of-the-art on multiple\ntext coherence tasks, outperforming more complex and specialized approaches.\nAdditionally, we demonstrate that these embeddings can provide insights useful\nto writers for improving writing quality and informing document structuring,\nand assisting readers in summarizing and locating information. \n\n"}
{"id": "1804.08130", "contents": "Title: Sparse Travel Time Estimation from Streaming Data Abstract: We address two shortcomings in online travel time estimation methods for\ncongested urban traffic. The first shortcoming is related to the determination\nof the number of mixture modes, which can change dynamically, within day and\nfrom day to day. The second shortcoming is the wide-spread use of Gaussian\nprobability densities as mixture components. Gaussian densities fail to capture\nthe positive skew in travel time distributions and, consequently, large numbers\nof mixture components are needed for reasonable fitting accuracy when applied\nas mixture components. They also assign positive probabilities to negative\ntravel times. To address these issues, this paper derives a mixture\ndistribution with Gamma component densities, which are asymmetric and supported\non the positive numbers. We use sparse estimation techniques to ensure\nparsimonious models and propose a generalization of Gamma mixture densities\nusing Mittag-Leffler functions, which provides enhanced fitting flexibility and\nimproved parsimony. In order to accommodate within-day variability and allow\nfor online implementation of the proposed methodology (i.e., fast computations\non streaming travel time data), we introduce a recursive algorithm which\nefficiently updates the fitted distribution whenever new data become available.\nExperimental results using real-world travel time data illustrate the efficacy\nof the proposed methods. \n\n"}
{"id": "1804.08198", "contents": "Title: A neural interlingua for multilingual machine translation Abstract: We incorporate an explicit neural interlingua into a multilingual\nencoder-decoder neural machine translation (NMT) architecture. We demonstrate\nthat our model learns a language-independent representation by performing\ndirect zero-shot translation (without using pivot translation), and by using\nthe source sentence embeddings to create an English Yelp review classifier\nthat, through the mediation of the neural interlingua, can also classify French\nand German reviews. Furthermore, we show that, despite using a smaller number\nof parameters than a pairwise collection of bilingual NMT models, our approach\nproduces comparable BLEU scores for each language pair in WMT15. \n\n"}
{"id": "1804.08302", "contents": "Title: Deep cross-domain building extraction for selective depth estimation\n  from oblique aerial imagery Abstract: With the technological advancements of aerial imagery and accurate 3d\nreconstruction of urban environments, more and more attention has been paid to\nthe automated analyses of urban areas. In our work, we examine two important\naspects that allow live analysis of building structures in city models given\noblique aerial imagery, namely automatic building extraction with convolutional\nneural networks (CNNs) and selective real-time depth estimation from aerial\nimagery. We use transfer learning to train the Faster R-CNN method for\nreal-time deep object detection, by combining a large ground-based dataset for\nurban scene understanding with a smaller number of images from an aerial\ndataset. We achieve an average precision (AP) of about 80% for the task of\nbuilding extraction on a selected evaluation dataset. Our evaluation focuses on\nboth dataset-specific learning and transfer learning. Furthermore, we present\nan algorithm that allows for multi-view depth estimation from aerial imagery in\nreal-time. We adopt the semi-global matching (SGM) optimization strategy to\npreserve sharp edges at object boundaries. In combination with the Faster\nR-CNN, it allows a selective reconstruction of buildings, identified with\nregions of interest (RoIs), from oblique aerial imagery. \n\n"}
{"id": "1804.08607", "contents": "Title: Benchmarking projective simulation in navigation problems Abstract: Projective simulation (PS) is a model for intelligent agents with a\ndeliberation capacity that is based on episodic memory. The model has been\nshown to provide a flexible framework for constructing reinforcement-learning\nagents, and it allows for quantum mechanical generalization, which leads to a\nspeed-up in deliberation time. PS agents have been applied successfully in the\ncontext of complex skill learning in robotics, and in the design of\nstate-of-the-art quantum experiments. In this paper, we study the performance\nof projective simulation in two benchmarking problems in navigation, namely the\ngrid world and the mountain car problem. The performance of PS is compared to\nstandard tabular reinforcement learning approaches, Q-learning and SARSA. Our\ncomparison demonstrates that the performance of PS and standard learning\napproaches are qualitatively and quantitatively similar, while it is much\neasier to choose optimal model parameters in case of projective simulation,\nwith a reduced computational effort of one to two orders of magnitude. Our\nresults show that the projective simulation model stands out for its simplicity\nin terms of the number of model parameters, which makes it simple to set up the\nlearning agent in unknown task environments. \n\n"}
{"id": "1804.08641", "contents": "Title: Quantum generative adversarial networks Abstract: Quantum machine learning is expected to be one of the first potential\ngeneral-purpose applications of near-term quantum devices. A major recent\nbreakthrough in classical machine learning is the notion of generative\nadversarial training, where the gradients of a discriminator model are used to\ntrain a separate generative model. In this work and a companion paper, we\nextend adversarial training to the quantum domain and show how to construct\ngenerative adversarial networks using quantum circuits. Furthermore, we also\nshow how to compute gradients -- a key element in generative adversarial\nnetwork training -- using another quantum circuit. We give an example of a\nsimple practical circuit ansatz to parametrize quantum machine learning models\nand perform a simple numerical experiment to demonstrate that quantum\ngenerative adversarial networks can be trained successfully. \n\n"}
{"id": "1804.09299", "contents": "Title: Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models Abstract: Neural Sequence-to-Sequence models have proven to be accurate and robust for\nmany sequence prediction tasks, and have become the standard approach for\nautomatic translation of text. The models work in a five stage blackbox process\nthat involves encoding a source sequence to a vector space and then decoding\nout to a new target sequence. This process is now standard, but like many deep\nlearning methods remains quite difficult to understand or debug. In this work,\nwe present a visual analysis tool that allows interaction with a trained\nsequence-to-sequence model through each stage of the translation process. The\naim is to identify which patterns have been learned and to detect model errors.\nWe demonstrate the utility of our tool through several real-world large-scale\nsequence-to-sequence use cases. \n\n"}
{"id": "1804.09314", "contents": "Title: Deep Learning for Predicting Asset Returns Abstract: Deep learning searches for nonlinear factors for predicting asset returns.\nPredictability is achieved via multiple layers of composite factors as opposed\nto additive ones. Viewed in this way, asset pricing studies can be revisited\nusing multi-layer deep learners, such as rectified linear units (ReLU) or\nlong-short-term-memory (LSTM) for time-series effects. State-of-the-art\nalgorithms including stochastic gradient descent (SGD), TensorFlow and dropout\ndesign provide imple- mentation and efficient factor exploration. To illustrate\nour methodology, we revisit the equity market risk premium dataset of Welch and\nGoyal (2008). We find the existence of nonlinear factors which explain\npredictability of returns, in particular at the extremes of the characteristic\nspace. Finally, we conclude with directions for future research. \n\n"}
{"id": "1804.09619", "contents": "Title: Identifying and Alleviating Concept Drift in Streaming Tensor\n  Decomposition Abstract: Tensor decompositions are used in various data mining applications from\nsocial network to medical applications and are extremely useful in discovering\nlatent structures or concepts in the data. Many real-world applications are\ndynamic in nature and so are their data. To deal with this dynamic nature of\ndata, there exist a variety of online tensor decomposition algorithms. A\ncentral assumption in all those algorithms is that the number of latent\nconcepts remains fixed throughout the entire stream. However, this need not be\nthe case. Every incoming batch in the stream may have a different number of\nlatent concepts, and the difference in latent concepts from one tensor batch to\nanother can provide insights into how our findings in a particular application\nbehave and deviate over time. In this paper, we define \"concept\" and \"concept\ndrift\" in the context of streaming tensor decomposition, as the manifestation\nof the variability of latent concepts throughout the stream. Furthermore, we\nintroduce SeekAndDestroy, an algorithm that detects concept drift in streaming\ntensor decomposition and is able to produce results robust to that drift. To\nthe best of our knowledge, this is the first work that investigates concept\ndrift in streaming tensor decomposition. We extensively evaluate SeekAndDestroy\non synthetic datasets, which exhibit a wide variety of realistic drift. Our\nexperiments demonstrate the effectiveness of SeekAndDestroy, both in the\ndetection of concept drift and in the alleviation of its effects, producing\nresults with similar quality to decomposing the entire tensor in one shot.\nAdditionally, in real datasets, SeekAndDestroy outperforms other streaming\nbaselines, while discovering novel useful components. \n\n"}
{"id": "1804.09849", "contents": "Title: The Best of Both Worlds: Combining Recent Advances in Neural Machine\n  Translation Abstract: The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)\nmodeling for Machine Translation (MT). The classic RNN-based approaches to MT\nwere first out-performed by the convolutional seq2seq model, which was then\nout-performed by the more recent Transformer model. Each of these new\napproaches consists of a fundamental architecture accompanied by a set of\nmodeling and training techniques that are in principle applicable to other\nseq2seq architectures. In this paper, we tease apart the new architectures and\ntheir accompanying techniques in two ways. First, we identify several key\nmodeling and training techniques, and apply them to the RNN architecture,\nyielding a new RNMT+ model that outperforms all of the three fundamental\narchitectures on the benchmark WMT'14 English to French and English to German\ntasks. Second, we analyze the properties of each fundamental seq2seq\narchitecture and devise new hybrid architectures intended to combine their\nstrengths. Our hybrid models obtain further improvements, outperforming the\nRNMT+ model on both benchmark datasets. \n\n"}
{"id": "1804.10140", "contents": "Title: Securing Distributed Gradient Descent in High Dimensional Statistical\n  Learning Abstract: We consider unreliable distributed learning systems wherein the training data\nis kept confidential by external workers, and the learner has to interact\nclosely with those workers to train a model. In particular, we assume that\nthere exists a system adversary that can adaptively compromise some workers;\nthe compromised workers deviate from their local designed specifications by\nsending out arbitrarily malicious messages.\n  We assume in each communication round, up to $q$ out of the $m$ workers\nsuffer Byzantine faults. Each worker keeps a local sample of size $n$ and the\ntotal sample size is $N=nm$. We propose a secured variant of the gradient\ndescent method that can tolerate up to a constant fraction of Byzantine\nworkers, i.e., $q/m = O(1)$. Moreover, we show the statistical estimation error\nof the iterates converges in $O(\\log N)$ rounds to $O(\\sqrt{q/N} +\n\\sqrt{d/N})$, where $d$ is the model dimension. As long as $q=O(d)$, our\nproposed algorithm achieves the optimal error rate $O(\\sqrt{d/N})$. Our results\nare obtained under some technical assumptions. Specifically, we assume\nstrongly-convex population risk. Nevertheless, the empirical risk (sample\nversion) is allowed to be non-convex. The core of our method is to robustly\naggregate the gradients computed by the workers based on the filtering\nprocedure proposed by Steinhardt et al. On the technical front, deviating from\nthe existing literature on robustly estimating a finite-dimensional mean\nvector, we establish a {\\em uniform} concentration of the sample covariance\nmatrix of gradients, and show that the aggregated gradient, as a function of\nmodel parameter, converges uniformly to the true gradient function. To get a\nnear-optimal uniform concentration bound, we develop a new matrix concentration\ninequality, which might be of independent interest. \n\n"}
{"id": "1804.10200", "contents": "Title: The loss landscape of overparameterized neural networks Abstract: We explore some mathematical features of the loss landscape of\noverparameterized neural networks. A priori one might imagine that the loss\nfunction looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in\nparticular, nonconvex, with discrete global minima. In this paper, we prove\nthat in at least one important way, the loss function of an overparameterized\nneural network does not look like a typical function. If a neural net has $n$\nparameters and is trained on $d$ data points, with $n>d$, we show that the\nlocus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$\ndimensional submanifold of $\\mathbb{R}^n$. In practice, neural nets commonly\nhave orders of magnitude more parameters than data points, so this observation\nimplies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. \n\n"}
{"id": "1804.10752", "contents": "Title: Syllable-Based Sequence-to-Sequence Speech Recognition with the\n  Transformer in Mandarin Chinese Abstract: Sequence-to-sequence attention-based models have recently shown very\npromising results on automatic speech recognition (ASR) tasks, which integrate\nan acoustic, pronunciation and language model into a single neural network. In\nthese models, the Transformer, a new sequence-to-sequence attention-based model\nrelying entirely on self-attention without using RNNs or convolutions, achieves\na new single-model state-of-the-art BLEU on neural machine translation (NMT)\ntasks. Since the outstanding performance of the Transformer, we extend it to\nspeech and concentrate on it as the basic architecture of sequence-to-sequence\nattention-based model on Mandarin Chinese ASR tasks. Furthermore, we\ninvestigate a comparison between syllable based model and context-independent\nphoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese.\nAdditionally, a greedy cascading decoder with the Transformer is proposed for\nmapping CI-phoneme sequences and syllable sequences into word sequences.\nExperiments on HKUST datasets demonstrate that syllable based model with the\nTransformer performs better than CI-phoneme based counterpart, and achieves a\ncharacter error rate (CER) of \\emph{$28.77\\%$}, which is competitive to the\nstate-of-the-art CER of $28.0\\%$ by the joint CTC-attention based\nencoder-decoder network. \n\n"}
{"id": "1804.10959", "contents": "Title: Subword Regularization: Improving Neural Network Translation Models with\n  Multiple Subword Candidates Abstract: Subword units are an effective way to alleviate the open vocabulary problems\nin neural machine translation (NMT). While sentences are usually converted into\nunique subword sequences, subword segmentation is potentially ambiguous and\nmultiple segmentations are possible even with the same vocabulary. The question\naddressed in this paper is whether it is possible to harness the segmentation\nambiguity as a noise to improve the robustness of NMT. We present a simple\nregularization method, subword regularization, which trains the model with\nmultiple subword segmentations probabilistically sampled during training. In\naddition, for better subword sampling, we propose a new subword segmentation\nalgorithm based on a unigram language model. We experiment with multiple\ncorpora and report consistent improvements especially on low resource and\nout-of-domain settings. \n\n"}
{"id": "1805.00108", "contents": "Title: Conditional molecular design with deep generative models Abstract: Although machine learning has been successfully used to propose novel\nmolecules that satisfy desired properties, it is still challenging to explore a\nlarge chemical space efficiently. In this paper, we present a conditional\nmolecular design method that facilitates generating new molecules with desired\nproperties. The proposed model, which simultaneously performs both property\nprediction and molecule generation, is built as a semi-supervised variational\nautoencoder trained on a set of existing molecules with only a partial\nannotation. We generate new molecules with desired properties by sampling from\nthe generative distribution estimated by the model. We demonstrate the\neffectiveness of the proposed model by evaluating it on drug-like molecules.\nThe model improves the performance of property prediction by exploiting\nunlabeled molecules, and efficiently generates novel molecules fulfilling\nvarious target conditions. \n\n"}
{"id": "1805.00915", "contents": "Title: Trainability and Accuracy of Neural Networks: An Interacting Particle\n  System Approach Abstract: Neural networks, a central tool in machine learning, have demonstrated\nremarkable, high fidelity performance on image recognition and classification\ntasks. These successes evince an ability to accurately represent high\ndimensional functions, but rigorous results about the approximation error of\nneural networks after training are few. Here we establish conditions for global\nconvergence of the standard optimization algorithm used in machine learning\napplications, stochastic gradient descent (SGD), and quantify the scaling of\nits error with the size of the network. This is done by reinterpreting SGD as\nthe evolution of a particle system with interactions governed by a potential\nrelated to the objective or \"loss\" function used to train the network. We show\nthat, when the number $n$ of units is large, the empirical distribution of the\nparticles descends on a convex landscape towards the global minimum at a rate\nindependent of $n$, with a resulting approximation error that universally\nscales as $O(n^{-1})$. These properties are established in the form of a Law of\nLarge Numbers and a Central Limit Theorem for the empirical distribution. Our\nanalysis also quantifies the scale and nature of the noise introduced by SGD\nand provides guidelines for the step size and batch size to use when training a\nneural network. We illustrate our findings on examples in which we train neural\nnetworks to learn the energy function of the continuous 3-spin model on the\nsphere. The approximation error scales as our analysis predicts in as high a\ndimension as $d=25$. \n\n"}
{"id": "1805.01033", "contents": "Title: Unsupervised Learning using Pretrained CNN and Associative Memory Bank Abstract: Deep Convolutional features extracted from a comprehensive labeled dataset,\ncontain substantial representations which could be effectively used in a new\ndomain. Despite the fact that generic features achieved good results in many\nvisual tasks, fine-tuning is required for pretrained deep CNN models to be more\neffective and provide state-of-the-art performance. Fine tuning using the\nbackpropagation algorithm in a supervised setting, is a time and resource\nconsuming process. In this paper, we present a new architecture and an approach\nfor unsupervised object recognition that addresses the above mentioned problem\nwith fine tuning associated with pretrained CNN-based supervised deep learning\napproaches while allowing automated feature extraction. Unlike existing works,\nour approach is applicable to general object recognition tasks. It uses a\npretrained (on a related domain) CNN model for automated feature extraction\npipelined with a Hopfield network based associative memory bank for storing\npatterns for classification purposes. The use of associative memory bank in our\nframework allows eliminating backpropagation while providing competitive\nperformance on an unseen dataset. \n\n"}
{"id": "1805.01053", "contents": "Title: Mean Field Analysis of Neural Networks: A Law of Large Numbers Abstract: Machine learning, and in particular neural network models, have\nrevolutionized fields such as image, text, and speech recognition. Today, many\nimportant real-world applications in these areas are driven by neural networks.\nThere are also growing applications in engineering, robotics, medicine, and\nfinance. Despite their immense success in practice, there is limited\nmathematical understanding of neural networks. This paper illustrates how\nneural networks can be studied via stochastic analysis, and develops approaches\nfor addressing some of the technical challenges which arise. We analyze\none-layer neural networks in the asymptotic regime of simultaneously (A) large\nnetwork sizes and (B) large numbers of stochastic gradient descent training\niterations. We rigorously prove that the empirical distribution of the neural\nnetwork parameters converges to the solution of a nonlinear partial\ndifferential equation. This result can be considered a law of large numbers for\nneural networks. In addition, a consequence of our analysis is that the trained\nparameters of the neural network asymptotically become independent, a property\nwhich is commonly called \"propagation of chaos\". \n\n"}
{"id": "1805.01500", "contents": "Title: Noisin: Unbiased Regularization for Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are powerful models of sequential data. They\nhave been successfully used in domains such as text and speech. However, RNNs\nare susceptible to overfitting; regularization is important. In this paper we\ndevelop Noisin, a new method for regularizing RNNs. Noisin injects random noise\ninto the hidden states of the RNN and then maximizes the corresponding marginal\nlikelihood of the data. We show how Noisin applies to any RNN and we study many\ndifferent types of noise. Noisin is unbiased--it preserves the underlying RNN\non average. We characterize how Noisin regularizes its RNN both theoretically\nand empirically. On language modeling benchmarks, Noisin improves over dropout\nby as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We\nalso compared the state-of-the-art language model of Yang et al. 2017, both\nwith and without Noisin. On the Penn Treebank, the method with Noisin more\nquickly reaches state-of-the-art performance. \n\n"}
{"id": "1805.02094", "contents": "Title: Exploring Hyper-Parameter Optimization for Neural Machine Translation on\n  GPU Architectures Abstract: Neural machine translation (NMT) has been accelerated by deep learning neural\nnetworks over statistical-based approaches, due to the plethora and\nprogrammability of commodity heterogeneous computing architectures such as\nFPGAs and GPUs and the massive amount of training corpuses generated from news\noutlets, government agencies and social media. Training a learning classifier\nfor neural networks entails tuning hyper-parameters that would yield the best\nperformance. Unfortunately, the number of parameters for machine translation\ninclude discrete categories as well as continuous options, which makes for a\ncombinatorial explosive problem. This research explores optimizing\nhyper-parameters when training deep learning neural networks for machine\ntranslation. Specifically, our work investigates training a language model with\nMarian NMT. Results compare NMT under various hyper-parameter settings across a\nvariety of modern GPU architecture generations in single node and multi-node\nsettings, revealing insights on which hyper-parameters matter most in terms of\nperformance, such as words processed per second, convergence rates, and\ntranslation accuracy, and provides insights on how to best achieve\nhigh-performing NMT systems. \n\n"}
{"id": "1805.02442", "contents": "Title: Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations Abstract: Revealing the implicit semantic relation between the constituents of a\nnoun-compound is important for many NLP applications. It has been addressed in\nthe literature either as a classification task to a set of pre-defined\nrelations or by producing free text paraphrases explicating the relations. Most\nexisting paraphrasing methods lack the ability to generalize, and have a hard\ntime interpreting infrequent or new noun-compounds. We propose a neural model\nthat generalizes better by representing paraphrases in a continuous space,\ngeneralizing for both unseen noun-compounds and rare paraphrases. Our model\nhelps improving performance on both the noun-compound paraphrasing and\nclassification tasks. \n\n"}
{"id": "1805.02896", "contents": "Title: Survey and cross-benchmark comparison of remaining time prediction\n  methods in business process monitoring Abstract: Predictive business process monitoring methods exploit historical process\nexecution logs to generate predictions about running instances (called cases)\nof a business process, such as the prediction of the outcome, next activity or\nremaining cycle time of a given process case. These insights could be used to\nsupport operational managers in taking remedial actions as business processes\nunfold, e.g. shifting resources from one case onto another to ensure this\nlatter is completed on time. A number of methods to tackle the remaining cycle\ntime prediction problem have been proposed in the literature. However, due to\ndifferences in their experimental setup, choice of datasets, evaluation\nmeasures and baselines, the relative merits of each method remain unclear. This\narticle presents a systematic literature review and taxonomy of methods for\nremaining time prediction in the context of business processes, as well as a\ncross-benchmark comparison of 16 such methods based on 16 real-life datasets\noriginating from different industry domains. \n\n"}
{"id": "1805.03278", "contents": "Title: Fully Automated Segmentation of Hyperreflective Foci in Optical\n  Coherence Tomography Images Abstract: The automatic detection of disease related entities in retinal imaging data\nis relevant for disease- and treatment monitoring. It enables the quantitative\nassessment of large amounts of data and the corresponding study of disease\ncharacteristics. The presence of hyperreflective foci (HRF) is related to\ndisease progression in various retinal diseases. Manual identification of HRF\nin spectral-domain optical coherence tomography (SD-OCT) scans is error-prone\nand tedious. We present a fully automated machine learning approach for\nsegmenting HRF in SD-OCT scans. Evaluation on annotated OCT images of the\nretina demonstrates that a residual U-Net allows to segment HRF with high\naccuracy. As our dataset comprised data from different retinal diseases\nincluding age-related macular degeneration, diabetic macular edema and retinal\nvein occlusion, the algorithm can safely be applied in all of them though\ndifferent pathophysiological origins are known. \n\n"}
{"id": "1805.04803", "contents": "Title: Zero-Shot Dialog Generation with Cross-Domain Latent Actions Abstract: This paper introduces zero-shot dialog generation (ZSDG), as a step towards\nneural dialog systems that can instantly generalize to new situations with\nminimal data. ZSDG enables an end-to-end generative dialog system to generalize\nto a new domain for which only a domain description is provided and no training\ndialogs are available. Then a novel learning framework, Action Matching, is\nproposed. This algorithm can learn a cross-domain embedding space that models\nthe semantics of dialog responses which, in turn, lets a neural dialog\ngeneration model generalize to new domains. We evaluate our methods on a new\nsynthetic dialog dataset, and an existing human-human dialog dataset. Results\nshow that our method has superior performance in learning dialog models that\nrapidly adapt their behavior to new domains and suggests promising future\nresearch. \n\n"}
{"id": "1805.04988", "contents": "Title: Word learning and the acquisition of syntactic--semantic overhypotheses Abstract: Children learning their first language face multiple problems of induction:\nhow to learn the meanings of words, and how to build meaningful phrases from\nthose words according to syntactic rules. We consider how children might solve\nthese problems efficiently by solving them jointly, via a computational model\nthat learns the syntax and semantics of multi-word utterances in a grounded\nreference game. We select a well-studied empirical case in which children are\naware of patterns linking the syntactic and semantic properties of words ---\nthat the properties picked out by base nouns tend to be related to shape, while\nprenominal adjectives tend to refer to other properties such as color. We show\nthat children applying such inductive biases are accurately reflecting the\nstatistics of child-directed speech, and that inducing similar biases in our\ncomputational model captures children's behavior in a classic adjective\nlearning experiment. Our model incorporating such biases also demonstrates a\nclear data efficiency in learning, relative to a baseline model that learns\nwithout forming syntax-sensitive overhypotheses of word meaning. Thus solving a\nmore complex joint inference problem may make the full problem of language\nacquisition easier, not harder. \n\n"}
{"id": "1805.05532", "contents": "Title: Knowledge Distillation with Adversarial Samples Supporting Decision\n  Boundary Abstract: Many recent works on knowledge distillation have provided ways to transfer\nthe knowledge of a trained network for improving the learning process of a new\none, but finding a good technique for knowledge distillation is still an open\nproblem. In this paper, we provide a new perspective based on a decision\nboundary, which is one of the most important component of a classifier. The\ngeneralization performance of a classifier is closely related to the adequacy\nof its decision boundary, so a good classifier bears a good decision boundary.\nTherefore, transferring information closely related to the decision boundary\ncan be a good attempt for knowledge distillation. To realize this goal, we\nutilize an adversarial attack to discover samples supporting a decision\nboundary. Based on this idea, to transfer more accurate information about the\ndecision boundary, the proposed algorithm trains a student classifier based on\nthe adversarial samples supporting the decision boundary. Experiments show that\nthe proposed method indeed improves knowledge distillation and achieves the\nstate-of-the-arts performance. \n\n"}
{"id": "1805.06530", "contents": "Title: Improving the Gaussian Mechanism for Differential Privacy: Analytical\n  Calibration and Optimal Denoising Abstract: The Gaussian mechanism is an essential building block used in multitude of\ndifferentially private data analysis algorithms. In this paper we revisit the\nGaussian mechanism and show that the original analysis has several important\nlimitations. Our analysis reveals that the variance formula for the original\nmechanism is far from tight in the high privacy regime ($\\varepsilon \\to 0$)\nand it cannot be extended to the low privacy regime ($\\varepsilon \\to \\infty$).\nWe address these limitations by developing an optimal Gaussian mechanism whose\nvariance is calibrated directly using the Gaussian cumulative density function\ninstead of a tail bound approximation. We also propose to equip the Gaussian\nmechanism with a post-processing step based on adaptive estimation techniques\nby leveraging that the distribution of the perturbation is known. Our\nexperiments show that analytical calibration removes at least a third of the\nvariance of the noise compared to the classical Gaussian mechanism, and that\ndenoising dramatically improves the accuracy of the Gaussian mechanism in the\nhigh-dimensional regime. \n\n"}
{"id": "1805.06846", "contents": "Title: RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant\n  Deep Networks Abstract: Explicit encoding of group actions in deep features makes it possible for\nconvolutional neural networks (CNNs) to handle global deformations of images,\nwhich is critical to success in many vision tasks. This paper proposes to\ndecompose the convolutional filters over joint steerable bases across the space\nand the group geometry simultaneously, namely a rotation-equivariant CNN with\ndecomposed convolutional filters (RotDCF). This decomposition facilitates\ncomputing the joint convolution, which is proved to be necessary for the group\nequivariance. It significantly reduces the model size and computational\ncomplexity while preserving performance, and truncation of the bases expansion\nserves implicitly to regularize the filters. On datasets involving in-plane and\nout-of-plane object rotations, RotDCF deep features demonstrate greater\nrobustness and interpretability than regular CNNs. The stability of the\nequivariant representation to input variations is also proved theoretically\nunder generic assumptions on the filters in the decomposed form. The RotDCF\nframework can be extended to groups other than rotations, providing a general\napproach which achieves both group equivariance and representation stability at\na reduced model size. \n\n"}
{"id": "1805.07731", "contents": "Title: Generating High-Quality Surface Realizations Using Data Augmentation and\n  Factored Sequence Models Abstract: This work presents a new state of the art in reconstruction of surface\nrealizations from obfuscated text. We identify the lack of sufficient training\ndata as the major obstacle to training high-performing models, and solve this\nissue by generating large amounts of synthetic training data. We also propose\npreprocessing techniques which make the structure contained in the input\nfeatures more accessible to sequence models. Our models were ranked first on\nall evaluation metrics in the English portion of the 2018 Surface Realization\nshared task. \n\n"}
{"id": "1805.07852", "contents": "Title: Accelerated Bayesian Optimization throughWeight-Prior Tuning Abstract: Bayesian optimization (BO) is a widely-used method for optimizing expensive\n(to evaluate) problems. At the core of most BO methods is the modeling of the\nobjective function using a Gaussian Process (GP) whose covariance is selected\nfrom a set of standard covariance functions. From a weight-space view, this\nmodels the objective as a linear function in a feature space implied by the\ngiven covariance K, with an arbitrary Gaussian weight prior ${\\bf w} \\sim\n\\mathcal{N} ({\\bf 0}, {\\bf I})$. In many practical applications there is data\navailable that has a similar (covariance) structure to the objective, but\nwhich, having different form, cannot be used directly in standard transfer\nlearning. In this paper we show how such auxiliary data may be used to\nconstruct a GP covariance corresponding to a more appropriate weight prior for\nthe objective function. Building on this, we show that we may accelerate BO by\nmodeling the objective function using this (learned) weight prior, which we\ndemonstrate on both test functions and a practical application to short-polymer\nfibre manufacture. \n\n"}
{"id": "1805.07932", "contents": "Title: Bilinear Attention Networks Abstract: Attention networks in multimodal learning provide an efficient way to utilize\ngiven visual information selectively. However, the computational cost to learn\nattention distributions for every pair of multimodal input channels is\nprohibitively expensive. To solve this problem, co-attention builds two\nseparate attention distributions for each modality neglecting the interaction\nbetween multimodal inputs. In this paper, we propose bilinear attention\nnetworks (BAN) that find bilinear attention distributions to utilize given\nvision-language information seamlessly. BAN considers bilinear interactions\namong two groups of input channels, while low-rank bilinear pooling extracts\nthe joint representations for each pair of channels. Furthermore, we propose a\nvariant of multimodal residual networks to exploit eight-attention maps of the\nBAN efficiently. We quantitatively and qualitatively evaluate our model on\nvisual question answering (VQA 2.0) and Flickr30k Entities datasets, showing\nthat BAN significantly outperforms previous methods and achieves new\nstate-of-the-arts on both datasets. \n\n"}
{"id": "1805.08096", "contents": "Title: Understanding Self-Paced Learning under Concave Conjugacy Theory Abstract: By simulating the easy-to-hard learning manners of humans/animals, the\nlearning regimes called curriculum learning~(CL) and self-paced learning~(SPL)\nhave been recently investigated and invoked broad interests. However, the\nintrinsic mechanism for analyzing why such learning regimes can work has not\nbeen comprehensively investigated. To this issue, this paper proposes a concave\nconjugacy theory for looking into the insight of CL/SPL. Specifically, by using\nthis theory, we prove the equivalence of the SPL regime and a latent concave\nobjective, which is closely related to the known non-convex regularized penalty\nwidely used in statistics and machine learning. Beyond the previous theory for\nexplaining CL/SPL insights, this new theoretical framework on one hand\nfacilitates two direct approaches for designing new SPL models for certain\ntasks, and on the other hand can help conduct the latent objective of\nself-paced curriculum learning, which is the advanced version of both CL/SPL\nand possess advantages of both learning regimes to a certain extent. This\nfurther facilitates a theoretical understanding for SPCL, instead of only\nCL/SPL as conventional. Under this theory, we attempt to attain intrinsic\nlatent objectives of two curriculum forms, the partial order and group\ncurriculums, which easily follow the theoretical understanding of the\ncorresponding SPCL regimes. \n\n"}
{"id": "1805.08550", "contents": "Title: Anticipating cryptocurrency prices using machine learning Abstract: Machine learning and AI-assisted trading have attracted growing interest for\nthe past few years. Here, we use this approach to test the hypothesis that the\ninefficiency of the cryptocurrency market can be exploited to generate abnormal\nprofits. We analyse daily data for $1,681$ cryptocurrencies for the period\nbetween Nov. 2015 and Apr. 2018. We show that simple trading strategies\nassisted by state-of-the-art machine learning algorithms outperform standard\nbenchmarks. Our results show that nontrivial, but ultimately simple,\nalgorithmic mechanisms can help anticipate the short-term evolution of the\ncryptocurrency market. \n\n"}
{"id": "1805.08914", "contents": "Title: Enhancing Chinese Intent Classification by Dynamically Integrating\n  Character Features into Word Embeddings with Ensemble Techniques Abstract: Intent classification has been widely researched on English data with deep\nlearning approaches that are based on neural networks and word embeddings. The\nchallenge for Chinese intent classification stems from the fact that, unlike\nEnglish where most words are made up of 26 phonologic alphabet letters, Chinese\nis logographic, where a Chinese character is a more basic semantic unit that\ncan be informative and its meaning does not vary too much in contexts. Chinese\nword embeddings alone can be inadequate for representing words, and pre-trained\nembeddings can suffer from not aligning well with the task at hand. To account\nfor the inadequacy and leverage Chinese character information, we propose a\nlow-effort and generic way to dynamically integrate character embedding based\nfeature maps with word embedding based inputs, whose resulting word-character\nembeddings are stacked with a contextual information extraction module to\nfurther incorporate context information for predictions. On top of the proposed\nmodel, we employ an ensemble method to combine single models and obtain the\nfinal result. The approach is data-independent without relying on external\nsources like pre-trained word embeddings. The proposed model outperforms\nbaseline models and existing methods. \n\n"}
{"id": "1805.09238", "contents": "Title: Highway State Gating for Recurrent Highway Networks: improving\n  information flow through time Abstract: Recurrent Neural Networks (RNNs) play a major role in the field of sequential\nlearning, and have outperformed traditional algorithms on many benchmarks.\nTraining deep RNNs still remains a challenge, and most of the state-of-the-art\nmodels are structured with a transition depth of 2-4 layers. Recurrent Highway\nNetworks (RHNs) were introduced in order to tackle this issue. These have\nachieved state-of-the-art performance on a few benchmarks using a depth of 10\nlayers. However, the performance of this architecture suffers from a\nbottleneck, and ceases to improve when an attempt is made to add more layers.\nIn this work, we analyze the causes for this, and postulate that the main\nsource is the way that the information flows through time. We introduce a novel\nand simple variation for the RHN cell, called Highway State Gating (HSG), which\nallows adding more layers, while continuing to improve performance. By using a\ngating mechanism for the state, we allow the net to \"choose\" whether to pass\ninformation directly through time, or to gate it. This mechanism also allows\nthe gradient to back-propagate directly through time and, therefore, results in\na slightly faster convergence. We use the Penn Treebank (PTB) dataset as a\nplatform for empirical proof of concept. Empirical results show that the\nimprovement due to Highway State Gating is for all depths, and as the depth\nincreases, the improvement also increases. \n\n"}
{"id": "1805.09317", "contents": "Title: Communication Algorithms via Deep Learning Abstract: Coding theory is a central discipline underpinning wireline and wireless\nmodems that are the workhorses of the information age. Progress in coding\ntheory is largely driven by individual human ingenuity with sporadic\nbreakthroughs over the past century. In this paper we study whether it is\npossible to automate the discovery of decoding algorithms via deep learning. We\nstudy a family of sequential codes parameterized by recurrent neural network\n(RNN) architectures. We show that creatively designed and trained RNN\narchitectures can decode well known sequential codes such as the convolutional\nand turbo codes with close to optimal performance on the additive white\nGaussian noise (AWGN) channel, which itself is achieved by breakthrough\nalgorithms of our times (Viterbi and BCJR decoders, representing dynamic\nprograming and forward-backward algorithms). We show strong generalizations,\ni.e., we train at a specific signal to noise ratio and block length but test at\na wide range of these quantities, as well as robustness and adaptivity to\ndeviations from the AWGN setting. \n\n"}
{"id": "1805.09785", "contents": "Title: Entropy and mutual information in models of deep neural networks Abstract: We examine a class of deep learning models with a tractable method to compute\ninformation-theoretic quantities. Our contributions are three-fold: (i) We show\nhow entropies and mutual informations can be derived from heuristic statistical\nphysics methods, under the assumption that weight matrices are independent and\northogonally-invariant. (ii) We extend particular cases in which this result is\nknown to be rigorously exact by providing a proof for two-layers networks with\nGaussian random weights, using the recently introduced adaptive interpolation\nmethod. (iii) We propose an experiment framework with generative models of\nsynthetic datasets, on which we train deep neural networks with a weight\nconstraint designed so that the assumption in (i) is verified during learning.\nWe study the behavior of entropies and mutual informations throughout learning\nand conclude that, in the proposed setting, the relationship between\ncompression and generalization remains elusive. \n\n"}
{"id": "1805.09929", "contents": "Title: DSGAN: Generative Adversarial Training for Distant Supervision Relation\n  Extraction Abstract: Distant supervision can effectively label data for relation extraction, but\nsuffers from the noise labeling problem. Recent works mainly perform soft\nbag-level noise reduction strategies to find the relatively better samples in a\nsentence bag, which is suboptimal compared with making a hard decision of false\npositive samples in sentence level. In this paper, we introduce an adversarial\nlearning framework, which we named DSGAN, to learn a sentence-level\ntrue-positive generator. Inspired by Generative Adversarial Networks, we regard\nthe positive samples generated by the generator as the negative samples to\ntrain the discriminator. The optimal generator is obtained until the\ndiscrimination ability of the discriminator has the greatest decline. We adopt\nthe generator to filter distant supervision training dataset and redistribute\nthe false positive instances into the negative set, in which way to provide a\ncleaned dataset for relation classification. The experimental results show that\nthe proposed strategy significantly improves the performance of distant\nsupervision relation extraction comparing to state-of-the-art systems. \n\n"}
{"id": "1805.10014", "contents": "Title: KONG: Kernels for ordered-neighborhood graphs Abstract: We present novel graph kernels for graphs with node and edge labels that have\nordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with\nordered neighborhoods are a natural data representation for evolving graphs\nwhere edges are created over time, which induces an order. Combining\nconvolutional subgraph kernels and string kernels, we design new scalable\nalgorithms for generation of explicit graph feature maps using sketching\ntechniques. We obtain precise bounds for the approximation accuracy and\ncomputational complexity of the proposed approaches and demonstrate their\napplicability on real datasets. In particular, our experiments demonstrate that\nneighborhood ordering results in more informative features. For the special\ncase of general graphs, i.e. graphs without ordered neighborhoods, the new\ngraph kernels yield efficient and simple algorithms for the comparison of label\ndistributions between graphs. \n\n"}
{"id": "1805.10615", "contents": "Title: A Local Information Criterion for Dynamical Systems Abstract: Encoding a sequence of observations is an essential task with many\napplications. The encoding can become highly efficient when the observations\nare generated by a dynamical system. A dynamical system imposes regularities on\nthe observations that can be leveraged to achieve a more efficient code. We\npropose a method to encode a given or learned dynamical system. Apart from its\napplication for encoding a sequence of observations, we propose to use the\ncompression achieved by this encoding as a criterion for model selection. Given\na dataset, different learning algorithms result in different models. But not\nall learned models are equally good. We show that the proposed encoding\napproach can be used to choose the learned model which is closer to the true\nunderlying dynamics. We provide experiments for both encoding and model\nselection, and theoretical results that shed light on why the approach works. \n\n"}
{"id": "1805.10724", "contents": "Title: RetainVis: Visual Analytics with Interpretable and Interactive Recurrent\n  Neural Networks on Electronic Medical Records Abstract: We have recently seen many successful applications of recurrent neural\nnetworks (RNNs) on electronic medical records (EMRs), which contain histories\nof patients' diagnoses, medications, and other various events, in order to\npredict the current and future states of patients. Despite the strong\nperformance of RNNs, it is often challenging for users to understand why the\nmodel makes a particular prediction. Such black-box nature of RNNs can impede\nits wide adoption in clinical practice. Furthermore, we have no established\nmethods to interactively leverage users' domain expertise and prior knowledge\nas inputs for steering the model. Therefore, our design study aims to provide a\nvisual analytics solution to increase interpretability and interactivity of\nRNNs via a joint effort of medical experts, artificial intelligence scientists,\nand visual analytics researchers. Following the iterative design process\nbetween the experts, we design, implement, and evaluate a visual analytics tool\ncalled RetainVis, which couples a newly improved, interpretable and interactive\nRNN-based model called RetainEX and visualizations for users' exploration of\nEMR data in the context of prediction tasks. Our study shows the effective use\nof RetainVis for gaining insights into how individual medical codes contribute\nto making risk predictions, using EMRs of patients with heart failure and\ncataract symptoms. Our study also demonstrates how we made substantial changes\nto the state-of-the-art RNN model called RETAIN in order to make use of\ntemporal information and increase interactivity. This study will provide a\nuseful guideline for researchers that aim to design an interpretable and\ninteractive visual analytics tool for RNNs. \n\n"}
{"id": "1805.10777", "contents": "Title: Object-Level Representation Learning for Few-Shot Image Classification Abstract: Few-shot learning that trains image classifiers over few labeled examples per\ncategory is a challenging task. In this paper, we propose to exploit an\nadditional big dataset with different categories to improve the accuracy of\nfew-shot learning over our target dataset. Our approach is based on the\nobservation that images can be decomposed into objects, which may appear in\nimages from both the additional dataset and our target dataset. We use the\nobject-level relation learned from the additional dataset to infer the\nsimilarity of images in our target dataset with unseen categories. Nearest\nneighbor search is applied to do image classification, which is a\nnon-parametric model and thus does not need fine-tuning. We evaluate our\nalgorithm on two popular datasets, namely Omniglot and MiniImagenet. We obtain\n8.5\\% and 2.7\\% absolute improvements for 5-way 1-shot and 5-way 5-shot\nexperiments on MiniImagenet, respectively. Source code will be published upon\nacceptance. \n\n"}
{"id": "1805.11085", "contents": "Title: More Than a Feeling: Learning to Grasp and Regrasp using Vision and\n  Touch Abstract: For humans, the process of grasping an object relies heavily on rich tactile\nfeedback. Most recent robotic grasping work, however, has been based only on\nvisual input, and thus cannot easily benefit from feedback after initiating\ncontact. In this paper, we investigate how a robot can learn to use tactile\ninformation to iteratively and efficiently adjust its grasp. To this end, we\npropose an end-to-end action-conditional model that learns regrasping policies\nfrom raw visuo-tactile data. This model -- a deep, multimodal convolutional\nnetwork -- predicts the outcome of a candidate grasp adjustment, and then\nexecutes a grasp by iteratively selecting the most promising actions. Our\napproach requires neither calibration of the tactile sensors, nor any\nanalytical modeling of contact forces, thus reducing the engineering effort\nrequired to obtain efficient grasping policies. We train our model with data\nfrom about 6,450 grasping trials on a two-finger gripper equipped with GelSight\nhigh-resolution tactile sensors on each finger. Across extensive experiments,\nour approach outperforms a variety of baselines at (i) estimating grasp\nadjustment outcomes, (ii) selecting efficient grasp adjustments for quick\ngrasping, and (iii) reducing the amount of force applied at the fingers, while\nmaintaining competitive performance. Finally, we study the choices made by our\nmodel and show that it has successfully acquired useful and interpretable\ngrasping behaviors. \n\n"}
{"id": "1805.11718", "contents": "Title: Random mesh projectors for inverse problems Abstract: We propose a new learning-based approach to solve ill-posed inverse problems\nin imaging. We address the case where ground truth training samples are rare\nand the problem is severely ill-posed - both because of the underlying physics\nand because we can only get few measurements. This setting is common in\ngeophysical imaging and remote sensing. We show that in this case the common\napproach to directly learn the mapping from the measured data to the\nreconstruction becomes unstable. Instead, we propose to first learn an ensemble\nof simpler mappings from the data to projections of the unknown image into\nrandom piecewise-constant subspaces. We then combine the projections to form a\nfinal reconstruction by solving a deconvolution-like problem. We show\nexperimentally that the proposed method is more robust to measurement noise and\ncorruptions not seen during training than a directly learned inverse. \n\n"}
{"id": "1805.11917", "contents": "Title: The Dynamics of Learning: A Random Matrix Approach Abstract: Understanding the learning dynamics of neural networks is one of the key\nissues for the improvement of optimization algorithms as well as for the\ntheoretical comprehension of why deep neural nets work so well today. In this\npaper, we introduce a random matrix-based framework to analyze the learning\ndynamics of a single-layer linear network on a binary classification problem,\nfor data of simultaneously large dimension and size, trained by gradient\ndescent. Our results provide rich insights into common questions in neural\nnets, such as overfitting, early stopping and the initialization of training,\nthereby opening the door for future studies of more elaborate structures and\nmodels appearing in today's neural networks. \n\n"}
{"id": "1805.12070", "contents": "Title: Code-Switching Language Modeling using Syntax-Aware Multi-Task Learning Abstract: Lack of text data has been the major issue on code-switching language\nmodeling. In this paper, we introduce multi-task learning based language model\nwhich shares syntax representation of languages to leverage linguistic\ninformation and tackle the low resource data issue. Our model jointly learns\nboth language modeling and Part-of-Speech tagging on code-switched utterances.\nIn this way, the model is able to identify the location of code-switching\npoints and improves the prediction of next word. Our approach outperforms\nstandard LSTM based language model, with an improvement of 9.7% and 7.4% in\nperplexity on SEAME Phase I and Phase II dataset respectively. \n\n"}
{"id": "1805.12076", "contents": "Title: Towards Understanding the Role of Over-Parametrization in Generalization\n  of Neural Networks Abstract: Despite existing work on ensuring generalization of neural networks in terms\nof scale sensitive complexity measures, such as norms, margin and sharpness,\nthese complexity measures do not offer an explanation of why neural networks\ngeneralize better with over-parametrization. In this work we suggest a novel\ncomplexity measure based on unit-wise capacities resulting in a tighter\ngeneralization bound for two layer ReLU networks. Our capacity bound correlates\nwith the behavior of test error with increasing network sizes, and could\npotentially explain the improvement in generalization with\nover-parametrization. We further present a matching lower bound for the\nRademacher complexity that improves over previous capacity lower bounds for\nneural networks. \n\n"}
{"id": "1805.12164", "contents": "Title: What the Vec? Towards Probabilistically Grounded Embeddings Abstract: Word2Vec (W2V) and GloVe are popular, fast and efficient word embedding\nalgorithms. Their embeddings are widely used and perform well on a variety of\nnatural language processing tasks. Moreover, W2V has recently been adopted in\nthe field of graph embedding, where it underpins several leading algorithms.\nHowever, despite their ubiquity and relatively simple model architecture, a\ntheoretical understanding of what the embedding parameters of W2V and GloVe\nlearn and why that is useful in downstream tasks has been lacking. We show that\ndifferent interactions between PMI vectors reflect semantic word relationships,\nsuch as similarity and paraphrasing, that are encoded in low dimensional word\nembeddings under a suitable projection, theoretically explaining why embeddings\nof W2V and GloVe work. As a consequence, we also reveal an interesting\nmathematical interconnection between the considered semantic relationships\nthemselves. \n\n"}
{"id": "1805.12381", "contents": "Title: Imbalanced Ensemble Classifier for learning from imbalanced business\n  school data set Abstract: Private business schools in India face a common problem of selecting quality\nstudents for their MBA programs to achieve the desired placement percentage.\nGenerally, such data sets are biased towards one class, i.e., imbalanced in\nnature. And learning from the imbalanced dataset is a difficult proposition.\nThis paper proposes an imbalanced ensemble classifier which can handle the\nimbalanced nature of the dataset and achieves higher accuracy in case of the\nfeature selection (selection of important characteristics of students) cum\nclassification problem (prediction of placements based on the students'\ncharacteristics) for Indian business school dataset. The optimal value of an\nimportant model parameter is found. Numerical evidence is also provided using\nIndian business school dataset to assess the outstanding performance of the\nproposed classifier. \n\n"}
{"id": "1806.00064", "contents": "Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors Abstract: Multimodal research is an emerging field of artificial intelligence, and one\nof the main research problems in this field is multimodal fusion. The fusion of\nmultimodal data is the process of integrating multiple unimodal representations\ninto one compact multimodal representation. Previous research in this field has\nexploited the expressiveness of tensors for multimodal representation. However,\nthese methods often suffer from exponential increase in dimensions and in\ncomputational complexity introduced by transformation of input into tensor. In\nthis paper, we propose the Low-rank Multimodal Fusion method, which performs\nmultimodal fusion using low-rank tensors to improve efficiency. We evaluate our\nmodel on three different tasks: multimodal sentiment analysis, speaker trait\nanalysis, and emotion recognition. Our model achieves competitive results on\nall these tasks while drastically reducing computational complexity. Additional\nexperiments also show that our model can perform robustly for a wide range of\nlow-rank settings, and is indeed much more efficient in both training and\ninference compared to other methods that utilize tensor representations. \n\n"}
{"id": "1806.00148", "contents": "Title: Interpreting Deep Learning: The Machine Learning Rorschach Test? Abstract: Theoretical understanding of deep learning is one of the most important tasks\nfacing the statistics and machine learning communities. While deep neural\nnetworks (DNNs) originated as engineering methods and models of biological\nnetworks in neuroscience and psychology, they have quickly become a centerpiece\nof the machine learning toolbox. Unfortunately, DNN adoption powered by recent\nsuccesses combined with the open-source nature of the machine learning\ncommunity, has outpaced our theoretical understanding. We cannot reliably\nidentify when and why DNNs will make mistakes. In some applications like text\ntranslation these mistakes may be comical and provide for fun fodder in\nresearch talks, a single error can be very costly in tasks like medical\nimaging. As we utilize DNNs in increasingly sensitive applications, a better\nunderstanding of their properties is thus imperative. Recent advances in DNN\ntheory are numerous and include many different sources of intuition, such as\nlearning theory, sparse signal analysis, physics, chemistry, and psychology. An\ninteresting pattern begins to emerge in the breadth of possible\ninterpretations. The seemingly limitless approaches are mostly constrained by\nthe lens with which the mathematical operations are viewed. Ultimately, the\ninterpretation of DNNs appears to mimic a type of Rorschach test --- a\npsychological test wherein subjects interpret a series of seemingly ambiguous\nink-blots. Validation for DNN theory requires a convergence of the literature.\nWe must distinguish between universal results that are invariant to the\nanalysis perspective and those that are specific to a particular network\nconfiguration. Simultaneously we must deal with the fact that many standard\nstatistical tools for quantifying generalization or empirically assessing\nimportant network features are difficult to apply to DNNs. \n\n"}
{"id": "1806.00176", "contents": "Title: Reparameterization Gradient for Non-differentiable Models Abstract: We present a new algorithm for stochastic variational inference that targets\nat models with non-differentiable densities. One of the key challenges in\nstochastic variational inference is to come up with a low-variance estimator of\nthe gradient of a variational objective. We tackle the challenge by\ngeneralizing the reparameterization trick, one of the most effective techniques\nfor addressing the variance issue for differentiable models, so that the trick\nworks for non-differentiable models as well. Our algorithm splits the space of\nlatent variables into regions where the density of the variables is\ndifferentiable, and their boundaries where the density may fail to be\ndifferentiable. For each differentiable region, the algorithm applies the\nstandard reparameterization trick and estimates the gradient restricted to the\nregion. For each potentially non-differentiable boundary, it uses a form of\nmanifold sampling and computes the direction for variational parameters that,\nif followed, would increase the boundary's contribution to the variational\nobjective. The sum of all the estimates becomes the gradient estimate of our\nalgorithm. Our estimator enjoys the reduced variance of the reparameterization\ngradient while remaining unbiased even for non-differentiable models. The\nexperiments with our preliminary implementation confirm the benefit of reduced\nvariance and unbiasedness. \n\n"}
{"id": "1806.00273", "contents": "Title: Sparse Pursuit and Dictionary Learning for Blind Source Separation in\n  Polyphonic Music Recordings Abstract: We propose an algorithm for the blind separation of single-channel audio\nsignals. It is based on a parametric model that describes the spectral\nproperties of the sounds of musical instruments independently of pitch. We\ndevelop a novel sparse pursuit algorithm that can match the discrete frequency\nspectra from the recorded signal with the continuous spectra delivered by the\nmodel. We first use this algorithm to convert an STFT spectrogram from the\nrecording into a novel form of log-frequency spectrogram whose resolution\nexceeds that of the mel spectrogram. We then make use of the pitch-invariant\nproperties of that representation in order to identify the sounds of the\ninstruments via the same sparse pursuit method. As the model parameters which\ncharacterize the musical instruments are not known beforehand, we train a\ndictionary that contains them, using a modified version of Adam. Applying the\nalgorithm on various audio samples, we find that it is capable of producing\nhigh-quality separation results when the model assumptions are satisfied and\nthe instruments are clearly distinguishable, but combinations of instruments\nwith similar spectral characteristics pose a conceptual difficulty. While a key\nfeature of the model is that it explicitly models inharmonicity, its presence\ncan also still impede performance of the sparse pursuit algorithm. In general,\ndue to its pitch-invariance, our method is especially suitable for dealing with\nspectra from acoustic instruments, requiring only a minimal number of\nhyperparameters to be preset. Additionally, we demonstrate that the dictionary\nthat is constructed for one recording can be applied to a different recording\nwith similar instruments without additional training. \n\n"}
{"id": "1806.00437", "contents": "Title: Large-Margin Classification in Hyperbolic Space Abstract: Representing data in hyperbolic space can effectively capture latent\nhierarchical relationships. With the goal of enabling accurate classification\nof points in hyperbolic space while respecting their hyperbolic geometry, we\nintroduce hyperbolic SVM, a hyperbolic formulation of support vector machine\nclassifiers, and elucidate through new theoretical work its connection to the\nEuclidean counterpart. We demonstrate the performance improvement of hyperbolic\nSVM for multi-class prediction tasks on real-world complex networks as well as\nsimulated datasets. Our work allows analytic pipelines that take the inherent\nhyperbolic geometry of the data into account in an end-to-end fashion without\nresorting to ill-fitting tools developed for Euclidean space. \n\n"}
{"id": "1806.00451", "contents": "Title: Do CIFAR-10 Classifiers Generalize to CIFAR-10? Abstract: Machine learning is currently dominated by largely experimental work focused\non improvements in a few key tasks. However, the impressive accuracy numbers of\nthe best performing models are questionable because the same test sets have\nbeen used to select these models for multiple years now. To understand the\ndanger of overfitting, we measure the accuracy of CIFAR-10 classifiers by\ncreating a new test set of truly unseen images. Although we ensure that the new\ntest set is as close to the original data distribution as possible, we find a\nlarge drop in accuracy (4% to 10%) for a broad range of deep learning models.\nYet more recent models with higher original accuracy show a smaller drop and\nbetter overall performance, indicating that this drop is likely not due to\noverfitting based on adaptivity. Instead, we view our results as evidence that\ncurrent accuracy numbers are brittle and susceptible to even minute natural\nvariations in the data distribution. \n\n"}
{"id": "1806.00512", "contents": "Title: Structurally Sparsified Backward Propagation for Faster Long Short-Term\n  Memory Training Abstract: Exploiting sparsity enables hardware systems to run neural networks faster\nand more energy-efficiently. However, most prior sparsity-centric optimization\ntechniques only accelerate the forward pass of neural networks and usually\nrequire an even longer training process with iterative pruning and retraining.\nWe observe that artificially inducing sparsity in the gradients of the gates in\nan LSTM cell has little impact on the training quality. Further, we can enforce\nstructured sparsity in the gate gradients to make the LSTM backward pass up to\n45% faster than the state-of-the-art dense approach and 168% faster than the\nstate-of-the-art sparsifying method on modern GPUs. Though the structured\nsparsifying method can impact the accuracy of a model, this performance gap can\nbe eliminated by mixing our sparse training method and the standard dense\ntraining method. Experimental results show that the mixed method can achieve\ncomparable results in a shorter time span than using purely dense training. \n\n"}
{"id": "1806.00692", "contents": "Title: Stress Test Evaluation for Natural Language Inference Abstract: Natural language inference (NLI) is the task of determining if a natural\nlanguage hypothesis can be inferred from a given premise in a justifiable\nmanner. NLI was proposed as a benchmark task for natural language\nunderstanding. Existing models perform well at standard datasets for NLI,\nachieving impressive results across different genres of text. However, the\nextent to which these models understand the semantic content of sentences is\nunclear. In this work, we propose an evaluation methodology consisting of\nautomatically constructed \"stress tests\" that allow us to examine whether\nsystems have the ability to make real inferential decisions. Our evaluation of\nsix sentence-encoder models on these stress tests reveals strengths and\nweaknesses of these models with respect to challenging linguistic phenomena,\nand suggests important directions for future work in this area. \n\n"}
{"id": "1806.00738", "contents": "Title: Contextualize, Show and Tell: A Neural Visual Storyteller Abstract: We present a neural model for generating short stories from image sequences,\nwhich extends the image description model by Vinyals et al. (Vinyals et al.,\n2015). This extension relies on an encoder LSTM to compute a context vector of\neach story from the image sequence. This context vector is used as the first\nstate of multiple independent decoder LSTMs, each of which generates the\nportion of the story corresponding to each image in the sequence by taking the\nimage embedding as the first input. Our model showed competitive results with\nthe METEOR metric and human ratings in the internal track of the Visual\nStorytelling Challenge 2018. \n\n"}
{"id": "1806.00913", "contents": "Title: Self-Normalization Properties of Language Modeling Abstract: Self-normalizing discriminative models approximate the normalized probability\nof a class without having to compute the partition function. In the context of\nlanguage modeling, this property is particularly appealing as it may\nsignificantly reduce run-times due to large word vocabularies. In this study,\nwe provide a comprehensive investigation of language modeling\nself-normalization. First, we theoretically analyze the inherent\nself-normalization properties of Noise Contrastive Estimation (NCE) language\nmodels. Then, we compare them empirically to softmax-based approaches, which\nare self-normalized using explicit regularization, and suggest a hybrid model\nwith compelling properties. Finally, we uncover a surprising negative\ncorrelation between self-normalization and perplexity across the board, as well\nas some regularity in the observed errors, which may potentially be used for\nimproving self-normalization algorithms in the future. \n\n"}
{"id": "1806.00939", "contents": "Title: Lagrange Coded Computing: Optimal Design for Resiliency, Security and\n  Privacy Abstract: We consider a scenario involving computations over a massive dataset stored\ndistributedly across multiple workers, which is at the core of distributed\nlearning algorithms. We propose Lagrange Coded Computing (LCC), a new framework\nto simultaneously provide (1) resiliency against stragglers that may prolong\ncomputations; (2) security against Byzantine (or malicious) workers that\ndeliberately modify the computation for their benefit; and (3)\n(information-theoretic) privacy of the dataset amidst possible collusion of\nworkers. LCC, which leverages the well-known Lagrange polynomial to create\ncomputation redundancy in a novel coded form across workers, can be applied to\nany computation scenario in which the function of interest is an arbitrary\nmultivariate polynomial of the input dataset, hence covering many computations\nof interest in machine learning. LCC significantly generalizes prior works to\ngo beyond linear computations. It also enables secure and private computing in\ndistributed settings, improving the computation and communication efficiency of\nthe state-of-the-art. Furthermore, we prove the optimality of LCC by showing\nthat it achieves the optimal tradeoff between resiliency, security, and\nprivacy, i.e., in terms of tolerating the maximum number of stragglers and\nadversaries, and providing data privacy against the maximum number of colluding\nworkers. Finally, we show via experiments on Amazon EC2 that LCC speeds up the\nconventional uncoded implementation of distributed least-squares linear\nregression by up to $13.43\\times$, and also achieves a\n$2.36\\times$-$12.65\\times$ speedup over the state-of-the-art straggler\nmitigation strategies. \n\n"}
{"id": "1806.01528", "contents": "Title: The universal approximation power of finite-width deep ReLU networks Abstract: We show that finite-width deep ReLU neural networks yield rate-distortion\noptimal approximation (B\\\"olcskei et al., 2018) of polynomials, windowed\nsinusoidal functions, one-dimensional oscillatory textures, and the Weierstrass\nfunction, a fractal function which is continuous but nowhere differentiable.\nTogether with their recently established universal approximation property of\naffine function systems (B\\\"olcskei et al., 2018), this shows that deep neural\nnetworks approximate vastly different signal structures generated by the affine\ngroup, the Weyl-Heisenberg group, or through warping, and even certain\nfractals, all with approximation error decaying exponentially in the number of\nneurons. We also prove that in the approximation of sufficiently smooth\nfunctions finite-width deep networks require strictly smaller connectivity than\nfinite-depth wide networks. \n\n"}
{"id": "1806.01768", "contents": "Title: Evidential Deep Learning to Quantify Classification Uncertainty Abstract: Deterministic neural nets have been shown to learn effective predictors on a\nwide range of machine learning problems. However, as the standard approach is\nto train the network to minimize a prediction loss, the resultant model remains\nignorant to its prediction confidence. Orthogonally to Bayesian neural nets\nthat indirectly infer prediction uncertainty through weight uncertainties, we\npropose explicit modeling of the same using the theory of subjective logic. By\nplacing a Dirichlet distribution on the class probabilities, we treat\npredictions of a neural net as subjective opinions and learn the function that\ncollects the evidence leading to these opinions by a deterministic neural net\nfrom data. The resultant predictor for a multi-class classification problem is\nanother Dirichlet distribution whose parameters are set by the continuous\noutput of a neural net. We provide a preliminary analysis on how the\npeculiarities of our new loss function drive improved uncertainty estimation.\nWe observe that our method achieves unprecedented success on detection of\nout-of-distribution queries and endurance against adversarial perturbations. \n\n"}
{"id": "1806.01830", "contents": "Title: Relational Deep Reinforcement Learning Abstract: We introduce an approach for deep reinforcement learning (RL) that improves\nupon the efficiency, generalization capacity, and interpretability of\nconventional approaches through structured perception and relational reasoning.\nIt uses self-attention to iteratively reason about the relations between\nentities in a scene and to guide a model-free policy. Our results show that in\na novel navigation and planning task called Box-World, our agent finds\ninterpretable solutions that improve upon baselines in terms of sample\ncomplexity, ability to generalize to more complex scenes than experienced\nduring training, and overall performance. In the StarCraft II Learning\nEnvironment, our agent achieves state-of-the-art performance on six mini-games\n-- surpassing human grandmaster performance on four. By considering\narchitectural inductive biases, our work opens new directions for overcoming\nimportant, but stubborn, challenges in deep RL. \n\n"}
{"id": "1806.01845", "contents": "Title: Deep Neural Networks with Multi-Branch Architectures Are Less Non-Convex Abstract: Several recently proposed architectures of neural networks such as ResNeXt,\nInception, Xception, SqueezeNet and Wide ResNet are based on the designing idea\nof having multiple branches and have demonstrated improved performance in many\napplications. We show that one cause for such success is due to the fact that\nthe multi-branch architecture is less non-convex in terms of duality gap. The\nduality gap measures the degree of intrinsic non-convexity of an optimization\nproblem: smaller gap in relative value implies lower degree of intrinsic\nnon-convexity. The challenge is to quantitatively measure the duality gap of\nhighly non-convex problems such as deep neural networks. In this work, we\nprovide strong guarantees of this quantity for two classes of network\narchitectures. For the neural networks with arbitrary activation functions,\nmulti-branch architecture and a variant of hinge loss, we show that the duality\ngap of both population and empirical risks shrinks to zero as the number of\nbranches increases. This result sheds light on better understanding the power\nof over-parametrization where increasing the network width tends to make the\nloss surface less non-convex. For the neural networks with linear activation\nfunction and $\\ell_2$ loss, we show that the duality gap of empirical risk is\nzero. Our two results work for arbitrary depths and adversarial data, while the\nanalytical techniques might be of independent interest to non-convex\noptimization more broadly. Experiments on both synthetic and real-world\ndatasets validate our results. \n\n"}
{"id": "1806.01973", "contents": "Title: Graph Convolutional Neural Networks for Web-Scale Recommender Systems Abstract: Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. However,\nmaking these methods practical and scalable to web-scale recommendation tasks\nwith billions of items and hundreds of millions of users remains a challenge.\nHere we describe a large-scale deep recommendation engine that we developed and\ndeployed at Pinterest. We develop a data-efficient Graph Convolutional Network\n(GCN) algorithm PinSage, which combines efficient random walks and graph\nconvolutions to generate embeddings of nodes (i.e., items) that incorporate\nboth graph structure as well as node feature information. Compared to prior GCN\napproaches, we develop a novel method based on highly efficient random walks to\nstructure the convolutions and design a novel training strategy that relies on\nharder-and-harder training examples to improve robustness and convergence of\nthe model. We also develop an efficient MapReduce model inference algorithm to\ngenerate embeddings using a trained model. We deploy PinSage at Pinterest and\ntrain it on 7.5 billion examples on a graph with 3 billion nodes representing\npins and boards, and 18 billion edges. According to offline metrics, user\nstudies and A/B tests, PinSage generates higher-quality recommendations than\ncomparable deep learning and graph-based alternatives. To our knowledge, this\nis the largest application of deep graph embeddings to date and paves the way\nfor a new generation of web-scale recommender systems based on graph\nconvolutional architectures. \n\n"}
{"id": "1806.02190", "contents": "Title: Mitigation of Policy Manipulation Attacks on Deep Q-Networks with\n  Parameter-Space Noise Abstract: Recent developments have established the vulnerability of deep reinforcement\nlearning to policy manipulation attacks via intentionally perturbed inputs,\nknown as adversarial examples. In this work, we propose a technique for\nmitigation of such attacks based on addition of noise to the parameter space of\ndeep reinforcement learners during training. We experimentally verify the\neffect of parameter-space noise in reducing the transferability of adversarial\nexamples, and demonstrate the promising performance of this technique in\nmitigating the impact of whitebox and blackbox attacks at both test and\ntraining times. \n\n"}
{"id": "1806.02338", "contents": "Title: Towards Dependability Metrics for Neural Networks Abstract: Artificial neural networks (NN) are instrumental in realizing\nhighly-automated driving functionality. An overarching challenge is to identify\nbest safety engineering practices for NN and other learning-enabled components.\nIn particular, there is an urgent need for an adequate set of metrics for\nmeasuring all-important NN dependability attributes. We address this challenge\nby proposing a number of NN-specific and efficiently computable metrics for\nmeasuring NN dependability attributes including robustness, interpretability,\ncompleteness, and correctness. \n\n"}
{"id": "1806.02437", "contents": "Title: Studying the Difference Between Natural and Programming Language Corpora Abstract: Code corpora, as observed in large software systems, are now known to be far\nmore repetitive and predictable than natural language corpora. But why? Does\nthe difference simply arise from the syntactic limitations of programming\nlanguages? Or does it arise from the differences in authoring decisions made by\nthe writers of these natural and programming language texts? We conjecture that\nthe differences are not entirely due to syntax, but also from the fact that\nreading and writing code is un-natural for humans, and requires substantial\nmental effort; so, people prefer to write code in ways that are familiar to\nboth reader and writer. To support this argument, we present results from two\nsets of studies: 1) a first set aimed at attenuating the effects of syntax, and\n2) a second, aimed at measuring repetitiveness of text written in other\nsettings (e.g. second language, technical/specialized jargon), which are also\neffortful to write. We find find that this repetition in source code is not\nentirely the result of grammar constraints, and thus some repetition must\nresult from human choice. While the evidence we find of similar repetitive\nbehavior in technical and learner corpora does not conclusively show that such\nlanguage is used by humans to mitigate difficulty, it is consistent with that\ntheory. \n\n"}
{"id": "1806.02450", "contents": "Title: A Finite Time Analysis of Temporal Difference Learning With Linear\n  Function Approximation Abstract: Temporal difference learning (TD) is a simple iterative algorithm used to\nestimate the value function corresponding to a given policy in a Markov\ndecision process. Although TD is one of the most widely used algorithms in\nreinforcement learning, its theoretical analysis has proved challenging and few\nguarantees on its statistical efficiency are available. In this work, we\nprovide a simple and explicit finite time analysis of temporal difference\nlearning with linear function approximation. Except for a few key insights, our\nanalysis mirrors standard techniques for analyzing stochastic gradient descent\nalgorithms, and therefore inherits the simplicity and elegance of that\nliterature. Final sections of the paper show how all of our main results extend\nto the study of TD learning with eligibility traces, known as TD($\\lambda$),\nand to Q-learning applied in high-dimensional optimal stopping problems. \n\n"}
{"id": "1806.02507", "contents": "Title: Large scale classification in deep neural network with Label Mapping Abstract: In recent years, deep neural network is widely used in machine learning. The\nmulti-class classification problem is a class of important problem in machine\nlearning. However, in order to solve those types of multi-class classification\nproblems effectively, the required network size should have hyper-linear growth\nwith respect to the number of classes. Therefore, it is infeasible to solve the\nmulti-class classification problem using deep neural network when the number of\nclasses are huge. This paper presents a method, so called Label Mapping (LM),\nto solve this problem by decomposing the original classification problem to\nseveral smaller sub-problems which are solvable theoretically. Our method is an\nensemble method like error-correcting output codes (ECOC), but it allows base\nlearners to be multi-class classifiers with different number of class labels.\nWe propose two design principles for LM, one is to maximize the number of base\nclassifier which can separate two different classes, and the other is to keep\nall base learners to be independent as possible in order to reduce the\nredundant information. Based on these principles, two different LM algorithms\nare derived using number theory and information theory. Since each base learner\ncan be trained independently, it is easy to scale our method into a large scale\ntraining system. Experiments show that our proposed method outperforms the\nstandard one-hot encoding and ECOC significantly in terms of accuracy and model\ncomplexity. \n\n"}
{"id": "1806.03191", "contents": "Title: Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text\n  Corpora Abstract: Methods for unsupervised hypernym detection may broadly be categorized\naccording to two paradigms: pattern-based and distributional methods. In this\npaper, we study the performance of both approaches on several hypernymy tasks\nand find that simple pattern-based methods consistently outperform\ndistributional methods on common benchmark datasets. Our results show that\npattern-based models provide important contextual constraints which are not yet\ncaptured in distributional methods. \n\n"}
{"id": "1806.03281", "contents": "Title: Blind Justice: Fairness with Encrypted Sensitive Attributes Abstract: Recent work has explored how to train machine learning models which do not\ndiscriminate against any subgroup of the population as determined by sensitive\nattributes such as gender or race. To avoid disparate treatment, sensitive\nattributes should not be considered. On the other hand, in order to avoid\ndisparate impact, sensitive attributes must be examined, e.g., in order to\nlearn a fair model, or to check if a given model is fair. We introduce methods\nfrom secure multi-party computation which allow us to avoid both. By encrypting\nsensitive attributes, we show how an outcome-based fair model may be learned,\nchecked, or have its outputs verified and held to account, without users\nrevealing their sensitive attributes. \n\n"}
{"id": "1806.03847", "contents": "Title: A Multimodal Classifier Generative Adversarial Network for Carry and\n  Place Tasks from Ambiguous Language Instructions Abstract: This paper focuses on a multimodal language understanding method for\ncarry-and-place tasks with domestic service robots. We address the case of\nambiguous instructions, that is, when the target area is not specified. For\ninstance \"put away the milk and cereal\" is a natural instruction where there is\nambiguity regarding the target area, considering environments in daily life.\nConventionally, this instruction can be disambiguated from a dialogue system,\nbut at the cost of time and cumbersome interaction. Instead, we propose a\nmultimodal approach, in which the instructions are disambiguated using the\nrobot's state and environment context. We develop the Multi-Modal Classifier\nGenerative Adversarial Network (MMC-GAN) to predict the likelihood of different\ntarget areas considering the robot's physical limitation and the target\nclutter. Our approach, MMC-GAN, significantly improves accuracy compared with\nbaseline methods that use instructions only or simple deep neural networks. \n\n"}
{"id": "1806.04189", "contents": "Title: Navigating with Graph Representations for Fast and Scalable Decoding of\n  Neural Language Models Abstract: Neural language models (NLMs) have recently gained a renewed interest by\nachieving state-of-the-art performance across many natural language processing\n(NLP) tasks. However, NLMs are very computationally demanding largely due to\nthe computational cost of the softmax layer over a large vocabulary. We observe\nthat, in decoding of many NLP tasks, only the probabilities of the top-K\nhypotheses need to be calculated preciously and K is often much smaller than\nthe vocabulary size. This paper proposes a novel softmax layer approximation\nalgorithm, called Fast Graph Decoder (FGD), which quickly identifies, for a\ngiven context, a set of K words that are most likely to occur according to a\nNLM. We demonstrate that FGD reduces the decoding time by an order of magnitude\nwhile attaining close to the full softmax baseline accuracy on neural machine\ntranslation and language modeling tasks. We also prove the theoretical\nguarantee on the softmax approximation quality. \n\n"}
{"id": "1806.04245", "contents": "Title: Learning to Speed Up Structured Output Prediction Abstract: Predicting structured outputs can be computationally onerous due to the\ncombinatorially large output spaces. In this paper, we focus on reducing the\nprediction time of a trained black-box structured classifier without losing\naccuracy. To do so, we train a speedup classifier that learns to mimic a\nblack-box classifier under the learning-to-search approach. As the structured\nclassifier predicts more examples, the speedup classifier will operate as a\nlearned heuristic to guide search to favorable regions of the output space. We\npresent a mistake bound for the speedup classifier and identify inference\nsituations where it can independently make correct judgments without input\nfeatures. We evaluate our method on the task of entity and relation extraction\nand show that the speedup classifier outperforms even greedy search in terms of\nspeed without loss of accuracy. \n\n"}
{"id": "1806.04321", "contents": "Title: Energy-Constrained Compression for Deep Neural Networks via Weighted\n  Sparse Projection and Layer Input Masking Abstract: Deep Neural Networks (DNNs) are increasingly deployed in highly\nenergy-constrained environments such as autonomous drones and wearable devices\nwhile at the same time must operate in real-time. Therefore, reducing the\nenergy consumption has become a major design consideration in DNN training.\nThis paper proposes the first end-to-end DNN training framework that provides\nquantitative energy consumption guarantees via weighted sparse projection and\ninput masking. The key idea is to formulate the DNN training as an optimization\nproblem in which the energy budget imposes a previously unconsidered\noptimization constraint. We integrate the quantitative DNN energy estimation\ninto the DNN training process to assist the constrained optimization. We prove\nthat an approximate algorithm can be used to efficiently solve the optimization\nproblem. Compared to the best prior energy-saving methods, our framework trains\nDNNs that provide higher accuracies under same or lower energy budgets. Code is\npublicly available. \n\n"}
{"id": "1806.04339", "contents": "Title: When Will Gradient Methods Converge to Max-margin Classifier under ReLU\n  Models? Abstract: We study the implicit bias of gradient descent methods in solving a binary\nclassification problem over a linearly separable dataset. The classifier is\ndescribed by a nonlinear ReLU model and the objective function adopts the\nexponential loss function. We first characterize the landscape of the loss\nfunction and show that there can exist spurious asymptotic local minima besides\nasymptotic global minima. We then show that gradient descent (GD) can converge\nto either a global or a local max-margin direction, or may diverge from the\ndesired max-margin direction in a general context. For stochastic gradient\ndescent (SGD), we show that it converges in expectation to either the global or\nthe local max-margin direction if SGD converges. We further explore the\nimplicit bias of these algorithms in learning a multi-neuron network under\ncertain stationary conditions, and show that the learned classifier maximizes\nthe margins of each sample pattern partition under the ReLU activation. \n\n"}
{"id": "1806.04535", "contents": "Title: Automatic Target Recovery for Hindi-English Code Mixed Puns Abstract: In order for our computer systems to be more human-like, with a higher\nemotional quotient, they need to be able to process and understand intrinsic\nhuman language phenomena like humour. In this paper, we consider a subtype of\nhumour - puns, which are a common type of wordplay-based jokes. In particular,\nwe consider code-mixed puns which have become increasingly mainstream on social\nmedia, in informal conversations and advertisements and aim to build a system\nwhich can automatically identify the pun location and recover the target of\nsuch puns. We first study and classify code-mixed puns into two categories\nnamely intra-sentential and intra-word, and then propose a four-step algorithm\nto recover the pun targets for puns belonging to the intra-sentential category.\nOur algorithm uses language models, and phonetic similarity-based features to\nget the desired results. We test our approach on a small set of code-mixed\npunning advertisements, and observe that our system is successfully able to\nrecover the targets for 67% of the puns. \n\n"}
{"id": "1806.04830", "contents": "Title: Deep Multiscale Model Learning Abstract: The objective of this paper is to design novel multi-layer neural network\narchitectures for multiscale simulations of flows taking into account the\nobserved data and physical modeling concepts. Our approaches use deep learning\nconcepts combined with local multiscale model reduction methodologies to\npredict flow dynamics. Using reduced-order model concepts is important for\nconstructing robust deep learning architectures since the reduced-order models\nprovide fewer degrees of freedom. Flow dynamics can be thought of as\nmulti-layer networks. More precisely, the solution (e.g., pressures and\nsaturations) at the time instant $n+1$ depends on the solution at the time\ninstant $n$ and input parameters, such as permeability fields, forcing terms,\nand initial conditions. One can regard the solution as a multi-layer network,\nwhere each layer, in general, is a nonlinear forward map and the number of\nlayers relates to the internal time steps. We will rely on rigorous model\nreduction concepts to define unknowns and connections for each layer. In each\nlayer, our reduced-order models will provide a forward map, which will be\nmodified (\"trained\") using available data. It is critical to use reduced-order\nmodels for this purpose, which will identify the regions of influence and the\nappropriate number of variables. Because of the lack of available data, the\ntraining will be supplemented with computational data as needed and the\ninterpolation between data-rich and data-deficient models. We will also use\ndeep learning algorithms to train the elements of the reduced model discrete\nsystem. We will present main ingredients of our approach and numerical results.\nNumerical results show that using deep learning and multiscale models, we can\nimprove the forward models, which are conditioned to the available data. \n\n"}
{"id": "1806.05096", "contents": "Title: Introducing user-prescribed constraints in Markov chains for nonlinear\n  dimensionality reduction Abstract: Stochastic kernel based dimensionality reduction approaches have become\npopular in the last decade. The central component of many of these methods is a\nsymmetric kernel that quantifies the vicinity between pairs of data points and\na kernel-induced Markov chain on the data. Typically, the Markov chain is fully\nspecified by the kernel through row normalization. However, in many cases, it\nis desirable to impose user-specified stationary-state and dynamical\nconstraints on the Markov chain. Unfortunately, no systematic framework exists\nto impose such user-defined constraints. Here, we introduce a path entropy\nmaximization based approach to derive the transition probabilities of Markov\nchains using a kernel and additional user-specified constraints. We illustrate\nthe usefulness of these Markov chains with examples. \n\n"}
{"id": "1806.05112", "contents": "Title: Comparing Fairness Criteria Based on Social Outcome Abstract: Fairness in algorithmic decision-making processes is attracting increasing\nconcern. When an algorithm is applied to human-related decision-making an\nestimator solely optimizing its predictive power can learn biases on the\nexisting data, which motivates us the notion of fairness in machine learning.\nwhile several different notions are studied in the literature, little studies\nare done on how these notions affect the individuals. We demonstrate such a\ncomparison between several policies induced by well-known fairness criteria,\nincluding the color-blind (CB), the demographic parity (DP), and the equalized\nodds (EO). We show that the EO is the only criterion among them that removes\ngroup-level disparity. Empirical studies on the social welfare and disparity of\nthese policies are conducted. \n\n"}
{"id": "1806.05134", "contents": "Title: Marginal Policy Gradients: A Unified Family of Estimators for Bounded\n  Action Spaces with Applications Abstract: Many complex domains, such as robotics control and real-time strategy (RTS)\ngames, require an agent to learn a continuous control. In the former, an agent\nlearns a policy over $\\mathbb{R}^d$ and in the latter, over a discrete set of\nactions each of which is parametrized by a continuous parameter. Such problems\nare naturally solved using policy based reinforcement learning (RL) methods,\nbut unfortunately these often suffer from high variance leading to instability\nand slow convergence. Unnecessary variance is introduced whenever policies over\nbounded action spaces are modeled using distributions with unbounded support by\napplying a transformation $T$ to the sampled action before execution in the\nenvironment. Recently, the variance reduced clipped action policy gradient\n(CAPG) was introduced for actions in bounded intervals, but to date no variance\nreduced methods exist when the action is a direction, something often seen in\nRTS games. To this end we introduce the angular policy gradient (APG), a\nstochastic policy gradient method for directional control. With the marginal\npolicy gradients family of estimators we present a unified analysis of the\nvariance reduction properties of APG and CAPG; our results provide a stronger\nguarantee than existing analyses for CAPG. Experimental results on a popular\nRTS game and a navigation task show that the APG estimator offers a substantial\nimprovement over the standard policy gradient. \n\n"}
{"id": "1806.05138", "contents": "Title: Generative Neural Machine Translation Abstract: We introduce Generative Neural Machine Translation (GNMT), a latent variable\narchitecture which is designed to model the semantics of the source and target\nsentences. We modify an encoder-decoder translation model by adding a latent\nvariable as a language agnostic representation which is encouraged to learn the\nmeaning of the sentence. GNMT achieves competitive BLEU scores on pure\ntranslation tasks, and is superior when there are missing words in the source\nsentence. We augment the model to facilitate multilingual translation and\nsemi-supervised learning without adding parameters. This framework\nsignificantly reduces overfitting when there is limited paired data available,\nand is effective for translating between pairs of languages not seen during\ntraining. \n\n"}
{"id": "1806.05438", "contents": "Title: Stochastic Gradient Descent with Exponential Convergence Rates of\n  Expected Classification Errors Abstract: We consider stochastic gradient descent and its averaging variant for binary\nclassification problems in a reproducing kernel Hilbert space. In the\ntraditional analysis using a consistency property of loss functions, it is\nknown that the expected classification error converges more slowly than the\nexpected risk even when assuming a low-noise condition on the conditional label\nprobabilities. Consequently, the resulting rate is sublinear. Therefore, it is\nimportant to consider whether much faster convergence of the expected\nclassification error can be achieved. In recent research, an exponential\nconvergence rate for stochastic gradient descent was shown under a strong\nlow-noise condition but provided theoretical analysis was limited to the\nsquared loss function, which is somewhat inadequate for binary classification\ntasks. In this paper, we show an exponential convergence of the expected\nclassification error in the final phase of the stochastic gradient descent for\na wide class of differentiable convex loss functions under similar assumptions.\nAs for the averaged stochastic gradient descent, we show that the same\nconvergence rate holds from the early phase of training. In experiments, we\nverify our analyses on the $L_2$-regularized logistic regression. \n\n"}
{"id": "1806.05819", "contents": "Title: BubbleRank: Safe Online Learning to Re-Rank via Implicit Click Feedback Abstract: In this paper, we study the problem of safe online learning to re-rank, where\nuser feedback is used to improve the quality of displayed lists. Learning to\nrank has traditionally been studied in two settings. In the offline setting,\nrankers are typically learned from relevance labels created by judges. This\napproach has generally become standard in industrial applications of ranking,\nsuch as search. However, this approach lacks exploration and thus is limited by\nthe information content of the offline training data. In the online setting, an\nalgorithm can experiment with lists and learn from feedback on them in a\nsequential fashion. Bandit algorithms are well-suited for this setting but they\ntend to learn user preferences from scratch, which results in a high initial\ncost of exploration. This poses an additional challenge of safe exploration in\nranked lists. We propose BubbleRank, a bandit algorithm for safe re-ranking\nthat combines the strengths of both the offline and online settings. The\nalgorithm starts with an initial base list and improves it online by gradually\nexchanging higher-ranked less attractive items for lower-ranked more attractive\nitems. We prove an upper bound on the n-step regret of BubbleRank that degrades\ngracefully with the quality of the initial base list. Our theoretical findings\nare supported by extensive experiments on a large-scale real-world click\ndataset. \n\n"}
{"id": "1806.06200", "contents": "Title: Study of Semi-supervised Approaches to Improving English-Mandarin\n  Code-Switching Speech Recognition Abstract: In this paper, we present our overall efforts to improve the performance of a\ncode-switching speech recognition system using semi-supervised training methods\nfrom lexicon learning to acoustic modeling, on the South East Asian\nMandarin-English (SEAME) data. We first investigate semi-supervised lexicon\nlearning approach to adapt the canonical lexicon, which is meant to alleviate\nthe heavily accented pronunciation issue within the code-switching conversation\nof the local area. As a result, the learned lexicon yields improved\nperformance. Furthermore, we attempt to use semi-supervised training to deal\nwith those transcriptions that are highly mismatched between human transcribers\nand ASR system. Specifically, we conduct semi-supervised training assuming\nthose poorly transcribed data as unsupervised data. We found the\nsemi-supervised acoustic modeling can lead to improved results. Finally, to\nmake up for the limitation of the conventional n-gram language models due to\ndata sparsity issue, we perform lattice rescoring using neural network language\nmodels, and significant WER reduction is obtained. \n\n"}
{"id": "1806.06384", "contents": "Title: Multi-variable LSTM neural network for autoregressive exogenous model Abstract: In this paper, we propose multi-variable LSTM capable of accurate forecasting\nand variable importance interpretation for time series with exogenous\nvariables. Current attention mechanism in recurrent neural networks mostly\nfocuses on the temporal aspect of data and falls short of characterizing\nvariable importance. To this end, the multi-variable LSTM equipped with\ntensorized hidden states is developed to learn hidden states for individual\nvariables, which give rise to our mixture temporal and variable attention.\nBased on such attention mechanism, we infer and quantify variable importance.\nExtensive experiments using real datasets with Granger-causality test and the\nsynthetic dataset with ground truth demonstrate the prediction performance and\ninterpretability of multi-variable LSTM in comparison to a variety of\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\nframework for both forecasting and knowledge discovery. \n\n"}
{"id": "1806.06908", "contents": "Title: Designing Optimal Binary Rating Systems Abstract: Modern online platforms rely on effective rating systems to learn about\nitems. We consider the optimal design of rating systems that collect binary\nfeedback after transactions. We make three contributions. First, we formalize\nthe performance of a rating system as the speed with which it recovers the true\nunderlying ranking on items (in a large deviations sense), accounting for both\nitems' underlying match rates and the platform's preferences. Second, we\nprovide an efficient algorithm to compute the binary feedback system that\nyields the highest such performance. Finally, we show how this theoretical\nperspective can be used to empirically design an implementable, approximately\noptimal rating system, and validate our approach using real-world experimental\ndata collected on Amazon Mechanical Turk. \n\n"}
{"id": "1806.07237", "contents": "Title: Magnetic Resonance Spectroscopy Quantification using Deep Learning Abstract: Magnetic resonance spectroscopy (MRS) is an important technique in biomedical\nresearch and it has the unique capability to give a non-invasive access to the\nbiochemical content (metabolites) of scanned organs. In the literature, the\nquantification (the extraction of the potential biomarkers from the MRS\nsignals) involves the resolution of an inverse problem based on a parametric\nmodel of the metabolite signal. However, poor signal-to-noise ratio (SNR),\npresence of the macromolecule signal or high correlation between metabolite\nspectral patterns can cause high uncertainties for most of the metabolites,\nwhich is one of the main reasons that prevents use of MRS in clinical routine.\nIn this paper, quantification of metabolites in MR Spectroscopic imaging using\ndeep learning is proposed. A regression framework based on the Convolutional\nNeural Networks (CNN) is introduced for an accurate estimation of spectral\nparameters. The proposed model learns the spectral features from a large-scale\nsimulated data set with different variations of human brain spectra and SNRs.\nExperimental results demonstrate the accuracy of the proposed method, compared\nto state of the art standard quantification method (QUEST), on concentration of\n20 metabolites and the macromolecule. \n\n"}
{"id": "1806.07537", "contents": "Title: DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity\n  through Unified Recurrent and Convolutional Neural Networks Abstract: Motivation: Drug discovery demands rapid quantification of compound-protein\ninteraction (CPI). However, there is a lack of methods that can predict\ncompound-protein affinity from sequences alone with high applicability,\naccuracy, and interpretability.\n  Results: We present a seamless integration of domain knowledges and\nlearning-based approaches. Under novel representations of\nstructurally-annotated protein sequences, a semi-supervised deep learning model\nthat unifies recurrent and convolutional neural networks has been proposed to\nexploit both unlabeled and labeled data, for jointly encoding molecular\nrepresentations and predicting affinities. Our representations and models\noutperform conventional options in achieving relative error in IC$_{50}$ within\n5-fold for test cases and 20-fold for protein classes not included for\ntraining. Performances for new protein classes with few labeled data are\nfurther improved by transfer learning. Furthermore, separate and joint\nattention mechanisms are developed and embedded to our model to add to its\ninterpretability, as illustrated in case studies for predicting and explaining\nselective drug-target interactions. Lastly, alternative representations using\nprotein sequences or compound graphs and a unified RNN/GCNN-CNN model using\ngraph CNN (GCNN) are also explored to reveal algorithmic challenges ahead.\n  Availability: Data and source codes are available at\nhttps://github.com/Shen-Lab/DeepAffinity\n  Supplementary Information: Supplementary data are available at\nhttp://shen-lab.github.io/deep-affinity-bioinf18-supp-rev.pdf \n\n"}
{"id": "1806.07538", "contents": "Title: Towards Robust Interpretability with Self-Explaining Neural Networks Abstract: Most recent work on interpretability of complex machine learning models has\nfocused on estimating $\\textit{a posteriori}$ explanations for previously\ntrained models around specific predictions. $\\textit{Self-explaining}$ models\nwhere interpretability plays a key role already during learning have received\nmuch less attention. We propose three desiderata for explanations in general --\nexplicitness, faithfulness, and stability -- and show that existing methods do\nnot satisfy them. In response, we design self-explaining models in stages,\nprogressively generalizing linear classifiers to complex yet architecturally\nexplicit models. Faithfulness and stability are enforced via regularization\nspecifically tailored to such models. Experimental results across various\nbenchmark datasets show that our framework offers a promising direction for\nreconciling model complexity and interpretability. \n\n"}
{"id": "1806.07572", "contents": "Title: Neural Tangent Kernel: Convergence and Generalization in Neural Networks Abstract: At initialization, artificial neural networks (ANNs) are equivalent to\nGaussian processes in the infinite-width limit, thus connecting them to kernel\nmethods. We prove that the evolution of an ANN during training can also be\ndescribed by a kernel: during gradient descent on the parameters of an ANN, the\nnetwork function $f_\\theta$ (which maps input vectors to output vectors)\nfollows the kernel gradient of the functional cost (which is convex, in\ncontrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel\n(NTK). This kernel is central to describe the generalization features of ANNs.\nWhile the NTK is random at initialization and varies during training, in the\ninfinite-width limit it converges to an explicit limiting kernel and it stays\nconstant during training. This makes it possible to study the training of ANNs\nin function space instead of parameter space. Convergence of the training can\nthen be related to the positive-definiteness of the limiting NTK. We prove the\npositive-definiteness of the limiting NTK when the data is supported on the\nsphere and the non-linearity is non-polynomial. We then focus on the setting of\nleast-squares regression and show that in the infinite-width limit, the network\nfunction $f_\\theta$ follows a linear differential equation during training. The\nconvergence is fastest along the largest kernel principal components of the\ninput data with respect to the NTK, hence suggesting a theoretical motivation\nfor early stopping. Finally we study the NTK numerically, observe its behavior\nfor wide networks, and compare it to the infinite-width limit. \n\n"}
{"id": "1806.07822", "contents": "Title: Learning Neural Parsers with Deterministic Differentiable Imitation\n  Learning Abstract: We explore the problem of learning to decompose spatial tasks into segments,\nas exemplified by the problem of a painting robot covering a large object.\nInspired by the ability of classical decision tree algorithms to construct\nstructured partitions of their input spaces, we formulate the problem of\ndecomposing objects into segments as a parsing approach. We make the insight\nthat the derivation of a parse-tree that decomposes the object into segments\nclosely resembles a decision tree constructed by ID3, which can be done when\nthe ground-truth available. We learn to imitate an expert parsing oracle, such\nthat our neural parser can generalize to parse natural images without ground\ntruth. We introduce a novel deterministic policy gradient update, DRAG (i.e.,\nDeteRministically AGgrevate) in the form of a deterministic actor-critic\nvariant of AggreVaTeD, to train our neural parser. From another perspective,\nour approach is a variant of the Deterministic Policy Gradient suitable for the\nimitation learning setting. The deterministic policy representation offered by\ntraining our neural parser with DRAG allows it to outperform state of the art\nimitation and reinforcement learning approaches. \n\n"}
{"id": "1806.08028", "contents": "Title: Gradient Adversarial Training of Neural Networks Abstract: We propose gradient adversarial training, an auxiliary deep learning\nframework applicable to different machine learning problems. In gradient\nadversarial training, we leverage a prior belief that in many contexts,\nsimultaneous gradient updates should be statistically indistinguishable from\neach other. We enforce this consistency using an auxiliary network that\nclassifies the origin of the gradient tensor, and the main network serves as an\nadversary to the auxiliary network in addition to performing standard\ntask-based training. We demonstrate gradient adversarial training for three\ndifferent scenarios: (1) as a defense to adversarial examples we classify\ngradient tensors and tune them to be agnostic to the class of their\ncorresponding example, (2) for knowledge distillation, we do binary\nclassification of gradient tensors derived from the student or teacher network\nand tune the student gradient tensor to mimic the teacher's gradient tensor;\nand (3) for multi-task learning we classify the gradient tensors derived from\ndifferent task loss functions and tune them to be statistically\nindistinguishable. For each of the three scenarios we show the potential of\ngradient adversarial training procedure. Specifically, gradient adversarial\ntraining increases the robustness of a network to adversarial attacks, is able\nto better distill the knowledge from a teacher network to a student network\ncompared to soft targets, and boosts multi-task learning by aligning the\ngradient tensors derived from the task specific loss functions. Overall, our\nexperiments demonstrate that gradient tensors contain latent information about\nwhatever tasks are being trained, and can support diverse machine learning\nproblems when intelligently guided through adversarialization using a auxiliary\nnetwork. \n\n"}
{"id": "1806.08235", "contents": "Title: Semi-supervised Seizure Prediction with Generative Adversarial Networks Abstract: In this article, we propose an approach that can make use of not only labeled\nEEG signals but also the unlabeled ones which is more accessible. We also\nsuggest the use of data fusion to further improve the seizure prediction\naccuracy. Data fusion in our vision includes EEG signals, cardiogram signals,\nbody temperature and time. We use the short-time Fourier transform on 28-s EEG\nwindows as a pre-processing step. A generative adversarial network (GAN) is\ntrained in an unsupervised manner where information of seizure onset is\ndisregarded. The trained Discriminator of the GAN is then used as feature\nextractor. Features generated by the feature extractor are classified by two\nfully-connected layers (can be replaced by any classifier) for the labeled EEG\nsignals. This semi-supervised seizure prediction method achieves area under the\noperating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp\nEEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively.\nUnsupervised training without the need of labeling is important because not\nonly it can be performed in real-time during EEG signal recording, but also it\ndoes not require feature engineering effort for each patient. \n\n"}
{"id": "1806.08764", "contents": "Title: Learning Traffic Flow Dynamics using Random Fields Abstract: This paper presents a mesoscopic traffic flow model that explicitly describes\nthe spatio-temporal evolution of the probability distributions of vehicle\ntrajectories. The dynamics are represented by a sequence of factor graphs,\nwhich enable learning of traffic dynamics from limited Lagrangian measurements\nusing an efficient message passing technique. The approach ensures that\nestimated speeds and traffic densities are non-negative with probability one.\nThe estimation technique is tested using vehicle trajectory datasets generated\nusing an independent microscopic traffic simulator and is shown to efficiently\nreproduce traffic conditions with probe vehicle penetration levels as little as\n10\\%. The proposed algorithm is also compared with state-of-the-art traffic\nstate estimation techniques developed for the same purpose and it is shown that\nthe proposed approach can outperform the state-of-the-art techniques in terms\nreconstruction accuracy. \n\n"}
{"id": "1806.08782", "contents": "Title: Finding Local Minima via Stochastic Nested Variance Reduction Abstract: We propose two algorithms that can find local minima faster than the\nstate-of-the-art algorithms in both finite-sum and general stochastic nonconvex\noptimization. At the core of the proposed algorithms is\n$\\text{One-epoch-SNVRG}^+$ using stochastic nested variance reduction (Zhou et\nal., 2018a), which outperforms the state-of-the-art variance reduction\nalgorithms such as SCSG (Lei et al., 2017). In particular, for finite-sum\noptimization problems, the proposed\n$\\text{SNVRG}^{+}+\\text{Neon2}^{\\text{finite}}$ algorithm achieves\n$\\tilde{O}(n^{1/2}\\epsilon^{-2}+n\\epsilon_H^{-3}+n^{3/4}\\epsilon_H^{-7/2})$\ngradient complexity to converge to an $(\\epsilon, \\epsilon_H)$-second-order\nstationary point, which outperforms $\\text{SVRG}+\\text{Neon2}^{\\text{finite}}$\n(Allen-Zhu and Li, 2017) , the best existing algorithm, in a wide regime. For\ngeneral stochastic optimization problems, the proposed\n$\\text{SNVRG}^{+}+\\text{Neon2}^{\\text{online}}$ achieves\n$\\tilde{O}(\\epsilon^{-3}+\\epsilon_H^{-5}+\\epsilon^{-2}\\epsilon_H^{-3})$\ngradient complexity, which is better than both\n$\\text{SVRG}+\\text{Neon2}^{\\text{online}}$ (Allen-Zhu and Li, 2017) and\nNatasha2 (Allen-Zhu, 2017) in certain regimes. Furthermore, we explore the\nacceleration brought by third-order smoothness of the objective function. \n\n"}
{"id": "1806.09511", "contents": "Title: A Hierarchical Deep Learning Natural Language Parser for Fashion Abstract: This work presents a hierarchical deep learning natural language parser for\nfashion. Our proposal intends not only to recognize fashion-domain entities but\nalso to expose syntactic and morphologic insights. We leverage the usage of an\narchitecture of specialist models, each one for a different task (from parsing\nto entity recognition). Such architecture renders a hierarchical model able to\ncapture the nuances of the fashion language. The natural language parser is\nable to deal with textual ambiguities which are left unresolved by our\ncurrently existing solution. Our empirical results establish a robust baseline,\nwhich justifies the use of hierarchical architectures of deep learning models\nwhile opening new research avenues to explore. \n\n"}
{"id": "1806.09976", "contents": "Title: The decoupled extended Kalman filter for dynamic exponential-family\n  factorization models Abstract: Motivated by the needs of online large-scale recommender systems, we\nspecialize the decoupled extended Kalman filter (DEKF) to factorization models,\nincluding factorization machines, matrix and tensor factorization, and\nillustrate the effectiveness of the approach through numerical experiments on\nsynthetic and on real-world data. Online learning of model parameters through\nthe DEKF makes factorization models more broadly useful by (i) allowing for\nmore flexible observations through the entire exponential family, (ii) modeling\nparameter drift, and (iii) producing parameter uncertainty estimates that can\nenable explore/exploit and other applications. We use a different parameter\ndynamics than the standard DEKF, allowing parameter drift while encouraging\nreasonable values. We also present an alternate derivation of the extended\nKalman filter and DEKF that highlights the role of the Fisher information\nmatrix in the EKF. \n\n"}
{"id": "1806.10722", "contents": "Title: DeepTag: inferring all-cause diagnoses from clinical notes in\n  under-resourced medical domain Abstract: Large scale veterinary clinical records can become a powerful resource for\npatient care and research. However, clinicians lack the time and resource to\nannotate patient records with standard medical diagnostic codes and most\nveterinary visits are captured in free text notes. The lack of standard coding\nmakes it challenging to use the clinical data to improve patient care. It is\nalso a major impediment to cross-species translational research, which relies\non the ability to accurately identify patient cohorts with specific diagnostic\ncriteria in humans and animals. In order to reduce the coding burden for\nveterinary clinical practice and aid translational research, we have developed\na deep learning algorithm, DeepTag, which automatically infers diagnostic codes\nfrom veterinary free text notes. DeepTag is trained on a newly curated dataset\nof 112,558 veterinary notes manually annotated by experts. DeepTag extends\nmulti-task LSTM with an improved hierarchical objective that captures the\nsemantic structures between diseases. To foster human-machine collaboration,\nDeepTag also learns to abstain in examples when it is uncertain and defers them\nto human experts, resulting in improved performance. DeepTag accurately infers\ndisease codes from free text even in challenging cross-hospital settings where\nthe text comes from different clinical settings than the ones used for\ntraining. It enables automated disease annotation across a broad range of\nclinical diagnoses with minimal pre-processing. The technical framework in this\nwork can be applied in other medical domains that currently lack medical coding\nresources. \n\n"}
{"id": "1806.10728", "contents": "Title: Deep Echo State Networks with Uncertainty Quantification for\n  Spatio-Temporal Forecasting Abstract: Long-lead forecasting for spatio-temporal systems can often entail complex\nnonlinear dynamics that are difficult to specify it a priori. Current\nstatistical methodologies for modeling these processes are often highly\nparameterized and thus, challenging to implement from a computational\nperspective. One potential parsimonious solution to this problem is a method\nfrom the dynamical systems and engineering literature referred to as an echo\nstate network (ESN). ESN models use so-called {\\it reservoir computing} to\nefficiently compute recurrent neural network (RNN) forecasts. Moreover,\nso-called \"deep\" models have recently been shown to be successful at predicting\nhigh-dimensional complex nonlinear processes, particularly those with multiple\nspatial and temporal scales of variability (such as we often find in\nspatio-temporal environmental data). Here we introduce a deep ensemble ESN\n(D-EESN) model. We present two versions of this model for spatio-temporal\nprocesses that both produce forecasts and associated measures of uncertainty.\nThe first approach utilizes a bootstrap ensemble framework and the second is\ndeveloped within a hierarchical Bayesian framework (BD-EESN). This more general\nhierarchical Bayesian framework naturally accommodates non-Gaussian data types\nand multiple levels of uncertainties. The methodology is first applied to a\ndata set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical\nsystem simulation model and then to a long-lead United States (U.S.) soil\nmoisture forecasting application. \n\n"}
{"id": "1806.10787", "contents": "Title: How To Extract Fashion Trends From Social Media? A Robust Object\n  Detector With Support For Unsupervised Learning Abstract: With the proliferation of social media, fashion inspired from celebrities,\nreputed designers as well as fashion influencers has shortened the cycle of\nfashion design and manufacturing. However, with the explosion of fashion\nrelated content and large number of user generated fashion photos, it is an\narduous task for fashion designers to wade through social media photos and\ncreate a digest of trending fashion. This necessitates deep parsing of fashion\nphotos on social media to localize and classify multiple fashion items from a\ngiven fashion photo. While object detection competitions such as MSCOCO have\nthousands of samples for each of the object categories, it is quite difficult\nto get large labeled datasets for fast fashion items. Moreover,\nstate-of-the-art object detectors do not have any functionality to ingest large\namount of unlabeled data available on social media in order to fine tune object\ndetectors with labeled datasets. In this work, we show application of a generic\nobject detector, that can be pretrained in an unsupervised manner, on 24\ncategories from recently released Open Images V4 dataset. We first train the\nbase architecture of the object detector using unsupervisd learning on 60K\nunlabeled photos from 24 categories gathered from social media, and then\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.\nOn 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\nphotos while performing 11% to 17% better as compared to the state-of-the-art\nobject detectors. We show that this improvement is due to our choice of\narchitecture that lets us do unsupervised learning and that performs\nsignificantly better in identifying small objects. \n\n"}
{"id": "1806.10827", "contents": "Title: Deep Learning-Aided Projected Gradient Detector for Massive Overloaded\n  MIMO Channels Abstract: The paper presents a deep learning-aided iterative detection algorithm for\nmassive overloaded MIMO systems. Since the proposed algorithm is based on the\nprojected gradient descent method with trainable parameters, it is named as\ntrainable projected descent-detector (TPG-detector). The trainable internal\nparameters can be optimized with standard deep learning techniques such as back\npropagation and stochastic gradient descent algorithms. This approach referred\nto as data-driven tuning brings notable advantages of the proposed scheme such\nas fast convergence. The numerical experiments show that TPG-detector achieves\ncomparable detection performance to those of the known algorithms for massive\noverloaded MIMO channels with lower computation cost. \n\n"}
{"id": "1806.11416", "contents": "Title: Bounds on the Approximation Power of Feedforward Neural Networks Abstract: The approximation power of general feedforward neural networks with piecewise\nlinear activation functions is investigated. First, lower bounds on the size of\na network are established in terms of the approximation error and network depth\nand width. These bounds improve upon state-of-the-art bounds for certain\nclasses of functions, such as strongly convex functions. Second, an upper bound\nis established on the difference of two neural networks with identical weights\nbut different activation functions. \n\n"}
{"id": "1807.00042", "contents": "Title: Neural Networks Trained to Solve Differential Equations Learn General\n  Representations Abstract: We introduce a technique based on the singular vector canonical correlation\nanalysis (SVCCA) for measuring the generality of neural network layers across a\ncontinuously-parametrized set of tasks. We illustrate this method by studying\ngenerality in neural networks trained to solve parametrized boundary value\nproblems based on the Poisson partial differential equation. We find that the\nfirst hidden layer is general, and that deeper layers are successively more\nspecific. Next, we validate our method against an existing technique that\nmeasures layer generality using transfer learning experiments. We find\nexcellent agreement between the two methods, and note that our method is much\nfaster, particularly for continuously-parametrized problems. Finally, we\nvisualize the general representations of the first layers, and interpret them\nas generalized coordinates over the input domain. \n\n"}
{"id": "1807.00053", "contents": "Title: Task-Driven Convolutional Recurrent Models of the Visual System Abstract: Feed-forward convolutional neural networks (CNNs) are currently\nstate-of-the-art for object classification tasks such as ImageNet. Further,\nthey are quantitatively accurate models of temporally-averaged responses of\nneurons in the primate brain's visual system. However, biological visual\nsystems have two ubiquitous architectural features not shared with typical\nCNNs: local recurrence within cortical areas, and long-range feedback from\ndownstream areas to upstream areas. Here we explored the role of recurrence in\nimproving classification performance. We found that standard forms of\nrecurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the\nImageNet task. In contrast, novel cells that incorporated two structural\nfeatures, bypassing and gating, were able to boost task accuracy substantially.\nWe extended these design principles in an automated search over thousands of\nmodel architectures, which identified novel local recurrent cells and\nlong-range feedback connections useful for object recognition. Moreover, these\ntask-optimized ConvRNNs matched the dynamics of neural activity in the primate\nvisual system better than feedforward networks, suggesting a role for the\nbrain's recurrent connections in performing difficult visual behaviors. \n\n"}
{"id": "1807.00095", "contents": "Title: Probabilistic Bisection with Spatial Metamodels Abstract: Probabilistic Bisection Algorithm performs root finding based on knowledge\nacquired from noisy oracle responses. We consider the generalized PBA setting\n(G-PBA) where the statistical distribution of the oracle is unknown and\nlocation-dependent, so that model inference and Bayesian knowledge updating\nmust be performed simultaneously. To this end, we propose to leverage the\nspatial structure of a typical oracle by constructing a statistical surrogate\nfor the underlying logistic regression step. We investigate several\nnon-parametric surrogates, including Binomial Gaussian Processes (B-GP),\nPolynomial, Kernel, and Spline Logistic Regression. In parallel, we develop\nsampling policies that adaptively balance learning the oracle distribution and\nlearning the root. One of our proposals mimics active learning with B-GPs and\nprovides a novel look-ahead predictive variance formula. The resulting gains of\nour Spatial PBA algorithm relative to earlier G-PBA models are illustrated with\nsynthetic examples and a challenging stochastic root finding problem from\nBermudan option pricing. \n\n"}
{"id": "1807.00099", "contents": "Title: Generating Titles for Web Tables Abstract: Descriptive titles provide crucial context for interpreting tables that are\nextracted from web pages and are a key component of table-based web\napplications. Prior approaches have attempted to produce titles by selecting\nexisting text snippets associated with the table. These approaches, however,\nare limited by their dependence on suitable titles existing a priori. In our\nuser study, we observe that the relevant information for the title tends to be\nscattered across the page, and often--more than 80% of the time--does not\nappear verbatim anywhere in the page. We propose instead the application of a\nsequence-to-sequence neural network model as a more generalizable means of\ngenerating high-quality titles. This is accomplished by extracting many text\nsnippets that have potentially relevant information to the table, encoding them\ninto an input sequence, and using both copy and generation mechanisms in the\ndecoder to balance relevance and readability of the generated title. We\nvalidate this approach with human evaluation on sample web tables and report\nthat while sequence models with only a copy mechanism or only a generation\nmechanism are easily outperformed by simple selection-based baselines, the\nmodel with both capabilities outperforms them all, approaching the quality of\ncrowdsourced titles while training on fewer than ten thousand examples. To the\nbest of our knowledge, the proposed technique is the first to consider text\ngeneration methods for table titles and establishes a new state of the art. \n\n"}
{"id": "1807.00516", "contents": "Title: Balanced Distribution Adaptation for Transfer Learning Abstract: Transfer learning has achieved promising results by leveraging knowledge from\nthe source domain to annotate the target domain which has few or none labels.\nExisting methods often seek to minimize the distribution divergence between\ndomains, such as the marginal distribution, the conditional distribution or\nboth. However, these two distances are often treated equally in existing\nalgorithms, which will result in poor performance in real applications.\nMoreover, existing methods usually assume that the dataset is balanced, which\nalso limits their performances on imbalanced tasks that are quite common in\nreal problems. To tackle the distribution adaptation problem, in this paper, we\npropose a novel transfer learning approach, named as Balanced Distribution\n\\underline{A}daptation~(BDA), which can adaptively leverage the importance of\nthe marginal and conditional distribution discrepancies, and several existing\nmethods can be treated as special cases of BDA. Based on BDA, we also propose a\nnovel Weighted Balanced Distribution Adaptation~(W-BDA) algorithm to tackle the\nclass imbalance issue in transfer learning. W-BDA not only considers the\ndistribution adaptation between domains but also adaptively changes the weight\nof each class. To evaluate the proposed methods, we conduct extensive\nexperiments on several transfer learning tasks, which demonstrate the\neffectiveness of our proposed algorithms over several state-of-the-art methods. \n\n"}
{"id": "1807.01298", "contents": "Title: Generalized Bilinear Deep Convolutional Neural Networks for Multimodal\n  Biometric Identification Abstract: In this paper, we propose to employ a bank of modality-dedicated\nConvolutional Neural Networks (CNNs), fuse, train, and optimize them together\nfor person classification tasks. A modality-dedicated CNN is used for each\nmodality to extract modality-specific features. We demonstrate that, rather\nthan spatial fusion at the convolutional layers, the fusion can be performed on\nthe outputs of the fully-connected layers of the modality-specific CNNs without\nany loss of performance and with significant reduction in the number of\nparameters. We show that, using multiple CNNs with multimodal fusion at the\nfeature-level, we significantly outperform systems that use unimodal\nrepresentation. We study weighted feature, bilinear, and compact bilinear\nfeature-level fusion algorithms for multimodal biometric person identification.\nFinally, We propose generalized compact bilinear fusion algorithm to deploy\nboth the weighted feature fusion and compact bilinear schemes. We provide the\nresults for the proposed algorithms on three challenging databases: CMU\nMulti-PIE, BioCop, and BIOMDATA. \n\n"}
{"id": "1807.01406", "contents": "Title: Connecting Weighted Automata and Recurrent Neural Networks through\n  Spectral Learning Abstract: In this paper, we unravel a fundamental connection between weighted finite\nautomata~(WFAs) and second-order recurrent neural networks~(2-RNNs): in the\ncase of sequences of discrete symbols, WFAs and 2-RNNs with linear activation\nfunctions are expressively equivalent. Motivated by this result, we build upon\na recent extension of the spectral learning algorithm to vector-valued WFAs and\npropose the first provable learning algorithm for linear 2-RNNs defined over\nsequences of continuous input vectors. This algorithm relies on estimating low\nrank sub-blocks of the so-called Hankel tensor, from which the parameters of a\nlinear 2-RNN can be provably recovered. The performances of the proposed method\nare assessed in a simulation study. \n\n"}
{"id": "1807.01679", "contents": "Title: BCSAT : A Benchmark Corpus for Sentiment Analysis in Telugu Using\n  Word-level Annotations Abstract: The presented work aims at generating a systematically annotated corpus that\ncan support the enhancement of sentiment analysis tasks in Telugu using\nword-level sentiment annotations. From OntoSenseNet, we extracted 11,000\nadjectives, 253 adverbs, 8483 verbs and sentiment annotation is being done by\nlanguage experts. We discuss the methodology followed for the polarity\nannotations and validate the developed resource. This work aims at developing a\nbenchmark corpus, as an extension to SentiWordNet, and baseline accuracy for a\nmodel where lexeme annotations are applied for sentiment predictions. The\nfundamental aim of this paper is to validate and study the possibility of\nutilizing machine learning algorithms, word-level sentiment annotations in the\ntask of automated sentiment identification. Furthermore, accuracy is improved\nby annotating the bi-grams extracted from the target corpus. \n\n"}
{"id": "1807.01705", "contents": "Title: Transfer Learning for Clinical Time Series Analysis using Recurrent\n  Neural Networks Abstract: Deep neural networks have shown promising results for various clinical\nprediction tasks such as diagnosis, mortality prediction, predicting duration\nof stay in hospital, etc. However, training deep networks -- such as those\nbased on Recurrent Neural Networks (RNNs) -- requires large labeled data, high\ncomputational resources, and significant hyperparameter tuning effort. In this\nwork, we investigate as to what extent can transfer learning address these\nissues when using deep RNNs to model multivariate clinical time series. We\nconsider transferring the knowledge captured in an RNN trained on several\nsource tasks simultaneously using a large labeled dataset to build the model\nfor a target task with limited labeled data. An RNN pre-trained on several\ntasks provides generic features, which are then used to build simpler linear\nmodels for new target tasks without training task-specific RNNs. For\nevaluation, we train a deep RNN to identify several patient phenotypes on time\nseries from MIMIC-III database, and then use the features extracted using that\nRNN to build classifiers for identifying previously unseen phenotypes, and also\nfor a seemingly unrelated task of in-hospital mortality. We demonstrate that\n(i) models trained on features extracted using pre-trained RNN outperform or,\nin the worst case, perform as well as task-specific RNNs; (ii) the models using\nfeatures from pre-trained models are more robust to the size of labeled data\nthan task-specific RNNs; and (iii) features extracted using pre-trained RNN are\ngeneric enough and perform better than typical statistical hand-crafted\nfeatures. \n\n"}
{"id": "1807.01889", "contents": "Title: Learning in Variational Autoencoders with Kullback-Leibler and Renyi\n  Integral Bounds Abstract: In this paper we propose two novel bounds for the log-likelihood based on\nKullback-Leibler and the R\\'{e}nyi divergences, which can be used for\nvariational inference and in particular for the training of Variational\nAutoEncoders. Our proposal is motivated by the difficulties encountered in\ntraining VAEs on continuous datasets with high contrast images, such as those\nwith handwritten digits and characters, where numerical issues often appear\nunless noise is added, either to the dataset during training or to the\ngenerative model given by the decoder. The new bounds we propose, which are\nobtained from the maximization of the likelihood of an interval for the\nobservations, allow numerically stable training procedures without the\nnecessity of adding any extra source of noise to the data. \n\n"}
{"id": "1807.01961", "contents": "Title: A Boo(n) for Evaluating Architecture Performance Abstract: We point out important problems with the common practice of using the best\nsingle model performance for comparing deep learning architectures, and we\npropose a method that corrects these flaws. Each time a model is trained, one\ngets a different result due to random factors in the training process, which\ninclude random parameter initialization and random data shuffling. Reporting\nthe best single model performance does not appropriately address this\nstochasticity. We propose a normalized expected best-out-of-$n$ performance\n($\\text{Boo}_n$) as a way to correct these problems. \n\n"}
{"id": "1807.01975", "contents": "Title: Universality of jamming of non-spherical particles Abstract: Amorphous packings of non-spherical particles such as ellipsoids and\nspherocylinders are known to be hypostatic: the number of mechanical contacts\nbetween particles is smaller than the number of degrees of freedom, thus\nviolating Maxwell's mechanical stability criterion. In this work, we propose a\ngeneral theory of hypostatic amorphous packings and the associated jamming\ntransition. First, we show that many systems fall into a same universality\nclass. As an example, we explicitly map ellipsoids into a system of `breathing'\nparticles. We show by using a marginal stability argument that in both cases\njammed packings are hypostatic, and that the critical exponents related to the\ncontact number and the vibrational density of states are the same. Furthermore,\nwe introduce a generalized perceptron model which can be solved analytically by\nthe replica method. The analytical solution predicts critical exponents in the\nsame hypostatic jamming universality class. Our analysis further reveals that\nthe force and gap distributions of hypostatic jamming do not show power-law\nbehavior, in marked contrast to the isostatic jamming of spherical particles.\nFinally, we confirm our theoretical predictions by numerical simulations. \n\n"}
{"id": "1807.02567", "contents": "Title: Deep Learning for Launching and Mitigating Wireless Jamming Attacks Abstract: An adversarial machine learning approach is introduced to launch jamming\nattacks on wireless communications and a defense strategy is presented. A\ncognitive transmitter uses a pre-trained classifier to predict the current\nchannel status based on recent sensing results and decides whether to transmit\nor not, whereas a jammer collects channel status and ACKs to build a deep\nlearning classifier that reliably predicts the next successful transmissions\nand effectively jams them. This jamming approach is shown to reduce the\ntransmitter's performance much more severely compared with random or\nsensing-based jamming. The deep learning classification scores are used by the\njammer for power control subject to an average power constraint. Next, a\ngenerative adversarial network (GAN) is developed for the jammer to reduce the\ntime to collect the training dataset by augmenting it with synthetic samples.\nAs a defense scheme, the transmitter deliberately takes a small number of wrong\nactions in spectrum access (in form of a causative attack against the jammer)\nand therefore prevents the jammer from building a reliable classifier. The\ntransmitter systematically selects when to take wrong actions and adapts the\nlevel of defense to mislead the jammer into making prediction errors and\nconsequently increase its throughput. \n\n"}
{"id": "1807.02599", "contents": "Title: From Text to Topics in Healthcare Records: An Unsupervised Graph\n  Partitioning Methodology Abstract: Electronic Healthcare Records contain large volumes of unstructured data,\nincluding extensive free text. Yet this source of detailed information often\nremains under-used because of a lack of methodologies to extract interpretable\ncontent in a timely manner. Here we apply network-theoretical tools to analyse\nfree text in Hospital Patient Incident reports from the National Health\nService, to find clusters of documents with similar content in an unsupervised\nmanner at different levels of resolution. We combine deep neural network\nparagraph vector text-embedding with multiscale Markov Stability community\ndetection applied to a sparsified similarity graph of document vectors, and\nshowcase the approach on incident reports from Imperial College Healthcare NHS\nTrust, London. The multiscale community structure reveals different levels of\nmeaning in the topics of the dataset, as shown by descriptive terms extracted\nfrom the clusters of records. We also compare a posteriori against hand-coded\ncategories assigned by healthcare personnel, and show that our approach\noutperforms LDA-based models. Our content clusters exhibit good correspondence\nwith two levels of hand-coded categories, yet they also provide further medical\ndetail in certain areas and reveal complementary descriptors of incidents\nbeyond the external classification taxonomy. \n\n"}
{"id": "1807.02919", "contents": "Title: Domain2Vec: Deep Domain Generalization Abstract: We address the problem of domain generalization where a decision function is\nlearned from the data of several related domains, and the goal is to apply it\non an unseen domain successfully. It is assumed that there is plenty of labeled\ndata available in source domains (also called as training domain), but no\nlabeled data is available for the unseen domain (also called a target domain or\ntest domain). We propose a novel neural network architecture, Domain2Vec (D2V)\nthat learns domain-specific embedding and then uses this embedding to\ngeneralize the learning across related domains. The proposed algorithm, D2V\nextends the idea of distribution regression and kernelized domain\ngeneralization to the neural networks setting. We propose a neural network\narchitecture to learn domain-specific embedding and then use this embedding\nalong with the data point specific features to label it. We show the\neffectiveness of the architecture by accurately estimating domain to domain\nsimilarity. We evaluate our algorithm against standard domain generalization\ndatasets for image classification and outperform other state of the art\nalgorithms. \n\n"}
{"id": "1807.03396", "contents": "Title: On Training Recurrent Networks with Truncated Backpropagation Through\n  Time in Speech Recognition Abstract: Recurrent neural networks have been the dominant models for many speech and\nlanguage processing tasks. However, we understand little about the behavior and\nthe class of functions recurrent networks can realize. Moreover, the heuristics\nused during training complicate the analyses. In this paper, we study recurrent\nnetworks' ability to learn long-term dependency in the context of speech\nrecognition. We consider two decoding approaches, online and batch decoding,\nand show the classes of functions to which the decoding approaches correspond.\nWe then draw a connection between batch decoding and a popular training\napproach for recurrent networks, truncated backpropagation through time.\nChanging the decoding approach restricts the amount of past history recurrent\nnetworks can use for prediction, allowing us to analyze their ability to\nremember. Empirically, we utilize long-term dependency in subphonetic states,\nphonemes, and words, and show how the design decisions, such as the decoding\napproach, lookahead, context frames, and consecutive prediction, characterize\nthe behavior of recurrent networks. Finally, we draw a connection between\nMarkov processes and vanishing gradients. These results have implications for\nstudying the long-term dependency in speech data and how these properties are\nlearned by recurrent networks. \n\n"}
{"id": "1807.03399", "contents": "Title: Jointly Embedding Entities and Text with Distant Supervision Abstract: Learning representations for knowledge base entities and concepts is becoming\nincreasingly important for NLP applications. However, recent entity embedding\nmethods have relied on structured resources that are expensive to create for\nnew domains and corpora. We present a distantly-supervised method for jointly\nlearning embeddings of entities and text from an unnanotated corpus, using only\na list of mappings between entities and surface forms. We learn embeddings from\nopen-domain and biomedical corpora, and compare against prior methods that rely\non human-annotated text or large knowledge graph structure. Our embeddings\ncapture entity similarity and relatedness better than prior work, both in\nexisting biomedical datasets and a new Wikipedia-based dataset that we release\nto the community. Results on analogy completion and entity sense disambiguation\nindicate that entities and words capture complementary information that can be\neffectively combined for downstream use. \n\n"}
{"id": "1807.03520", "contents": "Title: Multiresolution Tree Networks for 3D Point Cloud Processing Abstract: We present multiresolution tree-structured networks to process point clouds\nfor 3D shape understanding and generation tasks. Our network represents a 3D\nshape as a set of locality-preserving 1D ordered list of points at multiple\nresolutions. This allows efficient feed-forward processing through 1D\nconvolutions, coarse-to-fine analysis through a multi-grid architecture, and it\nleads to faster convergence and small memory footprint during training. The\nproposed tree-structured encoders can be used to classify shapes and outperform\nexisting point-based architectures on shape classification benchmarks, while\ntree-structured decoders can be used for generating point clouds directly and\nthey outperform existing approaches for image-to-shape inference tasks learned\nusing the ShapeNet dataset. Our model also allows unsupervised learning of\npoint-cloud based shapes by using a variational autoencoder, leading to\nhigher-quality generated shapes. \n\n"}
{"id": "1807.03625", "contents": "Title: Foreign English Accent Adjustment by Learning Phonetic Patterns Abstract: State-of-the-art automatic speech recognition (ASR) systems struggle with the\nlack of data for rare accents. For sufficiently large datasets, neural engines\ntend to outshine statistical models in most natural language processing\nproblems. However, a speech accent remains a challenge for both approaches.\nPhonologists manually create general rules describing a speaker's accent, but\ntheir results remain underutilized. In this paper, we propose a model that\nautomatically retrieves phonological generalizations from a small dataset. This\nmethod leverages the difference in pronunciation between a particular dialect\nand General American English (GAE) and creates new accented samples of words.\nThe proposed model is able to learn all generalizations that previously were\nmanually obtained by phonologists. We use this statistical method to generate a\nmillion phonological variations of words from the CMU Pronouncing Dictionary\nand train a sequence-to-sequence RNN to recognize accented words with 59%\naccuracy. \n\n"}
{"id": "1807.03711", "contents": "Title: Geometric Generalization Based Zero-Shot Learning Dataset Infinite\n  World: Simple Yet Powerful Abstract: Raven's Progressive Matrices are one of the widely used tests in evaluating\nthe human test taker's fluid intelligence. Analogously, this paper introduces\ngeometric generalization based zero-shot learning tests to measure the rapid\nlearning ability and the internal consistency of deep generative models. Our\nempirical research analysis on state-of-the-art generative models discern their\nability to generalize concepts across classes. In the process, we introduce\nInfinite World, an evaluable, scalable, multi-modal, light-weight dataset and\nZero-Shot Intelligence Metric ZSI. The proposed tests condenses human-level\nspatial and numerical reasoning tasks to its simplistic geometric forms. The\ndataset is scalable to a theoretical limit of infinity, in numerical features\nof the generated geometric figures, image size and in quantity. We\nsystematically analyze state-of-the-art model's internal consistency, identify\ntheir bottlenecks and propose a pro-active optimization method for few-shot and\nzero-shot learning. \n\n"}
{"id": "1807.03915", "contents": "Title: Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment\n  Analysis Abstract: Multimodal machine learning is a core research area spanning the language,\nvisual and acoustic modalities. The central challenge in multimodal learning\ninvolves learning representations that can process and relate information from\nmultiple modalities. In this paper, we propose two methods for unsupervised\nlearning of joint multimodal representations using sequence to sequence\n(Seq2Seq) methods: a \\textit{Seq2Seq Modality Translation Model} and a\n\\textit{Hierarchical Seq2Seq Modality Translation Model}. We also explore\nmultiple different variations on the multimodal inputs and outputs of these\nseq2seq models. Our experiments on multimodal sentiment analysis using the\nCMU-MOSI dataset indicate that our methods learn informative multimodal\nrepresentations that outperform the baselines and achieve improved performance\non multimodal sentiment analysis, specifically in the Bimodal case where our\nmodel is able to improve F1 Score by twelve points. We also discuss future\ndirections for multimodal Seq2Seq methods. \n\n"}
{"id": "1807.03920", "contents": "Title: Discovering Interesting Plots in Production Yield Data Analytics Abstract: An analytic process is iterative between two agents, an analyst and an\nanalytic toolbox. Each iteration comprises three main steps: preparing a\ndataset, running an analytic tool, and evaluating the result, where dataset\npreparation and result evaluation, conducted by the analyst, are largely\ndomain-knowledge driven. In this work, the focus is on automating the result\nevaluation step. The underlying problem is to identify plots that are deemed\ninteresting by an analyst. We propose a methodology to learn such analyst's\nintent based on Generative Adversarial Networks (GANs) and demonstrate its\napplications in the context of production yield optimization using data\ncollected from several product lines. \n\n"}
{"id": "1807.04010", "contents": "Title: Causal Discovery in the Presence of Missing Data Abstract: Missing data are ubiquitous in many domains including healthcare. When these\ndata entries are not missing completely at random, the (conditional)\nindependence relations in the observed data may be different from those in the\ncomplete data generated by the underlying causal process. Consequently, simply\napplying existing causal discovery methods to the observed data may lead to\nwrong conclusions. In this paper, we aim at developing a causal discovery\nmethod to recover the underlying causal structure from observed data that\nfollow different missingness mechanisms, including missing completely at random\n(MCAR), missing at random (MAR), and missing not at random (MNAR). With\nmissingness mechanisms represented by missingness graphs, we analyse conditions\nunder which additional correction is needed to derive conditional\nindependence/dependence relations in the complete data. Based on our analysis,\nwe propose the Missing Value PC (MVPC) algorithm for both continuous and binary\nvariables, which extends the PC algorithm to incorporate additional\ncorrections. Our proposed MVPC is shown in theory to give asymptotically\ncorrect results even on data that are MAR or MNAR. Experimental results on\nsynthetic data show that the proposed algorithm is able to find correct causal\nrelations even in the general case of MNAR. Moreover, we create a neuropathic\npain diagnostic simulator for evaluating causal discovery methods. Evaluated on\nsuch simulated neuropathic pain diagnosis records and the other two real world\napplications, MVPC outperforms the other benchmark methods. \n\n"}
{"id": "1807.04511", "contents": "Title: Training Neural Networks Using Features Replay Abstract: Training a neural network using backpropagation algorithm requires passing\nerror gradients sequentially through the network. The backward locking prevents\nus from updating network layers in parallel and fully leveraging the computing\nresources. Recently, there are several works trying to decouple and parallelize\nthe backpropagation algorithm. However, all of them suffer from severe accuracy\nloss or memory explosion when the neural network is deep. To address these\nchallenging issues, we propose a novel parallel-objective formulation for the\nobjective function of the neural network. After that, we introduce features\nreplay algorithm and prove that it is guaranteed to converge to critical points\nfor the non-convex problem under certain conditions. Finally, we apply our\nmethod to training deep convolutional neural networks, and the experimental\nresults show that the proposed method achieves {faster} convergence, {lower}\nmemory consumption, and {better} generalization error than compared methods. \n\n"}
{"id": "1807.04689", "contents": "Title: Explorations in Homeomorphic Variational Auto-Encoding Abstract: The manifold hypothesis states that many kinds of high-dimensional data are\nconcentrated near a low-dimensional manifold. If the topology of this data\nmanifold is non-trivial, a continuous encoder network cannot embed it in a\none-to-one manner without creating holes of low density in the latent space.\nThis is at odds with the Gaussian prior assumption typically made in\nVariational Auto-Encoders (VAEs), because the density of a Gaussian\nconcentrates near a blob-like manifold.\n  In this paper we investigate the use of manifold-valued latent variables.\nSpecifically, we focus on the important case of continuously differentiable\nsymmetry groups (Lie groups), such as the group of 3D rotations\n$\\operatorname{SO}(3)$. We show how a VAE with $\\operatorname{SO}(3)$-valued\nlatent variables can be constructed, by extending the reparameterization trick\nto compact connected Lie groups. Our experiments show that choosing\nmanifold-valued latent variables that match the topology of the latent data\nmanifold, is crucial to preserve the topological structure and learn a\nwell-behaved latent space. \n\n"}
{"id": "1807.04863", "contents": "Title: Avoiding Latent Variable Collapse With Generative Skip Models Abstract: Variational autoencoders learn distributions of high-dimensional data. They\nmodel data with a deep latent-variable model and then fit the model by\nmaximizing a lower bound of the log marginal likelihood. VAEs can capture\ncomplex distributions, but they can also suffer from an issue known as \"latent\nvariable collapse,\" especially if the likelihood model is powerful.\nSpecifically, the lower bound involves an approximate posterior of the latent\nvariables; this posterior \"collapses\" when it is set equal to the prior, i.e.,\nwhen the approximate posterior is independent of the data. While VAEs learn\ngood generative models, latent variable collapse prevents them from learning\nuseful representations. In this paper, we propose a simple new way to avoid\nlatent variable collapse by including skip connections in our generative model;\nthese connections enforce strong links between the latent variables and the\nlikelihood function. We study generative skip models both theoretically and\nempirically. Theoretically, we prove that skip models increase the mutual\ninformation between the observations and the inferred latent variables.\nEmpirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to\nexisting VAE architectures, we show that generative skip models maintain\nsimilar predictive performance but lead to less collapse and provide more\nmeaningful representations of the data. \n\n"}
{"id": "1807.05077", "contents": "Title: Maximizing Invariant Data Perturbation with Stochastic Optimization Abstract: Feature attribution methods, or saliency maps, are one of the most popular\napproaches for explaining the decisions of complex machine learning models such\nas deep neural networks. In this study, we propose a stochastic optimization\napproach for the perturbation-based feature attribution method. While the\noriginal optimization problem of the perturbation-based feature attribution is\ndifficult to solve because of the complex constraints, we propose to\nreformulate the problem as the maximization of a differentiable function, which\ncan be solved using gradient-based algorithms. In particular, stochastic\noptimization is well-suited for the proposed reformulation, and we can solve\nthe problem using popular algorithms such as SGD, RMSProp, and Adam. The\nexperiment on the image classification with VGG16 shows that the proposed\nmethod could identify relevant parts of the images effectively. \n\n"}
{"id": "1807.05118", "contents": "Title: Tune: A Research Platform for Distributed Model Selection and Training Abstract: Modern machine learning algorithms are increasingly computationally\ndemanding, requiring specialized hardware and distributed computation to\nachieve high performance in a reasonable time frame. Many hyperparameter search\nalgorithms have been proposed for improving the efficiency of model selection,\nhowever their adaptation to the distributed compute environment is often\nad-hoc. We propose Tune, a unified framework for model selection and training\nthat provides a narrow-waist interface between training scripts and search\nalgorithms. We show that this interface meets the requirements for a broad\nrange of hyperparameter search algorithms, allows straightforward scaling of\nsearch to large clusters, and simplifies algorithm implementation. We\ndemonstrate the implementation of several state-of-the-art hyperparameter\nsearch algorithms in Tune. Tune is available at\nhttp://ray.readthedocs.io/en/latest/tune.html. \n\n"}
{"id": "1807.06343", "contents": "Title: Learning with SGD and Random Features Abstract: Sketching and stochastic gradient methods are arguably the most common\ntechniques to derive efficient large scale learning algorithms. In this paper,\nwe investigate their application in the context of nonparametric statistical\nlearning. More precisely, we study the estimator defined by stochastic gradient\nwith mini batches and random features. The latter can be seen as form of\nnonlinear sketching and used to define approximate kernel methods. The\nconsidered estimator is not explicitly penalized/constrained and regularization\nis implicit. Indeed, our study highlights how different parameters, such as\nnumber of features, iterations, step-size and mini-batch size control the\nlearning properties of the solutions. We do this by deriving optimal finite\nsample bounds, under standard assumptions. The obtained results are\ncorroborated and illustrated by numerical experiments. \n\n"}
{"id": "1807.06358", "contents": "Title: IntroVAE: Introspective Variational Autoencoders for Photographic Image\n  Synthesis Abstract: We present a novel introspective variational autoencoder (IntroVAE) model for\nsynthesizing high-resolution photographic images. IntroVAE is capable of\nself-evaluating the quality of its generated samples and improving itself\naccordingly. Its inference and generator models are jointly trained in an\nintrospective way. On one hand, the generator is required to reconstruct the\ninput images from the noisy outputs of the inference model as normal VAEs. On\nthe other hand, the inference model is encouraged to classify between the\ngenerated and real samples while the generator tries to fool it as GANs. These\ntwo famous generative frameworks are integrated in a simple yet efficient\nsingle-stream architecture that can be trained in a single stage. IntroVAE\npreserves the advantages of VAEs, such as stable training and nice latent\nmanifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires\nno extra discriminators, because the inference model itself serves as a\ndiscriminator to distinguish between the generated and real samples.\nExperiments demonstrate that our method produces high-resolution\nphoto-realistic images (e.g., CELEBA images at \\(1024^{2}\\)), which are\ncomparable to or better than the state-of-the-art GANs. \n\n"}
{"id": "1807.06630", "contents": "Title: Expressive power of outer product manifolds on feed-forward neural\n  networks Abstract: Hierarchical neural networks are exponentially more efficient than their\ncorresponding \"shallow\" counterpart with the same expressive power, but involve\nhuge number of parameters and require tedious amounts of training. Our main\nidea is to mathematically understand and describe the hierarchical structure of\nfeedforward neural networks by reparametrization invariant Riemannian metrics.\nBy computing or approximating the tangent subspace, we better utilize the\noriginal network via sparse representations that enables switching to shallow\nnetworks after a very early training stage. Our experiments show that the\nproposed approximation of the metric improves and sometimes even surpasses the\nachievable performance of the original network significantly even after a few\nepochs of training the original feedforward network. \n\n"}
{"id": "1807.06689", "contents": "Title: Efficient Deep Learning on Multi-Source Private Data Abstract: Machine learning models benefit from large and diverse datasets. Using such\ndatasets, however, often requires trusting a centralized data aggregator. For\nsensitive applications like healthcare and finance this is undesirable as it\ncould compromise patient privacy or divulge trade secrets. Recent advances in\nsecure and privacy-preserving computation, including trusted hardware enclaves\nand differential privacy, offer a way for mutually distrusting parties to\nefficiently train a machine learning model without revealing the training data.\nIn this work, we introduce Myelin, a deep learning framework which combines\nthese privacy-preservation primitives, and use it to establish a baseline level\nof performance for fully private machine learning. \n\n"}
{"id": "1807.06786", "contents": "Title: Deep Content-User Embedding Model for Music Recommendation Abstract: Recently deep learning based recommendation systems have been actively\nexplored to solve the cold-start problem using a hybrid approach. However, the\nmajority of previous studies proposed a hybrid model where collaborative\nfiltering and content-based filtering modules are independently trained. The\nend-to-end approach that takes different modality data as input and jointly\ntrains the model can provide better optimization but it has not been fully\nexplored yet. In this work, we propose deep content-user embedding model, a\nsimple and intuitive architecture that combines the user-item interaction and\nmusic audio content. We evaluate the model on music recommendation and music\nauto-tagging tasks. The results show that the proposed model significantly\noutperforms the previous work. We also discuss various directions to improve\nthe proposed model further. \n\n"}
{"id": "1807.06972", "contents": "Title: Data-Efficient Weakly Supervised Learning for Low-Resource Audio Event\n  Detection Using Deep Learning Abstract: We propose a method to perform audio event detection under the common\nconstraint that only limited training data are available. In training a deep\nlearning system to perform audio event detection, two practical problems arise.\nFirstly, most datasets are \"weakly labelled\" having only a list of events\npresent in each recording without any temporal information for training.\nSecondly, deep neural networks need a very large amount of labelled training\ndata to achieve good quality performance, yet in practice it is difficult to\ncollect enough samples for most classes of interest. In this paper, we propose\na data-efficient training of a stacked convolutional and recurrent neural\nnetwork. This neural network is trained in a multi instance learning setting\nfor which we introduce a new loss function that leads to improved training\ncompared to the usual approaches for weakly supervised learning. We\nsuccessfully test our approach on two low-resource datasets that lack temporal\nlabels. \n\n"}
{"id": "1807.09245", "contents": "Title: Visual Dynamics: Stochastic Future Generation via Layered Cross\n  Convolutional Networks Abstract: We study the problem of synthesizing a number of likely future frames from a\nsingle input image. In contrast to traditional methods that have tackled this\nproblem in a deterministic or non-parametric way, we propose to model future\nframes in a probabilistic manner. Our probabilistic model makes it possible for\nus to sample and synthesize many possible future frames from a single input\nimage. To synthesize realistic movement of objects, we propose a novel network\nstructure, namely a Cross Convolutional Network; this network encodes image and\nmotion information as feature maps and convolutional kernels, respectively. In\nexperiments, our model performs well on synthetic data, such as 2D shapes and\nanimated game sprites, and on real-world video frames. We present analyses of\nthe learned network representations, showing it is implicitly learning a\ncompact encoding of object appearance and motion. We also demonstrate a few of\nits applications, including visual analogy-making and video extrapolation. \n\n"}
{"id": "1807.09434", "contents": "Title: Distinctive-attribute Extraction for Image Captioning Abstract: Image captioning, an open research issue, has been evolved with the progress\nof deep neural networks. Convolutional neural networks (CNNs) and recurrent\nneural networks (RNNs) are employed to compute image features and generate\nnatural language descriptions in the research. In previous works, a caption\ninvolving semantic description can be generated by applying additional\ninformation into the RNNs. In this approach, we propose a distinctive-attribute\nextraction (DaE) which explicitly encourages significant meanings to generate\nan accurate caption describing the overall meaning of the image with their\nunique situation. Specifically, the captions of training images are analyzed by\nterm frequency-inverse document frequency (TF-IDF), and the analyzed semantic\ninformation is trained to extract distinctive-attributes for inferring\ncaptions. The proposed scheme is evaluated on a challenge data, and it improves\nan objective performance while describing images in more detail. \n\n"}
{"id": "1807.09586", "contents": "Title: Perturb and Combine to Identify Influential Spreaders in Real-World\n  Networks Abstract: Some of the most effective influential spreader detection algorithms are\nunstable to small perturbations of the network structure. Inspired by bagging\nin Machine Learning, we propose the first Perturb and Combine (P&C) procedure\nfor networks. It (1) creates many perturbed versions of a given graph, (2)\napplies a node scoring function separately to each graph, and (3) combines the\nresults. Experiments conducted on real-world networks of various sizes with the\nk-core, generalized k-core, and PageRank algorithms reveal that P&C brings\nsubstantial improvements. Moreover, this performance boost can be obtained at\nalmost no extra cost through parallelization. Finally, a bias-variance analysis\nsuggests that P&C works mainly by reducing bias, and that therefore, it should\nbe capable of improving the performance of all vertex scoring functions,\nincluding stable ones. \n\n"}
{"id": "1807.09901", "contents": "Title: Neural State Classification for Hybrid Systems Abstract: We introduce the State Classification Problem (SCP) for hybrid systems, and\npresent Neural State Classification (NSC) as an efficient solution technique.\nSCP generalizes the model checking problem as it entails classifying each state\n$s$ of a hybrid automaton as either positive or negative, depending on whether\nor not $s$ satisfies a given time-bounded reachability specification. This is\nan interesting problem in its own right, which NSC solves using\nmachine-learning techniques, Deep Neural Networks in particular. State\nclassifiers produced by NSC tend to be very efficient (run in constant time and\nspace), but may be subject to classification errors. To quantify and mitigate\nsuch errors, our approach comprises: i) techniques for certifying, with\nstatistical guarantees, that an NSC classifier meets given accuracy levels; ii)\ntuning techniques, including a novel technique based on adversarial sampling,\nthat can virtually eliminate false negatives (positive states classified as\nnegative), thereby making the classifier more conservative. We have applied NSC\nto six nonlinear hybrid system benchmarks, achieving an accuracy of 99.25% to\n99.98%, and a false-negative rate of 0.0033 to 0, which we further reduced to\n0.0015 to 0 after tuning the classifier. We believe that this level of accuracy\nis acceptable in many practical applications, and that these results\ndemonstrate the promise of the NSC approach. \n\n"}
{"id": "1807.10478", "contents": "Title: Interpreting recurrent neural networks behaviour via excitable network\n  attractors Abstract: Introduction: Machine learning provides fundamental tools both for scientific\nresearch and for the development of technologies with significant impact on\nsociety. It provides methods that facilitate the discovery of regularities in\ndata and that give predictions without explicit knowledge of the rules\ngoverning a system. However, a price is paid for exploiting such flexibility:\nmachine learning methods are typically black-boxes where it is difficult to\nfully understand what the machine is doing or how it is operating. This poses\nconstraints on the applicability and explainability of such methods. Methods:\nOur research aims to open the black-box of recurrent neural networks, an\nimportant family of neural networks used for processing sequential data. We\npropose a novel methodology that provides a mechanistic interpretation of\nbehaviour when solving a computational task. Our methodology uses mathematical\nconstructs called excitable network attractors, which are invariant sets in\nphase space composed of stable attractors and excitable connections between\nthem. Results and Discussion: As the behaviour of recurrent neural networks\ndepends both on training and on inputs to the system, we introduce an algorithm\nto extract network attractors directly from the trajectory of a neural network\nwhile solving tasks. Simulations conducted on a controlled benchmark task\nconfirm the relevance of these attractors for interpreting the behaviour of\nrecurrent neural networks, at least for tasks that involve learning a finite\nnumber of stable states and transitions between them. \n\n"}
{"id": "1807.10570", "contents": "Title: Embedded Implementation of a Deep Learning Smile Detector Abstract: In this paper we study the real time deployment of deep learning algorithms\nin low resource computational environments. As the use case, we compare the\naccuracy and speed of neural networks for smile detection using different\nneural network architectures and their system level implementation on NVidia\nJetson embedded platform. We also propose an asynchronous multithreading scheme\nfor parallelizing the pipeline. Within this framework, we experimentally\ncompare thirteen widely used network topologies. The experiments show that low\ncomplexity architectures can achieve almost equal performance as larger ones,\nwith a fraction of computation required. \n\n"}
{"id": "1807.10584", "contents": "Title: Uncertainty and Interpretability in Convolutional Neural Networks for\n  Semantic Segmentation of Colorectal Polyps Abstract: Convolutional Neural Networks (CNNs) are propelling advances in a range of\ndifferent computer vision tasks such as object detection and object\nsegmentation. Their success has motivated research in applications of such\nmodels for medical image analysis. If CNN-based models are to be helpful in a\nmedical context, they need to be precise, interpretable, and uncertainty in\npredictions must be well understood. In this paper, we develop and evaluate\nrecent advances in uncertainty estimation and model interpretability in the\ncontext of semantic segmentation of polyps from colonoscopy images. We evaluate\nand enhance several architectures of Fully Convolutional Networks (FCNs) for\nsemantic segmentation of colorectal polyps and provide a comparison between\nthese models. Our highest performing model achieves a 76.06\\% mean IOU accuracy\non the EndoScene dataset, a considerable improvement over the previous\nstate-of-the-art. \n\n"}
{"id": "1807.10588", "contents": "Title: A Modality-Adaptive Method for Segmenting Brain Tumors and\n  Organs-at-Risk in Radiation Therapy Planning Abstract: In this paper we present a method for simultaneously segmenting brain tumors\nand an extensive set of organs-at-risk for radiation therapy planning of\nglioblastomas. The method combines a contrast-adaptive generative model for\nwhole-brain segmentation with a new spatial regularization model of tumor shape\nusing convolutional restricted Boltzmann machines. We demonstrate\nexperimentally that the method is able to adapt to image acquisitions that\ndiffer substantially from any available training data, ensuring its\napplicability across treatment sites; that its tumor segmentation accuracy is\ncomparable to that of the current state of the art; and that it captures most\norgans-at-risk sufficiently well for radiation therapy planning purposes. The\nproposed method may be a valuable step towards automating the delineation of\nbrain tumors and organs-at-risk in glioblastoma patients undergoing radiation\ntherapy. \n\n"}
{"id": "1807.10707", "contents": "Title: End-to-end Deep Learning from Raw Sensor Data: Atrial Fibrillation\n  Detection using Wearables Abstract: We present a convolutional-recurrent neural network architecture with long\nshort-term memory for real-time processing and classification of digital sensor\ndata. The network implicitly performs typical signal processing tasks such as\nfiltering and peak detection, and learns time-resolved embeddings of the input\nsignal. We use a prototype multi-sensor wearable device to collect over 180h of\nphotoplethysmography (PPG) data sampled at 20Hz, of which 36h are during atrial\nfibrillation (AFib). We use end-to-end learning to achieve state-of-the-art\nresults in detecting AFib from raw PPG data. For classification labels output\nevery 0.8s, we demonstrate an area under ROC curve of 0.9999, with false\npositive and false negative rates both below $2\\times 10^{-3}$. This\nconstitutes a significant improvement on previous results utilising\ndomain-specific feature engineering, such as heart rate extraction, and brings\nlarge-scale atrial fibrillation screenings within imminent reach. \n\n"}
{"id": "1807.10728", "contents": "Title: Deep PDF: Probabilistic Surface Optimization and Density Estimation Abstract: A probability density function (pdf) encodes the entire stochastic knowledge\nabout data distribution, where data may represent stochastic observations in\nrobotics, transition state pairs in reinforcement learning or any other\nempirically acquired modality. Inferring data pdf is of prime importance,\nallowing to analyze various model hypotheses and perform smart decision making.\nHowever, most density estimation techniques are limited in their representation\nexpressiveness to specific kernel type or predetermined distribution family,\nand have other restrictions. For example, kernel density estimation (KDE)\nmethods require meticulous parameter search and are extremely slow at querying\nnew points. In this paper we present a novel non-parametric density estimation\napproach, DeepPDF, that uses a neural network to approximate a target pdf given\nsamples from thereof. Such a representation provides high inference accuracy\nfor a wide range of target pdfs using a relatively simple network structure,\nmaking our method highly statistically robust. This is done via a new\nstochastic optimization algorithm, \\emph{Probabilistic Surface Optimization}\n(PSO), that turns to advantage the stochastic nature of sample points in order\nto force network output to be identical to the output of a target pdf. Once\ntrained, query point evaluation can be efficiently done in DeepPDF by a simple\nnetwork forward pass, with linear complexity in the number of query points.\nMoreover, the PSO algorithm is capable of inferring the frequency of data\nsamples and may also be used in other statistical tasks such as conditional\nestimation and distribution transformation. We compare the derived approach\nwith KDE methods showing its superior performance and accuracy. \n\n"}
{"id": "1807.10756", "contents": "Title: False Positive Reduction by Actively Mining Negative Samples for\n  Pulmonary Nodule Detection in Chest Radiographs Abstract: Generating large quantities of quality labeled data in medical imaging is\nvery time consuming and expensive. The performance of supervised algorithms for\nvarious tasks on imaging has improved drastically over the years, however the\navailability of data to train these algorithms have become one of the main\nbottlenecks for implementation. To address this, we propose a semi-supervised\nlearning method where pseudo-negative labels from unlabeled data are used to\nfurther refine the performance of a pulmonary nodule detection network in chest\nradiographs. After training with the proposed network, the false positive rate\nwas reduced to 0.1266 from 0.4864 while maintaining sensitivity at 0.89. \n\n"}
{"id": "1807.10876", "contents": "Title: Transportation Modes Classification Using Feature Engineering Abstract: Predicting transportation modes from GPS (Global Positioning System) records\nis a hot topic in the trajectory mining domain. Each GPS record is called a\ntrajectory point and a trajectory is a sequence of these points. Trajectory\nmining has applications including but not limited to transportation mode\ndetection, tourism, traffic congestion, smart cities management, animal\nbehaviour analysis, environmental preservation, and traffic dynamics are some\nof the trajectory mining applications. Transportation modes prediction as one\nof the tasks in human mobility and vehicle mobility applications plays an\nimportant role in resource allocation, traffic management systems, tourism\nplanning and accident detection. In this work, the proposed framework in Etemad\net al. is extended to consider other aspects in the task of transportation\nmodes prediction. Wrapper search and information retrieval methods were\ninvestigated to find the best subset of trajectory features. Finding the best\nclassifier and the best feature subset, the framework is compared against two\nrelated papers that applied deep learning methods. The results show that our\nframework achieved better performance. Moreover, the ground truth noise removal\nimproved accuracy of transportation modes prediction task; however, the\nassumption of having access to test set labels in pre-processing task is\ninvalid. Furthermore, the cross validation approaches were investigated and the\nperformance results show that the random cross validation method provides\noptimistic results. \n\n"}
{"id": "1807.10956", "contents": "Title: Group-sparse SVD Models and Their Applications in Biological Data Abstract: Sparse Singular Value Decomposition (SVD) models have been proposed for\nbiclustering high dimensional gene expression data to identify block patterns\nwith similar expressions. However, these models do not take into account prior\ngroup effects upon variable selection. To this end, we first propose\ngroup-sparse SVD models with group Lasso (GL1-SVD) and group L0-norm penalty\n(GL0-SVD) for non-overlapping group structure of variables. However, such\ngroup-sparse SVD models limit their applicability in some problems with\noverlapping structure. Thus, we also propose two group-sparse SVD models with\noverlapping group Lasso (OGL1-SVD) and overlapping group L0-norm penalty\n(OGL0-SVD). We first adopt an alternating iterative strategy to solve GL1-SVD\nbased on a block coordinate descent method, and GL0-SVD based on a projection\nmethod. The key of solving OGL1-SVD is a proximal operator with overlapping\ngroup Lasso penalty. We employ an alternating direction method of multipliers\n(ADMM) to solve the proximal operator. Similarly, we develop an approximate\nmethod to solve OGL0-SVD. Applications of these methods and comparison with\ncompeting ones using simulated data demonstrate their effectiveness. Extensive\napplications of them onto several real gene expression data with gene prior\ngroup knowledge identify some biologically interpretable gene modules. \n\n"}
{"id": "1807.11167", "contents": "Title: A Group-Theoretic Approach to Computational Abstraction: Symmetry-Driven\n  Hierarchical Clustering Abstract: Abstraction plays a key role in concept learning and knowledge discovery;\nthis paper is concerned with computational abstraction. In particular, we study\nthe nature of abstraction through a group-theoretic approach, formalizing it as\nsymmetry-driven---as opposed to data-driven---hierarchical clustering. Thus,\nthe resulting clustering framework is data-free, feature-free, similarity-free,\nand globally hierarchical---the four key features that distinguish it from\ncommon data clustering models such as $k$-means. Beyond a theoretical\nfoundation for abstraction, we also present a top-down and a bottom-up approach\nto establish an algorithmic foundation for practical abstraction-generating\nmethods. Lastly, via both a theoretical explanation and a real-world\napplication, we illustrate that further coupling of our abstraction framework\nwith statistics realizes Shannon's information lattice and even further, brings\nlearning into the picture. This not only presents one use case of our proposed\ncomputational abstraction, but also gives a first step towards a principled and\ncognitive way of automatic concept learning and knowledge discovery. \n\n"}
{"id": "1807.11605", "contents": "Title: Doubly Attentive Transformer Machine Translation Abstract: In this paper a doubly attentive transformer machine translation model\n(DATNMT) is presented in which a doubly-attentive transformer decoder normally\njoins spatial visual features obtained via pretrained convolutional neural\nnetworks, conquering any gap between image captioning and translation. In this\nframework, the transformer decoder figures out how to take care of\nsource-language words and parts of an image freely by methods for two separate\nattention components in an Enhanced Multi-Head Attention Layer of doubly\nattentive transformer, as it generates words in the target language. We find\nthat the proposed model can effectively exploit not just the scarce multimodal\nmachine translation data, but also large general-domain text-only machine\ntranslation corpora, or image-text image captioning corpora. The experimental\nresults show that the proposed doubly-attentive transformer-decoder performs\nbetter than a single-decoder transformer model, and gives the state-of-the-art\nresults in the English-German multimodal machine translation task. \n\n"}
{"id": "1807.11712", "contents": "Title: RiTUAL-UH at TRAC 2018 Shared Task: Aggression Identification Abstract: This paper presents our system for \"TRAC 2018 Shared Task on Aggression\nIdentification\". Our best systems for the English dataset use a combination of\nlexical and semantic features. However, for Hindi data using only lexical\nfeatures gave us the best results. We obtained weighted F1- measures of 0.5921\nfor the English Facebook task (ranked 12th), 0.5663 for the English Social\nMedia task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and\n0.4853 for the Hindi Social Media task (ranked 2nd). \n\n"}
{"id": "1808.00387", "contents": "Title: Just Interpolate: Kernel \"Ridgeless\" Regression Can Generalize Abstract: In the absence of explicit regularization, Kernel \"Ridgeless\" Regression with\nnonlinear kernels has the potential to fit the training data perfectly. It has\nbeen observed empirically, however, that such interpolated solutions can still\ngeneralize well on test data. We isolate a phenomenon of implicit\nregularization for minimum-norm interpolated solutions which is due to a\ncombination of high dimensionality of the input data, curvature of the kernel\nfunction, and favorable geometric properties of the data such as an eigenvalue\ndecay of the empirical covariance and kernel matrices. In addition to deriving\na data-dependent upper bound on the out-of-sample error, we present\nexperimental evidence suggesting that the phenomenon occurs in the MNIST\ndataset. \n\n"}
{"id": "1808.01204", "contents": "Title: Learning Overparameterized Neural Networks via Stochastic Gradient\n  Descent on Structured Data Abstract: Neural networks have many successful applications, while much less\ntheoretical understanding has been gained. Towards bridging this gap, we study\nthe problem of learning a two-layer overparameterized ReLU neural network for\nmulti-class classification via stochastic gradient descent (SGD) from random\ninitialization. In the overparameterized setting, when the data comes from\nmixtures of well-separated distributions, we prove that SGD learns a network\nwith a small generalization error, albeit the network has enough capacity to\nfit arbitrary labels. Furthermore, the analysis provides interesting insights\ninto several aspects of learning neural networks and can be verified based on\nempirical studies on synthetic data and on the MNIST dataset. \n\n"}
{"id": "1808.01664", "contents": "Title: Structured Adversarial Attack: Towards General Implementation and Better\n  Interpretability Abstract: When generating adversarial examples to attack deep neural networks (DNNs),\nLp norm of the added perturbation is usually used to measure the similarity\nbetween original image and adversarial example. However, such adversarial\nattacks perturbing the raw input spaces may fail to capture structural\ninformation hidden in the input. This work develops a more general attack\nmodel, i.e., the structured attack (StrAttack), which explores group sparsity\nin adversarial perturbations by sliding a mask through images aiming for\nextracting key spatial structures. An ADMM (alternating direction method of\nmultipliers)-based framework is proposed that can split the original problem\ninto a sequence of analytically solvable subproblems and can be generalized to\nimplement other attacking methods. Strong group sparsity is achieved in\nadversarial perturbations even with the same level of Lp norm distortion as the\nstate-of-the-art attacks. We demonstrate the effectiveness of StrAttack by\nextensive experimental results onMNIST, CIFAR-10, and ImageNet. We also show\nthat StrAttack provides better interpretability (i.e., better correspondence\nwith discriminative image regions)through adversarial saliency map (Papernot et\nal., 2016b) and class activation map(Zhou et al., 2016). \n\n"}
{"id": "1808.01960", "contents": "Title: Distributional Multivariate Policy Evaluation and Exploration with the\n  Bellman GAN Abstract: The recently proposed distributional approach to reinforcement learning\n(DiRL) is centered on learning the distribution of the reward-to-go, often\nreferred to as the value distribution. In this work, we show that the\ndistributional Bellman equation, which drives DiRL methods, is equivalent to a\ngenerative adversarial network (GAN) model. In this formulation, DiRL can be\nseen as learning a deep generative model of the value distribution, driven by\nthe discrepancy between the distribution of the current value, and the\ndistribution of the sum of current reward and next value. We use this insight\nto propose a GAN-based approach to DiRL, which leverages the strengths of GANs\nin learning distributions of high-dimensional data. In particular, we show that\nour GAN approach can be used for DiRL with multivariate rewards, an important\nsetting which cannot be tackled with prior methods. The multivariate setting\nalso allows us to unify learning the distribution of values and state\ntransitions, and we exploit this idea to devise a novel exploration method that\nis driven by the discrepancy in estimating both values and states. \n\n"}
{"id": "1808.01974", "contents": "Title: A Survey on Deep Transfer Learning Abstract: As a new classification platform, deep learning has recently received\nincreasing attention from researchers and has been successfully applied to many\ndomains. In some domains, like bioinformatics and robotics, it is very\ndifficult to construct a large-scale well-annotated dataset due to the expense\nof data acquisition and costly annotation, which limits its development.\nTransfer learning relaxes the hypothesis that the training data must be\nindependent and identically distributed (i.i.d.) with the test data, which\nmotivates us to use transfer learning to solve the problem of insufficient\ntraining data. This survey focuses on reviewing the current researches of\ntransfer learning by using deep neural network and its applications. We defined\ndeep transfer learning, category and review the recent research works based on\nthe techniques used in deep transfer learning. \n\n"}
{"id": "1808.02234", "contents": "Title: Deep Stacked Stochastic Configuration Networks for Lifelong Learning of\n  Non-Stationary Data Streams Abstract: The concept of SCN offers a fast framework with universal approximation\nguarantee for lifelong learning of non-stationary data streams. Its adaptive\nscope selection property enables for proper random generation of hidden unit\nparameters advancing conventional randomized approaches constrained with a\nfixed scope of random parameters. This paper proposes deep stacked stochastic\nconfiguration network (DSSCN) for continual learning of non-stationary data\nstreams which contributes two major aspects: 1) DSSCN features a\nself-constructing methodology of deep stacked network structure where hidden\nunit and hidden layer are extracted automatically from continuously generated\ndata streams; 2) the concept of SCN is developed to randomly assign inverse\ncovariance matrix of multivariate Gaussian function in the hidden node addition\nstep bypassing its computationally prohibitive tuning phase. Numerical\nevaluation and comparison with prominent data stream algorithms under two\nprocedures: periodic hold-out and prequential test-then-train processes\ndemonstrate the advantage of proposed methodology. \n\n"}
{"id": "1808.02480", "contents": "Title: Deep context: end-to-end contextual speech recognition Abstract: In automatic speech recognition (ASR) what a user says depends on the\nparticular context she is in. Typically, this context is represented as a set\nof word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E)\nASR sys- tem that utilizes such context. Our approach, which we re- fer to as\nContextual Listen, Attend and Spell (CLAS) jointly- optimizes the ASR\ncomponents along with embeddings of the context n-grams. During inference, the\nCLAS system can be presented with context phrases which might contain out-of-\nvocabulary (OOV) terms not seen during training. We com- pare our proposed\nsystem to a more traditional contextualiza- tion approach, which performs\nshallow-fusion between inde- pendently trained LAS and contextual n-gram models\nduring beam search. Across a number of tasks, we find that the pro- posed CLAS\nsystem outperforms the baseline method by as much as 68% relative WER,\nindicating the advantage of joint optimization over individually trained\ncomponents. Index Terms: speech recognition, sequence-to-sequence models,\nlisten attend and spell, LAS, attention, embedded speech recognition. \n\n"}
{"id": "1808.02622", "contents": "Title: Learning to Write Notes in Electronic Health Records Abstract: Clinicians spend a significant amount of time inputting free-form textual\nnotes into Electronic Health Records (EHR) systems. Much of this documentation\nwork is seen as a burden, reducing time spent with patients and contributing to\nclinician burnout. With the aspiration of AI-assisted note-writing, we propose\na new language modeling task predicting the content of notes conditioned on\npast data from a patient's medical record, including patient demographics,\nlabs, medications, and past notes. We train generative models using the public,\nde-identified MIMIC-III dataset and compare generated notes with those in the\ndataset on multiple measures. We find that much of the content can be\npredicted, and that many common templates found in notes can be learned. We\ndiscuss how such models can be useful in supporting assistive note-writing\nfeatures such as error-detection and auto-complete. \n\n"}
{"id": "1808.03096", "contents": "Title: On feature selection and evaluation of transportation mode prediction\n  strategies Abstract: Transportation modes prediction is a fundamental task for decision making in\nsmart cities and traffic management systems. Traffic policies designed based on\ntrajectory mining can save money and time for authorities and the public. It\nmay reduce the fuel consumption and commute time and moreover, may provide more\npleasant moments for residents and tourists. Since the number of features that\nmay be used to predict a user transportation mode can be substantial, finding a\nsubset of features that maximizes a performance measure is worth investigating.\nIn this work, we explore wrapper and information retrieval methods to find the\nbest subset of trajectory features. After finding the best classifier and the\nbest feature subset, our results were compared with two related papers that\napplied deep learning methods and the results showed that our framework\nachieved better performance. Furthermore, two types of cross-validation\napproaches were investigated, and the performance results show that the random\ncross-validation method provides optimistic results. \n\n"}
{"id": "1808.03703", "contents": "Title: LemmaTag: Jointly Tagging and Lemmatizing for Morphologically-Rich\n  Languages with BRNNs Abstract: We present LemmaTag, a featureless neural network architecture that jointly\ngenerates part-of-speech tags and lemmas for sentences by using bidirectional\nRNNs with character-level and word-level embeddings. We demonstrate that both\ntasks benefit from sharing the encoding part of the network, predicting tag\nsubcategories, and using the tagger output as an input to the lemmatizer. We\nevaluate our model across several languages with complex morphology, which\nsurpasses state-of-the-art accuracy in both part-of-speech tagging and\nlemmatization in Czech, German, and Arabic. \n\n"}
{"id": "1808.03715", "contents": "Title: This Time with Feeling: Learning Expressive Musical Performance Abstract: Music generation has generally been focused on either creating scores or\ninterpreting them. We discuss differences between these two problems and\npropose that, in fact, it may be valuable to work in the space of direct $\\it\nperformance$ generation: jointly predicting the notes $\\it and$ $\\it also$\ntheir expressive timing and dynamics. We consider the significance and\nqualities of the data set needed for this. Having identified both a problem\ndomain and characteristics of an appropriate data set, we show an LSTM-based\nrecurrent network model that subjectively performs quite well on this task.\nCritically, we provide generated examples. We also include feedback from\nprofessional composers and musicians about some of these examples. \n\n"}
{"id": "1808.03857", "contents": "Title: Ranking with Features: Algorithm and A Graph Theoretic Analysis Abstract: We consider the problem of ranking a set of items from pairwise comparisons\nin the presence of features associated with the items. Recent works have\nestablished that $O(n\\log(n))$ samples are needed to rank well when there is no\nfeature information present. However, this might be sub-optimal in the presence\nof associated features. We introduce a new probabilistic preference model\ncalled feature-Bradley-Terry-Luce (f-BTL) model that generalizes the standard\nBTL model to incorporate feature information. We present a new least squares\nbased algorithm called fBTL-LS which we show requires much lesser than\n$O(n\\log(n))$ pairs to obtain a good ranking -- precisely our new sample\ncomplexity bound is of $O(\\alpha\\log \\alpha)$, where $\\alpha$ denotes the\nnumber of `independent items' of the set, in general $\\alpha << n$. Our\nanalysis is novel and makes use of tools from classical graph matching theory\nto provide tighter bounds that sheds light on the true complexity of the\nranking problem, capturing the item dependencies in terms of their feature\nrepresentations. This was not possible with earlier matrix completion based\ntools used for this problem. We also prove an information theoretic lower bound\non the required sample complexity for recovering the underlying ranking, which\nessentially shows the tightness of our proposed algorithms. The efficacy of our\nproposed algorithms are validated through extensive experimental evaluations on\na variety of synthetic and real world datasets. \n\n"}
{"id": "1808.03920", "contents": "Title: Multimodal Language Analysis with Recurrent Multistage Fusion Abstract: Computational modeling of human multimodal language is an emerging research\narea in natural language processing spanning the language, visual and acoustic\nmodalities. Comprehending multimodal language requires modeling not only the\ninteractions within each modality (intra-modal interactions) but more\nimportantly the interactions between modalities (cross-modal interactions). In\nthis paper, we propose the Recurrent Multistage Fusion Network (RMFN) which\ndecomposes the fusion problem into multiple stages, each of them focused on a\nsubset of multimodal signals for specialized, effective fusion. Cross-modal\ninteractions are modeled using this multistage fusion approach which builds\nupon intermediate representations of previous stages. Temporal and intra-modal\ninteractions are modeled by integrating our proposed fusion approach with a\nsystem of recurrent neural networks. The RMFN displays state-of-the-art\nperformance in modeling human multimodal language across three public datasets\nrelating to multimodal sentiment analysis, emotion recognition, and speaker\ntraits recognition. We provide visualizations to show that each stage of fusion\nfocuses on a different subset of multimodal signals, learning increasingly\ndiscriminative multimodal representations. \n\n"}
{"id": "1808.04311", "contents": "Title: Design Flow of Accelerating Hybrid Extremely Low Bit-width Neural\n  Network in Embedded FPGA Abstract: Neural network accelerators with low latency and low energy consumption are\ndesirable for edge computing. To create such accelerators, we propose a design\nflow for accelerating the extremely low bit-width neural network (ELB-NN) in\nembedded FPGAs with hybrid quantization schemes. This flow covers both network\ntraining and FPGA-based network deployment, which facilitates the design space\nexploration and simplifies the tradeoff between network accuracy and\ncomputation efficiency. Using this flow helps hardware designers to deliver a\nnetwork accelerator in edge devices under strict resource and power\nconstraints. We present the proposed flow by supporting hybrid ELB settings\nwithin a neural network. Results show that our design can deliver very high\nperformance peaking at 10.3 TOPS and classify up to 325.3 image/s/watt while\nrunning large-scale neural networks for less than 5W using embedded FPGA. To\nthe best of our knowledge, it is the most energy efficient solution in\ncomparison to GPU or other FPGA implementations reported so far in the\nliterature. \n\n"}
{"id": "1808.04357", "contents": "Title: RedSync : Reducing Synchronization Traffic for Distributed Deep Learning Abstract: Data parallelism has become a dominant method to scale Deep Neural Network\n(DNN) training across multiple nodes. Since synchronizing a large number of\ngradients of the local model can be a bottleneck for large-scale distributed\ntraining, compressing communication data has gained widespread attention\nrecently. Among several recent proposed compression algorithms, Residual\nGradient Compression (RGC) is one of the most successful approaches---it can\nsignificantly compress the transmitting message size (0.1\\% of the gradient\nsize) of each node and still achieve correct accuracy and the same convergence\nspeed. However, the literature on compressing deep networks focuses almost\nexclusively on achieving good theoretical compression rate, while the\nefficiency of RGC in real distributed implementation has been less\ninvestigated. In this paper, we develop an RGC-based system that is able to\nreduce the end-to-end training time on real-world multi-GPU systems. Our\nproposed design called RedSync, which introduces a set of optimizations to\nreduce communication bandwidth requirement while introducing limited overhead.\nWe evaluate the performance of RedSync on two different multiple GPU platforms,\nincluding 128 GPUs of a supercomputer and an 8-GPU server. Our test cases\ninclude image classification tasks on Cifar10 and ImageNet, and language\nmodeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high\ncommunication to computation ratio, which have long been considered with poor\nscalability, RedSync brings significant performance improvements. \n\n"}
{"id": "1808.04759", "contents": "Title: An Overview and a Benchmark of Active Learning for Outlier Detection\n  with One-Class Classifiers Abstract: Active learning methods increase classification quality by means of user\nfeedback. An important subcategory is active learning for outlier detection\nwith one-class classifiers. While various methods in this category exist,\nselecting one for a given application scenario is difficult. This is because\nexisting methods rely on different assumptions, have different objectives, and\noften are tailored to a specific use case. All this calls for a comprehensive\ncomparison, the topic of this article. This article starts with a\ncategorization of the various methods. We then propose ways to evaluate active\nlearning results. Next, we run extensive experiments to compare existing\nmethods, for a broad variety of scenarios. Based on our results, we formulate\nguidelines on how to select active learning methods for outlier detection with\none-class classifiers. \n\n"}
{"id": "1808.04776", "contents": "Title: Retrieve and Refine: Improved Sequence Generation Models For Dialogue Abstract: Sequence generation models for dialogue are known to have several problems:\nthey tend to produce short, generic sentences that are uninformative and\nunengaging. Retrieval models on the other hand can surface interesting\nresponses, but are restricted to the given retrieval set leading to erroneous\nreplies that cannot be tuned to the specific context. In this work we develop a\nmodel that combines the two approaches to avoid both their deficiencies: first\nretrieve a response and then refine it -- the final sequence generator treating\nthe retrieval as additional context. We show on the recent CONVAI2 challenge\ntask our approach produces responses superior to both standard retrieval and\ngeneration models in human evaluations. \n\n"}
{"id": "1808.04926", "contents": "Title: How Much Reading Does Reading Comprehension Require? A Critical\n  Investigation of Popular Benchmarks Abstract: Many recent papers address reading comprehension, where examples consist of\n(question, passage, answer) tuples. Presumably, a model must combine\ninformation from both questions and passages to predict corresponding answers.\nHowever, despite intense interest in the topic, with hundreds of published\npapers vying for leaderboard dominance, basic questions about the difficulty of\nmany popular benchmarks remain unanswered. In this paper, we establish sensible\nbaselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding\nthat question- and passage-only models often perform surprisingly well. On $14$\nout of $20$ bAbI tasks, passage-only models achieve greater than $50\\%$\naccuracy, sometimes matching the full model. Interestingly, while CBT provides\n$20$-sentence stories only the last is needed for comparably accurate\nprediction. By comparison, SQuAD and CNN appear better-constructed. \n\n"}
{"id": "1808.04947", "contents": "Title: Collapse of Deep and Narrow Neural Nets Abstract: Recent theoretical work has demonstrated that deep neural networks have\nsuperior performance over shallow networks, but their training is more\ndifficult, e.g., they suffer from the vanishing gradient problem. This problem\ncan be typically resolved by the rectified linear unit (ReLU) activation.\nHowever, here we show that even for such activation, deep and narrow neural\nnetworks (NNs) will converge to erroneous mean or median states of the target\nfunction depending on the loss with high probability. Deep and narrow NNs are\nencountered in solving partial differential equations with high-order\nderivatives. We demonstrate this collapse of such NNs both numerically and\ntheoretically, and provide estimates of the probability of collapse. We also\nconstruct a diagram of a safe region for designing NNs that avoid the collapse\nto erroneous states. Finally, we examine different ways of initialization and\nnormalization that may avoid the collapse problem. Asymmetric initializations\nmay reduce the probability of collapse but do not totally eliminate it. \n\n"}
{"id": "1808.05163", "contents": "Title: A Simple Convolutional Generative Network for Next Item Recommendation Abstract: Convolutional Neural Networks (CNNs) have been recently introduced in the\ndomain of session-based next item recommendation. An ordered collection of past\nitems the user has interacted with in a session (or sequence) are embedded into\na 2-dimensional latent matrix, and treated as an image. The convolution and\npooling operations are then applied to the mapped item embeddings. In this\npaper, we first examine the typical session-based CNN recommender and show that\nboth the generative model and network architecture are suboptimal when modeling\nlong-range dependencies in the item sequence. To address the issues, we\nintroduce a simple, but very effective generative model that is capable of\nlearning high-level representation from both short- and long-range item\ndependencies. The network architecture of the proposed model is formed of a\nstack of \\emph{holed} convolutional layers, which can efficiently increase the\nreceptive fields without relying on the pooling operation. Another contribution\nis the effective use of residual block structure in recommender systems, which\ncan ease the optimization for much deeper networks. The proposed generative\nmodel attains state-of-the-art accuracy with less training time in the next\nitem recommendation task. It accordingly can be used as a powerful\nrecommendation baseline to beat in future, especially when there are long\nsequences of user feedback. \n\n"}
{"id": "1808.05535", "contents": "Title: Combining time-series and textual data for taxi demand prediction in\n  event areas: a deep learning approach Abstract: Accurate time-series forecasting is vital for numerous areas of application\nsuch as transportation, energy, finance, economics, etc. However, while modern\ntechniques are able to explore large sets of temporal data to build forecasting\nmodels, they typically neglect valuable information that is often available\nunder the form of unstructured text. Although this data is in a radically\ndifferent format, it often contains contextual explanations for many of the\npatterns that are observed in the temporal data. In this paper, we propose two\ndeep learning architectures that leverage word embeddings, convolutional layers\nand attention mechanisms for combining text information with time-series data.\nWe apply these approaches for the problem of taxi demand forecasting in event\nareas. Using publicly available taxi data from New York, we empirically show\nthat by fusing these two complementary cross-modal sources of information, the\nproposed models are able to significantly reduce the error in the forecasts. \n\n"}
{"id": "1808.06537", "contents": "Title: Ricean K-factor Estimation based on Channel Quality Indicator in OFDM\n  Systems using Neural Network Abstract: Ricean channel model is widely used in wireless communications to\ncharacterize the channels with a line-of-sight path. The Ricean K factor,\ndefined as the ratio of direct path and scattered paths, provides a good\nindication of the link quality. Most existing works estimate K factor based on\neither maximum-likelihood criterion or higher-order moments, and the existing\nworks are targeted at K-factor estimation at receiver side. In this work, a\nnovel approach is proposed. Cast as a classification problem, the estimation of\nK factor by neural network provides high accuracy. Moreover, the proposed\nK-factor estimation is done at transmitter side for transmit processing, thus\nsaving the limited feedback bandwidth. \n\n"}
{"id": "1808.06640", "contents": "Title: Adversarial Removal of Demographic Attributes from Text Data Abstract: Recent advances in Representation Learning and Adversarial Training seem to\nsucceed in removing unwanted features from the learned representation. We show\nthat demographic information of authors is encoded in -- and can be recovered\nfrom -- the intermediate representations learned by text-based neural\nclassifiers. The implication is that decisions of classifiers trained on\ntextual data are not agnostic to -- and likely condition on -- demographic\nattributes. When attempting to remove such demographic information using\nadversarial training, we find that while the adversarial component achieves\nchance-level development-set accuracy during training, a post-hoc classifier,\ntrained on the encoded sentences from the first part, still manages to reach\nsubstantially higher classification accuracies on the same data. This behavior\nis consistent across several tasks, demographic properties and datasets. We\nexplore several techniques to improve the effectiveness of the adversarial\ncomponent. Our main conclusion is a cautionary one: do not rely on the\nadversarial training to achieve invariant representation to sensitive features. \n\n"}
{"id": "1808.06645", "contents": "Title: Stochastic Combinatorial Ensembles for Defending Against Adversarial\n  Examples Abstract: Many deep learning algorithms can be easily fooled with simple adversarial\nexamples. To address the limitations of existing defenses, we devised a\nprobabilistic framework that can generate an exponentially large ensemble of\nmodels from a single model with just a linear cost. This framework takes\nadvantage of neural network depth and stochastically decides whether or not to\ninsert noise removal operators such as VAEs between layers. We show empirically\nthe important role that model gradients have when it comes to determining\ntransferability of adversarial examples, and take advantage of this result to\ndemonstrate that it is possible to train models with limited adversarial attack\ntransferability. Additionally, we propose a detection method based on metric\nlearning in order to detect adversarial examples that have no hope of being\ncleaned of maliciously engineered noise. \n\n"}
{"id": "1808.08124", "contents": "Title: Insect cyborgs: Bio-mimetic feature generators improve machine learning\n  accuracy on limited data Abstract: Machine learning (ML) classifiers always benefit from more informative input\nfeatures. We seek to auto-generate stronger feature sets in order to address\nthe difficulty that ML methods often experience given limited training data. A\nwide range of biological neural nets (BNNs) excel at fast learning, implying\nthat they are adept at extracting informative features. We can thus look to\nBNNs for tools to improve ML performance in this low-data regime. The insect\nolfactory network learns new odors very rapidly, by means of three key\nelements: A competitive inhibition layer; a high-dimensional sparse plastic\nlayer; and Hebbian updates of synaptic weights.\n  In this work, we deployed MothNet, a computational model of the insect\nolfactory network, as an automatic feature generator: Attached as a front-end\npre-processor, its Readout Neurons provided new features, derived from the\noriginal features, for use by standard ML classifiers. We found that these\n\"insect cyborgs\", i.e. classifiers that are part-insect model and part-ML\nmethod, had significantly better performance than baseline ML methods alone on\na vectorized MNIST dataset. The MothNet feature generator also substantially\nout-performed other feature generating methods such as PCA, PLS, and NNs, as\nwell as pre-training to initialize NN weights. Cyborgs improved relative test\nset accuracy by an average of 6% to 33% depending on baseline ML accuracy,\nwhile relative reduction in test set error exceeded 50% for higher baseline\naccuracy ML models. These results indicate the potential value of BNN-inspired\nfeature generators in the ML context. \n\n"}
{"id": "1808.08317", "contents": "Title: To Cluster, or Not to Cluster: An Analysis of Clusterability Methods Abstract: Clustering is an essential data mining tool that aims to discover inherent\ncluster structure in data. For most applications, applying clustering is only\nappropriate when cluster structure is present. As such, the study of\nclusterability, which evaluates whether data possesses such structure, is an\nintegral part of cluster analysis. However, methods for evaluating\nclusterability vary radically, making it challenging to select a suitable\nmeasure. In this paper, we perform an extensive comparison of measures of\nclusterability and provide guidelines that clustering users can reference to\nselect suitable measures for their applications. \n\n"}
{"id": "1808.08531", "contents": "Title: DeepTracker: Visualizing the Training Process of Convolutional Neural\n  Networks Abstract: Deep convolutional neural networks (CNNs) have achieved remarkable success in\nvarious fields. However, training an excellent CNN is practically a\ntrial-and-error process that consumes a tremendous amount of time and computer\nresources. To accelerate the training process and reduce the number of trials,\nexperts need to understand what has occurred in the training process and why\nthe resulting CNN behaves as such. However, current popular training platforms,\nsuch as TensorFlow, only provide very little and general information, such as\ntraining/validation errors, which is far from enough to serve this purpose. To\nbridge this gap and help domain experts with their training tasks in a\npractical environment, we propose a visual analytics system, DeepTracker, to\nfacilitate the exploration of the rich dynamics of CNN training processes and\nto identify the unusual patterns that are hidden behind the huge amount of\ntraining log. Specifically,we combine a hierarchical index mechanism and a set\nof hierarchical small multiples to help experts explore the entire training log\nfrom different levels of detail. We also introduce a novel cube-style\nvisualization to reveal the complex correlations among multiple types of\nheterogeneous training data including neuron weights, validation images, and\ntraining iterations. Three case studies are conducted to demonstrate how\nDeepTracker provides its users with valuable knowledge in an industry-level CNN\ntraining process, namely in our case, training ResNet-50 on the ImageNet\ndataset. We show that our method can be easily applied to other\nstate-of-the-art \"very deep\" CNN models. \n\n"}
{"id": "1808.08850", "contents": "Title: WiSeBE: Window-based Sentence Boundary Evaluation Abstract: Sentence Boundary Detection (SBD) has been a major research topic since\nAutomatic Speech Recognition transcripts have been used for further Natural\nLanguage Processing tasks like Part of Speech Tagging, Question Answering or\nAutomatic Summarization. But what about evaluation? Do standard evaluation\nmetrics like precision, recall, F-score or classification error; and more\nimportant, evaluating an automatic system against a unique reference is enough\nto conclude how well a SBD system is performing given the final application of\nthe transcript? In this paper we propose Window-based Sentence Boundary\nEvaluation (WiSeBE), a semi-supervised metric for evaluating Sentence Boundary\nDetection systems based on multi-reference (dis)agreement. We evaluate and\ncompare the performance of different SBD systems over a set of Youtube\ntranscripts using WiSeBE and standard metrics. This double evaluation gives an\nunderstanding of how WiSeBE is a more reliable metric for the SBD task. \n\n"}
{"id": "1808.09115", "contents": "Title: All You Need is \"Love\": Evading Hate-speech Detection Abstract: With the spread of social networks and their unfortunate use for hate speech,\nautomatic detection of the latter has become a pressing problem. In this paper,\nwe reproduce seven state-of-the-art hate speech detection models from prior\nwork, and show that they perform well only when tested on the same type of data\nthey were trained on. Based on these results, we argue that for successful hate\nspeech detection, model architecture is less important than the type of data\nand labeling criteria. We further show that all proposed detection techniques\nare brittle against adversaries who can (automatically) insert typos, change\nword boundaries or add innocuous words to the original hate speech. A\ncombination of these methods is also effective against Google Perspective -- a\ncutting-edge solution from industry. Our experiments demonstrate that\nadversarial training does not completely mitigate the attacks, and using\ncharacter-level features makes the models systematically more attack-resistant\nthan using word-level features. \n\n"}
{"id": "1808.09401", "contents": "Title: Temporal Information Extraction by Predicting Relative Time-lines Abstract: The current leading paradigm for temporal information extraction from text\nconsists of three phases: (1) recognition of events and temporal expressions,\n(2) recognition of temporal relations among them, and (3) time-line\nconstruction from the temporal relations. In contrast to the first two phases,\nthe last phase, time-line construction, received little attention and is the\nfocus of this work. In this paper, we propose a new method to construct a\nlinear time-line from a set of (extracted) temporal relations. But more\nimportantly, we propose a novel paradigm in which we directly predict start and\nend-points for events from the text, constituting a time-line without going\nthrough the intermediate step of prediction of temporal relations as in earlier\nwork. Within this paradigm, we propose two models that predict in linear\ncomplexity, and a new training loss using TimeML-style annotations, yielding\npromising results. \n\n"}
{"id": "1808.09492", "contents": "Title: Learning to Attend On Essential Terms: An Enhanced Retriever-Reader\n  Model for Open-domain Question Answering Abstract: Open-domain question answering remains a challenging task as it requires\nmodels that are capable of understanding questions and answers, collecting\nuseful information, and reasoning over evidence. Previous work typically\nformulates this task as a reading comprehension or entailment problem given\nevidence retrieved from search engines. However, existing techniques struggle\nto retrieve indirectly related evidence when no directly related evidence is\nprovided, especially for complex questions where it is hard to parse precisely\nwhat the question asks. In this paper we propose a retriever-reader model that\nlearns to attend on essential terms during the question answering process. We\nbuild (1) an essential term selector which first identifies the most important\nwords in a question, then reformulates the query and searches for related\nevidence; and (2) an enhanced reader that distinguishes between essential terms\nand distracting words to predict the answer. We evaluate our model on multiple\nopen-domain multiple-choice QA datasets, notably performing at the level of the\nstate-of-the-art on the AI2 Reasoning Challenge (ARC) dataset. \n\n"}
{"id": "1808.09588", "contents": "Title: Mapping Language to Code in Programmatic Context Abstract: Source code is rarely written in isolation. It depends significantly on the\nprogrammatic context, such as the class that the code would reside in. To study\nthis phenomenon, we introduce the task of generating class member functions\ngiven English documentation and the programmatic context provided by the rest\nof the class. This task is challenging because the desired code can vary\ngreatly depending on the functionality the class provides (e.g., a sort\nfunction may or may not be available when we are asked to \"return the smallest\nelement\" in a particular member variable list). We introduce CONCODE, a new\nlarge dataset with over 100,000 examples consisting of Java classes from online\ncode repositories, and develop a new encoder-decoder architecture that models\nthe interaction between the method documentation and the class environment. We\nalso present a detailed error analysis suggesting that there is significant\nroom for future work on this task. \n\n"}
{"id": "1808.09633", "contents": "Title: Improved Semantic-Aware Network Embedding with Fine-Grained Word\n  Alignment Abstract: Network embeddings, which learn low-dimensional representations for each\nvertex in a large-scale network, have received considerable attention in recent\nyears. For a wide range of applications, vertices in a network are typically\naccompanied by rich textual information such as user profiles, paper abstracts,\netc. We propose to incorporate semantic features into network embeddings by\nmatching important words between text sequences for all pairs of vertices. We\nintroduce a word-by-word alignment framework that measures the compatibility of\nembeddings between word pairs, and then adaptively accumulates these alignment\nfeatures with a simple yet effective aggregation function. In experiments, we\nevaluate the proposed framework on three real-world benchmarks for downstream\ntasks, including link prediction and multi-label vertex classification. Results\ndemonstrate that our model outperforms state-of-the-art network embedding\nmethods by a large margin. \n\n"}
{"id": "1808.09802", "contents": "Title: Modelling Irregular Spatial Patterns using Graph Convolutional Neural\n  Networks Abstract: The understanding of geographical reality is a process of data representation\nand pattern discovery. Former studies mainly adopted continuous-field models to\nrepresent spatial variables and to investigate the underlying spatial\ncontinuity/heterogeneity in the regular spatial domain. In this article, we\nintroduce a more generalized model based on graph convolutional neural networks\n(GCNs) that can capture the complex parameters of spatial patterns underlying\ngraph-structured spatial data, which generally contain both Euclidean spatial\ninformation and non-Euclidean feature information. A trainable semi-supervised\nprediction framework is proposed to model the spatial distribution patterns of\nintra-urban points of interest(POI) check-ins. This work demonstrates the\nfeasibility of GCNs in complex geographic decision problems and provides a\npromising tool to analyze irregular spatial data. \n\n"}
{"id": "1808.10122", "contents": "Title: Learning Neural Templates for Text Generation Abstract: While neural, encoder-decoder models have had significant empirical success\nin text generation, there remain several unaddressed problems with this style\nof generation. Encoder-decoder models are largely (a) uninterpretable, and (b)\ndifficult to control in terms of their phrasing or content. This work proposes\na neural generation system using a hidden semi-markov model (HSMM) decoder,\nwhich learns latent, discrete templates jointly with learning to generate. We\nshow that this model learns useful templates, and that these templates make\ngeneration both more interpretable and controllable. Furthermore, we show that\nthis approach scales to real data sets and achieves strong performance nearing\nthat of encoder-decoder text generation models. \n\n"}
{"id": "1808.10134", "contents": "Title: Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and\n  Learning based Vehicle Longitude Dynamic Calibrating Algorithm Abstract: For any autonomous driving vehicle, control module determines its road\nperformance and safety, i.e. its precision and stability should stay within a\ncarefully-designed range. Nonetheless, control algorithms require vehicle\ndynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are\nobscure to calibrate in real time. As a result, to achieve reasonable\nperformance, most, if not all, research-oriented autonomous vehicles do manual\ncalibrations in a one-by-one fashion. Since manual calibration is not\nsustainable once entering into mass production stage for industrial purposes,\nwe here introduce a machine-learning based auto-calibration system for\nautonomous driving vehicles. In this paper, we will show how we build a\ndata-driven longitudinal calibration procedure using machine learning\ntechniques. We first generated offline calibration tables from human driving\ndata. The offline table serves as an initial guess for later uses and it only\nneeds twenty-minutes data collection and process. We then used an\nonline-learning algorithm to appropriately update the initial table (the\noffline table) based on real-time performance analysis. This longitudinal\nauto-calibration system has been deployed to more than one hundred Baidu Apollo\nself-driving vehicles (including hybrid family vehicles and electronic\ndelivery-only vehicles) since April 2018. By August 27, 2018, it had been\ntested for more than two thousands hours, ten thousands kilometers (6,213\nmiles) and yet proven to be effective. \n\n"}
{"id": "1808.10551", "contents": "Title: Dynamic mode decomposition in vector-valued reproducing kernel Hilbert\n  spaces for extracting dynamical structure among observables Abstract: Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety\nof engineering and scientific fields. Dynamic mode decomposition (DMD), which\nis a numerical algorithm for the spectral analysis of Koopman operators, has\nbeen attracting attention as a way of obtaining global modal descriptions of\nNLDSs without requiring explicit prior knowledge. However, since existing DMD\nalgorithms are in principle formulated based on the concatenation of scalar\nobservables, it is not directly applicable to data with dependent structures\namong observables, which take, for example, the form of a sequence of graphs.\nIn this paper, we formulate Koopman spectral analysis for NLDSs with structures\namong observables and propose an estimation algorithm for this problem. This\nmethod can extract and visualize the underlying low-dimensional global dynamics\nof NLDSs with structures among observables from data, which can be useful in\nunderstanding the underlying dynamics of such NLDSs. To this end, we first\nformulate the problem of estimating spectra of the Koopman operator defined in\nvector-valued reproducing kernel Hilbert spaces, and then develop an estimation\nprocedure for this problem by reformulating tensor-based DMD. As a special case\nof our method, we propose the method named as Graph DMD, which is a numerical\nalgorithm for Koopman spectral analysis of graph dynamical systems, using a\nsequence of adjacency matrices. We investigate the empirical performance of our\nmethod by using synthetic and real-world data. \n\n"}
{"id": "1808.10632", "contents": "Title: A novel extension of Generalized Low-Rank Approximation of Matrices\n  based on multiple-pairs of transformations Abstract: Dimensionality reduction is a main step in the learning process which plays\nan essential role in many applications. The most popular methods in this field\nlike SVD, PCA, and LDA, only can be applied to data with vector format. This\nmeans that for higher order data like matrices or more generally tensors, data\nshould be fold to the vector format. So, in this approach, the spatial\nrelations of features are not considered and also the probability of\nover-fitting is increased. Due to these issues, in recent years some methods\nlike Generalized low-rank approximation of matrices (GLRAM) and Multilinear PCA\n(MPCA) are proposed which deal with the data in their own format. So, in these\nmethods, the spatial relationships of features are preserved and the\nprobability of overfitting could be fallen. Also, their time and space\ncomplexities are less than vector-based ones. However, because of the fewer\nparameters, the search space in a multilinear approach is much smaller than the\nsearch space of the vector-based approach. To overcome this drawback of\nmultilinear methods like GLRAM, we proposed a new method which is a general\nform of GLRAM and by preserving the merits of it have a larger search space.\nExperimental results confirm the quality of the proposed method. Also, applying\nthis approach to the other multilinear dimensionality reduction methods like\nMPCA and MLDA is straightforward. \n\n"}
{"id": "1808.10692", "contents": "Title: APES: a Python toolbox for simulating reinforcement learning\n  environments Abstract: Assisted by neural networks, reinforcement learning agents have been able to\nsolve increasingly complex tasks over the last years. The simulation\nenvironment in which the agents interact is an essential component in any\nreinforcement learning problem. The environment simulates the dynamics of the\nagents' world and hence provides feedback to their actions in terms of state\nobservations and external rewards. To ease the design and simulation of such\nenvironments this work introduces $\\texttt{APES}$, a highly customizable and\nopen source package in Python to create 2D grid-world environments for\nreinforcement learning problems. $\\texttt{APES}$ equips agents with algorithms\nto simulate any field of vision, it allows the creation and positioning of\nitems and rewards according to user-defined rules, and supports the interaction\nof multiple agents. \n\n"}
{"id": "1809.00410", "contents": "Title: Modeling Topical Coherence in Discourse without Supervision Abstract: Coherence of text is an important attribute to be measured for both manually\nand automatically generated discourse; but well-defined quantitative metrics\nfor it are still elusive. In this paper, we present a metric for scoring\ntopical coherence of an input paragraph on a real-valued scale by analyzing its\nunderlying topical structure. We first extract all possible topics that the\nsentences of a paragraph of text are related to. Coherence of this text is then\nmeasured by computing: (a) the degree of uncertainty of the topics with respect\nto the paragraph, and (b) the relatedness between these topics. All components\nof our modular framework rely only on unlabeled data and WordNet, thus making\nit completely unsupervised, which is an important feature for general-purpose\nusage of any metric. Experiments are conducted on two datasets - a publicly\navailable dataset for essay grading (representing human discourse), and a\nsynthetic dataset constructed by mixing content from multiple paragraphs\ncovering diverse topics. Our evaluation shows that the measured coherence\nscores are positively correlated with the ground truth for both the datasets.\nFurther validation to our coherence scores is provided by conducting human\nevaluation on the synthetic data, showing a significant agreement of 79.3% \n\n"}
{"id": "1809.00537", "contents": "Title: Crowdsourcing Semantic Label Propagation in Relation Classification Abstract: Distant supervision is a popular method for performing relation extraction\nfrom text that is known to produce noisy labels. Most progress in relation\nextraction and classification has been made with crowdsourced corrections to\ndistant-supervised labels, and there is evidence that indicates still more\nwould be better. In this paper, we explore the problem of propagating human\nannotation signals gathered for open-domain relation classification through the\nCrowdTruth methodology for crowdsourcing, that captures ambiguity in\nannotations by measuring inter-annotator disagreement. Our approach propagates\nannotations to sentences that are similar in a low dimensional embedding space,\nexpanding the number of labels by two orders of magnitude. Our experiments show\nsignificant improvement in a sentence-level multi-class relation classifier. \n\n"}
{"id": "1809.00653", "contents": "Title: Towards Dynamic Computation Graphs via Sparse Latent Structure Abstract: Deep NLP models benefit from underlying structures in the data---e.g., parse\ntrees---typically extracted using off-the-shelf parsers. Recent attempts to\njointly learn the latent structure encounter a tradeoff: either make\nfactorization assumptions that limit expressiveness, or sacrifice end-to-end\ndifferentiability. Using the recently proposed SparseMAP inference, which\nretrieves a sparse distribution over latent structures, we propose a novel\napproach for end-to-end learning of latent structure predictors jointly with a\ndownstream predictor. To the best of our knowledge, our method is the first to\nenable unrestricted dynamic computation graph construction from the global\nlatent structure, while maintaining differentiability. \n\n"}
{"id": "1809.00676", "contents": "Title: A3Net: Adversarial-and-Attention Network for Machine Reading\n  Comprehension Abstract: In this paper, we introduce Adversarial-and-attention Network (A3Net) for\nMachine Reading Comprehension. This model extends existing approaches from two\nperspectives. First, adversarial training is applied to several target\nvariables within the model, rather than only to the inputs or embeddings. We\ncontrol the norm of adversarial perturbations according to the norm of original\ntarget variables, so that we can jointly add perturbations to several target\nvariables during training. As an effective regularization method, adversarial\ntraining improves robustness and generalization of our model. Second, we\npropose a multi-layer attention network utilizing three kinds of\nhigh-efficiency attention mechanisms. Multi-layer attention conducts\ninteraction between question and passage within each layer, which contributes\nto reasonable representation and understanding of the model. Combining these\ntwo contributions, we enhance the diversity of dataset and the information\nextracting ability of the model at the same time. Meanwhile, we construct A3Net\nfor the WebQA dataset. Results show that our model outperforms the\nstate-of-the-art models (improving Fuzzy Score from 73.50% to 77.0%). \n\n"}
{"id": "1809.01499", "contents": "Title: Extractive Adversarial Networks: High-Recall Explanations for\n  Identifying Personal Attacks in Social Media Posts Abstract: We introduce an adversarial method for producing high-recall explanations of\nneural text classifier decisions. Building on an existing architecture for\nextractive explanations via hard attention, we add an adversarial layer which\nscans the residual of the attention for remaining predictive signal. Motivated\nby the important domain of detecting personal attacks in social media comments,\nwe additionally demonstrate the importance of manually setting a semantically\nappropriate `default' behavior for the model by explicitly manipulating its\nbias term. We develop a validation set of human-annotated personal attacks to\nevaluate the impact of these changes. \n\n"}
{"id": "1809.01534", "contents": "Title: Utilizing Character and Word Embeddings for Text Normalization with\n  Sequence-to-Sequence Models Abstract: Text normalization is an important enabling technology for several NLP tasks.\nRecently, neural-network-based approaches have outperformed well-established\nmodels in this task. However, in languages other than English, there has been\nlittle exploration in this direction. Both the scarcity of annotated data and\nthe complexity of the language increase the difficulty of the problem. To\naddress these challenges, we use a sequence-to-sequence model with\ncharacter-based attention, which in addition to its self-learned character\nembeddings, uses word embeddings pre-trained with an approach that also models\nsubword information. This provides the neural model with access to more\nlinguistic information especially suitable for text normalization, without\nlarge parallel corpora. We show that providing the model with word-level\nfeatures bridges the gap for the neural network approach to achieve a\nstate-of-the-art F1 score on a standard Arabic language correction shared task\ndataset. \n\n"}
{"id": "1809.01765", "contents": "Title: Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method\n  for Stochastic Sparse Linear Regression with Limited Attribute Observation Abstract: We develop new stochastic gradient methods for efficiently solving sparse\nlinear regression in a partial attribute observation setting, where learners\nare only allowed to observe a fixed number of actively chosen attributes per\nexample at training and prediction times. It is shown that the methods achieve\nessentially a sample complexity of $O(1/\\varepsilon)$ to attain an error of\n$\\varepsilon$ under a variant of restricted eigenvalue condition, and the rate\nhas better dependency on the problem dimension than existing methods.\nParticularly, if the smallest magnitude of the non-zero components of the\noptimal solution is not too small, the rate of our proposed {\\it Hybrid}\nalgorithm can be boosted to near the minimax optimal sample complexity of {\\it\nfull information} algorithms. The core ideas are (i) efficient construction of\nan unbiased gradient estimator by the iterative usage of the hard thresholding\noperator for configuring an exploration algorithm; and (ii) an adaptive\ncombination of the exploration and an exploitation algorithms for quickly\nidentifying the support of the optimum and efficiently searching the optimal\nparameter in its support. Experimental results are presented to validate our\ntheoretical findings and the superiority of our proposed methods. \n\n"}
{"id": "1809.02145", "contents": "Title: GANs beyond divergence minimization Abstract: Generative adversarial networks (GANs) can be interpreted as an adversarial\ngame between two players, a discriminator D and a generator G, in which D\nlearns to classify real from fake data and G learns to generate realistic data\nby \"fooling\" D into thinking that fake data is actually real data. Currently, a\ndominating view is that G actually learns by minimizing a divergence given that\nthe general objective function is a divergence when D is optimal. However, this\nview has been challenged due to inconsistencies between theory and practice. In\nthis paper, we discuss of the properties associated with most loss functions\nfor G (e.g., saturating/non-saturating f-GAN, LSGAN, WGAN, etc.). We show that\nthese loss functions are not divergences and do not have the same equilibrium\nas expected of divergences. This suggests that G does not need to minimize the\nsame objective function as D maximize, nor maximize the objective of D after\nswapping real data with fake data (non-saturating GAN) but can instead use a\nwide range of possible loss functions to learn to generate realistic data. We\ndefine GANs through two separate and independent D maximization and G\nminimization steps. We generalize the generator step to four new classes of\nloss functions, most of which are actual divergences (while traditional G loss\nfunctions are not). We test a wide variety of loss functions from these four\nclasses on a synthetic dataset and on CIFAR-10. We observe that most loss\nfunctions converge well and provide comparable data generation quality to\nnon-saturating GAN, LSGAN, and WGAN-GP generator loss functions, whether we use\ndivergences or non-divergences. These results suggest that GANs do not conform\nwell to the divergence minimization theory and form a much broader range of\nmodels than previously assumed. \n\n"}
{"id": "1809.02230", "contents": "Title: Deep Neural Net with Attention for Multi-channel Multi-touch Attribution Abstract: Customers are usually exposed to online digital advertisement channels, such\nas email marketing, display advertising, paid search engine marketing, along\ntheir way to purchase or subscribe products( aka. conversion). The marketers\ntrack all the customer journey data and try to measure the effectiveness of\neach advertising channel. The inference about the influence of each channel\nplays an important role in budget allocation and inventory pricing decisions.\nSeveral simplistic rule-based strategies and data-driven algorithmic strategies\nhave been widely used in marketing field, but they do not address the issues,\nsuch as channel interaction, time dependency, user characteristics. In this\npaper, we propose a novel attribution algorithm based on deep learning to\nassess the impact of each advertising channel. We present Deep Neural Net With\nAttention multi-touch attribution model (DNAMTA) model in a supervised learning\nfashion of predicting if a series of events leads to conversion, and it leads\nus to have a deep understanding of the dynamic interaction effects between\nmedia channels. DNAMTA also incorporates user-context information, such as user\ndemographics and behavior, as control variables to reduce the estimation biases\nof media effects. We used computational experiment of large real world\nmarketing dataset to demonstrate that our proposed model is superior to\nexisting methods in both conversion prediction and media channel influence\nevaluation. \n\n"}
{"id": "1809.02237", "contents": "Title: 82 Treebanks, 34 Models: Universal Dependency Parsing with\n  Multi-Treebank Models Abstract: We present the Uppsala system for the CoNLL 2018 Shared Task on universal\ndependency parsing. Our system is a pipeline consisting of three components:\nthe first performs joint word and sentence segmentation; the second predicts\npart-of- speech tags and morphological features; the third predicts dependency\ntrees from words and tags. Instead of training a single parsing model for each\ntreebank, we trained models with multiple treebanks for one language or closely\nrelated languages, greatly reducing the number of models. On the official test\nrun, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system\nobtained the best scores overall for word segmentation, universal POS tagging,\nand morphological features. \n\n"}
{"id": "1809.02393", "contents": "Title: Improving Neural Question Generation using Answer Separation Abstract: Neural question generation (NQG) is the task of generating a question from a\ngiven passage with deep neural networks. Previous NQG models suffer from a\nproblem that a significant proportion of the generated questions include words\nin the question target, resulting in the generation of unintended questions. In\nthis paper, we propose answer-separated seq2seq, which better utilizes the\ninformation from both the passage and the target answer. By replacing the\ntarget answer in the original passage with a special token, our model learns to\nidentify which interrogative word should be used. We also propose a new module\ntermed keyword-net, which helps the model better capture the key information in\nthe target answer and generate an appropriate question. Experimental results\ndemonstrate that our answer separation method significantly reduces the number\nof improper questions which include answers. Consequently, our model\nsignificantly outperforms previous state-of-the-art NQG models. \n\n"}
{"id": "1809.02727", "contents": "Title: Decentralized Differentially Private Without-Replacement Stochastic\n  Gradient Descent Abstract: While machine learning has achieved remarkable results in a wide variety of\ndomains, the training of models often requires large datasets that may need to\nbe collected from different individuals. As sensitive information may be\ncontained in the individual's dataset, sharing training data may lead to severe\nprivacy concerns. Therefore, there is a compelling need to develop\nprivacy-aware machine learning methods, for which one effective approach is to\nleverage the generic framework of differential privacy. Considering that\nstochastic gradient descent (SGD) is one of the most commonly adopted methods\nfor large-scale machine learning problems, a decentralized differentially\nprivate SGD algorithm is proposed in this work. Particularly, we focus on SGD\nwithout replacement due to its favorable structure for practical\nimplementation. Both privacy and convergence analysis are provided for the\nproposed algorithm. Finally, extensive experiments are performed to demonstrate\nthe effectiveness of the proposed method. \n\n"}
{"id": "1809.02836", "contents": "Title: Context-Free Transductions with Neural Stacks Abstract: This paper analyzes the behavior of stack-augmented recurrent neural network\n(RNN) models. Due to the architectural similarity between stack RNNs and\npushdown transducers, we train stack RNN models on a number of tasks, including\nstring reversal, context-free language modelling, and cumulative XOR\nevaluation. Examining the behavior of our networks, we show that\nstack-augmented RNNs can discover intuitive stack-based strategies for solving\nour tasks. However, stack RNNs are more difficult to train than classical\narchitectures such as LSTMs. Rather than employ stack-based strategies, more\ncomplex networks often find approximate solutions by using the stack as\nunstructured memory. \n\n"}
{"id": "1809.02926", "contents": "Title: Probabilistic Prediction of Interactive Driving Behavior via\n  Hierarchical Inverse Reinforcement Learning Abstract: Autonomous vehicles (AVs) are on the road. To safely and efficiently interact\nwith other road participants, AVs have to accurately predict the behavior of\nsurrounding vehicles and plan accordingly. Such prediction should be\nprobabilistic, to address the uncertainties in human behavior. Such prediction\nshould also be interactive, since the distribution over all possible\ntrajectories of the predicted vehicle depends not only on historical\ninformation, but also on future plans of other vehicles that interact with it.\nTo achieve such interaction-aware predictions, we propose a probabilistic\nprediction approach based on hierarchical inverse reinforcement learning (IRL).\nFirst, we explicitly consider the hierarchical trajectory-generation process of\nhuman drivers involving both discrete and continuous driving decisions. Based\non this, the distribution over all future trajectories of the predicted vehicle\nis formulated as a mixture of distributions partitioned by the discrete\ndecisions. Then we apply IRL hierarchically to learn the distributions from\nreal human demonstrations. A case study for the ramp-merging driving scenario\nis provided. The quantitative results show that the proposed approach can\naccurately predict both the discrete driving decisions such as yield or pass as\nwell as the continuous trajectories. \n\n"}
{"id": "1809.03316", "contents": "Title: Hierarchical Video Understanding Abstract: We introduce a hierarchical architecture for video understanding that\nexploits the structure of real world actions by capturing targets at different\nlevels of granularity. We design the model such that it first learns simpler\ncoarse-grained tasks, and then moves on to learn more fine-grained targets. The\nmodel is trained with a joint loss on different granularity levels. We\ndemonstrate empirical results on the recent release of Something-Something\ndataset, which provides a hierarchy of targets, namely coarse-grained action\ngroups, fine-grained action categories, and captions. Experiments suggest that\nmodels that exploit targets at different levels of granularity achieve better\nperformance on all levels. \n\n"}
{"id": "1809.03705", "contents": "Title: Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3D\n  Pedestrian Pose and Gait Prediction Abstract: In applications such as autonomous driving, it is important to understand,\ninfer, and anticipate the intention and future behavior of pedestrians. This\nability allows vehicles to avoid collisions and improve ride safety and\nquality. This paper proposes a biomechanically inspired recurrent neural\nnetwork (Bio-LSTM) that can predict the location and 3D articulated body pose\nof pedestrians in a global coordinate frame, given 3D poses and locations\nestimated in prior frames with inaccuracy. The proposed network is able to\npredict poses and global locations for multiple pedestrians simultaneously, for\npedestrians up to 45 meters from the cameras (urban intersection scale). The\noutputs of the proposed network are full-body 3D meshes represented in Skinned\nMulti-Person Linear (SMPL) model parameters. The proposed approach relies on a\nnovel objective function that incorporates the periodicity of human walking\n(gait), the mirror symmetry of the human body, and the change of ground\nreaction forces in a human gait cycle. This paper presents prediction results\non the PedX dataset, a large-scale, in-the-wild data set collected at real\nurban intersections with heavy pedestrian traffic. Results show that the\nproposed network can successfully learn the characteristics of pedestrian gait\nand produce accurate and consistent 3D pose predictions. \n\n"}
{"id": "1809.03779", "contents": "Title: Probabilistic approach to limited-data computed tomography\n  reconstruction Abstract: In this work, we consider the inverse problem of reconstructing the internal\nstructure of an object from limited x-ray projections. We use a Gaussian\nprocess prior to model the target function and estimate its (hyper)parameters\nfrom measured data. In contrast to other established methods, this comes with\nthe advantage of not requiring any manual parameter tuning, which usually\narises in classical regularization strategies. Our method uses a basis function\nexpansion technique for the Gaussian process which significantly reduces the\ncomputational complexity and avoids the need for numerical integration. The\napproach also allows for reformulation of come classical regularization methods\nas Laplacian and Tikhonov regularization as Gaussian process regression, and\nhence provides an efficient algorithm and principled means for their parameter\ntuning. Results from simulated and real data indicate that this approach is\nless sensitive to streak artifacts as compared to the commonly used method of\nfiltered backprojection. \n\n"}
{"id": "1809.04019", "contents": "Title: Training and Prediction Data Discrepancies: Challenges of Text\n  Classification with Noisy, Historical Data Abstract: Industry datasets used for text classification are rarely created for that\npurpose. In most cases, the data and target predictions are a by-product of\naccumulated historical data, typically fraught with noise, present in both the\ntext-based document, as well as in the targeted labels. In this work, we\naddress the question of how well performance metrics computed on noisy,\nhistorical data reflect the performance on the intended future machine learning\nmodel input. The results demonstrate the utility of dirty training datasets\nused to build prediction models for cleaner (and different) prediction inputs. \n\n"}
{"id": "1809.04022", "contents": "Title: Can LSTM Learn to Capture Agreement? The Case of Basque Abstract: Sequential neural networks models are powerful tools in a variety of Natural\nLanguage Processing (NLP) tasks. The sequential nature of these models raises\nthe questions: to what extent can these models implicitly learn hierarchical\nstructures typical to human language, and what kind of grammatical phenomena\ncan they acquire?\n  We focus on the task of agreement prediction in Basque, as a case study for a\ntask that requires implicit understanding of sentence structure and the\nacquisition of a complex but consistent morphological system. Analyzing\nexperimental results from two syntactic prediction tasks -- verb number\nprediction and suffix recovery -- we find that sequential models perform worse\non agreement prediction in Basque than one might expect on the basis of a\nprevious agreement prediction work in English. Tentative findings based on\ndiagnostic classifiers suggest the network makes use of local heuristics as a\nproxy for the hierarchical structure of the sentence. We propose the Basque\nagreement prediction task as challenging benchmark for models that attempt to\nlearn regularities in human language. \n\n"}
{"id": "1809.04322", "contents": "Title: Reinforcement Learning in Topology-based Representation for Human Body\n  Movement with Whole Arm Manipulation Abstract: Moving a human body or a large and bulky object can require the strength of\nwhole arm manipulation (WAM). This type of manipulation places the load on the\nrobot's arms and relies on global properties of the interaction to\nsucceed---rather than local contacts such as grasping or non-prehensile\npushing. In this paper, we learn to generate motions that enable WAM for\nholding and transporting of humans in certain rescue or patient care scenarios.\nWe model the task as a reinforcement learning problem in order to provide a\nbehavior that can directly respond to external perturbation and human motion.\nFor this, we represent global properties of the robot-human interaction with\ntopology-based coordinates that are computed from arm and torso positions.\nThese coordinates also allow transferring the learned policy to other body\nshapes and sizes. For training and evaluation, we simulate a dynamic sea rescue\nscenario and show in quantitative experiments that the policy can solve unseen\nscenarios with differently-shaped humans, floating humans, or with perception\nnoise. Our qualitative experiments show the subsequent transporting after\nholding is achieved and we demonstrate that the policy can be directly\ntransferred to a real world setting. \n\n"}
{"id": "1809.04403", "contents": "Title: Label Denoising with Large Ensembles of Heterogeneous Neural Networks Abstract: Despite recent advances in computer vision based on various convolutional\narchitectures, video understanding remains an important challenge. In this\nwork, we present and discuss a top solution for the large-scale video\nclassification (labeling) problem introduced as a Kaggle competition based on\nthe YouTube-8M dataset. We show and compare different approaches to\npreprocessing, data augmentation, model architectures, and model combination.\nOur final model is based on a large ensemble of video- and frame-level models\nbut fits into rather limiting hardware constraints. We apply an approach based\non knowledge distillation to deal with noisy labels in the original dataset and\nthe recently developed mixup technique to improve the basic models. \n\n"}
{"id": "1809.04506", "contents": "Title: Combined Reinforcement Learning via Abstract Representations Abstract: In the quest for efficient and robust reinforcement learning methods, both\nmodel-free and model-based approaches offer advantages. In this paper we\npropose a new way of explicitly bridging both approaches via a shared\nlow-dimensional learned encoding of the environment, meant to capture\nsummarizing abstractions. We show that the modularity brought by this approach\nleads to good generalization while being computationally efficient, with\nplanning happening in a smaller latent state space. In addition, this approach\nrecovers a sufficient low-dimensional representation of the environment, which\nopens up new strategies for interpretable AI, exploration and transfer\nlearning. \n\n"}
{"id": "1809.04640", "contents": "Title: Jump to better conclusions: SCAN both left and right Abstract: Lake and Baroni (2018) recently introduced the SCAN data set, which consists\nof simple commands paired with action sequences and is intended to test the\nstrong generalization abilities of recurrent sequence-to-sequence models. Their\ninitial experiments suggested that such models may fail because they lack the\nability to extract systematic rules. Here, we take a closer look at SCAN and\nshow that it does not always capture the kind of generalization that it was\ndesigned for. To mitigate this we propose a complementary dataset, which\nrequires mapping actions back to the original commands, called NACS. We show\nthat models that do well on SCAN do not necessarily do well on NACS, and that\nNACS exhibits properties more closely aligned with realistic use-cases for\nsequence-to-sequence models. \n\n"}
{"id": "1809.04790", "contents": "Title: Adversarial Examples: Opportunities and Challenges Abstract: Deep neural networks (DNNs) have shown huge superiority over humans in image\nrecognition, speech processing, autonomous vehicles and medical diagnosis.\nHowever, recent studies indicate that DNNs are vulnerable to adversarial\nexamples (AEs), which are designed by attackers to fool deep learning models.\nDifferent from real examples, AEs can mislead the model to predict incorrect\noutputs while hardly be distinguished by human eyes, therefore threaten\nsecurity-critical deep-learning applications. In recent years, the generation\nand defense of AEs have become a research hotspot in the field of artificial\nintelligence (AI) security. This article reviews the latest research progress\nof AEs. First, we introduce the concept, cause, characteristics and evaluation\nmetrics of AEs, then give a survey on the state-of-the-art AE generation\nmethods with the discussion of advantages and disadvantages. After that, we\nreview the existing defenses and discuss their limitations. Finally, future\nresearch opportunities and challenges on AEs are prospected. \n\n"}
{"id": "1809.04988", "contents": "Title: Sequential Coordination of Deep Models for Learning Visual Arithmetic Abstract: Achieving machine intelligence requires a smooth integration of perception\nand reasoning, yet models developed to date tend to specialize in one or the\nother; sophisticated manipulation of symbols acquired from rich perceptual\nspaces has so far proved elusive. Consider a visual arithmetic task, where the\ngoal is to carry out simple arithmetical algorithms on digits presented under\nnatural conditions (e.g. hand-written, placed randomly). We propose a\ntwo-tiered architecture for tackling this problem. The lower tier consists of a\nheterogeneous collection of information processing modules, which can include\npre-trained deep neural networks for locating and extracting characters from\nthe image, as well as modules performing symbolic transformations on the\nrepresentations extracted by perception. The higher tier consists of a\ncontroller, trained using reinforcement learning, which coordinates the modules\nin order to solve the high-level task. For instance, the controller may learn\nin what contexts to execute the perceptual networks and what symbolic\ntransformations to apply to their outputs. The resulting model is able to solve\na variety of tasks in the visual arithmetic domain, and has several advantages\nover standard, architecturally homogeneous feedforward networks including\nimproved sample efficiency. \n\n"}
{"id": "1809.05214", "contents": "Title: Model-Based Reinforcement Learning via Meta-Policy Optimization Abstract: Model-based reinforcement learning approaches carry the promise of being data\nefficient. However, due to challenges in learning dynamics models that\nsufficiently match the real-world dynamics, they struggle to achieve the same\nasymptotic performance as model-free methods. We propose Model-Based\nMeta-Policy-Optimization (MB-MPO), an approach that foregoes the strong\nreliance on accurate learned dynamics models. Using an ensemble of learned\ndynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model\nin the ensemble with one policy gradient step. This steers the meta-policy\ntowards internalizing consistent dynamics predictions among the ensemble while\nshifting the burden of behaving optimally w.r.t. the model discrepancies\ntowards the adaptation step. Our experiments show that MB-MPO is more robust to\nmodel imperfections than previous model-based approaches. Finally, we\ndemonstrate that our approach is able to match the asymptotic performance of\nmodel-free methods while requiring significantly less experience. \n\n"}
{"id": "1809.05483", "contents": "Title: A Multi-Stage Algorithm for Acoustic Physical Model Parameters\n  Estimation Abstract: One of the challenges in computational acoustics is the identification of\nmodels that can simulate and predict the physical behavior of a system\ngenerating an acoustic signal. Whenever such models are used for commercial\napplications an additional constraint is the time-to-market, making automation\nof the sound design process desirable. In previous works, a computational sound\ndesign approach has been proposed for the parameter estimation problem\ninvolving timbre matching by deep learning, which was applied to the synthesis\nof pipe organ tones. In this work we refine previous results by introducing the\nformer approach in a multi-stage algorithm that also adds heuristics and a\nstochastic optimization method operating on objective cost functions based on\npsychoacoustics. The optimization method shows to be able to refine the first\nestimate given by the deep learning approach and substantially improve the\nobjective metrics, with the additional benefit of reducing the sound design\nprocess time. Subjective listening tests are also conducted to gather\nadditional insights on the results. \n\n"}
{"id": "1809.05807", "contents": "Title: Dual Memory Network Model for Biased Product Review Classification Abstract: In sentiment analysis (SA) of product reviews, both user and product\ninformation are proven to be useful. Current tasks handle user profile and\nproduct information in a unified model which may not be able to learn salient\nfeatures of users and products effectively. In this work, we propose a dual\nuser and product memory network (DUPMN) model to learn user profiles and\nproduct reviews using separate memory networks. Then, the two representations\nare used jointly for sentiment prediction. The use of separate models aims to\ncapture user profiles and product information more effectively. Compared to\nstate-of-the-art unified prediction models, the evaluations on three benchmark\ndatasets, IMDB, Yelp13, and Yelp14, show that our dual learning model gives\nperformance gain of 0.6%, 1.2%, and 0.9%, respectively. The improvements are\nalso deemed very significant measured by p-values. \n\n"}
{"id": "1809.05972", "contents": "Title: Generating Informative and Diverse Conversational Responses via\n  Adversarial Information Maximization Abstract: Responses generated by neural conversational models tend to lack\ninformativeness and diversity. We present Adversarial Information Maximization\n(AIM), an adversarial learning strategy that addresses these two related but\ndistinct problems. To foster response diversity, we leverage adversarial\ntraining that allows distributional matching of synthetic and real responses.\nTo improve informativeness, our framework explicitly optimizes a variational\nlower bound on pairwise mutual information between query and response.\nEmpirical results from automatic and human evaluations demonstrate that our\nmethods significantly boost informativeness and diversity. \n\n"}
{"id": "1809.06559", "contents": "Title: User Information Augmented Semantic Frame Parsing using Coarse-to-Fine\n  Neural Networks Abstract: Semantic frame parsing is a crucial component in spoken language\nunderstanding (SLU) to build spoken dialog systems. It has two main tasks:\nintent detection and slot filling. Although state-of-the-art approaches showed\ngood results, they require large annotated training data and long training\ntime. In this paper, we aim to alleviate these drawbacks for semantic frame\nparsing by utilizing the ubiquitous user information. We design a novel\ncoarse-to-fine deep neural network model to incorporate prior knowledge of user\ninformation intermediately to better and quickly train a semantic frame parser.\nDue to the lack of benchmark dataset with real user information, we synthesize\nthe simplest type of user information (location and time) on ATIS benchmark\ndata. The results show that our approach leverages such simple user information\nto outperform state-of-the-art approaches by 0.25% for intent detection and\n0.31% for slot filling using standard training data. When using smaller\ntraining data, the performance improvement on intent detection and slot filling\nreaches up to 1.35% and 1.20% respectively. We also show that our approach can\nachieve similar performance as state-of-the-art approaches by using less than\n80% annotated training data. Moreover, the training time to achieve the similar\nperformance is also reduced by over 60%. \n\n"}
{"id": "1809.07258", "contents": "Title: DPPy: Sampling DPPs with Python Abstract: Determinantal point processes (DPPs) are specific probability distributions\nover clouds of points that are used as models and computational tools across\nphysics, probability, statistics, and more recently machine learning. Sampling\nfrom DPPs is a challenge and therefore we present DPPy, a Python toolbox that\ngathers known exact and approximate sampling algorithms for both finite and\ncontinuous DPPs. The project is hosted on GitHub and equipped with an extensive\ndocumentation. \n\n"}
{"id": "1809.07572", "contents": "Title: Challenges for Toxic Comment Classification: An In-Depth Error Analysis Abstract: Toxic comment classification has become an active research field with many\nrecently proposed approaches. However, while these approaches address some of\nthe task's challenges others still remain unsolved and directions for further\nresearch are needed. To this end, we compare different deep learning and\nshallow approaches on a new, large comment dataset and propose an ensemble that\noutperforms all individual models. Further, we validate our findings on a\nsecond dataset. The results of the ensemble enable us to perform an extensive\nerror analysis, which reveals open challenges for state-of-the-art methods and\ndirections towards pending future research. These challenges include missing\nparadigmatic context and inconsistent dataset labels. \n\n"}
{"id": "1809.07945", "contents": "Title: SCC: Automatic Classification of Code Snippets Abstract: Determining the programming language of a source code file has been\nconsidered in the research community; it has been shown that Machine Learning\n(ML) and Natural Language Processing (NLP) algorithms can be effective in\nidentifying the programming language of source code files. However, determining\nthe programming language of a code snippet or a few lines of source code is\nstill a challenging task. Online forums such as Stack Overflow and code\nrepositories such as GitHub contain a large number of code snippets. In this\npaper, we describe Source Code Classification (SCC), a classifier that can\nidentify the programming language of code snippets written in 21 different\nprogramming languages. A Multinomial Naive Bayes (MNB) classifier is employed\nwhich is trained using Stack Overflow posts. It is shown to achieve an accuracy\nof 75% which is higher than that with Programming Languages Identification (PLI\na proprietary online classifier of snippets) whose accuracy is only 55.5%. The\naverage score for precision, recall and the F1 score with the proposed tool are\n0.76, 0.75 and 0.75, respectively. In addition, it can distinguish between code\nsnippets from a family of programming languages such as C, C++ and C#, and can\nalso identify the programming language version such as C# 3.0, C# 4.0 and C#\n5.0. \n\n"}
{"id": "1809.08098", "contents": "Title: Efficient Formal Safety Analysis of Neural Networks Abstract: Neural networks are increasingly deployed in real-world safety-critical\ndomains such as autonomous driving, aircraft collision avoidance, and malware\ndetection. However, these networks have been shown to often mispredict on\ninputs with minor adversarial or even accidental perturbations. Consequences of\nsuch errors can be disastrous and even potentially fatal as shown by the recent\nTesla autopilot crash. Thus, there is an urgent need for formal analysis\nsystems that can rigorously check neural networks for violations of different\nsafety properties such as robustness against adversarial perturbations within a\ncertain $L$-norm of a given image. An effective safety analysis system for a\nneural network must be able to either ensure that a safety property is\nsatisfied by the network or find a counterexample, i.e., an input for which the\nnetwork will violate the property. Unfortunately, most existing techniques for\nperforming such analysis struggle to scale beyond very small networks and the\nones that can scale to larger networks suffer from high false positives and\ncannot produce concrete counterexamples in case of a property violation. In\nthis paper, we present a new efficient approach for rigorously checking\ndifferent safety properties of neural networks that significantly outperforms\nexisting approaches by multiple orders of magnitude. Our approach can check\ndifferent safety properties and find concrete counterexamples for networks that\nare 10$\\times$ larger than the ones supported by existing analysis techniques.\nWe believe that our approach to estimating tight output bounds of a network for\na given input range can also help improve the explainability of neural networks\nand guide the training process of more robust neural networks. \n\n"}
{"id": "1809.08343", "contents": "Title: Interpretable Multi-Objective Reinforcement Learning through Policy\n  Orchestration Abstract: Autonomous cyber-physical agents and systems play an increasingly large role\nin our lives. To ensure that agents behave in ways aligned with the values of\nthe societies in which they operate, we must develop techniques that allow\nthese agents to not only maximize their reward in an environment, but also to\nlearn and follow the implicit constraints of society. These constraints and\nnorms can come from any number of sources including regulations, business\nprocess guidelines, laws, ethical principles, social norms, and moral values.\nWe detail a novel approach that uses inverse reinforcement learning to learn a\nset of unspecified constraints from demonstrations of the task, and\nreinforcement learning to learn to maximize the environment rewards. More\nprecisely, we assume that an agent can observe traces of behavior of members of\nthe society but has no access to the explicit set of constraints that give rise\nto the observed behavior. Inverse reinforcement learning is used to learn such\nconstraints, that are then combined with a possibly orthogonal value function\nthrough the use of a contextual bandit-based orchestrator that picks a\ncontextually-appropriate choice between the two policies (constraint-based and\nenvironment reward-based) when taking actions. The contextual bandit\norchestrator allows the agent to mix policies in novel ways, taking the best\nactions from either a reward maximizing or constrained policy. In addition, the\norchestrator is transparent on which policy is being employed at each time\nstep. We test our algorithms using a Pac-Man domain and show that the agent is\nable to learn to act optimally, act within the demonstrated constraints, and\nmix these two functions in complex ways. \n\n"}
{"id": "1809.08706", "contents": "Title: Is Ordered Weighted $\\ell_1$ Regularized Regression Robust to\n  Adversarial Perturbation? A Case Study on OSCAR Abstract: Many state-of-the-art machine learning models such as deep neural networks\nhave recently shown to be vulnerable to adversarial perturbations, especially\nin classification tasks. Motivated by adversarial machine learning, in this\npaper we investigate the robustness of sparse regression models with strongly\ncorrelated covariates to adversarially designed measurement noises.\nSpecifically, we consider the family of ordered weighted $\\ell_1$ (OWL)\nregularized regression methods and study the case of OSCAR (octagonal shrinkage\nclustering algorithm for regression) in the adversarial setting. Under a\nnorm-bounded threat model, we formulate the process of finding a maximally\ndisruptive noise for OWL-regularized regression as an optimization problem and\nillustrate the steps towards finding such a noise in the case of OSCAR.\nExperimental results demonstrate that the regression performance of grouping\nstrongly correlated features can be severely degraded under our adversarial\nsetting, even when the noise budget is significantly smaller than the\nground-truth signals. \n\n"}
{"id": "1809.08911", "contents": "Title: Understanding Compressive Adversarial Privacy Abstract: Designing a data sharing mechanism without sacrificing too much privacy can\nbe considered as a game between data holders and malicious attackers. This\npaper describes a compressive adversarial privacy framework that captures the\ntrade-off between the data privacy and utility. We characterize the optimal\ndata releasing mechanism through convex optimization when assuming that both\nthe data holder and attacker can only modify the data using linear\ntransformations. We then build a more realistic data releasing mechanism that\ncan rely on a nonlinear compression model while the attacker uses a neural\nnetwork. We demonstrate in a series of empirical applications that this\nframework, consisting of compressive adversarial privacy, can preserve\nsensitive information. \n\n"}
{"id": "1809.09096", "contents": "Title: Text Summarization as Tree Transduction by Top-Down TreeLSTM Abstract: Extractive compression is a challenging natural language processing problem.\nThis work contributes by formulating neural extractive compression as a parse\ntree transduction problem, rather than a sequence transduction task. Motivated\nby this, we introduce a deep neural model for learning\nstructure-to-substructure tree transductions by extending the standard Long\nShort-Term Memory, considering the parent-child relationships in the structural\nrecursion. The proposed model can achieve state of the art performance on\nsentence compression benchmarks, both in terms of accuracy and compression\nrate. \n\n"}
{"id": "1809.09296", "contents": "Title: Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for\n  Language Generation Abstract: Mixture of Softmaxes (MoS) has been shown to be effective at addressing the\nexpressiveness limitation of Softmax-based models. Despite the known advantage,\nMoS is practically sealed by its large consumption of memory and computational\ntime due to the need of computing multiple Softmaxes. In this work, we set out\nto unleash the power of MoS in practical applications by investigating improved\nword coding schemes, which could effectively reduce the vocabulary size and\nhence relieve the memory and computation burden. We show both BPE and our\nproposed Hybrid-LightRNN lead to improved encoding mechanisms that can halve\nthe time and memory consumption of MoS without performance losses. With MoS, we\nachieve an improvement of 1.5 BLEU scores on IWSLT 2014 German-to-English\ncorpus and an improvement of 0.76 CIDEr score on image captioning. Moreover, on\nthe larger WMT 2014 machine translation dataset, our MoS-boosted Transformer\nyields 29.5 BLEU score for English-to-German and 42.1 BLEU score for\nEnglish-to-French, outperforming the single-Softmax Transformer by 0.8 and 0.4\nBLEU scores respectively and achieving the state-of-the-art result on WMT 2014\nEnglish-to-German task. \n\n"}
{"id": "1809.09349", "contents": "Title: The jamming transition as a paradigm to understand the loss landscape of\n  deep neural networks Abstract: Deep learning has been immensely successful at a variety of tasks, ranging\nfrom classification to AI. Learning corresponds to fitting training data, which\nis implemented by descending a very high-dimensional loss function.\nUnderstanding under which conditions neural networks do not get stuck in poor\nminima of the loss, and how the landscape of that loss evolves as depth is\nincreased remains a challenge. Here we predict, and test empirically, an\nanalogy between this landscape and the energy landscape of repulsive ellipses.\nWe argue that in FC networks a phase transition delimits the over- and\nunder-parametrized regimes where fitting can or cannot be achieved. In the\nvicinity of this transition, properties of the curvature of the minima of the\nloss are critical. This transition shares direct similarities with the jamming\ntransition by which particles form a disordered solid as the density is\nincreased, which also occurs in certain classes of computational optimization\nand learning problems such as the perceptron. Our analysis gives a simple\nexplanation as to why poor minima of the loss cannot be encountered in the\noverparametrized regime, and puts forward the surprising result that the\nability of fully connected networks to fit random data is independent of their\ndepth. Our observations suggests that this independence also holds for real\ndata. We also study a quantity $\\Delta$ which characterizes how well\n($\\Delta<0$) or badly ($\\Delta>0$) a datum is learned. At the critical point it\nis power-law distributed, $P_+(\\Delta)\\sim\\Delta^\\theta$ for $\\Delta>0$ and\n$P_-(\\Delta)\\sim(-\\Delta)^{-\\gamma}$ for $\\Delta<0$, with $\\theta\\approx0.3$\nand $\\gamma\\approx0.2$. This observation suggests that near the transition the\nloss landscape has a hierarchical structure and that the learning dynamics is\nprone to avalanche-like dynamics, with abrupt changes in the set of patterns\nthat are learned. \n\n"}
{"id": "1809.09945", "contents": "Title: Jamming in multilayer supervised learning models Abstract: Critical jamming transitions are characterized by an astonishing degree of\nuniversality. Analytic and numerical evidence points to the existence of a\nlarge universality class that encompasses finite and infinite dimensional\nspheres and continuous constraint satisfaction problems (CCSP) such as the\nnon-convex perceptron and related models. In this paper we investigate\nmultilayer neural networks (MLNN) learning random associations as models for\nCCSP which could potentially define different jamming universality classes. As\nopposed to simple perceptrons and infinite dimensional spheres, which are\ndescribed by a single effective field in terms of which the constraints appear\nto be one-dimensional, the description of MLNN, involves multiple fields, and\nthe constraints acquire a multidimensional character. We first study the models\nnumerically and show that similarly to the perceptron, whenever jamming is\nisostatic, the sphere universality class is recovered, we then write the exact\nmean-field equations for the models and identify a dimensional reduction\nmechanism that leads to a scaling regime identical to one of the infinite\ndimensional spheres. We suggest that this mechanism could be general enough to\nexplain finite dimensional universality. \n\n"}
{"id": "1809.10007", "contents": "Title: Learning through Probing: a decentralized reinforcement learning\n  architecture for social dilemmas Abstract: Multi-agent reinforcement learning has received significant interest in\nrecent years notably due to the advancements made in deep reinforcement\nlearning which have allowed for the developments of new architectures and\nlearning algorithms. Using social dilemmas as the training ground, we present a\nnovel learning architecture, Learning through Probing (LTP), where agents\nutilize a probing mechanism to incorporate how their opponent's behavior\nchanges when an agent takes an action. We use distinct training phases and\nadjust rewards according to the overall outcome of the experiences accounting\nfor changes to the opponents behavior. We introduce a parameter eta to\ndetermine the significance of these future changes to opponent behavior. When\napplied to the Iterated Prisoner's Dilemma (IPD), LTP agents demonstrate that\nthey can learn to cooperate with each other, achieving higher average\ncumulative rewards than other reinforcement learning methods while also\nmaintaining good performance in playing against static agents that are present\nin Axelrod tournaments. We compare this method with traditional reinforcement\nlearning algorithms and agent-tracking techniques to highlight key differences\nand potential applications. We also draw attention to the differences between\nsolving games and societal-like interactions and analyze the training of\nQ-learning agents in makeshift societies. This is to emphasize how cooperation\nmay emerge in societies and demonstrate this using environments where\ninteractions with opponents are determined through a random encounter format of\nthe IPD. \n\n"}
{"id": "1809.10388", "contents": "Title: Queue-based Resampling for Online Class Imbalance Learning Abstract: Online class imbalance learning constitutes a new problem and an emerging\nresearch topic that focusses on the challenges of online learning under class\nimbalance and concept drift. Class imbalance deals with data streams that have\nvery skewed distributions while concept drift deals with changes in the class\nimbalance status. Little work exists that addresses these challenges and in\nthis paper we introduce queue-based resampling, a novel algorithm that\nsuccessfully addresses the co-existence of class imbalance and concept drift.\nThe central idea of the proposed resampling algorithm is to selectively include\nin the training set a subset of the examples that appeared in the past. Results\non two popular benchmark datasets demonstrate the effectiveness of queue-based\nresampling over state-of-the-art methods in terms of learning speed and\nquality. \n\n"}
{"id": "1810.00123", "contents": "Title: Generalization and Regularization in DQN Abstract: Deep reinforcement learning algorithms have shown an impressive ability to\nlearn complex control policies in high-dimensional tasks. However, despite the\never-increasing performance on popular benchmarks, policies learned by deep\nreinforcement learning algorithms can struggle to generalize when evaluated in\nremarkably similar environments. In this paper we propose a protocol to\nevaluate generalization in reinforcement learning through different modes of\nAtari 2600 games. With that protocol we assess the generalization capabilities\nof DQN, one of the most traditional deep reinforcement learning algorithms, and\nwe provide evidence suggesting that DQN overspecializes to the training\nenvironment. We then comprehensively evaluate the impact of dropout and\n$\\ell_2$ regularization, as well as the impact of reusing learned\nrepresentations to improve the generalization capabilities of DQN. Despite\nregularization being largely underutilized in deep reinforcement learning, we\nshow that it can, in fact, help DQN learn more general features. These features\ncan be reused and fine-tuned on similar tasks, considerably improving DQN's\nsample efficiency. \n\n"}
{"id": "1810.00924", "contents": "Title: Joint On-line Learning of a Zero-shot Spoken Semantic Parser and a\n  Reinforcement Learning Dialogue Manager Abstract: Despite many recent advances for the design of dialogue systems, a true\nbottleneck remains the acquisition of data required to train its components.\nUnlike many other language processing applications, dialogue systems require\ninteractions with users, therefore it is complex to develop them with\npre-recorded data. Building on previous works, on-line learning is pursued here\nas a most convenient way to address the issue. Data collection, annotation and\nuse in learning algorithms are performed in a single process. The main\ndifficulties are then: to bootstrap an initial basic system, and to control the\nlevel of additional cost on the user side. Considering that well-performing\nsolutions can be used directly off the shelf for speech recognition and\nsynthesis, the study is focused on learning the spoken language understanding\nand dialogue management modules only. Several variants of joint learning are\ninvestigated and tested with user trials to confirm that the overall on-line\nlearning can be obtained after only a few hundred training dialogues and can\noverstep an expert-based system. \n\n"}
{"id": "1810.01256", "contents": "Title: Continual Learning of Context-dependent Processing in Neural Networks Abstract: Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it. \n\n"}
{"id": "1810.01406", "contents": "Title: Super-Resolution via Conditional Implicit Maximum Likelihood Estimation Abstract: Single-image super-resolution (SISR) is a canonical problem with diverse\napplications. Leading methods like SRGAN produce images that contain various\nartifacts, such as high-frequency noise, hallucinated colours and shape\ndistortions, which adversely affect the realism of the result. In this paper,\nwe propose an alternative approach based on an extension of the method of\nImplicit Maximum Likelihood Estimation (IMLE). We demonstrate greater\neffectiveness at noise reduction and preservation of the original colours and\nshapes, yielding more realistic super-resolved images. \n\n"}
{"id": "1810.01468", "contents": "Title: Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree\n  Decoding Abstract: We propose a model for tagging unstructured texts with an arbitrary number of\nterms drawn from a tree-structured vocabulary (i.e., an ontology). We treat\nthis as a special case of sequence-to-sequence learning in which the decoder\nbegins at the root node of an ontological tree and recursively elects to expand\nchild nodes as a function of the input text, the current node, and the latent\ndecoder state. In our experiments the proposed method outperforms\nstate-of-the-art approaches on the important task of automatically assigning\nMeSH terms to biomedical abstracts. \n\n"}
{"id": "1810.01920", "contents": "Title: Generalized Inverse Optimization through Online Learning Abstract: Inverse optimization is a powerful paradigm for learning preferences and\nrestrictions that explain the behavior of a decision maker, based on a set of\nexternal signal and the corresponding decision pairs. However, most inverse\noptimization algorithms are designed specifically in batch setting, where all\nthe data is available in advance. As a consequence, there has been rare use of\nthese methods in an online setting suitable for real-time applications. In this\npaper, we propose a general framework for inverse optimization through online\nlearning. Specifically, we develop an online learning algorithm that uses an\nimplicit update rule which can handle noisy data. Moreover, under additional\nregularity assumptions in terms of the data and the model, we prove that our\nalgorithm converges at a rate of $\\mathcal{O}(1/\\sqrt{T})$ and is statistically\nconsistent. In our experiments, we show the online learning approach can learn\nthe parameters with great accuracy and is very robust to noises, and achieves a\ndramatic improvement in computational efficacy over the batch learning\napproach. \n\n"}
{"id": "1810.02363", "contents": "Title: Recurrent Transition Networks for Character Locomotion Abstract: Manually authoring transition animations for a complete locomotion system can\nbe a tedious and time-consuming task, especially for large games that allow\ncomplex and constrained locomotion movements, where the number of transitions\ngrows exponentially with the number of states. In this paper, we present a\nnovel approach, based on deep recurrent neural networks, to automatically\ngenerate such transitions given a past context of a few frames and a target\ncharacter state to reach. We present the Recurrent Transition Network (RTN),\nbased on a modified version of the Long-Short-Term-Memory (LSTM) network,\ndesigned specifically for transition generation and trained without any gait,\nphase, contact or action labels. We further propose a simple yet principled way\nto initialize the hidden states of the LSTM layer for a given sequence which\nimproves the performance and generalization to new motions. We both\nquantitatively and qualitatively evaluate our system and show that making the\nnetwork terrain-aware by adding a local terrain representation to the input\nyields better performance for rough-terrain navigation on long transitions. Our\nsystem produces realistic and fluid transitions that rival the quality of\nMotion Capture-based ground-truth motions, even before applying any\ninverse-kinematics postprocess. Direct benefits of our approach could be to\naccelerate the creation of transition variations for large coverage, or even to\nentirely replace transition nodes in an animation graph. We further explore\napplications of this model in a animation super-resolution setting where we\ntemporally decompress animations saved at 1 frame per second and show that the\nnetwork is able to reconstruct motions that are hard to distinguish from\nun-compressed locomotion sequences. \n\n"}
{"id": "1810.02565", "contents": "Title: Continuous-time Models for Stochastic Optimization Algorithms Abstract: We propose new continuous-time formulations for first-order stochastic\noptimization algorithms such as mini-batch gradient descent and\nvariance-reduced methods. We exploit these continuous-time models, together\nwith simple Lyapunov analysis as well as tools from stochastic calculus, in\norder to derive convergence bounds for various types of non-convex functions.\nGuided by such analysis, we show that the same Lyapunov arguments hold in\ndiscrete-time, leading to matching rates. In addition, we use these models and\nIto calculus to infer novel insights on the dynamics of SGD, proving that a\ndecreasing learning rate acts as time warping or, equivalently, as landscape\nstretching. \n\n"}
{"id": "1810.02959", "contents": "Title: Higher-order Spectral Clustering for Heterogeneous Graphs Abstract: Higher-order connectivity patterns such as small induced sub-graphs called\ngraphlets (network motifs) are vital to understand the important components\n(modules/functional units) governing the configuration and behavior of complex\nnetworks. Existing work in higher-order clustering has focused on simple\nhomogeneous graphs with a single node/edge type. However, heterogeneous graphs\nconsisting of nodes and edges of different types are seemingly ubiquitous in\nthe real-world. In this work, we introduce the notion of typed-graphlet that\nexplicitly captures the rich (typed) connectivity patterns in heterogeneous\nnetworks. Using typed-graphlets as a basis, we develop a general principled\nframework for higher-order clustering in heterogeneous networks. The framework\nprovides mathematical guarantees on the optimality of the higher-order\nclustering obtained. The experiments demonstrate the effectiveness of the\nframework quantitatively for three important applications including (i)\nclustering, (ii) link prediction, and (iii) graph compression. In particular,\nthe approach achieves a mean improvement of 43x over all methods and graphs for\nclustering while achieving a 18.7% and 20.8% improvement for link prediction\nand graph compression, respectively. \n\n"}
{"id": "1810.03032", "contents": "Title: Constructing Graph Node Embeddings via Discrimination of Similarity\n  Distributions Abstract: The problem of unsupervised learning node embeddings in graphs is one of the\nimportant directions in modern network science. In this work we propose a novel\nframework, which is aimed to find embeddings by \\textit{discriminating\ndistributions of similarities (DDoS)} between nodes in the graph. The general\nidea is implemented by maximizing the \\textit{earth mover distance} between\ndistributions of decoded similarities of similar and dissimilar nodes. The\nresulting algorithm generates embeddings which give a state-of-the-art\nperformance in the problem of link prediction in real-world graphs. \n\n"}
{"id": "1810.03167", "contents": "Title: Unsupervised Neural Word Segmentation for Chinese via Segmental Language\n  Modeling Abstract: Previous traditional approaches to unsupervised Chinese word segmentation\n(CWS) can be roughly classified into discriminative and generative models. The\nformer uses the carefully designed goodness measures for candidate\nsegmentation, while the latter focuses on finding the optimal segmentation of\nthe highest generative probability. However, while there exists a trivial way\nto extend the discriminative models into neural version by using neural\nlanguage models, those of generative ones are non-trivial. In this paper, we\npropose the segmental language models (SLMs) for CWS. Our approach explicitly\nfocuses on the segmental nature of Chinese, as well as preserves several\nproperties of language models. In SLMs, a context encoder encodes the previous\ncontext and a segment decoder generates each segment incrementally. As far as\nwe know, we are the first to propose a neural model for unsupervised CWS and\nachieve competitive performance to the state-of-the-art statistical models on\nfour different datasets from SIGHAN 2005 bakeoff. \n\n"}
{"id": "1810.03292", "contents": "Title: Sanity Checks for Saliency Maps Abstract: Saliency methods have emerged as a popular tool to highlight features in an\ninput deemed relevant for the prediction of a learned model. Several saliency\nmethods have been proposed, often guided by visual appeal on image data. In\nthis work, we propose an actionable methodology to evaluate what kinds of\nexplanations a given method can and cannot provide. We find that reliance,\nsolely, on visual assessment can be misleading. Through extensive experiments\nwe show that some existing saliency methods are independent both of the model\nand of the data generating process. Consequently, methods that fail the\nproposed tests are inadequate for tasks that are sensitive to either data or\nmodel, such as, finding outliers in the data, explaining the relationship\nbetween inputs and outputs that the model learned, and debugging the model. We\ninterpret our findings through an analogy with edge detection in images, a\ntechnique that requires neither training data nor model. Theory in the case of\na linear model and a single-layer convolutional neural network supports our\nexperimental findings. \n\n"}
{"id": "1810.03581", "contents": "Title: Improving the Transformer Translation Model with Document-Level Context Abstract: Although the Transformer translation model (Vaswani et al., 2017) has\nachieved state-of-the-art performance in a variety of translation tasks, how to\nuse document-level context to deal with discourse phenomena problematic for\nTransformer still remains a challenge. In this work, we extend the Transformer\nmodel with a new context encoder to represent document-level context, which is\nthen incorporated into the original encoder and decoder. As large-scale\ndocument-level parallel corpora are usually not available, we introduce a\ntwo-step training method to take full advantage of abundant sentence-level\nparallel corpora and limited document-level parallel corpora. Experiments on\nthe NIST Chinese-English datasets and the IWSLT French-English datasets show\nthat our approach improves over Transformer significantly. \n\n"}
{"id": "1810.03875", "contents": "Title: Towards Verifying Semantic Roles Co-occurrence Abstract: Semantic role theory considers roles as a small universal set of unanalyzed\nentities. It means that formally there are no restrictions on role\ncombinations. We argue that the semantic roles co-occur in verb\nrepresentations. It means that there are hidden restrictions on role\ncombinations. To demonstrate that a practical and evidence-based approach has\nbeen built on in-depth analysis of the largest verb database VerbNet. The\nconsequences of this approach are considered. \n\n"}
{"id": "1810.03979", "contents": "Title: Extended Bit-Plane Compression for Convolutional Neural Network\n  Accelerators Abstract: After the tremendous success of convolutional neural networks in image\nclassification, object detection, speech recognition, etc., there is now rising\ndemand for deployment of these compute-intensive ML models on tightly power\nconstrained embedded and mobile systems at low cost as well as for pushing the\nthroughput in data centers. This has triggered a wave of research towards\nspecialized hardware accelerators. Their performance is often constrained by\nI/O bandwidth and the energy consumption is dominated by I/O transfers to\noff-chip memory. We introduce and evaluate a novel, hardware-friendly\ncompression scheme for the feature maps present within convolutional neural\nnetworks. We show that an average compression ratio of 4.4x relative to\nuncompressed data and a gain of 60% over existing method can be achieved for\nResNet-34 with a compression block requiring <300 bit of sequential cells and\nminimal combinational logic. \n\n"}
{"id": "1810.04152", "contents": "Title: Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives Abstract: Deep latent variable models have become a popular model choice due to the\nscalable learning algorithms introduced by (Kingma & Welling, 2013; Rezende et\nal., 2014). These approaches maximize a variational lower bound on the\nintractable log likelihood of the observed data. Burda et al. (2015) introduced\na multi-sample variational bound, IWAE, that is at least as tight as the\nstandard variational lower bound and becomes increasingly tight as the number\nof samples increases. Counterintuitively, the typical inference network\ngradient estimator for the IWAE bound performs poorly as the number of samples\nincreases (Rainforth et al., 2018; Le et al., 2018). Roeder et al. (2017)\npropose an improved gradient estimator, however, are unable to show it is\nunbiased. We show that it is in fact biased and that the bias can be estimated\nefficiently with a second application of the reparameterization trick. The\ndoubly reparameterized gradient (DReG) estimator does not suffer as the number\nof samples increases, resolving the previously raised issues. The same idea can\nbe used to improve many recently introduced training techniques for latent\nvariable models. In particular, we show that this estimator reduces the\nvariance of the IWAE gradient, the reweighted wake-sleep update (RWS)\n(Bornschein & Bengio, 2014), and the jackknife variational inference (JVI)\ngradient (Nowozin, 2018). Finally, we show that this computationally efficient,\nunbiased drop-in gradient estimator translates to improved performance for all\nthree objectives on several modeling tasks. \n\n"}
{"id": "1810.04437", "contents": "Title: Persistence pays off: Paying Attention to What the LSTM Gating Mechanism\n  Persists Abstract: Language Models (LMs) are important components in several Natural Language\nProcessing systems. Recurrent Neural Network LMs composed of LSTM units,\nespecially those augmented with an external memory, have achieved\nstate-of-the-art results. However, these models still struggle to process long\nsequences which are more likely to contain long-distance dependencies because\nof information fading and a bias towards more recent information. In this paper\nwe demonstrate an effective mechanism for retrieving information in a memory\naugmented LSTM LM based on attending to information in memory in proportion to\nthe number of timesteps the LSTM gating mechanism persisted the information. \n\n"}
{"id": "1810.04468", "contents": "Title: Decentralized Cooperative Stochastic Bandits Abstract: We study a decentralized cooperative stochastic multi-armed bandit problem\nwith $K$ arms on a network of $N$ agents. In our model, the reward distribution\nof each arm is the same for each agent and rewards are drawn independently\nacross agents and time steps. In each round, each agent chooses an arm to play\nand subsequently sends a message to her neighbors. The goal is to minimize the\noverall regret of the entire network. We design a fully decentralized algorithm\nthat uses an accelerated consensus procedure to compute (delayed) estimates of\nthe average of rewards obtained by all the agents for each arm, and then uses\nan upper confidence bound (UCB) algorithm that accounts for the delay and error\nof the estimates. We analyze the regret of our algorithm and also provide a\nlower bound. The regret is bounded by the optimal centralized regret plus a\nnatural and simple term depending on the spectral gap of the communication\nmatrix. Our algorithm is simpler to analyze than those proposed in prior work\nand it achieves better regret bounds, while requiring less information about\nthe underlying network. It also performs better empirically. \n\n"}
{"id": "1810.04534", "contents": "Title: Random matrix-improved estimation of covariance matrix distances Abstract: Given two sets $x_1^{(1)},\\ldots,x_{n_1}^{(1)}$ and\n$x_1^{(2)},\\ldots,x_{n_2}^{(2)}\\in\\mathbb{R}^p$ (or $\\mathbb{C}^p$) of random\nvectors with zero mean and positive definite covariance matrices $C_1$ and\n$C_2\\in\\mathbb{R}^{p\\times p}$ (or $\\mathbb{C}^{p\\times p}$), respectively,\nthis article provides novel estimators for a wide range of distances between\n$C_1$ and $C_2$ (along with divergences between some zero mean and covariance\n$C_1$ or $C_2$ probability measures) of the form $\\frac1p\\sum_{i=1}^n\nf(\\lambda_i(C_1^{-1}C_2))$ (with $\\lambda_i(X)$ the eigenvalues of matrix $X$).\nThese estimators are derived using recent advances in the field of random\nmatrix theory and are asymptotically consistent as $n_1,n_2,p\\to\\infty$ with\nnon trivial ratios $p/n_1<1$ and $p/n_2<1$ (the case $p/n_2>1$ is also\ndiscussed). A first \"generic\" estimator, valid for a large set of $f$\nfunctions, is provided under the form of a complex integral. Then, for a\nselected set of $f$'s of practical interest (namely, $f(t)=t$, $f(t)=\\log(t)$,\n$f(t)=\\log(1+st)$ and $f(t)=\\log^2(t)$), a closed-form expression is provided.\nBeside theoretical findings, simulation results suggest an outstanding\nperformance advantage for the proposed estimators when compared to the\nclassical \"plug-in\" estimator $\\frac1p\\sum_{i=1}^n f(\\lambda_i(\\hat\nC_1^{-1}\\hat C_2))$ (with $\\hat\nC_a=\\frac1{n_a}\\sum_{i=1}^{n_a}x_i^{(a)}x_i^{(a){\\sf T}}$), and this even for\nvery small values of $n_1,n_2,p$. \n\n"}
{"id": "1810.04635", "contents": "Title: Multimodal Speech Emotion Recognition Using Audio and Text Abstract: Speech emotion recognition is a challenging task, and extensive reliance has\nbeen placed on models that use audio features in building well-performing\nclassifiers. In this paper, we propose a novel deep dual recurrent encoder\nmodel that utilizes text data and audio signals simultaneously to obtain a\nbetter understanding of speech data. As emotional dialogue is composed of sound\nand spoken content, our model encodes the information from audio and text\nsequences using dual recurrent neural networks (RNNs) and then combines the\ninformation from these sources to predict the emotion class. This architecture\nanalyzes speech data from the signal level to the language level, and it thus\nutilizes the information within the data more comprehensively than models that\nfocus on audio features. Extensive experiments are conducted to investigate the\nefficacy and properties of the proposed model. Our proposed model outperforms\nprevious state-of-the-art methods in assigning data to one of four emotion\ncategories (i.e., angry, happy, sad and neutral) when the model is applied to\nthe IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%. \n\n"}
{"id": "1810.05236", "contents": "Title: Practical Design Space Exploration Abstract: Multi-objective optimization is a crucial matter in computer systems design\nspace exploration because real-world applications often rely on a trade-off\nbetween several objectives. Derivatives are usually not available or\nimpractical to compute and the feasibility of an experiment can not always be\ndetermined in advance. These problems are particularly difficult when the\nfeasible region is relatively small, and it may be prohibitive to even find a\nfeasible experiment, let alone an optimal one.\n  We introduce a new methodology and corresponding software framework,\nHyperMapper 2.0, which handles multi-objective optimization, unknown\nfeasibility constraints, and categorical/ordinal variables. This new\nmethodology also supports injection of the user prior knowledge in the search\nwhen available. All of these features are common requirements in computer\nsystems but rarely exposed in existing design space exploration systems. The\nproposed methodology follows a white-box model which is simple to understand\nand interpret (unlike, for example, neural networks) and can be used by the\nuser to better understand the results of the automatic search.\n  We apply and evaluate the new methodology to the automatic static tuning of\nhardware accelerators within the recently introduced Spatial programming\nlanguage, with minimization of design run-time and compute logic under the\nconstraint of the design fitting in a target field-programmable gate array\nchip. Our results show that HyperMapper 2.0 provides better Pareto fronts\ncompared to state-of-the-art baselines, with better or competitive hypervolume\nindicator and with 8x improvement in sampling budget for most of the benchmarks\nexplored. \n\n"}
{"id": "1810.05237", "contents": "Title: SyntaxSQLNet: Syntax Tree Networks for Complex and\n  Cross-DomainText-to-SQL Task Abstract: Most existing studies in text-to-SQL tasks do not require generating complex\nSQL queries with multiple clauses or sub-queries, and generalizing to new,\nunseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network\nto address the complex and cross-domain text-to-SQL generation task.\nSyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL\ngeneration path history and table-aware column attention encoders. We evaluate\nSyntaxSQLNet on the Spider text-to-SQL task, which contains databases with\nmultiple tables and complex SQL queries with multiple SQL clauses and nested\nqueries. We use a database split setting where databases in the test set are\nunseen during training. Experimental results show that SyntaxSQLNet can handle\na significantly greater number of complex SQL examples than prior work,\noutperforming the previous state-of-the-art model by 7.3% in exact matching\naccuracy. We also show that SyntaxSQLNet can further improve the performance by\nan additional 7.5% using a cross-domain augmentation method, resulting in a\n14.8% improvement in total. To our knowledge, we are the first to study this\ncomplex and cross-domain text-to-SQL task. \n\n"}
{"id": "1810.05241", "contents": "Title: One Size Does Not Fit All: Generating and Evaluating Variable Number of\n  Keyphrases Abstract: Different texts shall by nature correspond to different number of keyphrases.\nThis desideratum is largely missing from existing neural keyphrase generation\nmodels. In this study, we address this problem from both modeling and\nevaluation perspectives.\n  We first propose a recurrent generative model that generates multiple\nkeyphrases as delimiter-separated sequences. Generation diversity is further\nenhanced with two novel techniques by manipulating decoder hidden states. In\ncontrast to previous approaches, our model is capable of generating diverse\nkeyphrases and controlling number of outputs.\n  We further propose two evaluation metrics tailored towards the\nvariable-number generation. We also introduce a new dataset StackEx that\nexpands beyond the only existing genre (i.e., academic writing) in keyphrase\ngeneration tasks. With both previous and new evaluation metrics, our model\noutperforms strong baselines on all datasets. \n\n"}
{"id": "1810.05597", "contents": "Title: Learning Grid Cells as Vector Representation of Self-Position Coupled\n  with Matrix Representation of Self-Motion Abstract: This paper proposes a representational model for grid cells. In this model,\nthe 2D self-position of the agent is represented by a high-dimensional vector,\nand the 2D self-motion or displacement of the agent is represented by a matrix\nthat transforms the vector. Each component of the vector is a unit or a cell.\nThe model consists of the following three sub-models. (1) Vector-matrix\nmultiplication. The movement from the current position to the next position is\nmodeled by matrix-vector multiplication, i.e., the vector of the next position\nis obtained by multiplying the matrix of the motion to the vector of the\ncurrent position. (2) Magnified local isometry. The angle between two nearby\nvectors equals the Euclidean distance between the two corresponding positions\nmultiplied by a magnifying factor. (3) Global adjacency kernel. The inner\nproduct between two vectors measures the adjacency between the two\ncorresponding positions, which is defined by a kernel function of the Euclidean\ndistance between the two positions. Our representational model has explicit\nalgebra and geometry. It can learn hexagon patterns of grid cells, and it is\ncapable of error correction, path integral and path planning. \n\n"}
{"id": "1810.05713", "contents": "Title: Improving Generalization of Sequence Encoder-Decoder Networks for\n  Inverse Imaging of Cardiac Transmembrane Potential Abstract: Deep learning models have shown state-of-the-art performance in many inverse\nreconstruction problems. However, it is not well understood what properties of\nthe latent representation may improve the generalization ability of the\nnetwork. Furthermore, limited models have been presented for inverse\nreconstructions over time sequences. In this paper, we study the generalization\nability of a sequence encoder decoder model for solving inverse reconstructions\non time sequences. Our central hypothesis is that the generalization ability of\nthe network can be improved by 1) constrained stochasticity and 2) global\naggregation of temporal information in the latent space. First, drawing from\nanalytical learning theory, we theoretically show that a stochastic latent\nspace will lead to an improved generalization ability. Second, we consider an\nLSTM encoder-decoder architecture that compresses a global latent vector from\nall last-layer units in the LSTM encoder. This model is compared with\nalternative LSTM encoder-decoder architectures, each in deterministic and\nstochastic versions. The results demonstrate that the generalization ability of\nan inverse reconstruction network can be improved by constrained stochasticity\ncombined with global aggregation of temporal information in the latent space. \n\n"}
{"id": "1810.05739", "contents": "Title: MeanSum: A Neural Model for Unsupervised Multi-document Abstractive\n  Summarization Abstract: Abstractive summarization has been studied using neural sequence transduction\nmethods with datasets of large, paired document-summary examples. However, such\ndatasets are rare and the models trained from them do not generalize to other\ndomains. Recently, some progress has been made in learning sequence-to-sequence\nmappings with only unpaired examples. In our work, we consider the setting\nwhere there are only documents (product or business reviews) with no summaries\nprovided, and propose an end-to-end, neural model architecture to perform\nunsupervised abstractive summarization. Our proposed model consists of an\nauto-encoder where the mean of the representations of the input reviews decodes\nto a reasonable summary-review while not relying on any review-specific\nfeatures. We consider variants of the proposed architecture and perform an\nablation study to show the importance of specific components. We show through\nautomated metrics and human evaluation that the generated summaries are highly\nabstractive, fluent, relevant, and representative of the average sentiment of\nthe input reviews. Finally, we collect a reference evaluation dataset and show\nthat our model outperforms a strong extractive baseline. \n\n"}
{"id": "1810.05741", "contents": "Title: Explaining Black Boxes on Sequential Data using Weighted Automata Abstract: Understanding how a learned black box works is of crucial interest for the\nfuture of Machine Learning. In this paper, we pioneer the question of the\nglobal interpretability of learned black box models that assign numerical\nvalues to symbolic sequential data. To tackle that task, we propose a spectral\nalgorithm for the extraction of weighted automata (WA) from such black boxes.\nThis algorithm does not require the access to a dataset or to the inner\nrepresentation of the black box: the inferred model can be obtained solely by\nquerying the black box, feeding it with inputs and analyzing its outputs.\nExperiments using Recurrent Neural Networks (RNN) trained on a wide collection\nof 48 synthetic datasets and 2 real datasets show that the obtained\napproximation is of great quality. \n\n"}
{"id": "1810.06767", "contents": "Title: Approximate Fisher Information Matrix to Characterise the Training of\n  Deep Neural Networks Abstract: In this paper, we introduce a novel methodology for characterising the\nperformance of deep learning networks (ResNets and DenseNet) with respect to\ntraining convergence and generalisation as a function of mini-batch size and\nlearning rate for image classification. This methodology is based on novel\nmeasurements derived from the eigenvalues of the approximate Fisher information\nmatrix, which can be efficiently computed even for high capacity deep models.\nOur proposed measurements can help practitioners to monitor and control the\ntraining process (by actively tuning the mini-batch size and learning rate) to\nallow for good training convergence and generalisation. Furthermore, the\nproposed measurements also allow us to show that it is possible to optimise the\ntraining process with a new dynamic sampling training approach that\ncontinuously and automatically change the mini-batch size and learning rate\nduring the training process. Finally, we show that the proposed dynamic\nsampling training approach has a faster training time and a competitive\nclassification accuracy compared to the current state of the art. \n\n"}
{"id": "1810.06793", "contents": "Title: Learning Two-layer Neural Networks with Symmetric Inputs Abstract: We give a new algorithm for learning a two-layer neural network under a\ngeneral class of input distributions. Assuming there is a ground-truth\ntwo-layer network $$ y = A \\sigma(Wx) + \\xi, $$ where $A,W$ are weight\nmatrices, $\\xi$ represents noise, and the number of neurons in the hidden layer\nis no larger than the input or output, our algorithm is guaranteed to recover\nthe parameters $A,W$ of the ground-truth network. The only requirement on the\ninput $x$ is that it is symmetric, which still allows highly complicated and\nstructured input.\n  Our algorithm is based on the method-of-moments framework and extends several\nresults in tensor decompositions. We use spectral algorithms to avoid the\ncomplicated non-convex optimization in learning neural networks. Experiments\nshow that our algorithm can robustly learn the ground-truth neural network with\na small number of samples for many symmetric input distributions. \n\n"}
{"id": "1810.07766", "contents": "Title: Distributed Learning over Unreliable Networks Abstract: Most of today's distributed machine learning systems assume {\\em reliable\nnetworks}: whenever two machines exchange information (e.g., gradients or\nmodels), the network should guarantee the delivery of the message. At the same\ntime, recent work exhibits the impressive tolerance of machine learning\nalgorithms to errors or noise arising from relaxed communication or\nsynchronization. In this paper, we connect these two trends, and consider the\nfollowing question: {\\em Can we design machine learning systems that are\ntolerant to network unreliability during training?} With this motivation, we\nfocus on a theoretical problem of independent interest---given a standard\ndistributed parameter server architecture, if every communication between the\nworker and the server has a non-zero probability $p$ of being dropped, does\nthere exist an algorithm that still converges, and at what speed? The technical\ncontribution of this paper is a novel theoretical analysis proving that\ndistributed learning over unreliable network can achieve comparable convergence\nrate to centralized or distributed learning over reliable networks. Further, we\nprove that the influence of the packet drop rate diminishes with the growth of\nthe number of \\textcolor{black}{parameter servers}. We map this theoretical\nresult onto a real-world scenario, training deep neural networks over an\nunreliable network layer, and conduct network simulation to validate the system\nimprovement by allowing the networks to be unreliable. \n\n"}
{"id": "1810.08130", "contents": "Title: Private Machine Learning in TensorFlow using Secure Computation Abstract: We present a framework for experimenting with secure multi-party computation\ndirectly in TensorFlow. By doing so we benefit from several properties valuable\nto both researchers and practitioners, including tight integration with\nordinary machine learning processes, existing optimizations for distributed\ncomputation in TensorFlow, high-level abstractions for expressing complex\nalgorithms and protocols, and an expanded set of familiar tooling. We give an\nopen source implementation of a state-of-the-art protocol and report on\nconcrete benchmarks using typical models from private machine learning. \n\n"}
{"id": "1810.08217", "contents": "Title: Deep Learning Methods for Reynolds-Averaged Navier-Stokes Simulations of\n  Airfoil Flows Abstract: With this study we investigate the accuracy of deep learning models for the\ninference of Reynolds-Averaged Navier-Stokes solutions. We focus on a\nmodernized U-net architecture, and evaluate a large number of trained neural\nnetworks with respect to their accuracy for the calculation of pressure and\nvelocity distributions. In particular, we illustrate how training data size and\nthe number of weights influence the accuracy of the solutions. With our best\nmodels we arrive at a mean relative pressure and velocity error of less than 3%\nacross a range of previously unseen airfoil shapes. In addition all source code\nis publicly available in order to ensure reproducibility and to provide a\nstarting point for researchers interested in deep learning methods for physics\nproblems. While this work focuses on RANS solutions, the neural network\narchitecture and learning setup are very generic, and applicable to a wide\nrange of PDE boundary value problems on Cartesian grids. \n\n"}
{"id": "1810.08591", "contents": "Title: A Modern Take on the Bias-Variance Tradeoff in Neural Networks Abstract: The bias-variance tradeoff tells us that as model complexity increases, bias\nfalls and variances increases, leading to a U-shaped test error curve. However,\nrecent empirical results with over-parameterized neural networks are marked by\na striking absence of the classic U-shaped test error curve: test error keeps\ndecreasing in wider networks. This suggests that there might not be a\nbias-variance tradeoff in neural networks with respect to network width, unlike\nwas originally claimed by, e.g., Geman et al. (1992). Motivated by the shaky\nevidence used to support this claim in neural networks, we measure bias and\nvariance in the modern setting. We find that both bias and variance can\ndecrease as the number of parameters grows. To better understand this, we\nintroduce a new decomposition of the variance to disentangle the effects of\noptimization and data sampling. We also provide theoretical analysis in a\nsimplified setting that is consistent with our empirical findings. \n\n"}
{"id": "1810.08591", "contents": "Title: A Modern Take on the Bias-Variance Tradeoff in Neural Networks Abstract: The bias-variance tradeoff tells us that as model complexity increases, bias\nfalls and variances increases, leading to a U-shaped test error curve. However,\nrecent empirical results with over-parameterized neural networks are marked by\na striking absence of the classic U-shaped test error curve: test error keeps\ndecreasing in wider networks. This suggests that there might not be a\nbias-variance tradeoff in neural networks with respect to network width, unlike\nwas originally claimed by, e.g., Geman et al. (1992). Motivated by the shaky\nevidence used to support this claim in neural networks, we measure bias and\nvariance in the modern setting. We find that both bias and variance can\ndecrease as the number of parameters grows. To better understand this, we\nintroduce a new decomposition of the variance to disentangle the effects of\noptimization and data sampling. We also provide theoretical analysis in a\nsimplified setting that is consistent with our empirical findings. \n\n"}
{"id": "1810.08765", "contents": "Title: Attribute-aware Collaborative Filtering: Survey and Classification Abstract: Attribute-aware CF models aims at rating prediction given not only the\nhistorical rating from users to items, but also the information associated with\nusers (e.g. age), items (e.g. price), or even ratings (e.g. rating time). This\npaper surveys works in the past decade developing attribute-aware CF systems,\nand discovered that mathematically they can be classified into four different\ncategories. We provide the readers not only the high level mathematical\ninterpretation of the existing works in this area but also the mathematical\ninsight for each category of models. Finally we provide in-depth experiment\nresults comparing the effectiveness of the major works in each category. \n\n"}
{"id": "1810.09079", "contents": "Title: Sparsemax and Relaxed Wasserstein for Topic Sparsity Abstract: Topic sparsity refers to the observation that individual documents usually\nfocus on several salient topics instead of covering a wide variety of topics,\nand a real topic adopts a narrow range of terms instead of a wide coverage of\nthe vocabulary. Understanding this topic sparsity is especially important for\nanalyzing user-generated web content and social media, which are featured in\nthe form of extremely short posts and discussions. As topic sparsity of\nindividual documents in online social media increases, so does the difficulty\nof analyzing the online text sources using traditional methods.\n  In this paper, we propose two novel neural models by providing sparse\nposterior distributions over topics based on the Gaussian sparsemax\nconstruction, enabling efficient training by stochastic backpropagation. We\nconstruct an inference network conditioned on the input data and infer the\nvariational distribution with the relaxed Wasserstein (RW) divergence. Unlike\nexisting works based on Gaussian softmax construction and Kullback-Leibler (KL)\ndivergence, our approaches can identify latent topic sparsity with training\nstability, predictive performance, and topic coherence. Experiments on\ndifferent genres of large text corpora have demonstrated the effectiveness of\nour models as they outperform both probabilistic and neural methods. \n\n"}
{"id": "1810.09536", "contents": "Title: Ordered Neurons: Integrating Tree Structures into Recurrent Neural\n  Networks Abstract: Natural language is hierarchically structured: smaller units (e.g., phrases)\nare nested within larger units (e.g., clauses). When a larger constituent ends,\nall of the smaller constituents that are nested within it must also be closed.\nWhile the standard LSTM architecture allows different neurons to track\ninformation at different time scales, it does not have an explicit bias towards\nmodeling a hierarchy of constituents. This paper proposes to add such an\ninductive bias by ordering the neurons; a vector of master input and forget\ngates ensures that when a given neuron is updated, all the neurons that follow\nit in the ordering are also updated. Our novel recurrent architecture, ordered\nneurons LSTM (ON-LSTM), achieves good performance on four different tasks:\nlanguage modeling, unsupervised parsing, targeted syntactic evaluation, and\nlogical inference. \n\n"}
{"id": "1810.09656", "contents": "Title: Hierarchical Approaches for Reinforcement Learning in Parameterized\n  Action Space Abstract: We explore Deep Reinforcement Learning in a parameterized action space.\nSpecifically, we investigate how to achieve sample-efficient end-to-end\ntraining in these tasks. We propose a new compact architecture for the tasks\nwhere the parameter policy is conditioned on the output of the discrete action\npolicy. We also propose two new methods based on the state-of-the-art\nalgorithms Trust Region Policy Optimization (TRPO) and Stochastic Value\nGradient (SVG) to train such an architecture. We demonstrate that these methods\noutperform the state of the art method, Parameterized Action DDPG, on test\ndomains. \n\n"}
{"id": "1810.09665", "contents": "Title: A jamming transition from under- to over-parametrization affects loss\n  landscape and generalization Abstract: We argue that in fully-connected networks a phase transition delimits the\nover- and under-parametrized regimes where fitting can or cannot be achieved.\nUnder some general conditions, we show that this transition is sharp for the\nhinge loss. In the whole over-parametrized regime, poor minima of the loss are\nnot encountered during training since the number of constraints to satisfy is\ntoo small to hamper minimization. Our findings support a link between this\ntransition and the generalization properties of the network: as we increase the\nnumber of parameters of a given model, starting from an under-parametrized\nnetwork, we observe that the generalization error displays three phases: (i)\ninitial decay, (ii) increase until the transition point --- where it displays a\ncusp --- and (iii) slow decay toward a constant for the rest of the\nover-parametrized regime. Thereby we identify the region where the classical\nphenomenon of over-fitting takes place, and the region where the model keeps\nimproving, in line with previous empirical observations for modern neural\nnetworks. \n\n"}
{"id": "1810.09945", "contents": "Title: Analyzing Neuroimaging Data Through Recurrent Deep Learning Models Abstract: The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples. \n\n"}
{"id": "1810.10181", "contents": "Title: Exploiting Deep Representations for Neural Machine Translation Abstract: Advanced neural machine translation (NMT) models generally implement encoder\nand decoder as multiple layers, which allows systems to model complex functions\nand capture complicated linguistic structures. However, only the top layers of\nencoder and decoder are leveraged in the subsequent process, which misses the\nopportunity to exploit the useful information embedded in other layers. In this\nwork, we propose to simultaneously expose all of these signals with layer\naggregation and multi-layer attention mechanisms. In addition, we introduce an\nauxiliary regularization term to encourage different layers to capture diverse\ninformation. Experimental results on widely-used WMT14 English-German and WMT17\nChinese-English translation data demonstrate the effectiveness and universality\nof the proposed approach. \n\n"}
{"id": "1810.10328", "contents": "Title: Label Propagation for Learning with Label Proportions Abstract: Learning with Label Proportions (LLP) is the problem of recovering the\nunderlying true labels given a dataset when the data is presented in the form\nof bags. This paradigm is particularly suitable in contexts where providing\nindividual labels is expensive and label aggregates are more easily obtained.\nIn the healthcare domain, it is a burden for a patient to keep a detailed diary\nof their daily routines, but often they will be amenable to provide higher\nlevel summaries of daily behavior. We present a novel and efficient graph-based\nalgorithm that encourages local smoothness and exploits the global structure of\nthe data, while preserving the `mass' of each bag. \n\n"}
{"id": "1810.10690", "contents": "Title: SpiderBoost and Momentum: Faster Stochastic Variance Reduction\n  Algorithms Abstract: SARAH and SPIDER are two recently developed stochastic variance-reduced\nalgorithms, and SPIDER has been shown to achieve a near-optimal first-order\noracle complexity in smooth nonconvex optimization. However, SPIDER uses an\naccuracy-dependent stepsize that slows down the convergence in practice, and\ncannot handle objective functions that involve nonsmooth regularizers. In this\npaper, we propose SpiderBoost as an improved scheme, which allows to use a much\nlarger constant-level stepsize while maintaining the same near-optimal oracle\ncomplexity, and can be extended with proximal mapping to handle composite\noptimization (which is nonsmooth and nonconvex) with provable convergence\nguarantee. In particular, we show that proximal SpiderBoost achieves an oracle\ncomplexity of $\\mathcal{O}(\\min\\{n^{1/2}\\epsilon^{-2},\\epsilon^{-3}\\})$ in\ncomposite nonconvex optimization, improving the state-of-the-art result by a\nfactor of $\\mathcal{O}(\\min\\{n^{1/6},\\epsilon^{-1/3}\\})$. We further develop a\nnovel momentum scheme to accelerate SpiderBoost for composite optimization,\nwhich achieves the near-optimal oracle complexity in theory and substantial\nimprovement in experiments. \n\n"}
{"id": "1810.10802", "contents": "Title: Tackling Sequence to Sequence Mapping Problems with Neural Networks Abstract: In Natural Language Processing (NLP), it is important to detect the\nrelationship between two sequences or to generate a sequence of tokens given\nanother observed sequence. We call the type of problems on modelling sequence\npairs as sequence to sequence (seq2seq) mapping problems. A lot of research has\nbeen devoted to finding ways of tackling these problems, with traditional\napproaches relying on a combination of hand-crafted features, alignment models,\nsegmentation heuristics, and external linguistic resources. Although great\nprogress has been made, these traditional approaches suffer from various\ndrawbacks, such as complicated pipeline, laborious feature engineering, and the\ndifficulty for domain adaptation. Recently, neural networks emerged as a\npromising solution to many problems in NLP, speech recognition, and computer\nvision. Neural models are powerful because they can be trained end to end,\ngeneralise well to unseen examples, and the same framework can be easily\nadapted to a new domain.\n  The aim of this thesis is to advance the state-of-the-art in seq2seq mapping\nproblems with neural networks. We explore solutions from three major aspects:\ninvestigating neural models for representing sequences, modelling interactions\nbetween sequences, and using unpaired data to boost the performance of neural\nmodels. For each aspect, we propose novel models and evaluate their efficacy on\nvarious tasks of seq2seq mapping. \n\n"}
{"id": "1810.10927", "contents": "Title: Bayesian Compression for Natural Language Processing Abstract: In natural language processing, a lot of the tasks are successfully solved\nwith recurrent neural networks, but such models have a huge number of\nparameters. The majority of these parameters are often concentrated in the\nembedding layer, which size grows proportionally to the vocabulary length. We\npropose a Bayesian sparsification technique for RNNs which allows compressing\nthe RNN dozens or hundreds of times without time-consuming hyperparameters\ntuning. We also generalize the model for vocabulary sparsification to filter\nout unnecessary words and compress the RNN even further. We show that the\nchoice of the kept words is interpretable. Code is available on github:\nhttps://github.com/tipt0p/SparseBayesianRNN \n\n"}
{"id": "1810.11546", "contents": "Title: Mobile Sensor Data Anonymization Abstract: Motion sensors such as accelerometers and gyroscopes measure the instant\nacceleration and rotation of a device, in three dimensions. Raw data streams\nfrom motion sensors embedded in portable and wearable devices may reveal\nprivate information about users without their awareness. For example, motion\ndata might disclose the weight or gender of a user, or enable their\nre-identification. To address this problem, we propose an on-device\ntransformation of sensor data to be shared for specific applications, such as\nmonitoring selected daily activities, without revealing information that\nenables user identification. We formulate the anonymization problem using an\ninformation-theoretic approach and propose a new multi-objective loss function\nfor training deep autoencoders. This loss function helps minimizing\nuser-identity information as well as data distortion to preserve the\napplication-specific utility. The training process regulates the encoder to\ndisregard user-identifiable patterns and tunes the decoder to shape the output\nindependently of users in the training set. The trained autoencoder can be\ndeployed on a mobile or wearable device to anonymize sensor data even for users\nwho are not included in the training dataset. Data from 24 users transformed by\nthe proposed anonymizing autoencoder lead to a promising trade-off between\nutility and privacy, with an accuracy for activity recognition above 92% and an\naccuracy for user identification below 7%. \n\n"}
{"id": "1810.11677", "contents": "Title: The Variational Deficiency Bottleneck Abstract: We introduce a bottleneck method for learning data representations based on\ninformation deficiency, rather than the more traditional information\nsufficiency. A variational upper bound allows us to implement this method\nefficiently. The bound itself is bounded above by the variational information\nbottleneck objective, and the two methods coincide in the regime of single-shot\nMonte Carlo approximations. The notion of deficiency provides a principled way\nof approximating complicated channels by relatively simpler ones. We show that\nthe deficiency of one channel with respect to another has an operational\ninterpretation in terms of the optimal risk gap of decision problems, capturing\nclassification as a special case. Experiments demonstrate that the deficiency\nbottleneck can provide advantages in terms of minimal sufficiency as measured\nby information bottleneck curves, while retaining robust test performance in\nclassification tasks. \n\n"}
{"id": "1810.11730", "contents": "Title: Low-shot Learning via Covariance-Preserving Adversarial Augmentation\n  Networks Abstract: Deep neural networks suffer from over-fitting and catastrophic forgetting\nwhen trained with small data. One natural remedy for this problem is data\naugmentation, which has been recently shown to be effective. However, previous\nworks either assume that intra-class variances can always be generalized to new\nclasses, or employ naive generation methods to hallucinate finite examples\nwithout modeling their latent distributions. In this work, we propose\nCovariance-Preserving Adversarial Augmentation Networks to overcome existing\nlimits of low-shot learning. Specifically, a novel Generative Adversarial\nNetwork is designed to model the latent distribution of each novel class given\nits related base counterparts. Since direct estimation of novel classes can be\ninductively biased, we explicitly preserve covariance information as the\n`variability' of base examples during the generation process. Empirical results\nshow that our model can generate realistic yet diverse examples, leading to\nsubstantial improvements on the ImageNet benchmark over the state of the art. \n\n"}
{"id": "1810.11874", "contents": "Title: Learning with Bad Training Data via Iterative Trimmed Loss Minimization Abstract: In this paper, we study a simple and generic framework to tackle the problem\nof learning model parameters when a fraction of the training samples are\ncorrupted. We first make a simple observation: in a variety of such settings,\nthe evolution of training accuracy (as a function of training epochs) is\ndifferent for clean and bad samples. Based on this we propose to iteratively\nminimize the trimmed loss, by alternating between (a) selecting samples with\nlowest current loss, and (b) retraining a model on only these samples. We prove\nthat this process recovers the ground truth (with linear convergence rate) in\ngeneralized linear models with standard statistical assumptions.\nExperimentally, we demonstrate its effectiveness in three settings: (a) deep\nimage classifiers with errors only in labels, (b) generative adversarial\nnetworks with bad training images, and (c) deep image classifiers with\nadversarial (image, label) pairs (i.e., backdoor attacks). For the well-studied\nsetting of random label noise, our algorithm achieves state-of-the-art\nperformance without having access to any a-priori guaranteed clean samples. \n\n"}
{"id": "1810.12136", "contents": "Title: Phase Harmonic Correlations and Convolutional Neural Networks Abstract: A major issue in harmonic analysis is to capture the phase dependence of\nfrequency representations, which carries important signal properties. It seems\nthat convolutional neural networks have found a way. Over time-series and\nimages, convolutional networks often learn a first layer of filters which are\nwell localized in the frequency domain, with different phases. We show that a\nrectifier then acts as a filter on the phase of the resulting coefficients. It\ncomputes signal descriptors which are local in space, frequency and phase. The\nnon-linear phase filter becomes a multiplicative operator over phase harmonics\ncomputed with a Fourier transform along the phase. We prove that it defines a\nbi-Lipschitz and invertible representation. The correlations of phase harmonics\ncoefficients characterise coherent structures from their phase dependence\nacross frequencies. For wavelet filters, we show numerically that signals\nhaving sparse wavelet coefficients can be recovered from few phase harmonic\ncorrelations, which provide a compressive representation \n\n"}
{"id": "1810.12730", "contents": "Title: Audiovisual speaker conversion: jointly and simultaneously transforming\n  facial expression and acoustic characteristics Abstract: An audiovisual speaker conversion method is presented for simultaneously\ntransforming the facial expressions and voice of a source speaker into those of\na target speaker. Transforming the facial and acoustic features together makes\nit possible for the converted voice and facial expressions to be highly\ncorrelated and for the generated target speaker to appear and sound natural. It\nuses three neural networks: a conversion network that fuses and transforms the\nfacial and acoustic features, a waveform generation network that produces the\nwaveform from both the converted facial and acoustic features, and an image\nreconstruction network that outputs an RGB facial image also based on both the\nconverted features. The results of experiments using an emotional audiovisual\ndatabase showed that the proposed method achieved significantly higher\nnaturalness compared with one that separately transformed acoustic and facial\nfeatures. \n\n"}
{"id": "1810.12744", "contents": "Title: Cluster Size Management in Multi-Stage Agglomerative Hierarchical\n  Clustering of Acoustic Speech Segments Abstract: Agglomerative hierarchical clustering (AHC) requires only the similarity\nbetween objects to be known. This is attractive when clustering signals of\nvarying length, such as speech, which are not readily represented in\nfixed-dimensional vector space. However, AHC is characterised by $O(N^2)$ space\nand time complexity, making it infeasible for partitioning large datasets. This\nhas recently been addressed by an approach based on the iterative re-clustering\nof independent subsets of the larger dataset. We show that, due to its\niterative nature, this procedure can sometimes lead to unchecked growth of\nindividual subsets, thereby compromising its effectiveness. We propose the\nintegration of a simple space management strategy into the iterative process,\nand show experimentally that this leads to no loss in performance in terms of\nF-measure while guaranteeing that a threshold space complexity is not breached. \n\n"}
{"id": "1810.12754", "contents": "Title: Recurrent Attention Unit Abstract: Recurrent Neural Network (RNN) has been successfully applied in many sequence\nlearning problems. Such as handwriting recognition, image description, natural\nlanguage processing and video motion analysis. After years of development,\nresearchers have improved the internal structure of the RNN and introduced many\nvariants. Among others, Gated Recurrent Unit (GRU) is one of the most widely\nused RNN model. However, GRU lacks the capability of adaptively paying\nattention to certain regions or locations, so that it may cause information\nredundancy or loss during leaning. In this paper, we propose a RNN model,\ncalled Recurrent Attention Unit (RAU), which seamlessly integrates the\nattention mechanism into the interior of GRU by adding an attention gate. The\nattention gate can enhance GRU's ability to remember long-term memory and help\nmemory cells quickly discard unimportant content. RAU is capable of extracting\ninformation from the sequential data by adaptively selecting a sequence of\nregions or locations and pay more attention to the selected regions during\nlearning. Extensive experiments on image classification, sentiment\nclassification and language modeling show that RAU consistently outperforms GRU\nand other baseline methods. \n\n"}
{"id": "1811.00073", "contents": "Title: Deep Generative Model with Beta Bernoulli Process for Modeling and\n  Learning Confounding Factors Abstract: While deep representation learning has become increasingly capable of\nseparating task-relevant representations from other confounding factors in the\ndata, two significant challenges remain. First, there is often an unknown and\npotentially infinite number of confounding factors coinciding in the data.\nSecond, not all of these factors are readily observable. In this paper, we\npresent a deep conditional generative model that learns to disentangle a\ntask-relevant representation from an unknown number of confounding factors that\nmay grow infinitely. This is achieved by marrying the representational power of\ndeep generative models with Bayesian non-parametric factor models, where a\nsupervised deterministic encoder learns task-related representation and a\nprobabilistic encoder with an Indian Buffet Process (IBP) learns the unknown\nnumber of unobservable confounding factors. We tested the presented model in\ntwo datasets: a handwritten digit dataset (MNIST) augmented with colored digits\nand a clinical ECG dataset with significant inter-subject variations and\naugmented with signal artifacts. These diverse data sets highlighted the\nability of the presented model to grow with the complexity of the data and\nidentify the absence or presence of unobserved confounding factors. \n\n"}
{"id": "1811.00147", "contents": "Title: DOLORES: Deep Contextualized Knowledge Graph Embeddings Abstract: We introduce a new method DOLORES for learning knowledge graph embeddings\nthat effectively captures contextual cues and dependencies among entities and\nrelations. First, we note that short paths on knowledge graphs comprising of\nchains of entities and relations can encode valuable information regarding\ntheir contextual usage. We operationalize this notion by representing knowledge\ngraphs not as a collection of triples but as a collection of entity-relation\nchains, and learn embeddings for entities and relations using deep neural\nmodels that capture such contextual usage. In particular, our model is based on\nBi-Directional LSTMs and learn deep representations of entities and relations\nfrom constructed entity-relation chains. We show that these representations can\nvery easily be incorporated into existing models to significantly advance the\nstate of the art on several knowledge graph prediction tasks like link\nprediction, triple classification, and missing relation type prediction (in\nsome cases by at least 9.5%). \n\n"}
{"id": "1811.00217", "contents": "Title: META-DES.Oracle: Meta-learning and feature selection for ensemble\n  selection Abstract: The key issue in Dynamic Ensemble Selection (DES) is defining a suitable\ncriterion for calculating the classifiers' competence. There are several\ncriteria available to measure the level of competence of base classifiers, such\nas local accuracy estimates and ranking. However, using only one criterion may\nlead to a poor estimation of the classifier's competence. In order to deal with\nthis issue, we have proposed a novel dynamic ensemble selection framework using\nmeta-learning, called META-DES. An important aspect of the META-DES framework\nis that multiple criteria can be embedded in the system encoded as different\nsets of meta-features. However, some DES criteria are not suitable for every\nclassification problem. For instance, local accuracy estimates may produce poor\nresults when there is a high degree of overlap between the classes. Moreover, a\nhigher classification accuracy can be obtained if the performance of the\nmeta-classifier is optimized for the corresponding data. In this paper, we\npropose a novel version of the META-DES framework based on the formal\ndefinition of the Oracle, called META-DES.Oracle. The Oracle is an abstract\nmethod that represents an ideal classifier selection scheme. A meta-feature\nselection scheme using an overfitting cautious Binary Particle Swarm\nOptimization (BPSO) is proposed for improving the performance of the\nmeta-classifier. The difference between the outputs obtained by the\nmeta-classifier and those presented by the Oracle is minimized. Thus, the\nmeta-classifier is expected to obtain results that are similar to the Oracle.\nExperiments carried out using 30 classification problems demonstrate that the\noptimization procedure based on the Oracle definition leads to a significant\nimprovement in classification accuracy when compared to previous versions of\nthe META-DES framework and other state-of-the-art DES techniques. \n\n"}
{"id": "1811.00225", "contents": "Title: Understanding Learning Dynamics Of Language Models with SVCCA Abstract: Research has shown that neural models implicitly encode linguistic features,\nbut there has been no research showing \\emph{how} these encodings arise as the\nmodels are trained. We present the first study on the learning dynamics of\nneural language models, using a simple and flexible analysis method called\nSingular Vector Canonical Correlation Analysis (SVCCA), which enables us to\ncompare learned representations across time and across models, without the need\nto evaluate directly on annotated data. We probe the evolution of syntactic,\nsemantic, and topic representations and find that part-of-speech is learned\nearlier than topic; that recurrent layers become more similar to those of a\ntagger during training; and embedding layers less similar. Our results and\nmethods could inform better learning algorithms for NLP models, possibly to\nincorporate linguistic information more effectively. \n\n"}
{"id": "1811.00741", "contents": "Title: Stronger Data Poisoning Attacks Break Data Sanitization Defenses Abstract: Machine learning models trained on data from the outside world can be\ncorrupted by data poisoning attacks that inject malicious points into the\nmodels' training sets. A common defense against these attacks is data\nsanitization: first filter out anomalous training points before training the\nmodel. In this paper, we develop three attacks that can bypass a broad range of\ncommon data sanitization defenses, including anomaly detectors based on nearest\nneighbors, training loss, and singular-value decomposition. By adding just 3%\npoisoned data, our attacks successfully increase test error on the Enron spam\ndetection dataset from 3% to 24% and on the IMDB sentiment classification\ndataset from 12% to 29%. In contrast, existing attacks which do not explicitly\naccount for these data sanitization defenses are defeated by them. Our attacks\nare based on two ideas: (i) we coordinate our attacks to place poisoned points\nnear one another, and (ii) we formulate each attack as a constrained\noptimization problem, with constraints designed to ensure that the poisoned\npoints evade detection. As this optimization involves solving an expensive\nbilevel problem, our three attacks correspond to different ways of\napproximating this problem, based on influence functions; minimax duality; and\nthe Karush-Kuhn-Tucker (KKT) conditions. Our results underscore the need to\ndevelop more robust defenses against data poisoning attacks. \n\n"}
{"id": "1811.00821", "contents": "Title: OrthoNet: Multilayer Network Data Clustering Abstract: Network data appears in very diverse applications, like biological, social,\nor sensor networks. Clustering of network nodes into categories or communities\nhas thus become a very common task in machine learning and data mining. Network\ndata comes with some information about the network edges. In some cases, this\nnetwork information can even be given with multiple views or multiple layers,\neach one representing a different type of relationship between the network\nnodes. Increasingly often, network nodes also carry a feature vector. We\npropose in this paper to extend the node clustering problem, that commonly\nconsiders only the network information, to a problem where both the network\ninformation and the node features are considered together for learning a\nclustering-friendly representation of the feature space. Specifically, we\ndesign a generic two-step algorithm for multilayer network data clustering. The\nfirst step aggregates the different layers of network information into a graph\nrepresentation given by the geometric mean of the network Laplacian matrices.\nThe second step uses a neural net to learn a feature embedding that is\nconsistent with the structure given by the network layers. We propose a novel\nalgorithm for efficiently training the neural net via stochastic gradient\ndescent, which encourages the neural net outputs to span the leading\neigenvectors of the aggregated Laplacian matrix, in order to capture the\npairwise interactions on the network, and provide a clustering-friendly\nrepresentation of the feature space. We demonstrate with an extensive set of\nexperiments on synthetic and real datasets that our method leads to a\nsignificant improvement w.r.t. state-of-the-art multilayer graph clustering\nalgorithms, as it judiciously combines nodes features and network information\nin the node embedding algorithms. \n\n"}
{"id": "1811.00852", "contents": "Title: Understanding Deep Neural Networks Using Topological Data Analysis Abstract: Deep neural networks (DNN) are black box algorithms. They are trained using a\ngradient descent back propagation technique which trains weights in each layer\nfor the sole goal of minimizing training error. Hence, the resulting weights\ncannot be directly explained. Using Topological Data Analysis (TDA) we can get\nan insight on how the neural network is thinking, specifically by analyzing the\nactivation values of validation images as they pass through each layer. \n\n"}
{"id": "1811.01088", "contents": "Title: Sentence Encoders on STILTs: Supplementary Training on Intermediate\n  Labeled-data Tasks Abstract: Pretraining sentence encoders with language modeling and related unsupervised\ntasks has recently been shown to be very effective for language understanding\ntasks. By supplementing language model-style pretraining with further training\non data-rich supervised tasks, such as natural language inference, we obtain\nadditional performance improvements on the GLUE benchmark. Applying\nsupplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of\n81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over\nBERT. We also observe reduced variance across random restarts in this setting.\nOur approach yields similar improvements when applied to ELMo (Peters et al.,\n2018a) and Radford et al. (2018)'s model. In addition, the benefits of\nsupplementary training are particularly pronounced in data-constrained regimes,\nas we show in experiments with artificially limited training data. \n\n"}
{"id": "1811.01294", "contents": "Title: Learning Contextual Hierarchical Structure of Medical Concepts with\n  Poincair\\'e Embeddings to Clarify Phenotypes Abstract: Biomedical association studies are increasingly done using clinical concepts,\nand in particular diagnostic codes from clinical data repositories as\nphenotypes. Clinical concepts can be represented in a meaningful, vector space\nusing word embedding models. These embeddings allow for comparison between\nclinical concepts or for straightforward input to machine learning models.\nUsing traditional approaches, good representations require high dimensionality,\nmaking downstream tasks such as visualization more difficult. We applied\nPoincar\\'e embeddings in a 2-dimensional hyperbolic space to a large-scale\nadministrative claims database and show performance comparable to\n100-dimensional embeddings in a euclidean space. We then examine disease\nrelationships under different disease contexts to better understand potential\nphenotypes. \n\n"}
{"id": "1811.01305", "contents": "Title: Block-wise Partitioning for Extreme Multi-label Classification Abstract: Extreme multi-label classification aims to learn a classifier that annotates\nan instance with a relevant subset of labels from an extremely large label set.\nMany existing solutions embed the label matrix to a low-dimensional linear\nsubspace, or examine the relevance of a test instance to every label via a\nlinear scan. In practice, however, those approaches can be computationally\nexorbitant. To alleviate this drawback, we propose a Block-wise Partitioning\n(BP) pretreatment that divides all instances into disjoint clusters, to each of\nwhich the most frequently tagged label subset is attached. One multi-label\nclassifier is trained on one pair of instance and label clusters, and the label\nset of a test instance is predicted by first delivering it to the most\nappropriate instance cluster. Experiments on benchmark multi-label data sets\nreveal that BP pretreatment significantly reduces prediction time, and retains\nalmost the same level of prediction accuracy. \n\n"}
{"id": "1811.01574", "contents": "Title: Low-Rank Phase Retrieval via Variational Bayesian Learning Abstract: In this paper, we consider the problem of low-rank phase retrieval whose\nobjective is to estimate a complex low-rank matrix from magnitude-only\nmeasurements. We propose a hierarchical prior model for low-rank phase\nretrieval, in which a Gaussian-Wishart hierarchical prior is placed on the\nunderlying low-rank matrix to promote the low-rankness of the matrix. Based on\nthe proposed hierarchical model, a variational expectation-maximization (EM)\nalgorithm is developed. The proposed method is less sensitive to the choice of\nthe initialization point and works well with random initialization. Simulation\nresults are provided to illustrate the effectiveness of the proposed algorithm. \n\n"}
{"id": "1811.01715", "contents": "Title: Multi-armed Bandits with Compensation Abstract: We propose and study the known-compensation multi-arm bandit (KCMAB) problem,\nwhere a system controller offers a set of arms to many short-term players for\n$T$ steps. In each step, one short-term player arrives to the system. Upon\narrival, the player aims to select an arm with the current best average reward\nand receives a stochastic reward associated with the arm. In order to\nincentivize players to explore other arms, the controller provides a proper\npayment compensation to players. The objective of the controller is to maximize\nthe total reward collected by players while minimizing the compensation. We\nfirst provide a compensation lower bound $\\Theta(\\sum_i {\\Delta_i\\log T\\over\nKL_i})$, where $\\Delta_i$ and $KL_i$ are the expected reward gap and\nKullback-Leibler (KL) divergence between distributions of arm $i$ and the best\narm, respectively. We then analyze three algorithms to solve the KCMAB problem,\nand obtain their regrets and compensations. We show that the algorithms all\nachieve $O(\\log T)$ regret and $O(\\log T)$ compensation that match the\ntheoretical lower bound. Finally, we present experimental results to\ndemonstrate the performance of the algorithms. \n\n"}
{"id": "1811.01747", "contents": "Title: The Knowref Coreference Corpus: Removing Gender and Number Cues for\n  Difficult Pronominal Anaphora Resolution Abstract: We introduce a new benchmark for coreference resolution and NLI, Knowref,\nthat targets common-sense understanding and world knowledge. Previous\ncoreference resolution tasks can largely be solved by exploiting the number and\ngender of the antecedents, or have been handcrafted and do not reflect the\ndiversity of naturally occurring text. We present a corpus of over 8,000\nannotated text passages with ambiguous pronominal anaphora. These instances are\nboth challenging and realistic. We show that various coreference systems,\nwhether rule-based, feature-rich, or neural, perform significantly worse on the\ntask than humans, who display high inter-annotator agreement. To explain this\nperformance gap, we show empirically that state-of-the art models often fail to\ncapture context, instead relying on the gender or number of candidate\nantecedents to make a decision. We then use problem-specific insights to\npropose a data-augmentation trick called antecedent switching to alleviate this\ntendency in models. Finally, we show that antecedent switching yields promising\nresults on other tasks as well: we use it to achieve state-of-the-art results\non the GAP coreference task. \n\n"}
{"id": "1811.01778", "contents": "Title: How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the\n  Winograd Schema Challenge and SWAG Abstract: Recent studies have significantly improved the state-of-the-art on\ncommon-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge\n(WSC) and SWAG. The question we ask in this paper is whether improved\nperformance on these benchmarks represents genuine progress towards\ncommon-sense-enabled systems. We make case studies of both benchmarks and\ndesign protocols that clarify and qualify the results of previous work by\nanalyzing threats to the validity of previous experimental designs. Our\nprotocols account for several properties prevalent in common-sense benchmarks\nincluding size limitations, structural regularities, and variable instance\ndifficulty. \n\n"}
{"id": "1811.01908", "contents": "Title: Fast Non-Bayesian Poisson Factorization for Implicit-Feedback\n  Recommendations Abstract: This work explores non-negative low-rank matrix factorization based on\nregularized Poisson models (PF or \"Poisson factorization\" for short) for\nrecommender systems with implicit-feedback data. The properties of Poisson\nlikelihood allow a shortcut for very fast computations over zero-valued inputs,\nand oftentimes results in very sparse factors for both users and items.\nCompared to HPF (a popular Bayesian formulation of the problem with\nhierarchical priors), the frequentist optimization-based approach presented\nhere tends to produce better top-N recommendations with significantly shorter\nfitting times, on top of having sparse solutions. \n\n"}
{"id": "1811.02113", "contents": "Title: On the role of neurogenesis in overcoming catastrophic forgetting Abstract: Lifelong learning capabilities are crucial for artificial autonomous agents\noperating on real-world data, which is typically non-stationary and temporally\ncorrelated. In this work, we demonstrate that dynamically grown networks\noutperform static networks in incremental learning scenarios, even when bounded\nby the same amount of memory in both cases. Learning is unsupervised in our\nmodels, a condition that additionally makes training more challenging whilst\nincreasing the realism of the study, since humans are able to learn without\ndense manual annotation. Our results on artificial neural networks reinforce\nthat structural plasticity constitutes effective prevention against\ncatastrophic forgetting in non-stationary environments, as well as empirically\nsupporting the importance of neurogenesis in the mammalian brain. \n\n"}
{"id": "1811.02122", "contents": "Title: Robust and fine-grained prosody control of end-to-end speech synthesis Abstract: We propose prosody embeddings for emotional and expressive speech synthesis\nnetworks. The proposed methods introduce temporal structures in the embedding\nnetworks, thus enabling fine-grained control of the speaking style of the\nsynthesized speech. The temporal structures can be designed either on the\nspeech side or the text side, leading to different control resolutions in time.\nThe prosody embedding networks are plugged into end-to-end speech synthesis\nnetworks and trained without any other supervision except for the target speech\nfor synthesizing. It is demonstrated that the prosody embedding networks\nlearned to extract prosodic features. By adjusting the learned prosody\nfeatures, we could change the pitch and amplitude of the synthesized speech\nboth at the frame level and the phoneme level. We also introduce the temporal\nnormalization of prosody embeddings, which shows better robustness against\nspeaker perturbations during prosody transfer tasks. \n\n"}
{"id": "1811.02182", "contents": "Title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for\n  Speech Recognition Abstract: Many speech enhancement methods try to learn the relationship between noisy\nand clean speech, obtained using an acoustic room simulator. We point out\nseveral limitations of enhancement methods relying on clean speech targets; the\ngoal of this work is proposing an alternative learning algorithm, called\nacoustic and adversarial supervision (AAS). AAS makes the enhanced output both\nmaximizing the likelihood of transcription on the pre-trained acoustic model\nand having general characteristics of clean speech, which improve\ngeneralization on unseen noisy speeches. We employ the connectionist temporal\nclassification and the unpaired conditional boundary equilibrium generative\nadversarial network as the loss function of AAS. AAS is tested on two datasets\nincluding additive noise without and with reverberation, Librispeech + DEMAND\nand CHiME-4. By visualizing the enhanced speech with different loss\ncombinations, we demonstrate the role of each supervision. AAS achieves a lower\nword error rate than other state-of-the-art methods using the clean speech\ntarget in both datasets. \n\n"}
{"id": "1811.02394", "contents": "Title: DeepChannel: Salience Estimation by Contrastive Learning for Extractive\n  Document Summarization Abstract: We propose DeepChannel, a robust, data-efficient, and interpretable neural\nmodel for extractive document summarization. Given any document-summary pair,\nwe estimate a salience score, which is modeled using an attention-based deep\nneural network, to represent the salience degree of the summary for yielding\nthe document. We devise a contrastive training strategy to learn the salience\nestimation network, and then use the learned salience score as a guide and\niteratively extract the most salient sentences from the document as our\ngenerated summary. In experiments, our model not only achieves state-of-the-art\nROUGE scores on CNN/Daily Mail dataset, but also shows strong robustness in the\nout-of-domain test on DUC2007 test set. Moreover, our model reaches a ROUGE-1\nF-1 score of 39.41 on CNN/Daily Mail test set with merely $1 / 100$ training\nset, demonstrating a tremendous data efficiency. \n\n"}
{"id": "1811.02480", "contents": "Title: Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement\n  in Multi-Talker Environments Abstract: In this paper, we address the problem of enhancing the speech of a speaker of\ninterest in a cocktail party scenario when visual information of the speaker of\ninterest is available. Contrary to most previous studies, we do not learn\nvisual features on the typically small audio-visual datasets, but use an\nalready available face landmark detector (trained on a separate image dataset).\nThe landmarks are used by LSTM-based models to generate time-frequency masks\nwhich are applied to the acoustic mixed-speech spectrogram. Results show that:\n(i) landmark motion features are very effective features for this task, (ii)\nsimilarly to previous work, reconstruction of the target speaker's spectrogram\nmediated by masking is significantly more accurate than direct spectrogram\nreconstruction, and (iii) the best masks depend on both motion landmark\nfeatures and the input mixed-speech spectrogram. To the best of our knowledge,\nour proposed models are the first models trained and evaluated on the limited\nsize GRID and TCD-TIMIT datasets, that achieve speaker-independent speech\nenhancement in a multi-talker setting. \n\n"}
{"id": "1811.02783", "contents": "Title: YASENN: Explaining Neural Networks via Partitioning Activation Sequences Abstract: We introduce a novel approach to feed-forward neural network interpretation\nbased on partitioning the space of sequences of neuron activations. In line\nwith this approach, we propose a model-specific interpretation method, called\nYASENN. Our method inherits many advantages of model-agnostic distillation,\nsuch as an ability to focus on the particular input region and to express an\nexplanation in terms of features different from those observed by a neural\nnetwork. Moreover, examination of distillation error makes the method\napplicable to the problems with low tolerance to interpretation mistakes.\nTechnically, YASENN distills the network with an ensemble of layer-wise\ngradient boosting decision trees and encodes the sequences of neuron\nactivations with leaf indices. The finite number of unique codes induces a\npartitioning of the input space. Each partition may be described in a variety\nof ways, including examination of an interpretable model (e.g. a logistic\nregression or a decision tree) trained to discriminate between objects of those\npartitions. Our experiments provide an intuition behind the method and\ndemonstrate revealed artifacts in neural network decision making. \n\n"}
{"id": "1811.02820", "contents": "Title: Construction and Quality Evaluation of Heterogeneous Hierarchical Topic\n  Models Abstract: In our work, we propose to represent HTM as a set of flat models, or layers,\nand a set of topical hierarchies, or edges. We suggest several quality measures\nfor edges of hierarchical models, resembling those proposed for flat models. We\nconduct an assessment experimentation and show strong correlation between the\nproposed measures and human judgement on topical edge quality. We also\nintroduce heterogeneous algorithm to build hierarchical topic models for\nheterogeneous data sources. We show how making certain adjustments to learning\nprocess helps to retain original structure of customized models while allowing\nfor slight coherent modifications for new documents. We evaluate this approach\nusing the proposed measures and show that the proposed heterogeneous algorithm\nsignificantly outperforms the baseline concat approach. Finally, we implement\nour own ESE called Rysearch, which demonstrates the potential of ARTM approach\nfor visualizing large heterogeneous document collections. \n\n"}
{"id": "1811.03081", "contents": "Title: Forging new worlds: high-resolution synthetic galaxies with chained\n  generative adversarial networks Abstract: Astronomy of the 21st century increasingly finds itself with extreme\nquantities of data. This growth in data is ripe for modern technologies such as\ndeep image processing, which has the potential to allow astronomers to\nautomatically identify, classify, segment and deblend various astronomical\nobjects. In this paper, we explore the use of chained generative adversarial\nnetworks (GANs), a class of generative models that learn mappings from latent\nspaces to data distributions by modelling the joint distribution of the data,\nto produce physically realistic galaxy images as one use case of such models.\nIn cosmology, such datasets can aid in the calibration of shape measurements\nfor weak lensing by augmenting data with synthetic images. By measuring the\ndistributions of multiple physical properties, we show that images generated\nwith our approach closely follow the distributions of real galaxies, further\nestablishing state-of-the-art GAN architectures as a valuable tool for\nmodern-day astronomy. \n\n"}
{"id": "1811.03194", "contents": "Title: AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning Abstract: Perceptual ad-blocking is a novel approach that detects online advertisements\nbased on their visual content. Compared to traditional filter lists, the use of\nperceptual signals is believed to be less prone to an arms race with web\npublishers and ad networks. We demonstrate that this may not be the case. We\ndescribe attacks on multiple perceptual ad-blocking techniques, and unveil a\nnew arms race that likely disfavors ad-blockers. Unexpectedly, perceptual\nad-blocking can also introduce new vulnerabilities that let an attacker bypass\nweb security boundaries and mount DDoS attacks.\n  We first analyze the design space of perceptual ad-blockers and present a\nunified architecture that incorporates prior academic and commercial work. We\nthen explore a variety of attacks on the ad-blocker's detection pipeline, that\nenable publishers or ad networks to evade or detect ad-blocking, and at times\neven abuse its high privilege level to bypass web security boundaries.\n  On one hand, we show that perceptual ad-blocking must visually classify\nrendered web content to escape an arms race centered on obfuscation of page\nmarkup. On the other, we present a concrete set of attacks on visual\nad-blockers by constructing adversarial examples in a real web page context.\nFor seven ad-detectors, we create perturbed ads, ad-disclosure logos, and\nnative web content that misleads perceptual ad-blocking with 100% success\nrates. In one of our attacks, we demonstrate how a malicious user can upload\nadversarial content, such as a perturbed image in a Facebook post, that fools\nthe ad-blocker into removing another users' non-ad content.\n  Moving beyond the Web and visual domain, we also build adversarial examples\nfor AdblockRadio, an open source radio client that uses machine learning to\ndetects ads in raw audio streams. \n\n"}
{"id": "1811.03307", "contents": "Title: Memory-based Deep Reinforcement Learning for Obstacle Avoidance in UAV\n  with Limited Environment Knowledge Abstract: This paper presents our method for enabling a UAV quadrotor, equipped with a\nmonocular camera, to autonomously avoid collisions with obstacles in\nunstructured and unknown indoor environments. When compared to obstacle\navoidance in ground vehicular robots, UAV navigation brings in additional\nchallenges because the UAV motion is no more constrained to a well-defined\nindoor ground or street environment. Horizontal structures in indoor and\noutdoor environments like decorative items, furnishings, ceiling fans,\nsign-boards, tree branches etc., also become relevant obstacles unlike those\nfor ground vehicular robots. Thus, methods of obstacle avoidance developed for\nground robots are clearly inadequate for UAV navigation. Current control\nmethods using monocular images for UAV obstacle avoidance are heavily dependent\non environment information. These controllers do not fully retain and utilize\nthe extensively available information about the ambient environment for\ndecision making. We propose a deep reinforcement learning based method for UAV\nobstacle avoidance (OA) and autonomous exploration which is capable of doing\nexactly the same. The crucial idea in our method is the concept of partial\nobservability and how UAVs can retain relevant information about the\nenvironment structure to make better future navigation decisions. Our OA\ntechnique uses recurrent neural networks with temporal attention and provides\nbetter results compared to prior works in terms of distance covered during\nnavigation without collisions. In addition, our technique has a high inference\nrate (a key factor in robotic applications) and is energy-efficient as it\nminimizes oscillatory motion of UAV and reduces power wastage. \n\n"}
{"id": "1811.03717", "contents": "Title: Fast determinantal point processes via distortion-free intermediate\n  sampling Abstract: Given a fixed $n\\times d$ matrix $\\mathbf{X}$, where $n\\gg d$, we study the\ncomplexity of sampling from a distribution over all subsets of rows where the\nprobability of a subset is proportional to the squared volume of the\nparallelepiped spanned by the rows (a.k.a. a determinantal point process). In\nthis task, it is important to minimize the preprocessing cost of the procedure\n(performed once) as well as the sampling cost (performed repeatedly). To that\nend, we propose a new determinantal point process algorithm which has the\nfollowing two properties, both of which are novel: (1) a preprocessing step\nwhich runs in time $O(\\text{number-of-non-zeros}(\\mathbf{X})\\cdot\\log\nn)+\\text{poly}(d)$, and (2) a sampling step which runs in $\\text{poly}(d)$\ntime, independent of the number of rows $n$. We achieve this by introducing a\nnew regularized determinantal point process (R-DPP), which serves as an\nintermediate distribution in the sampling procedure by reducing the number of\nrows from $n$ to $\\text{poly}(d)$. Crucially, this intermediate distribution\ndoes not distort the probabilities of the target sample. Our key novelty in\ndefining the R-DPP is the use of a Poisson random variable for controlling the\nprobabilities of different subset sizes, leading to new determinantal formulas\nsuch as the normalization constant for this distribution. Our algorithm has\napplications in many diverse areas where determinantal point processes have\nbeen used, such as machine learning, stochastic optimization, data\nsummarization and low-rank matrix reconstruction. \n\n"}
{"id": "1811.03897", "contents": "Title: Deep Ensemble Bayesian Active Learning : Addressing the Mode Collapse\n  issue in Monte Carlo dropout via Ensembles Abstract: In image classification tasks, the ability of deep CNNs to deal with complex\nimage data has proven to be unrivalled. However, they require large amounts of\nlabeled training data to reach their full potential. In specialised domains\nsuch as healthcare, labeled data can be difficult and expensive to obtain.\nActive Learning aims to alleviate this problem, by reducing the amount of\nlabelled data needed for a specific task while delivering satisfactory\nperformance. We propose DEBAL, a new active learning strategy designed for deep\nneural networks. This method improves upon the current state-of-the-art deep\nBayesian active learning method, which suffers from the mode collapse problem.\nWe correct for this deficiency by making use of the expressive power and\nstatistical properties of model ensembles. Our proposed method manages to\ncapture superior data uncertainty, which translates into improved\nclassification performance. We demonstrate empirically that our ensemble method\nyields faster convergence of CNNs trained on the MNIST and CIFAR-10 datasets. \n\n"}
{"id": "1811.04104", "contents": "Title: Deep Learning Super-Diffusion in Multiplex Networks Abstract: Complex network theory has shown success in understanding the emergent and\ncollective behavior of complex systems [1]. Many real-world complex systems\nwere recently discovered to be more accurately modeled as multiplex networks\n[2-6]---in which each interaction type is mapped to its own network layer;\ne.g.~multi-layer transportation networks, coupled social networks, metabolic\nand regulatory networks, etc. A salient physical phenomena emerging from\nmultiplexity is super-diffusion: exhibited by an accelerated diffusion admitted\nby the multi-layer structure as compared to any single layer. Theoretically\nsuper-diffusion was only known to be predicted using the spectral gap of the\nfull Laplacian of a multiplex network and its interacting layers. Here we turn\nto machine learning which has developed techniques to recognize, classify, and\ncharacterize complex sets of data. We show that modern machine learning\narchitectures, such as fully connected and convolutional neural networks, can\nclassify and predict the presence of super-diffusion in multiplex networks with\n94.12\\% accuracy. Such predictions can be done {\\it in situ}, without the need\nto determine spectral properties of a network. \n\n"}
{"id": "1811.04351", "contents": "Title: Generalization Bounds for Vicinal Risk Minimization Principle Abstract: The vicinal risk minimization (VRM) principle, first proposed by\n\\citet{vapnik1999nature}, is an empirical risk minimization (ERM) variant that\nreplaces Dirac masses with vicinal functions. Although there is strong\nnumerical evidence showing that VRM outperforms ERM if appropriate vicinal\nfunctions are chosen, a comprehensive theoretical understanding of VRM is still\nlacking. In this paper, we study the generalization bounds for VRM. Our results\nsupport Vapnik's original arguments and additionally provide deeper insights\ninto VRM. First, we prove that the complexity of function classes convolving\nwith vicinal functions can be controlled by that of the original function\nclasses under the assumption that the function class is composed of\nLipschitz-continuous functions. Then, the resulting generalization bounds for\nVRM suggest that the generalization performance of VRM is also effected by the\nchoice of vicinity function and the quality of function classes. These findings\ncan be used to examine whether the choice of vicinal function is appropriate\nfor the VRM-based learning setting. Finally, we provide a theoretical\nexplanation for existing VRM models, e.g., uniform distribution-based models,\nGaussian distribution-based models, and mixup models. \n\n"}
{"id": "1811.04624", "contents": "Title: Importance Weighted Evolution Strategies Abstract: Evolution Strategies (ES) emerged as a scalable alternative to popular\nReinforcement Learning (RL) techniques, providing an almost perfect speedup\nwhen distributed across hundreds of CPU cores thanks to a reduced communication\noverhead. Despite providing large improvements in wall-clock time, ES is data\ninefficient when compared to competing RL methods. One of the main causes of\nsuch inefficiency is the collection of large batches of experience, which are\ndiscarded after each policy update. In this work, we study how to perform more\nthan one update per batch of experience by means of Importance Sampling while\npreserving the scalability of the original method. The proposed method,\nImportance Weighted Evolution Strategies (IW-ES), shows promising results and\nis a first step towards designing efficient ES algorithms. \n\n"}
{"id": "1811.04682", "contents": "Title: Learning Segmentation Masks with the Independence Prior Abstract: An instance with a bad mask might make a composite image that uses it look\nfake. This encourages us to learn segmentation by generating realistic\ncomposite images. To achieve this, we propose a novel framework that exploits a\nnew proposed prior called the independence prior based on Generative\nAdversarial Networks (GANs). The generator produces an image with multiple\ncategory-specific instance providers, a layout module and a composition module.\nFirstly, each provider independently outputs a category-specific instance image\nwith a soft mask. Then the provided instances' poses are corrected by the\nlayout module. Lastly, the composition module combines these instances into a\nfinal image. Training with adversarial loss and penalty for mask area, each\nprovider learns a mask that is as small as possible but enough to cover a\ncomplete category-specific instance. Weakly supervised semantic segmentation\nmethods widely use grouping cues modeling the association between image parts,\nwhich are either artificially designed or learned with costly segmentation\nlabels or only modeled on local pairs. Unlike them, our method automatically\nmodels the dependence between any parts and learns instance segmentation. We\napply our framework in two cases: (1) Foreground segmentation on\ncategory-specific images with box-level annotation. (2) Unsupervised learning\nof instance appearances and masks with only one image of homogeneous object\ncluster (HOC). We get appealing results in both tasks, which shows the\nindependence prior is useful for instance segmentation and it is possible to\nunsupervisedly learn instance masks with only one image. \n\n"}
{"id": "1811.04689", "contents": "Title: Adversarial Learning of Label Dependency: A Novel Framework for\n  Multi-class Classification Abstract: Recent work has shown that exploiting relations between labels improves the\nperformance of multi-label classification. We propose a novel framework based\non generative adversarial networks (GANs) to model label dependency. The\ndiscriminator learns to model label dependency by discriminating real and\ngenerated label sets. To fool the discriminator, the classifier, or generator,\nlearns to generate label sets with dependencies close to real data. Extensive\nexperiments and comparisons on two large-scale image classification benchmark\ndatasets (MS-COCO and NUS-WIDE) show that the discriminator improves\ngeneralization ability for different kinds of models \n\n"}
{"id": "1811.04773", "contents": "Title: Syntax Helps ELMo Understand Semantics: Is Syntax Still Relevant in a\n  Deep Neural Architecture for SRL? Abstract: Do unsupervised methods for learning rich, contextualized token\nrepresentations obviate the need for explicit modeling of linguistic structure\nin neural network models for semantic role labeling (SRL)? We address this\nquestion by incorporating the massively successful ELMo embeddings (Peters et\nal., 2018) into LISA (Strubell et al., 2018), a strong, linguistically-informed\nneural network architecture for SRL. In experiments on the CoNLL-2005 shared\ntask we find that though ELMo out-performs typical word embeddings, beginning\nto close the gap in F1 between LISA with predicted and gold syntactic parses,\nsyntactically-informed models still out-perform syntax-free models when both\nuse ELMo, especially on out-of-domain data. Our results suggest that linguistic\nstructures are indeed still relevant in this golden age of deep learning for\nNLP. \n\n"}
{"id": "1811.04903", "contents": "Title: Stream attention-based multi-array end-to-end speech recognition Abstract: Automatic Speech Recognition (ASR) using multiple microphone arrays has\nachieved great success in the far-field robustness. Taking advantage of all the\ninformation that each array shares and contributes is crucial in this task.\nMotivated by the advances of joint Connectionist Temporal Classification\n(CTC)/attention mechanism in the End-to-End (E2E) ASR, a stream attention-based\nmulti-array framework is proposed in this work. Microphone arrays, acting as\ninformation streams, are activated by separate encoders and decoded under the\ninstruction of both CTC and attention networks. In terms of attention, a\nhierarchical structure is adopted. On top of the regular attention networks,\nstream attention is introduced to steer the decoder toward the most informative\nencoders. Experiments have been conducted on AMI and DIRHA multi-array corpora\nusing the encoder-decoder architecture. Compared with the best single-array\nresults, the proposed framework has achieved relative Word Error Rates (WERs)\nreduction of 3.7% and 9.7% in the two datasets, respectively, which is better\nthan conventional strategies as well. \n\n"}
{"id": "1811.04973", "contents": "Title: Eliminating Latent Discrimination: Train Then Mask Abstract: How can we control for latent discrimination in predictive models? How can we\nprovably remove it? Such questions are at the heart of algorithmic fairness and\nits impacts on society. In this paper, we define a new operational fairness\ncriteria, inspired by the well-understood notion of omitted variable-bias in\nstatistics and econometrics. Our notion of fairness effectively controls for\nsensitive features and provides diagnostics for deviations from fair decision\nmaking. We then establish analytical and algorithmic results about the\nexistence of a fair classifier in the context of supervised learning. Our\nresults readily imply a simple, but rather counter-intuitive, strategy for\neliminating latent discrimination. In order to prevent other features proxying\nfor sensitive features, we need to include sensitive features in the training\nphase, but exclude them in the test/evaluation phase while controlling for\ntheir effects. We evaluate the performance of our algorithm on several\nreal-world datasets and show how fairness for these datasets can be improved\nwith a very small loss in accuracy. \n\n"}
{"id": "1811.05095", "contents": "Title: A Local Regret in Nonconvex Online Learning Abstract: We consider an online learning process to forecast a sequence of outcomes for\nnonconvex models. A typical measure to evaluate online learning algorithms is\nregret but such standard definition of regret is intractable for nonconvex\nmodels even in offline settings. Hence, gradient based definition of regrets\nare common for both offline and online nonconvex problems. Recently, a notion\nof local gradient based regret was introduced. Inspired by the concept of\ncalibration and a local gradient based regret, we introduce another definition\nof regret and we discuss why our definition is more interpretable for\nforecasting problems. We also provide bound analysis for our regret under\ncertain assumptions. \n\n"}
{"id": "1811.05247", "contents": "Title: An Online Attention-based Model for Speech Recognition Abstract: Attention-based end-to-end models such as Listen, Attend and Spell (LAS),\nsimplify the whole pipeline of traditional automatic speech recognition (ASR)\nsystems and become popular in the field of speech recognition. In previous\nwork, researchers have shown that such architectures can acquire comparable\nresults to state-of-the-art ASR systems, especially when using a bidirectional\nencoder and global soft attention (GSA) mechanism. However, bidirectional\nencoder and GSA are two obstacles for real-time speech recognition. In this\nwork, we aim to stream LAS baseline by removing the above two obstacles. On the\nencoder side, we use a latency-controlled (LC) bidirectional structure to\nreduce the delay of forward computation. Meanwhile, an adaptive monotonic\nchunk-wise attention (AMoChA) mechanism is proposed to replace GSA for the\ncalculation of attention weight distribution. Furthermore, we propose two\nmethods to alleviate the huge performance degradation when combining LC and\nAMoChA. Finally, we successfully acquire an online LAS model, LC-AMoChA, which\nhas only 3.5% relative performance reduction to LAS baseline on our internal\nMandarin corpus. \n\n"}
{"id": "1811.05402", "contents": "Title: Embedding Electronic Health Records for Clinical Information Retrieval Abstract: Neural network representation learning frameworks have recently shown to be\nhighly effective at a wide range of tasks ranging from radiography\ninterpretation via data-driven diagnostics to clinical decision support. This\noften superior performance comes at the price of dramatically increased\ntraining data requirements that cannot be satisfied in every given institution\nor scenario. As a means of countering such data sparsity effects, distant\nsupervision alleviates the need for scarce in-domain data by relying on a\nrelated, resource-rich, task for training.\n  This study presents an end-to-end neural clinical decision support system\nthat recommends relevant literature for individual patients (few available\nresources) via distant supervision on the well-known MIMIC-III collection\n(abundant resource). Our experiments show significant improvements in retrieval\neffectiveness over traditional statistical as well as purely locally supervised\nretrieval models. \n\n"}
{"id": "1811.05467", "contents": "Title: Towards Neural Machine Translation for African Languages Abstract: Given that South African education is in crisis, strategies for improvement\nand sustainability of high-quality, up-to-date education must be explored. In\nthe migration of education online, inclusion of machine translation for\nlow-resourced local languages becomes necessary. This paper aims to spur the\nuse of current neural machine translation (NMT) techniques for low-resourced\nlocal languages. The paper demonstrates state-of-the-art performance on\nEnglish-to-Setswana translation using the Autshumato dataset. The use of the\nTransformer architecture beat previous techniques by 5.33 BLEU points. This\ndemonstrates the promise of using current NMT techniques for African languages. \n\n"}
{"id": "1811.05521", "contents": "Title: Deep Q learning for fooling neural networks Abstract: Deep learning models are vulnerable to external attacks. In this paper, we\npropose a Reinforcement Learning (RL) based approach to generate adversarial\nexamples for the pre-trained (target) models. We assume a semi black-box\nsetting where the only access an adversary has to the target model is the class\nprobabilities obtained for the input queries. We train a Deep Q Network (DQN)\nagent which, with experience, learns to attack only a small portion of image\npixels to generate non-targeted adversarial images. Initially, an agent\nexplores an environment by sequentially modifying random sets of image pixels\nand observes its effect on the class probabilities. At the end of an episode,\nit receives a positive (negative) reward if it succeeds (fails) to alter the\nlabel of the image. Experimental results with MNIST, CIFAR-10 and Imagenet\ndatasets demonstrate that our RL framework is able to learn an effective attack\npolicy. \n\n"}
{"id": "1811.05592", "contents": "Title: Controllability, Multiplexing, and Transfer Learning in Networks using\n  Evolutionary Learning Abstract: Networks are fundamental building blocks for representing data, and\ncomputations. Remarkable progress in learning in structurally defined (shallow\nor deep) networks has recently been achieved. Here we introduce evolutionary\nexploratory search and learning method of topologically flexible networks under\nthe constraint of producing elementary computational steady-state input-output\noperations.\n  Our results include; (1) the identification of networks, over four orders of\nmagnitude, implementing computation of steady-state input-output functions,\nsuch as a band-pass filter, a threshold function, and an inverse band-pass\nfunction. Next, (2) the learned networks are technically controllable as only a\nsmall number of driver nodes are required to move the system to a new state.\nFurthermore, we find that the fraction of required driver nodes is constant\nduring evolutionary learning, suggesting a stable system design. (3), our\nframework allows multiplexing of different computations using the same network.\nFor example, using a binary representation of the inputs, the network can\nreadily compute three different input-output functions. Finally, (4) the\nproposed evolutionary learning demonstrates transfer learning. If the system\nlearns one function A, then learning B requires on average less number of steps\nas compared to learning B from tabula rasa.\n  We conclude that the constrained evolutionary learning produces large robust\ncontrollable circuits, capable of multiplexing and transfer learning. Our study\nsuggests that network-based computations of steady-state functions,\nrepresenting either cellular modules of cell-to-cell communication networks or\ninternal molecular circuits communicating within a cell, could be a powerful\nmodel for biologically inspired computing. This complements conceptualizations\nsuch as attractor based models, or reservoir computing. \n\n"}
{"id": "1811.05770", "contents": "Title: Internal Wiring of Cartesian Verbs and Prepositions Abstract: Categorical compositional distributional semantics (CCDS) allows one to\ncompute the meaning of phrases and sentences from the meaning of their\nconstituent words. A type-structure carried over from the traditional\ncategorial model of grammar a la Lambek becomes a 'wire-structure' that\nmediates the interaction of word meanings. However, CCDS has a much richer\nlogical structure than plain categorical semantics in that certain words can\nalso be given an 'internal wiring' that either provides their entire meaning or\nreduces the size their meaning space. Previous examples of internal wiring\ninclude relative pronouns and intersective adjectives. Here we establish the\nsame for a large class of well-behaved transitive verbs to which we refer as\nCartesian verbs, and reduce the meaning space from a ternary tensor to a unary\none. Some experimental evidence is also provided. \n\n"}
{"id": "1811.05852", "contents": "Title: Predicting the time-evolution of multi-physics systems with\n  sequence-to-sequence models Abstract: In this work, sequence-to-sequence (seq2seq) models, originally developed for\nlanguage translation, are used to predict the temporal evolution of complex,\nmulti-physics computer simulations. The predictive performance of seq2seq\nmodels is compared to state transition models for datasets generated with\nmulti-physics codes with varying levels of complexity - from simple 1D\ndiffusion calculations to simulations of inertial confinement fusion\nimplosions. Seq2seq models demonstrate the ability to accurately emulate\ncomplex systems, enabling the rapid estimation of the evolution of quantities\nof interest in computationally expensive simulations. \n\n"}
{"id": "1811.06067", "contents": "Title: Interpretable deep learning for guided structure-property explorations\n  in photovoltaics Abstract: The performance of an organic photovoltaic device is intricately connected to\nits active layer morphology. This connection between the active layer and\ndevice performance is very expensive to evaluate, either experimentally or\ncomputationally. Hence, designing morphologies to achieve higher performances\nis non-trivial and often intractable. To solve this, we first introduce a deep\nconvolutional neural network (CNN) architecture that can serve as a fast and\nrobust surrogate for the complex structure-property map. Several tests were\nperformed to gain trust in this trained model. Then, we utilize this fast\nframework to perform robust microstructural design to enhance device\nperformance. \n\n"}
{"id": "1811.06315", "contents": "Title: Effect of data reduction on sequence-to-sequence neural TTS Abstract: Recent speech synthesis systems based on sampling from autoregressive neural\nnetworks models can generate speech almost undistinguishable from human\nrecordings. However, these models require large amounts of data. This paper\nshows that the lack of data from one speaker can be compensated with data from\nother speakers. The naturalness of Tacotron2-like models trained on a blend of\n5k utterances from 7 speakers is better than that of speaker dependent models\ntrained on 15k utterances, but in terms of stability multi-speaker models are\nalways more stable. We also demonstrate that models mixing only 1250 utterances\nfrom a target speaker with 5k utterances from another 6 speakers can produce\nsignificantly better quality than state-of-the-art DNN-guided unit selection\nsystems trained on more than 10 times the data from the target speaker. \n\n"}
{"id": "1811.06407", "contents": "Title: Neural Predictive Belief Representations Abstract: Unsupervised representation learning has succeeded with excellent results in\nmany applications. It is an especially powerful tool to learn a good\nrepresentation of environments with partial or noisy observations. In partially\nobservable domains it is important for the representation to encode a belief\nstate, a sufficient statistic of the observations seen so far. In this paper,\nwe investigate whether it is possible to learn such a belief representation\nusing modern neural architectures. Specifically, we focus on one-step frame\nprediction and two variants of contrastive predictive coding (CPC) as the\nobjective functions to learn the representations. To evaluate these learned\nrepresentations, we test how well they can predict various pieces of\ninformation about the underlying state of the environment, e.g., position of\nthe agent in a 3D maze. We show that all three methods are able to learn belief\nrepresentations of the environment, they encode not only the state information,\nbut also its uncertainty, a crucial aspect of belief states. We also find that\nfor CPC multi-step predictions and action-conditioning are critical for\naccurate belief representations in visually complex environments. The ability\nof neural representations to capture the belief information has the potential\nto spur new advances for learning and planning in partially observable domains,\nwhere leveraging uncertainty is essential for optimal decision making. \n\n"}
{"id": "1811.07017", "contents": "Title: Towards Training Recurrent Neural Networks for Lifelong Learning Abstract: Catastrophic forgetting and capacity saturation are the central challenges of\nany parametric lifelong learning system. In this work, we study these\nchallenges in the context of sequential supervised learning with an emphasis on\nrecurrent neural networks. To evaluate the models in the lifelong learning\nsetting, we propose a curriculum-based, simple, and intuitive benchmark where\nthe models are trained on tasks with increasing levels of difficulty. To\nmeasure the impact of catastrophic forgetting, the model is tested on all the\nprevious tasks as it completes any task. As a step towards developing true\nlifelong learning systems, we unify Gradient Episodic Memory (a catastrophic\nforgetting alleviation approach) and Net2Net(a capacity expansion approach).\nBoth these models are proposed in the context of feedforward networks and we\nevaluate the feasibility of using them for recurrent networks. Evaluation on\nthe proposed benchmark shows that the unified model is more suitable than the\nconstituent models for lifelong learning setting. \n\n"}
{"id": "1811.07032", "contents": "Title: Mining Entity Synonyms with Efficient Neural Set Generation Abstract: Mining entity synonym sets (i.e., sets of terms referring to the same entity)\nis an important task for many entity-leveraging applications. Previous work\neither rank terms based on their similarity to a given query term, or treats\nthe problem as a two-phase task (i.e., detecting synonymy pairs, followed by\norganizing these pairs into synonym sets). However, these approaches fail to\nmodel the holistic semantics of a set and suffer from the error propagation\nissue. Here we propose a new framework, named SynSetMine, that efficiently\ngenerates entity synonym sets from a given vocabulary, using example sets from\nexternal knowledge bases as distant supervision. SynSetMine consists of two\nnovel modules: (1) a set-instance classifier that jointly learns how to\nrepresent a permutation invariant synonym set and whether to include a new\ninstance (i.e., a term) into the set, and (2) a set generation algorithm that\nenumerates the vocabulary only once and applies the learned set-instance\nclassifier to detect all entity synonym sets in it. Experiments on three real\ndatasets from different domains demonstrate both effectiveness and efficiency\nof SynSetMine for mining entity synonym sets. \n\n"}
{"id": "1811.07056", "contents": "Title: Domain Adaptive Transfer Learning with Specialist Models Abstract: Transfer learning is a widely used method to build high performing computer\nvision models. In this paper, we study the efficacy of transfer learning by\nexamining how the choice of data impacts performance. We find that more\npre-training data does not always help, and transfer performance depends on a\njudicious choice of pre-training data. These findings are important given the\ncontinued increase in dataset sizes. We further propose domain adaptive\ntransfer learning, a simple and effective pre-training method using importance\nweights computed based on the target dataset. Our method to compute importance\nweights follow from ideas in domain adaptation, and we show a novel application\nto transfer learning. Our methods achieve state-of-the-art results on multiple\nfine-grained classification datasets and are well-suited for use in practice. \n\n"}
{"id": "1811.07078", "contents": "Title: An Affect-Rich Neural Conversational Model with Biased Attention and\n  Weighted Cross-Entropy Loss Abstract: Affect conveys important implicit information in human communication. Having\nthe capability to correctly express affect during human-machine conversations\nis one of the major milestones in artificial intelligence. In recent years,\nextensive research on open-domain neural conversational models has been\nconducted. However, embedding affect into such models is still under explored.\nIn this paper, we propose an end-to-end affect-rich open-domain neural\nconversational model that produces responses not only appropriate in syntax and\nsemantics, but also with rich affect. Our model extends the Seq2Seq model and\nadopts VAD (Valence, Arousal and Dominance) affective notations to embed each\nword with affects. In addition, our model considers the effect of negators and\nintensifiers via a novel affective attention mechanism, which biases attention\ntowards affect-rich words in input sentences. Lastly, we train our model with\nan affect-incorporated objective function to encourage the generation of\naffect-rich words in the output responses. Evaluations based on both perplexity\nand human evaluations show that our model outperforms the state-of-the-art\nbaseline model of comparable size in producing natural and affect-rich\nresponses. \n\n"}
{"id": "1811.07253", "contents": "Title: Quantifying Uncertainties in Natural Language Processing Tasks Abstract: Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks. \n\n"}
{"id": "1811.07609", "contents": "Title: Outlier Aware Network Embedding for Attributed Networks Abstract: Attributed network embedding has received much interest from the research\ncommunity as most of the networks come with some content in each node, which is\nalso known as node attributes. Existing attributed network approaches work well\nwhen the network is consistent in structure and attributes, and nodes behave as\nexpected. But real world networks often have anomalous nodes. Typically these\noutliers, being relatively unexplainable, affect the embeddings of other nodes\nin the network. Thus all the downstream network mining tasks fail miserably in\nthe presence of such outliers. Hence an integrated approach to detect anomalies\nand reduce their overall effect on the network embedding is required.\n  Towards this end, we propose an unsupervised outlier aware network embedding\nalgorithm (ONE) for attributed networks, which minimizes the effect of the\noutlier nodes, and hence generates robust network embeddings. We align and\njointly optimize the loss functions coming from structure and attributes of the\nnetwork. To the best of our knowledge, this is the first generic network\nembedding approach which incorporates the effect of outliers for an attributed\nnetwork without any supervision. We experimented on publicly available real\nnetworks and manually planted different types of outliers to check the\nperformance of the proposed algorithm. Results demonstrate the superiority of\nour approach to detect the network outliers compared to the state-of-the-art\napproaches. We also consider different downstream machine learning applications\non networks to show the efficiency of ONE as a generic network embedding\ntechnique. The source code is made available at\nhttps://github.com/sambaranban/ONE. \n\n"}
{"id": "1811.07755", "contents": "Title: Building Efficient Deep Neural Networks with Unitary Group Convolutions Abstract: We propose unitary group convolutions (UGConvs), a building block for CNNs\nwhich compose a group convolution with unitary transforms in feature space to\nlearn a richer set of representations than group convolution alone. UGConvs\ngeneralize two disparate ideas in CNN architecture, channel shuffling (i.e.\nShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying\ninsights that lead to a deeper understanding of each technique. We\nexperimentally demonstrate that dense unitary transforms can outperform channel\nshuffling in DNN accuracy. On the other hand, different dense transforms\nexhibit comparable accuracy performance. Based on these observations we propose\nHadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar\naccuracy to circulant networks with lower computation complexity, and better\naccuracy than ShuffleNets with the same number of parameters and floating-point\nmultiplies. \n\n"}
{"id": "1811.07821", "contents": "Title: Efficient random graph matching via degree profiles Abstract: Random graph matching refers to recovering the underlying vertex\ncorrespondence between two random graphs with correlated edges; a prominent\nexample is when the two random graphs are given by Erd\\H{o}s-R\\'{e}nyi graphs\n$G(n,\\frac{d}{n})$. This can be viewed as an average-case and noisy version of\nthe graph isomorphism problem. Under this model, the maximum likelihood\nestimator is equivalent to solving the intractable quadratic assignment\nproblem. This work develops an $\\tilde{O}(n d^2+n^2)$-time algorithm which\nperfectly recovers the true vertex correspondence with high probability,\nprovided that the average degree is at least $d = \\Omega(\\log^2 n)$ and the two\ngraphs differ by at most $\\delta = O( \\log^{-2}(n) )$ fraction of edges. For\ndense graphs and sparse graphs, this can be improved to $\\delta = O(\n\\log^{-2/3}(n) )$ and $\\delta = O( \\log^{-2}(d) )$ respectively, both in\npolynomial time. The methodology is based on appropriately chosen distance\nstatistics of the degree profiles (empirical distribution of the degrees of\nneighbors). Before this work, the best known result achieves $\\delta=O(1)$ and\n$n^{o(1)} \\leq d \\leq n^c$ for some constant $c$ with an $n^{O(\\log n)}$-time\nalgorithm \\cite{barak2018nearly} and $\\delta=\\tilde O((d/n)^4)$ and $d =\n\\tilde{\\Omega}(n^{4/5})$ with a polynomial-time algorithm\n\\cite{dai2018performance}. \n\n"}
{"id": "1811.07957", "contents": "Title: Model change detection with application to machine learning Abstract: Model change detection is studied, in which there are two sets of samples\nthat are independently and identically distributed (i.i.d.) according to a\npre-change probabilistic model with parameter $\\theta$, and a post-change model\nwith parameter $\\theta'$, respectively. The goal is to detect whether the\nchange in the model is significant, i.e., whether the difference between the\npre-change parameter and the post-change parameter $\\|\\theta-\\theta'\\|_2$ is\nlarger than a pre-determined threshold $\\rho$. The problem is considered in a\nNeyman-Pearson setting, where the goal is to maximize the probability of\ndetection under a false alarm constraint. Since the generalized likelihood\nratio test (GLRT) is difficult to compute in this problem, we construct an\nempirical difference test (EDT), which approximates the GLRT and has low\ncomputational complexity. Moreover, we provide an approximation method to set\nthe threshold of the EDT to meet the false alarm constraint. Experiments with\nlinear regression and logistic regression are conducted to validate the\nproposed algorithms. \n\n"}
{"id": "1811.08039", "contents": "Title: Fenchel Lifted Networks: A Lagrange Relaxation of Neural Network\n  Training Abstract: Despite the recent successes of deep neural networks, the corresponding\ntraining problem remains highly non-convex and difficult to optimize. Classes\nof models have been proposed that introduce greater structure to the objective\nfunction at the cost of lifting the dimension of the problem. However, these\nlifted methods sometimes perform poorly compared to traditional neural\nnetworks. In this paper, we introduce a new class of lifted models, Fenchel\nlifted networks, that enjoy the same benefits as previous lifted models,\nwithout suffering a degradation in performance over classical networks. Our\nmodel represents activation functions as equivalent biconvex constraints and\nuses Lagrange Multipliers to arrive at a rigorous lower bound of the\ntraditional neural network training problem. This model is efficiently trained\nusing block-coordinate descent and is parallelizable across data points and/or\nlayers. We compare our model against standard fully connected and convolutional\nnetworks and show that we are able to match or beat their performance. \n\n"}
{"id": "1811.08084", "contents": "Title: Multiple-Instance Learning by Boosting Infinitely Many Shapelet-based\n  Classifiers Abstract: We propose a new formulation of Multiple-Instance Learning (MIL). In typical\nMIL settings, a unit of data is given as a set of instances called a bag and\nthe goal is to find a good classifier of bags based on similarity from a single\nor finitely many \"shapelets\" (or patterns), where the similarity of the bag\nfrom a shapelet is the maximum similarity of instances in the bag. Classifiers\nbased on a single shapelet are not sufficiently strong for certain\napplications. Additionally, previous work with multiple shapelets has\nheuristically chosen some of the instances as shapelets with no theoretical\nguarantee of its generalization ability. Our formulation provides a richer\nclass of the final classifiers based on infinitely many shapelets. We provide\nan efficient algorithm for the new formulation, in addition to generalization\nbound. Our empirical study demonstrates that our approach is effective not only\nfor MIL tasks but also for Shapelet Learning for time-series classification. \n\n"}
{"id": "1811.08100", "contents": "Title: Another Diversity-Promoting Objective Function for Neural Dialogue\n  Generation Abstract: Although generation-based dialogue systems have been widely researched, the\nresponse generations by most existing systems have very low diversities. The\nmost likely reason for this problem is Maximum Likelihood Estimation (MLE) with\nSoftmax Cross-Entropy (SCE) loss. MLE trains models to generate the most\nfrequent responses from enormous generation candidates, although in actual\ndialogues there are various responses based on the context. In this paper, we\npropose a new objective function called Inverse Token Frequency (ITF) loss,\nwhich individually scales smaller loss for frequent token classes and larger\nloss for rare token classes. This function encourages the model to generate\nrare tokens rather than frequent tokens. It does not complicate the model and\nits training is stable because we only replace the objective function. On the\nOpenSubtitles dialogue dataset, our loss model establishes a state-of-the-art\nDIST-1 of 7.56, which is the unigram diversity score, while maintaining a good\nBLEU-1 score. On a Japanese Twitter replies dataset, our loss model achieves a\nDIST-1 score comparable to the ground truth. \n\n"}
{"id": "1811.08120", "contents": "Title: Explaining Latent Factor Models for Recommendation with Influence\n  Functions Abstract: Latent factor models (LFMs) such as matrix factorization achieve the\nstate-of-the-art performance among various Collaborative Filtering (CF)\napproaches for recommendation. Despite the high recommendation accuracy of\nLFMs, a critical issue to be resolved is the lack of explainability. Extensive\nefforts have been made in the literature to incorporate explainability into\nLFMs. However, they either rely on auxiliary information which may not be\navailable in practice, or fail to provide easy-to-understand explanations. In\nthis paper, we propose a fast influence analysis method named FIA, which\nsuccessfully enforces explicit neighbor-style explanations to LFMs with the\ntechnique of influence functions stemmed from robust statistics. We first\ndescribe how to employ influence functions to LFMs to deliver neighbor-style\nexplanations. Then we develop a novel influence computation algorithm for\nmatrix factorization with high efficiency. We further extend it to the more\ngeneral neural collaborative filtering and introduce an approximation algorithm\nto accelerate influence analysis over neural network models. Experimental\nresults on real datasets demonstrate the correctness, efficiency and usefulness\nof our proposed method. \n\n"}
{"id": "1811.08417", "contents": "Title: WEST: Word Encoded Sequence Transducers Abstract: Most of the parameters in large vocabulary models are used in embedding layer\nto map categorical features to vectors and in softmax layer for classification\nweights. This is a bottle-neck in memory constraint on-device training\napplications like federated learning and on-device inference applications like\nautomatic speech recognition (ASR). One way of compressing the embedding and\nsoftmax layers is to substitute larger units such as words with smaller\nsub-units such as characters. However, often the sub-unit models perform poorly\ncompared to the larger unit models. We propose WEST, an algorithm for encoding\ncategorical features and output classes with a sequence of random or domain\ndependent sub-units and demonstrate that this transduction can lead to\nsignificant compression without compromising performance. WEST bridges the gap\nbetween larger unit and sub-unit models and can be interpreted as a MaxEnt\nmodel over sub-unit features, which can be of independent interest. \n\n"}
{"id": "1811.08853", "contents": "Title: Resource Mention Extraction for MOOC Discussion Forums Abstract: In discussions hosted on discussion forums for MOOCs, references to online\nlearning resources are often of central importance. They contextualize the\ndiscussion, anchoring the discussion participants' presentation of the issues\nand their understanding. However they are usually mentioned in free text,\nwithout appropriate hyperlinking to their associated resource. Automated\nlearning resource mention hyperlinking and categorization will facilitate\ndiscussion and searching within MOOC forums, and also benefit the\ncontextualization of such resources across disparate views. We propose the\nnovel problem of learning resource mention identification in MOOC forums. As\nthis is a novel task with no publicly available data, we first contribute a\nlarge-scale labeled dataset, dubbed the Forum Resource Mention (FoRM) dataset,\nto facilitate our current research and future research on this task. We then\nformulate this task as a sequence tagging problem and investigate solution\narchitectures to address the problem. Importantly, we identify two major\nchallenges that hinder the application of sequence tagging models to the task:\n(1) the diversity of resource mention expression, and (2) long-range contextual\ndependencies. We address these challenges by incorporating character-level and\nthread context information into a LSTM-CRF model. First, we incorporate a\ncharacter encoder to address the out-of-vocabulary problem caused by the\ndiversity of mention expressions. Second, to address the context dependency\nchallenge, we encode thread contexts using an RNN-based context encoder, and\napply the attention mechanism to selectively leverage useful context\ninformation during sequence tagging. Experiments on FoRM show that the proposed\nmethod improves the baseline deep sequence tagging models notably,\nsignificantly bettering performance on instances that exemplify the two\nchallenges. \n\n"}
{"id": "1811.09347", "contents": "Title: A Simple Non-i.i.d. Sampling Approach for Efficient Training and Better\n  Generalization Abstract: While training on samples drawn from independent and identical distribution\nhas been a de facto paradigm for optimizing image classification networks,\nhumans learn new concepts in an easy-to-hard manner and on the selected\nexamples progressively. Driven by this fact, we investigate the training\nparadigms where the samples are not drawn from independent and identical\ndistribution. We propose a data sampling strategy, named Drop-and-Refresh\n(DaR), motivated by the learning behaviors of humans that selectively drop easy\nsamples and refresh them only periodically. We show in our experiments that the\nproposed DaR strategy can maintain (and in many cases improve) the predictive\naccuracy even when the training cost is reduced by 15% on various datasets\n(CIFAR 10, CIFAR 100 and ImageNet) and with different backbone architectures\n(ResNets, DenseNets and MobileNets). Furthermore and perhaps more importantly,\nwe find the ImageNet pre-trained models using our DaR sampling strategy\nachieves better transferability for the downstream tasks including object\ndetection (+0.3 AP), instance segmentation (+0.3 AP), scene parsing (+0.5 mIoU)\nand human pose estimation (+0.6 AP). Our investigation encourages people to\nrethink the connections between the sampling strategy for training and the\ntransferability of its learned features for pre-training ImageNet models. \n\n"}
{"id": "1811.09725", "contents": "Title: Interpretable Convolutional Filters with SincNet Abstract: Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal \"black-box\" representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs. \n\n"}
{"id": "1811.11214", "contents": "Title: Understanding the impact of entropy on policy optimization Abstract: Entropy regularization is commonly used to improve policy optimization in\nreinforcement learning. It is believed to help with \\emph{exploration} by\nencouraging the selection of more stochastic policies. In this work, we analyze\nthis claim using new visualizations of the optimization landscape based on\nrandomly perturbing the loss function. We first show that even with access to\nthe exact gradient, policy optimization is difficult due to the geometry of the\nobjective function. Then, we qualitatively show that in some environments, a\npolicy with higher entropy can make the optimization landscape smoother,\nthereby connecting local optima and enabling the use of larger learning rates.\nThis paper presents new tools for understanding the optimization landscape,\nshows that policy entropy serves as a regularizer, and highlights the challenge\nof designing general-purpose policy optimization algorithms. \n\n"}
{"id": "1811.11880", "contents": "Title: Predicting the Computational Cost of Deep Learning Models Abstract: Deep learning is rapidly becoming a go-to tool for many artificial\nintelligence problems due to its ability to outperform other approaches and\neven humans at many problems. Despite its popularity we are still unable to\naccurately predict the time it will take to train a deep learning network to\nsolve a given problem. This training time can be seen as the product of the\ntraining time per epoch and the number of epochs which need to be performed to\nreach the desired level of accuracy. Some work has been carried out to predict\nthe training time for an epoch -- most have been based around the assumption\nthat the training time is linearly related to the number of floating point\noperations required. However, this relationship is not true and becomes\nexacerbated in cases where other activities start to dominate the execution\ntime. Such as the time to load data from memory or loss of performance due to\nnon-optimal parallel execution. In this work we propose an alternative approach\nin which we train a deep learning network to predict the execution time for\nparts of a deep learning network. Timings for these individual parts can then\nbe combined to provide a prediction for the whole execution time. This has\nadvantages over linear approaches as it can model more complex scenarios. But,\nalso, it has the ability to predict execution times for scenarios unseen in the\ntraining data. Therefore, our approach can be used not only to infer the\nexecution time for a batch, or entire epoch, but it can also support making a\nwell-informed choice for the appropriate hardware and model. \n\n"}
{"id": "1811.12354", "contents": "Title: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments Abstract: We study the problem of jointly reasoning about language and vision through a\nnavigation and spatial reasoning task. We introduce the Touchdown task and\ndataset, where an agent must first follow navigation instructions in a\nreal-life visual urban environment, and then identify a location described in\nnatural language to find a hidden object at the goal position. The data\ncontains 9,326 examples of English instructions and spatial descriptions paired\nwith demonstrations. Empirical analysis shows the data presents an open\nchallenge to existing methods, and qualitative linguistic analysis shows that\nthe data displays richer use of spatial reasoning compared to related\nresources. \n\n"}
{"id": "1811.12557", "contents": "Title: Deep Multi-Agent Reinforcement Learning with Relevance Graphs Abstract: Over recent years, deep reinforcement learning has shown strong successes in\ncomplex single-agent tasks, and more recently this approach has also been\napplied to multi-agent domains. In this paper, we propose a novel approach,\ncalled MAGnet, to multi-agent reinforcement learning (MARL) that utilizes a\nrelevance graph representation of the environment obtained by a self-attention\nmechanism, and a message-generation technique inspired by the NerveNet\narchitecture. We applied our MAGnet approach to the Pommerman game and the\nresults show that it significantly outperforms state-of-the-art MARL solutions,\nincluding DQN, MADDPG, and MCTS. \n\n"}
{"id": "1811.12728", "contents": "Title: Document Structure Measure for Hypernym discovery Abstract: Hypernym discovery is the problem of finding terms that have is-a\nrelationship with a given term. We introduce a new context type, and a\nrelatedness measure to differentiate hypernyms from other types of semantic\nrelationships. Our Document Structure measure is based on hierarchical position\nof terms in a document, and their presence or otherwise in definition text.\nThis measure quantifies the document structure using multiple attributes, and\nclasses of weighted distance functions. \n\n"}
{"id": "1812.00139", "contents": "Title: Number of Connected Components in a Graph: Estimation via Counting\n  Patterns Abstract: Due to the limited resources and the scale of the graphs in modern datasets,\nwe often get to observe a sampled subgraph of a larger original graph of\ninterest, whether it is the worldwide web that has been crawled or social\nconnections that have been surveyed. Inferring a global property of the\noriginal graph from such a sampled subgraph is of a fundamental interest. In\nthis work, we focus on estimating the number of connected components. It is a\nchallenging problem and, for general graphs, little is known about the\nconnection between the observed subgraph and the number of connected components\nof the original graph. In order to make this connection, we propose a highly\nredundant and large-dimensional representation of the subgraph, which at first\nglance seems counter-intuitive. A subgraph is represented by the counts of\npatterns, known as network motifs. This representation is crucial in\nintroducing a novel estimator for the number of connected components for\ngeneral graphs, under the knowledge of the spectral gap of the original graph.\nThe connection is made precise via the Schatten $k$-norms of the graph\nLaplacian and the spectral representation of the number of connected\ncomponents. We provide a guarantee on the resulting mean squared error that\ncharacterizes the bias variance tradeoff. Experiments on synthetic and\nreal-world graphs suggest that we improve upon competing algorithms for graphs\nwith spectral gaps bounded away from zero. \n\n"}
{"id": "1812.00141", "contents": "Title: A Dynamic Network and Representation LearningApproach for Quantifying\n  Economic Growth fromSatellite Imagery Abstract: Quantifying the improvement in human living standard, as well as the city\ngrowth in developing countries, is a challenging problem due to the lack of\nreliable economic data. Therefore, there is a fundamental need for alternate,\nlargely unsupervised, computational methods that can estimate the economic\nconditions in the developing regions. To this end, we propose a new network\nscience- and representation learning-based approach that can quantify economic\nindicators and visualize the growth of various regions. More precisely, we\nfirst create a dynamic network drawn out of high-resolution nightlight\nsatellite images. We then demonstrate that using representation learning to\nmine the resulting network, our proposed approach can accurately predict\nspatial gross economic expenditures over large regions. Our method, which\nrequires only nightlight images and limited survey data, can capture\ncity-growth, as well as how people's living standard is changing; this can\nultimately facilitate the decision makers' understanding of growth without\nheavily relying on expensive and time-consuming surveys. \n\n"}
{"id": "1812.00225", "contents": "Title: Discovering hierarchies using Imitation Learning from hierarchy aware\n  policies Abstract: Learning options that allow agents to exhibit temporally higher order\nbehavior has proven to be useful in increasing exploration, reducing sample\ncomplexity and for various transfer scenarios. Deep Discovery of Options (DDO)\nis a generative algorithm that learns a hierarchical policy along with options\ndirectly from expert trajectories. We perform a qualitative and quantitative\nanalysis of options inferred from DDO in different domains. To this end, we\nsuggest different value metrics like option termination condition, hinge value\nfunction error and KL-Divergence based distance metric to compare different\nmethods. Analyzing the termination condition of the options and number of time\nsteps the options were run revealed that the options were terminating\nprematurely. We suggest modifications which can be incorporated easily and\nalleviates the problem of shorter options and a collapse of options to the same\nmode. \n\n"}
{"id": "1812.00335", "contents": "Title: GAN-EM: GAN based EM learning framework Abstract: Expectation maximization (EM) algorithm is to find maximum likelihood\nsolution for models having latent variables. A typical example is Gaussian\nMixture Model (GMM) which requires Gaussian assumption, however, natural images\nare highly non-Gaussian so that GMM cannot be applied to perform clustering\ntask on pixel space. To overcome such limitation, we propose a GAN based EM\nlearning framework that can maximize the likelihood of images and estimate the\nlatent variables with only the constraint of L-Lipschitz continuity. We call\nthis model GAN-EM, which is a framework for image clustering, semi-supervised\nclassification and dimensionality reduction. In M-step, we design a novel loss\nfunction for discriminator of GAN to perform maximum likelihood estimation\n(MLE) on data with soft class label assignments. Specifically, a conditional\ngenerator captures data distribution for $K$ classes, and a discriminator tells\nwhether a sample is real or fake for each class. Since our model is\nunsupervised, the class label of real data is regarded as latent variable,\nwhich is estimated by an additional network (E-net) in E-step. The proposed\nGAN-EM achieves state-of-the-art clustering and semi-supervised classification\nresults on MNIST, SVHN and CelebA, as well as comparable quality of generated\nimages to other recently developed generative models. \n\n"}
{"id": "1812.00660", "contents": "Title: Knowledge Distillation with Feature Maps for Image Classification Abstract: The model reduction problem that eases the computation costs and latency of\ncomplex deep learning architectures has received an increasing number of\ninvestigations owing to its importance in model deployment. One promising\nmethod is knowledge distillation (KD), which creates a fast-to-execute student\nmodel to mimic a large teacher network. In this paper, we propose a method,\ncalled KDFM (Knowledge Distillation with Feature Maps), which improves the\neffectiveness of KD by learning the feature maps from the teacher network. Two\nmajor techniques used in KDFM are shared classifier and generative adversarial\nnetwork. Experimental results show that KDFM can use a four layers CNN to mimic\nDenseNet-40 and use MobileNet to mimic DenseNet-100. Both student networks have\nless than 1\\% accuracy loss comparing to their teacher models for CIFAR-100\ndatasets. The student networks are 2-6 times faster than their teacher models\nfor inference, and the model size of MobileNet is less than half of\nDenseNet-100's. \n\n"}
{"id": "1812.00975", "contents": "Title: Structure Learning Using Forced Pruning Abstract: Markov networks are widely used in many Machine Learning applications\nincluding natural language processing, computer vision, and bioinformatics .\nLearning Markov networks have many complications ranging from intractable\ncomputations involved to the possibility of learning a model with a huge number\nof parameters. In this report, we provide a computationally tractable greedy\nheuristic for learning Markov networks structure. The proposed heuristic\nresults in a model with a limited predefined number of parameters. We ran our\nmethod on 3 fully-observed real datasets, and we observed that our method is\ndoing comparably good to the state of the art methods. \n\n"}
{"id": "1812.01097", "contents": "Title: LEAF: A Benchmark for Federated Settings Abstract: Modern federated networks, such as those comprised of wearable devices,\nmobile phones, or autonomous vehicles, generate massive amounts of data each\nday. This wealth of data can help to learn models that can improve the user\nexperience on each device. However, the scale and heterogeneity of federated\ndata presents new challenges in research areas such as federated learning,\nmeta-learning, and multi-task learning. As the machine learning community\nbegins to tackle these challenges, we are at a critical time to ensure that\ndevelopments made in these areas are grounded with realistic benchmarks. To\nthis end, we propose LEAF, a modular benchmarking framework for learning in\nfederated settings. LEAF includes a suite of open-source federated datasets, a\nrigorous evaluation framework, and a set of reference implementations, all\ngeared towards capturing the obstacles and intricacies of practical federated\nenvironments. \n\n"}
{"id": "1812.01115", "contents": "Title: On learning with shift-invariant structures Abstract: We describe new results and algorithms for two different, but related,\nproblems which deal with circulant matrices: learning shift-invariant\ncomponents from training data and calculating the shift (or alignment) between\ntwo given signals. In the first instance, we deal with the shift-invariant\ndictionary learning problem while the latter bears the name of (compressive)\nshift retrieval. We formulate these problems using circulant and convolutional\nmatrices (including unions of such matrices), define optimization problems that\ndescribe our goals and propose efficient ways to solve them. Based on these\nfindings, we also show how to learn a wavelet-like dictionary from training\ndata. We connect our work with various previous results from the literature and\nwe show the effectiveness of our proposed algorithms using synthetic, ECG\nsignals and images. \n\n"}
{"id": "1812.01194", "contents": "Title: A Retrieve-and-Edit Framework for Predicting Structured Outputs Abstract: For the task of generating complex outputs such as source code, editing\nexisting outputs can be easier than generating complex outputs from scratch.\nWith this motivation, we propose an approach that first retrieves a training\nexample based on the input (e.g., natural language description) and then edits\nit to the desired output (e.g., code). Our contribution is a computationally\nefficient method for learning a retrieval model that embeds the input in a\ntask-dependent way without relying on a hand-crafted metric or incurring the\nexpense of jointly training the retriever with the editor. Our\nretrieve-and-edit framework can be applied on top of any base model. We show\nthat on a new autocomplete task for GitHub Python code and the Hearthstone\ncards benchmark, retrieve-and-edit significantly boosts the performance of a\nvanilla sequence-to-sequence model on both tasks. \n\n"}
{"id": "1812.01662", "contents": "Title: Feed-Forward Neural Networks Need Inductive Bias to Learn Equality\n  Relations Abstract: Basic binary relations such as equality and inequality are fundamental to\nrelational data structures. Neural networks should learn such relations and\ngeneralise to new unseen data. We show in this study, however, that this\ngeneralisation fails with standard feed-forward networks on binary vectors.\nEven when trained with maximal training data, standard networks do not reliably\ndetect equality.We introduce differential rectifier (DR) units that we add to\nthe network in different configurations. The DR units create an inductive bias\nin the networks, so that they do learn to generalise, even from small numbers\nof examples and we have not found any negative effect of their inclusion in the\nnetwork. Given the fundamental nature of these relations, we hypothesize that\nfeed-forward neural network learning benefits from inductive bias in other\nrelations as well. Consequently, the further development of suitable inductive\nbiases will be beneficial to many tasks in relational learning with neural\nnetworks. \n\n"}
{"id": "1812.01696", "contents": "Title: Learning Individualized Cardiovascular Responses from Large-scale\n  Wearable Sensors Data Abstract: We consider the problem of modeling cardiovascular responses to physical\nactivity and sleep changes captured by wearable sensors in free living\nconditions. We use an attentional convolutional neural network to learn\nparsimonious signatures of individual cardiovascular response from data\nrecorded at the minute level resolution over several months on a cohort of 80k\npeople. We demonstrate internal validity by showing that signatures generated\non an individual's 2017 data generalize to predict minute-level heart rate from\nphysical activity and sleep for the same individual in 2018, outperforming\nseveral time-series forecasting baselines. We also show external validity\ndemonstrating that signatures outperform plain resting heart rate (RHR) in\npredicting variables associated with cardiovascular functions, such as age and\nBody Mass Index (BMI). We believe that the computed cardiovascular signatures\nhave utility in monitoring cardiovascular health over time, including detecting\nabnormalities and quantifying recovery from acute events. \n\n"}
{"id": "1812.01712", "contents": "Title: Multiview Based 3D Scene Understanding On Partial Point Sets Abstract: Deep learning within the context of point clouds has gained much research\ninterest in recent years mostly due to the promising results that have been\nachieved on a number of challenging benchmarks, such as 3D shape recognition\nand scene semantic segmentation. In many realistic settings however, snapshots\nof the environment are often taken from a single view, which only contains a\npartial set of the scene due to the field of view restriction of commodity\ncameras. 3D scene semantic understanding on partial point clouds is considered\nas a challenging task. In this work, we propose a processing approach for 3D\npoint cloud data based on a multiview representation of the existing 360{\\deg}\npoint clouds. By fusing the original 360{\\deg} point clouds and their\ncorresponding 3D multiview representations as input data, a neural network is\nable to recognize partial point sets while improving the general performance on\ncomplete point sets, resulting in an overall increase of 31.9% and 4.3% in\nsegmentation accuracy for partial and complete scene semantic understanding,\nrespectively. This method can also be applied in a wider 3D recognition context\nsuch as 3D part segmentation. \n\n"}
{"id": "1812.01803", "contents": "Title: ECC: Platform-Independent Energy-Constrained Deep Neural Network\n  Compression via a Bilinear Regression Model Abstract: Many DNN-enabled vision applications constantly operate under severe energy\nconstraints such as unmanned aerial vehicles, Augmented Reality headsets, and\nsmartphones. Designing DNNs that can meet a stringent energy budget is becoming\nincreasingly important. This paper proposes ECC, a framework that compresses\nDNNs to meet a given energy constraint while minimizing accuracy loss. The key\nidea of ECC is to model the DNN energy consumption via a novel bilinear\nregression function. The energy estimate model allows us to formulate DNN\ncompression as a constrained optimization that minimizes the DNN loss function\nover the energy constraint. The optimization problem, however, has nontrivial\nconstraints. Therefore, existing deep learning solvers do not apply directly.\nWe propose an optimization algorithm that combines the essence of the\nAlternating Direction Method of Multipliers (ADMM) framework with\ngradient-based learning algorithms. The algorithm decomposes the original\nconstrained optimization into several subproblems that are solved iteratively\nand efficiently. ECC is also portable across different hardware platforms\nwithout requiring hardware knowledge. Experiments show that ECC achieves higher\naccuracy under the same or lower energy budget compared to state-of-the-art\nresource-constrained DNN compression techniques. \n\n"}
{"id": "1812.01804", "contents": "Title: Random Spiking and Systematic Evaluation of Defenses Against Adversarial\n  Examples Abstract: Image classifiers often suffer from adversarial examples, which are generated\nby strategically adding a small amount of noise to input images to trick\nclassifiers into misclassification. Over the years, many defense mechanisms\nhave been proposed, and different researchers have made seemingly contradictory\nclaims on their effectiveness. We present an analysis of possible adversarial\nmodels, and propose an evaluation framework for comparing different defense\nmechanisms. As part of the framework, we introduce a more powerful and\nrealistic adversary strategy. Furthermore, we propose a new defense mechanism\ncalled Random Spiking (RS), which generalizes dropout and introduces random\nnoises in the training process in a controlled manner. Evaluations under our\nproposed framework suggest RS delivers better protection against adversarial\nexamples than many existing schemes. \n\n"}
{"id": "1812.01995", "contents": "Title: Deep Learning Model for Finding New Superconductors Abstract: Exploration of new superconductors still relies on the experience and\nintuition of experts and is largely a process of experimental trial and error.\nIn one study, only 3% of the candidate materials showed superconductivity.\nHere, we report the first deep learning model for finding new superconductors.\nWe introduced the method named \"reading periodic table\" which represented the\nperiodic table in a way that allows deep learning to learn to read the periodic\ntable and to learn the law of elements for the purpose of discovering novel\nsuperconductors that are outside the training data. It is recognized that it is\ndifficult for deep learning to predict something outside the training data.\nAlthough we used only the chemical composition of materials as information, we\nobtained an $R^{2}$ value of 0.92 for predicting $T_\\text{c}$ for materials in\na database of superconductors. We also introduced the method named \"garbage-in\"\nto create synthetic data of non-superconductors that do not exist.\nNon-superconductors are not reported, but the data must be required for deep\nlearning to distinguish between superconductors and non-superconductors. We\nobtained three remarkable results. The deep learning can predict\nsuperconductivity for a material with a precision of 62%, which shows the\nusefulness of the model; it found the recently discovered superconductor CaBi2\nand another one Hf0.5Nb0.2V2Zr0.3, neither of which is in the superconductor\ndatabase; and it found Fe-based high-temperature superconductors (discovered in\n2008) from the training data before 2008. These results open the way for the\ndiscovery of new high-temperature superconductor families. The candidate\nmaterials list, data, and method are openly available from the link\nhttps://github.com/tomo835g/Deep-Learning-to-find-Superconductors. \n\n"}
{"id": "1812.02207", "contents": "Title: Better Trees: An empirical study on hyperparameter tuning of\n  classification decision tree induction algorithms Abstract: Machine learning algorithms often contain many hyperparameters (HPs) whose\nvalues affect the predictive performance of the induced models in intricate\nways. Due to the high number of possibilities for these HP configurations and\ntheir complex interactions, it is common to use optimization techniques to find\nsettings that lead to high predictive performance. However, insights into\nefficiently exploring this vast space of configurations and dealing with the\ntrade-off between predictive and runtime performance remain challenging.\nFurthermore, there are cases where the default HPs fit the suitable\nconfiguration. Additionally, for many reasons, including model validation and\nattendance to new legislation, there is an increasing interest in interpretable\nmodels, such as those created by the Decision Tree (DT) induction algorithms.\nThis paper provides a comprehensive approach for investigating the effects of\nhyperparameter tuning for the two DT induction algorithms most often used, CART\nand C4.5. DT induction algorithms present high predictive performance and\ninterpretable classification models, though many HPs need to be adjusted.\nExperiments were carried out with different tuning strategies to induce models\nand to evaluate HPs' relevance using 94 classification datasets from OpenML.\nThe experimental results point out that different HP profiles for the tuning of\neach algorithm provide statistically significant improvements in most of the\ndatasets for CART, but only in one-third for C4.5. Although different\nalgorithms may present different tuning scenarios, the tuning techniques\ngenerally required few evaluations to find accurate solutions. Furthermore, the\nbest technique for all the algorithms was the IRACE. Finally, we found out that\ntuning a specific small subset of HPs is a good alternative for achieving\noptimal predictive performance. \n\n"}
{"id": "1812.02706", "contents": "Title: Coarse-Graining Auto-Encoders for Molecular Dynamics Abstract: Molecular dynamics simulations provide theoretical insight into the\nmicroscopic behavior of materials in condensed phase and, as a predictive tool,\nenable computational design of new compounds. However, because of the large\ntemporal and spatial scales involved in thermodynamic and kinetic phenomena in\nmaterials, atomistic simulations are often computationally unfeasible.\nCoarse-graining methods allow simulating larger systems, by reducing the\ndimensionality of the simulation, and propagating longer timesteps, by\naveraging out fast motions. Coarse-graining involves two coupled learning\nproblems; defining the mapping from an all-atom to a reduced representation,\nand the parametrization of a Hamiltonian over coarse-grained coordinates.\nMultiple statistical mechanics approaches have addressed the latter, but the\nformer is generally a hand-tuned process based on chemical intuition. Here we\npresent Autograin, an optimization framework based on auto-encoders to learn\nboth tasks simultaneously. Autograin is trained to learn the optimal mapping\nbetween all-atom and reduced representation, using the reconstruction loss to\nfacilitate the learning of coarse-grained variables. In addition, a\nforce-matching method is applied to variationally determine the coarse-grained\npotential energy function. This procedure is tested on a number of model\nsystems including single-molecule and bulk-phase periodic simulations. \n\n"}
{"id": "1812.03285", "contents": "Title: Sampling-based Bayesian Inference with gradient uncertainty Abstract: Deep neural networks(NNs) have achieved impressive performance, often exceed\nhuman performance on many computer vision tasks. However, one of the most\nchallenging issues that still remains is that NNs are overconfident in their\npredictions, which can be very harmful when this arises in safety critical\napplications. In this paper, we show that predictive uncertainty can be\nefficiently estimated when we incorporate the concept of gradients uncertainty\ninto posterior sampling. The proposed method is tested on two different\ndatasets, MNIST for in-distribution confusing examples and notMNIST for\nout-of-distribution data. We show that our method is able to efficiently\nrepresent predictive uncertainty on both datasets. \n\n"}
{"id": "1812.03337", "contents": "Title: Secure Federated Transfer Learning Abstract: Machine learning relies on the availability of a vast amount of data for\ntraining. However, in reality, most data are scattered across different\norganizations and cannot be easily integrated under many legal and practical\nconstraints. In this paper, we introduce a new technique and framework, known\nas federated transfer learning (FTL), to improve statistical models under a\ndata federation. The federation allows knowledge to be shared without\ncompromising user privacy, and enables complimentary knowledge to be\ntransferred in the network. As a result, a target-domain party can build more\nflexible and powerful models by leveraging rich labels from a source-domain\nparty. A secure transfer cross validation approach is also proposed to guard\nthe FTL performance under the federation. The framework requires minimal\nmodifications to the existing model structure and provides the same level of\naccuracy as the non-privacy-preserving approach. This framework is very\nflexible and can be effectively adapted to various secure multi-party machine\nlearning tasks. \n\n"}
{"id": "1812.03483", "contents": "Title: To Reverse the Gradient or Not: An Empirical Comparison of Adversarial\n  and Multi-task Learning in Speech Recognition Abstract: Transcribed datasets typically contain speaker identity for each instance in\nthe data. We investigate two ways to incorporate this information during\ntraining: Multi-Task Learning and Adversarial Learning. In multi-task learning,\nthe goal is speaker prediction; we expect a performance improvement with this\njoint training if the two tasks of speech recognition and speaker recognition\nshare a common set of underlying features. In contrast, adversarial learning is\na means to learn representations invariant to the speaker. We then expect\nbetter performance if this learnt invariance helps generalizing to new\nspeakers. While the two approaches seem natural in the context of speech\nrecognition, they are incompatible because they correspond to opposite\ngradients back-propagated to the model. In order to better understand the\neffect of these approaches in terms of error rates, we compare both strategies\nin controlled settings. Moreover, we explore the use of additional\nuntranscribed data in a semi-supervised, adversarial learning manner to improve\nerror rates. Our results show that deep models trained on big datasets already\ndevelop invariant representations to speakers without any auxiliary loss. When\nconsidering adversarial learning and multi-task learning, the impact on the\nacoustic model seems minor. However, models trained in a semi-supervised manner\ncan improve error-rates. \n\n"}
{"id": "1812.04064", "contents": "Title: Attentional Heterogeneous Graph Neural Network: Application to Program\n  Reidentification Abstract: Program or process is an integral part of almost every IT/OT system. Can we\ntrust the identity/ID (e.g., executable name) of the program? To avoid\ndetection, malware may disguise itself using the ID of a legitimate program,\nand a system tool (e.g., PowerShell) used by the attackers may have the fake ID\nof another common software, which is less sensitive. However, existing\nintrusion detection techniques often overlook this critical program\nreidentification problem (i.e., checking the program's identity). In this\npaper, we propose an attentional heterogeneous graph neural network model\n(DeepHGNN) to verify the program's identity based on its system behaviors. The\nkey idea is to leverage the representation learning of the heterogeneous\nprogram behavior graph to guide the reidentification process. We formulate the\nprogram reidentification as a graph classification problem and develop an\neffective attentional heterogeneous graph embedding algorithm to solve it.\nExtensive experiments --- using real-world enterprise monitoring data and real\nattacks --- demonstrate the effectiveness of DeepHGNN across multiple popular\nmetrics and the robustness to the normal dynamic changes like program version\nupgrades. \n\n"}
{"id": "1812.04352", "contents": "Title: Layer-Parallel Training of Deep Residual Neural Networks Abstract: Residual neural networks (ResNets) are a promising class of deep neural\nnetworks that have shown excellent performance for a number of learning tasks,\ne.g., image classification and recognition. Mathematically, ResNet\narchitectures can be interpreted as forward Euler discretizations of a\nnonlinear initial value problem whose time-dependent control variables\nrepresent the weights of the neural network. Hence, training a ResNet can be\ncast as an optimal control problem of the associated dynamical system. For\nsimilar time-dependent optimal control problems arising in engineering\napplications, parallel-in-time methods have shown notable improvements in\nscalability. This paper demonstrates the use of those techniques for efficient\nand effective training of ResNets. The proposed algorithms replace the\nclassical (sequential) forward and backward propagation through the network\nlayers by a parallel nonlinear multigrid iteration applied to the layer domain.\nThis adds a new dimension of parallelism across layers that is attractive when\ntraining very deep networks. From this basic idea, we derive multiple\nlayer-parallel methods. The most efficient version employs a simultaneous\noptimization approach where updates to the network parameters are based on\ninexact gradient information in order to speed up the training process. Using\nnumerical examples from supervised classification, we demonstrate that the new\napproach achieves similar training performance to traditional methods, but\nenables layer-parallelism and thus provides speedup over layer-serial methods\nthrough greater concurrency. \n\n"}
{"id": "1812.04359", "contents": "Title: Efficient Model-Free Reinforcement Learning Using Gaussian Process Abstract: Efficient Reinforcement Learning usually takes advantage of demonstration or\ngood exploration strategy. By applying posterior sampling in model-free RL\nunder the hypothesis of GP, we propose Gaussian Process Posterior Sampling\nReinforcement Learning(GPPSTD) algorithm in continuous state space, giving\ntheoretical justifications and empirical results. We also provide theoretical\nand empirical results that various demonstration could lower expected\nuncertainty and benefit posterior sampling exploration. In this way, we\ncombined the demonstration and exploration process together to achieve a more\nefficient reinforcement learning. \n\n"}
{"id": "1812.04599", "contents": "Title: Adversarial Framing for Image and Video Classification Abstract: Neural networks are prone to adversarial attacks. In general, such attacks\ndeteriorate the quality of the input by either slightly modifying most of its\npixels, or by occluding it with a patch. In this paper, we propose a method\nthat keeps the image unchanged and only adds an adversarial framing on the\nborder of the image. We show empirically that our method is able to\nsuccessfully attack state-of-the-art methods on both image and video\nclassification problems. Notably, the proposed method results in a universal\nattack which is very fast at test time. Source code can be found at\nhttps://github.com/zajaczajac/adv_framing . \n\n"}
{"id": "1812.05676", "contents": "Title: A Probe Towards Understanding GAN and VAE Models Abstract: This project report compares some known GAN and VAE models proposed prior to\n2017. There has been significant progress after we finished this report. We\nupload this report as an introduction to generative models and provide some\npersonal interpretations supported by empirical evidence. Both generative\nadversarial network models and variational autoencoders have been widely used\nto approximate probability distributions of data sets. Although they both use\nparametrized distributions to approximate the underlying data distribution,\nwhose exact inference is intractable, their behaviors are very different. We\nsummarize our experiment results that compare these two categories of models in\nterms of fidelity and mode collapse. We provide a hypothesis to explain their\ndifferent behaviors and propose a new model based on this hypothesis. We\nfurther tested our proposed model on MNIST dataset and CelebA dataset. \n\n"}
{"id": "1812.06070", "contents": "Title: The Boosted DC Algorithm for nonsmooth functions Abstract: The Boosted Difference of Convex functions Algorithm (BDCA) was recently\nproposed for minimizing smooth difference of convex (DC) functions. BDCA\naccelerates the convergence of the classical Difference of Convex functions\nAlgorithm (DCA) thanks to an additional line search step. The purpose of this\npaper is twofold. Firstly, to show that this scheme can be generalized and\nsuccessfully applied to certain types of nonsmooth DC functions, namely, those\nthat can be expressed as the difference of a smooth function and a possibly\nnonsmooth one. Secondly, to show that there is complete freedom in the choice\nof the trial step size for the line search, which is something that can further\nimprove its performance. We prove that any limit point of the BDCA iterative\nsequence is a critical point of the problem under consideration, and that the\ncorresponding objective value is monotonically decreasing and convergent. The\nglobal convergence and convergent rate of the iterations are obtained under the\nKurdyka-Lojasiewicz property. Applications and numerical experiments for two\nproblems in data science are presented, demonstrating that BDCA outperforms\nDCA. Specifically, for the Minimum Sum-of-Squares Clustering problem, BDCA was\non average sixteen times faster than DCA, and for the Multidimensional Scaling\nproblem, BDCA was three times faster than DCA. \n\n"}
{"id": "1812.06127", "contents": "Title: Federated Optimization in Heterogeneous Networks Abstract: Federated Learning is a distributed learning paradigm with two key challenges\nthat differentiate it from traditional distributed optimization: (1)\nsignificant variability in terms of the systems characteristics on each device\nin the network (systems heterogeneity), and (2) non-identically distributed\ndata across the network (statistical heterogeneity). In this work, we introduce\na framework, FedProx, to tackle heterogeneity in federated networks. FedProx\ncan be viewed as a generalization and re-parametrization of FedAvg, the current\nstate-of-the-art method for federated learning. While this re-parameterization\nmakes only minor modifications to the method itself, these modifications have\nimportant ramifications both in theory and in practice. Theoretically, we\nprovide convergence guarantees for our framework when learning over data from\nnon-identical distributions (statistical heterogeneity), and while adhering to\ndevice-level systems constraints by allowing each participating device to\nperform a variable amount of work (systems heterogeneity). Practically, we\ndemonstrate that FedProx allows for more robust convergence than FedAvg across\na suite of realistic federated datasets. In particular, in highly heterogeneous\nsettings, FedProx demonstrates significantly more stable and accurate\nconvergence behavior relative to FedAvg---improving absolute test accuracy by\n22% on average. \n\n"}
{"id": "1812.06181", "contents": "Title: Efficient Interpretation of Deep Learning Models Using Graph Structure\n  and Cooperative Game Theory: Application to ASD Biomarker Discovery Abstract: Discovering imaging biomarkers for autism spectrum disorder (ASD) is critical\nto help explain ASD and predict or monitor treatment outcomes. Toward this end,\ndeep learning classifiers have recently been used for identifying ASD from\nfunctional magnetic resonance imaging (fMRI) with higher accuracy than\ntraditional learning strategies. However, a key challenge with deep learning\nmodels is understanding just what image features the network is using, which\ncan in turn be used to define the biomarkers. Current methods extract\nbiomarkers, i.e., important features, by looking at how the prediction changes\nif \"ignoring\" one feature at a time. In this work, we go beyond looking at only\nindividual features by using Shapley value explanation (SVE) from cooperative\ngame theory. Cooperative game theory is advantageous here because it directly\nconsiders the interaction between features and can be applied to any machine\nlearning method, making it a novel, more accurate way of determining\ninstance-wise biomarker importance from deep learning models. A barrier to\nusing SVE is its computational complexity: $2^N$ given $N$ features. We\nexplicitly reduce the complexity of SVE computation by two approaches based on\nthe underlying graph structure of the input data: 1) only consider the\ncentralized coalition of each feature; 2) a hierarchical pipeline which first\nclusters features into small communities, then applies SVE in each community.\nMonte Carlo approximation can be used for large permutation sets. We first\nvalidate our methods on the MNIST dataset and compare to human perception.\nNext, to insure plausibility of our biomarker results, we train a Random Forest\n(RF) to classify ASD/control subjects from fMRI and compare SVE results to\nstandard RF-based feature importance. Finally, we show initial results on\nranked fMRI biomarkers using SVE on a deep learning classifier for the\nASD/control dataset. \n\n"}
{"id": "1812.06401", "contents": "Title: What's to know? Uncertainty as a Guide to Asking Goal-oriented Questions Abstract: One of the core challenges in Visual Dialogue problems is asking the question\nthat will provide the most useful information towards achieving the required\nobjective. Encouraging an agent to ask the right questions is difficult because\nwe don't know a-priori what information the agent will need to achieve its\ntask, and we don't have an explicit model of what it knows already. We propose\na solution to this problem based on a Bayesian model of the uncertainty in the\nimplicit model maintained by the visual dialogue agent, and in the function\nused to select an appropriate output. By selecting the question that minimises\nthe predicted regret with respect to this implicit model the agent actively\nreduces ambiguity. The Bayesian model of uncertainty also enables a principled\nmethod for identifying when enough information has been acquired, and an action\nshould be selected. We evaluate our approach on two goal-oriented dialogue\ndatasets, one for visual-based collaboration task and the other for a\nnegotiation-based task. Our uncertainty-aware information-seeking model\noutperforms its counterparts in these two challenging problems. \n\n"}
{"id": "1812.07211", "contents": "Title: Interpretable Optimal Stopping Abstract: Optimal stopping is the problem of deciding when to stop a stochastic system\nto obtain the greatest reward, arising in numerous application areas such as\nfinance, healthcare and marketing. State-of-the-art methods for\nhigh-dimensional optimal stopping involve approximating the value function or\nthe continuation value, and then using that approximation within a greedy\npolicy. Although such policies can perform very well, they are generally not\nguaranteed to be interpretable; that is, a decision maker may not be able to\neasily see the link between the current system state and the policy's action.\nIn this paper, we propose a new approach to optimal stopping, wherein the\npolicy is represented as a binary tree, in the spirit of naturally\ninterpretable tree models commonly used in machine learning. We show that the\nclass of tree policies is rich enough to approximate the optimal policy. We\nformulate the problem of learning such policies from observed trajectories of\nthe stochastic system as a sample average approximation (SAA) problem. We prove\nthat the SAA problem converges under mild conditions as the sample size\nincreases, but that computationally even immediate simplifications of the SAA\nproblem are theoretically intractable. We thus propose a tractable heuristic\nfor approximately solving the SAA problem, by greedily constructing the tree\nfrom the top down. We demonstrate the value of our approach by applying it to\nthe canonical problem of option pricing, using both synthetic instances and\ninstances using real S&P-500 data. Our method obtains policies that (1)\noutperform state-of-the-art non-interpretable methods, based on\nsimulation-regression and martingale duality, and (2) possess a remarkably\nsimple and intuitive structure. \n\n"}
{"id": "1812.07956", "contents": "Title: On Lazy Training in Differentiable Programming Abstract: In a series of recent theoretical works, it was shown that strongly\nover-parameterized neural networks trained with gradient-based methods could\nconverge exponentially fast to zero training loss, with their parameters hardly\nvarying. In this work, we show that this \"lazy training\" phenomenon is not\nspecific to over-parameterized neural networks, and is due to a choice of\nscaling, often implicit, that makes the model behave as its linearization\naround the initialization, thus yielding a model equivalent to learning with\npositive-definite kernels. Through a theoretical analysis, we exhibit various\nsituations where this phenomenon arises in non-convex optimization and we\nprovide bounds on the distance between the lazy and linearized optimization\npaths. Our numerical experiments bring a critical note, as we observe that the\nperformance of commonly used non-linear deep convolutional neural networks in\ncomputer vision degrades when trained in the lazy regime. This makes it\nunlikely that \"lazy training\" is behind the many successes of neural networks\nin difficult high dimensional tasks. \n\n"}
{"id": "1812.07996", "contents": "Title: Mining Interpretable AOG Representations from Convolutional Networks via\n  Active Question Answering Abstract: In this paper, we present a method to mine object-part patterns from\nconv-layers of a pre-trained convolutional neural network (CNN). The mined\nobject-part patterns are organized by an And-Or graph (AOG). This interpretable\nAOG representation consists of a four-layer semantic hierarchy, i.e., semantic\nparts, part templates, latent patterns, and neural units. The AOG associates\neach object part with certain neural units in feature maps of conv-layers. The\nAOG is constructed in a weakly-supervised manner, i.e., very few annotations\n(e.g., 3-20) of object parts are used to guide the learning of AOGs. We develop\na question-answering (QA) method that uses active human-computer communications\nto mine patterns from a pre-trained CNN, in order to incrementally explain more\nfeatures in conv-layers. During the learning process, our QA method uses the\ncurrent AOG for part localization. The QA method actively identifies objects,\nwhose feature maps cannot be explained by the AOG. Then, our method asks people\nto annotate parts on the unexplained objects, and uses answers to discover CNN\npatterns corresponding to the newly labeled parts. In this way, our method\ngradually grows new branches and refines existing branches on the AOG to\nsemanticize CNN representations. In experiments, our method exhibited a high\nlearning efficiency. Our method used about 1/6-1/3 of the part annotations for\ntraining, but achieved similar or better part-localization performance than\nfast-RCNN methods. \n\n"}
{"id": "1812.08011", "contents": "Title: Training Deep Neural Networks with 8-bit Floating Point Numbers Abstract: The state-of-the-art hardware platforms for training Deep Neural Networks\n(DNNs) are moving from traditional single precision (32-bit) computations\ntowards 16 bits of precision -- in large part due to the high energy efficiency\nand smaller bit storage associated with using reduced-precision\nrepresentations. However, unlike inference, training with numbers represented\nwith less than 16 bits has been challenging due to the need to maintain\nfidelity of the gradient computations during back-propagation. Here we\ndemonstrate, for the first time, the successful training of DNNs using 8-bit\nfloating point numbers while fully maintaining the accuracy on a spectrum of\nDeep Learning models and datasets. In addition to reducing the data and\ncomputation precision to 8 bits, we also successfully reduce the arithmetic\nprecision for additions (used in partial product accumulation and weight\nupdates) from 32 bits to 16 bits through the introduction of a number of key\nideas including chunk-based accumulation and floating point stochastic\nrounding. The use of these novel techniques lays the foundation for a new\ngeneration of hardware training platforms with the potential for 2-4x improved\nthroughput over today's systems. \n\n"}
{"id": "1812.08407", "contents": "Title: Context, Attention and Audio Feature Explorations for Audio Visual\n  Scene-Aware Dialog Abstract: With the recent advancements in AI, Intelligent Virtual Assistants (IVA) have\nbecome a ubiquitous part of every home. Going forward, we are witnessing a\nconfluence of vision, speech and dialog system technologies that are enabling\nthe IVAs to learn audio-visual groundings of utterances and have conversations\nwith users about the objects, activities and events surrounding them. As a part\nof the 7th Dialog System Technology Challenges (DSTC7), for Audio Visual\nScene-Aware Dialog (AVSD) track, We explore `topics' of the dialog as an\nimportant contextual feature into the architecture along with explorations\naround multimodal Attention. We also incorporate an end-to-end audio\nclassification ConvNet, AclNet, into our models. We present detailed analysis\nof the experiments and show that some of our model variations outperform the\nbaseline system presented for this task. \n\n"}
{"id": "1812.08808", "contents": "Title: Reducing Sampling Ratios Improves Bagging in Sparse Regression Abstract: Bagging, a powerful ensemble method from machine learning, improves the\nperformance of unstable predictors. Although the power of Bagging has been\nshown mostly in classification problems, we demonstrate the success of\nemploying Bagging in sparse regression over the baseline method (L1\nminimization). The framework employs the generalized version of the original\nBagging with various bootstrap ratios. The performance limits associated with\ndifferent choices of bootstrap sampling ratio L/m and number of estimates K is\nanalyzed theoretically. Simulation shows that the proposed method yields\nstate-of-the-art recovery performance, outperforming L1 minimization and\nBolasso in the challenging case of low levels of measurements. A lower L/m\nratio (60% - 90%) leads to better performance, especially with a small number\nof measurements. With the reduced sampling rate, SNR improves over the original\nBagging by up to 24%. With a properly chosen sampling ratio, a reasonably small\nnumber of estimates K = 30 gives satisfying result, even though increasing K is\ndiscovered to always improve or at least maintain the performance. \n\n"}
{"id": "1812.08985", "contents": "Title: Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors Abstract: Unconditional image generation has recently been dominated by generative\nadversarial networks (GANs). GAN methods train a generator which regresses\nimages from random noise vectors, as well as a discriminator that attempts to\ndifferentiate between the generated images and a training set of real images.\nGANs have shown amazing results at generating realistic looking images. Despite\ntheir success, GANs suffer from critical drawbacks including: unstable training\nand mode-dropping. The weaknesses in GANs have motivated research into\nalternatives including: variational auto-encoders (VAEs), latent embedding\nlearning methods (e.g. GLO) and nearest-neighbor based implicit maximum\nlikelihood estimation (IMLE). Unfortunately at the moment, GANs still\nsignificantly outperform the alternative methods for image generation. In this\nwork, we present a novel method - Generative Latent Nearest Neighbors (GLANN) -\nfor training generative models without adversarial training. GLANN combines the\nstrengths of IMLE and GLO in a way that overcomes the main drawbacks of each\nmethod. Consequently, GLANN generates images that are far better than GLO and\nIMLE. Our method does not suffer from mode collapse which plagues GAN training\nand is much more stable. Qualitative results show that GLANN outperforms a\nbaseline consisting of 800 GANs and VAEs on commonly used datasets. Our models\nare also shown to be effective for training truly non-adversarial unsupervised\nimage translation. \n\n"}
{"id": "1812.10061", "contents": "Title: Noise Flooding for Detecting Audio Adversarial Examples Against\n  Automatic Speech Recognition Abstract: Neural models enjoy widespread use across a variety of tasks and have grown\nto become crucial components of many industrial systems. Despite their\neffectiveness and extensive popularity, they are not without their exploitable\nflaws. Initially applied to computer vision systems, the generation of\nadversarial examples is a process in which seemingly imperceptible\nperturbations are made to an image, with the purpose of inducing a deep\nlearning based classifier to misclassify the image. Due to recent trends in\nspeech processing, this has become a noticeable issue in speech recognition\nmodels. In late 2017, an attack was shown to be quite effective against the\nSpeech Commands classification model. Limited-vocabulary speech classifiers,\nsuch as the Speech Commands model, are used quite frequently in a variety of\napplications, particularly in managing automated attendants in telephony\ncontexts. As such, adversarial examples produced by this attack could have\nreal-world consequences. While previous work in defending against these\nadversarial examples has investigated using audio preprocessing to reduce or\ndistort adversarial noise, this work explores the idea of flooding particular\nfrequency bands of an audio signal with random noise in order to detect\nadversarial examples. This technique of flooding, which does not require\nretraining or modifying the model, is inspired by work done in computer vision\nand builds on the idea that speech classifiers are relatively robust to natural\nnoise. A combined defense incorporating 5 different frequency bands for\nflooding the signal with noise outperformed other existing defenses in the\naudio space, detecting adversarial examples with 91.8% precision and 93.5%\nrecall. \n\n"}
{"id": "1812.10604", "contents": "Title: Cross-relation Cross-bag Attention for Distantly-supervised Relation\n  Extraction Abstract: Distant supervision leverages knowledge bases to automatically label\ninstances, thus allowing us to train relation extractor without human\nannotations. However, the generated training data typically contain massive\nnoise, and may result in poor performances with the vanilla supervised\nlearning. In this paper, we propose to conduct multi-instance learning with a\nnovel Cross-relation Cross-bag Selective Attention (C$^2$SA), which leads to\nnoise-robust training for distant supervised relation extractor. Specifically,\nwe employ the sentence-level selective attention to reduce the effect of noisy\nor mismatched sentences, while the correlation among relations were captured to\nimprove the quality of attention weights. Moreover, instead of treating all\nentity-pairs equally, we try to pay more attention to entity-pairs with a\nhigher quality. Similarly, we adopt the selective attention mechanism to\nachieve this goal. Experiments with two types of relation extractor demonstrate\nthe superiority of the proposed approach over the state-of-the-art, while\nfurther ablation studies verify our intuitions and demonstrate the\neffectiveness of our proposed two techniques. \n\n"}
{"id": "1812.10730", "contents": "Title: Neuromemrisitive Architecture of HTM with On-Device Learning and\n  Neurogenesis Abstract: Hierarchical temporal memory (HTM) is a biomimetic sequence memory algorithm\nthat holds promise for invariant representations of spatial and spatiotemporal\ninputs. This paper presents a comprehensive neuromemristive crossbar\narchitecture for the spatial pooler (SP) and the sparse distributed\nrepresentation classifier, which are fundamental to the algorithm. There are\nseveral unique features in the proposed architecture that tightly link with the\nHTM algorithm. A memristor that is suitable for emulating the HTM synapses is\nidentified and a new Z-window function is proposed. The architecture exploits\nthe concept of synthetic synapses to enable potential synapses in the HTM. The\ncrossbar for the SP avoids dark spots caused by unutilized crossbar regions and\nsupports rapid on-chip training within 2 clock cycles. This research also\nleverages plasticity mechanisms such as neurogenesis and homeostatic intrinsic\nplasticity to strengthen the robustness and performance of the SP. The proposed\ndesign is benchmarked for image recognition tasks using MNIST and Yale faces\ndatasets, and is evaluated using different metrics including entropy,\nsparseness, and noise robustness. Detailed power analysis at different stages\nof the SP operations is performed to demonstrate the suitability for mobile\nplatforms. \n\n"}
{"id": "1812.10747", "contents": "Title: Off-the-grid model based deep learning (O-MODL) Abstract: We introduce a model based off-the-grid image reconstruction algorithm using\ndeep learned priors. The main difference of the proposed scheme with current\ndeep learning strategies is the learning of non-linear annihilation relations\nin Fourier space. We rely on a model based framework, which allows us to use a\nsignificantly smaller deep network, compared to direct approaches that also\nlearn how to invert the forward model. Preliminary comparisons against image\ndomain MoDL approach demonstrates the potential of the off-the-grid\nformulation. The main benefit of the proposed scheme compared to structured\nlow-rank methods is the quite significant reduction in computational\ncomplexity. \n\n"}
{"id": "1812.10775", "contents": "Title: 3D Point Capsule Networks Abstract: In this paper, we propose 3D point-capsule networks, an auto-encoder designed\nto process sparse 3D point clouds while preserving spatial arrangements of the\ninput data. 3D capsule networks arise as a direct consequence of our novel\nunified 3D auto-encoder formulation. Their dynamic routing scheme and the\npeculiar 2D latent space deployed by our approach bring in improvements for\nseveral common point cloud-related tasks, such as object classification, object\nreconstruction and part segmentation as substantiated by our extensive\nevaluations. Moreover, it enables new applications such as part interpolation\nand replacement. \n\n"}
{"id": "1812.10860", "contents": "Title: Can You Tell Me How to Get Past Sesame Street? Sentence-Level\n  Pretraining Beyond Language Modeling Abstract: Natural language understanding has recently seen a surge of progress with the\nuse of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et\nal., 2019) which are pretrained on variants of language modeling. We conduct\nthe first large-scale systematic study of candidate pretraining tasks,\ncomparing 19 different tasks both as alternatives and complements to language\nmodeling. Our primary results support the use language modeling, especially\nwhen combined with pretraining on additional labeled-data tasks. However, our\nresults are mixed across pretraining tasks and show some concerning trends: In\nELMo's pretrain-then-freeze paradigm, random baselines are worryingly strong\nand results vary strikingly across target tasks. In addition, fine-tuning BERT\non an intermediate task often negatively impacts downstream transfer. In a more\npositive trend, we see modest gains from multitask training, suggesting the\ndevelopment of more sophisticated multitask and transfer learning techniques as\nan avenue for further research. \n\n"}
{"id": "1812.11459", "contents": "Title: A neural joint model for Vietnamese word segmentation, POS tagging and\n  dependency parsing Abstract: We propose the first multi-task learning model for joint Vietnamese word\nsegmentation, part-of-speech (POS) tagging and dependency parsing. In\nparticular, our model extends the BIST graph-based dependency parser\n(Kiperwasser and Goldberg, 2016) with BiLSTM-CRF-based neural layers (Huang et\nal., 2015) for word segmentation and POS tagging. On Vietnamese benchmark\ndatasets, experimental results show that our joint model obtains\nstate-of-the-art or competitive performances. \n\n"}
{"id": "1812.11652", "contents": "Title: Using Machine Learning for Handover Optimization in Vehicular Fog\n  Computing Abstract: Smart mobility management would be an important prerequisite for future fog\ncomputing systems. In this research, we propose a learning-based handover\noptimization for the Internet of Vehicles that would assist the smooth\ntransition of device connections and offloaded tasks between fog nodes. To\naccomplish this, we make use of machine learning algorithms to learn from\nvehicle interactions with fog nodes. Our approach uses a three-layer\nfeed-forward neural network to predict the correct fog node at a given location\nand time with 99.2 % accuracy on a test set. We also implement a dual stacked\nrecurrent neural network (RNN) with long short-term memory (LSTM) cells capable\nof learning the latency, or cost, associated with these service requests. We\ncreate a simulation in JAMScript using a dataset of real-world vehicle\nmovements to create a dataset to train these networks. We further propose the\nuse of this predictive system in a smarter request routing mechanism to\nminimize the service interruption during handovers between fog nodes and to\nanticipate areas of low coverage through a series of experiments and test the\nmodels' performance on a test set. \n\n"}
{"id": "1812.11971", "contents": "Title: Mid-Level Visual Representations Improve Generalization and Sample\n  Efficiency for Learning Visuomotor Policies Abstract: How much does having visual priors about the world (e.g. the fact that the\nworld is 3D) assist in learning to perform downstream motor tasks (e.g.\ndelivering a package)? We study this question by integrating a generic\nperceptual skill set (e.g. a distance estimator, an edge detector, etc.) within\na reinforcement learning framework--see Figure 1. This skill set (hereafter\nmid-level perception) provides the policy with a more processed state of the\nworld compared to raw images.\n  We find that using a mid-level perception confers significant advantages over\ntraining end-to-end from scratch (i.e. not leveraging priors) in\nnavigation-oriented tasks. Agents are able to generalize to situations where\nthe from-scratch approach fails and training becomes significantly more sample\nefficient. However, we show that realizing these gains requires careful\nselection of the mid-level perceptual skills. Therefore, we refine our findings\ninto an efficient max-coverage feature set that can be adopted in lieu of raw\nimages. We perform our study in completely separate buildings for training and\ntesting and compare against visually blind baseline policies and\nstate-of-the-art feature learning methods. \n\n"}
{"id": "1901.00188", "contents": "Title: Complementary reinforcement learning towards explainable agents Abstract: Reinforcement learning (RL) algorithms allow agents to learn skills and\nstrategies to perform complex tasks without detailed instructions or expensive\nlabelled training examples. That is, RL agents can learn, as we learn. Given\nthe importance of learning in our intelligence, RL has been thought to be one\nof key components to general artificial intelligence, and recent breakthroughs\nin deep reinforcement learning suggest that neural networks (NN) are natural\nplatforms for RL agents. However, despite the efficiency and versatility of\nNN-based RL agents, their decision-making remains incomprehensible, reducing\ntheir utilities. To deploy RL into a wider range of applications, it is\nimperative to develop explainable NN-based RL agents. Here, we propose a method\nto derive a secondary comprehensible agent from a NN-based RL agent, whose\ndecision-makings are based on simple rules. Our empirical evaluation of this\nsecondary agent's performance supports the possibility of building a\ncomprehensible and transparent agent using a NN-based RL agent. \n\n"}
{"id": "1901.00248", "contents": "Title: A Survey on Multi-output Learning Abstract: Multi-output learning aims to simultaneously predict multiple outputs given\nan input. It is an important learning problem due to the pressing need for\nsophisticated decision making in real-world applications. Inspired by big data,\nthe 4Vs characteristics of multi-output imposes a set of challenges to\nmulti-output learning, in terms of the volume, velocity, variety and veracity\nof the outputs. Increasing number of works in the literature have been devoted\nto the study of multi-output learning and the development of novel approaches\nfor addressing the challenges encountered. However, it lacks a comprehensive\noverview on different types of challenges of multi-output learning brought by\nthe characteristics of the multiple outputs and the techniques proposed to\novercome the challenges. This paper thus attempts to fill in this gap to\nprovide a comprehensive review on this area. We first introduce different\nstages of the life cycle of the output labels. Then we present the paradigm on\nmulti-output learning, including its myriads of output structures, definitions\nof its different sub-problems, model evaluation metrics and popular data\nrepositories used in the study. Subsequently, we review a number of\nstate-of-the-art multi-output learning methods, which are categorized based on\nthe challenges. \n\n"}
{"id": "1901.00660", "contents": "Title: Deep Speech Enhancement for Reverberated and Noisy Signals using Wide\n  Residual Networks Abstract: This paper proposes a deep speech enhancement method which exploits the high\npotential of residual connections in a wide neural network architecture, a\ntopology known as Wide Residual Network. This is supported on single\ndimensional convolutions computed alongside the time domain, which is a\npowerful approach to process contextually correlated representations through\nthe temporal domain, such as speech feature sequences. We find the residual\nmechanism extremely useful for the enhancement task since the signal always has\na linear shortcut and the non-linear path enhances it in several steps by\nadding or subtracting corrections. The enhancement capacity of the proposal is\nassessed by objective quality metrics and the performance of a speech\nrecognition system. This was evaluated in the framework of the REVERB Challenge\ndataset, including simulated and real samples of reverberated and noisy speech\nsignals. Results showed that enhanced speech from the proposed method succeeded\nfor both, the enhancement task with intelligibility purposes and the speech\nrecognition system. The DNN model, trained with artificial synthesized\nreverberation data, was able to deal with far-field reverberated speech from\nreal scenarios. Furthermore, the method was able to take advantage of the\nresidual connection achieving to enhance signals with low noise level, which is\nusually a strong handicap of traditional enhancement methods. \n\n"}
{"id": "1901.01216", "contents": "Title: Transfer learning from language models to image caption generators:\n  Better models may not transfer better Abstract: When designing a neural caption generator, a convolutional neural network can\nbe used to extract image features. Is it possible to also use a neural language\nmodel to extract sentence prefix features? We answer this question by trying\ndifferent ways to transfer the recurrent neural network and embedding layer\nfrom a neural language model to an image caption generator. We find that image\ncaption generators with transferred parameters perform better than those\ntrained from scratch, even when simply pre-training them on the text of the\nsame captions dataset it will later be trained on. We also find that the best\nlanguage models (in terms of perplexity) do not result in the best caption\ngenerators after transfer learning. \n\n"}
{"id": "1901.01331", "contents": "Title: The ISTI Rapid Response on Exploring Cloud Computing 2018 Abstract: This report describes eighteen projects that explored how commercial cloud\ncomputing services can be utilized for scientific computation at national\nlaboratories. These demonstrations ranged from deploying proprietary software\nin a cloud environment to leveraging established cloud-based analytics\nworkflows for processing scientific datasets. By and large, the projects were\nsuccessful and collectively they suggest that cloud computing can be a valuable\ncomputational resource for scientific computation at national laboratories. \n\n"}
{"id": "1901.02871", "contents": "Title: The Lingering of Gradients: Theory and Applications Abstract: Classically, the time complexity of a first-order method is estimated by its\nnumber of gradient computations. In this paper, we study a more refined\ncomplexity by taking into account the `lingering' of gradients: once a gradient\nis computed at $x_k$, the additional time to compute gradients at\n$x_{k+1},x_{k+2},\\dots$ may be reduced.\n  We show how this improves the running time of several first-order methods.\nFor instance, if the `additional time' scales linearly with respect to the\ntraveled distance, then the `convergence rate' of gradient descent can be\nimproved from $1/T$ to $\\exp(-T^{1/3})$. On the application side, we solve a\nhypothetical revenue management problem on the Yahoo! Front Page Today Module\nwith 4.6m users to $10^{-6}$ error using only 6 passes of the dataset; and\nsolve a real-life support vector machine problem to an accuracy that is two\norders of magnitude better comparing to the state-of-the-art algorithm. \n\n"}
{"id": "1901.03035", "contents": "Title: Self-Monitoring Navigation Agent via Auxiliary Progress Estimation Abstract: The Vision-and-Language Navigation (VLN) task entails an agent following\nnavigational instruction in photo-realistic unknown environments. This\nchallenging task demands that the agent be aware of which instruction was\ncompleted, which instruction is needed next, which way to go, and its\nnavigation progress towards the goal. In this paper, we introduce a\nself-monitoring agent with two complementary components: (1) visual-textual\nco-grounding module to locate the instruction completed in the past, the\ninstruction required for the next action, and the next moving direction from\nsurrounding images and (2) progress monitor to ensure the grounded instruction\ncorrectly reflects the navigation progress. We test our self-monitoring agent\non a standard benchmark and analyze our proposed approach through a series of\nablation studies that elucidate the contributions of the primary components.\nUsing our proposed method, we set the new state of the art by a significant\nmargin (8% absolute increase in success rate on the unseen test set). Code is\navailable at https://github.com/chihyaoma/selfmonitoring-agent . \n\n"}
{"id": "1901.05560", "contents": "Title: Off-Policy Evaluation of Probabilistic Identity Data in Lookalike\n  Modeling Abstract: We evaluate the impact of probabilistically-constructed digital identity data\ncollected from Sep. to Dec. 2017 (approx.), in the context of\nLookalike-targeted campaigns. The backbone of this study is a large set of\nprobabilistically-constructed \"identities\", represented as small bags of\ncookies and mobile ad identifiers with associated metadata, that are likely all\nowned by the same underlying user. The identity data allows to generate\n\"identity-based\", rather than \"identifier-based\", user models, giving a fuller\npicture of the interests of the users underlying the identifiers. We employ\noff-policy techniques to evaluate the potential of identity-powered lookalike\nmodels without incurring the risk of allowing untested models to direct large\namounts of ad spend or the large cost of performing A/B tests. We add to\nhistorical work on off-policy evaluation by noting a significant type of\n\"finite-sample bias\" that occurs for studies combining modestly-sized datasets\nand evaluation metrics involving rare events (e.g., conversions). We illustrate\nthis bias using a simulation study that later informs the handling of inverse\npropensity weights in our analyses on real data. We demonstrate significant\nlift in identity-powered lookalikes versus an identity-ignorant baseline: on\naverage ~70% lift in conversion rate. This rises to factors of ~(4-32)x for\nidentifiers having little data themselves, but that can be inferred to belong\nto users with substantial data to aggregate across identifiers. This implies\nthat identity-powered user modeling is especially important in the context of\nidentifiers having very short lifespans (i.e., frequently churned cookies). Our\nwork motivates and informs the use of probabilistically-constructed identities\nin marketing. It also deepens the canon of examples in which off-policy\nlearning has been employed to evaluate the complex systems of the internet\neconomy. \n\n"}
{"id": "1901.06283", "contents": "Title: Improving Sequence-to-Sequence Learning via Optimal Transport Abstract: Sequence-to-sequence models are commonly trained via maximum likelihood\nestimation (MLE). However, standard MLE training considers a word-level\nobjective, predicting the next word given the previous ground-truth partial\nsentence. This procedure focuses on modeling local syntactic patterns, and may\nfail to capture long-range semantic structure. We present a novel solution to\nalleviate these issues. Our approach imposes global sequence-level guidance via\nnew supervision based on optimal transport, enabling the overall\ncharacterization and preservation of semantic features. We further show that\nthis method can be understood as a Wasserstein gradient flow trying to match\nour model to the ground truth sequence distribution. Extensive experiments are\nconducted to validate the utility of the proposed approach, showing consistent\nimprovements over a wide variety of NLP tasks, including machine translation,\nabstractive text summarization, and image captioning. \n\n"}
{"id": "1901.06610", "contents": "Title: Hierarchical Attentional Hybrid Neural Networks for Document\n  Classification Abstract: Document classification is a challenging task with important applications.\nThe deep learning approaches to the problem have gained much attention\nrecently. Despite the progress, the proposed models do not incorporate the\nknowledge of the document structure in the architecture efficiently and not\ntake into account the contexting importance of words and sentences. In this\npaper, we propose a new approach based on a combination of convolutional neural\nnetworks, gated recurrent units, and attention mechanisms for document\nclassification tasks. The main contribution of this work is the use of\nconvolution layers to extract more meaningful, generalizable and abstract\nfeatures by the hierarchical representation. The proposed method in this paper\nimproves the results of the current attention-based approaches for document\nclassification. \n\n"}
{"id": "1901.06773", "contents": "Title: AccUDNN: A GPU Memory Efficient Accelerator for Training Ultra-deep\n  Neural Networks Abstract: Typically, Ultra-deep neural network(UDNN) tends to yield high-quality model,\nbut its training process is usually resource intensive and time-consuming.\nModern GPU's scarce DRAM capacity is the primary bottleneck that hinders the\ntrainability and the training efficiency of UDNN. In this paper, we present\n\"AccUDNN\", an accelerator that aims to make the utmost use of finite GPU memory\nresources to speed up the training process of UDNN. AccUDNN mainly includes two\nmodules: memory optimizer and hyperparameter tuner. Memory optimizer develops a\nperformance-model guided dynamic swap out/in strategy, by offloading\nappropriate data to host memory, GPU memory footprint can be significantly\nslashed to overcome the restriction of trainability of UDNN. After applying the\nmemory optimization strategy, hyperparameter tuner is designed to explore the\nefficiency-optimal minibatch size and the matched learning rate. Evaluations\ndemonstrate that AccUDNN cuts down the GPU memory requirement of ResNet-152\nfrom more than 24GB to 8GB. In turn, given 12GB GPU memory budget, the\nefficiency-optimal minibatch size can reach 4.2x larger than original Caffe.\nBenefiting from better utilization of single GPU's computing resources and\nfewer parameter synchronization of large minibatch size, 7.7x speed-up is\nachieved by 8 GPUs' cluster without any communication optimization and no\naccuracy losses. \n\n"}
{"id": "1901.07061", "contents": "Title: Prior Information Guided Regularized Deep Learning for Cell Nucleus\n  Detection Abstract: Cell nuclei detection is a challenging research topic because of limitations\nin cellular image quality and diversity of nuclear morphology, i.e. varying\nnuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been\na topic of enduring interest with promising recent success shown by deep\nlearning methods. These methods train Convolutional Neural Networks (CNNs) with\na training set of input images and known, labeled nuclei locations. Many such\nmethods are supplemented by spatial or morphological processing. Using a set of\ncanonical cell nuclei shapes, prepared with the help of a domain expert, we\ndevelop a new approach that we call Shape Priors with Convolutional Neural\nNetworks (SP-CNN). We further extend the network to introduce a shape prior\n(SP) layer and then allowing it to become trainable (i.e. optimizable). We call\nthis network tunable SP-CNN (TSP-CNN). In summary, we present new network\nstructures that can incorporate 'expected behavior' of nucleus shapes via two\ncomponents: learnable layers that perform the nucleus detection and a fixed\nprocessing part that guides the learning with prior information. Analytically,\nwe formulate two new regularization terms that are targeted at: 1) learning the\nshapes, 2) reducing false positives while simultaneously encouraging detection\ninside the cell nucleus boundary. Experimental results on two challenging\ndatasets reveal that the proposed SP-CNN and TSP-CNN can outperform\nstate-of-the-art alternatives. \n\n"}
{"id": "1901.07165", "contents": "Title: Generation High resolution 3D model from natural language by Generative\n  Adversarial Network Abstract: We present a method of generating high resolution 3D shapes from natural\nlanguage descriptions. To achieve this goal, we propose two steps that\ngenerating low resolution shapes which roughly reflect texts and generating\nhigh resolution shapes which reflect the detail of texts. In a previous paper,\nthe authors have shown a method of generating low resolution shapes. We improve\nit to generate 3D shapes more faithful to natural language and test the\neffectiveness of the method. To generate high resolution 3D shapes, we use the\nframework of Conditional Wasserstein GAN. We propose two roles of Critic\nseparately, which calculate the Wasserstein distance between two probability\ndistribution, so that we achieve generating high quality shapes or acceleration\nof learning speed of model. To evaluate our approach, we performed quantitive\nevaluation with several numerical metrics for Critic models. Our method is\nfirst to realize the generation of high quality model by propagating text\nembedding information to high resolution task when generating 3D model. \n\n"}
{"id": "1901.07666", "contents": "Title: Rapid identification of pathogenic bacteria using Raman spectroscopy and\n  deep learning Abstract: Rapid identification of bacteria is essential to prevent the spread of\ninfectious disease, help combat antimicrobial resistance, and improve patient\noutcomes. Raman optical spectroscopy promises to combine bacterial detection,\nidentification, and antibiotic susceptibility testing in a single step.\nHowever, achieving clinically relevant speeds and accuracies remains\nchallenging due to the weak Raman signal from bacterial cells and the large\nnumber of bacterial species and phenotypes. By amassing the largest known\ndataset of bacterial Raman spectra, we are able to apply state-of-the-art deep\nlearning approaches to identify 30 of the most common bacterial pathogens from\nnoisy Raman spectra, achieving antibiotic treatment identification accuracies\nof 99.0$\\pm$0.1%. This novel approach distinguishes between\nmethicillin-resistant and -susceptible isolates of Staphylococcus aureus (MRSA\nand MSSA) as well as a pair of isogenic MRSA and MSSA that are genetically\nidentical apart from deletion of the mecA resistance gene, indicating the\npotential for culture-free detection of antibiotic resistance. Results from\ninitial clinical validation are promising: using just 10 bacterial spectra from\neach of 25 isolates, we achieve 99.0$\\pm$1.9% species identification accuracy.\nOur combined Raman-deep learning system represents an important\nproof-of-concept for rapid, culture-free identification of bacterial isolates\nand antibiotic resistance and could be readily extended for diagnostics on\nblood, urine, and sputum. \n\n"}
{"id": "1901.08201", "contents": "Title: ISeeU: Visually interpretable deep learning for mortality prediction\n  inside the ICU Abstract: To improve the performance of Intensive Care Units (ICUs), the field of\nbio-statistics has developed scores which try to predict the likelihood of\nnegative outcomes. These help evaluate the effectiveness of treatments and\nclinical practice, and also help to identify patients with unexpected outcomes.\nHowever, they have been shown by several studies to offer sub-optimal\nperformance. Alternatively, Deep Learning offers state of the art capabilities\nin certain prediction tasks and research suggests deep neural networks are able\nto outperform traditional techniques. Nevertheless, a main impediment for the\nadoption of Deep Learning in healthcare is its reduced interpretability, for in\nthis field it is crucial to gain insight on the why of predictions, to assure\nthat models are actually learning relevant features instead of spurious\ncorrelations. To address this, we propose a deep multi-scale convolutional\narchitecture trained on the Medical Information Mart for Intensive Care III\n(MIMIC-III) for mortality prediction, and the use of concepts from coalitional\ngame theory to construct visual explanations aimed to show how important these\ninputs are deemed by the network. Our results show our model attains state of\nthe art performance while remaining interpretable. Supporting code can be found\nat https://github.com/williamcaicedo/ISeeU. \n\n"}
{"id": "1901.08276", "contents": "Title: Traditional and Heavy-Tailed Self Regularization in Neural Network\n  Models Abstract: Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep\nNeural Networks (DNNs), including both production quality, pre-trained models\nsuch as AlexNet and Inception, and smaller models trained from scratch, such as\nLeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly\nindicate that the empirical spectral density (ESD) of DNN layer matrices\ndisplays signatures of traditionally-regularized statistical models, even in\nthe absence of exogenously specifying traditional forms of regularization, such\nas Dropout or Weight Norm constraints. Building on recent results in RMT, most\nnotably its extension to Universality classes of Heavy-Tailed matrices, we\ndevelop a theory to identify \\emph{5+1 Phases of Training}, corresponding to\nincreasing amounts of \\emph{Implicit Self-Regularization}. For smaller and/or\nolder DNNs, this Implicit Self-Regularization is like traditional Tikhonov\nregularization, in that there is a `size scale' separating signal from noise.\nFor state-of-the-art DNNs, however, we identify a novel form of\n\\emph{Heavy-Tailed Self-Regularization}, similar to the self-organization seen\nin the statistical physics of disordered systems. This implicit\nSelf-Regularization can depend strongly on the many knobs of the training\nprocess. By exploiting the generalization gap phenomena, we demonstrate that we\ncan cause a small model to exhibit all 5+1 phases of training simply by\nchanging the batch size. \n\n"}
{"id": "1901.08486", "contents": "Title: Never Forget: Balancing Exploration and Exploitation via Learning\n  Optical Flow Abstract: Exploration bonus derived from the novelty of the states in an environment\nhas become a popular approach to motivate exploration for deep reinforcement\nlearning agents in the past few years. Recent methods such as curiosity-driven\nexploration usually estimate the novelty of new observations by the prediction\nerrors of their system dynamics models. Due to the capacity limitation of the\nmodels and difficulty of performing next-frame prediction, however, these\nmethods typically fail to balance between exploration and exploitation in\nhigh-dimensional observation tasks, resulting in the agents forgetting the\nvisited paths and exploring those states repeatedly. Such inefficient\nexploration behavior causes significant performance drops, especially in large\nenvironments with sparse reward signals. In this paper, we propose to introduce\nthe concept of optical flow estimation from the field of computer vision to\ndeal with the above issue. We propose to employ optical flow estimation errors\nto examine the novelty of new observations, such that agents are able to\nmemorize and understand the visited states in a more comprehensive fashion. We\ncompare our method against the previous approaches in a number of experimental\nexperiments. Our results indicate that the proposed method appears to deliver\nsuperior and long-lasting performance than the previous methods. We further\nprovide a set of comprehensive ablative analysis of the proposed method, and\ninvestigate the impact of optical flow estimation on the learning curves of the\nDRL agents. \n\n"}
{"id": "1901.08492", "contents": "Title: Feudal Multi-Agent Hierarchies for Cooperative Reinforcement Learning Abstract: We investigate how reinforcement learning agents can learn to cooperate.\nDrawing inspiration from human societies, in which successful coordination of\nmany individuals is often facilitated by hierarchical organisation, we\nintroduce Feudal Multi-agent Hierarchies (FMH). In this framework, a 'manager'\nagent, which is tasked with maximising the environmentally-determined reward\nfunction, learns to communicate subgoals to multiple, simultaneously-operating,\n'worker' agents. Workers, which are rewarded for achieving managerial subgoals,\ntake concurrent actions in the world. We outline the structure of FMH and\ndemonstrate its potential for decentralised learning and control. We find that,\ngiven an adequate set of subgoals from which to choose, FMH performs, and\nparticularly scales, substantially better than cooperative approaches that use\na shared reward function. \n\n"}
{"id": "1901.08556", "contents": "Title: Visualized Insights into the Optimization Landscape of Fully\n  Convolutional Networks Abstract: Many image processing tasks involve image-to-image mapping, which can be\naddressed well by fully convolutional networks (FCN) without any heavy\npreprocessing. Although empirically designing and training FCNs can achieve\nsatisfactory results, reasons for the improvement in performance are slightly\nambiguous. Our study is to make progress in understanding their generalization\nabilities through visualizing the optimization landscapes. The visualization of\nobjective functions is obtained by choosing a solution and projecting its\nvicinity onto a 3D space. We compare three FCN-based networks (two existing\nmodels and a new proposed in this paper for comparison) on multiple datasets.\nIt has been observed in practice that the connections from the pre-pooled\nfeature maps to the post-upsampled can achieve better results. We investigate\nthe cause and provide experiments to shows that the skip-layer connections in\nFCN can promote flat optimization landscape, which is well known to generalize\nbetter. Additionally, we explore the relationship between the models\ngeneralization ability and loss surface under different batch sizes. Results\nshow that large-batch training makes the model converge to sharp minimizers\nwith chaotic vicinities while small-batch method leads the model to flat\nminimizers with smooth and nearly convex regions. Our work may contribute to\ninsights and analysis for designing and training FCNs. \n\n"}
{"id": "1901.08576", "contents": "Title: Learning Interpretable Models with Causal Guarantees Abstract: Machine learning has shown much promise in helping improve the quality of\nmedical, legal, and financial decision-making. In these applications, machine\nlearning models must satisfy two important criteria: (i) they must be causal,\nsince the goal is typically to predict individual treatment effects, and (ii)\nthey must be interpretable, so that human decision makers can validate and\ntrust the model predictions. There has recently been much progress along each\ndirection independently, yet the state-of-the-art approaches are fundamentally\nincompatible. We propose a framework for learning interpretable models from\nobservational data that can be used to predict individual treatment effects\n(ITEs). In particular, our framework converts any supervised learning algorithm\ninto an algorithm for estimating ITEs. Furthermore, we prove an error bound on\nthe treatment effects predicted by our model. Finally, in an experiment on\nreal-world data, we show that the models trained using our framework\nsignificantly outperform a number of baselines. \n\n"}
{"id": "1901.08624", "contents": "Title: AutoShuffleNet: Learning Permutation Matrices via an Exact Lipschitz\n  Continuous Penalty in Deep Convolutional Neural Networks Abstract: ShuffleNet is a state-of-the-art light weight convolutional neural network\narchitecture. Its basic operations include group, channel-wise convolution and\nchannel shuffling. However, channel shuffling is manually designed empirically.\nMathematically, shuffling is a multiplication by a permutation matrix. In this\npaper, we propose to automate channel shuffling by learning permutation\nmatrices in network training. We introduce an exact Lipschitz continuous\nnon-convex penalty so that it can be incorporated in the stochastic gradient\ndescent to approximate permutation at high precision. Exact permutations are\nobtained by simple rounding at the end of training and are used in inference.\nThe resulting network, referred to as AutoShuffleNet, achieved improved\nclassification accuracies on CIFAR-10 and ImageNet data sets. In addition, we\nfound experimentally that the standard convex relaxation of permutation\nmatrices into stochastic matrices leads to poor performance. We prove\ntheoretically the exactness (error bounds) in recovering permutation matrices\nwhen our penalty function is zero (very small). We present examples of\npermutation optimization through graph matching and two-layer neural network\nmodels where the loss functions are calculated in closed analytical form. In\nthe examples, convex relaxation failed to capture permutations whereas our\npenalty succeeded. \n\n"}
{"id": "1901.08817", "contents": "Title: State-Regularized Recurrent Neural Networks Abstract: Recurrent neural networks are a widely used class of neural architectures.\nThey have, however, two shortcomings. First, it is difficult to understand what\nexactly they learn. Second, they tend to work poorly on sequences requiring\nlong-term memorization, despite having this capacity in principle. We aim to\naddress both shortcomings with a class of recurrent networks that use a\nstochastic state transition mechanism between cell applications. This\nmechanism, which we term state-regularization, makes RNNs transition between a\nfinite set of learnable states. We evaluate state-regularized RNNs on (1)\nregular languages for the purpose of automata extraction; (2) nonregular\nlanguages such as balanced parentheses, palindromes, and the copy task where\nexternal memory is required; and (3) real-word sequence learning tasks for\nsentiment analysis, visual object recognition, and language modeling. We show\nthat state-regularization (a) simplifies the extraction of finite state\nautomata modeling an RNN's state transition dynamics; (b) forces RNNs to\noperate more like automata with external memory and less like finite state\nmachines; (c) makes RNNs have better interpretability and explainability. \n\n"}
{"id": "1901.09187", "contents": "Title: Discovery of Important Subsequences in Electrocardiogram Beats Using the\n  Nearest Neighbour Algorithm Abstract: The classification of time series data is a well-studied problem with\nnumerous practical applications, such as medical diagnosis and speech\nrecognition. A popular and effective approach is to classify new time series in\nthe same way as their nearest neighbours, whereby proximity is defined using\nDynamic Time Warping (DTW) distance, a measure analogous to sequence alignment\nin bioinformatics. However, practitioners are not only interested in accurate\nclassification, they are also interested in why a time series is classified a\ncertain way. To this end, we introduce here the problem of finding a minimum\nlength subsequence of a time series, the removal of which changes the outcome\nof the classification under the nearest neighbour algorithm with DTW distance.\nInformally, such a subsequence is expected to be relevant for the\nclassification and can be helpful for practitioners in interpreting the\noutcome. We describe a simple but optimized implementation for detecting these\nsubsequences and define an accompanying measure to quantify the relevance of\nevery time point in the time series for the classification. In tests on\nelectrocardiogram data we show that the algorithm allows discovery of important\nsubsequences and can be helpful in detecting abnormalities in cardiac rhythms\ndistinguishing sick from healthy patients. \n\n"}
{"id": "1901.09192", "contents": "Title: SelectiveNet: A Deep Neural Network with an Integrated Reject Option Abstract: We consider the problem of selective prediction (also known as reject option)\nin deep neural networks, and introduce SelectiveNet, a deep neural architecture\nwith an integrated reject option. Existing rejection mechanisms are based\nmostly on a threshold over the prediction confidence of a pre-trained network.\nIn contrast, SelectiveNet is trained to optimize both classification (or\nregression) and rejection simultaneously, end-to-end. The result is a deep\nneural network that is optimized over the covered domain. In our experiments,\nwe show a consistently improved risk-coverage trade-off over several well-known\nclassification and regression datasets, thus reaching new state-of-the-art\nresults for deep selective classification. \n\n"}
{"id": "1901.09229", "contents": "Title: DELTA: DEep Learning Transfer using Feature Map with Attention for\n  Convolutional Networks Abstract: Transfer learning through fine-tuning a pre-trained neural network with an\nextremely large dataset, such as ImageNet, can significantly accelerate\ntraining while the accuracy is frequently bottlenecked by the limited dataset\nsize of the new target task. To solve the problem, some regularization methods,\nconstraining the outer layer weights of the target network using the starting\npoint as references (SPAR), have been studied. In this paper, we propose a\nnovel regularized transfer learning framework DELTA, namely DEep Learning\nTransfer using Feature Map with Attention. Instead of constraining the weights\nof neural network, DELTA aims to preserve the outer layer outputs of the target\nnetwork. Specifically, in addition to minimizing the empirical loss, DELTA\nintends to align the outer layer outputs of two networks, through constraining\na subset of feature maps that are precisely selected by attention that has been\nlearned in an supervised learning manner. We evaluate DELTA with the\nstate-of-the-art algorithms, including L2 and L2-SP. The experiment results\nshow that our proposed method outperforms these baselines with higher accuracy\nfor new tasks. \n\n"}
{"id": "1901.09283", "contents": "Title: Money on the Table: Statistical information ignored by Softmax can\n  improve classifier accuracy Abstract: Softmax is a standard final layer used in Neural Nets (NNs) to summarize\ninformation encoded in the trained NN and return a prediction. However, Softmax\nleverages only a subset of the class-specific structure encoded in the trained\nmodel and ignores potentially valuable information: During training, models\nencode an array $D$ of class response distributions, where $D_{ij}$ is the\ndistribution of the $j^{th}$ pre-Softmax readout neuron's responses to the\n$i^{th}$ class. Given a test sample, Softmax implicitly uses only the row of\nthis array $D$ that corresponds to the readout neurons' responses to the\nsample's true class. Leveraging more of this array $D$ can improve classifier\naccuracy, because the likelihoods of two competing classes can be encoded in\nother rows of $D$.\n  To explore this potential resource, we develop a hybrid classifier\n(Softmax-Pooling Hybrid, $SPH$) that uses Softmax on high-scoring samples, but\non low-scoring samples uses a log-likelihood method that pools the information\nfrom the full array $D$. We apply $SPH$ to models trained on a vectorized MNIST\ndataset to varying levels of accuracy. $SPH$ replaces only the final Softmax\nlayer in the trained NN, at test time only. All training is the same as for\nSoftmax. Because the pooling classifier performs better than Softmax on\nlow-scoring samples, $SPH$ reduces test set error by 6% to 23%, using the exact\nsame trained model, whatever the baseline Softmax accuracy. This reduction in\nerror reflects hidden capacity of the trained NN that is left unused by\nSoftmax. \n\n"}
{"id": "1901.09330", "contents": "Title: Reward Shaping via Meta-Learning Abstract: Reward shaping is one of the most effective methods to tackle the crucial yet\nchallenging problem of credit assignment in Reinforcement Learning (RL).\nHowever, designing shaping functions usually requires much expert knowledge and\nhand-engineering, and the difficulties are further exacerbated given multiple\nsimilar tasks to solve. In this paper, we consider reward shaping on a\ndistribution of tasks, and propose a general meta-learning framework to\nautomatically learn the efficient reward shaping on newly sampled tasks,\nassuming only shared state space but not necessarily action space. We first\nderive the theoretically optimal reward shaping in terms of credit assignment\nin model-free RL. We then propose a value-based meta-learning algorithm to\nextract an effective prior over the optimal reward shaping. The prior can be\napplied directly to new tasks, or provably adapted to the task-posterior while\nsolving the task within few gradient updates. We demonstrate the effectiveness\nof our shaping through significantly improved learning efficiency and\ninterpretable visualizations across various settings, including notably a\nsuccessful transfer from DQN to DDPG. \n\n"}
{"id": "1901.09455", "contents": "Title: Off-Policy Deep Reinforcement Learning by Bootstrapping the Covariate\n  Shift Abstract: In this paper we revisit the method of off-policy corrections for\nreinforcement learning (COP-TD) pioneered by Hallak et al. (2017). Under this\nmethod, online updates to the value function are reweighted to avoid divergence\nissues typical of off-policy learning. While Hallak et al.'s solution is\nappealing, it cannot easily be transferred to nonlinear function approximation.\nFirst, it requires a projection step onto the probability simplex; second, even\nthough the operator describing the expected behavior of the off-policy learning\nalgorithm is convergent, it is not known to be a contraction mapping, and\nhence, may be more unstable in practice. We address these two issues by\nintroducing a discount factor into COP-TD. We analyze the behavior of\ndiscounted COP-TD and find it better behaved from a theoretical perspective. We\nalso propose an alternative soft normalization penalty that can be minimized\nonline and obviates the need for an explicit projection step. We complement our\nanalysis with an empirical evaluation of the two techniques in an off-policy\nsetting on the game Pong from the Atari domain where we find discounted COP-TD\nto be better behaved in practice than the soft normalization penalty. Finally,\nwe perform a more extensive evaluation of discounted COP-TD in 5 games of the\nAtari domain, where we find performance gains for our approach. \n\n"}
{"id": "1901.09465", "contents": "Title: Deconstructing Generative Adversarial Networks Abstract: We deconstruct the performance of GANs into three components:\n  1. Formulation: we propose a perturbation view of the population target of\nGANs. Building on this interpretation, we show that GANs can be viewed as a\ngeneralization of the robust statistics framework, and propose a novel GAN\narchitecture, termed as Cascade GANs, to provably recover meaningful\nlow-dimensional generator approximations when the real distribution is\nhigh-dimensional and corrupted by outliers.\n  2. Generalization: given a population target of GANs, we design a systematic\nprinciple, projection under admissible distance, to design GANs to meet the\npopulation requirement using finite samples. We implement our principle in\nthree cases to achieve polynomial and sometimes near-optimal sample\ncomplexities: (1) learning an arbitrary generator under an arbitrary\npseudonorm; (2) learning a Gaussian location family under TV distance, where we\nutilize our principle provide a new proof for the optimality of Tukey median\nviewed as GANs; (3) learning a low-dimensional Gaussian approximation of a\nhigh-dimensional arbitrary distribution under Wasserstein distance. We\ndemonstrate a fundamental trade-off in the approximation error and statistical\nerror in GANs, and show how to apply our principle with empirical samples to\npredict how many samples are sufficient for GANs in order not to suffer from\nthe discriminator winning problem.\n  3. Optimization: we demonstrate alternating gradient descent is provably not\nlocally asymptotically stable in optimizing the GAN formulation of PCA. We\ndiagnose the problem as the minimax duality gap being non-zero, and propose a\nnew GAN architecture whose duality gap is zero, where the value of the game is\nequal to the previous minimax value (not the maximin value). We prove the new\nGAN architecture is globally asymptotically stable in optimization under\nalternating gradient descent. \n\n"}
{"id": "1901.09614", "contents": "Title: A Simple Method to Reduce Off-chip Memory Accesses on Convolutional\n  Neural Networks Abstract: For convolutional neural networks, a simple algorithm to reduce off-chip\nmemory accesses is proposed by maximally utilizing on-chip memory in a neural\nprocess unit. Especially, the algorithm provides an effective way to process a\nmodule which consists of multiple branches and a merge layer. For Inception-V3\non Samsung's NPU in Exynos, our evaluation shows that the proposed algorithm\nmakes off-chip memory accesses reduced by 1/50, and accordingly achieves 97.59\n% reduction in the amount of feature-map data to be transferred from/to\noff-chip memory. \n\n"}
{"id": "1901.09712", "contents": "Title: Ising Models with Latent Conditional Gaussian Variables Abstract: Ising models describe the joint probability distribution of a vector of\nbinary feature variables. Typically, not all the variables interact with each\nother and one is interested in learning the presumably sparse network structure\nof the interacting variables. However, in the presence of latent variables, the\nconventional method of learning a sparse model might fail. This is because the\nlatent variables induce indirect interactions of the observed variables. In the\ncase of only a few latent conditional Gaussian variables these spurious\ninteractions contribute an additional low-rank component to the interaction\nparameters of the observed Ising model. Therefore, we propose to learn a sparse\n+ low-rank decomposition of the parameters of an Ising model using a convex\nregularized likelihood problem. We show that the same problem can be obtained\nas the dual of a maximum-entropy problem with a new type of relaxation, where\nthe sample means collectively need to match the expected values only up to a\ngiven tolerance. The solution to the convex optimization problem has\nconsistency properties in the high-dimensional setting, where the number of\nobserved binary variables and the number of latent conditional Gaussian\nvariables are allowed to grow with the number of training samples. \n\n"}
{"id": "1901.09878", "contents": "Title: CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule\n  Networks Abstract: Capsule Networks preserve the hierarchical spatial relationships between\nobjects, and thereby bears a potential to surpass the performance of\ntraditional Convolutional Neural Networks (CNNs) in performing tasks like image\nclassification. A large body of work has explored adversarial examples for\nCNNs, but their effectiveness on Capsule Networks has not yet been well\nstudied. In our work, we perform an analysis to study the vulnerabilities in\nCapsule Networks to adversarial attacks. These perturbations, added to the test\ninputs, are small and imperceptible to humans, but can fool the network to\nmispredict. We propose a greedy algorithm to automatically generate targeted\nimperceptible adversarial examples in a black-box attack scenario. We show that\nthis kind of attacks, when applied to the German Traffic Sign Recognition\nBenchmark (GTSRB), mislead Capsule Networks. Moreover, we apply the same kind\nof adversarial attacks to a 5-layer CNN and a 9-layer CNN, and analyze the\noutcome, compared to the Capsule Networks to study differences in their\nbehavior. \n\n"}
{"id": "1901.09906", "contents": "Title: Hierarchically Clustered Representation Learning Abstract: The joint optimization of representation learning and clustering in the\nembedding space has experienced a breakthrough in recent years. In spite of the\nadvance, clustering with representation learning has been limited to flat-level\ncategories, which often involves cohesive clustering with a focus on instance\nrelations. To overcome the limitations of flat clustering, we introduce\nhierarchically-clustered representation learning (HCRL), which simultaneously\noptimizes representation learning and hierarchical clustering in the embedding\nspace. Compared with a few prior works, HCRL firstly attempts to consider a\ngeneration of deep embeddings from every component of the hierarchy, not just\nleaf components. In addition to obtaining hierarchically clustered embeddings,\nwe can reconstruct data by the various abstraction levels, infer the intrinsic\nhierarchical structure, and learn the level-proportion features. We conducted\nevaluations with image and text domains, and our quantitative analyses showed\ncompetent likelihoods and the best accuracies compared with the baselines. \n\n"}
{"id": "1901.10263", "contents": "Title: TiFi: Taxonomy Induction for Fictional Domains [Extended version] Abstract: Taxonomies are important building blocks of structured knowledge bases, and\ntheir construction from text sources and Wikipedia has received much attention.\nIn this paper we focus on the construction of taxonomies for fictional domains,\nusing noisy category systems from fan wikis or text extraction as input. Such\nfictional domains are archetypes of entity universes that are poorly covered by\nWikipedia, such as also enterprise-specific knowledge bases or highly\nspecialized verticals. Our fiction-targeted approach, called TiFi, consists of\nthree phases: (i) category cleaning, by identifying candidate categories that\ntruly represent classes in the domain of interest, (ii) edge cleaning, by\nselecting subcategory relationships that correspond to class subsumption, and\n(iii) top-level construction, by mapping classes onto a subset of high-level\nWordNet categories. A comprehensive evaluation shows that TiFi is able to\nconstruct taxonomies for a diverse range of fictional domains such as Lord of\nthe Rings, The Simpsons or Greek Mythology with very high precision and that it\noutperforms state-of-the-art baselines for taxonomy induction by a substantial\nmargin. \n\n"}
{"id": "1901.10621", "contents": "Title: Enhanced Variational Inference with Dyadic Transformation Abstract: Variational autoencoder is a powerful deep generative model with variational\ninference. The practice of modeling latent variables in the VAE's original\nformulation as normal distributions with a diagonal covariance matrix limits\nthe flexibility to match the true posterior distribution. We propose a new\ntransformation, dyadic transformation (DT), that can model a multivariate\nnormal distribution. DT is a single-stage transformation with low computational\nrequirements. We demonstrate empirically on MNIST dataset that DT enhances the\nposterior flexibility and attains competitive results compared to other VAE\nenhancements. \n\n"}
{"id": "1901.10653", "contents": "Title: Evaluating Bregman Divergences for Probability Learning from Crowd Abstract: The crowdsourcing scenarios are a good example of having a probability\ndistribution over some categories showing what the people in a global\nperspective thinks. Learn a predictive model of this probability distribution\ncan be of much more valuable that learn only a discriminative model that gives\nthe most likely category of the data. Here we present differents models that\nadapts having probability distribution as target to train a machine learning\nmodel. We focus on the Bregman divergences framework to used as objective\nfunction to minimize. The results show that special care must be taken when\nbuild a objective function and consider a equal optimization on neural network\nin Keras framework. \n\n"}
{"id": "1901.11058", "contents": "Title: HyperGAN: A Generative Model for Diverse, Performant Neural Networks Abstract: Standard neural networks are often overconfident when presented with data\noutside the training distribution. We introduce HyperGAN, a new generative\nmodel for learning a distribution of neural network parameters. HyperGAN does\nnot require restrictive assumptions on priors, and networks sampled from it can\nbe used to quickly create very large and diverse ensembles. HyperGAN employs a\nnovel mixer to project prior samples to a latent space with correlated\ndimensions, and samples from the latent space are then used to generate weights\nfor each layer of a deep neural network. We show that HyperGAN can learn to\ngenerate parameters which label the MNIST and CIFAR-10 datasets with\ncompetitive performance to fully supervised learning, while learning a rich\ndistribution of effective parameters. We also show that HyperGAN can also\nprovide better uncertainty estimates than standard ensembles by evaluating on\nout of distribution data as well as adversarial examples. \n\n"}
{"id": "1901.11143", "contents": "Title: Natural Analysts in Adaptive Data Analysis Abstract: Adaptive data analysis is frequently criticized for its pessimistic\ngeneralization guarantees. The source of these pessimistic bounds is a model\nthat permits arbitrary, possibly adversarial analysts that optimally use\ninformation to bias results. While being a central issue in the field, still\nlacking are notions of natural analysts that allow for more optimistic bounds\nfaithful to the reality that typical analysts aren't adversarial.\n  In this work, we propose notions of natural analysts that smoothly\ninterpolate between the optimal non-adaptive bounds and the best-known adaptive\ngeneralization bounds. To accomplish this, we model the analyst's knowledge as\nevolving according to the rules of an unknown dynamical system that takes in\nrevealed information and outputs new statistical queries to the data. This\nallows us to restrict the analyst through different natural control-theoretic\nnotions. One such notion corresponds to a recency bias, formalizing an\ninability to arbitrarily use distant information. Another complementary notion\nformalizes an anchoring bias, a tendency to weight initial information more\nstrongly. Both notions come with quantitative parameters that smoothly\ninterpolate between the non-adaptive case and the fully adaptive case, allowing\nfor a rich spectrum of intermediate analysts that are neither non-adaptive nor\nadversarial.\n  Natural not only from a cognitive perspective, we show that our notions also\ncapture standard optimization methods, like gradient descent in various\nsettings. This gives a new interpretation to the fact that gradient descent\ntends to overfit much less than its adaptive nature might suggest. \n\n"}
{"id": "1901.11356", "contents": "Title: Functional Regularisation for Continual Learning with Gaussian Processes Abstract: We introduce a framework for Continual Learning (CL) based on Bayesian\ninference over the function space rather than the parameters of a deep neural\nnetwork. This method, referred to as functional regularisation for Continual\nLearning, avoids forgetting a previous task by constructing and memorising an\napproximate posterior belief over the underlying task-specific function. To\nachieve this we rely on a Gaussian process obtained by treating the weights of\nthe last layer of a neural network as random and Gaussian distributed. Then,\nthe training algorithm sequentially encounters tasks and constructs posterior\nbeliefs over the task-specific functions by using inducing point sparse\nGaussian process methods. At each step a new task is first learnt and then a\nsummary is constructed consisting of (i) inducing inputs -- a fixed-size subset\nof the task inputs selected such that it optimally represents the task -- and\n(ii) a posterior distribution over the function values at these inputs. This\nsummary then regularises learning of future tasks, through Kullback-Leibler\nregularisation terms. Our method thus unites approaches focused on\n(pseudo-)rehearsal with those derived from a sequential Bayesian inference\nperspective in a principled way, leading to strong results on accepted\nbenchmarks. \n\n"}
{"id": "1901.11528", "contents": "Title: Shaping the Narrative Arc: An Information-Theoretic Approach to\n  Collaborative Dialogue Abstract: We consider the problem of designing an artificial agent capable of\ninteracting with humans in collaborative dialogue to produce creative, engaging\nnarratives. In this task, the goal is to establish universe details, and to\ncollaborate on an interesting story in that universe, through a series of\nnatural dialogue exchanges. Our model can augment any probabilistic\nconversational agent by allowing it to reason about universe information\nestablished and what potential next utterances might reveal. Ideally, with each\nutterance, agents would reveal just enough information to add specificity and\nreduce ambiguity without limiting the conversation. We empirically show that\nour model allows control over the rate at which the agent reveals information\nand that doing so significantly improves accuracy in predicting the next line\nof dialogues from movies. We close with a case-study with four professional\ntheatre performers, who preferred interactions with our model-augmented agent\nover an unaugmented agent. \n\n"}
{"id": "cmp-lg/9608003", "contents": "Title: Stylistic Variation in an Information Retrieval Experiment Abstract: Texts exhibit considerable stylistic variation. This paper reports an\nexperiment where a corpus of documents (N= 75 000) is analyzed using various\nsimple stylistic metrics. A subset (n = 1000) of the corpus has been previously\nassessed to be relevant for answering given information retrieval queries. The\nexperiment shows that this subset differs significantly from the rest of the\ncorpus in terms of the stylistic metrics studied. \n\n"}
{"id": "cmp-lg/9708011", "contents": "Title: Similarity-Based Approaches to Natural Language Processing Abstract: This thesis presents two similarity-based approaches to sparse data problems.\nThe first approach is to build soft, hierarchical clusters: soft, because each\nevent belongs to each cluster with some probability; hierarchical, because\ncluster centroids are iteratively split to model finer distinctions. Our second\napproach is a nearest-neighbor approach: instead of calculating a centroid for\neach class, as in the hierarchical clustering approach, we in essence build a\ncluster around each word. We compare several such nearest-neighbor approaches\non a word sense disambiguation task and find that as a whole, their performance\nis far superior to that of standard methods. In another set of experiments, we\nshow that using estimation techniques based on the nearest-neighbor model\nenables us to achieve perplexity reductions of more than 20 percent over\nstandard techniques in the prediction of low-frequency events, and\nstatistically significant speech recognition error-rate reduction. \n\n"}
{"id": "cond-mat/0009165", "contents": "Title: Occam factors and model-independent Bayesian learning of continuous\n  distributions Abstract: Learning of a smooth but nonparametric probability density can be regularized\nusing methods of Quantum Field Theory. We implement a field theoretic prior\nnumerically, test its efficacy, and show that the data and the phase space\nfactors arising from the integration over the model space determine the free\nparameter of the theory (\"smoothness scale\") self-consistently. This persists\neven for distributions that are atypical in the prior and is a step towards a\nmodel-independent theory for learning continuous distributions. Finally, we\npoint out that a wrong parameterization of a model family may sometimes be\nadvantageous for small data sets. \n\n"}
{"id": "cs/0208020", "contents": "Title: Using the DIFF Command for Natural Language Processing Abstract: Diff is a software program that detects differences between two data sets and\nis useful in natural language processing. This paper shows several examples of\nthe application of diff. They include the detection of differences between two\ndifferent datasets, extraction of rewriting rules, merging of two different\ndatasets, and the optimal matching of two different data sets. Since diff comes\nwith any standard UNIX system, it is readily available and very easy to use.\nOur studies showed that diff is a practical tool for research into natural\nlanguage processing. \n\n"}
{"id": "cs/0511058", "contents": "Title: On-line regression competitive with reproducing kernel Hilbert spaces Abstract: We consider the problem of on-line prediction of real-valued labels, assumed\nbounded in absolute value by a known constant, of new objects from known\nlabeled objects. The prediction algorithm's performance is measured by the\nsquared deviation of the predictions from the actual labels. No stochastic\nassumptions are made about the way the labels and objects are generated.\nInstead, we are given a benchmark class of prediction rules some of which are\nhoped to produce good predictions. We show that for a wide range of\ninfinite-dimensional benchmark classes one can construct a prediction algorithm\nwhose cumulative loss over the first N examples does not exceed the cumulative\nloss of any prediction rule in the class plus O(sqrt(N)); the main differences\nfrom the known results are that we do not impose any upper bound on the norm of\nthe considered prediction rules and that we achieve an optimal leading term in\nthe excess loss of our algorithm. If the benchmark class is \"universal\" (dense\nin the class of continuous functions on each compact set), this provides an\non-line non-stochastic analogue of universally consistent prediction in\nnon-parametric statistics. We use two proof techniques: one is based on the\nAggregating Algorithm and the other on the recently developed method of\ndefensive forecasting. \n\n"}
{"id": "q-bio/0401033", "contents": "Title: Parametric Inference for Biological Sequence Analysis Abstract: One of the major successes in computational biology has been the unification,\nusing the graphical model formalism, of a multitude of algorithms for\nannotating and comparing biological sequences. Graphical models that have been\napplied towards these problems include hidden Markov models for annotation,\ntree models for phylogenetics, and pair hidden Markov models for alignment. A\nsingle algorithm, the sum-product algorithm, solves many of the inference\nproblems associated with different statistical models. This paper introduces\nthe \\emph{polytope propagation algorithm} for computing the Newton polytope of\nan observation from a graphical model. This algorithm is a geometric version of\nthe sum-product algorithm and is used to analyze the parametric behavior of\nmaximum a posteriori inference calculations for graphical models. \n\n"}
