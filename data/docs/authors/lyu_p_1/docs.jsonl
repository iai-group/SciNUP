{"id": "0706.2576", "contents": "Title: First performance studies of a prototype for the CASTOR forward\n  calorimeter at the CMS experiment Abstract: We present results on the performance of the first prototype of the CASTOR\nquartz-tungsten sampling calorimeter, to be installed in the very forward\nregion of the CMS experiment at the LHC. This study includes GEANT Monte Carlo\nsimulations of the Cherenkov light transmission efficiency of different types\nof air-core light guides, as well as analysis of the calorimeter linearity and\nresolution as a function of energy and impact-point, obtained with 20-200 GeV\nelectron beams from CERN/SPS tests in 2003. Several configurations of the\ncalorimeter have been tested and compared, including different combinations of\n(i) structures for the active material of the calorimeter (quartz plates and\nfibres), (ii) various light-guide reflecting materials (glass and foil\nreflectors) and (iii) photodetector devices (photomultipliers and avalanche\nphotodiodes). \n\n"}
{"id": "0706.2641", "contents": "Title: Performance Studies of Prototype II for the CASTOR forward Calorimeter\n  at the CMS Experiment Abstract: We present results of the performance of the second prototype of the CASTOR\nquartz-tungsten sampling calorimeter, to be installed in the very forward\nregion of the CMS experiment at the LHC. The energy linearity and resolution,\nas well as the spatial resolution of the prototype to electromagnetic and\nhadronic showers are studied with E=20-200 GeV electrons, E=20-350 GeV pions,\nand E=50,150 GeV muons from beam tests carried out at CERN/SPS in 2004. The\nresponses of the calorimeter using two different types of photodetectors\n(avalanche photodiodes APDs, and photomultiplier tubes PMTs) are compared. \n\n"}
{"id": "0707.1489", "contents": "Title: Is there an eta-3He quasi--bound state ? Abstract: The observed variation of the total cross section for the dp -> 3He eta\nreaction near threshold means that the magnitude of the s--wave amplitude falls\nvery rapidly with the eta centre--of--mass momentum. It is shown here that\nrecent measurements of the momentum dependence of the angular distribution\nimply a strong variation also in the phase of this amplitude. Such a behaviour\nis that expected from a quasi--bound or virtual eta-3He state. The\ninterpretation can be investigated further through measurements of the deuteron\nor proton analysing powers and/or spin--correlations. \n\n"}
{"id": "0708.1076", "contents": "Title: Early evolution of transversally thermalized partons Abstract: The idea that the parton system created in relativistic heavy-ion collisions\n(i) emerges in a state with transverse momenta close to thermodynamic\nequilibrium and (ii) its evolution at early times is dominated by the\n2-dimensional (transverse) hydrodynamics of the ideal fluid is investigated. It\nis argued that this mechanism may help to solve the problem of early\nequilibration. \n\n"}
{"id": "0708.2251", "contents": "Title: First real time detection of Be7 solar neutrinos by Borexino Abstract: This paper reports a direct measurement of the Be7 solar neutrino signal rate\nperformed with the Borexino low background liquid scintillator detector. This\nis the first real-time spectral measurement of sub-MeV solar neutrinos. The\nresult for 0.862 MeV Be7 is 47 +- 7 (stat} +- 12 (sys} counts/(day x 100 ton),\nconsistent with predictions of Standard Solar Models and neutrino oscillations\nwith LMA-MSW parameters. \n\n"}
{"id": "0710.0118", "contents": "Title: Pion and kaon production in central Pb+Pb collisions at 20A and 30A GeV:\n  Evidence for the onset of deconfinement Abstract: Results on charged pion and kaon production in central Pb+Pb collisions at\n20A and 30A GeV are presented and compared to data at lower and higher\nenergies. A rapid change of the energy dependence is observed around 30A GeV\nfor the yields of pions and kaons as well as for the shape of the transverse\nmass spectra. The change is compatible with the prediction that the threshold\nfor production of a state of deconfined matter at the early stage of the\ncollisions is located at low SPS energies. \n\n"}
{"id": "0801.4555", "contents": "Title: Energy dependence of pi-zero production in Cu+Cu collisions at\n  sqrt(s_NN) = 22.4, 62.4, and 200 GeV Abstract: Neutral pion transverse momentum (pT) spectra at mid-rapidity (|y| < 0.35)\nwere measured in Cu+Cu collisions at \\sqrt s_NN = 22.4, 62.4, and 200 GeV.\nRelative to pi -zero yields in p+p collisions scaled by the number of inelastic\nnucleon-nucleon collisions (Ncoll) at the respective energies, the pi-zero\nyields for pT \\ge 2 GeV/c in central Cu+Cu collisions at 62.4 and 200 GeV are\nsuppressed, whereas an enhancement is observed at 22.4 GeV. A comparison with a\njet quenching model suggests that final state parton energy loss dominates in\ncentral Cu+Cu collisions at 62.4 GeV and 200 GeV, while the enhancement at 22.4\nGeV is consistent with nuclear modifications in the initial state alone. \n\n"}
{"id": "0806.0358", "contents": "Title: Universal upper bound on the energy of a parton escaping from the\n  strongly coupled quark-gluon matter Abstract: It has been shown through the AdS/CFT correspondence that the energy loss of\na fast quark in a strongly coupled ${\\cal N}=4$ SUSY Yang--Mills matter in the\nlarge N limit is given by the classical Lienard formula. I demonstrate that\nunder quite natural assumptions about the dynamics of heavy ion collisions this\nleads to a universal (i.e. independent of the initial parton energy, but\ndependent on flavor and centrality) upper bound on the energy of the partons\nescaping from the plasma. This bound is a Yang--Mills analog of the Pomeranchuk\nbound in classical electrodynamics, where it is a consequence of radiation in a\nstrong external field acting on a relativistic charge. Since as a result the\nmassive constituent partons are slowed down to a velocity v < c, the angular\ndistribution of the emitted radiation exhibits a broad \"dead cone\". If the\nproperties of conformal and QCD matter at strong coupling are qualitatively\nsimilar, the existence of this universal upper bound would have dramatic\nimplications for heavy ion experiments. \n\n"}
{"id": "0807.3145", "contents": "Title: Application of Hamamatsu MPPC to T2K Neutrino Detectors Abstract: A special type of Hamamatsu MPPC, with a sensitive area of 1.3x1.3mm^2\ncontaining 667 pixels with 50x50um^2 each, has been developed for the near\nneutrino detector in the T2K long baseline neutrino experiment. About 60 000\nMPPCs will be used in total to read out the plastic scintillator detectors with\nwavelength shifting fibers. We report on the basic performance of MPPCs\nproduced for T2K. \n\n"}
{"id": "0809.2049", "contents": "Title: Particle-Number Restoration within the Energy Density Functional\n  formalism: Nonviability of terms depending on noninteger powers of the\n  density matrices Abstract: We discuss the origin of pathological behaviors that have been recently\nidentified in particle-number-restoration calculations performed within the\nnuclear energy density functional framework. A regularization method that\nremoves the problematic terms from the multi-reference energy density\nfunctional and which applies (i) to any symmetry restoration- and/or\ngenerator-coordinate-method-based configuration mixing calculation and (ii) to\nenergy density functionals depending only on integer powers of the density\nmatrices, was proposed in [D. Lacroix, T. Duguet, M. Bender, arXiv:0809.2041]\nand implemented for particle-number restoration calculations in [M. Bender, T.\nDuguet, D. Lacroix, arXiv:0809.2045]. In the present paper, we address the\nviability of non-integer powers of the density matrices in the nuclear energy\ndensity functional. Our discussion builds upon the analysis already carried out\nin [J. Dobaczewski \\emph{et al.}, Phys. Rev. C \\textbf{76}, 054315 (2007)].\nFirst, we propose to reduce the pathological nature of terms depending on a\nnon-integer power of the density matrices by regularizing the fraction that\nrelates to the integer part of the exponent using the method proposed in [D.\nLacroix, T. Duguet, M. Bender, arXiv:0809.2041]. Then, we discuss the spurious\nfeatures brought about by the remaining fractional power. Finally, we conclude\nthat non-integer powers of the density matrices are not viable and should be\navoided in the first place when constructing nuclear energy density functionals\nthat are eventually meant to be used in multi-reference calculations. \n\n"}
{"id": "0902.1377", "contents": "Title: Fundamental Symmetries and Conservation Laws Abstract: I discuss recent progress in low-energy tests of symmetries and conservation\nlaws, including parity nonconservation in atoms and nuclei, electric dipole\nmoment tests of time-reversal invariance, beta-decay correlation studies, and\ndecays violating separate (family) and total lepton number. \n\n"}
{"id": "0903.2273", "contents": "Title: Cosmogenic Production as a Background in Searching for Rare Physics\n  Processes Abstract: We revisit calculations of the cosmogenic production rates for several\nlong-lived isotopes that are potential sources of background in searching for\nrare physics processes such as the detection of dark matter and neutrinoless\ndouble-beta decay. Using updated cosmic-ray neutron flux measurements, we use\nTALYS 1.0 to investigate the cosmogenic activation of stable isotopes of\nseveral detector targets and find that the cosmogenic isotopes produced inside\nthe target materials and cryostat can result in large backgrounds for dark\nmatter searches and neutrinoless double-beta decay. We use previously published\nlow-background HPGe data to constrain the production of $^{3}H$ on the surface\nand the upper limit is consistent with our calculation. We note that cosmogenic\nproduction of several isotopes in various targets can generate potential\nbackgrounds for dark matter detection and neutrinoless double-beta decay with a\nmassive detector, thus great care should be taken to limit and/or deal with the\ncosmogenic activation of the targets. \n\n"}
{"id": "0905.2132", "contents": "Title: Development of Neutron Detectors for the Next Generation of Radioactive\n  Ion-Beam Facilities Abstract: The next generation of radioactive ion beam facilities, which will give\nexperimental access to many exotic nuclei, are presently being developed. These\nfacilities will make it possible to study very short lived exotic nuclei with\nextreme values of isospin far from the line of beta stability. Such nuclei will\nbe produced with very low cross sections and to study them, new detector arrays\nare being developed. At the SPIRAL facility in GANIL a neutron detector array,\nthe Neutron Wall, is located. In this work the Neutron Wall has been\ncharacterized regarding neutron detection efficiency and discrimination between\nneutrons and gamma rays. The possibility to increase the efficiency by\nincreasing the high voltage of the photomultiplier tubes has also been studied.\nFor SPIRAL2 a neutron detector array, NEDA, is being developed. NEDA will\noperate in a high gamma-ray background environment which puts a high demand on\nthe quality of discrimination between neutrons and gamma rays. To increase the\nquality of the discrimination methods pulse-shape discrimination techniques\nutilizing digital electronics have been developed and evaluated regarding bit\nresolution and sampling frequency of the ADC. The conclusion is that an ADC\nwith a bit resolution of 12 bits and a sampling frequency of 100 MS/s is\nadequate for pulse-shape discrimination of neutrons and gamma rays for a\nneutron energy range of 0.3-12 MeV. \n\n"}
{"id": "0907.2172", "contents": "Title: A Xenon Condenser with a Remote Liquid Storage Vessel Abstract: We describe the design and operation of a system for xenon liquefaction in\nwhich the condenser is separated from the liquid storage vessel. The condenser\nis cooled by a pulse tube cryocooler, while the vessel is cooled only by the\nliquid xenon itself. This arrangement facilitates liquid particle detector\nresearch by allowing easy access to the upper and lower flanges of the vessel.\nWe find that an external xenon gas pump is useful for increasing the rate at\nwhich cooling power is delivered to the vessel, and we present measurements of\nthe power and efficiency of the apparatus. \n\n"}
{"id": "0907.3119", "contents": "Title: Physics with the ALICE Electromagnetic Calorimeter Abstract: I will present physics measurements which are achievable in the ALICE\nexperiment at the LHC through the inclusion of a new electromagnetic\ncalorimeter. I will focus on jet measurements in proton proton and heavy ion\ncollisions. Detailed simulations have been performed on jet reconstruction, jet\ntriggering, heavy flavor jet reconstruction through electron identification,\ngamma-jet reconstruction and the measurements of identified hadrons and\nresonances in jets. I will show the physics capabilities which are made\npossible through the combination of calorimeter information with the other\ndetector components in ALICE. \n\n"}
{"id": "0907.3385", "contents": "Title: Study of spin sum rules (and the strong coupling constant at large\n  distances) Abstract: We present recent results from Jefferson Lab on sum rules related to the spin\nstructure of the nucleon. We then discuss how the Bjorken sum rule, with its\nconnection to the Gerasimov-Drell-Hearn sum, allows us to conveniently define\nan effective coupling for the strong force at all distances. \n\n"}
{"id": "0907.3706", "contents": "Title: UV Degradation of the Optical Properties of Acrylic for Neutrino and\n  Dark Matter Experiments Abstract: UV-transmitting (UVT) acrylic is a commonly used light-propagating material\nin neutrino and dark matter detectors as it has low intrinsic radioactivity and\nexhibits low absorption in the detectors' light producing regions, from 350 nm\nto 500 nm. Degradation of optical transmittance in this region lowers light\nyields in the detector, which can affect energy reconstruction, resolution, and\nexperimental sensitivities. We examine transmittance loss as a result of short-\nand long-term UV exposure for a variety of UVT acrylic samples from a number of\nacrylic manufacturers. Significant degradation peaking at 343 nm was observed\nin some UVT acrylics with as little as three hours of direct sunlight, while\nothers exhibited softer degradation peaking at 310 nm over many days of\nexposure to sunlight. Based on their measured degradation results, safe time\nlimits for indoor and outdoor UV exposure of UVT acrylic are formulated. \n\n"}
{"id": "0907.4696", "contents": "Title: Quarkonia measurement in p+p and d+Au collisions at sqrt(s)=200 GeV by\n  PHENIX Detector Abstract: We report new quarkonia measurements necessary to understand production\nmechanisms and cold nuclear matter effects in the yields observed at RHIC\nenergy. Results obtained in p+p collisions collected during the 2006 RHIC Run\ninclude J/psi, Psi' and Upsilon differential cross sections as well as J/psi\npolarization. Revisited interpretations of the published J/psi nuclear\nmodification factors and statistically improved observations in d+Au collisions\ntaken in the 2008 Run are also discussed in the view of the recent\nunderstanding of the initial state effects and breakup cross section. \n\n"}
{"id": "0909.0485", "contents": "Title: Search for the QCD critical point at SPS energies Abstract: Lattice QCD calculations locate the QCD critical point at energies accessible\nat the CERN Super Proton Synchrotron (SPS). We present average transverse\nmomentum and multiplicity fluctuations, as well as baryon and anti-baryon\ntransverse mass spectra which are expected to be sensitive to effects of the\ncritical point. The future CP search strategy of the NA61/SHINE experiment at\nthe SPS is also discussed. \n\n"}
{"id": "0909.2349", "contents": "Title: Electroexcitation of nucleon resonances from CLAS data on single pion\n  electroproduction Abstract: We present results on the electroexcitation of the low mass resonances\nDelta(1232)P33, N(1440)P11, N(1520)D13, and N(1535)S11 in a wide range of Q2.\nThe results were obtained in the comprehensive analysis of JLab-CLAS data on\ndifferential cross sections, longitudinally polarized beam asymmetries, and\nlongitudinal target and beam-target asymmetries for pion electroproduction off\nthe proton. The data were analysed using two conceptually different approaches,\nfixed-t dispersion relations and a unitary isobar model, allowing us to draw\nconclusions on the model sensitivity of the obtained electrocoupling\namplitudes. The amplitudes for the Delta(1232)P33} show the importance of a\nmeson-cloud contribution to quantitatively explain the magnetic dipole\nstrength, as well as the electric and scalar quadrupole transitions. They do\nnot show any tendency of approaching the pQCD regime for Q2<6 GeV2. For the\nRoper resonance, N(1440)P11, the data provide strong evidence for this state as\na predominantly radial excitation of a 3-quark ground state. Measured in pion\nelectroproduction, the transverse helicity amplitude for the N(1535)S11 allowed\nus to obtain the branching ratios of this state to the piN and etaN channels\nvia comparison to the results extracted from eta electroproduction. The\nextensive CLAS data also enabled the extraction of the gamma*p -> N(1520)D13\nand N(1535)S11 longitudinal helicity amplitudes with good precision. \n\n"}
{"id": "0909.2881", "contents": "Title: The Missing Three-Nucleon Forces: Where Are They? Abstract: In recent years, there has been substantial progress in the derivation of\nnuclear forces from chiral effective field theory. Accurate two-nucleon forces\n(2NF) have been constructed up to next-to-next-to-next-to-leading order (N3LO)\nof chiral perturbation theory and applied in microscopic nuclear structure\ncalculations with a good degree of success. However, chiral three-nucleon\nforces (3NF) have been used only at N2LO, improving some miscroscopic\npredictions, but leaving also several issues, like the \"Ay puzzle'\" of\nnucleon-deuteron scattering, unresolved. Thus, the 3NF at N3LO is needed for\nessentially two reasons: For consistency with the 2NF, and to (hopefully)\nimprove some critical predictions of nuclear structure and reactions. However,\nthere are indications that the 3NF at N3LO (in the so-called Delta-less version\nof the theory) is rather weak and may not solve any of the outstanding\nproblems. If this suspicion is confirmed, we have to go beyond, which may be\nsimilar to opening Pandora's Box. In this talk, I will discuss the various\npossible scenarios and how to deal with them. \n\n"}
{"id": "1001.3851", "contents": "Title: Reply to 'Comment for \"Limits on a nucleon-nucleon monopole-dipole\n  (axionlike) P,T-noninvariant interaction from spin relaxation of polarized\n  He-3\" [arXiv:0912.4963], by A.P. Serebrov' Abstract: It is shown, that criticism, presented in [arXiv:0912.4963], is based on an\nelementary error in the calculation of the collision frequency of an atom in a\ngas with walls of a container and misunderstanding of the method used in\n[arXiv:0902.1682v2] for obtaining constraints on new short-range spin-dependent\ninteractions. \n\n"}
{"id": "1002.1738", "contents": "Title: The Isospin Dependence Of The Nuclear Equation Of State Near The\n  Critical Point Abstract: We discuss experimental evidence for a nuclear phase transition driven by the\ndifferent concentration of neutrons to protons. Different ratios of the neutron\nto proton concentrations lead to different critical points for the phase\ntransition. This is analogous to the phase transitions occurring in 4He-3He\nliquid mixtures. We present experimental results which reveal the N/A (or Z/A)\ndependence of the phase transition and discuss possible implications of these\nobservations in terms of the Landau Free Energy description of critical\nphenomena. \n\n"}
{"id": "1004.2318", "contents": "Title: Solar fusion cross sections II: the pp chain and CNO cycles Abstract: We summarize and critically evaluate the available data on nuclear fusion\ncross sections important to energy generation in the Sun and other\nhydrogen-burning stars and to solar neutrino production. Recommended values and\nuncertainties are provided for key cross sections, and a recommended spectrum\nis given for 8B solar neutrinos. We also discuss opportunities for further\nincreasing the precision of key rates, including new facilities, new\nexperimental techniques, and improvements in theory. This review, which\nsummarizes the conclusions of a workshop held at the Institute for Nuclear\nTheory, Seattle, in January 2009, is intended as a 10-year update and\nsupplement to Reviews of Modern Physics 70 (1998) 1265. \n\n"}
{"id": "1004.4884", "contents": "Title: Recent STAR results in high-energy polarized proton-proton collisions at\n  RHIC Abstract: The STAR experiment at the Relativistic Heavy-Ion Collider at Brookhaven\nNational Laboratory is carrying out a spin physics program in high-energy\npolarized $\\vec{p}+\\vec{p}$ collisions at $\\sqrt{s}=200-500\\,$GeV to gain a\ndeeper insight into the spin structure and dynamics of the proton.\n  One of the main objectives of the spin physics program at RHIC is the\nextraction of the polarized gluon distribution function based on measurements\nof gluon initiated processes, such as hadron and jet production. The STAR\ndetector is well suited for the reconstruction of various final states\ninvolving jets, $\\pi^{0}$, $\\pi^{\\pm}$, e$^{\\pm}$ and $\\gamma$, which allows to\nmeasure several different processes. Recent results will be shown on the\nmeasurement of jet production and hadron production at $\\sqrt{s}=200\\,$GeV. The\nRHIC spin physics program has recently completed the first data taking period\nin 2009 of polarized $\\vec{p}+\\vec{p}$ collisions at $\\sqrt{s}=500\\,$GeV. This\nopens a new era in the study of the spin-flavor structure of the proton based\non the production of $W^{-(+)}$ bosons. Recent STAR results on the first\nmeasurement of $W$ boson production in polarized $\\vec{p}+\\vec{p}$ collisions\nwill be shown. \n\n"}
{"id": "1005.2022", "contents": "Title: Radiopurity of Micromegas readout planes Abstract: Micromesh Gas Amplification Structures (Micromegas) are being used in an\nincreasing number of Particle Physics applications since their conception\nfourteen years ago. More recently, they are being used or considered as readout\nof Time Projection Chambers (TPCs) in the field of Rare Event searches (dealing\nwith dark matter, axions or double beta decay). In these experiments, the\nradiopurity of the detector components and surrounding materials is measured\nand finely controlled in order to keep the experimental background as low as\npossible. In the present paper, the first measurement of the radiopurity of\nMicromegas planes obtained by high purity germanium spectrometry in the low\nbackground facilities of the Canfranc Underground Laboratory (LSC) is\npresented. The obtained results prove that Micromegas readouts of the microbulk\ntype are currently manufactured with radiopurity levels below 30 microBq/cm2\nfor Th and U chains and ~60 microBq/cm2 for 40K, already comparable to the\ncleanest detector components of the most stringent low background experiments\nat present. Taking into account that the studied readouts were manufactured\nwithout any specific control of the radiopurity, it should be possible to\nimprove these levels after dedicated development. \n\n"}
{"id": "1005.2957", "contents": "Title: Associated strangeness production in the pp to pK^+K^-p and pp to pK^+\n  pi^0 Sigma^0 reactions Abstract: The total and differential cross sections for associated strangeness\nproduction in the $pp \\to pK^+K^-p$ and $pp \\to pK^+\\pi^0\\Sigma^0$ reactions\nhave been studied in a unified approach using an effective Lagrangian model. It\nis assumed that both the $K^-p$ and $\\pi^0\\Sigma^0$ final states originate from\nthe decay of the $\\Lambda(1405)$ resonance which was formed in the production\nchain $pp\\to p(N^*(1535)\\to K^+\\Lambda(1405))$. The available experimental data\nare well reproduced, especially the ratio of the two total cross sections,\nwhich is much less sensitive to the particular model of the entrance channel.\nThe significant coupling of the $N^*(1535)$ resonance to $\\Lambda(1405) K$ is\nfurther evidence for large $s \\bar{s}$ components in the quark wave function of\nthe $N^*(1535)$ resonance. \n\n"}
{"id": "1006.1113", "contents": "Title: Combined analysis of KamLAND and Borexino neutrino signals from Th and U\n  decays in the Earth's interior Abstract: The KamLAND and Borexino experiments have detected electron antineutrinos\nproduced in the decay chains of natural thorium and uranium (Th and U\ngeoneutrinos). We analyze the energy spectra of current geoneutrino data in\ncombination with solar and long-baseline reactor neutrino data, with\nmarginalized three-neutrino oscillation parameters. We consider the case with\nunconstrained Th and U event rates in KamLAND and Borexino, as well as cases\nwith fewer degrees of freedom, as obtained by successively assuming for both\nexperiments a common Th/U ratio, a common scaling of Th+U event rates, and a\nchondritic Th/U value. In combination, KamLAND and Borexino can reject the null\nhypothesis (no geoneutrino signal) at 5 sigma. Interesting bounds or\nindications emerge on the Th+U geoneutrino rates and on the Th/U ratio, in\nbroad agreement with typical Earth model expectations. Conversely, the results\ndisfavor the hypothesis of a georeactor in the Earth's core, if its power\nexceeds a few TW. The interplay of KamLAND and Borexino geoneutrino data is\nhighlighted. \n\n"}
{"id": "1006.2389", "contents": "Title: The physics of $Z^0/\\gamma^*$-tagged jets at the LHC Abstract: Electroweak bosons produced in conjunction with jets in high-energy collider\nexperiments is one of the principle final-state channels that can be used to\ntest the accuracy of perturbative Quantum Chromodynamics calculations and to\nassess the potential to uncover new physics through comparison between data and\ntheory. In this paper we present results for the $Z^0/\\gamma^*$+jet production\ncross sections at the LHC at leading and next-to-leading orders. In\nproton-proton reactions we elucidate up to ${\\cal O}(G_F\\alpha_s^2)$ the\nconstraints that jet tagging via the $Z^0/\\gamma^*$ decay dileptons provides on\nthe momentum distribution of jets. In nucleus-nucleus reactions we demonstrate\nthat tagged jets can probe important aspects of the dynamics of quark and gluon\npropagation in hot and dense nuclear matter and characterize the properties of\nthe medium-induced parton showers in ways not possible with more inclusive\nmeasurements. Finally, we present specific predictions for the anticipated\nsuppression of the $Z^0/\\gamma^*$+jet production cross section in the\nquark-gluon plasma that is expected to be created in central lead-lead\ncollisions at the LHC relative to the naive superposition of independent\nnucleon-nucleon scatterings. \n\n"}
{"id": "1006.4043", "contents": "Title: Model of the response function of large mass bolometric detectors Abstract: Large mass bolometers are used in particle physics experiments to search for\nrare processes. By operating at low temperature, they are able to detect\nparticle energies from few keV up to several MeV, measuring the temperature\nrise produced by the energy released. This study was performed on the\nbolometers of the CUORE experiment. The response function of these detectors is\nnot linear in the energy range of interest, and it changes with the operating\ntemperature. The non-linearity is found to be dominated by the thermistor and\nits biasing circuit. A method to obtain a linear response is the result of this\nwork. It allows a great simplification of the data analysis. \n\n"}
{"id": "1007.3027", "contents": "Title: Impact of Neutron Decay Experiments on non-Standard Model Physics Abstract: This paper gives a brief overview of the present and expected future limits\non physics beyond the Standard Model (SM) from neutron beta decay, which is\ndescribed by two parameters only within the SM. Since more than two observables\nare accessible, the problem is over-determined. Thus, precise measurements of\ncorrelations in neutron decay can be used to study the SM as well to search for\nevidence of possible extensions to it. Of particular interest in this context\nare the search for right-handed currents or for scalar and tensor interactions.\nPrecision measurements of neutron decay observables address important open\nquestions of particle physics and cosmology, and are generally complementary to\ndirect searches for new physics beyond the SM in high-energy physics. Free\nneutron decay is therefore a very active field, with a number of new\nmeasurements underway worldwide. We present the impact of recent developments. \n\n"}
{"id": "1009.3471", "contents": "Title: Two source emission behaviour of alpha fragments of projectile having\n  energy around 1 GeV per nucleon Abstract: The emission of projectile fragments alpha has been studied in ^{84}Kr\ninteractions with nuclei of the nuclear emulsion detector composition at\nrelativistic energy below 2 GeV per nucleon. The angular distribution of\nprojectile fragments alpha in terms of transverse momentum could not be\nexplained by a straight and clean-cut collision geometry hypothesis of\nParticipant - Spectator (PS) Model. Therefore, it is assumed that projectile\nfragments alpha were produced from two separate sources that belong to the\nprojectile spectator region differing drastically in their temperatures. It has\nbeen clearly observed that the emission of projectile fragments alpha are from\ntwo different sources. The contribution of projectile fragments alpha from\ncontact layer or hot source is a few percent of the total emission of\nprojectile fragments alphas. Most of the projectile fragments alphas are\nemitted from the cold source. It has been noticed that the temperature of hot\nand cold regions are dependent on the projectile mass number. \n\n"}
{"id": "1011.5249", "contents": "Title: Jets, Mach cone, hot spots, ridges, harmonic flow, dihadron and\n  $\\gamma$-hadron correlation in high-energy heavy-ion collisions Abstract: Within the AMPT Monte Carlo model, hot spots in the initial transverse parton\ndensity are shown to lead to harmonic flows which all contribute to dihadron\nazimuthal correlation. The net back-to-back dihadron correlation after\nsubtraction of harmonic flows still has a double-peak that is independent of\nthe initial geometric triangularity at a fixed impact-parameter and unique to\nthe structure of jet-induced Mach cone and expanding hot spots dragged by\nradial flow. The longitudinal structure of hot spots also leads to a near-side\nridge in dihadron correlation with a large rapidity gap. By successively\nrandomizing the azimuthal angle of the transverse momenta and positions of\ninitial partons, one can isolate the effects of jet-induced medium excitation\nand expanding hot spots on the dihadron azimuthal correlation with both small\nand large rapidity gap. The double-peak in the net dihadron correlation is\nquantitatively different from that in $\\gamma$-hadron that is free of the\ncontributions from harmonic flow and hot spots and caused only by jet-induced\nMach cone. \n\n"}
{"id": "1011.6592", "contents": "Title: Axial and Vector Structure Functions for Electron- and Neutrino- Nucleon\n  Scattering Cross Sections at all $Q^2$ using Effective Leading order Parton\n  Distribution Functions Abstract: We construct a model for inelastic neutrino- and electron-nucleon scattering\ncross sections using effective leading order parton distribution functions with\na new scaling variable $\\xi_w$. Non-perturbative effects are well described\nusing the $\\xi_w$ scaling variable, in combination with multiplicative $K$\nfactors at low $Q^2$.Our model describes all inelastic charged lepton-nucleon\nscattering (including resonance) data (HERA/NMC/BCDMS/SLAC/JLab) ranging from\nvery high $Q^2$ to very low $Q^2$ and down to the photo-production region. The\nmodel describes existing inelastic neutrino-nucleon scattering measurements,\nand has been developed to be used in analysis of neutrino oscillation\nexperiments in the few GeV region. \n\n"}
{"id": "1012.4300", "contents": "Title: Signal modeling of high-purity Ge detectors with a small read-out\n  electrode and application to neutrinoless double beta decay search in Ge-76 Abstract: The GERDA experiment searches for the neutrinoless double beta decay of Ge-76\nusing high-purity germanium detectors enriched in Ge-76. The analysis of the\nsignal time structure provides a powerful tool to identify neutrinoless double\nbeta decay events and to discriminate them from gamma-ray induced backgrounds.\nEnhanced pulse shape discrimination capabilities of \"Broad Energy Germanium\"\ndetectors with a small read-out electrode have been recently reported. This\npaper describes the full simulation of the response of such a detector,\nincluding the Monte Carlo modeling of radiation interaction and subsequent\nsignal shape calculation. A pulse shape discrimination method based on the\nratio between the maximum current signal amplitude and the event energy applied\nto the simulated data shows quantitative agreement with the experimental data\nacquired with calibration sources. The simulation has been used to study the\nsurvival probabilities of the decays which occur inside the detector volume and\nare difficult to assess experimentally. Such internal decay events are produced\nby the cosmogenic radio-isotopes Ge-68 and Co-60 and the neutrinoless double\nbeta decay of Ge-76. Fixing the experimental acceptance of the double escape\npeak of the 2.614 MeV photon to 90%, the estimated survival probabilities at\nQbb = 2.039 MeV are (86+-3)% for Ge-76 neutrinoless double beta decays,\n(4.5+-0.3)% for the Ge-68 daughter Ga-68, and (0.9+0.4-0.2)% for Co-60 decays. \n\n"}
{"id": "1101.4000", "contents": "Title: A large HPGe detector for the non-destructive radioassay of an\n  ultra-low-background counting facility Abstract: We present the use of a low background counting facility, equipped with a\np-type 80% relative efficiency HPGe detector, protected by active and passive\nshielding, and large enough to count a 10\" photo-multiplier tube (PMT). A\nGEANT4 Monte-Carlo of this detector was developed and tuned to 3% accuracy. We\nreport the U, Th, and K contents in three different types of PMTs used in\ncurrent neutrino experiments, with accuracies of $\\sim 10$ ppb for U and Th and\nof $\\sim 15$ ppm for K. \n\n"}
{"id": "1102.3819", "contents": "Title: Enhancement of flow anisotropies due to magnetic field in relativistic\n  heavy-ion collisions Abstract: It is known that the presence of background magnetic field in cosmic plasma\ndistorts the acoustic peaks in CMBR. This primarily results from different\ntypes of waves in the plasma with velocities depending on the angle between the\nmagnetic field and the wave vector. We consider the consequences of these\neffects in relativistic heavy-ion collisions where very strong magnetic fields\narise during early stages of the plasma evolution. We show that flow\ncoefficients can be significantly affected by these effects when the magnetic\nfield remains strong during early stages due to strong induced fields in the\nconducting plasma. In particular, the presence of magnetic field can lead to\nenhancement in the elliptic flow coefficient $v_2$. \n\n"}
{"id": "1103.6279", "contents": "Title: Thermal Relaxation of Charm in Hadronic Matter Abstract: The thermal relaxation rate of open-charm ($D$) mesons in hot and dense\nhadronic matter is calculated using empirical elastic scattering amplitudes.\n$D$-meson interactions with thermal pions are approximated by $D^*$ resonances,\nwhile scattering off other hadrons ($K$, $\\eta$, $\\rho$, $\\omega$, $K^*$, $N$,\n$\\Delta$) is evaluated using vacuum scattering amplitudes as available in the\nliterature based on effective Lagrangians and constrained by realistic\nspectroscopy. The thermal relaxation time of $D$-mesons in a hot $\\pi$ gas is\nfound to be around 25-50\\,fm/$c$ for temperatures $T$=150-180\\,MeV, which\nreduces to 10-25\\,fm/$c$ in a hadron-resonance gas. The latter values, argued\nto be conservative estimates, imply significant modifications of $D$-meson\nspectra in heavy-ion collisions. Close to the critical temperature ($T_c$), the\nspatial diffusion coefficient ($D_s$) is surprisingly similar to recent\ncalculations for charm quarks in the Quark-Gluon Plasma using non-perturbative\n$T$-matrix interactions. This suggests a possibly continuous minimum structure\nof $D_s$ around $T_c$. \n\n"}
{"id": "1105.0193", "contents": "Title: A Large-Scale FPGA-Based Trigger and Dead-Time Free DAQ System for the\n  Kaos Spectrometer at MAMI Abstract: The Kaos spectrometer is maintained by the A1 collaboration at the Mainz\nMicrotron MAMI with a focus on the study of (e,e'K^+) coincidence reactions.\nFor its electron-arm two vertical planes of fiber arrays, each comprising\napproximately 10 000 fibers, are operated close to zero degree scattering angle\nand in close proximity to the electron beam. A nearly dead-time free DAQ system\nto acquire timing and tracking information has been installed for this\nspectrometer arm. The signals of 144 multi-anode photomultipliers are collected\nby 96-channel front-end boards, digitized by double-threshold discriminators\nand the signal time is picked up by state-of-the-art F1 time-to-digital\nconverter chips. In order to minimize background rates a sophisticated trigger\nlogic was implemented in newly developed Vuprom modules. The trigger performs\nnoise suppression, signal cluster finding, particle tracking, and coincidence\ntiming, and can be expanded for kinematical matching (e'K^+) coincidences. The\nfull system was designed to process more than 4 000 read-out channels and to\ncope with the high electron flux in the spectrometer and the high count rate\nrequirement of the detectors. It was successfully in-beam tested at MAMI in\n2009. \n\n"}
{"id": "1105.1080", "contents": "Title: Number of Collisions in the Glauber Model and Beyond Abstract: The so called number of hadron-nucleus collisions n_coll(b) at impact\nparameter b, and its integral value N_coll, which are used to normalize the\nmeasured fractional cross section of a hard process, are calculated within the\nGlauber-Gribov theory including the effects of nucleon short-range\ncorrelations. The Gribov inelastic shadowing corrections are summed to all\norders by employing the dipole representation. Numerical calculations are\nperformed at the energies of the BNL Relativistic Heavy Ion Collider (RHIC) and\nCERN Large Hadron Collider (LHC). We found that whereas the Gribov corrections\ngenerally increase the value of N_coll, the inclusion of nucleon correlations,\nacting in the opposite directions, decreases it by a comparable amount. The\ninterplay of the two effects varies with the value of the impact parameter. \n\n"}
{"id": "1105.3275", "contents": "Title: The Ridge from the BFKL evolution and beyond Abstract: We show that the long-range rapidity correlations between the produced\ncharged-hadron pairs from two BFKL parton showers generate considerable\nazimuthal angle correlations. These correlations have no 1/N_c suppression. The\neffect of gluon saturation on these correlations are discussed and we show that\nit is important. We show that a pronounced ridge-like structure emerges by\ngoing from the BFKL to the saturation region. We show that the ridge structure\nat high-energy proton-proton and nucleus-nucleus collisions has the same origin\nand its main feature can be understood due to initial-state effects. Although\nthe effects of final-state interactions in the latter case can be\nnon-negligible. \n\n"}
{"id": "1106.4458", "contents": "Title: Fast Neutron Detection with 6Li-loaded Liquid Scintillator Abstract: We report on the development of a fast neutron detector using a liquid\nscintillator doped with enriched Li-6. The lithium was introduced in the form\nof an aqueous LiCl micro-emulsion with a di-isopropylnaphthalene-based liquid\nscintillator. A Li-6 concentration of 0.15 % by weight was obtained. A 125 mL\nglass cell was filled with the scintillator and irradiated with fission-source\nneutrons. Fast neutrons may produce recoil protons in the scintillator, and\nthose neutrons that thermalize within the detector volume can be captured on\nthe Li-6. The energy of the neutron may be determined by the light output from\nrecoiling protons, and the capture of the delayed thermal neutron reduces\nbackground events. In this paper, we discuss the development of this 6Li-loaded\nliquid scintillator, demonstrate the operation of it in a detector, and compare\nits efficiency and capture lifetime with Monte Carlo simulations. Data from a\nboron-loaded plastic scintillator were acquired for comparison. We also present\na pulse-shape discrimination method for differentiating between electronic and\nnuclear recoil events based on the Matusita distance between a normalized\nobserved waveform and nuclear and electronic recoil template waveforms. The\ndetails of the measurements are discussed along with specifics of the data\nanalysis and its comparison with the Monte Carlo simulation. \n\n"}
{"id": "1107.2335", "contents": "Title: A proposed search for a fourth neutrino with a PBq antineutrino source Abstract: Several observed anomalies in neutrino oscillation data can be explained by a\nhypothetical fourth neutrino separated from the three standard neutrinos by a\nsquared mass difference of a few eV^2. We show that this hypothesis can be\ntested with a PBq (ten kilocurie scale) 144Ce or 106Ru antineutrino beta-source\ndeployed at the center of a large low background liquid scintillator detector.\nIn particular, the compact size of such a source could yield an\nenergy-dependent oscillating pattern in event spatial distribution that would\nunabiguously determine neutrino mass differences and mixing angles. \n\n"}
{"id": "1108.3047", "contents": "Title: BESIII: \"charming\" physics at an e$^+$e$^-$ collider machine Abstract: Despite the successes of the standard model, the non-perturbative dynamics of\nthe strong interaction are not fully understood yet. Charmonium spectroscopy\nserves as an ideal tool to shed light on the dynamics of the strong interaction\nsuch as quark confinement and the generation of hadron masses. The BESIII\ncollaboration studies extensively the strong interaction and various aspects\nthat could shed light on physics beyond the standard model via copious\ne$^+$e$^-$ collisions at the BESIII/BEPCII facility in Beijing, China, in the\ncharmonium mass regime. We present a few of the recent results with the\nemphasis on charmonium spectroscopy studies using 106$\\times10^6$ $\\psi^\\prime$\nevents. \n\n"}
{"id": "1109.0593", "contents": "Title: Error Estimation for Moments Analysis in Heavy-Ion Collision Experiments Abstract: Fluctuations of conserved quantities are predicted to be sensitive to the\ncorrelation length and connected to the thermodynamic susceptibility. Thus,\nmoments of net-baryon, net-charge and net-strangeness have been extensively\nstudied theoretically and experimentally to explore phase structure and bulk\nproperties of QCD matters created in heavy ion collision experiment. As the\nmoments analysis is statistics hungry study, the error estimation is crucial to\nextract physics information from the limited experimental data. In this paper,\nwe will derive the limit distributions and error formula based on Delta theorem\nin statistics for various order moments used in the experimental data analysis.\nThe Monte Carlo simulation is also applied to test the error formula. \n\n"}
{"id": "1109.0760", "contents": "Title: Measuring Parton Energy Loss at RHIC compared to LHC Abstract: The method of measuring $\\hat{x}_h=\\hat{p}_{Ta}/\\hat{p}_{Tt}$, the ratio of\nthe away-parton transverse momentum, $\\hat{p}_{T_a}$, to the trigger-parton\ntransverse momentum, $\\hat{p}_{T_t}$, using two-particle correlations at RHIC,\nwill be reviewed. This measurement is simply related to the two new variables\nintroduced at LHC for the di-jet fractional transverse momentum imbalance:\nATLAS $A_J=(\\hat{p}_{Tt}-\\hat{p}_{Ta})/(\\hat{p}_{Tt}+\\hat{p}_{Ta})=\n(1-\\hat{x}_h)/(1+\\hat{x}_h)$; and CMS\n$\\mean{(\\hat{p}_{Tt}-\\hat{p}_{Ta})/\\hat{p}_{Tt}}= \\mean{1-\\hat{x}_h}$. Results\nfrom two-particle correlations at RHIC for $\\hat{x}_h$ in p-p and A+A\ncollisions will be reviewed and new results will be presented and compared to\nLHC results. The importance of comparing any effect in A+A collisions to the\nsame effect effect in p-p collisions will be illustrated and emphasized. \n\n"}
{"id": "1109.1046", "contents": "Title: Xenon purity analysis for EXO-200 via mass spectrometry Abstract: We describe purity measurements of the natural and enriched xenon stockpiles\nused by the EXO-200 double beta decay experiment based on a mass spectrometry\ntechnique. The sensitivity of the spectrometer is enhanced by several orders of\nmagnitude by the presence of a liquid nitrogen cold trap, and many impurity\nspecies of interest can be detected at the level of one part-per-billion or\nbetter. We have used the technique to screen the EXO-200 xenon before, during,\nand after its use in our detector, and these measurements have proven useful.\nThis is the first application of the cold trap mass spectrometry technique to\nan operating physics experiment. \n\n"}
{"id": "1109.4909", "contents": "Title: Sparse Online Low-Rank Projection and Outlier Rejection (SOLO) for 3-D\n  Rigid-Body Motion Registration Abstract: Motivated by an emerging theory of robust low-rank matrix representation, in\nthis paper, we introduce a novel solution for online rigid-body motion\nregistration. The goal is to develop algorithmic techniques that enable a\nrobust, real-time motion registration solution suitable for low-cost, portable\n3-D camera devices. Assuming 3-D image features are tracked via a standard\ntracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape\nrepresentation of the rigid body. Robust PCA finds the global optimal solution\nof the initialization, while its complexity is comparable to singular value\ndecomposition. In the online update stage, we propose a more efficient\nalgorithm for sparse subspace projection to sequentially project new feature\nobservations onto the shape subspace. The lightweight update stage guarantees\nthe real-time performance of the solution while maintaining good registration\neven when the image sequence is contaminated by noise, gross data corruption,\noutlying features, and missing data. The state-of-the-art accuracy of the\nsolution is validated through extensive simulation and a real-world experiment,\nwhile the system enjoys one to two orders of magnitude speed-up compared to\nwell-established RANSAC solutions. The new algorithm will be released online to\naid peer evaluation. \n\n"}
{"id": "1110.3341", "contents": "Title: The Nuclear Matter Symmetry Energy at $0.03\\leq \\rho/\\rho_0\\leq 0.2$ Abstract: Measurements of the density dependence of the free symmetry energy in low\ndensity clustered matter have been extended using the NIMROD multi-detector at\nTexas A&M University. Thermal coalescence models were employed to extract\ndensities, $\\rho$, and temperatures, $T$, for evolving systems formed in\ncollisions of 47 $A$ MeV $^{40}$Ar + $^{112}$Sn,$^{124}$Sn and $^{64}$Zn +\n$^{112}$Sn, $^{124}$Sn. Densities of $0.03 \\leq \\rho/\\rho_0 \\leq 0.2$ and\ntemperatures in the range 5 to 10 MeV have been sampled. The free symmetry\nenergy coefficients are found to be in good agreement with values calculated\nusing a quantum statistical model. Values of the corresponding symmetry energy\ncoefficient are derived from the data using entropies derived from the model. \n\n"}
{"id": "1110.6370", "contents": "Title: An analytic technique for the estimation of the light yield of a\n  scintillation detector Abstract: A simple model for the estimation of the light yield of a scintillation\ndetector is developed under general assumptions and relying exclusively on the\nknowledge of its optical properties. The model allows to easily incorporate\neffects related to Rayleigh scattering and absorption of the photons.The\npredictions of the model are benchmarked with the outcomes of Monte Carlo\nsimulations of specific scintillation detectors. An accuracy at the level of\nfew percent is achieved. The case of a real liquid argon based detector is\nexplicitly treated and the predicted light yield is compared with the measured\nvalue. \n\n"}
{"id": "1111.1892", "contents": "Title: Jet studies with STAR at RHIC: jet algorithms, jet shapes, jets in AA Abstract: Hard scattered partons are predicted to be well calibrated probes of the hot\nand dense medium produced in heavy ion collisions. Interactions of these\npartons with the medium w ill result in modifications of internal jet structure\nin Au+Au events compared to that observed in the p+p/d+Au reference. Full jet\nreconstruction is a promising tool to measu re these effects without the\nsignificant biases present in measurements with high-$\\pT$ hadrons.\n  One of the most significant challenges for jet reconstruction in the heavy\nion environment comes from the correct characterization of the background\nfluctuations. The jet mome ntum irresolution due to background fluctuations has\nto be understood in order to recover the correct jet spectrum. Recent progress\nin jet reconstruction methodology is discu ssed, as well as recent measurements\nfrom p+p, d+Au and Au+Au collisions at $\\sqrt{s_\\mathrm{NN}}=200 \\gev$. \n\n"}
{"id": "1112.0915", "contents": "Title: Collective flow in p-Pb and d-Pb collisions at TeV energies Abstract: We apply the hydrodynamic model for the dynamics of matter created in p-Pb\ncollisions at 4.4TeV and d-Pb collisions at 3.11TeV. The fluctuating initial\nconditions are calculated in the Glauber Monte-Carlo model for several\ncentrality classes. The expansion is performed event by event in\n3+1-dimensional viscous hydrodynamics. Noticeable elliptic and triangular flows\nappear in the distributions of produced particles. \n\n"}
{"id": "1112.2212", "contents": "Title: Nucleon and Roper electromagnetic elastic and transition form factors Abstract: We compute nucleon and Roper e.m. elastic and transition form factors using a\nsymmetry-preserving treatment of a contact-interaction. Obtained thereby, the\ne.m. interactions of baryons are typically described by hard form factors. In\ncontrasting this behaviour with that produced by a momentum-dependent\ninteraction, one achieves comparisons which highlight that elastic scattering\nand resonance electroproduction experiments probe the infrared evolution of\nQCD's running masses; e.g., the existence, and location if so, of a zero in the\nratio of nucleon Sachs form factors are strongly influenced by the running of\nthe dressed-quark mass. In our description of baryons, diquark correlations are\nimportant. These correlations are instrumental in producing a zero in the Dirac\nform factor of the proton's d-quark; and in determining d_v/u_v(x=1), as we\nshow via a formula that expresses d_v/u_v(x=1) in terms of the nucleon's\ndiquark content. The contact interaction produces a first excitation of the\nnucleon that is constituted predominantly from axial-vector diquark\ncorrelations. This impacts greatly on the gamma*p->P_{11}(1440) form factors.\nNotably, our quark core contribution to F_2*(Q^2) exhibits a zero at\nQ^2~0.5mN^2. Faddeev equation treatments of a hadron's quark core usually\nunderestimate its magnetic properties, hence we consider the effect produced by\na dressed-quark anomalous e.m. moment. Its inclusion much improves agreement\nwith experiment. On the domain 0<Q^2<2GeV^2, meson-cloud effects are important\nin making a realistic comparison between experiment and hadron structure\ncalculations. Our computed helicity amplitudes are similar to the bare\namplitudes in coupled-channels analyses of the electroproduction process. Thus\nsupports a view that extant structure calculations should directly be compared\nwith the bare-couplings, etc., determined via coupled-channels analyses. \n\n"}
{"id": "1201.3366", "contents": "Title: Collective perspective on advances in Dyson-Schwinger Equation QCD Abstract: We survey contemporary studies of hadrons and strongly interacting quarks\nusing QCD's Dyson-Schwinger equations, addressing: aspects of confinement and\ndynamical chiral symmetry breaking; the hadron spectrum; hadron elastic and\ntransition form factors, from small- to large-Q^2; parton distribution\nfunctions; the physics of hadrons containing one or more heavy quarks; and\nproperties of the quark gluon plasma. \n\n"}
{"id": "1201.4094", "contents": "Title: GeV Photon Beams for Nuclear/Particle Physics Abstract: Production of a GeV photon beam by laser backward-Compton scattering has been\nplaying an important role as a tool for nuclear and particle physics\nexperiments. Its production techniques are now established at electron storage\nrings, which are increasing worldwide. A typical photon intensity has reached\n$\\sim$ 10 $^6$ sec$^{-1}$. In the present article, the LEPS beamline facility\nat SPring-8 is mainly described with an overview of experimental applications,\nfor the purpose to summarize the GeV photon beam production. Finally, possible\nfuture upgrades are discussed with new developments of laser injection. \n\n"}
{"id": "1202.1255", "contents": "Title: The Qweak Experiment: A Search for New Physics at the TeV Scale via a\n  Measurement of the Proton's Weak Charge Abstract: We propose a new precision measurement of parity-violating electron\nscattering on the proton at very low Q^2 and forward angles to challenge\npredictions of the Standard Model and search for new physics. A unique\nopportunity exists to carry out the first precision measurement of the proton's\nweak charge, $Q_W =1 - 4\\sin^2\\theta_W$. A 2200 hour measurement of the parity\nviolating asymmetry in elastic ep scattering at Q^2=0.03 (GeV/c)^2 employing\n180 $\\mu$A of 85% polarized beam on a 35 cm liquid Hydrogen target will\ndetermine the proton's weak charge with approximately 4% combined statistical\nand systematic errors. The Standard Model makes a firm prediction of $Q_W$,\nbased on the running of the weak mixing angle from the Z0 pole down to low\nenergies, corresponding to a 10 sigma effect in this experiment. \n\n"}
{"id": "1202.2262", "contents": "Title: Quark-to-gluon composition of the quark-gluon plasma in relativistic\n  heavy-ion collisions Abstract: We study the evolution of the quark-gluon composition of the plasma created\nin ultra-Relativistic Heavy-Ion Collisions (uRHIC's) employing a partonic\ntransport theory that includes both elastic and inelastic collisions plus a\nmean fields dynamics associated to the widely used quasi-particle model. The\nlatter, able to describe lattice QCD thermodynamics, implies a \"chemical\"\nequilibrium ratio between quarks and gluons strongly increasing as\n$T\\rightarrow T_c$, the phase transition temperature. Accordingly we see in\nrealistic simulations of uRHIC's a rapid evolution from a gluon dominated\ninitial state to a quark dominated plasma close to $T_c$. The quark to gluon\nratio can be modified by about a factor of $\\sim 20$ in the bulk of the system\nand appears to be large also in the high $p_T$ region.\n  We discuss how this aspect, often overflown, can be important for a\nquantitative study of several key issues in the QGP physics: shear viscosity,\njet quenching, quarkonia suppression. Furthermore a bulk plasma made by more\nthan $80\\%$ of quarks plus antiquarks provides a theoretical basis for\nhadronization via quark coalescence. \n\n"}
{"id": "1202.3679", "contents": "Title: Production of Photons and Dileptons in the Glasma Abstract: We study the production of photons and dileptons during the pre-equilibrium\nGlasma stage in heavy ion collisions and discuss the implications in light of\nthe PHENIX data. We find that the measured distributions of such\nelectromagnetic emissions, while having some features not well understood if\nhypothesized to entirely arise from a thermalized Quark-Gluon Plasma, have some\nqualitative features that might be described after including effects from a\nthermalizing Glasma. The shape and centrality dependence of the transverse\nmomentum spectra of the so-called \"thermal photons\" are well described. The\nmass and transverse momentum dependence of intermediate mass dileptons also\nagree with our estimates. The low transverse momenta from which the excessive\ndileptons (in low to intermediate mass region) arise is suggestive of emissions\nfrom a Bose condensate. We also predict the centrality dependence of dilepton\nproduction. Uncertainties in the current approach and improvements in the\nfuture are discussed. \n\n"}
{"id": "1203.0781", "contents": "Title: Posterior Mean Super-Resolution with a Compound Gaussian Markov Random\n  Field Prior Abstract: This manuscript proposes a posterior mean (PM) super-resolution (SR) method\nwith a compound Gaussian Markov random field (MRF) prior. SR is a technique to\nestimate a spatially high-resolution image from observed multiple\nlow-resolution images. A compound Gaussian MRF model provides a preferable\nprior for natural images that preserves edges. PM is the optimal estimator for\nthe objective function of peak signal-to-noise ratio (PSNR). This estimator is\nnumerically determined by using variational Bayes (VB). We then solve the\nconjugate prior problem on VB and the exponential-order calculation cost\nproblem of a compound Gaussian MRF prior with simple Taylor approximations. In\nexperiments, the proposed method roughly overcomes existing methods. \n\n"}
{"id": "1203.3604", "contents": "Title: Search for the $\\Theta^{+}$ pentaquark via the $\\pi^-p\\to K^-X$ reaction\n  at 1.92 GeV/$c$ Abstract: The $\\Theta^+$ pentaquark baryon was searched for via the $\\pi^-p\\to K^-X$\nreaction in a missing-mass resolution of 1.4 MeV/$c^2$(FWHM) at J-PARC. $\\pi^-$\nmeson beams were incident on the liquid hydrogen target with the beam momentum\nof 1.92 GeV/$c$. No peak structure corresponding to the $\\Theta^+$ mass was\nobserved. The upper limit of the production cross section averaged over the\nscattering angle of 2$^{\\circ}$ to 15$^{\\circ}$ in the laboratory frame was\nobtained to be 0.26 $\\mu$b/sr in the mass region of 1.51$-$1.55 GeV/$c^2$.The\nupper limit of the $\\Theta^+$ decay width using the effective Lagrangian\napproach was obtained to be 0.72 MeV/$c^2$ and 3.1 MeV/$c^2$ for\n$J^P_{\\Theta}=1/2^+$ and $J^P_{\\Theta}=1/2^-$, respectively. \n\n"}
{"id": "1204.6413", "contents": "Title: The lead-glass electromagnetic calorimeters for the magnetic\n  spectrometers in Hall C at Jefferson Lab Abstract: The electromagnetic calorimeters of the various magnetic spectrometers in\nHall C at Jefferson Lab are presented. For the existing HMS and SOS\nspectrometers design considerations, relevant construction information, and\ncomparisons of simulated and experimental results are included. The energy\nresolution of the HMS and SOS calorimeters is better than $\\sigma/E \\sim\n6%/\\sqrt E $, and pion/electron ($\\pi/e$) separation of about 100:1 has been\nachieved in energy range 1 -- 5 GeV. Good agreement has been observed between\nthe experimental and simulated energy resolutions, but simulations\nsystematically exceed experimentally determined $\\pi^-$ suppression factors by\nclose to a factor of two. For the SHMS spectrometer presently under\nconstruction details on the design and accompanying GEANT4 simulation efforts\nare given. The anticipated performance of the new calorimeter is predicted over\nthe full momentum range of the SHMS. Good electron/hadron separation is\nanticipated by combining the energy deposited in an initial (preshower)\ncalorimeter layer with the total energy deposited in the calorimeter. \n\n"}
{"id": "1204.6433", "contents": "Title: High Precision Measurements of the Pion Proton Differential Cross\n  Section Abstract: Study of the elastic scattering can produce a rich information on the\ndynamics of the strong interaction. The EPECUR collaboration is aimed at the\nresearch of baryon resonances in the second resonance region via pion-proton\nelastic scattering and kaon-lambda production. The experiment features high\nstatistics and better than 1 MeV resolution in the invariant mass thus allowing\nsearches for narrow resonances with the coupling to the pi p channel as low as\n5%. The experiment is of \"formation\" type, i.e. the resonances are produced in\ns-channel and the scan over the invariant mass is done by the variation of the\nincident pion momentum which is measured with the accuracy of 0.1% with a set\nof 1 mm pitch proportional chambers located in the first focus of the beam\nline. The reaction is identified by a magnetless spectrometer based on wire\ndrift chambers with a hexagonal structure. Background suppression in this case\ndepends on the angular resolution, so the amount of matter in the chambers and\nthe setup was minimized to reduce multiple scattering.\n  The measurements started in 2009 with the setup optimized for elastic\npion-proton scattering. With 3 billions of triggers already recorded the\ndifferential cross section of the elastic pi p-scattering on a liquid hydrogen\ntarget in the region of the diffraction minimum is measured with statistical\naccuracy about 1% in 1 MeV steps in terms of the invariant mass. The paper\ncovers the experimental setup, current status and some preliminary results. \n\n"}
{"id": "1205.0649", "contents": "Title: Theory of neutrinoless double beta decay Abstract: Neutrinoless double beta decay, which is a very old and yet elusive process,\nis reviewed. Its observation will signal that lepton number is not conserved\nand the neutrinos are Majorana particles. More importantly it is our best hope\nfor determining the absolute neutrino mass scale at the level of a few tens of\nmeV. To achieve the last goal certain hurdles have to be overcome involving\nparticle, nuclear and experimental physics. Nuclear physics is important for\nextracting the useful information from the data. One must accurately evaluate\nthe relevant nuclear matrix elements, a formidable task. To this end, we review\nthe sophisticated nuclear structure approaches recently been developed, which\ngive confidence that the needed nuclear matrix elements can be reliably\ncalculated. From an experimental point of view it is challenging, since the\nlife times are long and one has to fight against formidable backgrounds. If a\nsignal is found, it will be a tremendous accomplishment. Then, of course, the\nreal task is going to be the extraction of the neutrino mass from the\nobservations. This is not trivial, since current particle models predict the\npresence of many mechanisms other than the neutrino mass, which may contribute\nor even dominate this process. We will, in particular, consider the following\nprocesses: (i)The neutrino induced, but neutrino mass independent contribution.\n(ii)Heavy left and/or right handed neutrino mass contributions.\n(iii)Intermediate scalars (doubly charged etc). (iv)Supersymmetric (SUSY)\ncontributions. We will show that it is possible to disentangle the various\nmechanisms and unambiguously extract the important neutrino mass scale, if all\nthe signatures of the reaction are searched in a sufficient number of nuclear\nisotopes. \n\n"}
{"id": "1205.6427", "contents": "Title: Towards energy resolution at the statistical limit from a negative ion\n  time projection chamber Abstract: We make a proof-of-principle demonstration that improved energy resolution\ncan be obtained in a negative-ion time projection chamber, by individually\ncounting each electron produced by ionizing radiation. \n\n"}
{"id": "1207.3695", "contents": "Title: On some fundamental peculiarities of the traveling wave reactor Abstract: On the basis of the condition for nuclear burning wave existence in the\nneutron-multiplicating media (U-Pu and Th-U cycles) we show the possibility of\nsurmounting the so-called dpa-parameter problem, and suggest an algorithm of\nthe optimal nuclear burning wave mode adjustment, which is supposed to yield\nthe wave parameters (fluence/neutron flux, width and speed of nuclear burning\nwave) that satisfy the dpa-condition associated with the tolerable level of the\nreactor materials radioactive stability, in particular that of the cladding\nmaterials.\n  It is shown for the first time that the capture and fission cross-sections of\n$^{238}$U and $^{239}$Pu increase with temperature within 1000-3000K range,\nwhich under certain conditions may lead to a global loss of the nuclear burning\nwave stability. Some variants of the possible stability loss due to the\nso-called blow-up modes (anomalous nuclear fuel temperature and neutron flow\nevolution) are discussed and are found to possibly become a reason for a\ntrivial violation of the traveling wave reactor internal safety. \n\n"}
{"id": "1208.2710", "contents": "Title: Multichannel parametrization of \\pi N scattering amplitudes and\n  extraction of resonance parameters Abstract: We present results of a new multichannel partial-wave analysis for \\pi N\nscattering in the c.m. energy range 1080 to 2100 MeV. This work explicitly\nincludes \\eta N and K \\Lambda channels and the single pion photoproduction\nchannel. Resonance parameters were extracted by fitting partial-wave amplitudes\nfrom all considered channels using a multichannel parametrization that is\nconsistent with S-matrix unitarity. The resonance parameters so obtained are\ncompared to predictions of quark models. \n\n"}
{"id": "1208.4391", "contents": "Title: Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev\n  Descent Abstract: We present a method to track the precise shape of an object in video based on\nnew modeling and optimization on a new Riemannian manifold of parameterized\nregions.\n  Joint dynamic shape and appearance models, in which a template of the object\nis propagated to match the object shape and radiance in the next frame, are\nadvantageous over methods employing global image statistics in cases of complex\nobject radiance and cluttered background. In cases of 3D object motion and\nviewpoint change, self-occlusions and dis-occlusions of the object are\nprominent, and current methods employing joint shape and appearance models are\nunable to adapt to new shape and appearance information, leading to inaccurate\nshape detection. In this work, we model self-occlusions and dis-occlusions in a\njoint shape and appearance tracking framework.\n  Self-occlusions and the warp to propagate the template are coupled, thus a\njoint problem is formulated. We derive a coarse-to-fine optimization scheme,\nadvantageous in object tracking, that initially perturbs the template by coarse\nperturbations before transitioning to finer-scale perturbations, traversing all\nscales, seamlessly and automatically. The scheme is a gradient descent on a\nnovel infinite-dimensional Riemannian manifold that we introduce. The manifold\nconsists of planar parameterized regions, and the metric that we introduce is a\nnovel Sobolev-type metric defined on infinitesimal vector fields on regions.\nThe metric has the property of resulting in a gradient descent that\nautomatically favors coarse-scale deformations (when they reduce the energy)\nbefore moving to finer-scale deformations.\n  Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and\nbackground show that occlusion/dis-occlusion modeling leads to superior shape\naccuracy compared to recent methods employing joint shape/appearance models or\nemploying global statistics. \n\n"}
{"id": "1208.5365", "contents": "Title: A Missing and Found Recognition System for Hajj and Umrah Abstract: This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety. \n\n"}
{"id": "1209.4910", "contents": "Title: Measurement of high p_T isolated prompt photons in lead-lead collisions\n  at sqrt(s_NN)=2.76 TeV with the ATLAS detector at the LHC Abstract: Prompt photons are a powerful tool to study heavy ion collisions. Their\nproduction rates provide access to the initial state parton distribution\nfunctions and also provide a means to calibrate the expected energy of jets\nthat are produced in the medium. The ATLAS detector measures photons with its\nhermetic, longitudinally segmented calorimeter, which gives excellent spatial\nand energy resolutions, and detailed information about the shower shape of each\nmeasured photon. This provides significant rejection against the expected\nbackground from the decays of neutral pions in jets. Rejection against jet\nfragmentation products is further enhanced by requiring candidate photons to be\nisolated. First results on the spectra of isolated prompt photons from a\ndataset with an integrated luminosity of approximately 0.13 nb^-1 of lead-lead\ncollisions at sqrt(s_NN)=2.76 TeV are shown as a function of transverse\nmomentum and centrality. The measured spectra are compared to expectations from\nperturbative QCD calculations. \n\n"}
{"id": "1210.7010", "contents": "Title: Resonance Decay Contributions to Higher-Order Anisotropic Flow\n  Coefficients Abstract: We show that in hydrodynamic simulations for relativistic heavy-ion\ncollisions, strong resonance decay calculations can be performed with fewer\nspecies of particle resonances while preserving good accuracy in single\nparticle spectra and flow anisotropies. Such partial resonance calculations\nboost computation efficiency by a factor of 10 which is essential for large\nscale event-by-event simulations. \n\n"}
{"id": "1210.8126", "contents": "Title: The study of quark-gluon matter in high-energy nucleus-nucleus\n  collisions Abstract: A short overview is given on the study of hot matter produced in relativistic\nnucleus-nucleus collisions, with emphasis on recent measurements at the LHC. \n\n"}
{"id": "1210.8218", "contents": "Title: Tracking fast neutrons Abstract: Based on elastic collisions, the linear momentum of a fast neutron can be\nmeasured from as few as two consecutive recoil ion tracks plus the vertex\nposition of the third collision, or `two and half' ion tracks. If the time\ndelay between the first two consecutive ion tracks is also measured, the number\nof ion tracks can be reduced to one and a half. The angular and magnitude\nresolutions are limited by ion range straggling to about ten percent.\nMulti-wire proportional chambers and light-field imaging are discussed for fast\nneutron tracking. Single-charge or single-photon detection sensitivity is\nrequired in either approach. Light-field imaging is free of\ncharge-diffusion-induced image blur, but the limited number of photons\navailable can be a challenge. $^1$H,$^2$H and $^3$He could be used for the\ninitial development of fast neutron trackers based on light-field imaging. \n\n"}
{"id": "1211.2677", "contents": "Title: p-Shell Hypernuclear Spectroscopy at JLab's Hall A and Future Prospects Abstract: The first systematic study of 1p-shell and medium-heavy hypernuclei by\nelectroproduction of strangeness has started at Jefferson Laboratory with the\nexperiments E89-009, E94-107, E01-011, E05-115. The main results obtained in\nHall A and future prospects of the investigation of hypernuclei at Jefferson\nLaboratory regarding the study of the angular dependence of electroproduction\nof strangeness and the possibility of performing the spectroscopy of\n208^Tl_Lambda are reported here. \n\n"}
{"id": "1211.3635", "contents": "Title: Lessons from RHIC for the LHC and vice versa Abstract: For the past decade, measurements of semi-inclusive single identified\nparticle spectra and two particle correlations in p-p and A+A collisions at\nRHIC have produced a treasure trove of results which indicate that the medium\nproduced in Au+Au collisions is a strongly interacting quark gluon liquid in\nwhich both light and heavy quarks are suppressed, presumably by energy loss in\nthe hot, dense medium. These results have been confirmed in Pb+Pb collisions at\nthe LHC along with the addition of measurements of jets and di-jets. Results\nand methods at RHIC and LHC are compared which leads to some interesting\nconclusions. \n\n"}
{"id": "1211.3961", "contents": "Title: Radiopurity control in the NEXT-100 double beta decay experiment:\n  procedures and initial measurements Abstract: The Neutrino Experiment with a Xenon TPC (NEXT) is intended to investigate\nthe neutrinoless double beta decay of 136Xe, which requires a severe\nsuppression of potential backgrounds. An extensive screening and material\nselection process is underway for NEXT since the control of the radiopurity\nlevels of the materials to be used in the experimental set-up is a must for\nrare event searches. First measurements based on Glow Discharge Mass\nSpectrometry and gamma-ray spectroscopy using ultra-low background germanium\ndetectors at the Laboratorio Subterr\\'aneo de Canfranc (Spain) are described\nhere. Activity results for natural radioactive chains and other common\nradionuclides are summarized, being the values obtained for some materials like\ncopper and stainless steel very competitive. The implications of these results\nfor the NEXT experiment are also discussed. \n\n"}
{"id": "1211.6257", "contents": "Title: Impact of a low-energy enhancement in the gamma-ray strength function on\n  the radiative neutron-capture Abstract: A low-energy enhancement of the gamma-ray strength function in several light\nand medium-mass nuclei has been observed recently in 3He-induced reactions. The\neffect of this enhancement on (n,gamma) cross-sections is investigated for\nstable and unstable neutron-rich Fe, Mo and Cd isotopes. Our results indicate\nthat the radiative neutron capture cross sections may increase considerably due\nto the low-energy enhancement when approaching the neutron drip line. This\ncould have non-negligible consequences on r-process nucleosynthesis\ncalculations. \n\n"}
{"id": "1212.0433", "contents": "Title: Compressive Schlieren Deflectometry Abstract: Schlieren deflectometry aims at characterizing the deflections undergone by\nrefracted incident light rays at any surface point of a transparent object. For\nsmooth surfaces, each surface location is actually associated with a sparse\ndeflection map (or spectrum). This paper presents a novel method to\ncompressively acquire and reconstruct such spectra. This is achieved by\naltering the way deflection information is captured in a common Schlieren\nDeflectometer, i.e., the deflection spectra are indirectly observed by the\nprinciple of spread spectrum compressed sensing. These observations are\nrealized optically using a 2-D Spatial Light Modulator (SLM) adjusted to the\ncorresponding sensing basis and whose modulations encode the light deviation\nsubsequently recorded by a CCD camera. The efficiency of this approach is\ndemonstrated experimentally on the observation of few test objects. Further,\nusing a simple parametrization of the deflection spectra we show that relevant\nkey parameters can be directly computed using the measurements, avoiding full\nreconstruction. \n\n"}
{"id": "1212.1840", "contents": "Title: The Polarized H and D Atomic Beam Source for ANKE at COSY-J\\\"ulich Abstract: A polarized atomic beam source was developed for the polarized internal\nstorage-cell gas target at the magnet spectrometer ANKE of COSY-J\\\"ulich. The\nintensities of the beams injected into the storage cell, measured with a\ncompression tube, are $7.5\\cdot 10^{16}$ hydrogen atoms/s (two hyperfine\nstates) and $3.9\\cdot 10^{16}$ deuterium atoms/s (three hyperfine states). For\nthe hydrogen beam the achieved vector polarizations are $p_{\\rm\nz}\\approx\\pm0.92$. For the deuterium beam, the obtained combinations of vector\nand tensor ($p_{\\rm zz}$) polarizations are $p_{\\rm z}\\approx\\pm 0.90$ (with a\nconstant $p_{\\rm zz}\\approx +0.86$), and $p_{\\rm zz}=+0.90$ or $p_{\\rm\nzz}=-1.71$ (both with vanishing $p_{\\rm z}$). The paper includes a detailed\ntechnical description of the apparatus and of the investigations performed\nduring the development. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1212.4050", "contents": "Title: Suitability of high-pressure xenon as scintillator for gamma ray\n  spectroscopy Abstract: In this paper we report the experimental study of high-pressure xenon used as\na scintillator, in the context of developing a gamma ray detector. We measure a\nlight yield near 2 photoelectrons per keV for xenon at 40 bar. Together with\nthe light yield, we also measured an energy resolution of ~9% (FWHM) at 662\nkeV, dominated by the statistical fluctuations in the number of photoelectrons. \n\n"}
{"id": "1212.4724", "contents": "Title: Event Identification in $^3$He Proportional Counters Using Risetime\n  Discrimination Abstract: We present a straightforward method for particle identification and\nbackground rejection in $^3$He proportional counters for use in neutron\ndetection. By measuring the risetime and pulse height of the preamplifier\nsignals, one may define a region in the risetime versus pulse height space\nwhere the events are predominately from neutron interactions. For six\nproportional counters surveyed in a low-background environment, we demonstrate\nthe ability to reject alpha-particle events with an efficiency of 99%. By\napplying the same method, we also show an effective rejection of microdischarge\nnoise events that, when passed through a shaping amplifier, are\nindistinguishable from physical events in the counters. The primary application\nof this method is in measurements where the signal-to-background for counting\nneutrons is very low, such as in underground laboratories. \n\n"}
{"id": "1212.5016", "contents": "Title: Ultra-stable implanted 83Rb/83mKr electron sources for the energy scale\n  monitoring in the KATRIN experiment Abstract: The KATRIN experiment aims at the direct model-independent determination of\nthe average electron neutrino mass via the measurement of the endpoint region\nof the tritium beta decay spectrum. The electron spectrometer of the MAC-E\nfilter type is used, requiring very high stability of the electric filtering\npotential. This work proves the feasibility of implanted 83Rb/83mKr calibration\nelectron sources which will be utilised in the additional monitor spectrometer\nsharing the high voltage with the main spectrometer of KATRIN. The source\nemploys conversion electrons of 83mKr which is continuously generated by 83Rb.\nThe K-32 conversion line (kinetic energy of 17.8 keV, natural line width of 2.7\neV) is shown to fulfill the KATRIN requirement of the relative energy stability\nof +/-1.6 ppm/month. The sources will serve as a standard tool for continuous\nmonitoring of KATRIN's energy scale stability with sub-ppm precision. They may\nalso be used in other applications where the precise conversion lines can be\nseparated from the low energy spectrum caused by the electron inelastic\nscattering in the substrate. \n\n"}
{"id": "1301.1241", "contents": "Title: Comment on recent Strangeness -2 predictions Abstract: It is pointed out that the Chiral Constituent Quark Model (CCQM) interactions\nthat bind the H dibaryon and Lambda-Lambda-3H overbind Lambda-Lambda-6He by\nmore than 4 MeV, thus outdating the CCQM in the strangeness S=-2 sector. \n\n"}
{"id": "1301.4248", "contents": "Title: Random coincidence of $2\\nu2\\beta$ decay events as a background source\n  in bolometric $0\\nu2\\beta$ decay experiments Abstract: Two neutrino double $\\beta$ decay can create irremovable background even in\nhigh energy resolution detectors searching for neutrinoless double $\\beta$\ndecay due to random coincidence of $2\\nu2\\beta$ events in case of poor time\nresolution. Possibilities to suppress this background in cryogenic\nscintillating bolometers are discussed. It is shown that the present bolometric\ndetector technologies enable to control this form of background at the level\nrequired to explore the inverted hierarchy of the neutrino mass pattern,\nincluding the case of bolometers searching for the neutrinoless double $\\beta$\ndecay of $^{100}$Mo, which is characterized by a relatively short two neutrino\ndouble $\\beta$ decay half-life. \n\n"}
{"id": "1301.5557", "contents": "Title: First LHC results on coherent J/psi photoproduction in ultra-peripheral\n  Pb-Pb collisions at sqrt{s_NN} = 2.76 TeV Abstract: The first LHC measurement on ultra-peripheral heavy-ion collisions was\ncarried out with the ALICE experiment. In this paper, ALICE results on\nexclusive J/psi studies in Pb-Pb collisions at sqrt(s_NN) = 2.76 TeV, in the\nrapidity region -3.6 < y < -2.6, are given. The coherent J/psi cross section\nwas found to be dsigma/dy_coh_J/\\psi = 1.00 +/- 0.18 (stat) +0.24 -0.26 (syst)\nmb. These studies favour theoretical models that include strong modifications\nto the nuclear gluon density, also known as nuclear gluon shadowing. \n\n"}
{"id": "1302.4277", "contents": "Title: HEROICA: an Underground Facility for the Fast Screening of Germanium\n  Detectors Abstract: An infrastructure to characterize germanium detectors has been designed and\nconstructed at the HADES Underground Research Laboratory, located in Mol\n(Belgium). Thanks to the 223m overburden of clay and sand, the muon flux is\nlowered by four orders of magnitude. This natural shield minimizes the exposure\nof radio-pure germanium material to cosmic radiation resulting in a significant\nsuppression of cosmogenic activation in the germanium detectors. The project\nhas been strongly motivated by a special production of germanium detectors for\nthe GERDA experiment. GERDA, currently collecting data at the Laboratori\nNazionali del Gran Sasso of INFN, is searching for the neutrinoless double beta\ndecay of 76Ge. In the near future, GERDA will increase its mass and sensitivity\nby adding new Broad Energy Germanium (BEGe) detectors. The production of the\nBEGe detectors is done at Canberra in Olen (Belgium), located about 30km from\nthe underground test site. Therefore, HADES is used both for storage of the\ncrystals over night, during diode production, and for the characterization\nmeasurements. A full quality control chain has been setup and tested on the\nfirst seven prototype detectors delivered by the manufacturer at the beginning\nof 2012. The screening capabilities demonstrate that the installed setup\nfulfills a fast and complete set of measurements on the diodes and it can be\nseen as a general test facility for the fast screening of high purity germanium\ndetectors. The results are of major importance for a future massive production\nand characterization chain of germanium diodes foreseen for a possible next\ngeneration 1-tonne double beta decay experiment with 76Ge. \n\n"}
{"id": "1302.4324", "contents": "Title: Hall A Annual Report 2012 Abstract: Report over the experimental activities in Hall A at Thomas Jefferson\nNational Accelerator Facility. \n\n"}
{"id": "1302.6441", "contents": "Title: Effect of jets on $v_4/v_2^2$ ratio and constituent quark scaling in\n  relativistic heavy-ion collisions Abstract: The Monte Carlo HYDJET++ model, that combines parametrized hydrodynamics with\njets, is employed to study formation of second v_2 and fourth v_4 components of\nthe anisotropic flow in ultrarelativistic heavy-ion collisions at energies of\nthe Relativistic Heavy Ion Collider (RHIC) and the Large Hadron Collider (LHC),\nsqrt{s}=200 AGeV and sqrt{s}=2.76 ATeV, respectively. It is shown that the\nquenched jets contribute to the soft part of the v_2(p_T) and v_4(p_T) spectra.\nThe jets increase the ratio v_4/v_2^2 thus leading to deviations of the ratio\nfrom the value of 0.5 predicted by the ideal hydrodynamics. Together with the\nevent-by-event fluctuations, the influence of jets can explain quantitatively\nthe ratio v_4/v_2^2 at p_T < 2 GeV/c for both energies and qualitatively the\nrise of its high-p_T tail at LHC. Jets are also responsible for violation of\nthe number-of-constituent-quark (NCQ) scaling at LHC despite the fact that the\nscaling is fulfilled for the hydro-part of particle spectra. \n\n"}
{"id": "1303.1944", "contents": "Title: Ultracold neutron accumulation in a superfluid-helium converter with\n  magnetic multipole reflector Abstract: We analyze accumulation of ultracold neutrons (UCN) in a superfluid-helium\nconverter vessel surrounded by a magnetic multipole reflector. We solved the\nspin-dependent rate equation, employing formulas valid for adiabatic spin\ntransport of trapped UCN in mechanical equilibrium. Results for saturation UCN\ndensities are obtained in dependence of order and strength of the multipolar\nfield. The addition of magnetic storage to neutron optical potentials can\nincrease the density and energy of the low field seeking UCN produced and\nserves to mitigate the effects of wall losses on the source performance. It\nalso can provide a highly polarized sample of UCN without need to polarize the\nneutron beam incident on the converter. This work was performed in preparation\nof the UCN source project SuperSUN at the ILL. \n\n"}
{"id": "1303.4080", "contents": "Title: Performances of a large mass ZnSe bolometer to search for rare events Abstract: Scintillating bolometers of ZnSe are the baseline choice of the LUCIFER\nexperiment, whose aim is to observe the neutrinoless double beta decay of 82Se.\nThe independent read-out of the heat and scintillation signals allows to\nidentify and reject alpha particle interactions, the dominant background source\nfor bolometric detectors. In this paper we report the performances of a ZnSe\ncrystal operated within the LUCIFER R&D. We measured the scintillation yield,\nthe energy resolution and the background in the energy region where the signal\nfrom neutrinoless double beta decay of 82Se is expected with an exposure of 9.4\nkg x days. With a newly developed analysis algorithm we improved the rejection\nof alpha events, and we estimated the increase in energy resolution obtained by\nthe combination of the heat and light signals. For the first time we measured\nthe light emitted by nuclear recoils, and found it to be compatible with zero.\nWe conclude that the discrimination of nuclear recoils from beta/gamma\ninteractions in the WIMPs energy region is possible, but low-noise light\ndetectors are needed. \n\n"}
{"id": "1303.6768", "contents": "Title: Isotopically modified Ge detectors for {\\sc Gerda}: from production to\n  operation Abstract: The \\textsc{Gerda} experiment searches for the neutrinoless double beta\n($0\\nu\\beta$beta$) decay of $^{76}$Ge using high-purity germanium detectors\nmade of material enriched in $^{76}$Ge. For Phase II of the experiment a\nsensitivity for the half life $T_{1/2}^{0\\nu}\\,\\,\\sim2\\cdot10^{26}$ yr is\nenvisioned. Modified Broad Energy Germanium detectors (BEGe) with thick n$^+$\nelectrodes provide the capability to efficiently identify and reject background\nevents, while keeping a large acceptance for the $0\\nu\\beta$beta$-decay signal\nthrough novel pulse-shape discrimination (PSD) techniques.\n  The viability of producing thick-window BEGe-type detectors for the\n\\textsc{Gerda} experiment is demonstrated by testing all the production steps\nfrom the procurement of isotopically modified germanium up to working BEGe\ndetectors. Comprehensive testing of the spectroscopic as well as PSD\nperformance of the \\textsc{Gerda} Phase II prototype BEGe detectors proved that\nthe properties of these detectors are identical to those produced previously\nfrom natural germanium material following the standard production line of the\nmanufacturer.\n  Furthermore, the production of BEGe detectors from a limited amount of\nisotopically modified germanium served to optimize the production, in order to\nmaximize the overall detector mass yield. The results of this test campaign\nprovided direct input for the subsequent production of the enriched germanium\ndetectors. \n\n"}
{"id": "1304.4891", "contents": "Title: Neutrino Absorption in the Earth, Neutrino Cross-Sections, and New\n  Physics Abstract: Large neutrino telescopes can measure the neutrino-nucleon cross-section by\nstudying neutrino absorption in the Earth. \n\n"}
{"id": "1304.6289", "contents": "Title: Characterization of bolometric Light Detectors for rare event searches Abstract: Bolometers have proven to be very good detectors to search for rare processes\nthanks to their excellent energy resolution and their low intrinsic background.\nFurther active background rejection can be obtained by the simultaneous readout\nof the heat and light signals produced by particles interacting in\nscintillating bolometers, as proposed by the LUCIFER experiment. In this\nframework, the choice of the light detector and the optimization of its working\nconditions play a crucial role. In this paper, we report a study of the\nperformances of a Germanium bolometric light detector in terms of signal\namplitude, energy resolution and signal time development. The impact of various\noperational parameters on the detector performances is discussed. \n\n"}
{"id": "1305.1981", "contents": "Title: Bulk Viscosity Effects in Event-by-Event Relativistic Hydrodynamics Abstract: Bulk viscosity effects on the collective flow harmonics in heavy ion\ncollisions are investigated, on an event by event basis, using a newly\ndeveloped 2+1 Lagrangian hydrodynamic code named v-USPhydro which implements\nthe Smoothed Particle Hydrodynamics (SPH) algorithm for viscous hydrodynamics.\nA new formula for the bulk viscous corrections present in the distribution\nfunction at freeze-out is derived starting from the Boltzmann equation for\nmulti-hadron species. Bulk viscosity is shown to enhance the collective flow\nFourier coefficients from $v_2(p_T)$ to $v_5(p_T)$ when $% p_{T}\\sim 1-3$ GeV\neven when the bulk viscosity to entropy density ratio, $% \\zeta/s$, is\nsignificantly smaller than $1/(4\\pi)$. \n\n"}
{"id": "1305.2804", "contents": "Title: ALICE status and plans Abstract: The ALICE experiment has been running successfully since 2010 and made an\nimpressive progress towards understanding of hot and dense QCD matter produced\nin heavy ion collisions at LHC energies. Recent results on identified particle\nspectra, azimuthal anisotropy, heavy flavour and quarkonium production in Pb-Pb\ncollisions at sqrt(s_NN) = 2.76 TeV are presented. First results on p-Pb\ncollisions and ALICE upgrade plans are briefly reviewed. \n\n"}
{"id": "1306.4538", "contents": "Title: Performance of Geant4 in simulating semiconductor particle detector\n  response in the energy range below 1 MeV Abstract: Geant4 simulations play a crucial role in the analysis and interpretation of\nexperiments providing low energy precision tests of the Standard Model. This\npaper focuses on the accuracy of the description of the electron processes in\nthe energy range between 100 and 1000 keV. The effect of the different\nsimulation parameters and multiple scattering models on the backscattering\ncoefficients is investigated. Simulations of the response of HPGe and\npassivated implanted planar Si detectors to \\beta{} particles are compared to\nexperimental results. An overall good agreement is found between Geant4\nsimulations and experimental data. \n\n"}
{"id": "1307.0864", "contents": "Title: Comparison of magnetic field uniformities for discretized and\n  finite-sized standard $\\cos\\theta$, solenoidal, and spherical coils Abstract: A significant challenge for experiments requiring a highly uniform magnetic\nfield concerns the identification and design of a discretized and finite-sized\nmagnetic field coil of minimal size. In this work we compare calculations of\nthe magnetic field uniformities and field gradients for three different\nstandard (i.e., non-optimized) types of coils: $\\cos\\theta$, solenoidal, and\nspherical coils. For an experiment with a particular requirement on either the\nfield uniformity or the field gradient, we show that the volume required by a\nspherical coil form which satisfies these requirements can be significantly\nless than the volumes required by $\\cos\\theta$ and solenoidal coil forms. \n\n"}
{"id": "1307.1094", "contents": "Title: Multiplicity dependence of the average transverse momentum in pp, p-Pb,\n  and Pb-Pb collisions at the LHC Abstract: The average transverse momentum $\\langle p_{\\rm T}\\rangle$ versus the\ncharged-particle multiplicity $N_{\\rm ch}$ was measured in p-Pb collisions at a\ncollision energy per nucleon-nucleon pair $\\sqrt{s_{\\rm NN}}=5.02$ TeV and in\npp collisions at collision energies of $\\sqrt{s}=0.9$, 2.76, and 7 TeV in the\nkinematic range $0.15<p_{\\rm T}<10.0$ GeV/$c$ and $|\\eta|<0.3$ with the ALICE\napparatus at the LHC. These data are compared to results in Pb-Pb collisions at\n$\\sqrt{s_{\\rm NN}}=2.76$ TeV at similar charged-particle multiplicities. In pp\nand p-Pb collisions, a strong increase of $\\langle p_{\\rm T}\\rangle$ with\n$N_{\\rm ch}$ is observed, which is much stronger than that measured in Pb-Pb\ncollisions. For pp collisions, this could be attributed, within a model of\nhadronizing strings, to multiple-parton interactions and to a final-state color\nreconnection mechanism. The data in p-Pb and Pb-Pb collisions cannot be\ndescribed by an incoherent superposition of nucleon-nucleon collisions and pose\na challenge to most of the event generators. \n\n"}
{"id": "1307.2610", "contents": "Title: Pulse shape discrimination for GERDA Phase I data Abstract: The GERDA experiment located at the LNGS searches for neutrinoless double\nbeta (0\\nu\\beta\\beta) decay of ^{76}Ge using germanium diodes as source and\ndetector. In Phase I of the experiment eight semi-coaxial and five BEGe type\ndetectors have been deployed. The latter type is used in this field of research\nfor the first time. All detectors are made from material with enriched ^{76}Ge\nfraction. The experimental sensitivity can be improved by analyzing the pulse\nshape of the detector signals with the aim to reject background events. This\npaper documents the algorithms developed before the data of Phase I were\nunblinded. The double escape peak (DEP) and Compton edge events of 2.615 MeV\n\\gamma\\ rays from ^{208}Tl decays as well as 2\\nu\\beta\\beta\\ decays of ^{76}Ge\nare used as proxies for 0\\nu\\beta\\beta\\ decay. For BEGe detectors the chosen\nselection is based on a single pulse shape parameter. It accepts 0.92$\\pm$0.02\nof signal-like events while about 80% of the background events at\nQ_{\\beta\\beta}=2039 keV are rejected.\n  For semi-coaxial detectors three analyses are developed. The one based on an\nartificial neural network is used for the search of 0\\nu\\beta\\beta\\ decay. It\nretains 90% of DEP events and rejects about half of the events around\nQ_{\\beta\\beta}. The 2\\nu\\beta\\beta\\ events have an efficiency of 0.85\\pm0.02\nand the one for 0\\nu\\beta\\beta\\ decays is estimated to be 0.90^{+0.05}_{-0.09}.\nA second analysis uses a likelihood approach trained on Compton edge events.\nThe third approach uses two pulse shape parameters. The latter two methods\nconfirm the classification of the neural network since about 90% of the data\nevents rejected by the neural network are also removed by both of them. In\ngeneral, the selection efficiency extracted from DEP events agrees well with\nthose determined from Compton edge events or from 2\\nu\\beta\\beta\\ decays. \n\n"}
{"id": "1307.6411", "contents": "Title: First Beam Observation and Near Future Plans at SPring-8 LEPS2\n  Experiments Abstract: The first photon beam was successfully produced by laser Compton\nbackscattering at the LEPS2 beamline, which was newly constructed at SPring-8\nfor the purpose to increase the beam intensity one order of magnitude more than\nthat of the LEPS experiments and to achieve the large acceptance coverage with\nhigh resolution detectors. The BGOegg electromagnetic calorimeter with\nassociated detectors are being set up at the LEPS2 experimental building for\nthe physics programs, including the searches for $\\eta$'-bound nuclei and\nhighly excited baryon resonances. In parallel to the BGOegg experiments, the\nLEPS2 charged particle spectrometer will be prepared inside the 1 Tesla\nsolenoidal magnet, transported from the BNL-E949 experiment. \n\n"}
{"id": "1308.1438", "contents": "Title: Profiling hot and dense nuclear medium with high transverse momentum\n  hadrons produced in d+Au and Au+Au collisions by the PHENIX experiment at\n  RHIC Abstract: PHENIX measurements of high transverse momentum ($p_T$) identified hadrons in\n$d$+Au and Au+Au collisions are presented. The nuclear modification factors\n($R_{d{\\rm A}}$ and $R_{\\rm AA}$) for $\\pi^0$ and $\\eta$ are found to be very\nconsistent in both collision systems, respectively. Using large amount of $p+p$\nand Au+Au datasets, the fractional momentum loss ($S_{\\rm loss}$) and the\npath-length dependent yield of $\\pi^0$ in Au+Au collisions are obtained. The\nhadron spectra in the most central $d$+Au and the most peripheral Au+Au\ncollisions are studied. The spectra shapes are found to be similar in both\nsystems, but the yield is suppressed in the most peripheral Au+Au collisions. \n\n"}
{"id": "1308.1633", "contents": "Title: The {\\sc Majorana Demonstrator} Neutrinoless Double-Beta Decay\n  Experiment Abstract: The {\\sc Majorana Demonstrator will search for the neutrinoless double-beta\ndecay of the isotope Ge-76 with a mixed array of enriched and natural germanium\ndetectors. The observation of this rare decay would indicate the neutrino is\nits own antiparticle, demonstrate that lepton number is not conserved, and\nprovide information on the absolute mass scale of the neutrino. The {\\sc\nDemonstrator} is being assembled at the 4850-foot level of the Sanford\nUnderground Research Facility in Lead, South Dakota. The array will be situated\nin a low-background environment and surrounded by passive and active shielding.\nHere we describe the science goals of the {\\sc Demonstrator} and the details of\nits design. \n\n"}
{"id": "1309.2074", "contents": "Title: Learning Transformations for Clustering and Classification Abstract: A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification. \n\n"}
{"id": "1310.3870", "contents": "Title: A new way of comparing double beta decay experiments Abstract: Many experiments whose goal is the search for neutrino-less double beta decay\nare taking data or in a final construction stage. The need for a tool that\nallows for an objective comparison between the sensitivity of different\nexperiments is mandatory in order to understand the potential of the next\ngeneration projects and focus on the best promising technologies. \n\n"}
{"id": "1310.4249", "contents": "Title: Mapping the stereotyped behaviour of freely-moving fruit flies Abstract: Most animals possess the ability to actuate a vast diversity of movements,\nostensibly constrained only by morphology and physics. In practice, however, a\nfrequent assumption in behavioral science is that most of an animal's\nactivities can be described in terms of a small set of stereotyped motifs. Here\nwe introduce a method for mapping the behavioral space of organisms, relying\nonly upon the underlying structure of postural movement data to organize and\nclassify behaviors. We find that six different drosophilid species each perform\na mix of non-stereotyped actions and over one hundred hierarchically-organized,\nstereotyped behaviors. Moreover, we use this approach to compare these species'\nbehavioral spaces, systematically identifying subtle behavioral differences\nbetween closely-related species. \n\n"}
{"id": "1310.4692", "contents": "Title: Challenges in Double Beta Decay Abstract: After nearly 80 years since the first guess on its existence, neutrino still\nescapes our insight: the mass and the true nature (Majorana or Dirac) of this\nparticle is still unknown. In the past ten years, neutrino oscillation\nexperiments have finally provided the incontrovertible evidence that neutrinos\nmix and have finite masses. These results represent the strongest demonstration\nthat the Standard Model of electroweak interactions is incomplete and that new\nPhysics beyond it must exist. None of these experimental efforts could however\nshade light on some of the basic features of neutrinos. Indeed, absolute scale\nand ordering of the masses of the three generations as well as charge\nconjugation and lepton number conservation properties are still unknown. In\nthis scenario, a unique role is played by the Neutrinoless Double Beta Decay\nsearches: these experiments can probe lepton number conservation, investigate\nthe Dirac/Majorana nature of the neutrinos and their absolute mass scale\n(hierarchy problem) with unprecedented sensitivity. Today Neutrinoless Double\nBeta Decay faces a new era where large scale experiments with a sensitivity\napproaching the so-called degenerate-hierarchy region are nearly ready to start\nand where the challenge for the next future is the construction of detectors\ncharacterized by a tonne-scale size and an incredibly low background, to fully\nprobe the inverted-hierarchy region. A number of new proposed projects took up\nthis challenge. These are based either on large expansions of the present\nexperiments or on new ideas to improve the technical performance and/or reduce\nthe background contributions. n this paper, a review of the most relevant\nongoing experiments is given. The most relevant parameters contributing to the\nexperimental sensitivity are discussed and a critical comparison of the future\nprojects is proposed. \n\n"}
{"id": "1310.5759", "contents": "Title: Storage of ultracold neutrons in the UCN$\\tau$ magneto-gravitational\n  trap Abstract: The UCN$\\tau$ experiment is designed to measure the lifetime $\\tau_{n}$ of\nthe free neutron by trapping ultracold neutrons (UCN) in a\nmagneto-gravitational trap. An asymmetric bowl-shaped NdFeB magnet Halbach\narray confines low-field-seeking UCN within the apparatus, and a set of\nelectromagnetic coils in a toroidal geometry provide a background \"holding\"\nfield to eliminate depolarization-induced UCN loss caused by magnetic field\nnodes. We present a measurement of the storage time $\\tau_{store}$ of the trap\nby storing UCN for various times, and counting the survivors. The data are\nconsistent with a single exponential decay, and we find $\\tau_{store}=860\\pm19$\ns: within $1 \\sigma$ of current global averages for $\\tau_{n}$. The storage\ntime with the holding field deactiveated is found to be $\\tau_{store}=470 \\pm\n160$ s; this decreased storage time is due to the loss of UCN which undergo\nMajorana spin-flips while being stored. We discuss plans to increase the\nstatistical sensitivity of the measurement and investigate potential systematic\neffects. \n\n"}
{"id": "1310.7671", "contents": "Title: Second order WSGD operators II: A new family of difference schemes for\n  space fractional advection diffusion equation Abstract: The second order weighted and shifted Gr\\\"{u}nwald difference (WSGD)\noperators are developed in [Tian et al., arXiv:1201.5949] to solve space\nfractional partial differential equations. Along this direction, we further\ndesign a new family of second order WSGD operators; by properly choosing the\nweighted parameters, they can be effectively used to discretize space\n(Riemann-Liouville) fractional derivatives. Based on the new second order WSGD\noperators, we derive a family of difference schemes for the space fractional\nadvection diffusion equation. By von Neumann stability analysis, it is proved\nthat the obtained schemes are unconditionally stable. Finally, extensive\nnumerical experiments are performed to demonstrate the performance of the\nschemes and confirm the convergent orders. \n\n"}
{"id": "1310.7732", "contents": "Title: Results on ultra-peripheral interactions in Pb-Pb and p-Pb collisions in\n  ALICE Abstract: Ultra-relativistic heavy ions generate strong electromagnetic fields which\noffer the possibility to study $\\gamma$-nucleus and $\\gamma$-proton\ninteractions at the LHC in the so called ultra-peripheral collisions (UPC).\n  Here we report ALICE results on J/psi photoproduction measured in Pb-Pb\ncollisions at $\\sqrt{s_{\\rm NN}}$ = 2.76 TeV and in p-Pb collisions at\n$\\sqrt{s_{\\rm NN}}$ = 5.02 TeV. \n\n"}
{"id": "1311.2447", "contents": "Title: Analysis of an attempt at detection of neutrons produced in a plasma\n  discharge electrolytic cell Abstract: R. Faccini et al. \\cite{Faccini:2013} have attempted a replication of an\nearlier experiment by D. Cirillo et al. \\cite{Cirillo:2012} in which neutrons\n[as well as nuclear transmutations] were observed in a modified Mizuno cell. No\nneutron production is observed in the recent experiment \\cite{Faccini:2013} and\nno evidence for microwave radiation or nuclear transmutations are reported. A\ncareful analysis shows major technical differences in the two experiments and\nwe explore the underlying reasons for the lack of any nuclear activity in the\nnewer experiment. \n\n"}
{"id": "1311.3310", "contents": "Title: Status of the MAJORANA DEMONSTRATOR experiment Abstract: The MAJORANA DEMONSTRATOR neutrinoless double beta-decay experiment is\ncurrently under construction at the Sanford Underground Research Facility in\nSouth Dakota, USA. An overview and status of the experiment are given. \n\n"}
{"id": "1311.4779", "contents": "Title: Characterization and modeling of a low background HPGe detector Abstract: A high efficiency, low background counting setup has been made at TIFR\nconsisting of a special HPGe detector ($\\sim$ 70%) surrounded by a low activity\ncopper+lead shield. Detailed measurements are performed with point and extended\ngeometry sources to obtain a complete response of the detector. An effective\nmodel of the detector has been made with GEANT4 based Monte Carlo simulations\nwhich agrees with experimental data within 5%. This setup will be used for\nqualification and selection of radio-pure materials to be used in a cryogenic\nbolometer for the study of Neutrinoless Double Beta Decay in $^{124}$Sn as well\nas for other rare event studies. Using this setup, radio-impurities in the rock\nsample from India-based Neutrino Observatory (INO) site have been estimated. \n\n"}
{"id": "1311.5463", "contents": "Title: Jet Quenching and Its Azimuthal Anisotropy in AA and possibly High\n  Multiplicity pA and dA Collisions Abstract: We report our calculation of jet quenching and its azimuthal anisotropy in\nthe high energy AA and high multiplicity pA and dA collisions. The purpose of\nthis study is twofold. First, we improve our previous event-by-event studies,\nby properly implementing $p_t$ dependence in the modeling. We show that, within\nthe jet \"monography\" scenario featuring a strong near-Tc-enhancement of jet\nenergy loss, the computed high-$p_t$ nuclear modification factor $R_{AA}$ and\nthe harmonic coefficients in its azimuthal anisotropy $v_{2,\\,3,\\,4}$, agree\nwell with the available data from both Relativistic Heavy Ion Collider (RHIC)\nand Large Hadron Collider (LHC). Second, in light of current discussions on\npossible final state collective behavior in the high-multiplicity pPb and dAu\ncollisions, we examine the implication of final state jet attenuation in such\ncollisions, by applying the same model used in AA collisions and quantifying\nthe corresponding $R_{pA}$ and $R_{dA}$ and their azimuthal anisotropy. The\nhigh-$p_t$ $V_n$ provide a set of clean indicators of final state energy loss.\nIn particular we find in the most central pPb (5.02 TeV) and dAu (200 GeV)\ncollisions, $V_2$ is on the order of $0.01$ and $0.1$ respectively, measurable\nwith current experimental accuracy. In addition, our high-$p_t$ $R_{dA}$ is\naround $0.6$ which is compatible with preliminary dAu results from RHIC. \n\n"}
{"id": "1312.0250", "contents": "Title: Simulations of gamma quanta scattering in a single module of the J-PET\n  detector Abstract: This article describes simulations of scattering of annihilation gamma quanta\nin a strip of plastic scintillator. Such strips constitute basic detection\nmodules in a newly proposed Positron Emission Tomography which utilizes plastic\nscintillators instead of inorganic crystals. An algorithm simulating chain of\nCompton scatterings was elaborated and series of simulations have been\nconducted for the scintillator strip with the cross section of 5 mm x 19 mm.\nObtained results indicate that secondary interactions occur only in the case of\nabout 8% of events and out of them only 25$\\%$ take place in the distance\nlarger than 0.5 cm from the primary interaction. It was also established that\nlight signals produced at primary and secondary interactions overlap with the\ndelay which distribution is characterized by FWHM of about 40 ps. \n\n"}
{"id": "1312.1091", "contents": "Title: Status of the NICA/MPD project Abstract: A general-purpose detector for studying heavy-ion collisions at the NICA\nfacility is under construction at JINR. The NICA/MPD physics program, basic\ndesign requirements, and the MPD experimental setup will be described. Results\nof detector simulation and the expected performance for selected observables\nwill be presented \n\n"}
{"id": "1312.1730", "contents": "Title: The OLYMPUS Experiment Abstract: The OLYMPUS experiment was designed to measure the ratio between the\npositron-proton and electron-proton elastic scattering cross sections, with the\ngoal of determining the contribution of two-photon exchange to the elastic\ncross section. Two-photon exchange might resolve the discrepancy between\nmeasurements of the proton form factor ratio, $\\mu_p G^p_E/G^p_M$, made using\npolarization techniques and those made in unpolarized experiments. OLYMPUS\noperated on the DORIS storage ring at DESY, alternating between 2.01~GeV\nelectron and positron beams incident on an internal hydrogen gas target. The\nexperiment used a toroidal magnetic spectrometer instrumented with drift\nchambers and time-of-flight detectors to measure rates for elastic scattering\nover the polar angular range of approximately $25^\\circ$--$75^\\circ$. Symmetric\nM{\\o}ller/Bhabha calorimeters at $1.29^\\circ$ and telescopes of GEM and MWPC\ndetectors at $12^\\circ$ served as luminosity monitors. A total luminosity of\napproximately 4.5~fb$^{-1}$ was collected over two running periods in 2012.\nThis paper provides details on the accelerator, target, detectors, and\noperation of the experiment. \n\n"}
{"id": "1312.6186", "contents": "Title: GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network\n  Training Abstract: The ability to train large-scale neural networks has resulted in\nstate-of-the-art performance in many areas of computer vision. These results\nhave largely come from computational break throughs of two forms: model\nparallelism, e.g. GPU accelerated training, which has seen quick adoption in\ncomputer vision circles, and data parallelism, e.g. A-SGD, whose large scale\nhas been used mostly in industry. We report early experiments with a system\nthat makes use of both model parallelism and data parallelism, we call GPU\nA-SGD. We show using GPU A-SGD it is possible to speed up training of large\nconvolutional neural networks useful for computer vision. We believe GPU A-SGD\nwill make it possible to train larger networks on larger training sets in a\nreasonable amount of time. \n\n"}
{"id": "1401.1973", "contents": "Title: Jets in multiparticle production in and beyond geometry of proton-proton\n  collisions at the LHC Abstract: Experimental findings of CMS on properties of jets and underlying events at\nhigh multiplicities in proton-proton interactions at 7 TeV are interpreted as\nan indication of increasing role of central collisions with small impact\nparameters. We find an indication that the rates of different hard processes\nobserved by CMS and ALICE universally depend on underlying event\ncharged-particles multiplicity until it becomes four times more than average.\nIt is shown that the increase of the overlap area of colliding protons is not\nsufficient for explanation of the rate of jet production in events with\ncharged-particle multiplicity three times higher than average and some new\nmechanisms are necessary like interaction of protons in rare configurations of\nhigher than average gluon density. Such mechanisms are not included in the\npresent Monte Carlo event generators. Further studies are proposed \n\n"}
{"id": "1401.3650", "contents": "Title: Final-state interactions effects in neutral-current neutrino and\n  antineutrino cross sections at MiniBooNE kinematics Abstract: The analysis of the recent neutral-current elastic neutrino and\nantineutrino-nucleus scattering cross sections measured by the MiniBooNE\nCollaboration requires relativistic theoretical descriptions also accounting\nfor the role of final-state interactions. In this work we investigate the\nsensitivity to final-state interactions and compare the MiniBooNE data with the\nresults obtained in the relativistic Green's function model with different\nparameterizations for the phenomenological relativistic optical potential. \n\n"}
{"id": "1401.4699", "contents": "Title: NA61/SHINE facility at the CERN SPS: beams and detector system Abstract: NA61/SHINE (SPS Heavy Ion and Neutrino Experiment) is a multi-purpose\nexperimental facility to study hadron production in hadron-proton,\nhadron-nucleus and nucleus-nucleus collisions at the CERN Super Proton\nSynchrotron. It recorded the first physics data with hadron beams in 2009 and\nwith ion beams (secondary 7Be beams) in 2011.\n  NA61/SHINE has greatly profited from the long development of the CERN proton\nand ion sources and the accelerator chain as well as the H2 beamline of the\nCERN North Area. The latter has recently been modified to also serve as a\nfragment separator as needed to produce the Be beams for NA61/SHINE. Numerous\ncomponents of the NA61/SHINE set-up were inherited from its predecessors, in\nparticular, the last one, the NA49 experiment. Important new detectors and\nupgrades of the legacy equipment were introduced by the NA61/SHINE\nCollaboration.\n  This paper describes the state of the NA61/SHINE facility - the beams and the\ndetector system - before the CERN Long Shutdown I, which started in March 2013. \n\n"}
{"id": "1401.6858", "contents": "Title: Heavy-flavour decay lepton measurements in pp, p-Pb, and Pb-Pb\n  collisions with ALICE at the LHC Abstract: We present the measurements of electrons and muons from the semi-leptonic\ndecays of heavy-flavour hadrons measured in the central and forward rapidity\nregions with ALICE in pp, Pb-Pb, and p-Pb, collisions at the LHC. The\npT-differential production cross section in pp collisions, the elliptic flow in\nPb-Pb collisions, and the nuclear modification factor in Pb-Pb and p-Pb\ncollisions are shown. The results are compared to theoretical predictions. \n\n"}
{"id": "1401.8126", "contents": "Title: Extrinsic Methods for Coding and Dictionary Learning on Grassmann\n  Manifolds Abstract: Sparsity-based representations have recently led to notable results in\nvarious visual recognition tasks. In a separate line of research, Riemannian\nmanifolds have been shown useful for dealing with features and models that do\nnot lie in Euclidean spaces. With the aim of building a bridge between the two\nrealms, we address the problem of sparse coding and dictionary learning over\nthe space of linear subspaces, which form Riemannian structures known as\nGrassmann manifolds. To this end, we propose to embed Grassmann manifolds into\nthe space of symmetric matrices by an isometric mapping. This in turn enables\nus to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we\npropose closed-form solutions for learning a Grassmann dictionary, atom by\natom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann\nsparse coding and dictionary learning algorithms through embedding into Hilbert\nspaces.\n  Experiments on several classification tasks (gender recognition, gesture\nclassification, scene analysis, face recognition, action recognition and\ndynamic texture classification) show that the proposed approaches achieve\nconsiderable improvements in discrimination accuracy, in comparison to\nstate-of-the-art methods such as kernelized Affine Hull Method and\ngraph-embedding Grassmann discriminant analysis. \n\n"}
{"id": "1402.0240", "contents": "Title: Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\n  Algorithms Abstract: We study an extension of the classical graph cut problem, wherein we replace\nthe modular (sum of edge weights) cost function by a submodular set function\ndefined over graph edges. Special cases of this problem have appeared in\ndifferent applications in signal processing, machine learning, and computer\nvision. In this paper, we connect these applications via the generic\nformulation of \"cooperative graph cuts\", for which we study complexity,\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\nthe proposed algorithms empirically. \n\n"}
{"id": "1402.6072", "contents": "Title: Searching for neutrinoless double-beta decay of $^{130}$Te with CUORE Abstract: Neutrinoless double-beta ($0\\nu\\beta\\beta$) decay is a hypothesized\nlepton-number-violating process that offers the only known means of asserting\nthe possible Majorana nature of neutrino mass. The Cryogenic Underground\nObservatory for Rare Events (CUORE) is an upcoming experiment designed to\nsearch for $0\\nu\\beta\\beta$ decay of $^{130}$Te using an array of 988 TeO$_2$\ncrystal bolometers operated at 10 mK. The detector will contain 206 kg of\n$^{130}$Te and have an average energy resolution of 5 keV; the projected\n$0\\nu\\beta\\beta$ decay half-life sensitivity after five years of live time is\n$1.6\\times 10^{26}$ y at $1\\sigma$ ($9.5\\times10^{25}$ y at the 90% confidence\nlevel), which corresponds to an upper limit on the effective Majorana mass in\nthe range 40--100 meV (50--130 meV). In this paper we review the experimental\ntechniques used in CUORE as well as its current status and anticipated physics\nreach. \n\n"}
{"id": "1402.7147", "contents": "Title: PHENIX recent heavy flavor results Abstract: Cold nuclear matter (CNM) effects provide an important baseline for the\ninterpretation of data in heavy ion collisions. Such effects include nuclear\nshadowing, Cronin effect, and initial patron energy loss, and it is interesting\nto study the dependence on impact parameter and kinematic region. Heavy quark\nproduction is a good measurement to probe the CNM effects particularly on\ngluons, since heavy quarks are mainly produced via gluon fusions at RHIC\nenergy. The PHENIX experiment has experiment has ability to study the CNM\neffects by measuring heavy quark production in $d$$+$Au collisions at variety\nof kinematic ranges. Comparisons of heavy quark production at different\nrapidities allow us to study modification of gluon density function in the Au\nnucleus depending on momentum fraction. Furthermore, comparisons to the results\nfrom heavy ion collisions (Au$+$Au and Cu$+$Cu) measured by PHENIX provide\ninsight into the role of CNM effects in such collisions. Recent PHENIX results\non heavy quark production are discussed. \n\n"}
{"id": "1403.0107", "contents": "Title: Method of Fission Product Beta Spectra Measurements for Predicting\n  Reactor Anti-neutrino Emission Abstract: The nuclear fission process that occurs in the core of nuclear reactors\nresults in unstable, neutron rich fission products that subsequently beta decay\nand emit electron anti-neutrinos. These reactor neutrinos have served neutrino\nphysics research from the initial discovery of the neutrino to current\nprecision measurements of neutrino mixing angles. The prediction of the\nabsolute flux and energy spectrum of the emitted reactor neutrinos hinges upon\na series of seminal papers based on measurements performed in the 1970s and\n1980s. The steadily improving reactor neutrino measurement techniques and\nrecent re-considerations of the agreement between the predicted and observed\nreactor neutrino flux motivates revisiting the underlying beta spectra\nmeasurements. A method is proposed to use an accelerator proton beam delivered\nto an engineered target to yield a neutron field tailored to reproduce the\nneutron energy spectrum present in the core of an operating nuclear reactor.\nFoils of the primary reactor fissionable isotopes placed in this tailored\nneutron flux will ultimately emit beta particles from the resultant fission\nproducts. Measurement of these beta particles in a time projection chamber with\na perpendicular magnetic field provides a distinctive set of systematic\nconsiderations for comparison to the original seminal beta spectra\nmeasurements. Ancillary measurements such as gamma-ray emission and\npost-irradiation radiochemical analysis will further constrain the absolute\nnormalization of beta emissions per fission. The requirements for unfolding the\nbeta spectra measured with this method into a predicted reactor neutrino\nspectrum are explored. \n\n"}
{"id": "1404.3251", "contents": "Title: Design and operation of a cryogenic charge-integrating preamplifier for\n  the MuSun experiment Abstract: The central detector in the MuSun experiment is a pad-plane time projection\nionization chamber that operates without gas amplification in deuterium at 31\nK; it is used to measure the rate of the muon capture process $\\mu^- + d\n\\rightarrow n + n + \\nu_\\mu$. A new charge-sensitive preamplifier, operated at\n140 K, has been developed for this detector. It achieved a resolution of 4.5\nkeV(D$_2$) or 120 $e^-$ RMS with zero detector capacitance at 1.1 $\\mu$s\nintegration time in laboratory tests. In the experimental environment, the\nelectronic resolution is 10 keV(D$_2$) or 250 $e^-$ RMS at a 0.5 $\\mu$s\nintegration time. The excellent energy resolution of this amplifier has enabled\ndiscrimination between signals from muon-catalyzed fusion and muon capture on\nchemical impurities, which will precisely determine systematic corrections due\nto these processes. It is also expected to improve the muon tracking and\ndetermination of the stopping location. \n\n"}
{"id": "1404.3527", "contents": "Title: New source for ultracold neutrons at the Institut Laue-Langevin Abstract: A new intense superthermal source for ultracold neutrons (UCN) has been\ninstalled at a dedicated beam line at the Institut Laue-Langevin. Incident\nneutrons with a wavelength of 0.89 nm are converted to UCN in a five liter\nvolume filled with superfluid $^4$He at a temperature of about 0.7 K. The UCN\ncan be extracted to room temperature experiments. We present the cryogenic\nsetup of the source, a characterization of the cold neutron beam, and UCN\nproduction measurements, where a UCN density in the production volume of at\nleast 55 per cm$^3$ was determined. \n\n"}
{"id": "1404.3723", "contents": "Title: QCD and strongly coupled gauge theories: challenges and perspectives Abstract: We highlight the progress, current status, and open challenges of QCD-driven\nphysics, in theory and in experiment. We discuss how the strong interaction is\nintimately connected to a broad sweep of physical problems, in settings ranging\nfrom astrophysics and cosmology to strongly-coupled, complex systems in\nparticle and condensed-matter physics, as well as to searches for physics\nbeyond the Standard Model. We also discuss how success in describing the strong\ninteraction impacts other fields, and, in turn, how such subjects can impact\nstudies of the strong interaction. In the course of the work we offer a\nperspective on the many research streams which flow into and out of QCD, as\nwell as a vision for future developments. \n\n"}
{"id": "1404.4129", "contents": "Title: Long-range azimuthal correlations in proton-proton and proton-nucleus\n  collisions from the incoherent scattering of partons Abstract: We show that the incoherent elastic scattering of partons, as present in a\nmulti-phase transport model (AMPT), with a modest parton-parton cross-section\nof $\\sigma=1.5 - 3$ mb, naturally explains the long-range two-particle\nazimuthal correlation as observed in proton-proton and proton-nucleus\ncollisions at the Large Hadron Collider. \n\n"}
{"id": "1404.7399", "contents": "Title: Testing the Ge detectors for the MAJORANA DEMONSTRATOR Abstract: High purity germanium (HPGe) crystals will be used for the MAJORANA\nDEMONSTRATOR, where they serve as both the source and the detector for\nneutrinoless double beta decay. It is crucial for the experiment to understand\nthe performance of the HPGe crystals. A variety of crystal properties are being\ninvestigated, including basic properties such as energy resolution, efficiency,\nuniformity, capacitance, leakage current and crystal axis orientation, as well\nas more sophisticated properties, e.g. pulse shapes and dead layer and\ntransition layer distributions. In this paper, we will present our measurements\nthat characterize the HPGe crystals. We will also discuss our simulation\npackage for the detector characterization setup, and show that additional\ninformation can be extracted from data-simulation comparisons. \n\n"}
{"id": "1405.0957", "contents": "Title: The magnetic shielding for the neutron decay spectrometer aSPECT Abstract: Many experiments in nuclear and neutron physics are confronted with the\nproblem that they use a superconducting magnetic spectrometer which potentially\naffects other experiments by their stray magnetic field. The retardation\nspectrometer aSPECT consists, inter alia, of a superconducting magnet system\nthat produces a strong longitudinal magnetic field of up to 6.2T. In order not\nto disturb other experiments in the vicinity of aSPECT, we had to develop a\nmagnetic field return yoke for the magnet system. While the return yoke must\nreduce the stray magnetic field, the internal magnetic field and its\nhomogeneity should not be affected. As in many cases, the magnetic shielding\nfor aSPECT must manage with limited space. In addition, we must ensure that the\nadditional magnetic forces on the magnet coils are not destructive. In order to\ndetermine the most suitable geometry for the magnetic shielding for aSPECT, we\nsimulated a variety of possible geometries and combinations of shielding\nmaterials of non-linear permeability. The results of our simulations were\nchecked through magnetic field measurements both with Hall and nuclear magnetic\nresonance probes. The experimental data are in good agreement with the\nsimulated values: The mean deviation from the simulated exterior magnetic field\nis (-1.7+/-4.8)%. However, in the two critical regions, the internal magnetic\nfield deviates by 0.2% respectively <1E-4 from the simulated values. \n\n"}
{"id": "1405.2853", "contents": "Title: A high-pressure hydrogen time projection chamber for the MuCap\n  experiment Abstract: The MuCap experiment at the Paul Scherrer Institute performed a\nhigh-precision measurement of the rate of the basic electroweak process of\nnuclear muon capture by the proton, $\\mu^- + p \\rightarrow n + \\nu_\\mu$. The\nexperimental approach was based on the use of a time projection chamber (TPC)\nthat operated in pure hydrogen gas at a pressure of 10 bar and functioned as an\nactive muon stopping target. The TPC detected the tracks of individual muon\narrivals in three dimensions, while the trajectories of outgoing decay (Michel)\nelectrons were measured by two surrounding wire chambers and a plastic\nscintillation hodoscope. The muon and electron detectors together enabled a\nprecise measurement of the $\\mu p$ atom's lifetime, from which the nuclear muon\ncapture rate was deduced. The TPC was also used to monitor the purity of the\nhydrogen gas by detecting the nuclear recoils that follow muon capture by\nelemental impurities. This paper describes the TPC design and performance in\ndetail. \n\n"}
{"id": "1405.4864", "contents": "Title: Probing New Physics with Underground Accelerators and Radioactive\n  Sources Abstract: New light, weakly coupled particles can be efficiently produced at existing\nand future high-intensity accelerators and radioactive sources in deep\nunderground laboratories. Once produced, these particles can scatter or decay\nin large neutrino detectors (e.g Super-K and Borexino) housed in the same\nfacilities. We discuss the production of weakly coupled scalars $\\phi$ via\nnuclear de-excitation of an excited element into the ground state in two viable\nconcrete reactions: the decay of the $0^+$ excited state of $^{16}$O populated\nvia a $(p,\\alpha)$ reaction on fluorine and from radioactive $^{144}$Ce decay\nwhere the scalar is produced in the de-excitation of $^{144}$Nd$^*$, which\noccurs along the decay chain. Subsequent scattering on electrons,\n$e(\\phi,\\gamma)e$, yields a mono-energetic signal that is observable in\nneutrino detectors. We show that this proposed experimental set-up can cover\nnew territory for masses $250\\, {\\rm keV}\\leq m_\\phi \\leq 2 m_e$ and couplings\nto protons and electrons, $10^{-11} < g_e g_p < 10^{-7}$. This parameter space\nis motivated by explanations of the \"proton charge radius puzzle\", thus this\nstrategy adds a viable new physics component to the neutrino and nuclear\nastrophysics programs at underground facilities. \n\n"}
{"id": "1405.5060", "contents": "Title: Statistical sensitivity of 163-Ho electron capture neutrino mass\n  experiments Abstract: Large calorimetric neutrino mass experiments using thermal detectors are\npossibly going to play a crucial role in the challenge for assessing the\nneutrino mass. This paper describe a tool based on Monte Carlo methods which\nhas been developed to estimate the statistical sensitivity of calorimetric\nneutrino mass experiments using the 163-Ho electron capture decay. The tool is\napplied to investigate the effect of various experimental parameters and the\nresults useful for designing an experiment with sub-eV sensitivity are given. \n\n"}
{"id": "1405.7209", "contents": "Title: The TITAN in-trap decay spectroscopy facility at TRIUMF Abstract: This article presents an upgraded in-trap decay spectroscopy apparatus which\nhas been developed and constructed for use with TRIUMF's Ion Trap for Atomic\nand Nuclear science (TITAN). This device consists of an open-access\nelectron-beam ion-trap (EBIT), which is surrounded radially by seven low-energy\nplanar Si(Li) detectors. The environment of the EBIT allows for the detection\nof low-energy photons by providing backing-free storage of the radioactive\nions, while guiding charged decay particles away from the trap centre via the\nstrong (up to 6 T) magnetic field. In addition to excellent ion confinement and\nstorage, the EBIT also provides a venue for performing decay spectroscopy on\nhighly-charged radioactive ions. Recent technical advancements have been able\nto provide a significant increase in sensitivity for low-energy photon\ndetection, towards the goal of measuring weak electron-capture branching ratios\nof the intermediate nuclei in the two-neutrino double beta ($2\\nu\\beta\\beta$)\ndecay process. The design, development, and commissioning of this apparatus are\npresented together with the main physics objectives. The future of the device\nand experimental technique are discussed. \n\n"}
{"id": "1406.0830", "contents": "Title: Results from PHENIX at RHIC with Implications for LHC Abstract: This article is based on my Proceedings for the 47th Course of the\nInternational School of Subnuclear Physics on the Most Unexpected at LHC and\nthe Status of High Energy Frontier, Erice, Sicily, Italy, 2009. Results from\nthe PHENIX experiment at the Relativistic Heavy Ion Collider (RHIC) in\nnucleus-nucleus and proton-proton collisions at c.m. energy $\\sqrt{s_{NN}}=200$\nGeV are presented in the context of the methods of single and two-particle\ninclusive reactions which were used in the discovery of hard-scattering in p-p\ncollisions at the CERN ISR in the 1970's. These techniques are used at RHIC in\nA+A collisions because of the huge combinatoric background from the large\nparticle multiplicity. Topics include $J/\\Psi$ suppression, jet quenching in\nthe dense medium (sQGP) as observed with $\\pi^0$ at large transverse momentum,\nthermal photons, collective flow, two-particle correlations, suppression of\nheavy quarks at large $p_T$ and its possible relation to Higgs searches at the\nLHC. The differences and similarities of the measurements in p-p and A+A\ncollisions are presented. The two discussion sessions which followed the\nlectures on which this article is based are included at the end. \n\n"}
{"id": "1406.1528", "contents": "Title: Towards building a Crowd-Sourced Sky Map Abstract: We describe a system that builds a high dynamic-range and wide-angle image of\nthe night sky by combining a large set of input images. The method makes use of\npixel-rank information in the individual input images to improve a \"consensus\"\npixel rank in the combined image. Because it only makes use of ranks and the\ncomplexity of the algorithm is linear in the number of images, the method is\nuseful for large sets of uncalibrated images that might have undergone unknown\nnon-linear tone mapping transformations for visualization or aesthetic reasons.\nWe apply the method to images of the night sky (of unknown provenance)\ndiscovered on the Web. The method permits discovery of astronomical objects or\nfeatures that are not visible in any of the input images taken individually.\nMore importantly, however, it permits scientific exploitation of a huge source\nof astronomical images that would not be available to astronomical research\nwithout our automatic system. \n\n"}
{"id": "1407.5724", "contents": "Title: Test and characterization of a prototype silicon-tungsten\n  electromagnetic calorimeter Abstract: New generation high-energy physics experiments demand high precision tracking\nand accurate measurements of a large number of particles produced in the\ncollisions of lementary particles and heavy-ions. Silicon-tungsten (Si-W)\ncalorimeters provide the most viable technological option to meet the\nrequirements of particle detection in high multiplicity environments. We report\na novel Si-W calorimeter design, which is optimized for $\\gamma/\\pi^0$\ndiscrimination up to high momenta. In order to test the feasibility of the\ncalorimeter, a prototype mini-tower was constructed using silicon pad detector\narrays and tungsten layers. The performance of the mini-tower was tested using\npion and electron beams at the CERN Proton Synchrotron (PS). The experimental\nresults are compared with the results from a detailed GEANT-4 simulation. A\nlinear relationship between the observed energy deposition and simulated\nresponse of the mini-tower has been obtained, in line with our expectations. \n\n"}
{"id": "1407.6601", "contents": "Title: Fast Neutron Detection with a Segmented Spectrometer Abstract: A fast neutron spectrometer consisting of segmented plastic scintillator and\nHe-3 proportional counters was constructed for the measurement of neutrons in\nthe energy range 1 MeV to 200 MeV. We discuss its design, principles of\noperation, and the method of analysis. The detector is capable of observing\nvery low neutron fluxes in the presence of ambient gamma background and does\nnot require scintillator pulseshape discrimination. The spectrometer was\ncharacterized for its energy response in fast neutron fields of 2.5 MeV and 14\nMeV, and the results are compared with Monte Carlo simulations. Measurements of\nthe fast neutron flux and energy response at 120 m above sea-level (39.130 deg.\nN, 77.218 deg. W) and at a depth of 560 m in a limestone mine are presented.\nFinally, the design of a spectrometer with improved sensitivity and energy\nresolution is discussed. \n\n"}
{"id": "1408.0414", "contents": "Title: Overview of ALICE Results at Quark Matter 2014 Abstract: The results released by the ALICE collaboration at Quark Matter 2014 address\ntopics from identified-particle jet fragmentation functions in pp collisions,\nto the search for collective signatures in p-Pb collisions to precision\nmeasurements of jet quenching with D mesons in Pb-Pb collisions. This paper\ngives an overview of the contributions (31 parallel talks, 2 flash talks and 80\nposters) by the ALICE collaboration at Quark Matter 2014. \n\n"}
{"id": "1408.1648", "contents": "Title: Progress towards precision measurements of beta-decay correlation\n  parameters using atom and ion traps Abstract: The correlations of the decay products following the beta decay of nuclei\nhave a long history of providing a low-energy probe of the fundamental\nsymmetries of our universe. Over half a century ago, the correlation of the\nelectrons following the decay of polarized 60Co demonstrated that parity is not\nconserved in weak interactions. Today, the same basic idea continues to be\napplied to search for physics beyond the standard model: make precision\nmeasurements of correlation parameters and look for deviations compared to\ntheir standard model predictions. Efforts to measure these parameters to the\n0.1% level utilizing atom and ion trapping techniques are described. \n\n"}
{"id": "1409.3622", "contents": "Title: Investigation of Hamamatsu H8500 phototubes as single photon detectors Abstract: We have investigated the response of a significant sample of Hamamatsu H8500\nMultiAnode PhotoMultiplier Tubes (MAPMTs) as single photon detectors, in view\nof their use in a ring imaging Cherenkov counter for the CLAS12 spectrometer at\nthe Thomas Jefferson National Accelerator Facility. For this, a laser working\nat 407.2nm wavelength was employed. The sample is divided equally into standard\nwindow type, with a spectral response in the visible light region, and\nUV-enhanced window type MAPMTs. The studies confirm the suitability of these\nMAPMTs for single photon detection in such a Cherenkov imaging application. \n\n"}
{"id": "1409.5755", "contents": "Title: Measuring the Neutrino Mass Hierarchy with Atmospheric Neutrinos Abstract: The proposed PINGU experiment to measure the neutrino mass hierarchy is\npresented, in the context of long-range planning by the U.S. nuclear physics\ncommunity. \n\n"}
{"id": "1409.6041", "contents": "Title: Domain Adaptive Neural Networks for Object Recognition Abstract: We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks. \n\n"}
{"id": "1409.7100", "contents": "Title: The Q_weak Experimental Apparatus Abstract: The Jefferson Lab Q_weak experiment determined the weak charge of the proton\nby measuring the parity-violating elastic scattering asymmetry of\nlongitudinally polarized electrons from an unpolarized liquid hydrogen target\nat small momentum transfer. A custom apparatus was designed for this experiment\nto meet the technical challenges presented by the smallest and most precise\n${\\vec{e}}$p asymmetry ever measured. Technical milestones were achieved at\nJefferson Lab in target power, beam current, beam helicity reversal rate,\npolarimetry, detected rates, and control of helicity-correlated beam\nproperties. The experiment employed 180 microA of 89% longitudinally polarized\nelectrons whose helicity was reversed 960 times per second. The electrons were\naccelerated to 1.16 GeV and directed to a beamline with extensive\ninstrumentation to measure helicity-correlated beam properties that can induce\nfalse asymmetries. Moller and Compton polarimetry were used to measure the\nelectron beam polarization to better than 1%. The electron beam was incident on\na 34.4 cm liquid hydrogen target. After passing through a triple collimator\nsystem, scattered electrons between 5.8 degrees and 11.6 degrees were bent in\nthe toroidal magnetic field of a resistive copper-coil magnet. The electrons\ninside this acceptance were focused onto eight fused silica Cerenkov detectors\narrayed symmetrically around the beam axis. A total scattered electron rate of\nabout 7 GHz was incident on the detector array. The detectors were read out in\nintegrating mode by custom-built low-noise pre-amplifiers and 18-bit sampling\nADC modules. The momentum transfer Q^2 = 0.025 GeV^2 was determined using\ndedicated low-current (~100 pA) measurements with a set of drift chambers\nbefore (and a set of drift chambers and trigger scintillation counters after)\nthe toroidal magnet. \n\n"}
{"id": "1410.0042", "contents": "Title: Offline trapping of $^{221}$Fr in a magneto-optical trap from\n  implantation of an $^{225}$Ac ion beam Abstract: We demonstrate a new technique to prepare an offline source of francium for\ntrapping in a magneto-optical trap. Implanting a radioactive beam of\n$^{225}$Ac, $t_{1/2} = 9.920(3)$ days, in a foil, allows use of the decay\nproducts, i.e.$^{221}$Fr, $t_{1/2} = 288.0(4)$ s. $^{221}$Fr is ejected from\nthe foil by the $\\alpha$ decay of $^{225}$Ac. This technique is compatible with\nthe online accumulation of a laser-cooled atomic francium sample for a series\nof planned parity non-conservation measurements at TRIUMF. We obtain a 34%\nrelease efficiency for $^{221}$Fr from the recoil source based on particle\ndetector measurements. We find that laser cooling operation with the source is\n$8^{+10}_{-5}$ times less efficient than from a mass-separated ion beam of\n$^{221}$Fr in the current geometry. While the flux of this source is two to\nthree orders of magnitude lower than typical francium beams from ISOL\nfacilities, the source provides a longer-term supply of francium for offline\nstudies. \n\n"}
{"id": "1410.1761", "contents": "Title: J/$\\psi$ and $\\psi$(2S) production in p-Pb collisions with ALICE at the\n  LHC Abstract: The ALICE collaboration has studied the inclusive J/$\\psi$ and\n$\\psi(\\mathrm{2S})$ production in p-Pb collisions at $\\sqrt{s_{NN}}$ = 5.02 TeV\nat the CERN LHC. The J/$\\psi$ measurement is performed in the $\\mu^{+}\\mu^{-}$\n( - 4.46 < $y_{cms}$ < - 2.96 and 2.03 < $y_{cms}$ < 3.53 ) and in the\n$e^{+}e^{-}$ ( - 1.37 < $y_{cms}$ < 0.46 ) decay channels, down to zero\ntransverse momentum. The results are in fair agreement with theoretical\npredictions based on nuclear shadowing, as well as with models including, in\naddition, a contribution from partonic energy loss. Finally, the\n$\\psi(\\mathrm{2S})$ measurement in the $\\mu^{+}\\mu^{-}$ decay channel has been\nperformed. In particular, a significantly smaller $\\psi(\\mathrm{2S})$ nuclear\nmodification factor, with respect to the J$/\\psi$ one, has been observed. \n\n"}
{"id": "1410.5224", "contents": "Title: Supervised mid-level features for word image representation Abstract: This paper addresses the problem of learning word image representations:\ngiven the cropped image of a word, we are interested in finding a descriptive,\nrobust, and compact fixed-length representation. Machine learning techniques\ncan then be supplied with these representations to produce models useful for\nword retrieval or recognition tasks. Although many works have focused on the\nmachine learning aspect once a global representation has been produced, little\nwork has been devoted to the construction of those base image representations:\nmost works use standard coding and aggregation techniques directly on top of\nstandard computer vision features such as SIFT or HOG.\n  We propose to learn local mid-level features suitable for building word image\nrepresentations. These features are learnt by leveraging character bounding box\nannotations on a small set of training images. However, contrary to other\napproaches that use character bounding box information, our approach does not\nrely on detecting the individual characters explicitly at testing time. Our\nlocal mid-level features can then be aggregated to produce a global word image\nsignature. When pairing these features with the recent word attributes\nframework of Almaz\\'an et al., we obtain results comparable with or better than\nthe state-of-the-art on matching and recognition tasks using global descriptors\nof only 96 dimensions. \n\n"}
{"id": "1410.5263", "contents": "Title: Building pattern recognition applications with the SPARE library Abstract: This paper presents the SPARE C++ library, an open source software tool\nconceived to build pattern recognition and soft computing systems. The library\nfollows the requirement of the generality: most of the implemented algorithms\nare able to process user-defined input data types transparently, such as\nlabeled graphs and sequences of objects, as well as standard numeric vectors.\nHere we present a high-level picture of the SPARE library characteristics,\nfocusing instead on the specific practical possibility of constructing pattern\nrecognition systems for different input data types. In particular, as a proof\nof concept, we discuss two application instances involving clustering of\nreal-valued multidimensional sequences and classification of labeled graphs. \n\n"}
{"id": "1411.0022", "contents": "Title: Generalized Adaptive Dictionary Learning via Domain Shift Minimization Abstract: Visual data driven dictionaries have been successfully employed for various\nobject recognition and classification tasks. However, the task becomes more\nchallenging if the training and test data are from contrasting domains. In this\npaper, we propose a novel and generalized approach towards learning an adaptive\nand common dictionary for multiple domains. Precisely, we project the data from\ndifferent domains onto a low dimensional space while preserving the intrinsic\nstructure of data from each domain. We also minimize the domain-shift among the\ndata from each pair of domains. Simultaneously, we learn a common adaptive\ndictionary. Our algorithm can also be modified to learn class-specific\ndictionaries which can be used for classification. We additionally propose a\ndiscriminative manifold regularization which imposes the intrinsic structure of\nclass specific features onto the sparse coefficients. Experiments on image\nclassification show that our approach fares better compared to the existing\nstate-of-the-art methods. \n\n"}
{"id": "1411.0026", "contents": "Title: Sensitivity Increases for the TITAN Decay Spectroscopy Program Abstract: The TITAN facility at TRIUMF has recently initiated a program of performing\ndecay spectroscopy measurements in an electron-beam ion-trap (EBIT). The unique\nenvironment of the EBIT provides backing-free storage of the radioactive ions,\nwhile guiding charged decay particles from the trap centre via the strong\nmagnetic field. This measurement technique is able to provide a significant\nincrease in detection sensitivity for photons which result from radioactive\ndecay. A brief overview of this device is presented, along with methods of\nimproving the signal-to-background ratio for photon detection by reducing\nCompton scattered events, and eliminating vibrational noise. \n\n"}
{"id": "1411.1962", "contents": "Title: Large Magnetic Shielding Factor Measured by Nonlinear Magneto-optical\n  Rotation Abstract: A passive magnetic shield was designed and constructed for magnetometer tests\nfor the future neutron electric dipole moment experiment at TRIUMF. The axial\nshielding factor of the magnetic shield was measured using a magnetometer based\non non-linear magneto-optical rotation of the plane of polarized laser light\nupon passage through a paraffin-coated vapour cell containing natural Rb at\nroom temperature. The laser was tuned to the Rb D1 line, near the $^{85}$Rb\n$F=2\\rightarrow 2,3$ transition. The shielding factor was measured by applying\nan axial field externally and measuring the magnetic field internally using the\nmagnetometer. The axial shielding factor was determined to be $(1.3\\pm\n0.1)\\times 10^{7}$, from an applied axial field of 1.45~$\\mu$T in the\nbackground of Earth's magnetic field. \n\n"}
{"id": "1411.3230", "contents": "Title: Sparse Modeling for Image and Vision Processing Abstract: In recent years, a large amount of multi-disciplinary research has been\nconducted on sparse models and their applications. In statistics and machine\nlearning, the sparsity principle is used to perform model selection---that is,\nautomatically selecting a simple model among a large collection of them. In\nsignal processing, sparse coding consists of representing data with linear\ncombinations of a few dictionary elements. Subsequently, the corresponding\ntools have been widely adopted by several scientific communities such as\nneuroscience, bioinformatics, or computer vision. The goal of this monograph is\nto offer a self-contained view of sparse modeling for visual recognition and\nimage processing. More specifically, we focus on applications where the\ndictionary is learned and adapted to data, yielding a compact representation\nthat has been successful in various contexts. \n\n"}
{"id": "1411.4524", "contents": "Title: Evidence of delayed light emission of TetraPhenyl Butadiene excited by\n  liquid Argon scintillation light Abstract: TetraPhenyl Butadiene is the wavelength shifter most widely used in\ncombination with liquid Argon. The latter emits scintillation photons with a\nwavelength of 127 nm that need to be downshifted to be detected by\nphotomultipliers with glass or quartz windows. TetraPhenyl Butadiene has been\ndemonstrated to have an extremely high conversion efficiency, possibly higher\nthan 100 % for 127 nm photons, while there is no precise information about the\ntime dependence of its emission. It is usually assumed to be exponentially\ndecaying with a characteristic time of the order of one ns, as an extrapolation\nfrom measurements with exciting radiation in the near UV. This work shows that\nTetraPhenyl Butadiene, when excited by 127 nm photons, reemits photons not only\nwith a very short decay time, but also with slower ones due to triplet states\nde-excitations. This fact can strongly contribute to clarify the anomalies in\nliquid Argon scintillation light reported in literature since seventies, namely\nthe inconsistency in the measured values of the long decay time constant and\nthe appearance of an intermediate component. Similar effects should be also\nexpected when the TPB is used in combination with Helium and Neon, that emit\nscintillation photons with wavelengths shorter than 127 nm. \n\n"}
{"id": "1411.4952", "contents": "Title: From Captions to Visual Concepts and Back Abstract: This paper presents a novel approach for automatically generating image\ndescriptions: visual detectors, language models, and multimodal similarity\nmodels learnt directly from a dataset of image captions. We use multiple\ninstance learning to train visual detectors for words that commonly occur in\ncaptions, including many different parts of speech such as nouns, verbs, and\nadjectives. The word detector outputs serve as conditional inputs to a\nmaximum-entropy language model. The language model learns from a set of over\n400,000 image descriptions to capture the statistics of word usage. We capture\nglobal semantics by re-ranking caption candidates using sentence-level features\nand a deep multimodal similarity model. Our system is state-of-the-art on the\nofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When\nhuman judges compare the system captions to ones written by other people on our\nheld-out test set, the system captions have equal or better quality 34% of the\ntime. \n\n"}
{"id": "1411.5190", "contents": "Title: A Pooling Approach to Modelling Spatial Relations for Image Retrieval\n  and Annotation Abstract: Over the last two decades we have witnessed strong progress on modeling\nvisual object classes, scenes and attributes that have significantly\ncontributed to automated image understanding. On the other hand, surprisingly\nlittle progress has been made on incorporating a spatial representation and\nreasoning in the inference process. In this work, we propose a pooling\ninterpretation of spatial relations and show how it improves image retrieval\nand annotations tasks involving spatial language. Due to the complexity of the\nspatial language, we argue for a learning-based approach that acquires a\nrepresentation of spatial relations by learning parameters of the pooling\noperator. We show improvements on previous work on two datasets and two\ndifferent tasks as well as provide additional insights on a new dataset with an\nexplicit focus on spatial relations. \n\n"}
{"id": "1411.5654", "contents": "Title: Learning a Recurrent Visual Representation for Image Caption Generation Abstract: In this paper we explore the bi-directional mapping between images and their\nsentence-based descriptions. We propose learning this mapping using a recurrent\nneural network. Unlike previous approaches that map both sentences and images\nto a common embedding, we enable the generation of novel sentences given an\nimage. Using the same model, we can also reconstruct the visual features\nassociated with an image given its visual description. We use a novel recurrent\nvisual memory that automatically learns to remember long-term visual concepts\nto aid in both sentence generation and visual feature reconstruction. We\nevaluate our approach on several tasks. These include sentence generation,\nsentence retrieval and image retrieval. State-of-the-art results are shown for\nthe task of generating novel image descriptions. When compared to human\ngenerated captions, our automatically generated captions are preferred by\nhumans over $19.8\\%$ of the time. Results are better than or comparable to\nstate-of-the-art results on the image and sentence retrieval tasks for methods\nusing similar visual features. \n\n"}
{"id": "1411.6406", "contents": "Title: Encoding High Dimensional Local Features by Sparse Coding Based Fisher\n  Vectors Abstract: Deriving from the gradient vector of a generative model of local features,\nFisher vector coding (FVC) has been identified as an effective coding method\nfor image classification. Most, if not all, % FVC implementations employ the\nGaussian mixture model (GMM) to characterize the generation process of local\nfeatures. This choice has shown to be sufficient for traditional low\ndimensional local features, e.g., SIFT; and typically, good performance can be\nachieved with only a few hundred Gaussian distributions. However, the same\nnumber of Gaussians is insufficient to model the feature space spanned by\nhigher dimensional local features, which have become popular recently. In order\nto improve the modeling capacity for high dimensional features, it turns out to\nbe inefficient and computationally impractical to simply increase the number of\nGaussians. In this paper, we propose a model in which each local feature is\ndrawn from a Gaussian distribution whose mean vector is sampled from a\nsubspace. With certain approximation, this model can be converted to a sparse\ncoding procedure and the learning/inference problems can be readily solved by\nstandard sparse coding methods. By calculating the gradient vector of the\nproposed model, we derive a new fisher vector encoding strategy, termed Sparse\nCoding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently\ndeveloped Deep Convolutional Neural Network (CNN) descriptor as a high\ndimensional local feature and implement image classification with the proposed\nSCFVC. Our experimental evaluations demonstrate that our method not only\nsignificantly outperforms the traditional GMM based Fisher vector encoding but\nalso achieves the state-of-the-art performance in generic object recognition,\nindoor scene, and fine-grained image classification problems. \n\n"}
{"id": "1412.0619", "contents": "Title: Low-Mass Dielectron Production in pp, p-Pb and Pb-Pb Collisions with\n  ALICE Abstract: The ALICE Collaboration measures the production of low-mass dielectrons in\npp, p-Pb and Pb-Pb collisions at the LHC. The main detectors used in the\nanalyses are the Inner Tracking System, Time Projection Chamber and\nTime-Of-Flight detector, all located around mid-rapidity. The production of\nvirtual photons relative to the inclusive yield in pp collisions is determined\nby analyzing the dielectron excess with respect to the expected hadronic\nsources. The direct photon cross section is then calculated and found to be in\nagreement with NLO pQCD calculations. Results from the invariant mass analysis\nin p-Pb collisions show an overall agreement between data and hadronic\ncocktail. In Pb-Pb collisions, uncorrected background-subtracted yields have\nbeen extracted in two centrality classes. A feasibility study for LHC run 3\nafter the ALICE upgrade indicates the possibility for a future measurement of\nthe early effective temperature. \n\n"}
{"id": "1412.0727", "contents": "Title: Parameterization of the statistical rate function for select\n  superallowed transitions Abstract: We present a parameterization of the statistical rate function, f, for 20\nsuperallowed 0+-to-0+ nuclear beta transitions between T=1 analog states, and\nfor 18 superallowed \"mirror\" transitions between analog T=1/2 states. All these\ntransitions are of interest in the determination of V_{ud}. Although most of\nthe transition Q_{EC} values have been measured, their precision will\nundoubtedly be improved in future. Our parameterization allows a user to easily\ncalculate the corresponding new f value to high precision (+/-0.01%) without\ncomplicated computing. \n\n"}
{"id": "1412.1144", "contents": "Title: An RF-only ion-funnel for extraction from high-pressure gases Abstract: An RF ion-funnel technique has been developed to extract ions from a\nhigh-pressure (10 bar) noble-gas environment into vacuum ($10^{-6}$ mbar).\nDetailed simulations have been performed and a prototype has been developed for\nthe purpose of extracting $^{136}$Ba ions from Xe gas with high efficiency.\nWith this prototype, ions have been extracted for the first time from\nhigh-pressure xenon gas and argon gas. Systematic studies have been carried out\nand compared to the simulations. This demonstration of extraction of ions with\nmass comparable to that of the gas generating the high-pressure into vacuum has\napplications to Ba tagging from a Xe-gas time-projection chamber (TPC) for\ndouble beta decay as well as to the general problem of recovering trace amounts\nof an ionized element in a heavy (m$>40$ u) carrier gas. \n\n"}
{"id": "1412.4102", "contents": "Title: Representing Data by a Mixture of Activated Simplices Abstract: We present a new model which represents data as a mixture of simplices.\nSimplices are geometric structures that generalize triangles. We give a simple\ngeometric understanding that allows us to learn a simplicial structure\nefficiently. Our method requires that the data are unit normalized (and thus\nlie on the unit sphere). We show that under this restriction, building a model\nwith simplices amounts to constructing a convex hull inside the sphere whose\nboundary facets is close to the data. We call the boundary facets of the convex\nhull that are close to the data Activated Simplices. While the total number of\nbases used to build the simplices is a parameter of the model, the dimensions\nof the individual activated simplices are learned from the data. Simplices can\nhave different dimensions, which facilitates modeling of inhomogeneous data\nsources. The simplicial structure is bounded --- this is appropriate for\nmodeling data with constraints, such as human elbows can not bend more than 180\ndegrees. The simplices are easy to interpret and extremes within the data can\nbe discovered among the vertices. The method provides good reconstruction and\nregularization. It supports good nearest neighbor classification and it allows\nrealistic generative models to be constructed. It achieves state-of-the-art\nresults on benchmark datasets, including 3D poses and digits. \n\n"}
{"id": "1412.4438", "contents": "Title: Fixed Point Algorithm Based on Quasi-Newton Method for Convex\n  Minimization Problem with Application to Image Deblurring Abstract: Solving an optimization problem whose objective function is the sum of two\nconvex functions has received considerable interests in the context of image\nprocessing recently. In particular, we are interested in the scenario when a\nnon-differentiable convex function such as the total variation (TV) norm is\nincluded in the objective function due to many variational models established\nin image processing have this nature. In this paper, we propose a fast fixed\npoint algorithm based on the quasi-Newton method for solving this class of\nproblem, and apply it in the field of TV-based image deblurring. The novel\nmethod is derived from the idea of the quasi-Newton method, and the fixed-point\nalgorithms based on the proximity operator, which were widely investigated very\nrecently. Utilizing the non-expansion property of the proximity operator we\nfurther investigate the global convergence of the proposed algorithm. Numerical\nexperiments on image deblurring problem with additive or multiplicative noise\nare presented to demonstrate that the proposed algorithm is superior to the\nrecently developed fixed-point algorithm in the computational efficiency. \n\n"}
{"id": "1412.5903", "contents": "Title: Deep Structured Output Learning for Unconstrained Text Recognition Abstract: We develop a representation suitable for the unconstrained recognition of\nwords in natural images: the general case of no fixed lexicon and unknown\nlength.\n  To this end we propose a convolutional neural network (CNN) based\narchitecture which incorporates a Conditional Random Field (CRF) graphical\nmodel, taking the whole word image as a single input. The unaries of the CRF\nare provided by a CNN that predicts characters at each position of the output,\nwhile higher order terms are provided by another CNN that detects the presence\nof N-grams. We show that this entire model (CRF, character predictor, N-gram\npredictor) can be jointly optimised by back-propagating the structured output\nloss, essentially requiring the system to perform multi-task learning, and\ntraining uses purely synthetically generated data. The resulting model is a\nmore accurate system on standard real-world text recognition benchmarks than\ncharacter prediction alone, setting a benchmark for systems that have not been\ntrained on a particular lexicon. In addition, our model achieves\nstate-of-the-art accuracy in lexicon-constrained scenarios, without being\nspecifically modelled for constrained recognition. To test the generalisation\nof our model, we also perform experiments with random alpha-numeric strings to\nevaluate the method when no visual language model is applicable. \n\n"}
{"id": "1412.6963", "contents": "Title: A novel method for the line-of-response and time-of-flight\n  reconstruction in TOF-PET detectors based on a library of synchronized model\n  signals Abstract: A novel method of hit time and hit position reconstruction in scintillator\ndetectors is described. The method is based on comparison of detector signals\nwith results stored in a library of synchronized model signals registered for a\nset of well-defined positions of scintillation points. The hit position is\nreconstructed as the one corresponding to the signal from the library which is\nmost similar to the measurement signal. The time of the interaction is\ndetermined as a relative time between the measured signal and the most similar\none in the library. A degree of similarity of measured and model signals is\ndefined as the distance between points representing the measurement- and\nmodel-signal in the multi-dimensional measurement space. Novelty of the method\nlies also in the proposed way of synchronization of model signals enabling\ndirect determination of the difference between time-of-flights (TOF) of\nannihilation quanta from the annihilation point to the detectors. The\nintroduced method was validated using experimental data obtained by means of\nthe double strip prototype of the J-PET detector and $^{22}$Na sodium isotope\nas a source of annihilation gamma quanta.The detector was built out from\nplastic scintillator strips with dimensions of 5 mm x 19 mm x 300 mm, optically\nconnected at both sides to photomultipliers,from which signals were sampled by\nmeans of the Serial Data Analyzer.Using the introduced method, the spatial and\nTOF resolution of about 1.3 cm ($\\sigma$) and 125 ps ($\\sigma$) were\nestablished, respectively. \n\n"}
{"id": "1412.8307", "contents": "Title: Fast, simple and accurate handwritten digit classification by training\n  shallow neural network classifiers with the 'extreme learning machine'\n  algorithm Abstract: Recent advances in training deep (multi-layer) architectures have inspired a\nrenaissance in neural network use. For example, deep convolutional networks are\nbecoming the default option for difficult tasks on large datasets, such as\nimage and speech recognition. However, here we show that error rates below 1%\non the MNIST handwritten digit benchmark can be replicated with shallow\nnon-convolutional neural networks. This is achieved by training such networks\nusing the 'Extreme Learning Machine' (ELM) approach, which also enables a very\nrapid training time (~10 minutes). Adding distortions, as is common practise\nfor MNIST, reduces error rates even further. Our methods are also shown to be\ncapable of achieving less than 5.5% error rates on the NORB image database. To\nachieve these results, we introduce several enhancements to the standard ELM\nalgorithm, which individually and in combination can significantly improve\nperformance. The main innovation is to ensure each hidden-unit operates only on\na randomly sized and positioned patch of each image. This form of random\n`receptive field' sampling of the input ensures the input weight matrix is\nsparse, with about 90% of weights equal to zero. Furthermore, combining our\nmethods with a small number of iterations of a single-batch backpropagation\nmethod can significantly reduce the number of hidden-units required to achieve\na particular performance. Our close to state-of-the-art results for MNIST and\nNORB suggest that the ease of use and accuracy of the ELM algorithm for\ndesigning a single-hidden-layer neural network classifier should cause it to be\ngiven greater consideration either as a standalone method for simpler problems,\nor as the final classification stage in deep neural networks applied to more\ndifficult problems. \n\n"}
{"id": "1501.00092", "contents": "Title: Image Super-Resolution Using Deep Convolutional Networks Abstract: We propose a deep learning method for single image super-resolution (SR). Our\nmethod directly learns an end-to-end mapping between the low/high-resolution\nimages. The mapping is represented as a deep convolutional neural network (CNN)\nthat takes the low-resolution image as the input and outputs the\nhigh-resolution one. We further show that traditional sparse-coding-based SR\nmethods can also be viewed as a deep convolutional network. But unlike\ntraditional methods that handle each component separately, our method jointly\noptimizes all layers. Our deep CNN has a lightweight structure, yet\ndemonstrates state-of-the-art restoration quality, and achieves fast speed for\npractical on-line usage. We explore different network structures and parameter\nsettings to achieve trade-offs between performance and speed. Moreover, we\nextend our network to cope with three color channels simultaneously, and show\nbetter overall reconstruction quality. \n\n"}
{"id": "1501.01832", "contents": "Title: Parity violation in neutron capture on the proton: determining the weak\n  pion-nucleon coupling Abstract: We investigate the parity-violating analyzing power in neutron capture on the\nproton at thermal energies in the framework of chiral effective field theory.\nBy combining this analysis with a previous analysis of parity violation in\nproton-proton scattering, we are able to extract the size of the weak\npion-nucleon coupling constant. The uncertainty is significant and dominated by\nthe experimental error which is expected to be reduced soon. \n\n"}
{"id": "1501.07738", "contents": "Title: Co-Regularized Deep Representations for Video Summarization Abstract: Compact keyframe-based video summaries are a popular way of generating\nviewership on video sharing platforms. Yet, creating relevant and compelling\nsummaries for arbitrarily long videos with a small number of keyframes is a\nchallenging task. We propose a comprehensive keyframe-based summarization\nframework combining deep convolutional neural networks and restricted Boltzmann\nmachines. An original co-regularization scheme is used to discover meaningful\nsubject-scene associations. The resulting multimodal representations are then\nused to select highly-relevant keyframes. A comprehensive user study is\nconducted comparing our proposed method to a variety of schemes, including the\nsummarization currently in use by one of the most popular video sharing\nwebsites. The results show that our method consistently outperforms the\nbaseline schemes for any given amount of keyframes both in terms of\nattractiveness and informativeness. The lead is even more significant for\nsmaller summaries. \n\n"}
{"id": "1502.00144", "contents": "Title: Direct probes of neutrino mass Abstract: The discovery of neutrino oscillations has shown that neutrinos, in\ncontradiction to a prediction of the minimal standard model, have mass.\nOscillations do not yield a value for the mass, but do set a lower limit of\n0.02 eV on the average of the 3 known eigenmasses. Moreover, they make it\npossible to determine or limit all 3 masses from measurements of\nelectron-flavor neutrinos in beta decay. The present upper limit from such\nmeasurements is 2 eV. We review the status of laboratory work toward closing\nthe remaining window between 2 and 0.02 eV, and measuring the mass. \n\n"}
{"id": "1502.00339", "contents": "Title: Quantifying properties of hot and dense QCD matter through systematic\n  model-to-data comparison Abstract: We systematically compare an event-by-event heavy-ion collision model to data\nfrom the Large Hadron Collider. Using a general Bayesian method, we probe\nmultiple model parameters including fundamental quark-gluon plasma properties\nsuch as the specific shear viscosity $\\eta/s$, calibrate the model to optimally\nreproduce experimental data, and extract quantitative constraints for all\nparameters simultaneously. The method is universal and easily extensible to\nother data and collision models. \n\n"}
{"id": "1502.02901", "contents": "Title: A pilot study of the novel J-PET plastic scintillator with\n  2-(4-styrylphenyl)benzoxazole as a wavelength shifter Abstract: For the first time a molecule of 2-(4-styrylphenyl)benzoxazole containing\nbenzoxazole and stilbene groups is applied as a scintillator dopant acting as a\nwavelength shifter. In this article a light yield of the plastic scintillator,\nprepared from styrene doped with 2 wt% of 2,5-diphenylbenzoxazole and 0.03 wt%\nof 2-(4-styrylphenyl)benzoxazole, is determined to be as large as 60% $\\pm$ 2%\nof the anthracene light output. There is a potential to improve this value in\nthe future by the optimization of the additives concentrations. \n\n"}
{"id": "1502.03174", "contents": "Title: Low Background Signal Readout Electronics for the MAJORANA DEMONSTRATOR Abstract: The MAJORANA DEMONSTRATOR is a planned 40 kg array of Germanium detectors\nintended to demonstrate the feasibility of constructing a tonne-scale\nexperiment that will seek neutrinoless double beta decay ($0\\nu\\beta\\beta$) in\n$^{76}\\mathrm{Ge}$. Such an experiment would require backgrounds of less than 1\ncount/tonne-year in the 4 keV region of interest around the 2039 keV Q-value of\nthe $\\beta\\beta$ decay. Designing low-noise electronics, which must be placed\nin close proximity to the detectors, presents a challenge to reaching this\nbackground target. This paper will discuss the MAJORANA collaboration's\nsolutions to some of these challenges. \n\n"}
{"id": "1502.03687", "contents": "Title: Low-energy antiproton physics and the FLAIR facility Abstract: FLAIR, the Facility for Low-energy Antiproton and Ion Research has been\nproposed in 2004 as an extension of the planned FAIR facility at Darmstadt,\nGermany. FLAIR was not included into the Modularized Start Version of FAIR, but\nthe recent installation of the CRYRING storage ring at GSI Darmstadt has opened\nnew perspectives for physics with low-energy antiprotons at FAIR. \n\n"}
{"id": "1502.04392", "contents": "Title: Improvement of the Energy Resolution via an Optimized Digital Signal\n  Processing in GERDA Phase I Abstract: An optimized digital shaping filter has been developed for the GERDA\nexperiment which searches for neutrinoless double beta decay in 76Ge. The GERDA\nPhase I energy calibration data have been reprocessed and an average\nimprovement of 0.3 keV in energy resolution (FWHM) at the 76Ge Q value for\n0\\nu\\beta\\beta decay is obtained. This is possible thanks to the enhanced\nlow-frequency noise rejection of this Zero Area Cusp (ZAC) signal shaping\nfillter. \n\n"}
{"id": "1502.04492", "contents": "Title: Towards Building Deep Networks with Bayesian Factor Graphs Abstract: We propose a Multi-Layer Network based on the Bayesian framework of the\nFactor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional\nlattice. The Latent Variable Model (LVM) is the basic building block of a\nquadtree hierarchy built on top of a bottom layer of random variables that\nrepresent pixels of an image, a feature map, or more generally a collection of\nspatially distributed discrete variables. The multi-layer architecture\nimplements a hierarchical data representation that, via belief propagation, can\nbe used for learning and inference. Typical uses are pattern completion,\ncorrection and classification. The FGrn paradigm provides great flexibility and\nmodularity and appears as a promising candidate for building deep networks: the\nsystem can be easily extended by introducing new and different (in cardinality\nand in type) variables. Prior knowledge, or supervised information, can be\nintroduced at different scales. The FGrn paradigm provides a handy way for\nbuilding all kinds of architectures by interconnecting only three types of\nunits: Single Input Single Output (SISO) blocks, Sources and Replicators. The\nnetwork is designed like a circuit diagram and the belief messages flow\nbidirectionally in the whole system. The learning algorithms operate only\nlocally within each block. The framework is demonstrated in this paper in a\nthree-layer structure applied to images extracted from a standard data set. \n\n"}
{"id": "1502.05243", "contents": "Title: SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks Abstract: The task of classifying videos of natural dynamic scenes into appropriate\nclasses has gained lot of attention in recent years. The problem especially\nbecomes challenging when the camera used to capture the video is dynamic. In\nthis paper, we analyse the performance of statistical aggregation (SA)\ntechniques on various pre-trained convolutional neural network(CNN) models to\naddress this problem. The proposed approach works by extracting CNN activation\nfeatures for a number of frames in a video and then uses an aggregation scheme\nin order to obtain a robust feature descriptor for the video. We show through\nresults that the proposed approach performs better than the-state-of-the arts\nfor the Maryland and YUPenn dataset. The final descriptor obtained is powerful\nenough to distinguish among dynamic scenes and is even capable of addressing\nthe scenario where the camera motion is dominant and the scene dynamics are\ncomplex. Further, this paper shows an extensive study on the performance of\nvarious aggregation methods and their combinations. We compare the proposed\napproach with other dynamic scene classification algorithms on two publicly\navailable datasets - Maryland and YUPenn to demonstrate the superior\nperformance of the proposed approach. \n\n"}
{"id": "1502.05742", "contents": "Title: Application of Independent Component Analysis Techniques in Speckle\n  Noise Reduction of Retinal OCT Images Abstract: Optical Coherence Tomography (OCT) is an emerging technique in the field of\nbiomedical imaging, with applications in ophthalmology, dermatology, coronary\nimaging etc. OCT images usually suffer from a granular pattern, called speckle\nnoise, which restricts the process of interpretation. Therefore the need for\nspeckle noise reduction techniques is of high importance. To the best of our\nknowledge, use of Independent Component Analysis (ICA) techniques has never\nbeen explored for speckle reduction of OCT images. Here, a comparative study of\nseveral ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noise\nreduction of retinal OCT images. Having multiple B-scans of the same location,\nthe eye movements are compensated using a rigid registration technique. Then,\ndifferent ICA techniques are applied to the aggregated set of B-scans for\nextracting the noise-free image. Signal-to-Noise-Ratio (SNR),\nContrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well as\nanalysis on the computational complexity of the methods, are considered as\nmetrics for comparison. The results show that use of ICA can be beneficial,\nespecially in case of having fewer number of B-scans. \n\n"}
{"id": "1502.07478", "contents": "Title: GPU accelerated image reconstruction in a two-strip J-PET tomograph Abstract: We present a fast GPU implementation of the image reconstruction routine, for\na novel two strip PET detector that relies solely on the time of flight\nmeasurements. \n\n"}
{"id": "1502.07886", "contents": "Title: Hit time and hit position reconstruction in the J-PET detector based on\n  a library of averaged model signals Abstract: In this article we present a novel method of hit time and hit position\nreconstruction in long scintillator detectors. We take advantage of the fact\nthat for this kind of detectors amplitude and shape of registered signals\ndepends strongly on the position where particle hit the detector. The\nreconstruction is based on determination of the degree of similarity between\nmeasured and averaged signals stored in a library for a set of well-defined\npositions along the scintillator. Preliminary results of validation of the\nintroduced method with experimental data obtained by means of the double strip\nprototype of the J-PET detector are presented. \n\n"}
{"id": "1502.08039", "contents": "Title: Probabilistic Zero-shot Classification with Semantic Rankings Abstract: In this paper we propose a non-metric ranking-based representation of\nsemantic similarity that allows natural aggregation of semantic information\nfrom multiple heterogeneous sources. We apply the ranking-based representation\nto zero-shot learning problems, and present deterministic and probabilistic\nzero-shot classifiers which can be built from pre-trained classifiers without\nretraining. We demonstrate their the advantages on two large real-world image\ndatasets. In particular, we show that aggregating different sources of semantic\ninformation, including crowd-sourcing, leads to more accurate classification. \n\n"}
{"id": "1503.01224", "contents": "Title: Temporal Pyramid Pooling Based Convolutional Neural Networks for Action\n  Recognition Abstract: Encouraged by the success of Convolutional Neural Networks (CNNs) in image\nclassification, recently much effort is spent on applying CNNs to video based\naction recognition problems. One challenge is that video contains a varying\nnumber of frames which is incompatible to the standard input format of CNNs.\nExisting methods handle this issue either by directly sampling a fixed number\nof frames or bypassing this issue by introducing a 3D convolutional layer which\nconducts convolution in spatial-temporal domain.\n  To solve this issue, here we propose a novel network structure which allows\nan arbitrary number of frames as the network input. The key of our solution is\nto introduce a module consisting of an encoding layer and a temporal pyramid\npooling layer. The encoding layer maps the activation from previous layers to a\nfeature vector suitable for pooling while the temporal pyramid pooling layer\nconverts multiple frame-level activations into a fixed-length video-level\nrepresentation. In addition, we adopt a feature concatenation layer which\ncombines appearance information and motion information. Compared with the frame\nsampling strategy, our method avoids the risk of missing any important frames.\nCompared with the 3D convolutional method which requires a huge video dataset\nfor network training, our model can be learned on a small target dataset\nbecause we can leverage the off-the-shelf image-level CNN for model parameter\ninitialization. Experiments on two challenging datasets, Hollywood2 and HMDB51,\ndemonstrate that our method achieves superior performance over state-of-the-art\nmethods while requiring much fewer training data. \n\n"}
{"id": "1503.02243", "contents": "Title: Mississippi State Axion Search: A Light Shining though a Wall ALP Search Abstract: The elegant solutions to the strong CP problem predict the existence of a\nparticle called axion. Thus, the search for axion like particles (ALP) has been\nan ongoing endeavor. The possibility that these axion like particles couple to\nphotons in presence of magnetic field gives rise to a technique of detecting\nthese particles known as light shining through a wall (LSW). Mississippi State\nAxion Search (MASS) is an experiment employing the LSW technique in search for\naxion like particles. The apparatus consists of two radio frequency (RF)\ncavities, both under the influence of strong magnetic field and separated by a\nlead wall. While one of the cavities houses a strong RF generator, the other\ncavity houses the detector systems. The MASS apparatus looks for excesses in RF\nphotons that tunnel through the wall as a signature of candidate axion-like\nparticles. The concept behind the experiment as well as the projected\nsensitivities are presented here. \n\n"}
{"id": "1503.02302", "contents": "Title: DESAT: an SSW tool for SDO/AIA image de-saturation Abstract: Saturation affects a significant rate of images recorded by the Atmospheric\nImaging Assembly on the Solar Dynamics Observatory. This paper describes a\ncomputational method and a technological pipeline for the de-saturation of such\nimages, based on several mathematical ingredients like Expectation\nMaximization, image correlation and interpolation. An analysis of the\ncomputational properties and demands of the pipeline, together with an\nassessment of its reliability are performed against a set of data recorded from\nthe Feburary 25 2014 flaring event. \n\n"}
{"id": "1503.03138", "contents": "Title: A Threshold Gas \\v{C}erenkov Detector for the Spin Asymmetries of the\n  Nucleon Experiment Abstract: We report on the design, construction, commissioning, and performance of a\nthreshold gas \\v{C}erenkov counter in an open configuration, which operates in\na high luminosity environment and produces a high photo-electron yield. Part of\na unique open geometry detector package known as the Big Electron Telescope\nArray, this \\v{C}erenkov counter served to identify scattered electrons and\nreject produced pions in an inclusive scattering experiment known as the Spin\nAsymmetries of the Nucleon Experiment E07-003 at the Thomas Jefferson National\nAccelerator Facility (TJNAF) also known as Jefferson Lab. The experiment\nconsisted of a measurement of double spin asymmetries $A_{\\parallel}$ and\n$A_{\\perp}$ of a polarized electron beam impinging on a polarized ammonia\ntarget. The \\v{C}erenkov counter's performance is characterised by a yield of\nabout 20 photoelectrons per electron or positron track. Thanks to this large\nnumber of photoelectrons per track, the \\v{C}erenkov counter had enough\nresolution to identify electron-positron pairs from the conversion of photons\nresulting mainly from $\\pi^0$ decays. \n\n"}
{"id": "1503.07064", "contents": "Title: Comments on the radial distribution of charged particles in a magnetic\n  field Abstract: Magnetic guiding fields in combination with energy dispersive semiconductor\ndetectors have been employed already more than 50 years ago for in-beam\ninternal conversion electron spectroscopy. Even then it was recognized that\nefficiency modulations may appear as function of the electron energy, arising\nwhen electrons hit a baffle or miss the sensitive area of the detector. Current\nhigh precision beta decay experiments of polarized neutrons with conceptional\nsimilar experimental devices resulted in a detailed study of the point spread\nfunction (PSF). The latter describes the radial probability distribution of\nmono-energetic electrons at the detector plane. Singularities occur as function\nof the radial detector coordinate which have been investigated and discussed by\nSjue at al. (Rev. Scient. Instr. 86, 023102 (2015)), and Dubbers\n(arXiv:1501.05131v1 [physics.ins-det]). In this comment a rather precise\nnumerical representation of the PSF is presented and compared with\napproximations in the mentioned papers. \n\n"}
{"id": "1503.07790", "contents": "Title: Transductive Multi-label Zero-shot Learning Abstract: Zero-shot learning has received increasing interest as a means to alleviate\nthe often prohibitive expense of annotating training data for large scale\nrecognition problems. These methods have achieved great success via learning\nintermediate semantic representations in the form of attributes and more\nrecently, semantic word vectors. However, they have thus far been constrained\nto the single-label case, in contrast to the growing popularity and importance\nof more realistic multi-label data. In this paper, for the first time, we\ninvestigate and formalise a general framework for multi-label zero-shot\nlearning, addressing the unique challenge therein: how to exploit multi-label\ncorrelation at test time with no training data for those classes? In\nparticular, we propose (1) a multi-output deep regression model to project an\nimage into a semantic word space, which explicitly exploits the correlations in\nthe intermediate semantic layer of word vectors; (2) a novel zero-shot learning\nalgorithm for multi-label data that exploits the unique compositionality\nproperty of semantic word vector representations; and (3) a transductive\nlearning strategy to enable the regression model learned from seen classes to\ngeneralise well to unseen classes. Our zero-shot learning experiments on a\nnumber of standard multi-label datasets demonstrate that our method outperforms\na variety of baselines. \n\n"}
{"id": "1504.04719", "contents": "Title: Selecting specific initial configuration using spectator neutrons in U+U\n  collisions Abstract: We present a method to select events with specific initial configuration,\nnamely body-tip, in heavy-ion collisions using deformed Uranium nuclei. We\npropose to use asymmetry in spectator neutron numbers to filter out these\nbody-tip events from the unbiased configurations in U+U collisions. We have\nused a variable S_eta to differentiate between the body-tip and unbiased\nconfigurations. We have calculated the second order azimuthal anisotropy,\nnamely elliptic flow (v2), for this body-tip configuration in the framework of\na transport model and found it to be consistently lower compared to that in\nunbiased configurations as we expected. The purity of selecting such events in\na real experiment is also discussed. \n\n"}
{"id": "1504.06787", "contents": "Title: Max-margin Deep Generative Models Abstract: Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, little work has been done on\nexamining or empowering the discriminative ability of DGMs on making accurate\npredictions. This paper presents max-margin deep generative models (mmDGMs),\nwhich explore the strongly discriminative principle of max-margin learning to\nimprove the discriminative power of DGMs, while retaining the generative\ncapability. We develop an efficient doubly stochastic subgradient algorithm for\nthe piecewise linear objective. Empirical results on MNIST and SVHN datasets\ndemonstrate that (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; and\n(2) mmDGMs are competitive to the state-of-the-art fully discriminative\nnetworks by employing deep convolutional neural networks (CNNs) as both\nrecognition and generative models. \n\n"}
{"id": "1504.07488", "contents": "Title: Speeding Up Neural Networks for Large Scale Classification using WTA\n  Hashing Abstract: In this paper we propose to use the Winner Takes All hashing technique to\nspeed up forward propagation and backward propagation in fully connected layers\nin convolutional neural networks. The proposed technique reduces significantly\nthe computational complexity, which in turn, allows us to train layers with a\nlarge number of kernels with out the associated time penalty.\n  As a consequence we are able to train convolutional neural network on a very\nlarge number of output classes with only a small increase in the computational\ncost. To show the effectiveness of the technique we train a new output layer on\na pretrained network using both the regular multiplicative approach and our\nproposed hashing methodology. Our results showed no drop in performance and\ndemonstrate, with our implementation, a 7 fold speed up during the training. \n\n"}
{"id": "1504.07575", "contents": "Title: Becoming the Expert - Interactive Multi-Class Machine Teaching Abstract: Compared to machines, humans are extremely good at classifying images into\ncategories, especially when they possess prior knowledge of the categories at\nhand. If this prior information is not available, supervision in the form of\nteaching images is required. To learn categories more quickly, people should\nsee important and representative images first, followed by less important\nimages later - or not at all. However, image-importance is individual-specific,\ni.e. a teaching image is important to a student if it changes their overall\nability to discriminate between classes. Further, students keep learning, so\nwhile image-importance depends on their current knowledge, it also varies with\ntime.\n  In this work we propose an Interactive Machine Teaching algorithm that\nenables a computer to teach challenging visual concepts to a human. Our\nadaptive algorithm chooses, online, which labeled images from a teaching set\nshould be shown to the student as they learn. We show that a teaching strategy\nthat probabilistically models the student's ability and progress, based on\ntheir correct and incorrect answers, produces better 'experts'. We present\nresults using real human participants across several varied and challenging\nreal-world datasets. \n\n"}
{"id": "1505.00571", "contents": "Title: Higher Order Maximum Persistency and Comparison Theorems Abstract: We address combinatorial problems that can be formulated as minimization of a\npartially separable function of discrete variables (energy minimization in\ngraphical models, weighted constraint satisfaction, pseudo-Boolean\noptimization, 0-1 polynomial programming). For polyhedral relaxations of such\nproblems it is generally not true that variables integer in the relaxed\nsolution will retain the same values in the optimal discrete solution. Those\nwhich do are called persistent. Such persistent variables define a part of a\nglobally optimal solution. Once identified, they can be excluded from the\nproblem, reducing its size.\n  To any polyhedral relaxation we associate a sufficient condition proving\npersistency of a subset of variables. We set up a specially constructed linear\nprogram which determines the set of persistent variables maximal with respect\nto the relaxation. The condition improves as the relaxation is tightened and\npossesses all its invariances. The proposed framework explains a variety of\nexisting methods originating from different areas of research and based on\ndifferent principles. A theoretical comparison is established that relates\nthese methods to the standard linear relaxation and proves that the proposed\ntechnique identifies same or larger set of persistent variables. \n\n"}
{"id": "1505.01953", "contents": "Title: The structure of optimal parameters for image restoration problems Abstract: We study the qualitative properties of optimal regularisation parameters in\nvariational models for image restoration. The parameters are solutions of\nbilevel optimisation problems with the image restoration problem as constraint.\nA general type of regulariser is considered, which encompasses total variation\n(TV), total generalized variation (TGV) and infimal-convolution total variation\n(ICTV). We prove that under certain conditions on the given data optimal\nparameters derived by bilevel optimisation problems exist. A crucial point in\nthe existence proof turns out to be the boundedness of the optimal parameters\naway from $0$ which we prove in this paper. The analysis is done on the\noriginal -- in image restoration typically non-smooth variational problem -- as\nwell as on a smoothed approximation set in Hilbert space which is the one\nconsidered in numerical computations. For the smoothed bilevel problem we also\nprove that it $\\Gamma$ converges to the original problem as the smoothing\nvanishes. All analysis is done in function spaces rather than on the\ndiscretised learning problem. \n\n"}
{"id": "1505.05612", "contents": "Title: Are You Talking to a Machine? Dataset and Methods for Multilingual Image\n  Question Answering Abstract: In this paper, we present the mQA model, which is able to answer questions\nabout the content of an image. The answer can be a sentence, a phrase or a\nsingle word. Our model contains four components: a Long Short-Term Memory\n(LSTM) to extract the question representation, a Convolutional Neural Network\n(CNN) to extract the visual representation, an LSTM for storing the linguistic\ncontext in an answer, and a fusing component to combine the information from\nthe first three components and generate the answer. We construct a Freestyle\nMultilingual Image Question Answering (FM-IQA) dataset to train and evaluate\nour mQA model. It contains over 150,000 images and 310,000 freestyle Chinese\nquestion-answer pairs and their English translations. The quality of the\ngenerated answers of our mQA model on this dataset is evaluated by human judges\nthrough a Turing Test. Specifically, we mix the answers provided by humans and\nour model. The human judges need to distinguish our model from the human. They\nwill also provide a score (i.e. 0, 1, 2, the larger the better) indicating the\nquality of the answer. We propose strategies to monitor the quality of this\nevaluation process. The experiments show that in 64.7% of cases, the human\njudges cannot distinguish our model from humans. The average score is 1.454\n(1.918 for human). The details of this work, including the FM-IQA dataset, can\nbe found on the project page: http://idl.baidu.com/FM-IQA.html \n\n"}
{"id": "1506.00446", "contents": "Title: Observation of gravitationally induced vertical striation of polarized\n  ultracold neutrons by spin-echo spectroscopy Abstract: We describe a spin-echo method for ultracold neutrons (UCNs) confined in a\nprecession chamber and exposed to a $|B_0|=1~\\text{\\mu T}$ magnetic field. We\nhave demonstrated that the analysis of UCN spin-echo resonance signals in\ncombination with knowledge of the ambient magnetic field provides an excellent\nmethod by which to reconstruct the energy spectrum of a confined ensemble of\nneutrons. The method takes advantage of the relative dephasing of spins arising\nfrom a gravitationally induced striation of stored UCN of different energies,\nand also permits an improved determination of the vertical magnetic-field\ngradient with an exceptional accuracy of $1.1~\\text{pT/cm}$. This novel\ncombination of a well-known nuclear resonance method and gravitationally\ninduced vertical striation is unique in the realm of nuclear and particle\nphysics and should prove to be invaluable for the assessment of systematic\neffects in precision experiments such as searches for an electric dipole moment\nof the neutron or the measurement of the neutron lifetime. \n\n"}
{"id": "1506.01938", "contents": "Title: Radioluminescence and photoluminescence of Th:CaF$_2$ crystals Abstract: We study thorium-doped CaF$_2$ crystals as a possible platform for optical\nspectroscopy of the Th-229 nuclear isomer transition. We anticipate two major\nsources of background signal that might cover the nuclear spectroscopy signal:\nVUV-photoluminescence, caused by the probe light, and radioluminescence, caused\nby the radioactive decay of Th-229 and its daughters. We find a rich\nphotoluminescence spectrum at wavelengths above 260 nm, and radioluminescence\nemission above 220 nm. This is very promising, as fluorescence originating from\nthe isomer transition, predicted at a wavelength shorter than 200 nm, could be\nfiltered spectrally from the crystal luminescence. Furthermore, we investigate\nthe temperature-dependent decay time of the luminescence, as well as\nthermoluminescence properties. Our findings allow for an immediate optimization\nof spectroscopy protocols for both the initial search for the nuclear\ntransition using synchrotron radiation, as well as future optical clock\noperation with narrow-linewidth lasers. \n\n"}
{"id": "1506.03013", "contents": "Title: Generalized perturbations in neutrino mixing Abstract: We derive expressions for the neutrino mixing parameters that result from\ncomplex perturbations on (1) the Majorana neutrino mass matrix (in the basis of\ncharged lepton mass eigenstates) and on (2) the charged lepton mass matrix, for\narbitrary initial (unperturbed) mixing matrices. In the first case, we find\nthat the phases of the elements of the perturbation matrix, and the initial\nvalues of the Dirac and Majorana phases, strongly impact the leading order\ncorrections to the neutrino mixing parameters and phases. For experimentally\ncompatible scenarios wherein the initial neutrino mass matrix has $\\mu-\\tau$\nsymmetry, we find that the Dirac phase can take any value under small\nperturbations. Similarly, in the second case, perturbations to the charged\nlepton mass matrix can generate large corrections to the mixing angles and\nphases of the PMNS matrix. As an illustration of our generalized procedure, we\napply it to a situation in which nonstandard scalar and nonstandard vector\ninteractions simultaneously affect neutrino oscillations. \n\n"}
{"id": "1506.03652", "contents": "Title: Research and Development of Commercially Manufactured Large GEM Foils Abstract: With future experiments proposing detectors that utilize very large-area GEM\nfoils, there is a need for commercially available GEM foils. Double-mask\netching techniques pose a clear limitation in the maximum size of GEM foils. In\ncontrast, single-mask techniques developed at CERN would allow one to overcome\nthose limitations. However with interest in GEM foils increasing and CERN being\nthe only main distributor, keeping up with the demand for GEM foils will be\ndifficult. Thus the commercialization of GEMs has been established by Tech-Etch\nof Plymouth, MA, USA using single-mask techniques.\n  We report on the electrical and geometrical properties, along with the inner\nand outer hole diameter size uniformity of 10 $\\times$ 10 cm$^2$ and\n40$\\times$40 cm$^2$ GEM foils. The Tech-Etch foils were found to have excellent\nelectrical properties. The measured mean optical properties were found to\nreflect the desired parameters and are consistent with those measured in\ndouble-mask GEM foils, and show good hole diameter uniformity over the active\narea. These foils are well suited for future applications in nuclear and\nparticle physics where tracking devices are needed. \n\n"}
{"id": "1506.04279", "contents": "Title: Low Background Signal Readout Electronics for the MAJORANA DEMONSTRATOR Abstract: The MAJORANA Collaboration will seek neutrinoless double beta decay (0nbb) in\n76Ge using isotopically enriched p-type point contact (PPC) high purity\nGermanium (HPGe) detectors. A tonne-scale array of HPGe detectors would require\nbackground levels below 1 count/ROI-tonne-year in the 4 keV region of interest\n(ROI) around the 2039 keV Q-value of the decay. In order to demonstrate the\nfeasibility of such an experiment, the MAJORANA DEMONSTRATOR, a 40 kg HPGe\ndetector array, is being constructed with a background goal of <3\ncounts/ROI-tonne-year, which is expected to scale down to <1\ncount/ROI-tonne-year for a tonne-scale experiment. The signal readout\nelectronics, which must be placed in close proximity to the detectors, present\na challenge toward reaching this background goal. This talk will discuss the\nmaterials and design used to construct signal readout electronics with low\nenough backgrounds for the MAJORANA DEMONSTRATOR. \n\n"}
{"id": "1506.04954", "contents": "Title: A Tensor-Based Dictionary Learning Approach to Tomographic Image\n  Reconstruction Abstract: We consider tomographic reconstruction using priors in the form of a\ndictionary learned from training images. The reconstruction has two stages:\nfirst we construct a tensor dictionary prior from our training data, and then\nwe pose the reconstruction problem in terms of recovering the expansion\ncoefficients in that dictionary. Our approach differs from past approaches in\nthat a) we use a third-order tensor representation for our images and b) we\nrecast the reconstruction problem using the tensor formulation. The dictionary\nlearning problem is presented as a non-negative tensor factorization problem\nwith sparsity constraints. The reconstruction problem is formulated in a convex\noptimization framework by looking for a solution with a sparse representation\nin the tensor dictionary. Numerical results show that our tensor formulation\nleads to very sparse representations of both the training images and the\nreconstructions due to the ability of representing repeated features compactly\nin the dictionary. \n\n"}
{"id": "1506.05439", "contents": "Title: Learning with a Wasserstein Loss Abstract: Learning to predict multi-label outputs is challenging, but in many problems\nthere is a natural metric on the outputs that can be used to improve\npredictions. In this paper we develop a loss function for multi-label learning,\nbased on the Wasserstein distance. The Wasserstein distance provides a natural\nnotion of dissimilarity for probability measures. Although optimizing with\nrespect to the exact Wasserstein distance is costly, recent work has described\na regularized approximation that is efficiently computed. We describe an\nefficient learning algorithm based on this regularization, as well as a novel\nextension of the Wasserstein distance from probability measures to unnormalized\nmeasures. We also describe a statistical learning bound for the loss. The\nWasserstein loss can encourage smoothness of the predictions with respect to a\nchosen metric on the output space. We demonstrate this property on a real-data\ntag prediction problem, using the Yahoo Flickr Creative Commons dataset,\noutperforming a baseline that doesn't use the metric. \n\n"}
{"id": "1506.06868", "contents": "Title: Learning Discriminative Bayesian Networks from High-dimensional\n  Continuous Neuroimaging Data Abstract: Due to its causal semantics, Bayesian networks (BN) have been widely employed\nto discover the underlying data relationship in exploratory studies, such as\nbrain research. Despite its success in modeling the probability distribution of\nvariables, BN is naturally a generative model, which is not necessarily\ndiscriminative. This may cause the ignorance of subtle but critical network\nchanges that are of investigation values across populations. In this paper, we\npropose to improve the discriminative power of BN models for continuous\nvariables from two different perspectives. This brings two general\ndiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In the\nfirst framework, we employ Fisher kernel to bridge the generative models of GBN\nand the discriminative classifiers of SVMs, and convert the GBN parameter\nlearning to Fisher kernel learning via minimizing a generalization error bound\nof SVMs. In the second framework, we employ the max-margin criterion and build\nit directly upon GBN models to explicitly optimize the classification\nperformance of the GBNs. The advantages and disadvantages of the two frameworks\nare discussed and experimentally compared. Both of them demonstrate strong\npower in learning discriminative parameters of GBNs for neuroimaging based\nbrain network analysis, as well as maintaining reasonable representation\ncapacity. The contributions of this paper also include a new Directed Acyclic\nGraph (DAG) constraint with theoretical guarantee to ensure the graph validity\nof GBN. \n\n"}
{"id": "1506.08141", "contents": "Title: Search for critical behavior of strongly interacting matter at the CERN\n  Super Proton Synchrotron Abstract: History, status and plans of the search for critical behavior of strongly\ninteracting matter created in nucleus-nucleus collisions at the CERN Super\nProton Synchrotron is reviewed. In particular, it is expected that the search\nshould answer the question whether the critical point of strongly interacting\nmatter exists and, if it does, where it is located.\n  First, the search strategies are presented and a short introduction is given\nto expected fluctuation signals and to the quantities used by experiments to\ndetect th The most important background effects are also discussed.\n  Second, relevant experimental results are summarized and discussed. It is\nintriguing that both the fluctuations of quantities integrated over the full\nexperimental acceptance (event multiplicity and transverse momentum) as well as\nthe bin size dependence of the second factorial moment of pion and proton\nmultiplicities in medium-sized Si+Si collisions at 158A GeV/c suggest critical\nbehaviour of the created matter.\n  These results provide strong motivation for the ongoing systematic scan of\nthe phase diagram by the NA61/SHINE experiment at the SPS and the continuing\nsearch at the Brookhaven Relativistic Hadron Collider. \n\n"}
{"id": "1507.01578", "contents": "Title: Beyond Semantic Image Segmentation : Exploring Efficient Inference in\n  Video Abstract: We explore the efficiency of the CRF inference module beyond image level\nsemantic segmentation. The key idea is to combine the best of two worlds of\nsemantic co-labeling and exploiting more expressive models. Similar to\n[Alvarez14] our formulation enables us perform inference over ten thousand\nimages within seconds. On the other hand, it can handle higher-order clique\npotentials similar to [vineet2014] in terms of region-level label consistency\nand context in terms of co-occurrences. We follow the mean-field updates for\nhigher order potentials similar to [vineet2014] and extend the spatial\nsmoothness and appearance kernels [DenseCRF13] to address video data inspired\nby [Alvarez14]; thus making the system amenable to perform video semantic\nsegmentation most effectively. \n\n"}
{"id": "1507.02620", "contents": "Title: Deep filter banks for texture recognition, description, and segmentation Abstract: Visual textures have played a key role in image understanding because they\nconvey important semantics of images, and because texture representations that\npool local image descriptors in an orderless manner have had a tremendous\nimpact in diverse applications. In this paper we make several contributions to\ntexture understanding. First, instead of focusing on texture instance and\nmaterial category recognition, we propose a human-interpretable vocabulary of\ntexture attributes to describe common texture patterns, complemented by a new\ndescribable texture dataset for benchmarking. Second, we look at the problem of\nrecognizing materials and texture attributes in realistic imaging conditions,\nincluding when textures appear in clutter, developing corresponding benchmarks\non top of the recently proposed OpenSurfaces dataset. Third, we revisit classic\ntexture representations, including bag-of-visual-words and the Fisher vectors,\nin the context of deep learning and show that these have excellent efficiency\nand generalization properties if the convolutional layers of a deep model are\nused as filter banks. We obtain in this manner state-of-the-art performance in\nnumerous datasets well beyond textures, an efficient method to apply deep\nfeatures to image regions, as well as benefit in transferring features from one\ndomain to another. \n\n"}
{"id": "1507.03752", "contents": "Title: Parton Distribution Functions properties of the statistical model Abstract: We show that the parton distribution functions (PDF) described by the\nstatistical model have very interesting physical properties which help to\nunderstand the structure of partons. The role of the quark helicity components\nis emphasized as they represent the building blocks of the PDF. In the model\nthe sign of the polarized quarks PDF comes out in a quite natural way once the\nthermodynamical potentials with a given helicity are known. Introducing the\nconcept of entropy we study the states madeof |2u + d >, |u +d +s > and $|2\\bar\nu +\\bar d >$, for a fixed Q^2, the variation with x shows that the first state\nhas a dominant entropy due to the effect of u quark. We prove that the PDF\nparameters obtained from experiments give in fact an optimal solution of an\nentropy equation subject to constraints. We develop a new approach of the\npolarized gluon density based on a neural model which explains its property, in\nparticular, a large positivity value and an agreement with the positvity\nconstraint. An extension of this neural approach is applied to quarks giving a\ncoherent description of the partons structure. \n\n"}
{"id": "1507.08177", "contents": "Title: The COBRA demonstrator at the LNGS underground laboratory Abstract: The COBRA demonstrator, a prototype for a large-scale experiment searching\nfor neutrinoless double beta-decay, was built at the underground laboratory\nLaboratori Nazionali del Gran Sasso (LNGS) in Italy. It consists of an array of\n64 monolithic, calorimetric CdZnTe semiconductor detectors with a coplanar-grid\ndesign and a total mass of 380g. It is used to investigate the experimental\nchallenges faced when operating CdZnTe detectors in low-background mode, to\nidentify potential background sources and to show the long-term stability of\nthe detectors. The first data-taking period started in 2011 with a subset of\nthe detectors, while the demonstrator was completed in November 2013. To date,\nmore than 250kg d of data have been collected. This paper describes technical\ndetails of the experimental setup and the hardware components. \n\n"}
{"id": "1507.08787", "contents": "Title: Large area Si low-temperature light detectors with Neganov-Luke effect Abstract: Next generation calorimetric experiments for the search of rare events rely\non the detection of tiny amounts of light (of the order of 20 optical photons)\nto discriminate and reduce background sources and improve sensitivity.\nCalorimetric detectors are the simplest solution for photon detection at\ncryogenic (mK) temperatures. The development of silicon based light detectors\nwith enhanced performance thanks to the use of the Neganov-Luke effect is\ndescribed. The aim of this research line is the production of high performance\ndetectors with industrial-grade reproducibility and reliability. \n\n"}
{"id": "1508.01108", "contents": "Title: Evaluating color texture descriptors under large variations of\n  controlled lighting conditions Abstract: The recognition of color texture under varying lighting conditions is still\nan open issue. Several features have been proposed for this purpose, ranging\nfrom traditional statistical descriptors to features extracted with neural\nnetworks. Still, it is not completely clear under what circumstances a feature\nperforms better than the others. In this paper we report an extensive\ncomparison of old and new texture features, with and without a color\nnormalization step, with a particular focus on how they are affected by small\nand large variation in the lighting conditions. The evaluation is performed on\na new texture database including 68 samples of raw food acquired under 46\nconditions that present single and combined variations of light color,\ndirection and intensity. The database allows to systematically investigate the\nrobustness of texture descriptors across a large range of variations of imaging\nconditions. \n\n"}
{"id": "1508.02707", "contents": "Title: Toward an automated analysis of slow ions in nuclear track emulsion Abstract: Application of the nuclear track emulsion technique (NTE) in radioactivity\nand nuclear fission studies is discussed. It is suggested to use a HSP-1000\nautomated microscope for searching for a collinear cluster tri-partition of\nheavy nuclei implanted in NTE. Calibrations of $\\alpha $-particles and ion\nranges in a novel NTE are carried out. Surface exposures of NTE samples to a\n${}^{252}$Cf source started. Planar events containing fragments and long-range\n$\\alpha $-particles as well as fragment triples only are studied. NTE samples\nare calibrated by ions Kr and Xe of energy of 1.2 and 3 A MeV. \n\n"}
{"id": "1508.04133", "contents": "Title: A prototype vector magnetic field monitoring system for a neutron\n  electric dipole moment experiment Abstract: We present results from a first demonstration of a magnetic field monitoring\nsystem for a neutron electric dipole moment experiment. The system is designed\nto reconstruct the vector components of the magnetic field in the interior\nmeasurement region solely from exterior measurements. \n\n"}
{"id": "1508.05306", "contents": "Title: Exemplar Based Deep Discriminative and Shareable Feature Learning for\n  Scene Image Classification Abstract: In order to encode the class correlation and class specific information in\nimage representation, we propose a new local feature learning approach named\nDeep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to\nhierarchically learn feature transformation filter banks to transform raw pixel\nimage patches to features. The learned filter banks are expected to: (1) encode\ncommon visual patterns of a flexible number of categories; (2) encode\ndiscriminative information; and (3) hierarchically extract patterns at\ndifferent visual levels. Particularly, in each single layer of DDSFL, shareable\nfilters are jointly learned for classes which share the similar patterns.\nDiscriminative power of the filters is achieved by enforcing the features from\nthe same category to be close, while features from different categories to be\nfar away from each other. Furthermore, we also propose two exemplar selection\nmethods to iteratively select training data for more efficient and effective\nlearning. Based on the experimental results, DDSFL can achieve very promising\nperformance, and it also shows great complementary effect to the\nstate-of-the-art Caffe features. \n\n"}
{"id": "1508.06708", "contents": "Title: Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose\n  Estimation Abstract: This paper focuses on structured-output learning using deep neural networks\nfor 3D human pose estimation from monocular images. Our network takes an image\nand 3D pose as inputs and outputs a score value, which is high when the\nimage-pose pair matches and low otherwise. The network structure consists of a\nconvolutional neural network for image feature extraction, followed by two\nsub-networks for transforming the image features and pose into a joint\nembedding. The score function is then the dot-product between the image and\npose embeddings. The image-pose embedding and score function are jointly\ntrained using a maximum-margin cost function. Our proposed framework can be\ninterpreted as a special form of structured support vector machines where the\njoint feature space is discriminatively learned using deep neural networks. We\ntest our framework on the Human3.6m dataset and obtain state-of-the-art results\ncompared to other recent methods. Finally, we present visualizations of the\nimage-pose embedding space, demonstrating the network has learned a high-level\nembedding of body-orientation and pose-configuration. \n\n"}
{"id": "1508.06820", "contents": "Title: PALS investigations of free volumes thermal expansion of J-PET plastic\n  scintillator synthesized in polystyrene matrix Abstract: The polystyrene dopped with 2,5-diphenyloxazole as a primary fluor and\n2-(4-styrylphenyl)benzoxazole as a wavelength shifter, prepared as a plastic\nscintillator was investigated using positronium probe in wide range of\ntemperatures from 123 to 423 K. Three structural transitions at 260 K, 283 K\nand 370 K were found in the material. In the o-Ps intensity dependence on\ntemperature, the significant hysteresis is observed. Heated to 370 K, the\nmaterial exhibits the o-Ps intensity variations in time. \n\n"}
{"id": "1509.00770", "contents": "Title: Low energy neutron background in deep underground laboratories Abstract: The natural neutron background influences the maximum achievable sensitivity\nin most deep underground nuclear, astroparticle and double-beta decay physics\nexperiments. Reliable neutron flux numbers are an important ingredient in the\ndesign of the shielding of new large-scale experiments as well as in the\nanalysis of experimental data.\n  Using a portable setup of He-3 counters we measured the thermal neutron flux\nat the Kimballton Underground Research Facility, the Soudan Underground\nLaboratory, on the 4100 ft and the 4850 ft levels of the Sanford Underground\nResearch Facility, at the Waste Isolation Pilot Plant and at the Gran Sasso\nNational Laboratory. Absolute neutron fluxes at these laboratories are\npresented. \n\n"}
{"id": "1509.01602", "contents": "Title: Object Recognition from Short Videos for Robotic Perception Abstract: Deep neural networks have become the primary learning technique for object\nrecognition. Videos, unlike still images, are temporally coherent which makes\nthe application of deep networks non-trivial. Here, we investigate how motion\ncan aid object recognition in short videos. Our approach is based on Long\nShort-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs,\nwe implement each gate as a convolution. We show that convolutional-based LSTM\nmodels are capable of learning motion dependencies and are able to improve the\nrecognition accuracy when more frames in a sequence are available. We evaluate\nour approach on the Washington RGBD Object dataset and on the Washington RGBD\nScenes dataset. Our approach outperforms deep nets applied to still images and\nsets a new state-of-the-art in this domain. \n\n"}
{"id": "1509.01788", "contents": "Title: Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM)\n  for unsupervised RGB-D image segmentation Abstract: Recent advances in depth imaging sensors provide easy access to the\nsynchronized depth with color, called RGB-D image. In this paper, we propose an\nunsupervised method for indoor RGB-D image segmentation and analysis. We\nconsider a statistical image generation model based on the color and geometry\nof the scene. Our method consists of a joint color-spatial-directional\nclustering method followed by a statistical planar region merging method. We\nevaluate our method on the NYU depth database and compare it with existing\nunsupervised RGB-D segmentation methods. Results show that, it is comparable\nwith the state of the art methods and it needs less computation time. Moreover,\nit opens interesting perspectives to fuse color and geometry in an unsupervised\nmanner. \n\n"}
{"id": "1509.02361", "contents": "Title: Characterization of a large CdZnTe coplanar quad-grid semiconductor\n  detector Abstract: The COBRA collaboration aims to search for neutrinoless double beta-decay of\n$^{116}$Cd. A demonstrator setup with 64 CdZnTe semiconductor detectors, each\nwith a volume of 1cm$^3$, is currently being operated at the LNGS underground\nlaboratory in Italy. This paper reports on the characterization of a large (2\n$\\times$ 2 $\\times$ 1.5)cm$^3$ CdZnTe detector with a new coplanar-grid design\nfor applications in $\\gamma$-ray spectroscopy and low-background operation.\nSeveral studies of electric properties as well as of the spectrometric\nperformance, like energy response and resolution, are conducted. Furthermore,\nmeasurements including investigating the operational stability and a\npossibility to identify multiple-scattered photons are presented. \n\n"}
{"id": "1509.02669", "contents": "Title: Response of a proportional counter to $^{37}$Ar and $^{71}$Ge: measured\n  spectra versus Geant4 simulation Abstract: The energy deposition spectra of $^{37}$Ar and $^{71}$Ge in a miniature\nproportional counter are measured and compared in detail to the model response\nsimulated with Geant4. A certain modification of the Geant4 code, making it\npossible to trace the deexcitation of atomic shells properly, is suggested.\nModified Geant4 is able to reproduce a response of particle detectors in detail\nin the keV energy range. This feature is very important for the laboratory\nexperiments that search for massive sterile neutrinos as well as for dark\nmatter searches that employ direct detection of recoil nuclei. This work\ndemonstrates the reliability of Geant4 simulation at low energies. \n\n"}
{"id": "1509.02818", "contents": "Title: Test of the CLAS12 RICH large scale prototype in the direct proximity\n  focusing configuration Abstract: A large area ring-imaging Cherenkov detector has been designed to provide\nclean hadron identification capability in the momentum range from 3 GeV/c up to\n8 GeV/c for the CLAS12 experiments at the upgraded 12 GeV continuous electron\nbeam accelerator facility of Jefferson Laboratory. The adopted solution\nforesees a novel hybrid optics design based on aerogel radiator, composite\nmirrors and high-packed and high-segmented photon detectors. Cherenkov light\nwill either be imaged directly (forward tracks) or after two mirror reflections\n(large angle tracks). We report here the results of the tests of a large scale\nprototype of the RICH detector performed with the hadron beam of the CERN T9\nexperimental hall for the direct detection configuration. The tests\ndemonstrated that the proposed design provides the required pion-to-kaon\nrejection factor of 1:500 in the whole momentum range. \n\n"}
{"id": "1509.04007", "contents": "Title: Symmetry violations in nuclear and neutron $\\beta$ decay Abstract: The role of $\\beta$ decay as a low-energy probe of physics beyond the\nStandard Model is reviewed. Traditional searches for deviations from the\nStandard Model structure of the weak interaction in $\\beta$ decay are discussed\nin the light of constraints from the LHC and the neutrino mass. Limits on the\nviolation of time-reversal symmetry in $\\beta$ decay are compared to the strong\nconstraints from electric dipole moments. Novel searches for Lorentz symmetry\nbreaking in the weak interaction in $\\beta$ decay are also included, where we\ndiscuss the unique sensitivity of $\\beta$ decay to test Lorentz invariance. We\nend with a roadmap for future $\\beta$-decay experiments. \n\n"}
{"id": "1509.04874", "contents": "Title: DenseBox: Unifying Landmark Localization with End to End Object\n  Detection Abstract: How can a single fully convolutional neural network (FCN) perform on object\ndetection? We introduce DenseBox, a unified end-to-end FCN framework that\ndirectly predicts bounding boxes and object class confidences through all\nlocations and scales of an image. Our contribution is two-fold. First, we show\nthat a single FCN, if designed and optimized carefully, can detect multiple\ndifferent objects extremely accurately and efficiently. Second, we show that\nwhen incorporating with landmark localization during multi-task learning,\nDenseBox further improves object detection accuray. We present experimental\nresults on public benchmark datasets including MALF face detection and KITTI\ncar detection, that indicate our DenseBox is the state-of-the-art system for\ndetecting challenging objects such as faces and cars. \n\n"}
{"id": "1509.05186", "contents": "Title: Accelerated Distance Computation with Encoding Tree for High Dimensional\n  Data Abstract: We propose a novel distance to calculate distance between high dimensional\nvector pairs, utilizing vector quantization generated encodings. Vector\nquantization based methods are successful in handling large scale high\ndimensional data. These methods compress vectors into short encodings, and\nallow efficient distance computation between an uncompressed vector and\ncompressed dataset without decompressing explicitly. However for large\ndatasets, these distance computing methods perform excessive computations. We\navoid excessive computations by storing the encodings on an Encoding\nTree(E-Tree), interestingly the memory consumption is also lowered. We also\npropose Encoding Forest(E-Forest) to further lower the computation cost. E-Tree\nand E-Forest is compatible with various existing quantization-based methods. We\nshow by experiments our methods speed-up distance computing for high\ndimensional data drastically, and various existing algorithms can benefit from\nour methods. \n\n"}
{"id": "1509.05460", "contents": "Title: New Results from the Studies of the $N(1440)1/2^+$, $N(1520)3/2^-$, and\n  $\\Delta(1620)1/2^-$ Resonances in Exclusive $ep \\to e'p' \\pi^+ \\pi^-$\n  Electroproduction with the CLAS Detector Abstract: The transition helicity amplitudes from the proton ground state to the\n$N(1440)1/2^+$, $N(1520)3/2^-$, and $\\Delta(1620)1/2^-$ resonances\n($\\gamma_vpN^*$ electrocouplings) were determined from the analysis of nine\nindependent one-fold differential $\\pi^+ \\pi^- p$ electroproduction cross\nsections off a proton target, taken with CLAS at photon virtualities 0.5\nGeV$^2$ $< Q^2 <$ 1.5 GeV$^2$. The phenomenological reaction model employed for\nseparation of the resonant and non-resonant contributions to this exclusive\nchannel was further developed. The $N(1440)1/2^+$, $N(1520)3/2^-$, and\n$\\Delta(1620)1/2^-$ electrocouplings were obtained from the resonant amplitudes\nof charged double-pion electroproduction off the proton in the aforementioned\narea of photon virtualities for the first time. Consistent results on\n$\\gamma_vpN^*$ electrocouplings available from independent analyses of several\n$W$-intervals with different non-resonant contributions offer clear evidence\nfor the reliable extraction of these fundamental quantities. These studies also\nimproved the knowledge on hadronic branching ratios for the $N(1440)1/2^+$,\n$N(1520)3/2^-$, and $\\Delta(1620)1/2^-$ decays to the $\\pi \\Delta$ and $\\rho N$\nfinal states. These new results provide a substantial impact on the QCD-based\napproaches that describe the $N^*$ structure and demonstrate the capability to\nexplore fundamental ingredients of the non-perturbative strong interaction that\nare behind the excited nucleon state formation. \n\n"}
{"id": "1509.06690", "contents": "Title: Invariants of objects and their images under surjective maps Abstract: We examine the relationships between the differential invariants of objects\nand of their images under a surjective map. We analyze both the case when the\nunderlying transformation group is projectable and hence induces an action on\nthe image, and the case when only a proper subgroup of the entire group acts\nprojectably. In the former case, we establish a constructible isomorphism\nbetween the algebra of differential invariants of the images and the algebra of\nfiber-wise constant (gauge) differential invariants of the objects. In the\nlatter case, we describe residual effects of the full transformation group on\nthe image invariants. Our motivation comes from the problem of reconstruction\nof an object from multiple-view images, with central and parallel projections\nof curves from three-dimensional space to the two-dimensional plane serving as\nour main examples. \n\n"}
{"id": "1509.07781", "contents": "Title: Light fragments from (C + Be) interactions at 0.6 GeV/nucleon Abstract: Nuclear fragments emitted at 3.5 degrees in 12C fragmentation at 0.6\nGeV/nucleon have been measured. The spectra obtained are used for testing the\npredictions of four ion-ion interaction models: INCL++, BC, LAQGSM03.03 and QMD\nas well as for the comparison with the analytical parametrization in the\nframework of thermodynamical picture of fragmentation. \n\n"}
{"id": "1509.08067", "contents": "Title: Online Object Tracking, Learning and Parsing with And-Or Graphs Abstract: This paper presents a method, called AOGTracker, for simultaneously tracking,\nlearning and parsing (TLP) of unknown objects in video sequences with a\nhierarchical and compositional And-Or graph (AOG) representation. %The AOG\ncaptures both structural and appearance variations of a target object in a\nprincipled way. The TLP method is formulated in the Bayesian framework with a\nspatial and a temporal dynamic programming (DP) algorithms inferring object\nbounding boxes on-the-fly. During online learning, the AOG is discriminatively\nlearned using latent SVM to account for appearance (e.g., lighting and partial\nocclusion) and structural (e.g., different poses and viewpoints) variations of\na tracked object, as well as distractors (e.g., similar objects) in background.\nThree key issues in online inference and learning are addressed: (i)\nmaintaining purity of positive and negative examples collected online, (ii)\ncontroling model complexity in latent structure learning, and (iii) identifying\ncritical moments to re-learn the structure of AOG based on its intrackability.\nThe intrackability measures uncertainty of an AOG based on its score maps in a\nframe. In experiments, our AOGTracker is tested on two popular tracking\nbenchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks,\nand the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery\ntracking). In the former, our AOGTracker outperforms state-of-the-art tracking\nalgorithms including two trackers based on deep convolutional network. In the\nlatter, our AOGTracker outperforms all other trackers in VOT2013 and is\ncomparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015. \n\n"}
{"id": "1510.01490", "contents": "Title: Directional Global Three-part Image Decomposition Abstract: We consider the task of image decomposition and we introduce a new model\ncoined directional global three-part decomposition (DG3PD) for solving it. As\nkey ingredients of the DG3PD model, we introduce a discrete multi-directional\ntotal variation norm and a discrete multi-directional G-norm. Using these novel\nnorms, the proposed discrete DG3PD model can decompose an image into two parts\nor into three parts. Existing models for image decomposition by Vese and Osher,\nby Aujol and Chambolle, by Starck et al., and by Thai and Gottschlich are\nincluded as special cases in the new model. Decomposition of an image by DG3PD\nresults in a cartoon image, a texture image and a residual image. Advantages of\nthe DG3PD model over existing ones lie in the properties enforced on the\ncartoon and texture images. The geometric objects in the cartoon image have a\nvery smooth surface and sharp edges. The texture image yields oscillating\npatterns on a defined scale which is both smooth and sparse. Moreover, the\nDG3PD method achieves the goal of perfect reconstruction by summation of all\ncomponents better than the other considered methods. Relevant applications of\nDG3PD are a novel way of image compression as well as feature extraction for\napplications such as latent fingerprint processing and optical character\nrecognition. \n\n"}
{"id": "1510.02344", "contents": "Title: First tests of the applicability of $\\gamma$-ray imaging for background\n  discrimination in time-of-flight neutron capture measurements Abstract: In this work we explore for the first time the applicability of using\n$\\gamma$-ray imaging in neutron capture measurements to identify and suppress\nspatially localized background. For this aim, a pinhole gamma camera is\nassembled, tested and characterized in terms of energy and spatial performance.\nIt consists of a monolithic CeBr$_3$ scintillating crystal coupled to a\nposition-sensitive photomultiplier and readout through an integrated circuit\nAMIC2GR. The pinhole collimator is a massive carven block of lead. A series of\ndedicated measurements with calibrated sources and with a neutron beam incident\non a $^{197}$Au sample have been carried out at n_TOF, achieving an enhancement\nof a factor of two in the signal-to-background ratio when selecting only those\nevents coming from the direction of the sample. \n\n"}
{"id": "1510.02799", "contents": "Title: Structure of the neutral pion and its electromagnetic transition form\n  factor Abstract: The $\\gamma^\\ast \\gamma \\to \\pi^0$ transition form factor, $G(Q^2)$, is\ncomputed on the entire domain of spacelike momenta using a continuum approach\nto the two valence-body bound-state problem in relativistic quantum field\ntheory: the result agrees with data obtained by the CELLO, CLEO and Belle\nCollaborations. The analysis unifies this prediction with that of the pion's\nvalence-quark parton distribution amplitude (PDA) and elastic electromagnetic\nform factor, and demonstrates, too, that a fully self-consistent treatment can\nreadily connect a pion PDA that is a broad, concave function at the hadronic\nscale with the perturbative QCD prediction for the transition form factor in\nthe hard photon limit. The normalisation of that limit is set by the scale of\ndynamical chiral symmetry breaking, which is a crucial feature of the Standard\nModel. Understanding of the latter will thus remain incomplete until definitive\ntransition form factor data is available on $Q^2>10\\,$GeV$^2$. \n\n"}
{"id": "1510.02906", "contents": "Title: Temporal Dynamic Appearance Modeling for Online Multi-Person Tracking Abstract: Robust online multi-person tracking requires the correct associations of\nonline detection responses with existing trajectories. We address this problem\nby developing a novel appearance modeling approach to provide accurate\nappearance affinities to guide data association. In contrast to most existing\nalgorithms that only consider the spatial structure of human appearances, we\nexploit the temporal dynamic characteristics within temporal appearance\nsequences to discriminate different persons. The temporal dynamic makes a\nsufficient complement to the spatial structure of varying appearances in the\nfeature space, which significantly improves the affinity measurement between\ntrajectories and detections. We propose a feature selection algorithm to\ndescribe the appearance variations with mid-level semantic features, and\ndemonstrate its usefulness in terms of temporal dynamic appearance modeling.\nMoreover, the appearance model is learned incrementally by alternatively\nevaluating newly-observed appearances and adjusting the model parameters to be\nsuitable for online tracking. Reliable tracking of multiple persons in complex\nscenes is achieved by incorporating the learned model into an online\ntracking-by-detection framework. Our experiments on the challenging benchmark\nMOTChallenge 2015 demonstrate that our method outperforms the state-of-the-art\nmulti-person tracking algorithms. \n\n"}
{"id": "1510.05109", "contents": "Title: Results of measurements of an environment neutron background at BNO INR\n  RAS objects with the helium proportional counter Abstract: A method of measurements of the environmental neutron background at the\nBaksan Neutrino Observatory of the INR RAS are described. Measurements were\ndone by using of a proportional counter filled with mixture of Ar(2\nat)+$^3$He(4 at). The results obtained at the surface and the underground\nlaboratory of the BNO INR RAS are presented. It is shown that a neutron\nbackground in the underground laboratory at the 4900 m w.e. depth is decreased\nby $\\sim 260$ times without any special shield in a comparison with the Earth\nsurface. A neutron flux density in the 5-1323.5~cm air height region is\nconstant within the determination error and equal to\n$(7.1\\pm0.1_{\\rm{stat}}\\pm0.3_{\\rm{syst}})\\times10^{-3}$\ns$^{-1}\\cdot$cm$^{-2}$. \n\n"}
{"id": "1510.05415", "contents": "Title: Response of diamond detector sandwich to 14 MeV neutrons Abstract: In this paper we present the measurement of the response of 50 $\\mu$m thin\ndiamond detectors to 14 MeV neutrons. Such neutrons are produced in fusion\nreactors and are of particular interest for ITER neutron diagnostics. Among\nsemiconductor detectors diamond has properties most appropriate for harsh\nradiation and temperature conditions of a fusion reactor. However, 300-500\n$\\mu$m thick diamond detectors suffer significant radiation damage already at\nneutron fluences of the order of $10^{14}$ n/cm$^2$. It is expected that a 50\n$\\mu$m thick diamond will withstand a fluence of $>10^{16}$ n/cm$^2$. We tested\ntwo 50 $\\mu$m thick single crystal CVD diamonds, stacked to form a \"sandwich\"\ndetector for coincidence measurements. The detector measured the conversion of\n14 MeV neutrons, impinging on one diamond, into $\\alpha$ particles which were\ndetected in the second diamond in coincidence with nuclear recoil. For\n$^{12}C(n,\\alpha)^{9}Be$ reaction the total energy deposited in the detector\ngives access to the initial neutron energy value. The measured 14 MeV neutron\ndetection sensitivity through this reaction by a detector of effective area\n3$\\times$3 mm$^2$ was $5\\times 10^{-7}$ counts cm$^2$/n. This value is in good\nagreement with Geant4 simulations. The intrinsic energy resolution of the\ndetector was found to be 240 keV FWHM which adds only 10 % to ITER's 14 MeV\nneutron energy spread. \n\n"}
{"id": "1510.05633", "contents": "Title: Global Antineutrino Modeling for a Web Application Abstract: Antineutrinos stream freely from rapidly decaying fission products within the\ncores of nuclear reactors and from long-lived natural radioactivity within the\nrocky layers of the Earth. These global antineutrinos produce detectable\nsignals in large ultra-clear volumes of water- or hydrocarbon-based target\nliquids, which are viewed by inward-facing photomultiplier tubes. Detected\nantineutrinos provide information about their shrouded sources and about the\nfundamental properties of neutrinos themselves. This paper presents the input\ndata, formulae, and plots resulting from the calculations, which, in addition\nto the time-dependent reaction rates and energy spectra, model the directions\nof the antineutrinos from IAEA-registered nuclear power reactors and of the\nneutrinos from $^8$B decay in the Sun. The model includes estimates of the\nsteady state reaction rates and energy spectra of the antineutrinos from the\ncrust and mantle of the Earth. Results are available for any location near the\nsurface of the Earth and comprise both quasi-elastic scattering on free protons\nand elastic scattering on atomic electrons. This paper compares model results\nfor two underground locations, the Boulby Mine in the United Kingdom and the\nMorton Salt Mine in the United States. Operational nuclear power reactors are\nwithin about $20$ kilometers of these mines, making them candidate sites for\nantineutrino detectors capable of identifying, monitoring, and locating remote\nnuclear activity. The model, which is implemented in a web application at\nhttps://geoneutrinos.org/reactors/, provides references for the input data and\nthe formulae, as well as an interactive calculator of the significance of the\nrate of any of the neutrino sources relative to other sources taken as\nbackground. \n\n"}
{"id": "1510.06068", "contents": "Title: An apparatus for studying electrical breakdown in liquid helium at 0.4 K\n  and testing electrode materials for the SNS nEDM experiment Abstract: We have constructed an apparatus to study DC electrical breakdown in liquid\nhelium at temperatures as low as 0.4 K and at pressures between the saturated\nvapor pressure and $\\sim$600 torr. The apparatus can house a set of electrodes\nthat are 12 cm in diameter with a gap of $1-2$ cm between them, and a potential\nup to $\\pm 50$ kV can be applied to each electrode. Initial results\ndemonstrated that it is possible to apply fields exceeding 100 kV/cm in a 1 cm\ngap between two electropolished stainless steel electrodes 12 cm in diameter\nfor a wide range of pressures at 0.4 K. We also measured the current between\ntwo electrodes. Our initial results, $I<1$ pA at 45 kV, correspond to a lower\nbound on the effective volume resistivity of LHe of $\\rho_V > 5\\times10^{18}$\n$\\Omega\\cdot$cm. This lower bound is 5 times larger than the bound previously\nmeasured. We report the design, construction, and operational experience of the\napparatus, as well as initial results. \n\n"}
{"id": "1510.08239", "contents": "Title: Recent results from NA61/SHINE on spectra and correlations in p+p and\n  Be+Be interactions at the CERN SPS Abstract: The problem of pinning down the critical point of strongly interacting matter\nstill puzzles the community. One of the answers suspected to emerge in the near\nfuture will surely come from NA61/SHINE - a fixed-target experiment aiming to\ndiscover the critical point as well as to study the properties of the onset of\ndeconfinement. This goal will be pursued by obtaining precise data on hadron\nproduction in proton-proton, proton-nucleus and nucleus-nucleus interactions in\na wide range of system size and collision energy.\n  This contribution presents new results on inclusive spectra of identified\nhadrons and on fluctuations in inelastic p+p and Be+Be interactions at the SPS\nenergies. These are compared with the world data, in particular with the\ncorresponding measurements of NA49 for central Pb+Pb collisions as well as with\nsome model predictions. \n\n"}
{"id": "1510.09002", "contents": "Title: Characteristics of a thermal neutrons scintillation detector with the\n  [ZnS(Ag)+$^6$LiF] at different conditions of measurements Abstract: A construction of a thermal neutron testing detector with a thin\n[ZnS(Ag)+$^6$LiF] scintillator is described. Results of an investigation of\nsources of the detector pulse origin and the pulse features in a ground and\nunderground conditions are presented. Measurements of the scintillator own\nbackground, registration efficiency and a neutron flux at different objects of\nthe BNO INR RAS were performed. The results are compared with the ones measured\nby the $^3$He proportional counter. \n\n"}
{"id": "1511.00395", "contents": "Title: Status Update of the MAJORANA DEMONSTRATOR Neutrinoless Double Beta\n  Decay Experiment Abstract: Neutrinoless double beta decay searches play a major role in determining\nneutrino properties, in particular the Majorana or Dirac nature of the neutrino\nand the absolute scale of the neutrino mass. The consequences of these searches\ngo beyond neutrino physics, with implications for Grand Unification and\nleptogenesis. The \\textsc{Majorana} Collaboration is assembling a\nlow-background array of high purity Germanium (HPGe) detectors to search for\nneutrinoless double-beta decay in $^{76}$Ge. The \\textsc{Majorana\nDemonstrator}, which is currently being constructed and commissioned at the\nSanford Underground Research Facility in Lead, South Dakota, will contain 44 kg\n(30 kg enriched in $^{76}$Ge) of HPGe detectors. Its primary goal is to\ndemonstrate the scalability and background required for a tonne-scale Ge\nexperiment. This is accomplished via a modular design and projected background\nof less than 3 cnts/tonne-yr in the region of interest. The experiment is\ncurrently taking data with the first of its enriched detectors. \n\n"}
{"id": "1511.03055", "contents": "Title: Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing Abstract: A typical image retrieval pipeline starts with the comparison of global\ndescriptors from a large database to find a short list of candidate matches. A\ngood image descriptor is key to the retrieval pipeline and should reconcile two\ncontradictory requirements: providing recall rates as high as possible and\nbeing as compact as possible for fast matching. Following the recent successes\nof Deep Convolutional Neural Networks (DCNN) for large scale image\nclassification, descriptors extracted from DCNNs are increasingly used in place\nof the traditional hand crafted descriptors such as Fisher Vectors (FV) with\nbetter retrieval performances. Nevertheless, the dimensionality of a typical\nDCNN descriptor --extracted either from the visual feature pyramid or the\nfully-connected layers-- remains quite high at several thousands of scalar\nvalues. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully\nunsupervised method to compute extremely compact binary hashes --in the 32-256\nbits range-- from high-dimensional global descriptors. UTH consists of two\nsuccessive deep learning steps. First, Stacked Restricted Boltzmann Machines\n(SRBM), a type of unsupervised deep neural nets, are used to learn binary\nembedding functions able to bring the descriptor size down to the desired\nbitrate. SRBMs are typically able to ensure a very high compression rate at the\nexpense of loosing some desirable metric properties of the original DCNN\ndescriptor space. Then, triplet networks, a rank learning scheme based on\nweight sharing nets is used to fine-tune the binary embedding functions to\nretain as much as possible of the useful metric properties of the original\nspace. A thorough empirical evaluation conducted on multiple publicly available\ndataset using DCNN descriptors shows that our method is able to significantly\noutperform state-of-the-art unsupervised schemes in the target bit range. \n\n"}
{"id": "1511.03328", "contents": "Title: Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs\n  and a Discriminatively Trained Domain Transform Abstract: Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality. \n\n"}
{"id": "1511.03607", "contents": "Title: Complete Dictionary Recovery over the Sphere I: Overview and the\n  Geometric Picture Abstract: We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms either only guarantee recovery when\n$\\mathbf X_0$ has $O(\\sqrt{n})$ zeros per column, or require multiple rounds of\nSDP relaxation to work when $\\mathbf X_0$ has $O(n^{1-\\delta})$ nonzeros per\ncolumn (for any constant $\\delta \\in (0, 1)$). }\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint. In this paper, we provide a\ngeometric characterization of the objective landscape. In particular, we show\nthat the problem is highly structured: with high probability, (1) there are no\n\"spurious\" local minimizers; and (2) around all saddle points the objective has\na negative directional curvature. This distinctive structure makes the problem\namenable to efficient optimization algorithms. In a companion paper\n(arXiv:1511.04777), we design a second-order trust-region algorithm over the\nsphere that provably converges to a local minimizer from arbitrary\ninitializations, despite the presence of saddle points. \n\n"}
{"id": "1511.04050", "contents": "Title: Chiral Magnetic and Vortical Effects in High-Energy Nuclear Collisions\n  --- A Status Report Abstract: The interplay of quantum anomalies with magnetic field and vorticity results\nin a variety of novel non-dissipative transport phenomena in systems with\nchiral fermions, including the quark-gluon plasma. Among them is the Chiral\nMagnetic Effect (CME) -- the generation of electric current along an external\nmagnetic field induced by chirality imbalance. Because the chirality imbalance\nis related to the global topology of gauge fields, the CME current is\ntopologically protected and hence non-dissipative even in the presence of\nstrong interactions. As a result, the CME and related quantum phenomena affect\nthe hydrodynamical and transport behavior of strongly coupled quark-gluon\nplasma, and can be studied in relativistic heavy ion collisions where strong\nmagnetic fields are created by the colliding ions. Evidence for the CME and\nrelated phenomena has been reported by the STAR Collaboration at Relativistic\nHeavy Ion Collider at BNL, and by the ALICE Collaboration at the Large Hadron\nCollider at CERN. The goal of the present review is to provide an elementary\nintroduction into the physics of anomalous chiral effects, to describe the\ncurrent status of experimental studies in heavy ion physics, and to outline the\nfuture work, both in experiment and theory, needed to eliminate the existing\nuncertainties in the interpretation of the data. \n\n"}
{"id": "1511.05296", "contents": "Title: Towards Predicting the Likeability of Fashion Images Abstract: In this paper, we propose a method for ranking fashion images to find the\nones which might be liked by more people. We collect two new datasets from\nimage sharing websites (Pinterest and Polyvore). We represent fashion images\nbased on attributes: semantic attributes and data-driven attributes. To learn\nsemantic attributes from limited training data, we use an algorithm on\nmulti-task convolutional neural networks to share visual knowledge among\ndifferent semantic attribute categories. To discover data-driven attributes\nunsupervisedly, we propose an algorithm to simultaneously discover visual\nclusters and learn fashion-specific feature representations. Given attributes\nas representations, we propose to learn a ranking SPN (sum product networks) to\nrank pairs of fashion images. The proposed ranking SPN can capture the\nhigh-order correlations of the attributes. We show the effectiveness of our\nmethod on our two newly collected datasets. \n\n"}
{"id": "1511.05607", "contents": "Title: Identifying the Absorption Bump with Deep Learning Abstract: The pervasive interstellar dust grains provide significant insights to\nunderstand the formation and evolution of the stars, planetary systems, and the\ngalaxies, and may harbor the building blocks of life. One of the most effective\nway to analyze the dust is via their interaction with the light from background\nsources. The observed extinction curves and spectral features carry the size\nand composition information of dust. The broad absorption bump at 2175 Angstrom\nis the most prominent feature in the extinction curves. Traditionally,\nstatistical methods are applied to detect the existence of the absorption bump.\nThese methods require heavy preprocessing and the co-existence of other\nreference features to alleviate the influence from the noises. In this paper,\nwe apply Deep Learning techniques to detect the broad absorption bump. We\ndemonstrate the key steps for training the selected models and their results.\nThe success of Deep Learning based method inspires us to generalize a common\nmethodology for broader science discovery problems. We present our on-going\nwork to build the DeepDis system for such kind of applications. \n\n"}
{"id": "1511.06015", "contents": "Title: Active Object Localization with Deep Reinforcement Learning Abstract: We present an active detection model for localizing objects in scenes. The\nmodel is class-specific and allows an agent to focus attention on candidate\nregions for identifying the correct location of a target object. This agent\nlearns to deform a bounding box using simple transformation actions, with the\ngoal of determining the most specific location of target objects following\ntop-down reasoning. The proposed localization agent is trained using deep\nreinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show\nthat agents guided by the proposed model are able to localize a single instance\nof an object after analyzing only between 11 and 25 regions in an image, and\nobtain the best detection results among systems that do not use object\nproposals for object localization. \n\n"}
{"id": "1511.06078", "contents": "Title: Learning Deep Structure-Preserving Image-Text Embeddings Abstract: This paper proposes a method for learning joint embeddings of images and text\nusing a two-branch neural network with multiple layers of linear projections\nfollowed by nonlinearities. The network is trained using a large margin\nobjective that combines cross-view ranking constraints with within-view\nneighborhood structure preservation constraints inspired by metric learning\nliterature. Extensive experiments show that our approach gains significant\nimprovements in accuracy for image-to-text and text-to-image retrieval. Our\nmethod achieves new state-of-the-art results on the Flickr30K and MSCOCO\nimage-sentence datasets and shows promise on the new task of phrase\nlocalization on the Flickr30K Entities dataset. \n\n"}
{"id": "1511.06449", "contents": "Title: Learning to decompose for object detection and instance segmentation Abstract: Although deep convolutional neural networks(CNNs) have achieved remarkable\nresults on object detection and segmentation, pre- and post-processing steps\nsuch as region proposals and non-maximum suppression(NMS), have been required.\nThese steps result in high computational complexity and sensitivity to\nhyperparameters, e.g. thresholds for NMS. In this work, we propose a novel\nend-to-end trainable deep neural network architecture, which consists of\nconvolutional and recurrent layers, that generates the correct number of object\ninstances and their bounding boxes (or segmentation masks) given an image,\nusing only a single network evaluation without any pre- or post-processing\nsteps. We have tested on detecting digits in multi-digit images synthesized\nusing MNIST, automatically segmenting digits in these images, and detecting\ncars in the KITTI benchmark dataset. The proposed approach outperforms a strong\nCNN baseline on the synthesized digits datasets and shows promising results on\nKITTI car detection. \n\n"}
{"id": "1511.06489", "contents": "Title: A Simple Hierarchical Pooling Data Structure for Loop Closure Abstract: We propose a data structure obtained by hierarchically averaging bag-of-word\ndescriptors during a sequence of views that achieves average speedups in\nlarge-scale loop closure applications ranging from 4 to 20 times on benchmark\ndatasets. Although simple, the method works as well as sophisticated\nagglomerative schemes at a fraction of the cost with minimal loss of\nperformance. \n\n"}
{"id": "1511.06783", "contents": "Title: Recognizing Activities of Daily Living with a Wrist-mounted Camera Abstract: We present a novel dataset and a novel algorithm for recognizing activities\nof daily living (ADL) from a first-person wearable camera. Handled objects are\ncrucially important for egocentric ADL recognition. For specific examination of\nobjects related to users' actions separately from other objects in an\nenvironment, many previous works have addressed the detection of handled\nobjects in images captured from head-mounted and chest-mounted cameras.\nNevertheless, detecting handled objects is not always easy because they tend to\nappear small in images. They can be occluded by a user's body. As described\nherein, we mount a camera on a user's wrist. A wrist-mounted camera can capture\nhandled objects at a large scale, and thus it enables us to skip object\ndetection process. To compare a wrist-mounted camera and a head-mounted camera,\nwe also develop a novel and publicly available dataset that includes videos and\nannotations of daily activities captured simultaneously by both cameras.\nAdditionally, we propose a discriminative video representation that retains\nspatial and temporal information after encoding frame descriptors extracted by\nConvolutional Neural Networks (CNN). \n\n"}
{"id": "1511.07187", "contents": "Title: Measuring the Th-229 nuclear isomer transition with U-233 doped crystals Abstract: We propose a simple approach to measure the energy of the few-eV isomeric\nstate in Th-229. To this end, U-229 nuclei are doped into VUV-transparent\ncrystals, where they undergo alpha decay into Th-229, and, with a probability\nof 2%, populate the isomeric state. These Th-229m nuclei may decay into the\nnuclear ground state under emission of the sought-after VUV gamma, whose\nwavelength can be determined with a spectrometer. Based on measurements of the\noptical transmission of U:CaF2 crystals in the VUV range, we expect a signal at\nleast 2 orders of magnitude larger compared to current schemes using\nsurface-implantation of recoil nuclei. The signal background is dominated by\nCherenkov radiation induced by beta decays of the thorium decay chain. We\nestimate that, even if the isomer undergoes radiative de-excitation with a\nprobability of only 0.1%, the VUV gamma can be detected within a reasonable\nmeasurement time. \n\n"}
{"id": "1511.07474", "contents": "Title: Hall A Annual Report 2014 Abstract: Report of the experimental activities in Hall A at Thomas Jefferson National\nAccelerator Facility during 2013. \n\n"}
{"id": "1512.00596", "contents": "Title: The MegaFace Benchmark: 1 Million Faces for Recognition at Scale Abstract: Recent face recognition experiments on a major benchmark LFW show stunning\nperformance--a number of algorithms achieve near to perfect score, surpassing\nhuman recognition rates. In this paper, we advocate evaluations at the million\nscale (LFW includes only 13K photos of 5K people). To this end, we have\nassembled the MegaFace dataset and created the first MegaFace challenge. Our\ndataset includes One Million photos that capture more than 690K different\nindividuals. The challenge evaluates performance of algorithms with increasing\nnumbers of distractors (going from 10 to 1M) in the gallery set. We present\nboth identification and verification performance, evaluate performance with\nrespect to pose and a person's age, and compare as a function of training data\nsize (number of photos and people). We report results of state of the art and\nbaseline algorithms. Our key observations are that testing at the million scale\nreveals big performance differences (of algorithms that perform similarly well\non smaller scale) and that age invariant recognition as well as pose are still\nchallenging for most. The MegaFace dataset, baseline code, and evaluation\nscripts, are all publicly released for further experimentations at:\nmegaface.cs.washington.edu. \n\n"}
{"id": "1512.01824", "contents": "Title: The Dortmund Low Background Facility - Low-Background Gamma Ray\n  Spectrometry with an Artificial Overburden Abstract: High-purity germanium (HPGe) detectors used for low-background gamma ray\nspectrometry are usually operated under either a fairly low overburden of the\norder of one meter of water equivalent (mw.e.) or a high overburden of the\norder of 100mw.e. or more, e.g. in specialized underground laboratories. The\nDortmund Low Background Facility (DLB) combines the advantages of both\napproaches. The artificial overburden of 10mw.e. already shields the hadronic\ncomponent of cosmic rays. The inner shielding, featuring a state-of-the-art\nneutron shielding and an active muon veto, enables low-background gamma ray\nspectrometry at an easy-accessible location at the campus of the Technische\nUniversit\\\"at Dortmund.\n  The integral background count rate between 40keV and 2700keV is\n2.528+-0.004counts/kg/minute. This enables activity measurements of primordial\nradionuclides in the range of some 10mBq/kg within a week of measurement time. \n\n"}
{"id": "1512.02167", "contents": "Title: Simple Baseline for Visual Question Answering Abstract: We describe a very simple bag-of-words baseline for visual question\nanswering. This baseline concatenates the word features from the question and\nCNN features from the image to predict the answer. When evaluated on the\nchallenging VQA dataset [2], it shows comparable performance to many recent\napproaches using recurrent neural networks. To explore the strength and\nweakness of the trained model, we also provide an interactive web demo and\nopen-source code. . \n\n"}
{"id": "1512.02188", "contents": "Title: Pseudo-Bayesian Robust PCA: Algorithms and Analyses Abstract: Commonly used in computer vision and other applications, robust PCA\nrepresents an algorithmic attempt to reduce the sensitivity of classical PCA to\noutliers. The basic idea is to learn a decomposition of some data matrix of\ninterest into low rank and sparse components, the latter representing unwanted\noutliers. Although the resulting optimization problem is typically NP-hard,\nconvex relaxations provide a computationally-expedient alternative with\ntheoretical support. However, in practical regimes performance guarantees break\ndown and a variety of non-convex alternatives, including Bayesian-inspired\nmodels, have been proposed to boost estimation quality. Unfortunately though,\nwithout additional a priori knowledge none of these methods can significantly\nexpand the critical operational range such that exact principal subspace\nrecovery is possible. Into this mix we propose a novel pseudo-Bayesian\nalgorithm that explicitly compensates for design weaknesses in many existing\nnon-convex approaches leading to state-of-the-art performance with a sound\nanalytical foundation. Surprisingly, our algorithm can even outperform convex\nmatrix completion despite the fact that the latter is provided with perfect\nknowledge of which entries are not corrupted. \n\n"}
{"id": "1512.02949", "contents": "Title: Video captioning with recurrent networks based on frame- and video-level\n  features and visual content classification Abstract: In this paper, we describe the system for generating textual descriptions of\nshort video clips using recurrent neural networks (RNN), which we used while\nparticipating in the Large Scale Movie Description Challenge 2015 in ICCV 2015.\nOur work builds on static image captioning systems with RNN based language\nmodels and extends this framework to videos utilizing both static image\nfeatures and video-specific features. In addition, we study the usefulness of\nvisual content classifiers as a source of additional information for caption\ngeneration. With experimental results we show that utilizing keyframe based\nfeatures, dense trajectory video features and content classifier outputs\ntogether gives better performance than any one of them individually. \n\n"}
{"id": "1512.03738", "contents": "Title: Low-Mass Dielectron Production in pp, p-Pb and Pb-Pb Collisions with\n  ALICE Abstract: The ALICE Collaboration measures the production of low-mass dielectrons in\npp, p-Pb and Pb-Pb collisions at the LHC. The main detectors used in the\nanalyses are the Inner Tracking System, Time Projection Chamber and\nTime-Of-Flight detector, all located at mid-rapidity. The dielectron yield in\np-Pb collisions shows an overall agreement with the hadronic cocktail. The pair\ntransverse momentum distributions are sensitive to the contributions from open\nheavy-flavours. In Pb-Pb collisions, uncorrected background-subtracted yields\nhave been extracted in two centrality classes. In pp collisions the production\nof virtual photons relative to the inclusive yield is determined by analyzing\nthe dielectron excess with respect to the expected hadronic sources. The direct\nphoton cross section is then calculated and found to be in agreement with NLO\npQCD calculations. A feasibility study for LHC Run 3 after the ALICE upgrade\nindicates the possibility for a future measurement of the early effective\ntemperature. \n\n"}
{"id": "1512.04065", "contents": "Title: Cross-dimensional Weighting for Aggregated Deep Convolutional Features Abstract: We propose a simple and straightforward way of creating powerful image\nrepresentations via cross-dimensional weighting and aggregation of deep\nconvolutional neural network layer outputs. We first present a generalized\nframework that encompasses a broad family of approaches and includes\ncross-dimensional pooling and weighting steps. We then propose specific\nnon-parametric schemes for both spatial- and channel-wise weighting that boost\nthe effect of highly active spatial responses and at the same time regulate\nburstiness effects. We experiment on different public datasets for image search\nand show that our approach outperforms the current state-of-the-art for\napproaches based on pre-trained networks. We also provide an easy-to-use, open\nsource implementation that reproduces our results. \n\n"}
{"id": "1601.00493", "contents": "Title: The ALICE Transition Radiation Detector: status and perspectives for Run\n  II Abstract: The ALICE Transition Radiation Detector contributes to the tracking, particle\nidentification, and triggering capabilities of the experiment. It is composed\nof six layers of multi-wire proportional chambers, each of which is preceded by\na radiator and a Xe/CO$_2$-filled drift volume. The signal is sampled in\ntimebins of 100~ns over the drift length which allows for the reconstruction of\nchamber-wise track segments, both online and offline. The particle\nidentification is based on the specific energy loss of charged particles and\nadditional transition radiation photons, the latter being a signature for\nelectrons.\n  The detector is segmented into 18 sectors, of which 13 were installed in Run\nI. The TRD was included in data taking since the LHC start-up and was\nsuccessfully used for electron identification and triggering. During the Long\nShutdown 1, the detector was completed and now covers the full azimuthal\nacceptance. Furthermore, the readout and trigger components were upgraded. When\ndata taking was started for \\runii{}, their performance fulfilled the\nexpectations. \n\n"}
{"id": "1601.01339", "contents": "Title: Quality Adaptive Low-Rank Based JPEG Decoding with Applications Abstract: Small compression noises, despite being transparent to human eyes, can\nadversely affect the results of many image restoration processes, if left\nunaccounted for. Especially, compression noises are highly detrimental to\ninverse operators of high-boosting (sharpening) nature, such as deblurring and\nsuperresolution against a convolution kernel. By incorporating the non-linear\nDCT quantization mechanism into the formulation for image restoration, we\npropose a new sparsity-based convex programming approach for joint compression\nnoise removal and image restoration. Experimental results demonstrate\nsignificant performance gains of the new approach over existing image\nrestoration methods. \n\n"}
{"id": "1601.01903", "contents": "Title: Studies of relative gain and timing response of fine-mesh\n  photomultiplier tubes in high magnetic fields Abstract: We investigated the use of Hamamatsu fine-mesh photomultiplier tube\nassemblies H6152-70 and H6614-70 with regards to their gain and timing\nresolution in magnetic fields up to 1.9 T. Our results show that the H6614-70\nassembly can operate reliably in magnetic fields exceeding 1.5 T, while\npreserving a reasonable timing resolution even with a gain reduction of a\nfactor of approximately 100. The reduction of the relative gain of the H6152-70\nis similar to the H6614-70's near 1.5 T, but its timing resolution worsens\nconsiderably at this high field. \n\n"}
{"id": "1601.03599", "contents": "Title: A gas cell for stopping, storing and polarizing radioactive particles Abstract: A radioactive beam of 20Na is stopped in a gas cell filled with Ne gas. The\nstopped particles are polarized by optical pumping. The degree of polarization\nthat can be achieved is studied. A maximum polarization of 50% was found. The\ndynamic processes in the cell are described with a phenomenological model. \n\n"}
{"id": "1601.05631", "contents": "Title: Specific Heat of Matter Formed in Relativistic Nuclear Collisions Abstract: We report the excitation energy dependence of specific heat (\\cv) of hadronic\nmatter at freeze-out in Au+Au and Cu+Cu collisions at the Relativistic Heavy\nIon Collider energies by analyzing the published data on event-by-event mean\ntransverse momentum (\\meanpt) distributions. The \\meanpt~distributions in\nfinite \\pt~ranges are converted to distributions of effective temperatures, and\ndynamical fluctuations in temperature are extracted by subtracting widths of\nthe corresponding mixed event distributions. The heat capacity per particle at\nthe kinetic freeze-out surface is presented as a function of collision energy,\nwhich shows a sharp rise in \\cv~below \\sNN~=~62.4~GeV. We employ the Hadron\nResonance Gas (HRG) model to estimate \\cv~at the chemical and kinetic\nfreeze-out surfaces. The experimental results are compared to the HRG and other\ntheoretical model calculations. HRG results show good agreement with data.\nModel predictions for \\cv~at the Large Hadron Collider energy are presented. \n\n"}
{"id": "1601.06057", "contents": "Title: Topological descriptors for 3D surface analysis Abstract: We investigate topological descriptors for 3D surface analysis, i.e. the\nclassification of surfaces according to their geometric fine structure. On a\ndataset of high-resolution 3D surface reconstructions we compute persistence\ndiagrams for a 2D cubical filtration. In the next step we investigate different\ntopological descriptors and measure their ability to discriminate structurally\ndifferent 3D surface patches. We evaluate their sensitivity to different\nparameters and compare the performance of the resulting topological descriptors\nto alternative (non-topological) descriptors. We present a comprehensive\nevaluation that shows that topological descriptors are (i) robust, (ii) yield\nstate-of-the-art performance for the task of 3D surface analysis and (iii)\nimprove classification performance when combined with non-topological\ndescriptors. \n\n"}
{"id": "1601.07048", "contents": "Title: An improvement of isochronous mass spectrometry: Velocity measurements\n  using two time-of-flight detectors Abstract: Isochronous mass spectrometry (IMS) in storage rings is a powerful tool for\nmass measurements of exotic nuclei with very short half-lives down to several\ntens of microseconds, using a multicomponent secondary beam separated in-flight\nwithout cooling. However, the inevitable momentum spread of secondary ions\nlimits the precision of nuclear masses determined by using IMS. Therefore, the\nmomentum measurement in addition to the revolution period of stored ions is\ncrucial to reduce the influence of the momentum spread on the standard\ndeviation of the revolution period, which would lead to a much improved mass\nresolving power of IMS. One of the proposals to upgrade IMS is that the\nvelocity of secondary ions could be directly measured by using two\ntime-of-flight (double TOF) detectors installed in a straight section of a\nstorage ring. In this paper, we outline the principle of IMS with double TOF\ndetectors and the method to correct the momentum spread of stored ions. \n\n"}
{"id": "1601.07140", "contents": "Title: COCO-Text: Dataset and Benchmark for Text Detection and Recognition in\n  Natural Images Abstract: This paper describes the COCO-Text dataset. In recent years large-scale\ndatasets like SUN and Imagenet drove the advancement of scene understanding and\nobject recognition. The goal of COCO-Text is to advance state-of-the-art in\ntext detection and recognition in natural images. The dataset is based on the\nMS COCO dataset, which contains images of complex everyday scenes. The images\nwere not collected with text in mind and thus contain a broad variety of text\ninstances. To reflect the diversity of text in natural scenes, we annotate text\nwith (a) location in terms of a bounding box, (b) fine-grained classification\ninto machine printed text and handwritten text, (c) classification into legible\nand illegible text, (d) script of the text and (e) transcriptions of legible\ntext. The dataset contains over 173k text annotations in over 63k images. We\nprovide a statistical analysis of the accuracy of our annotations. In addition,\nwe present an analysis of three leading state-of-the-art photo Optical\nCharacter Recognition (OCR) approaches on our dataset. While scene text\ndetection and recognition enjoys strong advances in recent years, we identify\nsignificant shortcomings motivating future work. \n\n"}
{"id": "1602.00970", "contents": "Title: Visual descriptors for content-based retrieval of remote sensing images Abstract: In this paper we present an extensive evaluation of visual descriptors for\nthe content-based retrieval of remote sensing (RS) images. The evaluation\nincludes global hand-crafted, local hand-crafted, and Convolutional Neural\nNetwork (CNNs) features coupled with four different Content-Based Image\nRetrieval schemes. We conducted all the experiments on two publicly available\ndatasets: the 21-class UC Merced Land Use/Land Cover (LandUse) dataset and\n19-class High-resolution Satellite Scene dataset (SceneSat). The content of RS\nimages might be quite heterogeneous, ranging from images containing fine\ngrained textures, to coarse grained ones or to images containing objects. It is\ntherefore not obvious in this domain, which descriptor should be employed to\ndescribe images having such a variability. Results demonstrate that CNN-based\nfeatures perform better than both global and and local hand-crafted features\nwhatever is the retrieval scheme adopted. Features extracted from SatResNet-50,\na residual CNN suitable fine-tuned on the RS domain, shows much better\nperformance than a residual CNN pre-trained on multimedia scene and object\nimages. Features extracted from NetVLAD, a CNN that considers both CNN and\nlocal features, works better than others CNN solutions on those images that\ncontain fine-grained textures and objects. \n\n"}
{"id": "1602.01392", "contents": "Title: Particle identification in ALICE: a Bayesian approach Abstract: We present a Bayesian approach to particle identification (PID) within the\nALICE experiment. The aim is to more effectively combine the particle\nidentification capabilities of its various detectors. After a brief explanation\nof the adopted methodology and formalism, the performance of the Bayesian PID\napproach for charged pions, kaons and protons in the central barrel of ALICE is\nstudied. PID is performed via measurements of specific energy loss\n($\\mathrm{d}E/\\mathrm{d}x$) and time-of-flight. PID efficiencies and\nmisidentification probabilities are extracted and compared with Monte Carlo\nsimulations using high-purity samples of identified particles in the decay\nchannels ${\\rm K}^0_S \\rightarrow \\pi^-\\pi^+$, $\\phi \\rightarrow {\\rm K}^-{\\rm\nK}^+$, and $\\Lambda \\rightarrow {\\rm p}\\pi^-$ in p-Pb collisions at\n$\\sqrt{s_{\\rm NN}}=5.02$ TeV. In order to thoroughly assess the validity of the\nBayesian approach, this methodology was used to obtain corrected $p_{\\rm T}$\nspectra of pions, kaons, protons, and D$^0$ mesons in pp collisions at\n$\\sqrt{s}=7$ TeV. In all cases, the results using Bayesian PID were found to be\nconsistent with previous measurements performed by ALICE using a standard PID\napproach. For the measurement of D$^0 \\rightarrow {\\rm K}^-\\pi^+$, it was found\nthat a Bayesian PID approach gave a higher signal-to-background ratio and a\nsimilar or larger statistical significance when compared with standard PID\nselections, despite a reduced identification efficiency. Finally, we present an\nexploratory study of the measurement of $\\Lambda_{\\rm c}^{+}\\rightarrow {\\rm p}\n{\\rm K}^-\\pi^+$ in pp collisions at $\\sqrt{s}=7$ TeV, using the Bayesian\napproach for the identification of its decay products. \n\n"}
{"id": "1602.01755", "contents": "Title: Time-of-flight Fourier UCN spectrometer Abstract: We describe a new time-of-flight Fourier spectrometer for investigation of\nUCN diffraction by a moving grating. The device operates in the regime of a\ndiscrete set of modulation frequencies. The results of the first experiments\nshow that the spectrometer may be used for obtaining UCN energy spectra in the\nenergy range of 60$\\div$200 neV with a resolution of about 5 neV. The accuracy\nof determination of the line position was estimated to be several units of\n$10^{-10}$ eV \n\n"}
{"id": "1602.03377", "contents": "Title: Multi-particle correlations in transverse momenta from statistical\n  clusters Abstract: We evaluate $n$-particle ($n=2,3,4,5$) transverse momentum correlations for\npions and kaons following from the decay of statistical clusters. These\ncorrelation functions could provide strong constraints on a possible existence\nof thermal clusters in the process of particle production. \n\n"}
{"id": "1602.03616", "contents": "Title: Multifaceted Feature Visualization: Uncovering the Different Types of\n  Features Learned By Each Neuron in Deep Neural Networks Abstract: We can better understand deep neural networks by identifying which features\neach of their neurons have learned to detect. To do so, researchers have\ncreated Deep Visualization techniques including activation maximization, which\nsynthetically generates inputs (e.g. images) that maximally activate each\nneuron. A limitation of current techniques is that they assume each neuron\ndetects only one type of feature, but we know that neurons can be multifaceted,\nin that they fire in response to many different types of features: for example,\na grocery store class neuron must activate either for rows of produce or for a\nstorefront. Previous activation maximization techniques constructed images\nwithout regard for the multiple different facets of a neuron, creating\ninappropriate mixes of colors, parts of objects, scales, orientations, etc.\nHere, we introduce an algorithm that explicitly uncovers the multiple facets of\neach neuron by producing a synthetic visualization of each of the types of\nimages that activate a neuron. We also introduce regularization methods that\nproduce state-of-the-art results in terms of the interpretability of images\nobtained by activation maximization. By separately synthesizing each type of\nimage a neuron fires in response to, the visualizations have more appropriate\ncolors and coherent global structure. Multifaceted feature visualization thus\nprovides a clearer and more comprehensive description of the role of each\nneuron. \n\n"}
{"id": "1602.03832", "contents": "Title: Improved empirical parametrizations of the $\\gamma^\\ast N \\to\n  \\Delta(1232)$ and $\\gamma^\\ast N \\to N(1520)$ helicity amplitudes and the\n  Siegert's theorem Abstract: In the nucleon electroexcitation reactions, $\\gamma^\\ast N \\to R$, where $R$\nis a nucleon resonance ($N^\\ast$), the electric amplitude $E$, and the\nlongitudinal amplitude $S_{1/2}$, are related by $E \\propto \\frac{\\omega}{|{\\bf\nq}|}S_{1/2}$, at the pseudo-threshold limit ($|{\\bf q}| \\to 0$), where $\\omega$\nand $|{\\bf q}|$ are respectively the energy and the magnitude of three-momentum\nof the photon. The previous relation is usually refereed as the Siegert's\ntheorem. The form of the electric amplitude, defined in terms of the transverse\namplitudes $A_{1/2}$ and $A_{3/2}$, and the explicit coefficients of the\nrelation, depend on the angular momentum and parity ($J^P$) of the resonance\n$R$. The Siegert's theorem is the consequence of the structure of the\nelectromagnetic transition current, which induces constraints between the\nelectromagnetic form factors in the pseudo-threshold limit. In the present\nwork, we study the implications of the Siegert's theorem for the $\\gamma^\\ast N\n\\to \\Delta(1232)$ and $\\gamma^\\ast N \\to N(1520)$ transitions. For the\n$\\gamma^\\ast N \\to N(1520)$ transition, in addition to the relation between\nelectric amplitude and longitudinal amplitude, we obtain also a relation\nbetween the two transverse amplitudes: $A_{1/2}= A_{3/2} /\\sqrt{3}$, at the\npseudo-threshold. % The constraints at the pseudo-threshold are tested for the\nMAID2007 parametrizations of the reactions under discussion. New\nparametrizations for the amplitudes $A_{1/2}$, $A_{3/2}$ and $S_{1/2}$, for the\n$\\gamma^\\ast N \\to \\Delta(1232)$ and $\\gamma^\\ast N \\to N(1520)$ transitions,\nvalid for small and large $Q^2$, are proposed. The new parametrizations are\nconsistent with both: the pseudo-threshold constraints (Siegert's theorem) and\nthe empirical data. \n\n"}
{"id": "1602.07401", "contents": "Title: A CaMoO4 Crystal Low Temperature Detector for the AMoRE Neutrinoless\n  Double Beta Decay Search Abstract: We report the development of a CaMoO4 crystal low temperature detector for\nthe AMoRE neutrinoless double beta decay (0{\\nu}\\b{eta}\\b{eta}) search\nexperiment. The prototype detector cell was composed of a 216 g CaMoO4 crystal\nand a metallic magnetic calorimeter. An over-ground measurement demonstrated\nFWHM resolution of 6-11 keV for full absorption gamma peaks. Pulse shape\ndiscrimination was clearly demonstrated in the phonon signals, and 7.6 {\\sigma}\nof discrimination power was found for the {\\alpha} and \\b{eta}/{\\gamma}\nseparation. The phonon signals showed rise-times of about 1 ms. It is expected\nthat the relatively fast rise-time will increase the rejection efficiency of\ntwo-neutrino double beta decay pile-up events which can be one of the major\nbackground sources in 0{\\nu}\\b{eta}\\b{eta} searches. \n\n"}
{"id": "1602.07475", "contents": "Title: A fine-grained approach to scene text script identification Abstract: This paper focuses on the problem of script identification in unconstrained\nscenarios. Script identification is an important prerequisite to recognition,\nand an indispensable condition for automatic text understanding systems\ndesigned for multi-language environments. Although widely studied for document\nimages and handwritten documents, it remains an almost unexplored territory for\nscene text images.\n  We detail a novel method for script identification in natural images that\ncombines convolutional features and the Naive-Bayes Nearest Neighbor\nclassifier. The proposed framework efficiently exploits the discriminative\npower of small stroke-parts, in a fine-grained classification framework.\n  In addition, we propose a new public benchmark dataset for the evaluation of\njoint text detection and script identification in natural scenes. Experiments\ndone in this new dataset demonstrate that the proposed method yields state of\nthe art results, while it generalizes well to different datasets and variable\nnumber of scripts. The evidence provided shows that multi-lingual scene text\nrecognition in the wild is a viable proposition. Source code of the proposed\nmethod is made available online. \n\n"}
{"id": "1602.07528", "contents": "Title: Trilateration-based reconstruction of ortho-positronium decays into\n  three photons with the J-PET detector Abstract: This work reports on a new reconstruction algorithm allowing to reconstruct\nthe decays of ortho-positronium atoms into three photons using the places and\ntimes of photons recorded in the detector. The method is based on trilateration\nand allows for a simultaneous reconstruction of both location and time of the\ndecay. Results of resolution tests of the new reconstruction in the J-PET\ndetector based on Monte Carlo simulations are presented, which yield a spatial\nresolution at the level of 2 cm (FWHM) for X and Y and at the level of 1 cm\n(FWHM) for Z available with the present resolution of J-PET after application\nof a kinematic fit. Prospects of employment of this method for studying angular\ncorrelations of photons in decays of polarized ortho-positronia for the needs\nof tests of CP and CPT discrete symmetries are also discussed. The new\nreconstruction method allows for discrimination of background from random\nthree-photon coincidences as well as for application of a novel method for\ndetermination of the linear polarization of ortho-positronium atoms, which is\nalso introduced in this work. \n\n"}
{"id": "1602.08680", "contents": "Title: Measuring and Predicting Tag Importance for Image Retrieval Abstract: Textual data such as tags, sentence descriptions are combined with visual\ncues to reduce the semantic gap for image retrieval applications in today's\nMultimodal Image Retrieval (MIR) systems. However, all tags are treated as\nequally important in these systems, which may result in misalignment between\nvisual and textual modalities during MIR training. This will further lead to\ndegenerated retrieval performance at query time. To address this issue, we\ninvestigate the problem of tag importance prediction, where the goal is to\nautomatically predict the tag importance and use it in image retrieval. To\nachieve this, we first propose a method to measure the relative importance of\nobject and scene tags from image sentence descriptions. Using this as the\nground truth, we present a tag importance prediction model to jointly exploit\nvisual, semantic and context cues. The Structural Support Vector Machine (SSVM)\nformulation is adopted to ensure efficient training of the prediction model.\nThen, the Canonical Correlation Analysis (CCA) is employed to learn the\nrelation between the image visual feature and tag importance to obtain robust\nretrieval performance. Experimental results on three real-world datasets show a\nsignificant performance improvement of the proposed MIR with Tag Importance\nPrediction (MIR/TIP) system over other MIR systems. \n\n"}
{"id": "1603.00993", "contents": "Title: Self-localization from Images with Small Overlap Abstract: With the recent success of visual features from deep convolutional neural\nnetworks (DCNN) in visual robot self-localization, it has become important and\npractical to address more general self-localization scenarios. In this paper,\nwe address the scenario of self-localization from images with small overlap. We\nexplicitly introduce a localization difficulty index as a decreasing function\nof view overlap between query and relevant database images and investigate\nperformance versus difficulty for challenging cross-view self-localization\ntasks. We then reformulate the self-localization as a scalable\nbag-of-visual-features (BoVF) scene retrieval and present an efficient solution\ncalled PCA-NBNN, aiming to facilitate fast and yet discriminative\ncorrespondence between partially overlapping images. The proposed approach\nadopts recent findings in discriminativity preserving encoding of DCNN features\nusing principal component analysis (PCA) and cross-domain scene matching using\nnaive Bayes nearest neighbor distance metric (NBNN). We experimentally\ndemonstrate that the proposed PCA-NBNN framework frequently achieves comparable\nresults to previous DCNN features and that the BoVF model is significantly more\nefficient. We further address an important alternative scenario of\n\"self-localization from images with NO overlap\" and report the result. \n\n"}
{"id": "1603.01067", "contents": "Title: Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain\n  Decoding Abstract: We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)\nbrain volumes recorded during a cognitive stimulus by a graph which consists of\na set of local meshes. The corresponding cognitive process, encoded in the\nbrain, is then represented by these meshes each of which is estimated assuming\na linear relationship among the voxel time series in a predefined locality.\nFirst, we define the concept of locality in two neighborhood systems, namely,\nthe spatial and functional neighborhoods. Then, we construct spatially and\nfunctionally local meshes around each voxel, called seed voxel, by connecting\nit either to its spatial or functional p-nearest neighbors. The mesh formed\naround a voxel is a directed sub-graph with a star topology, where the\ndirection of the edges is taken towards the seed voxel at the center of the\nmesh. We represent the time series recorded at each seed voxel in terms of\nlinear combination of the time series of its p-nearest neighbors in the mesh.\nThe relationships between a seed voxel and its neighbors are represented by the\nedge weights of each mesh, and are estimated by solving a linear regression\nequation. The estimated mesh edge weights lead to a better representation of\ninformation in the brain for encoding and decoding of the cognitive tasks. We\ntest our model on a visual object recognition and emotional memory retrieval\nexperiments using Support Vector Machines that are trained using the mesh edge\nweights as features. In the experimental analysis, we observe that the edge\nweights of the spatial and functional meshes perform better than the\nstate-of-the-art brain decoding models. \n\n"}
{"id": "1603.01068", "contents": "Title: First Steps Toward Camera Model Identification with Convolutional Neural\n  Networks Abstract: Detecting the camera model used to shoot a picture enables to solve a wide\nseries of forensic problems, from copyright infringement to ownership\nattribution. For this reason, the forensic community has developed a set of\ncamera model identification algorithms that exploit characteristic traces left\non acquired images by the processing pipelines specific of each camera model.\nIn this paper, we investigate a novel approach to solve camera model\nidentification problem. Specifically, we propose a data-driven algorithm based\non convolutional neural networks, which learns features characterizing each\ncamera model directly from the acquired pictures. Results on a well-known\ndataset of 18 camera models show that: (i) the proposed method outperforms\nup-to-date state-of-the-art algorithms on classification of 64x64 color image\npatches; (ii) features learned by the proposed network generalize to camera\nmodels never used for training. \n\n"}
{"id": "1603.02814", "contents": "Title: Image Captioning and Visual Question Answering Based on Attributes and\n  External Knowledge Abstract: Much recent progress in Vision-to-Language problems has been achieved through\na combination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs). This approach does not explicitly represent high-level\nsemantic concepts, but rather seeks to progress directly from image features to\ntext. In this paper we first propose a method of incorporating high-level\nconcepts into the successful CNN-RNN approach, and show that it achieves a\nsignificant improvement on the state-of-the-art in both image captioning and\nvisual question answering. We further show that the same mechanism can be used\nto incorporate external knowledge, which is critically important for answering\nhigh level visual questions. Specifically, we design a visual question\nanswering model that combines an internal representation of the content of an\nimage with information extracted from a general knowledge base to answer a\nbroad range of image-based questions. It particularly allows questions to be\nasked about the contents of an image, even when the image itself does not\ncontain a complete answer. Our final model achieves the best reported results\non both image captioning and visual question answering on several benchmark\ndatasets. \n\n"}
{"id": "1603.03402", "contents": "Title: Centrality dependence of charged jet production in p-Pb collisions at\n  $\\sqrt{s_\\mathrm{NN}}$ = 5.02 TeV Abstract: Measurements of charged jet production as a function of centrality are\npresented for p-Pb collisions recorded at $\\sqrt{s_{\\rm NN}} = 5.02$ TeV with\nthe ALICE detector. Centrality classes are determined via the energy deposit in\nneutron calorimeters at zero degree, close to the beam direction, to minimise\ndynamical biases of the selection. The corresponding number of participants or\nbinary nucleon-nucleon collisions is determined based on the particle\nproduction in the Pb-going rapidity region. Jets have been reconstructed in the\ncentral rapidity region from charged particles with the anti-$k_{\\rm T}$\nalgorithm for resolution parameters $R = 0.2$ and $R = 0.4$ in the transverse\nmomentum range 20 to 120 GeV/$c$. The reconstructed jet momentum and yields\nhave been corrected for detector effects and underlying-event background. In\nthe five centrality bins considered, the charged jet production in p-Pb\ncollisions is consistent with the production expected from binary scaling from\npp collisions. The ratio of jet yields reconstructed with the two different\nresolution parameters is also independent of the centrality selection,\ndemonstrating the absence of major modifications of the radial jet structure in\nthe reported centrality classes. \n\n"}
{"id": "1603.03925", "contents": "Title: Image Captioning with Semantic Attention Abstract: Automatically generating a natural language description of an image has\nattracted interests recently both because of its importance in practical\napplications and because it connects two major artificial intelligence fields:\ncomputer vision and natural language processing. Existing approaches are either\ntop-down, which start from a gist of an image and convert it into words, or\nbottom-up, which come up with words describing various aspects of an image and\nthen combine them. In this paper, we propose a new algorithm that combines both\napproaches through a model of semantic attention. Our algorithm learns to\nselectively attend to semantic concept proposals and fuse them into hidden\nstates and outputs of recurrent neural networks. The selection and fusion form\na feedback connecting the top-down and bottom-up computation. We evaluate our\nalgorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental\nresults show that our algorithm significantly outperforms the state-of-the-art\napproaches consistently across different evaluation metrics. \n\n"}
{"id": "1603.03958", "contents": "Title: Template Adaptation for Face Verification and Identification Abstract: Face recognition performance evaluation has traditionally focused on\none-to-one verification, popularized by the Labeled Faces in the Wild dataset\nfor imagery and the YouTubeFaces dataset for videos. In contrast, the newly\nreleased IJB-A face recognition dataset unifies evaluation of one-to-many face\nidentification with one-to-one face verification over templates, or sets of\nimagery and videos for a subject. In this paper, we study the problem of\ntemplate adaptation, a form of transfer learning to the set of media in a\ntemplate. Extensive performance evaluations on IJB-A show a surprising result,\nthat perhaps the simplest method of template adaptation, combining deep\nconvolutional network features with template specific linear SVMs, outperforms\nthe state-of-the-art by a wide margin. We study the effects of template size,\nnegative set construction and classifier fusion on performance, then compare\ntemplate adaptation to convolutional networks with metric learning, 2D and 3D\nalignment. Our unexpected conclusion is that these other methods, when combined\nwith template adaptation, all achieve nearly the same top performance on IJB-A\nfor template-based face verification and identification. \n\n"}
{"id": "1603.05474", "contents": "Title: Neural Aggregation Network for Video Face Recognition Abstract: This paper presents a Neural Aggregation Network (NAN) for video face\nrecognition. The network takes a face video or face image set of a person with\na variable number of face images as its input, and produces a compact,\nfixed-dimension feature representation for recognition. The whole network is\ncomposed of two modules. The feature embedding module is a deep Convolutional\nNeural Network (CNN) which maps each face image to a feature vector. The\naggregation module consists of two attention blocks which adaptively aggregate\nthe feature vectors to form a single feature inside the convex hull spanned by\nthem. Due to the attention mechanism, the aggregation is invariant to the image\norder. Our NAN is trained with a standard classification or verification loss\nwithout any extra supervision signal, and we found that it automatically learns\nto advocate high-quality face images while repelling low-quality ones such as\nblurred, occluded and improperly exposed faces. The experiments on IJB-A,\nYouTube Face, Celebrity-1000 video face recognition benchmarks show that it\nconsistently outperforms naive aggregation methods and achieves the\nstate-of-the-art accuracy. \n\n"}
{"id": "1603.06098", "contents": "Title: Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image\n  Segmentation Abstract: We introduce a new loss function for the weakly-supervised training of\nsemantic image segmentation models based on three guiding principles: to seed\nwith weak localization cues, to expand objects based on the information about\nwhich classes can occur in an image, and to constrain the segmentations to\ncoincide with object boundaries. We show experimentally that training a deep\nconvolutional neural network using the proposed loss function leads to\nsubstantially better segmentations than previous state-of-the-art methods on\nthe challenging PASCAL VOC 2012 dataset. We furthermore give insight into the\nworking mechanism of our method by a detailed experimental study that\nillustrates how the segmentation quality is affected by each term of the\nproposed loss function as well as their combinations. \n\n"}
{"id": "1603.07234", "contents": "Title: Lightweight Unsupervised Domain Adaptation by Convolutional Filter\n  Reconstruction Abstract: End-to-end learning methods have achieved impressive results in many areas of\ncomputer vision. At the same time, these methods still suffer from a\ndegradation in performance when testing on new datasets that stem from a\ndifferent distribution. This is known as the domain shift effect. Recently\nproposed adaptation methods focus on retraining the network parameters.\nHowever, this requires access to all (labeled) source data, a large amount of\n(unlabeled) target data, and plenty of computational resources. In this work,\nwe propose a lightweight alternative, that allows adapting to the target domain\nbased on a limited number of target samples in a matter of minutes rather than\nhours, days or even weeks. To this end, we first analyze the output of each\nconvolutional layer from a domain adaptation perspective. Surprisingly, we find\nthat already at the very first layer, domain shift effects pop up. We then\npropose a new domain adaptation method, where first layer convolutional filters\nthat are badly affected by the domain shift are reconstructed based on less\naffected ones. This improves the performance of the deep network on various\nbenchmark datasets. \n\n"}
{"id": "1603.07810", "contents": "Title: Conditional Similarity Networks Abstract: What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately. \n\n"}
{"id": "1603.08049", "contents": "Title: Cerenkov light identification with Si low-temperature detectors with\n  Neganov-Luke effect-enhanced sensitivity Abstract: A new generation of cryogenic light detectors exploiting Neganov-Luke effect\nto enhance the thermal signal has been used to detect the Cherenkov light\nemitted by the electrons interacting in TeO$_{2}$ crystals. With this mechanism\na high significance event-by-event discrimination between alpha and beta/gamma\ninteractions at the $^{130}$Te neutrino-less double beta decay Q-value -\n(2527.515 $\\pm$ 0.013) keV - has been demonstrated. This measurement opens the\npossibility of drastically reducing the background in cryogenic experiments\nbased on TeO$_{2}$. \n\n"}
{"id": "1603.08390", "contents": "Title: A Generic Inverted Index Framework for Similarity Search on the GPU -\n  Technical Report Abstract: We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named $\\tau$-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source. \n\n"}
{"id": "1603.08695", "contents": "Title: Learning to Refine Object Segments Abstract: Object segmentation requires both object-level information and low-level\npixel data. This presents a challenge for feedforward networks: lower layers in\nconvolutional nets capture rich spatial information, while upper layers encode\nobject-level knowledge but are invariant to factors such as pose and\nappearance. In this work we propose to augment feedforward nets for object\nsegmentation with a novel top-down refinement approach. The resulting\nbottom-up/top-down architecture is capable of efficiently generating\nhigh-fidelity object masks. Similarly to skip connections, our approach\nleverages features at all layers of the net. Unlike skip connections, our\napproach does not attempt to output independent predictions at each layer.\nInstead, we first output a coarse `mask encoding' in a feedforward pass, then\nrefine this mask encoding in a top-down pass utilizing features at successively\nlower layers. The approach is simple, fast, and effective. Building on the\nrecent DeepMask network for generating object proposals, we show accuracy\nimprovements of 10-20% in average recall for various setups. Additionally, by\noptimizing the overall network architecture, our approach, which we call\nSharpMask, is 50% faster than the original DeepMask network (under .8s per\nimage). \n\n"}
{"id": "1603.08895", "contents": "Title: Latent Embeddings for Zero-shot Classification Abstract: We present a novel latent embedding model for learning a compatibility\nfunction between image and class embeddings, in the context of zero-shot\nclassification. The proposed method augments the state-of-the-art bilinear\ncompatibility model by incorporating latent variables. Instead of learning a\nsingle bilinear map, it learns a collection of maps with the selection, of\nwhich map to use, being a latent variable for the current image-class pair. We\ntrain the model with a ranking based objective function which penalizes\nincorrect rankings of the true class for a given image. We empirically\ndemonstrate that our model improves the state-of-the-art for various class\nembeddings consistently on three challenging publicly available datasets for\nthe zero-shot setting. Moreover, our method leads to visually highly\ninterpretable results with clear clusters of different fine-grained object\nproperties that correspond to different latent variable maps. \n\n"}
{"id": "1603.09046", "contents": "Title: Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for\n  Locally Robust Captioning Abstract: The workflow of extracting features from images using convolutional neural\nnetworks (CNN) and generating captions with recurrent neural networks (RNN) has\nbecome a de-facto standard for image captioning task. However, since CNN\nfeatures are originally designed for classification task, it is mostly\nconcerned with the main conspicuous element of the image, and often fails to\ncorrectly convey information on local, secondary elements. We propose to\nincorporate coding with vector of locally aggregated descriptors (VLAD) on\nspatial pyramid for CNN features of sub-regions in order to generate image\nrepresentations that better reflect the local information of the images. Our\nresults show that our method of compact VLAD coding can match CNN features with\nas little as 3% of dimensionality and, when combined with spatial pyramid, it\nresults in image captions that more accurately take local elements into\naccount. \n\n"}
{"id": "1603.09382", "contents": "Title: Deep Networks with Stochastic Depth Abstract: Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10). \n\n"}
{"id": "1604.00676", "contents": "Title: Multi-Bias Non-linear Activation in Deep Neural Networks Abstract: As a widely used non-linear activation, Rectified Linear Unit (ReLU)\nseparates noise and signal in a feature map by learning a threshold or bias.\nHowever, we argue that the classification of noise and signal not only depends\non the magnitude of responses, but also the context of how the feature\nresponses would be used to detect more abstract patterns in higher layers. In\norder to output multiple response maps with magnitude in different ranges for a\nparticular visual pattern, existing networks employing ReLU and its variants\nhave to learn a large number of redundant filters. In this paper, we propose a\nmulti-bias non-linear activation (MBA) layer to explore the information hidden\nin the magnitudes of responses. It is placed after the convolution layer to\ndecouple the responses to a convolution kernel into multiple maps by\nmulti-thresholding magnitudes, thus generating more patterns in the feature\nspace at a low computational cost. It provides great flexibility of selecting\nresponses to different visual patterns in different magnitude ranges to form\nrich representations in higher layers. Such a simple and yet effective scheme\nachieves the state-of-the-art performance on several benchmarks. \n\n"}
{"id": "1604.01587", "contents": "Title: Model for the Cherenkov light emission of TeO$_{2}$ cryogenic\n  calorimeters Abstract: The most sensitive process able to probe the Majorana nature of neutrinos and\ndiscover Lepton Number Violation is the neutrino-less double beta decay. Thanks\nto the excellent energy resolution, efficiency and intrinsic radio-purity,\ncryogenic calorimeters are primed for the search for this process. A novel\napproach able to improve the sensitivity of the current experiments is the\nrejection of $\\alpha$ interactions, that represents the dominant background\nsource. In TeO$_2$ calorimeters, $\\alpha$ particles can be tagged as, in\ncontrast to electrons, they do not emit Cherenkov light. Nevertheless, the very\nlow amount of detected Cherenkov light undermines the complete rejection of\n$\\alpha$ background. In this paper we compare the results obtained in previous\nmeasurements of the TeO$_2$ light yield with a detailed Monte Carlo simulation\nable to reproduce the number of Cherenkov photons produced in $\\beta/\\gamma$\ninteractions within the calorimeter and their propagation in the experimental\nset-up. We demonstrate that the light yield detectable from a\n$5\\times5\\times5$~cm$^{3}$ TeO$_2$ bolometer can be increased by up to 60% by\nincreasing the surface roughness of the crystal and improving the light\ndetector design. Moreover, we study the possibility to disentangle $\\alpha$,\n$\\beta$ and $\\gamma$ interactions, which represent the ultimate background\nsource. Unfortunately $\\gamma$ rejection is not feasible but $\\alpha$ rejection\ncan be achieved exploiting high sensitivity light detectors. \n\n"}
{"id": "1604.01685", "contents": "Title: The Cityscapes Dataset for Semantic Urban Scene Understanding Abstract: Visual understanding of complex urban street scenes is an enabling factor for\na wide range of applications. Object detection has benefited enormously from\nlarge-scale datasets, especially in the context of deep learning. For semantic\nurban scene understanding, however, no current dataset adequately captures the\ncomplexity of real-world urban scenes.\n  To address this, we introduce Cityscapes, a benchmark suite and large-scale\ndataset to train and test approaches for pixel-level and instance-level\nsemantic labeling. Cityscapes is comprised of a large, diverse set of stereo\nvideo sequences recorded in streets from 50 different cities. 5000 of these\nimages have high quality pixel-level annotations; 20000 additional images have\ncoarse annotations to enable methods that leverage large volumes of\nweakly-labeled data. Crucially, our effort exceeds previous attempts in terms\nof dataset size, annotation richness, scene variability, and complexity. Our\naccompanying empirical study provides an in-depth analysis of the dataset\ncharacteristics, as well as a performance evaluation of several\nstate-of-the-art approaches based on our benchmark. \n\n"}
{"id": "1604.03314", "contents": "Title: New application of superconductors: high sensitivity cryogenic light\n  detectors Abstract: In this paper we describe the current status of the CALDER project, which is\ndeveloping ultra-sensitive light detectors based on superconductors for\ncryogenic applications. When we apply an AC current to a superconductor, the\nCooper pairs oscillate and acquire kinetic inductance, that can be measured by\ninserting the superconductor in a LC circuit with high merit factor.\nInteractions in the superconductor can break the Cooper pairs, causing sizable\nvariations in the kinetic inductance and, thus, in the response of the LC\ncircuit. The continuous monitoring of the amplitude and frequency modulation\nallows to reconstruct the incident energy with excellent sensitivity. This\nconcept is at the basis of Kinetic Inductance Detectors (KIDs), that are\ncharacterized by natural aptitude to multiplexed read-out (several sensors can\nbe tuned to different resonant frequencies and coupled to the same line),\nresolution of few eV, stable behavior over a wide temperature range, and ease\nin fabrication. We present the results obtained by the CALDER collaboration\nwith 2x2 cm2 substrates sampled by 1 or 4 Aluminum KIDs. We show that the\nperformances of the first prototypes are already competitive with those of\nother commonly used light detectors, and we discuss the strategies for a\nfurther improvement. \n\n"}
{"id": "1604.04333", "contents": "Title: Latent Model Ensemble with Auto-localization Abstract: Deep Convolutional Neural Networks (CNN) have exhibited superior performance\nin many visual recognition tasks including image classification, object\ndetection, and scene label- ing, due to their large learning capacity and\nresistance to overfit. For the image classification task, most of the current\ndeep CNN- based approaches take the whole size-normalized image as input and\nhave achieved quite promising results. Compared with the previously dominating\napproaches based on feature extraction, pooling, and classification, the deep\nCNN-based approaches mainly rely on the learning capability of deep CNN to\nachieve superior results: the burden of minimizing intra-class variation while\nmaximizing inter-class difference is entirely dependent on the implicit feature\nlearning component of deep CNN; we rely upon the implicitly learned filters and\npooling component to select the discriminative regions, which correspond to the\nactivated neurons. However, if the irrelevant regions constitute a large\nportion of the image of interest, the classification performance of the deep\nCNN, which takes the whole image as input, can be heavily affected. To solve\nthis issue, we propose a novel latent CNN framework, which treats the most\ndiscriminate region as a latent variable. We can jointly learn the global CNN\nwith the latent CNN to avoid the aforementioned big irrelevant region issue,\nand our experimental results show the evident advantage of the proposed latent\nCNN over traditional deep CNN: latent CNN outperforms the state-of-the-art\nperformance of deep CNN on standard benchmark datasets including the CIFAR-10,\nCIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset. \n\n"}
{"id": "1604.05091", "contents": "Title: End-to-End Tracking and Semantic Segmentation Using Recurrent Neural\n  Networks Abstract: In this work we present a novel end-to-end framework for tracking and\nclassifying a robot's surroundings in complex, dynamic and only partially\nobservable real-world environments. The approach deploys a recurrent neural\nnetwork to filter an input stream of raw laser measurements in order to\ndirectly infer object locations, along with their identity in both visible and\noccluded areas. To achieve this we first train the network using unsupervised\nDeep Tracking, a recently proposed theoretical framework for end-to-end space\noccupancy prediction. We show that by learning to track on a large amount of\nunsupervised data, the network creates a rich internal representation of its\nenvironment which we in turn exploit through the principle of inductive\ntransfer of knowledge to perform the task of it's semantic classification. As a\nresult, we show that only a small amount of labelled data suffices to steer the\nnetwork towards mastering this additional task. Furthermore we propose a novel\nrecurrent neural network architecture specifically tailored to tracking and\nsemantic classification in real-world robotics applications. We demonstrate the\ntracking and classification performance of the method on real-world data\ncollected at a busy road junction. Our evaluation shows that the proposed\nend-to-end framework compares favourably to a state-of-the-art, model-free\ntracking solution and that it outperforms a conventional one-shot training\nscheme for semantic classification. \n\n"}
{"id": "1604.05096", "contents": "Title: Pixel-level Encoding and Depth Layering for Instance-level Semantic\n  Labeling Abstract: Recent approaches for instance-aware semantic labeling have augmented\nconvolutional neural networks (CNNs) with complex multi-task architectures or\ncomputationally expensive graphical models. We present a method that leverages\na fully convolutional network (FCN) to predict semantic labels, depth and an\ninstance-based encoding using each pixel's direction towards its corresponding\ninstance center. Subsequently, we apply low-level computer vision techniques to\ngenerate state-of-the-art instance segmentation on the street scene datasets\nKITTI and Cityscapes. Our approach outperforms existing works by a large margin\nand can additionally predict absolute distances of individual instances from a\nmonocular image as well as a pixel-level semantic labeling. \n\n"}
{"id": "1604.06169", "contents": "Title: A Database for Storing the Results of Material Radiopurity Measurements Abstract: Searches for rare nuclear processes, such as neutrinoless double beta-decay\nand the interactions of WIMP dark matter, are motivating experiments with\never-decreasing levels of radioactive backgrounds. These background reductions\nare achieved using various techniques, but amongst the most important is\nminimizing radioactive contamination in the materials from which the experiment\nis constructed. To this end there have been decades of advances in material\nsourcing, manufacture and certification, during which researchers have\naccumulated many thousands of measurements of material radiopurity. Some of\nthese assays are described in publications, others are in databases, but many\nare still communicated informally. Until this work, there has been no standard\nformat for encoding assay results and no effective, central location for\nstoring them. The aim of this work is to address these long-standing problems\nby creating a concise and flexible material assay data format and powerful\nsoftware application to manipulate it. A public installation of this software,\navailable at http://www.radiopurity.org, is the largest database of assay\nresults ever compiled and is intended as a long-term repository for the\ncommunity's data. \n\n"}
{"id": "1604.07457", "contents": "Title: Modeling the Contribution of Central Versus Peripheral Vision in Scene,\n  Object, and Face Recognition Abstract: It is commonly believed that the central visual field is important for\nrecognizing objects and faces, and the peripheral region is useful for scene\nrecognition. However, the relative importance of central versus peripheral\ninformation for object, scene, and face recognition is unclear. In a behavioral\nstudy, Larson and Loschky (2009) investigated this question by measuring the\nscene recognition accuracy as a function of visual angle, and demonstrated that\nperipheral vision was indeed more useful in recognizing scenes than central\nvision. In this work, we modeled and replicated the result of Larson and\nLoschky (2009), using deep convolutional neural networks. Having fit the data\nfor scenes, we used the model to predict future data for large-scale scene\nrecognition as well as for objects and faces. Our results suggest that the\nrelative order of importance of using central visual field information is face\nrecognition>object recognition>scene recognition, and vice-versa for peripheral\ninformation. \n\n"}
{"id": "1604.07944", "contents": "Title: DASC: Robust Dense Descriptor for Multi-modal and Multi-spectral\n  Correspondence Estimation Abstract: Establishing dense correspondences between multiple images is a fundamental\ntask in many applications. However, finding a reliable correspondence in\nmulti-modal or multi-spectral images still remains unsolved due to their\nchallenging photometric and geometric variations. In this paper, we propose a\nnovel dense descriptor, called dense adaptive self-correlation (DASC), to\nestimate multi-modal and multi-spectral dense correspondences. Based on an\nobservation that self-similarity existing within images is robust to imaging\nmodality variations, we define the descriptor with a series of an adaptive\nself-correlation similarity measure between patches sampled by a randomized\nreceptive field pooling, in which a sampling pattern is obtained using a\ndiscriminative learning. The computational redundancy of dense descriptors is\ndramatically reduced by applying fast edge-aware filtering. Furthermore, in\norder to address geometric variations including scale and rotation, we propose\na geometry-invariant DASC (GI-DASC) descriptor that effectively leverages the\nDASC through a superpixel-based representation. For a quantitative evaluation\nof the GI-DASC, we build a novel multi-modal benchmark as varying photometric\nand geometric conditions. Experimental results demonstrate the outstanding\nperformance of the DASC and GI-DASC in many cases of multi-modal and\nmulti-spectral dense correspondences. \n\n"}
{"id": "1604.08772", "contents": "Title: Towards Conceptual Compression Abstract: We introduce a simple recurrent variational auto-encoder architecture that\nsignificantly improves image modeling. The system represents the\nstate-of-the-art in latent variable models for both the ImageNet and Omniglot\ndatasets. We show that it naturally separates global conceptual information\nfrom lower level details, thus addressing one of the fundamentally desired\nproperties of unsupervised learning. Furthermore, the possibility of\nrestricting ourselves to storing only global information about an image allows\nus to achieve high quality 'conceptual compression'. \n\n"}
{"id": "1605.00055", "contents": "Title: DisturbLabel: Regularizing CNN on the Loss Layer Abstract: During a long period of time we are combating over-fitting in the CNN\ntraining process with model regularization, including weight decay, model\naveraging, data augmentation, etc. In this paper, we present DisturbLabel, an\nextremely simple algorithm which randomly replaces a part of labels as\nincorrect values in each iteration. Although it seems weird to intentionally\ngenerate incorrect training labels, we show that DisturbLabel prevents the\nnetwork training from over-fitting by implicitly averaging over exponentially\nmany networks which are trained with different label sets. To the best of our\nknowledge, DisturbLabel serves as the first work which adds noises on the loss\nlayer. Meanwhile, DisturbLabel cooperates well with Dropout to provide\ncomplementary regularization functions. Experiments demonstrate competitive\nrecognition results on several popular image recognition datasets. \n\n"}
{"id": "1605.00642", "contents": "Title: A new scheme for short baseline electron antineutrino disappearance\n  study Abstract: A new scheme for the short baseline electron antineutrino (${\\bar{\\nu}}_{e}$)\ndisappearance study is investigated. We propose to use an intense neutron\nemitter, $^{252}$Cf, which produces $^{8}$Li isotope through the\n$^{7}$Li(n,$\\gamma$)$^{8}$Li reaction; $^{8}$Li is a ${\\bar{\\nu}}_{e}$ emitter\nvia $\\beta^{-}$ decay. Because this ${\\bar{\\nu}}_{e}$ source needs neither\naccelerator nor reactor facilities, the ${\\bar{\\nu}}_{e}$ can be placed on any\nneutrino detectors as closely as possible. This short baseline circumstance\nwith a suitable detector enables us to study the existence of possible sterile\nneutrinos, in particular, on 1 eV mass scale. Also, complementary comparison\nstudies among different neutrino detectors can become feasible by using\n${\\bar{\\nu}}_{e}$ from the $^{8}$Li source. As an example, applications to\nhemisphere and cylinder shape scintillator detectors are performed in detail\nwith the expectation signal modification by the sterile neutrino. Sensitivities\nto mass and mixing angles of sterile neutrinos are also presented for\ncomparison with those of other neutrino experiments. \n\n"}
{"id": "1605.01101", "contents": "Title: WEPSAM: Weakly Pre-Learnt Saliency Model Abstract: Visual saliency detection tries to mimic human vision psychology which\nconcentrates on sparse, important areas in natural image. Saliency prediction\nresearch has been traditionally based on low level features such as contrast,\nedge, etc. Recent thrust in saliency prediction research is to learn high level\nsemantics using ground truth eye fixation datasets. In this paper we present,\nWEPSAM : Weakly Pre-Learnt Saliency Model as a pioneering effort of using\ndomain specific pre-learing on ImageNet for saliency prediction using a light\nweight CNN architecture. The paper proposes a two step hierarchical learning,\nin which the first step is to develop a framework for weakly pre-training on a\nlarge scale dataset such as ImageNet which is void of human eye fixation maps.\nThe second step refines the pre-trained model on a limited set of ground truth\nfixations. Analysis of loss on iSUN and SALICON datasets reveal that\npre-trained network converges much faster compared to randomly initialized\nnetwork. WEPSAM also outperforms some recent state-of-the-art saliency\nprediction models on the challenging MIT300 dataset. \n\n"}
{"id": "1605.03954", "contents": "Title: Applying Bayesian parameter estimation to relativistic heavy-ion\n  collisions: simultaneous characterization of the initial state and\n  quark-gluon plasma medium Abstract: We quantitatively estimate properties of the quark-gluon plasma created in\nultra-relativistic heavy-ion collisions utilizing Bayesian statistics and a\nmulti-parameter model-to-data comparison. The study is performed using a\nrecently developed parametric initial condition model, TRENTO, which\ninterpolates among a general class of particle production schemes, and a modern\nhybrid model which couples viscous hydrodynamics to a hadronic cascade. We\ncalibrate the model to multiplicity, transverse momentum, and flow data and\nreport constraints on the parametrized initial conditions and the\ntemperature-dependent transport coefficients of the quark-gluon plasma. We show\nthat initial entropy deposition is consistent with a saturation-based picture,\nextract a relation between the minimum value and slope of the\ntemperature-dependent specific shear viscosity, and find a clear signal for a\nnonzero bulk viscosity. \n\n"}
{"id": "1605.05212", "contents": "Title: Multimodal Sparse Coding for Event Detection Abstract: Unsupervised feature learning methods have proven effective for\nclassification tasks based on a single modality. We present multimodal sparse\ncoding for learning feature representations shared across multiple modalities.\nThe shared representations are applied to multimedia event detection (MED) and\nevaluated in comparison to unimodal counterparts, as well as other feature\nlearning methods such as GMM supervectors and sparse RBM. We report the\ncross-validated classification accuracy and mean average precision of the MED\nsystem trained on features learned from our unimodal and multimodal settings\nfor a subset of the TRECVID MED 2014 dataset. \n\n"}
{"id": "1605.05244", "contents": "Title: Discrimination of nuclear and electronic recoil events using plasma\n  effect in germanium detectors Abstract: We report a new method of using the plasma time difference, which results\nfrom the plasma effect, between the nuclear and electronic recoil events in\nhigh-purity germanium detectors to distinguish these two types of events in the\nsearch for rare physics processes. The physics mechanism of the plasma effect\nis discussed in detail. A numerical model is developed to calculate the plasma\ntime for nuclear and electronic recoils at various energies in germanium\ndetectors. It can be shown that under certain conditions the plasma time\ndifference is large enough to be observable. The experimental aspects in\nrealizing such a discrimination in germanium detectors is discussed. \n\n"}
{"id": "1605.06215", "contents": "Title: Efficient Feature-based Image Registration by Mapping Sparsified\n  Surfaces Abstract: With the advancement in the digital camera technology, the use of high\nresolution images and videos has been widespread in the modern society. In\nparticular, image and video frame registration is frequently applied in\ncomputer graphics and film production. However, conventional registration\napproaches usually require long computational time for high resolution images\nand video frames. This hinders the application of the registration approaches\nin the modern industries. In this work, we first propose a new image\nrepresentation method to accelerate the registration process by triangulating\nthe images effectively. For each high resolution image or video frame, we\ncompute an optimal coarse triangulation which captures the important features\nof the image. Then, we apply a surface registration algorithm to obtain a\nregistration map which is used to compute the registration of the high\nresolution image. Experimental results suggest that our overall algorithm is\nefficient and capable to achieve a high compression rate while the accuracy of\nthe registration is well retained when compared with the conventional\ngrid-based approach. Also, the computational time of the registration is\nsignificantly reduced using our triangulation-based approach. \n\n"}
{"id": "1605.06878", "contents": "Title: Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained\n  Image Recognition Abstract: Fine-grained image recognition is a challenging computer vision problem, due\nto the small inter-class variations caused by highly similar subordinate\ncategories, and the large intra-class variations in poses, scales and\nrotations. In this paper, we propose a novel end-to-end Mask-CNN model without\nthe fully connected layers for fine-grained recognition. Based on the part\nannotations of fine-grained images, the proposed model consists of a fully\nconvolutional network to both locate the discriminative parts (e.g., head and\ntorso), and more importantly generate object/part masks for selecting useful\nand meaningful convolutional descriptors. After that, a four-stream Mask-CNN\nmodel is built for aggregating the selected object- and part-level descriptors\nsimultaneously. The proposed Mask-CNN model has the smallest number of\nparameters, lowest feature dimensionality and highest recognition accuracy when\ncompared with state-of-the-arts fine-grained approaches. \n\n"}
{"id": "1605.09582", "contents": "Title: Model-driven Simulations for Deep Convolutional Neural Networks Abstract: The use of simulated virtual environments to train deep convolutional neural\nnetworks (CNN) is a currently active practice to reduce the\n(real)data-hungriness of the deep CNN models, especially in application domains\nin which large scale real data and/or groundtruth acquisition is difficult or\nlaborious. Recent approaches have attempted to harness the capabilities of\nexisting video games, animated movies to provide training data with high\nprecision groundtruth. However, a stumbling block is in how one can certify\ngeneralization of the learned models and their usefulness in real world data\nsets. This opens up fundamental questions such as: What is the role of\nphotorealism of graphics simulations in training CNN models? Are the trained\nmodels valid in reality? What are possible ways to reduce the performance bias?\nIn this work, we begin to address theses issues systematically in the context\nof urban semantic understanding with CNNs. Towards this end, we (a) propose a\nsimple probabilistic urban scene model, (b) develop a parametric rendering tool\nto synthesize the data with groundtruth, followed by (c) a systematic\nexploration of the impact of level-of-realism on the generality of the trained\nCNN model to real world; and domain adaptation concepts to minimize the\nperformance bias. \n\n"}
{"id": "1605.09673", "contents": "Title: Dynamic Filter Networks Abstract: In a traditional convolutional layer, the learned filters stay fixed after\ntraining. In contrast, we introduce a new framework, the Dynamic Filter\nNetwork, where filters are generated dynamically conditioned on an input. We\nshow that this architecture is a powerful one, with increased flexibility\nthanks to its adaptive nature, yet without an excessive increase in the number\nof model parameters. A wide variety of filtering operations can be learned this\nway, including local spatial transformations, but also others like selective\n(de)blurring or adaptive feature extraction. Moreover, multiple such layers can\nbe combined, e.g. in a recurrent architecture. We demonstrate the effectiveness\nof the dynamic filter network on the tasks of video and stereo prediction, and\nreach state-of-the-art performance on the moving MNIST dataset with a much\nsmaller model. By visualizing the learned filters, we illustrate that the\nnetwork has picked up flow information by only looking at unlabelled training\ndata. This suggests that the network can be used to pretrain networks for\nvarious supervised tasks in an unsupervised way, like optical flow and depth\nestimation. \n\n"}
{"id": "1606.02287", "contents": "Title: Rejection of randomly coinciding events in Li$_2$$^{100}$MoO$_4$\n  scintillating bolometers using light detectors based on the Neganov-Luke\n  effect Abstract: Random coincidences of nuclear events can be one of the main background\nsources in low-temperature calorimetric experiments looking for neutrinoless\ndouble-beta decay, especially in those searches based on scintillating\nbolometers embedding the promising double-beta candidate $^{100}$Mo, because of\nthe relatively short half-life of the two-neutrino double-beta decay of this\nnucleus. We show in this work that randomly coinciding events of the\ntwo-neutrino double decay of $^{100}$Mo in enriched Li$_2$$^{100}$MoO$_4$\ndetectors can be effectively discriminated by pulse-shape analysis in the light\nchannel if the scintillating bolometer is provided with a Neganov-Luke light\ndetector, which can improve the signal-to-noise ratio by a large factor,\nassumed here at the level of $\\sim 750$ on the basis of preliminary\nexperimental results obtained with these devices. The achieved pile-up\nrejection efficiency results in a very low contribution, of the order of $\\sim\n6\\times10^{-5}$ counts/(keV$\\cdot$kg$\\cdot$y), to the background counting rate\nin the region of interest for a large volume ($\\sim 90$ cm$^3$)\nLi$_2$$^{100}$MoO$_4$ detector. This background level is very encouraging in\nview of a possible use of the Li$_2$$^{100}$MoO$_4$ solution for a bolometric\ntonne-scale next-generation experiment as that proposed in the CUPID project. \n\n"}
{"id": "1606.02407", "contents": "Title: Structured Convolution Matrices for Energy-efficient Deep learning Abstract: We derive a relationship between network representation in energy-efficient\nneuromorphic architectures and block Toplitz convolutional matrices. Inspired\nby this connection, we develop deep convolutional networks using a family of\nstructured convolutional matrices and achieve state-of-the-art trade-off\nbetween energy efficiency and classification accuracy for well-known image\nrecognition tasks. We also put forward a novel method to train binary\nconvolutional networks by utilising an existing connection between\nnoisy-rectified linear units and binary activations. \n\n"}
{"id": "1606.03445", "contents": "Title: Broadband Neutron Interferometer Abstract: We demonstrate a two phase-grating, multi-beam neutron interferometer by\nusing a modified Ronchi setup in a far-field regime. The functionality of the\ninterferometer is based on the universal \\moire effect that was recently\nimplemented for X-ray phase-contrast imaging in the far-field regime.\nInterference fringes were achieved with monochromatic, bichromatic, and\npolychromatic neutron beams; for both continuous and pulsed beams. This\nfar-field neutron interferometry allows for the utilization of the full neutron\nflux for precise measurements of potential gradients, and expands neutron\nphase-contrast imaging techniques to more intense polycromatic neutron beams. \n\n"}
{"id": "1606.03968", "contents": "Title: Visual-Inertial-Semantic Scene Representation for 3-D Object Detection Abstract: We describe a system to detect objects in three-dimensional space using video\nand inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile\nplatforms from phones to drones. Inertials afford the ability to impose\nclass-specific scale priors for objects, and provide a global orientation\nreference. A minimal sufficient representation, the posterior of semantic\n(identity) and syntactic (pose) attributes of objects in space, can be\ndecomposed into a geometric term, which can be maintained by a\nlocalization-and-mapping filter, and a likelihood function, which can be\napproximated by a discriminatively-trained convolutional neural network. The\nresulting system can process the video stream causally in real time, and\nprovides a representation of objects in the scene that is persistent:\nConfidence in the presence of objects grows with evidence, and objects\npreviously seen are kept in memory even when temporarily occluded, with their\nreturn into view automatically predicted to prime re-detection. \n\n"}
{"id": "1606.04621", "contents": "Title: Watch What You Just Said: Image Captioning with Text-Conditional\n  Attention Abstract: Attention mechanisms have attracted considerable interest in image captioning\ndue to its powerful performance. However, existing methods use only visual\ncontent as attention and whether textual context can improve attention in image\ncaptioning remains unsolved. To explore this problem, we propose a novel\nattention mechanism, called \\textit{text-conditional attention}, which allows\nthe caption generator to focus on certain image features given previously\ngenerated text. To obtain text-related image features for our attention model,\nwe adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture\nwith CNN fine-tuning. Our proposed method allows joint learning of the image\nembedding, text embedding, text-conditional attention and language model with\none network architecture in an end-to-end manner. We perform extensive\nexperiments on the MS-COCO dataset. The experimental results show that our\nmethod outperforms state-of-the-art captioning methods on various quantitative\nmetrics as well as in human evaluation, which supports the use of our\ntext-conditional attention in image captioning. \n\n"}
{"id": "1606.04917", "contents": "Title: Performance studies of MRPC prototypes for CBM Abstract: Multi-gap Resistive Plate Chambers (MRPCs) with multi-strip readout are\nconsidered to be the optimal detector candidate for the Time-of-Flight (ToF)\nwall in the Compressed Baryonic Matter (CBM) experiment. In the R&D phase MRPCs\nwith different granularities, low-resistive materials and high voltage stack\nconfigurations were developed and tested. Here, we focus on two prototypes\ncalled HD-P2 and THU-strip, both with strips of 27 cm$^2$ length and\nlow-resistive glass electrodes. The HD-P2 prototype has a single-stack\nconfiguration with 8 gaps while the THU-strip prototype is constructed in a\ndouble-stack configuration with 2 $\\times$ 4 gaps. The performance results of\nthese counters in terms of efficiency and time resolution carried out in a test\nbeam time with heavy-ion beam at GSI in 2014 are presented in this proceeding. \n\n"}
{"id": "1606.05645", "contents": "Title: The GlueX DIRC Project Abstract: The GlueX experiment was designed to search for and study the pattern of\ngluonic excitations in the meson spectrum produced through photoproduction\nreactions at a new tagged photon beam facility in Hall D at Jefferson\nLaboratory. The particle identification capabilities of the GlueX experiment\nwill be enhanced by constructing a DIRC (Detection of Internally Reflected\nCherenkov light) detector, utilizing components of the decommissioned BaBar\nDIRC. The DIRC will allow systematic studies of kaon final states that are\nessential for inferring the quark flavor content of both hybrid and\nconventional mesons. The design for the GlueX DIRC is presented, including the\nnew expansion volumes that are currently under development. \n\n"}
{"id": "1606.07806", "contents": "Title: First test of an enriched $^{116}$CdWO$_4$ scintillating bolometer for\n  neutrinoless double-beta-decay searches Abstract: For the first time, a cadmium tungstate crystal scintillator enriched in\n$^{116}$Cd has been succesfully tested as a scintillating bolometer. The\nmeasurement was performed above ground at a temperature of 18 mK. The crystal\nmass was 34.5 g and the enrichment level ~82 %. Despite a substantial pile-up\neffect due to above-ground operation, the detector demonstrated a high energy\nresolution (2-7 keV FWHM in 0.2-2.6 MeV $\\gamma$ energy range), a powerful\nparticle identification capability and a high level of internal radiopurity.\nThese results prove that cadmium tungstate is an extremely promising detector\nmaterial for a next-generation neutrinoless double-beta decay bolometric\nexperiment, like that proposed in the CUPID project (CUORE Upgrade with\nParticle IDentification). \n\n"}
{"id": "1606.08572", "contents": "Title: Diversified Visual Attention Networks for Fine-Grained Object\n  Classification Abstract: Fine-grained object classification is a challenging task due to the subtle\ninter-class difference and large intra-class variation. Recently, visual\nattention models have been applied to automatically localize the discriminative\nregions of an image for better capturing critical difference and demonstrated\npromising performance. However, without consideration of the diversity in\nattention process, most of existing attention models perform poorly in\nclassifying fine-grained objects. In this paper, we propose a diversified\nvisual attention network (DVAN) to address the problems of fine-grained object\nclassification, which substan- tially relieves the dependency on\nstrongly-supervised information for learning to localize discriminative regions\ncompared with attentionless models. More importantly, DVAN explicitly pursues\nthe diversity of attention and is able to gather discriminative information to\nthe maximal extent. Multiple attention canvases are generated to extract\nconvolutional features for attention. An LSTM recurrent unit is employed to\nlearn the attentiveness and discrimination of attention canvases. The proposed\nDVAN has the ability to attend the object from coarse to fine granularity, and\na dynamic internal representation for classification is built up by\nincrementally combining the information from different locations and scales of\nthe image. Extensive experiments con- ducted on CUB-2011, Stanford Dogs and\nStanford Cars datasets have demonstrated that the proposed diversified visual\nattention networks achieve competitive performance compared to the state-\nof-the-art approaches, without using any prior knowledge, user interaction or\nexternal resource in training or testing. \n\n"}
{"id": "1607.02556", "contents": "Title: Action Recognition with Joint Attention on Multi-Level Deep Features Abstract: We propose a novel deep supervised neural network for the task of action\nrecognition in videos, which implicitly takes advantage of visual tracking and\nshares the robustness of both deep Convolutional Neural Network (CNN) and\nRecurrent Neural Network (RNN). In our method, a multi-branch model is proposed\nto suppress noise from background jitters. Specifically, we firstly extract\nmulti-level deep features from deep CNNs and feed them into 3d-convolutional\nnetwork. After that we feed those feature cubes into our novel joint LSTM\nmodule to predict labels and to generate attention regularization. We evaluate\nour model on two challenging datasets: UCF101 and HMDB51. The results show that\nour model achieves the state-of-art by only using convolutional features. \n\n"}
{"id": "1607.02656", "contents": "Title: Detection System for Neutron $\\beta$ Decay Correlations in the UCNB and\n  Nab experiments Abstract: We describe a detection system designed for precise measurements of angular\ncorrelations in neutron $\\beta$ decay. The system is based on thick, large\narea, highly segmented silicon detectors developed in collaboration with Micron\nSemiconductor, Ltd. The prototype system meets specifications for $\\beta$\nelectron detection with energy thresholds below 10 keV, energy resolution of\n$\\sim$3 keV FWHM, and rise time of $\\sim$50 ns with 19 of the 127 detector\npixels instrumented. Using ultracold neutrons at the Los Alamos Neutron Science\nCenter, we have demonstrated the coincident detection of $\\beta$ particles and\nrecoil protons from neutron $\\beta$ decay. The fully instrumented detection\nsystem will be implemented in the UCNB and Nab experiments, to determine the\nneutron $\\beta$ decay parameters $B$, $a$, and $b$. \n\n"}
{"id": "1607.03682", "contents": "Title: Hierarchical learning for DNN-based acoustic scene classification Abstract: In this paper, we present a deep neural network (DNN)-based acoustic scene\nclassification framework. Two hierarchical learning methods are proposed to\nimprove the DNN baseline performance by incorporating the hierarchical taxonomy\ninformation of environmental sounds. Firstly, the parameters of the DNN are\ninitialized by the proposed hierarchical pre-training. Multi-level objective\nfunction is then adopted to add more constraint on the cross-entropy based loss\nfunction. A series of experiments were conducted on the Task1 of the Detection\nand Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The\nfinal DNN-based system achieved a 22.9% relative improvement on average scene\nclassification error as compared with the Gaussian Mixture Model (GMM)-based\nbenchmark system across four standard folds. \n\n"}
{"id": "1607.04117", "contents": "Title: Improvement of radiopurity level of enriched $^{116}$CdWO$_4$ and\n  ZnWO$_4$ crystal scintillators by recrystallization Abstract: As low as possible radioactive contamination of a detector plays a crucial\nrole to improve sensitivity of a double beta decay experiment. The radioactive\ncontamination of a sample of $^{116}$CdWO$_4$ crystal scintillator by thorium\nwas reduced by a factor $\\approx 10$, down to the level 0.01 mBq/kg\n($^{228}$Th), by exploiting the recrystallization procedure. The total alpha\nactivity of uranium and thorium daughters was reduced by a factor $\\approx 3$,\ndown to 1.6 mBq/kg. No change in the specific activity (the total $\\alpha$\nactivity and $^{228}$Th) was observed in a sample of ZnWO$_4$ crystal produced\nby recrystallization after removing $\\approx 0.4$ mm surface layer of the\ncrystal. \n\n"}
{"id": "1607.04405", "contents": "Title: Dissecting nucleon transition electromagnetic form factors Abstract: In Poincar\\'e-covariant continuum treatments of the three valence-quark\nbound-state problem, the force behind dynamical chiral symmetry breaking also\ngenerates nonpointlike, interacting diquark correlations in the nucleon and its\nresonances. We detail the impact of these correlations on the\nelectromagnetically-induced nucleon-$\\Delta$ and nucleon-Roper transitions,\nproviding a flavour-separation of the latter and associated predictions that\ncan be tested at modern facilities. \n\n"}
{"id": "1607.04433", "contents": "Title: End-to-End Learning for Image Burst Deblurring Abstract: We present a neural network model approach for multi-frame blind\ndeconvolution. The discriminative approach adopts and combines two recent\ntechniques for image deblurring into a single neural network architecture. Our\nproposed hybrid-architecture combines the explicit prediction of a\ndeconvolution filter and non-trivial averaging of Fourier coefficients in the\nfrequency domain. In order to make full use of the information contained in all\nimages in one burst, the proposed network embeds smaller networks, which\nexplicitly allow the model to transfer information between images in early\nlayers. Our system is trained end-to-end using standard backpropagation on a\nset of artificially generated training examples, enabling competitive\nperformance in multi-frame blind deconvolution, both with respect to quality\nand runtime. \n\n"}
{"id": "1607.04730", "contents": "Title: Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction Abstract: Computational saliency models for still images have gained significant\npopularity in recent years. Saliency prediction from videos, on the other hand,\nhas received relatively little interest from the community. Motivated by this,\nin this work, we study the use of deep learning for dynamic saliency prediction\nand propose the so-called spatio-temporal saliency networks. The key to our\nmodels is the architecture of two-stream networks where we investigate\ndifferent fusion mechanisms to integrate spatial and temporal information. We\nevaluate our models on the DIEM and UCF-Sports datasets and present highly\ncompetitive results against the existing state-of-the-art models. We also carry\nout some experiments on a number of still images from the MIT300 dataset by\nexploiting the optical flow maps predicted from these images. Our results show\nthat considering inherent motion information in this way can be helpful for\nstatic saliency estimation. \n\n"}
{"id": "1607.05264", "contents": "Title: The Aerogel Cherenkov Detector for the SHMS magnetic spectrometer in\n  Hall C at Jefferson Lab Abstract: Hadronic reactions producing strange quarks such as exclusive or\nsemi-inclusive kaon production, play an important role in studies of hadron\nstructure and the dynamics that bind the most basic elements of nuclear\nphysics. The small-angle capability of the new Super High Momentum Spectrometer\n(SHMS) in Hall C, coupled with its high momentum reach - up to the anticipated\n11-GeV beam energy in Hall C - and coincidence capability with the\nwell-understood High Momentum Spectrometer, will allow for probes of such\nhadron structure involving strangeness down to the smallest distance scales to\ndate. To cleanly select the kaons, a threshold aerogel Cerenkov detector has\nbeen constructed for the SHMS. The detector consists of an aerogel tray\nfollowed by a diffusion box. Four trays for aerogel of nominal refractive\nindices of n=1.030, 1.020, 1.015 and 1.011 were constructed. The tray\ncombination will allow for identification of kaons from 1 GeV/c up to 7.2\nGeV/c, reaching 10^-2 proton and 10^-3 pion rejection, with kaon detection\nefficiency better than 95%. The diffusion box of the detector is equipped with\n14 five-inch diameter photomultiplier tubes. Its interior walls are covered\nwith Gore diffusive reflector, which is superior to the commonly used Millipore\npaper and improved the detector performance by 35%. The inner surface of the\ntwo aerogel trays with higher refractive index is covered with Millipore paper,\nhowever, those two trays with lower aerogel refractive index are again covered\nwith Gore diffusive reflector for higher performance. The measured mean number\nof photoelectrons in saturation is ~12 for n=1.030, ~sim8 for n=1.020, ~10 for\nn=1.015, and ~5.5 for n=1.011. The design details, the results of component\ncharacterization, and initial performance tests and optimization of the\ndetector are presented. \n\n"}
{"id": "1607.07215", "contents": "Title: DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation Abstract: In this work, we consider the task of generating highly-realistic images of a\ngiven face with a redirected gaze. We treat this problem as a specific instance\nof conditional image generation and suggest a new deep architecture that can\nhandle this task very well as revealed by numerical comparison with prior art\nand a user study. Our deep architecture performs coarse-to-fine warping with an\nadditional intensity correction of individual pixels. All these operations are\nperformed in a feed-forward manner, and the parameters associated with\ndifferent operations are learned jointly in the end-to-end fashion. After\nlearning, the resulting neural network can synthesize images with manipulated\ngaze, while the redirection angle can be selected arbitrarily from a certain\nrange and provided as an input to the network. \n\n"}
{"id": "1607.07220", "contents": "Title: Local- and Holistic- Structure Preserving Image Super Resolution via\n  Deep Joint Component Learning Abstract: Recently, machine learning based single image super resolution (SR)\napproaches focus on jointly learning representations for high-resolution (HR)\nand low-resolution (LR) image patch pairs to improve the quality of the\nsuper-resolved images. However, due to treat all image pixels equally without\nconsidering the salient structures, these approaches usually fail to produce\nvisual pleasant images with sharp edges and fine details. To address this\nissue, in this work we present a new novel SR approach, which replaces the main\nbuilding blocks of the classical interpolation pipeline by a flexible,\ncontent-adaptive deep neural networks. In particular, two well-designed\nstructure-aware components, respectively capturing local- and holistic- image\ncontents, are naturally incorporated into the fully-convolutional\nrepresentation learning to enhance the image sharpness and naturalness.\nExtensively evaluations on several standard benchmarks (e.g., Set5, Set14 and\nBSD200) demonstrate that our approach can achieve superior results, especially\non the image with salient structures, over many existing state-of-the-art SR\nmethods under both quantitative and qualitative measures. \n\n"}
{"id": "1607.07988", "contents": "Title: ATGV-Net: Accurate Depth Super-Resolution Abstract: In this work we present a novel approach for single depth map\nsuper-resolution. Modern consumer depth sensors, especially Time-of-Flight\nsensors, produce dense depth measurements, but are affected by noise and have a\nlow lateral resolution. We propose a method that combines the benefits of\nrecent advances in machine learning based single image super-resolution, i.e.\ndeep convolutional networks, with a variational method to recover accurate\nhigh-resolution depth maps. In particular, we integrate a variational method\nthat models the piecewise affine structures apparent in depth data via an\nanisotropic total generalized variation regularization term on top of a deep\nnetwork. We call our method ATGV-Net and train it end-to-end by unrolling the\noptimization procedure of the variational method. To train deep networks, a\nlarge corpus of training data with accurate ground-truth is required. We\ndemonstrate that it is feasible to train our method solely on synthetic data\nthat we generate in large quantities for this task. Our evaluations show that\nwe achieve state-of-the-art results on three different benchmarks, as well as\non a challenging Time-of-Flight dataset, all without utilizing an additional\nintensity image as guidance. \n\n"}
{"id": "1607.08020", "contents": "Title: Unstable nuclei in dissociation of light stable and radioactive nuclei\n  in nuclear track emulsion Abstract: A role of the unstable nuclei ${}^{6}$Be, ${}^{8}$Be and ${}^{9}$B in the\ndissociation of relativistic nuclei ${}^{7,9}$Be, ${}^{10}$B and ${}^{10,11}$C\nis under study on the basis of nuclear track emulsion exposed to secondary\nbeams of the JINR Nuclotron. Contribution of the configuration ${}^{6}$Be +\n$\\mit{n}$ to the ${}^{7}$Be nucleus structure is 8 $\\pm$ 1% which is near the\nvalue for the configuration ${}^{6}$Li + $\\mit{p}$. Distributions over the\nopening angle of $\\alpha$-particle pairs indicate to a simultaneous presence of\nvirtual ${}^{8}$Be$_{g.s.}$ and ${}^{8}$Be$_{2^+}$ states in the ground states\nof the ${}^{9}$Be and ${}^{10}$C nuclei. The core ${}^{9}$B is manifested in\nthe {${}^{10}$C} nucleus with a probability of 30 $\\pm$ 4%. Selection of the\n${}^{10}$C \"white\" stars accompanied by ${}^{8}$Be$_{g.s.}$ (${}^{9}$B) leads\nto appearance in the excitation energy distribution of 2$\\alpha$2$\\mit{p}$\n\"quartets\" of the distinct peak with a maximum at 4.1 $\\pm$ 0.3 MeV.\n${}^{8}$Be$_{g.s.}$ decays are presented in 24 $\\pm$ 7% of 2He + 2H events of\nthe ${}^{11}$C coherent dissociation and 27 $\\pm$ 11% of the 3He ones. The\nchannel ${}^{9}$B + H amounts 14 $\\pm$ 3%. The ${}^{8}$Be$_{g.s.}$ nucleus is\nmanifested in the coherent dissociation ${}^{10}$B $\\to$ 2He + H with a\nprobability of 25 $\\pm$ 5% including 14 $\\pm$ 3% of ${}^{9}$B decays. A\nprobability ratio of the mirror channels ${}^{9}$B + $\\mit{n}$ and ${}^{9}$Be +\n$\\mit{p}$ is estimated to be 6 $\\pm$ 1. \n\n"}
{"id": "1608.00367", "contents": "Title: Accelerating the Super-Resolution Convolutional Neural Network Abstract: As a successful deep model applied in image super-resolution (SR), the\nSuper-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior\nperformance to the previous hand-crafted models either in speed and restoration\nquality. However, the high computational cost still hinders it from practical\nusage that demands real-time performance (24 fps). In this paper, we aim at\naccelerating the current SRCNN, and propose a compact hourglass-shape CNN\nstructure for faster and better SR. We re-design the SRCNN structure mainly in\nthree aspects. First, we introduce a deconvolution layer at the end of the\nnetwork, then the mapping is learned directly from the original low-resolution\nimage (without interpolation) to the high-resolution one. Second, we\nreformulate the mapping layer by shrinking the input feature dimension before\nmapping and expanding back afterwards. Third, we adopt smaller filter sizes but\nmore mapping layers. The proposed model achieves a speed up of more than 40\ntimes with even superior restoration quality. Further, we present the parameter\nsettings that can achieve real-time performance on a generic CPU while still\nmaintaining good performance. A corresponding transfer strategy is also\nproposed for fast training and testing across different upscaling factors. \n\n"}
{"id": "1608.01126", "contents": "Title: High Precision Momentum Calibration of the Magnetic Spectrometers at\n  MAMI for Hypernuclear Binding Energy Determination Abstract: We propose a new method for absolute momentum calibration of magnetic\nspectrometers used in nuclear physics, using the time-of-flight (TOF),\ndifferences of pairs of particles with different masses. In cases where the\nflight path is not known, a calibration can be determined by using the TOF\ndifferences of two pair combinations of three particles. A Cherenkov detector,\nread out by a radio frequency photomultiplier tube, is considered as the\nhigh-resolution and highly stable TOF detector. By means of Monte Carlo\nsimulations it is demonstrated that the magnetic spectrometers at the MAMI\nelectron-scattering facility can be calibrated absolutely with an accuracy\n$\\delta p/p\\leq 10^{-4}$, which will be crucial for high precision\ndetermination of hypernuclear masses. \n\n"}
{"id": "1608.01137", "contents": "Title: Cascaded Continuous Regression for Real-time Incremental Face Tracking Abstract: This paper introduces a novel real-time algorithm for facial landmark\ntracking. Compared to detection, tracking has both additional challenges and\nopportunities. Arguably the most important aspect in this domain is updating a\ntracker's models as tracking progresses, also known as incremental (face)\ntracking. While this should result in more accurate localisation, how to do\nthis online and in real time without causing a tracker to drift is still an\nimportant open research question. We address this question in the cascaded\nregression framework, the state-of-the-art approach for facial landmark\nlocalisation. Because incremental learning for cascaded regression is costly,\nwe propose a much more efficient yet equally accurate alternative using\ncontinuous regression. More specifically, we first propose cascaded continuous\nregression (CCR) and show its accuracy is equivalent to the Supervised Descent\nMethod. We then derive the incremental learning updates for CCR (iCCR) and show\nthat it is an order of magnitude faster than standard incremental learning for\ncascaded regression, bringing the time required for the update from seconds\ndown to a fraction of a second, thus enabling real-time tracking. Finally, we\nevaluate iCCR and show the importance of incremental learning in achieving\nstate-of-the-art performance. Code for our iCCR is available from\nhttp://www.cs.nott.ac.uk/~psxes1 \n\n"}
{"id": "1608.01745", "contents": "Title: Play and Learn: Using Video Games to Train Computer Vision Models Abstract: Video games are a compelling source of annotated data as they can readily\nprovide fine-grained groundtruth for diverse tasks. However, it is not clear\nwhether the synthetically generated data has enough resemblance to the\nreal-world images to improve the performance of computer vision models in\npractice. We present experiments assessing the effectiveness on real-world data\nof systems trained on synthetic RGB images that are extracted from a video\ngame. We collected over 60000 synthetic samples from a modern video game with\nsimilar conditions to the real-world CamVid and Cityscapes datasets. We provide\nseveral experiments to demonstrate that the synthetically generated RGB images\ncan be used to improve the performance of deep neural networks on both image\nsegmentation and depth estimation. These results show that a convolutional\nnetwork trained on synthetic data achieves a similar test error to a network\nthat is trained on real-world data for dense image classification. Furthermore,\nthe synthetically generated RGB images can provide similar or better results\ncompared to the real-world datasets if a simple domain adaptation technique is\napplied. Our results suggest that collaboration with game developers for an\naccessible interface to gather data is potentially a fruitful direction for\nfuture work in computer vision. \n\n"}
{"id": "1608.02912", "contents": "Title: Characterization of a scintillating lithium glass ultra-cold neutron\n  detector Abstract: A $^{6}$Li glass based scintillation detector developed for the TRIUMF\nneutron electric dipole moment experiment was characterized using the\nultra-cold neutron source at the Paul Scherrer Institute (PSI). The data\nacquisition system for this detector was demonstrated to perform well at\nrejecting backgrounds. An estimate of the absolute efficiency of background\nrejection of $99.7\\pm0.1$% is made. For variable ultra-cold neutron rate\n(varying from $<$1 kHz to approx. 100 kHz per channel) and background rate seen\nat the Paul Scherrer Institute, we estimate that the absolute detector\nefficiency is $89.7^{+1.3}_{-1.9}$%. Finally a comparison with a commercial\nCascade detector was performed for a specific setup at the West-2 beamline of\nthe ultra-cold neutron source at PSI. \n\n"}
{"id": "1608.03179", "contents": "Title: High-rate axial-field ionization chamber for particle identification of\n  radioactive beams Abstract: The design, construction and performance characteristics of a simple\naxial-field ionization chamber suitable for identifying ions in a radioactive\nbeam are presented. Optimized for use with low-energy radioactive beams (< 5\nMeV/A) the detector presents only three 0.5 $\\mu$m/cm$^2$ foils to the beam in\naddition to the detector gas. A fast charge sensitive amplifier (CSA)\nintegrated into the detector design is also described. Coupling this fast CSA\nto the axial field ionization chamber produces an output pulse with a risetime\nof 60-70 ns and a fall time of 100 ns, making the detector capable of\nsustaining a relatively high rate. Tests with an $\\alpha$ source establish the\ndetector energy resolution as $\\sim$8 $\\%$ for an energy deposit of $\\sim$3.5\nMeV. The energy resolution with beams of 2.5 and 4.0 MeV/A $^{39}$K ions and\nthe dependence of the energy resolution on beam intensity is measured. At an\ninstantaneous rate of 3 x 10$^5$ ions/s the energy resolution has degraded to\n14% with a pileup of 12%. The good energy resolution of this detector at rates\nup to 3 x 10$^5$ ions/s makes it an effective tool in the characterization of\nlow-energy radioactive beams. \n\n"}
{"id": "1608.03474", "contents": "Title: Learning Dynamic Hierarchical Models for Anytime Scene Labeling Abstract: With increasing demand for efficient image and video analysis, test-time cost\nof scene parsing becomes critical for many large-scale or time-sensitive vision\napplications. We propose a dynamic hierarchical model for anytime scene\nlabeling that allows us to achieve flexible trade-offs between efficiency and\naccuracy in pixel-level prediction. In particular, our approach incorporates\nthe cost of feature computation and model inference, and optimizes the model\nperformance for any given test-time budget by learning a sequence of\nimage-adaptive hierarchical models. We formulate this anytime representation\nlearning as a Markov Decision Process with a discrete-continuous state-action\nspace. A high-quality policy of feature and model selection is learned based on\nan approximate policy iteration method with action proposal mechanism. We\ndemonstrate the advantages of our dynamic non-myopic anytime scene parsing on\nthree semantic segmentation datasets, which achieves $90\\%$ of the\nstate-of-the-art performances by using $15\\%$ of their overall costs. \n\n"}
{"id": "1608.05159", "contents": "Title: Multi-stage Object Detection with Group Recursive Learning Abstract: Most of existing detection pipelines treat object proposals independently and\npredict bounding box locations and classification scores over them separately.\nHowever, the important semantic and spatial layout correlations among proposals\nare often ignored, which are actually useful for more accurate object\ndetection. In this work, we propose a new EM-like group recursive learning\napproach to iteratively refine object proposals by incorporating such context\nof surrounding proposals and provide an optimal spatial configuration of object\ndetections. In addition, we propose to incorporate the weakly-supervised object\nsegmentation cues and region-based object detection into a multi-stage\narchitecture in order to fully exploit the learned segmentation features for\nbetter object detection in an end-to-end way. The proposed architecture\nconsists of three cascaded networks which respectively learn to perform\nweakly-supervised object segmentation, object proposal generation and recursive\ndetection refinement. Combining the group recursive learning and the\nmulti-stage architecture provides competitive mAPs of 78.6% and 74.9% on the\nPASCAL VOC2007 and VOC2012 datasets respectively, which outperforms many\nwell-established baselines [10] [20] significantly. \n\n"}
{"id": "1608.05889", "contents": "Title: Online Feature Selection with Group Structure Analysis Abstract: Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods. \n\n"}
{"id": "1608.06019", "contents": "Title: Domain Separation Networks Abstract: The cost of large scale data collection and annotation often makes the\napplication of machine learning algorithms to new tasks or datasets\nprohibitively expensive. One approach circumventing this cost is training\nmodels on synthetic data where annotations are provided automatically. Despite\ntheir appeal, such models often fail to generalize from synthetic to real\nimages, necessitating domain adaptation algorithms to manipulate these models\nbefore they can be successfully applied. Existing approaches focus either on\nmapping representations from one domain to the other, or on learning to extract\nfeatures that are invariant to the domain from which they were extracted.\nHowever, by focusing only on creating a mapping or shared representation\nbetween the two domains, they ignore the individual characteristics of each\ndomain. We suggest that explicitly modeling what is unique to each domain can\nimprove a model's ability to extract domain-invariant features. Inspired by\nwork on private-shared component analysis, we explicitly learn to extract image\nrepresentations that are partitioned into two subspaces: one component which is\nprivate to each domain and one which is shared across domains. Our model is\ntrained not only to perform the task we care about in the source domain, but\nalso to use the partitioned representation to reconstruct the images from both\ndomains. Our novel architecture results in a model that outperforms the\nstate-of-the-art on a range of unsupervised domain adaptation scenarios and\nadditionally produces visualizations of the private and shared representations\nenabling interpretation of the domain adaptation process. \n\n"}
{"id": "1608.07340", "contents": "Title: The Majorana Demonstrator search for neutrinoless double beta decay Abstract: The MAJORANA Collaboration is constructing the MAJORANA DEMONSTRATOR, an\nultra-low background, modular, HPGe detector array with a mass of 44.8-kg (29.7\nkg enriched >88% in Ge-76) to search for neutrinoless double beta decay in\nGe-76. The next generation of tonnescale Ge-based neutrinoless double beta\ndecay searches will probe the neutrino mass scale in the inverted-hierarchy\nregion. The MAJORANA DEMONSTRATOR is envisioned to demonstrate a path forward\nto achieve a background rate at or below 1 count/tonne/year in the 4 keV region\nof interest around the Q-value of 2039 keV. The MAJORANA DEMONSTRATOR follows a\nmodular implementation to be easily scalable to the next generation experiment.\nFirst data taken with the DEMONSTRATOR are introduced here. \n\n"}
{"id": "1608.08434", "contents": "Title: Multi-Class Multi-Object Tracking using Changing Point Detection Abstract: This paper presents a robust multi-class multi-object tracking (MCMOT)\nformulated by a Bayesian filtering framework. Multi-object tracking for\nunlimited object classes is conducted by combining detection responses and\nchanging point detection (CPD) algorithm. The CPD model is used to observe\nabrupt or abnormal changes due to a drift and an occlusion based spatiotemporal\ncharacteristics of track states. The ensemble of convolutional neural network\n(CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion\ndetector is employed to compute the likelihoods of foreground regions as the\ndetection responses of different object classes. Extensive experiments are\nperformed using lately introduced challenging benchmark videos; ImageNet VID\nand MOT benchmark dataset. The comparison to state-of-the-art video tracking\ntechniques shows very encouraging results. \n\n"}
{"id": "1609.00361", "contents": "Title: Autonomous driving challenge: To Infer the property of a dynamic object\n  based on its motion pattern using recurrent neural network Abstract: In autonomous driving applications a critical challenge is to identify action\nto take to avoid an obstacle on collision course. For example, when a heavy\nobject is suddenly encountered it is critical to stop the vehicle or change the\nlane even if it causes other traffic disruptions. However,there are situations\nwhen it is preferable to collide with the object rather than take an action\nthat would result in a much more serious accident than collision with the\nobject. For example, a heavy object which falls from a truck should be avoided\nwhereas a bouncing ball or a soft target such as a foam box need not be.We\npresent a novel method to discriminate between the motion characteristics of\nthese types of objects based on their physical properties such as bounciness,\nelasticity, etc.In this preliminary work, we use recurrent neural net-work with\nLSTM cells to train a classifier to classify objects based on their motion\ntrajectories. We test the algorithm on synthetic data, and, as a proof of\nconcept, demonstrate its effectiveness on a limited set of real-world data. \n\n"}
{"id": "1609.01859", "contents": "Title: Automatic Visual Theme Discovery from Joint Image and Text Corpora Abstract: A popular approach to semantic image understanding is to manually tag images\nwith keywords and then learn a mapping from vi- sual features to keywords.\nManually tagging images is a subjective pro- cess and the same or very similar\nvisual contents are often tagged with different keywords. Furthermore, not all\ntags have the same descriptive power for visual contents and large vocabulary\navailable from natural language could result in a very diverse set of keywords.\nIn this paper, we propose an unsupervised visual theme discovery framework as a\nbetter (more compact, efficient and effective) alternative to semantic\nrepresen- tation of visual contents. We first show that tag based annotation\nlacks consistency and compactness for describing visually similar contents. We\nthen learn the visual similarity between tags based on the visual features of\nthe images containing the tags. At the same time, we use a natural language\nprocessing technique (word embedding) to measure the seman- tic similarity\nbetween tags. Finally, we cluster tags into visual themes based on their visual\nsimilarity and semantic similarity measures using a spectral clustering\nalgorithm. We conduct user studies to evaluate the effectiveness and\nrationality of the visual themes discovered by our unsu- pervised algorithm and\nobtains promising result. We then design three common computer vision tasks,\nexample based image search, keyword based image search and image labelling to\nexplore potential applica- tion of our visual themes discovery framework. In\nexperiments, visual themes significantly outperforms tags on semantic image\nunderstand- ing and achieve state-of-art performance in all three tasks. This\nagain demonstrate the effectiveness and versatility of proposed framework. \n\n"}
{"id": "1609.02077", "contents": "Title: Visual Saliency Detection Based on Multiscale Deep CNN Features Abstract: Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this paper, we discover that a\nhigh-quality visual saliency model can be learned from multiscale features\nextracted using deep convolutional neural networks (CNNs), which have had many\nsuccesses in visual recognition tasks. For learning such saliency models, we\nintroduce a neural network architecture, which has fully connected layers on\ntop of CNNs responsible for feature extraction at three different scales. The\npenultimate layer of our neural network has been confirmed to be a\ndiscriminative high-level feature vector for saliency detection, which we call\ndeep contrast feature. To generate a more robust feature, we integrate\nhandcrafted low-level features with our deep contrast feature. To promote\nfurther research and evaluation of visual saliency models, we also construct a\nnew large database of 4447 challenging images and their pixelwise saliency\nannotations. Experimental results demonstrate that our proposed method is\ncapable of achieving state-of-the-art performance on all public benchmarks,\nimproving the F- measure by 6.12% and 10.0% respectively on the DUT-OMRON\ndataset and our new dataset (HKU-IS), and lowering the mean absolute error by\n9% and 35.3% respectively on these two datasets. \n\n"}
{"id": "1609.02974", "contents": "Title: Learning-Based View Synthesis for Light Field Cameras Abstract: With the introduction of consumer light field cameras, light field imaging\nhas recently become widespread. However, there is an inherent trade-off between\nthe angular and spatial resolution, and thus, these cameras often sparsely\nsample in either spatial or angular domain. In this paper, we use machine\nlearning to mitigate this trade-off. Specifically, we propose a novel\nlearning-based approach to synthesize new views from a sparse set of input\nviews. We build upon existing view synthesis techniques and break down the\nprocess into disparity and color estimation components. We use two sequential\nconvolutional neural networks to model these two components and train both\nnetworks simultaneously by minimizing the error between the synthesized and\nground truth images. We show the performance of our approach using only four\ncorner sub-aperture views from the light fields captured by the Lytro Illum\ncamera. Experimental results show that our approach synthesizes high-quality\nimages that are superior to the state-of-the-art techniques on a variety of\nchallenging real-world scenes. We believe our method could potentially decrease\nthe required angular resolution of consumer light field cameras, which allows\ntheir spatial resolution to increase. \n\n"}
{"id": "1609.03783", "contents": "Title: Commissioning of the COBRA demonstrator and investigation of surface\n  events as its main background Abstract: The COBRA collaboration investigates 0{\\nu}\\beta\\beta-decays (neutrinoless\ndouble beta-decays). Therefore, a demonstrator setup using coplanar-grid CdZnTe\ndetectors is operated at the LNGS underground laboratory. In this work, the\ndemonstrator was commissioned and completed, which is discussed extensively.\nThe demonstrator works reliably and collects low-background physics data. One\nresult of the analysis of the data is that surface events are the dominating\nbackground component. To better understand and possibly discriminate this\nbackground, surface events were studied in detail. This was done mainly using\nlaboratory measurements. For a better interpretation of these measurements,\nsimulations of particle trajectories and ranges were done. The surface\nsensitivity tests showed large differences between the individual detectors.\nOften, a dead-layer was determined, especially at the surfaces where the\nnon-collecting anode (NCA) is the outermost anode rail. Due to this, the\nsensitivity of the surfaces where the collecting anode (CA) is adjacent was\ntypically about a factor of three larger than the NCA sensitivity. A comparison\nof the pulse shape analysis methods LSE and A/E was done. Laboratory\nmeasurements indicate, that the latter performs better. Alpha scanning\nmeasurements were done to spatially investigate the surface sensitivity.\nPlausible variations were measured. However, no hints were found how to improve\nthe surface event recognition. The instrumentation of the guard ring, which\nsurrounds the anode structure, was tested and improved the surface event\ndiscrimination significantly. The fraction of surviving alpha events was at a\nper-mill level. Important steps for a future large-scale COBRA experiment are\ndiscussed briefly, mainly the use of an integrated read-out system. Overall,\nthe results indicate a large potential in background reduction for the COBRA\nexperiment. \n\n"}
{"id": "1609.07982", "contents": "Title: Optimistic and Pessimistic Neural Networks for Scene and Object\n  Recognition Abstract: In this paper the application of uncertainty modeling to convolutional neural\nnetworks is evaluated. A novel method for adjusting the network's predictions\nbased on uncertainty information is introduced. This allows the network to be\neither optimistic or pessimistic in its prediction scores. The proposed method\nbuilds on the idea of applying dropout at test time and sampling a predictive\nmean and variance from the network's output. Besides the methodological\naspects, implementation details allowing for a fast evaluation are presented.\nFurthermore, a multilabel network architecture is introduced that strongly\nbenefits from the presented approach. In the evaluation it will be shown that\nmodeling uncertainty allows for improving the performance of a given model\npurely at test time without any further training steps. The evaluation\nconsiders several applications in the field of computer vision, including\nobject classification and detection as well as scene attribute recognition. \n\n"}
{"id": "1609.09850", "contents": "Title: Latent fingerprint minutia extraction using fully convolutional network Abstract: Minutiae play a major role in fingerprint identification. Extracting reliable\nminutiae is difficult for latent fingerprints which are usually of poor\nquality. As the limitation of traditional handcrafted features, a fully\nconvolutional network (FCN) is utilized to learn features directly from data to\novercome complex background noises. Raw fingerprints are mapped to a\ncorrespondingly-sized minutia-score map with a fixed stride. And thus a large\nnumber of minutiae will be extracted through a given threshold. Then small\nregions centering at these minutia points are entered into a convolutional\nneural network (CNN) to reclassify these minutiae and calculate their\norientations. The CNN shares convolutional layers with the fully convolutional\nnetwork to speed up. 0.45 second is used on average to detect one fingerprint\non a GPU. On the NIST SD27 database, we achieve 53\\% recall rate and 53\\%\nprecise rate that outperform many other algorithms. Our trained model is also\nvisualized to show that we have successfully extracted features preserving\nridge information of a latent fingerprint. \n\n"}
{"id": "1610.01076", "contents": "Title: Tutorial on Answering Questions about Images with Deep Learning Abstract: Together with the development of more accurate methods in Computer Vision and\nNatural Language Understanding, holistic architectures that answer on questions\nabout the content of real-world images have emerged. In this tutorial, we build\na neural-based approach to answer questions about images. We base our tutorial\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\nmodels that we present here can achieve a competitive performance on both\ndatasets, in fact, they are among the best methods that use a combination of\nLSTM with a global, full frame CNN representation of an image. We hope that\nafter reading this tutorial, the reader will be able to use Deep Learning\nframeworks, such as Keras and introduced Kraino, to build various architectures\nthat will lead to a further performance improvement on this challenging task. \n\n"}
{"id": "1610.02744", "contents": "Title: Identified particle production in pp collisions at $\\sqrt{s}=$ 7 and 13\n  TeV measured with ALICE Abstract: Proton-proton (pp) collisions have been used extensively as a reference for\nthe study of interactions of larger colliding systems at the LHC. Recent\nmeasurements performed in high-multiplicity pp and proton-lead (p-Pb)\ncollisions have shown features that are reminiscent of those observed in\nlead-lead (Pb-Pb) collisions. In this context, the study of identified particle\nspectra and yields as a function of multiplicity is a key tool for the\nunderstanding of similarities and differences between small and large systems.\nWe report on the production of pions, kaons, protons, $K^{0}_{\\rm S}$,\n$\\Lambda$, $\\Xi$, $\\Omega$ and $K^{*0}$ as a function of multiplicity in pp\ncollisions at $\\sqrt{s}=$ 7 TeV measured with the ALICE experiment. The work\npresented here represents the most comprehensive set of results on identified\nparticle production in pp collisions at the LHC. Spectral shapes, studied both\nfor individual particles and via particle ratios as a function of $p_{\\rm T}$,\nexhibit an evolution with charged particle multiplicity that is similar to the\none observed in larger systems. In addition, results on the production of light\nflavour hadrons in pp collisions at $\\sqrt{s}=$ 13 TeV, the highest\ncentre-of-mass energy ever reached in the laboratory, are also presented and\ncompared with previous, lower energy results. \n\n"}
{"id": "1610.02924", "contents": "Title: The role of meson exchange currents in charged current\n  (anti)neutrino-nucleus scattering Abstract: We present our recent progress in the description of neutrino-nucleus\ninteraction in the GeV region, of interest for ongoing and future oscillation\nexperiments. In particular, we discuss the weak excitation of\ntwo-particle-two-hole states induced by meson exchange currents in a fully\nrelativistic framework. We compare the results of our model with recent\nmeasurements of neutrino scattering cross sections, showing the crucial role\nplayed by two-nucleon knockout in the interpretation of the data. \n\n"}
{"id": "1610.03054", "contents": "Title: Delayed charge recovery discrimination of passivated surface alpha\n  events in P-type point-contact detectors Abstract: The Majorana Demonstrator searches for neutrinoless double-beta decay of\n$^{76}$Ge using arrays of high-purity germanium detectors. If observed, this\nprocess would demonstrate that lepton number is not a conserved quantity in\nnature, with implications for grand-unification and for explaining the\npredominance of matter over antimatter in the universe. A problematic\nbackground in such large granular detector arrays is posed by alpha particles.\nIn the Majorana Demonstrator, events have been observed that are consistent\nwith energy- degraded alphas originating on the passivated surface, leading to\na potential background contribution in the region-of-interest for neutrinoless\ndouble-beta decay. However, it is also observed that when energy deposition\noccurs very close to the passivated surface, charges drift through the bulk\nonto that surface, and then drift along it with greatly reduced mobility. This\nleads to both a reduced prompt signal and a measurable change in slope of the\ntail of a recorded pulse. In this contribution we discuss the characteristics\nof these events and the development of a filter that can identify the\noccurrence of this delayed charge recovery, allowing for the efficient\nrejection of passivated surface alpha events in analysis. \n\n"}
{"id": "1610.03055", "contents": "Title: Determination of the event collision time with the ALICE detector at the\n  LHC Abstract: Particle identification is an important feature of the ALICE detector at the\nLHC. In particular, for particle identification via the time-of-flight\ntechnique, the precise determination of the event collision time represents an\nimportant ingredient of the quality of the measurement. In this paper, the\ndifferent methods used for such a measurement in ALICE by means of the T0 and\nthe TOF detectors are reviewed. Efficiencies, resolution and the improvement of\nthe particle identification separation power of the methods used are presented\nfor the different LHC colliding systems (pp , p-Pb and Pb-Pb) during the first\nperiod of data taking of LHC (Run 1). \n\n"}
{"id": "1610.03513", "contents": "Title: Enriched TeO$_2$ bolometers with active particle discrimination: towards\n  the CUPID experiment Abstract: We present the performances of two 92% enriched $^{130}$TeO$_2$ crystals\noperated as thermal bolometers in view of a next generation experiment to\nsearch for neutrinoless double beta decay of $^{130}$Te. The crystals, 435 g\neach, show an energy resolution, evaluated at the 2615 keV $\\gamma$-line of\n$^{208}$Tl, of 6.5 and 4.3 keV FWHM. The only observable internal radioactive\ncontamination arises from $^{238}$U (15 and 8 $\\mu$Bq/kg, respectively). The\ninternal activity of the most problematic nuclei for neutrinoless double beta\ndecay, $^{226}$Ra and $^{228}$Th, are both evaluated as $<$3.1 $\\mu$Bq/kg for\none crystal and $<$2.3 $\\mu$Bq/kg for the second. Thanks to the readout of the\nweak Cherenkov light emitted by $\\beta/\\gamma$ particles by means of\nNeganov-Luke bolometric light detectors we were able to perform an\nevent-by-event identification of $\\beta/\\gamma$ events with a 95% acceptance\nlevel, while establishing a rejection factor of 98.21% and 99.99% for $\\alpha$\nparticles. \n\n"}
{"id": "1610.03777", "contents": "Title: Deep disentangled representations for volumetric reconstruction Abstract: We introduce a convolutional neural network for inferring a compact\ndisentangled graphical description of objects from 2D images that can be used\nfor volumetric reconstruction. The network comprises an encoder and a\ntwin-tailed decoder. The encoder generates a disentangled graphics code. The\nfirst decoder generates a volume, and the second decoder reconstructs the input\nimage using a novel training regime that allows the graphics code to learn a\nseparate representation of the 3D object and a description of its lighting and\npose conditions. We demonstrate this method by generating volumes and\ndisentangled graphical descriptions from images and videos of faces and chairs. \n\n"}
{"id": "1610.05111", "contents": "Title: Measurement of Fast Neutron Rate for NEOS Experiment Abstract: The fast neutron rate is measured at the site of NEOS experiment, a short\nbaseline neutrino experiment located in a tendon gallery of a commercial\nnuclear power plant, using a 0.78-liter liquid scintillator detector. A pulse\nshape discrimination technique is used to identify neutron signals. The\nmeasurements are performed during the nuclear reactor-on and off periods and\nfound to be ~20 per day for both periods. The fast neutron rate is also\nmeasured at an overground site with a negligible overburden and is found to be\n~100 times higher than that at the NEOS experiment site. \n\n"}
{"id": "1610.05854", "contents": "Title: Mixed context networks for semantic segmentation Abstract: Semantic segmentation is challenging as it requires both object-level\ninformation and pixel-level accuracy. Recently, FCN-based systems gained great\nimprovement in this area. Unlike classification networks, combining features of\ndifferent layers plays an important role in these dense prediction models, as\nthese features contains information of different levels. A number of models\nhave been proposed to show how to use these features. However, what is the best\narchitecture to make use of features of different layers is still a question.\nIn this paper, we propose a module, called mixed context network, and show that\nour presented system outperforms most existing semantic segmentation systems by\nmaking use of this module. \n\n"}
{"id": "1610.06083", "contents": "Title: A novel comparison of M{\\o}ller and Compton electron-beam polarimeters Abstract: We have performed a novel comparison between electron-beam polarimeters based\non M{\\o}ller and Compton scattering. A sequence of electron-beam polarization\nmeasurements were performed at low beam currents ($<$ 5 $\\mu$A) during the\n$Q_{\\rm weak}$ experiment in Hall C at Jefferson Lab. These low current\nmeasurements were bracketed by the regular high current (180 $\\mu$A) operation\nof the Compton polarimeter. All measurements were found to be consistent within\nexperimental uncertainties of 1% or less, demonstrating that electron\npolarization does not depend significantly on the beam current. This result\nlends confidence to the common practice of applying M{\\o}ller measurements made\nat low beam currents to physics experiments performed at higher beam currents.\nThe agreement between two polarimetry techniques based on independent physical\nprocesses sets an important benchmark for future precision asymmetry\nmeasurements that require sub-1% precision in polarimetry. \n\n"}
{"id": "1610.06682", "contents": "Title: KATANA - a charge-sensitive triggering system for the S$\\pi$RIT\n  experiment Abstract: KATANA - the Krakow Array for Triggering with Amplitude discrimiNAtion - has\nbeen built and used as a trigger and veto detector for the S$\\pi$RIT TPC at\nRIKEN. Its construction allows operating in magnetic field and providing fast\nresponse for ionizing particles, giving the approximate forward multiplicity\nand charge information. Depending on this information, trigger and veto signals\nare generated. The article presents performance of the detector and details of\nits construction. A simple phenomenological parametrization of the number of\nemitted scintillation photons in plastic scintillator is proposed. The effect\nof the light output deterioration in the plastic scintillator due to the\nin-beam irradiation is discussed. \n\n"}
{"id": "1610.07804", "contents": "Title: mdBrief - A Fast Online Adaptable, Distorted Binary Descriptor for\n  Real-Time Applications Using Calibrated Wide-Angle Or Fisheye Cameras Abstract: Fast binary descriptors build the core for many vision based applications\nwith real-time demands like object detection, Visual Odometry or SLAM. Commonly\nit is assumed, that the acquired images and thus the patches extracted around\nkeypoints originate from a perspective projection ignoring image distortion or\ncompletely different types of projections such as omnidirectional or fisheye.\nUsually the deviations from a perfect perspective projection are corrected by\nundistortion. Latter, however, introduces severe artifacts if the cameras\nfield-of-view gets larger. In this paper, we propose a distorted and masked\nversion of the BRIEF descriptor for calibrated cameras. Instead of correcting\nthe distortion holistically, we distort the binary tests and thus adapt the\ndescriptor to different image regions. \n\n"}
{"id": "1610.08053", "contents": "Title: CAKE: The Coincidence Array for K600 Experiments Abstract: The combination of a magnetic spectrometer and ancillary detectors such as\nsilicon detectors is a powerful tool for the study of nuclear reactions and\nnuclear structure. This paper discusses the recently commissioned silicon array\ncalled the CAKE which is designed for use with the K600 magnetic spectrometer\nat iThemba LABS. \n\n"}
{"id": "1611.00137", "contents": "Title: Embedding Deep Metric for Person Re-identication A Study Against Large\n  Variations Abstract: Person re-identification is challenging due to the large variations of pose,\nillumination, occlusion and camera view. Owing to these variations, the\npedestrian data is distributed as highly-curved manifolds in the feature space,\ndespite the current convolutional neural networks (CNN)'s capability of feature\nextraction. However, the distribution is unknown, so it is difficult to use the\ngeodesic distance when comparing two samples. In practice, the current deep\nembedding methods use the Euclidean distance for the training and test. On the\nother hand, the manifold learning methods suggest to use the Euclidean distance\nin the local range, combining with the graphical relationship between samples,\nfor approximating the geodesic distance. From this point of view, selecting\nsuitable positive i.e. intra-class) training samples within a local range is\ncritical for training the CNN embedding, especially when the data has large\nintra-class variations. In this paper, we propose a novel moderate positive\nsample mining method to train robust CNN for person re-identification, dealing\nwith the problem of large variation. In addition, we improve the learning by a\nmetric weight constraint, so that the learned metric has a better\ngeneralization ability. Experiments show that these two strategies are\neffective in learning robust deep metrics for person re-identification, and\naccordingly our deep model significantly outperforms the state-of-the-art\nmethods on several benchmarks of person re-identification. Therefore, the study\npresented in this paper may be useful in inspiring new designs of deep models\nfor person re-identification. \n\n"}
{"id": "1611.01731", "contents": "Title: Deep Label Distribution Learning with Label Ambiguity Abstract: Convolutional Neural Networks (ConvNets) have achieved excellent recognition\nperformance in various visual recognition tasks. A large labeled training set\nis one of the most important factors for its success. However, it is difficult\nto collect sufficient training images with precise labels in some domains such\nas apparent age estimation, head pose estimation, multi-label classification\nand semantic segmentation. Fortunately, there is ambiguous information among\nlabels, which makes these tasks different from traditional classification.\nBased on this observation, we convert the label of each image into a discrete\nlabel distribution, and learn the label distribution by minimizing a\nKullback-Leibler divergence between the predicted and ground-truth label\ndistributions using deep ConvNets. The proposed DLDL (Deep Label Distribution\nLearning) method effectively utilizes the label ambiguity in both feature\nlearning and classifier learning, which help prevent the network from\nover-fitting even when the training set is small. Experimental results show\nthat the proposed approach produces significantly better results than\nstate-of-the-art methods for age estimation and head pose estimation. At the\nsame time, it also improves recognition performance for multi-label\nclassification and semantic segmentation tasks. \n\n"}
{"id": "1611.02557", "contents": "Title: Charmonium physics with heavy ions: experimental results Abstract: Thirty years ago, the suppression of charmonium production in heavy-ion\ncollisions was first proposed as an unambiguous signature for the formation of\na Quark-Gluon Plasma. Since then, experiments at fixed-target accelerators\n(SPS) and hadronic colliders (RHIC, LHC) have investigated this observable and\ndiscovered a wide range of effects, that have been related to the original\nproposal but at the same time have also prompted a strong development in the\nunderlying theory concepts. In this contribution, I will review the main\nachievements of this field, with emphasis on recent results obtained by LHC\nexperiments. \n\n"}
{"id": "1611.02644", "contents": "Title: Multispectral Deep Neural Networks for Pedestrian Detection Abstract: Multispectral pedestrian detection is essential for around-the-clock\napplications, e.g., surveillance and autonomous driving. We deeply analyze\nFaster R-CNN for multispectral pedestrian detection task and then model it into\na convolutional network (ConvNet) fusion problem. Further, we discover that\nConvNet-based pedestrian detectors trained by color or thermal images\nseparately provide complementary information in discriminating human instances.\nThus there is a large potential to improve pedestrian detection by using color\nand thermal images in DNNs simultaneously. We carefully design four ConvNet\nfusion architectures that integrate two-branch ConvNets on different DNNs\nstages, all of which yield better performance compared with the baseline\ndetector. Our experimental results on KAIST pedestrian benchmark show that the\nHalfway Fusion model that performs fusion on the middle-level convolutional\nfeatures outperforms the baseline method by 11% and yields a missing rate 3.5%\nlower than the other proposed architectures. \n\n"}
{"id": "1611.02767", "contents": "Title: A backward pass through a CNN using a generative model of its\n  activations Abstract: Neural networks have shown to be a practical way of building a very complex\nmapping between a pre-specified input space and output space. For example, a\nconvolutional neural network (CNN) mapping an image into one of a thousand\nobject labels is approaching human performance in this particular task. However\nthe mapping (neural network) does not automatically lend itself to other forms\nof queries, for example, to detect/reconstruct object instances, to enforce\ntop-down signal on ambiguous inputs, or to recover object instances from\nocclusion. One way to address these queries is a backward pass through the\nnetwork that fuses top-down and bottom-up information. In this paper, we show a\nway of building such a backward pass by defining a generative model of the\nneural network's activations. Approximate inference of the model would\nnaturally take the form of a backward pass through the CNN layers, and it\naddresses the aforementioned queries in a unified framework. \n\n"}
{"id": "1611.03003", "contents": "Title: sPHENIX: The next generation heavy ion detector at RHIC Abstract: sPHENIX is a new collaboration and future detector project at Brookhaven\nNational Laboratory's Relativistic Heavy Ion Collider (RHIC). It seeks to\nanswer fundamental questions on the nature of the quark gluon plasma (QGP),\nincluding its temperature dependence and coupling strength, by using a suite of\nprecision jet and upsilon measurements that probe different length scales of\nthe QGP. This will be achieved with large acceptance, $|\\eta| < 1$ and\n$0$-$2\\pi$ in $\\phi$, electromagentic and hadronic calorimeters and precision\ntracking enabled by a $1.5$ T superconducting magnet. With the increased\nluminosity afforded by accelerator upgrades, sPHENIX will perform high\nstatistics measurements extending the kinematic reach at RHIC to overlap the\nLHC's. This overlap with the LHC will facilitate better understanding of the\nrole of temperature, density and parton virtuality in QGP dynamics and for jet\nquenching in particular. This talk will focus on key future measurements and\nthe current state of the sPHENIX project. \n\n"}
{"id": "1611.03673", "contents": "Title: Learning to Navigate in Complex Environments Abstract: Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities. \n\n"}
{"id": "1611.04205", "contents": "Title: Neutron-antineutron oscillations beyond the quasi-free limit Abstract: Prompted by plans to conduct a new neutron oscillation experiment at the\nEuropean Spallation Source (ESS), we consider issues associated with the\nmagnetic field that must be present, some of which are potentially exacerbated\nby the significantly larger length $l$ contemplated for the neutron propagation\nregion. To this end, we introduce a stochastic model of the residual magnetic\nfield within the propagation region which draws on features of magnetic\nprofiles measured during the last free neutron oscillation experiment\n[conducted at the Institut Laue-Langevin (ILL) in the 1990's]. We average over\nboth fluctuations in the magnetic field sampled by neutrons, and representative\nspectra of neutron speeds. We find that deviations from the quasi-free result\nfor the antineutron probability do not depend quadratically on $l$ (as a naive\nperturbative estimate would suggest) but increase only linearly with $l$. As\nregards the large spikes in the magnetic field which can be expected at, for\nexample, joints in the magnetic shielding of the propagation region (despite\ncompensating currents and magnetic idealization of the shield), we demonstrate\nthat their effect scales as $l/D^{3/2}$, where $D$ is the diameter of the\ncylindrical magnetic shielding. Our arguments suggest that, provided the\ndimensions of the propagation region are such that the ratio $l/D^{3/2}$ does\nnot exceed the value pertinent to the ILL experiment, and these spikes occur\nclose to either end of the propagation region, they can be neglected. We also\nestablish that any large magnetic field encountered after the propagation\nregion is exited will not diminish the probability for antineutron detection.\nFor the range of values of $l$ of most interest to the ESS experiment, it\nshould suffice to improve on the level of magnetic suppression achieved at the\nILL by a factor of two. \n\n"}
{"id": "1611.05037", "contents": "Title: Spin flip loss in magnetic confinement of ultracold neutrons for neutron\n  lifetime experiments Abstract: We analyze the spin flip loss for ultracold neutrons in magnetic bottles of\nthe type used in experiments aiming at a precise measurement of the neutron\nlifetime, extending the one-dimensional field model used previously by Steyerl\n$\\textit{et al.}$ [Phys.Rev.C $\\mathbf{86}$, 065501 (2012)] to two dimensions\nfor cylindrical multipole fields. We also develop a general analysis applicable\nto three dimensions. Here we apply it to multipole fields and to the bowl-type\nfield configuration used for the Los Alamos UCN$\\tau$ experiment. In all cases\nconsidered the spin flip loss calculated exceeds the Majorana estimate by many\norders of magnitude but can be suppressed sufficiently by applying a holding\nfield of appropriate magnitude to allow high-precision neutron lifetime\nmeasurements, provided other possible sources of systematic error are under\ncontrol. \n\n"}
{"id": "1611.05148", "contents": "Title: Variational Deep Embedding: An Unsupervised and Generative Approach to\n  Clustering Abstract: Clustering is among the most fundamental tasks in computer vision and machine\nlearning. In this paper, we propose Variational Deep Embedding (VaDE), a novel\nunsupervised generative clustering approach within the framework of Variational\nAuto-Encoder (VAE). Specifically, VaDE models the data generative procedure\nwith a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM\npicks a cluster; 2) from which a latent embedding is generated; 3) then the DNN\ndecodes the latent embedding into observables. Inference in VaDE is done in a\nvariational way: a different DNN is used to encode observables to latent\nembeddings, so that the evidence lower bound (ELBO) can be optimized using\nStochastic Gradient Variational Bayes (SGVB) estimator and the\nreparameterization trick. Quantitative comparisons with strong baselines are\nincluded in this paper, and experimental results show that VaDE significantly\noutperforms the state-of-the-art clustering methods on 4 benchmarks from\nvarious modalities. Moreover, by VaDE's generative nature, we show its\ncapability of generating highly realistic samples for any specified cluster,\nwithout using supervised information during training. Lastly, VaDE is a\nflexible and extensible framework for unsupervised generative clustering, more\ngeneral mixture models than GMM can be easily plugged in. \n\n"}
{"id": "1611.05328", "contents": "Title: Image Credibility Analysis with Effective Domain Transferred Deep\n  Networks Abstract: Numerous fake images spread on social media today and can severely jeopardize\nthe credibility of online content to public. In this paper, we employ deep\nnetworks to learn distinct fake image related features. In contrast to\nauthentic images, fake images tend to be eye-catching and visually striking.\nCompared with traditional visual recognition tasks, it is extremely challenging\nto understand these psychologically triggered visual patterns in fake images.\nTraditional general image classification datasets, such as ImageNet set, are\ndesigned for feature learning at the object level but are not suitable for\nlearning the hyper-features that would be required by image credibility\nanalysis. In order to overcome the scarcity of training samples of fake images,\nwe first construct a large-scale auxiliary dataset indirectly related to this\ntask. This auxiliary dataset contains 0.6 million weakly-labeled fake and real\nimages collected automatically from social media. Through an AdaBoost-like\ntransfer learning algorithm, we train a CNN model with a few instances in the\ntarget training set and 0.6 million images in the collected auxiliary set. This\nlearning algorithm is able to leverage knowledge from the auxiliary set and\ngradually transfer it to the target task. Experiments on a real-world testing\nset show that our proposed domain transferred CNN model outperforms several\ncompeting baselines. It obtains superiror results over transfer learning\nmethods based on the general ImageNet set. Moreover, case studies show that our\nproposed method reveals some interesting patterns for distinguishing fake and\nauthentic images. \n\n"}
{"id": "1611.05503", "contents": "Title: On the Exploration of Convolutional Fusion Networks for Visual\n  Recognition Abstract: Despite recent advances in multi-scale deep representations, their\nlimitations are attributed to expensive parameters and weak fusion modules.\nHence, we propose an efficient approach to fuse multi-scale deep\nrepresentations, called convolutional fusion networks (CFN). Owing to using\n1$\\times$1 convolution and global average pooling, CFN can efficiently generate\nthe side branches while adding few parameters. In addition, we present a\nlocally-connected fusion module, which can learn adaptive weights for the side\nbranches and form a discriminatively fused feature. CFN models trained on the\nCIFAR and ImageNet datasets demonstrate remarkable improvements over the plain\nCNNs. Furthermore, we generalize CFN to three new tasks, including scene\nrecognition, fine-grained recognition and image retrieval. Our experiments show\nthat it can obtain consistent improvements towards the transferring tasks. \n\n"}
{"id": "1611.05507", "contents": "Title: Deep Feature Interpolation for Image Content Changes Abstract: We propose Deep Feature Interpolation (DFI), a new data-driven baseline for\nautomatic high-resolution image transformation. As the name suggests, it relies\nonly on simple linear interpolation of deep convolutional features from\npre-trained convnets. We show that despite its simplicity, DFI can perform\nhigh-level semantic transformations like \"make older/younger\", \"make\nbespectacled\", \"add smile\", among others, surprisingly well - sometimes even\nmatching or outperforming the state-of-the-art. This is particularly unexpected\nas DFI requires no specialized network architecture or even any deep network to\nbe trained for these tasks. DFI therefore can be used as a new baseline to\nevaluate more complex algorithms and provides a practical answer to the\nquestion of which image transformation tasks are still challenging in the rise\nof deep learning. \n\n"}
{"id": "1611.05552", "contents": "Title: DelugeNets: Deep Networks with Efficient and Flexible Cross-layer\n  Information Inflows Abstract: Deluge Networks (DelugeNets) are deep neural networks which efficiently\nfacilitate massive cross-layer information inflows from preceding layers to\nsucceeding layers. The connections between layers in DelugeNets are established\nthrough cross-layer depthwise convolutional layers with learnable filters,\nacting as a flexible yet efficient selection mechanism. DelugeNets can\npropagate information across many layers with greater flexibility and utilize\nnetwork parameters more effectively compared to ResNets, whilst being more\nefficient than DenseNets. Remarkably, a DelugeNet model with just model\ncomplexity of 4.31 GigaFLOPs and 20.2M network parameters, achieve\nclassification errors of 3.76% and 19.02% on CIFAR-10 and CIFAR-100 dataset\nrespectively. Moreover, DelugeNet-122 performs competitively to ResNet-200 on\nImageNet dataset, despite costing merely half of the computations needed by the\nlatter. \n\n"}
{"id": "1611.05770", "contents": "Title: Squeezed back-to-back correlations of $\\phi\\phi$ in Au+Au and d+Au\n  collisions at the Relativistic Heavy Ion Collider energies Abstract: We investigate the squeezed back-to-back correlations (BBC) of $\\phi\\phi$,\ncaused by the mass modification of the particles in the source medium, in the\nheavy-ion collisions of Au+Au and d+Au at the Relativistic Heavy Ion Collider\n(RHIC) energies. The BBC functions are calculated using the modified masses\nextracted from experimental data and the source space-time distributions\nprovided by the viscous hydrodynamic code VISH2+1. Our investigations indicate\nthat the BBC of $\\phi\\phi$ may perhaps be observed in the collisions of d+Au\nand the peripheral collisions of Au+Au at the RHIC. We suggest to measure the\nBBC experimentally for understanding the mass modifications of the $\\phi$ meson\nin the collisions. \n\n"}
{"id": "1611.05947", "contents": "Title: Minimal Problems for the Calibrated Trifocal Variety Abstract: We determine the algebraic degree of minimal problems for the calibrated\ntrifocal variety in computer vision. We rely on numerical algebraic geometry\nand the homotopy continuation software Bertini. \n\n"}
{"id": "1611.05971", "contents": "Title: Finding Mirror Symmetry via Registration Abstract: Symmetry is prevalent in nature and a common theme in man-made designs. Both\nthe human visual system and computer vision algorithms can use symmetry to\nfacilitate object recognition and other tasks. Detecting mirror symmetry in\nimages and data is, therefore, useful for a number of applications. Here, we\ndemonstrate that the problem of fitting a plane of mirror symmetry to data in\nany Euclidian space can be reduced to the problem of registering two datasets.\nThe exactness of the resulting solution depends entirely on the registration\naccuracy. This new Mirror Symmetry via Registration (MSR) framework involves\n(1) data reflection with respect to an arbitrary plane, (2) registration of\noriginal and reflected datasets, and (3) calculation of the eigenvector of\neigenvalue -1 for the transformation matrix representing the reflection and\nregistration mappings. To support MSR, we also introduce a novel 2D\nregistration method based on random sample consensus of an ensemble of\nnormalized cross-correlation matches. With this as its registration back-end,\nMSR achieves state-of-the-art performance for symmetry line detection in two\nindependent 2D testing databases. We further demonstrate the generality of MSR\nby testing it on a database of 3D shapes with an iterative closest point\nregistration back-end. Finally, we explore its applicability to examining\nsymmetry in natural systems by assessing the degree of symmetry present in\nmyelinated axon reconstructions from a larval zebrafish. \n\n"}
{"id": "1611.06211", "contents": "Title: NoiseOut: A Simple Way to Prune Neural Networks Abstract: Neural networks are usually over-parameterized with significant redundancy in\nthe number of required neurons which results in unnecessary computation and\nmemory usage at inference time. One common approach to address this issue is to\nprune these big networks by removing extra neurons and parameters while\nmaintaining the accuracy. In this paper, we propose NoiseOut, a fully automated\npruning algorithm based on the correlation between activations of neurons in\nthe hidden layers. We prove that adding additional output neurons with entirely\nrandom targets results into a higher correlation between neurons which makes\npruning by NoiseOut even more efficient. Finally, we test our method on various\nnetworks and datasets. These experiments exhibit high pruning rates while\nmaintaining the accuracy of the original network. \n\n"}
{"id": "1611.06430", "contents": "Title: Semi-Supervised Learning with Context-Conditional Generative Adversarial\n  Networks Abstract: We introduce a simple semi-supervised learning approach for images based on\nin-painting using an adversarial loss. Images with random patches removed are\npresented to a generator whose task is to fill in the hole, based on the\nsurrounding pixels. The in-painted images are then presented to a discriminator\nnetwork that judges if they are real (unaltered training images) or not. This\ntask acts as a regularizer for standard supervised training of the\ndiscriminator. Using our approach we are able to directly train large VGG-style\nnetworks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL\ndatasets, where our approach obtains performance comparable or superior to\nexisting methods. \n\n"}
{"id": "1611.06656", "contents": "Title: ResFeats: Residual Network Based Features for Image Classification Abstract: Deep residual networks have recently emerged as the state-of-the-art\narchitecture in image segmentation and object detection. In this paper, we\npropose new image features (called ResFeats) extracted from the last\nconvolutional layer of deep residual networks pre-trained on ImageNet. We\npropose to use ResFeats for diverse image classification tasks namely, object\nclassification, scene classification and coral classification and show that\nResFeats consistently perform better than their CNN counterparts on these\nclassification tasks. Since the ResFeats are large feature vectors, we propose\nto use PCA for dimensionality reduction. Experimental results are provided to\nshow the effectiveness of ResFeats with state-of-the-art classification\naccuracies on Caltech-101, Caltech-256 and MLC datasets and a significant\nperformance improvement on MIT-67 dataset compared to the widely used CNN\nfeatures. \n\n"}
{"id": "1611.06765", "contents": "Title: Computational Nuclear Physics and Post Hartree-Fock Methods Abstract: We present a computational approach to infinite nuclear matter employing\nHartree-Fock theory, many-body perturbation theory and coupled cluster theory.\nThese lectures are closely linked with those of chapters 9, 10 and 11 and serve\nas input for the correlation functions employed in Monte Carlo calculations in\nchapter 9, the in-medium similarity renormalization group theory of dense\nfermionic systems of chapter 10 and the Green's function approach in chapter\n11. We provide extensive code examples and benchmark calculations, allowing\nthereby an eventual reader to start writing her/his own codes. We start with an\nobject-oriented serial code and end with discussions on strategies for porting\nthe code to present and planned high-performance computing facilities. \n\n"}
{"id": "1611.06969", "contents": "Title: Kernel Cross-View Collaborative Representation based Classification for\n  Person Re-Identification Abstract: Person re-identification aims at the maintenance of a global identity as a\nperson moves among non-overlapping surveillance cameras. It is a hard task due\nto different illumination conditions, viewpoints and the small number of\nannotated individuals from each pair of cameras (small-sample-size problem).\nCollaborative Representation based Classification (CRC) has been employed\nsuccessfully to address the small-sample-size problem in computer vision.\nHowever, the original CRC formulation is not well-suited for person\nre-identification since it does not consider that probe and gallery samples are\nfrom different cameras. Furthermore, it is a linear model, while appearance\nchanges caused by different camera conditions indicate a strong nonlinear\ntransition between cameras. To overcome such limitations, we propose the Kernel\nCross-View Collaborative Representation based Classification (Kernel X-CRC)\nthat represents probe and gallery images by balancing representativeness and\nsimilarity nonlinearly. It assumes that a probe and its corresponding gallery\nimage are represented with similar coding vectors using individuals from the\ntraining set. Experimental results demonstrate that our assumption is true when\nusing a high-dimensional feature vector and becomes more compelling when\ndealing with a low-dimensional and discriminative representation computed using\na common subspace learning method. We achieve state-of-the-art for rank-1\nmatching rates in two person re-identification datasets (PRID450S and GRID) and\nthe second best results on VIPeR and CUHK01 datasets. \n\n"}
{"id": "1611.07145", "contents": "Title: Learning Multi-level Deep Representations for Image Emotion\n  Classification Abstract: In this paper, we propose a new deep network that learns multi-level deep\nrepresentations for image emotion classification (MldrNet). Image emotion can\nbe recognized through image semantics, image aesthetics and low-level visual\nfeatures from both global and local views. Existing image emotion\nclassification works using hand-crafted features or deep features mainly focus\non either low-level visual features or semantic-level image representations\nwithout taking all factors into consideration. The proposed MldrNet combines\ndeep representations of different levels, i.e. image semantics, image\naesthetics, and low-level visual features to effectively classify the emotion\ntypes of different kinds of images, such as abstract paintings and web images.\nExtensive experiments on both Internet images and abstract paintings\ndemonstrate the proposed method outperforms the state-of-the-art methods using\ndeep features or hand-crafted features. The proposed approach also outperforms\nthe state-of-the-art methods with at least 6% performance improvement in terms\nof overall classification accuracy. \n\n"}
{"id": "1611.08061", "contents": "Title: Recalling Holistic Information for Semantic Segmentation Abstract: Semantic segmentation requires a detailed labeling of image pixels by object\ncategory. Information derived from local image patches is necessary to describe\nthe detailed shape of individual objects. However, this information is\nambiguous and can result in noisy labels. Global inference of image content can\ninstead capture the general semantic concepts present. We advocate that\nhigh-recall holistic inference of image concepts provides valuable information\nfor detailed pixel labeling. We build a two-stream neural network architecture\nthat facilitates information flow from holistic information to local pixels,\nwhile keeping common image features shared among the low-level layers of both\nthe holistic analysis and segmentation branches. We empirically evaluate our\nnetwork on four standard semantic segmentation datasets. Our network obtains\nstate-of-the-art performance on PASCAL-Context and NYUDv2, and ablation studies\nverify its effectiveness on ADE20K and SIFT-Flow. \n\n"}
{"id": "1611.08796", "contents": "Title: Deep Deformable Registration: Enhancing Accuracy by Fully Convolutional\n  Neural Net Abstract: Deformable registration is ubiquitous in medical image analysis. Many\ndeformable registration methods minimize sum of squared difference (SSD) as the\nregistration cost with respect to deformable model parameters. In this work, we\nconstruct a tight upper bound of the SSD registration cost by using a fully\nconvolutional neural network (FCNN) in the registration pipeline. The upper\nbound SSD (UB-SSD) enhances the original deformable model parameter space by\nadding a heatmap output from FCNN. Next, we minimize this UB-SSD by adjusting\nboth the parameters of the FCNN and the parameters of the deformable model in\ncoordinate descent. Our coordinate descent framework is end-to-end and can work\nwith any deformable registration method that uses SSD. We demonstrate\nexperimentally that our method enhances the accuracy of deformable registration\nalgorithms significantly on two publicly available 3D brain MRI data sets. \n\n"}
{"id": "1612.00437", "contents": "Title: Efficient Pose and Cell Segmentation using Column Generation Abstract: We study the problems of multi-person pose segmentation in natural images and\ninstance segmentation in biological images with crowded cells. We formulate\nthese distinct tasks as integer programs where variables correspond to\nposes/cells. To optimize, we propose a generic relaxation scheme for solving\nthese combinatorial problems using a column generation formulation where the\nprogram for generating a column is solved via exact optimization of very small\nscale integer programs. This results in efficient exploration of the spaces of\nposes and cells. \n\n"}
{"id": "1612.00892", "contents": "Title: Resolving the $R_{AA}$ to $v_n$ puzzle Abstract: After 10 years of struggling to simultaneously describe the nuclear\nmodification factor $R_{AA}$ and flow harmonics $v_n$'s at high $p_T$, now\ntheoretical models are able to reproduce experimental data well. The necessary\ntheoretical developments such as event-by-event fluctuations, choice of initial\nconditions, and the scalar product method to calculate flow harmonics at high\n$p_T$ are reviewed. Additionally, a discussion of new proposed experimental\nobservables known as Soft Hard Event Engineering (SHEE) that are sensitive to\nthe path length dependence of the energy loss is included. \n\n"}
{"id": "1612.02028", "contents": "Title: Measurement Of Energy Resolution For TES Microcalorimeter With Optical\n  Pulses Abstract: Energy resolution is an important figure of merit for TES microcalorimeter.\nWe propose a laser system to measure the energy resolution of TES\nmicrocalorimeter with a 1550 nm laser source. Compared to method that\ncharacterizes the performance by irradiating the detector using X-ray photons\nfrom a radioactive source placed inside the refrigerator, our system is safer\nand more convenient. The feasibility of this system has been demonstrated in\nthe measurement of an Al/Ti bilayer TES microcalorimeter. In this experiment,\nthe tested detector showed a energy resolution of 72 eV in the energy range\nfrom 0.2 keV to 0.9 keV \n\n"}
{"id": "1612.02046", "contents": "Title: Charged-particle detection efficiencies of close-packed CsI arrays Abstract: Detector efficiency determination is essential to correct the measured yields\nand extract reliable cross sections of particles emitted in nuclear reactions.\nWe investigate the efficiencies for measuring the full energies of light\ncharged particle in arrays of CsI crystals employed in particle detection\narrays such as HiRA, LASSA and MUST2. We perform these simulations with a\nGEANT4 Monte Carlo transport code implemented in the NPTool framework. Both\nCoulomb multiple scattering and nuclear reactions within the crystal can\nsignificantly reduce the efficiency of detecting the full energy of high energy\nparticles. The calculated efficiencies decrease exponentially as a function of\nthe range of the particle and are quite similar for both the hydrogen ($p, d,\nt$) and helium ($^3$He, $\\alpha$) isotopes. The use of a close-packed array\nintroduces significant position dependent efficiency losses at the interior\nboundaries between crystals that need to be considered in the design of an\narray and in the efficiency corrections of measured energy spectra. \n\n"}
{"id": "1612.03928", "contents": "Title: Paying More Attention to Attention: Improving the Performance of\n  Convolutional Neural Networks via Attention Transfer Abstract: Attention plays a critical role in human visual experience. Furthermore, it\nhas recently been demonstrated that attention can also play an important role\nin the context of applying artificial neural networks to a variety of tasks\nfrom fields such as computer vision and NLP. In this work we show that, by\nproperly defining attention for convolutional neural networks, we can actually\nuse this type of information in order to significantly improve the performance\nof a student CNN network by forcing it to mimic the attention maps of a\npowerful teacher network. To that end, we propose several novel methods of\ntransferring attention, showing consistent improvement across a variety of\ndatasets and convolutional neural network architectures. Code and models for\nour experiments are available at\nhttps://github.com/szagoruyko/attention-transfer \n\n"}
{"id": "1612.03959", "contents": "Title: Autoencoder-based holographic image restoration Abstract: We propose a holographic image restoration method using an autoencoder, which\nis an artificial neural network. Because holographic reconstructed images are\noften contaminated by direct light, conjugate light, and speckle noise, the\ndiscrimination of reconstructed images may be difficult. In this paper, we\ndemonstrate the restoration of reconstructed images from holograms that record\npage data in holographic memory and QR codes by using the proposed method. \n\n"}
{"id": "1612.05079", "contents": "Title: SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor\n  Trajectories with Ground Truth Abstract: We introduce SceneNet RGB-D, expanding the previous work of SceneNet to\nenable large scale photorealistic rendering of indoor scene trajectories. It\nprovides pixel-perfect ground truth for scene understanding problems such as\nsemantic segmentation, instance segmentation, and object detection, and also\nfor geometric computer vision problems such as optical flow, depth estimation,\ncamera pose estimation, and 3D reconstruction. Random sampling permits\nvirtually unlimited scene configurations, and here we provide a set of 5M\nrendered RGB-D images from over 15K trajectories in synthetic layouts with\nrandom but physically simulated object poses. Each layout also has random\nlighting, camera trajectories, and textures. The scale of this dataset is well\nsuited for pre-training data-driven computer vision techniques from scratch\nwith RGB-D inputs, which previously has been limited by relatively small\nlabelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for\ninvestigating 3D scene labelling tasks by providing perfect camera poses and\ndepth data as proxy for a SLAM system. We host the dataset at\nhttp://robotvault.bitbucket.io/scenenet-rgbd.html \n\n"}
{"id": "1612.05318", "contents": "Title: The CUORE slow monitoring systems Abstract: CUORE is a cryogenic experiment searching primarily for neutrinoless double\ndecay in $^{130}$Te. It will begin data-taking operations in 2016. To monitor\nthe cryostat and detector during commissioning and data taking, we have\ndesigned and developed Slow Monitoring systems. In addition to real-time\nsystems using LabVIEW, we have an alarm, analysis, and archiving website that\nuses MongoDB, AngularJS, and Bootstrap software. These modern, state of the art\nsoftware packages make the monitoring system transparent, easily maintainable,\nand accessible on many platforms including mobile devices. \n\n"}
{"id": "1612.05846", "contents": "Title: Joint Spatial-Angular Sparse Coding for dMRI with Separable Dictionaries Abstract: Diffusion MRI (dMRI) provides the ability to reconstruct neuronal fibers in\nthe brain, $\\textit{in vivo}$, by measuring water diffusion along angular\ngradient directions in q-space. High angular resolution diffusion imaging\n(HARDI) can produce better estimates of fiber orientation than the popularly\nused diffusion tensor imaging, but the high number of samples needed to\nestimate diffusivity requires longer patient scan times. To accelerate dMRI,\ncompressed sensing (CS) has been utilized by exploiting a sparse dictionary\nrepresentation of the data, discovered through sparse coding. The sparser the\nrepresentation, the fewer samples are needed to reconstruct a high resolution\nsignal with limited information loss, and so an important area of research has\nfocused on finding the sparsest possible representation of dMRI. Current\nreconstruction methods however, rely on an angular representation $\\textit{per\nvoxel}$ with added spatial regularization, and so, for non-zero signals, one is\nrequired to have at least one non-zero coefficient per voxel. This means that\nthe global level of sparsity must be greater than the number of voxels. In\ncontrast, we propose a joint spatial-angular representation of dMRI that will\nallow us to achieve levels of global sparsity that are below the number of\nvoxels. A major challenge, however, is the computational complexity of solving\na global sparse coding problem over large-scale dMRI. In this work, we present\nnovel adaptations of popular sparse coding algorithms that become better suited\nfor solving large-scale problems by exploiting spatial-angular separability.\nOur experiments show that our method achieves significantly sparser\nrepresentations of HARDI than is possible by the state of the art. \n\n"}
{"id": "1612.05877", "contents": "Title: Deep Learning on Lie Groups for Skeleton-based Action Recognition Abstract: In recent years, skeleton-based action recognition has become a popular 3D\nclassification problem. State-of-the-art methods typically first represent each\nmotion sequence as a high-dimensional trajectory on a Lie group with an\nadditional dynamic time warping, and then shallowly learn favorable Lie group\nfeatures. In this paper we incorporate the Lie group structure into a deep\nnetwork architecture to learn more appropriate Lie group features for 3D action\nrecognition. Within the network structure, we design rotation mapping layers to\ntransform the input Lie group features into desirable ones, which are aligned\nbetter in the temporal domain. To reduce the high feature dimensionality, the\narchitecture is equipped with rotation pooling layers for the elements on the\nLie group. Furthermore, we propose a logarithm mapping layer to map the\nresulting manifold data into a tangent space that facilitates the application\nof regular output layers for the final classification. Evaluations of the\nproposed network for standard 3D human action recognition datasets clearly\ndemonstrate its superiority over existing shallow Lie group feature learning\nmethods as well as most conventional deep learning methods. \n\n"}
{"id": "1612.09235", "contents": "Title: Polynomial Chaos Expansion method as a tool to evaluate and quantify\n  field homogeneities of a novel waveguide RF Wien Filter Abstract: For the measurement of the electric dipole moment of protons and deuterons, a\nnovel waveguide RF Wien filter has been designed and will soon be integrated at\nthe COoler SYnchrotron at J\\\"ulich. The device operates at the harmonic\nfrequencies of the spin motion. It is based on a waveguide structure that is\ncapable of fulfilling the Wien filter condition ($\\vec{E} \\perp \\vec{B}$)\n\\textit{by design}. The full-wave calculations demonstrated that the waveguide\nRF Wien filter is able to generate high-quality RF electric and magnetic\nfields. In reality, mechanical tolerances and misalignments decrease the\nsimulated field quality, and it is therefore important to consider them in the\nsimulations. In particular, for the electric dipole moment measurement, it is\nimportant to quantify the field errors systematically. Since Monte-Carlo\nsimulations are computationally very expensive, we discuss here an efficient\nsurrogate modeling scheme based on the Polynomial Chaos Expansion method to\ncompute the field quality in the presence of tolerances and misalignments and\nsubsequently to perform the sensitivity analysis at zero additional\ncomputational cost. \n\n"}
{"id": "1701.00458", "contents": "Title: Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient\n  Detection Abstract: We introduce Deep-HiTS, a rotation invariant convolutional neural network\n(CNN) model for classifying images of transients candidates into artifacts or\nreal sources for the High cadence Transient Survey (HiTS). CNNs have the\nadvantage of learning the features automatically from the data while achieving\nhigh performance. We compare our CNN model against a feature engineering\napproach using random forests (RF). We show that our CNN significantly\noutperforms the RF model reducing the error by almost half. Furthermore, for a\nfixed number of approximately 2,000 allowed false transient candidates per\nnight we are able to reduce the miss-classified real transients by\napproximately 1/5. To the best of our knowledge, this is the first time CNNs\nhave been used to detect astronomical transient events. Our approach will be\nvery useful when processing images from next generation instruments such as the\nLarge Synoptic Survey Telescope (LSST). We have made all our code and data\navailable to the community for the sake of allowing further developments and\ncomparisons at https://github.com/guille-c/Deep-HiTS. \n\n"}
{"id": "1701.01486", "contents": "Title: Motion Deblurring in the Wild Abstract: The task of image deblurring is a very ill-posed problem as both the image\nand the blur are unknown. Moreover, when pictures are taken in the wild, this\ntask becomes even more challenging due to the blur varying spatially and the\nocclusions between the object. Due to the complexity of the general image model\nwe propose a novel convolutional network architecture which directly generates\nthe sharp image.This network is built in three stages, and exploits the\nbenefits of pyramid schemes often used in blind deconvolution. One of the main\ndifficulties in training such a network is to design a suitable dataset. While\nuseful data can be obtained by synthetically blurring a collection of images,\nmore realistic data must be collected in the wild. To obtain such data we use a\nhigh frame rate video camera and keep one frame as the sharp image and frame\naverage as the corresponding blurred image. We show that this realistic dataset\nis key in achieving state-of-the-art performance and dealing with occlusions. \n\n"}
{"id": "1701.02780", "contents": "Title: Parity Violation in Deep Inelastic Scattering with the SoLID\n  Spectrometer at JLab Abstract: Measurements of parity-violating asymmetries in DIS region using the SoLID\nspectrometer at Jefferson Lab (JLab) Hall A in the 12 GeV era are presented. A\nproposal with a polarized electron beam on unpolarized deuteron and proton\ntargets has been approved with an A rating by the JLab PAC. The deuteron\nmeasurement aims to measure the weak mixing angle $\\sin^2 \\theta_W $ with a\nprecision of $\\pm$ 0.0006 as well as to access the fundamental coupling\nconstants $C_{2q}$ with a high precision. This measurement is ideally suited\nfor testing the Standard Model with the potential to probe charge symmetry\nviolation and resolve the quark-quark correlations in the DIS region. The\nproton experiment provides a clean measurement of $d/u$ ratio in the high-$x$\nregion free of nuclear corrections. To achieve these goals, the SoLID\nspectrometer was proposed and designed to handle a high luminosity with a large\nacceptance. In this article, the details of the approved measurements are\ndiscussed, along with new ideas with PVDIS using a polarized $^3$He target to\naccess new $\\gamma-Z$ interference polarized structure functions and a\nunpolarized $^{48}$Ca target to study the EMC effect. \n\n"}
{"id": "1701.02898", "contents": "Title: Modeling Retinal Ganglion Cell Population Activity with Restricted\n  Boltzmann Machines Abstract: The retina is a complex nervous system which encodes visual stimuli before\nhigher order processing occurs in the visual cortex. In this study we evaluated\nwhether information about the stimuli received by the retina can be retrieved\nfrom the firing rate distribution of Retinal Ganglion Cells (RGCs), exploiting\nHigh-Density 64x64 MEA technology. To this end, we modeled the RGC population\nactivity using mean-covariance Restricted Boltzmann Machines, latent variable\nmodels capable of learning the joint distribution of a set of continuous\nobserved random variables and a set of binary unobserved random units. The idea\nwas to figure out if binary latent states encode the regularities associated to\ndifferent visual stimuli, as modes in the joint distribution. We measured the\ngoodness of mcRBM encoding by calculating the Mutual Information between the\nlatent states and the stimuli shown to the retina. Results show that binary\nstates can encode the regularities associated to different stimuli, using both\ngratings and natural scenes as stimuli. We also discovered that hidden\nvariables encode interesting properties of retinal activity, interpreted as\npopulation receptive fields. We further investigated the ability of the model\nto learn different modes in population activity by comparing results associated\nto a retina in normal conditions and after pharmacologically blocking GABA\nreceptors (GABAC at first, and then also GABAA and GABAB). As expected, Mutual\nInformation tends to decrease if we pharmacologically block receptors. We\nfinally stress that the computational method described in this work could\npotentially be applied to any kind of neural data obtained through MEA\ntechnology, though different techniques should be applied to interpret the\nresults. \n\n"}
{"id": "1701.03151", "contents": "Title: Guaranteed Parameter Estimation for Discrete Energy Minimization Abstract: Structural learning, a method to estimate the parameters for discrete energy\nminimization, has been proven to be effective in solving computer vision\nproblems, especially in 3D scene parsing. As the complexity of the models\nincreases, structural learning algorithms turn to approximate inference to\nretain tractability. Unfortunately, such methods often fail because the\napproximation can be arbitrarily poor. In this work, we propose a method to\novercome this limitation through exploiting the properties of the joint problem\nof training time inference and learning. With the help of the learning\nframework, we transform the inapproximable inference problem into a polynomial\ntime solvable one, thereby enabling tractable exact inference while still\nallowing an arbitrary graph structure and full potential interactions. Our\nlearning algorithm is guaranteed to return a solution with a bounded error to\nthe global optimal within the feasible parameter space. We demonstrate the\neffectiveness of this method on two point cloud scene parsing datasets. Our\napproach runs much faster and solves a problem that is intractable for\nprevious, well-known approaches. \n\n"}
{"id": "1701.05184", "contents": "Title: aCORN: an experiment to measure the electron-antineutrino correlation\n  coefficient in free neutron decay Abstract: We describe an apparatus used to measure the electron-antineutrino angular\ncorrelation coefficient in free neutron decay. The apparatus employs a novel\nmeasurement technique in which the angular correlation is converted into a\nproton time-of-flight asymmetry that is counted directly, avoiding the need for\nproton spectroscopy. Details of the method, apparatus, detectors, data\nacquisition, and data reduction scheme are presented, along with a discussion\nof the important systematic effects. \n\n"}
{"id": "1701.05419", "contents": "Title: Moving to VideoKifu: the last steps toward a fully automatic\n  record-keeping of a Go game Abstract: In a previous paper [ arXiv:1508.03269 ] we described the techniques we\nsuccessfully employed for automatically reconstructing the whole move sequence\nof a Go game by means of a set of pictures. Now we describe how it is possible\nto reconstruct the move sequence by means of a video stream (which may be\nprovided by an unattended webcam), possibly in real-time. Although the basic\nalgorithms remain the same, we will discuss the new problems that arise when\ndealing with videos, with special care for the ones that could block a\nreal-time analysis and require an improvement of our previous techniques or\neven a completely brand new approach. Eventually we present a number of\npreliminary but positive experimental results supporting the effectiveness of\nthe software we are developing, built on the ideas here outlined. \n\n"}
{"id": "1701.06123", "contents": "Title: Optimization on Product Submanifolds of Convolution Kernels Abstract: Recent advances in optimization methods used for training convolutional\nneural networks (CNNs) with kernels, which are normalized according to\nparticular constraints, have shown remarkable success. This work introduces an\napproach for training CNNs using ensembles of joint spaces of kernels\nconstructed using different constraints. For this purpose, we address a problem\nof optimization on ensembles of products of submanifolds (PEMs) of convolution\nkernels. To this end, we first propose three strategies to construct ensembles\nof PEMs in CNNs. Next, we expound their geometric properties (metric and\ncurvature properties) in CNNs. We make use of our theoretical results by\ndeveloping a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles\nof PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD\nconsidering geometric properties of PEMs. In the experimental analyses, we\nemploy G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The\nresults show that geometric adaptive step size computation methods of G-SGD can\nimprove training loss and convergence properties of CNNs. Moreover, we observe\nthat classification performance of baseline CNNs can be boosted using G-SGD on\nensembles of PEMs identified by multiple constraints. \n\n"}
{"id": "1701.07431", "contents": "Title: Consistent description of UCN transport properties Abstract: We have investigated the diffuse reflection probabilities of Replica guides\nfor ultra-cold neutrons (UCN) using the so-called helium method. For the first\ntime we could establish a consistent description of the diffuse reflection\nmechanism for different lengths of the guide system. The transmission of the\nguides is measured depending on the helium pressure inside of the guides. A\nseries of simulations was done to reproduce the experimental data. These\nsimulations showed that a diffuse reflection probability of $d = (3.0 \\pm 0.5)\n\\cdot 10^{-2}$ sufficiently describes the experimental data. \n\n"}
{"id": "1701.07432", "contents": "Title: Suppression of alpha-induced lateral surface events in the COBRA\n  experiment using CdZnTe detectors with an instrumented guard-ring electrode Abstract: The COBRA collaboration searches for neutrinoless double beta-decay\n($0\\nu\\beta\\beta$-decay) using CdZnTe semiconductor detectors with a\ncoplanar-grid readout and a surrounding guard-ring structure. The operation of\nthe COBRA demonstrator at the Gran Sasso underground laboratory (LNGS)\nindicates that alpha-induced lateral surface events are the dominant source of\nbackground events. By instrumenting the guard-ring electrode it is possible to\nsuppress this type of background. In laboratory measurements this method\nachieved a suppression factor of alpha-induced lateral surface events of\n$5300^{+2660}_{-1380}$, while retaining $85.3\\pm0.1\\%$ of gamma events\noccurring in the entire detector volume. This suppression is superior to the\npulse-shape analysis methods used so far in COBRA by three orders of magnitude. \n\n"}
{"id": "1701.08349", "contents": "Title: Supervised Deep Sparse Coding Networks Abstract: In this paper, we describe the deep sparse coding network (SCN), a novel deep\nnetwork that encodes intermediate representations with nonnegative sparse\ncoding. The SCN is built upon a number of cascading bottleneck modules, where\neach module consists of two sparse coding layers with relatively wide and slim\ndictionaries that are specialized to produce high dimensional discriminative\nfeatures and low dimensional representations for clustering, respectively.\nDuring training, both the dictionaries and regularization parameters are\noptimized with an end-to-end supervised learning algorithm based on multilevel\noptimization. Effectiveness of an SCN with seven bottleneck modules is verified\non several popular benchmark datasets. Remarkably, with few parameters to\nlearn, our SCN achieves 5.81% and 19.93% classification error rate on CIFAR-10\nand CIFAR-100, respectively. \n\n"}
{"id": "1701.08380", "contents": "Title: The HASYv2 dataset Abstract: This paper describes the HASYv2 dataset. HASY is a publicly available, free\nof charge dataset of single symbols similar to MNIST. It contains 168233\ninstances of 369 classes. HASY contains two challenges: A classification\nchallenge with 10 pre-defined folds for 10-fold cross-validation and a\nverification challenge. \n\n"}
{"id": "1701.08398", "contents": "Title: Re-ranking Person Re-identification with k-reciprocal Encoding Abstract: When considering person re-identification (re-ID) as a retrieval process,\nre-ranking is a critical step to improve its accuracy. Yet in the re-ID\ncommunity, limited effort has been devoted to re-ranking, especially those\nfully automatic, unsupervised solutions. In this paper, we propose a\nk-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is\nthat if a gallery image is similar to the probe in the k-reciprocal nearest\nneighbors, it is more likely to be a true match. Specifically, given an image,\na k-reciprocal feature is calculated by encoding its k-reciprocal nearest\nneighbors into a single vector, which is used for re-ranking under the Jaccard\ndistance. The final distance is computed as the combination of the original\ndistance and the Jaccard distance. Our re-ranking method does not require any\nhuman interaction or any labeled data, so it is applicable to large-scale\ndatasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW\ndatasets confirm the effectiveness of our method. \n\n"}
{"id": "1702.02463", "contents": "Title: Video Frame Synthesis using Deep Voxel Flow Abstract: We address the problem of synthesizing new video frames in an existing video,\neither in-between existing frames (interpolation), or subsequent to them\n(extrapolation). This problem is challenging because video appearance and\nmotion can be highly complex. Traditional optical-flow-based solutions often\nfail where flow estimation is challenging, while newer neural-network-based\nmethods that hallucinate pixel values directly often produce blurry results. We\ncombine the advantages of these two methods by training a deep network that\nlearns to synthesize video frames by flowing pixel values from existing ones,\nwhich we call deep voxel flow. Our method requires no human supervision, and\nany video can be used as training data by dropping, and then learning to\npredict, existing frames. The technique is efficient, and can be applied at any\nvideo resolution. We demonstrate that our method produces results that both\nquantitatively and qualitatively improve upon the state-of-the-art. \n\n"}
{"id": "1702.02466", "contents": "Title: The Majorana Demonstrator calibration system Abstract: The MAJORANA Collaboration is searching for the neutrinoless double-beta\ndecay of the nucleus $^{76}$Ge. The MAJORANA DEMONSTRATOR is an array of\ngermanium detectors deployed with the aim of implementing background reduction\ntechniques suitable for a 1-tonne $^{76}$Ge-based search. The ultra\nlow-background conditions require regular calibrations to verify proper\nfunction of the detectors. Radioactive line sources can be deployed around the\ncryostats containing the detectors for regular energy calibrations. When\nmeasuring in low-background mode, these line sources have to be stored outside\nthe shielding so they do not contribute to the background. The deployment and\nthe retraction of the source are designed to be controlled by the data\nacquisition system and do not require any direct human interaction. In this\npaper, we detail the design requirements and implementation of the calibration\napparatus, which provides the event rates needed to define the pulse-shape cuts\nand energy calibration used in the final analysis as well as data that can be\ncompared to simulations. \n\n"}
{"id": "1702.03431", "contents": "Title: Crossing Nets: Combining GANs and VAEs with a Shared Latent Space for\n  Hand Pose Estimation Abstract: State-of-the-art methods for 3D hand pose estimation from depth images\nrequire large amounts of annotated training data. We propose to model the\nstatistical relationships of 3D hand poses and corresponding depth images using\ntwo deep generative models with a shared latent space. By design, our\narchitecture allows for learning from unlabeled image data in a semi-supervised\nmanner. Assuming a one-to-one mapping between a pose and a depth map, any given\npoint in the shared latent space can be projected into both a hand pose and a\ncorresponding depth map. Regressing the hand pose can then be done by learning\na discriminator to estimate the posterior of the latent pose given some depth\nmaps. To improve generalization and to better exploit unlabeled depth maps, we\njointly train a generator and a discriminator. At each iteration, the generator\nis updated with the back-propagated gradient from the discriminator to\nsynthesize realistic depth maps of the articulated hand, while the\ndiscriminator benefits from an augmented training set of synthesized and\nunlabeled samples. The proposed discriminator network architecture is highly\nefficient and runs at 90 FPS on the CPU with accuracies comparable or better\nthan state-of-art on 3 publicly available benchmarks. \n\n"}
{"id": "1702.05270", "contents": "Title: Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers\n  from Vision Abstract: People can refer to quantities in a visual scene by using either exact\ncardinals (e.g. one, two, three) or natural language quantifiers (e.g. few,\nmost, all). In humans, these two processes underlie fairly different cognitive\nand neural mechanisms. Inspired by this evidence, the present study proposes\ntwo models for learning the objective meaning of cardinals and quantifiers from\nvisual scenes containing multiple objects. We show that a model capitalizing on\na 'fuzzy' measure of similarity is effective for learning quantifiers, whereas\nthe learning of exact cardinals is better accomplished when information about\nnumber is provided. \n\n"}
{"id": "1702.06117", "contents": "Title: Evaluating Reactor Antineutrino Signals for WATCHMAN Abstract: Increasing the distance from which an antineutrino detector is capable of\nmonitoring the operation of a registered reactor, or discovering a clandestine\nreactor, strengthens the Non-Proliferation of Nuclear Weapons Treaty. This\npaper presents calculations of reactor antineutrino interactions from\nquasi-elastic neutrino-proton scattering and elastic neutrino-electron\nscattering in a water-based detector operated $\\gtrsim10$ km from a commercial\npower reactor. It separately calculates signal from the proximal reactor and\nbackground from all other registered reactors. The main results are\ndifferential and integral interaction rates from the quasi-elastic and elastic\nprocesses. There are two underground facilities capable of hosting a detector\n($\\sim1$ kT H$_2$O) project nearby ($L\\sim20$ km) an operating commercial\nreactor ($P_{th}\\sim3$ GW). These reactor-site combinations, which are under\nconsideration for project WATCHMAN, are Perry-Morton on the southern shore of\nLake Erie in the United States and Hartlepool-Boulby on the western shore of\nthe North Sea in England. The signal rate from the proximal reactor is about\nfive times greater at the Morton site than at the Boulby site due to shorter\nreactor-site separation distance, larger reactor thermal power, and greater\nneutrino oscillation survival probability. Although the background rate from\nall other reactors is larger at Morton than at Boulby, it is a smaller fraction\nof the signal rate from the proximal reactor at Morton than at Boulby.\nMoreover, the Hartlepool power plant has two cores whereas the Perry plant has\na single core. The Boulby site, therefore, offers an opportunity for remotely\nmonitoring the on/off cycle of a reactor core under more stringent conditions\nthan does the Morton site. \n\n"}
{"id": "1702.06521", "contents": "Title: VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization Abstract: Machine learning techniques, namely convolutional neural networks (CNN) and\nregression forests, have recently shown great promise in performing 6-DoF\nlocalization of monocular images. However, in most cases image-sequences,\nrather only single images, are readily available. To this extent, none of the\nproposed learning-based approaches exploit the valuable constraint of temporal\nsmoothness, often leading to situations where the per-frame error is larger\nthan the camera motion. In this paper we propose a recurrent model for\nperforming 6-DoF localization of video-clips. We find that, even by considering\nonly short sequences (20 frames), the pose estimates are smoothed and the\nlocalization error can be drastically reduced. Finally, we consider means of\nobtaining probabilistic pose estimates from our model. We evaluate our method\non openly-available real-world autonomous driving and indoor localization\ndatasets. \n\n"}
{"id": "1702.06946", "contents": "Title: Study of $\\mathrm{CdMoO_4}$ crystal for a neutrinoless double beta decay\n  experiment with $\\mathrm{{}^{116}Cd}$ and $\\mathrm{{}^{100}Mo}$ nuclides Abstract: The scintillation properties of a $\\mathrm{CdMoO_4}$ crystal have been\ninvestigated experimentally. The fluorescence yields and decay times measured\nfrom 22 K to 300 K demonstrate that $\\mathrm{CdMoO_4}$ crystal is a good\ncandidate for an absorber for a bolometer readout, for both heat and\nscintillation signals. The results from Monte Carlo studies taking the\nbackgrounds from $\\mathrm{2\\nu2\\beta}$ of $\\mathrm{{}_{42}^{100}Mo}$\n($\\mathrm{{}_{48}^{116}Cd}$) and internal trace nuclides $\\mathrm{{}^{214}Bi}$\nand $\\mathrm{{}^{208}Tl}$ into account show that the expected sensitivity of\n$\\mathrm{CdMoO_4}$ bolometer for neutrinoless double beta decay experiment with\nan exposure of 100 $\\mathrm{{kg}\\cdot{years}}$ is one order of magnitude higher\nthan those of the current sets of the $\\mathrm{\\lim{T^{0\\nu\\beta\\beta}_{1/2}}}$\nof $\\mathrm{{}_{42}^{100}Mo}$ and $\\mathrm{{}_{48}^{116}Cd}$. \n\n"}
{"id": "1702.07811", "contents": "Title: Adaptive Neural Networks for Efficient Inference Abstract: We present an approach to adaptively utilize deep neural networks in order to\nreduce the evaluation time on new examples without loss of accuracy. Rather\nthan attempting to redesign or approximate existing networks, we propose two\nschemes that adaptively utilize networks. We first pose an adaptive network\nevaluation scheme, where we learn a system to adaptively choose the components\nof a deep network to be evaluated for each example. By allowing examples\ncorrectly classified using early layers of the system to exit, we avoid the\ncomputational time associated with full evaluation of the network. We extend\nthis to learn a network selection system that adaptively selects the network to\nbe evaluated for each example. We show that computational time can be\ndramatically reduced by exploiting the fact that many examples can be correctly\nclassified using relatively efficient networks and that complex,\ncomputationally costly networks are only necessary for a small fraction of\nexamples. We pose a global objective for learning an adaptive early exit or\nnetwork selection policy and solve it by reducing the policy learning problem\nto a layer-by-layer weighted binary classification problem. Empirically, these\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\nspeedup on state-of-the-art networks from the ImageNet image recognition\nchallenge with minimal (<1%) loss of top5 accuracy. \n\n"}
{"id": "1703.00508", "contents": "Title: Evaluation of commercial nickel-phosphorus coating for ultracold neutron\n  guides using a pinhole bottling method Abstract: We report on the evaluation of commercial electroless nickel phosphorus (NiP)\ncoatings for ultracold neutron (UCN) transport and storage. The material\npotential of 50~$\\mu$m thick NiP coatings on stainless steel and aluminum\nsubstrates was measured to be $V_F = 213(5.2)$~neV using the time-of-flight\nspectrometer ASTERIX at the Lujan Center. The loss per bounce probability was\nmeasured in pinhole bottling experiments carried out at ultracold neutron\nsources at Los Alamos Neutron Science Center and the Institut Laue-Langevin.\nFor these tests a new guide coupling design was used to minimize gaps between\nthe guide sections. The observed UCN loss in the bottle was interpreted in\nterms of an energy independent effective loss per bounce, which is the\nappropriate model when gaps in the system and upscattering are the dominate\nloss mechanisms, yielding a loss per bounce of $1.3(1) \\times 10^{-4}$. We also\npresent a detailed discussion of the pinhole bottling methodology and an energy\ndependent analysis of the experimental results. \n\n"}
{"id": "1703.00663", "contents": "Title: Introduction to Nonnegative Matrix Factorization Abstract: In this paper, we introduce and provide a short overview of nonnegative\nmatrix factorization (NMF). Several aspects of NMF are discussed, namely, the\napplication in hyperspectral imaging, geometry and uniqueness of NMF solutions,\ncomplexity, algorithms, and its link with extended formulations of polyhedra.\nIn order to put NMF into perspective, the more general problem class of\nconstrained low-rank matrix approximation problems is first briefly introduced. \n\n"}
{"id": "1703.01086", "contents": "Title: Arbitrary-Oriented Scene Text Detection via Rotation Proposals Abstract: This paper introduces a novel rotation-based framework for arbitrary-oriented\ntext detection in natural scene images. We present the Rotation Region Proposal\nNetworks (RRPN), which are designed to generate inclined proposals with text\norientation angle information. The angle information is then adapted for\nbounding box regression to make the proposals more accurately fit into the text\nregion in terms of the orientation. The Rotation Region-of-Interest (RRoI)\npooling layer is proposed to project arbitrary-oriented proposals to a feature\nmap for a text region classifier. The whole framework is built upon a\nregion-proposal-based architecture, which ensures the computational efficiency\nof the arbitrary-oriented text detection compared with previous text detection\nsystems. We conduct experiments using the rotation-based framework on three\nreal-world scene text detection datasets and demonstrate its superiority in\nterms of effectiveness and efficiency over previous approaches. \n\n"}
{"id": "1703.01398", "contents": "Title: Sparse Depth Sensing for Resource-Constrained Robots Abstract: We consider the case in which a robot has to navigate in an unknown\nenvironment but does not have enough on-board power or payload to carry a\ntraditional depth sensor (e.g., a 3D lidar) and thus can only acquire a few\n(point-wise) depth measurements. We address the following question: is it\npossible to reconstruct the geometry of an unknown environment using sparse and\nincomplete depth measurements? Reconstruction from incomplete data is not\npossible in general, but when the robot operates in man-made environments, the\ndepth exhibits some regularity (e.g., many planar surfaces with only a few\nedges); we leverage this regularity to infer depth from a small number of\nmeasurements. Our first contribution is a formulation of the depth\nreconstruction problem that bridges robot perception with the compressive\nsensing literature in signal processing. The second contribution includes a set\nof formal results that ascertain the exactness and stability of the depth\nreconstruction in 2D and 3D problems, and completely characterize the geometry\nof the profiles that we can reconstruct. Our third contribution is a set of\npractical algorithms for depth reconstruction: our formulation directly\ntranslates into algorithms for depth estimation based on convex programming. In\nreal-world problems, these convex programs are very large and general-purpose\nsolvers are relatively slow. For this reason, we discuss ad-hoc solvers that\nenable fast depth reconstruction in real problems. The last contribution is an\nextensive experimental evaluation in 2D and 3D problems, including Monte Carlo\nruns on simulated instances and testing on multiple real datasets. Empirical\nresults confirm that the proposed approach ensures accurate depth\nreconstruction, outperforms interpolation-based strategies, and performs well\neven when the assumption of structured environment is violated. \n\n"}
{"id": "1703.02168", "contents": "Title: Deep View Morphing Abstract: Recently, convolutional neural networks (CNN) have been successfully applied\nto view synthesis problems. However, such CNN-based methods can suffer from\nlack of texture details, shape distortions, or high computational complexity.\nIn this paper, we propose a novel CNN architecture for view synthesis called\n\"Deep View Morphing\" that does not suffer from these issues. To synthesize a\nmiddle view of two input images, a rectification network first rectifies the\ntwo input images. An encoder-decoder network then generates dense\ncorrespondences between the rectified images and blending masks to predict the\nvisibility of pixels of the rectified images in the middle view. A view\nmorphing network finally synthesizes the middle view using the dense\ncorrespondences and blending masks. We experimentally show the proposed method\nsignificantly outperforms the state-of-the-art CNN-based view synthesis method. \n\n"}
{"id": "1703.02952", "contents": "Title: A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n  Analytics Abstract: Internet of Things (IoT) devices and applications are being deployed in our\nhomes and workplaces. These devices often rely on continuous data collection to\nfeed machine learning models. However, this approach introduces several privacy\nand efficiency challenges, as the service operator can perform unwanted\ninferences on the available data. Recently, advances in edge processing have\npaved the way for more efficient, and private, data processing at the source\nfor simple tasks and lighter models, though they remain a challenge for larger,\nand more complicated models. In this paper, we present a hybrid approach for\nbreaking down large, complex deep neural networks for cooperative,\nprivacy-preserving analytics. To this end, instead of performing the whole\noperation on the cloud, we let an IoT device to run the initial layers of the\nneural network, and then send the output to the cloud to feed the remaining\nlayers and produce the final result. In order to ensure that the user's device\ncontains no extra information except what is necessary for the main task and\npreventing any secondary inference on the data, we introduce Siamese\nfine-tuning. We evaluate the privacy benefits of this approach based on the\ninformation exposed to the cloud service. We also assess the local inference\ncost of different layers on a modern handset. Our evaluations show that by\nusing Siamese fine-tuning and at a small processing cost, we can greatly reduce\nthe level of unnecessary, potentially sensitive information in the personal\ndata, and thus achieving the desired trade-off between utility, privacy, and\nperformance. \n\n"}
{"id": "1703.03626", "contents": "Title: Multi-Grid Detector for Neutron Spectroscopy: Results Obtained on\n  Time-of-Flight Spectrometer CNCS Abstract: The Multi-Grid detector technology has evolved from the proof-of-principle\nand characterisation stages. Here we report on the performance of the\nMulti-Grid detector, the MG.CNCS prototype, which has been installed and tested\nat the Cold Neutron Chopper Spectrometer, CNCS at SNS. This has allowed a\nside-by-side comparison to the performance of $^3$He detectors on an\noperational instrument. The demonstrator has an active area of 0.2 m$^2$. It is\nspecifically tailored to the specifications of CNCS. The detector was installed\nin June 2016 and has operated since then, collecting neutron scattering data in\nparallel to the He-3 detectors of CNCS. In this paper, we present a\ncomprehensive analysis of this data, in particular on instrument energy\nresolution, rate capability, background and relative efficiency. Stability,\ngamma-ray and fast neutron sensitivity have also been investigated. The effect\nof scattering in the detector components has been measured and provides input\nto comparison for Monte Carlo simulations. All data is presented in comparison\nto that measured by the $^3$He detectors simultaneously, showing that all\nfeatures recorded by one detector are also recorded by the other. The energy\nresolution matches closely. We find that the Multi-Grid is able to match the\ndata collected by $^3$He, and see an indication of a considerable advantage in\nthe count rate capability. Based on these results, we are confident that the\nMulti-Grid detector will be capable of producing high quality scientific data\non chopper spectrometers utilising the unprecedented neutron flux of the ESS. \n\n"}
{"id": "1703.04044", "contents": "Title: Colorization as a Proxy Task for Visual Understanding Abstract: We investigate and improve self-supervision as a drop-in replacement for\nImageNet pretraining, focusing on automatic colorization as the proxy task.\nSelf-supervised training has been shown to be more promising for utilizing\nunlabeled data than other, traditional unsupervised learning methods. We build\non this success and evaluate the ability of our self-supervised network in\nseveral contexts. On VOC segmentation and classification tasks, we present\nresults that are state-of-the-art among methods not using ImageNet labels for\npretraining representations.\n  Moreover, we present the first in-depth analysis of self-supervision via\ncolorization, concluding that formulation of the loss, training details and\nnetwork architecture play important roles in its effectiveness. This\ninvestigation is further expanded by revisiting the ImageNet pretraining\nparadigm, asking questions such as: How much training data is needed? How many\nlabels are needed? How much do features change when fine-tuned? We relate these\nquestions back to self-supervision by showing that colorization provides a\nsimilarly powerful supervisory signal as various flavors of ImageNet\npretraining. \n\n"}
{"id": "1703.05272", "contents": "Title: A pulsed, mono-energetic and angular-selective UV photo-electron source\n  for the commissioning of the KATRIN experiment Abstract: The KATRIN experiment aims to determine the neutrino mass scale with a\nsensitivity of 200 meV/c^2 (90% C.L.) by a precision measurement of the shape\nof the tritium $\\beta$-spectrum in the endpoint region. The energy analysis of\nthe decay electrons is achieved by a MAC-E filter spectrometer. To determine\nthe transmission properties of the KATRIN main spectrometer, a mono-energetic\nand angular-selective electron source has been developed. In preparation for\nthe second commissioning phase of the main spectrometer, a measurement phase\nwas carried out at the KATRIN monitor spectrometer where the device was\noperated in a MAC-E filter setup for testing. The results of these measurements\nare compared with simulations using the particle-tracking software\n\"Kassiopeia\", which was developed in the KATRIN collaboration over recent\nyears. \n\n"}
{"id": "1703.06618", "contents": "Title: Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media\n  Retrieval Abstract: This paper contributes a new large-scale dataset for weakly supervised\ncross-media retrieval, named Twitter100k. Current datasets, such as Wikipedia,\nNUS Wide and Flickr30k, have two major limitations. First, these datasets are\nlacking in content diversity, i.e., only some pre-defined classes are covered.\nSecond, texts in these datasets are written in well-organized language, leading\nto inconsistency with realistic applications. To overcome these drawbacks, the\nproposed Twitter100k dataset is characterized by two aspects: 1) it has 100,000\nimage-text pairs randomly crawled from Twitter and thus has no constraint in\nthe image categories; 2) text in Twitter100k is written in informal language by\nthe users.\n  Since strongly supervised methods leverage the class labels that may be\nmissing in practice, this paper focuses on weakly supervised learning for\ncross-media retrieval, in which only text-image pairs are exploited during\ntraining. We extensively benchmark the performance of four subspace learning\nmethods and three variants of the Correspondence AutoEncoder, along with\nvarious text features on Wikipedia, Flickr30k and Twitter100k. Novel insights\nare provided. As a minor contribution, inspired by the characteristic of\nTwitter100k, we propose an OCR-based cross-media retrieval method. In\nexperiment, we show that the proposed OCR-based method improves the baseline\nperformance. \n\n"}
{"id": "1703.07715", "contents": "Title: Classifying Symmetrical Differences and Temporal Change in Mammography\n  Using Deep Neural Networks Abstract: We investigate the addition of symmetry and temporal context information to a\ndeep Convolutional Neural Network (CNN) with the purpose of detecting malignant\nsoft tissue lesions in mammography. We employ a simple linear mapping that\ntakes the location of a mass candidate and maps it to either the contra-lateral\nor prior mammogram and Regions Of Interest (ROI) are extracted around each\nlocation. We subsequently explore two different architectures (1) a fusion\nmodel employing two datastreams were both ROIs are fed to the network during\ntraining and testing and (2) a stage-wise approach where a single ROI CNN is\ntrained on the primary image and subsequently used as feature extractor for\nboth primary and symmetrical or prior ROIs. A 'shallow' Gradient Boosted Tree\n(GBT) classifier is then trained on the concatenation of these features and\nused to classify the joint representation. Results shown a significant increase\nin performance using the first architecture and symmetry information, but only\nmarginal gains in performance using temporal data and the other setting. We\nfeel results are promising and can greatly be improved when more temporal data\nbecomes available. \n\n"}
{"id": "1703.07875", "contents": "Title: Design and construction of a high-energy photon polarimeter Abstract: We report on the design and construction of a high-energy photon polarimeter\nfor measuring the degree of polarization of a linearly-polarized photon beam.\nThe photon polarimeter uses the process of pair production on an atomic\nelectron (triplet production). The azimuthal distribution of scattered atomic\nelectrons following triplet production yields information regarding the degree\nof linear polarization of the incident photon beam. The polarimeter, operated\nin conjunction with a pair spectrometer, uses a silicon strip detector to\nmeasure the recoil electron distribution resulting from triplet photoproduction\nin a beryllium target foil. The analyzing power $\\Sigma_A$ for the device using\na 75 $\\rm{\\mu m}$ beryllium converter foil is about 0.2, with a relative\nsystematic uncertainty in $\\Sigma_A$ of 1.5%. \n\n"}
{"id": "1703.08136", "contents": "Title: Visually grounded learning of keyword prediction from untranscribed\n  speech Abstract: During language acquisition, infants have the benefit of visual cues to\nground spoken language. Robots similarly have access to audio and visual\nsensors. Recent work has shown that images and spoken captions can be mapped\ninto a meaningful common space, allowing images to be retrieved using speech\nand vice versa. In this setting of images paired with untranscribed spoken\ncaptions, we consider whether computer vision systems can be used to obtain\ntextual labels for the speech. Concretely, we use an image-to-words multi-label\nvisual classifier to tag images with soft textual labels, and then train a\nneural network to map from the speech to these soft targets. We show that the\nresulting speech system is able to predict which words occur in an\nutterance---acting as a spoken bag-of-words classifier---without seeing any\nparallel speech and text. We find that the model often confuses semantically\nrelated words, e.g. \"man\" and \"person\", making it even more effective as a\nsemantic keyword spotter. \n\n"}
{"id": "1703.08493", "contents": "Title: Multi-stage Multi-recursive-input Fully Convolutional Networks for\n  Neuronal Boundary Detection Abstract: In the field of connectomics, neuroscientists seek to identify cortical\nconnectivity comprehensively. Neuronal boundary detection from the Electron\nMicroscopy (EM) images is often done to assist the automatic reconstruction of\nneuronal circuit. But the segmentation of EM images is a challenging problem,\nas it requires the detector to be able to detect both filament-like thin and\nblob-like thick membrane, while suppressing the ambiguous intracellular\nstructure. In this paper, we propose multi-stage multi-recursive-input fully\nconvolutional networks to address this problem. The multiple recursive inputs\nfor one stage, i.e., the multiple side outputs with different receptive field\nsizes learned from the lower stage, provide multi-scale contextual boundary\ninformation for the consecutive learning. This design is\nbiologically-plausible, as it likes a human visual system to compare different\npossible segmentation solutions to address the ambiguous boundary issue. Our\nmulti-stage networks are trained end-to-end. It achieves promising results on\ntwo public available EM segmentation datasets, the mouse piriform cortex\ndataset and the ISBI 2012 EM dataset. \n\n"}
{"id": "1703.08987", "contents": "Title: LIDAR-based Driving Path Generation Using Fully Convolutional Neural\n  Networks Abstract: In this work, a novel learning-based approach has been developed to generate\ndriving paths by integrating LIDAR point clouds, GPS-IMU information, and\nGoogle driving directions. The system is based on a fully convolutional neural\nnetwork that jointly learns to carry out perception and path generation from\nreal-world driving sequences and that is trained using automatically generated\ntraining examples. Several combinations of input data were tested in order to\nassess the performance gain provided by specific information modalities. The\nfully convolutional neural network trained using all the available sensors\ntogether with driving directions achieved the best MaxF score of 88.13% when\nconsidering a region of interest of 60x60 meters. By considering a smaller\nregion of interest, the agreement between predicted paths and ground-truth\nincreased to 92.60%. The positive results obtained in this work indicate that\nthe proposed system may help fill the gap between low-level scene parsing and\nbehavior-reflex approaches by generating outputs that are close to vehicle\ncontrol and at the same time human-interpretable. \n\n"}
{"id": "1703.09771", "contents": "Title: Deep 6-DOF Tracking Abstract: We present a temporal 6-DOF tracking method which leverages deep learning to\nachieve state-of-the-art performance on challenging datasets of real world\ncapture. Our method is both more accurate and more robust to occlusions than\nthe existing best performing approaches while maintaining real-time\nperformance. To assess its efficacy, we evaluate our approach on several\nchallenging RGBD sequences of real objects in a variety of conditions. Notably,\nwe systematically evaluate robustness to occlusions through a series of\nsequences where the object to be tracked is increasingly occluded. Finally, our\napproach is purely data-driven and does not require any hand-designed features:\nrobust tracking is automatically learned from data. \n\n"}
{"id": "1703.10221", "contents": "Title: Indirect searches of Galactic diffuse dark matter in INO-MagICAL\n  detector Abstract: The signatures for the existence of dark matter are revealed only through its\ngravitational interaction. Theoretical arguments support that the Weakly\nInteracting Massive Particle (WIMP) can be a class of dark matter and it can\nannihilate and/or decay to Standard Model particles, among which neutrino is a\nfavorable candidate. We show that the proposed 50 kt Magnetized Iron\nCALorimeter (MagICAL) detector under the India-based Neutrino Observatory (INO)\nproject can play an important role in the indirect searches of Galactic diffuse\ndark matter in the neutrino and antineutrino mode separately. We present the\nsensitivity of 500 kt$\\cdot$yr MagICAL detector to set limits on the\nvelocity-averaged self-annihilation cross-section ($\\langle\\sigma v\\rangle$)\nand decay lifetime ($\\tau$) of dark matter having mass in the range of 2 GeV\n$\\leq m_\\chi \\leq $ 90 GeV and 4 GeV $\\leq m_\\chi \\leq $ 180 GeV respectively,\nassuming no excess over the conventional atmospheric neutrino and antineutrino\nfluxes at the INO site. Our limits for low mass dark matter constrain the\nparameter space which has not been explored before. We show that MagICAL will\nbe able to set competitive constraints, $\\langle\\sigma v\\rangle\\leq\n1.87\\,\\times\\,10^{-24}$ cm$^3$ s$^{-1}$ for $\\chi\\chi\\rightarrow\\nu\\bar\\nu$ and\n$\\tau\\geq 4.8\\,\\times\\,10^{24}$ s for $\\chi\\rightarrow\\nu\\bar\\nu$ at 90$\\%$\nC.L. (1 d.o.f.) for $m_\\chi$ = 10 GeV assuming the NFW as dark matter density\nprofile. \n\n"}
{"id": "1704.00498", "contents": "Title: Convolutional neural networks for segmentation and object detection of\n  human semen Abstract: We compare a set of convolutional neural network (CNN) architectures for the\ntask of segmenting and detecting human sperm cells in an image taken from a\nsemen sample. In contrast to previous work, samples are not stained or washed\nto allow for full sperm quality analysis, making analysis harder due to\nclutter. Our results indicate that training on full images is superior to\ntraining on patches when class-skew is properly handled. Full image training\nincluding up-sampling during training proves to be beneficial in deep CNNs for\npixel wise accuracy and detection performance. Predicted sperm cells are found\nby using connected components on the CNN predictions. We investigate\noptimization of a threshold parameter on the size of detected components. Our\nbest network achieves 93.87% precision and 91.89% recall on our test dataset\nafter thresholding outperforming a classical mage analysis approach. \n\n"}
{"id": "1704.00979", "contents": "Title: Optic Disc and Cup Segmentation Methods for Glaucoma Detection with\n  Modification of U-Net Convolutional Neural Network Abstract: Glaucoma is the second leading cause of blindness all over the world, with\napproximately 60 million cases reported worldwide in 2010. If undiagnosed in\ntime, glaucoma causes irreversible damage to the optic nerve leading to\nblindness. The optic nerve head examination, which involves measurement of\ncup-to-disc ratio, is considered one of the most valuable methods of structural\ndiagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation\nof optic disc and optic cup on eye fundus images and can be performed by modern\ncomputer vision algorithms. This work presents universal approach for automatic\noptic disc and cup segmentation, which is based on deep learning, namely,\nmodification of U-Net convolutional neural network. Our experiments include\ncomparison with the best known methods on publicly available databases\nDRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,\nour method achieves quality comparable to current state-of-the-art methods,\noutperforming them in terms of the prediction time. \n\n"}
{"id": "1704.01758", "contents": "Title: Development of $^{100}$Mo-containing scintillating bolometers for a\n  high-sensitivity neutrinoless double-beta decay search Abstract: This paper reports on the development of a technology involving\n$^{100}$Mo-enriched scintillating bolometers, compatible with the goals of\nCUPID, a proposed next-generation bolometric experiment to search for\nneutrinoless double-beta decay. Large mass ($\\sim$1~kg), high optical quality,\nradiopure $^{100}$Mo-containing zinc and lithium molybdate crystals have been\nproduced and used to develop high performance single detector modules based on\n0.2--0.4~kg scintillating bolometers. In particular, the energy resolution of\nthe lithium molybdate detectors near the $Q$-value of the double-beta\ntransition of $^{100}$Mo (3034~keV) is 4--6~keV FWHM. The rejection of the\n$\\alpha$-induced dominant background above 2.6~MeV is better than 8$\\sigma$.\nLess than 10~$\\mu$Bq/kg activity of $^{232}$Th ($^{228}$Th) and $^{226}$Ra in\nthe crystals is ensured by boule recrystallization. The potential of\n$^{100}$Mo-enriched scintillating bolometers to perform high sensitivity\ndouble-beta decay searches has been demonstrated with only 10~kg$\\times$d\nexposure: the two neutrino double-beta decay half-life of $^{100}$Mo has been\nmeasured with the up-to-date highest accuracy as $T_{1/2}$ = [6.90 $\\pm$\n0.15(stat.) $\\pm$ 0.37(syst.)] $\\times$ 10$^{18}$~yr. Both crystallization and\ndetector technologies favor lithium molybdate, which has been selected for the\nongoing construction of the CUPID-0/Mo demonstrator, containing several kg of\n$^{100}$Mo. \n\n"}
{"id": "1704.03533", "contents": "Title: Learning Detection with Diverse Proposals Abstract: To predict a set of diverse and informative proposals with enriched\nrepresentations, this paper introduces a differentiable Determinantal Point\nProcess (DPP) layer that is able to augment the object detection architectures.\nMost modern object detection architectures, such as Faster R-CNN, learn to\nlocalize objects by minimizing deviations from the ground-truth but ignore\ncorrelation between multiple proposals and object categories. Non-Maximum\nSuppression (NMS) as a widely used proposal pruning scheme ignores label- and\ninstance-level relations between object candidates resulting in multi-labeled\ndetections. In the multi-class case, NMS selects boxes with the largest\nprediction scores ignoring the semantic relation between categories of\npotential election. In contrast, our trainable DPP layer, allowing for Learning\nDetection with Diverse Proposals (LDDP), considers both label-level contextual\ninformation and spatial layout relationships between proposals without\nincreasing the number of parameters of the network, and thus improves location\nand category specifications of final detected bounding boxes substantially\nduring both training and inference schemes. Furthermore, we show that LDDP\nkeeps it superiority over Faster R-CNN even if the number of proposals\ngenerated by LDPP is only ~30% as many as those for Faster R-CNN. \n\n"}
{"id": "1704.04337", "contents": "Title: Evolving theoretical descriptions of heavy-ion fusion :from\n  phenomenological to microscopic approaches Abstract: We overview the current status of theoretical approaches for heavy-ion fusion\nreactions at subbarrier energies. We particularly discuss theoretical\nchallenges in the coupled-channels approach, that include i) a description of\ndeep subbarrier hindrance of fusion cross sections, ii) the role of nuclear\ndissipation, iii) fusion of unstable nuclei, and iv) an interplay between\nfusion and multi-nucleon transfer processes. We also present results of a\nsemi-microscopic approach to heavy-ion fusion reactions, that combines the\ncoupled-channels approach with state-of-the-art microscopic nuclear structure\ncalculations. \n\n"}
{"id": "1704.04613", "contents": "Title: Integrating Scene Text and Visual Appearance for Fine-Grained Image\n  Classification Abstract: Text in natural images contains rich semantics that are often highly relevant\nto objects or scene. In this paper, we focus on the problem of fully exploiting\nscene text for visual understanding. The main idea is combining word\nrepresentations and deep visual features into a globally trainable deep\nconvolutional neural network. First, the recognized words are obtained by a\nscene text reading system. Then, we combine the word embedding of the\nrecognized words and the deep visual features into a single representation,\nwhich is optimized by a convolutional neural network for fine-grained image\nclassification. In our framework, the attention mechanism is adopted to reveal\nthe relevance between each recognized word and the given image, which further\nenhances the recognition performance. We have performed experiments on two\ndatasets: Con-Text dataset and Drink Bottle dataset, that are proposed for\nfine-grained classification of business places and drink bottles, respectively.\nThe experimental results consistently demonstrate that the proposed method\ncombining textual and visual cues significantly outperforms classification with\nonly visual representations. Moreover, we have shown that the learned\nrepresentation improves the retrieval performance on the drink bottle images by\na large margin, making it potentially useful in product search. \n\n"}
{"id": "1704.05831", "contents": "Title: Learning to Generate Long-term Future via Hierarchical Prediction Abstract: We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art. \n\n"}
{"id": "1704.05838", "contents": "Title: Generative Face Completion Abstract: In this paper, we propose an effective face completion algorithm using a deep\ngenerative model. Different from well-studied background completion, the face\ncompletion task is more challenging as it often requires to generate\nsemantically new pixels for the missing key components (e.g., eyes and mouths)\nthat contain large appearance variations. Unlike existing nonparametric\nalgorithms that search for patches to synthesize, our algorithm directly\ngenerates contents for missing regions based on a neural network. The model is\ntrained with a combination of a reconstruction loss, two adversarial losses and\na semantic parsing loss, which ensures pixel faithfulness and local-global\ncontents consistency. With extensive experimental results, we demonstrate\nqualitatively and quantitatively that our model is able to deal with a large\narea of missing pixels in arbitrary shapes and generate realistic face\ncompletion results. \n\n"}
{"id": "1704.06065", "contents": "Title: End-to-End Unsupervised Deformable Image Registration with a\n  Convolutional Neural Network Abstract: In this work we propose a deep learning network for deformable image\nregistration (DIRNet). The DIRNet consists of a convolutional neural network\n(ConvNet) regressor, a spatial transformer, and a resampler. The ConvNet\nanalyzes a pair of fixed and moving images and outputs parameters for the\nspatial transformer, which generates the displacement vector field that enables\nthe resampler to warp the moving image to the fixed image. The DIRNet is\ntrained end-to-end by unsupervised optimization of a similarity metric between\ninput image pairs. A trained DIRNet can be applied to perform registration on\nunseen image pairs in one pass, thus non-iteratively. Evaluation was performed\nwith registration of images of handwritten digits (MNIST) and cardiac cine MR\nscans (Sunnybrook Cardiac Data). The results demonstrate that registration with\nDIRNet is as accurate as a conventional deformable image registration method\nwith substantially shorter execution times. \n\n"}
{"id": "1704.06228", "contents": "Title: Temporal Action Detection with Structured Segment Networks Abstract: Detecting actions in untrimmed videos is an important yet challenging task.\nIn this paper, we present the structured segment network (SSN), a novel\nframework which models the temporal structure of each action instance via a\nstructured temporal pyramid. On top of the pyramid, we further introduce a\ndecomposed discriminative model comprising two classifiers, respectively for\nclassifying actions and determining completeness. This allows the framework to\neffectively distinguish positive proposals from background or incomplete ones,\nthus leading to both accurate recognition and localization. These components\nare integrated into a unified network that can be efficiently trained in an\nend-to-end fashion. Additionally, a simple yet effective temporal action\nproposal scheme, dubbed temporal actionness grouping (TAG) is devised to\ngenerate high quality action proposals. On two challenging benchmarks, THUMOS14\nand ActivityNet, our method remarkably outperforms previous state-of-the-art\nmethods, demonstrating superior accuracy and strong adaptivity in handling\nactions with various temporal structures. \n\n"}
{"id": "1704.06857", "contents": "Title: A Review on Deep Learning Techniques Applied to Semantic Segmentation Abstract: Image semantic segmentation is more and more being of interest for computer\nvision and machine learning researchers. Many applications on the rise need\naccurate and efficient segmentation mechanisms: autonomous driving, indoor\nnavigation, and even virtual or augmented reality systems to name a few. This\ndemand coincides with the rise of deep learning approaches in almost every\nfield or application target related to computer vision, including semantic\nsegmentation or scene understanding. This paper provides a review on deep\nlearning methods for semantic segmentation applied to various application\nareas. Firstly, we describe the terminology of this field as well as mandatory\nbackground concepts. Next, the main datasets and challenges are exposed to help\nresearchers decide which are the ones that best suit their needs and their\ntargets. Then, existing methods are reviewed, highlighting their contributions\nand their significance in the field. Finally, quantitative results are given\nfor the described methods and the datasets in which they were evaluated,\nfollowing up with a discussion of the results. At last, we point out a set of\npromising future works and draw our own conclusions about the state of the art\nof semantic segmentation using deep learning techniques. \n\n"}
{"id": "1704.07813", "contents": "Title: Unsupervised Learning of Depth and Ego-Motion from Video Abstract: We present an unsupervised learning framework for the task of monocular depth\nand camera motion estimation from unstructured video sequences. We achieve this\nby simultaneously training depth and camera pose estimation networks using the\ntask of view synthesis as the supervisory signal. The networks are thus coupled\nvia the view synthesis objective during training, but can be applied\nindependently at test time. Empirical evaluation on the KITTI dataset\ndemonstrates the effectiveness of our approach: 1) monocular depth performing\ncomparably with supervised methods that use either ground-truth pose or depth\nfor training, and 2) pose estimation performing favorably with established SLAM\nsystems under comparable input settings. \n\n"}
{"id": "1704.08689", "contents": "Title: A spallation-based neutron target for direct studies of neutron-induced\n  reactions in inverse kinematics Abstract: We discuss the possibility to build a neutron target for nuclear reaction\nstudies in inverse kinematics utilizing a storage ring and radioactive ion\nbeams. The proposed neutron target is a specially designed spallation target\nsurrounded by a large moderator of heavy water (D$_2$O). We present the\nresulting neutron spectra and their properties as a target. We discuss possible\nrealizations at different experimental facilities. \n\n"}
{"id": "1704.08992", "contents": "Title: A Unified Approach of Multi-scale Deep and Hand-crafted Features for\n  Defocus Estimation Abstract: In this paper, we introduce robust and synergetic hand-crafted features and a\nsimple but efficient deep feature from a convolutional neural network (CNN)\narchitecture for defocus estimation. This paper systematically analyzes the\neffectiveness of different features, and shows how each feature can compensate\nfor the weaknesses of other features when they are concatenated. For a full\ndefocus map estimation, we extract image patches on strong edges sparsely,\nafter which we use them for deep and hand-crafted feature extraction. In order\nto reduce the degree of patch-scale dependency, we also propose a multi-scale\npatch extraction strategy. A sparse defocus map is generated using a neural\nnetwork classifier followed by a probability-joint bilateral filter. The final\ndefocus map is obtained from the sparse defocus map with guidance from an\nedge-preserving filtered input image. Experimental results show that our\nalgorithm is superior to state-of-the-art algorithms in terms of defocus\nestimation. Our work can be used for applications such as segmentation, blur\nmagnification, all-in-focus image generation, and 3-D estimation. \n\n"}
{"id": "1705.00464", "contents": "Title: Speech-Based Visual Question Answering Abstract: This paper introduces speech-based visual question answering (VQA), the task\nof generating an answer given an image and a spoken question. Two methods are\nstudied: an end-to-end, deep neural network that directly uses audio waveforms\nas input versus a pipelined approach that performs ASR (Automatic Speech\nRecognition) on the question, followed by text-based visual question answering.\nFurthermore, we investigate the robustness of both methods by injecting various\nlevels of noise into the spoken question and find both methods to be tolerate\nnoise at similar levels. \n\n"}
{"id": "1705.01262", "contents": "Title: Learning to segment with image-level supervision Abstract: Deep convolutional networks have achieved the state-of-the-art for semantic\nimage segmentation tasks. However, training these networks requires access to\ndensely labeled images, which are known to be very expensive to obtain. On the\nother hand, the web provides an almost unlimited source of images annotated at\nthe image level. How can one utilize this much larger weakly annotated set for\ntasks that require dense labeling? Prior work often relied on localization\ncues, such as saliency maps, objectness priors, bounding boxes etc., to address\nthis challenging problem. In this paper, we propose a model that generates\nauxiliary labels for each image, while simultaneously forcing the output of the\nCNN to satisfy the mean-field constraints imposed by a conditional random\nfield. We show that one can enforce the CRF constraints by forcing the\ndistribution at each pixel to be close to the distribution of its neighbors.\nThis is in stark contrast with methods that compute a recursive expansion of\nthe mean-field distribution using a recurrent architecture and train the\nresultant distribution. Instead, the proposed model adds an extra loss term to\nthe output of the CNN, and hence, is faster than recursive implementations. We\nachieve the state-of-the-art for weakly supervised semantic image segmentation\non VOC 2012 dataset, assuming no manually labeled pixel level information is\navailable. Furthermore, the incorporation of conditional random fields in CNN\nincurs little extra time during training. \n\n"}
{"id": "1705.01352", "contents": "Title: Optical Flow in Mostly Rigid Scenes Abstract: The optical flow of natural scenes is a combination of the motion of the\nobserver and the independent motion of objects. Existing algorithms typically\nfocus on either recovering motion and structure under the assumption of a\npurely static world or optical flow for general unconstrained scenes. We\ncombine these approaches in an optical flow algorithm that estimates an\nexplicit segmentation of moving objects from appearance and physical\nconstraints. In static regions we take advantage of strong constraints to\njointly estimate the camera motion and the 3D structure of the scene over\nmultiple frames. This allows us to also regularize the structure instead of the\nmotion. Our formulation uses a Plane+Parallax framework, which works even under\nsmall baselines, and reduces the motion estimation to a one-dimensional search\nproblem, resulting in more accurate estimation. In moving regions the flow is\ntreated as unconstrained, and computed with an existing optical flow method.\nThe resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art\nresults on both the MPI-Sintel and KITTI-2015 benchmarks. \n\n"}
{"id": "1705.01583", "contents": "Title: VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera Abstract: We present the first real-time method to capture the full global 3D skeletal\npose of a human in a stable, temporally consistent manner using a single RGB\ncamera. Our method combines a new convolutional neural network (CNN) based pose\nregressor with kinematic skeleton fitting. Our novel fully-convolutional pose\nformulation regresses 2D and 3D joint positions jointly in real time and does\nnot require tightly cropped input frames. A real-time kinematic skeleton\nfitting method uses the CNN output to yield temporally stable 3D global pose\nreconstructions on the basis of a coherent kinematic skeleton. This makes our\napproach the first monocular RGB method usable in real-time applications such\nas 3D character control---thus far, the only monocular methods for such\napplications employed specialized RGB-D cameras. Our method's accuracy is\nquantitatively on par with the best offline 3D monocular RGB pose estimation\nmethods. Our results are qualitatively comparable to, and sometimes better\nthan, results from monocular RGB-D approaches, such as the Kinect. However, we\nshow that our approach is more broadly applicable than RGB-D solutions, i.e. it\nworks for outdoor scenes, community videos, and low quality commodity RGB\ncameras. \n\n"}
{"id": "1705.02727", "contents": "Title: Automatic Recognition of Mammal Genera on Camera-Trap Images using\n  Multi-Layer Robust Principal Component Analysis and Mixture Neural Networks Abstract: The segmentation and classification of animals from camera-trap images is due\nto the conditions under which the images are taken, a difficult task. This work\npresents a method for classifying and segmenting mammal genera from camera-trap\nimages. Our method uses Multi-Layer Robust Principal Component Analysis (RPCA)\nfor segmenting, Convolutional Neural Networks (CNNs) for extracting features,\nLeast Absolute Shrinkage and Selection Operator (LASSO) for selecting features,\nand Artificial Neural Networks (ANNs) or Support Vector Machines (SVM) for\nclassifying mammal genera present in the Colombian forest. We evaluated our\nmethod with the camera-trap images from the Alexander von Humboldt Biological\nResources Research Institute. We obtained an accuracy of 92.65% classifying 8\nmammal genera and a False Positive (FP) class, using automatic-segmented\nimages. On the other hand, we reached 90.32% of accuracy classifying 10 mammal\ngenera, using ground-truth images only. Unlike almost all previous works, we\nconfront the animal segmentation and genera classification in the camera-trap\nrecognition. This method shows a new approach toward a fully-automatic\ndetection of animals from camera-trap images. \n\n"}
{"id": "1705.03239", "contents": "Title: Convolutional Dictionary Learning via Local Processing Abstract: Convolutional Sparse Coding (CSC) is an increasingly popular model in the\nsignal and image processing communities, tackling some of the limitations of\ntraditional patch-based sparse representations. Although several works have\naddressed the dictionary learning problem under this model, these relied on an\nADMM formulation in the Fourier domain, losing the sense of locality and the\nrelation to the traditional patch-based sparse pursuit. A recent work suggested\na novel theoretical analysis of this global model, providing guarantees that\nrely on a localized sparsity measure. Herein, we extend this local-global\nrelation by showing how one can efficiently solve the convolutional sparse\npursuit problem and train the filters involved, while operating locally on\nimage patches. Our approach provides an intuitive algorithm that can leverage\nstandard techniques from the sparse representations field. The proposed method\nis fast to train, simple to implement, and flexible enough that it can be\neasily deployed in a variety of applications. We demonstrate the proposed\ntraining scheme for image inpainting and image separation, while achieving\nstate-of-the-art results. \n\n"}
{"id": "1705.07356", "contents": "Title: Structural Compression of Convolutional Neural Networks Abstract: Deep convolutional neural networks (CNNs) have been successful in many tasks\nin machine vision, however, millions of weights in the form of thousands of\nconvolutional filters in CNNs makes them difficult for human intepretation or\nunderstanding in science. In this article, we introduce CAR, a greedy\nstructural compression scheme to obtain smaller and more interpretable CNNs,\nwhile achieving close to original accuracy. The compression is based on pruning\nfilters with the least contribution to the classification accuracy. We\ndemonstrate the interpretability of CAR-compressed CNNs by showing that our\nalgorithm prunes filters with visually redundant functionalities such as color\nfilters. These compressed networks are easier to interpret because they retain\nthe filter diversity of uncompressed networks with order of magnitude less\nfilters. Finally, a variant of CAR is introduced to quantify the importance of\neach image category to each CNN filter. Specifically, the most and the least\nimportant class labels are shown to be meaningful interpretations of each\nfilter. \n\n"}
{"id": "1705.08199", "contents": "Title: A kinetic regime of hydrodynamic fluctuations and long time tails for a\n  Bjorken expansion Abstract: We develop a set of kinetic equations for hydrodynamic fluctuations which are\nequivalent to nonlinear hydrodynamics with noise. The hydro-kinetic equations\ncan be coupled to existing second order hydrodynamic codes to incorporate the\nphysics of these fluctuations, which become dominant near the critical point.\nWe use the hydro-kinetic equations to calculate the modifications of energy\nmomentum tensor by thermal fluctuations from the earliest moments and at late\ntimes in Bjorken expansion. The solution to the kinetic equations precisely\ndetermines the coefficient of the first fractional power ($\\propto\n\\tau^{-3/2}$) in the energy momentum tensor gradient expansion. Numerically, we\nfind that the contribution to the longitudinal pressure from hydrodynamic\nfluctuations is comparable to the second order hydrodynamic terms for typical\nmedium parameters used to simulate heavy ion collisions. \n\n"}
{"id": "1705.08214", "contents": "Title: Ridiculously Fast Shot Boundary Detection with Fully Convolutional\n  Neural Networks Abstract: Shot boundary detection (SBD) is an important component of many video\nanalysis tasks, such as action recognition, video indexing, summarization and\nediting. Previous work typically used a combination of low-level features like\ncolor histograms, in conjunction with simple models such as SVMs. Instead, we\npropose to learn shot detection end-to-end, from pixels to final shot\nboundaries. For training such a model, we rely on our insight that all shot\nboundaries are generated. Thus, we create a dataset with one million frames and\nautomatically generated transitions such as cuts, dissolves and fades. In order\nto efficiently analyze hours of videos, we propose a Convolutional Neural\nNetwork (CNN) which is fully convolutional in time, thus allowing to use a\nlarge temporal context without the need to repeatedly processing frames. With\nthis architecture our method obtains state-of-the-art results while running at\nan unprecedented speed of more than 120x real-time. \n\n"}
{"id": "1705.08850", "contents": "Title: Semi-supervised Learning with GANs: Manifold Invariance with Improved\n  Inference Abstract: Semi-supervised learning methods using Generative Adversarial Networks (GANs)\nhave shown promising empirical success recently. Most of these methods use a\nshared discriminator/classifier which discriminates real examples from fake\nwhile also predicting the class label. Motivated by the ability of the GANs\ngenerator to capture the data manifold well, we propose to estimate the tangent\nspace to the data manifold using GANs and employ it to inject invariances into\nthe classifier. In the process, we propose enhancements over existing methods\nfor learning the inverse mapping (i.e., the encoder) which greatly improves in\nterms of semantic similarity of the reconstructed sample with the input sample.\nWe observe considerable empirical gains in semi-supervised learning over\nbaselines, particularly in the cases when the number of labeled examples is\nlow. We also provide insights into how fake examples influence the\nsemi-supervised learning procedure. \n\n"}
{"id": "1705.09283", "contents": "Title: GXNOR-Net: Training deep neural networks with ternary weights and\n  activations without full-precision memory under a unified discretization\n  framework Abstract: There is a pressing need to build an architecture that could subsume these\nnetworks under a unified framework that achieves both higher performance and\nless overhead. To this end, two fundamental issues are yet to be addressed. The\nfirst one is how to implement the back propagation when neuronal activations\nare discrete. The second one is how to remove the full-precision hidden weights\nin the training phase to break the bottlenecks of memory/computation\nconsumption. To address the first issue, we present a multi-step neuronal\nactivation discretization method and a derivative approximation technique that\nenable the implementing the back propagation algorithm on discrete DNNs. While\nfor the second issue, we propose a discrete state transition (DST) methodology\nto constrain the weights in a discrete space without saving the hidden weights.\nThrough this way, we build a unified framework that subsumes the binary or\nternary networks as its special cases, and under which a heuristic algorithm is\nprovided at the website https://github.com/AcrossV/Gated-XNOR. More\nparticularly, we find that when both the weights and activations become ternary\nvalues, the DNNs can be reduced to sparse binary networks, termed as gated XNOR\nnetworks (GXNOR-Nets) since only the event of non-zero weight and non-zero\nactivation enables the control gate to start the XNOR logic operations in the\noriginal binary networks. This promises the event-driven hardware design for\nefficient mobile intelligence. We achieve advanced performance compared with\nstate-of-the-art algorithms. Furthermore, the computational sparsity and the\nnumber of states in the discrete space can be flexibly modified to make it\nsuitable for various hardware platforms. \n\n"}
{"id": "1705.10816", "contents": "Title: CUORE Sensitivity to $0\\nu\\beta\\beta$ Decay Abstract: We report a study of the CUORE sensitivity to neutrinoless double beta\n($0\\nu\\beta\\beta$) decay. We used a Bayesian analysis based on a toy Monte\nCarlo (MC) approach to extract the exclusion sensitivity to the\n$0\\nu\\beta\\beta$ decay half-life ($T_{1/2}^{0\\nu}$) at $90\\%$ credibility\ninterval (CI) -- i.e. the interval containing the true value of\n$T_{1/2}^{0\\nu}$ with $90\\%$ probability -- and the $3 \\sigma$ discovery\nsensitivity. We consider various background levels and energy resolutions, and\ndescribe the influence of the data division in subsets with different\nbackground levels. If the background level and the energy resolution meet the\nexpectation, CUORE will reach a $90\\%$ CI exclusion sensitivity of\n$2\\cdot10^{25}$ yr with $3$ months, and $9\\cdot10^{25}$ yr with $5$ years of\nlive time. Under the same conditions, the discovery sensitivity after $3$\nmonths and $5$ years will be $7\\cdot10^{24}$ yr and $4\\cdot10^{25}$ yr,\nrespectively. \n\n"}
{"id": "1706.00530", "contents": "Title: Integrated Deep and Shallow Networks for Salient Object Detection Abstract: Deep convolutional neural network (CNN) based salient object detection\nmethods have achieved state-of-the-art performance and outperform those\nunsupervised methods with a wide margin. In this paper, we propose to integrate\ndeep and unsupervised saliency for salient object detection under a unified\nframework. Specifically, our method takes results of unsupervised saliency\n(Robust Background Detection, RBD) and normalized color images as inputs, and\ndirectly learns an end-to-end mapping between inputs and the corresponding\nsaliency maps. The color images are fed into a Fully Convolutional Neural\nNetworks (FCNN) adapted from semantic segmentation to exploit high-level\nsemantic cues for salient object detection. Then the results from deep FCNN and\nRBD are concatenated to feed into a shallow network to map the concatenated\nfeature maps to saliency maps. Finally, to obtain a spatially consistent\nsaliency map with sharp object boundaries, we fuse superpixel level saliency\nmap at multi-scale. Extensive experimental results on 8 benchmark datasets\ndemonstrate that the proposed method outperforms the state-of-the-art\napproaches with a margin. \n\n"}
{"id": "1706.01340", "contents": "Title: Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor Abstract: Using supporting backchannel (BC) cues can make human-computer interaction\nmore social. BCs provide a feedback from the listener to the speaker indicating\nto the speaker that he is still listened to. BCs can be expressed in different\nways, depending on the modality of the interaction, for example as gestures or\nacoustic cues. In this work, we only considered acoustic cues. We are proposing\nan approach towards detecting BC opportunities based on acoustic input features\nlike power and pitch. While other works in the field rely on the use of a\nhand-written rule set or specialized features, we made use of artificial neural\nnetworks. They are capable of deriving higher order features from input\nfeatures themselves. In our setup, we first used a fully connected feed-forward\nnetwork to establish an updated baseline in comparison to our previously\nproposed setup. We also extended this setup by the use of Long Short-Term\nMemory (LSTM) networks which have shown to outperform feed-forward based setups\non various tasks. Our best system achieved an F1-Score of 0.37 using power and\npitch features. Adding linguistic information using word2vec, the score\nincreased to 0.39. \n\n"}
{"id": "1706.02417", "contents": "Title: Evaluating (and improving) the correspondence between deep neural\n  networks and human representations Abstract: Decades of psychological research have been aimed at modeling how people\nlearn features and categories. The empirical validation of these theories is\noften based on artificial stimuli with simple representations. Recently, deep\nneural networks have reached or surpassed human accuracy on tasks such as\nidentifying objects in natural images. These networks learn representations of\nreal-world stimuli that can potentially be leveraged to capture psychological\nrepresentations. We find that state-of-the-art object classification networks\nprovide surprisingly accurate predictions of human similarity judgments for\nnatural images, but fail to capture some of the structure represented by\npeople. We show that a simple transformation that corrects these discrepancies\ncan be obtained through convex optimization. We use the resulting\nrepresentations to predict the difficulty of learning novel categories of\nnatural images. Our results extend the scope of psychological experiments and\ncomputational modeling by enabling tractable use of large natural stimulus\nsets. \n\n"}
{"id": "1706.02747", "contents": "Title: Prospect for Measuring ${G_E^n}$ at High Momentum Transfers Abstract: Experiment E02-013, approved by PAC21, will measure the neutron electric form\nfactor at \\qsq~up to 3.4 \\gvsq, which is twice that achieved to date. The main\nfeatures of the new experiment will be the use of the electron spectrometer\nBigBite, a large array of neutron detectors, and a polarized \\he3~target. We\npresent the parameters and optimization of the experimental setup. A concept of\nan experiment for \\GEN~where precision \\GEP~data is used for calibration of the\nsystematics of a Rosenbluth type measurement is also discussed. A concept of\nthe circulating gas flow in the polarized He$^3$ target is presented. \n\n"}
{"id": "1706.03646", "contents": "Title: Point Linking Network for Object Detection Abstract: Object detection is a core problem in computer vision. With the development\nof deep ConvNets, the performance of object detectors has been dramatically\nimproved. The deep ConvNets based object detectors mainly focus on regressing\nthe coordinates of bounding box, e.g., Faster-R-CNN, YOLO and SSD. Different\nfrom these methods that considering bounding box as a whole, we propose a novel\nobject bounding box representation using points and links and implemented using\ndeep ConvNets, termed as Point Linking Network (PLN). Specifically, we regress\nthe corner/center points of bounding-box and their links using a fully\nconvolutional network; then we map the corner points and their links back to\nmultiple bounding boxes; finally an object detection result is obtained by\nfusing the multiple bounding boxes. PLN is naturally robust to object occlusion\nand flexible to object scale variation and aspect ratio variation. In the\nexperiments, PLN with the Inception-v2 model achieves state-of-the-art\nsingle-model and single-scale results on the PASCAL VOC 2007, the PASCAL VOC\n2012 and the COCO detection benchmarks without bells and whistles. The source\ncode will be released. \n\n"}
{"id": "1706.04261", "contents": "Title: The \"something something\" video database for learning and evaluating\n  visual common sense Abstract: Neural networks trained on datasets such as ImageNet have led to major\nadvances in visual object classification. One obstacle that prevents networks\nfrom reasoning more deeply about complex scenes and situations, and from\nintegrating visual knowledge with natural language, like humans do, is their\nlack of common sense knowledge about the physical world. Videos, unlike still\nimages, contain a wealth of detailed information about the physical world.\nHowever, most labelled video datasets represent high-level concepts rather than\ndetailed physical aspects about actions and scenes. In this work, we describe\nour ongoing collection of the \"something-something\" database of video\nprediction tasks whose solutions require a common sense understanding of the\ndepicted situation. The database currently contains more than 100,000 videos\nacross 174 classes, which are defined as caption-templates. We also describe\nthe challenges in crowd-sourcing this data at scale. \n\n"}
{"id": "1706.04737", "contents": "Title: Suggestive Annotation: A Deep Active Learning Framework for Biomedical\n  Image Segmentation Abstract: Image segmentation is a fundamental problem in biomedical image analysis.\nRecent advances in deep learning have achieved promising results on many\nbiomedical image segmentation benchmarks. However, due to large variations in\nbiomedical images (different modalities, image settings, objects, noise, etc),\nto utilize deep learning on a new application, it usually needs a new set of\ntraining data. This can incur a great deal of annotation effort and cost,\nbecause only biomedical experts can annotate effectively, and often there are\ntoo many instances in images (e.g., cells) to annotate. In this paper, we aim\nto address the following question: With limited effort (e.g., time) for\nannotation, what instances should be annotated in order to attain the best\nperformance? We present a deep active learning framework that combines fully\nconvolutional network (FCN) and active learning to significantly reduce\nannotation effort by making judicious suggestions on the most effective\nannotation areas. We utilize uncertainty and similarity information provided by\nFCN and formulate a generalized version of the maximum set cover problem to\ndetermine the most representative and uncertain areas for annotation. Extensive\nexperiments using the 2015 MICCAI Gland Challenge dataset and a lymph node\nultrasound image segmentation dataset show that, using annotation suggestions\nby our method, state-of-the-art segmentation performance can be achieved by\nusing only 50% of training data. \n\n"}
{"id": "1706.05157", "contents": "Title: A Fully Trainable Network with RNN-based Pooling Abstract: Pooling is an important component in convolutional neural networks (CNNs) for\naggregating features and reducing computational burden. Compared with other\ncomponents such as convolutional layers and fully connected layers which are\ncompletely learned from data, the pooling component is still handcrafted such\nas max pooling and average pooling. This paper proposes a learnable pooling\nfunction using recurrent neural networks (RNN) so that the pooling can be fully\nadapted to data and other components of the network, leading to an improved\nperformance. Such a network with learnable pooling function is referred to as a\nfully trainable network (FTN). Experimental results have demonstrated that the\nproposed RNN-based pooling can well approximate the existing pooling functions\nand improve the performance of the network. Especially for small networks, the\nproposed FTN can improve the performance by seven percentage points in terms of\nerror rate on the CIFAR-10 dataset compared with the traditional CNN. \n\n"}
{"id": "1706.05202", "contents": "Title: Investigation of pion-induced $f_1(1285)$ production off a nucleon\n  target within an interpolating Reggeized approach Abstract: In this work, the pion-induced $f_1(1285)$ production off a nucleon target is\ninvestigated in an effective Lagrangian approach with an interpolating\nReggeized treatment in a large range of the pion-beam momentum from threshold\nup to several tens of GeV. The $s$-channel, $u$-channel, and $t $-channel Born\nterms are included to calculate production cross sections. An interpolating\nReggeized treatment is applied to the $t$ channel, which is found to be\nimportant to reproduce the behavior of the existent experimental total cross\nsections at both low ($\\lesssim$ 8 GeV) and high pion-beam momenta ($\\gtrsim$ 8\nGeV). It is found that the $t$-channel contribution is dominant in the\npion-induced $f_1(1285)$ production at low beam momentum and still dominant at\nvery forward angles at high momentum. The interpolated Reggeized treatment of\nthe $u$ channel is also discussed. The $u$-channel contribution is small and\nnegligible at low momentum, and it becomes dominant at backward angles at\nmomenta higher than 10 GeV. The differential cross sections are predicted with\nthe model fixed by the fitting existent experimental data. The results are\nhelpful to the possible experiments at J-PARC and COMPASS. \n\n"}
{"id": "1706.05507", "contents": "Title: Variants of RMSProp and Adagrad with Logarithmic Regret Bounds Abstract: Adaptive gradient methods have become recently very popular, in particular as\nthey have been shown to be useful in the training of deep neural networks. In\nthis paper we have analyzed RMSProp, originally proposed for the training of\ndeep neural networks, in the context of online convex optimization and show\n$\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and\nSC-RMSProp for which we show logarithmic regret bounds for strongly convex\nfunctions. Finally, we demonstrate in the experiments that these new variants\noutperform other adaptive gradient techniques or stochastic gradient descent in\nthe optimization of strongly convex functions as well as in training of deep\nneural networks. \n\n"}
{"id": "1706.06012", "contents": "Title: Radiopurity assessment of the energy readout for the NEXT double beta\n  decay experiment Abstract: The Neutrino Experiment with a Xenon Time-Projection Chamber (NEXT)\nexperiment intends to investigate the neutrinoless double beta decay of 136Xe,\nand therefore requires a severe suppression of potential backgrounds. An\nextensive material screening and selection process was undertaken to quantify\nthe radioactivity of the materials used in the experiment. Separate energy and\ntracking readout planes using different sensors allow us to combine the\nmeasurement of the topological signature of the event for background\ndiscrimination with the energy resolution optimization. The design of radiopure\nreadout planes, in direct contact with the gas detector medium, was especially\nchallenging since the required components typically have activities too large\nfor experiments demanding ultra-low background conditions. After studying the\ntracking plane, here the radiopurity control of the energy plane is presented,\nmainly based on gamma-ray spectroscopy using ultra-low background germanium\ndetectors at the Laboratorio Subterr\\'aneo de Canfranc (Spain). All the\navailable units of the selected model of photomultiplier have been screened\ntogether with most of the components for the bases, enclosures and windows.\nAccording to these results for the activity of the relevant radioisotopes, the\nselected components of the energy plane would give a contribution to the\noverall background level in the region of interest of at most 2.4 x 10-4 counts\nkeV-1 kg-1 y-1, satisfying the sensitivity requirements of the NEXT experiment. \n\n"}
{"id": "1706.07515", "contents": "Title: Comparing Neural and Attractiveness-based Visual Features for Artwork\n  Recommendation Abstract: Advances in image processing and computer vision in the latest years have\nbrought about the use of visual features in artwork recommendation. Recent\nworks have shown that visual features obtained from pre-trained deep neural\nnetworks (DNNs) perform very well for recommending digital art. Other recent\nworks have shown that explicit visual features (EVF) based on attractiveness\ncan perform well in preference prediction tasks, but no previous work has\ncompared DNN features versus specific attractiveness-based visual features\n(e.g. brightness, texture) in terms of recommendation performance. In this\nwork, we study and compare the performance of DNN and EVF features for the\npurpose of physical artwork recommendation using transactional data from\nUGallery, an online store of physical paintings. In addition, we perform an\nexploratory analysis to understand if DNN embedded features have some relation\nwith certain EVF. Our results show that DNN features outperform EVF, that\ncertain EVF features are more suited for physical artwork recommendation and,\nfinally, we show evidence that certain neurons in the DNN might be partially\nencoding visual features such as brightness, providing an opportunity for\nexplaining recommendations based on visual neural models. \n\n"}
{"id": "1706.07680", "contents": "Title: Training Adversarial Discriminators for Cross-channel Abnormal Event\n  Detection in Crowds Abstract: Abnormal crowd behaviour detection attracts a large interest due to its\nimportance in video surveillance scenarios. However, the ambiguity and the lack\nof sufficient abnormal ground truth data makes end-to-end training of large\ndeep networks hard in this domain. In this paper we propose to use Generative\nAdversarial Nets (GANs), which are trained to generate only the normal\ndistribution of the data. During the adversarial GAN training, a discriminator\n(D) is used as a supervisor for the generator network (G) and vice versa. At\ntesting time we use D to solve our discriminative task (abnormality detection),\nwhere D has been trained without the need of manually-annotated abnormal data.\nMoreover, in order to prevent G learn a trivial identity function, we use a\ncross-channel approach, forcing G to transform raw-pixel data in motion\ninformation and vice versa. The quantitative results on standard benchmarks\nshow that our method outperforms previous state-of-the-art methods in both the\nframe-level and the pixel-level evaluation. \n\n"}
{"id": "1706.07795", "contents": "Title: Upgrade of the ultracold neutron source at the pulsed reactor TRIGA\n  Mainz Abstract: The performance of the upgraded solid deuterium ultracold neutron source at\nthe pulsed reactor TRIGA Mainz is described. The current configuration stage\ncomprises the installation of a He liquefier to run UCN experiments over\nlong-term periods, the use of stainless steel neutron guides with improved\ntransmission as well as sputter-coated non-magnetic $^{58}$NiMo alloy at the\ninside walls of the thermal bridge and the converter cup. The UCN yield was\nmeasured in a `standard' UCN storage bottle (stainless steel) with a volume of\n32 litres outside the biological shield at the experimental area yielding UCN\ndensities of 8.5 /cm$^3$; an increase by a factor of 3.5 compared to the former\nsetup. The measured UCN storage curve is in good agreement with the predictions\nfrom a Monte Carlo simulation developed to model the source. The growth and\nformation of the solid deuterium converter during freeze-out are affected by\nthe ortho/para ratio of the H$_2$ premoderator. \n\n"}
{"id": "1706.08088", "contents": "Title: Efficient and accurate monitoring of the depth information in a Wireless\n  Multimedia Sensor Network based surveillance Abstract: Wireless Multimedia Sensor Network (WMSN) is a promising technology capturing\nrich multimedia data like audio and video, which can be useful to monitor an\nenvironment under surveillance. However, many scenarios in real time monitoring\nrequires 3D depth information. In this research work, we propose to use the\ndisparity map that is computed from two or multiple images, in order to monitor\nthe depth information in an object or event under surveillance using WMSN. Our\nsystem is based on distributed wireless sensors allowing us to notably reduce\nthe computational time needed for 3D depth reconstruction, thus permitting the\nsuccess of real time solutions. Each pair of sensors will capture images for a\ntargeted place/object and will operate a Stereo Matching in order to create a\nDisparity Map. Disparity maps will give us the ability to decrease traffic on\nthe bandwidth, because they are of low size. This will increase WMSN lifetime.\nAny event can be detected after computing the depth value for the target object\nin the scene, and also 3D scene reconstruction can be achieved with a disparity\nmap and some reference(s) image(s) taken by the node(s). \n\n"}
{"id": "1706.09262", "contents": "Title: Hierarchical Attentive Recurrent Tracking Abstract: Class-agnostic object tracking is particularly difficult in cluttered\nenvironments as target specific discriminative models cannot be learned a\npriori. Inspired by how the human visual cortex employs spatial attention and\nseparate \"where\" and \"what\" processing pathways to actively suppress irrelevant\nvisual features, this work develops a hierarchical attentive recurrent model\nfor single object tracking in videos. The first layer of attention discards the\nmajority of background by selecting a region containing the object of interest,\nwhile the subsequent layers tune in on visual features particular to the\ntracked object. This framework is fully differentiable and can be trained in a\npurely data driven fashion by gradient methods. To improve training\nconvergence, we augment the loss function with terms for a number of auxiliary\ntasks relevant for tracking. Evaluation of the proposed model is performed on\ntwo datasets: pedestrian tracking on the KTH activity recognition dataset and\nthe more difficult KITTI object tracking dataset. \n\n"}
{"id": "1706.09302", "contents": "Title: Deep Learning Based Large-Scale Automatic Satellite Crosswalk\n  Classification Abstract: High-resolution satellite imagery have been increasingly used on remote\nsensing classification problems. One of the main factors is the availability of\nthis kind of data. Even though, very little effort has been placed on the zebra\ncrossing classification problem. In this letter, crowdsourcing systems are\nexploited in order to enable the automatic acquisition and annotation of a\nlarge-scale satellite imagery database for crosswalks related tasks. Then, this\ndataset is used to train deep-learning-based models in order to accurately\nclassify satellite images that contains or not zebra crossings. A novel dataset\nwith more than 240,000 images from 3 continents, 9 countries and more than 20\ncities was used in the experiments. Experimental results showed that freely\navailable crowdsourcing data can be used to accurately (97.11%) train robust\nmodels to perform crosswalk classification on a global scale. \n\n"}
{"id": "1706.09990", "contents": "Title: The SeaQuest Spectrometer at Fermilab Abstract: The SeaQuest spectrometer at Fermilab was designed to detect\noppositely-charged pairs of muons (dimuons) produced by interactions between a\n120 GeV proton beam and liquid hydrogen, liquid deuterium and solid nuclear\ntargets. The primary physics program uses the Drell-Yan process to probe\nantiquark distributions in the target nucleon. The spectrometer consists of a\ntarget system, two dipole magnets and four detector stations. The upstream\nmagnet is a closed-aperture solid iron magnet which also serves as the beam\ndump, while the second magnet is an open aperture magnet. Each of the detector\nstations consists of scintillator hodoscopes and a high-resolution tracking\ndevice. The FPGA-based trigger compares the hodoscope signals to a set of\npre-programmed roads to determine if the event contains oppositely-signed,\nhigh-mass muon pairs. \n\n"}
{"id": "1707.00058", "contents": "Title: Multiple VLAD encoding of CNNs for image classification Abstract: Despite the effectiveness of convolutional neural networks (CNNs) especially\nin image classification tasks, the effect of convolution features on learned\nrepresentations is still limited. It mostly focuses on the salient object of\nthe images, but ignores the variation information on clutter and local. In this\npaper, we propose a special framework, which is the multiple VLAD encoding\nmethod with the CNNs features for image classification. Furthermore, in order\nto improve the performance of the VLAD coding method, we explore the\nmultiplicity of VLAD encoding with the extension of three kinds of encoding\nalgorithms, which are the VLAD-SA method, the VLAD-LSA and the VLAD-LLC method.\nFinally, we equip the spatial pyramid patch (SPM) on VLAD encoding to add the\nspatial information of CNNs feature. In particular, the power of SPM leads our\nframework to yield better performance compared to the existing method. \n\n"}
{"id": "1707.00095", "contents": "Title: Exploring the Imposition of Synaptic Precision Restrictions For\n  Evolutionary Synthesis of Deep Neural Networks Abstract: A key contributing factor to incredible success of deep neural networks has\nbeen the significant rise on massively parallel computing devices allowing\nresearchers to greatly increase the size and depth of deep neural networks,\nleading to significant improvements in modeling accuracy. Although deeper,\nlarger, or complex deep neural networks have shown considerable promise, the\ncomputational complexity of such networks is a major barrier to utilization in\nresource-starved scenarios. We explore the synaptogenesis of deep neural\nnetworks in the formation of efficient deep neural network architectures within\nan evolutionary deep intelligence framework, where a probabilistic generative\nmodeling strategy is introduced to stochastically synthesize increasingly\nefficient yet effective offspring deep neural networks over generations,\nmimicking evolutionary processes such as heredity, random mutation, and natural\nselection in a probabilistic manner. In this study, we primarily explore the\nimposition of synaptic precision restrictions and its impact on the\nevolutionary synthesis of deep neural networks to synthesize more efficient\nnetwork architectures tailored for resource-starved scenarios. Experimental\nresults show significant improvements in synaptic efficiency (~10X decrease for\nGoogLeNet-based DetectNet) and inference speed (>5X increase for\nGoogLeNet-based DetectNet) while preserving modeling accuracy. \n\n"}
{"id": "1707.00893", "contents": "Title: Optimization Beyond the Convolution: Generalizing Spatial Relations with\n  End-to-End Metric Learning Abstract: To operate intelligently in domestic environments, robots require the ability\nto understand arbitrary spatial relations between objects and to generalize\nthem to objects of varying sizes and shapes. In this work, we present a novel\nend-to-end approach to generalize spatial relations based on distance metric\nlearning. We train a neural network to transform 3D point clouds of objects to\na metric space that captures the similarity of the depicted spatial relations,\nusing only geometric models of the objects. Our approach employs gradient-based\noptimization to compute object poses in order to imitate an arbitrary target\nrelation by reducing the distance to it under the learned metric. Our results\nbased on simulated and real-world experiments show that the proposed method\nenables robots to generalize spatial relations to unknown objects over a\ncontinuous spectrum. \n\n"}
{"id": "1707.01243", "contents": "Title: Exploration of object recognition from 3D point cloud Abstract: We present our latest experiment results of object recognition from 3D point\ncloud data collected through moving car. \n\n"}
{"id": "1707.01700", "contents": "Title: CNN features are also great at unsupervised classification Abstract: This paper aims at providing insight on the transferability of deep CNN\nfeatures to unsupervised problems. We study the impact of different pretrained\nCNN feature extractors on the problem of image set clustering for object\nclassification as well as fine-grained classification. We propose a rather\nstraightforward pipeline combining deep-feature extraction using a CNN\npretrained on ImageNet and a classic clustering algorithm to classify sets of\nimages. This approach is compared to state-of-the-art algorithms in\nimage-clustering and provides better results. These results strengthen the\nbelief that supervised training of deep CNN on large datasets, with a large\nvariability of classes, extracts better features than most carefully designed\nengineering approaches, even for unsupervised tasks. We also validate our\napproach on a robotic application, consisting in sorting and storing objects\nsmartly based on clustering. \n\n"}
{"id": "1707.04318", "contents": "Title: Discriminative Optimization: Theory and Applications to Computer Vision\n  Problems Abstract: Many computer vision problems are formulated as the optimization of a cost\nfunction. This approach faces two main challenges: (i) designing a cost\nfunction with a local optimum at an acceptable solution, and (ii) developing an\nefficient numerical method to search for one (or multiple) of these local\noptima. While designing such functions is feasible in the noiseless case, the\nstability and location of local optima are mostly unknown under noise,\nocclusion, or missing data. In practice, this can result in undesirable local\noptima or not having a local optimum in the expected place. On the other hand,\nnumerical optimization algorithms in high-dimensional spaces are typically\nlocal and often rely on expensive first or second order information to guide\nthe search. To overcome these limitations, this paper proposes Discriminative\nOptimization (DO), a method that learns search directions from data without the\nneed of a cost function. Specifically, DO explicitly learns a sequence of\nupdates in the search space that leads to stationary points that correspond to\ndesired solutions. We provide a formal analysis of DO and illustrate its\nbenefits in the problem of 3D point cloud registration, camera pose estimation,\nand image denoising. We show that DO performed comparably or outperformed\nstate-of-the-art algorithms in terms of accuracy, robustness to perturbations,\nand computational efficiency. \n\n"}
{"id": "1707.04771", "contents": "Title: Original Loop-closure Detection Algorithm for Monocular vSLAM Abstract: Vision-based simultaneous localization and mapping (vSLAM) is a\nwell-established problem in mobile robotics and monocular vSLAM is one of the\nmost challenging variations of that problem nowadays. In this work we study one\nof the core post-processing optimization mechanisms in vSLAM, e.g. loop-closure\ndetection. We analyze the existing methods and propose original algorithm for\nloop-closure detection, which is suitable for dense, semi-dense and\nfeature-based vSLAM methods. We evaluate the algorithm experimentally and show\nthat it contribute to more accurate mapping while speeding up the monocular\nvSLAM pipeline to the extent the latter can be used in real-time for\ncontrolling small multi-rotor vehicle (drone). \n\n"}
{"id": "1707.04931", "contents": "Title: Pathological OCT Retinal Layer Segmentation using Branch Residual\n  U-shape Networks Abstract: The automatic segmentation of retinal layer structures enables\nclinically-relevant quantification and monitoring of eye disorders over time in\nOCT imaging. Eyes with late-stage diseases are particularly challenging to\nsegment, as their shape is highly warped due to pathological biomarkers. In\nthis context, we propose a novel fully Convolutional Neural Network (CNN)\narchitecture which combines dilated residual blocks in an asymmetric U-shape\nconfiguration, and can segment multiple layers of highly pathological eyes in\none shot. We validate our approach on a dataset of late-stage AMD patients and\ndemonstrate lower computational costs and higher performance compared to other\nstate-of-the-art methods. \n\n"}
{"id": "1707.04960", "contents": "Title: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory\n  Network Based Approach Abstract: Recent years have witnessed a resurgence of interest in video summarization.\nHowever, one of the main obstacles to the research on video summarization is\nthe user subjectivity - users have various preferences over the summaries. The\nsubjectiveness causes at least two problems. First, no single video summarizer\nfits all users unless it interacts with and adapts to the individual users.\nSecond, it is very challenging to evaluate the performance of a video\nsummarizer.\n  To tackle the first problem, we explore the recently proposed query-focused\nvideo summarization which introduces user preferences in the form of text\nqueries about the video into the summarization process. We propose a memory\nnetwork parameterized sequential determinantal point process in order to attend\nthe user query onto different video frames and shots. To address the second\nchallenge, we contend that a good evaluation metric for video summarization\nshould focus on the semantic information that humans can perceive rather than\nthe visual features or temporal overlaps. To this end, we collect dense\nper-video-shot concept annotations, compile a new dataset, and suggest an\nefficient evaluation method defined upon the concept annotations. We conduct\nextensive experiments contrasting our video summarizer to existing ones and\npresent detailed analyses about the dataset and the new evaluation method. \n\n"}
{"id": "1707.05414", "contents": "Title: Wide Inference Network for Image Denoising via Learning\n  Pixel-distribution Prior Abstract: We explore an innovative strategy for image denoising by using convolutional\nneural networks (CNN) to learn similar pixel-distribution features from noisy\nimages. Many types of image noise follow a certain pixel-distribution in\ncommon, such as additive white Gaussian noise (AWGN). By increasing CNN's width\nwith larger reception fields and more channels in each layer, CNNs can reveal\nthe ability to extract more accurate pixel-distribution features. The key to\nour approach is a discovery that wider CNNs with more convolutions tend to\nlearn the similar pixel-distribution features, which reveals a new strategy to\nsolve low-level vision problems effectively that the inference mapping\nprimarily relies on the priors behind the noise property instead of deeper CNNs\nwith more stacked nonlinear layers. We evaluate our work, Wide inference\nNetworks (WIN), on AWGN and demonstrate that by learning pixel-distribution\nfeatures from images, WIN-based network consistently achieves significantly\nbetter performance than current state-of-the-art deep CNN-based methods in both\nquantitative and visual evaluations. \\textit{Code and models are available at\n\\url{https://github.com/cswin/WIN}}. \n\n"}
{"id": "1707.05847", "contents": "Title: The Devil is in the Decoder: Classification, Regression and GANs Abstract: Many machine vision applications, such as semantic segmentation and depth\nprediction, require predictions for every pixel of the input image. Models for\nsuch problems usually consist of encoders which decrease spatial resolution\nwhile learning a high-dimensional representation, followed by decoders who\nrecover the original input resolution and result in low-dimensional\npredictions. While encoders have been studied rigorously, relatively few\nstudies address the decoder side. This paper presents an extensive comparison\nof a variety of decoders for a variety of pixel-wise tasks ranging from\nclassification, regression to synthesis. Our contributions are: (1) Decoders\nmatter: we observe significant variance in results between different types of\ndecoders on various problems. (2) We introduce new residual-like connections\nfor decoders. (3) We introduce a novel decoder: bilinear additive upsampling.\n(4) We explore prediction artifacts. \n\n"}
{"id": "1707.05950", "contents": "Title: Image Projective Invariants Abstract: In this paper, we propose relative projective differential invariants (RPDIs)\nwhich are invariant to general projective transformations. By using RPDIs and\nthe structural frame of integral invariant, projective weighted moment\ninvariants (PIs) can be constructed very easily. It is first proved that a kind\nof projective invariants exists in terms of weighted integration of images,\nwith relative differential invariants as the weight functions. Then, some\nsimple instances of PIs are given. In order to ensure the stability and\ndiscriminability of PIs, we discuss how to calculate partial derivatives of\ndiscrete images more accurately. Since the number of pixels in discrete images\nbefore and after the geometric transformation may be different, we design the\nmethod to normalize the number of pixels. These ways enhance the performance of\nPIs. Finally, we carry out some experiments based on synthetic and real image\ndatasets. We choose commonly used moment invariants for comparison. The results\nindicate that PIs have better performance than other moment invariants in image\nretrieval and classification. With PIs, one can compare the similarity between\nimages under the projective transformation without knowing the parameters of\nthe transformation, which provides a good tool to shape analysis in image\nprocessing, computer vision and pattern recognition. \n\n"}
{"id": "1707.06017", "contents": "Title: EnzyNet: enzyme classification using 3D convolutional neural networks on\n  spatial representation Abstract: During the past decade, with the significant progress of computational power\nas well as ever-rising data availability, deep learning techniques became\nincreasingly popular due to their excellent performance on computer vision\nproblems. The size of the Protein Data Bank has increased more than 15 fold\nsince 1999, which enabled the expansion of models that aim at predicting\nenzymatic function via their amino acid composition. Amino acid sequence\nhowever is less conserved in nature than protein structure and therefore\nconsidered a less reliable predictor of protein function. This paper presents\nEnzyNet, a novel 3D-convolutional neural networks classifier that predicts the\nEnzyme Commission number of enzymes based only on their voxel-based spatial\nstructure. The spatial distribution of biochemical properties was also examined\nas complementary information. The 2-layer architecture was investigated on a\nlarge dataset of 63,558 enzymes from the Protein Data Bank and achieved an\naccuracy of 78.4% by exploiting only the binary representation of the protein\nshape. Code and datasets are available at https://github.com/shervinea/enzynet. \n\n"}
{"id": "1707.06255", "contents": "Title: The Processing of Enriched Germanium for the MAJORANA DEMONSTRATOR and\n  R&D for a Possible Future Ton-Scale Ge-76 Double-Beta Decay Experiment Abstract: The MAJORANA DEMONSTRATOR is an array of point-contact Ge detectors\nfabricated from Ge isotopically enriched to 88% in Ge-76 to search for\nneutrinoless double beta decay. The processing of Ge for germanium detectors is\na well-known technology. However, because of the high cost of Ge enriched in\nGe-76, special procedures were required to maximize the yield of detector mass\nand to minimize exposure to cosmic rays. These procedures include careful\naccounting for the material; shielding it to reduce cosmogenic generation of\nradioactive isotopes; and development of special reprocessing techniques for\ncontaminated solid germanium, shavings, grindings, acid etchant and cutting\nfluids from detector fabrication. Processing procedures were developed that\nresulted in a total yield in detector mass of 70%. However, none of the\nacid-etch solution and only 50% of the cutting fluids from detector fabrication\nwere reprocessed. Had they been processed, the projections for the recovery\nyield would be between 80 -- 85%. Maximizing yield is critical to justify a\npossible future ton-scale experiment. A process for recovery of germanium from\nthe acid-etch solution was developed with yield of about 90%. All material was\nshielded or stored underground whenever possible to minimize the formation of\nGe-68 by cosmic rays, which contributes background in the double-beta decay\nregion of interest and cannot be removed by zone refinement and crystal growth.\nFormation of Ge-68 was reduced by a significant factor over that in natural\nabundance detectors not protected from cosmic rays. \n\n"}
{"id": "1707.06426", "contents": "Title: Semantic Segmentation with Reverse Attention Abstract: Recent development in fully convolutional neural network enables efficient\nend-to-end learning of semantic segmentation. Traditionally, the convolutional\nclassifiers are taught to learn the representative semantic features of labeled\nsemantic objects. In this work, we propose a reverse attention network (RAN)\narchitecture that trains the network to capture the opposite concept (i.e.,\nwhat are not associated with a target class) as well. The RAN is a three-branch\nnetwork that performs the direct, reverse and reverse-attention learning\nprocesses simultaneously. Extensive experiments are conducted to show the\neffectiveness of the RAN in semantic segmentation. Being built upon the\nDeepLabv2-LargeFOV, the RAN achieves the state-of-the-art mIoU score (48.1%)\nfor the challenging PASCAL-Context dataset. Significant performance\nimprovements are also observed for the PASCAL-VOC, Person-Part, NYUDv2 and\nADE20K datasets. \n\n"}
{"id": "1707.06786", "contents": "Title: Head Detection with Depth Images in the Wild Abstract: Head detection and localization is a demanding task and a key element for\nmany computer vision applications, like video surveillance, Human Computer\nInteraction and face analysis. The stunning amount of work done for detecting\nfaces on RGB images, together with the availability of huge face datasets,\nallowed to setup very effective systems on that domain. However, due to\nillumination issues, infrared or depth cameras may be required in real\napplications. In this paper, we introduce a novel method for head detection on\ndepth images that exploits the classification ability of deep learning\napproaches. In addition to reduce the dependency on the external illumination,\ndepth images implicitly embed useful information to deal with the scale of the\ntarget objects. Two public datasets have been exploited: the first one, called\nPandora, is used to train a deep binary classifier with face and non-face\nimages. The second one, collected by Cornell University, is used to perform a\ncross-dataset test during daily activities in unconstrained environments.\nExperimental results show that the proposed method overcomes the performance of\nstate-of-art methods working on depth images. \n\n"}
{"id": "1707.07013", "contents": "Title: Confidence estimation in Deep Neural networks via density modelling Abstract: State-of-the-art Deep Neural Networks can be easily fooled into providing\nincorrect high-confidence predictions for images with small amounts of\nadversarial noise. Does this expose a flaw with deep neural networks, or do we\nsimply need a better way to estimate confidence? In this paper we consider the\nproblem of accurately estimating predictive confidence. We formulate this\nproblem as that of density modelling, and show how traditional methods such as\nsoftmax produce poor estimates. To address this issue, we propose a novel\nconfidence measure based on density modelling approaches. We test these\nmeasures on images distorted by blur, JPEG compression, random noise and\nadversarial noise. Experiments show that our confidence measure consistently\nshows reduced confidence scores in the presence of such distortions - a\nproperty which softmax often lacks. \n\n"}
{"id": "1707.07074", "contents": "Title: What-and-Where to Match: Deep Spatially Multiplicative Integration\n  Networks for Person Re-identification Abstract: Matching pedestrians across disjoint camera views, known as person\nre-identification (re-id), is a challenging problem that is of importance to\nvisual recognition and surveillance. Most existing methods exploit local\nregions within spatial manipulation to perform matching in local\ncorrespondence. However, they essentially extract \\emph{fixed} representations\nfrom pre-divided regions for each image and perform matching based on the\nextracted representation subsequently. For models in this pipeline, local finer\npatterns that are crucial to distinguish positive pairs from negative ones\ncannot be captured, and thus making them underperformed. In this paper, we\npropose a novel deep multiplicative integration gating function, which answers\nthe question of \\emph{what-and-where to match} for effective person re-id. To\naddress \\emph{what} to match, our deep network emphasizes common local patterns\nby learning joint representations in a multiplicative way. The network\ncomprises two Convolutional Neural Networks (CNNs) to extract convolutional\nactivations, and generates relevant descriptors for pedestrian matching. This\nthus, leads to flexible representations for pair-wise images. To address\n\\emph{where} to match, we combat the spatial misalignment by performing\nspatially recurrent pooling via a four-directional recurrent neural network to\nimpose spatial dependency over all positions with respect to the entire image.\nThe proposed network is designed to be end-to-end trainable to characterize\nlocal pairwise feature interactions in a spatially aligned manner. To\ndemonstrate the superiority of our method, extensive experiments are conducted\nover three benchmark data sets: VIPeR, CUHK03 and Market-1501. \n\n"}
{"id": "1707.07169", "contents": "Title: Comparing Apples and Oranges: Off-Road Pedestrian Detection on the NREC\n  Agricultural Person-Detection Dataset Abstract: Person detection from vehicles has made rapid progress recently with the\nadvent of multiple highquality datasets of urban and highway driving, yet no\nlarge-scale benchmark is available for the same problem in off-road or\nagricultural environments. Here we present the NREC Agricultural\nPerson-Detection Dataset to spur research in these environments. It consists of\nlabeled stereo video of people in orange and apple orchards taken from two\nperception platforms (a tractor and a pickup truck), along with vehicle\nposition data from RTK GPS. We define a benchmark on part of the dataset that\ncombines a total of 76k labeled person images and 19k sampled person-free\nimages. The dataset highlights several key challenges of the domain, including\nvarying environment, substantial occlusion by vegetation, people in motion and\nin non-standard poses, and people seen from a variety of distances; meta-data\nare included to allow targeted evaluation of each of these effects. Finally, we\npresent baseline detection performance results for three leading approaches\nfrom urban pedestrian detection and our own convolutional neural network\napproach that benefits from the incorporation of additional image context. We\nshow that the success of existing approaches on urban data does not transfer\ndirectly to this domain. \n\n"}
{"id": "1707.07321", "contents": "Title: Exploiting Deep Features for Remote Sensing Image Retrieval: A\n  Systematic Investigation Abstract: Remote sensing (RS) image retrieval is of great significant for geological\ninformation mining. Over the past two decades, a large amount of research on\nthis task has been carried out, which mainly focuses on the following three\ncore issues: feature extraction, similarity metric and relevance feedback. Due\nto the complexity and multiformity of ground objects in high-resolution remote\nsensing (HRRS) images, there is still room for improvement in the current\nretrieval approaches. In this paper, we analyze the three core issues of RS\nimage retrieval and provide a comprehensive review on existing methods.\nFurthermore, for the goal to advance the state-of-the-art in HRRS image\nretrieval, we focus on the feature extraction issue and delve how to use\npowerful deep representations to address this task. We conduct systematic\ninvestigation on evaluating correlative factors that may affect the performance\nof deep features. By optimizing each factor, we acquire remarkable retrieval\nresults on publicly available HRRS datasets. Finally, we explain the\nexperimental phenomenon in detail and draw conclusions according to our\nanalysis. Our work can serve as a guiding role for the research of\ncontent-based RS image retrieval. \n\n"}
{"id": "1707.07391", "contents": "Title: Contrastive-center loss for deep neural networks Abstract: The deep convolutional neural network(CNN) has significantly raised the\nperformance of image classification and face recognition. Softmax is usually\nused as supervision, but it only penalizes the classification loss. In this\npaper, we propose a novel auxiliary supervision signal called contrastivecenter\nloss, which can further enhance the discriminative power of the features, for\nit learns a class center for each class. The proposed contrastive-center loss\nsimultaneously considers intra-class compactness and inter-class separability,\nby penalizing the contrastive values between: (1)the distances of training\nsamples to their corresponding class centers, and (2)the sum of the distances\nof training samples to their non-corresponding class centers. Experiments on\ndifferent datasets demonstrate the effectiveness of contrastive-center loss. \n\n"}
{"id": "1707.07394", "contents": "Title: Wavelet Convolutional Neural Networks for Texture Classification Abstract: Texture classification is an important and challenging problem in many image\nprocessing applications. While convolutional neural networks (CNNs) achieved\nsignificant successes for image classification, texture classification remains\na difficult problem since textures usually do not contain enough information\nregarding the shape of object. In image processing, texture classification has\nbeen traditionally studied well with spectral analyses which exploit repeated\nstructures in many textures. Since CNNs process images as-is in the spatial\ndomain whereas spectral analyses process images in the frequency domain, these\nmodels have different characteristics in terms of performance. We propose a\nnovel CNN architecture, wavelet CNNs, which integrates a spectral analysis into\nCNNs. Our insight is that the pooling layer and the convolution layer can be\nviewed as a limited form of a spectral analysis. Based on this insight, we\ngeneralize both layers to perform a spectral analysis with wavelet transform.\nWavelet CNNs allow us to utilize spectral information which is lost in\nconventional CNNs but useful in texture classification. The experiments\ndemonstrate that our model achieves better accuracy in texture classification\nthan existing models. We also show that our model has significantly fewer\nparameters than CNNs, making our model easier to train with less memory. \n\n"}
{"id": "1707.08040", "contents": "Title: A Simple Exponential Family Framework for Zero-Shot Learning Abstract: We present a simple generative framework for learning to predict previously\nunseen classes, based on estimating class-attribute-gated class-conditional\ndistributions. We model each class-conditional distribution as an exponential\nfamily distribution and the parameters of the distribution of each seen/unseen\nclass are defined as functions of the respective observed class attributes.\nThese functions can be learned using only the seen class data and can be used\nto predict the parameters of the class-conditional distribution of each unseen\nclass. Unlike most existing methods for zero-shot learning that represent\nclasses as fixed embeddings in some vector space, our generative model\nnaturally represents each class as a probability distribution. It is simple to\nimplement and also allows leveraging additional unlabeled data from unseen\nclasses to improve the estimates of their class-conditional distributions using\ntransductive/semi-supervised learning. Moreover, it extends seamlessly to\nfew-shot learning by easily updating these distributions when provided with a\nsmall number of additional labelled examples from unseen classes. Through a\ncomprehensive set of experiments on several benchmark data sets, we demonstrate\nthe efficacy of our framework. \n\n"}
{"id": "1707.09376", "contents": "Title: Face Deidentification with Generative Deep Neural Networks Abstract: Face deidentification is an active topic amongst privacy and security\nresearchers. Early deidentification methods relying on image blurring or\npixelization were replaced in recent years with techniques based on formal\nanonymity models that provide privacy guaranties and at the same time aim at\nretaining certain characteristics of the data even after deidentification. The\nlatter aspect is particularly important, as it allows to exploit the\ndeidentified data in applications for which identity information is irrelevant.\nIn this work we present a novel face deidentification pipeline, which ensures\nanonymity by synthesizing artificial surrogate faces using generative neural\nnetworks (GNNs). The generated faces are used to deidentify subjects in images\nor video, while preserving non-identity-related aspects of the data and\nconsequently enabling data utilization. Since generative networks are very\nadaptive and can utilize a diverse set of parameters (pertaining to the\nappearance of the generated output in terms of facial expressions, gender,\nrace, etc.), they represent a natural choice for the problem of face\ndeidentification. To demonstrate the feasibility of our approach, we perform\nexperiments using automated recognition tools and human annotators. Our results\nshow that the recognition performance on deidentified images is close to\nchance, suggesting that the deidentification process based on GNNs is highly\neffective. \n\n"}
{"id": "1707.09557", "contents": "Title: Improved Adversarial Systems for 3D Object Generation and Reconstruction Abstract: This paper describes a new approach for training generative adversarial\nnetworks (GAN) to understand the detailed 3D shape of objects. While GANs have\nbeen used in this domain previously, they are notoriously hard to train,\nespecially for the complex joint data distribution over 3D objects of many\ncategories and orientations. Our method extends previous work by employing the\nWasserstein distance normalized with gradient penalization as a training\nobjective. This enables improved generation from the joint object shape\ndistribution. Our system can also reconstruct 3D shape from 2D images and\nperform shape completion from occluded 2.5D range scans. We achieve notable\nquantitative improvements in comparison to existing baselines \n\n"}
{"id": "1708.00584", "contents": "Title: A Simple Loss Function for Improving the Convergence and Accuracy of\n  Visual Question Answering Models Abstract: Visual question answering as recently proposed multimodal learning task has\nenjoyed wide attention from the deep learning community. Lately, the focus was\non developing new representation fusion methods and attention mechanisms to\nachieve superior performance. On the other hand, very little focus has been put\non the models' loss function, arguably one of the most important aspects of\ntraining deep learning models. The prevailing practice is to use cross entropy\nloss function that penalizes the probability given to all the answers in the\nvocabulary except the single most common answer for the particular question.\nHowever, the VQA evaluation function compares the predicted answer with all the\nground-truth answers for the given question and if there is a matching, a\npartial point is given. This causes a discrepancy between the model's cross\nentropy loss and the model's accuracy as calculated by the VQA evaluation\nfunction. In this work, we propose a novel loss, termed as soft cross entropy,\nthat considers all ground-truth answers and thus reduces the loss-accuracy\ndiscrepancy. The proposed loss leads to an improved training convergence of VQA\nmodels and an increase in accuracy as much as 1.6%. \n\n"}
{"id": "1708.00983", "contents": "Title: Multi-Planar Deep Segmentation Networks for Cardiac Substructures from\n  MRI and CT Abstract: Non-invasive detection of cardiovascular disorders from radiology scans\nrequires quantitative image analysis of the heart and its substructures. There\nare well-established measurements that radiologists use for diseases assessment\nsuch as ejection fraction, volume of four chambers, and myocardium mass. These\nmeasurements are derived as outcomes of precise segmentation of the heart and\nits substructures. The aim of this paper is to provide such measurements\nthrough an accurate image segmentation algorithm that automatically delineates\nseven substructures of the heart from MRI and/or CT scans. Our proposed method\nis based on multi-planar deep convolutional neural networks (CNN) with an\nadaptive fusion strategy where we automatically utilize complementary\ninformation from different planes of the 3D scans for improved delineations.\nFor CT and MRI, we have separately designed three CNNs (the same architectural\nconfiguration) for three planes, and have trained the networks from scratch for\nvoxel-wise labeling for the following cardiac structures: myocardium of left\nventricle (Myo), left atrium (LA), left ventricle (LV), right atrium (RA),\nright ventricle (RV), ascending aorta (Ao), and main pulmonary artery (PA). We\nhave evaluated the proposed method with 4-fold-cross validation on the\nmulti-modality whole heart segmentation challenge (MM-WHS 2017) dataset. The\nprecision and dice index of 0.93 and 0.90, and 0.87 and 0.85 were achieved for\nCT and MR images, respectively. While a CT volume was segmented about 50\nseconds, an MRI scan was segmented around 17 seconds with the GPUs/CUDA\nimplementation. \n\n"}
{"id": "1708.01008", "contents": "Title: Beyond Low Rank: A Data-Adaptive Tensor Completion Method Abstract: Low rank tensor representation underpins much of recent progress in tensor\ncompletion. In real applications, however, this approach is confronted with two\nchallenging problems, namely (1) tensor rank determination; (2) handling real\ntensor data which only approximately fulfils the low-rank requirement. To\naddress these two issues, we develop a data-adaptive tensor completion model\nwhich explicitly represents both the low-rank and non-low-rank structures in a\nlatent tensor. Representing the non-low-rank structure separately from the\nlow-rank one allows priors which capture the important distinctions between the\ntwo, thus enabling more accurate modelling, and ultimately, completion. Through\ndefining a new tensor rank, we develop a sparsity induced prior for the\nlow-rank structure, with which the tensor rank can be automatically determined.\nThe prior for the non-low-rank structure is established based on a mixture of\nGaussians which is shown to be flexible enough, and powerful enough, to inform\nthe completion process for a variety of real tensor data. With these two\npriors, we develop a Bayesian minimum mean squared error estimate (MMSE)\nframework for inference which provides the posterior mean of missing entries as\nwell as their uncertainty. Compared with the state-of-the-art methods in\nvarious applications, the proposed model produces more accurate completion\nresults. \n\n"}
{"id": "1708.01471", "contents": "Title: Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for\n  Visual Question Answering Abstract: Visual question answering (VQA) is challenging because it requires a\nsimultaneous understanding of both the visual content of images and the textual\ncontent of questions. The approaches used to represent the images and questions\nin a fine-grained manner and questions and to fuse these multi-modal features\nplay key roles in performance. Bilinear pooling based models have been shown to\noutperform traditional linear models for VQA, but their high-dimensional\nrepresentations and high computational complexity may seriously limit their\napplicability in practice. For multi-modal feature fusion, here we develop a\nMulti-modal Factorized Bilinear (MFB) pooling approach to efficiently and\neffectively combine multi-modal features, which results in superior performance\nfor VQA compared with other bilinear pooling approaches. For fine-grained image\nand question representation, we develop a co-attention mechanism using an\nend-to-end deep network architecture to jointly learn both the image and\nquestion attentions. Combining the proposed MFB approach with co-attention\nlearning in a new network architecture provides a unified model for VQA. Our\nexperimental results demonstrate that the single MFB with co-attention model\nachieves new state-of-the-art performance on the real-world VQA dataset. Code\navailable at https://github.com/yuzcccc/mfb. \n\n"}
{"id": "1708.02072", "contents": "Title: Measuring Catastrophic Forgetting in Neural Networks Abstract: Deep neural networks are used in many state-of-the-art systems for machine\nperception. Once a network is trained to do a specific task, e.g., bird\nclassification, it cannot easily be trained to do new tasks, e.g.,\nincrementally learning to recognize additional bird species or learning an\nentirely different task such as flower recognition. When new tasks are added,\ntypical deep neural networks are prone to catastrophically forgetting previous\ntasks. Networks that are capable of assimilating new information incrementally,\nmuch like how humans form new memories over time, will be more efficient than\nre-training the model from scratch each time a new task needs to be learned.\nThere have been multiple attempts to develop schemes that mitigate catastrophic\nforgetting, but these methods have not been directly compared, the tests used\nto evaluate them vary considerably, and these methods have only been evaluated\non small-scale problems (e.g., MNIST). In this paper, we introduce new metrics\nand benchmarks for directly comparing five different mechanisms designed to\nmitigate catastrophic forgetting in neural networks: regularization,\nensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on\nreal-world images and sounds show that the mechanism(s) that are critical for\noptimal performance vary based on the incremental training paradigm and type of\ndata being used, but they all demonstrate that the catastrophic forgetting\nproblem has yet to be solved. \n\n"}
{"id": "1708.02314", "contents": "Title: Multibiometric Secure System Based on Deep Learning Abstract: In this paper, we propose a secure multibiometric system that uses deep\nneural networks and error-correction coding. We present a feature-level fusion\nframework to generate a secure multibiometric template from each user's\nmultiple biometrics. Two fusion architectures, fully connected architecture and\nbilinear architecture, are implemented to develop a robust multibiometric\nshared representation. The shared representation is used to generate a\ncancelable biometric template that involves the selection of a different set of\nreliable and discriminative features for each user. This cancelable template is\na binary vector and is passed through an appropriate error-correcting decoder\nto find a closest codeword and this codeword is hashed to generate the final\nsecure template. The efficacy of the proposed approach is shown using a\nmultimodal database where we achieve state-of-the-art matching performance,\nalong with cancelability and security. \n\n"}
{"id": "1708.02837", "contents": "Title: SPLODE: Semi-Probabilistic Point and Line Odometry with Depth Estimation\n  from RGB-D Camera Motion Abstract: Active depth cameras suffer from several limitations, which cause incomplete\nand noisy depth maps, and may consequently affect the performance of RGB-D\nOdometry. To address this issue, this paper presents a visual odometry method\nbased on point and line features that leverages both measurements from a depth\nsensor and depth estimates from camera motion. Depth estimates are generated\ncontinuously by a probabilistic depth estimation framework for both types of\nfeatures to compensate for the lack of depth measurements and inaccurate\nfeature depth associations. The framework models explicitly the uncertainty of\ntriangulating depth from both point and line observations to validate and\nobtain precise estimates. Furthermore, depth measurements are exploited by\npropagating them through a depth map registration module and using a\nframe-to-frame motion estimation method that considers 3D-to-2D and 2D-to-3D\nreprojection errors, independently. Results on RGB-D sequences captured on\nlarge indoor and outdoor scenes, where depth sensor limitations are critical,\nshow that the combination of depth measurements and estimates through our\napproach is able to overcome the absence and inaccuracy of depth measurements. \n\n"}
{"id": "1708.02863", "contents": "Title: CoupleNet: Coupling Global Structure with Local Parts for Object\n  Detection Abstract: The region-based Convolutional Neural Network (CNN) detectors such as Faster\nR-CNN or R-FCN have already shown promising results for object detection by\ncombining the region proposal subnetwork and the classification subnetwork\ntogether. Although R-FCN has achieved higher detection speed while keeping the\ndetection performance, the global structure information is ignored by the\nposition-sensitive score maps. To fully explore the local and global\nproperties, in this paper, we propose a novel fully convolutional network,\nnamed as CoupleNet, to couple the global structure with local parts for object\ndetection. Specifically, the object proposals obtained by the Region Proposal\nNetwork (RPN) are fed into the the coupling module which consists of two\nbranches. One branch adopts the position-sensitive RoI (PSRoI) pooling to\ncapture the local part information of the object, while the other employs the\nRoI pooling to encode the global and context information. Next, we design\ndifferent coupling strategies and normalization ways to make full use of the\ncomplementary advantages between the global and local branches. Extensive\nexperiments demonstrate the effectiveness of our approach. We achieve\nstate-of-the-art results on all three challenging datasets, i.e. a mAP of 82.7%\non VOC07, 80.4% on VOC12, and 34.4% on COCO. Codes will be made publicly\navailable. \n\n"}
{"id": "1708.03417", "contents": "Title: GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from\n  Remote Sensing Imagery Abstract: Advances in remote sensing technologies have made it possible to use\nhigh-resolution visual data for weather observation and forecasting tasks. We\npropose the use of multi-layer neural networks for understanding complex\natmospheric dynamics based on multichannel satellite images. The capability of\nour model was evaluated by using a linear regression task for single typhoon\ncoordinates prediction. A specific combination of models and different\nactivation policies enabled us to obtain an interesting prediction result in\nthe northeastern hemisphere (ENH). \n\n"}
{"id": "1708.03880", "contents": "Title: Image Quality Assessment Guided Deep Neural Networks Training Abstract: For many computer vision problems, the deep neural networks are trained and\nvalidated based on the assumption that the input images are pristine (i.e.,\nartifact-free). However, digital images are subject to a wide range of\ndistortions in real application scenarios, while the practical issues regarding\nimage quality in high level visual information understanding have been largely\nignored. In this paper, in view of the fact that most widely deployed deep\nlearning models are susceptible to various image distortions, the distorted\nimages are involved for data augmentation in the deep neural network training\nprocess to learn a reliable model for practical applications. In particular, an\nimage quality assessment based label smoothing method, which aims at\nregularizing the label distribution of training images, is further proposed to\ntune the objective functions in learning the neural network. Experimental\nresults show that the proposed method is effective in dealing with both low and\nhigh quality images in the typical image classification task. \n\n"}
{"id": "1708.04669", "contents": "Title: Convolutional Neural Networks for Non-iterative Reconstruction of\n  Compressively Sensed Images Abstract: Traditional algorithms for compressive sensing recovery are computationally\nexpensive and are ineffective at low measurement rates. In this work, we\npropose a data driven non-iterative algorithm to overcome the shortcomings of\nearlier iterative algorithms. Our solution, ReconNet, is a deep neural network,\nwhose parameters are learned end-to-end to map block-wise compressive\nmeasurements of the scene to the desired image blocks. Reconstruction of an\nimage becomes a simple forward pass through the network and can be done in\nreal-time. We show empirically that our algorithm yields reconstructions with\nhigher PSNRs compared to iterative algorithms at low measurement rates and in\npresence of measurement noise. We also propose a variant of ReconNet which uses\nadversarial loss in order to further improve reconstruction quality. We discuss\nhow adding a fully connected layer to the existing ReconNet architecture allows\nfor jointly learning the measurement matrix and the reconstruction algorithm in\na single network. Experiments on real data obtained from a block compressive\nimager show that our networks are robust to unseen sensor noise. Finally,\nthrough an experiment in object tracking, we show that even at very low\nmeasurement rates, reconstructions using our algorithm possess rich semantic\ncontent that can be used for high level inference. \n\n"}
{"id": "1708.04670", "contents": "Title: DeepFaceLIFT: Interpretable Personalized Models for Automatic Estimation\n  of Self-Reported Pain Abstract: Previous research on automatic pain estimation from facial expressions has\nfocused primarily on \"one-size-fits-all\" metrics (such as PSPI). In this work,\nwe focus on directly estimating each individual's self-reported visual-analog\nscale (VAS) pain metric, as this is considered the gold standard for pain\nmeasurement. The VAS pain score is highly subjective and context-dependent, and\nits range can vary significantly among different persons. To tackle these\nissues, we propose a novel two-stage personalized model, named DeepFaceLIFT,\nfor automatic estimation of VAS. This model is based on (1) Neural Network and\n(2) Gaussian process regression models, and is used to personalize the\nestimation of self-reported pain via a set of hand-crafted personal features\nand multi-task learning. We show on the benchmark dataset for pain analysis\n(The UNBC-McMaster Shoulder Pain Expression Archive) that the proposed\npersonalized model largely outperforms the traditional, unpersonalized models:\nthe intra-class correlation improves from a baseline performance of 19\\% to a\npersonalized performance of 35\\% while also providing confidence in the\nmodel\\textquotesingle s estimates -- in contrast to existing models for the\ntarget task. Additionally, DeepFaceLIFT automatically discovers the\npain-relevant facial regions for each person, allowing for an easy\ninterpretation of the pain-related facial cues. \n\n"}
{"id": "1708.04975", "contents": "Title: Training-image based geostatistical inversion using a spatial generative\n  adversarial neural network Abstract: Probabilistic inversion within a multiple-point statistics framework is often\ncomputationally prohibitive for high-dimensional problems. To partly address\nthis, we introduce and evaluate a new training-image based inversion approach\nfor complex geologic media. Our approach relies on a deep neural network of the\ngenerative adversarial network (GAN) type. After training using a training\nimage (TI), our proposed spatial GAN (SGAN) can quickly generate 2D and 3D\nunconditional realizations. A key characteristic of our SGAN is that it defines\na (very) low-dimensional parameterization, thereby allowing for efficient\nprobabilistic inversion using state-of-the-art Markov chain Monte Carlo (MCMC)\nmethods. In addition, available direct conditioning data can be incorporated\nwithin the inversion. Several 2D and 3D categorical TIs are first used to\nanalyze the performance of our SGAN for unconditional geostatistical\nsimulation. Training our deep network can take several hours. After training,\nrealizations containing a few millions of pixels/voxels can be produced in a\nmatter of seconds. This makes it especially useful for simulating many\nthousands of realizations (e.g., for MCMC inversion) as the relative cost of\nthe training per realization diminishes with the considered number of\nrealizations. Synthetic inversion case studies involving 2D steady-state flow\nand 3D transient hydraulic tomography with and without direct conditioning data\nare used to illustrate the effectiveness of our proposed SGAN-based inversion.\nFor the 2D case, the inversion rapidly explores the posterior model\ndistribution. For the 3D case, the inversion recovers model realizations that\nfit the data close to the target level and visually resemble the true model\nwell. \n\n"}
{"id": "1708.05125", "contents": "Title: Hyperspectral Unmixing: Ground Truth Labeling, Datasets, Benchmark\n  Performances and Survey Abstract: Hyperspectral unmixing (HU) is a very useful and increasingly popular\npreprocessing step for a wide range of hyperspectral applications. However, the\nHU research has been constrained a lot by three factors: (a) the number of\nhyperspectral images (especially the ones with ground truths) are very limited;\n(b) the ground truths of most hyperspectral images are not shared on the web,\nwhich may cause lots of unnecessary troubles for researchers to evaluate their\nalgorithms; (c) the codes of most state-of-the-art methods are not shared,\nwhich may also delay the testing of new methods.\n  Accordingly, this paper deals with the above issues from the following three\nperspectives: (1) as a profound contribution, we provide a general labeling\nmethod for the HU. With it, we labeled up to 15 hyperspectral images, providing\n18 versions of ground truths. To the best of our knowledge, this is the first\npaper to summarize and share up to 15 hyperspectral images and their 18\nversions of ground truths for the HU. Observing that the hyperspectral\nclassification (HyC) has much more standard datasets (whose ground truths are\ngenerally publicly shared) than the HU, we propose an interesting method to\ntransform the HyC datasets for the HU research. (2) To further facilitate the\nevaluation of HU methods under different conditions, we reviewed and\nimplemented the algorithm to generate a complex synthetic hyperspectral image.\nBy tuning the hyper-parameters in the code, we may verify the HU methods from\nfour perspectives. The code would also be shared on the web. (3) To provide a\nstandard comparison, we reviewed up to 10 state-of-the-art HU algorithms, then\nselected the 5 most benchmark HU algorithms, and compared them on the 15 real\nhyperspectral datasets. The experiment results are surely reproducible; the\nimplemented codes would be shared on the web. \n\n"}
{"id": "1708.05256", "contents": "Title: Deep Learning at 15PF: Supervised and Semi-Supervised Classification for\n  Scientific Data Abstract: This paper presents the first, 15-PetaFLOP Deep Learning system for solving\nscientific pattern classification problems on contemporary HPC architectures.\nWe develop supervised convolutional architectures for discriminating signals in\nhigh-energy physics data as well as semi-supervised architectures for\nlocalizing and classifying extreme weather in climate data. Our\nIntelcaffe-based implementation obtains $\\sim$2TFLOP/s on a single Cori\nPhase-II Xeon-Phi node. We use a hybrid strategy employing synchronous\nnode-groups, while using asynchronous communication across groups. We use this\nstrategy to scale training of a single model to $\\sim$9600 Xeon-Phi nodes;\nobtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of\n11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art\nclassification accuracy on a dataset with 10M images, exceeding that achieved\nby selections on high-level physics-motivated features. Our semi-supervised\narchitecture successfully extracts weather patterns in a 15TB climate dataset.\nOur results demonstrate that Deep Learning can be optimized and scaled\neffectively on many-core, HPC systems. \n\n"}
{"id": "1708.06086", "contents": "Title: Low Background Gamma Spectroscopy at the Boulby Underground Laboratory Abstract: The Boulby Underground Germanium Suite (BUGS) comprises three low background,\nhigh-purity germanium detectors operating in the Boulby Underground Laboratory,\nlocated 1.1 km underground in the north-east of England, UK. BUGS utilises\nthree types of detector to facilitate a high-sensitivity, high-throughput\nradioassay programme to support the development of rare-event search\nexperiments. A Broad Energy Germanium (BEGe) detector delivers sensitivity to\nlow-energy gamma-rays such as those emitted by 210Pb and 234Th. A Small Anode\nGermanium (SAGe) well-type detector is employed for efficient screening of\nsmall samples. Finally, a standard p-type coaxial detector provides fast\nscreening of standard samples. This paper presents the steps used to\ncharacterise the performance of these detectors for a variety of sample\ngeometries, including the corrections applied to account for cascade summing\neffects. For low-density materials, BUGS is able to radio-assay to specific\nactivities down to 3.6 mBq/kg for 234Th and 6.6 mBq/kg for 210Pb both of which\nhave uncovered some significant equilibrium breaks in the 238U chain. In denser\nmaterials, where gamma-ray self-absorption increases, sensitivity is\ndemonstrated to specific activities of 0.9 mBq/kg for 226Ra, 1.1 mBq/kg for 228\nRa, 0.3 mBq/kg for 224Ra, and 8.6 mBq/kg for 40K with all upper limits at a 90%\nconfidence level. These meet the requirements of most screening campaigns\npresently under way for rare-event search experiments, such as the LUX-ZEPLIN\n(LZ) dark matter experiment. We also highlight the ability of the BEGe detector\nto probe the X-ray fluorescence region which can be important to identify the\npresence of radioisotopes associated with neutron production; this is of\nparticular relevance in experiments sensitive to nuclear recoils. \n\n"}
{"id": "1708.06118", "contents": "Title: Distantly Supervised Road Segmentation Abstract: We present an approach for road segmentation that only requires image-level\nannotations at training time. We leverage distant supervision, which allows us\nto train our model using images that are different from the target domain.\nUsing large publicly available image databases as distant supervisors, we\ndevelop a simple method to automatically generate weak pixel-wise road masks.\nThese are used to iteratively train a fully convolutional neural network, which\nproduces our final segmentation model. We evaluate our method on the Cityscapes\ndataset, where we compare it with a fully supervised approach. Further, we\ndiscuss the trade-off between annotation cost and performance. Overall, our\ndistantly supervised approach achieves 93.8% of the performance of the fully\nsupervised approach, while using orders of magnitude less annotation work. \n\n"}
{"id": "1708.06320", "contents": "Title: Learning Spread-out Local Feature Descriptors Abstract: We propose a simple, yet powerful regularization technique that can be used\nto significantly improve both the pairwise and triplet losses in learning local\nfeature descriptors. The idea is that in order to fully utilize the expressive\npower of the descriptor space, good local feature descriptors should be\nsufficiently \"spread-out\" over the space. In this work, we propose a\nregularization term to maximize the spread in feature descriptor inspired by\nthe property of uniform distribution. We show that the proposed regularization\nwith triplet loss outperforms existing Euclidean distance based descriptor\nlearning techniques by a large margin. As an extension, the proposed\nregularization technique can also be used to improve image-level deep feature\nembedding. \n\n"}
{"id": "1708.07562", "contents": "Title: The Status and Initial Results of the MAJORANA DEMONSTRATOR Experiment Abstract: Neutrinoless double-beta decay searches play a major role in determining the\nnature of neutrinos, the existence of a lepton violating process, and the\neffective Majorana neutrino mass. The MAJORANA Collaboration assembled an array\nof high purity Ge detectors to search for neutrinoless double-beta decay in\nGe-76. The MAJORANA DEMONSTRATOR is comprised of 44.1 kg (29.7 kg enriched in\nGe-76) of Ge detectors divided between two modules contained in a\nlow-background shield at the Sanford Underground Research Facility in Lead,\nSouth Dakota, USA. The initial goals of the DEMONSTRATOR are to establish the\nrequired background and scalability of a Ge-based next-generation ton-scale\nexperiment. Following a commissioning run that started in 2015, the first\ndetector module started low-background data production in early 2016. The\nsecond detector module was added in August 2016 to begin operation of the\nentire array. We discuss results of the initial physics runs, as well as the\nstatus and physics reach of the full MAJORANA DEMONSTRATOR experiment. \n\n"}
{"id": "1708.08333", "contents": "Title: Framing U-Net via Deep Convolutional Framelets: Application to\n  Sparse-view CT Abstract: X-ray computed tomography (CT) using sparse projection views is a recent\napproach to reduce the radiation dose. However, due to the insufficient\nprojection views, an analytic reconstruction approach using the filtered back\nprojection (FBP) produces severe streaking artifacts. Recently, deep learning\napproaches using large receptive field neural networks such as U-Net have\ndemonstrated impressive performance for sparse- view CT reconstruction.\nHowever, theoretical justification is still lacking. Inspired by the recent\ntheory of deep convolutional framelets, the main goal of this paper is,\ntherefore, to reveal the limitation of U-Net and propose new multi-resolution\ndeep learning schemes. In particular, we show that the alternative U- Net\nvariants such as dual frame and the tight frame U-Nets satisfy the so-called\nframe condition which make them better for effective recovery of high frequency\nedges in sparse view- CT. Using extensive experiments with real patient data\nset, we demonstrate that the new network architectures provide better\nreconstruction performance. \n\n"}
{"id": "1708.09307", "contents": "Title: Radiopure tungstate and molybdate crystal scintillators for double beta\n  decay experiments Abstract: Crystal scintillators are very promising detectors to investigate double beta\ndecay of atomic nuclei. Recent achievements in development and application of\ntungstate and molybdate crystal scintillators as well as prospects for the next\ngeneration double beta decay experiments are discussed. \n\n"}
{"id": "1709.02153", "contents": "Title: Real-time convolutional networks for sonar image classification in\n  low-power embedded systems Abstract: Deep Neural Networks have impressive classification performance, but this\ncomes at the expense of significant computational resources at inference time.\nAutonomous Underwater Vehicles use low-power embedded systems for sonar image\nperception, and cannot execute large neural networks in real-time. We propose\nthe use of max-pooling aggressively, and we demonstrate it with a Fire-based\nmodule and a new Tiny module that includes max-pooling in each module. By\nstacking them we build networks that achieve the same accuracy as bigger ones,\nwhile reducing the number of parameters and considerably increasing\ncomputational performance. Our networks can classify a 96x96 sonar image with\n98.8 - 99.7 accuracy on only 41 to 61 milliseconds on a Raspberry Pi 2, which\ncorresponds to speedups of 28.6 - 19.7. \n\n"}
{"id": "1709.02482", "contents": "Title: Scalable Annotation of Fine-Grained Categories Without Experts Abstract: We present a crowdsourcing workflow to collect image annotations for visually\nsimilar synthetic categories without requiring experts. In animals, there is a\ndirect link between taxonomy and visual similarity: e.g. a collie (type of dog)\nlooks more similar to other collies (e.g. smooth collie) than a greyhound\n(another type of dog). However, in synthetic categories such as cars, objects\nwith similar taxonomy can have very different appearance: e.g. a 2011 Ford\nF-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very\ndifferent from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based\ncrowdsourcing algorithm to automatically group visually indistinguishable\nobjects together. Using our workflow, we label 712,430 images by ~1,000 Amazon\nMechanical Turk workers; resulting in the largest fine-grained visual dataset\nreported to date with 2,657 categories of cars annotated at 1/20th the cost of\nhiring experts. \n\n"}
{"id": "1709.04303", "contents": "Title: Reading Scene Text with Attention Convolutional Sequence Modeling Abstract: Reading text in the wild is a challenging task in the field of computer\nvision. Existing approaches mainly adopted Connectionist Temporal\nClassification (CTC) or Attention models based on Recurrent Neural Network\n(RNN), which is computationally expensive and hard to train. In this paper, we\npresent an end-to-end Attention Convolutional Network for scene text\nrecognition. Firstly, instead of RNN, we adopt the stacked convolutional layers\nto effectively capture the contextual dependencies of the input sequence, which\nis characterized by lower computational complexity and easier parallel\ncomputation. Compared to the chain structure of recurrent networks, the\nConvolutional Neural Network (CNN) provides a natural way to capture long-term\ndependencies between elements, which is 9 times faster than Bidirectional Long\nShort-Term Memory (BLSTM). Furthermore, in order to enhance the representation\nof foreground text and suppress the background noise, we incorporate the\nresidual attention modules into a small densely connected network to improve\nthe discriminability of CNN features. We validate the performance of our\napproach on the standard benchmarks, including the Street View Text, IIIT5K and\nICDAR datasets. As a result, state-of-the-art or highly-competitive performance\nand efficiency show the superiority of the proposed approach. \n\n"}
{"id": "1709.04725", "contents": "Title: Unsupervised object discovery for instance recognition Abstract: Severe background clutter is challenging in many computer vision tasks,\nincluding large-scale image retrieval. Global descriptors, that are popular due\nto their memory and search efficiency, are especially prone to corruption by\nsuch a clutter. Eliminating the impact of the clutter on the image descriptor\nincreases the chance of retrieving relevant images and prevents topic drift due\nto actually retrieving the clutter in the case of query expansion. In this\nwork, we propose a novel salient region detection method. It captures, in an\nunsupervised manner, patterns that are both discriminative and common in the\ndataset. Saliency is based on a centrality measure of a nearest neighbor graph\nconstructed from regional CNN representations of dataset images. The\ndescriptors derived from the salient regions improve particular object\nretrieval, most noticeably in a large collections containing small objects. \n\n"}
{"id": "1709.05324", "contents": "Title: Cystoid macular edema segmentation of Optical Coherence Tomography\n  images using fully convolutional neural networks and fully connected CRFs Abstract: In this paper we present a new method for cystoid macular edema (CME)\nsegmentation in retinal Optical Coherence Tomography (OCT) images, using a\nfully convolutional neural network (FCN) and a fully connected conditional\nrandom fields (dense CRFs). As a first step, the framework trains the FCN model\nto extract features from retinal layers in OCT images, which exhibit CME, and\nthen segments CME regions using the trained model. Thereafter, dense CRFs are\nused to refine the segmentation according to the edema appearance. We have\ntrained and tested the framework with OCT images from 10 patients with diabetic\nmacular edema (DME). Our experimental results show that fluid and concrete\nmacular edema areas were segmented with good adherence to boundaries. A\nsegmentation accuracy of $0.61\\pm 0.21$ (Dice coefficient) was achieved, with\nrespect to the ground truth, which compares favourably with the previous\nstate-of-the-art that used a kernel regression based method ($0.51\\pm 0.34$).\nOur approach is versatile and we believe it can be easily adapted to detect\nother macular defects. \n\n"}
{"id": "1709.05436", "contents": "Title: Scene-centric Joint Parsing of Cross-view Videos Abstract: Cross-view video understanding is an important yet under-explored area in\ncomputer vision. In this paper, we introduce a joint parsing framework that\nintegrates view-centric proposals into scene-centric parse graphs that\nrepresent a coherent scene-centric understanding of cross-view scenes. Our key\nobservations are that overlapping fields of views embed rich appearance and\ngeometry correlations and that knowledge fragments corresponding to individual\nvision tasks are governed by consistency constraints available in commonsense\nknowledge. The proposed joint parsing framework represents such correlations\nand constraints explicitly and generates semantic scene-centric parse graphs.\nQuantitative experiments show that scene-centric predictions in the parse graph\noutperform view-centric predictions. \n\n"}
{"id": "1709.06178", "contents": "Title: A Fast Algorithm Based on a Sylvester-like Equation for LS Regression\n  with GMRF Prior Abstract: This paper presents a fast approach for penalized least squares (LS)\nregression problems using a 2D Gaussian Markov random field (GMRF) prior. More\nprecisely, the computation of the proximity operator of the LS criterion\nregularized by different GMRF potentials is formulated as solving a\nSylvester-like matrix equation. By exploiting the structural properties of\nGMRFs, this matrix equation is solved columnwise in an analytical way. The\nproposed algorithm can be embedded into a wide range of proximal algorithms to\nsolve LS regression problems including a convex penalty. Experiments carried\nout in the case of a constrained LS regression problem arising in a\nmultichannel image processing application, provide evidence that an alternating\ndirection method of multipliers performs quite efficiently in this context. \n\n"}
{"id": "1709.06664", "contents": "Title: Curriculum Learning of Visual Attribute Clusters for Multi-Task\n  Classification Abstract: Visual attributes, from simple objects (e.g., backpacks, hats) to\nsoft-biometrics (e.g., gender, height, clothing) have proven to be a powerful\nrepresentational approach for many applications such as image description and\nhuman identification. In this paper, we introduce a novel method to combine the\nadvantages of both multi-task and curriculum learning in a visual attribute\nclassification framework. Individual tasks are grouped after performing\nhierarchical clustering based on their correlation. The clusters of tasks are\nlearned in a curriculum learning setup by transferring knowledge between\nclusters. The learning process within each cluster is performed in a multi-task\nclassification setup. By leveraging the acquired knowledge, we speed-up the\nprocess and improve performance. We demonstrate the effectiveness of our method\nvia ablation studies and a detailed analysis of the covariates, on a variety of\npublicly available datasets of humans standing with their full-body visible.\nExtensive experimentation has proven that the proposed approach boosts the\nperformance by 4% to 10%. \n\n"}
{"id": "1709.07200", "contents": "Title: Temporal Multimodal Fusion for Video Emotion Classification in the Wild Abstract: This paper addresses the question of emotion classification. The task\nconsists in predicting emotion labels (taken among a set of possible labels)\nbest describing the emotions contained in short video clips. Building on a\nstandard framework -- lying in describing videos by audio and visual features\nused by a supervised classifier to infer the labels -- this paper investigates\nseveral novel directions. First of all, improved face descriptors based on 2D\nand 3D Convo-lutional Neural Networks are proposed. Second, the paper explores\nseveral fusion methods, temporal and multimodal, including a novel hierarchical\nmethod combining features and scores. In addition, we carefully reviewed the\ndifferent stages of the pipeline and designed a CNN architecture adapted to the\ntask; this is important as the size of the training set is small compared to\nthe difficulty of the problem, making generalization difficult. The so-obtained\nmodel ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of\n58.8 %. \n\n"}
{"id": "1709.09753", "contents": "Title: Technical Design Report for the Paul Scherrer Institute Experiment\n  R-12-01.1: Studying the Proton \"Radius\" Puzzle with {\\mu}p Elastic Scattering Abstract: The difference in proton radii measured with $\\mu p$ atoms and with $ep$\natoms and scattering remains an unexplained puzzle. The PSI MUSE proposal is to\nmeasure $\\mu p$ and $e p$ scattering in the same experiment at the same time.\nThe experiment will determine cross sections, two-photon effects, form factors,\nand radii independently for the two reactions, and will allow $\\mu p$ and $ep$\nresults to be compared with reduced systematic uncertainties. These data should\nprovide the best test of lepton universality in a scattering experiment to\ndate, about an order of magnitude improvement over previous tests. Measuring\nscattering with both particle polarities will allow a test of two-photon\nexchange at the sub-percent level, about a factor of four improvement on\nuncertainties and over an order of magnitude more data points than previous low\nmomentum transfer determinations, and similar to the current generation of\nhigher momentum transfer electron experiments. The experiment has the potential\nto demonstrate whether the $\\mu p$ and $ep$ interactions are consistent or\ndifferent, and whether any difference results from novel physics or two-photon\nexchange. The uncertainties are such that if the discrepancy is real it should\nbe confirmed with $\\approx$5$\\sigma$ significance, similar to that already\nestablished between the regular and muonic hydrogen Lamb shift. \n\n"}
{"id": "1709.09780", "contents": "Title: Improving Dermoscopic Image Segmentation with Enhanced\n  Convolutional-Deconvolutional Networks Abstract: Automatic skin lesion segmentation on dermoscopic images is an essential step\nin computer-aided diagnosis of melanoma. However, this task is challenging due\nto significant variations of lesion appearances across different patients. This\nchallenge is further exacerbated when dealing with a large amount of image\ndata. In this paper, we extended our previous work by developing a deeper\nnetwork architecture with smaller kernels to enhance its discriminant capacity.\nIn addition, we explicitly included color information from multiple color\nspaces to facilitate network training and thus to further improve the\nsegmentation performance. We extensively evaluated our method on the ISBI 2017\nskin lesion segmentation challenge. By training with the 2000 challenge\ntraining images, our method achieved an average Jaccard Index (JA) of 0.765 on\nthe 600 challenge testing images, which ranked itself in the first place in the\nchallenge \n\n"}
{"id": "1710.01717", "contents": "Title: A New Correlator to Detect and Characterize the Chiral Magnetic Effect Abstract: A charge-sensitive in-event correlator is proposed and tested for its\nefficacy to detect and characterize charge separation associated with the\nChiral Magnetic Effect (CME) in heavy ion collisions. Tests, performed with the\naid of two reaction models, indicate discernible responses for background- and\nCME-driven charge separation, relative to the second- ($\\Psi_{2}$) and\nthird-order ($\\Psi_{3}$) event planes, which could serve to identify the CME.\nThe tests also indicate a degree of sensitivity which would enable robust\ncharacterization of the CME via Anomalous Viscous Fluid Dynamics (AVFD) model\ncomparisons. \n\n"}
{"id": "1710.01820", "contents": "Title: Energy-Based Spherical Sparse Coding Abstract: In this paper, we explore an efficient variant of convolutional sparse coding\nwith unit norm code vectors where reconstruction quality is evaluated using an\ninner product (cosine distance). To use these codes for discriminative\nclassification, we describe a model we term Energy-Based Spherical Sparse\nCoding (EB-SSC) in which the hypothesized class label introduces a learned\nlinear bias into the coding step. We evaluate and visualize performance of\nstacking this encoder to make a deep layered model for image classification. \n\n"}
{"id": "1710.02213", "contents": "Title: Video Denoising and Enhancement via Dynamic Video Layering Abstract: Video denoising refers to the problem of removing \"noise\" from a video\nsequence. Here the term \"noise\" is used in a broad sense to refer to any\ncorruption or outlier or interference that is not the quantity of interest. In\nthis work, we develop a novel approach to video denoising that is based on the\nidea that many noisy or corrupted videos can be split into three parts - the\n\"low-rank layer\", the \"sparse layer\", and a small residual (which is small and\nbounded). We show, using extensive experiments, that our denoising approach\noutperforms the state-of-the-art denoising algorithms. \n\n"}
{"id": "1710.04783", "contents": "Title: Retinal Vasculature Segmentation Using Local Saliency Maps and\n  Generative Adversarial Networks For Image Super Resolution Abstract: We propose an image super resolution(ISR) method using generative adversarial\nnetworks (GANs) that takes a low resolution input fundus image and generates a\nhigh resolution super resolved (SR) image upto scaling factor of $16$. This\nfacilitates more accurate automated image analysis, especially for small or\nblurred landmarks and pathologies. Local saliency maps, which define each\npixel's importance, are used to define a novel saliency loss in the GAN cost\nfunction. Experimental results show the resulting SR images have perceptual\nquality very close to the original images and perform better than competing\nmethods that do not weigh pixels according to their importance. When used for\nretinal vasculature segmentation, our SR images result in accuracy levels close\nto those obtained when using the original images. \n\n"}
{"id": "1710.04826", "contents": "Title: WeText: Scene Text Detection under Weak Supervision Abstract: The requiring of large amounts of annotated training data has become a common\nconstraint on various deep learning systems. In this paper, we propose a weakly\nsupervised scene text detection method (WeText) that trains robust and accurate\nscene text detection models by learning from unannotated or weakly annotated\ndata. With a \"light\" supervised model trained on a small fully annotated\ndataset, we explore semi-supervised and weakly supervised learning on a large\nunannotated dataset and a large weakly annotated dataset, respectively. For the\nunsupervised learning, the light supervised model is applied to the unannotated\ndataset to search for more character training samples, which are further\ncombined with the small annotated dataset to retrain a superior character\ndetection model. For the weakly supervised learning, the character searching is\nguided by high-level annotations of words/text lines that are widely available\nand also much easier to prepare. In addition, we design an unified scene\ncharacter detector by adapting regression based deep networks, which greatly\nrelieves the error accumulation issue that widely exists in most traditional\napproaches. Extensive experiments across different unannotated and weakly\nannotated datasets show that the scene text detection performance can be\nclearly boosted under both scenarios, where the weakly supervised learning can\nachieve the state-of-the-art performance by using only 229 fully annotated\nscene text images. \n\n"}
{"id": "1710.05182", "contents": "Title: Performance of the upgraded ultracold neutron source at Los Alamos\n  National Laboratory and its implication for a possible neutron electric\n  dipole moment experiment Abstract: The ultracold neutron (UCN) source at Los Alamos National Laboratory (LANL),\nwhich uses solid deuterium as the UCN converter and is driven by accelerator\nspallation neutrons, has been successfully operated for over 10 years,\nproviding UCN to various experiments, as the first production UCN source based\non the superthermal process. It has recently undergone a major upgrade. This\npaper describes the design and performance of the upgraded LANL UCN source.\nMeasurements of the cold neutron spectrum and UCN density are presented and\ncompared to Monte Carlo predictions. The source is shown to perform as modeled.\nThe UCN density measured at the exit of the biological shield was $184(32)$\nUCN/cm$^3$, a four-fold increase from the highest previously reported. The\npolarized UCN density stored in an external chamber was measured to be $39(7)$\nUCN/cm$^3$, which is sufficient to perform an experiment to search for the\nnonzero neutron electric dipole moment with a one-standard-deviation\nsensitivity of $\\sigma(d_n) = 3\\times 10^{-27}$ $e\\cdot$cm. \n\n"}
{"id": "1710.05611", "contents": "Title: Analysis procedure of the positronium lifetime spectra for the J-PET\n  detector Abstract: Positron Annihilation Lifetime Spectroscopy (PALS) has shown to be a powerful\ntool to study the nanostructures of porous materials. Positron Emissions\nTomography (PET) are devices allowing imaging of metabolic processes e.g. in\nhuman bodies. A newly developed device, the J-PET (Jagiellonian PET), will\nallow PALS in addition to imaging, thus combining both analyses providing new\nmethods for physics and medicine. In this contribution we present a computer\nprogram that is compatible with the J-PET software. We compare its performance\nwith the standard program LT 9.0 by using PALS data from hexane measurements at\ndifferent temperatures. Our program is based on an iterative procedure, and our\nfits prove that it performs as good as LT 9.0. \n\n"}
{"id": "1710.05711", "contents": "Title: Deep Self-Paced Learning for Person Re-Identification Abstract: Person re-identification (Re-ID) usually suffers from noisy samples with\nbackground clutter and mutual occlusion, which makes it extremely difficult to\ndistinguish different individuals across the disjoint camera views. In this\npaper, we propose a novel deep self-paced learning (DSPL) algorithm to\nalleviate this problem, in which we apply a self-paced constraint and symmetric\nregularization to help the relative distance metric training the deep neural\nnetwork, so as to learn the stable and discriminative features for person\nRe-ID. Firstly, we propose a soft polynomial regularizer term which can derive\nthe adaptive weights to samples based on both the training loss and model age.\nAs a result, the high-confidence fidelity samples will be emphasized and the\nlow-confidence noisy samples will be suppressed at early stage of the whole\ntraining process. Such a learning regime is naturally implemented under a\nself-paced learning (SPL) framework, in which samples weights are adaptively\nupdated based on both model age and sample loss using an alternative\noptimization method. Secondly, we introduce a symmetric regularizer term to\nrevise the asymmetric gradient back-propagation derived by the relative\ndistance metric, so as to simultaneously minimize the intra-class distance and\nmaximize the inter-class distance in each triplet unit. Finally, we build a\npart-based deep neural network, in which the features of different body parts\nare first discriminately learned in the lower convolutional layers and then\nfused in the higher fully connected layers. Experiments on several benchmark\ndatasets have demonstrated the superior performance of our method as compared\nwith the state-of-the-art approaches. \n\n"}
{"id": "1710.05956", "contents": "Title: Isointense Infant Brain Segmentation with a Hyper-dense Connected\n  Convolutional Neural Network Abstract: Neonatal brain segmentation in magnetic resonance (MR) is a challenging\nproblem due to poor image quality and low contrast between white and gray\nmatter regions. Most existing approaches for this problem are based on\nmulti-atlas label fusion strategies, which are time-consuming and sensitive to\nregistration errors. As alternative to these methods, we propose a\nhyper-densely connected 3D convolutional neural network that employs MR-T1 and\nT2 images as input, which are processed independently in two separated paths.\nAn important difference with previous densely connected networks is the use of\ndirect connections between layers from the same and different paths. Adopting\nsuch dense connectivity helps the learning process by including deep\nsupervision and improving gradient flow. We evaluated our approach on data from\nthe MICCAI Grand Challenge on 6-month infant Brain MRI Segmentation (iSEG),\nobtaining very competitive results. Among 21 teams, our approach ranked first\nor second in most metrics, translating into a state-of-the-art performance. \n\n"}
{"id": "1710.07106", "contents": "Title: Combining {\\gamma}-ray and particle spectroscopy with SONIC@HORUS Abstract: The particle spectrometer SONIC for particle-$\\gamma$ coincidence\nmeasurements was commissioned at the Institute for Nuclear Physics in Cologne,\nGermany. SONIC consists of up to 12 silicon $\\mathit{\\Delta}E$-$E$ telescopes\nwith a total solid angle coverage of 9%, and will complement HORUS, a\n$\\gamma$-ray spectrometer with 14 HPGe detectors. The combined setup\nSONIC@HORUS is used to investigate the $\\gamma$-decay behaviour of low-spin\nstates up to the neutron separation threshold excited by light-ion inelastic\nscattering and transfer reactions using beams provided by a 10 MV FN Tandem\naccelerator. The particle-$\\gamma$ coincidence method will be presented using\ndata from a $^{92}$Mo(p,p'$\\gamma$) experiment. In a $^{119}$Sn(d,X)\nexperiment, excellent particle identification has been achieved because of the\ngood energy resolution of the silicon detectors of approximately 20 keV. Due to\nthe non-negligible momentum transfer in the reaction, a Doppler correction of\nthe detected $\\gamma$-ray energy has to be performed, using the additional\ninformation from measuring the ejectile energy and direction. The high\nsensitivity of the setup is demonstrated by the results from a\n$^{94}$Mo(p,p'$\\gamma$) experiment, where small $\\gamma$-decay branching ratios\nhave been deduced. \n\n"}
{"id": "1710.07498", "contents": "Title: MR to X-Ray Projection Image Synthesis Abstract: Hybrid imaging promises large potential in medical imaging applications. To\nfully utilize the possibilities of corresponding information from different\nmodalities, the information must be transferable between the domains. In\nradiation therapy planning, existing methods make use of reconstructed 3D\nmagnetic resonance imaging data to synthesize corresponding X-ray attenuation\nmaps. In contrast, for fluoroscopic procedures only line integral data, i.e.,\n2D projection images, are present. The question arises which approaches could\npotentially be used for this MR to X-ray projection image-to-image translation.\nWe examine three network architectures and two loss-functions regarding their\nsuitability as generator networks for this task. All generators proved to yield\nsuitable results for this task. A cascaded refinement network paired with a\nperceptual-loss function achieved the best qualitative results in our\nevaluation. The perceptual-loss showed to be able to preserve most of the\nhigh-frequency details in the projection images and, thus, is recommended for\nthe underlying task and similar problems. \n\n"}
{"id": "1710.09505", "contents": "Title: Knowledge Projection for Deep Neural Networks Abstract: While deeper and wider neural networks are actively pushing the performance\nlimits of various computer vision and machine learning tasks, they often\nrequire large sets of labeled data for effective training and suffer from\nextremely high computational complexity. In this paper, we will develop a new\nframework for training deep neural networks on datasets with limited labeled\nsamples using cross-network knowledge projection which is able to improve the\nnetwork performance while reducing the overall computational complexity\nsignificantly. Specifically, a large pre-trained teacher network is used to\nobserve samples from the training data. A projection matrix is learned to\nproject this teacher-level knowledge and its visual representations from an\nintermediate layer of the teacher network to an intermediate layer of a thinner\nand faster student network to guide and regulate its training process. Both the\nintermediate layers from the teacher network and the injection layers from the\nstudent network are adaptively selected during training by evaluating a joint\nloss function in an iterative manner. This knowledge projection framework\nallows us to use crucial knowledge learned by large networks to guide the\ntraining of thinner student networks, avoiding over-fitting, achieving better\nnetwork performance, and significantly reducing the complexity. Extensive\nexperimental results on benchmark datasets have demonstrated that our proposed\nknowledge projection approach outperforms existing methods, improving accuracy\nby up to 4% while reducing network complexity by 4 to 10 times, which is very\nattractive for practical applications of deep neural networks. \n\n"}
{"id": "1710.09762", "contents": "Title: How to Fool Radiologists with Generative Adversarial Networks? A Visual\n  Turing Test for Lung Cancer Diagnosis Abstract: Discriminating lung nodules as malignant or benign is still an underlying\nchallenge. To address this challenge, radiologists need computer aided\ndiagnosis (CAD) systems which can assist in learning discriminative imaging\nfeatures corresponding to malignant and benign nodules. However, learning\nhighly discriminative imaging features is an open problem. In this paper, our\naim is to learn the most discriminative features pertaining to lung nodules by\nusing an adversarial learning methodology. Specifically, we propose to use\nunsupervised learning with Deep Convolutional-Generative Adversarial Networks\n(DC-GANs) to generate lung nodule samples realistically. We hypothesize that\nimaging features of lung nodules will be discriminative if it is hard to\ndifferentiate them (fake) from real (true) nodules. To test this hypothesis, we\npresent Visual Turing tests to two radiologists in order to evaluate the\nquality of the generated (fake) nodules. Extensive comparisons are performed in\ndiscerning real, generated, benign, and malignant nodules. This experimental\nset up allows us to validate the overall quality of the generated nodules,\nwhich can then be used to (1) improve diagnostic decisions by mining highly\ndiscriminative imaging features, (2) train radiologists for educational\npurposes, and (3) generate realistic samples to train deep networks with big\ndata. \n\n"}
{"id": "1710.10386", "contents": "Title: Dual Skipping Networks Abstract: Inspired by the recent neuroscience studies on the left-right asymmetry of\nthe human brain in processing low and high spatial frequency information, this\npaper introduces a dual skipping network which carries out coarse-to-fine\nobject categorization. Such a network has two branches to simultaneously deal\nwith both coarse and fine-grained classification tasks. Specifically, we\npropose a layer-skipping mechanism that learns a gating network to predict\nwhich layers to skip in the testing stage. This layer-skipping mechanism endows\nthe network with good flexibility and capability in practice. Evaluations are\nconducted on several widely used coarse-to-fine object categorization\nbenchmarks, and promising results are achieved by our proposed network model. \n\n"}
{"id": "1710.11126", "contents": "Title: The extraction of 229Th3+ from a buffer-gas stopping cell Abstract: In the whole landscape of atomic nuclei, $^{229}$Th is currently the only\nknown nucleus which could allow for the development of a nuclear-based\nfrequency standard, as it possesses an isomeric state of just 7.6 eV energy\nabove the ground state. The 3+ charge state is of special importance in this\ncontext, as Th$^{3+}$ allows for a simple laser-cooling scheme. Here we\nemphasize the direct extraction of triply-charged $^{229}$Th from a buffer-gas\nstopping cell. This finding will not only simplify any future approach of\n$^{229}$Th ion cooling, but is also used for thorium-beam purification and in\nthis way provides a powerful tool for the direct identification of the\n$^{229}$Th isomer to ground state nuclear transition. \n\n"}
{"id": "1710.11431", "contents": "Title: Physics-guided Neural Networks (PGNN): An Application in Lake\n  Temperature Modeling Abstract: This paper introduces a framework for combining scientific knowledge of\nphysics-based models with neural networks to advance scientific discovery. This\nframework, termed physics-guided neural networks (PGNN), leverages the output\nof physics-based model simulations along with observational features in a\nhybrid modeling setup to generate predictions using a neural network\narchitecture. Further, this framework uses physics-based loss functions in the\nlearning objective of neural networks to ensure that the model predictions not\nonly show lower errors on the training set but are also scientifically\nconsistent with the known physics on the unlabeled set. We illustrate the\neffectiveness of PGNN for the problem of lake temperature modeling, where\nphysical relationships between the temperature, density, and depth of water are\nused to design a physics-based loss function. By using scientific knowledge to\nguide the construction and learning of neural networks, we are able to show\nthat the proposed framework ensures better generalizability as well as\nscientific consistency of results. All the code and datasets used in this study\nhave been made available on this link \\url{https://github.com/arkadaw9/PGNN}. \n\n"}
{"id": "1711.00583", "contents": "Title: Deep Learning from Noisy Image Labels with Quality Embedding Abstract: There is an emerging trend to leverage noisy image datasets in many visual\nrecognition tasks. However, the label noise among the datasets severely\ndegenerates the \\mbox{performance of deep} learning approaches. Recently, one\nmainstream is to introduce the latent label to handle label noise, which has\nshown promising improvement in the network designs. Nevertheless, the mismatch\nbetween latent labels and noisy labels still affects the predictions in such\nmethods. To address this issue, we propose a quality embedding model, which\nexplicitly introduces a quality variable to represent the trustworthiness of\nnoisy labels. Our key idea is to identify the mismatch between the latent and\nnoisy labels by embedding the quality variables into different subspaces, which\neffectively minimizes the noise effect. At the same time, the high-quality\nlabels is still able to be applied for training. To instantiate the model, we\nfurther propose a Contrastive-Additive Noise network (CAN), which consists of\ntwo important layers: (1) the contrastive layer estimates the quality variable\nin the embedding space to reduce noise effect; and (2) the additive layer\naggregates the prior predictions and noisy labels as the posterior to train the\nclassifier. Moreover, to tackle the optimization difficulty, we deduce an SGD\nalgorithm with the reparameterization tricks, which makes our method scalable\nto big data. We conduct the experimental evaluation of the proposed method over\na range of noisy image datasets. Comprehensive results have demonstrated CAN\noutperforms the state-of-the-art deep learning approaches. \n\n"}
{"id": "1711.01043", "contents": "Title: A Taught-Obesrve-Ask (TOA) Method for Object Detection with Critical\n  Supervision Abstract: Being inspired by child's learning experience - taught first and followed by\nobservation and questioning, we investigate a critically supervised learning\nmethodology for object detection in this work. Specifically, we propose a\ntaught-observe-ask (TOA) method that consists of several novel components such\nas negative object proposal, critical example mining, and machine-guided\nquestion-answer (QA) labeling. To consider labeling time and performance\njointly, new evaluation methods are developed to compare the performance of the\nTOA method, with the fully and weakly supervised learning methods. Extensive\nexperiments are conducted on the PASCAL VOC and the Caltech benchmark datasets.\nThe TOA method provides significantly improved performance of weakly\nsupervision yet demands only about 3-6% of labeling time of full supervision.\nThe effectiveness of each novel component is also analyzed. \n\n"}
{"id": "1711.01452", "contents": "Title: Upgrade for Phase II of the GERDA Experiment Abstract: The GERDA collaboration is performing a sensitive search for neutrinoless\ndouble beta decay of $^{76}$Ge at the INFN Laboratori Nazionali del Gran Sasso,\nItaly. The upgrade of the GERDA experiment from Phase I to Phase II has been\nconcluded in December 2015. The first Phase II data release shows that the goal\nto suppress the background by one order of magnitude compared to Phase I has\nbeen achieved. GERDA is thus the first experiment that will remain\nbackground-free up to its design exposure (100 kg yr). It will reach thereby a\nhalf-life sensitivity of more than 10$^{26}$ yr within 3 years of data\ncollection. This paper describes in detail the modifications and improvements\nof the experimental setup for Phase II and discusses the performance of\nindividual detector components. \n\n"}
{"id": "1711.01573", "contents": "Title: The Local Dimension of Deep Manifold Abstract: Based on our observation that there exists a dramatic drop for the singular\nvalues of the fully connected layers or a single feature map of the\nconvolutional layer, and that the dimension of the concatenated feature vector\nalmost equals the summation of the dimension on each feature map, we propose a\nsingular value decomposition (SVD) based approach to estimate the dimension of\nthe deep manifolds for a typical convolutional neural network VGG19. We choose\nthree categories from the ImageNet, namely Persian Cat, Container Ship and\nVolcano, and determine the local dimension of the deep manifolds of the deep\nlayers through the tangent space of a target image. Through several\naugmentation methods, we found that the Gaussian noise method is closer to the\nintrinsic dimension, as by adding random noise to an image we are moving in an\narbitrary dimension, and when the rank of the feature matrix of the augmented\nimages does not increase we are very close to the local dimension of the\nmanifold. We also estimate the dimension of the deep manifold based on the\ntangent space for each of the maxpooling layers. Our results show that the\ndimensions of different categories are close to each other and decline quickly\nalong the convolutional layers and fully connected layers. Furthermore, we show\nthat the dimensions decline quickly inside the Conv5 layer. Our work provides\nnew insights for the intrinsic structure of deep neural networks and helps\nunveiling the inner organization of the black box of deep neural networks. \n\n"}
{"id": "1711.02718", "contents": "Title: Curve-Structure Segmentation from Depth Maps: A CNN-based Approach and\n  Its Application to Exploring Cultural Heritage Objects Abstract: Motivated by the important archaeological application of exploring cultural\nheritage objects, in this paper we study the challenging problem of\nautomatically segmenting curve structures that are very weakly stamped or\ncarved on an object surface in the form of a highly noisy depth map. Different\nfrom most classical low-level image segmentation methods that are known to be\nvery sensitive to the noise and occlusions, we propose a new supervised\nlearning algorithm based on Convolutional Neural Network (CNN) to implicitly\nlearn and utilize more curve geometry and pattern information for addressing\nthis challenging problem. More specifically, we first propose a Fully\nConvolutional Network (FCN) to estimate the skeleton of curve structures and at\neach skeleton pixel, a scale value is estimated to reflect the local curve\nwidth. Then we propose a dense prediction network to refine the estimated curve\nskeletons. Based on the estimated scale values, we finally develop an adaptive\nthresholding algorithm to achieve the final segmentation of curve structures.\nIn the experiment, we validate the performance of the proposed method on a\ndataset of depth images scanned from unearthed pottery sherds dating to the\nWoodland period of Southeastern North America. \n\n"}
{"id": "1711.03177", "contents": "Title: Spectral analysis for the Majorana Demonstrator experiment Abstract: The MAJORANA DEMONSTRATOR is an experiment constructed to search for\nneutrinoless double-beta decays in germanium-76 and to demonstrate the\nfeasibility to deploy a ton-scale experiment in a phased and modular fashion.\nIt consists of two modular arrays of natural and $^{76}\\textrm{Ge}$-enriched\ngermanium detectors totaling 44.1 kg (29.7 kg enriched detectors), located at\nthe 4850' level of the Sanford Underground Research Facility in Lead, South\nDakota, USA. Data taken with this setup since summer 2015 at different\nconstruction stages of the experiment show a clear reduction of the observed\nbackground index around the ROI for $0\\nu\\beta\\beta$-decay search due to\nimprovements in shielding. We discuss the statistical approaches to search for\na $0\\nu\\beta\\beta$-signal and derive the physics sensitivity for an expected\nexposure of $10\\,\\textrm{kg}{\\cdot}\\textrm{y}$ from enriched detectors using a\nprofile likelihood based hypothesis test in combination with toy Monte Carlo\ndata. \n\n"}
{"id": "1711.03677", "contents": "Title: Egocentric Hand Detection Via Dynamic Region Growing Abstract: Egocentric videos, which mainly record the activities carried out by the\nusers of the wearable cameras, have drawn much research attentions in recent\nyears. Due to its lengthy content, a large number of ego-related applications\nhave been developed to abstract the captured videos. As the users are\naccustomed to interacting with the target objects using their own hands while\ntheir hands usually appear within their visual fields during the interaction,\nan egocentric hand detection step is involved in tasks like gesture\nrecognition, action recognition and social interaction understanding. In this\nwork, we propose a dynamic region growing approach for hand region detection in\negocentric videos, by jointly considering hand-related motion and egocentric\ncues. We first determine seed regions that most likely belong to the hand, by\nanalyzing the motion patterns across successive frames. The hand regions can\nthen be located by extending from the seed regions, according to the scores\ncomputed for the adjacent superpixels. These scores are derived from four\negocentric cues: contrast, location, position consistency and appearance\ncontinuity. We discuss how to apply the proposed method in real-life scenarios,\nwhere multiple hands irregularly appear and disappear from the videos.\nExperimental results on public datasets show that the proposed method achieves\nsuperior performance compared with the state-of-the-art methods, especially in\ncomplicated scenarios. \n\n"}
{"id": "1711.04161", "contents": "Title: End-to-end Video-level Representation Learning for Action Recognition Abstract: From the frame/clip-level feature learning to the video-level representation\nbuilding, deep learning methods in action recognition have developed rapidly in\nrecent years. However, current methods suffer from the confusion caused by\npartial observation training, or without end-to-end learning, or restricted to\nsingle temporal scale modeling and so on. In this paper, we build upon\ntwo-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling\n(DTPP), an end-to-end video-level representation learning approach, to address\nthese problems. Specifically, at first, RGB images and optical flow stacks are\nsparsely sampled across the whole video. Then a temporal pyramid pooling layer\nis used to aggregate the frame-level features which consist of spatial and\ntemporal cues. Lastly, the trained model has compact video-level representation\nwith multiple temporal scales, which is both global and sequence-aware.\nExperimental results show that DTPP achieves the state-of-the-art performance\non two challenging video action datasets: UCF101 and HMDB51, either by ImageNet\npre-training or Kinetics pre-training. \n\n"}
{"id": "1711.05246", "contents": "Title: Loss Functions for Multiset Prediction Abstract: We study the problem of multiset prediction. The goal of multiset prediction\nis to train a predictor that maps an input to a multiset consisting of multiple\nitems. Unlike existing problems in supervised learning, such as classification,\nranking and sequence generation, there is no known order among items in a\ntarget multiset, and each item in the multiset may appear more than once,\nmaking this problem extremely challenging. In this paper, we propose a novel\nmultiset loss function by viewing this problem from the perspective of\nsequential decision making. The proposed multiset loss function is empirically\nevaluated on two families of datasets, one synthetic and the other real, with\nvarying levels of difficulty, against various baseline loss functions including\nreinforcement learning, sequence, and aggregated distribution matching loss\nfunctions. The experiments reveal the effectiveness of the proposed loss\nfunction over the others. \n\n"}
{"id": "1711.05859", "contents": "Title: Hybrid Approach of Relation Network and Localized Graph Convolutional\n  Filtering for Breast Cancer Subtype Classification Abstract: Network biology has been successfully used to help reveal complex mechanisms\nof disease, especially cancer. On the other hand, network biology requires\nin-depth knowledge to construct disease-specific networks, but our current\nknowledge is very limited even with the recent advances in human cancer\nbiology. Deep learning has shown a great potential to address the difficult\nsituation like this. However, deep learning technologies conventionally use\ngrid-like structured data, thus application of deep learning technologies to\nthe classification of human disease subtypes is yet to be explored. Recently,\ngraph based deep learning techniques have emerged, which becomes an opportunity\nto leverage analyses in network biology. In this paper, we proposed a hybrid\nmodel, which integrates two key components 1) graph convolution neural network\n(graph CNN) and 2) relation network (RN). We utilize graph CNN as a component\nto learn expression patterns of cooperative gene community, and RN as a\ncomponent to learn associations between learned patterns. The proposed model is\napplied to the PAM50 breast cancer subtype classification task, the standard\nbreast cancer subtype classification of clinical utility. In experiments of\nboth subtype classification and patient survival analysis, our proposed method\nachieved significantly better performances than existing methods. We believe\nthat this work is an important starting point to realize the upcoming\npersonalized medicine. \n\n"}
{"id": "1711.05929", "contents": "Title: Defense against Universal Adversarial Perturbations Abstract: Recent advances in Deep Learning show the existence of image-agnostic\nquasi-imperceptible perturbations that when applied to `any' image can fool a\nstate-of-the-art network classifier to change its prediction about the image\nlabel. These `Universal Adversarial Perturbations' pose a serious threat to the\nsuccess of Deep Learning in practice. We present the first dedicated framework\nto effectively defend the networks against such perturbations. Our approach\nlearns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a\ntargeted model, such that the targeted model needs no modification. The PRN is\nlearned from real and synthetic image-agnostic perturbations, where an\nefficient method to compute the latter is also proposed. A perturbation\ndetector is separately trained on the Discrete Cosine Transform of the\ninput-output difference of the PRN. A query image is first passed through the\nPRN and verified by the detector. If a perturbation is detected, the output of\nthe PRN is used for label prediction instead of the actual image. A rigorous\nevaluation shows that our framework can defend the network classifiers against\nunseen adversarial perturbations in the real-world scenarios with up to 97.5%\nsuccess rate. The PRN also generalizes well in the sense that training for one\ntargeted network defends another network with a comparable success rate. \n\n"}
{"id": "1711.06011", "contents": "Title: DIMAL: Deep Isometric Manifold Learning Using Sparse Geodesic Sampling Abstract: This paper explores a fully unsupervised deep learning approach for computing\ndistance-preserving maps that generate low-dimensional embeddings for a certain\nclass of manifolds. We use the Siamese configuration to train a neural network\nto solve the problem of least squares multidimensional scaling for generating\nmaps that approximately preserve geodesic distances. By training with only a\nfew landmarks, we show a significantly improved local and nonlocal\ngeneralization of the isometric mapping as compared to analogous non-parametric\ncounterparts. Importantly, the combination of a deep-learning framework with a\nmultidimensional scaling objective enables a numerical analysis of network\narchitectures to aid in understanding their representation power. This provides\na geometric perspective to the generalizability of deep learning. \n\n"}
{"id": "1711.06794", "contents": "Title: Co-attending Free-form Regions and Detections with Multi-modal\n  Multiplicative Feature Embedding for Visual Question Answering Abstract: Recently, the Visual Question Answering (VQA) task has gained increasing\nattention in artificial intelligence. Existing VQA methods mainly adopt the\nvisual attention mechanism to associate the input question with corresponding\nimage regions for effective question answering. The free-form region based and\nthe detection-based visual attention mechanisms are mostly investigated, with\nthe former ones attending free-form image regions and the latter ones attending\npre-specified detection-box regions. We argue that the two attention mechanisms\nare able to provide complementary information and should be effectively\nintegrated to better solve the VQA problem. In this paper, we propose a novel\ndeep neural network for VQA that integrates both attention mechanisms. Our\nproposed framework effectively fuses features from free-form image regions,\ndetection boxes, and question representations via a multi-modal multiplicative\nfeature embedding scheme to jointly attend question-related free-form image\nregions and detection boxes for more accurate question answering. The proposed\nmethod is extensively evaluated on two publicly available datasets, COCO-QA and\nVQA, and outperforms state-of-the-art approaches. Source code is available at\nhttps://github.com/lupantech/dual-mfa-vqa. \n\n"}
{"id": "1711.07131", "contents": "Title: CleanNet: Transfer Learning for Scalable Image Classifier Training with\n  Label Noise Abstract: In this paper, we study the problem of learning image classification models\nwith label noise. Existing approaches depending on human supervision are\ngenerally not scalable as manually identifying correct or incorrect labels is\ntime-consuming, whereas approaches not relying on human supervision are\nscalable but less effective. To reduce the amount of human supervision for\nlabel noise cleaning, we introduce CleanNet, a joint neural embedding network,\nwhich only requires a fraction of the classes being manually verified to\nprovide the knowledge of label noise that can be transferred to other classes.\nWe further integrate CleanNet and conventional convolutional neural network\nclassifier into one framework for image classification learning. We demonstrate\nthe effectiveness of the proposed algorithm on both of the label noise\ndetection task and the image classification on noisy data task on several\nlarge-scale datasets. Experimental results show that CleanNet can reduce label\nnoise detection error rate on held-out classes where no human supervision\navailable by 41.5% compared to current weakly supervised methods. It also\nachieves 47% of the performance gain of verifying all images with only 3.2%\nimages verified on an image classification task. Source code and dataset will\nbe available at kuanghuei.github.io/CleanNetProject. \n\n"}
{"id": "1711.07401", "contents": "Title: First extraction of the scalar proton dynamical polarizabilities from\n  real Compton scattering data Abstract: We present the first attempt to extract the scalar dipole dynamical\npolarizabilities from proton real Compton scattering data below pion-production\nthreshold. The theoretical framework combines dispersion relations technique,\nlow-energy expansion and multipole decomposition of the scattering amplitudes.\nThe results are obtained with statistical tools that have never been applied so\nfar to Compton scattering data and are crucial to overcome problems inherent to\nthe analysis of the available data set. \n\n"}
{"id": "1711.07624", "contents": "Title: A deep learning-based method for relative location prediction in CT scan\n  images Abstract: Relative location prediction in computed tomography (CT) scan images is a\nchallenging problem. In this paper, a regression model based on one-dimensional\nconvolutional neural networks is proposed to determine the relative location of\na CT scan image both robustly and precisely. A public dataset is employed to\nvalidate the performance of the study's proposed method using a 5-fold cross\nvalidation. Experimental results demonstrate an excellent performance of the\nproposed model when compared with the state-of-the-art techniques, achieving a\nmedian absolute error of 1.04 cm and mean absolute error of 1.69 cm. \n\n"}
{"id": "1711.07752", "contents": "Title: Repulsion Loss: Detecting Pedestrians in a Crowd Abstract: Detecting individual pedestrians in a crowd remains a challenging problem\nsince the pedestrians often gather together and occlude each other in\nreal-world scenarios. In this paper, we first explore how a state-of-the-art\npedestrian detector is harmed by crowd occlusion via experimentation, providing\ninsights into the crowd occlusion problem. Then, we propose a novel bounding\nbox regression loss specifically designed for crowd scenes, termed repulsion\nloss. This loss is driven by two motivations: the attraction by target, and the\nrepulsion by other surrounding objects. The repulsion term prevents the\nproposal from shifting to surrounding objects thus leading to more crowd-robust\nlocalization. Our detector trained by repulsion loss outperforms all the\nstate-of-the-art methods with a significant improvement in occlusion cases. \n\n"}
{"id": "1711.07936", "contents": "Title: Radon mitigation during the installation of the CUORE $0\\nu\\beta\\beta$\n  decay detector Abstract: CUORE - the Cryogenic Underground Observatory for Rare Events - is an\nexperiment searching for the neutrinoless double-beta ($0\\nu\\beta\\beta$) decay\nof $^{130}$Te with an array of 988 TeO$_2$ crystals operated as bolometers at\n$\\sim$10 mK in a large dilution refrigerator. With this detector, we aim for a\n$^{130}$Te $0\\nu\\beta\\beta$ decay half-life sensitivity of $9\\times10^{25}$ y\nwith 5 y of live time, and a background index of $\\lesssim 10^{-2}$\ncounts/keV/kg/y. Making an effort to maintain radiopurity by minimizing the\nbolometers' exposure to radon gas during their installation in the cryostat, we\nperform all operations inside a dedicated cleanroom environment with a\ncontrolled radon-reduced atmosphere. In this paper, we discuss the design and\nperformance of the CUORE Radon Abatement System and cleanroom, as well as a\nsystem to monitor the radon level in real time. \n\n"}
{"id": "1711.08324", "contents": "Title: Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky\n  Noisy-or Network Abstract: Automatic diagnosing lung cancer from Computed Tomography (CT) scans involves\ntwo steps: detect all suspicious lesions (pulmonary nodules) and evaluate the\nwhole-lung/pulmonary malignancy. Currently, there are many studies about the\nfirst step, but few about the second step. Since the existence of nodule does\nnot definitely indicate cancer, and the morphology of nodule has a complicated\nrelationship with cancer, the diagnosis of lung cancer demands careful\ninvestigations on every suspicious nodule and integration of information of all\nnodules. We propose a 3D deep neural network to solve this problem. The model\nconsists of two modules. The first one is a 3D region proposal network for\nnodule detection, which outputs all suspicious nodules for a subject. The\nsecond one selects the top five nodules based on the detection confidence,\nevaluates their cancer probabilities and combines them with a leaky noisy-or\ngate to obtain the probability of lung cancer for the subject. The two modules\nshare the same backbone network, a modified U-net. The over-fitting caused by\nthe shortage of training data is alleviated by training the two modules\nalternately. The proposed model won the first place in the Data Science Bowl\n2017 competition. The code has been made publicly available. \n\n"}
{"id": "1711.08502", "contents": "Title: Train, Diagnose and Fix: Interpretable Approach for Fine-grained Action\n  Recognition Abstract: Despite the growing discriminative capabilities of modern deep learning\nmethods for recognition tasks, the inner workings of the state-of-art models\nstill remain mostly black-boxes. In this paper, we propose a systematic\ninterpretation of model parameters and hidden representations of Residual\nTemporal Convolutional Networks (Res-TCN) for action recognition in time-series\ndata. We also propose a Feature Map Decoder as part of the interpretation\nanalysis, which outputs a representation of model's hidden variables in the\nsame domain as the input. Such analysis empowers us to expose model's\ncharacteristic learning patterns in an interpretable way. For example, through\nthe diagnosis analysis, we discovered that our model has learned to achieve\nview-point invariance by implicitly learning to perform rotational\nnormalization of the input to a more discriminative view. Based on the findings\nfrom the model interpretation analysis, we propose a targeted refinement\ntechnique, which can generalize to various other recognition models. The\nproposed work introduces a three-stage paradigm for model learning: training,\ninterpretable diagnosis and targeted refinement. We validate our approach on\nskeleton based 3D human action recognition benchmark of NTU RGB+D. We show that\nthe proposed workflow is an effective model learning strategy and the resulting\nMulti-stream Residual Temporal Convolutional Network (MS-Res-TCN) achieves the\nstate-of-the-art performance on NTU RGB+D. \n\n"}
{"id": "1711.08972", "contents": "Title: Image Generation from Sketch Constraint Using Contextual GAN Abstract: In this paper we investigate image generation guided by hand sketch. When the\ninput sketch is badly drawn, the output of common image-to-image translation\nfollows the input edges due to the hard condition imposed by the translation\nprocess. Instead, we propose to use sketch as weak constraint, where the output\nedges do not necessarily follow the input edges. We address this problem using\na novel joint image completion approach, where the sketch provides the image\ncontext for completing, or generating the output image. We train a generated\nadversarial network, i.e, contextual GAN to learn the joint distribution of\nsketch and the corresponding image by using joint images. Our contextual GAN\nhas several advantages. First, the simple joint image representation allows for\nsimple and effective learning of joint distribution in the same image-sketch\nspace, which avoids complicated issues in cross-domain learning. Second, while\nthe output is related to its input overall, the generated features exhibit more\nfreedom in appearance and do not strictly align with the input features as\nprevious conditional GANs do. Third, from the joint image's point of view,\nimage and sketch are of no difference, thus exactly the same deep joint image\ncompletion network can be used for image-to-sketch generation. Experiments\nevaluated on three different datasets show that our contextual GAN can generate\nmore realistic images than state-of-the-art conditional GANs on challenging\ninputs and generalize well on common categories. \n\n"}
{"id": "1711.09089", "contents": "Title: Design and Performance of the Spin Asymmetries of the Nucleon Experiment Abstract: The Spin Asymmetries of the Nucleon Experiment (SANE) performed inclusive,\ndouble-polarized electron scattering measurements of the proton at the\nContinuous Electron Beam Accelerator Facility at Jefferson Lab. A novel\ndetector array observed scattered electrons of four-momentum transfer $2.5 <\nQ^2< 6.5$ GeV$^2$ and Bjorken scaling $0.3<x<0.8$ from initial beam energies of\n4.7 and 5.9 GeV. Employing a polarized proton target whose magnetic field\ndirection could be rotated with respect to the incident electron beam, both\nparallel and near perpendicular spin asymmetries were measured, allowing\nmodel-independent access to transverse polarization observables $A_1$, $A_2$,\n$g_1$, $g_2$ and moment $d_2$ of the proton. This document summarizes the\noperation and performance of the polarized target, polarized electron beam, and\nnovel detector systems used during the course of the experiment, and describes\nanalysis techniques utilized to access the physics observables of interest. \n\n"}
{"id": "1711.09191", "contents": "Title: Multiple Instance Curriculum Learning for Weakly Supervised Object\n  Detection Abstract: When supervising an object detector with weakly labeled data, most existing\napproaches are prone to trapping in the discriminative object parts, e.g.,\nfinding the face of a cat instead of the full body, due to lacking the\nsupervision on the extent of full objects. To address this challenge, we\nincorporate object segmentation into the detector training, which guides the\nmodel to correctly localize the full objects. We propose the multiple instance\ncurriculum learning (MICL) method, which injects curriculum learning (CL) into\nthe multiple instance learning (MIL) framework. The MICL method starts by\nautomatically picking the easy training examples, where the extent of the\nsegmentation masks agree with detection bounding boxes. The training set is\ngradually expanded to include harder examples to train strong detectors that\nhandle complex images. The proposed MICL method with segmentation in the loop\noutperforms the state-of-the-art weakly supervised object detectors by a\nsubstantial margin on the PASCAL VOC datasets. \n\n"}
{"id": "1711.09345", "contents": "Title: Semantically Consistent Image Completion with Fine-grained Details Abstract: Image completion has achieved significant progress due to advances in\ngenerative adversarial networks (GANs). Albeit natural-looking, the synthesized\ncontents still lack details, especially for scenes with complex structures or\nimages with large holes. This is because there exists a gap between low-level\nreconstruction loss and high-level adversarial loss. To address this issue, we\nintroduce a perceptual network to provide mid-level guidance, which measures\nthe semantical similarity between the synthesized and original contents in a\nsimilarity-enhanced space. We conduct a detailed analysis on the effects of\ndifferent losses and different levels of perceptual features in image\ncompletion, showing that there exist complementarity between adversarial\ntraining and perceptual features. By combining them together, our model can\nachieve nearly seamless fusion results in an end-to-end manner. Moreover, we\ndesign an effective lightweight generator architecture, which can achieve\neffective image inpainting with far less parameters. Evaluated on CelebA Face\nand Paris StreetView dataset, our proposed method significantly outperforms\nexisting methods. \n\n"}
{"id": "1711.09553", "contents": "Title: Accessible Melanoma Detection using Smartphones and Mobile Image\n  Analysis Abstract: We investigate the design of an entire mobile imaging system for early\ndetection of melanoma. Different from previous work, we focus on\nsmartphone-captured visible light images. Our design addresses two major\nchallenges. First, images acquired using a smartphone under loosely-controlled\nenvironmental conditions may be subject to various distortions, and this makes\nmelanoma detection more difficult. Second, processing performed on a smartphone\nis subject to stringent computation and memory constraints. In our work, we\npropose a detection system that is optimized to run entirely on the\nresource-constrained smartphone. Our system intends to localize the skin lesion\nby combining a lightweight method for skin detection with a hierarchical\nsegmentation approach using two fast segmentation methods. Moreover, we study\nan extensive set of image features and propose new numerical features to\ncharacterize a skin lesion. Furthermore, we propose an improved feature\nselection algorithm to determine a small set of discriminative features used by\nthe final lightweight system. In addition, we study the human-computer\ninterface (HCI) design to understand the usability and acceptance issues of the\nproposed system. \n\n"}
{"id": "1711.10143", "contents": "Title: Revisiting hand-crafted feature for action recognition: a set of\n  improved dense trajectories Abstract: We propose a feature for action recognition called Trajectory-Set (TS), on\ntop of the improved Dense Trajectory (iDT). The TS feature encodes only\ntrajectories around densely sampled interest points, without any appearance\nfeatures. Experimental results on the UCF50, UCF101, and HMDB51 action datasets\ndemonstrate that TS is comparable to state-of-the-arts, and outperforms many\nother methods; for HMDB the accuracy of 85.4%, compared to the best accuracy of\n80.2% obtained by a deep method. Our code is available on-line at\nhttps://github.com/Gauffret/TrajectorySet . \n\n"}
{"id": "1711.10267", "contents": "Title: Differential Generative Adversarial Networks: Synthesizing Non-linear\n  Facial Variations with Limited Number of Training Data Abstract: In face-related applications with a public available dataset, synthesizing\nnon-linear facial variations (e.g., facial expression, head-pose, illumination,\netc.) through a generative model is helpful in addressing the lack of training\ndata. In reality, however, there is insufficient data to even train the\ngenerative model for face synthesis. In this paper, we propose Differential\nGenerative Adversarial Networks (D-GAN) that can perform photo-realistic face\nsynthesis even when training data is small. Two discriminators are devised to\nensure the generator to approximate a face manifold, which can express face\nchanges as it wants. Experimental results demonstrate that the proposed method\nis robust to the amount of training data and synthesized images are useful to\nimprove the performance of a face expression classifier. \n\n"}
{"id": "1711.10288", "contents": "Title: Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain\n  Adaptation Abstract: In this work, we face the problem of unsupervised domain adaptation with a\nnovel deep learning approach which leverages on our finding that entropy\nminimization is induced by the optimal alignment of second order statistics\nbetween source and target domains. We formally demonstrate this hypothesis and,\naiming at achieving an optimal alignment in practical cases, we adopt a more\nprincipled strategy which, differently from the current Euclidean approaches,\ndeploys alignment along geodesics. Our pipeline can be implemented by adding to\nthe standard classification loss (on the labeled source domain), a\nsource-to-target regularizer that is weighted in an unsupervised and\ndata-driven fashion. We provide extensive experiments to assess the superiority\nof our framework on standard domain and modality adaptation benchmarks. \n\n"}
{"id": "1711.10361", "contents": "Title: Contamination Control and Assay Results for the Majorana Demonstrator\n  Ultra Clean Components Abstract: The MAJORANA DEMONSTRATOR is a neutrinoless double beta decay experiment\nutilizing enriched Ge-76 detectors in 2 separate modules inside of a common\nsolid shield at the Sanford Underground Research Facility. The DEMONSTRATOR has\nutilized world leading assay sensitivities to develop clean materials and\nprocesses for producing ultra-pure copper and plastic components. This\nexperiment is now operating, and initial data provide new insights into the\nsuccess of cleaning and processing. Post production copper assays after the\ncompletion of Module 1 showed an increase in U and Th contamination in finished\nparts compared to starting bulk material. A revised cleaning method and\nadditional round of surface contamination studies prior to Module 2\nconstruction have provided evidence that more rigorous process control can\nreduce surface contamination. This article describes the assay results and\ndiscuss further studies to take advantage of assay capabilities for the purpose\nof maintaining ultra clean fabrication and process design. \n\n"}
{"id": "1711.11145", "contents": "Title: Initial Results from the Majorana Demonstrator Abstract: The MAJORANA Collaboration has assembled an array of high purity Ge detectors\nto search for neutrinoless double-beta decay in $^{76}$Ge with the goal of\nestablishing the required background and scalability of a Ge-based\nnext-generation ton-scale experiment. The MAJORANA DEMONSTRATOR consists of 44\nkg of high-purity Ge (HPGe) detectors (30 kg enriched in $^{76}$Ge) with a\nlow-noise p-type point contact (PPC) geometry. The detectors are split between\ntwo modules which are contained in a single lead and high-purity copper shield\nat the Sanford Underground Research Facility in Lead, South Dakota. Following a\ncommissioning run that started in June 2015, the full detector array has been\nacquiring data since August 2016. We will discuss the status of the MAJORANA\nDEMONSTRATOR and initial results from the first physics run; including current\nbackground estimates, exotic low-energy physics searches, projections on the\nphysics reach of the DEMONSTRATOR, and implications for a ton-scale Ge-based\nneutrinoless double-beta decay search. \n\n"}
{"id": "1711.11152", "contents": "Title: Optical Flow Guided Feature: A Fast and Robust Motion Representation for\n  Video Action Recognition Abstract: Motion representation plays a vital role in human action recognition in\nvideos. In this study, we introduce a novel compact motion representation for\nvideo action recognition, named Optical Flow guided Feature (OFF), which\nenables the network to distill temporal information through a fast and robust\napproach. The OFF is derived from the definition of optical flow and is\northogonal to the optical flow. The derivation also provides theoretical\nsupport for using the difference between two frames. By directly calculating\npixel-wise spatiotemporal gradients of the deep feature maps, the OFF could be\nembedded in any existing CNN based video action recognition framework with only\na slight additional cost. It enables the CNN to extract spatiotemporal\ninformation, especially the temporal information between frames simultaneously.\nThis simple but powerful idea is validated by experimental results. The network\nwith OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on\nUCF-101, which is comparable with the result obtained by two streams (RGB and\noptical flow), but is 15 times faster in speed. Experimental results also show\nthat OFF is complementary to other motion modalities such as optical flow. When\nthe proposed method is plugged into the state-of-the-art video action\nrecognition framework, it has 96:0% and 74:2% accuracy on UCF-101 and HMDB-51\nrespectively. The code for this project is available at\nhttps://github.com/kevin-ssy/Optical-Flow-Guided-Feature. \n\n"}
{"id": "1711.11155", "contents": "Title: Predicting Depression Severity by Multi-Modal Feature Engineering and\n  Fusion Abstract: We present our preliminary work to determine if patient's vocal acoustic,\nlinguistic, and facial patterns could predict clinical ratings of depression\nseverity, namely Patient Health Questionnaire depression scale (PHQ-8). We\nproposed a multi modal fusion model that combines three different modalities:\naudio, video , and text features. By training over AVEC 2017 data set, our\nproposed model outperforms each single modality prediction model, and surpasses\nthe data set baseline with ice margin. \n\n"}
{"id": "1712.00971", "contents": "Title: Face Translation between Images and Videos using Identity-aware CycleGAN Abstract: This paper presents a new problem of unpaired face translation between images\nand videos, which can be applied to facial video prediction and enhancement. In\nthis problem there exist two major technical challenges: 1) designing a robust\ntranslation model between static images and dynamic videos, and 2) preserving\nfacial identity during image-video translation. To address such two problems,\nwe generalize the state-of-the-art image-to-image translation network\n(Cycle-Consistent Adversarial Networks) to the image-to-video/video-to-image\ntranslation context by exploiting a image-video translation model and an\nidentity preservation model. In particular, we apply the state-of-the-art\nWasserstein GAN technique to the setting of image-video translation for better\nconvergence, and we meanwhile introduce a face verificator to ensure the\nidentity. Experiments on standard image/video face datasets demonstrate the\neffectiveness of the proposed model in both terms of qualitative and\nquantitative evaluations. \n\n"}
{"id": "1712.01831", "contents": "Title: Precise Neutron Lifetime Measurement with a Solenoidal Coil Abstract: The neutron lifetime, $\\tau$ = 880.2 $\\pm$ 1.0 sec , is an important\nparameter for particle physics and cosmology. There is, however, an 8.4 sec\n(4.0$\\,\\sigma$) deviation between the measured value of the neutron lifetime\nusing two methods : one method counts neutrons that survive after some time,\nwhile the other counts protons resulting from neutron beta decay. A new method\nis being implemented at J-PARC / MLF / BL05 using a pulsed cold neutron beam. A\nTime Projection Chamber (TPC) records both the electrons from neutron beta\ndecay and protons from the neutron-$^3$He capture reactions in order to\nestimate the neutron flux. Electron background signals require the largest\ncorrection and are source of uncertainty for this experiment. A solenoidal\nmagnetic field can greatly reduce this background. The TPC drift region must be\ndivided into three region in this case. A prototype detector was developed to\nstudy the multi drift layer TPC. The status of a study using a prototype\ndetector is reported in this paper. \n\n"}
{"id": "1712.02050", "contents": "Title: Unsupervised Multi-Domain Image Translation with Domain-Specific\n  Encoders/Decoders Abstract: Unsupervised Image-to-Image Translation achieves spectacularly advanced\ndevelopments nowadays. However, recent approaches mainly focus on one model\nwith two domains, which may face heavy burdens with large cost of $O(n^2)$\ntraining time and model parameters, under such a requirement that $n$ domains\nare freely transferred to each other in a general setting. To address this\nproblem, we propose a novel and unified framework named Domain-Bank, which\nconsists of a global shared auto-encoder and $n$ domain-specific\nencoders/decoders, assuming that a universal shared-latent sapce can be\nprojected. Thus, we yield $O(n)$ complexity in model parameters along with a\nhuge reduction of the time budgets. Besides the high efficiency, we show the\ncomparable (or even better) image translation results over state-of-the-arts on\nvarious challenging unsupervised image translation tasks, including face image\ntranslation, fashion-clothes translation and painting style translation. We\nalso apply the proposed framework to domain adaptation and achieve\nstate-of-the-art performance on digit benchmark datasets. Further, thanks to\nthe explicit representation of the domain-specific decoders as well as the\nuniversal shared-latent space, it also enables us to conduct incremental\nlearning to add a new domain encoder/decoder. Linear combination of different\ndomains' representations is also obtained by fusing the corresponding decoders. \n\n"}
{"id": "1712.02896", "contents": "Title: Audio-Visual Sentiment Analysis for Learning Emotional Arcs in Movies Abstract: Stories can have tremendous power -- not only useful for entertainment, they\ncan activate our interests and mobilize our actions. The degree to which a\nstory resonates with its audience may be in part reflected in the emotional\njourney it takes the audience upon. In this paper, we use machine learning\nmethods to construct emotional arcs in movies, calculate families of arcs, and\ndemonstrate the ability for certain arcs to predict audience engagement. The\nsystem is applied to Hollywood films and high quality shorts found on the web.\nWe begin by using deep convolutional neural networks for audio and visual\nsentiment analysis. These models are trained on both new and existing\nlarge-scale datasets, after which they can be used to compute separate audio\nand visual emotional arcs. We then crowdsource annotations for 30-second video\nclips extracted from highs and lows in the arcs in order to assess the\nmicro-level precision of the system, with precision measured in terms of\nagreement in polarity between the system's predictions and annotators' ratings.\nThese annotations are also used to combine the audio and visual predictions.\nNext, we look at macro-level characterizations of movies by investigating\nwhether there exist `universal shapes' of emotional arcs. In particular, we\ndevelop a clustering approach to discover distinct classes of emotional arcs.\nFinally, we show on a sample corpus of short web videos that certain emotional\narcs are statistically significant predictors of the number of comments a video\nreceives. These results suggest that the emotional arcs learned by our approach\nsuccessfully represent macroscopic aspects of a video story that drive audience\nengagement. Such machine understanding could be used to predict audience\nreactions to video stories, ultimately improving our ability as storytellers to\ncommunicate with each other. \n\n"}
{"id": "1712.02976", "contents": "Title: Defense against Adversarial Attacks Using High-Level Representation\n  Guided Denoiser Abstract: Neural networks are vulnerable to adversarial examples, which poses a threat\nto their application in security sensitive systems. We propose high-level\nrepresentation guided denoiser (HGD) as a defense for image classification.\nStandard denoiser suffers from the error amplification effect, in which small\nresidual adversarial noise is progressively amplified and leads to wrong\nclassifications. HGD overcomes this problem by using a loss function defined as\nthe difference between the target model's outputs activated by the clean image\nand denoised image. Compared with ensemble adversarial training which is the\nstate-of-the-art defending method on large images, HGD has three advantages.\nFirst, with HGD as a defense, the target model is more robust to either\nwhite-box or black-box adversarial attacks. Second, HGD can be trained on a\nsmall subset of the images and generalizes well to other images and unseen\nclasses. Third, HGD can be transferred to defend models other than the one\nguiding it. In NIPS competition on defense against adversarial attacks, our HGD\nsolution won the first place and outperformed other models by a large margin. \n\n"}
{"id": "1712.03257", "contents": "Title: Transformational Sparse Coding Abstract: A fundamental problem faced by object recognition systems is that objects and\ntheir features can appear in different locations, scales and orientations.\nCurrent deep learning methods attempt to achieve invariance to local\ntranslations via pooling, discarding the locations of features in the process.\nOther approaches explicitly learn transformed versions of the same feature,\nleading to representations that quickly explode in size. Instead of discarding\nthe rich and useful information about feature transformations to achieve\ninvariance, we argue that models should learn object features conjointly with\ntheir transformations to achieve equivariance. We propose a new model of\nunsupervised learning based on sparse coding that can learn object features\njointly with their affine transformations directly from images. Results based\non learning from natural images indicate that our approach matches the\nreconstruction quality of traditional sparse coding but with significantly\nfewer degrees of freedom while simultaneously learning transformations from\ndata. These results open the door to scaling up unsupervised learning to allow\ndeep feature+transformation learning in a manner consistent with the\nventral+dorsal stream architecture of the primate visual cortex. \n\n"}
{"id": "1712.03534", "contents": "Title: Dynamics Transfer GAN: Generating Video by Transferring Arbitrary\n  Temporal Dynamics from a Source Video to a Single Target Image Abstract: In this paper, we propose Dynamics Transfer GAN; a new method for generating\nvideo sequences based on generative adversarial learning. The spatial\nconstructs of a generated video sequence are acquired from the target image.\nThe dynamics of the generated video sequence are imported from a source video\nsequence, with arbitrary motion, and imposed onto the target image. To preserve\nthe spatial construct of the target image, the appearance of the source video\nsequence is suppressed and only the dynamics are obtained before being imposed\nonto the target image. That is achieved using the proposed appearance\nsuppressed dynamics feature. Moreover, the spatial and temporal consistencies\nof the generated video sequence are verified via two discriminator networks.\nOne discriminator validates the fidelity of the generated frames appearance,\nwhile the other validates the dynamic consistency of the generated video\nsequence. Experiments have been conducted to verify the quality of the video\nsequences generated by the proposed method. The results verified that Dynamics\nTransfer GAN successfully transferred arbitrary dynamics of the source video\nsequence onto a target image when generating the output video sequence. The\nexperimental results also showed that Dynamics Transfer GAN maintained the\nspatial constructs (appearance) of the target image while generating spatially\nand temporally consistent video sequences. \n\n"}
{"id": "1712.03770", "contents": "Title: Clusters and higher moments of proton number fluctuations Abstract: Production of deuterons by coalescence has an important influence on the\nmoments of the observed proton number distribution. Therefore, this effect must\nbe taken into account when physics conclusions about baryon number fluctuation\nare drawn from the measurement of proton number fluctuations. We also show that\na measurement of the third and fourth moments of the deuteron number\ndistribution would allow to distinguish whether deuterons are produced\nstatistically or by coalescence. \n\n"}
{"id": "1712.04741", "contents": "Title: Mathematics of Deep Learning Abstract: Recently there has been a dramatic increase in the performance of recognition\nsystems due to the introduction of deep architectures for representation\nlearning and classification. However, the mathematical reasons for this success\nremain elusive. This tutorial will review recent work that aims to provide a\nmathematical justification for several properties of deep networks, such as\nglobal optimality, geometric stability, and invariance of the learned\nrepresentations. \n\n"}
{"id": "1712.05558", "contents": "Title: CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven\n  Communication Abstract: In this work, we propose a goal-driven collaborative task that combines\nlanguage, perception, and action. Specifically, we develop a Collaborative\nimage-Drawing game between two agents, called CoDraw. Our game is grounded in a\nvirtual world that contains movable clip art objects. The game involves two\nplayers: a Teller and a Drawer. The Teller sees an abstract scene containing\nmultiple clip art pieces in a semantically meaningful configuration, while the\nDrawer tries to reconstruct the scene on an empty canvas using available clip\nart pieces. The two players communicate with each other using natural language.\nWe collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages\nexchanged between human players. We define protocols and metrics to evaluate\nlearned agents in this testbed, highlighting the need for a novel \"crosstalk\"\nevaluation condition which pairs agents trained independently on disjoint\nsubsets of the training data. We present models for our task and benchmark them\nusing both fully automated evaluation and by having them play the game live\nwith humans. \n\n"}
{"id": "1712.05700", "contents": "Title: Simulation Study of Energy Resolution with Changing Pixel Size for Radon\n  Monitor Based on \\textit{Topmetal-${II}^-$} TPC Abstract: In this paper, we study how pixel size influences energy resolution for a\nproposed pixelated detector---a high sensitivity, low cost, and real-time radon\nmonitor based on \\textit{Topmetal-${II}^-$} time projection chamber (TPC).\nUsing \\textit{Topmetal-${II}^-$} sensors assembled by 0.35 $\\mu$m CMOS\nIntegrated Circuit process, this monitor is designed to improve the spatial\nresolution of detecting radon alpha particles. Concerning small pixel size\nmight has a side effect of worsening energy resolution due to lower signal to\nnoise ratio, a Great4-based simulation is used to figure out energy resolution\ndependence on pixel size ranging from 60 $\\mu$m to 600 $\\mu$m. A non-monotonic\ntrend in this region shows a combination effect of pixel size with threshold on\npixel, and is analyzed by introducing an empirical expression. Noise on pixel\ncontributes 50 keV Full Width at Half Maximum (FWHM) energy resolution for 400\n$\\mu$m pixel size at 1 $\\sim$ 4 $\\sigma$ threshold, which is comparable to the\nenergy resolution caused by energy fluctuation in ionization process of TPC\n($\\sim$ 20 keV). The total energy resolution after combining both factors is\nestimated to be 54 keV for 400 $\\mu$m pixel size at 1 $\\sim$ 4 $\\sigma$\nthreshold. The analysis presented in this paper is helpful to choosing suitable\npixel size for future pixelated detectors. \n\n"}
{"id": "1712.05799", "contents": "Title: Multi-Attribute Robust Component Analysis for Facial UV Maps Abstract: Recently, due to the collection of large scale 3D face models, as well as the\nadvent of deep learning, a significant progress has been made in the field of\n3D face alignment \"in-the-wild\". That is, many methods have been proposed that\nestablish sparse or dense 3D correspondences between a 2D facial image and a 3D\nface model. The utilization of 3D face alignment introduces new challenges and\nresearch directions, especially on the analysis of facial texture images. In\nparticular, texture does not suffer any more from warping effects (that\noccurred when 2D face alignment methods were used). Nevertheless, since facial\nimages are commonly captured in arbitrary recording conditions, a considerable\namount of missing information and gross outliers is observed (e.g., due to\nself-occlusion, or subjects wearing eye-glasses). Given that many annotated\ndatabases have been developed for face analysis tasks, it is evident that\ncomponent analysis techniques need to be developed in order to alleviate issues\narising from the aforementioned challenges. In this paper, we propose a novel\ncomponent analysis technique that is suitable for facial UV maps containing a\nconsiderable amount of missing information and outliers, while additionally,\nincorporates knowledge from various attributes (such as age and identity). We\nevaluate the proposed Multi-Attribute Robust Component Analysis (MA-RCA) on\nproblems such as UV completion and age progression, where the proposed method\noutperforms compared techniques. Finally, we demonstrate that MA-RCA method is\npowerful enough to provide weak annotations for training deep learning systems\nfor various applications, such as illumination transfer. \n\n"}
{"id": "1712.06145", "contents": "Title: clcNet: Improving the Efficiency of Convolutional Neural Network using\n  Channel Local Convolutions Abstract: Depthwise convolution and grouped convolution has been successfully applied\nto improve the efficiency of convolutional neural network (CNN). We suggest\nthat these models can be considered as special cases of a generalized\nconvolution operation, named channel local convolution(CLC), where an output\nchannel is computed using a subset of the input channels. This definition\nentails computation dependency relations between input and output channels,\nwhich can be represented by a channel dependency graph(CDG). By modifying the\nCDG of grouped convolution, a new CLC kernel named interlaced grouped\nconvolution (IGC) is created. Stacking IGC and GC kernels results in a\nconvolution block (named CLC Block) for approximating regular convolution. By\nresorting to the CDG as an analysis tool, we derive the rule for setting the\nmeta-parameters of IGC and GC and the framework for minimizing the\ncomputational cost. A new CNN model named clcNet is then constructed using CLC\nblocks, which shows significantly higher computational efficiency and fewer\nparameters compared to state-of-the-art networks, when being tested using the\nImageNet-1K dataset. Source code is available at\nhttps://github.com/dqzhang17/clcnet.torch . \n\n"}
{"id": "1712.06914", "contents": "Title: Bipartite Graph Matching for Keyframe Summary Evaluation Abstract: A keyframe summary, or \"static storyboard\", is a collection of frames from a\nvideo designed to summarise its semantic content. Many algorithms have been\nproposed to extract such summaries automatically. How best to evaluate these\noutputs is an important but little-discussed question. We review the current\nmethods for matching frames between two summaries in the formalism of graph\ntheory. Our analysis revealed different behaviours of these methods, which we\nillustrate with a number of case studies. Based on the results, we recommend a\ngreedy matching algorithm due to Kannappan et al. \n\n"}
{"id": "1712.07233", "contents": "Title: Hyperparameters Optimization in Deep Convolutional Neural Network /\n  Bayesian Approach with Gaussian Process Prior Abstract: Convolutional Neural Network is known as ConvNet have been extensively used\nin many complex machine learning tasks. However, hyperparameters optimization\nis one of a crucial step in developing ConvNet architectures, since the\naccuracy and performance are reliant on the hyperparameters. This multilayered\narchitecture parameterized by a set of hyperparameters such as the number of\nconvolutional layers, number of fully connected dense layers & neurons, the\nprobability of dropout implementation, learning rate. Hence the searching the\nhyperparameter over the hyperparameter space are highly difficult to build such\ncomplex hierarchical architecture. Many methods have been proposed over the\ndecade to explore the hyperparameter space and find the optimum set of\nhyperparameter values. Reportedly, Gird search and Random search are said to be\ninefficient and extremely expensive, due to a large number of hyperparameters\nof the architecture. Hence, Sequential model-based Bayesian Optimization is a\npromising alternative technique to address the extreme of the unknown cost\nfunction. The recent study on Bayesian Optimization by Snoek in nine\nconvolutional network parameters is achieved the lowerest error report in the\nCIFAR-10 benchmark. This article is intended to provide the overview of the\nmathematical concept behind the Bayesian Optimization over a Gaussian prior. \n\n"}
{"id": "1712.07384", "contents": "Title: DeepFuse: A Deep Unsupervised Approach for Exposure Fusion with Extreme\n  Exposure Image Pairs Abstract: We present a novel deep learning architecture for fusing static\nmulti-exposure images. Current multi-exposure fusion (MEF) approaches use\nhand-crafted features to fuse input sequence. However, the weak hand-crafted\nrepresentations are not robust to varying input conditions. Moreover, they\nperform poorly for extreme exposure image pairs. Thus, it is highly desirable\nto have a method that is robust to varying input conditions and capable of\nhandling extreme exposure without artifacts. Deep representations have known to\nbe robust to input conditions and have shown phenomenal performance in a\nsupervised setting. However, the stumbling block in using deep learning for MEF\nwas the lack of sufficient training data and an oracle to provide the\nground-truth for supervision. To address the above issues, we have gathered a\nlarge dataset of multi-exposure image stacks for training and to circumvent the\nneed for ground truth images, we propose an unsupervised deep learning\nframework for MEF utilizing a no-reference quality metric as loss function. The\nproposed approach uses a novel CNN architecture trained to learn the fusion\noperation without reference ground truth image. The model fuses a set of common\nlow level features extracted from each image to generate artifact-free\nperceptually pleasing results. We perform extensive quantitative and\nqualitative evaluation and show that the proposed technique outperforms\nexisting state-of-the-art approaches for a variety of natural images. \n\n"}
{"id": "1712.08107", "contents": "Title: A Deep Learning Interpretable Classifier for Diabetic Retinopathy\n  Disease Grading Abstract: Deep neural network models have been proven to be very successful in image\nclassification tasks, also for medical diagnosis, but their main concern is its\nlack of interpretability. They use to work as intuition machines with high\nstatistical confidence but unable to give interpretable explanations about the\nreported results. The vast amount of parameters of these models make difficult\nto infer a rationale interpretation from them. In this paper we present a\ndiabetic retinopathy interpretable classifier able to classify retine images\ninto the different levels of disease severity and of explaining its results by\nassigning a score for every point in the hidden and input space, evaluating its\ncontribution to the final classification in a linear way. The generated visual\nmaps can be interpreted by an expert in order to compare its own knowledge with\nthe interpretation given by the model. \n\n"}
{"id": "1712.09153", "contents": "Title: Deep Meta Learning for Real-Time Target-Aware Visual Tracking Abstract: In this paper, we propose a novel on-line visual tracking framework based on\nthe Siamese matching network and meta-learner network, which run at real-time\nspeeds. Conventional deep convolutional feature-based discriminative visual\ntracking algorithms require continuous re-training of classifiers or\ncorrelation filters, which involve solving complex optimization tasks to adapt\nto the new appearance of a target object. To alleviate this complex process,\nour proposed algorithm incorporates and utilizes a meta-learner network to\nprovide the matching network with new appearance information of the target\nobjects by adding target-aware feature space. The parameters for the\ntarget-specific feature space are provided instantly from a single forward-pass\nof the meta-learner network. By eliminating the necessity of continuously\nsolving complex optimization tasks in the course of tracking, experimental\nresults demonstrate that our algorithm performs at a real-time speed while\nmaintaining competitive performance among other state-of-the-art tracking\nalgorithms. \n\n"}
{"id": "1712.10113", "contents": "Title: Directed flow in Au+Au collisions from the RHIC Beam Energy Scan at the\n  STAR experiment Abstract: We report results of $v_1(y)$ and $dv_1/dy$ near mid-rapidity for\n$\\pi^{\\pm}$, $K^{\\pm}$, $K_s^0$, $p$, $\\overline{p}$, $\\Lambda$,\n$\\overline{\\Lambda}$ and $\\phi$ from Beam Energy Scan Au+Au collisions at\n$\\sqrt{s_{NN}} = $ 7.7 - 200 GeV using the STAR detector at RHIC. The\n$dv_{1}/dy$ of $\\pi^{\\pm}$, $K^{\\pm}$ and $K_s^0$ mesons remains negative over\nall beam energies. The $dv_1/dy$ of $p$ and $\\Lambda$ baryons shows a sign\nchange around 10 - 15 GeV, while net baryons (net p and net $\\Lambda$) indicate\na double sign change. The $dv_1/dy$ of $\\overline{p}$, $\\overline{\\Lambda}$ and\n$\\phi$ show a similar trend for $\\sqrt{s_{NN}}>$ 14.5 GeV. For the first time,\n$v_{1}$ measurements are used to test a quark coalescence hypothesis. Many\nmeasurements are found to be consistent with the particles being formed via\ncoalescence of constituent quarks. The observed deviations from that\nconsistency offer a new approach for probing the collision process at the quark\nlevel. \n\n"}
{"id": "1801.00289", "contents": "Title: Deep Stacked Networks with Residual Polishing for Image Inpainting Abstract: Deep neural networks have shown promising results in image inpainting even if\nthe missing area is relatively large. However, most of the existing inpainting\nnetworks introduce undesired artifacts and noise to the repaired regions. To\nsolve this problem, we present a novel framework which consists of two stacked\nconvolutional neural networks that inpaint the image and remove the artifacts,\nrespectively. The first network considers the global structure of the damaged\nimage and coarsely fills the blank area. Then the second network modifies the\nrepaired image to cancel the noise introduced by the first network. The\nproposed framework splits the problem into two distinct partitions that can be\noptimized separately, therefore it can be applied to any inpainting algorithm\nby changing the first network. Second stage in our framework which aims at\npolishing the inpainted images can be treated as a denoising problem where a\nwide range of algorithms can be employed. Our results demonstrate that the\nproposed framework achieves significant improvement on both visual and\nquantitative evaluations. \n\n"}
{"id": "1801.02031", "contents": "Title: ReMotENet: Efficient Relevant Motion Event Detection for Large-scale\n  Home Surveillance Videos Abstract: This paper addresses the problem of detecting relevant motion caused by\nobjects of interest (e.g., person and vehicles) in large scale home\nsurveillance videos. The traditional method usually consists of two separate\nsteps, i.e., detecting moving objects with background subtraction running on\nthe camera, and filtering out nuisance motion events (e.g., trees, cloud,\nshadow, rain/snow, flag) with deep learning based object detection and tracking\nrunning on cloud. The method is extremely slow and therefore not cost\neffective, and does not fully leverage the spatial-temporal redundancies with a\npre-trained off-the-shelf object detector. To dramatically speedup relevant\nmotion event detection and improve its performance, we propose a novel network\nfor relevant motion event detection, ReMotENet, which is a unified, end-to-end\ndata-driven method using spatial-temporal attention-based 3D ConvNets to\njointly model the appearance and motion of objects-of-interest in a video.\nReMotENet parses an entire video clip in one forward pass of a neural network\nto achieve significant speedup. Meanwhile, it exploits the properties of home\nsurveillance videos, e.g., relevant motion is sparse both spatially and\ntemporally, and enhances 3D ConvNets with a spatial-temporal attention model\nand reference-frame subtraction to encourage the network to focus on the\nrelevant moving objects. Experiments demonstrate that our method can achieve\ncomparable or event better performance than the object detection based method\nbut with three to four orders of magnitude speedup (up to 20k times) on GPU\ndevices. Our network is efficient, compact and light-weight. It can detect\nrelevant motion on a 15s surveillance video clip within 4-8 milliseconds on a\nGPU and a fraction of second (0.17-0.39) on a CPU with a model size of less\nthan 1MB. \n\n"}
{"id": "1801.03977", "contents": "Title: The Gamow-state description of the decay energy spectrum of\n  neutron-unbound $^{25}$O Abstract: We show the feasibility of calculating the decay energy spectrum of neutron\nemitting nuclei within the Gamow-state description of resonances by obtaining\nthe decay energy spectrum of $^{25}$O. We model this nucleus as a valence\nneutron interacting with an $^{24}$O inert core, and we obtain the resulting\nresonant energies, widths and decay energy spectra for the ground and first\nexcited states. We also discuss the similarities and differences between the\ndecay energy spectrum of a Gamow state and the Breit-Wigner distribution with\nenergy-dependent width. \n\n"}
{"id": "1801.05387", "contents": "Title: StressedNets: Efficient Feature Representations via Stress-induced\n  Evolutionary Synthesis of Deep Neural Networks Abstract: The computational complexity of leveraging deep neural networks for\nextracting deep feature representations is a significant barrier to its\nwidespread adoption, particularly for use in embedded devices. One particularly\npromising strategy to addressing the complexity issue is the notion of\nevolutionary synthesis of deep neural networks, which was demonstrated to\nsuccessfully produce highly efficient deep neural networks while retaining\nmodeling performance. Here, we further extend upon the evolutionary synthesis\nstrategy for achieving efficient feature extraction via the introduction of a\nstress-induced evolutionary synthesis framework, where stress signals are\nimposed upon the synapses of a deep neural network during training to induce\nstress and steer the synthesis process towards the production of more efficient\ndeep neural networks over successive generations and improved model fidelity at\na greater efficiency. The proposed stress-induced evolutionary synthesis\napproach is evaluated on a variety of different deep neural network\narchitectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object\nclassification and object detection) to synthesize efficient StressedNets over\nmultiple generations. Experimental results demonstrate the efficacy of the\nproposed framework to synthesize StressedNets with significant improvement in\nnetwork architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and\nspeed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra\nX1 mobile processor). \n\n"}
{"id": "1801.05558", "contents": "Title: Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace Abstract: Gradient-based meta-learning methods leverage gradient descent to learn the\ncommonalities among various tasks. While previous such methods have been\nsuccessful in meta-learning tasks, they resort to simple gradient descent\nduring meta-testing. Our primary contribution is the {\\em MT-net}, which\nenables the meta-learner to learn on each layer's activation space a subspace\nthat the task-specific learner performs gradient descent on. Additionally, a\ntask-specific learner of an {\\em MT-net} performs gradient descent with respect\nto a meta-learned distance metric, which warps the activation space to be more\nsensitive to task identity. We demonstrate that the dimension of this learned\nsubspace reflects the complexity of the task-specific learner's adaptation\ntask, and also that our model is less sensitive to the choice of initial\nlearning rates than previous gradient-based meta-learning methods. Our method\nachieves state-of-the-art or comparable performance on few-shot classification\nand regression tasks. \n\n"}
{"id": "1801.05568", "contents": "Title: Image Captioning using Deep Neural Architectures Abstract: Automatically creating the description of an image using any natural\nlanguages sentence like English is a very challenging task. It requires\nexpertise of both image processing as well as natural language processing. This\npaper discuss about different available models for image captioning task. We\nhave also discussed about how the advancement in the task of object recognition\nand machine translation has greatly improved the performance of image\ncaptioning model in recent years. In addition to that we have discussed how\nthis model can be implemented. In the end, we have also evaluated the\nperformance of model using standard evaluation matrices. \n\n"}
{"id": "1801.05895", "contents": "Title: Sparsely Aggregated Convolutional Networks Abstract: We explore a key architectural aspect of deep convolutional neural networks:\nthe pattern of internal skip connections used to aggregate outputs of earlier\nlayers for consumption by deeper layers. Such aggregation is critical to\nfacilitate training of very deep networks in an end-to-end manner. This is a\nprimary reason for the widespread adoption of residual networks, which\naggregate outputs via cumulative summation. While subsequent works investigate\nalternative aggregation operations (e.g. concatenation), we focus on an\northogonal question: which outputs to aggregate at a particular point in the\nnetwork. We propose a new internal connection structure which aggregates only a\nsparse set of previous outputs at any given depth. Our experiments demonstrate\nthis simple design change offers superior performance with fewer parameters and\nlower computational requirements. Moreover, we show that sparse aggregation\nallows networks to scale more robustly to 1000+ layers, thereby opening future\navenues for training long-running visual processes. \n\n"}
{"id": "1801.06277", "contents": "Title: Deep Chain HDRI: Reconstructing a High Dynamic Range Image from a Single\n  Low Dynamic Range Image Abstract: In this paper, we propose a novel deep neural network model that reconstructs\na high dynamic range (HDR) image from a single low dynamic range (LDR) image.\nThe proposed model is based on a convolutional neural network composed of\ndilated convolutional layers, and infers LDR images with various exposures and\nillumination from a single LDR image of the same scene. Then, the final HDR\nimage can be formed by merging these inference results. It is relatively easy\nfor the proposed method to find the mapping between the LDR and an HDR with a\ndifferent bit depth because of the chaining structure inferring the\nrelationship between the LDR images with brighter (or darker) exposures from a\ngiven LDR image. The method not only extends the range, but also has the\nadvantage of restoring the light information of the actual physical world. For\nthe HDR images obtained by the proposed method, the HDR-VDP2 Q score, which is\nthe most popular evaluation metric for HDR images, was 56.36 for a display with\na 1920$\\times$1200 resolution, which is an improvement of 6 compared with the\nscores of conventional algorithms. In addition, when comparing the peak\nsignal-to-noise ratio values for tone mapped HDR images generated by the\nproposed and conventional algorithms, the average value obtained by the\nproposed algorithm is 30.86 dB, which is 10 dB higher than those obtained by\nthe conventional algorithms. \n\n"}
{"id": "1801.07481", "contents": "Title: Survey on Emotional Body Gesture Recognition Abstract: Automatic emotion recognition has become a trending research topic in the\npast decade. While works based on facial expressions or speech abound,\nrecognizing affect from body gestures remains a less explored topic. We present\na new comprehensive survey hoping to boost research in the field. We first\nintroduce emotional body gestures as a component of what is commonly known as\n\"body language\" and comment general aspects as gender differences and culture\ndependence. We then define a complete framework for automatic emotional body\ngesture recognition. We introduce person detection and comment static and\ndynamic body pose estimation methods both in RGB and 3D. We then comment the\nrecent literature related to representation learning and emotion recognition\nfrom images of emotionally expressive gestures. We also discuss multi-modal\napproaches that combine speech or face with body gestures for improved emotion\nrecognition. While pre-processing methodologies (e.g. human detection and pose\nestimation) are nowadays mature technologies fully developed for robust large\nscale analysis, we show that for emotion recognition the quantity of labelled\ndata is scarce, there is no agreement on clearly defined output spaces and the\nrepresentations are shallow and largely based on naive geometrical\nrepresentations. \n\n"}
{"id": "1801.07729", "contents": "Title: The Shape of Art History in the Eyes of the Machine Abstract: How does the machine classify styles in art? And how does it relate to art\nhistorians' methods for analyzing style? Several studies have shown the ability\nof the machine to learn and predict style categories, such as Renaissance,\nBaroque, Impressionism, etc., from images of paintings. This implies that the\nmachine can learn an internal representation encoding discriminative features\nthrough its visual analysis. However, such a representation is not necessarily\ninterpretable. We conducted a comprehensive study of several of the\nstate-of-the-art convolutional neural networks applied to the task of style\nclassification on 77K images of paintings, and analyzed the learned\nrepresentation through correlation analysis with concepts derived from art\nhistory. Surprisingly, the networks could place the works of art in a smooth\ntemporal arrangement mainly based on learning style labels, without any a\npriori knowledge of time of creation, the historical time and context of\nstyles, or relations between styles. The learned representations showed that\nthere are few underlying factors that explain the visual variations of style in\nart. Some of these factors were found to correlate with style patterns\nsuggested by Heinrich W\\\"olfflin (1846-1945). The learned representations also\nconsistently highlighted certain artists as the extreme distinctive\nrepresentative of their styles, which quantitatively confirms art historian\nobservations. \n\n"}
{"id": "1801.08486", "contents": "Title: Self-Learning to Detect and Segment Cysts in Lung CT Images without\n  Manual Annotation Abstract: Image segmentation is a fundamental problem in medical image analysis. In\nrecent years, deep neural networks achieve impressive performances on many\nmedical image segmentation tasks by supervised learning on large manually\nannotated data. However, expert annotations on big medical datasets are\ntedious, expensive or sometimes unavailable. Weakly supervised learning could\nreduce the effort for annotation but still required certain amounts of\nexpertise. Recently, deep learning shows a potential to produce more accurate\npredictions than the original erroneous labels. Inspired by this, we introduce\na very weakly supervised learning method, for cystic lesion detection and\nsegmentation in lung CT images, without any manual annotation. Our method works\nin a self-learning manner, where segmentation generated in previous steps\n(first by unsupervised segmentation then by neural networks) is used as ground\ntruth for the next level of network learning. Experiments on a cystic lung\nlesion dataset show that the deep learning could perform better than the\ninitial unsupervised annotation, and progressively improve itself after\nself-learning. \n\n"}
{"id": "1801.09086", "contents": "Title: A Generative Approach to Zero-Shot and Few-Shot Action Recognition Abstract: We present a generative framework for zero-shot action recognition where some\nof the possible action classes do not occur in the training data. Our approach\nis based on modeling each action class using a probability distribution whose\nparameters are functions of the attribute vector representing that action\nclass. In particular, we assume that the distribution parameters for any action\nclass in the visual space can be expressed as a linear combination of a set of\nbasis vectors where the combination weights are given by the attributes of the\naction class. These basis vectors can be learned solely using labeled data from\nthe known (i.e., previously seen) action classes, and can then be used to\npredict the parameters of the probability distributions of unseen action\nclasses. We consider two settings: (1) Inductive setting, where we use only the\nlabeled examples of the seen action classes to predict the unseen action class\nparameters; and (2) Transductive setting which further leverages unlabeled data\nfrom the unseen action classes. Our framework also naturally extends to\nfew-shot action recognition where a few labeled examples from unseen classes\nare available. Our experiments on benchmark datasets (UCF101, HMDB51 and\nOlympic) show significant performance improvements as compared to various\nbaselines, in both standard zero-shot (disjoint seen and unseen classes) and\ngeneralized zero-shot learning settings. \n\n"}
{"id": "1801.09184", "contents": "Title: Contextual Multi-Scale Region Convolutional 3D Network for Activity\n  Detection Abstract: Activity detection is a fundamental problem in computer vision. Detecting\nactivities of different temporal scales is particularly challenging. In this\npaper, we propose the contextual multi-scale region convolutional 3D network\n(CMS-RC3D) for activity detection. To deal with the inherent temporal scale\nvariability of activity instances, the temporal feature pyramid is used to\nrepresent activities of different temporal scales. On each level of the\ntemporal feature pyramid, an activity proposal detector and an activity\nclassifier are learned to detect activities of specific temporal scales.\nTemporal contextual information is fused into activity classifiers for better\nrecognition. More importantly, the entire model at all levels can be trained\nend-to-end. Our CMS-RC3D detector can deal with activities at all temporal\nscale ranges with only a single pass through the backbone network. We test our\ndetector on two public activity detection benchmarks, THUMOS14 and ActivityNet.\nExtensive experiments show that the proposed CMS-RC3D detector outperforms\nstate-of-the-art methods on THUMOS14 by a substantial margin and achieves\ncomparable results on ActivityNet despite using a shallow feature extractor. \n\n"}
{"id": "1801.10458", "contents": "Title: The impact of $\\mathbf{K^+\\Lambda}$ photoproduction on the resonance\n  spectrum Abstract: The J\\\"ulich-Bonn coupled-channel framework is extended to $K^+\\Lambda$\nphotoproduction. The spectrum of nucleon and $\\Delta$ resonances is extracted\nfrom simultaneous fits to several pion-induced reactions in addition to pion,\neta and $K^+\\Lambda$ photoproduction off the proton. More than 40,000 data\npoints up to a center-of-mass energy of E$\\sim$2.3 GeV including recently\nmeasured double-polarization observables are analyzed. The influence of the\n$\\gamma p\\to K^+\\Lambda$ channel on the extracted resonance parameters and the\nappearance of states not seen in other channels is investigated. The\nJ\\\"ulich-Bonn model includes effective three-body channels and guarantees\nunitarity and analyticity, which is a prerequisite for a reliable determination\nof the resonance spectrum in terms of poles and residues. \n\n"}
{"id": "1801.10562", "contents": "Title: Feature Decomposition Based Saliency Detection in Electron\n  Cryo-Tomograms Abstract: Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular\nstructures at the submolecular resolution in close to the native state.\nHowever, due to the high degree of structural complexity and imaging limits,\nthe automatic segmentation of cellular components from ECT images is very\ndifficult. To complement and speed up existing segmentation methods, it is\ndesirable to develop a generic cell component segmentation method that is 1)\nnot specific to particular types of cellular components, 2) able to segment\nunknown cellular components, 3) fully unsupervised and does not rely on the\navailability of training data. As an important step towards this goal, in this\npaper, we propose a saliency detection method that computes the likelihood that\na subregion in a tomogram stands out from the background. Our method consists\nof four steps: supervoxel over-segmentation, feature extraction, feature matrix\ndecomposition, and computation of saliency. The method produces a distribution\nmap that represents the regions' saliency in tomograms. Our experiments show\nthat our method can successfully label most salient regions detected by a human\nobserver, and able to filter out regions not containing cellular components.\nTherefore, our method can remove the majority of the background region, and\nsignificantly speed up the subsequent processing of segmentation and\nrecognition of cellular components captured by ECT. \n\n"}
{"id": "1802.00121", "contents": "Title: Interpreting CNNs via Decision Trees Abstract: This paper aims to quantitatively explain rationales of each prediction that\nis made by a pre-trained convolutional neural network (CNN). We propose to\nlearn a decision tree, which clarifies the specific reason for each prediction\nmade by the CNN at the semantic level. I.e., the decision tree decomposes\nfeature representations in high conv-layers of the CNN into elementary concepts\nof object parts. In this way, the decision tree tells people which object parts\nactivate which filters for the prediction and how much they contribute to the\nprediction score. Such semantic and quantitative explanations for CNN\npredictions have specific values beyond the traditional pixel-level analysis of\nCNNs. More specifically, our method mines all potential decision modes of the\nCNN, where each mode represents a common case of how the CNN uses object parts\nfor prediction. The decision tree organizes all potential decision modes in a\ncoarse-to-fine manner to explain CNN predictions at different fine-grained\nlevels. Experiments have demonstrated the effectiveness of the proposed method. \n\n"}
{"id": "1802.00491", "contents": "Title: A New Registration Approach for Dynamic Analysis of Calcium Signals in\n  Organs Abstract: Wing disc pouches of fruit flies are a powerful genetic model for studying\nphysiological intercellular calcium ($Ca^{2+}$) signals for dynamic analysis of\ncell signaling in organ development and disease studies. A key to analyzing\nspatial-temporal patterns of $Ca^{2+}$ signal waves is to accurately align the\npouches across image sequences. However, pouches in different image frames may\nexhibit extensive intensity oscillations due to $Ca^{2+}$ signaling dynamics,\nand commonly used multimodal non-rigid registration methods may fail to achieve\nsatisfactory results. In this paper, we develop a new two-phase non-rigid\nregistration approach to register pouches in image sequences. First, we conduct\nsegmentation of the region of interest. (i.e., pouches) using a deep neural\nnetwork model. Second, we obtain an optimal transformation and align pouches\nacross the image sequences. Evaluated using both synthetic data and real pouch\ndata, our method considerably outperforms the state-of-the-art non-rigid\nregistration methods. \n\n"}
{"id": "1802.01873", "contents": "Title: Every Smile is Unique: Landmark-Guided Diverse Smile Generation Abstract: Each smile is unique: one person surely smiles in different ways (e.g.,\nclosing/opening the eyes or mouth). Given one input image of a neutral face,\ncan we generate multiple smile videos with distinctive characteristics? To\ntackle this one-to-many video generation problem, we propose a novel deep\nlearning architecture named Conditional Multi-Mode Network (CMM-Net). To better\nencode the dynamics of facial expressions, CMM-Net explicitly exploits facial\nlandmarks for generating smile sequences. Specifically, a variational\nauto-encoder is used to learn a facial landmark embedding. This single\nembedding is then exploited by a conditional recurrent network which generates\na landmark embedding sequence conditioned on a specific expression (e.g.,\nspontaneous smile). Next, the generated landmark embeddings are fed into a\nmulti-mode recurrent landmark generator, producing a set of landmark sequences\nstill associated to the given smile class but clearly distinct from each other.\nFinally, these landmark sequences are translated into face videos. Our\nexperimental results demonstrate the effectiveness of our CMM-Net in generating\nrealistic videos of multiple smile expressions. \n\n"}
{"id": "1802.01888", "contents": "Title: Growth and characterization of a Li2Mg2(MoO4)3 scintillating bolometer Abstract: Lithium magnesium molybdate (Li$_2$Mg$_2$(MoO$_4$)$_3$) crystals were grown\nby the low-thermal-gradient Czochralski method. Luminescence properties of the\nmaterial (emission spectra, thermally stimulated luminescence, dependence of\nintensity on temperature, phosphorescence) have been studied under X-Ray\nexcitation in the temperature interval from 8 K to 400 K, while at the same\nbeing operated as a scintillating bolometer at 20 mK for the first time. We\ndemonstrated that Li$_2$Mg$_2$(MoO$_4)_3$ crystals are a potentially promising\ndetector material to search for neutrinoless double beta decay of $^{100}$Mo. \n\n"}
{"id": "1802.02488", "contents": "Title: SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial\n  Network Abstract: Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Supervised cross-modal hashing methods have achieved considerable\nprogress by incorporating semantic side information. However, they mainly have\ntwo limitations: (1) Heavily rely on large-scale labeled cross-modal training\ndata which are labor intensive and hard to obtain. (2) Ignore the rich\ninformation contained in the large amount of unlabeled data across different\nmodalities, especially the margin examples that are easily to be incorrectly\nretrieved, which can help to model the correlations. To address these problems,\nin this paper we propose a novel Semi-supervised Cross-Modal Hashing approach\nby Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's\nability for modeling data distributions to promote cross-modal hashing learning\nin an adversarial way. The main contributions can be summarized as follows: (1)\nWe propose a novel generative adversarial network for cross-modal hashing. In\nour proposed SCH-GAN, the generative model tries to select margin examples of\none modality from unlabeled data when giving a query of another modality. While\nthe discriminative model tries to distinguish the selected examples and true\npositive examples of the query. These two models play a minimax game so that\nthe generative model can promote the hashing performance of discriminative\nmodel. (2) We propose a reinforcement learning based algorithm to drive the\ntraining of proposed SCH-GAN. The generative model takes the correlation score\npredicted by discriminative model as a reward, and tries to select the examples\nclose to the margin to promote discriminative model by maximizing the margin\nbetween positive and negative data. Experiments on 3 widely-used datasets\nverify the effectiveness of our proposed approach. \n\n"}
{"id": "1802.04167", "contents": "Title: First transmission of electrons and ions through the KATRIN beamline Abstract: The Karlsruhe Tritium Neutrino (KATRIN) experiment is a large-scale effort to\nprobe the absolute neutrino mass scale with a sensitivity of 0.2 eV (90%\nconfidence level), via a precise measurement of the endpoint spectrum of\ntritium beta decay. This work documents several KATRIN commissioning\nmilestones: the complete assembly of the experimental beamline, the successful\ntransmission of electrons from three sources through the beamline to the\nprimary detector, and tests of ion transport and retention. In the First Light\ncommissioning campaign of Autumn 2016, photoelectrons were generated at the\nrear wall and ions were created by a dedicated ion source attached to the rear\nsection; in July 2017, gaseous Kr-83m was injected into the KATRIN source\nsection, and a condensed Kr-83m source was deployed in the transport section.\nIn this paper we describe the technical details of the apparatus and the\nconfiguration for each measurement, and give first results on source and system\nperformance. We have successfully achieved transmission from all four sources,\nestablished system stability, and characterized many aspects of the apparatus. \n\n"}
{"id": "1802.04557", "contents": "Title: Automatic localization and decoding of honeybee markers using deep\n  convolutional neural networks Abstract: The honeybee is a fascinating model animal to investigate how collective\nbehavior emerges from (inter-)actions of thousands of individuals. Bees may\nacquire unique memories throughout their lives. These experiences affect social\ninteractions even over large time frames. Tracking and identifying all bees in\nthe colony over their lifetimes therefore may likely shed light on the\ninterplay of individual differences and colony behavior. This paper proposes a\nsoftware pipeline based on two deep convolutional neural networks for the\nlocalization and decoding of custom binary markers that honeybees carry from\ntheir first to the last day in their life. We show that this approach\noutperforms similar systems proposed in recent literature. By opening this\nsoftware for the public, we hope that the resulting datasets will help\nadvancing the understanding of honeybee collective intelligence. \n\n"}
{"id": "1802.04713", "contents": "Title: Exclusive vector meson photoproduction in fixed - target collisions at\n  the LHC Abstract: The exclusive $\\rho$, $\\omega$ and $J/\\Psi$ photoproduction in fixed - target\ncollisions at the LHC is investigated. We estimate, for the first time, the\nrapidity and transverse momentum distributions of the vector meson\nphotoproduction in $p He$, $p Ar$, $Pb He$ and $Pb Ar$ fixed - target\ncollisions at the LHC using the STARlight Monte Carlo and present our results\nfor the total cross sections. Predictions for the kinematical range probed by\nthe LHCb detector are also presented. Our results indicate that the\nexperimental analysis of this process in fixed - target collisions at the LHC\nis feasible. Such future analysis will probe the QCD dynamics in a kinematical\nrange complementary to that studied in the collider mode. \n\n"}
{"id": "1802.05701", "contents": "Title: Inverting The Generator Of A Generative Adversarial Network (II) Abstract: Generative adversarial networks (GANs) learn a deep generative model that is\nable to synthesise novel, high-dimensional data samples. New data samples are\nsynthesised by passing latent samples, drawn from a chosen prior distribution,\nthrough the generative model. Once trained, the latent space exhibits\ninteresting properties, that may be useful for down stream tasks such as\nclassification or retrieval. Unfortunately, GANs do not offer an \"inverse\nmodel\", a mapping from data space back to latent space, making it difficult to\ninfer a latent representation for a given data sample. In this paper, we\nintroduce a technique, inversion, to project data samples, specifically images,\nto the latent space using a pre-trained GAN. Using our proposed inversion\ntechnique, we are able to identify which attributes of a dataset a trained GAN\nis able to model and quantify GAN performance, based on a reconstruction loss.\nWe demonstrate how our proposed inversion technique may be used to\nquantitatively compare performance of various GAN models trained on three image\ndatasets. We provide code for all of our experiments,\nhttps://github.com/ToniCreswell/InvertingGAN. \n\n"}
{"id": "1802.07687", "contents": "Title: Stochastic Video Generation with a Learned Prior Abstract: Generating video frames that accurately predict future world states is\nchallenging. Existing approaches either fail to capture the full distribution\nof outcomes, or yield blurry generations, or both. In this paper we introduce\nan unsupervised video generation model that learns a prior model of uncertainty\nin a given environment. Video frames are generated by drawing samples from this\nprior and combining them with a deterministic estimate of the future frame. The\napproach is simple and easily trained end-to-end on a variety of datasets.\nSample generations are both varied and sharp, even many frames into the future,\nand compare favorably to those from existing approaches. \n\n"}
{"id": "1802.07879", "contents": "Title: Work of a multi-cathode counter in a single electron counting mode Abstract: We describe a work of a multi-cathode counter of the developed design in a\nsingle electron counting mode with a cathode made of aluminum alloy. The\nresults of the calibration of the counter are presented. The coefficient of gas\namplification was found from the calibration spectra. The electric fields and\noperation of this detector in two configurations are described and the original\nidea to find the effect from electrons emitted from the surface of a cathode by\ndifference of the rates measured in two volume configurations is expounded.\nFurthermore, the advantage of using a multi-cathode counter for measurement of\nthe intensity of single electron emission from a metal is explained. \n\n"}
{"id": "1802.07971", "contents": "Title: Robustness of classifiers to uniform $\\ell\\_p$ and Gaussian noise Abstract: We study the robustness of classifiers to various kinds of random noise\nmodels. In particular, we consider noise drawn uniformly from the $\\ell\\_p$\nball for $p \\in [1, \\infty]$ and Gaussian noise with an arbitrary covariance\nmatrix. We characterize this robustness to random noise in terms of the\ndistance to the decision boundary of the classifier. This analysis applies to\nlinear classifiers as well as classifiers with locally approximately flat\ndecision boundaries, a condition which is satisfied by state-of-the-art deep\nneural networks. The predicted robustness is verified experimentally. \n\n"}
{"id": "1802.08797", "contents": "Title: Residual Dense Network for Image Super-Resolution Abstract: A very deep convolutional neural network (CNN) has recently achieved great\nsuccess for image super-resolution (SR) and offered hierarchical features as\nwell. However, most deep CNN based SR models do not make full use of the\nhierarchical features from the original low-resolution (LR) images, thereby\nachieving relatively-low performance. In this paper, we propose a novel\nresidual dense network (RDN) to address this problem in image SR. We fully\nexploit the hierarchical features from all the convolutional layers.\nSpecifically, we propose residual dense block (RDB) to extract abundant local\nfeatures via dense connected convolutional layers. RDB further allows direct\nconnections from the state of preceding RDB to all the layers of current RDB,\nleading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is\nthen used to adaptively learn more effective features from preceding and\ncurrent local features and stabilizes the training of wider network. After\nfully obtaining dense local features, we use global feature fusion to jointly\nand adaptively learn global hierarchical features in a holistic way. Extensive\nexperiments on benchmark datasets with different degradation models show that\nour RDN achieves favorable performance against state-of-the-art methods. \n\n"}
{"id": "1802.09058", "contents": "Title: Seeing Small Faces from Robust Anchor's Perspective Abstract: This paper introduces a novel anchor design to support anchor-based face\ndetection for superior scale-invariant performance, especially on tiny faces.\nTo achieve this, we explicitly address the problem that anchor-based detectors\ndrop performance drastically on faces with tiny sizes, e.g. less than 16x16\npixels. In this paper, we investigate why this is the case. We discover that\ncurrent anchor design cannot guarantee high overlaps between tiny faces and\nanchor boxes, which increases the difficulty of training. The new Expected Max\nOverlapping (EMO) score is proposed which can theoretically explain the low\noverlapping issue and inspire several effective strategies of new anchor design\nleading to higher face overlaps, including anchor stride reduction with new\nnetwork architectures, extra shifted anchors, and stochastic face shifting.\nComprehensive experiments show that our proposed method significantly\noutperforms the baseline anchor-based detector, while consistently achieving\nstate-of-the-art results on challenging face detection datasets with\ncompetitive runtime speed. \n\n"}
{"id": "1802.10171", "contents": "Title: Tell Me Where to Look: Guided Attention Inference Network Abstract: Weakly supervised learning with only coarse labels can obtain visual\nexplanations of deep neural network such as attention maps by back-propagating\ngradients. These attention maps are then available as priors for tasks such as\nobject localization and semantic segmentation. In one common framework we\naddress three shortcomings of previous approaches in modeling such attention\nmaps: We (1) first time make attention maps an explicit and natural component\nof the end-to-end training, (2) provide self-guidance directly on these maps by\nexploring supervision form the network itself to improve them, and (3)\nseamlessly bridge the gap between using weak and extra supervision if\navailable. Despite its simplicity, experiments on the semantic segmentation\ntask demonstrate the effectiveness of our methods. We clearly surpass the\nstate-of-the-art on Pascal VOC 2012 val. and test set. Besides, the proposed\nframework provides a way not only explaining the focus of the learner but also\nfeeding back with direct guidance towards specific tasks. Under mild\nassumptions our method can also be understood as a plug-in to existing weakly\nsupervised learners to improve their generalization performance. \n\n"}
{"id": "1803.00132", "contents": "Title: Development of GEM Detectors at Hampton University Abstract: Two GEM telescopes, each consisting of three 10x10 cm$^2$ triple-GEM chambers\nwere built, tested and operated by the Hampton University group. The GEMs are\nread out with APV25 frontend chips and FPGA based digitizing electronics\ndeveloped by INFN Rome.\n  The telescopes were used for the luminosity monitoring system at the OLYMPUS\nexperiment at DESY in Germany, with positron and electron beams at 2 GeV. The\nGEM elements have been recycled to serve in another two applications: Three GEM\nelements are used to track beam particles in the MUSE experiment at PSI in\nSwitzerland. A set of four elements has been configured as a prototype tracker\nfor phase 1a of the DarkLight experiment at the Low-Energy Recirculator\nFacility (LERF) at Jefferson Lab in Newport News, USA, in a first test run in\nsummer 2016.\n  The Hampton group is responsible for the DarkLight phase-I lepton tracker in\npreparation. Further efforts are ongoing to optimize the data acquisition speed\nfor GEM operations in MUSE and DarkLight. An overview of the group's GEM\ndetector related activities will be given. \n\n"}
{"id": "1803.00197", "contents": "Title: Temporally Identity-Aware SSD with Attentional LSTM Abstract: Temporal object detection has attracted significant attention, but most\npopular detection methods cannot leverage rich temporal information in videos.\nVery recently, many algorithms have been developed for video detection task,\nyet very few approaches can achieve \\emph{real-time online} object detection in\nvideos. In this paper, based on attention mechanism and convolutional long\nshort-term memory (ConvLSTM), we propose a temporal single-shot detector (TSSD)\nfor real-world detection. Distinct from previous methods, we take aim at\ntemporally integrating pyramidal feature hierarchy using ConvLSTM, and design a\nnovel structure including a low-level temporal unit as well as a high-level one\n(LH-TU) for multi-scale feature maps. Moreover, we develop a creative temporal\nanalysis unit, namely, attentional ConvLSTM (AC-LSTM), in which a temporal\nattention mechanism is specially tailored for background suppression and scale\nsuppression while a ConvLSTM integrates attention-aware features across time.\nAn association loss and a multi-step training are designed for temporal\ncoherence. Besides, an online tubelet analysis (OTA) is exploited for\nidentification. Our framework is evaluated on ImageNet VID dataset and 2DMOT15\ndataset. Extensive comparisons on the detection and tracking capability\nvalidate the superiority of the proposed approach. Consequently, the developed\nTSSD-OTA achieves a fast speed and an overall competitive performance in terms\nof detection and tracking. Finally, a real-world maneuver is conducted for\nunderwater object grasping. The source code is publicly available at\nhttps://github.com/SeanChenxy/TSSD-OTA. \n\n"}
{"id": "1803.00401", "contents": "Title: Unravelling Robustness of Deep Learning based Face Recognition Against\n  Adversarial Attacks Abstract: Deep neural network (DNN) architecture based models have high expressive\npower and learning capacity. However, they are essentially a black box method\nsince it is not easy to mathematically formulate the functions that are learned\nwithin its many layers of representation. Realizing this, many researchers have\nstarted to design methods to exploit the drawbacks of deep learning based\nalgorithms questioning their robustness and exposing their singularities. In\nthis paper, we attempt to unravel three aspects related to the robustness of\nDNNs for face recognition: (i) assessing the impact of deep architectures for\nface recognition in terms of vulnerabilities to attacks inspired by commonly\nobserved distortions in the real world that are well handled by shallow\nlearning methods along with learning based adversaries; (ii) detecting the\nsingularities by characterizing abnormal filter response behavior in the hidden\nlayers of deep networks; and (iii) making corrections to the processing\npipeline to alleviate the problem. Our experimental evaluation using multiple\nopen-source DNN-based face recognition networks, including OpenFace and\nVGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates\nthat the performance of deep learning based face recognition algorithms can\nsuffer greatly in the presence of such distortions. The proposed method is also\ncompared with existing detection algorithms and the results show that it is\nable to detect the attacks with very high accuracy by suitably designing a\nclassifier using the response of the hidden layers in the network. Finally, we\npresent several effective countermeasures to mitigate the impact of adversarial\nattacks and improve the overall robustness of DNN-based face recognition. \n\n"}
{"id": "1803.00853", "contents": "Title: Quantum distance-based classifier with constant size memory, distributed\n  knowledge and state recycling Abstract: In this work we examine recently proposed distance-based classification\nmethod designed for near-term quantum processing units with limited resources.\nWe further study possibilities to reduce the quantum resources without any\nefficiency decrease. We show that only a part of the information undergoes\ncoherent evolution and this fact allows us to introduce an algorithm with\nsignificantly reduced quantum memory size. Additionally, considering only\npartial information at a time, we propose a classification protocol with\ninformation distributed among a number of agents. Finally, we show that the\ninformation evolution during a measurement can lead to a better solution and\nthat accuracy of the algorithm can be improved by harnessing the state after\nthe final measurement. \n\n"}
{"id": "1803.01159", "contents": "Title: Enhancement of land-use change modeling using convolutional neural\n  networks and convolutional denoising autoencoders Abstract: The neighborhood effect is a key driving factor for the land-use change (LUC)\nprocess. This study applies convolutional neural networks (CNN) to capture\nneighborhood characteristics from satellite images and to enhance the\nperformance of LUC modeling. We develop a hybrid CNN model (conv-net) to\npredict the LU transition probability by combining satellite images and\ngeographical features. A spatial weight layer is designed to incorporate the\ndistance-decay characteristics of neighborhood effect into conv-net. As an\nalternative model, we also develop a hybrid convolutional denoising autoencoder\nand multi-layer perceptron model (CDAE-net), which specifically learns latent\nrepresentations from satellite images and denoises the image data. Finally, a\nDINAMICA-based cellular automata (CA) model simulates the LU pattern. The\nresults show that the convolutional-based models improve the modeling\nperformances compared with a model that accepts only the geographical features.\nOverall, conv-net outperforms CDAE-net in terms of LUC predictive performance.\nNonetheless, CDAE-net performs better when the data are noisy. \n\n"}
{"id": "1803.01577", "contents": "Title: Predicting Out-of-View Feature Points for Model-Based Camera Pose\n  Estimation Abstract: In this work we present a novel framework that uses deep learning to predict\nobject feature points that are out-of-view in the input image. This system was\ndeveloped with the application of model-based tracking in mind, particularly in\nthe case of autonomous inspection robots, where only partial views of the\nobject are available. Out-of-view prediction is enabled by applying scaling to\nthe feature point labels during network training. This is combined with a\nrecurrent neural network architecture designed to provide the final prediction\nlayers with rich feature information from across the spatial extent of the\ninput image. To show the versatility of these out-of-view predictions, we\ndescribe how to integrate them in both a particle filter tracker and an\noptimisation based tracker. To evaluate our work we compared our framework with\none that predicts only points inside the image. We show that as the amount of\nthe object in view decreases, being able to predict outside the image bounds\nadds robustness to the final pose estimation. \n\n"}
{"id": "1803.01599", "contents": "Title: AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation Abstract: Supervised deep learning methods have shown promising results for the task of\nmonocular depth estimation; but acquiring ground truth is costly, and prone to\nnoise as well as inaccuracies. While synthetic datasets have been used to\ncircumvent above problems, the resultant models do not generalize well to\nnatural scenes due to the inherent domain shift. Recent adversarial approaches\nfor domain adaption have performed well in mitigating the differences between\nthe source and target domains. But these methods are mostly limited to a\nclassification setup and do not scale well for fully-convolutional\narchitectures. In this work, we propose AdaDepth - an unsupervised domain\nadaptation strategy for the pixel-wise regression task of monocular depth\nestimation. The proposed approach is devoid of above limitations through a)\nadversarial learning and b) explicit imposition of content consistency on the\nadapted target representation. Our unsupervised approach performs competitively\nwith other established approaches on depth estimation tasks and achieves\nstate-of-the-art results in a semi-supervised setting. \n\n"}
{"id": "1803.03851", "contents": "Title: Revisiting Decomposable Submodular Function Minimization with Incidence\n  Relations Abstract: We introduce a new approach to decomposable submodular function minimization\n(DSFM) that exploits incidence relations. Incidence relations describe which\nvariables effectively influence the component functions, and when properly\nutilized, they allow for improving the convergence rates of DSFM solvers. Our\nmain results include the precise parametrization of the DSFM problem based on\nincidence relations, the development of new scalable alternative projections\nand parallel coordinate descent methods and an accompanying rigorous analysis\nof their convergence rates. \n\n"}
{"id": "1803.05494", "contents": "Title: Improving Object Counting with Heatmap Regulation Abstract: In this paper, we propose a simple and effective way to improve one-look\nregression models for object counting from images. We use class activation map\nvisualizations to illustrate the drawbacks of learning a pure one-look\nregression model for a counting task. Based on these insights, we enhance\none-look regression counting models by regulating activation maps from the\nfinal convolution layer of the network with coarse ground-truth activation maps\ngenerated from simple dot annotations. We call this strategy heatmap regulation\n(HR). We show that this simple enhancement effectively suppresses false\ndetections generated by the corresponding one-look baseline model and also\nimproves the performance in terms of false negatives. Evaluations are performed\non four different counting datasets --- two for car counting (CARPK, PUCPR+),\none for crowd counting (WorldExpo) and another for biological cell counting\n(VGG-Cells). Adding HR to a simple VGG front-end improves performance on all\nthese benchmarks compared to a simple one-look baseline model and results in\nstate-of-the-art performance for car counting. \n\n"}
{"id": "1803.05648", "contents": "Title: LEGO: Learning Edge with Geometry all at Once by Watching Videos Abstract: Learning to estimate 3D geometry in a single image by watching unlabeled\nvideos via deep convolutional network is attracting significant attention. In\nthis paper, we introduce a \"3D as-smooth-as-possible (3D-ASAP)\" prior inside\nthe pipeline, which enables joint estimation of edges and 3D scene, yielding\nresults with significant improvement in accuracy for fine detailed structures.\nSpecifically, we define the 3D-ASAP prior by requiring that any two points\nrecovered in 3D from an image should lie on an existing planar surface if no\nother cues provided. We design an unsupervised framework that Learns Edges and\nGeometry (depth, normal) all at Once (LEGO). The predicted edges are embedded\ninto depth and surface normal smoothness terms, where pixels without edges\nin-between are constrained to satisfy the prior. In our framework, the\npredicted depths, normals and edges are forced to be consistent all the time.\nWe conduct experiments on KITTI to evaluate our estimated geometry and\nCityScapes to perform edge evaluation. We show that in all of the tasks,\ni.e.depth, normal and edge, our algorithm vastly outperforms other\nstate-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach. \n\n"}
{"id": "1803.05729", "contents": "Title: Exploring Linear Relationship in Feature Map Subspace for ConvNets\n  Compression Abstract: While the research on convolutional neural networks (CNNs) is progressing\nquickly, the real-world deployment of these models is often limited by\ncomputing resources and memory constraints. In this paper, we address this\nissue by proposing a novel filter pruning method to compress and accelerate\nCNNs. Our work is based on the linear relationship identified in different\nfeature map subspaces via visualization of feature maps. Such linear\nrelationship implies that the information in CNNs is redundant. Our method\neliminates the redundancy in convolutional filters by applying subspace\nclustering to feature maps. In this way, most of the representative information\nin the network can be retained in each cluster. Therefore, our method provides\nan effective solution to filter pruning for which most existing methods\ndirectly remove filters based on simple heuristics. The proposed method is\nindependent of the network structure, thus it can be adopted by any\noff-the-shelf deep learning libraries. Experiments on different networks and\ntasks show that our method outperforms existing techniques before fine-tuning,\nand achieves the state-of-the-art results after fine-tuning. \n\n"}
{"id": "1803.05858", "contents": "Title: Pseudo Mask Augmented Object Detection Abstract: In this work, we present a novel and effective framework to facilitate object\ndetection with the instance-level segmentation information that is only\nsupervised by bounding box annotation. Starting from the joint object detection\nand instance segmentation network, we propose to recursively estimate the\npseudo ground-truth object masks from the instance-level object segmentation\nnetwork training, and then enhance the detection network with top-down\nsegmentation feedbacks. The pseudo ground truth mask and network parameters are\noptimized alternatively to mutually benefit each other. To obtain the promising\npseudo masks in each iteration, we embed a graphical inference that\nincorporates the low-level image appearance consistency and the bounding box\nannotations to refine the segmentation masks predicted by the segmentation\nnetwork. Our approach progressively improves the object detection performance\nby incorporating the detailed pixel-wise information learned from the\nweakly-supervised segmentation network. Extensive evaluation on the detection\ntask in PASCAL VOC 2007 and 2012 [12] verifies that the proposed approach is\neffective. \n\n"}
{"id": "1803.06077", "contents": "Title: Real-time Detection, Tracking, and Classification of Moving and\n  Stationary Objects using Multiple Fisheye Images Abstract: The ability to detect pedestrians and other moving objects is crucial for an\nautonomous vehicle. This must be done in real-time with minimum system\noverhead. This paper discusses the implementation of a surround view system to\nidentify moving as well as static objects that are close to the ego vehicle.\nThe algorithm works on 4 views captured by fisheye cameras which are merged\ninto a single frame. The moving object detection and tracking solution uses\nminimal system overhead to isolate regions of interest (ROIs) containing moving\nobjects. These ROIs are then analyzed using a deep neural network (DNN) to\ncategorize the moving object. With deployment and testing on a real car in\nurban environments, we have demonstrated the practical feasibility of the\nsolution. The video demos of our algorithm have been uploaded to Youtube:\nhttps://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs \n\n"}
{"id": "1803.06316", "contents": "Title: Temporal Gaussian Mixture Layer for Videos Abstract: We introduce a new convolutional layer named the Temporal Gaussian Mixture\n(TGM) layer and present how it can be used to efficiently capture longer-term\ntemporal information in continuous activity videos. The TGM layer is a temporal\nconvolutional layer governed by a much smaller set of parameters (e.g.,\nlocation/variance of Gaussians) that are fully differentiable. We present our\nfully convolutional video models with multiple TGM layers for activity\ndetection. The extensive experiments on multiple datasets, including Charades\nand MultiTHUMOS, confirm the effectiveness of TGM layers, significantly\noutperforming the state-of-the-arts. \n\n"}
{"id": "1803.06329", "contents": "Title: Learning deep structured active contours end-to-end Abstract: The world is covered with millions of buildings, and precisely knowing each\ninstance's position and extents is vital to a multitude of applications.\nRecently, automated building footprint segmentation models have shown superior\ndetection accuracy thanks to the usage of Convolutional Neural Networks (CNN).\nHowever, even the latest evolutions struggle to precisely delineating borders,\nwhich often leads to geometric distortions and inadvertent fusion of adjacent\nbuilding instances. We propose to overcome this issue by exploiting the\ndistinct geometric properties of buildings. To this end, we present Deep\nStructured Active Contours (DSAC), a novel framework that integrates priors and\nconstraints into the segmentation process, such as continuous boundaries,\nsmooth edges, and sharp corners. To do so, DSAC employs Active Contour Models\n(ACM), a family of constraint- and prior-based polygonal models. We learn ACM\nparameterizations per instance using a CNN, and show how to incorporate all\ncomponents in a structured output model, making DSAC trainable end-to-end. We\nevaluate DSAC on three challenging building instance segmentation datasets,\nwhere it compares favorably against state-of-the-art. Code will be made\navailable. \n\n"}
{"id": "1803.07360", "contents": "Title: Adaptive Co-weighting Deep Convolutional Features For Object Retrieval Abstract: Aggregating deep convolutional features into a global image vector has\nattracted sustained attention in image retrieval. In this paper, we propose an\nefficient unsupervised aggregation method that uses an adaptive Gaussian filter\nand an elementvalue sensitive vector to co-weight deep features. Specifically,\nthe Gaussian filter assigns large weights to features of region-of-interests\n(RoI) by adaptively determining the RoI's center, while the element-value\nsensitive channel vector suppresses burstiness phenomenon by assigning small\nweights to feature maps with large sum values of all locations. Experimental\nresults on benchmark datasets validate the proposed two weighting schemes both\neffectively improve the discrimination power of image vectors. Furthermore,\nwith the same experimental setting, our method outperforms other very recent\naggregation approaches by a considerable margin. \n\n"}
{"id": "1803.08842", "contents": "Title: Audio-Visual Event Localization in Unconstrained Videos Abstract: In this paper, we introduce a novel problem of audio-visual event\nlocalization in unconstrained videos. We define an audio-visual event as an\nevent that is both visible and audible in a video segment. We collect an\nAudio-Visual Event(AVE) dataset to systemically investigate three temporal\nlocalization tasks: supervised and weakly-supervised audio-visual event\nlocalization, and cross-modality localization. We develop an audio-guided\nvisual attention mechanism to explore audio-visual correlations, propose a dual\nmultimodal residual network (DMRN) to fuse information over the two modalities,\nand introduce an audio-visual distance learning network to handle the\ncross-modality localization. Our experiments support the following findings:\njoint modeling of auditory and visual modalities outperforms independent\nmodeling, the learned attention can capture semantics of sounding objects,\ntemporal alignment is important for audio-visual fusion, the proposed DMRN is\neffective in fusing audio-visual features, and strong correlations between the\ntwo modalities enable cross-modality localization. \n\n"}
{"id": "1803.09025", "contents": "Title: Realtime Time Synchronized Event-based Stereo Abstract: In this work, we propose a novel event based stereo method which addresses\nthe problem of motion blur for a moving event camera. Our method uses the\nvelocity of the camera and a range of disparities to synchronize the positions\nof the events, as if they were captured at a single point in time. We represent\nthese events using a pair of novel time synchronized event disparity volumes,\nwhich we show remove motion blur for pixels at the correct disparity in the\nvolume, while further blurring pixels at the wrong disparity. We then apply a\nnovel matching cost over these time synchronized event disparity volumes, which\nboth rewards similarity between the volumes while penalizing blurriness. We\nshow that our method outperforms more expensive, smoothing based event stereo\nmethods, by evaluating on the Multi Vehicle Stereo Event Camera dataset. \n\n"}
{"id": "1803.09132", "contents": "Title: Multi-Level Factorisation Net for Person Re-Identification Abstract: Key to effective person re-identification (Re-ID) is modelling discriminative\nand view-invariant factors of person appearance at both high and low semantic\nlevels. Recently developed deep Re-ID models either learn a holistic single\nsemantic level feature representation and/or require laborious human annotation\nof these factors as attributes. We propose Multi-Level Factorisation Net\n(MLFN), a novel network architecture that factorises the visual appearance of a\nperson into latent discriminative factors at multiple semantic levels without\nmanual annotation. MLFN is composed of multiple stacked blocks. Each block\ncontains multiple factor modules to model latent factors at a specific level,\nand factor selection modules that dynamically select the factor modules to\ninterpret the content of each input image. The outputs of the factor selection\nmodules also provide a compact latent factor descriptor that is complementary\nto the conventional deeply learned features. MLFN achieves state-of-the-art\nresults on three Re-ID datasets, as well as compelling results on the general\nobject categorisation CIFAR-100 dataset. \n\n"}
{"id": "1803.09331", "contents": "Title: StarMap for Category-Agnostic Keypoint and Viewpoint Estimation Abstract: Semantic keypoints provide concise abstractions for a variety of visual\nunderstanding tasks. Existing methods define semantic keypoints separately for\neach category with a fixed number of semantic labels in fixed indices. As a\nresult, this keypoint representation is in-feasible when objects have a varying\nnumber of parts, e.g. chairs with varying number of legs. We propose a\ncategory-agnostic keypoint representation, which combines a multi-peak heatmap\n(StarMap) for all the keypoints and their corresponding features as 3D\nlocations in the canonical viewpoint (CanViewFeature) defined for each\ninstance. Our intuition is that the 3D locations of the keypoints in canonical\nobject views contain rich semantic and compositional information. Using our\nflexible representation, we demonstrate competitive performance in keypoint\ndetection and localization compared to category-specific state-of-the-art\nmethods. Moreover, we show that when augmented with an additional depth channel\n(DepthMap) to lift the 2D keypoints to 3D, our representation can achieve\nstate-of-the-art results in viewpoint estimation. Finally, we show that our\ncategory-agnostic keypoint representation can be generalized to novel\ncategories. \n\n"}
{"id": "1803.09359", "contents": "Title: A Face Recognition Signature Combining Patch-based Features with Soft\n  Facial Attributes Abstract: This paper focuses on improving face recognition performance with a new\nsignature combining implicit facial features with explicit soft facial\nattributes. This signature has two components: the existing patch-based\nfeatures and the soft facial attributes. A deep convolutional neural network\nadapted from state-of-the-art networks is used to learn the soft facial\nattributes. Then, a signature matcher is introduced that merges the\ncontributions of both patch-based features and the facial attributes. In this\nmatcher, the matching scores computed from patch-based features and the facial\nattributes are combined to obtain a final matching score. The matcher is also\nextended so that different weights are assigned to different facial attributes.\nThe proposed signature and matcher have been evaluated with the UR2D system on\nthe UHDB31 and IJB-A datasets. The experimental results indicate that the\nproposed signature achieve better performance than using only patch-based\nfeatures. The Rank-1 accuracy is improved significantly by 4% and 0.37% on the\ntwo datasets when compared with the UR2D system. \n\n"}
{"id": "1803.09588", "contents": "Title: Efficient Image Dataset Classification Difficulty Estimation for\n  Predicting Deep-Learning Accuracy Abstract: In the deep-learning community new algorithms are published at an incredible\npace. Therefore, solving an image classification problem for new datasets\nbecomes a challenging task, as it requires to re-evaluate published algorithms\nand their different configurations in order to find a close to optimal\nclassifier. To facilitate this process, before biasing our decision towards a\nclass of neural networks or running an expensive search over the network space,\nwe propose to estimate the classification difficulty of the dataset. Our method\ncomputes a single number that characterizes the dataset difficulty 27x faster\nthan training state-of-the-art networks. The proposed method can be used in\ncombination with network topology and hyper-parameter search optimizers to\nefficiently drive the search towards promising neural-network configurations. \n\n"}
{"id": "1803.09926", "contents": "Title: Diagonalwise Refactorization: An Efficient Training Method for Depthwise\n  Convolutions Abstract: Depthwise convolutions provide significant performance benefits owing to the\nreduction in both parameters and mult-adds. However, training depthwise\nconvolution layers with GPUs is slow in current deep learning frameworks\nbecause their implementations cannot fully utilize the GPU capacity. To address\nthis problem, in this paper we present an efficient method (called diagonalwise\nrefactorization) for accelerating the training of depthwise convolution layers.\nOur key idea is to rearrange the weight vectors of a depthwise convolution into\na large diagonal weight matrix so as to convert the depthwise convolution into\none single standard convolution, which is well supported by the cuDNN library\nthat is highly-optimized for GPU computations. We have implemented our training\nmethod in five popular deep learning frameworks. Evaluation results show that\nour proposed method gains $15.4\\times$ training speedup on Darknet, $8.4\\times$\non Caffe, $5.4\\times$ on PyTorch, $3.5\\times$ on MXNet, and $1.4\\times$ on\nTensorFlow, compared to their original implementations of depthwise\nconvolutions. \n\n"}
{"id": "1803.10930", "contents": "Title: B-DCGAN:Evaluation of Binarized DCGAN for FPGA Abstract: We are trying to implement deep neural networks in the edge computing\nenvironment for real-world applications such as the IoT(Internet of Things),\nthe FinTech etc., for the purpose of utilizing the significant achievement of\nDeep Learning in recent years. Especially, we now focus algorithm\nimplementation on FPGA, because FPGA is one of the promising devices for\nlow-cost and low-power implementation of the edge computer. In this work, we\nintroduce Binary-DCGAN(B-DCGAN) - Deep Convolutional GAN model with binary\nweights and activations, and with using integer-valued operations in forward\npass(train-time and run-time). And we show how to implement B-DCGAN on\nFPGA(Xilinx Zynq). Using the B-DCGAN, we do feasibility study of FPGA's\ncharacteristic and performance for Deep Learning. Because the binarization and\nusing integer-valued operation reduce the memory capacity and the number of the\ncircuit gates, it is very effective for FPGA implementation. On the other hand,\nthe quality of generated data from the model will be decreased by these\nreductions. So we investigate the influence of these reductions. \n\n"}
{"id": "1803.11157", "contents": "Title: Security Consideration For Deep Learning-Based Image Forensics Abstract: Recently, image forensics community has paied attention to the research on\nthe design of effective algorithms based on deep learning technology and facts\nproved that combining the domain knowledge of image forensics and deep learning\nwould achieve more robust and better performance than the traditional schemes.\nInstead of improving it, in this paper, the safety of deep learning based\nmethods in the field of image forensics is taken into account. To the best of\nour knowledge, this is a first work focusing on this topic. Specifically, we\nexperimentally find that the method using deep learning would fail when adding\nthe slight noise into the images (adversarial images). Furthermore, two kinds\nof strategys are proposed to enforce security of deep learning-based method.\nFirstly, an extra penalty term to the loss function is added, which is referred\nto the 2-norm of the gradient of the loss with respect to the input images, and\nthen an novel training method are adopt to train the model by fusing the normal\nand adversarial images. Experimental results show that the proposed algorithm\ncan achieve good performance even in the case of adversarial images and provide\na safety consideration for deep learning-based image forensics \n\n"}
{"id": "1803.11182", "contents": "Title: Towards Open-Set Identity Preserving Face Synthesis Abstract: We propose a framework based on Generative Adversarial Networks to\ndisentangle the identity and attributes of faces, such that we can conveniently\nrecombine different identities and attributes for identity preserving face\nsynthesis in open domains. Previous identity preserving face synthesis\nprocesses are largely confined to synthesizing faces with known identities that\nare already in the training dataset. To synthesize a face with identity outside\nthe training dataset, our framework requires one input image of that subject to\nproduce an identity vector, and any other input face image to extract an\nattribute vector capturing, e.g., pose, emotion, illumination, and even the\nbackground. We then recombine the identity vector and the attribute vector to\nsynthesize a new face of the subject with the extracted attribute. Our proposed\nframework does not need to annotate the attributes of faces in any way. It is\ntrained with an asymmetric loss function to better preserve the identity and\nstabilize the training process. It can also effectively leverage large amounts\nof unlabeled training face images to further improve the fidelity of the\nsynthesized faces for subjects that are not presented in the labeled training\nface dataset. Our experiments demonstrate the efficacy of the proposed\nframework. We also present its usage in a much broader set of applications\nincluding face frontalization, face attribute morphing, and face adversarial\nexample detection. \n\n"}
{"id": "1803.11404", "contents": "Title: Cross-modal Deep Variational Hand Pose Estimation Abstract: The human hand moves in complex and high-dimensional ways, making estimation\nof 3D hand pose configurations from images alone a challenging task. In this\nwork we propose a method to learn a statistical hand model represented by a\ncross-modal trained latent space via a generative deep neural network. We\nderive an objective function from the variational lower bound of the VAE\nframework and jointly optimize the resulting cross-modal KL-divergence and the\nposterior reconstruction objective, naturally admitting a training regime that\nleads to a coherent latent space across multiple modalities such as RGB images,\n2D keypoint detections or 3D hand configurations. Additionally, it grants a\nstraightforward way of using semi-supervision. This latent space can be\ndirectly used to estimate 3D hand poses from RGB images, outperforming the\nstate-of-the art in different settings. Furthermore, we show that our proposed\nmethod can be used without changes on depth images and performs comparably to\nspecialized methods. Finally, the model is fully generative and can synthesize\nconsistent pairs of hand configurations across modalities. We evaluate our\nmethod on both RGB and depth datasets and analyze the latent space\nqualitatively. \n\n"}
{"id": "1803.11438", "contents": "Title: Reconstruction Network for Video Captioning Abstract: In this paper, the problem of describing visual contents of a video sequence\nwith natural language is addressed. Unlike previous video captioning work\nmainly exploiting the cues of video contents to make a language description, we\npropose a reconstruction network (RecNet) with a novel\nencoder-decoder-reconstructor architecture, which leverages both the forward\n(video to sentence) and backward (sentence to video) flows for video\ncaptioning. Specifically, the encoder-decoder makes use of the forward flow to\nproduce the sentence description based on the encoded video semantic features.\nTwo types of reconstructors are customized to employ the backward flow and\nreproduce the video features based on the hidden state sequence generated by\nthe decoder. The generation loss yielded by the encoder-decoder and the\nreconstruction loss introduced by the reconstructor are jointly drawn into\ntraining the proposed RecNet in an end-to-end fashion. Experimental results on\nbenchmark datasets demonstrate that the proposed reconstructor can boost the\nencoder-decoder models and leads to significant gains in video caption\naccuracy. \n\n"}
{"id": "1803.11493", "contents": "Title: 3D Pose Estimation and 3D Model Retrieval for Objects in the Wild Abstract: We propose a scalable, efficient and accurate approach to retrieve 3D models\nfor objects in the wild. Our contribution is twofold. We first present a 3D\npose estimation approach for object categories which significantly outperforms\nthe state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior\nto retrieve 3D models which accurately represent the geometry of objects in RGB\nimages. For this purpose, we render depth images from 3D models under our\npredicted pose and match learned image descriptors of RGB images against those\nof rendered depth images using a CNN-based multi-view metric learning approach.\nIn this way, we are the first to report quantitative results for 3D model\nretrieval on Pascal3D+, where our method chooses the same models as human\nannotators for 50% of the validation images on average. In addition, we show\nthat our method, which was trained purely on Pascal3D+, retrieves rich and\naccurate 3D models from ShapeNet given RGB images of objects in the wild. \n\n"}
{"id": "1804.00389", "contents": "Title: Low-Latency Video Semantic Segmentation Abstract: Recent years have seen remarkable progress in semantic segmentation. Yet, it\nremains a challenging task to apply segmentation techniques to video-based\napplications. Specifically, the high throughput of video streams, the sheer\ncost of running fully convolutional networks, together with the low-latency\nrequirements in many real-world applications, e.g. autonomous driving, present\na significant challenge to the design of the video segmentation framework. To\ntackle this combined challenge, we develop a framework for video semantic\nsegmentation, which incorporates two novel components: (1) a feature\npropagation module that adaptively fuses features over time via spatially\nvariant convolution, thus reducing the cost of per-frame computation; and (2)\nan adaptive scheduler that dynamically allocate computation based on accuracy\nprediction. Both components work together to ensure low latency while\nmaintaining high segmentation quality. On both Cityscapes and CamVid, the\nproposed framework obtained competitive performance compared to the state of\nthe art, while substantially reducing the latency, from 360 ms to 119 ms. \n\n"}
{"id": "1804.00393", "contents": "Title: Generative Spatiotemporal Modeling Of Neutrophil Behavior Abstract: Cell motion and appearance have a strong correlation with cell cycle and\ndisease progression. Many contemporary efforts in machine learning utilize\nspatio-temporal models to predict a cell's physical state and, consequently,\nthe advancement of disease. Alternatively, generative models learn the\nunderlying distribution of the data, creating holistic representations that can\nbe used in learning. In this work, we propose an aggregate model that combine\nGenerative Adversarial Networks (GANs) and Autoregressive (AR) models to\npredict cell motion and appearance in human neutrophils imaged by differential\ninterference contrast (DIC) microscopy. We bifurcate the task of learning cell\nstatistics by leveraging GANs for the spatial component and AR models for the\ntemporal component. The aggregate model learned results offer a promising\ncomputational environment for studying changes in organellar shape, quantity,\nand spatial distribution over large sequences. \n\n"}
{"id": "1804.00516", "contents": "Title: Towards Highly Accurate Coral Texture Images Classification Using Deep\n  Convolutional Neural Networks and Data Augmentation Abstract: The recognition of coral species based on underwater texture images pose a\nsignificant difficulty for machine learning algorithms, due to the three\nfollowing challenges embedded in the nature of this data: 1) datasets do not\ninclude information about the global structure of the coral; 2) several species\nof coral have very similar characteristics; and 3) defining the spatial borders\nbetween classes is difficult as many corals tend to appear together in groups.\nFor this reason, the classification of coral species has always required an aid\nfrom a domain expert. The objective of this paper is to develop an accurate\nclassification model for coral texture images. Current datasets contain a large\nnumber of imbalanced classes, while the images are subject to inter-class\nvariation. We have analyzed 1) several Convolutional Neural Network (CNN)\narchitectures, 2) data augmentation techniques and 3) transfer learning. We\nhave achieved the state-of-the art accuracies using different variations of\nResNet on the two current coral texture datasets, EILAT and RSMAS. \n\n"}
{"id": "1804.00819", "contents": "Title: End-to-End Dense Video Captioning with Masked Transformer Abstract: Dense video captioning aims to generate text descriptions for all events in\nan untrimmed video. This involves both detecting and describing events.\nTherefore, all previous methods on dense video captioning tackle this problem\nby building two models, i.e. an event proposal and a captioning model, for\nthese two sub-problems. The models are either trained separately or in\nalternation. This prevents direct influence of the language description to the\nevent proposal, which is important for generating accurate descriptions. To\naddress this problem, we propose an end-to-end transformer model for dense\nvideo captioning. The encoder encodes the video into appropriate\nrepresentations. The proposal decoder decodes from the encoding with different\nanchors to form video event proposals. The captioning decoder employs a masking\nnetwork to restrict its attention to the proposal event over the encoding\nfeature. This masking network converts the event proposal to a differentiable\nmask, which ensures the consistency between the proposal and captioning during\ntraining. In addition, our model employs a self-attention mechanism, which\nenables the use of efficient non-recurrent structure during encoding and leads\nto performance improvements. We demonstrate the effectiveness of this\nend-to-end model on ActivityNet Captions and YouCookII datasets, where we\nachieved 10.12 and 6.58 METEOR score, respectively. \n\n"}
{"id": "1804.00892", "contents": "Title: When will you do what? - Anticipating Temporal Occurrences of Activities Abstract: Analyzing human actions in videos has gained increased attention recently.\nWhile most works focus on classifying and labeling observed video frames or\nanticipating the very recent future, making long-term predictions over more\nthan just a few seconds is a task with many practical applications that has not\nyet been addressed. In this paper, we propose two methods to predict a\nconsiderably large amount of future actions and their durations. Both, a CNN\nand an RNN are trained to learn future video labels based on previously seen\ncontent. We show that our methods generate accurate predictions of the future\neven for long videos with a huge amount of different actions and can even deal\nwith noisy or erroneous input information. \n\n"}
{"id": "1804.01159", "contents": "Title: Crystal Loss and Quality Pooling for Unconstrained Face Verification and\n  Recognition Abstract: In recent years, the performance of face verification and recognition systems\nbased on deep convolutional neural networks (DCNNs) has significantly improved.\nA typical pipeline for face verification includes training a deep network for\nsubject classification with softmax loss, using the penultimate layer output as\nthe feature descriptor, and generating a cosine similarity score given a pair\nof face images or videos. The softmax loss function does not optimize the\nfeatures to have higher similarity score for positive pairs and lower\nsimilarity score for negative pairs, which leads to a performance gap. In this\npaper, we propose a new loss function, called Crystal Loss, that restricts the\nfeatures to lie on a hypersphere of a fixed radius. The loss can be easily\nimplemented using existing deep learning frameworks. We show that integrating\nthis simple step in the training pipeline significantly improves the\nperformance of face verification and recognition systems. We achieve\nstate-of-the-art performance for face verification and recognition on\nchallenging LFW, IJB-A, IJB-B and IJB-C datasets over a large range of false\nalarm rates (10-1 to 10-7). \n\n"}
{"id": "1804.02152", "contents": "Title: Adaptive Quantile Sparse Image (AQuaSI) Prior for Inverse Imaging\n  Problems Abstract: Inverse problems play a central role for many classical computer vision and\nimage processing tasks. Many inverse problems are ill-posed, and hence require\na prior to regularize the solution space. However, many of the existing priors,\nlike total variation, are based on ad-hoc assumptions that have difficulties to\nrepresent the actual distribution of natural images. Thus, a key challenge in\nresearch on image processing is to find better suited priors to represent\nnatural images.\n  In this work, we propose the Adaptive Quantile Sparse Image (AQuaSI) prior.\nIt is based on a quantile filter, can be used as a joint filter on guidance\ndata, and be readily plugged into a wide range of numerical optimization\nalgorithms. We demonstrate the efficacy of the proposed prior in joint\nRGB/depth upsampling, on RGB/NIR image restoration, and in a comparison with\nrelated regularization by denoising approaches. \n\n"}
{"id": "1804.02201", "contents": "Title: Ensemble Manifold Segmentation for Model Distillation and\n  Semi-supervised Learning Abstract: Manifold theory has been the central concept of many learning methods.\nHowever, learning modern CNNs with manifold structures has not raised due\nattention, mainly because of the inconvenience of imposing manifold structures\nonto the architecture of the CNNs. In this paper we present ManifoldNet, a\nnovel method to encourage learning of manifold-aware representations. Our\napproach segments the input manifold into a set of fragments. By assigning the\ncorresponding segmentation id as a pseudo label to every sample, we convert the\nproblem of preserving the local manifold structure into a point-wise\nclassification task. Due to its unsupervised nature, the segmentation tends to\nbe noisy. We mitigate this by introducing ensemble manifold segmentation (EMS).\nEMS accounts for the manifold structure by dividing the training data into an\nensemble of classification training sets that contain samples of local\nproximity. CNNs are trained on these ensembles under a multi-task learning\nframework to conform to the manifold. ManifoldNet can be trained with only the\npseudo labels or together with task-specific labels. We evaluate ManifoldNet on\ntwo different tasks: network imitation (distillation) and semi-supervised\nlearning. Our experiments show that the manifold structures are effectively\nutilized for both unsupervised and semi-supervised learning. \n\n"}
{"id": "1804.02595", "contents": "Title: Training Multi-organ Segmentation Networks with Sample Selection by\n  Relaxed Upper Confident Bound Abstract: Deep convolutional neural networks (CNNs), especially fully convolutional\nnetworks, have been widely applied to automatic medical image segmentation\nproblems, e.g., multi-organ segmentation. Existing CNN-based segmentation\nmethods mainly focus on looking for increasingly powerful network\narchitectures, but pay less attention to data sampling strategies for training\nnetworks more effectively. In this paper, we present a simple but effective\nsample selection method for training multi-organ segmentation networks. Sample\nselection exhibits an exploitation-exploration strategy, i.e., exploiting hard\nsamples and exploring less frequently visited samples. Based on the fact that\nvery hard samples might have annotation errors, we propose a new sample\nselection policy, named Relaxed Upper Confident Bound (RUCB). Compared with\nother sample selection policies, e.g., Upper Confident Bound (UCB), it exploits\na range of hard samples rather than being stuck with a small set of very hard\nones, which mitigates the influence of annotation errors during training. We\napply this new sample selection policy to training a multi-organ segmentation\nnetwork on a dataset containing 120 abdominal CT scans and show that it boosts\nsegmentation performance significantly. \n\n"}
{"id": "1804.02675", "contents": "Title: Anticipating Traffic Accidents with Adaptive Loss and Large-scale\n  Incident DB Abstract: In this paper, we propose a novel approach for traffic accident anticipation\nthrough (i) Adaptive Loss for Early Anticipation (AdaLEA) and (ii) a\nlarge-scale self-annotated incident database for anticipation. The proposed\nAdaLEA allows a model to gradually learn an earlier anticipation as training\nprogresses. The loss function adaptively assigns penalty weights depending on\nhow early the model can an- ticipate a traffic accident at each epoch.\nAdditionally, we construct a Near-miss Incident DataBase for anticipation. This\ndatabase contains an enormous number of traffic near- miss incident videos and\nannotations for detail evaluation of two tasks, risk anticipation and\nrisk-factor anticipation. In our experimental results, we found our proposal\nachieved the highest scores for risk anticipation (+6.6% better on mean average\nprecision (mAP) and 2.36 sec earlier than previous work on the average\ntime-to-collision (ATTC)) and risk-factor anticipation (+4.3% better on mAP and\n0.70 sec earlier than previous work on ATTC). \n\n"}
{"id": "1804.02690", "contents": "Title: Detecting Multi-Oriented Text with Corner-based Region Proposals Abstract: Previous approaches for scene text detection usually rely on manually defined\nsliding windows. This work presents an intuitive two-stage region-based method\nto detect multi-oriented text without any prior knowledge regarding the textual\nshape. In the first stage, we estimate the possible locations of text instances\nby detecting and linking corners instead of shifting a set of default anchors.\nThe quadrilateral proposals are geometry adaptive, which allows our method to\ncope with various text aspect ratios and orientations. In the second stage, we\ndesign a new pooling layer named Dual-RoI Pooling which embeds data\naugmentation inside the region-wise subnetwork for more robust classification\nand regression over these proposals. Experimental results on public benchmarks\nconfirm that the proposed method is capable of achieving comparable performance\nwith state-of-the-art methods. The code is publicly available at\nhttps://github.com/xhzdeng/crpn \n\n"}
{"id": "1804.02721", "contents": "Title: Image Segmentation using Sparse Subset Selection Abstract: In this paper, we present a new image segmentation method based on the\nconcept of sparse subset selection. Starting with an over-segmentation, we\nadopt local spectral histogram features to encode the visual information of the\nsmall segments into high-dimensional vectors, called superpixel features. Then,\nthe superpixel features are fed into a novel convex model which efficiently\nleverages the features to group the superpixels into a proper number of\ncoherent regions. Our model automatically determines the optimal number of\ncoherent regions and superpixels assignment to shape final segments. To solve\nour model, we propose a numerical algorithm based on the alternating direction\nmethod of multipliers (ADMM), whose iterations consist of two highly\nparallelizable sub-problems. We show each sub-problem enjoys closed-form\nsolution which makes the ADMM iterations computationally very efficient.\nExtensive experiments on benchmark image segmentation datasets demonstrate that\nour proposed method in combination with an over-segmentation can provide high\nquality and competitive results compared to the existing state-of-the-art\nmethods. \n\n"}
{"id": "1804.03193", "contents": "Title: An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural\n  Networks Abstract: Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. In a successful adversarial attack, the targeted mis-classification\nshould be achieved with the minimal distortion added. In the literature, the\nadded distortions are usually measured by L0, L1, L2, and L infinity norms,\nnamely, L0, L1, L2, and L infinity attacks, respectively. However, there lacks\na versatile framework for all types of adversarial attacks.\n  This work for the first time unifies the methods of generating adversarial\nexamples by leveraging ADMM (Alternating Direction Method of Multipliers), an\noperator splitting optimization approach, such that L0, L1, L2, and L infinity\nattacks can be effectively implemented by this general framework with little\nmodifications. Comparing with the state-of-the-art attacks in each category,\nour ADMM-based attacks are so far the strongest, achieving both the 100% attack\nsuccess rate and the minimal distortion. \n\n"}
{"id": "1804.03247", "contents": "Title: Fine-grained Activity Recognition in Baseball Videos Abstract: In this paper, we introduce a challenging new dataset, MLB-YouTube, designed\nfor fine-grained activity detection. The dataset contains two settings:\nsegmented video classification as well as activity detection in continuous\nvideos. We experimentally compare various recognition approaches capturing\ntemporal structure in activity videos, by classifying segmented videos and\nextending those approaches to continuous videos. We also compare models on the\nextremely difficult task of predicting pitch speed and pitch type from\nbroadcast baseball videos. We find that learning temporal structure is valuable\nfor fine-grained activity recognition. \n\n"}
{"id": "1804.03492", "contents": "Title: PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place\n  Recognition Abstract: Unlike its image based counterpart, point cloud based retrieval for place\nrecognition has remained as an unexplored and unsolved problem. This is largely\ndue to the difficulty in extracting local feature descriptors from a point\ncloud that can subsequently be encoded into a global descriptor for the\nretrieval task. In this paper, we propose the PointNetVLAD where we leverage on\nthe recent success of deep networks to solve point cloud based retrieval for\nplace recognition. Specifically, our PointNetVLAD is a combination/modification\nof the existing PointNet and NetVLAD, which allows end-to-end training and\ninference to extract the global descriptor from a given 3D point cloud.\nFurthermore, we propose the \"lazy triplet and quadruplet\" loss functions that\ncan achieve more discriminative and generalizable global descriptors to tackle\nthe retrieval task. We create benchmark datasets for point cloud based\nretrieval for place recognition, and the experimental results on these datasets\nshow the feasibility of our PointNetVLAD. Our code and the link for the\nbenchmark dataset downloads are available in our project website.\nhttp://github.com/mikacuy/pointnetvlad/ \n\n"}
{"id": "1804.04326", "contents": "Title: STAIR Actions: A Video Dataset of Everyday Home Actions Abstract: A new large-scale video dataset for human action recognition, called STAIR\nActions is introduced. STAIR Actions contains 100 categories of action labels\nrepresenting fine-grained everyday home actions so that it can be applied to\nresearch in various home tasks such as nursing, caring, and security. In STAIR\nActions, each video has a single action label. Moreover, for each action\ncategory, there are around 1,000 videos that were obtained from YouTube or\nproduced by crowdsource workers. The duration of each video is mostly five to\nsix seconds. The total number of videos is 102,462. We explain how we\nconstructed STAIR Actions and show the characteristics of STAIR Actions\ncompared to existing datasets for human action recognition. Experiments with\nthree major models for action recognition show that STAIR Actions can train\nlarge models and achieve good performance. STAIR Actions can be downloaded from\nhttp://actions.stair.center \n\n"}
{"id": "1804.04610", "contents": "Title: Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling Abstract: We study 3D shape modeling from a single image and make contributions to it\nin three aspects. First, we present Pix3D, a large-scale benchmark of diverse\nimage-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications\nin shape-related tasks including reconstruction, retrieval, viewpoint\nestimation, etc. Building such a large-scale dataset, however, is highly\nchallenging; existing datasets either contain only synthetic data, or lack\nprecise alignment between 2D images and 3D shapes, or only have a small number\nof images. Second, we calibrate the evaluation criteria for 3D shape\nreconstruction through behavioral studies, and use them to objectively and\nsystematically benchmark cutting-edge reconstruction algorithms on Pix3D.\nThird, we design a novel model that simultaneously performs 3D reconstruction\nand pose estimation; our multi-task learning approach achieves state-of-the-art\nperformance on both tasks. \n\n"}
{"id": "1804.05930", "contents": "Title: Searching Neutrino-Nucleus Coherent Scattering with M\\\"ossbauer\n  Spectroscopy Abstract: The M\\\"ossbauer technique is proposed as an alternative experimental\nprocedure to be used in the detection of Coherent Elastic $\\nu$-Nucleus\nScattering (CENNS). The $Z_{0}$-boson exchange is considered as a perturbation\non the nuclear mean-field potential with a change in the valence neutron\nquantum states in the $^{57}Fe$ nucleus of the M\\\"ossbauer detector sample.\nThis nuclei is a typical isotope used in M\\\"ossbauer spectroscopy. Perturbed\nlevel of the valence neutron accommodates the transferred energy, modifying the\nlocation of the isometric peak of the M\\\"ossbauer electromagnetic resonance. We\ncalculate the isometric shift correction due CENNS and show that this quantity\nis able to be detected with enough precision. Therefore, the difference between\nthe M\\\"ossbauer isometric shift in the presence of a reactor-neutrino beam and\nwithout the neutrinos flux is pointed out as a figure of merit to manifest\nCENNS. In this work, we show that the CENNS correction of the Isomeric Shift is\nof $\\approx 10^{-7}$eV, which is greater than the $10^{-10}$eV resolution of\nthe technique. \n\n"}
{"id": "1804.06202", "contents": "Title: IGCV$2$: Interleaved Structured Sparse Convolutional Neural Networks Abstract: In this paper, we study the problem of designing efficient convolutional\nneural network architectures with the interest in eliminating the redundancy in\nconvolution kernels. In addition to structured sparse kernels, low-rank kernels\nand the product of low-rank kernels, the product of structured sparse kernels,\nwhich is a framework for interpreting the recently-developed interleaved group\nconvolutions (IGC) and its variants (e.g., Xception), has been attracting\nincreasing interests.\n  Motivated by the observation that the convolutions contained in a group\nconvolution in IGC can be further decomposed in the same manner, we present a\nmodularized building block, {IGCV$2$:} interleaved structured sparse\nconvolutions. It generalizes interleaved group convolutions, which is composed\nof two structured sparse kernels, to the product of more structured sparse\nkernels, further eliminating the redundancy. We present the complementary\ncondition and the balance condition to guide the design of structured sparse\nkernels, obtaining a balance among three aspects: model size, computation\ncomplexity and classification accuracy. Experimental results demonstrate the\nadvantage on the balance among these three aspects compared to interleaved\ngroup convolutions and Xception, and competitive performance compared to other\nstate-of-the-art architecture design methods. \n\n"}
{"id": "1804.10871", "contents": "Title: CRAFT: Complementary Recommendations Using Adversarial Feature\n  Transformer Abstract: Traditional approaches for complementary product recommendations rely on\nbehavioral and non-visual data such as customer co-views or co-buys. However,\ncertain domains such as fashion are primarily visual. We propose a framework\nthat harnesses visual cues in an unsupervised manner to learn the distribution\nof co-occurring complementary items in real world images. Our model learns a\nnon-linear transformation between the two manifolds of source and target\ncomplementary item categories (e.g., tops and bottoms in outfits). Given a\nlarge dataset of images containing instances of co-occurring object categories,\nwe train a generative transformer network directly on the feature\nrepresentation space by casting it as an adversarial optimization problem. Such\na conditional generative model can produce multiple novel samples of\ncomplementary items (in the feature space) for a given query item. The final\nrecommendations are selected from the closest real world examples to the\nsynthesized complementary features. We apply our framework to the task of\nrecommending complementary tops for a given bottom clothing item. The\nrecommendations made by our system are diverse, and are favored by human\nexperts over the baseline approaches. \n\n"}
{"id": "1804.11027", "contents": "Title: Deep Co-attention based Comparators For Relative Representation Learning\n  in Person Re-identification Abstract: Person re-identification (re-ID) requires rapid, flexible yet discriminant\nrepresentations to quickly generalize to unseen observations on-the-fly and\nrecognize the same identity across disjoint camera views. Recent effective\nmethods are developed in a pair-wise similarity learning system to detect a\nfixed set of features from distinct regions which are mapped to their vector\nembeddings for the distance measuring. However, the most relevant and crucial\nparts of each image are detected independently without referring to the\ndependency conditioned on one and another. Also, these region based methods\nrely on spatial manipulation to position the local features in comparable\nsimilarity measuring. To combat these limitations, in this paper we introduce\nthe Deep Co-attention based Comparators (DCCs) that fuse the co-dependent\nrepresentations of the paired images so as to focus on the relevant parts of\nboth images and produce their \\textit{relative representations}. Given a pair\nof pedestrian images to be compared, the proposed model mimics the foveation of\nhuman eyes to detect distinct regions concurrent on both images, namely\nco-dependent features, and alternatively attend to relevant regions to fuse\nthem into the similarity learning. Our comparator is capable of producing\ndynamic representations relative to a particular sample every time, and thus\nwell-suited to the case of re-identifying pedestrians on-the-fly. We perform\nextensive experiments to provide the insights and demonstrate the effectiveness\nof the proposed DCCs in person re-ID. Moreover, our approach has achieved the\nstate-of-the-art performance on three benchmark data sets: DukeMTMC-reID\n\\cite{DukeMTMC}, CUHK03 \\cite{FPNN}, and Market-1501 \\cite{Market1501}. \n\n"}
{"id": "1804.11146", "contents": "Title: Cross-Modal Retrieval in the Cooking Context: Learning Semantic\n  Text-Image Embeddings Abstract: Designing powerful tools that support cooking activities has rapidly gained\npopularity due to the massive amounts of available data, as well as recent\nadvances in machine learning that are capable of analyzing them. In this paper,\nwe propose a cross-modal retrieval model aligning visual and textual data (like\npictures of dishes and their recipes) in a shared representation space. We\ndescribe an effective learning scheme, capable of tackling large-scale\nproblems, and validate it on the Recipe1M dataset containing nearly 1 million\npicture-recipe pairs. We show the effectiveness of our approach regarding\nprevious state-of-the-art models and present qualitative results over\ncomputational cooking use cases. \n\n"}
{"id": "1805.00251", "contents": "Title: Conditional Image-to-Image Translation Abstract: Image-to-image translation tasks have been widely investigated with\nGenerative Adversarial Networks (GANs) and dual learning. However, existing\nmodels lack the ability to control the translated results in the target domain\nand their results usually lack of diversity in the sense that a fixed image\nusually leads to (almost) deterministic translation result. In this paper, we\nstudy a new problem, conditional image-to-image translation, which is to\ntranslate an image from the source domain to the target domain conditioned on a\ngiven image in the target domain. It requires that the generated image should\ninherit some domain-specific features of the conditional image from the target\ndomain. Therefore, changing the conditional image in the target domain will\nlead to diverse translation results for a fixed input image from the source\ndomain, and therefore the conditional input image helps to control the\ntranslation results. We tackle this problem with unpaired data based on GANs\nand dual learning. We twist two conditional translation models (one translation\nfrom A domain to B domain, and the other one from B domain to A domain)\ntogether for inputs combination and reconstruction while preserving domain\nindependent features. We carry out experiments on men's faces from-to women's\nfaces translation and edges to shoes&bags translations. The results demonstrate\nthe effectiveness of our proposed method. \n\n"}
{"id": "1805.00324", "contents": "Title: A Deep Face Identification Network Enhanced by Facial Attributes\n  Prediction Abstract: In this paper, we propose a new deep framework which predicts facial\nattributes and leverage it as a soft modality to improve face identification\nperformance. Our model is an end to end framework which consists of a\nconvolutional neural network (CNN) whose output is fanned out into two separate\nbranches; the first branch predicts facial attributes while the second branch\nidentifies face images. Contrary to the existing multi-task methods which only\nuse a shared CNN feature space to train these two tasks jointly, we fuse the\npredicted attributes with the features from the face modality in order to\nimprove the face identification performance. Experimental results show that our\nmodel brings benefits to both face identification as well as facial attribute\nprediction performance, especially in the case of identity facial attributes\nsuch as gender prediction. We tested our model on two standard datasets\nannotated by identities and face attributes. Experimental results indicate that\nthe proposed model outperforms most of the current existing face identification\nand attribute prediction methods. \n\n"}
{"id": "1805.00652", "contents": "Title: MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories\n  and head poses Abstract: Recent approaches on trajectory forecasting use tracklets to predict the\nfuture positions of pedestrians exploiting Long Short Term Memory (LSTM)\narchitectures. This paper shows that adding vislets, that is, short sequences\nof head pose estimations, allows to increase significantly the trajectory\nforecasting performance. We then propose to use vislets in a novel framework\ncalled MX-LSTM, capturing the interplay between tracklets and vislets thanks to\na joint unconstrained optimization of full covariance matrices during the LSTM\nbackpropagation. At the same time, MX-LSTM predicts the future head poses,\nincreasing the standard capabilities of the long-term trajectory forecasting\napproaches. With standard head pose estimators and an attentional-based social\npooling, MX-LSTM scores the new trajectory forecasting state-of-the-art in all\nthe considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic\nmargin when the pedestrians slow down, a case where most of the forecasting\napproaches struggle to provide an accurate solution. \n\n"}
{"id": "1805.00862", "contents": "Title: Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs Abstract: We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers. \n\n"}
{"id": "1805.01818", "contents": "Title: Object and Text-guided Semantics for CNN-based Activity Recognition Abstract: Many previous methods have demonstrated the importance of considering\nsemantically relevant objects for carrying out video-based human activity\nrecognition, yet none of the methods have harvested the power of large text\ncorpora to relate the objects and the activities to be transferred into\nlearning a unified deep convolutional neural network. We present a novel\nactivity recognition CNN which co-learns the object recognition task in an\nend-to-end multitask learning scheme to improve upon the baseline activity\nrecognition performance. We further improve upon the multitask learning\napproach by exploiting a text-guided semantic space to select the most relevant\nobjects with respect to the target activities. To the best of our knowledge, we\nare the first to investigate this approach. \n\n"}
{"id": "1805.02335", "contents": "Title: Skeleton-Based Action Recognition with Spatial Reasoning and Temporal\n  Stack Learning Abstract: Skeleton-based action recognition has made great progress recently, but many\nproblems still remain unsolved. For example, most of the previous methods model\nthe representations of skeleton sequences without abundant spatial structure\ninformation and detailed temporal dynamics features. In this paper, we propose\na novel model with spatial reasoning and temporal stack learning (SR-TSL) for\nskeleton based action recognition, which consists of a spatial reasoning\nnetwork (SRN) and a temporal stack learning network (TSLN). The SRN can capture\nthe high-level spatial structural information within each frame by a residual\ngraph neural network, while the TSLN can model the detailed temporal dynamics\nof skeleton sequences by a composition of multiple skip-clip LSTMs. During\ntraining, we propose a clip-based incremental loss to optimize the model. We\nperform extensive experiments on the SYSU 3D Human-Object Interaction dataset\nand NTU RGB+D dataset and verify the effectiveness of each network of our\nmodel. The comparison results illustrate that our approach achieves much better\nresults than state-of-the-art methods. \n\n"}
{"id": "1805.02833", "contents": "Title: A neural network based algorithm for MRPC time reconstruction Abstract: Multi-gap Resistive Plate Chamber(MRPC) is a widely used timing detector with\na typical time resolution of about 60 ps. This makes MRPC an optimal choice for\nthe time of flight(ToF) system in many large physics experiments. The prior\nwork on improving the time resolution is mainly focused on altering the\ndetector geometry, and therefore the improvement of the data analysis algorithm\nhas not been fully explored. This paper proposes a new time reconstruction\nalgorithm based on the deep neural networks(NN) and improves the MRPC time\nresolution by about 10 ps. Since the development of the high energy physics\nexperiments has pushed the timing requirements for the MRPC to a higher level,\nthis algorithm could become a potential substitution of the time over\nthreshold(ToT) method to achieve a time resolution below 30 ps. \n\n"}
{"id": "1805.03922", "contents": "Title: Ensemble Soft-Margin Softmax Loss for Image Classification Abstract: Softmax loss is arguably one of the most popular losses to train CNN models\nfor image classification. However, recent works have exposed its limitation on\nfeature discriminability. This paper casts a new viewpoint on the weakness of\nsoftmax loss. On the one hand, the CNN features learned using the softmax loss\nare often inadequately discriminative. We hence introduce a soft-margin softmax\nfunction to explicitly encourage the discrimination between different classes.\nOn the other hand, the learned classifier of softmax loss is weak. We propose\nto assemble multiple these weak classifiers to a strong one, inspired by the\nrecognition that the diversity among weak classifiers is critical to a good\nensemble. To achieve the diversity, we adopt the Hilbert-Schmidt Independence\nCriterion (HSIC). Considering these two aspects in one framework, we design a\nnovel loss, named as Ensemble soft-Margin Softmax (EM-Softmax). Extensive\nexperiments on benchmark datasets are conducted to show the superiority of our\ndesign over the baseline softmax loss and several state-of-the-art\nalternatives. \n\n"}
{"id": "1805.04379", "contents": "Title: Measurements of low-$p_{\\rm T}$ electrons from semileptonic\n  heavy-flavour hadron decays at mid-rapidity in pp and Pb-Pb collisions at\n  $\\mathbf{\\sqrt{{\\it s}_\\mathrm{NN}}}$ = 2.76 TeV Abstract: Transverse-momentum ($p_{\\rm T}$) differential yields of electrons from\nsemileptonic heavy-flavour hadron decays have been measured in the most central\n(0-10%) and in semi-central (20-40%) Pb-Pb collisions at $\\sqrt{s_{\\rm NN}} =\n2.76$ TeV. The corresponding production cross section in pp collisions has been\nmeasured at the same energy with substantially reduced systematic uncertainties\nwith respect to previously published results. The modification of the yield in\nPb-Pb collisions with respect to the expectation from an incoherent\nsuperposition of nucleon-nucleon collisions is quantified at mid-rapidity\n($|y|$ $<$ 0.8) in the $p_{\\rm T}$ interval 0.5-3 GeV/$c$ via the nuclear\nmodification factor, $R_{\\rm AA}$. This paper extends the $p_{\\rm T}$ reach of\nthe $R_{\\rm AA}$ measurement towards significantly lower values with respect to\na previous publication. In Pb-Pb collisions the $p_{\\rm T}$-differential\nmeasurements of yields at low $p_{\\rm T}$ are essential to investigate the\nscaling of heavy-flavour production with the number of binary nucleon-nucleon\ncollisions. Heavy-quark hadronization, a collective expansion and even\ninitial-state effects, such as the nuclear modification of the Parton\nDistribution Functions, are also expected to have a significant effect on the\nmeasured distribution. \n\n"}
{"id": "1805.04383", "contents": "Title: Inclusive J/$\\psi$ production in Xe-Xe collisions at $\\sqrt{s_{\\rm NN}}$\n  = 5.44 TeV Abstract: Inclusive J/$\\psi$ production is studied in Xe-Xe interactions at a\ncentre-of-mass energy per nucleon pair of $\\sqrt{s_{\\rm NN}}= 5.44$ TeV, using\nthe ALICE detector at the CERN LHC. The J/$\\psi$ meson is reconstructed via its\ndecay into a muon pair, in the centre-of-mass rapidity interval $2.5<y<4$ and\ndown to zero transverse momentum. In this Letter, the nuclear modification\nfactors $R_{\\rm AA}$ for inclusive J/$\\psi$, measured in the centrality range\n0-90% as well as in the centrality intervals 0-20% and 20-90% are presented.\nThe $R_{\\rm AA}$ values are compared to previously published results for Pb-Pb\ncollisions at $\\sqrt{s_{\\rm NN}}= 5.02$ TeV and to the calculation of a\ntransport model. A good agreement is found between Xe-Xe and Pb-Pb results as\nwell as between data and the model. \n\n"}
{"id": "1805.04736", "contents": "Title: Constraining gluon distributions in nuclei using dijets in proton-proton\n  and proton-lead collisions at $\\sqrt{s_{_\\mathrm{NN}}} =$ 5.02 TeV Abstract: The pseudorapidity distributions of dijets as a function of their average\ntransverse momentum ($p_\\mathrm{T}^\\text{ave}$) are measured in proton-lead\n(pPb) and proton-proton (pp) collisions. The data samples were collected by the\nCMS experiment at the CERN LHC, at a nucleon-nucleon center-of-mass energy of\n5.02 TeV. A significant modification of the pPb spectra with respect to the pp\nspectra is observed in all $p_\\mathrm{T}^\\text{ave}$ intervals investigated.\nThe ratios of the pPb and pp distributions are compared to next-to-leading\norder perturbative quantum chromodynamics calculations with unbound nucleon and\nnuclear parton distribution functions (PDFs). These results give the first\nevidence that the gluon PDF at large Bjorken $x$ in lead ions is strongly\nsuppressed with respect to the PDF in unbound nucleons. \n\n"}
{"id": "1805.04855", "contents": "Title: Covariance Pooling For Facial Expression Recognition Abstract: Classifying facial expressions into different categories requires capturing\nregional distortions of facial landmarks. We believe that second-order\nstatistics such as covariance is better able to capture such distortions in\nregional facial fea- tures. In this work, we explore the benefits of using a\nman- ifold network structure for covariance pooling to improve facial\nexpression recognition. In particular, we first employ such kind of manifold\nnetworks in conjunction with tradi- tional convolutional networks for spatial\npooling within in- dividual image feature maps in an end-to-end deep learning\nmanner. By doing so, we are able to achieve a recognition accuracy of 58.14% on\nthe validation set of Static Facial Expressions in the Wild (SFEW 2.0) and\n87.0% on the vali- dation set of Real-World Affective Faces (RAF) Database.\nBoth of these results are the best results we are aware of. Besides, we\nleverage covariance pooling to capture the tem- poral evolution of per-frame\nfeatures for video-based facial expression recognition. Our reported results\ndemonstrate the advantage of pooling image-set features temporally by stacking\nthe designed manifold network of covariance pool-ing on top of convolutional\nnetwork layers. \n\n"}
{"id": "1805.05569", "contents": "Title: Cross-connected Networks for Multi-task Learning of Detection and\n  Segmentation Abstract: Multi-task learning improves generalization performance by sharing knowledge\namong related tasks. Existing models are for task combinations annotated on the\nsame dataset, while there are cases where multiple datasets are available for\neach task. How to utilize knowledge of successful single-task CNNs that are\ntrained on each dataset has been explored less than multi-task learning with a\nsingle dataset. We propose a cross-connected CNN, a new architecture that\nconnects single-task CNNs through convolutional layers, which transfer useful\ninformation for the counterpart. We evaluated our proposed architecture on a\ncombination of detection and segmentation using two datasets. Experiments on\npedestrians show our CNN achieved a higher detection performance compared to\nbaseline CNNs, while maintaining high quality for segmentation. It is the first\nknown attempt to tackle multi-task learning with different training datasets\nbetween detection and segmentation. Experiments with wild birds demonstrate how\nour CNN learns general representations from limited datasets. \n\n"}
{"id": "1805.06374", "contents": "Title: Fast Retinomorphic Event Stream for Video Recognition and Reinforcement\n  Learning Abstract: Good temporal representations are crucial for video understanding, and the\nstate-of-the-art video recognition framework is based on two-stream networks.\nIn such framework, besides the regular ConvNets responsible for RGB frame\ninputs, a second network is introduced to handle the temporal representation,\nusually the optical flow (OF). However, OF or other task-oriented flow is\ncomputationally costly, and is thus typically pre-computed. Critically, this\nprevents the two-stream approach from being applied to reinforcement learning\n(RL) applications such as video game playing, where the next state depends on\ncurrent state and action choices. Inspired by the early vision systems of\nmammals and insects, we propose a fast event-driven representation (EDR) that\nmodels several major properties of early retinal circuits: (1) logarithmic\ninput response, (2) multi-timescale temporal smoothing to filter noise, and (3)\nbipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the\ndirectional information for fast speed (> 9000 fps), EDR en-ables fast\nreal-time inference/learning in video applications that require interaction\nbetween an agent and the world such as game-playing, virtual robotics, and\ndomain adaptation. In this vein, we use EDR to demonstrate performance\nimprovements over state-of-the-art reinforcement learning algorithms for Atari\ngames, something that has not been possible with pre-computed OF. Moreover,\nwith UCF-101 video action recognition experiments, we show that EDR performs\nnear state-of-the-art in accuracy while achieving a 1,500x speedup in input\nrepresentation processing, as compared to optical flow. \n\n"}
{"id": "1805.06558", "contents": "Title: Recurrent Neural Network for Learning DenseDepth and Ego-Motion from\n  Video Abstract: Learning-based, single-view depth estimation often generalizes poorly to\nunseen datasets. While learning-based, two-frame depth estimation solves this\nproblem to some extent by learning to match features across frames, it performs\npoorly at large depth where the uncertainty is high. There exists few\nlearning-based, multi-view depth estimation methods. In this paper, we present\na learning-based, multi-view dense depth map and ego-motion estimation method\nthat uses Recurrent Neural Networks (RNN). Our model is designed for 3D\nreconstruction from video where the input frames are temporally correlated. It\nis generalizable to single- or two-view dense depth estimation. Compared to\nrecent single- or two-view CNN-based depth estimation methods, our model\nleverages more views and achieves more accurate results, especially at large\ndistances. Our method produces superior results to the state-of-the-art\nlearning-based, single- or two-view depth estimation methods on both indoor and\noutdoor benchmark datasets. We also demonstrate that our method can even work\non extremely difficult sequences, such as endoscopic video, where none of the\nassumptions (static scene, constant lighting, Lambertian reflection, etc.) from\ntraditional 3D reconstruction methods hold. \n\n"}
{"id": "1805.06605", "contents": "Title: Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using\n  Generative Models Abstract: In recent years, deep neural network approaches have been widely adopted for\nmachine learning tasks, including classification. However, they were shown to\nbe vulnerable to adversarial perturbations: carefully crafted small\nperturbations can cause misclassification of legitimate images. We propose\nDefense-GAN, a new framework leveraging the expressive capability of generative\nmodels to defend deep neural networks against such attacks. Defense-GAN is\ntrained to model the distribution of unperturbed images. At inference time, it\nfinds a close output to a given image which does not contain the adversarial\nchanges. This output is then fed to the classifier. Our proposed method can be\nused with any classification model and does not modify the classifier structure\nor training procedure. It can also be used as a defense against any attack as\nit does not assume knowledge of the process for generating the adversarial\nexamples. We empirically show that Defense-GAN is consistently effective\nagainst different attack methods and improves on existing defense strategies.\nOur code has been made publicly available at\nhttps://github.com/kabkabm/defensegan \n\n"}
{"id": "1805.07258", "contents": "Title: Neural Network Compression using Transform Coding and Clustering Abstract: With the deployment of neural networks on mobile devices and the necessity of\ntransmitting neural networks over limited or expensive channels, the file size\nof the trained model was identified as bottleneck. In this paper, we propose a\ncodec for the compression of neural networks which is based on transform coding\nfor convolutional and dense layers and on clustering for biases and\nnormalizations. By using this codec, we achieve average compression factors\nbetween 7.9-9.3 while the accuracy of the compressed networks for image\nclassification decreases only by 1%-2%, respectively. \n\n"}
{"id": "1805.07290", "contents": "Title: Learning 3D Shape Completion under Weak Supervision Abstract: We address the problem of 3D shape completion from sparse and noisy point\nclouds, a fundamental problem in computer vision and robotics. Recent\napproaches are either data-driven or learning-based: Data-driven approaches\nrely on a shape model whose parameters are optimized to fit the observations;\nLearning-based approaches, in contrast, avoid the expensive optimization step\nby learning to directly predict complete shapes from incomplete observations in\na fully-supervised setting. However, full supervision is often not available in\npractice. In this work, we propose a weakly-supervised learning-based approach\nto 3D shape completion which neither requires slow optimization nor direct\nsupervision. While we also learn a shape prior on synthetic data, we amortize,\ni.e., learn, maximum likelihood fitting using deep neural networks resulting in\nefficient shape completion without sacrificing accuracy. On synthetic\nbenchmarks based on ShapeNet and ModelNet as well as on real robotics data from\nKITTI and Kinect, we demonstrate that the proposed amortized maximum likelihood\napproach is able to compete with recent fully supervised baselines and\noutperforms data-driven approaches, while requiring less supervision and being\nsignificantly faster. \n\n"}
{"id": "1805.07550", "contents": "Title: DenseImage Network: Video Spatial-Temporal Evolution Encoding and\n  Understanding Abstract: Many of the leading approaches for video understanding are data-hungry and\ntime-consuming, failing to capture the gist of spatial-temporal evolution in an\nefficient manner. The latest research shows that CNN network can reason about\nstatic relation of entities in images. To further exploit its capacity in\ndynamic evolution reasoning, we introduce a novel network module called\nDenseImage Network(DIN) with two main contributions. 1) A novel compact\nrepresentation of video which distills its significant spatial-temporal\nevolution into a matrix called DenseImage, primed for efficient video encoding.\n2) A simple yet powerful learning strategy based on DenseImage and a\ntemporal-order-preserving CNN network is proposed for video understanding,\nwhich contains a local temporal correlation constraint capturing temporal\nevolution at multiple time scales with different filter widths. Extensive\nexperiments on two recent challenging benchmarks demonstrate that our\nDenseImage Network can accurately capture the common spatial-temporal evolution\nbetween similar actions, even with enormous visual variations or different time\nscales. Moreover, we obtain the state-of-the-art results in action and gesture\nrecognition with much less time-and-memory cost, indicating its immense\npotential in video representing and understanding. \n\n"}
{"id": "1805.07857", "contents": "Title: Parallel Transport Convolution: A New Tool for Convolutional Neural\n  Networks on Manifolds Abstract: Convolution has been playing a prominent role in various applications in\nscience and engineering for many years. It is the most important operation in\nconvolutional neural networks. There has been a recent growth of interests of\nresearch in generalizing convolutions on curved domains such as manifolds and\ngraphs. However, existing approaches cannot preserve all the desirable\nproperties of Euclidean convolutions, namely compactly supported filters,\ndirectionality, transferability across different manifolds. In this paper we\ndevelop a new generalization of the convolution operation, referred to as\nparallel transport convolution (PTC), on Riemannian manifolds and their\ndiscrete counterparts. PTC is designed based on the parallel transportation\nwhich is able to translate information along a manifold and to intrinsically\npreserve directionality. PTC allows for the construction of compactly supported\nfilters and is also robust to manifold deformations. This enables us to preform\nwavelet-like operations and to define deep convolutional neural networks on\ncurved domains. \n\n"}
{"id": "1805.07888", "contents": "Title: DeepPhys: Video-Based Physiological Measurement Using Convolutional\n  Attention Networks Abstract: Non-contact video-based physiological measurement has many applications in\nhealth care and human-computer interaction. Practical applications require\nmeasurements to be accurate even in the presence of large head rotations. We\npropose the first end-to-end system for video-based measurement of heart and\nbreathing rate using a deep convolutional network. The system features a new\nmotion representation based on a skin reflection model and a new attention\nmechanism using appearance information to guide motion estimation, both of\nwhich enable robust measurement under heterogeneous lighting and major motions.\nOur approach significantly outperforms all current state-of-the-art methods on\nboth RGB and infrared video datasets. Furthermore, it allows spatial-temporal\ndistributions of physiological signals to be visualized via the attention\nmechanism. \n\n"}
{"id": "1805.08961", "contents": "Title: 3D Human Pose Estimation with Relational Networks Abstract: In this paper, we propose a novel 3D human pose estimation algorithm from a\nsingle image based on neural networks. We adopted the structure of the\nrelational networks in order to capture the relations among different body\nparts. In our method, each pair of different body parts generates features, and\nthe average of the features from all the pairs are used for 3D pose estimation.\nIn addition, we propose a dropout method that can be used in relational\nmodules, which inherently imposes robustness to the occlusions. The proposed\nnetwork achieves state-of-the-art performance for 3D pose estimation in Human\n3.6M dataset, and it effectively produces plausible results even in the\nexistence of missing joints. \n\n"}
{"id": "1805.08982", "contents": "Title: RGB-T Object Tracking:Benchmark and Baseline Abstract: RGB-Thermal (RGB-T) object tracking receives more and more attention due to\nthe strongly complementary benefits of thermal information to visible data.\nHowever, RGB-T research is limited by lacking a comprehensive evaluation\nplatform. In this paper, we propose a large-scale video benchmark dataset for\nRGB-T tracking.It has three major advantages over existing ones: 1) Its size is\nsufficiently large for large-scale performance evaluation (total frame number:\n234K, maximum frame per sequence: 8K). 2) The alignment between RGB-T sequence\npairs is highly accurate, which does not need pre- or post-processing. 3) The\nocclusion levels are annotated for occlusion-sensitive performance analysis of\ndifferent tracking algorithms.Moreover, we propose a novel graph-based approach\nto learn a robust object representation for RGB-T tracking. In particular, the\ntracked object is represented with a graph with image patches as nodes. This\ngraph including graph structure, node weights and edge weights is dynamically\nlearned in a unified ADMM (alternating direction method of multipliers)-based\noptimization framework, in which the modality weights are also incorporated for\nadaptive fusion of multiple source data.Extensive experiments on the\nlarge-scale dataset are executed to demonstrate the effectiveness of the\nproposed tracker against other state-of-the-art tracking methods. We also\nprovide new insights and potential research directions to the field of RGB-T\nobject tracking. \n\n"}
{"id": "1805.09742", "contents": "Title: Longitudinal Double-Spin Asymmetries for Dijet Production at\n  Intermediate Pseudorapidity in Polarized $pp$ Collisions at $\\sqrt{s}$ = 200\n  GeV Abstract: We present the first measurements of the longitudinal double-spin asymmetry\n$A_{LL}$ for dijets with at least one jet reconstructed within the\npseudorapidity range $0.8 < \\eta < 1.8$. The dijets were measured in polarized\n$pp$ collisions at a center-of-mass energy $\\sqrt{s}$ = 200 GeV. Values for\n$A_{LL}$ are determined for several distinct event topologies, defined by the\njet pseudorapidities, and span a range of parton momentum fraction $x$ down to\n$x \\sim$ 0.01. The measured asymmetries are found to be consistent with the\npredictions of global analyses that incorporate the results of previous RHIC\nmeasurements. They will provide new constraints on $\\Delta g(x)$ in this poorly\nconstrained region when included in future global analyses. \n\n"}
{"id": "1805.10241", "contents": "Title: SLSDeep: Skin Lesion Segmentation Based on Dilated Residual and Pyramid\n  Pooling Networks Abstract: Skin lesion segmentation (SLS) in dermoscopic images is a crucial task for\nautomated diagnosis of melanoma. In this paper, we present a robust deep\nlearning SLS model, so-called SLSDeep, which is represented as an\nencoder-decoder network. The encoder network is constructed by dilated residual\nlayers, in turn, a pyramid pooling network followed by three convolution layers\nis used for the decoder. Unlike the traditional methods employing a\ncross-entropy loss, we investigated a loss function by combining both Negative\nLog Likelihood (NLL) and End Point Error (EPE) to accurately segment the\nmelanoma regions with sharp boundaries. The robustness of the proposed model\nwas evaluated on two public databases: ISBI 2016 and 2017 for skin lesion\nanalysis towards melanoma detection challenge. The proposed model outperforms\nthe state-of-the-art methods in terms of segmentation accuracy. Moreover, it is\ncapable to segment more than $100$ images of size 384x384 per second on a\nrecent GPU. \n\n"}
{"id": "1805.10548", "contents": "Title: Deep Watershed Detector for Music Object Recognition Abstract: Optical Music Recognition (OMR) is an important and challenging area within\nmusic information retrieval, the accurate detection of music symbols in digital\nimages is a core functionality of any OMR pipeline. In this paper, we introduce\na novel object detection method, based on synthetic energy maps and the\nwatershed transform, called Deep Watershed Detector (DWD). Our method is\nspecifically tailored to deal with high resolution images that contain a large\nnumber of very small objects and is therefore able to process full pages of\nwritten music. We present state-of-the-art detection results of common music\nsymbols and show DWD's ability to work with synthetic scores equally well as on\nhandwritten music. \n\n"}
{"id": "1805.10558", "contents": "Title: DPW-SDNet: Dual Pixel-Wavelet Domain Deep CNNs for Soft Decoding of\n  JPEG-Compressed Images Abstract: JPEG is one of the widely used lossy compression methods. JPEG-compressed\nimages usually suffer from compression artifacts including blocking and\nblurring, especially at low bit-rates. Soft decoding is an effective solution\nto improve the quality of compressed images without changing codec or\nintroducing extra coding bits. Inspired by the excellent performance of the\ndeep convolutional neural networks (CNNs) on both low-level and high-level\ncomputer vision problems, we develop a dual pixel-wavelet domain deep\nCNNs-based soft decoding network for JPEG-compressed images, namely DPW-SDNet.\nThe pixel domain deep network takes the four downsampled versions of the\ncompressed image to form a 4-channel input and outputs a pixel domain\nprediction, while the wavelet domain deep network uses the 1-level discrete\nwavelet transformation (DWT) coefficients to form a 4-channel input to produce\na DWT domain prediction. The pixel domain and wavelet domain estimates are\ncombined to generate the final soft decoded result. Experimental results\ndemonstrate the superiority of the proposed DPW-SDNet over several\nstate-of-the-art compression artifacts reduction algorithms. \n\n"}
{"id": "1805.11090", "contents": "Title: GenAttack: Practical Black-box Attacks with Gradient-Free Optimization Abstract: Deep neural networks are vulnerable to adversarial examples, even in the\nblack-box setting, where the attacker is restricted solely to query access.\nExisting black-box approaches to generating adversarial examples typically\nrequire a significant number of queries, either for training a substitute\nnetwork or performing gradient estimation. We introduce GenAttack, a\ngradient-free optimization technique that uses genetic algorithms for\nsynthesizing adversarial examples in the black-box setting. Our experiments on\ndifferent datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can\nsuccessfully generate visually imperceptible adversarial examples against\nstate-of-the-art image recognition models with orders of magnitude fewer\nqueries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack\nrequired roughly 2,126 and 2,568 times fewer queries respectively, than ZOO,\nthe prior state-of-the-art black-box attack. In order to scale up the attack to\nlarge-scale high-dimensional ImageNet models, we perform a series of\noptimizations that further improve the query efficiency of our attack leading\nto 237 times fewer queries against the Inception-v3 model than ZOO.\nFurthermore, we show that GenAttack can successfully attack some\nstate-of-the-art ImageNet defenses, including ensemble adversarial training and\nnon-differentiable or randomized input transformations. Our results suggest\nthat evolutionary algorithms open up a promising area of research into\neffective black-box attacks. \n\n"}
{"id": "1805.11592", "contents": "Title: Playing hard exploration games by watching YouTube Abstract: Deep reinforcement learning methods traditionally struggle with tasks where\nenvironment rewards are particularly sparse. One successful method of guiding\nexploration in these domains is to imitate trajectories provided by a human\ndemonstrator. However, these demonstrations are typically collected under\nartificial conditions, i.e. with access to the agent's exact environment setup\nand the demonstrator's action and reward trajectories. Here we propose a\ntwo-stage method that overcomes these limitations by relying on noisy,\nunaligned footage without access to such data. First, we learn to map unaligned\nvideos from multiple sources to a common representation using self-supervised\nobjectives constructed over both time and modality (i.e. vision and sound).\nSecond, we embed a single YouTube video in this representation to construct a\nreward function that encourages an agent to imitate human gameplay. This method\nof one-shot imitation allows our agent to convincingly exceed human-level\nperformance on the infamously hard exploration games Montezuma's Revenge,\nPitfall! and Private Eye for the first time, even if the agent is not presented\nwith any environment rewards. \n\n"}
{"id": "1805.11724", "contents": "Title: Rethinking Knowledge Graph Propagation for Zero-Shot Learning Abstract: Graph convolutional neural networks have recently shown great potential for\nthe task of zero-shot learning. These models are highly sample efficient as\nrelated concepts in the graph structure share statistical strength allowing\ngeneralization to new classes when faced with a lack of data. However,\nmulti-layer architectures, which are required to propagate knowledge to distant\nnodes in the graph, dilute the knowledge by performing extensive Laplacian\nsmoothing at each layer and thereby consequently decrease performance. In order\nto still enjoy the benefit brought by the graph structure while preventing\ndilution of knowledge from distant nodes, we propose a Dense Graph Propagation\n(DGP) module with carefully designed direct links among distant nodes. DGP\nallows us to exploit the hierarchical graph structure of the knowledge graph\nthrough additional connections. These connections are added based on a node's\nrelationship to its ancestors and descendants. A weighting scheme is further\nused to weigh their contribution depending on the distance to the node to\nimprove information propagation in the graph. Combined with finetuning of the\nrepresentations in a two-stage training approach our method outperforms\nstate-of-the-art zero-shot learning approaches. \n\n"}
{"id": "1805.12177", "contents": "Title: Why do deep convolutional networks generalize so poorly to small image\n  transformations? Abstract: Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to\nsmall image transformations: either because of the convolutional architecture\nor because they were trained using data augmentation. Recently, several authors\nhave shown that this is not the case: small translations or rescalings of the\ninput image can drastically change the network's prediction. In this paper, we\nquantify this phenomena and ask why neither the convolutional architecture nor\ndata augmentation are sufficient to achieve the desired invariance.\nSpecifically, we show that the convolutional architecture does not give\ninvariance since architectures ignore the classical sampling theorem, and data\naugmentation does not give invariance because the CNNs learn to be invariant to\ntransformations only for images that are very similar to typical images from\nthe training set. We discuss two possible solutions to this problem: (1)\nantialiasing the intermediate representations and (2) increasing data\naugmentation and show that they provide only a partial solution at best. Taken\ntogether, our results indicate that the problem of insuring invariance to small\nimage transformations in neural networks while preserving high accuracy remains\nunsolved. \n\n"}
{"id": "1805.12254", "contents": "Title: Multi-level 3D CNN for Learning Multi-scale Spatial Features Abstract: 3D object recognition accuracy can be improved by learning the multi-scale\nspatial features from 3D spatial geometric representations of objects such as\npoint clouds, 3D models, surfaces, and RGB-D data. Current deep learning\napproaches learn such features either using structured data representations\n(voxel grids and octrees) or from unstructured representations (graphs and\npoint clouds). Learning features from such structured representations is\nlimited by the restriction on resolution and tree depth while unstructured\nrepresentations creates a challenge due to non-uniformity among data samples.\nIn this paper, we propose an end-to-end multi-level learning approach on a\nmulti-level voxel grid to overcome these drawbacks. To demonstrate the utility\nof the proposed multi-level learning, we use a multi-level voxel representation\nof 3D objects to perform object recognition. The multi-level voxel\nrepresentation consists of a coarse voxel grid that contains volumetric\ninformation of the 3D object. In addition, each voxel in the coarse grid that\ncontains a portion of the object boundary is subdivided into multiple\nfine-level voxel grids. The performance of our multi-level learning algorithm\nfor object recognition is comparable to dense voxel representations while using\nsignificantly lower memory. \n\n"}
{"id": "1805.12277", "contents": "Title: Learning Factorized Representations for Open-set Domain Adaptation Abstract: Domain adaptation for visual recognition has undergone great progress in the\npast few years. Nevertheless, most existing methods work in the so-called\nclosed-set scenario, assuming that the classes depicted by the target images\nare exactly the same as those of the source domain. In this paper, we tackle\nthe more challenging, yet more realistic case of open-set domain adaptation,\nwhere new, unknown classes can be present in the target data. While, in the\nunsupervised scenario, one cannot expect to be able to identify each specific\nnew class, we aim to automatically detect which samples belong to these new\nclasses and discard them from the recognition process. To this end, we rely on\nthe intuition that the source and target samples depicting the known classes\ncan be generated by a shared subspace, whereas the target samples from unknown\nclasses come from a different, private subspace. We therefore introduce a\nframework that factorizes the data into shared and private parts, while\nencouraging the shared representation to be discriminative. Our experiments on\nstandard benchmarks evidence that our approach significantly outperforms the\nstate-of-the-art in open-set domain adaptation. \n\n"}
{"id": "1806.00102", "contents": "Title: Respond-CAM: Analyzing Deep Models for 3D Imaging Data by Visualizations Abstract: The convolutional neural network (CNN) has become a powerful tool for various\nbiomedical image analysis tasks, but there is a lack of visual explanation for\nthe machinery of CNNs. In this paper, we present a novel algorithm,\nRespond-weighted Class Activation Mapping (Respond-CAM), for making CNN-based\nmodels interpretable by visualizing input regions that are important for\npredictions, especially for biomedical 3D imaging data inputs. Our method uses\nthe gradients of any target concept (e.g. the score of target class) that flows\ninto a convolutional layer. The weighted feature maps are combined to produce a\nheatmap that highlights the important regions in the image for predicting the\ntarget concept. We prove a preferable sum-to-score property of the Respond-CAM\nand verify its significant improvement on 3D images from the current\nstate-of-the-art approach. Our tests on Cellular Electron Cryo-Tomography 3D\nimages show that Respond-CAM achieves superior performance on visualizing the\nCNNs with 3D biomedical images inputs, and is able to get reasonably good\nresults on visualizing the CNNs with natural image inputs. The Respond-CAM is\nan efficient and reliable approach for visualizing the CNN machinery, and is\napplicable to a wide variety of CNN model families and image analysis tasks. \n\n"}
{"id": "1806.00578", "contents": "Title: SCAN: Sliding Convolutional Attention Network for Scene Text Recognition Abstract: Scene text recognition has drawn great attentions in the community of\ncomputer vision and artificial intelligence due to its challenges and wide\napplications. State-of-the-art recurrent neural networks (RNN) based models map\nan input sequence to a variable length output sequence, but are usually applied\nin a black box manner and lack of transparency for further improvement, and the\nmaintaining of the entire past hidden states prevents parallel computation in a\nsequence. In this paper, we investigate the intrinsic characteristics of text\nrecognition, and inspired by human cognition mechanisms in reading texts, we\npropose a scene text recognition method with sliding convolutional attention\nnetwork (SCAN). Similar to the eye movement during reading, the process of SCAN\ncan be viewed as an alternation between saccades and visual fixations. Compared\nto the previous recurrent models, computations over all elements of SCAN can be\nfully parallelized during training. Experimental results on several challenging\nbenchmarks, including the IIIT5k, SVT and ICDAR 2003/2013 datasets, demonstrate\nthe superiority of SCAN over state-of-the-art methods in terms of both the\nmodel interpretability and performance. \n\n"}
{"id": "1806.00593", "contents": "Title: BoxNet: Deep Learning Based Biomedical Image Segmentation Using Boxes\n  Only Annotation Abstract: In recent years, deep learning (DL) methods have become powerful tools for\nbiomedical image segmentation. However, high annotation efforts and costs are\ncommonly needed to acquire sufficient biomedical training data for DL models.\nTo alleviate the burden of manual annotation, in this paper, we propose a new\nweakly supervised DL approach for biomedical image segmentation using boxes\nonly annotation. First, we develop a method to combine graph search (GS) and DL\nto generate fine object masks from box annotation, in which DL uses box\nannotation to compute a rough segmentation for GS and then GS is applied to\nlocate the optimal object boundaries. During the mask generation process, we\ncarefully utilize information from box annotation to filter out potential\nerrors, and then use the generated masks to train an accurate DL segmentation\nnetwork. Extensive experiments on gland segmentation in histology images, lymph\nnode segmentation in ultrasound images, and fungus segmentation in electron\nmicroscopy images show that our approach attains superior performance over the\nbest known state-of-the-art weakly supervised DL method and is able to achieve\n(1) nearly the same accuracy compared to fully supervised DL methods with far\nless annotation effort, (2) significantly better results with similar\nannotation time, and (3) robust performance in various applications. \n\n"}
{"id": "1806.01013", "contents": "Title: Synthetic data generation for end-to-end thermal infrared tracking Abstract: The usage of both off-the-shelf and end-to-end trained deep networks have\nsignificantly improved performance of visual tracking on RGB videos. However,\nthe lack of large labeled datasets hampers the usage of convolutional neural\nnetworks for tracking in thermal infrared (TIR) images. Therefore, most state\nof the art methods on tracking for TIR data are still based on handcrafted\nfeatures. To address this problem, we propose to use image-to-image translation\nmodels. These models allow us to translate the abundantly available labeled RGB\ndata to synthetic TIR data. We explore both the usage of paired and unpaired\nimage translation models for this purpose. These methods provide us with a\nlarge labeled dataset of synthetic TIR sequences, on which we can train\nend-to-end optimal features for tracking. To the best of our knowledge we are\nthe first to train end-to-end features for TIR tracking. We perform extensive\nexperiments on VOT-TIR2017 dataset. We show that a network trained on a large\ndataset of synthetic TIR data obtains better performance than one trained on\nthe available real TIR data. Combining both data sources leads to further\nimprovement. In addition, when we combine the network with motion features we\noutperform the state of the art with a relative gain of over 10%, clearly\nshowing the efficiency of using synthetic data to train end-to-end TIR\ntrackers. \n\n"}
{"id": "1806.02826", "contents": "Title: Analysis of cryogenic calorimeters with light and heat read-out for\n  double beta decay searches Abstract: The suppression of spurious events in the region of interest for neutrinoless\ndouble beta decay will play a major role in next generation experiments. The\nbackground of detectors based on the technology of cryogenic calorimeters is\nexpected to be dominated by {\\alpha} particles, that could be disentangled from\ndouble beta decay signals by exploiting the difference in the emission of the\nscintillation light. CUPID-0, an array of enriched Zn$^{82}$Se scintillating\ncalorimeters, is the first large mass demonstrator of this technology. The\ndetector started data-taking in 2017 at the Laboratori Nazionali del Gran Sasso\nwith the aim of proving that dual read-out of light and heat allows for an\nefficient suppression of the {\\alpha} background. In this paper we describe the\nsoftware tools we developed for the analysis of scintillating calorimeters and\nwe demonstrate that this technology allows to reach an unprecedented background\nfor cryogenic calorimeters. \n\n"}
{"id": "1806.02888", "contents": "Title: Correspondence of Deep Neural Networks and the Brain for Visual Textures Abstract: Deep convolutional neural networks (CNNs) trained on objects and scenes have\nshown intriguing ability to predict some response properties of visual cortical\nneurons. However, the factors and computations that give rise to such ability,\nand the role of intermediate processing stages in explaining changes that\ndevelop across areas of the cortical hierarchy, are poorly understood. We\nfocused on the sensitivity to textures as a paradigmatic example, since recent\nneurophysiology experiments provide rich data pointing to texture sensitivity\nin secondary but not primary visual cortex. We developed a quantitative\napproach for selecting a subset of the neural unit population from the CNN that\nbest describes the brain neural recordings. We found that the first two layers\nof the CNN showed qualitative and quantitative correspondence to the cortical\ndata across a number of metrics. This compatibility was reduced for the\narchitecture alone rather than the learned weights, for some other related\nhierarchical models, and only mildly in the absence of a nonlinear computation\nakin to local divisive normalization. Our results show that the CNN class of\nmodel is effective for capturing changes that develop across early areas of\ncortex, and has the potential to facilitate understanding of the computations\nthat give rise to hierarchical processing in the brain. \n\n"}
{"id": "1806.03430", "contents": "Title: Efficient Optimization Algorithms for Robust Principal Component\n  Analysis and Its Variants Abstract: Robust PCA has drawn significant attention in the last decade due to its\nsuccess in numerous application domains, ranging from bio-informatics,\nstatistics, and machine learning to image and video processing in computer\nvision. Robust PCA and its variants such as sparse PCA and stable PCA can be\nformulated as optimization problems with exploitable special structures. Many\nspecialized efficient optimization methods have been proposed to solve robust\nPCA and related problems. In this paper we review existing optimization methods\nfor solving convex and nonconvex relaxations/variants of robust PCA, discuss\ntheir advantages and disadvantages, and elaborate on their convergence\nbehaviors. We also provide some insights for possible future research\ndirections including new algorithmic frameworks that might be suitable for\nimplementing on multi-processor setting to handle large-scale problems. \n\n"}
{"id": "1806.03575", "contents": "Title: Accurate Spectral Super-resolution from Single RGB Image Using\n  Multi-scale CNN Abstract: Different from traditional hyperspectral super-resolution approaches that\nfocus on improving the spatial resolution, spectral super-resolution aims at\nproducing a high-resolution hyperspectral image from the RGB observation with\nsuper-resolution in spectral domain. However, it is challenging to accurately\nreconstruct a high-dimensional continuous spectrum from three discrete\nintensity values at each pixel, since too much information is lost during the\nprocedure where the latent hyperspectral image is downsampled (e.g., with x10\nscaling factor) in spectral domain to produce an RGB observation. To address\nthis problem, we present a multi-scale deep convolutional neural network (CNN)\nto explicitly map the input RGB image into a hyperspectral image. Through\nsymmetrically downsampling and upsampling the intermediate feature maps in a\ncascading paradigm, the local and non-local image information can be jointly\nencoded for spectral representation, ultimately improving the spectral\nreconstruction accuracy. Extensive experiments on a large hyperspectral dataset\ndemonstrate the effectiveness of the proposed method. \n\n"}
{"id": "1806.03772", "contents": "Title: DOOBNet: Deep Object Occlusion Boundary Detection from an Image Abstract: Object occlusion boundary detection is a fundamental and crucial research\nproblem in computer vision. This is challenging to solve as encountering the\nextreme boundary/non-boundary class imbalance during training an object\nocclusion boundary detector. In this paper, we propose to address this class\nimbalance by up-weighting the loss contribution of false negative and false\npositive examples with our novel Attention Loss function. We also propose a\nunified end-to-end multi-task deep object occlusion boundary detection network\n(DOOBNet) by sharing convolutional features to simultaneously predict object\nboundary and occlusion orientation. DOOBNet adopts an encoder-decoder structure\nwith skip connection in order to automatically learn multi-scale and\nmulti-level features. We significantly surpass the state-of-the-art on the PIOD\ndataset (ODS F-score of .702) and the BSDS ownership dataset (ODS F-score of\n.555), as well as improving the detecting speed to as 0.037s per image on the\nPIOD dataset. \n\n"}
{"id": "1806.03981", "contents": "Title: Rethinking Radiology: An Analysis of Different Approaches to BraTS Abstract: This paper discusses the deep learning architectures currently used for\npixel-wise segmentation of primary and secondary glioblastomas and low-grade\ngliomas. We implement various models such as the popular UNet architecture and\ncompare the performance of these implementations on the BRATS dataset. This\npaper will explore the different approaches and combinations, offering an in\ndepth discussion of how they perform and how we may improve upon them using\nmore recent advancements in deep learning architectures. \n\n"}
{"id": "1806.04391", "contents": "Title: Qiniu Submission to ActivityNet Challenge 2018 Abstract: In this paper, we introduce our submissions for the tasks of trimmed activity\nrecognition (Kinetics) and trimmed event recognition (Moments in Time) for\nActivitynet Challenge 2018. In the two tasks, non-local neural networks and\ntemporal segment networks are implemented as our base models. Multi-modal cues\nsuch as RGB image, optical flow and acoustic signal have also been used in our\nmethod. We also propose new non-local-based models for further improvement on\nthe recognition accuracy. The final submissions after ensembling the models\nachieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics\nvalidation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT\nvalidation set. \n\n"}
{"id": "1806.04935", "contents": "Title: Convolutional sparse coding for capturing high speed video content Abstract: Video capture is limited by the trade-off between spatial and temporal\nresolution: when capturing videos of high temporal resolution, the spatial\nresolution decreases due to bandwidth limitations in the capture system.\nAchieving both high spatial and temporal resolution is only possible with\nhighly specialized and very expensive hardware, and even then the same basic\ntrade-off remains. The recent introduction of compressive sensing and sparse\nreconstruction techniques allows for the capture of single-shot high-speed\nvideo, by coding the temporal information in a single frame, and then\nreconstructing the full video sequence from this single coded image and a\ntrained dictionary of image patches. In this paper, we first analyze this\napproach, and find insights that help improve the quality of the reconstructed\nvideos. We then introduce a novel technique, based on convolutional sparse\ncoding (CSC), and show how it outperforms the state-of-the-art, patch-based\napproach in terms of flexibility and efficiency, due to the convolutional\nnature of its filter banks. The key idea for CSC high-speed video acquisition\nis extending the basic formulation by imposing an additional constraint in the\ntemporal dimension, which enforces sparsity of the first-order derivatives over\ntime. \n\n"}
{"id": "1806.05409", "contents": "Title: A Cryogenic Supersonic Jet Target for Electron Scattering Experiments at\n  MAGIX@MESA and MAMI Abstract: High-performance cluster-jet targets are ideally suited and applied since\nyears in hadron and laser plasma physics. Therefore, the forthcoming MAGIX\nexperiment at the future energy recovering electron accelerator MESA will use a\ncluster-jet target to perform high precision measurements on electron\nscattering experiments, i.e., determination of the proton radius. For this\npurpose, a cluster-jet target was designed, built up and set successfully into\noperation at the University of M\\\"unster considering the requirements of the\nexperimental setup of MAGIX. The details on these requirements, calculations to\ntheir realization, e.g., on the nozzle geometry and stagnation conditions of\nthe target gas, their technical implementation and the features of the target\nwhich make the target a powerful state-of-the-art target, are highlighted in\nthis publication. Furthermore, the measured and analysed jet beam\ncharacteristics from this target using a Mach Zehnder interferometer are\npresented and discussed. These are of highest interest for the final design of\nthe complete experimental setup of MAGIX. Moreover, first measurements from\ncommissioning beam times performed with the target installed at the already\nrunning MAinzer MIkrotron will be presented. \n\n"}
{"id": "1806.06778", "contents": "Title: BinGAN: Learning Compact Binary Descriptors with a Regularized GAN Abstract: In this paper, we propose a novel regularization method for Generative\nAdversarial Networks, which allows the model to learn discriminative yet\ncompact binary representations of image patches (image descriptors). We employ\nthe dimensionality reduction that takes place in the intermediate layers of the\ndiscriminator network and train binarized low-dimensional representation of the\npenultimate layer to mimic the distribution of the higher-dimensional preceding\nlayers. To achieve this, we introduce two loss terms that aim at: (i) reducing\nthe correlation between the dimensions of the binarized low-dimensional\nrepresentation of the penultimate layer i. e. maximizing joint entropy) and\n(ii) propagating the relations between the dimensions in the high-dimensional\nspace to the low-dimensional space. We evaluate the resulting binary image\ndescriptors on two challenging applications, image matching and retrieval, and\nachieve state-of-the-art results. \n\n"}
{"id": "1806.07441", "contents": "Title: Wall Stress Estimation of Cerebral Aneurysm based on Zernike\n  Convolutional Neural Networks Abstract: Convolutional neural networks (ConvNets) have demonstrated an exceptional\ncapacity to discern visual patterns from digital images and signals.\nUnfortunately, such powerful ConvNets do not generalize well to\narbitrary-shaped manifolds, where data representation does not fit into a\ntensor-like grid. Hence, many fields of science and engineering, where data\npoints possess some manifold structure, cannot enjoy the full benefits of the\nrecent advances in ConvNets. The aneurysm wall stress estimation problem\nintroduced in this paper is one of many such problems. The problem is\nwell-known to be of a paramount clinical importance, but yet, traditional\nConvNets cannot be applied due to the manifold structure of the data, neither\ndoes the state-of-the-art geometric ConvNets perform well. Motivated by this,\nwe propose a new geometric ConvNet method named ZerNet, which builds upon our\nnovel mathematical generalization of convolution and pooling operations on\nmanifolds. Our study shows that the ZerNet outperforms the other\nstate-of-the-art geometric ConvNets in terms of accuracy. \n\n"}
{"id": "1806.08205", "contents": "Title: Synaptic partner prediction from point annotations in insect brains Abstract: High-throughput electron microscopy allows recording of lar- ge stacks of\nneural tissue with sufficient resolution to extract the wiring diagram of the\nunderlying neural network. Current efforts to automate this process focus\nmainly on the segmentation of neurons. However, in order to recover a wiring\ndiagram, synaptic partners need to be identi- fied as well. This is especially\nchallenging in insect brains like Drosophila melanogaster, where one\npresynaptic site is associated with multiple post- synaptic elements. Here we\npropose a 3D U-Net architecture to directly identify pairs of voxels that are\npre- and postsynaptic to each other. To that end, we formulate the problem of\nsynaptic partner identification as a classification problem on long-range edges\nbetween voxels to encode both the presence of a synaptic pair and its\ndirection. This formulation allows us to directly learn from synaptic point\nannotations instead of more ex- pensive voxel-based synaptic cleft or vesicle\nannotations. We evaluate our method on the MICCAI 2016 CREMI challenge and\nimprove over the current state of the art, producing 3% fewer errors than the\nnext best method. \n\n"}
{"id": "1806.08756", "contents": "Title: Dense Object Nets: Learning Dense Visual Object Descriptors By and For\n  Robotic Manipulation Abstract: What is the right object representation for manipulation? We would like\nrobots to visually perceive scenes and learn an understanding of the objects in\nthem that (i) is task-agnostic and can be used as a building block for a\nvariety of manipulation tasks, (ii) is generally applicable to both rigid and\nnon-rigid objects, (iii) takes advantage of the strong priors provided by 3D\nvision, and (iv) is entirely learned from self-supervision. This is hard to\nachieve with previous methods: much recent work in grasping does not extend to\ngrasping specific objects or other tasks, whereas task-specific learning may\nrequire many trials to generalize well across object configurations or other\ntasks. In this paper we present Dense Object Nets, which build on recent\ndevelopments in self-supervised dense descriptor learning, as a consistent\nobject representation for visual understanding and manipulation. We demonstrate\nthey can be trained quickly (approximately 20 minutes) for a wide variety of\npreviously unseen and potentially non-rigid objects. We additionally present\nnovel contributions to enable multi-object descriptor learning, and show that\nby modifying our training procedure, we can either acquire descriptors which\ngeneralize across classes of objects, or descriptors that are distinct for each\nobject instance. Finally, we demonstrate the novel application of learned dense\ndescriptors to robotic manipulation. We demonstrate grasping of specific points\non an object across potentially deformed object configurations, and demonstrate\nusing class general descriptors to transfer specific grasps across objects in a\nclass. \n\n"}
{"id": "1806.09183", "contents": "Title: A Deeper Look at Power Normalizations Abstract: Power Normalizations (PN) are very useful non-linear operators in the context\nof Bag-of-Words data representations as they tackle problems such as feature\nimbalance. In this paper, we reconsider these operators in the deep learning\nsetup by introducing a novel layer that implements PN for non-linear pooling of\nfeature maps. Specifically, by using a kernel formulation, our layer combines\nthe feature vectors and their respective spatial locations in the feature maps\nproduced by the last convolutional layer of CNN. Linearization of such a kernel\nresults in a positive definite matrix capturing the second-order statistics of\nthe feature vectors, to which PN operators are applied. We study two types of\nPN functions, namely (i) MaxExp and (ii) Gamma, addressing their role and\nmeaning in the context of nonlinear pooling. We also provide a probabilistic\ninterpretation of these operators and derive their surrogates with well-behaved\ngradients for end-to-end CNN learning. We apply our theory to practice by\nimplementing the PN layer on a ResNet-50 model and showcase experiments on four\nbenchmarks for fine-grained recognition, scene recognition, and material\nclassification. Our results demonstrate state-of-the-art performance across all\nthese tasks. \n\n"}
{"id": "1806.10354", "contents": "Title: Learn-to-Score: Efficient 3D Scene Exploration by Predicting View\n  Utility Abstract: Camera equipped drones are nowadays being used to explore large scenes and\nreconstruct detailed 3D maps. When free space in the scene is approximately\nknown, an offline planner can generate optimal plans to efficiently explore the\nscene. However, for exploring unknown scenes, the planner must predict and\nmaximize usefulness of where to go on the fly. Traditionally, this has been\nachieved using handcrafted utility functions. We propose to learn a better\nutility function that predicts the usefulness of future viewpoints. Our learned\nutility function is based on a 3D convolutional neural network. This network\ntakes as input a novel volumetric scene representation that implicitly captures\npreviously visited viewpoints and generalizes to new scenes. We evaluate our\nmethod on several large 3D models of urban scenes using simulated depth\ncameras. We show that our method outperforms existing utility measures in terms\nof reconstruction performance and is robust to sensor noise. \n\n"}
{"id": "1806.10417", "contents": "Title: Divergence-Free Shape Interpolation and Correspondence Abstract: We present a novel method to model and calculate deformation fields between\nshapes embedded in $\\mathbb{R}^D$. Our framework combines naturally\ninterpolating the two input shapes and calculating correspondences at the same\ntime. The key idea is to compute a divergence-free deformation field\nrepresented in a coarse-to-fine basis using the Karhunen-Lo\\`eve expansion. The\nadvantages are that there is no need to discretize the embedding space and the\ndeformation is volume-preserving. Furthermore, the optimization is done on\ndownsampled versions of the shapes but the morphing can be applied to any\nresolution without a heavy increase in complexity. We show results for shape\ncorrespondence, registration, inter- and extrapolation on the TOSCA and FAUST\ndata sets. \n\n"}
{"id": "1806.10694", "contents": "Title: Imaging individual barium atoms in solid xenon for barium tagging in\n  nEXO Abstract: The search for neutrinoless double beta decay probes the fundamental\nproperties of neutrinos, including whether or not the neutrino and antineutrino\nare distinct. Double beta detectors are large and expensive, so background\nreduction is essential for extracting the highest sensitivity. The\nidentification, or 'tagging', of the $^{136}$Ba daughter atom from double beta\ndecay of $^{136}$Xe provides a technique for eliminating backgrounds in the\nnEXO neutrinoless double beta decay experiment. The tagging scheme studied in\nthis work utilizes a cryogenic probe to trap the barium atom in solid xenon,\nwhere the barium atom is tagged via fluorescence imaging in the solid xenon\nmatrix. Here we demonstrate imaging and counting of individual atoms of barium\nin solid xenon by scanning a focused laser across a solid xenon matrix\ndeposited on a sapphire window. When the laser sits on an individual atom, the\nfluorescence persists for $\\sim$30~s before dropping abruptly to the background\nlevel, a clear confirmation of one-atom imaging. No barium fluorescence\npersists following evaporation of a barium deposit to a limit of $\\leq$0.16\\%.\nThis is the first time that single atoms have been imaged in solid noble\nelement. It establishes the basic principle of a barium tagging technique for\nnEXO. \n\n"}
{"id": "1806.10781", "contents": "Title: Accurate and efficient video de-fencing using convolutional neural\n  networks and temporal information Abstract: De-fencing is to eliminate the captured fence on an image or a video,\nproviding a clear view of the scene. It has been applied for many purposes\nincluding assisting photographers and improving the performance of computer\nvision algorithms such as object detection and recognition. However, the\nstate-of-the-art de-fencing methods have limited performance caused by the\ndifficulty of fence segmentation and also suffer from the motion of the camera\nor objects. To overcome these problems, we propose a novel method consisting of\nsegmentation using convolutional neural networks and a fast/robust recovery\nalgorithm. The segmentation algorithm using convolutional neural network\nachieves significant improvement in the accuracy of fence segmentation. The\nrecovery algorithm using optical flow produces plausible de-fenced images and\nvideos. The proposed method is experimented on both our diverse and complex\ndataset and publicly available datasets. The experimental results demonstrate\nthat the proposed method achieves the state-of-the-art performance for both\nsegmentation and content recovery. \n\n"}
{"id": "1807.00456", "contents": "Title: Evenly Cascaded Convolutional Networks Abstract: We introduce Evenly Cascaded convolutional Network (ECN), a neural network\ntaking inspiration from the cascade algorithm of wavelet analysis. ECN employs\ntwo feature streams - a low-level and high-level steam. At each layer these\nstreams interact, such that low-level features are modulated using advanced\nperspectives from the high-level stream. ECN is evenly structured through\nresizing feature map dimensions by a consistent ratio, which removes the burden\nof ad-hoc specification of feature map dimensions. ECN produces easily\ninterpretable features maps, a result whose intuition can be understood in the\ncontext of scale-space theory. We demonstrate that ECN's design facilitates the\ntraining process through providing easily trainable shortcuts. We report new\nstate-of-the-art results for small networks, without the need for additional\ntreatment such as pruning or compression - a consequence of ECN's simple\nstructure and direct training. A 6-layered ECN design with under 500k\nparameters achieves 95.24% and 78.99% accuracy on CIFAR-10 and CIFAR-100\ndatasets, respectively, outperforming the current state-of-the-art on small\nparameter networks, and a 3 million parameter ECN produces results competitive\nto the state-of-the-art. \n\n"}
{"id": "1807.00686", "contents": "Title: YH Technologies at ActivityNet Challenge 2018 Abstract: This notebook paper presents an overview and comparative analysis of our\nsystems designed for the following five tasks in ActivityNet Challenge 2018:\ntemporal action proposals, temporal action localization, dense-captioning\nevents in videos, trimmed action recognition, and spatio-temporal action\nlocalization. \n\n"}
{"id": "1807.00864", "contents": "Title: Semi-supervised Learning: Fusion of Self-supervised, Supervised\n  Learning, and Multimodal Cues for Tactical Driver Behavior Detection Abstract: In this paper, we presented a preliminary study for tactical driver behavior\ndetection from untrimmed naturalistic driving recordings. While supervised\nlearning based detection is a common approach, it suffers when labeled data is\nscarce. Manual annotation is both time-consuming and expensive. To emphasize\nthis problem, we experimented on a 104-hour real-world naturalistic driving\ndataset with a set of predefined driving behaviors annotated. There are three\nchallenges in the dataset. First, predefined driving behaviors are sparse in a\nnaturalistic driving setting. Second, the distribution of driving behaviors is\nlong-tail. Third, a huge intra-class variation is observed. To address these\nissues, recent self-supervised and supervised learning and fusion of multimodal\ncues are leveraged into our architecture design. Preliminary experiments and\ndiscussions are reported. \n\n"}
{"id": "1807.02070", "contents": "Title: The CBM Time-of-Flight system Abstract: The Compressed Baryonic Matter spectrometer (CBM) is a future fixed-target\nheavy-ion experiment located at the Facility for Anti-proton and Ion Research\n(FAIR) in Darmstadt, Germany. The key element in CBM providing hadron\nidentification at incident beam energies between 2 and 11 AGeV (for Au-nuclei)\nwill be a 120 m$^2$ large Time-of-Flight (ToF) wall composed of Multi-gap\nResistive Plate Chambers (MRPC) with a system time resolution better than 80\nps. Aiming for an interaction rate of 10 MHz for Au+Au collisions the MRPCs\nhave to cope with an incident particle flux between 0.1~kHz/cm$^2$ and\n100~kHz/cm$^2$ depending on their location. Characterized by granularity and\nrate capability the actual conceptual design of the ToF-wall foresees 6\ndifferent counter granularities and 4 different counter designs. In order to\nelaborate the final MRPC design of these counters several heavy-ion in-beam and\ncosmic tests were performed. In this contribution we present the conceptual\ndesign of the TOF wall and in particular discuss performance results of\nfull-size MRPC prototypes. \n\n"}
{"id": "1807.02247", "contents": "Title: Adversarial Learning for Fine-grained Image Search Abstract: Fine-grained image search is still a challenging problem due to the\ndifficulty in capturing subtle differences regardless of pose variations of\nobjects from fine-grained categories. In practice, a dynamic inventory with new\nfine-grained categories adds another dimension to this challenge. In this work,\nwe propose an end-to-end network, called FGGAN, that learns discriminative\nrepresentations by implicitly learning a geometric transformation from\nmulti-view images for fine-grained image search. We integrate a generative\nadversarial network (GAN) that can automatically handle complex view and pose\nvariations by converting them to a canonical view without any predefined\ntransformations. Moreover, in an open-set scenario, our network is able to\nbetter match images from unseen and unknown fine-grained categories. Extensive\nexperiments on two public datasets and a newly collected dataset have\ndemonstrated the outstanding robust performance of the proposed FGGAN in both\nclosed-set and open-set scenarios, providing as much as 10% relative\nimprovement compared to baselines. \n\n"}
{"id": "1807.02740", "contents": "Title: Data-driven Upsampling of Point Clouds Abstract: High quality upsampling of sparse 3D point clouds is critically useful for a\nwide range of geometric operations such as reconstruction, rendering, meshing,\nand analysis. In this paper, we propose a data-driven algorithm that enables an\nupsampling of 3D point clouds without the need for hard-coded rules. Our\napproach uses a deep network with Chamfer distance as the loss function,\ncapable of learning the latent features in point clouds belonging to different\nobject categories. We evaluate our algorithm across different amplification\nfactors, with upsampling learned and performed on objects belonging to the same\ncategory as well as different categories. We also explore the desirable\ncharacteristics of input point clouds as a function of the distribution of the\npoint samples. Finally, we demonstrate the performance of our algorithm in\nsingle-category training versus multi-category training scenarios. The final\nproposed model is compared against a baseline, optimization-based upsampling\nmethod. Results indicate that our algorithm is capable of generating more\nuniform and accurate upsamplings. \n\n"}
{"id": "1807.02848", "contents": "Title: Few- and many-nucleon systems with semilocal coordinate-space\n  regularized chiral two- and three-body forces Abstract: We present a complete calculation of nucleon-deuteron scattering as well as\nground and low-lying excited states of light nuclei in the mass range A=3-16 up\nthrough next-to-next-to-leading order in chiral effective field theory using\nsemilocal coordinate-space regularized two- and three-nucleon forces. It is\nshown that both of the low-energy constants entering the three-nucleon force at\nthis order can be reliably determined from the triton binding energy and the\ndifferential cross section minimum in elastic nucleon-deuteron scattering. The\ninclusion of the three-nucleon force is found to improve the agreement with the\ndata for most of the considered observables. \n\n"}
{"id": "1807.03095", "contents": "Title: Mammography Assessment using Multi-Scale Deep Classifiers Abstract: Applying deep learning methods to mammography assessment has remained a\nchallenging topic. Dense noise with sparse expressions, mega-pixel raw data\nresolution, lack of diverse examples have all been factors affecting\nperformance. The lack of pixel-level ground truths have especially limited\nsegmentation methods in pushing beyond approximately bounding regions. We\npropose a classification approach grounded in high performance tissue\nassessment as an alternative to all-in-one localization and assessment models\nthat is also capable of pinpointing the causal pixels. First, the objective of\nthe mammography assessment task is formalized in the context of local tissue\nclassifiers. Then, the accuracy of a convolutional neural net is evaluated on\nclassifying patches of tissue with suspicious findings at varying scales, where\nhighest obtained AUC is above $0.9$. The local evaluations of one such expert\ntissue classifier is used to augment the results of a heatmap regression model\nand additionally recover the exact causal regions at high resolution as a\nsaliency image suitable for clinical settings. \n\n"}
{"id": "1807.03146", "contents": "Title: Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning Abstract: This paper presents KeypointNet, an end-to-end geometric reasoning framework\nto learn an optimal set of category-specific 3D keypoints, along with their\ndetectors. Given a single image, KeypointNet extracts 3D keypoints that are\noptimized for a downstream task. We demonstrate this framework on 3D pose\nestimation by proposing a differentiable objective that seeks the optimal set\nof keypoints for recovering the relative pose between two views of an object.\nOur model discovers geometrically and semantically consistent keypoints across\nviewing angles and instances of an object category. Importantly, we find that\nour end-to-end framework using no ground-truth keypoint annotations outperforms\na fully supervised baseline using the same neural network architecture on the\ntask of pose estimation. The discovered 3D keypoints on the car, chair, and\nplane categories of ShapeNet are visualized at http://keypointnet.github.io/. \n\n"}
{"id": "1807.03149", "contents": "Title: Learning models for visual 3D localization with implicit mapping Abstract: We consider learning based methods for visual localization that do not\nrequire the construction of explicit maps in the form of point clouds or\nvoxels. The goal is to learn an implicit representation of the environment at a\nhigher, more abstract level. We propose to use a generative approach based on\nGenerative Query Networks (GQNs, Eslami et al. 2018), asking the following\nquestions: 1) Can GQN capture more complex scenes than those it was originally\ndemonstrated on? 2) Can GQN be used for localization in those scenes? To study\nthis approach we consider procedurally generated Minecraft worlds, for which we\ncan generate images of complex 3D scenes along with camera pose coordinates. We\nfirst show that GQNs, enhanced with a novel attention mechanism can capture the\nstructure of 3D scenes in Minecraft, as evidenced by their samples. We then\napply the models to the localization problem, comparing the results to a\ndiscriminative baseline, and comparing the ways each approach captures the task\nuncertainty. \n\n"}
{"id": "1807.03165", "contents": "Title: Sparse Deep Neural Network Exact Solutions Abstract: Deep neural networks (DNNs) have emerged as key enablers of machine learning.\nApplying larger DNNs to more diverse applications is an important challenge.\nThe computations performed during DNN training and inference are dominated by\noperations on the weight matrices describing the DNN. As DNNs incorporate more\nlayers and more neurons per layers, these weight matrices may be required to be\nsparse because of memory limitations. Sparse DNNs are one possible approach,\nbut the underlying theory is in the early stages of development and presents a\nnumber of challenges, including determining the accuracy of inference and\nselecting nonzero weights for training. Associative array algebra has been\ndeveloped by the big data community to combine and extend database, matrix, and\ngraph/network concepts for use in large, sparse data problems. Applying this\nmathematics to DNNs simplifies the formulation of DNN mathematics and reveals\nthat DNNs are linear over oscillating semirings. This work uses associative\narray DNNs to construct exact solutions and corresponding perturbation models\nto the rectified linear unit (ReLU) DNN equations that can be used to construct\ntest vectors for sparse DNN implementations over various precisions. These\nsolutions can be used for DNN verification, theoretical explorations of DNN\nproperties, and a starting point for the challenge of sparse training. \n\n"}
{"id": "1807.03326", "contents": "Title: Adaptive Adversarial Attack on Scene Text Recognition Abstract: Recent studies have shown that state-of-the-art deep learning models are\nvulnerable to the inputs with small perturbations (adversarial examples). We\nobserve two critical obstacles in adversarial examples: (i) Strong adversarial\nattacks (e.g., C&W attack) require manually tuning hyper-parameters and take a\nlong time to construct an adversarial example, making it impractical to attack\nreal-time systems; (ii) Most of the studies focus on non-sequential tasks, such\nas image classification, yet only a few consider sequential tasks. In this\nwork, we speed up adversarial attacks, especially on sequential learning tasks.\nBy leveraging the uncertainty of each task, we directly learn the adaptive\nmulti-task weightings, without manually searching hyper-parameters. A unified\narchitecture is developed and evaluated for both non-sequential tasks and\nsequential ones. To validate the effectiveness, we take the scene text\nrecognition task as a case study. To our best knowledge, our proposed method is\nthe first attempt to adversarial attack for scene text recognition. Adaptive\nAttack achieves over 99.9\\% success rate with 3-6X speedup compared to\nstate-of-the-art adversarial attacks. \n\n"}
{"id": "1807.03513", "contents": "Title: Automatic trajectory recognition in Active Target Time Projection\n  Chambers data by means of hierarchical clustering Abstract: The automatic reconstruction of three-dimensional particle tracks from Active\nTarget Time Projection Chambers data can be a challenging task, especially in\nthe presence of noise. In this article, we propose a non-parametric algorithm\nthat is based on the idea of clustering point triplets instead of the original\npoints. We define an appropriate distance measure on point triplets and then\napply a single-link hierarchical clustering on the triplets. Compared to\nparametric approaches like RANSAC or the Hough transform, the new algorithm has\nthe advantage of potentially finding trajectories even of shapes that are not\nknown beforehand. This feature is particularly important in low-energy nuclear\nphysics experiments with Active Targets operating inside a magnetic field. The\nalgorithm has been validated using data from experiments performed with the\nActive Target Time Projection Chamber developed at the National Superconducting\nCyclotron Laboratory (NSCL).The results demonstrate the capability of the\nalgorithm to identify and isolate particle tracks that describe non-analytical\ntrajectories. For curved tracks, the vertex detection recall was 86\\% and the\nprecision 94\\%. For straight tracks, the vertex detection recall was 96\\% and\nthe precision 98\\%. In the case of a test set containing only straight linear\ntracks, the algorithm performed better than an iterative Hough transform. \n\n"}
{"id": "1807.03959", "contents": "Title: Deep attention-based classification network for robust depth prediction Abstract: In this paper, we present our deep attention-based classification (DABC)\nnetwork for robust single image depth prediction, in the context of the Robust\nVision Challenge 2018 (ROB 2018). Unlike conventional depth prediction, our\ngoal is to design a model that can perform well in both indoor and outdoor\nscenes with a single parameter set. However, robust depth prediction suffers\nfrom two challenging problems: a) How to extract more discriminative features\nfor different scenes (compared to a single scene)? b) How to handle the large\ndifferences of depth ranges between indoor and outdoor datasets? To address\nthese two problems, we first formulate depth prediction as a multi-class\nclassification task and apply a softmax classifier to classify the depth label\nof each pixel. We then introduce a global pooling layer and a channel-wise\nattention mechanism to adaptively select the discriminative channels of\nfeatures and to update the original features by assigning important channels\nwith higher weights. Further, to reduce the influence of quantization errors,\nwe employ a soft-weighted sum inference strategy for the final prediction.\nExperimental results on both indoor and outdoor datasets demonstrate the\neffectiveness of our method. It is worth mentioning that we won the 2-nd place\nin single image depth prediction entry of ROB 2018, in conjunction with IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) 2018. \n\n"}
{"id": "1807.04355", "contents": "Title: Deepwound: Automated Postoperative Wound Assessment and Surgical Site\n  Surveillance through Convolutional Neural Networks Abstract: Postoperative wound complications are a significant cause of expense for\nhospitals, doctors, and patients. Hence, an effective method to diagnose the\nonset of wound complications is strongly desired. Algorithmically classifying\nwound images is a difficult task due to the variability in the appearance of\nwound sites. Convolutional neural networks (CNNs), a subgroup of artificial\nneural networks that have shown great promise in analyzing visual imagery, can\nbe leveraged to categorize surgical wounds. We present a multi-label CNN\nensemble, Deepwound, trained to classify wound images using only image pixels\nand corresponding labels as inputs. Our final computational model can\naccurately identify the presence of nine labels: drainage, fibrinous exudate,\ngranulation tissue, surgical site infection, open wound, staples, steri strips,\nand sutures. Our model achieves receiver operating curve (ROC) area under curve\n(AUC) scores, sensitivity, specificity, and F1 scores superior to prior work in\nthis area. Smartphones provide a means to deliver accessible wound care due to\ntheir increasing ubiquity. Paired with deep neural networks, they offer the\ncapability to provide clinical insight to assist surgeons during postoperative\ncare. We also present a mobile application frontend to Deepwound that assists\npatients in tracking their wound and surgical recovery from the comfort of\ntheir home. \n\n"}
{"id": "1807.04812", "contents": "Title: Latent Transformations for Object View Points Synthesis Abstract: We propose a fully-convolutional conditional generative model, the latent\ntransformation neural network (LTNN), capable of view synthesis using a\nlight-weight neural network suited for real-time applications. In contrast to\nexisting conditional generative models which incorporate conditioning\ninformation via concatenation, we introduce a dedicated network component, the\nconditional transformation unit (CTU), designed to learn the latent space\ntransformations corresponding to specified target views. In addition, a\nconsistency loss term is defined to guide the network toward learning the\ndesired latent space mappings, a task-divided decoder is constructed to refine\nthe quality of generated views, and an adaptive discriminator is introduced to\nimprove the adversarial training process. The generality of the proposed\nmethodology is demonstrated on a collection of three diverse tasks: multi-view\nreconstruction on real hand depth images, view synthesis of real and synthetic\nfaces, and the rotation of rigid objects. The proposed model is shown to exceed\nstate-of-the-art results in each category while simultaneously achieving a\nreduction in the computational demand required for inference by 30% on average. \n\n"}
{"id": "1807.05119", "contents": "Title: Learning-based Natural Geometric Matching with Homography Prior Abstract: Geometric matching is a key step in computer vision tasks. Previous\nlearning-based methods for geometric matching concentrate more on improving\nalignment quality, while we argue the importance of naturalness issue\nsimultaneously. To deal with this, firstly, Pearson correlation is applied to\nhandle large intra-class variations of features in feature matching stage.\nThen, we parametrize homography transformation with 9 parameters in full\nconnected layer of our network, to better characterize large viewpoint\nvariations compared with affine transformation. Furthermore, a novel loss\nfunction with Gaussian weights guarantees the model accuracy and efficiency in\ntraining procedure. Finally, we provide two choices for different purposes in\ngeometric matching. When compositing homography with affine transformation, the\nalignment accuracy improves and all lines are preserved, which results in a\nmore natural transformed image. When compositing homography with non-rigid\nthin-plate-spline transformation, the alignment accuracy further improves.\nExperimental results on Proposal Flow dataset show that our method outperforms\nstate-of-the-art methods, both in terms of alignment accuracy and naturalness. \n\n"}
{"id": "1807.05162", "contents": "Title: Large-Scale Visual Speech Recognition Abstract: This work presents a scalable solution to open-vocabulary visual speech\nrecognition. To achieve this, we constructed the largest existing visual speech\nrecognition dataset, consisting of pairs of text and video clips of faces\nspeaking (3,886 hours of video). In tandem, we designed and trained an\nintegrated lipreading system, consisting of a video processing pipeline that\nmaps raw video to stable videos of lips and sequences of phonemes, a scalable\ndeep neural network that maps the lip videos to sequences of phoneme\ndistributions, and a production-level speech decoder that outputs sequences of\nwords. The proposed system achieves a word error rate (WER) of 40.9% as\nmeasured on a held-out set. In comparison, professional lipreaders achieve\neither 86.4% or 92.9% WER on the same dataset when having access to additional\ntypes of contextual information. Our approach significantly improves on other\nlipreading approaches, including variants of LipNet and of Watch, Attend, and\nSpell (WAS), which are only capable of 89.8% and 76.8% WER respectively. \n\n"}
{"id": "1807.05361", "contents": "Title: Non-local RoIs for Instance Segmentation Abstract: We introduce the concept of Non-Local RoI (NL-RoI) Block as a generic and\nflexible module that can be seamlessly adapted into different Mask R-CNN heads\nfor various tasks. Mask R-CNN treats RoIs (Regions of Interest) independently\nand performs the prediction based on individual object bounding boxes. However,\nthe correlation between objects may provide useful information for detection\nand segmentation. The proposed NL-RoI Block enables each RoI to refer to all\nother RoIs' information, and results in a simple, low-cost but effective\nmodule. Our experimental results show that generalizations with NL-RoI Blocks\ncan improve the performance of Mask R-CNN for instance segmentation on the\nRobust Vision Challenge benchmarks. \n\n"}
{"id": "1807.06081", "contents": "Title: A Dataset of Laryngeal Endoscopic Images with Comparative Study on\n  Convolution Neural Network Based Semantic Segmentation Abstract: Purpose Automated segmentation of anatomical structures in medical image\nanalysis is a prerequisite for autonomous diagnosis as well as various computer\nand robot aided interventions. Recent methods based on deep convolutional\nneural networks (CNN) have outperformed former heuristic methods. However,\nthose methods were primarily evaluated on rigid, real-world environments. In\nthis study, existing segmentation methods were evaluated for their use on a new\ndataset of transoral endoscopic exploration. Methods Four machine learning\nbased methods SegNet, UNet, ENet and ErfNet were trained with supervision on a\nnovel 7-class dataset of the human larynx. The dataset contains 536 manually\nsegmented images from two patients during laser incisions. The\nIntersection-over-Union (IoU) evaluation metric was used to measure the\naccuracy of each method. Data augmentation and network ensembling were employed\nto increase segmentation accuracy. Stochastic inference was used to show\nuncertainties of the individual models. Patient-to-patient transfer was\ninvestigated using patient-specific fine-tuning. Results In this study, a\nweighted average ensemble network of UNet and ErfNet was best suited for the\nsegmentation of laryngeal soft tissue with a mean IoU of 84.7 %. The highest\nefficiency was achieved by ENet with a mean inference time of 9.22 ms per\nimage. It is shown that 10 additional images from a new patient are sufficient\nfor patient-specific fine-tuning. Conclusion CNN-based methods for semantic\nsegmentation are applicable to endoscopic images of laryngeal soft tissue. The\nsegmentation can be used for active constraints or to monitor morphological\nchanges and autonomously detect pathologies. Further improvements could be\nachieved by using a larger dataset or training the models in a self-supervised\nmanner on additional unlabeled data. \n\n"}
{"id": "1807.07433", "contents": "Title: Real-Time Stereo Vision for Road Surface 3-D Reconstruction Abstract: Stereo vision techniques have been widely used in civil engineering to\nacquire 3-D road data. The two important factors of stereo vision are accuracy\nand speed. However, it is very challenging to achieve both of them\nsimultaneously and therefore the main aim of developing a stereo vision system\nis to improve the trade-off between these two factors. In this paper, we\npresent a real-time stereo vision system used for road surface 3-D\nreconstruction. The proposed system is developed from our previously published\n3-D reconstruction algorithm where the perspective view of the target image is\nfirst transformed into the reference view, which not only increases the\ndisparity accuracy but also improves the processing speed. Then, the\ncorrelation cost between each pair of blocks is computed and stored in two 3-D\ncost volumes. To adaptively aggregate the matching costs from neighbourhood\nsystems, bilateral filtering is performed on the cost volumes. This greatly\nreduces the ambiguities during stereo matching and further improves the\nprecision of the estimated disparities. Finally, the subpixel resolution is\nachieved by conducting a parabola interpolation and the subpixel disparity map\nis used to reconstruct the 3-D road surface. The proposed algorithm is\nimplemented on an NVIDIA GTX 1080 GPU for the real-time purpose. The\nexperimental results illustrate that the reconstruction accuracy is around 3\nmm. \n\n"}
{"id": "1807.07663", "contents": "Title: Automatically Designing CNN Architectures for Medical Image Segmentation Abstract: Deep neural network architectures have traditionally been designed and\nexplored with human expertise in a long-lasting trial-and-error process. This\nprocess requires huge amount of time, expertise, and resources. To address this\ntedious problem, we propose a novel algorithm to optimally find hyperparameters\nof a deep network architecture automatically. We specifically focus on\ndesigning neural architectures for medical image segmentation task. Our\nproposed method is based on a policy gradient reinforcement learning for which\nthe reward function is assigned a segmentation evaluation utility (i.e., dice\nindex). We show the efficacy of the proposed method with its low computational\ncost in comparison with the state-of-the-art medical image segmentation\nnetworks. We also present a new architecture design, a densely connected\nencoder-decoder CNN, as a strong baseline architecture to apply the proposed\nhyperparameter search algorithm. We apply the proposed algorithm to each layer\nof the baseline architectures. As an application, we train the proposed system\non cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC)\nMICCAI 2017. Starting from a baseline segmentation architecture, the resulting\nnetwork architecture obtains the state-of-the-art results in accuracy without\nperforming any trial-and-error based architecture design approaches or close\nsupervision of the hyperparameters changes. \n\n"}
{"id": "1807.07787", "contents": "Title: Upgrade of the ALICE central barrel tracking detectors: ITS and TPC Abstract: The ALICE Collaboration will undertake a major upgrade of the detector\napparatus during the second LHC Long Shutdown LS2 (2019-2020) in view of the\nRuns 3 and 4 (2021-2029). The objective of the upgrade is two-fold: i) an\nimprovement of the tracking precision and efficiency, in particular in the\nlow-momentum range; ii) an improvement of the readout capabilities of the\nexperiment, in order to fully exploit the luminosity for heavy ions envisaged\nafter LS2. The first goal will be achieved by replacing the Inner Tracking\nSystem with a new tracker, composed of seven layers of silicon pixel detectors.\nThe new tracker will be made up of about 25000 Monolithic Active Pixel Sensors\nwith fast readout, resulting in a material thickness reduced to 0.3% (inner\nlayers) - 1% (outer layers) of the radiation length and a granularity of\n$28\\times28$ $\\mu$m$^2$. The second goal will be achieved, among other\nmeasures, by replacing the readout chambers of the 90 m$^3$ Time Projection\nChamber with Micro Pattern Gaseous Detectors. In particular, the new readout\nchambers will consist of stacks of 4 Gas Electron Multiplier foils combining\ndifferent hole pitches. The upgraded detector will operate continuously without\nthe use of a triggered gating grid. It will thus be able to record all Pb--Pb\ncollisions at the LHC interaction rate of 50 kHz. \n\n"}
{"id": "1807.07803", "contents": "Title: Competition vs. Concatenation in Skip Connections of Fully Convolutional\n  Networks Abstract: Increased information sharing through short and long-range skip connections\nbetween layers in fully convolutional networks have demonstrated significant\nimprovement in performance for semantic segmentation. In this paper, we propose\nCompetitive Dense Fully Convolutional Networks (CDFNet) by introducing\ncompetitive maxout activations in place of naive feature concatenation for\ninducing competition amongst layers. Within CDFNet, we propose two\narchitectural contributions, namely competitive dense block (CDB) and\ncompetitive unpooling block (CUB) to induce competition at local and global\nscales for short and long-range skip connections respectively. This extension\nis demonstrated to boost learning of specialized sub-networks targeted at\nsegmenting specific anatomies, which in turn eases the training of complex\ntasks. We present the proof-of-concept on the challenging task of whole body\nsegmentation in the publicly available VISCERAL benchmark and demonstrate\nimproved performance over multiple learning and registration based\nstate-of-the-art methods. \n\n"}
{"id": "1807.08254", "contents": "Title: Understanding hand-object manipulation by modeling the contextual\n  relationship between actions, grasp types and object attributes Abstract: This paper proposes a novel method for understanding daily hand-object\nmanipulation by developing computer vision-based techniques. Specifically, we\nfocus on recognizing hand grasp types, object attributes and manipulation\nactions within an unified framework by exploring their contextual\nrelationships. Our hypothesis is that it is necessary to jointly model hands,\nobjects and actions in order to accurately recognize multiple tasks that are\ncorrelated to each other in hand-object manipulation. In the proposed model, we\nexplore various semantic relationships between actions, grasp types and object\nattributes, and show how the context can be used to boost the recognition of\neach component. We also explore the spatial relationship between the hand and\nobject in order to detect the manipulated object from hand in cluttered\nenvironment. Experiment results on all three recognition tasks show that our\nproposed method outperforms traditional appearance-based methods which are not\ndesigned to take into account contextual relationships involved in hand-object\nmanipulation. The visualization and generalizability study of the learned\ncontext further supports our hypothesis. \n\n"}
{"id": "1807.08368", "contents": "Title: Modeling Brain Networks with Artificial Neural Networks Abstract: In this study, we propose a neural network approach to capture the functional\nconnectivities among anatomic brain regions. The suggested approach estimates a\nset of brain networks, each of which represents the connectivity patterns of a\ncognitive process. We employ two different architectures of neural networks to\nextract directed and undirected brain networks from functional Magnetic\nResonance Imaging (fMRI) data. Then, we use the edge weights of the estimated\nbrain networks to train a classifier, namely, Support Vector Machines(SVM) to\nlabel the underlying cognitive process. We compare our brain network models\nwith popular models, which generate similar functional brain networks. We\nobserve that both undirected and directed brain networks surpass the\nperformances of the network models used in the fMRI literature. We also observe\nthat directed brain networks offer more discriminative features compared to the\nundirected ones for recognizing the cognitive processes. The representation\npower of the suggested brain networks are tested in a task-fMRI dataset of\nHuman Connectome Project and a Complex Problem Solving dataset. \n\n"}
{"id": "1807.08931", "contents": "Title: CReaM: Condensed Real-time Models for Depth Prediction using\n  Convolutional Neural Networks Abstract: Since the resurgence of CNNs the robotic vision community has developed a\nrange of algorithms that perform classification, semantic segmentation and\nstructure prediction (depths, normals, surface curvature) using neural\nnetworks. While some of these models achieve state-of-the art results and super\nhuman level performance, deploying these models in a time critical robotic\nenvironment remains an ongoing challenge. Real-time frameworks are of paramount\nimportance to build a robotic society where humans and robots integrate\nseamlessly. To this end, we present a novel real-time structure prediction\nframework that predicts depth at 30fps on an NVIDIA-TX2. At the time of\nwriting, this is the first piece of work to showcase such a capability on a\nmobile platform. We also demonstrate with extensive experiments that neural\nnetworks with very large model capacities can be leveraged in order to train\naccurate condensed model architectures in a \"from teacher to student\" style\nknowledge transfer. \n\n"}
{"id": "1807.09659", "contents": "Title: A Surprising Linear Relationship Predicts Test Performance in Deep\n  Networks Abstract: Given two networks with the same training loss on a dataset, when would they\nhave drastically different test losses and errors? Better understanding of this\nquestion of generalization may improve practical applications of deep networks.\nIn this paper we show that with cross-entropy loss it is surprisingly simple to\ninduce significantly different generalization performances for two networks\nthat have the same architecture, the same meta parameters and the same training\nerror: one can either pretrain the networks with different levels of\n\"corrupted\" data or simply initialize the networks with weights of different\nGaussian standard deviations. A corollary of recent theoretical results on\noverfitting shows that these effects are due to an intrinsic problem of\nmeasuring test performance with a cross-entropy/exponential-type loss, which\ncan be decomposed into two components both minimized by SGD -- one of which is\nnot related to expected classification performance. However, if we factor out\nthis component of the loss, a linear relationship emerges between training and\ntest losses. Under this transformation, classical generalization bounds are\nsurprisingly tight: the empirical/training loss is very close to the\nexpected/test loss. Furthermore, the empirical relation between classification\nerror and normalized cross-entropy loss seem to be approximately monotonic \n\n"}
{"id": "1807.10413", "contents": "Title: Adapting control policies from simulation to reality using a pairwise\n  loss Abstract: This paper proposes an approach to domain transfer based on a pairwise loss\nfunction that helps transfer control policies learned in simulation onto a real\nrobot. We explore the idea in the context of a 'category level' manipulation\ntask where a control policy is learned that enables a robot to perform a mating\ntask involving novel objects. We explore the case where depth images are used\nas the main form of sensor input. Our experimental results demonstrate that\nproposed method consistently outperforms baseline methods that train only in\nsimulation or that combine real and simulated data in a naive way. \n\n"}
{"id": "1807.10889", "contents": "Title: Pairwise Body-Part Attention for Recognizing Human-Object Interactions Abstract: In human-object interactions (HOI) recognition, conventional methods consider\nthe human body as a whole and pay a uniform attention to the entire body\nregion. They ignore the fact that normally, human interacts with an object by\nusing some parts of the body. In this paper, we argue that different body parts\nshould be paid with different attention in HOI recognition, and the\ncorrelations between different body parts should be further considered. This is\nbecause our body parts always work collaboratively. We propose a new pairwise\nbody-part attention model which can learn to focus on crucial parts, and their\ncorrelations for HOI recognition. A novel attention based feature selection\nmethod and a feature representation scheme that can capture pairwise\ncorrelations between body parts are introduced in the model. Our proposed\napproach achieved 4% improvement over the state-of-the-art results in HOI\nrecognition on the HICO dataset. We will make our model and source codes\npublicly available. \n\n"}
{"id": "1807.11079", "contents": "Title: ReenactGAN: Learning to Reenact Faces via Boundary Transfer Abstract: We present a novel learning-based framework for face reenactment. The\nproposed method, known as ReenactGAN, is capable of transferring facial\nmovements and expressions from monocular video input of an arbitrary person to\na target person. Instead of performing a direct transfer in the pixel space,\nwhich could result in structural artifacts, we first map the source face onto a\nboundary latent space. A transformer is subsequently used to adapt the boundary\nof source face to the boundary of target face. Finally, a target-specific\ndecoder is used to generate the reenacted target face. Thanks to the effective\nand reliable boundary-based transfer, our method can perform photo-realistic\nface reenactment. In addition, ReenactGAN is appealing in that the whole\nreenactment process is purely feed-forward, and thus the reenactment process\ncan run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model will be\npublicly available at\nhttps://wywu.github.io/projects/ReenactGAN/ReenactGAN.html \n\n"}
{"id": "1807.11089", "contents": "Title: Towards Automatic Speech Identification from Vocal Tract Shape Dynamics\n  in Real-time MRI Abstract: Vocal tract configurations play a vital role in generating distinguishable\nspeech sounds, by modulating the airflow and creating different resonant\ncavities in speech production. They contain abundant information that can be\nutilized to better understand the underlying speech production mechanism. As a\nstep towards automatic mapping of vocal tract shape geometry to acoustics, this\npaper employs effective video action recognition techniques, like Long-term\nRecurrent Convolutional Networks (LRCN) models, to identify different\nvowel-consonant-vowel (VCV) sequences from dynamic shaping of the vocal tract.\nSuch a model typically combines a CNN based deep hierarchical visual feature\nextractor with Recurrent Networks, that ideally makes the network\nspatio-temporally deep enough to learn the sequential dynamics of a short video\nclip for video classification tasks. We use a database consisting of 2D\nreal-time MRI of vocal tract shaping during VCV utterances by 17 speakers. The\ncomparative performances of this class of algorithms under various parameter\nsettings and for various classification tasks are discussed. Interestingly, the\nresults show a marked difference in the model performance in the context of\nspeech classification with respect to generic sequence or video classification\ntasks. \n\n"}
{"id": "1807.11122", "contents": "Title: Story Understanding in Video Advertisements Abstract: In order to resonate with the viewers, many video advertisements explore\ncreative narrative techniques such as \"Freytag's pyramid\" where a story begins\nwith exposition, followed by rising action, then climax, concluding with\ndenouement. In the dramatic structure of ads in particular, climax depends on\nchanges in sentiment. We dedicate our study to understand the dynamic structure\nof video ads automatically. To achieve this, we first crowdsource climax\nannotations on 1,149 videos from the Video Ads Dataset, which already provides\nsentiment annotations. We then use both unsupervised and supervised methods to\npredict the climax. Based on the predicted peak, the low-level visual and audio\ncues, and semantically meaningful context features, we build a sentiment\nprediction model that outperforms the current state-of-the-art model of\nsentiment prediction in video ads by 25%. In our ablation study, we show that\nusing our context features, and modeling dynamics with an LSTM, are both\ncrucial factors for improved performance. \n\n"}
{"id": "1807.11254", "contents": "Title: Extreme Network Compression via Filter Group Approximation Abstract: In this paper we propose a novel decomposition method based on filter group\napproximation, which can significantly reduce the redundancy of deep\nconvolutional neural networks (CNNs) while maintaining the majority of feature\nrepresentation. Unlike other low-rank decomposition algorithms which operate on\nspatial or channel dimension of filters, our proposed method mainly focuses on\nexploiting the filter group structure for each layer. For several commonly used\nCNN models, including VGG and ResNet, our method can reduce over 80%\nfloating-point operations (FLOPs) with less accuracy drop than state-of-the-art\nmethods on various image classification datasets. Besides, experiments\ndemonstrate that our method is conducive to alleviating degeneracy of the\ncompressed network, which hurts the convergence and performance of the network. \n\n"}
{"id": "1807.11272", "contents": "Title: Uncertainty Quantification in CNN-Based Surface Prediction Using Shape\n  Priors Abstract: Surface reconstruction is a vital tool in a wide range of areas of medical\nimage analysis and clinical research. Despite the fact that many methods have\nproposed solutions to the reconstruction problem, most, due to their\ndeterministic nature, do not directly address the issue of quantifying\nuncertainty associated with their predictions. We remedy this by proposing a\nnovel probabilistic deep learning approach capable of simultaneous surface\nreconstruction and associated uncertainty prediction. The method incorporates\nprior shape information in the form of a principal component analysis (PCA)\nmodel. Experiments using the UK Biobank data show that our probabilistic\napproach outperforms an analogous deterministic PCA-based method in the task of\n2D organ delineation and quantifies uncertainty by formulating distributions\nover predicted surface vertex positions. \n\n"}
{"id": "1807.11332", "contents": "Title: Action Detection from a Robot-Car Perspective Abstract: We present the new Road Event and Activity Detection (READ) dataset, designed\nand created from an autonomous vehicle perspective to take action detection\nchallenges to autonomous driving. READ will give scholars in computer vision,\nsmart cars and machine learning at large the opportunity to conduct research\ninto exciting new problems such as understanding complex (road) activities,\ndiscerning the behaviour of sentient agents, and predicting both the label and\nthe location of future actions and events, with the final goal of supporting\nautonomous decision making. \n\n"}
{"id": "1807.11573", "contents": "Title: State-of-the-art and gaps for deep learning on limited training data in\n  remote sensing Abstract: Deep learning usually requires big data, with respect to both volume and\nvariety. However, most remote sensing applications only have limited training\ndata, of which a small subset is labeled. Herein, we review three\nstate-of-the-art approaches in deep learning to combat this challenge. The\nfirst topic is transfer learning, in which some aspects of one domain, e.g.,\nfeatures, are transferred to another domain. The next is unsupervised learning,\ne.g., autoencoders, which operate on unlabeled data. The last is generative\nadversarial networks, which can generate realistic looking data that can fool\nthe likes of both a deep learning network and human. The aim of this article is\nto raise awareness of this dilemma, to direct the reader to existing work and\nto highlight current gaps that need solving. \n\n"}
{"id": "1808.01219", "contents": "Title: Justification of Landau Hydrodynamic-Tube Model in Central Relativistic\n  Heavy-Ion Collisions Abstract: The statistical event-by-event analysis of inelastic interactions of 16O and\n32S nuclei in emulsion at 60 A GeV/c and 200 A GeV/c reveals the existence of\ngroups of high multiplicity events belonging to very central nuclear\ninteractions with Gaussian pseudorapidity distributions for produced particles\nas suggested by the original hydrodynamic-tube model. Characteristics of these\nevents are presented. The experimental observations are interpreted as a result\nof quark-gluon plasma formation in the course of central nuclear interactions. \n\n"}
{"id": "1808.01415", "contents": "Title: On Lipschitz Bounds of General Convolutional Neural Networks Abstract: Many convolutional neural networks (CNNs) have a feed-forward structure. In\nthis paper, a linear program that estimates the Lipschitz bound of such CNNs is\nproposed. Several CNNs, including the scattering networks, the AlexNet and the\nGoogleNet, are studied numerically and compared to the theoretical bounds.\nNext, concentration inequalities of the output distribution to a stationary\nrandom input signal expressed in terms of the Lipschitz bound are established.\nThe Lipschitz bound is further used to establish a nonlinear discriminant\nanalysis designed to measure the separation between features of different\nclasses. \n\n"}
{"id": "1808.01424", "contents": "Title: Learning to Align Images using Weak Geometric Supervision Abstract: Image alignment tasks require accurate pixel correspondences, which are\nusually recovered by matching local feature descriptors. Such descriptors are\noften derived using supervised learning on existing datasets with ground truth\ncorrespondences. However, the cost of creating such datasets is usually\nprohibitive. In this paper, we propose a new approach to align two images\nrelated by an unknown 2D homography where the local descriptor is learned from\nscratch from the images and the homography is estimated simultaneously. Our key\ninsight is that a siamese convolutional neural network can be trained jointly\nwhile iteratively updating the homography parameters by optimizing a single\nloss function. Our method is currently weakly supervised because the input\nimages need to be roughly aligned.\n  We have used this method to align images of different modalities such as RGB\nand near-infra-red (NIR) without using any prior labeled data. Images\nautomatically aligned by our method were then used to train descriptors that\ngeneralize to new images. We also evaluated our method on RGB images. On the\nHPatches benchmark, our method achieves comparable accuracy to deep local\ndescriptors that were trained offline in a supervised setting. \n\n"}
{"id": "1808.01694", "contents": "Title: Skin Lesion Diagnosis using Ensembles, Unscaled Multi-Crop Evaluation\n  and Loss Weighting Abstract: In this paper we present the methods of our submission to the ISIC 2018\nchallenge for skin lesion diagnosis (Task 3). The dataset consists of 10000\nimages with seven image-level classes to be distinguished by an automated\nalgorithm. We employ an ensemble of convolutional neural networks for this\ntask. In particular, we fine-tune pretrained state-of-the-art deep learning\nmodels such as Densenet, SENet and ResNeXt. We identify heavy class imbalance\nas a key problem for this challenge and consider multiple balancing approaches\nsuch as loss weighting and balanced batch sampling. Another important feature\nof our pipeline is the use of a vast amount of unscaled crops for evaluation.\nLast, we consider meta learning approaches for the final predictions. Our team\nplaced second at the challenge while being the best approach using only\npublicly available data. \n\n"}
{"id": "1808.03959", "contents": "Title: Open-World Stereo Video Matching with Deep RNN Abstract: Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo. \n\n"}
{"id": "1808.04859", "contents": "Title: GestureGAN for Hand Gesture-to-Gesture Translation in the Wild Abstract: Hand gesture-to-gesture translation in the wild is a challenging task since\nhand gestures can have arbitrary poses, sizes, locations and self-occlusions.\nTherefore, this task requires a high-level understanding of the mapping between\nthe input source gesture and the output target gesture. To tackle this problem,\nwe propose a novel hand Gesture Generative Adversarial Network (GestureGAN).\nGestureGAN consists of a single generator $G$ and a discriminator $D$, which\ntakes as input a conditional hand image and a target hand skeleton image.\nGestureGAN utilizes the hand skeleton information explicitly, and learns the\ngesture-to-gesture mapping through two novel losses, the color loss and the\ncycle-consistency loss. The proposed color loss handles the issue of \"channel\npollution\" while back-propagating the gradients. In addition, we present the\nFr\\'echet ResNet Distance (FRD) to evaluate the quality of generated images.\nExtensive experiments on two widely used benchmark datasets demonstrate that\nthe proposed GestureGAN achieves state-of-the-art performance on the\nunconstrained hand gesture-to-gesture translation task. Meanwhile, the\ngenerated images are in high-quality and are photo-realistic, allowing them to\nbe used as data augmentation to improve the performance of a hand gesture\nclassifier. Our model and code are available at\nhttps://github.com/Ha0Tang/GestureGAN. \n\n"}
{"id": "1808.05205", "contents": "Title: Building medical image classifiers with very limited data using\n  segmentation networks Abstract: Deep learning has shown promising results in medical image analysis, however,\nthe lack of very large annotated datasets confines its full potential. Although\ntransfer learning with ImageNet pre-trained classification models can alleviate\nthe problem, constrained image sizes and model complexities can lead to\nunnecessary increase in computational cost and decrease in performance. As many\ncommon morphological features are usually shared by different classification\ntasks of an organ, it is greatly beneficial if we can extract such features to\nimprove classification with limited samples. Therefore, inspired by the idea of\ncurriculum learning, we propose a strategy for building medical image\nclassifiers using features from segmentation networks. By using a segmentation\nnetwork pre-trained on similar data as the classification task, the machine can\nfirst learn the simpler shape and structural concepts before tackling the\nactual classification problem which usually involves more complicated concepts.\nUsing our proposed framework on a 3D three-class brain tumor type\nclassification problem, we achieved 82% accuracy on 191 testing samples with 91\ntraining samples. When applying to a 2D nine-class cardiac semantic level\nclassification problem, we achieved 86% accuracy on 263 testing samples with\n108 training samples. Comparisons with ImageNet pre-trained classifiers and\nclassifiers trained from scratch are presented. \n\n"}
{"id": "1808.07823", "contents": "Title: High frame-rate cardiac ultrasound imaging with deep learning Abstract: Cardiac ultrasound imaging requires a high frame rate in order to capture\nrapid motion. This can be achieved by multi-line acquisition (MLA), where\nseveral narrow-focused received lines are obtained from each wide-focused\ntransmitted line. This shortens the acquisition time at the expense of\nintroducing block artifacts. In this paper, we propose a data-driven\nlearning-based approach to improve the MLA image quality. We train an\nend-to-end convolutional neural network on pairs of real ultrasound cardiac\ndata, acquired through MLA and the corresponding single-line acquisition (SLA).\nThe network achieves a significant improvement in image quality for both $5-$\nand $7-$line MLA resulting in a decorrelation measure similar to that of SLA\nwhile having the frame rate of MLA. \n\n"}
{"id": "1808.07935", "contents": "Title: Deconvolutional Networks for Point-Cloud Vehicle Detection and Tracking\n  in Driving Scenarios Abstract: Vehicle detection and tracking is a core ingredient for developing autonomous\ndriving applications in urban scenarios. Recent image-based Deep Learning (DL)\ntechniques are obtaining breakthrough results in these perceptive tasks.\nHowever, DL research has not yet advanced much towards processing 3D point\nclouds from lidar range-finders. These sensors are very common in autonomous\nvehicles since, despite not providing as semantically rich information as\nimages, their performance is more robust under harsh weather conditions than\nvision sensors. In this paper we present a full vehicle detection and tracking\nsystem that works with 3D lidar information only. Our detection step uses a\nConvolutional Neural Network (CNN) that receives as input a featured\nrepresentation of the 3D information provided by a Velodyne HDL-64 sensor and\nreturns a per-point classification of whether it belongs to a vehicle or not.\nThe classified point cloud is then geometrically processed to generate\nobservations for a multi-object tracking system implemented via a number of\nMulti-Hypothesis Extended Kalman Filters (MH-EKF) that estimate the position\nand velocity of the surrounding vehicles. The system is thoroughly evaluated on\nthe KITTI tracking dataset, and we show the performance boost provided by our\nCNN-based vehicle detector over a standard geometric approach. Our lidar-based\napproach uses about a 4% of the data needed for an image-based detector with\nsimilarly competitive results. \n\n"}
{"id": "1808.09208", "contents": "Title: DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning\n  from Synthetic Depth Abstract: Articulated hand pose and shape estimation is an important problem for\nvision-based applications such as augmented reality and animation. In contrast\nto the existing methods which optimize only for joint positions, we propose a\nfully supervised deep network which learns to jointly estimate a full 3D hand\nmesh representation and pose from a single depth image. To this end, a CNN\narchitecture is employed to estimate parametric representations i.e. hand pose,\nbone scales and complex shape parameters. Then, a novel hand pose and shape\nlayer, embedded inside our deep framework, produces 3D joint positions and hand\nmesh. Lack of sufficient training data with varying hand shapes limits the\ngeneralized performance of learning based methods. Also, manually annotating\nreal data is suboptimal. Therefore, we present SynHand5M: a million-scale\nsynthetic dataset with accurate joint annotations, segmentation masks and mesh\nfiles of depth maps. Among model based learning (hybrid) methods, we show\nimproved results on our dataset and two of the public benchmarks i.e. NYU and\nICVL. Also, by employing a joint training strategy with real and synthetic\ndata, we recover 3D hand mesh and pose from real images in 3.7ms. \n\n"}
{"id": "1808.09879", "contents": "Title: PanoRoom: From the Sphere to the 3D Layout Abstract: We propose a novel FCN able to work with omnidirectional images that outputs\naccurate probability maps representing the main structure of indoor scenes,\nwhich is able to generalize on different data. Our approach handles occlusions\nand recovers complex shaped rooms more faithful to the actual shape of the real\nscenes. We outperform the state of the art not only in accuracy of the 3D\nmodels but also in speed. \n\n"}
{"id": "1808.10710", "contents": "Title: MobiBits: Multimodal Mobile Biometric Database Abstract: This paper presents a novel database comprising representations of five\ndifferent biometric characteristics, collected in a mobile, unconstrained or\nsemi-constrained setting with three different mobile devices, including\ncharacteristics previously unavailable in existing datasets, namely hand\nimages, thermal hand images, and thermal face images, all acquired with a\nmobile, off-the-shelf device. In addition to this collection of data we perform\nan extensive set of experiments providing insight on benchmark recognition\nperformance that can be achieved with these data, carried out with existing\ncommercial and academic biometric solutions. This is the first known to us\nmobile biometric database introducing samples of biometric traits such as\nthermal hand images and thermal face images. We hope that this contribution\nwill make a valuable addition to the already existing databases and enable new\nexperiments and studies in the field of mobile authentication. The MobiBits\ndatabase is made publicly available to the research community at no cost for\nnon-commercial purposes. \n\n"}
{"id": "1809.00101", "contents": "Title: Attentive Crowd Flow Machines Abstract: Traffic flow prediction is crucial for urban traffic management and public\nsafety. Its key challenges lie in how to adaptively integrate the various\nfactors that affect the flow changes. In this paper, we propose a unified\nneural network module to address this problem, called Attentive Crowd Flow\nMachine~(ACFM), which is able to infer the evolution of the crowd flow by\nlearning dynamic representations of temporally-varying data with an attention\nmechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units\nconnected with a convolutional layer for spatial weight prediction. The first\nLSTM takes the sequential flow density representation as input and generates a\nhidden state at each time-step for attention map inference, while the second\nLSTM aims at learning the effective spatial-temporal feature expression from\nattentionally weighted crowd flow features. Based on the ACFM, we further build\na deep architecture with the application to citywide crowd flow prediction,\nwhich naturally incorporates the sequential and periodic data as well as other\nexternal influences. Extensive experiments on two standard benchmarks (i.e.,\ncrowd flow in Beijing and New York City) show that the proposed method achieves\nsignificant improvements over the state-of-the-art methods. \n\n"}
{"id": "1809.00588", "contents": "Title: Optical Flow Super-Resolution Based on Image Guidence Using\n  Convolutional Neural Network Abstract: The convolutional neural network model for optical flow estimation usually\noutputs a low-resolution(LR) optical flow field. To obtain the corresponding\nfull image resolution,interpolation and variational approach are the most\ncommon options, which do not effectively improve the results. With the\nmotivation of various convolutional neural network(CNN) structures succeeded in\nsingle image super-resolution(SISR) task, an end-to-end convolutional neural\nnetwork is proposed to reconstruct the high resolution(HR) optical flow field\nfrom initial LR optical flow with the guidence of the first frame used in\noptical flow estimation. Our optical flow super-resolution(OFSR) problem\ndiffers from the general SISR problem in two main aspects. Firstly, the optical\nflow includes less texture information than image so that the SISR CNN\nstructures can't be directly used in our OFSR problem. Secondly, the initial LR\noptical flow data contains estimation error, while the LR image data for SISR\nis generally a bicubic downsampled, blurred, and noisy version of HR ground\ntruth. We evaluate the proposed approach on two different optical flow\nestimation mehods and show that it can not only obtain the full image\nresolution, but generate more accurate optical flow field (Accuracy improve 15%\non FlyingChairs, 13% on MPI Sintel) with sharper edges than the estimation\nresult of original method. \n\n"}
{"id": "1809.00639", "contents": "Title: Effective Field Theory in The Study of Long Range Nuclear Parity\n  Violation on Lattice Abstract: A non-zero signal $A_\\gamma^\\mathrm{np}=(-3.0\\pm1.4\\pm0.2)\\times 10^{-8}$ of\nthe gamma-ray asymmetry in the neutron-proton capture was recently reported by\nthe NPDGamma Collaboration which provides the first determination of the\n$\\Delta I=1$ parity-odd pion-nucleon coupling constant $h_\\pi^1=(2.6\\pm 1.2\\pm\n0.2)\\times 10^{-7}$. The ability to reproduce this value from first principles\nserves as a direct test of our current understanding of the interplay between\nthe strong and weak interaction at low energy. To motivate new lattice studies\nof $h_\\pi^1$, we review the current status of the theoretical understanding of\nthis coupling, which includes our recent work that relates it to a nucleon\nmass-splitting by a soft-pion theorem. We further investigate the possibility\nof calculating the mass-splitting on the lattice by providing effective field\ntheory parameterizations of all the involved quark contraction diagrams. We\nshow that the lattice calculations of the easier connected diagrams will\nprovide information of the chiral logarithms in the much harder quark loop\ndiagrams and thus help in the chiral extrapolation of the latter. \n\n"}
{"id": "1809.01579", "contents": "Title: Modelling Point Spread Function in Fluorescence Microscopy with a Sparse\n  Combination of Gaussian Mixture: Trade-off between Accuracy and Efficiency Abstract: Deblurring is a fundamental inverse problem in bioimaging. It requires\nmodelling the point spread function (PSF), which captures the optical\ndistortions entailed by the image formation process. The PSF limits the spatial\nresolution attainable for a given microscope. However, recent applications\nrequire a higher resolution, and have prompted the development of\nsuper-resolution techniques to achieve sub-pixel accuracy. This requirement\nrestricts the class of suitable PSF models to analog ones. In addition,\ndeblurring is computationally intensive, hence further requiring\ncomputationally efficient models. A custom candidate fitting both requirements\nis the Gaussian model. However, this model cannot capture the rich tail\nstructures found in both theoretical and empirical PSFs. In this paper, we aim\nat improving the reconstruction accuracy beyond the Gaussian model, while\npreserving its computational efficiency. We introduce a new class of analog PSF\nmodels based on Gaussian mixtures. The number of Gaussian kernels controls both\nthe modelling accuracy and the computational efficiency of the model: the lower\nthe number of kernels, the lower accuracy and the higher efficiency. To explore\nthe accuracy--efficiency trade-off, we propose a variational formulation of the\nPSF calibration problem, where a convex sparsity-inducing penalty on the number\nof Gaussian kernels allows trading accuracy for efficiency. We derive an\nefficient algorithm based on a fully-split formulation of alternating split\nBregman. We assess our framework on synthetic and real data and demonstrate a\nbetter reconstruction accuracy in both geometry and photometry in point source\nlocalisation---a fundamental inverse problem in fluorescence microscopy. \n\n"}
{"id": "1809.01822", "contents": "Title: Driving Experience Transfer Method for End-to-End Control of\n  Self-Driving Cars Abstract: In this paper, we present a transfer learning method for the end-to-end\ncontrol of self-driving cars, which enables a convolutional neural network\n(CNN) trained on a source domain to be utilized for the same task in a\ndifferent target domain. A conventional CNN for the end-to-end control is\ndesigned to map a single front-facing camera image to a steering command. To\nenable the transfer learning, we let the CNN produce not only a steering\ncommand but also a lane departure level (LDL) by adding a new task module,\nwhich takes the output of the last convolutional layer as input. The CNN\ntrained on the source domain, called source network, is then utilized to train\nanother task module called target network, which also takes the output of the\nlast convolutional layer of the source network and is trained to produce a\nsteering command for the target domain. The steering commands from the source\nand target network are finally merged according to the LDL and the merged\ncommand is utilized for controlling a car in the target domain. To demonstrate\nthe effectiveness of the proposed method, we utilized two simulators, TORCS and\nGTAV, for the source and the target domains, respectively. Experimental results\nshow that the proposed method outperforms other baseline methods in terms of\nstable and safe control of cars. \n\n"}
{"id": "1809.02176", "contents": "Title: Multi-Adversarial Domain Adaptation Abstract: Recent advances in deep domain adaptation reveal that adversarial learning\ncan be embedded into deep networks to learn transferable features that reduce\ndistribution discrepancy between the source and target domains. Existing domain\nadversarial adaptation methods based on single domain discriminator only align\nthe source and target data distributions without exploiting the complex\nmultimode structures. In this paper, we present a multi-adversarial domain\nadaptation (MADA) approach, which captures multimode structures to enable\nfine-grained alignment of different data distributions based on multiple domain\ndiscriminators. The adaptation can be achieved by stochastic gradient descent\nwith the gradients computed by back-propagation in linear-time. Empirical\nevidence demonstrates that the proposed model outperforms state of the art\nmethods on standard domain adaptation datasets. \n\n"}
{"id": "1809.03313", "contents": "Title: A Global Alignment Kernel based Approach for Group-level Happiness\n  Intensity Estimation Abstract: With the progress in automatic human behavior understanding, analysing the\nperceived affect of multiple people has been recieved interest in affective\ncomputing community. Unlike conventional facial expression analysis, this paper\nprimarily focuses on analysing the behaviour of multiple people in an image.\nThe proposed method is based on support vector regression with the combined\nglobal alignment kernels (GAKs) to estimate the happiness intensity of a group\nof people. We first exploit Riesz-based volume local binary pattern (RVLBP) and\ndeep convolutional neural network (CNN) based features for characterizing\nfacial images. Furthermore, we propose to use the GAK for RVLBP and deep CNN\nfeatures, respectively for explicitly measuring the similarity of two\ngroup-level images. Specifically, we exploit the global weight sort scheme to\nsort the face images from group-level image according to their spatial weights,\nmaking an efficient data structure to GAK. Lastly, we propose Multiple kernel\nlearning based on three combination strategies for combining two respective\nGAKs based on RVLBP and deep CNN features, such that enhancing the\ndiscriminative ability of each GAK. Intensive experiments are performed on the\nchallenging group-level happiness intensity database, namely HAPPEI. Our\nexperimental results demonstrate that the proposed approach achieves promising\nperformance for group happiness intensity analysis, when compared with the\nrecent state-of-the-art methods. \n\n"}
{"id": "1809.03322", "contents": "Title: Guiding the Creation of Deep Learning-based Object Detectors Abstract: Object detection is a computer vision field that has applications in several\ncontexts ranging from biomedicine and agriculture to security. In the last\nyears, several deep learning techniques have greatly improved object detection\nmodels. Among those techniques, we can highlight the YOLO approach, that allows\nthe construction of accurate models that can be employed in real-time\napplications. However, as most deep learning techniques, YOLO has a steep\nlearning curve and creating models using this approach might be challenging for\nnon-expert users. In this work, we tackle this problem by constructing a suite\nof Jupyter notebooks that democratizes the construction of object detection\nmodels using YOLO. The suitability of our approach has been proven with a\ndataset of stomata images where we have achieved a mAP of 90.91%. \n\n"}
{"id": "1809.03470", "contents": "Title: ViZDoom Competitions: Playing Doom from Pixels Abstract: This paper presents the first two editions of Visual Doom AI Competition,\nheld in 2016 and 2017. The challenge was to create bots that compete in a\nmulti-player deathmatch in a first-person shooter (FPS) game, Doom. The bots\nhad to make their decisions based solely on visual information, i.e., a raw\nscreen buffer. To play well, the bots needed to understand their surroundings,\nnavigate, explore, and handle the opponents at the same time. These aspects,\ntogether with the competitive multi-agent aspect of the game, make the\ncompetition a unique platform for evaluating the state of the art reinforcement\nlearning algorithms. The paper discusses the rules, solutions, results, and\nstatistics that give insight into the agents' behaviors. Best-performing agents\nare described in more detail. The results of the competition lead to the\nconclusion that, although reinforcement learning can produce capable Doom bots,\nthey still are not yet able to successfully compete against humans in this\ngame. The paper also revisits the ViZDoom environment, which is a flexible,\neasy to use, and efficient 3D platform for research for vision-based\nreinforcement learning, based on a well-recognized first-person perspective\ngame Doom. \n\n"}
{"id": "1809.03668", "contents": "Title: Comparing Computing Platforms for Deep Learning on a Humanoid Robot Abstract: The goal of this study is to test two different computing platforms with\nrespect to their suitability for running deep networks as part of a humanoid\nrobot software system. One of the platforms is the CPU-centered Intel NUC7i7BNH\nand the other is a NVIDIA Jetson TX2 system that puts more emphasis on GPU\nprocessing. The experiments addressed a number of benchmarking tasks including\npedestrian detection using deep neural networks. Some of the results were\nunexpected but demonstrate that platforms exhibit both advantages and\ndisadvantages when taking computational performance and electrical power\nrequirements of such a system into account. \n\n"}
{"id": "1809.04111", "contents": "Title: Investigation of Amorphous Germanium Contact Properties with Planar\n  Detectors Made from Home-Grown Germanium Crystals Abstract: The characterization of detectors fabricated from home-grown crystals is the\nmost direct way to study crystal properties. We fabricated planar detectors\nfrom high-purity germanium (HPGe) crystals grown at the University of South\nDakota (USD). In the fabrication process, a HPGe crystal slice cut from a\nUSD-grown crystal was coated with a high resistivity thin film of amorphous Ge\n(a-Ge) followed by depositing a thin layer of aluminum on top of the a-Ge film\nto define the physical area of the contacts. We investigated the detector\nperformance including the $I$-$V$ characteristics, $C$-$V$ characteristics and\nspectroscopy measurements for a few detectors. The results document the good\nquality of the USD-grown crystals and electrical contacts. \n\n"}
{"id": "1809.05175", "contents": "Title: Commissioning of the J-PET detector in view of the positron annihilation\n  lifetime spectroscopy Abstract: The Jagiellonian Positron Emission Tomograph (J-PET) is the first PET device\nbuilt from plastic scintillators. It is a multi-purpose detector designed for\nmedical imaging and for studies of properties of positronium atoms in porous\nmatter and in living organisms. In this article we report on the commissioning\nof the J-PET detector in view of studies of positronium decays. We present\nresults of analysis of the positron lifetime measured in the porous polymer.\nThe obtained results prove that J-PET is capable of performing simultaneous\nimaging of the density distribution of annihilation points as well as positron\nannihilation lifetime spectroscopy. \n\n"}
{"id": "1809.06064", "contents": "Title: Object-sensitive Deep Reinforcement Learning Abstract: Deep reinforcement learning has become popular over recent years, showing\nsuperiority on different visual-input tasks such as playing Atari games and\nrobot navigation. Although objects are important image elements, few work\nconsiders enhancing deep reinforcement learning with object characteristics. In\nthis paper, we propose a novel method that can incorporate object recognition\nprocessing to deep reinforcement learning models. This approach can be adapted\nto any existing deep reinforcement learning frameworks. State-of-the-art\nresults are shown in experiments on Atari games. We also propose a new approach\ncalled \"object saliency maps\" to visually explain the actions made by deep\nreinforcement learning agents. \n\n"}
{"id": "1809.07082", "contents": "Title: Deep Learning Based Rib Centerline Extraction and Labeling Abstract: Automated extraction and labeling of rib centerlines is a typically needed\nprerequisite for more advanced assisted reading tools that help the radiologist\nto efficiently inspect all 24 ribs in a CT volume. In this paper, we combine a\ndeep learning-based rib detection with a dedicated centerline extraction\nalgorithm applied to the detection result for the purpose of fast, robust and\naccurate rib centerline extraction and labeling from CT volumes. More\nspecifically, we first apply a fully convolutional neural network (FCNN) to\ngenerate a probability map for detecting the first rib pair, the twelfth rib\npair, and the collection of all intermediate ribs. In a second stage, a newly\ndesigned centerline extraction algorithm is applied to this multi-label\nprobability map. Finally, the distinct detection of first and twelfth rib\nseparately, allows to derive individual rib labels by simple sorting and\ncounting the detected centerlines. We applied our method to CT volumes from 116\npatients which included a variety of different challenges and achieved a\ncenterline accuracy of 0.787 mm with respect to manual centerline annotations.\n  This article is a preprint version of: Lenga M., Klinder T., B\\\"urger C., von\nBerg J., Franz A., Lorenz C. (2019) Deep Learning Based Rib Centerline\nExtraction and Labeling. In: Vrtovec T., Yao J., Zheng G., Pozo J. (eds)\nComputational Methods and Clinical Applications in Musculoskeletal Imaging.\nMSKI 2018. Lecture Notes in Computer Science, vol 11404. Springer, Cham \n\n"}
{"id": "1809.07316", "contents": "Title: Towards Large-Scale Video Video Object Mining Abstract: We propose to leverage a generic object tracker in order to perform object\nmining in large-scale unlabeled videos, captured in a realistic automotive\nsetting. We present a dataset of more than 360'000 automatically mined object\ntracks from 10+ hours of video data (560'000 frames) and propose a method for\nautomated novel category discovery and detector learning. In addition, we show\npreliminary results on using the mined tracks for object detector adaptation. \n\n"}
{"id": "1809.07759", "contents": "Title: Implementing Adaptive Separable Convolution for Video Frame\n  Interpolation Abstract: As Deep Neural Networks are becoming more popular, much of the attention is\nbeing devoted to Computer Vision problems that used to be solved with more\ntraditional approaches. Video frame interpolation is one of such challenges\nthat has seen new research involving various techniques in deep learning. In\nthis paper, we replicate the work of Niklaus et al. on Adaptive Separable\nConvolution, which claims high quality results on the video frame interpolation\ntask. We apply the same network structure trained on a smaller dataset and\nexperiment with various different loss functions, in order to determine the\noptimal approach in data-scarce scenarios. The best resulting model is still\nable to provide visually pleasing videos, although achieving lower evaluation\nscores. \n\n"}
{"id": "1809.08317", "contents": "Title: Temporal Interpolation as an Unsupervised Pretraining Task for Optical\n  Flow Estimation Abstract: The difficulty of annotating training data is a major obstacle to using CNNs\nfor low-level tasks in video. Synthetic data often does not generalize to real\nvideos, while unsupervised methods require heuristic losses. Proxy tasks can\novercome these issues, and start by training a network for a task for which\nannotation is easier or which can be trained unsupervised. The trained network\nis then fine-tuned for the original task using small amounts of ground truth\ndata. Here, we investigate frame interpolation as a proxy task for optical\nflow. Using real movies, we train a CNN unsupervised for temporal\ninterpolation. Such a network implicitly estimates motion, but cannot handle\nuntextured regions. By fine-tuning on small amounts of ground truth flow, the\nnetwork can learn to fill in homogeneous regions and compute full optical flow\nfields. Using this unsupervised pre-training, our network outperforms similar\narchitectures that were trained supervised using synthetic optical flow. \n\n"}
{"id": "1809.08402", "contents": "Title: RPNet: an End-to-End Network for Relative Camera Pose Estimation Abstract: This paper addresses the task of relative camera pose estimation from raw\nimage pixels, by means of deep neural networks. The proposed RPNet network\ntakes pairs of images as input and directly infers the relative poses, without\nthe need of camera intrinsic/extrinsic. While state-of-the-art systems based on\nSIFT + RANSAC, are able to recover the translation vector only up to scale,\nRPNet is trained to produce the full translation vector, in an end-to-end way.\nExperimental results on the Cambridge Landmark dataset show very promising\nresults regarding the recovery of the full translation vector. They also show\nthat RPNet produces more accurate and more stable results than traditional\napproaches, especially for hard images (repetitive textures, textureless\nimages, etc). To the best of our knowledge, RPNet is the first attempt to\nrecover full translation vectors in relative pose estimation. \n\n"}
{"id": "1809.09293", "contents": "Title: Covfefe: A Computer Vision Approach For Estimating Force Exertion Abstract: Cumulative exposure to repetitive and forceful activities may lead to\nmusculoskeletal injuries which not only reduce workers' efficiency and\nproductivity, but also affect their quality of life. Thus, widely accessible\ntechniques for reliable detection of unsafe muscle force exertion levels for\nhuman activity is necessary for their well-being. However, measurement of force\nexertion levels is challenging and the existing techniques pose a great\nchallenge as they are either intrusive, interfere with human-machine interface,\nand/or subjective in the nature, thus are not scalable for all workers. In this\nwork, we use face videos and the photoplethysmography (PPG) signals to classify\nforce exertion levels of 0\\%, 50\\%, and 100\\% (representing rest, moderate\neffort, and high effort), thus providing a non-intrusive and scalable approach.\nEfficient feature extraction approaches have been investigated, including\nstandard deviation of the movement of different landmarks of the face,\ndistances between peaks and troughs in the PPG signals. We note that the PPG\nsignals can be obtained from the face videos, thus giving an efficient\nclassification algorithm for the force exertion levels using face videos. Based\non the data collected from 20 subjects, features extracted from the face videos\ngive 90\\% accuracy in classification among the 100\\% and the combination of 0\\%\nand 50\\% datasets. Further combining the PPG signals provide 81.7\\% accuracy.\nThe approach is also shown to be robust to the correctly identify force level\nwhen the person is talking, even though such datasets are not included in the\ntraining. \n\n"}
{"id": "1809.09421", "contents": "Title: Multipole sensitivity to phase variation in pion photo-and\n  electroproduction analyses Abstract: We use the Athens Model Independent Analysis Scheme (AMIAS) to examine the\nvalidity of using the Fermi-Watson theorem in the multipole analyses of pion\nphotoproduction and electroproduction data. A standard practice in this field\nis to fix the multipoles' phases from $\\pi N$ scattering data, making use of\nthe Fermi - Watson theorem. However, these phases are known with limited\naccuracy and the effect of this uncertainty on the obtained multipole\nextraction has not been fully explored yet. Using AMIAS we constrain the phases\nwithin their experimentally determined uncertainty. We first analyze sets of\npseudodata of increasing statistical precision and subsequently we apply the\nmethodology for a re-analysis of the Bates/Mainz electroproduction data. It is\nfound that the uncertainty induced by the $\\pi N$ phases uncertainty to the\nextracted solutions would be significant only in the analysis of data with much\nhigher precision than the current available experimental data. \n\n"}
{"id": "1809.10122", "contents": "Title: Latest Updates from the AlCap Experiment Abstract: The AlCap experiment is a joint venture between the COMET and Mu2e\ncollaborations that will measure the rate and spectrum of particles emitted\nafter nuclear muon capture on aluminium. Both collaborations will search for\nthe charged lepton flavour violating process of neutrinoless muon-to-electron\nconversion by stopping muons in an aluminium target. Knowledge of other\nparticles emitted during this process is important. The AlCap charged particle\nemission data was collected at the Paul Scherrer Institut in Switzerland over\ntwo runs in 2013 and 2015. In this talk, the experiment will be described and\nthe current status will be presented. \n\n"}
{"id": "1809.10274", "contents": "Title: Semantically Invariant Text-to-Image Generation Abstract: Image captioning has demonstrated models that are capable of generating\nplausible text given input images or videos. Further, recent work in image\ngeneration has shown significant improvements in image quality when text is\nused as a prior. Our work ties these concepts together by creating an\narchitecture that can enable bidirectional generation of images and text. We\ncall this network Multi-Modal Vector Representation (MMVR). Along with MMVR, we\npropose two improvements to the text conditioned image generation. Firstly, a\nn-gram metric based cost function is introduced that generalizes the caption\nwith respect to the image. Secondly, multiple semantically similar sentences\nare shown to help in generating better images. Qualitative and quantitative\nevaluations demonstrate that MMVR improves upon existing text conditioned image\ngeneration results by over 20%, while integrating visual and text modalities. \n\n"}
{"id": "1809.10820", "contents": "Title: Inverse Transport Networks Abstract: We introduce inverse transport networks as a learning architecture for\ninverse rendering problems where, given input image measurements, we seek to\ninfer physical scene parameters such as shape, material, and illumination.\nDuring training, these networks are evaluated not only in terms of how close\nthey can predict groundtruth parameters, but also in terms of whether the\nparameters they produce can be used, together with physically-accurate graphics\nrenderers, to reproduce the input image measurements. To en- able training of\ninverse transport networks using stochastic gradient descent, we additionally\ncreate a general-purpose, physically-accurate differentiable renderer, which\ncan be used to estimate derivatives of images with respect to arbitrary\nphysical scene parameters. Our experiments demonstrate that inverse transport\nnetworks can be trained efficiently using differentiable rendering, and that\nthey generalize to scenes with completely unseen geometry and illumination\nbetter than networks trained without appearance- matching regularization. \n\n"}
{"id": "1810.00602", "contents": "Title: Privado: Practical and Secure DNN Inference with Enclaves Abstract: Cloud providers are extending support for trusted hardware primitives such as\nIntel SGX. Simultaneously, the field of deep learning is seeing enormous\ninnovation as well as an increase in adoption. In this paper, we ask a timely\nquestion: \"Can third-party cloud services use Intel SGX enclaves to provide\npractical, yet secure DNN Inference-as-a-service?\" We first demonstrate that\nDNN models executing inside enclaves are vulnerable to access pattern based\nattacks. We show that by simply observing access patterns, an attacker can\nclassify encrypted inputs with 97% and 71% attack accuracy for MNIST and\nCIFAR10 datasets on models trained to achieve 99% and 79% original accuracy\nrespectively. This motivates the need for PRIVADO, a system we have designed\nfor secure, easy-to-use, and performance efficient inference-as-a-service.\nPRIVADO is input-oblivious: it transforms any deep learning framework that is\nwritten in C/C++ to be free of input-dependent access patterns thus eliminating\nthe leakage. PRIVADO is fully-automated and has a low TCB: with zero developer\neffort, given an ONNX description of a model, it generates compact and\nenclave-compatible code which can be deployed on an SGX cloud platform. PRIVADO\nincurs low performance overhead: we use PRIVADO with Torch framework and show\nits overhead to be 17.18% on average on 11 different contemporary neural\nnetworks. \n\n"}
{"id": "1810.00826", "contents": "Title: How Powerful are Graph Neural Networks? Abstract: Graph Neural Networks (GNNs) are an effective framework for representation\nlearning of graphs. GNNs follow a neighborhood aggregation scheme, where the\nrepresentation vector of a node is computed by recursively aggregating and\ntransforming representation vectors of its neighboring nodes. Many GNN variants\nhave been proposed and have achieved state-of-the-art results on both node and\ngraph classification tasks. However, despite GNNs revolutionizing graph\nrepresentation learning, there is limited understanding of their\nrepresentational properties and limitations. Here, we present a theoretical\nframework for analyzing the expressive power of GNNs to capture different graph\nstructures. Our results characterize the discriminative power of popular GNN\nvariants, such as Graph Convolutional Networks and GraphSAGE, and show that\nthey cannot learn to distinguish certain simple graph structures. We then\ndevelop a simple architecture that is provably the most expressive among the\nclass of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism\ntest. We empirically validate our theoretical findings on a number of graph\nclassification benchmarks, and demonstrate that our model achieves\nstate-of-the-art performance. \n\n"}
{"id": "1810.01032", "contents": "Title: Reinforcement Learning with Perturbed Rewards Abstract: Recent studies have shown that reinforcement learning (RL) models are\nvulnerable in various noisy scenarios. For instance, the observed reward\nchannel is often subject to noise in practice (e.g., when rewards are collected\nthrough sensors), and is therefore not credible. In addition, for applications\nsuch as robotics, a deep reinforcement learning (DRL) algorithm can be\nmanipulated to produce arbitrary errors by receiving corrupted rewards. In this\npaper, we consider noisy RL problems with perturbed rewards, which can be\napproximated with a confusion matrix. We develop a robust RL framework that\nenables agents to learn in noisy environments where only perturbed rewards are\nobserved. Our solution framework builds on existing RL/DRL algorithms and\nfirstly addresses the biased noisy reward setting without any assumptions on\nthe true distribution (e.g., zero-mean Gaussian noise as made in previous\nworks). The core ideas of our solution include estimating a reward confusion\nmatrix and defining a set of unbiased surrogate rewards. We prove the\nconvergence and sample complexity of our approach. Extensive experiments on\ndifferent DRL platforms show that trained policies based on our estimated\nsurrogate reward can achieve higher expected rewards, and converge faster than\nexisting baselines. For instance, the state-of-the-art PPO algorithm is able to\nobtain 84.6% and 80.8% improvements on average score for five Atari games, with\nerror rates as 10% and 30% respectively. \n\n"}
{"id": "1810.02419", "contents": "Title: Towards High Resolution Video Generation with Progressive Growing of\n  Sliced Wasserstein GANs Abstract: The extension of image generation to video generation turns out to be a very\ndifficult task, since the temporal dimension of videos introduces an extra\nchallenge during the generation process. Besides, due to the limitation of\nmemory and training stability, the generation becomes increasingly challenging\nwith the increase of the resolution/duration of videos. In this work, we\nexploit the idea of progressive growing of Generative Adversarial Networks\n(GANs) for higher resolution video generation. In particular, we begin to\nproduce video samples of low-resolution and short-duration, and then\nprogressively increase both resolution and duration alone (or jointly) by\nadding new spatiotemporal convolutional layers to the current networks.\nStarting from the learning on a very raw-level spatial appearance and temporal\nmovement of the video distribution, the proposed progressive method learns\nspatiotemporal information incrementally to generate higher resolution videos.\nFurthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to\nimprove the distribution learning on the video data of high-dimension and\nmixed-spatiotemporal distribution. SWGAN loss replaces the distance between\njoint distributions by that of one-dimensional marginal distributions, making\nthe loss easier to compute. We evaluate the proposed model on our collected\nface video dataset of 10,900 videos to generate photorealistic face videos of\n256x256x32 resolution. In addition, our model also reaches a record inception\nscore of 14.57 in unsupervised action recognition dataset UCF-101. \n\n"}
{"id": "1810.03969", "contents": "Title: A Generative Adversarial Model for Right Ventricle Segmentation Abstract: The clinical management of several cardiovascular conditions, such as\npulmonary hypertension, require the assessment of the right ventricular (RV)\nfunction. This work addresses the fully automatic and robust access to one of\nthe key RV biomarkers, its ejection fraction, from the gold standard imaging\nmodality, MRI. The problem becomes the accurate segmentation of the RV blood\npool from cine MRI sequences. This work proposes a solution based on Fully\nConvolutional Neural Networks (FCNN), where our first contribution is the\noptimal combination of three concepts (the convolution Gated Recurrent Units\n(GRU), the Generative Adversarial Networks (GAN), and the L1 loss function)\nthat achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff\nDistance respectively with respect to the baseline FCNN. This improvement is\nthen doubled by our second contribution, the ROI-GAN, that sets two GANs to\ncooperate working at two fields of view of the image, its full resolution and\nthe region of interest (ROI). Our rationale here is to better guide the FCNN\nlearning by combining global (full resolution) and local Region Of Interest\n(ROI) features. The study is conducted in a large in-house dataset of $\\sim$\n23.000 segmented MRI slices, and its generality is verified in a publicly\navailable dataset. \n\n"}
{"id": "1810.03977", "contents": "Title: DeepImageSpam: Deep Learning based Image Spam Detection Abstract: Hackers and spammers are employing innovative and novel techniques to deceive\nnovice and even knowledgeable internet users. Image spam is one of such\ntechnique where the spammer varies and changes some portion of the image such\nthat it is indistinguishable from the original image fooling the users. This\npaper proposes a deep learning based approach for image spam detection using\nthe convolutional neural networks which uses a dataset with 810 natural images\nand 928 spam images for classification achieving an accuracy of 91.7%\noutperforming the existing image processing and machine learning techniques \n\n"}
{"id": "1810.04246", "contents": "Title: Deep clustering: On the link between discriminative models and K-means Abstract: In the context of recent deep clustering studies, discriminative models\ndominate the literature and report the most competitive performances. These\nmodels learn a deep discriminative neural network classifier in which the\nlabels are latent. Typically, they use multinomial logistic regression\nposteriors and parameter regularization, as is very common in supervised\nlearning. It is generally acknowledged that discriminative objective functions\n(e.g., those based on the mutual information or the KL divergence) are more\nflexible than generative approaches (e.g., K-means) in the sense that they make\nfewer assumptions about the data distributions and, typically, yield much\nbetter unsupervised deep learning results. On the surface, several recent\ndiscriminative models may seem unrelated to K-means. This study shows that\nthese models are, in fact, equivalent to K-means under mild conditions and\ncommon posterior models and parameter regularization. We prove that, for the\ncommonly used logistic regression posteriors, maximizing the $L_2$ regularized\nmutual information via an approximate alternating direction method (ADM) is\nequivalent to a soft and regularized K-means loss. Our theoretical analysis not\nonly connects directly several recent state-of-the-art discriminative models to\nK-means, but also leads to a new soft and regularized deep K-means algorithm,\nwhich yields competitive performance on several image clustering benchmarks. \n\n"}
{"id": "1810.04452", "contents": "Title: AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer\n  Vision Challenge 2018 Abstract: Solving problems with Artificial intelligence in a competitive manner has\nlong been absent in Bangladesh and Bengali-speaking community. On the other\nhand, there has not been a well structured database for Bengali Handwritten\ndigits for mass public use. To bring out the best minds working in machine\nlearning and use their expertise to create a model which can easily recognize\nBengali Handwritten digits, we organized Bengali.AI Computer Vision\nChallenge.The challenge saw both local and international teams participating\nwith unprecedented efforts. \n\n"}
{"id": "1810.05239", "contents": "Title: Ernest Henley's Isospin and the Ensuing Progress Abstract: Ernest Henley's contributions to understanding isospin symmetry and the\nexperimental and theoretical progress that followed are reviewed. Many\nexperimentalists and theorists worked to bring Ernest's early vision of the\nsubject to fruition. This progress and wide-ranging impact is described. \n\n"}
{"id": "1810.05952", "contents": "Title: Comparison-Based Convolutional Neural Networks for Cervical Cell/Clumps\n  Detection in the Limited Data Scenario Abstract: Automated detection of cervical cancer cells or cell clumps has the potential\nto significantly reduce error rate and increase productivity in cervical cancer\nscreening. However, most traditional methods rely on the success of accurate\ncell segmentation and discriminative hand-crafted features extraction. Recently\nthere are emerging deep learning-based methods which train convolutional neural\nnetworks (CNN) to classify image patches, but they are computationally\nexpensive. In this paper we propose an efficient CNN-based object detection\nmethods for cervical cancer cells/clumps detection. Specifically, we utilize\nthe state-of-the-art two-stage object detection method, the Faster-RCNN with\nFeature Pyramid Network (FPN) as the baseline and propose a novel comparison\ndetector to deal with the limited data problem. The key idea is that classify\nthe proposals by comparing with the reference samples of each category in\nobject detection. In addition, we propose to learn the reference samples of the\nbackground from data instead of manually choosing them by some heuristic rules.\nExperimental results show that the proposed Comparison Detector yields\nsignificant improvement on the small dataset, achieving a mean Average\nPrecision (mAP) of 26.3% and an Average Recall (AR) of 35.7%, both improving\nabout 20 points compared to the baseline. Moreover, Comparison Detector\nimproved AR by 4.6 points and achieved marginally better performance in terms\nof mAP compared with baseline model when training on the medium dataset. Our\nmethod is promising for the development of automation-assisted cervical cancer\nscreening systems. Code is available at\nhttps://github.com/kuku-sichuan/ComparisonDetector. \n\n"}
{"id": "1810.06208", "contents": "Title: Solution for Large-Scale Hierarchical Object Detection Datasets with\n  Incomplete Annotation and Data Imbalance Abstract: This report demonstrates our solution for the Open Images 2018 Challenge.\nBased on our detailed analysis on the Open Images Datasets (OID), it is found\nthat there are four typical features: large-scale, hierarchical tag system,\nsevere annotation incompleteness and data imbalance. Considering these\ncharacteristics, an amount of strategies are employed, including SNIPER, soft\nsampling, class-aware sampling (CAS), hierarchical non-maximum suppression\n(HNMS) and so on. In virtue of these effective strategies, and further using\nthe powerful SENet154 armed with feature pyramid module and deformable ROIalign\nas the backbone, our best single model could achieve a mAP of 56.9%. After a\nfurther ensemble with 9 models, the final mAP is boosted to 62.2% in the public\nleaderboard (ranked the 2nd place) and 58.6% in the private leaderboard (ranked\nthe 3rd place, slightly inferior to the 1st place by only 0.04 point). \n\n"}
{"id": "1810.06282", "contents": "Title: Feature Representation Analysis of Deep Convolutional Neural Network\n  using Two-stage Feature Transfer -An Application for Diffuse Lung Disease\n  Classification- Abstract: Transfer learning is a machine learning technique designed to improve\ngeneralization performance by using pre-trained parameters obtained from other\nlearning tasks. For image recognition tasks, many previous studies have\nreported that, when transfer learning is applied to deep neural networks,\nperformance improves, despite having limited training data. This paper proposes\na two-stage feature transfer learning method focusing on the recognition of\ntextural medical images. During the proposed method, a model is successively\ntrained with massive amounts of natural images, some textural images, and the\ntarget images. We applied this method to the classification task of textural\nX-ray computed tomography images of diffuse lung diseases. In our experiment,\nthe two-stage feature transfer achieves the best performance compared to a\nfrom-scratch learning and a conventional single-stage feature transfer. We also\ninvestigated the robustness of the target dataset, based on size. Two-stage\nfeature transfer shows better robustness than the other two learning methods.\nMoreover, we analyzed the feature representations obtained from DLDs imagery\ninputs for each feature transfer models using a visualization method. We showed\nthat the two-stage feature transfer obtains both edge and textural features of\nDLDs, which does not occur in conventional single-stage feature transfer\nmodels. \n\n"}
{"id": "1810.06827", "contents": "Title: Combined Static and Motion Features for Deep-Networks Based Activity\n  Recognition in Videos Abstract: Activity recognition in videos in a deep-learning setting---or\notherwise---uses both static and pre-computed motion components. The method of\ncombining the two components, whilst keeping the burden on the deep network\nless, still remains uninvestigated. Moreover, it is not clear what the level of\ncontribution of individual components is, and how to control the contribution.\nIn this work, we use a combination of CNN-generated static features and motion\nfeatures in the form of motion tubes. We propose three schemas for combining\nstatic and motion components: based on a variance ratio, principal components,\nand Cholesky decomposition. The Cholesky decomposition based method allows the\ncontrol of contributions. The ratio given by variance analysis of static and\nmotion features match well with the experimental optimal ratio used in the\nCholesky decomposition based method. The resulting activity recognition system\nis better or on par with existing state-of-the-art when tested with three\npopular datasets. The findings also enable us to characterize a dataset with\nrespect to its richness in motion information. \n\n"}
{"id": "1810.07691", "contents": "Title: Monte Carlo Simulations of Trapped Ultracold Neutrons in the UCN{\\tau}\n  Experiment Abstract: In the UCN{\\tau} experiment, ultracold neutrons (UCN) are confined by\nmagnetic fields and the Earth's gravitational field. Field-trapping mitigates\nthe problem of UCN loss on material surfaces, which caused the largest\ncorrection in prior neutron experiments using material bottles. However, the\nneutron dynamics in field traps differ qualitatively from those in material\nbottles. In the latter case, neutrons bounce off material surfaces with\nsignificant diffusivity and the population quickly reaches a static spatial\ndistribution with a density gradient induced by the gravitational potential. In\ncontrast, the field-confined UCN -- whose dynamics can be described by\nHamiltonian mechanics -- do not exhibit the stochastic behaviors typical of an\nideal gas model as observed in material bottles. In this report, we will\ndescribe our efforts to simulate UCN trapping in the UCN{\\tau}\nmagneto-gravitational trap. We compare the simulation output to the\nexperimental results to determine the parameters of the neutron detector and\nthe input neutron distribution. The tuned model is then used to understand the\nphase space evolution of neutrons observed in the UCN{\\tau} experiment. We will\ndiscuss the implications of chaotic dynamics on controlling the systematic\neffects, such as spectral cleaning and microphonic heating, for a successful\nUCN lifetime experiment to reach a 0.01% level of precision. \n\n"}
{"id": "1810.07694", "contents": "Title: A novel experimental setup for rare events selection and its potential\n  application to super heavy elements search Abstract: The paper presents a novel instrumentation for rare events selection which\nwas tested in our research of short lived super heavy elements production and\ndetection. The instrumentation includes an active catcher multi elements system\nand dedicated electronics. The active catcher located in the forward hemisphere\nis composed of 63 scintillator detection modules. Reaction products of damped\ncollisions between heavy ion projectiles and heavy target nuclei are implanted\nin the fast plastic scintillators of the active catcher modules. The\nacquisition system trigger delivered by logical branch of the electronics\nallows to record the reaction products which decay via the alpha particle\nemissions or spontaneous fission which take place between beam bursts. One\nmicrosecond wave form signal from FADCs contains information on heavy implanted\nnucleus as well as its decays. \n\n"}
{"id": "1810.08770", "contents": "Title: Sequential Context Encoding for Duplicate Removal Abstract: Duplicate removal is a critical step to accomplish a reasonable amount of\npredictions in prevalent proposal-based object detection frameworks. Albeit\nsimple and effective, most previous algorithms utilize a greedy process without\nmaking sufficient use of properties of input data. In this work, we design a\nnew two-stage framework to effectively select the appropriate proposal\ncandidate for each object. The first stage suppresses most of easy negative\nobject proposals, while the second stage selects true positives in the reduced\nproposal set. These two stages share the same network structure, \\ie, an\nencoder and a decoder formed as recurrent neural networks (RNN) with global\nattention and context gate. The encoder scans proposal candidates in a\nsequential manner to capture the global context information, which is then fed\nto the decoder to extract optimal proposals. In our extensive experiments, the\nproposed method outperforms other alternatives by a large margin. \n\n"}
{"id": "1810.09941", "contents": "Title: Brand > Logo: Visual Analysis of Fashion Brands Abstract: While lots of people may think branding begins and ends with a logo, fashion\nbrands communicate their uniqueness through a wide range of visual cues such as\ncolor, patterns and shapes. In this work, we analyze learned visual\nrepresentations by deep networks that are trained to recognize fashion brands.\nIn particular, the activation strength and extent of neurons are studied to\nprovide interesting insights about visual brand expressions. The proposed\nmethod identifies where a brand stands in the spectrum of branding strategy,\ni.e., from trademark-emblazoned goods with bold logos to implicit no logo\nmarketing. By quantifying attention maps, we are able to interpret the visual\ncharacteristics of a brand present in a single image and model the general\ndesign direction of a brand as a whole. We further investigate versatility of\nneurons and discover \"specialists\" that are highly brand-specific and\n\"generalists\" that detect diverse visual features. A human experiment based on\nthree main visual scenarios of fashion brands is conducted to verify the\nalignment of our quantitative measures with the human perception of brands.\nThis paper demonstrate how deep networks go beyond logos in order to recognize\nclothing brands in an image. \n\n"}
{"id": "1810.10725", "contents": "Title: Convolutional Deblurring for Natural Imaging Abstract: In this paper, we propose a novel design of image deblurring in the form of\none-shot convolution filtering that can directly convolve with naturally\nblurred images for restoration. The problem of optical blurring is a common\ndisadvantage to many imaging applications that suffer from optical\nimperfections. Despite numerous deconvolution methods that blindly estimate\nblurring in either inclusive or exclusive forms, they are practically\nchallenging due to high computational cost and low image reconstruction\nquality. Both conditions of high accuracy and high speed are prerequisites for\nhigh-throughput imaging platforms in digital archiving. In such platforms,\ndeblurring is required after image acquisition before being stored, previewed,\nor processed for high-level interpretation. Therefore, on-the-fly correction of\nsuch images is important to avoid possible time delays, mitigate computational\nexpenses, and increase image perception quality. We bridge this gap by\nsynthesizing a deconvolution kernel as a linear combination of Finite Impulse\nResponse (FIR) even-derivative filters that can be directly convolved with\nblurry input images to boost the frequency fall-off of the Point Spread\nFunction (PSF) associated with the optical blur. We employ a Gaussian low-pass\nfilter to decouple the image denoising problem for image edge deblurring.\nFurthermore, we propose a blind approach to estimate the PSF statistics for two\nGaussian and Laplacian models that are common in many imaging pipelines.\nThorough experiments are designed to test and validate the efficiency of the\nproposed method using 2054 naturally blurred images across six imaging\napplications and seven state-of-the-art deconvolution methods. \n\n"}
{"id": "1810.10933", "contents": "Title: Practical Shape Analysis and Segmentation Methods for Point Cloud Models Abstract: Current point cloud processing algorithms do not have the capability to\nautomatically extract semantic information from the observed scenes, except in\nvery specialized cases. Furthermore, existing mesh analysis paradigms cannot be\ndirectly employed to automatically perform typical shape analysis tasks\ndirectly on point cloud models.\n  We present a potent framework for shape analysis, similarity, and\nsegmentation of noisy point cloud models for real objects of engineering\ninterest, models that may be incomplete. The proposed framework relies on\nspectral methods and the heat diffusion kernel to construct compact shape\nsignatures, and we show that the framework supports a variety of clustering\ntechniques that have traditionally been applied only on mesh models. We\ndeveloped and implemented one practical and convergent estimate of the\nLaplace-Beltrami operator for point clouds as well as a number of clustering\ntechniques adapted to work directly on point clouds to produce geometric\nfeatures of engineering interest. The key advantage of this framework is that\nit supports practical shape analysis capabilities that operate directly on\npoint cloud models of objects without requiring surface reconstruction or\nglobal meshing. We show that the proposed technique is robust against typical\nnoise present in possibly incomplete point clouds, and segment point clouds\nscanned by depth cameras (e.g. Kinect) into semantically-meaningful sub-shapes. \n\n"}
{"id": "1810.10941", "contents": "Title: Alzheimer's Disease Diagnosis Based on Cognitive Methods in Virtual\n  Environments and Emotions Analysis Abstract: Dementia is a syndrome characterised by the decline of different cognitive\nabilities. Alzheimer's Disease (AD) is the most common dementia affecting\ncognitive domains such as memory and learning, perceptual-motion or executive\nfunction. High rate of deaths and high cost for detection, treatments and\npatient's care count amongst its consequences. Early detection of AD is\nconsidered of high importance for improving the quality of life of patients and\ntheir families. The aim of this thesis is to introduce novel non-invasive early\ndiagnosis methods in order to speed the diagnosis, reduce the associated costs\nand make them widely accessible. Novel AD's screening tests based on virtual\nenvironments using new immersive technologies combined with advanced Human\nComputer Interaction (HCI) systems are introduced. Four tests demonstrate the\nwide range of screening mechanisms based on cognitive domain impairments that\ncan be designed using virtual environments. The use of emotion recognition to\nanalyse AD symptoms has been also proposed. A novel multimodal dataset was\nspecifically created to remark the autobiographical memory deficits of AD\npatients. Data from this dataset is used to introduce novel descriptors for\nElectroencephalogram (EEG) and facial images data. EEG features are based on\nquaternions in order to keep the correlation information between sensors,\nwhereas, for facial expression recognition, a preprocessing method for motion\nmagnification and descriptors based on origami crease pattern algorithm are\nproposed to enhance facial micro-expressions. These features have been proved\non classifiers such as SVM and Adaboost for the classification of reactions to\nautobiographical stimuli such as long and short term memories. \n\n"}
{"id": "1810.12241", "contents": "Title: Few-shot 3D Multi-modal Medical Image Segmentation using Generative\n  Adversarial Learning Abstract: We address the problem of segmenting 3D multi-modal medical images in\nscenarios where very few labeled examples are available for training.\nLeveraging the recent success of adversarial learning for semi-supervised\nsegmentation, we propose a novel method based on Generative Adversarial\nNetworks (GANs) to train a segmentation model with both labeled and unlabeled\nimages. The proposed method prevents over-fitting by learning to discriminate\nbetween true and fake patches obtained by a generator network. Our work extends\ncurrent adversarial learning approaches, which focus on 2D single-modality\nimages, to the more challenging context of 3D volumes of multiple modalities.\nThe proposed method is evaluated on the problem of segmenting brain MRI from\nthe iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement\nis reported, compared to state-of-art segmentation networks trained in a\nfully-supervised manner. In addition, our work presents a comprehensive\nanalysis of different GAN architectures for semi-supervised segmentation,\nshowing recent techniques like feature matching to yield a higher performance\nthan conventional adversarial training approaches. Our code is publicly\navailable at https://github.com/arnab39/FewShot_GAN-Unet3D \n\n"}
{"id": "1810.13292", "contents": "Title: Anomaly Detection With Multiple-Hypotheses Predictions Abstract: In one-class-learning tasks, only the normal case (foreground) can be modeled\nwith data, whereas the variation of all possible anomalies is too erratic to be\ndescribed by samples. Thus, due to the lack of representative data, the\nwide-spread discriminative approaches cannot cover such learning tasks, and\nrather generative models, which attempt to learn the input density of the\nforeground, are used. However, generative models suffer from a large input\ndimensionality (as in images) and are typically inefficient learners. We\npropose to learn the data distribution of the foreground more efficiently with\na multi-hypotheses autoencoder. Moreover, the model is criticized by a\ndiscriminator, which prevents artificial data modes not supported by data, and\nenforces diversity across hypotheses. Our multiple-hypothesesbased anomaly\ndetection framework allows the reliable identification of out-of-distribution\nsamples. For anomaly detection on CIFAR-10, it yields up to 3.9% points\nimprovement over previously reported results. On a real anomaly detection task,\nthe approach reduces the error of the baseline models from 6.8% to 1.5%. \n\n"}
{"id": "1811.00751", "contents": "Title: Show, Attend and Read: A Simple and Strong Baseline for Irregular Text\n  Recognition Abstract: Recognizing irregular text in natural scene images is challenging due to the\nlarge variance in text appearance, such as curvature, orientation and\ndistortion. Most existing approaches rely heavily on sophisticated model\ndesigns and/or extra fine-grained annotations, which, to some extent, increase\nthe difficulty in algorithm implementation and data collection. In this work,\nwe propose an easy-to-implement strong baseline for irregular scene text\nrecognition, using off-the-shelf neural network components and only word-level\nannotations. It is composed of a $31$-layer ResNet, an LSTM-based\nencoder-decoder framework and a 2-dimensional attention module. Despite its\nsimplicity, the proposed method is robust and achieves state-of-the-art\nperformance on both regular and irregular scene text recognition benchmarks.\nCode is available at: https://tinyurl.com/ShowAttendRead \n\n"}
{"id": "1811.01859", "contents": "Title: Experimental investigation of the low molecular weight fluoropolymer for\n  the ultracold neutrons storage Abstract: The experimental setup for examining the low-molecular-weight fluoropolymer\nCF$_{3}$(CF$_{2})_{3}$-O-CF$_{2}$-O-(CF$_{2})_{3}$CF$_{3}$, which is a\npromising coating material for the walls of storage chambers for ultracold\nneutrons, is described. The results are detailed. The measurement data are\ninterpreted in the model of a multilayer complex quantum-mechanical potential\nof the chamber walls. \n\n"}
{"id": "1811.02644", "contents": "Title: DeepDPM: Dynamic Population Mapping via Deep Neural Network Abstract: Dynamic high resolution data on human population distribution is of great\nimportance for a wide spectrum of activities and real-life applications, but is\ntoo difficult and expensive to obtain directly. Therefore, generating\nfine-scaled population distributions from coarse population data is of great\nsignificance. However, there are three major challenges: 1) the complexity in\nspatial relations between high and low resolution population; 2) the dependence\nof population distributions on other external information; 3) the difficulty in\nretrieving temporal distribution patterns. In this paper, we first propose the\nidea to generate dynamic population distributions in full-time series, then we\ndesign dynamic population mapping via deep neural network(DeepDPM), a model\nthat describes both spatial and temporal patterns using coarse data and point\nof interest information. In DeepDPM, we utilize super-resolution convolutional\nneural network(SRCNN) based model to directly map coarse data into higher\nresolution data, and a time-embedded long short-term memory model to\neffectively capture the periodicity nature to smooth the finer-scaled results\nfrom the previous static SRCNN model. We perform extensive experiments on a\nreal-life mobile dataset collected from Shanghai. Our results demonstrate that\nDeepDPM outperforms previous state-of-the-art methods and a suite of frequent\ndata-mining approaches. Moreover, DeepDPM breaks through the limitation from\nprevious works in time dimension so that dynamic predictions in all-day time\nslots can be obtained. \n\n"}
{"id": "1811.03188", "contents": "Title: Solving Jigsaw Puzzles By the Graph Connection Laplacian Abstract: We propose a novel mathematical framework to address the problem of\nautomatically solving large jigsaw puzzles. This problem assumes a large image,\nwhich is cut into equal square pieces that are arbitrarily rotated and\nshuffled, and asks to recover the original image given the transformed pieces.\nThe main contribution of this work is a method for recovering the rotations of\nthe pieces when both shuffles and rotations are unknown. A major challenge of\nthis procedure is estimating the graph connection Laplacian without the\nknowledge of shuffles. A careful combination of our proposed method for\nestimating rotations with any existing method for estimating shuffles results\nin a practical solution for the jigsaw puzzle problem. Our theory guarantees,\nin a clean setting, that our basic idea of recovering rotations is robust to\nsome corruption of the connection graph. Numerical experiments demonstrate the\ncompetitive accuracy of this solution, its robustness to corruption and, its\ncomputational advantage for large puzzles. \n\n"}
{"id": "1811.03692", "contents": "Title: Mode matching in GANs through latent space learning and inversion Abstract: Generative adversarial networks (GANs) have shown remarkable success in\ngeneration of unstructured data, such as, natural images. However, discovery\nand separation of modes in the generated space, essential for several tasks\nbeyond naive data generation, is still a challenge. In this paper, we address\nthe problem of imposing desired modal properties on the generated space using a\nlatent distribution, engineered in accordance with the modal properties of the\ntrue data distribution. This is achieved by training a latent space inversion\nnetwork in tandem with the generative network using a divergence loss. The\nlatent space is made to follow a continuous multimodal distribution generated\nby reparameterization of a pair of continuous and discrete random variables. In\naddition, the modal priors of the latent distribution are learned to match with\nthe true data distribution using minimal-supervision with negligible increment\nin number of learnable parameters. We validate our method on multiple tasks\nsuch as mode separation, conditional generation, and attribute discovery on\nmultiple real world image datasets and demonstrate its efficacy over other\nstate-of-the-art methods. \n\n"}
{"id": "1811.04346", "contents": "Title: Deep Face Quality Assessment Abstract: Face image quality is an important factor in facial recognition systems as\nits verification and recognition accuracy is highly dependent on the quality of\nimage presented. Rejecting low quality images can significantly increase the\naccuracy of any facial recognition system. In this project, a simple approach\nis presented to train a deep convolutional neural network to perform end-to-end\nface image quality assessment. The work is done in 2 stages : First, generation\nof quality score label and secondly, training a deep convolutional neural\nnetwork in a supervised manner to predict quality score between 0 and 1. The\ngeneration of quality labels is done by comparing the face image with a\ntemplate of best quality images and then evaluating the normalized score based\non the similarity. \n\n"}
{"id": "1811.05013", "contents": "Title: Blindfold Baselines for Embodied QA Abstract: We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object. \n\n"}
{"id": "1811.05621", "contents": "Title: Style and Content Disentanglement in Generative Adversarial Networks Abstract: Disentangling factors of variation within data has become a very challenging\nproblem for image generation tasks. Current frameworks for training a\nGenerative Adversarial Network (GAN), learn to disentangle the representations\nof the data in an unsupervised fashion and capture the most significant factors\nof the data variations. However, these approaches ignore the principle of\ncontent and style disentanglement in image generation, which means their\nlearned latent code may alter the content and style of the generated images at\nthe same time. This paper describes the Style and Content Disentangled GAN\n(SC-GAN), a new unsupervised algorithm for training GANs that learns\ndisentangled style and content representations of the data. We assume that the\nrepresentation of an image can be decomposed into a content code that\nrepresents the geometrical information of the data, and a style code that\ncaptures textural properties. Consequently, by fixing the style portion of the\nlatent representation, we can generate diverse images in a particular style.\nReversely, we can set the content code and generate a specific scene in a\nvariety of styles. The proposed SC-GAN has two components: a content code which\nis the input to the generator, and a style code which modifies the scene style\nthrough modification of the Adaptive Instance Normalization (AdaIN) layers'\nparameters. We evaluate the proposed SC-GAN framework on a set of baseline\ndatasets. \n\n"}
{"id": "1811.06017", "contents": "Title: Performance Estimation of Synthesis Flows cross Technologies using LSTMs\n  and Transfer Learning Abstract: Due to the increasing complexity of Integrated Circuits (ICs) and\nSystem-on-Chip (SoC), developing high-quality synthesis flows within a short\nmarket time becomes more challenging. We propose a general approach that\nprecisely estimates the Quality-of-Result (QoR), such as delay and area, of\nunseen synthesis flows for specific designs. The main idea is training a\nRecurrent Neural Network (RNN) regressor, where the flows are inputs and QoRs\nare ground truth. The RNN regressor is constructed with Long Short-Term Memory\n(LSTM) and fully-connected layers. This approach is demonstrated with 1.2\nmillion data points collected using 14nm, 7nm regular-voltage (RVT), and 7nm\nlow-voltage (LVT) FinFET technologies with twelve IC designs. The accuracy of\npredicting the QoRs (delay and area) within one technology is\n$\\boldsymbol{\\geq}$\\textbf{98.0}\\% over $\\sim$240,000 test points. To enable\naccurate predictions cross different technologies and different IC designs, we\npropose a transfer-learning approach that utilizes the model pre-trained with\n14nm datasets. Our transfer learning approach obtains estimation accuracy\n$\\geq$96.3\\% over $\\sim$960,000 test points, using only 100 data points for\ntraining. \n\n"}
{"id": "1811.06498", "contents": "Title: Adjusting for Confounding in Unsupervised Latent Representations of\n  Images Abstract: Biological imaging data are often partially confounded or contain unwanted\nvariability. Examples of such phenomena include variable lighting across\nmicroscopy image captures, stain intensity variation in histological slides,\nand batch effects for high throughput drug screening assays. Therefore, to\ndevelop \"fair\" models which generalise well to unseen examples, it is crucial\nto learn data representations that are insensitive to nuisance factors of\nvariation. In this paper, we present a strategy based on adversarial training,\ncapable of learning unsupervised representations invariant to confounders. As\nan empirical validation of our method, we use deep convolutional autoencoders\nto learn unbiased cellular representations from microscopy imaging. \n\n"}
{"id": "1811.06846", "contents": "Title: Improving Fingerprint Pore Detection with a Small FCN Abstract: In this work, we investigate if previously proposed CNNs for fingerprint pore\ndetection overestimate the number of required model parameters for this task.\nWe show that this is indeed the case by proposing a fully convolutional neural\nnetwork that has significantly fewer parameters. We evaluate this model using a\nrigorous and reproducible protocol, which was, prior to our work, not available\nto the community. Using our protocol, we show that the proposed model, when\ncombined with post-processing, performs better than previous methods, albeit\nbeing much more efficient. All our code is available at\nhttps://github.com/gdahia/fingerprint-pore-detection \n\n"}
{"id": "1811.07157", "contents": "Title: Recurrent Convolutions for Causal 3D CNNs Abstract: Recently, three dimensional (3D) convolutional neural networks (CNNs) have\nemerged as dominant methods to capture spatiotemporal representations in\nvideos, by adding to pre-existing 2D CNNs a third, temporal dimension. Such 3D\nCNNs, however, are anti-causal (i.e., they exploit information from both the\npast and the future frames to produce feature representations, thus preventing\ntheir use in online settings), constrain the temporal reasoning horizon to the\nsize of the temporal convolution kernel, and are not temporal\nresolution-preserving for video sequence-to-sequence modelling, as, for\ninstance, in action detection. To address these serious limitations, here we\npresent a new 3D CNN architecture for the causal/online processing of videos.\n  Namely, we propose a novel Recurrent Convolutional Network (RCN), which\nrelies on recurrence to capture the temporal context across frames at each\nnetwork level. Our network decomposes 3D convolutions into (1) a 2D spatial\nconvolution component, and (2) an additional hidden state $1\\times 1$\nconvolution, applied across time. The hidden state at any time $t$ is assumed\nto depend on the hidden state at $t-1$ and on the current output of the spatial\nconvolution component. As a result, the proposed network: (i) produces causal\noutputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal\nresolution. Our experiments on the large-scale large Kinetics and MultiThumos\ndatasets show that the proposed method performs comparably to anti-causal 3D\nCNNs, while being causal and using fewer parameters. \n\n"}
{"id": "1811.07211", "contents": "Title: Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep\n  Learning Transferable Examples Abstract: Although deep learning has shown great success in recent years, researchers\nhave discovered a critical flaw where small, imperceptible changes in the input\nto the system can drastically change the output classification. These attacks\nare exploitable in nearly all of the existing deep learning classification\nframeworks. However, the susceptibility of deep sparse coding models to\nadversarial examples has not been examined. Here, we show that classifiers\nbased on a deep sparse coding model whose classification accuracy is\ncompetitive with a variety of deep neural network models are robust to\nadversarial examples that effectively fool those same deep learning models. We\ndemonstrate both quantitatively and qualitatively that the robustness of deep\nsparse coding models to adversarial examples arises from two key properties.\nFirst, because deep sparse coding models learn general features corresponding\nto generators of the dataset as a whole, rather than highly discriminative\nfeatures for distinguishing specific classes, the resulting classifiers are\nless dependent on idiosyncratic features that might be more easily exploited.\nSecond, because deep sparse coding models utilize fixed point attractor\ndynamics with top-down feedback, it is more difficult to find small changes to\nthe input that drive the resulting representations out of the correct attractor\nbasin. \n\n"}
{"id": "1811.07483", "contents": "Title: Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image\n  Translation with Visual Attention Abstract: Recently unpaired multi-domain image-to-image translation has attracted great\ninterests and obtained remarkable progress, where a label vector is utilized to\nindicate multi-domain information. In this paper, we propose SAT (Show, Attend\nand Translate), an unified and explainable generative adversarial network\nequipped with visual attention that can perform unpaired image-to-image\ntranslation for multiple domains. By introducing an action vector, we treat the\noriginal translation tasks as problems of arithmetic addition and subtraction.\nVisual attention is applied to guarantee that only the regions relevant to the\ntarget domains are translated. Extensive experiments on a facial attribute\ndataset demonstrate the superiority of our approach and the generated attention\nmasks better explain what SAT attends when translating images. \n\n"}
{"id": "1811.08305", "contents": "Title: IVD-Net: Intervertebral disc localization and segmentation in MRI with a\n  multi-modal UNet Abstract: Accurate localization and segmentation of intervertebral disc (IVD) is\ncrucial for the assessment of spine disease diagnosis. Despite the\ntechnological advances in medical imaging, IVD localization and segmentation\nare still manually performed, which is time-consuming and prone to errors. If,\nin addition, multi-modal imaging is considered, the burden imposed on disease\nassessments increases substantially. In this paper, we propose an architecture\nfor IVD localization and segmentation in multi-modal MRI, which extends the\nwell-known UNet. Compared to single images, multi-modal data brings\ncomplementary information, contributing to better data representation and\ndiscriminative power. Our contributions are three-fold. First, how to\neffectively integrate and fully leverage multi-modal data remains almost\nunexplored. In this work, each MRI modality is processed in a different path to\nbetter exploit their unique information. Second, inspired by HyperDenseNet, the\nnetwork is densely-connected both within each path and across different paths,\ngranting the model the freedom to learn where and how the different modalities\nshould be processed and combined. Third, we improved standard U-Net modules by\nextending inception modules with two dilated convolutions blocks of different\nscale, which helps handling multi-scale context. We report experiments over the\ndata set of the public MICCAI 2018 Challenge on Automatic Intervertebral Disc\nLocalization and Segmentation, with 13 multi-modal MRI images used for training\nand 3 for validation. We trained IVD-Net on an NVidia TITAN XP GPU with 16 GBs\nRAM, using ADAM as optimizer and a learning rate of 10e-5 during 200 epochs.\nTraining took about 5 hours, and segmentation of a whole volume about 2-3\nseconds, on average. Several baselines, with different multi-modal fusion\nstrategies, were used to demonstrate the effectiveness of the proposed\narchitecture. \n\n"}
{"id": "1811.08383", "contents": "Title: TSM: Temporal Shift Module for Efficient Video Understanding Abstract: The explosive growth in video streaming gives rise to challenges on\nperforming video understanding at high accuracy and low computation cost.\nConventional 2D CNNs are computationally cheap but cannot capture temporal\nrelationships; 3D CNN based methods can achieve good performance but are\ncomputationally intensive, making it expensive to deploy. In this paper, we\npropose a generic and effective Temporal Shift Module (TSM) that enjoys both\nhigh efficiency and high performance. Specifically, it can achieve the\nperformance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the\nchannels along the temporal dimension; thus facilitate information exchanged\namong neighboring frames. It can be inserted into 2D CNNs to achieve temporal\nmodeling at zero computation and zero parameters. We also extended TSM to\nonline setting, which enables real-time low-latency online video recognition\nand video object detection. TSM is accurate and efficient: it ranks the first\nplace on the Something-Something leaderboard upon publication; on Jetson Nano\nand Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video\nrecognition. The code is available at:\nhttps://github.com/mit-han-lab/temporal-shift-module. \n\n"}
{"id": "1811.08458", "contents": "Title: Intermediate Level Adversarial Attack for Enhanced Transferability Abstract: Neural networks are vulnerable to adversarial examples, malicious inputs\ncrafted to fool trained models. Adversarial examples often exhibit black-box\ntransfer, meaning that adversarial examples for one model can fool another\nmodel. However, adversarial examples may be overfit to exploit the particular\narchitecture and feature representation of a source model, resulting in\nsub-optimal black-box transfer attacks to other target models. This leads us to\nintroduce the Intermediate Level Attack (ILA), which attempts to fine-tune an\nexisting adversarial example for greater black-box transferability by\nincreasing its perturbation on a pre-specified layer of the source model. We\nshow that our method can effectively achieve this goal and that we can decide a\nnearly-optimal layer of the source model to perturb without any knowledge of\nthe target models. \n\n"}
{"id": "1811.08982", "contents": "Title: Polarity Loss for Zero-shot Object Detection Abstract: Conventional object detection models require large amounts of training data.\nIn comparison, humans can recognize previously unseen objects by merely knowing\ntheir semantic description. To mimic similar behaviour, zero-shot object\ndetection aims to recognize and localize 'unseen' object instances by using\nonly their semantic information. The model is first trained to learn the\nrelationships between visual and semantic domains for seen objects, later\ntransferring the acquired knowledge to totally unseen objects. This setting\ngives rise to the need for correct alignment between visual and semantic\nconcepts, so that the unseen objects can be identified using only their\nsemantic attributes. In this paper, we propose a novel loss function called\n'Polarity loss', that promotes correct visual-semantic alignment for an\nimproved zero-shot object detection. On one hand, it refines the noisy semantic\nembeddings via metric learning on a 'Semantic vocabulary' of related concepts\nto establish a better synergy between visual and semantic domains. On the other\nhand, it explicitly maximizes the gap between positive and negative predictions\nto achieve better discrimination between seen, unseen and background objects.\nOur approach is inspired by embodiment theories in cognitive science, that\nclaim human semantic understanding to be grounded in past experiences (seen\nobjects), related linguistic concepts (word vocabulary) and visual perception\n(seen/unseen object images). We conduct extensive evaluations on MS-COCO and\nPascal VOC datasets, showing significant improvements over state of the art. \n\n"}
{"id": "1811.09243", "contents": "Title: FAIM -- A ConvNet Method for Unsupervised 3D Medical Image Registration Abstract: We present a new unsupervised learning algorithm, \"FAIM\", for 3D medical\nimage registration. With a different architecture than the popular \"U-net\", the\nnetwork takes a pair of full image volumes and predicts the displacement fields\nneeded to register source to target. Compared with \"U-net\" based registration\nnetworks such as VoxelMorph, FAIM has fewer trainable parameters but can\nachieve higher registration accuracy as judged by Dice score on region labels\nin the Mindboggle-101 dataset. Moreover, with the proposed penalty loss on\nnegative Jacobian determinants, FAIM produces deformations with many fewer\n\"foldings\", i.e. regions of non-invertibility where the surface folds over\nitself. In our experiment, we varied the strength of this penalty and\ninvestigated changes in registration accuracy and non-invertibility in terms of\nnumber of \"folding\" locations. We found that FAIM is able to maintain both the\nadvantages of higher accuracy and fewer \"folding\" locations over VoxelMorph,\nover a range of hyper-parameters (with the same values used for both networks).\nFurther, when trading off registration accuracy for better invertibility, FAIM\nrequired less sacrifice of registration accuracy. Codes for this paper will be\nreleased upon publication. \n\n"}
{"id": "1811.09426", "contents": "Title: Joint Neural Architecture Search and Quantization Abstract: Designing neural architectures is a fundamental step in deep learning\napplications. As a partner technique, model compression on neural networks has\nbeen widely investigated to gear the needs that the deep learning algorithms\ncould be run with the limited computation resources on mobile devices.\nCurrently, both the tasks of architecture design and model compression require\nexpertise tricks and tedious trials. In this paper, we integrate these two\ntasks into one unified framework, which enables the joint architecture search\nwith quantization (compression) policies for neural networks. This method is\nnamed JASQ. Here our goal is to automatically find a compact neural network\nmodel with high performance that is suitable for mobile devices. Technically, a\nmulti-objective evolutionary search algorithm is introduced to search the\nmodels under the balance between model size and performance accuracy. In\nexperiments, we find that our approach outperforms the methods that search only\nfor architectures or only for quantization policies. 1) Specifically, given\nexisting networks, our approach can provide them with learning-based\nquantization policies, and outperforms their 2 bits, 4 bits, 8 bits, and 16\nbits counterparts. It can yield higher accuracies than the float models, for\nexample, over 1.02% higher accuracy on MobileNet-v1. 2) What is more, under the\nbalance between model size and performance accuracy, two models are obtained\nwith joint search of architectures and quantization policies: a high-accuracy\nmodel and a small model, JASQNet and JASQNet-Small that achieves 2.97% error\nrate with 0.9 MB on CIFAR-10. \n\n"}
{"id": "1811.09763", "contents": "Title: Mean Local Group Average Precision (mLGAP): A New Performance Metric for\n  Hashing-based Retrieval Abstract: The research on hashing techniques for visual data is gaining increased\nattention in recent years due to the need for compact representations\nsupporting efficient search/retrieval in large-scale databases such as online\nimages. Among many possibilities, Mean Average Precision(mAP) has emerged as\nthe dominant performance metric for hashing-based retrieval. One glaring\nshortcoming of mAP is its inability in balancing retrieval accuracy and\nutilization of hash codes: pushing a system to attain higher mAP will\ninevitably lead to poorer utilization of the hash codes. Poor utilization of\nthe hash codes hinders good retrieval because of increased collision of samples\nin the hash space. This means that a model giving a higher mAP values does not\nnecessarily do a better job in retrieval. In this paper, we introduce a new\nmetric named Mean Local Group Average Precision (mLGAP) for better evaluation\nof the performance of hashing-based retrieval. The new metric provides a\nretrieval performance measure that also reconciles the utilization of hash\ncodes, leading to a more practically meaningful performance metric than\nconventional ones like mAP. To this end, we start by mathematical analysis of\nthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and\nshow why it is more appropriate for hashing-based retrieval. Experiments on\nimage retrieval are used to demonstrate the effectiveness of the proposed\nmetric. \n\n"}
{"id": "1811.09916", "contents": "Title: Generating Realistic Training Images Based on Tonality-Alignment\n  Generative Adversarial Networks for Hand Pose Estimation Abstract: Hand pose estimation from a monocular RGB image is an important but\nchallenging task. The main factor affecting its performance is the lack of a\nsufficiently large training dataset with accurate hand-keypoint annotations. In\nthis work, we circumvent this problem by proposing an effective method for\ngenerating realistic hand poses and show that state-of-the-art algorithms for\nhand pose estimation can be greatly improved by utilizing the generated hand\nposes as training data. Specifically, we first adopt an augmented reality (AR)\nsimulator to synthesize hand poses with accurate hand-keypoint labels. Although\nthe synthetic hand poses come with precise joint labels, eliminating the need\nof manual annotations, they look unnatural and are not the ideal training data.\nTo produce more realistic hand poses, we propose to blend a synthetic hand pose\nwith a real background, such as arms and sleeves. To this end, we develop\ntonality-alignment generative adversarial networks (TAGANs), which align the\ntonality and color distributions between synthetic hand poses and real\nbackgrounds, and can generate high quality hand poses. We evaluate TAGAN on\nthree benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With\nthe aid of the synthesized poses, our method performs favorably against the\nstate-of-the-arts in both 2D and 3D hand pose estimations. \n\n"}
{"id": "1811.09928", "contents": "Title: PCGAN: Partition-Controlled Human Image Generation Abstract: Human image generation is a very challenging task since it is affected by\nmany factors. Many human image generation methods focus on generating human\nimages conditioned on a given pose, while the generated backgrounds are often\nblurred.In this paper,we propose a novel Partition-Controlled GAN to generate\nhuman images according to target pose and background. Firstly, human poses in\nthe given images are extracted, and foreground/background are partitioned for\nfurther use. Secondly, we extract and fuse appearance features, pose features\nand background features to generate the desired images. Experiments on\nMarket-1501 and DeepFashion datasets show that our model not only generates\nrealistic human images but also produce the human pose and background as we\nwant. Extensive experiments on COCO and LIP datasets indicate the potential of\nour method. \n\n"}
{"id": "1811.10413", "contents": "Title: Structured Binary Neural Networks for Accurate Image Classification and\n  Semantic Segmentation Abstract: In this paper, we propose to train convolutional neural networks (CNNs) with\nboth binarized weights and activations, leading to quantized models\nspecifically} for mobile devices with limited power capacity and computation\nresources. Previous works on quantizing CNNs seek to approximate the\nfloating-point information using a set of discrete values, which we call value\napproximation, but typically assume the same architecture as the full-precision\nnetworks. In this paper, however, we take a novel 'structure approximation'\nview for quantization---it is very likely that a different architecture may be\nbetter for best performance. In particular, we propose a `network\ndecomposition' strategy, named \\textbf{Group-Net}, in which we divide the\nnetwork into groups. In this way, each full-precision group can be effectively\nreconstructed by aggregating a set of homogeneous binary branches.\n  In addition, we learn effective connections among groups to improve the\nrepresentational capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for highly\naccurate semantic segmentation by embedding rich context into the binary\nstructure.\n  Experiments on both classification and semantic segmentation tasks\ndemonstrate the superior performance of the proposed methods over various\npopular architectures. In particular, we outperform the previous best binary\nneural networks in terms of accuracy and major computation savings. \n\n"}
{"id": "1811.10884", "contents": "Title: UnDEMoN 2.0: Improved Depth and Ego Motion Estimation through Deep Image\n  Sampling Abstract: In this paper, we provide an improved version of UnDEMoN model for depth and\nego motion estimation from monocular images. The improvement is achieved by\ncombining the standard bi-linear sampler with a deep network based image\nsampling model (DIS-NET) to provide better image reconstruction capabilities on\nwhich the depth estimation accuracy depends in un-supervised learning models.\nWhile DIS-NET provides higher order regression and larger input search space,\nthe bi-linear sampler provides geometric constraints necessary for reducing the\nsize of the solution space for an ill-posed problem of this kind. This\ncombination is shown to provide significant improvement in depth and pose\nestimation accuracy outperforming all existing state-of-the-art methods in this\ncategory. In addition, the modified network uses far less number of tunable\nparameters making it one of the lightest deep network model for depth\nestimation. The proposed model is labeled as \"UnDEMoN 2.0\" indicating an\nimprovement over the existing UnDEMoN model. The efficacy of the proposed model\nis demonstrated through rigorous experimental analysis on the standard KITTI\ndataset. \n\n"}
{"id": "1811.10943", "contents": "Title: Deep Geometric Prior for Surface Reconstruction Abstract: The reconstruction of a discrete surface from a point cloud is a fundamental\ngeometry processing problem that has been studied for decades, with many\nmethods developed. We propose the use of a deep neural network as a geometric\nprior for surface reconstruction. Specifically, we overfit a neural network\nrepresenting a local chart parameterization to part of an input point cloud\nusing the Wasserstein distance as a measure of approximation. By jointly\nfitting many such networks to overlapping parts of the point cloud, while\nenforcing a consistency condition, we compute a manifold atlas. By sampling\nthis atlas, we can produce a dense reconstruction of the surface approximating\nthe input cloud. The entire procedure does not require any training data or\nexplicit regularization, yet, we show that it is able to perform remarkably\nwell: not introducing typical overfitting artifacts, and approximating sharp\nfeatures closely at the same time. We experimentally show that this geometric\nprior produces good results for both man-made objects containing sharp features\nand smoother organic objects, as well as noisy inputs. We compare our method\nwith a number of well-known reconstruction methods on a standard surface\nreconstruction benchmark. \n\n"}
{"id": "1811.11144", "contents": "Title: Detecting Cherenkov Light From 1-2 MeV Electrons in Linear Alkylbenzene Abstract: The FlatDot detector has been used to demonstrate the separation of Cherenkov\nand scintillation light for 1 to 2MeV electrons in linear alkylbenzene (LAB).\nWith an average PMT transit time spread (TTS) of 200ps, the early light in each\nevent is clearly dominated by the Cherenkov signal, which on average comprises\n$86^{+2}_{-3}\\%$ of the light collected in the first 4.1ns of each event. The\nspatial distributions of the Cherenkov and scintillation light are found to\nmatch those predicted in Monte Carlo simulations. This is a key step towards\ndemonstrating direction reconstruction of $\\beta$ decays, a technique that\ncould reduce $^8$B solar neutrino backgrounds for neutrinoless double-beta\ndecay experiments in liquid scintillator. \n\n"}
{"id": "1811.11147", "contents": "Title: Understanding and Improving Kernel Local Descriptors Abstract: We propose a multiple-kernel local-patch descriptor based on efficient match\nkernels from pixel gradients. It combines two parametrizations of gradient\nposition and direction, each parametrization provides robustness to a different\ntype of patch mis-registration: polar parametrization for noise in the patch\ndominant orientation detection, Cartesian for imprecise location of the feature\npoint. Combined with whitening of the descriptor space, that is learned with or\nwithout supervision, the performance is significantly improved. We analyze the\neffect of the whitening on patch similarity and demonstrate its semantic\nmeaning. Our unsupervised variant is the best performing descriptor constructed\nwithout the need of labeled data. Despite the simplicity of the proposed\ndescriptor, it competes well with deep learning approaches on a number of\ndifferent tasks. \n\n"}
{"id": "1811.12039", "contents": "Title: EV-SegNet: Semantic Segmentation for Event-based Cameras Abstract: Event cameras, or Dynamic Vision Sensor (DVS), are very promising sensors\nwhich have shown several advantages over frame based cameras. However, most\nrecent work on real applications of these cameras is focused on 3D\nreconstruction and 6-DOF camera tracking. Deep learning based approaches, which\nare leading the state-of-the-art in visual recognition tasks, could potentially\ntake advantage of the benefits of DVS, but some adaptations are needed still\nneeded in order to effectively work on these cameras. This work introduces a\nfirst baseline for semantic segmentation with this kind of data. We build a\nsemantic segmentation CNN based on state-of-the-art techniques which takes\nevent information as the only input. Besides, we propose a novel representation\nfor DVS data that outperforms previously used event representations for related\ntasks. Since there is no existing labeled dataset for this task, we propose how\nto automatically generate approximated semantic segmentation labels for some\nsequences of the DDD17 dataset, which we publish together with the model, and\ndemonstrate they are valid to train a model for DVS data only. We compare our\nresults on semantic segmentation from DVS data with results using corresponding\ngrayscale images, demonstrating how they are complementary and worth combining. \n\n"}
{"id": "1811.12167", "contents": "Title: Density Changes in Low Pressure Gas Targets for Electron Scattering\n  Experiments Abstract: A system of modular sealed gas target cells has been developed for use in\nelectron scattering experiments at the Thomas Jefferson National Accelerator\nFacility (Jefferson Lab). This system was initially developed to complete the\nMARATHON experiment which required, among other species, tritium as a target\nmaterial. Thus far, the cells have been loaded with the gas species 3H, 3He,\n2H, 1H and 40Ar and operated in nominal beam currents of up to 22.5 uA in\nJefferson Lab's Hall A. While the gas density of the cells at the time of\nloading is known, the density of each gas varies uniquely when heated by the\nelectron beam. To extract experimental cross sections using these cells,\ndensity dependence on beam current of each target fluid must be determined. In\nthis study, data from measurements with several beam currents within the range\nof 2.5 to 22.5 uA on each target fluid are presented. Additionally, expressions\nfor the beam current dependent fluid density of each target are developed. \n\n"}
{"id": "1811.12180", "contents": "Title: Studies Of The Polarization Of Gamma Photons Originating From The Decay\n  of Positronium Atoms Abstract: The precise measurements of the Compton scatterings of photons originating\nfrom the decay of positronium atoms can reveal information about their\npolarizations. J-PET detector is constructed of 192 plastic scintillators and\nis unique to study the scattering correlations of the annihilation photons with\nan angular precision of several degrees. In this work, we present the first\nexperimental evidence showing the feasibility of measuring the photons relative\npolarization using the J-PET detector. \n\n"}
{"id": "1811.12463", "contents": "Title: Fast and Flexible Indoor Scene Synthesis via Deep Convolutional\n  Generative Models Abstract: We present a new, fast and flexible pipeline for indoor scene synthesis that\nis based on deep convolutional generative models. Our method operates on a\ntop-down image-based representation, and inserts objects iteratively into the\nscene by predicting their category, location, orientation and size with\nseparate neural network modules. Our pipeline naturally supports automatic\ncompletion of partial scenes, as well as synthesis of complete scenes. Our\nmethod is significantly faster than the previous image-based method and\ngenerates result that outperforms it and other state-of-the-art deep generative\nscene models in terms of faithfulness to training data and perceived visual\nquality. \n\n"}
{"id": "1812.00518", "contents": "Title: Elastic Boundary Projection for 3D Medical Image Segmentation Abstract: We focus on an important yet challenging problem: using a 2D deep network to\ndeal with 3D segmentation for medical image analysis. Existing approaches\neither applied multi-view planar (2D) networks or directly used volumetric (3D)\nnetworks for this purpose, but both of them are not ideal: 2D networks cannot\ncapture 3D contexts effectively, and 3D networks are both memory-consuming and\nless stable arguably due to the lack of pre-trained models.\n  In this paper, we bridge the gap between 2D and 3D using a novel approach\nnamed Elastic Boundary Projection (EBP). The key observation is that, although\nthe object is a 3D volume, what we really need in segmentation is to find its\nboundary which is a 2D surface. Therefore, we place a number of pivot points in\nthe 3D space, and for each pivot, we determine its distance to the object\nboundary along a dense set of directions. This creates an elastic shell around\neach pivot which is initialized as a perfect sphere. We train a 2D deep network\nto determine whether each ending point falls within the object, and gradually\nadjust the shell so that it gradually converges to the actual shape of the\nboundary and thus achieves the goal of segmentation. EBP allows boundary-based\nsegmentation without cutting a 3D volume into slices or patches, which stands\nout from conventional 2D and 3D approaches. EBP achieves promising accuracy in\nabdominal organ segmentation. Our code has been open-sourced\nhttps://github.com/twni2016/Elastic-Boundary-Projection. \n\n"}
{"id": "1812.00535", "contents": "Title: Beyond Inferring Class Representatives: User-Level Privacy Leakage From\n  Federated Learning Abstract: Federated learning, i.e., a mobile edge computing framework for deep\nlearning, is a recent advance in privacy-preserving machine learning, where the\nmodel is trained in a decentralized manner by the clients, i.e., data curators,\npreventing the server from directly accessing those private data from the\nclients. This learning mechanism significantly challenges the attack from the\nserver side. Although the state-of-the-art attacking techniques that\nincorporated the advance of Generative adversarial networks (GANs) could\nconstruct class representatives of the global data distribution among all\nclients, it is still challenging to distinguishably attack a specific client\n(i.e., user-level privacy leakage), which is a stronger privacy threat to\nprecisely recover the private data from a specific client. This paper gives the\nfirst attempt to explore user-level privacy leakage against the federated\nlearning by the attack from a malicious server. We propose a framework\nincorporating GAN with a multi-task discriminator, which simultaneously\ndiscriminates category, reality, and client identity of input samples. The\nnovel discrimination on client identity enables the generator to recover user\nspecified private data. Unlike existing works that tend to interfere the\ntraining process of the federated learning, the proposed method works\n\"invisibly\" on the server side. The experimental results demonstrate the\neffectiveness of the proposed attacking approach and the superior to the\nstate-of-the-art. \n\n"}
{"id": "1812.00828", "contents": "Title: Novel Quality Metric for Duration Variability Compensation in Speaker\n  Verification using i-Vectors Abstract: Automatic speaker verification (ASV) is the process to recognize persons\nusing voice as biometric. The ASV systems show considerable recognition\nperformance with sufficient amount of speech from matched condition. One of the\ncrucial challenges of ASV technology is to improve recognition performance with\nspeech segments of short duration. In short duration condition, the model\nparameters are not properly estimated due to inadequate speech information, and\nthis results poor recognition accuracy even with the state-of-the-art i-vector\nbased ASV system. We hypothesize that considering the estimation quality during\nrecognition process would help to improve the ASV performance. This can be\nincorporated as a quality measure during fusion of ASV systems. This paper\ninvestigates a new quality measure for i-vector representation of speech\nutterances computed directly from Baum-Welch statistics. The proposed metric is\nsubsequently used as quality measure during fusion of ASV systems. In\nexperiments with the NIST SRE 2008 corpus, We have shown that inclusion of\nproposed quality metric exhibits considerable improvement in speaker\nverification performance. The results also indicate the potentiality of the\nproposed method in real-world scenario with short test utterances. \n\n"}
{"id": "1812.01068", "contents": "Title: Machine Friendly Machine Learning: Interpretation of Computed Tomography\n  Without Image Reconstruction Abstract: Recent advancements in deep learning for automated image processing and\nclassification have accelerated many new applications for medical image\nanalysis. However, most deep learning applications have been developed using\nreconstructed, human-interpretable medical images. While image reconstruction\nfrom raw sensor data is required for the creation of medical images, the\nreconstruction process only uses a partial representation of all the data\nacquired. Here we report the development of a system to directly process raw\ncomputed tomography (CT) data in sinogram-space, bypassing the intermediary\nstep of image reconstruction. Two classification tasks were evaluated for their\nfeasibility for sinogram-space machine learning: body region identification and\nintracranial hemorrhage (ICH) detection. Our proposed SinoNet performed\nfavorably compared to conventional reconstructed image-space-based systems for\nboth tasks, regardless of scanning geometries in terms of projections or\ndetectors. Further, SinoNet performed significantly better when using sparsely\nsampled sinograms than conventional networks operating in image-space. As a\nresult, sinogram-space algorithms could be used in field settings for binary\ndiagnosis testing, triage, and in clinical settings where low radiation dose is\ndesired. These findings also demonstrate another strength of deep learning\nwhere it can analyze and interpret sinograms that are virtually impossible for\nhuman experts. \n\n"}
{"id": "1812.02619", "contents": "Title: Tube-CNN: Modeling temporal evolution of appearance for object detection\n  in video Abstract: Object detection in video is crucial for many applications. Compared to\nimages, video provides additional cues which can help to disambiguate the\ndetection problem. Our goal in this paper is to learn discriminative models for\nthe temporal evolution of object appearance and to use such models for object\ndetection. To model temporal evolution, we introduce space-time tubes\ncorresponding to temporal sequences of bounding boxes. We propose two CNN\narchitectures for generating and classifying tubes, respectively. Our tube\nproposal network (TPN) first generates a large number of spatio-temporal tube\nproposals maximizing object recall. The Tube-CNN then implements a tube-level\nobject detector in the video. Our method improves state of the art on two\nlarge-scale datasets for object detection in video: HollywoodHeads and ImageNet\nVID. Tube models show particular advantages in difficult dynamic scenes. \n\n"}
{"id": "1812.02707", "contents": "Title: Video Action Transformer Network Abstract: We introduce the Action Transformer model for recognizing and localizing\nhuman actions in video clips. We repurpose a Transformer-style architecture to\naggregate features from the spatiotemporal context around the person whose\nactions we are trying to classify. We show that by using high-resolution,\nperson-specific, class-agnostic queries, the model spontaneously learns to\ntrack individual people and to pick up on semantic context from the actions of\nothers. Additionally its attention mechanism learns to emphasize hands and\nfaces, which are often crucial to discriminate an action - all without explicit\nsupervision other than boxes and class labels. We train and test our Action\nTransformer network on the Atomic Visual Actions (AVA) dataset, outperforming\nthe state-of-the-art by a significant margin using only raw RGB frames as\ninput. \n\n"}
{"id": "1812.02784", "contents": "Title: StoryGAN: A Sequential Conditional GAN for Story Visualization Abstract: We propose a new task, called Story Visualization. Given a multi-sentence\nparagraph, the story is visualized by generating a sequence of images, one for\neach sentence. In contrast to video generation, story visualization focuses\nless on the continuity in generated images (frames), but more on the global\nconsistency across dynamic scenes and characters -- a challenge that has not\nbeen addressed by any single-image or video generation methods. We therefore\npropose a new story-to-image-sequence generation model, StoryGAN, based on the\nsequential conditional GAN framework. Our model is unique in that it consists\nof a deep Context Encoder that dynamically tracks the story flow, and two\ndiscriminators at the story and image levels, to enhance the image quality and\nthe consistency of the generated sequences. To evaluate the model, we modified\nexisting datasets to create the CLEVR-SV and Pororo-SV datasets. Empirically,\nStoryGAN outperforms state-of-the-art models in image quality, contextual\nconsistency metrics, and human evaluation. \n\n"}
{"id": "1812.02897", "contents": "Title: Improved Search Strategies with Application to Estimating Facial\n  Blendshape Parameters Abstract: It is well known that popular optimization techniques can lead to overfitting\nor even a lack of convergence altogether; thus, practitioners often utilize ad\nhoc regularization terms added to the energy functional. When carefully\ncrafted, these regularizations can produce compelling results. However,\nregularization changes both the energy landscape and the solution to the\noptimization problem, which can result in underfitting. Surprisingly, many\npractitioners both add regularization and claim that their model lacks the\nexpressivity to fit the data. Motivated by a geometric interpretation of the\nlinearized search space, we propose an approach that ameliorates overfitting\nwithout the need for regularization terms that restrict the expressiveness of\nthe underlying model. We illustrate the efficacy of our approach on\nminimization problems related to three-dimensional facial expression estimation\nwhere overfitting clouds semantic understanding and regularization may lead to\nunderfitting that misses or misinterprets subtle expressions. \n\n"}
{"id": "1812.02898", "contents": "Title: TDAN: Temporally Deformable Alignment Network for Video Super-Resolution Abstract: Video super-resolution (VSR) aims to restore a photo-realistic\nhigh-resolution (HR) video frame from both its corresponding low-resolution\n(LR) frame (reference frame) and multiple neighboring frames (supporting\nframes). Due to varying motion of cameras or objects, the reference frame and\neach support frame are not aligned. Therefore, temporal alignment is a\nchallenging yet important problem for VSR. Previous VSR methods usually utilize\noptical flow between the reference frame and each supporting frame to wrap the\nsupporting frame for temporal alignment. Therefore, the performance of these\nimage-level wrapping-based models will highly depend on the prediction accuracy\nof optical flow, and inaccurate optical flow will lead to artifacts in the\nwrapped supporting frames, which also will be propagated into the reconstructed\nHR video frame. To overcome the limitation, in this paper, we propose a\ntemporal deformable alignment network (TDAN) to adaptively align the reference\nframe and each supporting frame at the feature level without computing optical\nflow. The TDAN uses features from both the reference frame and each supporting\nframe to dynamically predict offsets of sampling convolution kernels. By using\nthe corresponding kernels, TDAN transforms supporting frames to align with the\nreference frame. To predict the HR video frame, a reconstruction network taking\naligned frames and the reference frame is utilized. Experimental results\ndemonstrate the effectiveness of the proposed TDAN-based VSR model. \n\n"}
{"id": "1812.02970", "contents": "Title: The MORA project Abstract: The MORA (Matter's Origin from the RadioActivity of trapped and oriented\nions) project aims at measuring with unprecedented precision the D correlation\nin the nuclear beta decay of trapped and oriented ions. The D correlation\noffers the possibility to search for new CP-violating interactions,\ncomplementary to searches done at the LHC and with Electric Dipole Moments.\nTechnically, MORA uses an innovative in-trap orientation method which combines\nthe high trapping efficiency of a transparent Paul trap with laser orientation\ntechniques. The trapping, detection, and laser setups are under development,\nfor first tests at the Accelerator laboratory, JYFL, in the coming years. \n\n"}
{"id": "1812.03079", "contents": "Title: ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing\n  the Worst Abstract: Our goal is to train a policy for autonomous driving via imitation learning\nthat is robust enough to drive a real vehicle. We find that standard behavior\ncloning is insufficient for handling complex driving scenarios, even when we\nleverage a perception system for preprocessing the input and a controller for\nexecuting the output on the car: 30 million examples are still not enough. We\npropose exposing the learner to synthesized data in the form of perturbations\nto the expert's driving, which creates interesting situations such as\ncollisions and/or going off the road. Rather than purely imitating all data, we\naugment the imitation loss with additional losses that penalize undesirable\nevents and encourage progress -- the perturbations then provide an important\nsignal for these losses and lead to robustness of the learned model. We show\nthat the ChauffeurNet model can handle complex situations in simulation, and\npresent ablation experiments that emphasize the importance of each of our\nproposed changes and show that the model is responding to the appropriate\ncausal factors. Finally, we demonstrate the model driving a car in the real\nworld. \n\n"}
{"id": "1812.03807", "contents": "Title: Fissile material detection using neutron time-correlations from\n  photofission Abstract: The detection of special nuclear materials (SNM) in commercial cargoes is a\nmajor objective in the field of nuclear security. In this work we investigate\nthe use of two-neutron time-correlations from photo-fission using the Prompt\nNeutrons from Photofission (PNPF) detectors in Passport Systems Inc.'s (PSI)\nShielded Nuclear Alarm Resolution (SNAR) platform~\\cite{pnpf} for the purpose\nof detecting $\\sim$5~kg quantities of fissionable materials in seconds. The\ngoal of this effort was to extend the secondary scan mode of this system to\ndifferentiate fissile materials, such as highly enriched uranium, from\nfissionable materials, such as low enriched and depleted uranium (LEU and DU).\nExperiments were performed using a variety of material samples, and data were\nanalyzed using the variance-over-mean technique referred to as $Y_{2F}$ or\nFeynman-$\\alpha$. Results were compared to computational models to improve our\nability to predict system performance for distinguishing fissile materials.\nSimulations were then combined with empirical formulas to generate receiver\noperating characteristics (ROC) curves for a variety of shielding scenarios. We\nshow that a 10 second screening with a 200~$\\mu$A 9~MeV X-ray beam is\nsufficient to differentiate kilogram quantities of HEU from DU in various\nshielding scenarios in a standard cargo container. \n\n"}
{"id": "1812.03944", "contents": "Title: Data Fine-tuning Abstract: In real-world applications, commercial off-the-shelf systems are utilized for\nperforming automated facial analysis including face recognition, emotion\nrecognition, and attribute prediction. However, a majority of these commercial\nsystems act as black boxes due to the inaccessibility of the model parameters\nwhich makes it challenging to fine-tune the models for specific applications.\nStimulated by the advances in adversarial perturbations, this research proposes\nthe concept of Data Fine-tuning to improve the classification accuracy of a\ngiven model without changing the parameters of the model. This is accomplished\nby modeling it as data (image) perturbation problem. A small amount of \"noise\"\nis added to the input with the objective of minimizing the classification loss\nwithout affecting the (visual) appearance. Experiments performed on three\npublicly available datasets LFW, CelebA, and MUCT, demonstrate the\neffectiveness of the proposed concept. \n\n"}
{"id": "1812.04056", "contents": "Title: Accelerating Convolutional Neural Networks via Activation Map\n  Compression Abstract: The deep learning revolution brought us an extensive array of neural network\narchitectures that achieve state-of-the-art performance in a wide variety of\nComputer Vision tasks including among others, classification, detection and\nsegmentation. In parallel, we have also been observing an unprecedented demand\nin computational and memory requirements, rendering the efficient use of neural\nnetworks in low-powered devices virtually unattainable. Towards this end, we\npropose a three-stage compression and acceleration pipeline that sparsifies,\nquantizes and entropy encodes activation maps of Convolutional Neural Networks.\nSparsification increases the representational power of activation maps leading\nto both acceleration of inference and higher model accuracy. Inception-V3 and\nMobileNet-V1 can be accelerated by as much as $1.6\\times$ with an increase in\naccuracy of $0.38\\%$ and $0.54\\%$ on the ImageNet and CIFAR-10 datasets\nrespectively. Quantizing and entropy coding the sparser activation maps lead to\nhigher compression over the baseline, reducing the memory cost of the network\nexecution. Inception-V3 and MobileNet-V1 activation maps, quantized to $16$\nbits, are compressed by as much as $6\\times$ with an increase in accuracy of\n$0.36\\%$ and $0.55\\%$ respectively. \n\n"}
{"id": "1812.04732", "contents": "Title: First determination of the $\\rho $ parameter at $\\sqrt{s} = 13$ TeV --\n  probing the existence of a colourless three-gluon bound state Abstract: The TOTEM experiment at the LHC has performed the first measurement at\n$\\sqrt{s} = 13$ TeV of the $\\rho$ parameter, the real to imaginary ratio of the\nnuclear elastic scattering amplitude at $t=0$, obtaining the following results:\n$\\rho = 0.09 \\pm 0.01$ and $\\rho = 0.10 \\pm 0.01$, depending on different\nphysics assumptions and mathematical modelling. The unprecedented precision of\nthe $\\rho$ measurement, combined with the TOTEM total cross-section\nmeasurements in an energy range larger than 10 TeV (from 2.76 to 13 TeV), has\nimplied the exclusion of all the models classified and published by COMPETE.\nThe $\\rho$ results obtained by TOTEM are compatible with the predictions, from\nalternative theoretical models both in the Regge-like framework and in the QCD\nframework, of a colourless 3-gluon bound state exchange in the $t$-channel of\nthe proton-proton elastic scattering. On the contrary, if shown that the\n3-gluon bound state $t$-channel exchange is not of importance for the\ndescription of elastic scattering, the $\\rho$ value determined by TOTEM would\nrepresent a first evidence of a slowing down of the total cross-section growth\nat higher energies. The very low-$|t|$ reach allowed also to determine the\nabsolute normalisation using the Coulomb amplitude for the first time at the\nLHC and obtain a new total proton-proton cross-section measurement\n$\\sigma_{tot} = 110.3 \\pm 3.5$ mb, completely independent from the previous\nTOTEM determination. Combining the two TOTEM results yields $\\sigma_{tot} =\n110.5 \\pm 2.4$ mb. \n\n"}
{"id": "1812.05694", "contents": "Title: Background Discrimination for Neutrinoless Double Beta Decay in Liquid\n  Xenon Using Cherenkov Light Abstract: Neutrinoless double beta decays in liquid xenon produce a significant amount\nof Cherenkov light, with a photon number and angular distribution that\ndistinguishes these events from common backgrounds. A GEANT4 simulation was\nused to simulate Cherenkov photon production and measurement in a liquid xenon\ndetector and a multilayer perceptron was used to analyze the resulting\ndistributions to classify events based on their Cherenkov photons. Our results\nshow that a modest improvement in the sensitivity of neutrinoless double beta\ndecay searches is possible using this technique, but the kinematics of the\nneutrinoless double beta decay and electron scattering in liquid xenon\nsubstantially limit this approach. \n\n"}
{"id": "1812.06024", "contents": "Title: Fast Mitochondria Detection for Connectomics Abstract: High-resolution connectomics data allows for the identification of\ndysfunctional mitochondria which are linked to a variety of diseases such as\nautism or bipolar. However, manual analysis is not feasible since datasets can\nbe petabytes in size. We present a fully automatic mitochondria detector based\non a modified U-Net architecture that yields high accuracy and fast processing\ntimes. We evaluate our method on multiple real-world connectomics datasets,\nincluding an improved version of the EPFL mitochondria benchmark. Our results\nshow an Jaccard index of up to 0.90 with inference times lower than 16ms for a\n512x512px image tile. This speed is faster than the acquisition speed of modern\nelectron microscopes, enabling mitochondria detection in real-time. Our\ndetector ranks first for real-time detection when compared to previous works\nand data, results, and code are openly available. \n\n"}
{"id": "1812.06869", "contents": "Title: BriarPatches: Pixel-Space Interventions for Inducing Demographic Parity Abstract: We introduce the BriarPatch, a pixel-space intervention that obscures\nsensitive attributes from representations encoded in pre-trained classifiers.\nThe patches encourage internal model representations not to encode sensitive\ninformation, which has the effect of pushing downstream predictors towards\nexhibiting demographic parity with respect to the sensitive information. The\nnet result is that these BriarPatches provide an intervention mechanism\navailable at user level, and complements prior research on fair representations\nthat were previously only applicable by model developers and ML experts. \n\n"}
{"id": "1812.07145", "contents": "Title: Recurrent Calibration Network for Irregular Text Recognition Abstract: Scene text recognition has received increased attention in the research\ncommunity. Text in the wild often possesses irregular arrangements, typically\nincluding perspective text, curved text, oriented text. Most existing methods\nare hard to work well for irregular text, especially for severely distorted\ntext. In this paper, we propose a novel Recurrent Calibration Network (RCN) for\nirregular scene text recognition. The RCN progressively calibrates the\nirregular text to boost the recognition performance. By decomposing the\ncalibration process into multiple steps, the irregular text can be calibrated\nto normal one step by step. Besides, in order to avoid the accumulation of lost\ninformation caused by inaccurate transformation, we further design a\nfiducial-point refinement structure to keep the integrity of text during the\nrecurrent process. Instead of the calibrated images, the coordinates of\nfiducial points are tracked and refined, which implicitly models the\ntransformation information. Based on the refined fiducial points, we estimate\nthe transformation parameters and sample from the original image at each step.\nIn this way, the original character information is preserved until the final\ntransformation. Such designs lead to optimal calibration results to boost the\nperformance of succeeding recognition. Extensive experiments on challenging\ndatasets demonstrate the superiority of our method, especially on irregular\nbenchmarks. \n\n"}
{"id": "1812.08028", "contents": "Title: Accurate Hand Keypoint Localization on Mobile Devices Abstract: We present a novel approach for 2D hand keypoint localization from regular\ncolor input. The proposed approach relies on an appropriately designed\nConvolutional Neural Network (CNN) that computes a set of heatmaps, one per\nhand keypoint of interest. Extensive experiments with the proposed method\ncompare it against state of the art approaches and demonstrate its accuracy and\ncomputational performance on standard, publicly available datasets. The\nobtained results demonstrate that the proposed method matches or outperforms\nthe competing methods in accuracy, but clearly outperforms them in\ncomputational efficiency, making it a suitable building block for applications\nthat require hand keypoint estimation on mobile devices. \n\n"}
{"id": "1812.08139", "contents": "Title: Recent results from the MAJORANA DEMONSTRATOR Abstract: The MAJORANA DEMONSTRATOR is an experiment constructed to search for\nneutrinoless double-beta decay in $^{76}$Ge and to demonstrate the feasibility\nto deploy a large-scale experiment in a phased and modular fashion. It consists\nof two modules of natural and $^{76}$Ge-enriched germanium detectors totalling\n44.1 kg, operating at the 4850' level of the Sanford Underground Research\nFacility in Lead, South Dakota, USA. Commissioning of the experiment began in\nJune 2015, followed by data production with the full detector array in August\n2016. The ultra-low background and record energy resolution achieved by the\nMAJORANA DEMONSTRATOR enable a sensitive neutrinoless double-beta decay search,\nas well as additional searches for physics beyond the Standard Model. I will\ndiscuss the design elements that enable these searches, along with the latest\nresults, focusing on the neutrinoless double-beta decay search. I will also\ndiscuss the current status and the future plans of the MAJORANA DEMONSTRATOR,\nas well as the plans for a future tonne-scale $^{76}$Ge experiment. \n\n"}
{"id": "1812.08789", "contents": "Title: Steerable $e$PCA: Rotationally Invariant Exponential Family PCA Abstract: In photon-limited imaging, the pixel intensities are affected by photon count\nnoise. Many applications, such as 3-D reconstruction using correlation analysis\nin X-ray free electron laser (XFEL) single molecule imaging, require an\naccurate estimation of the covariance of the underlying 2-D clean images.\nAccurate estimation of the covariance from low-photon count images must take\ninto account that pixel intensities are Poisson distributed, hence the\nclassical sample covariance estimator is sub-optimal. Moreover, in single\nmolecule imaging, including in-plane rotated copies of all images could further\nimprove the accuracy of covariance estimation. In this paper we introduce an\nefficient and accurate algorithm for covariance matrix estimation of count\nnoise 2-D images, including their uniform planar rotations and possibly\nreflections. Our procedure, steerable $e$PCA, combines in a novel way two\nrecently introduced innovations. The first is a methodology for principal\ncomponent analysis (PCA) for Poisson distributions, and more generally,\nexponential family distributions, called $e$PCA. The second is steerable PCA, a\nfast and accurate procedure for including all planar rotations for PCA. The\nresulting principal components are invariant to the rotation and reflection of\nthe input images. We demonstrate the efficiency and accuracy of steerable\n$e$PCA in numerical experiments involving simulated XFEL datasets and rotated\nYale B face data. \n\n"}
{"id": "1812.08854", "contents": "Title: Response of a Li-glass/multi-anode photomultiplier detector to\n  $\\alpha$-particles from $^{241}$Am Abstract: The response of a position-sensitive Li-glass scintillator detector to\n$\\alpha$-particles from a collimated $^{241}$Am source scanned across the face\nof the detector has been measured. Scintillation light was read out by an 8 X 8\npixel multi-anode photomultiplier and the signal amplitude for each pixel has\nbeen recorded for every position on a scan. The pixel signal is strongly\ndependent on position and in general several pixels will register a signal (a\nhit) above a given threshold. The effect of this threshold on hit multiplicity\nis studied, with a view to optimize the single-hit efficiency of the detector. \n\n"}
{"id": "1812.10240", "contents": "Title: Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning Abstract: Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet. \n\n"}
{"id": "1812.10411", "contents": "Title: Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages Abstract: Cross-lingual speech emotion recognition is an important task for practical\napplications. The performance of automatic speech emotion recognition systems\ndegrades in cross-corpus scenarios, particularly in scenarios involving\nmultiple languages or a previously unseen language such as Urdu for which\nlimited or no data is available. In this study, we investigate the problem of\ncross-lingual emotion recognition for Urdu language and contribute URDU---the\nfirst ever spontaneous Urdu-language speech emotion database. Evaluations are\nperformed using three different Western languages against Urdu and experimental\nresults on different possible scenarios suggest various interesting aspects for\ndesigning more adaptive emotion recognition system for such limited languages.\nIn results, selecting training instances of multiple languages can deliver\ncomparable results to baseline and augmentation a fraction of testing language\ndata while training can help to boost accuracy for speech emotion recognition.\nURDU data is publicly available for further research. \n\n"}
{"id": "1812.10889", "contents": "Title: InstaGAN: Instance-aware Image-to-Image Translation Abstract: Unsupervised image-to-image translation has gained considerable attention due\nto the recent impressive progress based on generative adversarial networks\n(GANs). However, previous methods often fail in challenging cases, in\nparticular, when an image has multiple target instances and a translation task\ninvolves significant changes in shape, e.g., translating pants to skirts in\nfashion images. To tackle the issues, we propose a novel method, coined\ninstance-aware GAN (InstaGAN), that incorporates the instance information\n(e.g., object segmentation masks) and improves multi-instance transfiguration.\nThe proposed method translates both an image and the corresponding set of\ninstance attributes while maintaining the permutation invariance property of\nthe instances. To this end, we introduce a context preserving loss that\nencourages the network to learn the identity function outside of target\ninstances. We also propose a sequential mini-batch inference/training technique\nthat handles multiple instances with a limited GPU memory and enhances the\nnetwork to generalize better for multiple instances. Our comparative evaluation\ndemonstrates the effectiveness of the proposed method on different image\ndatasets, in particular, in the aforementioned challenging cases. Code and\nresults are available in https://github.com/sangwoomo/instagan \n\n"}
{"id": "1812.11092", "contents": "Title: Multi-resolution neural networks for tracking seismic horizons from few\n  training images Abstract: Detecting a specific horizon in seismic images is a valuable tool for\ngeological interpretation. Because hand-picking the locations of the horizon is\na time-consuming process, automated computational methods were developed\nstarting three decades ago. Older techniques for such picking include\ninterpolation of control points however, in recent years neural networks have\nbeen used for this task. Until now, most networks trained on small patches from\nlarger images. This limits the networks ability to learn from large-scale\ngeologic structures. Moreover, currently available networks and training\nstrategies require label patches that have full and continuous annotations,\nwhich are also time-consuming to generate.\n  We propose a projected loss-function for training convolutional networks with\na multi-resolution structure, including variants of the U-net. Our networks\nlearn from a small number of large seismic images without creating patches. The\nprojected loss-function enables training on labels with just a few annotated\npixels and has no issue with the other unknown label pixels. Training uses all\ndata without reserving some for validation. Only the labels are split into\ntraining/testing. Contrary to other work on horizon tracking, we train the\nnetwork to perform non-linear regression, and not classification. As such, we\npropose labels as the convolution of a Gaussian kernel and the known horizon\nlocations that indicate uncertainty in the labels. The network output is the\nprobability of the horizon location. We demonstrate the proposed computational\ningredients on two different datasets, for horizon extrapolation and\ninterpolation. We show that the predictions of our methodology are accurate\neven in areas far from known horizon locations because our learning strategy\nexploits all data in large seismic images. \n\n"}
{"id": "1812.11317", "contents": "Title: Support Vector Guided Softmax Loss for Face Recognition Abstract: Face recognition has witnessed significant progresses due to the advances of\ndeep convolutional neural networks (CNNs), the central challenge of which, is\nfeature discrimination. To address it, one group tries to exploit mining-based\nstrategies (\\textit{e.g.}, hard example mining and focal loss) to focus on the\ninformative examples. The other group devotes to designing margin-based loss\nfunctions (\\textit{e.g.}, angular, additive and additive angular margins) to\nincrease the feature margin from the perspective of ground truth class. Both of\nthem have been well-verified to learn discriminative features. However, they\nsuffer from either the ambiguity of hard examples or the lack of discriminative\npower of other classes. In this paper, we design a novel loss function, namely\nsupport vector guided softmax loss (SV-Softmax), which adaptively emphasizes\nthe mis-classified points (support vectors) to guide the discriminative\nfeatures learning. So the developed SV-Softmax loss is able to eliminate the\nambiguity of hard examples as well as absorb the discriminative power of other\nclasses, and thus results in more discrimiantive features. To the best of our\nknowledge, this is the first attempt to inherit the advantages of mining-based\nand margin-based losses into one framework. Experimental results on several\nbenchmarks have demonstrated the effectiveness of our approach over\nstate-of-the-arts. \n\n"}
{"id": "1812.11677", "contents": "Title: ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using\n  Alternating Direction Method of Multipliers Abstract: To facilitate efficient embedded and hardware implementations of deep neural\nnetworks (DNNs), two important categories of DNN model compression techniques:\nweight pruning and weight quantization are investigated. The former leverages\nthe redundancy in the number of weights, whereas the latter leverages the\nredundancy in bit representation of weights. However, there lacks a systematic\nframework of joint weight pruning and quantization of DNNs, thereby limiting\nthe available model compression ratio. Moreover, the computation reduction,\nenergy efficiency improvement, and hardware performance overhead need to be\naccounted for besides simply model size reduction.\n  To address these limitations, we present ADMM-NN, the first\nalgorithm-hardware co-optimization framework of DNNs using Alternating\nDirection Method of Multipliers (ADMM), a powerful technique to deal with\nnon-convex optimization problems with possibly combinatorial constraints. The\nfirst part of ADMM-NN is a systematic, joint framework of DNN weight pruning\nand quantization using ADMM. It can be understood as a smart regularization\ntechnique with regularization target dynamically updated in each ADMM\niteration, thereby resulting in higher performance in model compression than\nprior work. The second part is hardware-aware DNN optimizations to facilitate\nhardware-level implementations.\n  Without accuracy loss, we can achieve 85$\\times$ and 24$\\times$ pruning on\nLeNet-5 and AlexNet models, respectively, significantly higher than prior work.\nThe improvement becomes more significant when focusing on computation\nreductions. Combining weight pruning and quantization, we achieve 1,910$\\times$\nand 231$\\times$ reductions in overall model size on these two benchmarks, when\nfocusing on data storage. Highly promising results are also observed on other\nrepresentative DNNs such as VGGNet and ResNet-50. \n\n"}
{"id": "1901.00039", "contents": "Title: Mask-aware networks for crowd counting Abstract: Crowd counting problem aims to count the number of objects within an image or\na frame in the videos and is usually solved by estimating the density map\ngenerated from the object location annotations. The values in the density map,\nby nature, take two possible states: zero indicating no object around, a\nnon-zero value indicating the existence of objects and the value denoting the\nlocal object density. In contrast to traditional methods which do not\ndifferentiate the density prediction of these two states, we propose to use a\ndedicated network branch to predict the object/non-object mask and then combine\nits prediction with the input image to produce the density map. Our rationale\nis that the mask prediction could be better modeled as a binary segmentation\nproblem and the difficulty of estimating the density could be reduced if the\nmask is known. A key to the proposed scheme is the strategy of incorporating\nthe mask prediction into the density map estimator. To this end, we study five\npossible solutions, and via analysis and experimental validation we identify\nthe most effective one. Through extensive experiments on five public datasets,\nwe demonstrate the superior performance of the proposed approach over the\nbaselines and show that our network could achieve the state-of-the-art\nperformance. \n\n"}
{"id": "1901.00850", "contents": "Title: CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions Abstract: Referring object detection and referring image segmentation are important\ntasks that require joint understanding of visual information and natural\nlanguage. Yet there has been evidence that current benchmark datasets suffer\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\ntheir intermediate reasoning process. To address these issues and complement\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\ndiagnostic dataset for referring expression comprehension. The precise\nlocations and attributes of the objects are readily available, and the\nreferring expressions are automatically associated with functional programs.\nThe synthetic nature allows control over dataset bias (through sampling\nstrategy), and the modular programs enable intermediate reasoning ground truth\nwithout human annotators.\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\nalso propose IEP-Ref, a module network approach that significantly outperforms\nother models on our dataset. In particular, we present two interesting and\nimportant findings using IEP-Ref: (1) the module trained to transform feature\nmaps into segmentation masks can be attached to any intermediate module to\nreveal the entire reasoning process step-by-step; (2) even if all training data\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\nwhen presented with false-premise referring expressions. To the best of our\nknowledge, this is the first direct and quantitative proof that neural modules\nbehave in the way they are intended. \n\n"}
{"id": "1901.01028", "contents": "Title: Iris Recognition with Image Segmentation Employing Retrained\n  Off-the-Shelf Deep Neural Networks Abstract: This paper offers three new, open-source, deep learning-based iris\nsegmentation methods, and the methodology how to use irregular segmentation\nmasks in a conventional Gabor-wavelet-based iris recognition. To train and\nvalidate the methods, we used a wide spectrum of iris images acquired by\ndifferent teams and different sensors and offered publicly, including data\ntaken from CASIA-Iris-Interval-v4, BioSec, ND-Iris-0405, UBIRIS,\nWarsaw-BioBase-Post-Mortem-Iris v2.0 (post-mortem iris images), and\nND-TWINS-2009-2010 (iris images acquired from identical twins). This varied\ntraining data should increase the generalization capabilities of the proposed\nsegmentation techniques. In database-disjoint training and testing, we show\nthat deep learning-based segmentation outperforms the conventional (OSIRIS)\nsegmentation in terms of Intersection over Union calculated between the\nobtained results and manually annotated ground-truth. Interestingly, the\nGabor-based iris matching is not always better when deep learning-based\nsegmentation is used, and is on par with the method employing Daugman's based\nsegmentation. \n\n"}
{"id": "1901.01939", "contents": "Title: GASL: Guided Attention for Sparsity Learning in Deep Neural Networks Abstract: The main goal of network pruning is imposing sparsity on the neural network\nby increasing the number of parameters with zero value in order to reduce the\narchitecture size and the computational speedup. In most of the previous\nresearch works, sparsity is imposed stochastically without considering any\nprior knowledge of the weights distribution or other internal network\ncharacteristics. Enforcing too much sparsity may induce accuracy drop due to\nthe fact that a lot of important elements might have been eliminated. In this\npaper, we propose Guided Attention for Sparsity Learning (GASL) to achieve (1)\nmodel compression by having less number of elements and speed-up; (2) prevent\nthe accuracy drop by supervising the sparsity operation via a guided attention\nmechanism and (3) introduce a generic mechanism that can be adapted for any\ntype of architecture; Our work is aimed at providing a framework based on\ninterpretable attention mechanisms for imposing structured and non-structured\nsparsity in deep neural networks. For Cifar-100 experiments, we achieved the\nstate-of-the-art sparsity level and 2.91x speedup with competitive accuracy\ncompared to the best method. For MNIST and LeNet architecture we also achieved\nthe highest sparsity and speedup level. \n\n"}
{"id": "1901.02132", "contents": "Title: Spatial-Winograd Pruning Enabling Sparse Winograd Convolution Abstract: Deep convolutional neural networks (CNNs) are deployed in various\napplications but demand immense computational requirements. Pruning techniques\nand Winograd convolution are two typical methods to reduce the CNN computation.\nHowever, they cannot be directly combined because Winograd transformation fills\nin the sparsity resulting from pruning. Li et al. (2017) propose sparse\nWinograd convolution in which weights are directly pruned in the Winograd\ndomain, but this technique is not very practical because Winograd-domain\nretraining requires low learning rates and hence significantly longer training\ntime. Besides, Liu et al. (2018) move the ReLU function into the Winograd\ndomain, which can help increase the weight sparsity but requires changes in the\nnetwork structure. To achieve a high Winograd-domain weight sparsity without\nchanging network structures, we propose a new pruning method, spatial-Winograd\npruning. As the first step, spatial-domain weights are pruned in a structured\nway, which efficiently transfers the spatial-domain sparsity into the Winograd\ndomain and avoids Winograd-domain retraining. For the next step, we also\nperform pruning and retraining directly in the Winograd domain but propose to\nuse an importance factor matrix to adjust weight importance and weight\ngradients. This adjustment makes it possible to effectively retrain the pruned\nWinograd-domain network without changing the network structure. For the three\nmodels on the datasets of CIFAR10, CIFAR-100, and ImageNet, our proposed method\ncan achieve the Winograd domain sparsities of 63%, 50%, and 74%, respectively. \n\n"}
{"id": "1901.02425", "contents": "Title: Richer and Deeper Supervision Network for Salient Object Detection Abstract: Recent Salient Object Detection (SOD) systems are mostly based on\nConvolutional Neural Networks (CNNs). Specifically, Deeply Supervised Saliency\n(DSS) system has shown it is very useful to add short connections to the\nnetwork and supervising on the side output. In this work, we propose a new SOD\nsystem which aims at designing a more efficient and effective way to pass back\nglobal information. Richer and Deeper Supervision (RDS) is applied to better\ncombine features from each side output without demanding much extra\ncomputational space. Meanwhile, the backbone network used for SOD is normally\npre-trained on the object classification dataset, ImageNet. But the pre-trained\nmodel has been trained on cropped images in order to only focus on\ndistinguishing features within the region of the object. But the ignored\nbackground information is also significant in the task of SOD. We try to solve\nthis problem by introducing the training data designed for object detection. A\ncoarse global information is learned based on an entire image with its bounding\nbox before training on the SOD dataset. The large-scale of object images can\nslightly improve the performance of SOD. Our experiment shows the proposed RDS\nnetwork achieves the state-of-the-art results on five public SOD datasets. \n\n"}
{"id": "1901.02658", "contents": "Title: Improvement of charge resolution for radioactive heavy ions at\n  relativistic energies using a hybrid detector system Abstract: In typical nuclear physics experiments with radioactive ion beams (RIBs)\nselected by the in-flight separation technique, Si detectors or ionization\nchambers are usually equipped for the charge determination of RIBs. The\nobtained charge resolution relies on the performance of these detectors for\nenergy loss determination, and this affects the particle identification\ncapability of RIBs. We present an approach on improving the resolution of\ncharge measurement for heavy ions by using the abundant energy loss information\nfrom different types of existing detectors along the beam line. Without\naltering the beam line and detectors, this approach can improve the charge\nresolution by more than 12\\% relative to the multiple sampling ionization\nchamber of the best resolution. \n\n"}
{"id": "1901.02759", "contents": "Title: The GlueX Start Counter Detector Abstract: The design, simulation, fabrication, calibration, and performance of the\nGlueX Start Counter detector is described. The Start Counter was designed to\noperate at integrated rates of up to 9 MHz with a timing resolution in the\nrange of 500 to 825 ps (FWHM). The Start Counter provides excellent solid angle\ncoverage, a high degree of segmentation for background rejection, and can be\nutilized in the level 1 trigger for the experiment. It consists of a\ncylindrical array of 30 thin scintillators with pointed ends that bend towards\nthe beam line at the downstream end. Magnetic field insensitive silicon\nphotomultiplier detectors were used as the light sensors. \n\n"}
{"id": "1901.02844", "contents": "Title: Electron Radiated Power in Cyclotron Radiation Emission Spectroscopy\n  Experiments Abstract: The recently developed technique of Cyclotron Radiation Emission Spectroscopy\n(CRES) uses frequency information from the cyclotron motion of an electron in a\nmagnetic bottle to infer its kinetic energy. Here we derive the expected radio\nfrequency signal from an electron in a waveguide CRES apparatus from first\nprinciples. We demonstrate that the frequency-domain signal is rich in\ninformation about the electron's kinematic parameters, and extract a set of\nmeasurables that in a suitably designed system are sufficient for disentangling\nthe electron's kinetic energy from the rest of its kinematic features. This\nlays the groundwork for high-resolution energy measurements in future CRES\nexperiments, such as the Project 8 neutrino mass measurement. \n\n"}
{"id": "1901.03369", "contents": "Title: Progress toward Barium Tagging in High Pressure Xenon Gas with Single\n  Molecule Fluorescence Imaging Abstract: We present an update on the development of techniques to adapt Single\nMolecule Fluorescent Imaging for the tagging of individual barium ions in high\npressure xenon gas detectors, with the goal of realizing a background-free\nneutrinoless double beta decay technology. Previously reported progress is\nreviewed, including the recent demonstration of single barium dication\nsensitivity using SMFI. We then describe two important advances: 1) the\ndevelopment of a new class of custom barium sensing fluorescent dyes, which\nexhibit a significantly stronger response to barium than commercial calcium\nsensing compounds in aqueous solution; 2) the first demonstration of a\ndry-phase chemosensor for barium ions. This proceeding documents work presented\nat the 9th Symposium on Large TPCs for Rare Event Detection in Paris, France. \n\n"}
{"id": "1901.03546", "contents": "Title: Retrieving Similar E-Commerce Images Using Deep Learning Abstract: In this paper, we propose a deep convolutional neural network for learning\nthe embeddings of images in order to capture the notion of visual similarity.\nWe present a deep siamese architecture that when trained on positive and\nnegative pairs of images learn an embedding that accurately approximates the\nranking of images in order of visual similarity notion. We also implement a\nnovel loss calculation method using an angular loss metrics based on the\nproblems requirement. The final embedding of the image is combined\nrepresentation of the lower and top-level embeddings. We used fractional\ndistance matrix to calculate the distance between the learned embeddings in\nn-dimensional space. In the end, we compare our architecture with other\nexisting deep architecture and go on to demonstrate the superiority of our\nsolution in terms of image retrieval by testing the architecture on four\ndatasets. We also show how our suggested network is better than the other\ntraditional deep CNNs used for capturing fine-grained image similarities by\nlearning an optimum embedding. \n\n"}
{"id": "1901.03954", "contents": "Title: Auto-Retoucher(ART) - A framework for Background Replacement and Image\n  Editing Abstract: Replacing the background and simultaneously adjusting foreground objects is a\nchallenging task in image editing. Current techniques for generating such\nimages relies heavily on user interactions with image editing softwares, which\nis a tedious job for professional retouchers. To reduce their workload, some\nexciting progress has been made on generating images with a given background.\nHowever, these models can neither adjust the position and scale of the\nforeground objects, nor guarantee the semantic consistency between foreground\nand background. To overcome these limitations, we propose a framework --\nART(Auto-Retoucher), to generate images with sufficient semantic and spatial\nconsistency. Images are first processed by semantic matting and scene parsing\nmodules, then a multi-task verifier model will give two confidence scores for\nthe current background and position setting. We demonstrate that our jointly\noptimized verifier model successfully improves the visual consistency, and our\nART framework performs well on images with the human body as foregrounds. \n\n"}
{"id": "1901.04111", "contents": "Title: Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views Abstract: This paper addresses the problem of 3D pose estimation for multiple people in\na few calibrated camera views. The main challenge of this problem is to find\nthe cross-view correspondences among noisy and incomplete 2D pose predictions.\nMost previous methods address this challenge by directly reasoning in 3D using\na pictorial structure model, which is inefficient due to the huge state space.\nWe propose a fast and robust approach to solve this problem. Our key idea is to\nuse a multi-way matching algorithm to cluster the detected 2D poses in all\nviews. Each resulting cluster encodes 2D poses of the same person across\ndifferent views and consistent correspondences across the keypoints, from which\nthe 3D pose of each person can be effectively inferred. The proposed convex\noptimization based multi-way matching algorithm is efficient and robust against\nmissing and false detections, without knowing the number of people in the\nscene. Moreover, we propose to combine geometric and appearance cues for\ncross-view matching. The proposed approach achieves significant performance\ngains from the state-of-the-art (96.3% vs. 90.6% and 96.9% vs. 88% on the\nCampus and Shelf datasets, respectively), while being efficient for real-time\napplications. \n\n"}
{"id": "1901.04988", "contents": "Title: A Survey of FPGA Based Deep Learning Accelerators: Challenges and\n  Opportunities Abstract: With the rapid development of in-depth learning, neural network and deep\nlearning algorithms have been widely used in various fields, e.g., image, video\nand voice processing. However, the neural network model is getting larger and\nlarger, which is expressed in the calculation of model parameters. Although a\nwealth of existing efforts on GPU platforms currently used by researchers for\nimproving computing performance, dedicated hardware solutions are essential and\nemerging to provide advantages over pure software solutions. In this paper, we\nsystematically investigate the neural network accelerator based on FPGA.\nSpecifically, we respectively review the accelerators designed for specific\nproblems, specific algorithms, algorithm features, and general templates. We\nalso compared the design and implementation of the accelerator based on FPGA\nunder different devices and network models and compared it with the versions of\nCPU and GPU. Finally, we present to discuss the advantages and disadvantages of\naccelerators on FPGA platforms and to further explore the opportunities for\nfuture research. \n\n"}
{"id": "1901.05031", "contents": "Title: Analysis and algorithms for $\\ell_p$-based semi-supervised learning on\n  graphs Abstract: This paper addresses theory and applications of $\\ell_p$-based Laplacian\nregularization in semi-supervised learning. The graph $p$-Laplacian for $p>2$\nhas been proposed recently as a replacement for the standard ($p=2$) graph\nLaplacian in semi-supervised learning problems with very few labels, where\nLaplacian learning is degenerate.\n  In the first part of the paper we prove new discrete to continuum convergence\nresults for $p$-Laplace problems on $k$-nearest neighbor ($k$-NN) graphs, which\nare more commonly used in practice than random geometric graphs. Our analysis\nshows that, on $k$-NN graphs, the $p$-Laplacian retains information about the\ndata distribution as $p\\to \\infty$ and Lipschitz learning ($p=\\infty$) is\nsensitive to the data distribution. This situation can be contrasted with\nrandom geometric graphs, where the $p$-Laplacian forgets the data distribution\nas $p\\to \\infty$. We also present a general framework for proving discrete to\ncontinuum convergence results in graph-based learning that only requires\npointwise consistency and monotonicity.\n  In the second part of the paper, we develop fast algorithms for solving the\nvariational and game-theoretic $p$-Laplace equations on weighted graphs for\n$p>2$. We present several efficient and scalable algorithms for both\nformulations, and present numerical results on synthetic data indicating their\nconvergence properties. Finally, we conduct extensive numerical experiments on\nthe MNIST, FashionMNIST and EMNIST datasets that illustrate the effectiveness\nof the $p$-Laplacian formulation for semi-supervised learning with few labels.\nIn particular, we find that Lipschitz learning ($p=\\infty$) performs well with\nvery few labels on $k$-NN graphs, which experimentally validates our\ntheoretical findings that Lipschitz learning retains information about the data\ndistribution (the unlabeled data) on $k$-NN graphs. \n\n"}
{"id": "1901.05495", "contents": "Title: An Underwater Image Enhancement Benchmark Dataset and Beyond Abstract: Underwater image enhancement has been attracting much attention due to its\nsignificance in marine engineering and aquatic robotics. Numerous underwater\nimage enhancement algorithms have been proposed in the last few years. However,\nthese algorithms are mainly evaluated using either synthetic datasets or few\nselected real-world images. It is thus unclear how these algorithms would\nperform on images acquired in the wild and how we could gauge the progress in\nthe field. To bridge this gap, we present the first comprehensive perceptual\nstudy and analysis of underwater image enhancement using large-scale real-world\nimages. In this paper, we construct an Underwater Image Enhancement Benchmark\n(UIEB) including 950 real-world underwater images, 890 of which have the\ncorresponding reference images. We treat the rest 60 underwater images which\ncannot obtain satisfactory reference images as challenging data. Using this\ndataset, we conduct a comprehensive study of the state-of-the-art underwater\nimage enhancement algorithms qualitatively and quantitatively. In addition, we\npropose an underwater image enhancement network (called Water-Net) trained on\nthis benchmark as a baseline, which indicates the generalization of the\nproposed UIEB for training Convolutional Neural Networks (CNNs). The benchmark\nevaluations and the proposed Water-Net demonstrate the performance and\nlimitations of state-of-the-art algorithms, which shed light on future research\nin underwater image enhancement. The dataset and code are available at\nhttps://li-chongyi.github.io/proj_benchmark.html. \n\n"}
{"id": "1901.06185", "contents": "Title: A new gas-based proton-recoil telescope for quasi-absolute neutron flux\n  measurements between 0.2 and 2 MeV neutron energy Abstract: Absolute measurements of neutron flux are an essential prerequisite of\nneutron-induced cross section measurements, neutron beam lines characterization\nand dosimetric investigations. A new gaseous detector has been developed for\nmeasurements of 0.2 to 2 MeV neutron flux based on proton-recoil process. The\ndetector, consisting of two segmented ionization chambers read by Micromegas\ntechnology, has beed conceived to provide quasi-absolute neutron flux\nmeasurements with an accuracy of \\simeq3%. The gas pressure flexibility makes\nthe telescope non sensitive to {\\gamma} and electrons background, and therefore\nadvantageous over semi-conductor materials as a neutron flux instrument. The\nadjustable gas pressure and H-sample thickness, the use of Micromegas\ntechnology and the tracking capabilities allows the detection of neutrons on a\nlarge dynamical range and down to 200 keV with a good rejection of scattered\nneutron events and random background \n\n"}
{"id": "1901.06322", "contents": "Title: Learning Spatial Pyramid Attentive Pooling in Image Synthesis and\n  Image-to-Image Translation Abstract: Image synthesis and image-to-image translation are two important generative\nlearning tasks. Remarkable progress has been made by learning Generative\nAdversarial Networks (GANs)~\\cite{goodfellow2014generative} and\ncycle-consistent GANs (CycleGANs)~\\cite{zhu2017unpaired} respectively. This\npaper presents a method of learning Spatial Pyramid Attentive Pooling (SPAP)\nwhich is a novel architectural unit and can be easily integrated into both\ngenerators and discriminators in GANs and CycleGANs. The proposed SPAP\nintegrates Atrous spatial pyramid~\\cite{chen2018deeplab}, a proposed cascade\nattention mechanism and residual connections~\\cite{he2016deep}. It leverages\nthe advantages of the three components to facilitate effective end-to-end\ngenerative learning: (i) the capability of fusing multi-scale information by\nASPP; (ii) the capability of capturing relative importance between both spatial\nlocations (especially multi-scale context) or feature channels by attention;\n(iii) the capability of preserving information and enhancing optimization\nfeasibility by residual connections. Coarse-to-fine and fine-to-coarse SPAP are\nstudied and intriguing attention maps are observed in both tasks. In\nexperiments, the proposed SPAP is tested in GANs on the Celeba-HQ-128\ndataset~\\cite{karras2017progressive}, and tested in CycleGANs on the\nImage-to-Image translation datasets including the Cityscape\ndataset~\\cite{cordts2016cityscapes}, Facade and Aerial Maps\ndataset~\\cite{zhu2017unpaired}, both obtaining better performance. \n\n"}
{"id": "1901.06706", "contents": "Title: Visual Entailment: A Novel Task for Fine-Grained Image Understanding Abstract: Existing visual reasoning datasets such as Visual Question Answering (VQA),\noften suffer from biases conditioned on the question, image or answer\ndistributions. The recently proposed CLEVR dataset addresses these limitations\nand requires fine-grained reasoning but the dataset is synthetic and consists\nof similar objects and sentence structures across the dataset.\n  In this paper, we introduce a new inference task, Visual Entailment (VE) -\nconsisting of image-sentence pairs whereby a premise is defined by an image,\nrather than a natural language sentence as in traditional Textual Entailment\ntasks. The goal of a trained VE model is to predict whether the image\nsemantically entails the text. To realize this task, we build a dataset SNLI-VE\nbased on the Stanford Natural Language Inference corpus and Flickr30k dataset.\nWe evaluate various existing VQA baselines and build a model called Explainable\nVisual Entailment (EVE) system to address the VE task. EVE achieves up to 71%\naccuracy and outperforms several other state-of-the-art VQA based models.\nFinally, we demonstrate the explainability of EVE through cross-modal attention\nvisualizations. The SNLI-VE dataset is publicly available at\nhttps://github.com/ necla-ml/SNLI-VE. \n\n"}
{"id": "1901.07076", "contents": "Title: Robust Angular Local Descriptor Learning Abstract: In recent years, the learned local descriptors have outperformed handcrafted\nones by a large margin, due to the powerful deep convolutional neural network\narchitectures such as L2-Net [1] and triplet based metric learning [2].\nHowever, there are two problems in the current methods, which hinders the\noverall performance. Firstly, the widely-used margin loss is sensitive to\nincorrect correspondences, which are prevalent in the existing local descriptor\nlearning datasets. Second, the L2 distance ignores the fact that the feature\nvectors have been normalized to unit norm. To tackle these two problems and\nfurther boost the performance, we propose a robust angular loss which 1) uses\ncosine similarity instead of L2 distance to compare descriptors and 2) relies\non a robust loss function that gives smaller penalty to triplets with negative\nrelative similarity. The resulting descriptor shows robustness on different\ndatasets, reaching the state-of-the-art result on Brown dataset , as well as\ndemonstrating excellent generalization ability on the Hpatches dataset and a\nWide Baseline Stereo dataset. \n\n"}
{"id": "1901.07172", "contents": "Title: Efficient Image Splicing Localization via Contrastive Feature Extraction Abstract: In this work, we propose a new data visualization and clustering technique\nfor discovering discriminative structures in high-dimensional data. This\ntechnique, referred to as cPCA++, utilizes the fact that the interesting\nfeatures of a \"target\" dataset may be obscured by high variance components\nduring traditional PCA. By analyzing what is referred to as a \"background\"\ndataset (i.e., one that exhibits the high variance principal components but not\nthe interesting structures), our technique is capable of efficiently\nhighlighting the structure that is unique to the \"target\" dataset. Similar to\nanother recently proposed algorithm called \"contrastive PCA\" (cPCA), the\nproposed cPCA++ method identifies important dataset specific patterns that are\nnot detected by traditional PCA in a wide variety of settings. However, the\nproposed cPCA++ method is significantly more efficient than cPCA, because it\ndoes not require the parameter sweep in the latter approach. We applied the\ncPCA++ method to the problem of image splicing localization. In this\napplication, we utilize authentic edges as the background dataset and the\nspliced edges as the target dataset. The proposed method is significantly more\nefficient than state-of-the-art methods, as the former does not require\niterative updates of filter weights via stochastic gradient descent and\nbackpropagation, nor the training of a classifier. Furthermore, the cPCA++\nmethod is shown to provide performance scores comparable to the\nstate-of-the-art Multi-task Fully Convolutional Network (MFCN). \n\n"}
{"id": "1901.07821", "contents": "Title: Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff Abstract: Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example. \n\n"}
{"id": "1901.07827", "contents": "Title: Towards Compact ConvNets via Structure-Sparsity Regularized Filter\n  Pruning Abstract: The success of convolutional neural networks (CNNs) in computer vision\napplications has been accompanied by a significant increase of computation and\nmemory costs, which prohibits its usage on resource-limited environments such\nas mobile or embedded devices. To this end, the research of CNN compression has\nrecently become emerging. In this paper, we propose a novel filter pruning\nscheme, termed structured sparsity regularization (SSR), to simultaneously\nspeedup the computation and reduce the memory overhead of CNNs, which can be\nwell supported by various off-the-shelf deep learning libraries. Concretely,\nthe proposed scheme incorporates two different regularizers of structured\nsparsity into the original objective function of filter pruning, which fully\ncoordinates the global outputs and local pruning operations to adaptively prune\nfilters. We further propose an Alternative Updating with Lagrange Multipliers\n(AULM) scheme to efficiently solve its optimization. AULM follows the principle\nof ADMM and alternates between promoting the structured sparsity of CNNs and\noptimizing the recognition loss, which leads to a very efficient solver (2.5x\nto the most recent work that directly solves the group sparsity-based\nregularization). Moreover, by imposing the structured sparsity, the online\ninference is extremely memory-light, since the number of filters and the output\nfeature maps are simultaneously reduced. The proposed scheme has been deployed\nto a variety of state-of-the-art CNN structures including LeNet, AlexNet, VGG,\nResNet and GoogLeNet over different datasets. Quantitative results demonstrate\nthat the proposed scheme achieves superior performance over the\nstate-of-the-art methods. We further demonstrate the proposed compression\nscheme for the task of transfer learning, including domain adaptation and\nobject detection, which also show exciting performance gains over the\nstate-of-the-arts. \n\n"}
{"id": "1901.08097", "contents": "Title: Can Adversarial Networks Hallucinate Occluded People With a Plausible\n  Aspect? Abstract: When you see a person in a crowd, occluded by other persons, you miss visual\ninformation that can be used to recognize, re-identify or simply classify him\nor her. You can imagine its appearance given your experience, nothing more.\nSimilarly, AI solutions can try to hallucinate missing information with\nspecific deep learning architectures, suitably trained with people with and\nwithout occlusions. The goal of this work is to generate a complete image of a\nperson, given an occluded version in input, that should be a) without occlusion\nb) similar at pixel level to a completely visible people shape c) capable to\nconserve similar visual attributes (e.g. male/female) of the original one. For\nthe purpose, we propose a new approach by integrating the state-of-the-art of\nneural network architectures, namely U-nets and GANs, as well as discriminative\nattribute classification nets, with an architecture specifically designed to\nde-occlude people shapes. The network is trained to optimize a Loss function\nwhich could take into account the aforementioned objectives. As well we propose\ntwo datasets for testing our solution: the first one, occluded RAP, created\nautomatically by occluding real shapes of the RAP dataset (which collects also\nattributes of the people aspect); the second is a large synthetic dataset, AiC,\ngenerated in computer graphics with data extracted from the GTA video game,\nthat contains 3D data of occluded objects by construction. Results are\nimpressive and outperform any other previous proposal. This result could be an\ninitial step to many further researches to recognize people and their behavior\nin an open crowded world. \n\n"}
{"id": "1901.09321", "contents": "Title: Fixup Initialization: Residual Learning Without Normalization Abstract: Normalization layers are a staple in state-of-the-art deep neural network\narchitectures. They are widely believed to stabilize training, enable higher\nlearning rate, accelerate convergence and improve generalization, though the\nreason for their effectiveness is still an active research topic. In this work,\nwe challenge the commonly-held beliefs by showing that none of the perceived\nbenefits is unique to normalization. Specifically, we propose fixed-update\ninitialization (Fixup), an initialization motivated by solving the exploding\nand vanishing gradient problem at the beginning of training via properly\nrescaling a standard initialization. We find training residual networks with\nFixup to be as stable as training with normalization -- even for networks with\n10,000 layers. Furthermore, with proper regularization, Fixup enables residual\nnetworks without normalization to achieve state-of-the-art performance in image\nclassification and machine translation. \n\n"}
{"id": "1901.09447", "contents": "Title: Open Source Face Recognition Performance Evaluation Package Abstract: Biometrics-related research has been accelerated significantly by deep\nlearning technology. However, there are limited open-source resources to help\nresearchers evaluate their deep learning-based biometrics algorithms\nefficiently, especially for the face recognition tasks. In this work, we design\nand implement a light-weight, maintainable, scalable, generalizable, and\nextendable face recognition evaluation toolbox named FaRE that supports both\nonline and offline evaluation to provide feedback to algorithm development and\naccelerate biometrics-related research. FaRE consists of a set of evaluation\nmetric functions and provides various APIs for commonly-used face recognition\ndatasets including LFW, CFP, UHDB31, and IJB-series datasets, which can be\neasily extended to include other customized datasets. The package and the\npre-trained baseline models will be released for public academic research use\nafter obtaining university approval. \n\n"}
{"id": "1901.10277", "contents": "Title: High-Quality Self-Supervised Deep Image Denoising Abstract: We describe a novel method for training high-quality image denoising models\nbased on unorganized collections of corrupted images. The training does not\nneed access to clean reference images, or explicit pairs of corrupted images,\nand can thus be applied in situations where such data is unacceptably expensive\nor impossible to acquire. We build on a recent technique that removes the need\nfor reference data by employing networks with a \"blind spot\" in the receptive\nfield, and significantly improve two key aspects: image quality and training\nefficiency. Our result quality is on par with state-of-the-art neural network\ndenoisers in the case of i.i.d. additive Gaussian noise, and not far behind\nwith Poisson and impulse noise. We also successfully handle cases where\nparameters of the noise model are variable and/or unknown in both training and\nevaluation data. \n\n"}
{"id": "1901.11009", "contents": "Title: Cryogenic light detectors with enhanced performance for rare events\n  physics Abstract: We have developed and tested a new way of coupling bolometric light detectors\nto scintillating crystal bolometers based upon simply resting the light\ndetector on the crystal surface, held in position only by gravity. This\nstraightforward mounting results in three important improvements: (1) it\ndecreases the amount of non-active materials needed to assemble the detector,\n(2) it substantially increases the light collection efficiency by minimizing\nthe light losses induced by the mounting structure, and (3) it enhances the\nthermal signal induced in the light detector thanks to the extremely weak\nthermal link to the thermal bath. We tested this new technique with a 16 cm$^2$\nGe light detector with thermistor readout sitting on the surface of a large\nTeO$_2$ bolometer. The light collection efficiency was increased by greater\nthan 50\\% compared to previously tested alternative mountings. We obtained a\nbaseline energy resolution on the light detector of 20~eV RMS that, together\nwith increased light collection, enabled us to obtain the best $\\alpha$ vs\n$\\beta/\\gamma$ discrimination ever obtained with massive TeO$_2$ crystals. At\nthe same time we achieved rise and decay times of 0.8 and 1.6 ms, respectively.\nThis superb performance meets all of the requirements for the CUPID (CUORE\nUpgrade with Particle IDentification) experiment, which is a 1-ton\nscintillating bolometer follow up to CUORE. \n\n"}
{"id": "1901.11179", "contents": "Title: Human Face Expressions from Images - 2D Face Geometry and 3D Face Local\n  Motion versus Deep Neural Features Abstract: Several computer algorithms for recognition of visible human emotions are\ncompared at the web camera scenario using CNN/MMOD face detector. The\nrecognition refers to four face expressions: smile, surprise, anger, and\nneutral. At the feature extraction stage, the following three concepts of face\ndescription are confronted: (a) static 2D face geometry represented by its 68\ncharacteristic landmarks (FP68); (b) dynamic 3D geometry defined by motion\nparameters for eight distinguished face parts (denoted as AU8) of personalized\nCandide-3 model; (c) static 2D visual description as 2D array of gray scale\npixels (known as facial raw image). At the classification stage, the\nperformance of two major models are analyzed: (a) support vector machine (SVM)\nwith kernel options; (b) convolutional neural network (CNN) with variety of\nrelevant tensor processing layers and blocks of them. The models are trained\nfor frontal views of human faces while they are tested for arbitrary head\nposes. For geometric features, the success rate (accuracy) indicate nearly\ntriple increase of performance of CNN with respect to SVM classifiers. For raw\nimages, CNN outperforms in accuracy its best geometric counterpart (AU/CNN) by\nabout 30 percent while the best SVM solutions are inferior nearly four times.\nFor F-score the high advantage of raw/CNN over geometric/CNN and geometric/SVM\nis observed, as well. We conclude that contrary to CNN based emotion\nclassifiers, the generalization capability wrt human head pose is for SVM based\nemotion classifiers poor. \n\n"}
{"id": "1901.11342", "contents": "Title: Possibilities of future double beta decay experiments to investigate\n  inverted and normal ordering region of neutrino mass Abstract: An overview of modern experiments on the search for neutrinoless double decay\nis presented. The obtained limits on the effective mass of the Majorana\nneutrino $\\langle m_{\\nu} \\rangle$ are discussed taking into account the\nuncertainties in the value of the nuclear matrix elements (NMEs) and the value\nof the axial-vector constant $g_A$. Predictions for the values of $\\langle\nm_{\\nu} \\rangle$ from the results of oscillation experiments and modern\ncosmological data are presented. The possibilities of the next generation\nexperiments with sensitivity to $\\langle m_{\\nu} \\rangle$ at the level of\n$\\sim$ 10-50 meV (studying mainly the inverted ordering (IO) region) are\ndiscussed. %Description of the most developed and promising projects is\npresented. The prospects for studying the normal ordering (NO) region are\ndiscussed too. It is shown that the possibilities of studying the NO depend on\nthe mass of the lightest neutrino m$_0$. In the limiting case of small mass\n(m$_0$ $\\le$ 0.1 meV), the values of $\\langle m_{\\nu} \\rangle$ $\\approx$ 1-4\nmeV are predicted, which makes the study of this region inaccessible by the\nnext generation experiments. But there is an allowed region of m$_0$ (7-30 meV)\nin the framework of NO, where the predicted values for $\\langle m_{\\nu}\n\\rangle$ could be $\\sim$ 10-30 meV and that is quite achievable for the next\ngeneration experiments. The possibility to rich in the future sensitivity to\n$\\langle m_{\\nu} \\rangle$ at the level of $\\sim$ 1-10 meV is also discussed. \n\n"}
{"id": "1901.11382", "contents": "Title: Learning to Clean: A GAN Perspective Abstract: In the big data era, the impetus to digitize the vast reservoirs of data\ntrapped in unstructured scanned documents such as invoices, bank documents and\ncourier receipts has gained fresh momentum. The scanning process often results\nin the introduction of artifacts such as background noise, blur due to camera\nmotion, watermarkings, coffee stains, or faded text. These artifacts pose many\nreadability challenges to current text recognition algorithms and significantly\ndegrade their performance. Existing learning based denoising techniques require\na dataset comprising of noisy documents paired with cleaned versions. In such\nscenarios, a model can be trained to generate clean documents from noisy\nversions. However, very often in the real world such a paired dataset is not\navailable, and all we have for training our denoising model are unpaired sets\nof noisy and clean images. This paper explores the use of GANs to generate\ndenoised versions of the noisy documents. In particular, where paired\ninformation is available, we formulate the problem as an image-to-image\ntranslation task i.e, translating a document from noisy domain ( i.e.,\nbackground noise, blurred, faded, watermarked ) to a target clean document\nusing Generative Adversarial Networks (GAN). However, in the absence of paired\nimages for training, we employed CycleGAN which is known to learn a mapping\nbetween the distributions of the noisy images to the denoised images using\nunpaired data to achieve image-to-image translation for cleaning the noisy\ndocuments. We compare the performance of CycleGAN for document cleaning tasks\nusing unpaired images with a Conditional GAN trained on paired data from the\nsame dataset. Experiments were performed on a public document dataset on which\ndifferent types of noise were artificially induced, results demonstrate that\nCycleGAN learns a more robust mapping from the space of noisy to clean\ndocuments. \n\n"}
{"id": "1901.11383", "contents": "Title: Automatic Information Extraction from Piping and Instrumentation\n  Diagrams Abstract: One of the most common modes of representing engineering schematics are\nPiping and Instrumentation diagrams (P&IDs) that describe the layout of an\nengineering process flow along with the interconnected process equipment. Over\nthe years, P&ID diagrams have been manually generated, scanned and stored as\nimage files. These files need to be digitized for purposes of inventory\nmanagement and updation, and easy reference to different components of the\nschematics. There are several challenging vision problems associated with\ndigitizing real world P&ID diagrams. Real world P&IDs come in several different\nresolutions, and often contain noisy textual information. Extraction of\ninstrumentation information from these diagrams involves accurate detection of\nsymbols that frequently have minute visual differences between them.\nIdentification of pipelines that may converge and diverge at different points\nin the image is a further cause for concern. Due to these reasons, to the best\nof our knowledge, no system has been proposed for end-to-end data extraction\nfrom P&ID diagrams. However, with the advent of deep learning and the\nspectacular successes it has achieved in vision, we hypothesized that it is now\npossible to re-examine this problem armed with the latest deep learning models.\nTo that end, we present a novel pipeline for information extraction from P&ID\nsheets via a combination of traditional vision techniques and state-of-the-art\ndeep learning models to identify and isolate pipeline codes, pipelines, inlets\nand outlets, and for detecting symbols. This is followed by association of the\ndetected components with the appropriate pipeline. The extracted pipeline\ninformation is used to populate a tree-like data-structure for capturing the\nstructure of the piping schematics. We evaluated proposed method on a real\nworld dataset of P&ID sheets obtained from an oil firm and have obtained\npromising results. \n\n"}
{"id": "hep-ex/0105015", "contents": "Title: A new approach to neutrino and WIMP detection using telecom-grade\n  electrooptic and fiber-optic components Abstract: The dense energy deposition from a low-energy nuclear recoil produces, via\nthe thermoacoustic effect, a brief yet intense pressure pulse that can be used\nfor WIMP or neutrino detection in some fiber-optic devices sensitive to\nacoustooptic disturbances. Several possible detection schemes are described:\nall of them are inspired by modern fiber-optic sensor technologies and share\ncommon characteristics of low-cost and expected insensitivity to\nminimum-ionizing backgrounds. \n\n"}
{"id": "hep-ex/0208016", "contents": "Title: Nuclear shadowing at low Q^2 Abstract: We re-examine the role of vector meson dominance in nuclear shadowing at low\nQ^2. We find that models which incorporate both vector meson and partonic\nmechanisms are consistent with both the magnitude and the Q^2 slope of the\nshadowing data. \n\n"}
{"id": "hep-ex/0304013", "contents": "Title: A First Mass Production of Gas Electron Multipliers Abstract: We report on the manufacture of a first batch of approximately 2,000 Gas\nElectron Multipliers (GEMs) using 3M's fully automated roll to roll flexible\ncircuit production line. This process allows low-cost, reproducible fabrication\nof a high volume of GEMs of dimensions up to 30$\\times$30 cm$^{2}$. First tests\nindicate that the resulting GEMs have optimal properties as radiation\ndetectors. Production techniques and preliminary measurements of GEM\nperformance are described. This now demonstrated industrial capability should\nhelp further establish the prominence of micropattern gas detectors in\naccelerator based and non-accelerator particle physics, imaging and\nphotodetection. \n\n"}
{"id": "hep-ex/0608010", "contents": "Title: Search for tribaryon production in alpha-particles interactions with\n  protons at intermediate energies Abstract: The analysis of the data on the reactions $^4$Hep -> pppnn and $^4$Hep ->\ndppn obtained at the 2-m ITEP liquid-hydrogen bubble chamber exposed to beams\nof $^4$He nuclei with momenta of 2.7 and 5 GeV/c revealed a narrow structures\nin the effective-mass spectra of the trinucleon system (NNN) at 2.99 GeV (for\nisospin T=3/2) as well as at 3.04 GeV (T=1/2). The masses of the observed\nstructures are consistent with the masses of low-lying tribaryon resonances\npredicted by some theoretical models. Possible resonance nature of the\nstructures observed is discussed. \n\n"}
{"id": "hep-lat/0209067", "contents": "Title: Gauge-invariant two- and three- density correlators Abstract: Gauge-invariant spatial correlations between two and three quarks inside a\nhadron are measured within quenched and unquenched QCD. These correlators\nprovide information on the shape and multipole moments of the pion, the rho,\nthe nucleon and the $\\Delta$. \n\n"}
{"id": "hep-lat/0409126", "contents": "Title: Chiral dynamics in QCD at finite chemical potential Abstract: We present the renormalized Taylor expansion of the chiral condensate with\nchemical potential. The relation with the Taylor coefficients of meson\nsusceptibilities and the quark mass dependence of the quark number\nsusceptibilities are obtained through chiral Ward identities and Maxwell\nrelations. The continuum limit is obtained in quenched and two flavour QCD. In\nNf=2 QCD, the quadratic response coefficients (QRCs) of the chiral condensate\nto chemical potential indicate that pion fluctuations decrease at finite\nchemical potential toward the critical end point. We discuss how these\nobservables can be used to test other features of the QCD phase diagram. \n\n"}
{"id": "hep-ph/0003210", "contents": "Title: Determining the Flavour Content of the Low-Energy Solar Neutrino Flux Abstract: We study the sensitivity of the HELLAZ and Borexino solar neutrino\nexperiments on discriminating the neutrino species nu_e, anti-nu_e,\nnu_{mu,tau}, anti-nu_{mu,tau}, and nu_{sterile} using the difference in the\nrecoil electron kinetic energy spectra in elastic neutrino-electron scattering.\nWe find that one can observe a non-vanishing nu_{mu,tau} component in the solar\nneutrino flux, especially when the nu_e survival probability is low. Also, if\nthe data turn out to be consistent with nu_e <-> nu_{mu,tau} oscillations, an\nanti-nu_e component can be excluded effectively. \n\n"}
{"id": "hep-ph/0008119", "contents": "Title: Bremsstrahlung from a Microscopic Model of Relativistic Heavy Ion\n  Collisions Abstract: We compute bremsstrahlung arising from the acceleration of individual charged\nbaryons and mesons during the time evolution of high-energy Au+Au collisions at\nthe Relativistic Heavy Ion Collider using a microscopic transport model. We\nelucidate the connection between bremsstrahlung and charge stopping by\ncolliding artificial pure proton on pure neutron nuclei. From the intensity of\nlow energy bremsstrahlung, the time scale and the degree of stopping could be\naccurately extracted without measuring any hadronic observables. \n\n"}
{"id": "hep-ph/0111251", "contents": "Title: Nucleon Charge and Magnetization Densities Abstract: A Fourier-Bessel analysis is used to fit charge and magnetization densities\nto data for the nucleon Sachs form factors. The neutron and proton\nmagnetization densities are very similar, but the proton charge density is\nsignificantly softer. A useful measurement of the neutron charge density is\nobtained, although the relative uncertainty in the interior will remain\nsubstantially larger than for the other densities until precise new data at\nhigher Q^2 become available. \n\n"}
{"id": "hep-ph/0210391", "contents": "Title: Minijet Initial Conditions For Non-Equilibrium Parton Evolution at RHIC\n  and LHC Abstract: An important ingredient for the non-equilibrium evolution of partons at RHIC\nand LHC is to have some physically reasonable initial conditions for the single\nparticle phase space distribution functions for the partons. We consider\nseveral plausible parametrizations of initial conditions for the single\nparticle distribution function f_i(x,p) and fix the parameters by matching \\int\nf(x,p)p^\\mu d \\sigma_\\mu to the invariant momentum space semi-hard parton\ndistributions obtained using perturbative QCD (pQCD), as well as fitting low\norder moments of the distribution function. We consider parametrizations of\nf_i(x,p) with both boost invariant and boost non-invariant assumptions. We\ndetermine the initial number density, energy density and the corresponding\n(effective) temperature of the minijet plasma at RHIC and LHC energies. For a\nboost non-invariant minijet phase-space distribution function we obtain ~\n30(140) /fm^3 as the initial number density, ~ 50(520) GeV/fm^3 as the initial\nenergy density and ~ 520(930) MeV as the corresponding initial effective\ntemperature at RHIC(LHC). \n\n"}
{"id": "hep-ph/0305224", "contents": "Title: Are direct photons a clean signal of a thermalized quark gluon plasma? Abstract: Direct photon production from a quark gluon plasma (QGP) in thermal\nequilibrium is studied directly in real time. In contrast to the usual S-matrix\ncalculations, the real time approach is valid for a QGP that formed and reached\nLTE a short time after a collision and of finite lifetime ($\\sim 10-20\n\\mathrm{fm}/c$ as expected at RHIC or LHC). We point out that during such\nfinite QGP lifetime the spectrum of emitted photons carries information on the\ninitial state. There is an inherent ambiguity in separating the virtual from\nthe observable photons during the transient evolution of the QGP. We propose a\nreal time formulation to extract the photon yield which includes the initial\nstage of formation of the QGP parametrized by an effective time scale of\nformation $\\Gamma^{-1}$. This formulation coincides with the S-matrix approach\nin the infinite lifetime limit. It allows to separate the virtual cloud as well\nas the observable photons emitted during the pre- equilibrium stage from the\nyield during the QGP lifetime. We find that the lowest order contribution\n$\\mathcal{O}(\\alpha_{em})$ which does \\emph{not} contribute to the S-matrix\napproach, is of the same order of or larger than the S-matrix contribution\nduring the lifetime of the QGP for a typical formation time $\\sim 1\n\\mathrm{fm}/c$. The yield for momenta $\\gtrsim 3 \\mathrm{Gev}/c$ features a\npower law fall-off $\\sim T^3 \\Gamma^2/k^{5}$ and is larger than that obtained\nwith the S-matrix for momenta $\\geq 4 \\mathrm{Gev}/c$. We provide a\ncomprehensive numerical comparison between the real time and S-matrix yields\nand study the dynamics of the build-up of the photon cloud and the different\ncontributions to the radiative energy loss. The reliability of the current\nestimates on photon emission is discussed. \n\n"}
{"id": "hep-ph/0308234", "contents": "Title: Summary of Spin Physics Parallel Sessions Abstract: We summarize the activities in the spin physics parallel sessions of the\n$8^{\\rm th}$ conference on intersections between particle and nuclear physics. \n\n"}
{"id": "hep-ph/0403093", "contents": "Title: The Narrow $\\Theta_5$ Pentaquark As The First Non-planar Hadron With the\n  Diamond Structure And Negative Parity Abstract: Using the picture of the flux tube model, we propose that the $\\Theta_5$\npentaquark as the first candidate of the three-dimensional non-planar hadron\nwith the extremely stable diamond structure. The up and down quarks lie at the\ncorners of the diamond while the anti-strange quark sits in the center. Various\nun-excited color flux tubes between the five quarks bind them into a stable and\nnarrow color-singlet. Such a configuration allows the lowest state having the\nnegative parity naturally. The decay of the $\\Theta_5$ pentaquark into the\nnucleon and kaon requires the breakup of the non-planar diamond configuration\ninto two conventional planar hadrons, which involves some kind of structural\nphase transition as in the condensed matter physics. Hence the width of the\n$\\Theta^+$ pentaquark should be narrow despite that it lies above the kaon\nnucleon threshold. We suggest that future lattice QCD calculation adopt\nnon-planar interpolating currents to explore the underlying structure of the\n$\\Theta_5$ pentaquark. \n\n"}
{"id": "hep-ph/0501222", "contents": "Title: Production of Forward Rapidity Photons in High Energy Heavy Ion\n  Collisions Abstract: We consider production of prompt photons in high energy gold-gold and\ndeuteron-gold collisions in the forward rapidity region of RHIC ($y \\sim 3.8$).\nIn this kinematics, the projectile partons typically have large $x_{bj}$ while\nthe target partons are mostly at very small $x_{bj}$ so that the primary\npartonic collisions involve valence quarks from the projectile and gluons from\nthe target. We take the target nucleus to be a Color Glass Condensate while the\nprojectile deuteron or nucleus is treated as a dilute system of partons. We\nshow that the photon production cross section can be written as a convolution\nof a quark-nucleus scattering cross section, involving a quark anti-quark\ndipole, with the Leading Order quark-photon fragmentation function. We consider\ndifferent models of the quark anti-quark dipole and show that measurement of\nphotons in the forward rapidity region at RHIC can distinguish between\ndifferent parameterizations of the dipole cross section as well as help clarify\nthe role of parton coalescence models in hadron production at RHIC. \n\n"}
{"id": "hep-ph/0703228", "contents": "Title: Deconfinement transition for nonzero baryon density in the Field\n  Correlator Method Abstract: Deconfinement phase transition due to disappearance of confining\ncolorelectric field correlators is described using nonperturbative equation of\nstate. The resulting transition temperature $T_c(\\mu)$ at any chemical\npotential $\\mu$ is expressed in terms of the change of gluonic condensate\n$\\Delta G_2$ and absolute value of Polyakov loop $L_{fund} (T_c)$, known from\nlattice and analytic data, and is in good agreement with lattice data for\n$\\Delta G_2 \\approx 0.0035 $ GeV$^4$. E.g. $T_c(0) =0.27; 0.19; 0.17$ GeV for\n$n_f=0,2,3$ respectively. \n\n"}
{"id": "nucl-ex/0004005", "contents": "Title: Signals for a Transition from Surface to Bulk Emission in Thermal\n  Multifragmentation Abstract: Excitation-energy-gated two-fragment correlation functions have been studied\nbetween 2 to 9A MeV of excitation energy for equilibrium-like sources formed in\n$\\pi^-$ and p + $^{197}$Au reactions at beam momenta of 8,9.2 and 10.2 GeV/c.\nComparison of the data to an N-body Coulomb-trajectory code shows a decrease of\none order of magnitude in the fragment emission time in the excitation energy\ninterval 2-5A MeV, followed by a nearly constant breakup time at higher\nexcitation energy. The observed decrease in emission time is shown to be\nstrongly correlated with the increase of the fragment emission probability, and\nthe onset of thermally-induced radial expansion. This result is interpreted as\nevidence consistent with a transition from surface-dominated to bulk emission\nexpected for spinodal decomposition. \n\n"}
{"id": "nucl-ex/0007013", "contents": "Title: One-neutron removal reactions on neutron-rich psd-shell nuclei Abstract: A systematic study of high energy, one-neutron removal reactions on 23\nneutron-rich, psd--shell nuclei (Z=5-9, A=12-25) has been carried out. The\nlongitudinal momentum distributions of the core fragments and corresponding\nsingle-neutron removal cross sections are reported for reactions on a carbon\ntarget. Extended Glauber model calculations, weighted by the spectroscopic\nfactors obtained from shell model calculations, are compared to the\nexperimental results. Conclusions are drawn regarding the use of such reactions\nas a spectroscopic tool and spin-parity assignments are proposed for 15B, 17C,\n19-21N, 21,23O, 23-25F. The nature of the weakly bound systems 14B and 15,17C\nis discussed. \n\n"}
{"id": "nucl-ex/0101014", "contents": "Title: Low energy measurement of the 7Be(p,gamma)8B cross section Abstract: We have measured the cross section of the 7Be(p,gamma)8B reaction for E_cm =\n185.8 keV, 134.7 keV and 111.7 keV using a radioactive 7Be target (132 mCi).\nSingle and coincidence spectra of beta^+ and alpha particles from 8B and 8Be^*\ndecay, respectively, were measured using a large acceptance spectrometer. The\nzero energy S factor inferred from these data is 18.5 +/- 2.4 eV b and a\nweighted mean value of 18.8 +/- 1.7 eV b (theoretical uncertainty included) is\ndeduced when combining this value with our previous results at higher energies. \n\n"}
{"id": "nucl-ex/0104022", "contents": "Title: Mid-rapidity anti-proton to proton ratio from Au+Au collisions at $\n  \\sqrt{s_{NN}} = 130$ GeV Abstract: We report results on the ratio of mid-rapidity anti-proton to proton yields\nin Au+Au collisions at $\\rts = 130$ GeV per nucleon pair as measured by the\nSTAR experiment at RHIC. Within the rapidity and transverse momentum range of\n$|y|<0.5$ and 0.4 $<p_t<$ 1.0 GeV/$c$, the ratio is essentially independent of\neither transverse momentum or rapidity, with an average of $0.65\\pm 0.01_{\\rm\n(stat.)} \\pm 0.07_{\\rm (syst.)}$ for minimum bias collisions. Within errors, no\nstrong centrality dependence is observed. The results indicate that at this\nRHIC energy, although the $p$-$\\pb$ pair production becomes important at\nmid-rapidity, a significant excess of baryons over anti-baryons is still\npresent. \n\n"}
{"id": "nucl-ex/0105018", "contents": "Title: COBRA - Double beta decay searches using CdTe detectors Abstract: A new approach (called COBRA) for investigating double beta decay using CdTe\n(CdZnTe) semiconductor detectors is proposed. It follows the idea that source\nand detector are identical. This will allow simultaneous measurements of 5\n$\\beta^-\\beta^-$ - and 4 $\\beta^+\\beta^+$ - emitters at once. Half-life limits\nfor neutrinoless double beta decay of Cd-116 and Te-130 can be improved by more\nthan one order of magnitude with respect to current limits and sensitivities on\nthe effective Majorana neutrino mass of less than 1 eV can be obtained.\nFurthermore, for the first time a realistic chance of observing double electron\ncapture processes exists. Additional searches for rare processes like the\n4-fold forbidden Cd-113 $\\beta$-decay, the electron capture of Te-123 and dark\nmatter detection can be performed. The achievable limits are evaluated for 10\nkg of such detectors and can be scaled accordingly towards higher detector\nmasses because of the modular design of the proposed experiment. \n\n"}
{"id": "nucl-ex/0108024", "contents": "Title: Haloes and Clustering in Light, Neutron-Rich Nuclei Abstract: Clustering is a relatively widespread phenomena which takes on many guises\nacross the nuclear landscape. Selected topics concerning the study of halo\nsystems and clustering in light, neutron-rich nuclei are discussed here through\nillustrative examples taken from the Be isotopic chain. \n\n"}
{"id": "nucl-ex/0110012", "contents": "Title: Quark Gluon Plasma - Recent Advances Abstract: While heavy ion collisions at the SPS have produced excited strongly\ninteracting matter near the conditions for quark deconfinement, the RHIC may be\nthe first machine capable of creating quark-antiquark plasmas sufficiently\nlong-lived to allow deep penetration into the new phase. A comprehensive\nexperimental program addressing this exciting physics has been put into place.\nPresented here are preliminary results from Au+Au at $\\sqrt{S}$ = 130 GeV\nobtained during the first RHIC run and some CERN SPS results from Pb+Pb at\n$\\sqrt{S}$ = 17 GeV (particularly relevant to QGP search). \n\n"}
{"id": "nucl-ex/0204009", "contents": "Title: Measurement of Day and Night Neutrino Energy Spectra at SNO and\n  Constraints on Neutrino Mixing Parameters Abstract: The Sudbury Neutrino Observatory (SNO) has measured day and night solar\nneutrino energy spectra and rates. For charged current events, assuming an\nundistorted $^8$B spectrum, the night minus day rate is $14.0% \\pm 6.3%\n^{+1.5}_{-1.4}%$ of the average rate. If the total flux of active neutrinos is\nadditionally constrained to have no asymmetry, the $\\nu_e$ asymmetry is found\nto be $7.0% \\pm 4.9% ^{+1.3}_{-1.2}%$. A global solar neutrino analysis in\nterms of matter-enhanced oscillations of two active flavors strongly favors the\nLarge Mixing Angle (LMA) solution. \n\n"}
{"id": "nucl-ex/0209002", "contents": "Title: Energy Dependence of $\\Lambda$ and $\\bar{\\Lambda}$ Production at\n  CERN-SPS Energies Abstract: Rapidity distributions for $\\Lambda$ and $\\bar{\\Lambda}$ hyperons in central\nPb-Pb collisions at 40, 80 and 158 A$\\cdot$GeV and for ${\\rm K}_{s}^{0}$ mesons\nat 158 A$\\cdot$GeV are presented. The lambda multiplicities are studied as a\nfunction of collision energy together with AGS and RHIC measurements and\ncompared to model predictions. A different energy dependence of the\n$\\Lambda/\\pi$ and $\\bar{\\Lambda}/\\pi$ is observed. The $\\bar{\\Lambda}/\\Lambda$\nratio shows a steep increase with collision energy. Evidence for a\n$\\bar{\\Lambda}/\\bar{\\rm p}$ ratio greater than 1 is found at 40 A$\\cdot$GeV. \n\n"}
{"id": "nucl-ex/0303004", "contents": "Title: STAR Results on Strangeness Production at RHIC energies Abstract: Strangeness measurement at RHIC energies constitutes one of the favorite\ntheme of the STAR Collaboration. Besides the fact that strangeness enhancement\nhas been proposed as a quark gluon plasma signature, its production provides\nvarious and relevant information about the collision evolution and especially\non the hadronization process. The investigation of short-lived strange\nparticles as well as their anti-particles and resonances allows an attempt of\ncharacterization of the matter created in RHIC heavy ion collisions. \n\n"}
{"id": "nucl-ex/0303005", "contents": "Title: Clustering and Correlations at the Neutron Dripline Abstract: Some recent experimental studies of clustering and correlations within very\nneutron-rich light nuclei are reviewed. In particular, the development of the\nnovel probes of neutron-neutron interferometry and Dalitz-plot analyses is\npresented through the example of the dissociation of the two-neutron halo\nsystem $^{14}$Be. The utility of high-energy proton radiative capture is\nillustrated using a study of the $^{6}$He(p,$\\gamma$) reaction. A new approach\nto the production and detection of bound neutron clusters is also described,\nand the observation of events with the characteristics expected for\ntetraneutrons ($^{4}$n) liberated in the breakup of $^{14}$Be is discussed. The\nprospects for future work, including systems beyond the neutron dripline, are\nbriefly outlined. \n\n"}
{"id": "nucl-ex/0304014", "contents": "Title: D(18F,pa)15N reaction applied to nova gamma-ray emission Abstract: The 18F(p,alpha)15O reaction is recognized to be one of the most important\nreactions for nova gamma-ray astronomy as it governs the early E <= 511keV\ngamma emission. However in the nova temperature regime, its rate remains\nlargely uncertain due to unknown low-energy resonance strengths. We report here\nthe measurement of the D(18F,p)19F(alpha)15N one-nucleon transfer reaction,\ninduced by a 14 MeV 18F radioactive beam impinging on a CD2 target; outgoing\nprotons and 15N (or alpha-particles) were detected in coincidence in two\nsilicon strip detectors. A DWBA analysis of the data resulted in new limits to\nthe contribution of low-energy resonances to the rate of the 18F(p,alpha)15O\nreaction. \n\n"}
{"id": "nucl-ex/0307010", "contents": "Title: Single Identified Hadron Spectra from sqrt(s_NN)=130 GeV Au+Au\n  Collisions Abstract: Transverse momentum spectra and yields of hadrons are measured by the PHENIX\ncollaboration in Au + Au collisions at sqrt(s_NN) = 130 GeV at the Relativistic\nHeavy Ion Collider (RHIC). The time-of-flight resolution allows identification\nof pions to transverse momenta of 2 GeV/c and protons and antiprotons to 4\nGeV/c. The yield of pions rises approximately linearly with the number of\nnucleons participating in the collision, while the number of kaons, protons,\nand antiprotons increases more rapidly. The shape of the momentum distribution\nchanges between peripheral and central collisions. Simultaneous analysis of all\nthe p_T spectra indicates radial collective expansion, consistent with\npredictions of hydrodynamic models. Hydrodynamic analysis of the spectra shows\nthat the expansion velocity increases with collision centrality and collision\nenergy. This expansion boosts the particle momenta, causing the yield from soft\nprocesses to exceed that for hard to large transverse momentum, perhaps as\nlarge as 3 GeV/c. \n\n"}
{"id": "nucl-ex/0307022", "contents": "Title: Identified Charged Particle Spectra and Yields in Au+Au Collisions at\n  sqrt(s_NN) = 200 GeV Abstract: The centrality dependence of transverse momentum distributions and yields for\npi^+/-, K^+/-, p and p^bar in Au+Au collisions at sqrt(s_NN) = 200 GeV at\nmid-rapidity are measured by the PHENIX experiment at RHIC. We observe a clear\nparticle mass dependence of the shapes of transverse momentum spectra in\ncentral collisions below ~ 2 GeV/c in p_T. Both mean transverse momenta and\nparticle yields per participant pair increase from peripheral to mid-central\nand saturate at the most central collisions for all particle species. We also\nmeasure particle ratios of pi^-/pi^+, K^-/K^+, p^bar/p, K/pi, p/pi and p^bar/pi\nas a function of p_T and collision centrality. The ratios of equal mass\nparticle yields are independent of p_T and centrality within the experimental\nuncertainties. In central collisions at intermediate transverse momenta ~\n1.5-4.5 GeV/c, proton and anti-proton yields constitute a significant fraction\nof the charged hadron production and show a scaling behavior different from\nthat of pions. \n\n"}
{"id": "nucl-ex/0307024", "contents": "Title: Multi-Strange Baryon Production in Au-Au collisions at sqrt(s_NN) = 130\n  GeV Abstract: The transverse mass spectra and mid-rapidity yields for $\\Xi$s and $\\Omega$s\nplus their anti-particles are presented. The 10% most central collision yields\nsuggest that the amount of multi-strange particles produced per produced\ncharged hadron increases from SPS to RHIC energies. A hydrodynamically inspired\nmodel fit to the spectra, which assumes a thermalized source, seems to indicate\nthat these multi-strange particles experience a significant transverse flow\neffect, but are emitted when the system is hotter and the flow is smaller than\nvalues obtained from a combined fit to $\\pi$, K, p and $\\Lambda$s. \n\n"}
{"id": "nucl-ex/0401011", "contents": "Title: Total and Differential Cross Sections for the pp-->pp eta-prime Reaction\n  Near Threshold Abstract: The eta-prime meson production in the reaction pp-->pp eta-prime has been\nstudied at excess energies of Q = 26.5, 32.5 and 46.6 MeV using the internal\nbeam facility COSY-11 at the cooler synchrotron COSY. The total cross sections\nas well as one angular distribution for the highest Q-value are presented. The\nexcitation function of the near threshold data can be described by a pure\ns-wave phase space distribution with the inclusion of the proton-proton final\nstate interaction and Coulomb effects. The obtained angular distribution of the\neta-prime mesons is also consistent with pure s-wave production. \n\n"}
{"id": "nucl-ex/0403021", "contents": "Title: <pt> Systematics and mt-Scaling Abstract: An enhancement in the number of strange particles produced in relativistic\nheavy ion collisions is expected to coincide with the formation of a deconfined\nstate of partonic matter. Measurements of transverse momentum spectra for\nstrange particles emerging from p+p collisions are used as a baseline to which\nsimilar measurements from heavy ion collisions are compared. In addition,\nseveral observations from p+p collisions, such as the variation of <pt> with\nparticle mass and with event multiplicity, are interesting in their own right.\nWe present measurements of the transverse momentum spectra and <pt> systematics\nfor strange and non-strange particles from p+p collisions at sqrt{s}=200 GeV.\nWe show the dependence of the <pt> on measured charged multiplicity and on\nparticle mass. We will also demonstrate the ability to scale the transverse\nmass spectra of various species onto a single universal curve for our p+p data\n(an effect known as mt-scaling) and the failure of this scaling when applied to\nour Au+Au data. The work presented here was presented as a poster at Quark\nMatter 2004. \n\n"}
{"id": "nucl-ex/0403050", "contents": "Title: Charged Meson Rapidity Distributions in central Au+Au Collisions at\n  sqrt(s_nn) = 200 GeV Abstract: We have measured rapidity densities dN/dy of pions and kaons over a broad\nrapidity range (-0.1 < y < 3.5) for central Au+Au collisions at sqrt(snn) = 200\nGeV. These data have significant implications for the chemistry and dynamics of\nthe dense system that is initially created in the collisions. The full\nphase-space yields are 1660 +/- 15 +/- 133 (pi+), 1683 +/- 16 +/- 135 (pi-),\n286 +/- 5 +/- 23 (K+) and 242 +/- 4 +/- 19 (K-). The systematics of the strange\nto non--strange meson ratios are found to track the variation of the\nbaryo-chemical potential with rapidity and energy. Landau--Carruthers\nhydrodynamic is found to describe the bulk transport of the pions in the\nlongitudinal direction. \n\n"}
{"id": "nucl-ex/0404011", "contents": "Title: Overview of the results from the BRAHMS experiment Abstract: An overview of the most important experimental results obtained in the first\nthree running years with the BRAHMS experiment from Brookhaven National\nLaboratory(USA) is presented. The design of the experiment permits to measure\nthe interesting physical quantities in a large ranges of rapidity and\ntransverse momentum. Therefore, properties of hadron production vs rapidity and\ntransverse momenta are presented. \n\n"}
{"id": "nucl-ex/0407015", "contents": "Title: Excitation Functions of the Analyzing Power in Elastic Proton-Proton\n  Scattering from 0.45 to 2.5 GeV Abstract: Excitation functions A_N(p_{lab},Theta_{c.m.}) of the analyzing power in\nelastic proton-proton scattering have been measured in an internal target\nexperiment at the Cooler Synchrotron COSY with an unpolarized proton beam and a\npolarized atomic hydrogen target. Data were taken continuously during the\nacceleration and deceleration for proton kinetic energies T_{lab} (momenta\np_{lab}) between 0.45 and 2.5 GeV (1.0 and 3.3 GeV/c) and scattering angles 30\ndeg < Theta_{c.m.} < 90 deg. The results provide excitation functions and\nangular distributions of high precision and internal consistency. The data can\nbe used as calibration standard between 0.45 and 2.5 GeV. They have significant\nimpact on phase shift solutions, in particular on the spin triplet phase shifts\nbetween 1.0 and 1.8 GeV. \n\n"}
{"id": "nucl-ex/0407022", "contents": "Title: Thermal up-scattering of very cold and ultra-cold neutrons in solid\n  deuterium Abstract: The work presented in this thesis forms part of a program at the Paul\nScherrer Institute (PSI) to construct a high intensity superthermal ultra-cold\nneutron (UCN) source based on solid deuterium as UCN production medium. We\ncarried out a set of experiments to gain a better understanding of the\nproperties and the behaviour of solid deuterium as a cold neutron moderator and\nultra-cold neutron converter.\n  We present the measurements of the total neutron cross section as obtained by\ntransmission studies with very cold neutrons and ultra-cold neutrons in solid\ndeuterium. The experimental set-up and the methods of data analysis are\ndescribed and also the procedure of preparing the solid deuterium samples is\ngiven. The neutron transmission studies are supported by optical investigation\nof the crystal and by Raman spectroscopy. We have thus characterised the\ntemperature dependence of the neutron transmission through solid deuterium and\nwe have been able to identify the role that coherent neutron scattering plays\nfor the investigated deuterium samples. \n\n"}
{"id": "nucl-ex/0410002", "contents": "Title: Neutral Pions with Large Transverse Momentum in d+Au and Au+Au\n  Collisions Abstract: Measurements of transverse-momentum p_T spectra of neutral pions in Au+Au and\nd+Au collisions at sqrt{s_NN}=200 GeV and 62.4 GeV by the PHENIX experiment at\nRHIC in comparison to p+p reference spectra at the same sqrt{s_NN} are\npresented. In central Au+Au collisions at sqrt{s_NN}=200 GeV a factor 4-5\nsuppression for neutral pions and charged hadrons with p_T > 5 GeV/c is found\nrelative to the p+p reference scaled by the nuclear overlap function < T_AB >.\nIn contrast, such a suppression of high-p_T particles is absent in d+Au\ncollisions independent of the centrality of the collision. To study the\nsqrt{s_NN} dependence of the suppression Au+Au collisions at sqrt{s_NN}=200 GeV\nand 62.4 GeV are compared. \n\n"}
{"id": "nucl-ex/0411023", "contents": "Title: Multifragmentation and the liquid-gas phase transition: an experimental\n  overview Abstract: Two roads are presently being followed in order to establish the existence of\na liquid-gas phase transition in finite nuclear systems from nuclear reactions\nat high energy. The clean experiment of observing the thermodynamic properties\nof a finite number of nucleons in a container is presently only possible with\nthe computer. Performed with advanced nuclear transport models, it has revealed\nthe first-order character of the transition and allowed the extraction of the\npertinent thermodynamic parameters. The validity of the applied theory is being\nconfirmed by comparing its predictions for heavy-ion reactions with exclusive\nexperiments.\n  The second approach is experimentally more direct. Signals of the transition\nare searched for by analysing reaction data within the framework of\nthermodynamics of small systems. A variety of potential signals has been\ninvestigated and found to be qualitatively consistent with the expectations for\nthe phase transition. Many of them are well reproduced with percolation models\nwhich places the nuclear fragmentation into the more general context of\npartitioning phenomena in finite systems.\n  A wealth of new data on this subject has been obtained in recent experiments,\nsome of them with a new generation of multi-detector devices aiming at higher\nresolutions, isotopic identification of the fragments, and the coincident\ndetection of neutrons. Isotopic effects in multifragmentation were addressed\nquite intensively, with particular attention being given to their relation to\nthe symmetry energy and its dependence on density. \n\n"}
{"id": "nucl-ex/0505003", "contents": "Title: Heavy ion collisions: Correlations and Fluctuations in particle\n  production Abstract: Correlations and fluctuations (the latter are directly related to the\n2-particle correlations) is one of the important directions in analysis of\nheavy ion collisions. At the current stage of RHIC exploration, when the\ndetails matter, basically any physics question is addressed with help of\ncorrelation techniques. In this talk I start with a general introduction to the\ncorrelation and fluctuation formalism and discuss weak and strong sides of\ndifferent type of observables. In more detail, I discuss the two-particle $p_t$\ncorrelations/$\\mpt$ fluctuations. In spite of not observing any dramatic\nchanges in the event-by-event fluctuations with energy, which would indicate a\npossible phase transition, such correlations measurements remain an interesting\nand important subject, bringing valuable information. Lastly, I show how radial\nflow can generate characteristic azimuthal, transverse momentum and rapidity\ncorrelations, which could qualitatively explain many of recently observed\nphenomena in nuclear collisions. \n\n"}
{"id": "nucl-ex/0506005", "contents": "Title: Q^2-Dependence of the Neutron Spin Structure Function g_2^n at Low Q^2 Abstract: We present the first measurement of the Q^2-dependence of the neutron spin\nstructure function g_2^n at five kinematic points covering 0.57 (GeV/c)^2 <=\nQ^2 <= 1.34 (GeV/c)^2 at x~0.2. Though the naive quark-parton model predicts\ng_2=0, non-zero values for g_2 occur in more realistic models of the nucleon\nwhich include quark-gluon correlations, finite quark masses or orbital angular\nmomentum. When scattering from a non-interacting quark, $g_2^n$ can be\npredicted using next-to-leading order fits to world data for g_1^n. Deviations\nfrom this prediction provide an opportunity to examine QCD dynamics in nucleon\nstructure. Our results show a positive deviation from this prediction at lower\nQ^2, indicating that contributions such as quark-gluon interactions may be\nimportant. Precision data obtained for g_1^n are consistent with\nnext-to-leading order fits to world data. \n\n"}
{"id": "nucl-ex/0508018", "contents": "Title: Consequences of energy conservation in relativistic heavy-ion collisions Abstract: Complete characterization of particle production and emission in relativistic\nheavy-ion collisions is in general not feasible experimentally. This work\ndemonstrates, however, that the availability of essentially complete\npseudorapidity distributions for charged particles allows for a reliable\nestimate of the average transverse momenta and energy of emitted particles by\nrequiring energy conservation in the process. The results of such an analysis\nfor Au+Au collisions at sqrt{s_{NN}}= 130 and 200 GeV are compared with\nmeasurements of mean-p_T and mean-E_T in regions where such measurements are\navailable. The mean-p_T dependence on pseudorapidity for Au+Au collisions at\n130 and 200 GeV is given for different collision centralities. \n\n"}
{"id": "nucl-ex/0605033", "contents": "Title: Lifetime of 19Ne*(4.03 MeV) Abstract: The Doppler-shift attenuation method was applied to measure the lifetime of\nthe 4.03 MeV state in 19Ne. Utilizing a 3He-implanted Au foil as a target, the\nstate was populated using the 20Ne(3He,alpha)19Ne reaction in inverse\nkinematics at a 20Ne beam energy of 34 MeV. De-excitation gamma rays were\ndetected in coincidence with alpha particles. At the 1 sigma level, the\nlifetime was determined to be 11 +4, -3 fs and at the 95.45% confidence level\nthe lifetime is 11 +8, -7 fs. \n\n"}
{"id": "nucl-ex/0606002", "contents": "Title: Baryonic Resonance Studies with STAR Abstract: Yields and spectra of $\\Sigma(1385)$ are measured in $p+p$, d+Au and Au+Au\ncollisions at $\\sqrt{s_{NN}}=200$ GeV . The nuclear modification factors in\nd+Au collisions are presented. The $p_{T}$ dependent medium effects are\ninvestigated via the nuclear modification factors. The implications of these\nresults on various models are discussed. \n\n"}
{"id": "nucl-ex/0609015", "contents": "Title: Measurements of the Electron-Helicity Dependent Cross Sections of Deeply\n  Virtual Compton Scattering with CEBAF at 12 GeV Abstract: We propose precision measurements of the helicity-dependent and helicity\nindependent cross sections for the ep->epg reaction in Deeply Virtual Compton\nScattering (DVCS) kinematics. DVCS scaling is obtained in the limits\nQ^2>>Lambda_{QCD}^2, x_Bj fixed, and -\\Delta^2=-(q-q')^2<<Q^2. We consider the\nspecific kinematic range Q^2>2 GeV^2, W>2 GeV, and -\\Delta^21 GeV^2. We will\nuse our successful technique from the 5.75 GeV Hall A DVCS experiment\n(E00-110). With polarized 6.6, 8.8, and 11 GeV beams incident on the liquid\nhydrogen target, we will detect the scattered electron in the Hall A HRS-L\nspectrometer (maximum central momentum 4.3 GeV/c) and the emitted photon in a\nslightly expanded PbF_2 calorimeter. In general, we will not detect the recoil\nproton. The H(e,e'g)X missing mass resolution is sufficient to isolate the\nexclusive channel with 3% systematic precision. \n\n"}
{"id": "nucl-ex/0611004", "contents": "Title: Direct Photons in Heavy-Ion Collisions Abstract: A brief overview of direct-photon measurements in ultra-relativistic\nnucleus-nucleus collisions is given. The results for Pb+Pb collisions at\nsqrt{s_NN} = 17.3 GeV and for Au+Au collisions at sqrt{s_NN} = 200 GeV are\ncompared to estimates of the direct-photon yield from hard scattering. Both\nresults leave room for a significant thermal photon component. A description\npurely based on hard scattering processes, however, is not ruled out so far. \n\n"}
{"id": "nucl-ex/0611029", "contents": "Title: Heavy Flavor Production at PHENIX at RHIC Abstract: A study of heavy flavor production in different collision systems in various\nkinematic regions presents an opportunity to probe cold nuclear medium and hot\ndense matter effects. Results from the PHENIX experiment on $J/\\psi$ and open\ncharm production in Au+Au and Cu+Cu collisions at $\\sqrt{s_{NN}}$ =200 GeV are\npresented. The data show strong $J/\\psi$ suppression in central AA collisions,\nsimilar to NA50 results, and strong suppression in high $p_T$ open charm\nproduction. The $J/\\psi$ production in Au+Au and d+Au collisions is compared to\nunderstand the cold nuclear medium effects. The data show significant cold\nnuclear effects in charm production in d+Au collisions at forward and backward\nrapidity ranges. \n\n"}
{"id": "nucl-ex/0701038", "contents": "Title: Energy and system size dependence of charged particle elliptic flow and\n  $v_2/\\eps$ scaling Abstract: We report measurements of charged particle elliptic flow %($v_2$) at\nmid-rapidity in Au+Au and Cu+Cu collisions at $\\sqrt{s_{_{NN}}}=62$ and 200\nGeV. Using correlations between main STAR TPC and Forward TPCs ensures minimal\nbias due to non-flow effects. We further investigate the effect of flow\nfluctuations on $v_2/\\eps$ scaling studying initial geometry eccentricity\nfluctuations in Monte-Carlo Glauber model, consistent with STAR direct\nmeasurements of elliptic flow fluctuations. It is found that accounting for the\neffect of flow fluctuations improves $v_2/\\eps$ scaling. \n\n"}
{"id": "nucl-ex/0703012", "contents": "Title: Radiochemical solar neutrino experiments Abstract: Radiochemical experiments have been crucial to solar neutrino research. Even\ntoday, they provide the only direct measurement of the rate of the\nproton-proton fusion reaction, p + p --> d + e^+ + nu_e, which generates most\nof the Sun's energy. We first give a little history of radiochemical solar\nneutrino experiments with emphasis on the gallium experiment SAGE -- the only\ncurrently operating detector of this type. The combined result of all data from\nthe Ga experiments is a capture rate of 67.6 +/- 3.7 SNU. For comparison to\ntheory, we use the calculated flux at the Sun from a standard solar model, take\ninto account neutrino propagation from the Sun to the Earth and the results of\nneutrino source experiments with Ga, and obtain 67.3 ^{+3.9}_{-3.5} SNU. Using\nthe data from all solar neutrino experiments we calculate an electron neutrino\npp flux at the earth of (3.41 ^{+0.76}_{-0.77}) x 10^{10}/(cm^2-s), which\nagrees well with the prediction from a detailed solar model of (3.30 ^{+0.13}\n_{-0.14}) x 10^{10}/(cm^2-s). Four tests of the Ga experiments have been\ncarried out with very intense reactor-produced neutrino sources and the ratio\nof observed to calculated rates is 0.88 +/- 0.05. One explanation for this\nunexpectedly low result is that the cross section for neutrino capture by the\ntwo lowest-lying excited states in 71Ge has been overestimated. We end with\nconsideration of possible time variation in the Ga experiments and an\nenumeration of other possible radiochemical experiments that might have been. \n\n"}
{"id": "nucl-ex/0703025", "contents": "Title: PHENIX Studies of the Scaling Properties of Elliptic Flow at RHIC Abstract: Recent PHENIX elliptic flow ($v_2$) measurements for identified particles\nproduced in Au+Au and Cu+Cu collisions at $\\sqrt{s_{NN}}=200$ GeV are presented\nand compared to other RHIC measurements. They indicate universal scaling of\n$v_2$ compatible with partonic collectivity leading to the flow of light,\nstrange and heavy quarks with a common expansion velocity field. \n\n"}
{"id": "nucl-ex/9801003", "contents": "Title: A direct measurement of short range NN correlations in nuclei via the\n  reaction C(p,2p+n) Abstract: The reaction 12C(p,2p+n) was measured at beam momenta of 5.9 and 7.5 GeV/c..\nWe established the quasi-elastic character of the reaction C(p,2p) at\n$\\theta_{cm}\\simeq 90^o$, in a kinematically complete measurement. The neutron\nmomentum was measured in triple coincidence with the two emerging high momentum\nprotons. We present the correlation between the momenta of the struck target\nproton and the neutron. The events are associated with the high momentum\ncomponents of the nuclear wave function. We conclude that two-nucleon short\nrange correlations have been seen experimentally. The conclusion is based on\nkinematical correlations and is not based on specific theoretical models. \n\n"}
{"id": "nucl-ex/9810002", "contents": "Title: How to Test the Existence of the Early Parton Cascade Using Photon HBT\n  Correlations? Abstract: We report on a possible application of the HBT phenomenon in testing the\nexistence of two hypothetical phenomena. First, it is argued that the existence\nof a rapidly developing parton cascade in the earliest stages of a high energy\nnuclear collision process can be tested by studying two-photon HBT correlations\nover a wide longitudinal momentum scale - corresponding to the early photon\nemission time from the hypothetical parton system. This method provides the\nneeded selectivity for the early emitted photons, since the photons emitted at\nlater times correlate over progressively narrower momentum scales. Second, in a\nsimilar way we argue that the existence of a hypothetic dark matter candidate,\nthe Weakly Interacting Massive Particle (WIMP), may be tested by studying HBT\ncorrelations of cosmic gamma rays at a relatively long detection time scale -\ncorresponding to the very narrow spectral line of the photons emerging from\nWIMP annihilations. Background photons leave no signature since they\nessentially do not correlate. \n\n"}
{"id": "nucl-ex/9907016", "contents": "Title: Spin Correlation Coefficients in pp-->pnpi+ from 325 to 400 MeV Abstract: The spin correlation coefficient combinations Axx + Ayy, Axx - Ayy and the\nanalyzing powers Ay(theta) were measured for pp-->pnpi+ at beam energies of\n325, 350, 375 and 400 MeV. A polarized internal atomic hydrogen target and a\nstored, polarized proton beam were used. These polarization observables are\nsensitive to contributions of higher partial waves. A comparison with recent\ntheoretical calculations is provided. \n\n"}
{"id": "nucl-th/0007059", "contents": "Title: (Non)Thermal Aspects of Charmonium Production and a New Look at J/$\\psi$\n  Suppression Abstract: To investigate a recent proposal that J/$\\psi$ production in\nultra-relativistic nuclear collisions is of thermal origin we have reanalyzed\nthe data from the NA38/50 collaboration within a thermal model including charm.\nComparison of the calculated with measured yields demonstrates the non-thermal\norigin of hidden charm production at SPS energy. However, the ratio\n$\\psi^{'}$/(J/$\\psi)$ exhibits, in central nucleus-nucleus collisions, thermal\nfeatures which lead us to a new interpretation of open charm and charmonium\nproduction at SPS energy. Implications for RHIC and LHC energy measurements\nwill be discussed. \n\n"}
{"id": "nucl-th/0110040", "contents": "Title: Analysis of particle production in ultra-relativistic heavy ion\n  collisions within a two-source statistical model Abstract: The experimental data on hadron yields and ratios in central lead-lead and\ngold-gold collisions at 158 AGeV/$c$ (SPS) and $\\sqrt{s} = 130$ AGeV (RHIC),\nrespectively, are analysed within a two-source statistical model of an ideal\nhadron gas. A comparison with the standard thermal model is given. The two\nsources, which can reach the chemical and thermal equilibrium separately and\nmay have different temperatures, particle and strangeness densities, and other\nthermodynamic characteristics, represent the expanding system of colliding\nheavy ions, where the hot central fireball is embedded in a larger but cooler\nfireball. The volume of the central source increases with rising bombarding\nenergy. Results of the two-source model fit to RHIC experimental data at\nmidrapidity coincide with the results of the one-source thermal model fit,\nindicating the formation of an extended fireball, which is three times larger\nthan the corresponding core at SPS. \n\n"}
{"id": "nucl-th/0210070", "contents": "Title: Some remarks on the statistical model of heavy ion collisions Abstract: This contribution is an attempt to assess what can be learned from the\nremarkable success of the statistical model in describing ratios of particle\nabundances in ultra-relativistic heavy ion collisions. \n\n"}
{"id": "nucl-th/0301078", "contents": "Title: Bound Nucleon Form Factors, Quark-Hadron Duality, and Nuclear EMC Effect Abstract: We discuss the electromagnetic form factors, axial form factors, and\nstructure functions of a bound nucleon in the quark-meson coupling (QMC) model.\nFree space nucleon form factors are calculated using the improved cloudy bag\nmodel (ICBM). After describing finite nuclei and nuclear matter in the\nquark-based QMC model, we compute the in-medium modification of the bound\nnucleon form factors in the same framework. Finally, limits on the medium\nmodification of the bound nucleon $F_2$ structure function are obtained using\nthe calculated in-medium electromagnetic form factors and local quark-hadron\nduality. \n\n"}
{"id": "nucl-th/0307088", "contents": "Title: Light Baryon Resonances: Restrictions and Perspectives Abstract: The problem of nucleon resonances N' with masses below the Delta is\nconsidered. We derive bounds for the properties of such states. Some of these\nare new, while others improve upon existing limits. We discuss the nature of N'\nstates, and their unitary partners, assuming their existence can be verified. \n\n"}
{"id": "nucl-th/0309077", "contents": "Title: Universal Transition Curve in Pseudo-Rapidity Distribution Abstract: We show that an unambiguous way of determining the universal limiting\nfragmentation region is to consider the derivative ($d^2n/d\\eta^2$) of the\npseudo-rapidity distribution per participant pair.\n  In addition, we find that the transition region between the fragmentation and\nthe central plateau regions exhibits a second kind of universal behavior that\nis only apparent in $d^2n/d\\eta^2$.\n  The $\\sqrts$ dependence of the height of the central plateau\n$(dn/d\\eta)_{\\eta=0}$ and the total charged particle multiplicity $n_{\\rm\ntotal}$ critically depend on the behavior of this universal transition curve.\n  Analyzing available RHIC data, we show that $(dn/d\\eta)_{\\eta=0}$ can be\nbounded by $\\ln^2 s$ and $n_{\\rm total}$ can be bounded by $\\ln^3 s$.\n  We also show that the deuteron-gold data from RHIC has the exactly same\nfeatures as the gold-gold data indicating that these universal behaviors are a\nfeature of the initial state parton-nucleus interactions and not a consequence\nof final state interactions.\n  Predictions for LHC energy are also given. \n\n"}
{"id": "nucl-th/0404083", "contents": "Title: SHARE: Statistical Hadronization with Resonances Abstract: SHARE is a collection of programs designed for the statistical analysis of\nparticle production in relativistic heavy-ion collisions. With the physical\ninput of intensive statistical parameters, it generates the ratios of particle\nabundances. The program includes cascade decays of all confirmed resonances\nfrom the Particle Data Tables. The complete treatment of these resonances has\nbeen known to be a crucial factor behind the success of the statistical\napproach. An optional feature implemented is a Breit--Wigner type distribution\nfor strong resonances. An interface for fitting the parameters of the model to\nthe experimental data is provided. \n\n"}
{"id": "nucl-th/0508024", "contents": "Title: Nucleon-nucleon cross sections in neutron-rich matter and isospin\n  transport in heavy-ion reactions at intermediate energies Abstract: Nucleon-nucleon (NN) cross sections are evaluated in neutron-rich matter\nusing a scaling model according to nucleon effective masses. It is found that\nthe in-medium NN cross sections are not only reduced but also have a different\nisospin dependence compared with the free-space ones. Because of the\nneutron-proton effective mass splitting the difference between nn and pp\nscattering cross sections increases with the increasing isospin asymmetry of\nthe medium. Within the transport model IBUU04, the in-medium NN cross sections\nare found to influence significantly the isospin transport in heavy-ion\nreactions. With the in-medium NN cross sections, a symmetry energy of\n$E_{sym}(\\rho)\\approx 31.6(\\rho /\\rho_{0})^{0.69}$ was found most acceptable\ncompared with both the MSU isospin diffusion data and the presently acceptable\nneutron-skin thickness in $^{208}$Pb. The isospin dependent part $K_{asy}(\\rho\n_{0})$ of isobaric nuclear incompressibility was further narrowed down to\n$-500\\pm 50$ MeV. The possibility of determining simultaneously the in-medium\nNN cross sections and the symmetry energy was also studied. The proton\ntransverse flow, or even better the combined transverse flow of neutrons and\nprotons, can be used as a probe of the in-medium NN cross sections without much\nhindrance from the uncertainties of the symmetry energy. \n\n"}
{"id": "nucl-th/0511004", "contents": "Title: The structure of the nucleon Abstract: Experimental data on electromagnetic and weak form factors of the nucleon are\nanalyzed in a two-component model with a quark-like intrinsic structure\nsurrounded by a meson cloud. The contribution from strange quarks is discussed\nand compared with recent data from the G0 Collaboration. \n\n"}
{"id": "nucl-th/0512084", "contents": "Title: Back-to-Back Correlations for Finite Expanding Fireballs Abstract: Back-to-Back Correlations of particle-antiparticle pairs are related to the\nin-medium mass-modification and squeezing of the quanta involved. They are\npredicted to appear when hot and dense hadronic matter is formed in high energy\nnucleus-nucleus collisions. The survival and magnitude of the Back-to-Back\nCorrelations of boson-antiboson pairs generated by in-medium mass modifications\nare studied here in the case of a thermalized, finite-sized, spherically\nsymmetric expanding medium. We show that the BBC signal indeed survives the\nfinite-time emission, as well as the expansion and flow effects, with\nsufficient intensity to be observed at RHIC. \n\n"}
{"id": "nucl-th/0605070", "contents": "Title: A New Family of Simple Solutions of Perfect Fluid Hydrodynamics Abstract: A new class of accelerating, exact and explicit solutions of relativistic\nhydrodynamics is found - more than 50 years after the previous similar result,\nthe Landau-Khalatnikov solution. Surprisingly, the new solutions have a simple\nform, that generalizes the renowned, but accelerationless, Hwa-Bjorken\nsolution. These new solutions take into account the work done by the fluid\nelements on each other, and work not only in one temporal and one spatial\ndimensions, but also in arbitrary number of spatial dimensions. They are\napplied here for an advanced estimation of initial energy density and life-time\nof the reaction in ultra-relativistic heavy ion collisions. \n\n"}
{"id": "nucl-th/9904015", "contents": "Title: Novel features of $J/\\Psi$ dissociation in matter Abstract: We make a detailed study of the effect that the recently predicted\nmodification of the in-medium masses of charmed mesons would have on $J/\\Psi$\ndissociation on pion and $\\rho$-meson comovers in relativistic heavy ion\ncollisions. We find a substantial dependence of the $J/\\Psi$ absorption rates\non the density and temperature of the nuclear matter. This suggests that a\nquantitative analysis of $J/\\Psi$ dissociation in nucleus nucleus collisions\nshould include the effects of the modification of meson masses in dense matter. \n\n"}
{"id": "physics/0510181", "contents": "Title: CALICE Si/W electromagnetic calorimeter prototype, first testbeam\n  results Abstract: A highly granular electromagnetic calorimeter prototype based on tungsten\nabsorber and sampling units equipped with silicon pads as sensitive devices for\nsignal collection is under construction. The full prototype will have in total\n30 layers and be read out by about 10000 Si cells of 1 cm^2. A first module\nconsisting of 14 layers and depth of 7.2 radiation lengths at normal incidence,\nhaving in total 3024 channels of 1 cm^2, was tested recently with electron\nbeam. We describe the prototype and discuss some preliminary testbeam results. \n\n"}
{"id": "physics/0511167", "contents": "Title: Wasa at Cosy Abstract: The Wide Angle Shower Apparatus (WASA) has been transferred from the storage\nring CELSIUS (TSL, Uppsala, Sweden) to the cooler synchrotron COSY (FZ Juelich,\nGermany). In this presentation, the status and planning of the WASAatCOSY\nproject are reported. The main physics aspects are rare decays of eta and eta'\nmesons and isospin violation in deuteron-deuteron interactions. \n\n"}
{"id": "physics/0603231", "contents": "Title: High Voltage Test Apparatus for a Neutron EDM Experiment and Lower Limit\n  on the Dielectric Strength of Liquid Helium at Large Volumes Abstract: A new search for a permanent electric dipole moment (EDM) of the neutron is\nunderway using ultracold neutrons produced and held in a bath of superfluid\nhelium. Attaining the target sensitivity requires maintaining an electric field\nof several tens of kilovolts per centimeter across the experimental cell, which\nis nominally 7.5 cm wide and will contain about 4 liters of superfluid. The\nelectrical properties of liquid helium are expected to be sufficient to meet\nthe design goals, but little is known about these properties for volumes and\nelectrode spacings appropriate to the EDM experiment. Furthermore, direct\napplication of the necessary voltages from an external source to the\nexperimental test cell is impractical. An apparatus to amplify voltages in the\nliquid helium environment and to test the electrical properties of the liquid\nfor large volumes and electrode spacings has been constructed. The device\nconsists of a large-area parallel plate capacitor immersed in a 200 liter\nliquid helium dewar. Preliminary results show the breakdown strength of normal\nstate liquid helium is at least 90 kV/cm at these volumes, at the helium vapor\npressure corresponding to 4.38 K. These fields hold for more than 11 hours with\nleakage currents less than 170 pA (about 20% of the maximum tolerable in the\nEDM experiment). The system is also found to be robust against anticipated\nradiation backgrounds. Preliminary results for superfluid show that fields of\nat least 30 kV/cm can be sustained at the volumes required for the EDM\nexperiment, about 60% of the design goal. These results are likely limited by\nthe low pressure that must be maintained above the superfluid bath. \n\n"}
