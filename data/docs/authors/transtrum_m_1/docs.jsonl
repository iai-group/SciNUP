{"id": "0704.0726", "contents": "Title: Long-range correlation and multifractality in Bach's Inventions pitches Abstract: We show that it can be considered some of Bach pitches series as a stochastic\nprocess with scaling behavior. Using multifractal deterend fluctuation analysis\n(MF-DFA) method, frequency series of Bach pitches have been analyzed. In this\nview we find same second moment exponents (after double profiling) in ranges\n(1.7-1.8) in his works. Comparing MF-DFA results of original series to those\nfor shuffled and surrogate series we can distinguish multifractality due to\nlong-range correlations and a broad probability density function. Finally we\ndetermine the scaling exponents and singularity spectrum. We conclude fat tail\nhas more effect in its multifractality nature than long-range correlations. \n\n"}
{"id": "0704.1023", "contents": "Title: The Effect of Annealing Temperature on Statistical Properties of $WO_3$\n  Surface Abstract: We have studied the effect of annealing temperature on the statistical\nproperties of $WO_3$ surface using atomic force microscopy techniques (AFM). We\nhave applied both level crossing and structure function methods. Level crossing\nanalysis indicates an optimum annealing temperature of around 400$^oC$ at which\nthe effective area of the $WO_3$ thin film is maximum, whereas composition of\nthe surface remains stoichiometric. The complexity of the height fluctuation of\nsurfaces was characterized by roughness, roughness exponent and lateral size of\nsurface features. We have found that there is a phase transition at around\n400$^oC$ from one set to two sets of roughness parameters. This happens due to\nmicrostructural changes from amorphous to crystalline structure in the samples\nthat has been already found experimentally. \n\n"}
{"id": "0705.0313", "contents": "Title: Information flow and optimization in transcriptional control Abstract: In the simplest view of transcriptional regulation, the expression of a gene\nis turned on or off by changes in the concentration of a transcription factor\n(TF). We use recent data on noise levels in gene expression to show that it\nshould be possible to transmit much more than just one regulatory bit.\nRealizing this optimal information capacity would require that the dynamic\nrange of TF concentrations used by the cell, the input/output relation of the\nregulatory module, and the noise levels of binding and transcription satisfy\ncertain matching relations. This parameter-free prediction is in good agreement\nwith recent experiments on the Bicoid/Hunchback system in the early Drosophila\nembryo, and this system achieves ~90% of its theoretical maximum information\ntransmission. \n\n"}
{"id": "0706.1062", "contents": "Title: Power-law distributions in empirical data Abstract: Power-law distributions occur in many situations of scientific interest and\nhave significant consequences for our understanding of natural and man-made\nphenomena. Unfortunately, the detection and characterization of power laws is\ncomplicated by the large fluctuations that occur in the tail of the\ndistribution -- the part of the distribution representing large but rare events\n-- and by the difficulty of identifying the range over which power-law behavior\nholds. Commonly used methods for analyzing power-law data, such as\nleast-squares fitting, can produce substantially inaccurate estimates of\nparameters for power-law distributions, and even in cases where such methods\nreturn accurate answers they are still unsatisfactory because they give no\nindication of whether the data obey a power law at all. Here we present a\nprincipled statistical framework for discerning and quantifying power-law\nbehavior in empirical data. Our approach combines maximum-likelihood fitting\nmethods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic\nand likelihood ratios. We evaluate the effectiveness of the approach with tests\non synthetic data and give critical comparisons to previous approaches. We also\napply the proposed methods to twenty-four real-world data sets from a range of\ndifferent disciplines, each of which has been conjectured to follow a power-law\ndistribution. In some cases we find these conjectures to be consistent with the\ndata while in others the power law is ruled out. \n\n"}
{"id": "0707.1133", "contents": "Title: Stochastic Differential Games with Reflection and Related Obstacle\n  Problems for Isaacs Equations Abstract: In this paper we first investigate zero-sum two-player stochastic\ndifferential games with reflection with the help of theory of Reflected\nBackward Stochastic Differential Equations (RBSDEs). We will establish the\ndynamic programming principle for the upper and the lower value functions of\nthis kind of stochastic differential games with reflection in a\nstraight-forward way. Then the upper and the lower value functions are proved\nto be the unique viscosity solutions of the associated upper and the lower\nHamilton-Jacobi-Bellman-Isaacs equations with obstacles, respectively. The\nmethod differs heavily from those used for control problems with reflection, it\nhas its own techniques and its own interest. On the other hand, we also prove a\nnew estimate for RBSDEs being sharper than that in El Karoui, Kapoudjian,\nPardoux, Peng and Quenez [7], which turns out to be very useful because it\nallows to estimate the $L^p$-distance of the solutions of two different RBSDEs\nby the $p$-th power of the distance of the initial values of the driving\nforward equations. We also show that the unique viscosity solution of the\napproximating Isaacs equation which is constructed by the penalization method\nconverges to the viscosity solution of the Isaacs equation with obstacle. \n\n"}
{"id": "0707.1616", "contents": "Title: Modularity and community detection in bipartite networks Abstract: The modularity of a network quantifies the extent, relative to a null model\nnetwork, to which vertices cluster into community groups. We define a null\nmodel appropriate for bipartite networks, and use it to define a bipartite\nmodularity. The bipartite modularity is presented in terms of a modularity\nmatrix B; some key properties of the eigenspectrum of B are identified and used\nto describe an algorithm for identifying modules in bipartite networks. The\nalgorithm is based on the idea that the modules in the two parts of the network\nare dependent, with each part mutually being used to induce the vertices for\nthe other part into the modules. We apply the algorithm to real-world network\ndata, showing that the algorithm successfully identifies the modular structure\nof bipartite networks. \n\n"}
{"id": "0707.3462", "contents": "Title: Separable and Low-Rank Continuous Games Abstract: In this paper, we study nonzero-sum separable games, which are continuous\ngames whose payoffs take a sum-of-products form. Included in this subclass are\nall finite games and polynomial games. We investigate the structure of\nequilibria in separable games. We show that these games admit finitely\nsupported Nash equilibria. Motivated by the bounds on the supports of mixed\nequilibria in two-player finite games in terms of the ranks of the payoff\nmatrices, we define the notion of the rank of an n-player continuous game and\nuse this to provide bounds on the cardinality of the support of equilibrium\nstrategies. We present a general characterization theorem that states that a\ncontinuous game has finite rank if and only if it is separable. Using our rank\nresults, we present an efficient algorithm for computing approximate equilibria\nof two-player separable games with fixed strategy spaces in time polynomial in\nthe rank of the game. \n\n"}
{"id": "0708.0492", "contents": "Title: Brownian motion in a non-homogeneous force field and photonic force\n  microscope Abstract: The Photonic Force Microscope (PFM) is an opto-mechanical technique based on\nan optical trap that can be assumed to probe forces in microscopic systems.\nThis technique has been used to measure forces in the range of pico- and\nfemto-Newton, assessing the mechanical properties of biomolecules as well as of\nother microscopic systems. For a correct use of the PFM, the force field to\nmeasure has to be invariable (homogeneous) on the scale of the Brownian motion\nof the trapped probe. This condition implicates that the force field must be\nconservative, excluding the possibility of a rotational component. However,\nthere are cases where these assumptions are not fulfilled Here, we show how to\nimprove the PFM technique in order to be able to deal with these cases. We\nintroduce the theory of this enhanced PFM and we propose a concrete analysis\nworkflow to reconstruct the force field from the experimental time-series of\nthe probe position. Furthermore, we experimentally verify some particularly\nimportant cases, namely the case of a conservative or rotational force-field. \n\n"}
{"id": "0709.0380", "contents": "Title: Predictive protocol of flocks with small-world connection pattern Abstract: By introducing a predictive mechanism with small-world connections, we\npropose a new motion protocol for self-driven flocks. The small-world\nconnections are implemented by randomly adding long-range interactions from the\nleader to a few distant agents, namely pseudo-leaders. The leader can directly\naffect the pseudo-leaders, thereby influencing all the other agents through\nthem efficiently. Moreover, these pseudo-leaders are able to predict the\nleader's motion several steps ahead and use this information in decision making\ntowards coherent flocking with more stable formation. It is shown that drastic\nimprovement can be achieved in terms of both the consensus performance and the\ncommunication cost. From the industrial engineering point of view, the current\nprotocol allows for a significant improvement in the cohesion and rigidity of\nthe formation at a fairly low cost of adding a few long-range links embedded\nwith predictive capabilities. Significantly, this work uncovers an important\nfeature of flocks that predictive capability and long-range links can\ncompensate for the insufficiency of each other. These conclusions are valid for\nboth the attractive/repulsive swarm model and the Vicsek model. \n\n"}
{"id": "0709.0923", "contents": "Title: Communities in networks - a continuous approach Abstract: A system of differential equations is proposed designed as to identify\ncommunities in weighted networks. The input is a symmetric connectivity matrix\n$A_{ij}$. A priori information on the number of communities is not needed. To\nverify the dynamics, we prepared sets of separate, fully connected clusters. In\nthis case, the matrix $A$ has a block structure of zeros and units. A noise is\nintroduced as positive random numbers added to zeros and subtracted from units.\nThe task of the dynamics is to reproduce the initial block structure. In this\ntest, the system outperforms the modularity algorithm, if the number of\nclusters is larger than four. \n\n"}
{"id": "0709.3418", "contents": "Title: Spartan Random Processes in Time Series Modeling Abstract: A Spartan random process (SRP) is used to estimate the correlation structure\nof time series and to predict (extrapolate) the data values. SRP's are\nmotivated from statistical physics, and they can be viewed as Ginzburg-Landau\nmodels. The temporal correlations of the SRP are modeled in terms of\n`interactions' between the field values. Model parameter inference employs the\ncomputationally fast modified method of moments, which is based on matching\nsample energy moments with the respective stochastic constraints. The\nparameters thus inferred are then compared with those obtained by means of the\nmaximum likelihood method. The performance of the Spartan predictor (SP) is\ninvestigated using real time series of the quarterly S&P 500 index. SP\nprediction errors are compared with those of the Kolmogorov-Wiener predictor.\nTwo predictors, one of which explicit, are derived and used for extrapolation.\nThe performance of the predictors is similarly evaluated. \n\n"}
{"id": "0710.2992", "contents": "Title: Urban traffic from the perspective of dual graph Abstract: In this paper, urban traffic is modeled using dual graph representation of\nurban transportation network where roads are mapped to nodes and intersections\nare mapped to links. The proposed model considers both the navigation of\nvehicles on the network and the motion of vehicles along roads. The road's\ncapacity and the vehicle-turning ability at intersections are naturally\nincorporated in the model. The overall capacity of the system can be quantified\nby a phase transition from free flow to congestion. Simulation results show\nthat the system's capacity depends greatly on the topology of transportation\nnetworks. In general, a well-planned grid can hold more vehicles and its\noverall capacity is much larger than that of a growing scale-free network. \n\n"}
{"id": "0710.5441", "contents": "Title: Community Detecting By Signaling on Complex Networks Abstract: Based on signaling process on complex networks, a method for identification\ncommunity structure is proposed. For a network with $n$ nodes, every node is\nassumed to be a system which can send, receive, and record signals. Each node\nis taken as the initial signal source once to inspire the whole network by\nexciting its neighbors and then the source node is endowed a $n$d vector which\nrecording the effects of signaling process. So by this process, the topological\nrelationship of nodes on networks could be transferred into the geometrical\nstructure of vectors in $n$d Euclidian space. Then the best partition of groups\nis determined by $F$-statistic and the final community structure is given by\nFuzzy $C$-means clustering method (FCM). This method can detect community\nstructure both in unweighted and weighted networks without any extra\nparameters. It has been applied to ad hoc networks and some real networks\nincluding Zachary Karate Club network and football team network. The results\nare compared with that of other approaches and the evidence indicates that the\nalgorithm based on signaling process is effective. \n\n"}
{"id": "0710.5865", "contents": "Title: Fluctuation of the download network Abstract: The scaling behavior of fluctuation for a download network which we have\ninvestigated a few years ago based upon Zhang's Encophysics web page has been\npresented. A power law scaling, namely $\\sigma \\sim < f> ^ \\alpha $ exists\nbetween the dispersion $\\sigma$ and average flux $<f>$ of the download rates.\nThe fluctuation exponent $\\alpha$ is neither 1/2 nor 1 which was claimed as two\nuniversal fluctuation classes in previous publication, instead it varies from\n1/2 to 1 with the time window in which the download data were accumulated. The\ncrossover behavior of fluctuation exponents can be qualitatively understood by\nthe external driving fluctuation model for a small-size system or a network\ntraffic model which suggests congestion as the origin. \n\n"}
{"id": "0711.0530", "contents": "Title: Synchronization on complex networks with different sorts of communities Abstract: In this paper, inspired by the idea that many real networks are composed by\ndifferent sorts of communities, we investigate the synchronization property of\noscillators on such networks. We identify the communities by the intrinsic\nfrequencies probability density $g(\\omega)$ of Kuramoto oscillators. That is to\nsay, communities in different sorts are functional different. For a network\ncontaining two sorts of communities, when the community strength is strong,\nonly the oscillators in the same community synchronize. With the weakening of\nthe community strength, an interesting phenomenon, \\emph{Community Grouping},\nappears: although the global synchronization is not achieved, oscillators in\nthe same sort of communities will synchronize. Global synchronization will\nappear with the further reducing of the community strength, and the oscillators\nwill rotate around the average frequency. \n\n"}
{"id": "0711.3630", "contents": "Title: Prior-predictive value from fast growth simulations Abstract: Building on a variant of the Jarzynski equation we propose a new method to\nnumerically determine the prior-predictive value in a Bayesian inference\nproblem. The method generalizes thermodynamic integration and is not hampered\nby equilibration problems. We demonstrate its operation by applying it to two\nsimple examples and elucidate its performance. In the case of multi-modal\nposterior distributions the performance is superior to thermodynamic\nintegration. \n\n"}
{"id": "0711.4710", "contents": "Title: Effects of network topology on wealth distributions Abstract: We focus on the problem of how wealth is distributed among the units of a\nnetworked economic system. We first review the empirical results documenting\nthat in many economies the wealth distribution is described by a combination of\nlog--normal and power--law behaviours. We then focus on the Bouchaud--M\\'ezard\nmodel of wealth exchange, describing an economy of interacting agents connected\nthrough an exchange network. We report analytical and numerical results showing\nthat the system self--organises towards a stationary state whose associated\nwealth distribution depends crucially on the underlying interaction network. In\nparticular we show that if the network displays a homogeneous density of links,\nthe wealth distribution displays either the log--normal or the power--law form.\nThis means that the first--order topological properties alone (such as the\nscale--free property) are not enough to explain the emergence of the\nempirically observed \\emph{mixed} form of the wealth distribution. In order to\nreproduce this nontrivial pattern, the network has to be heterogeneously\ndivided into regions with variable density of links. We show new results\ndetailing how this effect is related to the higher--order correlation\nproperties of the underlying network. In particular, we analyse assortativity\nby degree and the pairwise wealth correlations, and discuss the effects that\nthese properties have on each other. \n\n"}
{"id": "0801.0253", "contents": "Title: Toward a statistical mechanics of four letter words Abstract: We consider words as a network of interacting letters, and approximate the\nprobability distribution of states taken on by this network. Despite the\nintuition that the rules of English spelling are highly combinatorial (and\narbitrary), we find that maximum entropy models consistent with pairwise\ncorrelations among letters provide a surprisingly good approximation to the\nfull statistics of four letter words, capturing ~92% of the multi-information\namong letters and even \"discovering\" real words that were not represented in\nthe data from which the pairwise correlations were estimated. The maximum\nentropy model defines an energy landscape on the space of possible words, and\nlocal minima in this landscape account for nearly two-thirds of words used in\nwritten English. \n\n"}
{"id": "0801.4525", "contents": "Title: When are Extreme Events the better predictable, the larger they are? Abstract: We investigate the predictability of extreme events in time series. The focus\nof this work is to understand under which circumstances large events are better\npredictable than smaller events. Therefore we use a simple prediction algorithm\nbased on precursory structures which are identified using the maximum\nlikelihood principle. Using the receiver operator characteristic curve as a\nmeasure for the quality of predictions we find that the dependence on the event\nmagnitude is closely linked to the probability distribution function of the\nunderlying stochastic process. We evaluate this dependence on the probability\ndistribution function analytically and numerically. If we assume that the\noptimal precursory structures are used to make the predictions, we find that\nlarge increments are better predictable if the underlying stochastic process\nhas a Gaussian probability distribution function, whereas larger increments are\nharder to predict if the underlying probability distribution function has a\npower law tail. In the case of an exponential distribution function we find no\nsignificant dependence on the event magnitude. Furthermore we compare these\nresults with predictions of increments in correlated data, namely, velocity\nincrements of a free jet flow. The velocity increments in the free jet flow are\nin dependence on the time scale either asymptotically Gaussian or\nasymptotically exponential distributed. The numerical results for predictions\nwithin free jet data are in good agreement with the previous analytical\nconsiderations for random numbers. \n\n"}
{"id": "0802.1233", "contents": "Title: Algebraic Degree of Polynomial Optimization Abstract: Consider the polynomial optimization problem whose objective and constraints\nare all described by multivariate polynomials. Under some genericity\nassumptions, %% on these polynomials, we prove that the optimality conditions\nalways hold on optimizers, and the coordinates of optimizers are algebraic\nfunctions of the coefficients of the input polynomials. We also give a general\nformula for the algebraic degree of the optimal coordinates. The derivation of\nthe algebraic degree is equivalent to counting the number of all complex\ncritical points. As special cases, we obtain the algebraic degrees of\nquadratically constrained quadratic programming (QCQP), second order cone\nprogramming (SOCP) and $p$-th order cone programming (pOCP), in analogy to the\nalgebraic degree of semidefinite programming. \n\n"}
{"id": "0802.3250", "contents": "Title: Valuation of Mortality Risk via the Instantaneous Sharpe Ratio:\n  Applications to Life Annuities Abstract: We develop a theory for valuing non-diversifiable mortality risk in an\nincomplete market. We do this by assuming that the company issuing a\nmortality-contingent claim requires compensation for this risk in the form of a\npre-specified instantaneous Sharpe ratio. We apply our method to value life\nannuities. One result of our paper is that the value of the life annuity is\n{\\it identical} to the upper good deal bound of Cochrane and Sa\\'{a}-Requejo\n(2000) and of Bj\\\"{o}rk and Slinko (2006) applied to our setting. A second\nresult of our paper is that the value per contract solves a {\\it linear}\npartial differential equation as the number of contracts approaches infinity.\nOne can represent the limiting value as an expectation with respect to an\nequivalent martingale measure (as in Blanchet-Scalliet, El Karoui, and\nMartellini (2005)), and from this representation, one can interpret the\ninstantaneous Sharpe ratio as an annuity market's price of mortality risk. \n\n"}
{"id": "0804.2442", "contents": "Title: Scaling and allometry in the building geometries of Greater London Abstract: Many aggregate distributions of urban activities such as city sizes reveal\nscaling but hardly any work exists on the properties of spatial distributions\nwithin individual cities, notwithstanding considerable knowledge about their\nfractal structure. We redress this here by examining scaling relationships in a\nworld city using data on the geometric properties of individual buildings. We\nfirst summarise how power laws can be used to approximate the size\ndistributions of buildings, in analogy to city-size distributions which have\nbeen widely studied as rank-size and lognormal distributions following Zipf and\nGibrat. We then extend this analysis to allometric relationships between\nbuildings in terms of their different geometric size properties. We present\nsome preliminary analysis of building heights from the Emporis database which\nsuggests very strong scaling in world cities. The data base for Greater London\nis then introduced from which we extract 3.6 million buildings whose scaling\nproperties we explore. We examine key allometric relationships between these\ndifferent properties illustrating how building shape changes according to size,\nand we extend this analysis to the classification of buildings according to\nland use types. We conclude with an analysis of two-point correlation functions\nof building geometries which supports our non-spatial analysis of scaling. \n\n"}
{"id": "0804.3853", "contents": "Title: Modelling coloured residual noise in gravitational-wave signal\n  processing Abstract: We introduce a signal processing model for signals in non-white noise, where\nthe exact noise spectrum is a priori unknown. The model is based on a Student's\nt distribution and constitutes a natural generalization of the widely used\nnormal (Gaussian) model. This way, it allows for uncertainty in the noise\nspectrum, or more generally is also able to accommodate outliers (heavy-tailed\nnoise) in the data. Examples are given pertaining to data from gravitational\nwave detectors. \n\n"}
{"id": "0805.0828", "contents": "Title: Gradient-like observers for invariant dynamics on a Lie group Abstract: This paper proposes a design methodology for non-linear state observers for\ninvariant kinematic systems posed on finite dimensional connected Lie groups,\nand studies the associated fundamental system structure. The concept of\nsynchrony of two dynamical systems is specialised to systems on Lie groups. For\ninvariant systems this leads to a general factorisation theorem of a nonlinear\nobserver into a synchronous (internal model) term and an innovation term. The\nsynchronous term is fully specified by the system model. We propose a design\nmethodology for the innovation term based on gradient-like terms derived from\ninvariant or non-invariant cost functions. The resulting nonlinear observers\nhave strong (almost) global convergence properties and examples are used to\ndemonstrate the relevance of the proposed approach. \n\n"}
{"id": "0805.0910", "contents": "Title: Lyapunov control of a quantum particle in a decaying potential Abstract: A Lyapunov-based approach for the trajectory generation of an $N$-dimensional\nSchr{\\\"o}dinger equation in whole $\\RR^N$ is proposed. For the case of a\nquantum particle in an $N$-dimensional decaying potential the convergence is\nprecisely analyzed. The free system admitting a mixed spectrum, the dispersion\nthrough the absolutely continuous part is the main obstacle to ensure such a\nstabilization result. Whenever, the system is completely initialized in the\ndiscrete part of the spectrum, a Lyapunov strategy encoding both the distance\nwith respect to the target state and the penalization of the passage through\nthe continuous part of the spectrum, ensures the approximate stabilization. \n\n"}
{"id": "0805.2340", "contents": "Title: Stochastic expansions and Hopf algebras Abstract: We study solutions to nonlinear stochastic differential systems driven by a\nmulti-dimensional Wiener process. A useful algorithm for strongly simulating\nsuch stochastic systems is the Castell--Gaines method, which is based on the\nexponential Lie series. When the diffusion vector fields commute, it has been\nproved that at low orders this method is more accurate in the mean-square error\nthan corresponding stochastic Taylor methods. However it has also been shown\nthat when the diffusion vector fields do not commute, this is not true for\nstrong order one methods. Here we prove that when there is no drift, and the\ndiffusion vector fields do not commute, the exponential Lie series is usurped\nby the sinh-log series. In other words, the mean-square error associated with a\nnumerical method based on the sinh-log series, is always smaller than the\ncorresponding stochastic Taylor error, in fact to all orders. Our proof\nutilizes the underlying Hopf algebra structure of these series, and a\ntwo-alphabet associative algebra of shuffle and concatenation operations. We\nillustrate the benefits of the proposed series in numerical studies. \n\n"}
{"id": "0806.4876", "contents": "Title: Inconsistency of the judgment matrix in the AHP method and the decision\n  maker's knowledge Abstract: In this paper we propose a method for a quantitative estimation of the\ndecision maker's knowledge in the context of the Analytic Hierarchy Process\n(AHP) in cases, where the judgment matrix is inconsistent. We show that the\nmatrix of deviation from the transitivity condition corresponds to the rate\nmatrix for transaction costs in the financial market. For the quantitative\nestimation of the decision maker's professionalism, we apply the Ising model\nand thermodynamics tools. \n\n"}
{"id": "0807.1330", "contents": "Title: Comment on \"Bayesian Analysis of Pentaquark Signals from CLAS Data\",\n  with Response to the Reply by Ireland and Protopopsecu Abstract: The CLAS Collaboration has published an analysis using Bayesian model\nselection. My Comment criticizing their use of arbitrary prior probability\ndensity functions, and a Reply by D.G. Ireland and D. Protopopsecu, have now\nbeen published as well. This paper responds to the Reply and discusses the\nissues in more detail, with particular emphasis on the problems of priors in\nBayesian model selection. \n\n"}
{"id": "0807.2149", "contents": "Title: A Test for the Presence of a Signal, with Multiple Channels and Marked\n  Poisson Abstract: We describe a statistical hypothesis test for the presence of a signal based\non the likelihood ratio statistic. We derive the test for a special case of\ninterest. We study extensions of the test to cases where there are multiple\nchannels and to marked Poisson distributions. We show the results of a number\nof performance studies which indicate that the test works very well, even far\nout in the tails of the distribution and with multiple channels and marked\nPoisson. \n\n"}
{"id": "0808.2573", "contents": "Title: A method for characterization of coherent backgrounds in real time and\n  its application in gravitational wave data analysis Abstract: Many experiments, and in particular gravitational wave detectors, produce\ncontinuous streams of data whose frequency representations contain discrete,\nrelatively narrowband coherent features at high amplitude. We discuss the\napplication of digital Fourier transforms (DFTs) to characterization of these\nfeatures, hereafter frequently referred to as lines. Application of DFTs to\ncontinuously produced time domain data are achieved through an algorithm\nhereafter referred to as EFC for efficient time-domain determination of the\nFourier coefficients of a data set. We first define EFC and discuss parameters\nrelating to the algorithm that determine its properties and action on the data.\nIn gravitational wave interferometers, these lines are commonly due to\nparasitic sources of coherent background interference coupling into the\ninstrument. Using GEO 600 data, we next demonstrate that time domain\nsubtraction of lines can proceed without detrimental effects either on features\nat frequencies separated from that of the subtracted line, or on features at\nthe frequency of the line but having different stationarity properties. \n\n"}
{"id": "0808.2793", "contents": "Title: The Weibull - log Weibull transition of interoccurrence times for\n  synthetic and natural earthquakes Abstract: We have studied interoccurrence time distributions by analyzing the synthetic\nand three natural catalogs of the Japan Meteorological Agency (JMA), the\nSouthern California Earthquake Data Center (SCEDC), and Taiwan Central Weather\nBureau (TCWB) and revealed the universal feature of the interoccurrence time\nstatistics, Weibull - log Weibull transition. This transition reinforces the\nview that the interoccurrence time statistics possess Weibull statistics and\nlog-Weibull statistics. Here in this paper, the crossover magnitude from the\nsuperposition regime to the Weibull regime $m_c^2$ is proportional to the plate\nvelocity. In addition, we have found the region-independent relation,\n$m_c^2/m_{max} = 0.54 \\pm 0.004$. \n\n"}
{"id": "0808.3643", "contents": "Title: Bayesian Methods for Parameter Estimation in Effective Field Theories Abstract: We demonstrate and explicate Bayesian methods for fitting the parameters that\nencode the impact of short-distance physics on observables in effective field\ntheories (EFTs). We use Bayes' theorem together with the principle of maximum\nentropy to account for the prior information that these parameters should be\nnatural, i.e.O(1) in appropriate units. Marginalization can then be employed to\nintegrate the resulting probability density function (pdf) over the EFT\nparameters that are not of specific interest in the fit. We also explore\nmarginalization over the order of the EFT calculation, M, and over the\nvariable, R, that encodes the inherent ambiguity in the notion that these\nparameters are O(1). This results in a very general formula for the pdf of the\nEFT parameters of interest given a data set, D. We use this formula and the\nsimpler \"augmented chi-squared\" in a toy problem for which we generate\npseudo-data. These Bayesian methods, when used in combination with the\n\"naturalness prior\", facilitate reliable extractions of EFT parameters in cases\nwhere chi-squared methods are ambiguous at best. We also examine the problem of\nextracting the nucleon mass in the chiral limit, M_0, and the nucleon sigma\nterm, from pseudo-data on the nucleon mass as a function of the pion mass. We\nfind that Bayesian techniques can provide reliable information on M_0, even if\nsome of the data points used for the extraction lie outside the region of\napplicability of the EFT. \n\n"}
{"id": "0809.1260", "contents": "Title: Necessary and Sufficient Conditions for Success of the Nuclear Norm\n  Heuristic for Rank Minimization Abstract: Minimizing the rank of a matrix subject to constraints is a challenging\nproblem that arises in many applications in control theory, machine learning,\nand discrete geometry. This class of optimization problems, known as rank\nminimization, is NP-HARD, and for most practical problems there are no\nefficient algorithms that yield exact solutions. A popular heuristic algorithm\nreplaces the rank function with the nuclear norm--equal to the sum of the\nsingular values--of the decision variable. In this paper, we provide a\nnecessary and sufficient condition that quantifies when this heuristic\nsuccessfully finds the minimum rank solution of a linear constraint set. We\nadditionally provide a probability distribution over instances of the affine\nrank minimization problem such that instances sampled from this distribution\nsatisfy our conditions for success with overwhelming probability provided the\nnumber of constraints is appropriately large. Finally, we give empirical\nevidence that these probabilistic bounds provide accurate predictions of the\nheuristic's performance in non-asymptotic scenarios. \n\n"}
{"id": "0810.0479", "contents": "Title: Mental States as Macrostates Emerging from EEG Dynamics Abstract: Correlations between psychological and physiological phenomena form the basis\nfor different medical and scientific disciplines, but the nature of this\nrelation has not yet been fully understood. One conceptual option is to\nunderstand the mental as \"emerging\" from neural processes in the specific sense\nthat psychology and physiology provide two different descriptions of the same\nsystem. Stating these descriptions in terms of coarser- and finer-grained\nsystem states (macro- and microstates), the two descriptions may be equally\nadequate if the coarse-graining preserves the possibility to obtain a dynamical\nrule for the system. To test the empirical viability of our approach, we\ndescribe an algorithm to obtain a specific form of such a coarse-graining from\ndata, and illustrate its operation using a simulated dynamical system. We then\napply the method to an electroencephalographic (EEG) recording, where we are\nable to identify macrostates from the physiological data that correspond to\nmental states of the subject. \n\n"}
{"id": "0810.4341", "contents": "Title: Entropy of Hidden Markov Processes via Cycle Expansion Abstract: Hidden Markov Processes (HMP) is one of the basic tools of the modern\nprobabilistic modeling. The characterization of their entropy remains however\nan open problem. Here the entropy of HMP is calculated via the cycle expansion\nof the zeta-function, a method adopted from the theory of dynamical systems.\nFor a class of HMP this method produces exact results both for the entropy and\nthe moment-generating function. The latter allows to estimate, via the Chernoff\nbound, the probabilities of large deviations for the HMP. More generally, the\nmethod offers a representation of the moment-generating function and of the\nentropy via convergent series. \n\n"}
{"id": "0811.0208", "contents": "Title: Biased tug-of-war, the biased infinity Laplacian, and comparison with\n  exponential cones Abstract: We prove that if U\\subset\\R^n is an open domain whose closure \\overline{U} is\ncompact in the path metric, and F is a Lipschitz function on \\partial{U}, then\nfor each \\beta\\in\\R there exists a unique viscosity solution to the\n\\beta-biased infinity Laplacian equation \\beta |\\nabla u| + \\Delta_\\infty u=0\non U that extends F, where \\Delta_\\infty u= |\\nabla u|^{-2} \\sum_{i,j}\nu_{x_i}u_{x_ix_j} u_{x_j}.\n  In the proof, we extend the tug-of-war ideas of Peres, Schramm, Sheffield and\nWilson, and define the \\beta-biased \\eps-game as follows. The starting position\nis x_0 \\in U. At the k^\\text{th} step the two players toss a suitably biased\ncoin (in our key example, player I wins with odds of \\exp(\\beta\\eps) to 1), and\nthe winner chooses x_k with d(x_k,x_{k-1}) < \\eps. The game ends when x_k \\in\n\\partial{U}, and player II pays the amount F(x_k) to player I. We prove that\nthe value u^{\\eps}(x_0) of this game exists, and that \\|u^\\eps - u\\|_\\infty \\to\n0 as \\eps \\to 0, where u is the unique extension of F to \\overline{U} that\nsatisfies comparison with \\beta-exponential cones. Comparison with exponential\ncones is a notion that we introduce here, and generalizing a theorem of\nCrandall, Evans and Gariepy regarding comparison with linear cones, we show\nthat a continuous function satisfies comparison with \\beta-exponential cones if\nand only if it is a viscosity solution to the \\beta-biased infinity Laplacian\nequation. \n\n"}
{"id": "0811.3856", "contents": "Title: Levy flights, dynamical duality and fractional quantum mechanics Abstract: We discuss dual time evolution scenarios which, albeit running according to\nthe same real time clock, in each considered case may be mapped among each\nother by means of an analytic continuation in time. This dynamical duality is a\ngeneric feature of diffusion-type processes. Technically that involves a\nfamiliar transformation from a non-Hermitian Fokker-Planck operator to the\nHermitian operator (e.g. Schroedinger Hamiltonian), whose negative is known to\ngenerate a dynamical semigroup. Under suitable restrictions upon the generator,\nthe semigroup admits an analytic continuation in time and ultimately yields\ndual motions. We analyze an extension of the duality concept to Levy flights,\nfree and with an external forcing, while presuming that the corresponding\nevolution rule (fractional dynamical semigroup) is a dual counterpart of the\nquantum motion (fractional unitary dynamics). \n\n"}
{"id": "0812.1245", "contents": "Title: Mathematical Framework for Fast and Rigorous Track Fit for the ZEUS\n  Detector Abstract: In this note we present a mathematical framework for a rigorous approach to a\ncommon track fit for trackers located in the inner region of the ZEUS detector.\nThe approach makes use of the Kalman filter and offers a rigorous treatment of\nmagnetic field inhomogeneity, multiple scattering and energy loss. We describe\nmathematical details of the implementation of the Kalman filter technique with\na reduced amount of computations for a cylindrical drift chamber, barrel and\nforward silicon strip detectors and a forward straw drift chamber. Options with\nhomogeneous and inhomogeneous field are discussed. The fitting of tracks in one\nZEUS event takes about of 20ms on standard PC. \n\n"}
{"id": "0901.3183", "contents": "Title: Heavy-tailed distributions in fatal traffic accidents: role of human\n  activities Abstract: Human activities can play a crucial role in the statistical properties of\nobservables in many complex systems such as social, technological and economic\nsystems. We demonstrate this by looking into the heavy-tailed distributions of\nobservables in fatal plane and car accidents. Their origin is examined and can\nbe understood as stochastic processes that are related to human activities.\nSimple mathematical models are proposed to illustrate such processes and\ncompared with empirical results obtained from existing databanks. \n\n"}
{"id": "0902.1189", "contents": "Title: An Answer to S. Simons' Question on the Maximal Monotonicity of the Sum\n  of a Maximal Monotone Linear Operator and a Normal Cone Operator Abstract: The question whether or not the sum of two maximal monotone operators is\nmaximal monotone under Rockafellar's constraint qualification - that is,\nwhether or not \"the sum theorem\" is true - is the most famous open problem in\nMonotone Operator Theory. In his 2008 monograph \"From Hahn-Banach to\nMonotonicity\", Stephen Simons asked whether or not the sum theorem holds for\nthe special case of a maximal monotone linear operator and a normal cone\noperator of a closed convex set provided that the interior of the set makes a\nnonempty intersection with the domain of the linear operator.\n  In this note, we provide an affirmative answer to Simons' question. In fact,\nwe show that the sum theorem is true for a maximal monotone linear relation and\na normal cone operator. The proof relies on Rockafellar's formula for the\nFenchel conjugate of the sum as well as some results featuring the Fitzpatrick\nfunction. \n\n"}
{"id": "0902.1565", "contents": "Title: Mathematically equivalent approaches for equality constrained Kalman\n  Filtering Abstract: Kalman Filtering problems often have inherent and known constraints in the\nphysical dynamics that are not exploited despite potentially significant gains\n(e.g., fixed speed of a motor). In this paper, we review existing methods and\npropose some new ideas for filtering in the presence of equality constraints.\nWe then show that three methods for incorporating state space equality\nconstraints are mathematically equivalent to the more general \"Projection\"\nmethod, which allows different weighting matrices when projecting the estimate.\nStill, the different approaches have advantages in implementations that may\nmake one better suited than another for a given application. \n\n"}
{"id": "0902.3841", "contents": "Title: Ultimate \"SIR\" in Autonomous Linear Networks with Symmetric Weight\n  Matrices, and Its Use to Stabilize the Network - A Hopfield-like network Abstract: In this paper, we present and analyse two Hopfield-like nonlinear networks,\nin continuous-time and discrete-time respectively. The proposed network is\nbased on an autonomous linear system with a symmetric weight matrix, which is\ndesigned to be unstable, and a nonlinear function stabilizing the whole network\nthanks to a manipulated state variable called``ultimate SIR''. This variable is\nobserved to be equal to the traditional Signal-to-Interference Ratio (SIR)\ndefinition in telecommunications engineering.\n  The underlying linear system of the proposed continuous-time network is\n$\\dot{{\\mathbf x}} = {\\mathbf B} {\\mathbf x}$ where {\\bf B} is a real symmetric\nmatrix whose diagonal elements are fixed to a constant. The nonlinear function,\non the other hand, is based on the defined system variables called ``SIR''s. We\nalso show that the ``SIR''s of all the states converge to a constant value,\ncalled ``system-specific Ultimate SIR''; which is equal to\n$\\frac{r}{\\lambda_{max}}$ where $r$ is the diagonal element of matrix ${\\bf B}$\nand $\\lambda_{max}$ is the maximum (positive) eigenvalue of diagonally-zero\nmatrix $({\\bf B} - r{\\bf I})$, where ${\\bf I}$ denotes the identity matrix. The\nsame result is obtained in its discrete-time version as well.\n  Computer simulations for binary associative memory design problem show the\neffectiveness of the proposed network as compared to the traditional Hopfield\nNetworks. \n\n"}
{"id": "0903.0010", "contents": "Title: Quantitative law describing market dynamics before and after\n  interest-rate change Abstract: We study the behavior of U.S. markets both before and after U.S. Federal Open\nMarket Committee (FOMC) meetings, and show that the announcement of a U.S.\nFederal Reserve rate change causes a financial shock, where the dynamics after\nthe announcement is described by an analogue of the Omori earthquake law. We\nquantify the rate n(t) of aftershocks following an interest rate change at time\nT, and find power-law decay which scales as n(t-T) (t-T)^-$\\Omega$, with\n$\\Omega$ positive. Surprisingly, we find that the same law describes the rate\nn'(|t-T|) of \"pre-shocks\" before the interest rate change at time T. This is\nthe first study to quantitatively relate the size of the market response to the\nnews which caused the shock and to uncover the presence of quantifiable\npreshocks. We demonstrate that the news associated with interest rate change is\nresponsible for causing both the anticipation before the announcement and the\nsurprise after the announcement. We estimate the magnitude of financial news\nusing the relative difference between the U. S. Treasury Bill and the Federal\nFunds Effective rate. Our results are consistent with the \"sign effect,\" in\nwhich \"bad news\" has a larger impact than \"good news.\" Furthermore, we observe\nsignificant volatility aftershocks, confirming a \"market underreaction\" that\nlasts at least 1 trading day. \n\n"}
{"id": "0903.3178", "contents": "Title: Link communities reveal multiscale complexity in networks Abstract: Networks have become a key approach to understanding systems of interacting\nobjects, unifying the study of diverse phenomena including biological organisms\nand human society. One crucial step when studying the structure and dynamics of\nnetworks is to identify communities: groups of related nodes that correspond to\nfunctional subunits such as protein complexes or social spheres. Communities in\nnetworks often overlap such that nodes simultaneously belong to several groups.\nMeanwhile, many networks are known to possess hierarchical organization, where\ncommunities are recursively grouped into a hierarchical structure. However, the\nfact that many real networks have communities with pervasive overlap, where\neach and every node belongs to more than one group, has the consequence that a\nglobal hierarchy of nodes cannot capture the relationships between overlapping\ngroups. Here we reinvent communities as groups of links rather than nodes and\nshow that this unorthodox approach successfully reconciles the antagonistic\norganizing principles of overlapping communities and hierarchy. In contrast to\nthe existing literature, which has entirely focused on grouping nodes, link\ncommunities naturally incorporate overlap while revealing hierarchical\norganization. We find relevant link communities in many networks, including\nmajor biological networks such as protein-protein interaction and metabolic\nnetworks, and show that a large social network contains hierarchically\norganized community structures spanning inner-city to regional scales while\nmaintaining pervasive overlap. Our results imply that link communities are\nfundamental building blocks that reveal overlap and hierarchical organization\nin networks to be two aspects of the same phenomenon. \n\n"}
{"id": "0904.4863", "contents": "Title: A two-stage algorithm for extracting the multiscale backbone of complex\n  weighted networks Abstract: The central problem of concern to Serrano, Boguna and Vespignani (\"Extracting\nthe multiscale backbone of complex weighted networks\", Proc Natl Acad Sci\n106:6483-6488 [2009]) can be effectively and elegantly addressed using a\nwell-established two-stage algorithm that has been applied to internal\nmigration flows for numerous nations and several other forms of \"transaction\nflow data\". \n\n"}
{"id": "0905.0724", "contents": "Title: sFit: a method for background subtraction in maximum likelihood fit Abstract: This paper presents a statistical method to subtract background in maximum\nlikelihood fit, without relying on any separate sideband or simulation for\nbackground modeling. The method, called sFit, is an extension to the sPlot\ntechnique originally developed to reconstruct true distribution for each date\ncomponent. The sWeights defined for the sPlot technique allow to construct a\nmodified likelihood function using only the signal probability density function\nand events in the signal region. Contribution of background events in the\nsignal region to the likelihood function cancels out on a statistical basis.\nMaximizing this likelihood function leads to unbiased estimates of the fit\nparameters in the signal probability density function. \n\n"}
{"id": "0906.0060", "contents": "Title: A Walk in Facebook: Uniform Sampling of Users in Online Social Networks Abstract: Our goal in this paper is to develop a practical framework for obtaining a\nuniform sample of users in an online social network (OSN) by crawling its\nsocial graph. Such a sample allows to estimate any user property and some\ntopological properties as well. To this end, first, we consider and compare\nseveral candidate crawling techniques. Two approaches that can produce\napproximately uniform samples are the Metropolis-Hasting random walk (MHRW) and\na re-weighted random walk (RWRW). Both have pros and cons, which we demonstrate\nthrough a comparison to each other as well as to the \"ground truth.\" In\ncontrast, using Breadth-First-Search (BFS) or an unadjusted Random Walk (RW)\nleads to substantially biased results. Second, and in addition to offline\nperformance assessment, we introduce online formal convergence diagnostics to\nassess sample quality during the data collection process. We show how these\ndiagnostics can be used to effectively determine when a random walk sample is\nof adequate size and quality. Third, as a case study, we apply the above\nmethods to Facebook and we collect the first, to the best of our knowledge,\nrepresentative sample of Facebook users. We make it publicly available and\nemploy it to characterize several key properties of Facebook. \n\n"}
{"id": "0906.3271", "contents": "Title: Regression modeling method of space weather prediction Abstract: A regression modeling method of space weather prediction is proposed. It\nallows forecasting Dst index up to 6 hours ahead with about 90% correlation. It\ncan also be used for constructing phenomenological models of interaction\nbetween the solar wind and the magnetosphere. With its help two new\ngeoeffective parameters were found: latitudinal and longitudinal flow angles of\nthe solar wind. It was shown that Dst index remembers its previous values for\n2000 hours. \n\n"}
{"id": "0907.1020", "contents": "Title: Convergence and Convergence Rate of Stochastic Gradient Search in the\n  Case of Multiple and Non-Isolated Extrema Abstract: The asymptotic behavior of stochastic gradient algorithms is studied. Relying\non results from differential geometry (Lojasiewicz gradient inequality), the\nsingle limit-point convergence of the algorithm iterates is demonstrated and\nrelatively tight bounds on the convergence rate are derived. In sharp contrast\nto the existing asymptotic results, the new results presented here allow the\nobjective function to have multiple and non-isolated minima. The new results\nalso offer new insights into the asymptotic properties of several classes of\nrecursive algorithms which are routinely used in engineering, statistics,\nmachine learning and operations research. \n\n"}
{"id": "0908.4334", "contents": "Title: One and two side generalisations of the log-Normal distribution by means\n  of a new product definition Abstract: In this manuscript we introduce a generalisation of the log-Normal\ndistribution that is inspired by a modification of the Kaypten multiplicative\nprocess using the $q$-product of Borges [Physica A \\textbf{340}, 95 (2004)].\nDepending on the value of q the distribution increases the tail for small (when\n$q<1$) or large (when $q>1$) values of the variable upon analysis. The usual\nlog-Normal distribution is retrieved when $q=1$. The main statistical features\nof this distribution are presented as well as a related random number\ngenerators and tables of quantiles of the Kolmogorov-Smirnov. Lastly, we\nillustrate the application of this distribution studying the adjustment of a\nset of variables of biological and financial origin. \n\n"}
{"id": "0909.0747", "contents": "Title: Parameter Estimation from Time-Series Data with Correlated Errors: A\n  Wavelet-Based Method and its Application to Transit Light Curves Abstract: We consider the problem of fitting a parametric model to time-series data\nthat are afflicted by correlated noise. The noise is represented by a sum of\ntwo stationary Gaussian processes: one that is uncorrelated in time, and\nanother that has a power spectral density varying as $1/f^\\gamma$. We present\nan accurate and fast [O(N)] algorithm for parameter estimation based on\ncomputing the likelihood in a wavelet basis. The method is illustrated and\ntested using simulated time-series photometry of exoplanetary transits, with\nparticular attention to estimating the midtransit time. We compare our method\nto two other methods that have been used in the literature, the time-averaging\nmethod and the residual-permutation method. For noise processes that obey our\nassumptions, the algorithm presented here gives more accurate results for\nmidtransit times and truer estimates of their uncertainties. \n\n"}
{"id": "0909.3871", "contents": "Title: Computational Geometric Optimal Control of Connected Rigid Bodies in a\n  Perfect Fluid Abstract: This paper formulates an optimal control problem for a system of rigid bodies\nthat are connected by ball joints and immersed in an irrotational and\nincompressible fluid. The rigid bodies can translate and rotate in\nthree-dimensional space, and each joint has three rotational degrees of\nfreedom. We assume that internal control moments are applied at each joint. We\npresent a computational procedure for numerically solving this optimal control\nproblem, based on a geometric numerical integrator referred to as a Lie group\nvariational integrator. This computational approach preserves the Hamiltonian\nstructure of the controlled system and the Lie group configuration manifold of\nthe connected rigid bodies, thereby finding complex optimal maneuvers of\nconnected rigid bodies accurately and efficiently. This is illustrated by\nnumerical computations. \n\n"}
{"id": "0910.0055", "contents": "Title: Statistical Tests for Scaling in the Inter-Event Times of Earthquakes in\n  California Abstract: We explore in depth the validity of a recently proposed scaling law for\nearthquake interevent time distributions in the case of the Southern\nCalifornia, using the waveform cross-correlation catalog of Shearer et al. Two\nstatistical tests are used: on the one hand, the standard two-sample\nKolmogorov-Smirnov test is in agreement with the scaling of the distributions.\nOn the other hand, the one-sample Kolmogorov-Smirnov statistic complemented\nwith Monte Carlo simulation of the inter-event times, as done by Clauset et\nal., supports the validity of the gamma distribution as a simple model of the\nscaling function appearing on the scaling law, for rescaled inter-event times\nabove 0.01, except for the largest data set (magnitude greater than 2). A\ndiscussion of these results is provided. \n\n"}
{"id": "0910.5502", "contents": "Title: Correction to \"An Efficient Game Form for Unicast Service Provisioning\" Abstract: A correction to the specification of the mechanism proposed in \"An Efficient\nGame Form for Unicast Service Provisioning\" is given. \n\n"}
{"id": "0910.5673", "contents": "Title: Synchronization and Transient Stability in Power Networks and\n  Non-Uniform Kuramoto Oscillators Abstract: Motivated by recent interest for multi-agent systems and smart power grid\narchitectures, we discuss the synchronization problem for the network-reduced\nmodel of a power system with non-trivial transfer conductances. Our key insight\nis to exploit the relationship between the power network model and a\nfirst-order model of coupled oscillators. Assuming overdamped generators\n(possibly due to local excitation controllers), a singular perturbation\nanalysis shows the equivalence between the classic swing equations and a\nnon-uniform Kuramoto model. Here, non-uniform Kuramoto oscillators are\ncharacterized by multiple time constants, non-homogeneous coupling, and\nnon-uniform phase shifts. Extending methods from transient stability,\nsynchronization theory, and consensus protocols, we establish sufficient\nconditions for synchronization of non-uniform Kuramoto oscillators. These\nconditions reduce to and improve upon previously-available tests for the\nstandard Kuramoto model. Combining our singular perturbation and Kuramoto\nanalyses, we derive concise and purely algebraic conditions that relate\nsynchronization and transient stability of a power network to the underlying\nsystem parameters and initial conditions. \n\n"}
{"id": "0911.2176", "contents": "Title: Individual popularity and activity in online social systems Abstract: We propose a stochastic model of web user behaviors in online social systems,\nand study the influence of attraction kernel on statistical property of user or\nitem occurrence. Combining the different growth patterns of new entities and\nattraction patterns of old ones, different heavy-tailed distributions for\npopularity and activity which have been observed in real life, can be obtained.\n  From a broader perspective, we explore the underlying principle governing the\nstatistical feature of individual popularity and activity in online social\nsystems and point out the potential simple mechanism underlying the complex\ndynamics of the systems. \n\n"}
{"id": "0911.2750", "contents": "Title: Positive Polynomials and Projections of Spectrahedra Abstract: This work is concerned with different aspects of spectrahedra and their\nprojections, sets that are important in semidefinite optimization. We prove\nresults on the limitations of so called Lasserre and theta body relaxation\nmethods for semialgebraic sets and varieties. As a special case we obtain the\nmain result of the paper \"Exposed faces of semidefinite representable sets\" of\nNetzer, Plaumann and Schweighofer. We also solve the open problems from that\nwork. We further prove some helpful facts which can not be found in the\nexisting literature, for example that the closure of a projection of a\nspectrahedron is again such a projection. We give a unified account of several\nresults on convex hulls of curves and images of polynomial maps. We finally\nprove a Positivstellensatz for projections of spectrahedra, which exceeds the\nknown results that only work for basic closed semialgebraic sets. \n\n"}
{"id": "0911.2850", "contents": "Title: A simple event weighting technique for optimizing the measurement of the\n  forward-backward asymmetry of Drell-Yan dilepton pairs at hadron colliders Abstract: We describe a simple technique for optimizing the extraction of the\nforward-backward asymmetry ($A_{fb}$) of Drell-Yan lepton pairs ($e^+e^-$,$\n\\mu^+\\mu^-$) produced in $\\bar{p}p$ and $pp$ collisions at hadron colliders.\nThe method employs simple event weights which are functions of the rapidity and\n$cos\\theta$ decay angle of the lepton pair. It yields the best estimate of the\nacceptance corrected parton level ($\\bar{q}q$) forward backward asymmetry as a\nfunction of final state dilepton mass ($M_{\\ell\\ell}$). Typically, when\ncompared to the simple count method, the technique reduces the statistical\nerrors by 20% for $\\bar{p}p$, and 40% for $pp$ collisions, respectively. The\ntechnique can be used to search for new high mass and large width Z' bosons\nwhich may be best detected through the observation of deviations from the\nStandard Model expectation for the forward-backward asymmetry. In addition, we\nderive expressions for the QCD angular coefficients for Drell-Yan events. \n\n"}
{"id": "0911.3249", "contents": "Title: Emergence of scale invariance and efficiency in a racetrack betting\n  market Abstract: We study the time change of the relation between the rank of a racehorse in\nthe Japan Racing Association and the result of victory or defeat. Horses are\nranked according to the win bet fractions. As the vote progresses, the\nracehorses are mixed on the win bet fraction axis. We see the emergence of a\nscale invariant relation between the cumulative distribution function of the\nwinning horse $x_{1}$ and that of the losing horse $x_{0}$. $x_{1}\\propto\nx_{0}^{\\alpha}$ holds in the small win bet fraction region. We also see the\nefficiency of the market as the vote proceeds. However, the convergence to the\nefficient state is not monotonic. The time change of the distribution of a vote\nis complicated. Votes resume concentration on popular horses, after the\ndistribution spreads to a certain extent. In order to explain scale invariance,\nwe introduce a simple voting model. In a `double' scaling limit, we show that\nthe exact scale invariance relation $x_{1}=x_{0}^{\\alpha}$ holds over the\nentire range $0\\le x_{0},x_{1}\\le 1$. \n\n"}
{"id": "0911.4207", "contents": "Title: An information theoretic approach to statistical dependence: copula\n  information Abstract: We discuss the connection between information and copula theories by showing\nthat a copula can be employed to decompose the information content of a\nmultivariate distribution into marginal and dependence components, with the\nlatter quantified by the mutual information. We define the information excess\nas a measure of deviation from a maximum entropy distribution. The idea of\nmarginal invariant dependence measures is also discussed and used to show that\nempirical linear correlation underestimates the amplitude of the actual\ncorrelation in the case of non-Gaussian marginals. The mutual information is\nshown to provide an upper bound for the asymptotic empirical log-likelihood of\na copula. An analytical expression for the information excess of T-copulas is\nprovided, allowing for simple model identification within this family. We\nillustrate the framework in a financial data set. \n\n"}
{"id": "0912.3033", "contents": "Title: When is multidimensional screening a convex program? Abstract: A principal wishes to transact business with a multidimensional distribution\nof agents whose preferences are known only in the aggregate. Assuming a twist\n(= generalized Spence-Mirrlees single-crossing) hypothesis and that agents can\nchoose only pure strategies, we identify a structural condition on the\npreference b(x,y) of agent type x for product type y -- and on the principal's\ncosts c(y) -- which is necessary and sufficient for reducing the profit\nmaximization problem faced by the principal to a convex program. This is a key\nstep toward making the principal's problem theoretically and computationally\ntractable; in particular, it allows us to derive uniqueness and stability of\nthe principal's optimum strategy -- and similarly of the strategy maximizing\nthe expected welfare of the agents when the principal's profitability is\nconstrained. We call this condition non-negative cross-curvature: it is also\n(i) necessary and sufficient to guarantee convexity of the set of b-convex\nfunctions, (ii) invariant under reparametrization of agent and/or product types\nby diffeomorphisms, and (iii) a strengthening of Ma, Trudinger and Wang's\nnecessary and sufficient condition (A3w) for continuity of the correspondence\nbetween an exogenously prescribed distribution of agents and of products. We\nderive the persistence of economic effects such as the desirability for a\nmonopoly to establish prices so high they effectively exclude a positive\nfraction of its potential customers, in nearly the full range of non-negatively\ncross-curved models. \n\n"}
{"id": "0912.4570", "contents": "Title: Fast Multiple Splitting Algorithms for Convex Optimization Abstract: We present in this paper two different classes of general $K$-splitting\nalgorithms for solving finite-dimensional convex optimization problems. Under\nthe assumption that the function being minimized has a Lipschitz continuous\ngradient, we prove that the number of iterations needed by the first class of\nalgorithms to obtain an $\\epsilon$-optimal solution is $O(1/\\epsilon)$. The\nalgorithms in the second class are accelerated versions of those in the first\nclass, where the complexity result is improved to $O(1/\\sqrt{\\epsilon})$ while\nthe computational effort required at each iteration is almost unchanged. To the\nbest of our knowledge, the complexity results presented in this paper are the\nfirst ones of this type that have been given for splitting and alternating\ndirection type methods. Moreover, all algorithms proposed in this paper are\nparallelizable, which makes them particularly attractive for solving certain\nlarge-scale problems. \n\n"}
{"id": "1001.2722", "contents": "Title: A Fractional Calculus of Variations for Multiple Integrals with\n  Application to Vibrating String Abstract: We introduce a fractional theory of the calculus of variations for multiple\nintegrals. Our approach uses the recent notions of Riemann-Liouville fractional\nderivatives and integrals in the sense of Jumarie. Main results provide\nfractional versions of the theorems of Green and Gauss, fractional\nEuler-Lagrange equations, and fractional natural boundary conditions. As an\napplication we discuss the fractional equation of motion of a vibrating string. \n\n"}
{"id": "1001.4351", "contents": "Title: Analytical continuation of imaginary axis data using maximum entropy Abstract: We study the maximum entropy (MaxEnt) approach for analytical continuation of\nspectral data from imaginary times to real frequencies. The total error is\ndivided in a statistical error, due to the noise in the input data, and a\nsystematic error, due to deviations of the default function, used in the MaxEnt\napproach, from the exact spectrum. We find that the MaxEnt approach in its\nclassical formulation can lead to a nonoptimal balance between the two types of\nerrors, leading to an unnecessary large statistical error. The statistical\nerror can be reduced by splitting up the data in several batches, performing a\nMaxEnt calculation for each batch and averaging. This can outweigh an increase\nin the systematic error resulting from this approach. The output from the\nMaxEnt result can be used as a default function for a new MaxEnt calculation.\nSuch iterations often lead to worse results due to an increase in the\nstatistical error. By splitting up the data in batches, the statistical error\nis reduced and and the increase resulting from iterations can be outweighed by\na decrease in the systematic error. Finally we consider a linearized version to\nobtain a better understanding of the method. \n\n"}
{"id": "1002.1043", "contents": "Title: Statistical measures applied to metal clusters: evidence of magic\n  numbers Abstract: In this work, a shell model for metal clusters up to 220 valence electrons is\nused to obtain the fractional occupation probabilities of the electronic\norbitals. Then, the calculation of a statistical measure of complexity and the\nFisher-Shannon information is carried out. An increase of both magnitudes with\nthe number of valence electrons is observed. The shell structure is reflected\nby the behavior of the statistical complexity. The magic numbers are indicated\nby the Fisher-Shannon information. So, as in the case of atomic nuclei, the\nstudy of statistical indicators also unveil the existence of magic numbers in\nmetal clusters. \n\n"}
{"id": "1002.3861", "contents": "Title: Zipf's Law Leads to Heaps' Law: Analyzing Their Relation in Finite-Size\n  Systems Abstract: Background: Zipf's law and Heaps' law are observed in disparate complex\nsystems. Of particular interests, these two laws often appear together. Many\ntheoretical models and analyses are performed to understand their co-occurrence\nin real systems, but it still lacks a clear picture about their relation.\nMethodology/Principal Findings: We show that the Heaps' law can be considered\nas a derivative phenomenon if the system obeys the Zipf's law. Furthermore, we\nrefine the known approximate solution of the Heaps' exponent provided the\nZipf's exponent. We show that the approximate solution is indeed an asymptotic\nsolution for infinite systems, while in the finite-size system the Heaps'\nexponent is sensitive to the system size. Extensive empirical analysis on tens\nof disparate systems demonstrates that our refined results can better capture\nthe relation between the Zipf's and Heaps' exponents. Conclusions/Significance:\nThe present analysis provides a clear picture about the relation between the\nZipf's law and Heaps' law without the help of any specific stochastic model,\nnamely the Heaps' law is indeed a derivative phenomenon from Zipf's law. The\npresented numerical method gives considerably better estimation of the Heaps'\nexponent given the Zipf's exponent and the system size. Our analysis provides\nsome insights and implications of real complex systems, for example, one can\nnaturally obtained a better explanation of the accelerated growth of scale-free\nnetworks. \n\n"}
{"id": "1003.5819", "contents": "Title: A unified controllability/observability theory for some stochastic and\n  deterministic partial differential equations Abstract: The purpose of this paper is to present a universal approach to the study of\ncontrollability/observability problems for infinite dimensional systems\ngoverned by some stochastic/deterministic partial differential equations. The\ncrucial analytic tool is a class of fundamental weighted identities for\nstochastic/deterministic partial differential operators, via which one can\nderive the desired global Carleman estimates. This method can also give a\nunified treatment of the stabilization, global unique continuation, and inverse\nproblems for some stochastic/deterministic partial differential equations. \n\n"}
{"id": "1004.1095", "contents": "Title: Control of one-dimensional guided formations using coarse information Abstract: Motivated by applications in intelligent highway systems, the paper studies\nthe problem of guiding mobile agents in a one-dimensional formation to their\ndesired relative positions. Only coarse information is used which is\ncommunicated from a guidance system that monitors in real time the agents'\nmotions. The desired relative positions are defined by the given distance\nconstraints between the agents under which the overall formation is rigid in\nshape and thus admits locally a unique realization. It is shown that even when\nthe guidance system can only transmit at most four bits of information to each\nagent, it is still possible to design control laws to guide the agents to their\ndesired positions. We further delineate the thin set of initial conditions for\nwhich the proposed control law may fail using the example of a three-agent\nformation. Tools from non-smooth analysis are utilized for the convergence\nanalysis. \n\n"}
{"id": "1004.1447", "contents": "Title: The Total s-Energy of a Multiagent System Abstract: We introduce the \"total s-energy\" of a multiagent system with time-dependent\nlinks. This provides a new analytical lens on bidirectional agreement dynamics,\nwhich we use to bound the convergence rates of dynamical systems for\nsynchronization, flocking, opinion dynamics, and social epistemology. \n\n"}
{"id": "1004.1690", "contents": "Title: Random Ancestor Trees Abstract: We investigate a network growth model in which the genealogy controls the\nevolution. In this model, a new node selects a random target node and links\neither to this target node, or to its parent, or to its grandparent, etc; all\nnodes from the target node to its most ancient ancestor are equiprobable\ndestinations. The emerging random ancestor tree is very shallow: the fraction\ng_n of nodes at distance n from the root decreases super-exponentially with n,\ng_n=e^{-1}/(n-1)!. We find that a macroscopic hub at the root coexists with\nhighly connected nodes at higher generations. The maximal degree of a node at\nthe nth generation grows algebraically as N^{1/beta_n} where N is the system\nsize. We obtain the series of nontrivial exponents which are roots of\ntranscendental equations: beta_1= 1.351746, beta_2=1.682201, etc. As a\nconsequence, the fraction p_k of nodes with degree k has algebraic tail, p_k ~\nk^{-gamma}, with gamma=beta_1+1=2.351746. \n\n"}
{"id": "1004.2316", "contents": "Title: Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable\n  Information Criterion in Singular Learning Theory Abstract: In regular statistical models, the leave-one-out cross-validation is\nasymptotically equivalent to the Akaike information criterion. However, since\nmany learning machines are singular statistical models, the asymptotic behavior\nof the cross-validation remains unknown. In previous studies, we established\nthe singular learning theory and proposed a widely applicable information\ncriterion, the expectation value of which is asymptotically equal to the\naverage Bayes generalization loss. In the present paper, we theoretically\ncompare the Bayes cross-validation loss and the widely applicable information\ncriterion and prove two theorems. First, the Bayes cross-validation loss is\nasymptotically equivalent to the widely applicable information criterion as a\nrandom variable. Therefore, model selection and hyperparameter optimization\nusing these two values are asymptotically equivalent. Second, the sum of the\nBayes generalization error and the Bayes cross-validation error is\nasymptotically equal to $2\\lambda/n$, where $\\lambda$ is the real log canonical\nthreshold and $n$ is the number of training samples. Therefore the relation\nbetween the cross-validation error and the generalization error is determined\nby the algebraic geometrical structure of a learning machine. We also clarify\nthat the deviance information criteria are different from the Bayes\ncross-validation and the widely applicable information criterion. \n\n"}
{"id": "1004.2778", "contents": "Title: Tropical polar cones, hypergraph transversals, and mean payoff games Abstract: We discuss the tropical analogues of several basic questions of convex\nduality. In particular, the polar of a tropical polyhedral cone represents the\nset of linear inequalities that its elements satisfy. We characterize the\nextreme rays of the polar in terms of certain minimal set covers which may be\nthought of as weighted generalizations of minimal transversals in hypergraphs.\nWe also give a tropical analogue of Farkas lemma, which allows one to check\nwhether a linear inequality is implied by a finite family of linear\ninequalities. Here, the certificate is a strategy of a mean payoff game. We\ndiscuss examples, showing that the number of extreme rays of the polar of the\ntropical cyclic polyhedral cone is polynomially bounded, and that there is no\nunique minimal system of inequalities defining a given tropical polyhedral\ncone. \n\n"}
{"id": "1004.3113", "contents": "Title: Minimal modified energy control for fractional linear control systems\n  with the Caputo derivative Abstract: Fractional control systems with the Caputo derivative are considered. The\nmodified controllability Gramian and the minimum energy optimal control problem\nare investigated. Construction of minimizing steering controls for the modified\nenergy functional are proposed. \n\n"}
{"id": "1004.4704", "contents": "Title: Homophily and Contagion Are Generically Confounded in Observational\n  Social Network Studies Abstract: We consider processes on social networks that can potentially involve three\nfactors: homophily, or the formation of social ties due to matching individual\ntraits; social contagion, also known as social influence; and the causal effect\nof an individual's covariates on their behavior or other measurable responses.\nWe show that, generically, all of these are confounded with each other.\nDistinguishing them from one another requires strong assumptions on the\nparametrization of the social process or on the adequacy of the covariates used\n(or both). In particular we demonstrate, with simple examples, that asymmetries\nin regression coefficients cannot identify causal effects, and that very simple\nmodels of imitation (a form of social contagion) can produce substantial\ncorrelations between an individual's enduring traits and their choices, even\nwhen there is no intrinsic affinity between them. We also suggest some possible\nconstructive responses to these results. \n\n"}
{"id": "1004.5151", "contents": "Title: Weak Recovery Conditions from Graph Partitioning Bounds and Order\n  Statistics Abstract: We study a weaker formulation of the nullspace property which guarantees\nrecovery of sparse signals from linear measurements by l_1 minimization. We\nrequire this condition to hold only with high probability, given a distribution\non the nullspace of the coding matrix A. Under some assumptions on the\ndistribution of the reconstruction error, we show that testing these weak\nconditions means bounding the optimal value of two classical graph partitioning\nproblems: the k-Dense-Subgraph and MaxCut problems. Both problems admit\nefficient, relatively tight relaxations and we use a randomization argument to\nproduce new approximation bounds for k-Dense-Subgraph. We test the performance\nof our results on several families of coding matrices. \n\n"}
{"id": "1005.0252", "contents": "Title: Discrete-Time Fractional Variational Problems Abstract: We introduce a discrete-time fractional calculus of variations on the time\nscale $h\\mathbb{Z}$, $h > 0$. First and second order necessary optimality\nconditions are established. Examples illustrating the use of the new\nEuler-Lagrange and Legendre type conditions are given. They show that solutions\nto the considered fractional problems become the classical discrete-time\nsolutions when the fractional order of the discrete-derivatives are integer\nvalues, and that they converge to the fractional continuous-time solutions when\n$h$ tends to zero. Our Legendre type condition is useful to eliminate false\ncandidates identified via the Euler-Lagrange fractional equation. \n\n"}
{"id": "1005.3579", "contents": "Title: Graph-Structured Multi-task Regression and an Efficient Optimization\n  Method for General Fused Lasso Abstract: We consider the problem of learning a structured multi-task regression, where\nthe output consists of multiple responses that are related by a graph and the\ncorrelated response variables are dependent on the common inputs in a sparse\nbut synergistic manner. Previous methods such as l1/l2-regularized multi-task\nregression assume that all of the output variables are equally related to the\ninputs, although in many real-world problems, outputs are related in a complex\nmanner. In this paper, we propose graph-guided fused lasso (GFlasso) for\nstructured multi-task regression that exploits the graph structure over the\noutput variables. We introduce a novel penalty function based on fusion penalty\nto encourage highly correlated outputs to share a common set of relevant\ninputs. In addition, we propose a simple yet efficient proximal-gradient method\nfor optimizing GFlasso that can also be applied to any optimization problems\nwith a convex smooth loss and the general class of fusion penalty defined on\narbitrary graph structures. By exploiting the structure of the non-smooth\n''fusion penalty'', our method achieves a faster convergence rate than the\nstandard first-order method, sub-gradient method, and is significantly more\nscalable than the widely adopted second-order cone-programming and\nquadratic-programming formulations. In addition, we provide an analysis of the\nconsistency property of the GFlasso model. Experimental results not only\ndemonstrate the superiority of GFlasso over the standard lasso but also show\nthe efficiency and scalability of our proximal-gradient method. \n\n"}
{"id": "1005.3680", "contents": "Title: Quantifying long-range correlations in complex networks beyond nearest\n  neighbors Abstract: We propose a fluctuation analysis to quantify spatial correlations in complex\nnetworks. The approach considers the sequences of degrees along shortest paths\nin the networks and quantifies the fluctuations in analogy to time series. In\nthis work, the Barabasi-Albert (BA) model, the Cayley tree at the percolation\ntransition, a fractal network model, and examples of real-world networks are\nstudied. While the fluctuation functions for the BA model show exponential\ndecay, in the case of the Cayley tree and the fractal network model the\nfluctuation functions display a power-law behavior. The fractal network model\ncomprises long-range anti-correlations. The results suggest that the\nfluctuation exponent provides complementary information to the fractal\ndimension. \n\n"}
{"id": "1005.5273", "contents": "Title: Holonomic Gradient Descent and its Application to Fisher-Bingham\n  Integral Abstract: We give a new algorithm to find local maximum and minimum of a holonomic\nfunction and apply it for the Fisher-Bingham integral on the sphere $S^n$,\nwhich is used in the directional statistics. The method utilizes the theory and\nalgorithms of holonomic systems. \n\n"}
{"id": "1006.0517", "contents": "Title: HepData reloaded: reinventing the HEP data archive Abstract: We describe the status of the HepData database system, following a major\nre-development in time for the advent of LHC data. The new HepData system\nbenefits from use of modern database and programming language technologies, as\nwell as a variety of high-quality tools for interfacing the data sources and\ntheir presentation, primarily via the Web. The new back-end provides much more\nflexible and semantic data representations than before, on which new external\napplications can be built to respond to the data demands of the LHC\nexperimental era. The HepData re-development was largely motivated by a desire\nto have a single source of reference data for Monte Carlo validation and tuning\ntools, whose status and connection to HepData we also briefly review. \n\n"}
{"id": "1006.3096", "contents": "Title: Non-Hermitean Wishart random matrices (I) Abstract: A non-Hermitean extension of paradigmatic Wishart random matrices is\nintroduced to set up a theoretical framework for statistical analysis of (real,\ncomplex and real quaternion) stochastic time series representing two \"remote\"\ncomplex systems. The first paper in a series provides a detailed spectral\ntheory of non-Hermitean Wishart random matrices composed of complex valued\nentries. The great emphasis is placed on an asymptotic analysis of the mean\neigenvalue density for which we derive, among other results, a complex-plane\nanalogue of the Marchenko-Pastur law. A surprising connection with a class of\nmatrix models previously invented in the context of quantum chromodynamics is\npointed out. \n\n"}
{"id": "1006.4895", "contents": "Title: On the complexity of nonlinear mixed-integer optimization Abstract: This is a survey on the computational complexity of nonlinear mixed-integer\noptimization. It highlights a selection of important topics, ranging from\nincomputability results that arise from number theory and logic, to recently\nobtained fully polynomial time approximation schemes in fixed dimension, and to\nstrongly polynomial-time algorithms for special cases. \n\n"}
{"id": "1007.0743", "contents": "Title: Fractional variational calculus in terms of a combined Caputo derivative Abstract: We generalize the fractional Caputo derivative to the fractional derivative\n${^CD^{\\alpha,\\beta}_{\\gamma}}$, which is a convex combination of the left\nCaputo fractional derivative of order $\\alpha$ and the right Caputo fractional\nderivative of order $\\beta$. The fractional variational problems under our\nconsideration are formulated in terms of ${^CD^{\\alpha,\\beta}_{\\gamma}}$. The\nEuler-Lagrange equations for the basic and isoperimetric problems, as well as\ntransversality conditions, are proved. \n\n"}
{"id": "1007.4014", "contents": "Title: Low Surface Brightness Galaxies in the SDSS: the link between\n  environment, star-forming properties and AGN Abstract: Using the Sloan Digital Sky Survey (SDSS) data release 4 (DR 4), we\ninvestigate the spatial distribution of low and high surface brightness\ngalaxies (LSBGs and HSBGs, respectively). In particular, we focus our attention\non the influence of interactions between galaxies on the star formation\nstrength in the redshift range $0.01 < z < 0.1$. With cylinder counts and\nprojected distance to the first and fifth-nearest neighbor as environment\ntracers, we find that LSBGs tend to have a lack of companions compared to HSBGs\nat small scales ($<2$ Mpc). Regarding the interactions, we have evidence that\nthe fraction of LSBGs with strong star formation activity increases when the\ndistance between pairs of galaxies ($r_{p}$) is smaller than about four times\nthe Petrosian radius ($r_{90}$) of one of the components. Our results suggest\nthat, rather than being a condition for their formation, the isolation of LSBGs\nis more connected with their survival and evolution. The effect of the\ninteraction on the star formation strength, measured by the average value of\nthe birthrate parameter $b$, seems to be stronger for HSBGs than for LSBGs. The\nanalysis of our population of LSBGs and HSBGs hosting an AGN show that,\nregardless of the mass range, the fraction of LSBGs having an AGN is lower than\nthe corresponding fraction of HSBGs with an AGN. Also, we observe that the\nfraction of HSBGs and LSBGs having an AGN increases with the bulge luminosity.\nThese results, and those concerning the star-forming properties of LSBGs as a\nfunction of the environment, fit with the scenario proposed by some authors\nwhere, below a given threshold of surface mass density, low surface brightness\ndisks are unable to propagate instabilities, preventing the formation and\nevolution of massive black holes in the centers of LSBGs. \n\n"}
{"id": "1008.3935", "contents": "Title: Noise-enhanced trapping in chaotic scattering Abstract: We show that noise enhances the trapping of trajectories in scattering\nsystems. In fully chaotic systems, the decay rate can decrease with increasing\nnoise due to a generic mismatch between the noiseless escape rate and the value\npredicted by the Liouville measure of the exit set. In Hamiltonian systems with\nmixed phase space we show that noise leads to a slower algebraic decay due to\ntrajectories performing a random walk inside Kolmogorov-Arnold-Moser islands.\nWe argue that these noise-enhanced trapping mechanisms exist in most scattering\nsystems and are likely to be dominant for small noise intensities, which is\nconfirmed through a detailed investigation in the Henon map. Our results can be\ntested in fluid experiments, affect the fractal Weyl's law of quantum systems,\nand modify the estimations of chemical reaction rates based on phase-space\ntransition state theory. \n\n"}
{"id": "1008.5280", "contents": "Title: Bayesian parameter estimation in the second LISA Pathfinder Mock Data\n  Challenge Abstract: A main scientific output of the LISA Pathfinder mission is to provide a noise\nmodel that can be extended to the future gravitational wave observatory, LISA.\nThe success of the mission depends thus upon a deep understanding of the\ninstrument, especially the ability to correctly determine the parameters of the\nunderlying noise model. In this work we estimate the parameters of a simplified\nmodel of the LISA Technology Package (LTP) instrument. We describe the LTP by\nmeans of a closed-loop model that is used to generate the data, both injected\nsignals and noise. Then, parameters are estimated using a Bayesian framework\nand it is shown that this method reaches the optimal attainable error, the\nCramer-Rao bound. We also address an important issue for the mission: how to\nefficiently combine the results of different experiments to obtain a unique set\nof parameters describing the instrument. \n\n"}
{"id": "1009.0125", "contents": "Title: A new look at nonnegativity on closed sets and polynomial optimization Abstract: We first show that a continuous function f is nonnegative on a closed set\n$K\\subseteq R^n$ if and only if (countably many) moment matrices of some signed\nmeasure $d\\nu =fd\\mu$ with support equal to K, are all positive semidefinite\n(if $K$ is compact $\\mu$ is an arbitrary finite Borel measure with support\nequal to K. In particular, we obtain a convergent explicit hierarchy of\nsemidefinite (outer) approximations with {\\it no} lifting, of the cone of\nnonnegative polynomials of degree at most $d$. Wen used in polynomial\noptimization on certain simple closed sets $\\K$ (like e.g., the whole space\n$\\R^n$, the positive orthant, a box, a simplex, or the vertices of the\nhypercube), it provides a nonincreasing sequence of upper bounds which\nconverges to the global minimum by solving a hierarchy of semidefinite programs\nwith only one variable. This convergent sequence of upper bounds complements\nthe convergent sequence of lower bounds obtained by solving a hierarchy of\nsemidefinite relaxations. \n\n"}
{"id": "1009.1128", "contents": "Title: Distributed Basis Pursuit Abstract: We propose a distributed algorithm for solving the optimization problem Basis\nPursuit (BP). BP finds the least L1-norm solution of the underdetermined linear\nsystem Ax = b and is used, for example, in compressed sensing for\nreconstruction. Our algorithm solves BP on a distributed platform such as a\nsensor network, and is designed to minimize the communication between nodes.\nThe algorithm only requires the network to be connected, has no notion of a\ncentral processing node, and no node has access to the entire matrix A at any\ntime. We consider two scenarios in which either the columns or the rows of A\nare distributed among the compute nodes. Our algorithm, named D-ADMM, is a\ndecentralized implementation of the alternating direction method of\nmultipliers. We show through numerical simulation that our algorithm requires\nconsiderably less communications between the nodes than the state-of-the-art\nalgorithms. \n\n"}
{"id": "1009.1185", "contents": "Title: Toward the Universal Rigidity of General Frameworks Abstract: Let (G,P) be a bar framework of n vertices in general position in R^d, d <=\nn-1, where G is a (d+1)-lateration graph. In this paper, we present a\nconstructive proof that (G,P) admits a positive semi-definite stress matrix\nwith rank n-d-1. We also prove a similar result for a sensor network where the\ngraph consists of m(>= d+1) anchors. \n\n"}
{"id": "1010.1960", "contents": "Title: Inverse problems in spin models Abstract: Several recent experiments in biology study systems composed of several\ninteracting elements, for example neuron networks. Normally, measurements\ndescribe only the collective behavior of the system, even if in most cases we\nwould like to characterize how its different parts interact. The goal of this\nthesis is to extract information about the microscopic interactions as a\nfunction of their collective behavior for two different cases. First, we will\nstudy a system described by a generalized Ising model. We find explicit\nformulas for the couplings as a function of the correlations and\nmagnetizations. In the following, we will study a system described by a\nHopfield model. In this case, we find not only explicit formula for inferring\nthe patterns, but also an analytical result that allows one to estimate how\nmuch data is necessary for a good inference. \n\n"}
{"id": "1011.0944", "contents": "Title: Irreversible Aggregation and Network Renormalization Abstract: Irreversible aggregation is revisited in view of recent work on\nrenormalization of complex networks. Its scaling laws and phase transitions are\nrelated to percolation transitions seen in the latter. We illustrate our points\nby giving the complete solution for the probability to find any given state in\nan aggregation process $(k+1)X\\to X$, given a fixed number of unit mass\nparticles in the initial state. Exactly the same probability distributions and\nscaling are found in one dimensional systems (a trivial network) and well-mixed\nsolutions. This reveals that scaling laws found in renormalization of complex\nnetworks do not prove that they are self-similar. \n\n"}
{"id": "1011.2564", "contents": "Title: From the Schr\\\"odinger problem to the Monge-Kantorovich problem Abstract: The aim of this article is to show that the Monge-Kantorovich problem is the\nlimit of a sequence of entropy minimization problems when a fluctuation\nparameter tends down to zero. We prove the convergence of the entropic values\nto the optimal transport cost as the fluctuations decrease to zero, and we also\nshow that the limit points of the entropic minimizers are optimal transport\nplans. We investigate the dynamic versions of these problems by considering\nrandom paths and describe the connections between the dynamic and static\nproblems. The proofs are essentially based on convex and functional analysis.\nWe also need specific properties of Gamma-convergence which we didn't find in\nthe literature. Hence we prove these Gamma-convergence results which are\ninteresting in their own right. \n\n"}
{"id": "1011.4161", "contents": "Title: Community characterization of heterogeneous complex systems Abstract: We introduce an analytical statistical method to characterize the communities\ndetected in heterogeneous complex systems. By posing a suitable null\nhypothesis, our method makes use of the hypergeometric distribution to assess\nthe probability that a given property is over-expressed in the elements of a\ncommunity with respect to all the elements of the investigated set. We apply\nour method to two specific complex networks, namely a network of world movies\nand a network of physics preprints. The characterization of the elements and of\nthe communities is done in terms of languages and countries for the movie\nnetwork and of journals and subject categories for papers. We find that our\nmethod is able to characterize clearly the identified communities. Moreover our\nmethod works well both for large and for small communities. \n\n"}
{"id": "1011.4963", "contents": "Title: Charm Physics Performance Studies for PANDA Abstract: The study of the charmonium (cbar c) system is a powerful tool to understand\nthe strong interaction. In pbar p annihilations studied with PANDA, the mass\nand width of the charmonium state, such as h_c, will be measured with an\nexcellent accuracy, determined by the very precise knowledge of the momentum,\np, beam resolution (dp/p=10e(-4)-10e(-5)) and not limited by the resolution of\nthe detector. The analysis of h_c demonstrates the feasibility to accurately\ndetermine a specific final state in the spectrum of charmed mesons. The\npreliminary background analysis of the pbar p -> pi0 pi0 pi0 decay competing\nwith a signal channel pbar p -> h_c -> eta_c + gamma -> (pi0 + pi0 + eta) +\ngamma is under control. A comparison of three decay modes of charmonium h_c via\nthe electromagnetic transition is presented. \n\n"}
{"id": "1012.0465", "contents": "Title: On combining significances. Some trivial examples Abstract: For Poisson distribution $Pois(n, \\lambda)$ with $\\lambda \\gg 1$, $n \\gg 1$\nwe propose to determine significance as $S =\n\\frac{n_{obs}-\\lambda}{\\sqrt{\\lambda}}$. The significance $S$ coincides up to\nsign with often used significance. For experiments which measure the same\nquantities the natural but not unique rule for significance combining is\n$S_{comb}(S_1, S_2) =\n\\frac{S_1\\sigma_1+S_2\\sigma_2}{\\sqrt{\\sigma^2_1+\\sigma^2_2}}$, where $\\sigma_1$\nand $\\sigma_2$ are variations. We also propose the rule for significances\ncombining for the case with systematic errors. \n\n"}
{"id": "1012.1016", "contents": "Title: Matrices with Eigenvectors in a Given Subspace Abstract: The Kalman variety of a linear subspace in a vector space consists of all\nendomorphism that possess an eigenvector in that subspace. We study the\ndefining polynomials and basic geometric invariants of the Kalman variety. \n\n"}
{"id": "1012.1908", "contents": "Title: NP-hardness of Deciding Convexity of Quartic Polynomials and Related\n  Problems Abstract: We show that unless P=NP, there exists no polynomial time (or even\npseudo-polynomial time) algorithm that can decide whether a multivariate\npolynomial of degree four (or higher even degree) is globally convex. This\nsolves a problem that has been open since 1992 when N. Z. Shor asked for the\ncomplexity of deciding convexity for quartic polynomials. We also prove that\ndeciding strict convexity, strong convexity, quasiconvexity, and\npseudoconvexity of polynomials of even degree four or higher is strongly\nNP-hard. By contrast, we show that quasiconvexity and pseudoconvexity of odd\ndegree polynomials can be decided in polynomial time. \n\n"}
{"id": "1012.1945", "contents": "Title: Utility Optimal Scheduling in Energy Harvesting Networks Abstract: In this paper, we show how to achieve close-to-optimal utility performance in\nenergy harvesting networks with only finite capacity energy storage devices. In\nthese networks, nodes are capable of harvesting energy from the environment.\nThe amount of energy that can be harvested is time varying and evolves\naccording to some probability law. We develop an \\emph{online} algorithm,\ncalled the Energy-limited Scheduling Algorithm (ESA), which jointly manages the\nenergy and makes power allocation decisions for packet transmissions. ESA only\nhas to keep track of the amount of energy left at the network nodes and\n\\emph{does not require any knowledge} of the harvestable energy process. We\nshow that ESA achieves a utility that is within $O(\\epsilon)$ of the optimal,\nfor any $\\epsilon>0$, while ensuring that the network congestion and the\nrequired capacity of the energy storage devices are \\emph{deterministically}\nupper bounded by bounds of size $O(1/\\epsilon)$. We then also develop the\nModified-ESA algorithm (MESA) to achieve the same $O(\\epsilon)$\nclose-to-utility performance, with the average network congestion and the\nrequired capacity of the energy storage devices being only\n$O([\\log(1/\\epsilon)]^2)$. \n\n"}
{"id": "1101.3501", "contents": "Title: Convergence rates of efficient global optimization algorithms Abstract: Efficient global optimization is the problem of minimizing an unknown\nfunction f, using as few evaluations f(x) as possible. It can be considered as\na continuum-armed bandit problem, with noiseless data and simple regret.\nExpected improvement is perhaps the most popular method for solving this\nproblem; the algorithm performs well in experiments, but little is known about\nits theoretical properties. Implementing expected improvement requires a choice\nof Gaussian process prior, which determines an associated space of functions,\nits reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected\nimprovement is known to converge on the minimum of any function in the RKHS. We\nbegin by providing convergence rates for this procedure. The rates are optimal\nfor functions of low smoothness, and we modify the algorithm to attain optimal\nrates for smoother functions. For practitioners, however, these results are\nsomewhat misleading. Priors are typically not held fixed, but depend on\nparameters estimated from the data. For standard estimators, we show this\nprocedure may never discover the minimum of f. We then propose alternative\nestimators, chosen to minimize the constants in the rate of convergence, and\nshow these estimators retain the convergence rates of a fixed prior. \n\n"}
{"id": "1102.1519", "contents": "Title: Detecting hidden spatial and spatio-temporal structures in glasses and\n  complex physical systems by multiresolution network clustering Abstract: We elaborate on a general method that we recently introduced for\ncharacterizing the \"natural\" structures in complex physical systems via a\nmultiscale network based approach for the data mining of such structures. The\napproach is based on \"community detection\" wherein interacting particles are\npartitioned into \"an ideal gas\" of optimally decoupled groups of particles.\nSpecifically, we construct a set of network representations (\"replicas\") of the\nphysical system based on interatomic potentials and apply a multiscale\nclustering (\"multiresolution community detection\") analysis using\ninformation-based correlations among the replicas. Replicas may be (i)\ndifferent representations of an identical static system or (ii) embody dynamics\nby when considering replicas to be time separated snapshots of the system (with\na tunable time separation) or (iii) encode general correlations when different\nreplicas correspond to different representations of the entire history of the\nsystem as it evolves in space-time. We apply our method to computer simulations\nof a binary Kob-Andersen Lennard-Jones system, a ternary model system, and to\natomic coordinates in a ZrPt system as gleaned by reverse Monte Carlo analysis\nof experimentally determined structure factors. We identify the dominant\nstructures (disjoint or overlapping) and general length scales by analyzing\nextrema of the information theory measures. We speculate on possible links\nbetween (i) physical transitions or crossovers and (ii) changes in structures\nfound by this method as well as phase transitions associated with the\ncomputational complexity of the community detection problem. We briefly also\nconsider continuum approaches and discuss the shear penetration depth in\nelastic media; this length scale increases as the system becomes increasingly\nrigid. \n\n"}
{"id": "1102.3360", "contents": "Title: Fractional variational problems depending on indefinite integrals Abstract: We obtain necessary optimality conditions for variational problems with a\nLagrangian depending on a Caputo fractional derivative, a fractional and an\nindefinite integral. Main results give fractional Euler-Lagrange type equations\nand natural boundary conditions, which provide a generalization of previous\nresults found in the literature. Isoperimetric problems, problems with\nholonomic constraints and depending on higher-order Caputo derivatives, as well\nas fractional Lagrange problems, are considered. \n\n"}
{"id": "1102.3508", "contents": "Title: Online Learning of Rested and Restless Bandits Abstract: In this paper we study the online learning problem involving rested and\nrestless multiarmed bandits with multiple plays. The system consists of a\nsingle player/user and a set of K finite-state discrete-time Markov chains\n(arms) with unknown state spaces and statistics. At each time step the player\ncan play M arms. The objective of the user is to decide for each step which M\nof the K arms to play over a sequence of trials so as to maximize its long term\nreward. The restless multiarmed bandit is particularly relevant to the\napplication of opportunistic spectrum access (OSA), where a (secondary) user\nhas access to a set of K channels, each of time-varying condition as a result\nof random fading and/or certain primary users' activities. \n\n"}
{"id": "1103.0701", "contents": "Title: Analytical maximum-likelihood method to detect patterns in real networks Abstract: In order to detect patterns in real networks, randomized graph ensembles that\npreserve only part of the topology of an observed network are systematically\nused as fundamental null models. However, their generation is still\nproblematic. The existing approaches are either computationally demanding and\nbeyond analytic control, or analytically accessible but highly approximate.\nHere we propose a solution to this long-standing problem by introducing an\nexact and fast method that allows to obtain expectation values and standard\ndeviations of any topological property analytically, for any binary, weighted,\ndirected or undirected network. Remarkably, the time required to obtain the\nexpectation value of any property is as short as that required to compute the\nsame property on the single original network. Our method reveals that the null\nbehavior of various correlation properties is different from what previously\nbelieved, and highly sensitive to the particular network considered. Moreover,\nour approach shows that important structural properties (such as the modularity\nused in community detection problems) are currently based on incorrect\nexpressions, and provides the exact quantities that should replace them. \n\n"}
{"id": "1103.1243", "contents": "Title: Randomizing world trade. I. A binary network analysis Abstract: The international trade network (ITN) has received renewed multidisciplinary\ninterest due to recent advances in network theory. However, it is still unclear\nwhether a network approach conveys additional, nontrivial information with\nrespect to traditional international-economics analyses that describe world\ntrade only in terms of local (first-order) properties. In this and in a\ncompanion paper, we employ a recently proposed randomization method to assess\nin detail the role that local properties have in shaping higher-order patterns\nof the ITN in all its possible representations (binary/weighted,\ndirected/undirected, aggregated/disaggregated by commodity) and across several\nyears. Here we show that, remarkably, the properties of all binary projections\nof the network can be completely traced back to the degree sequence, which is\ntherefore maximally informative. Our results imply that explaining the observed\ndegree sequence of the ITN, which has not received particular attention in\neconomic theory, should instead become one the main focuses of models of trade. \n\n"}
{"id": "1103.1732", "contents": "Title: Semi-Global Approximate stabilization of an infinite dimensional quantum\n  stochastic system Abstract: In this paper we study the semi-global (approximate) state feedback\nstabilization of an infinite dimensional quantum stochastic system towards a\ntarget state. A discrete-time Markov chain on an infinite-dimensional Hilbert\nspace is used to model the dynamics of a quantum optical cavity. We can choose\nan (unbounded) strict Lyapunov function that is minimized at each time-step in\norder to prove (weak-$\\ast$) convergence of probability measures to a final\nstate that is concentrated on the target state with (a pre-specified)\nprobability that may be made arbitrarily close to 1. The feedback parameters\nand the Lyapunov function are chosen so that the stochastic flow that describes\nthe Markov process may be shown to be tight (concentrated on a compact set with\nprobability arbitrarily close to 1). We then use Prohorov's theorem and\nproperties of the Lyapunov function to prove the desired convergence result. \n\n"}
{"id": "1103.5431", "contents": "Title: Identification of Nonlinear Systems with Stable Limit Cycles via Convex\n  Optimization Abstract: We propose a convex optimization procedure for black-box identification of\nnonlinear state-space models for systems that exhibit stable limit cycles\n(unforced periodic solutions). It extends the \"robust identification error\"\nframework in which a convex upper bound on simulation error is optimized to fit\nrational polynomial models with a strong stability guarantee. In this work, we\nrelax the stability constraint using the concepts of transverse dynamics and\norbital stability, thus allowing systems with autonomous oscillations to be\nidentified. The resulting optimization problem is convex, and can be formulated\nas a semidefinite program. A simulation-error bound is proved without assuming\nthat the true system is in the model class, or that the number of measurements\ngoes to infinity. Conditions which guarantee existence of a unique limit cycle\nof the model are proved and related to the model class that we search over. The\nmethod is illustrated by identifying a high-fidelity model from experimental\nrecordings of a live rat hippocampal neuron in culture. \n\n"}
{"id": "1104.0183", "contents": "Title: Exact and Efficient Algorithm to Discover Extreme Stochastic Events in\n  Wind Generation over Transmission Power Grids Abstract: In this manuscript we continue the thread of [M. Chertkov, F. Pan, M.\nStepanov, Predicting Failures in Power Grids: The Case of Static Overloads,\nIEEE Smart Grid 2011] and suggest a new algorithm discovering most probable\nextreme stochastic events in static power grids associated with intermittent\ngeneration of wind turbines. The algorithm becomes EXACT and EFFICIENT\n(polynomial) in the case of the proportional (or other low parametric) control\nof standard generation, and log-concave probability distribution of the\nrenewable generation, assumed known from the wind forecast. We illustrate the\nalgorithm's ability to discover problematic extreme events on the example of\nthe IEEE RTS-96 model of transmission with additions of 10%, 20% and 30% of\nrenewable generation. We observe that the probability of failure may grow but\nit may also decrease with increase in renewable penetration, if the latter is\nsufficiently diversified and distributed. \n\n"}
{"id": "1104.0654", "contents": "Title: Block-Sparse Recovery via Convex Optimization Abstract: Given a dictionary that consists of multiple blocks and a signal that lives\nin the range space of only a few blocks, we study the problem of finding a\nblock-sparse representation of the signal, i.e., a representation that uses the\nminimum number of blocks. Motivated by signal/image processing and computer\nvision applications, such as face recognition, we consider the block-sparse\nrecovery problem in the case where the number of atoms in each block is\narbitrary, possibly much larger than the dimension of the underlying subspace.\nTo find a block-sparse representation of a signal, we propose two classes of\nnon-convex optimization programs, which aim to minimize the number of nonzero\ncoefficient blocks and the number of nonzero reconstructed vectors from the\nblocks, respectively. Since both classes of problems are NP-hard, we propose\nconvex relaxations and derive conditions under which each class of the convex\nprograms is equivalent to the original non-convex formulation. Our conditions\ndepend on the notions of mutual and cumulative subspace coherence of a\ndictionary, which are natural generalizations of existing notions of mutual and\ncumulative coherence. We evaluate the performance of the proposed convex\nprograms through simulations as well as real experiments on face recognition.\nWe show that treating the face recognition problem as a block-sparse recovery\nproblem improves the state-of-the-art results by 10% with only 25% of the\ntraining data. \n\n"}
{"id": "1104.2970", "contents": "Title: On the Triality Theory in Global Optimization Abstract: Triality theory is proved for a general unconstrained global optimization\nproblem. The method adopted is simple but mathematically rigorous. Results show\nthat if the primal problem and its canonical dual have the same dimension, the\ntriality theory holds strongly in the tri-duality form as it was originally\nproposed. Otherwise, both the canonical min-max duality and the double-max\nduality still hold strongly, but the double-min duality holds weakly in a\nsuper-symmetrical form as it was expected. Additionally, a complementary weak\nsaddle min-max duality theorem is discovered. Therefore, an open problem on\nthis statement left in 2003 is solved completely. This theory can be used to\nidentify not only the global minimum, but also the largest local minimum,\nmaximum, and saddle points. Application is illustrated. Some fundamental\nconcepts in optimization and remaining challenging problems in canonical\nduality theory are discussed. \n\n"}
{"id": "1104.3448", "contents": "Title: Detection of trend changes in time series using Bayesian inference Abstract: Change points in time series are perceived as isolated singularities where\ntwo regular trends of a given signal do not match. The detection of such\ntransitions is of fundamental interest for the understanding of the system's\ninternal dynamics. In practice observational noise makes it difficult to detect\nsuch change points in time series. In this work we elaborate a Bayesian method\nto estimate the location of the singularities and to produce some confidence\nintervals. We validate the ability and sensitivity of our inference method by\nestimating change points of synthetic data sets. As an application we use our\nalgorithm to analyze the annual flow volume of the Nile River at Aswan from\n1871 to 1970, where we confirm a well-established significant transition point\nwithin the time series. \n\n"}
{"id": "1105.0247", "contents": "Title: Liquidation in Limit Order Books with Controlled Intensity Abstract: We consider a framework for solving optimal liquidation problems in limit\norder books. In particular, order arrivals are modeled as a point process whose\nintensity depends on the liquidation price. We set up a stochastic control\nproblem in which the goal is to maximize the expected revenue from liquidating\nthe entire position held. We solve this optimal liquidation problem for\npower-law and exponential-decay order book models and discuss several\nextensions. We also consider the continuous selling (or fluid) limit when the\ntrading units are ever smaller and the intensity is ever larger. This limit\nprovides an analytical approximation to the value function and the optimal\nsolution. Using techniques from viscosity solutions we show that the discrete\nstate problem and its optimal solution converge to the corresponding quantities\nin the continuous selling limit uniformly on compacts. \n\n"}
{"id": "1105.0411", "contents": "Title: Non-parametric segmentation of non-stationary time series Abstract: The non-stationary evolution of observable quantities in complex systems can\nfrequently be described as a juxtaposition of quasi-stationary spells. Given\nthat standard theoretical and data analysis approaches usually rely on the\nassumption of stationarity, it is important to detect in real time series\nintervals holding that property. With that aim, we introduce a segmentation\nalgorithm based on a fully non-parametric approach. We illustrate its\napplicability through the analysis of real time series presenting diverse\ndegrees of non-stationarity, thus showing that this segmentation procedure\ngeneralizes and allows to uncover features unresolved by previous proposals\nbased on the discrepancy of low order statistical moments only. \n\n"}
{"id": "1105.2013", "contents": "Title: Weyl theory and explicit solutions of direct and inverse problems for a\n  Dirac system with rectangular matrix potential Abstract: A non-classical Weyl theory is developed for Dirac systems with rectangular\nmatrix potentials. The notion of the Weyl function is introduced and the\ncorresponding direct problem is treated. Furthermore, explicit solutions of the\ndirect and inverse problems are obtained for the case of rational Weyl matrix\nfunctions. \n\n"}
{"id": "1105.2924", "contents": "Title: On the derivative cones of polyhedral cones Abstract: Hyperbolic polynomials elegantly encode a rich class of convex cones that\nincludes polyhedral and spectrahedral cones. Hyperbolic polynomials are closed\nunder taking polars and the corresponding cones, the derivative cones, yield\nrelaxations for the associated optimization problem and exhibit interesting\nfacial properties. While it is unknown if every hyperbolicity cone is a section\nof the positive semidefinite cone, it is natural to ask whether spectrahedral\ncones are closed under taking polars. In this note we give an affirmative\nanswer for polyhedral cones by exhibiting an explicit spectrahedral\nrepresentation for the first derivative cone. We also proof that higher polars\ndo not have an determinantal representation which shows that the problem for\ngeneral spectrahedral cones is considerably more difficult. \n\n"}
{"id": "1105.3041", "contents": "Title: A Bayesian approach to evaluate confidence intervals in counting\n  experiments with background Abstract: In this paper we propose a procedure to evaluate Bayesian confidence\nintervals in counting experiments where both signal and background fluctuations\nare described by the Poisson statistics. The results obtained when the method\nis applied to the calculation of upper limits will also be illustrated. \n\n"}
{"id": "1105.3631", "contents": "Title: A majorization method for localizing graph topological indices Abstract: This paper presents a unified approach for localizing some relevant graph\ntopological indices via majorization techniques. Through this method, old and\nnew bounds are derived and numerical examples are provided, showing how former\nresults in the literature could be improved. \n\n"}
{"id": "1105.4005", "contents": "Title: Link prediction in complex networks: a local na\\\"{\\i}ve Bayes model Abstract: Common-neighbor-based method is simple yet effective to predict missing\nlinks, which assume that two nodes are more likely to be connected if they have\nmore common neighbors. In such method, each common neighbor of two nodes\ncontributes equally to the connection likelihood. In this Letter, we argue that\ndifferent common neighbors may play different roles and thus lead to different\ncontributions, and propose a local na\\\"{\\i}ve Bayes model accordingly.\nExtensive experiments were carried out on eight real networks. Compared with\nthe common-neighbor-based methods, the present method can provide more accurate\npredictions. Finally, we gave a detailed case study on the US air\ntransportation network. \n\n"}
{"id": "1105.4116", "contents": "Title: A Fast Algorithm for Muon Track Reconstruction and its Application to\n  the ANTARES Neutrino Telescope Abstract: An algorithm is presented, that provides a fast and robust reconstruction of\nneutrino induced upward-going muons and a discrimination of these events from\ndownward-going atmospheric muon background in data collected by the ANTARES\nneutrino telescope. The algorithm consists of a hit merging and hit selection\nprocedure followed by fitting steps for a track hypothesis and a point-like\nlight source. It is particularly well-suited for real time applications such as\nonline monitoring and fast triggering of optical follow-up observations for\nmulti-messenger studies. The performance of the algorithm is evaluated with\nMonte Carlo simulations and various distributions are compared with that\nobtained in ANTARES data. \n\n"}
{"id": "1105.4979", "contents": "Title: Unbiased acceleration measurements with an electrostatic accelerometer\n  on a rotating platform Abstract: The Gravity Advanced Package is an instrument composed of an electrostatic\naccelerometer called MicroSTAR and a rotating platform called Bias Rejection\nSystem. It aims at measuring with no bias the non-gravitational acceleration of\na spacecraft. It is envisioned to be embarked on an interplanetary spacecraft\nas a tool to test the laws of gravitation.\n  MicroSTAR is based on Onera's experience and inherits in orbit technology.\nThe addition of the rotating platform is a technological upgrade which allows\nusing an electrostatic accelerometer to make measurements at low frequencies\nwith no bias. To do so, the Bias Rejection System rotates MicroSTAR such that\nthe signal of interest is separated from the bias of the instrument in the\nfrequency domain. Making these unbiased low-frequency measurements requires\npost-processing the data. The signal processing technique developed for this\npurpose is the focus of this article. It allows giving the conditions under\nwhich the bias is completely removed from the signal of interest. And the\nprecision of the unbiased measurements can be fully characterized: given the\ncharacteristics of the subsystems, it is possible to reach a precision of 1 pm\ns$^{-2}$ on the non-gravitational acceleration for an integration time of 3 h. \n\n"}
{"id": "1105.6197", "contents": "Title: Optimal time travel in the Godel universe Abstract: Using the theory of optimal rocket trajectories in general relativity,\nrecently developed in arXiv:1105.5235, we present a candidate for the minimum\ntotal integrated acceleration closed timelike curve in the Godel universe, and\ngive evidence for its minimality. The total integrated acceleration of this\ncurve is lower than Malament's conjectured value (Malament, 1984), as was\nalready implicit in the work of Manchak (Manchak, 2011); however, Malament's\nconjecture does seem to hold for periodic closed timelike curves. \n\n"}
{"id": "1106.1780", "contents": "Title: GELATIO: a general framework for modular digital analysis of high-purity\n  Ge detector signals Abstract: GELATIO is a new software framework for advanced data analysis and digital\nsignal processing developed for the GERDA neutrinoless double beta decay\nexperiment. The framework is tailored to handle the full analysis flow of\nsignals recorded by high purity Ge detectors and photo-multipliers from the\nveto counters. It is designed to support a multi-channel modular and flexible\nanalysis, widely customizable by the user either via human-readable\ninitialization files or via a graphical interface. The framework organizes the\ndata into a multi-level structure, from the raw data up to the condensed\nanalysis parameters, and includes tools and utilities to handle the data stream\nbetween the different levels. GELATIO is implemented in C++. It relies upon\nROOT and its extension TAM, which provides compatibility with PROOF, enabling\nthe software to run in parallel on clusters of computers or many-core machines.\nIt was tested on different platforms and benchmarked in several GERDA-related\napplications. A stable version is presently available for the GERDA\nCollaboration and it is used to provide the reference analysis of the\nexperiment data. \n\n"}
{"id": "1106.4456", "contents": "Title: Convergence of an inverse problem for discrete wave equations Abstract: It is by now well-known that one can recover a potential in the wave equation\nfrom the knowledge of the initial waves, the boundary data and the flux on a\npart of the boundary satisfying the Gamma-conditions of J.-L. Lions. We are\ninterested in proving that trying to fit the discrete fluxes, given by discrete\napproximations of the wave equation, with the continuous one, one recovers, at\nthe limit, the potential of the continuous model. In order to do that, we shall\ndevelop a Lax-type argument, usually used for convergence results of numerical\nschemes, which states that consistency and uniform stability imply convergence.\nIn our case, the most difficult part of the analysis is the one corresponding\nto the uniform stability, that we shall prove using new uniform discrete\nCarleman estimates, where uniform means with respect to the discretization\nparameter. We shall then deduce a convergence result for the discrete inverse\nproblems. Our analysis will be restricted to the 1-d case for space\nsemi-discrete wave equations discretized on a uniform mesh using a finite\ndifferences approach. \n\n"}
{"id": "1106.4710", "contents": "Title: Proportionate vs disproportionate distribution of wealth of two\n  individuals in a tempered Paretian ensemble Abstract: We study the distribution P(\\omega) of the random variable \\omega = x_1/(x_1\n+ x_2), where x_1 and x_2 are the wealths of two individuals selected at random\nfrom the same tempered Paretian ensemble characterized by the distribution\n\\Psi(x) \\sim \\phi(x)/x^{1 + \\alpha}, where \\alpha > 0 is the Pareto index and\n$\\phi(x)$ is the cut-off function. We consider two forms of \\phi(x): a bounded\nfunction \\phi(x) = 1 for L \\leq x \\leq H, and zero otherwise, and a smooth\nexponential function \\phi(x) = \\exp(-L/x - x/H). In both cases \\Psi(x) has\nmoments of arbitrary order.\n  We show that, for \\alpha > 1, P(\\omega) always has a unimodal form and is\npeaked at \\omega = 1/2, so that most probably x_1 \\approx x_2. For 0 < \\alpha <\n1 we observe a more complicated behavior which depends on the value of \\delta =\nL/H. In particular, for \\delta < \\delta_c - a certain threshold value -\nP(\\omega) has a three-modal (for a bounded \\phi(x)) and a bimodal M-shape (for\nan exponential \\phi(x)) form which signifies that in such ensembles the wealths\nx_1 and x_2 are disproportionately different. \n\n"}
{"id": "1107.1639", "contents": "Title: Injectivity and flatness of semitopological modules Abstract: The spaces D, S and E' over \\mathbb{R}^(n) are known to be flat modules over\nA=\\mathbb{C}[\\partial_{1},...,\\partial_{n}], whereas their duals D', S' and E\nare known to be injective modules over the same ring. Let A be a Noetherian\nk-algebra (k=\\mathbb{R} or \\mathbb{C}). The above observation leads us to study\nin this paper the link existing between the flatness of an A-module E which is\na locally convex topological k-vector space and the injectivity of its dual. We\nshow that, for dual pairs (E,E') which are (K) over A--a notion which is\nexplained in the paper--, injectivity of E' is a stronger condition than\nflatness of E. A preprint of this paper (dated September 2009) has been quoted\nand discussed by Shankar. \n\n"}
{"id": "1108.0662", "contents": "Title: The Physics of Mergers: Theoretical and Statistical Techniques Applied\n  to Stellar Mergers in Dense Star Clusters Abstract: (abridged) This thesis presents theoretical and statistical techniques\nbroadly related to systems of dynamically-interacting particles composed of\nseveral different types of populations. They are applied to observations of\ndense star clusters (SCs) in order to study gravitational interactions between\nstars. We present a new analytic method of quantifying the frequency of\nencounters involving single, binary and triple stars. With this technique, we\nhave shown that dynamical encounters involving triple stars occur commonly in\nat least some SCs, and that they are likely to be an important dynamical\nchannel for stellar mergers to occur. We have also used our techniques to\nanalyze observational data for a large sample of SCs taken from the ACS Survey\nfor Globular Clusters. The results of this analysis are as follows: (1) We have\ncompiled a homogeneous catalogue of stellar populations for every cluster in\nour sample, including main-sequence (MS), red giant branch, horizontal branch\nand blue straggler (BS) stars. (2) With this catalogue, we have quantified the\neffects of the cluster dynamics in determining the relative sizes and spatial\ndistributions of these stellar populations. (3) These results are particularly\ninteresting for BSs since they provide compelling evidence that they are\ndescended from binary stars. (4) Our analysis of the MS populations is\nconsistent with a remarkably universal initial stellar mass function in old\nmassive SCs in the Milky Way. This is a new result with important implications\nfor our understanding of star formation in the early Universe and, more\ngenerally, the history of our Galaxy. Finally, we describe how our techniques\nare ideally suited for application to a number of other outstanding puzzles of\nmodern astrophysics, including chemical reactions in the interstellar medium\nand mergers between galaxies in galaxy clusters. \n\n"}
{"id": "1108.2018", "contents": "Title: Resource allocation with costly participation Abstract: We propose a new all-pay auction format in which risk-loving bidders pay a\nconstant fee each time they bid for an object whose monetary value is common\nknowledge among the bidders, and bidding fees are the only source of benefit\nfor the seller. We show that for the proposed model there exists a {unique}\nSymmetric Subgame Perfect Equilibrium (SSPE). The characterized SSPE is\nstationary when re-entry in the auction is allowed, and it is Markov perfect\nwhen re-entry is forbidden. Furthermore, we fully characterize the expected\nrevenue of the seller. Generally, with or without re-entry, it is more\nbeneficial for the seller to choose $v$ (value of the object), $s$ (sale\nprice), and $c$ (bidding fee) such that $\\frac{v-s}{c}$ becomes sufficiently\nlarge. In particular, when re-entry is permitted: the expected revenue of the\nseller is \\emph{independent} of the number of bidders, decreasing in the sale\nprice, increasing in the value of the object, and decreasing in the bidding\nfee; Moreover, the seller's revenue is equal to the value of the object when\nplayers are risk neutral, and it is strictly greater than the value of the\nobject when bidders are risk-loving. We further show that allowing re-entry can\nbe important in practice. Because, if the seller were to run such an auction\nwithout allowing re-entry, the auction would last a long time, and for almost\nall of its duration have only two remaining players. Thus, the seller's revenue\nrelies on those two players being willing to participate, without any breaks,\nin an auction that might last for thousands of rounds \n\n"}
{"id": "1108.2288", "contents": "Title: Statistical methods used in ATLAS for exclusion and discovery Abstract: The statistical methods used by the ATLAS Collaboration for setting upper\nlimits or establishing a discovery are reviewed, as they are fundamental\ningredients in the search for new phenomena. The analyses published so far\nadopted different approaches, choosing a frequentist or a Bayesian or a hybrid\nfrequentist-Bayesian method to perform a search for new physics and set upper\nlimits. In this note, after the introduction of the necessary basic concepts of\nstatistical hypothesis testing, a few recommendations are made about the\npreferred approaches to be followed in future analyses. \n\n"}
{"id": "1108.2701", "contents": "Title: Maximizing Boosted Top Identification by Minimizing N-subjettiness Abstract: N-subjettiness is a jet shape designed to identify boosted hadronic objects\nsuch as top quarks. Given N subjet axes within a jet, N-subjettiness sums the\nangular distances of jet constituents to their nearest subjet axis. Here, we\ngeneralize and improve on N-subjettiness by minimizing over all possible subjet\ndirections, using a new variant of the k-means clustering algorithm. On boosted\ntop benchmark samples from the BOOST2010 workshop, we demonstrate that a simple\ncut on the 3-subjettiness to 2-subjettiness ratio yields 20% (50%) tagging\nefficiency for a 0.23% (4.1%) fake rate, making N-subjettiness a highly\neffective boosted top tagger. N-subjettiness can be modified by adjusting an\nangular weighting exponent, and we find that the jet broadening measure is\npreferred for boosted top searches. We also explore multivariate techniques,\nand show that additional improvements are possible using a modified Fisher\ndiscriminant. Finally, we briefly mention how our minimization procedure can be\nextended to the entire event, allowing the event shape N-jettiness to act as a\nfixed N cone jet algorithm. \n\n"}
{"id": "1108.5316", "contents": "Title: Link Failure Detection in Multi-hop Control Networks Abstract: A Multi-hop Control Network (MCN) consists of a plant where the communication\nbetween sensors, actuators and computational unit is supported by a wireless\nmulti-hop communication network, and data flow is performed using scheduling\nand routing of sensing and actuation data. We characterize the problem of\ndetecting the failure of links of the radio connectivity graph and provide\nnecessary and sufficient conditions on the plant dynamics and on the\ncommunication protocol. We also provide a methodology to \\emph{explicitly}\ndesign the network topology, scheduling and routing of a communication protocol\nin order to satisfy the above conditions. \n\n"}
{"id": "1109.0045", "contents": "Title: Exact solutions for mass-dependent irreversible aggregations Abstract: We consider the mass-dependent aggregation process (k+1)X -> X, given a fixed\nnumber of unit mass particles in the initial state. One cluster is chosen\nproportional to its mass and is merged into one either with k-neighbors in one\ndimension, or -- in the well-mixed case -- with k other clusters picked\nrandomly. We find the same combinatorial exact solutions for the probability to\nfind any given configuration of particles on a ring or line, and in the\nwell-mixed case. The mass distribution of a single cluster exhibits scaling\nlaws and the finite size scaling form is given. The relation to the classical\nsum kernel of irreversible aggregation is discussed. \n\n"}
{"id": "1109.0593", "contents": "Title: Error Estimation for Moments Analysis in Heavy-Ion Collision Experiments Abstract: Fluctuations of conserved quantities are predicted to be sensitive to the\ncorrelation length and connected to the thermodynamic susceptibility. Thus,\nmoments of net-baryon, net-charge and net-strangeness have been extensively\nstudied theoretically and experimentally to explore phase structure and bulk\nproperties of QCD matters created in heavy ion collision experiment. As the\nmoments analysis is statistics hungry study, the error estimation is crucial to\nextract physics information from the limited experimental data. In this paper,\nwe will derive the limit distributions and error formula based on Delta theorem\nin statistics for various order moments used in the experimental data analysis.\nThe Monte Carlo simulation is also applied to test the error formula. \n\n"}
{"id": "1109.1900", "contents": "Title: Weakly-coupled systems in quantum control Abstract: This paper provides rigorous definitions and analysis of the dynamics of\nweakly-coupled systems and gives sufficient conditions for an infinite\ndimensional quantum control system to be weakly-coupled. As an illustration we\nprovide examples chosen among common physical systems. \n\n"}
{"id": "1109.3208", "contents": "Title: A decision between Bayesian and Frequentist upper limit in analyzing\n  continuous Gravitational Waves Abstract: Given the sensitivity of current ground-based Gravitational Wave (GW)\ndetectors, any continuous-wave signal we can realistically expect will be at a\nlevel or below the background noise. Hence, any data analysis of detector data\nwill need to rely on statistical techniques to separate the signal from the\nnoise. While with the current sensitivity of our detectors we do not expect to\ndetect any true GW signals in our data, we can still set upper limits (UL) on\ntheir amplitude. These upper limits, in fact, tell us how weak a signal\nstrength we would detect. In setting upper limit using two popular method,\nBayesian and Frequentist, there is always the question of a realistic results.\nIn this paper, we try to give an estimate of how realistically we can set the\nupper limit using the above mentioned methods. And if any, which one is\npreferred for our future data analysis work. \n\n"}
{"id": "1109.4184", "contents": "Title: A (k+1)-Slope Theorem for the k-Dimensional Infinite Group Relaxation Abstract: We prove that any minimal valid function for the k-dimensional infinite group\nrelaxation that is piecewise linear with at most k+1 slopes and does not factor\nthrough a linear map with non-trivial kernel is extreme. This generalizes a\ntheorem of Gomory and Johnson for k=1, and Cornuejols and Molinaro for k=2. \n\n"}
{"id": "1110.1432", "contents": "Title: A Sparse Semi-Blind Source Identification Method and Its Application to\n  Raman Spectroscopy for Explosives Detection Abstract: Rapid and reliable detection and identification of unknown chemical\nsubstances is critical to homeland security. It is challenging to identify\nchemical components from a wide range of explosives. There are two key steps\ninvolved. One is a nondestructive and informative spectroscopic technique for\ndata acquisition. The other is an associated library of reference features\nalong with a computational method for feature matching and meaningful detection\nwithin or beyond the library.\n  Recently several experimental techniques based on Raman scattering have been\ndeveloped to perform standoff detection and identification of explosives, and\nthey prove to be successful under certain idealized conditions. However data\nanalysis is limited to standard least squares method assuming the complete\nknowledge of the chemical components. In this paper, we develop a new iterative\nmethod to identify unknown substances from mixture samples of Raman\nspectroscopy. In the first step, a constrained least squares method decomposes\nthe data into a sum of linear combination of the known components and a\nnon-negative residual. In the second step, a sparse and convex blind source\nseparation method extracts components geometrically from the residuals.\nVerification based on the library templates or expert knowledge helps to\nconfirm these components. If necessary, the confirmed meaningful components are\nfed back into step one to refine the residual and then step two extracts\npossibly more hidden components. The two steps may be iterated until no more\ncomponents can be identified.\n  We illustrate the proposed method in processing a set of the so called swept\nwavelength optical resonant Raman spectroscopy experimental data by a\nsatisfactory blind extraction of a priori unknown chemical explosives from\nmixture samples. \n\n"}
{"id": "1110.2046", "contents": "Title: Fitting in a complex chi^2 landscape using an optimized hypersurface\n  sampling Abstract: Fitting a data set with a parametrized model can be seen geometrically as\nfinding the global minimum of the chi^2 hypersurface, depending on a set of\nparameters {P_i}. This is usually done using the Levenberg-Marquardt algorithm.\nThe main drawback of this algorithm is that despite of its fast convergence, it\ncan get stuck if the parameters are not initialized close to the final\nsolution. We propose a modification of the Metropolis algorithm introducing a\nparameter step tuning that optimizes the sampling of parameter space. The\nability of the parameter tuning algorithm together with simulated annealing to\nfind the global chi^2 hypersurface minimum, jumping across chi^2{P_i} barriers\nwhen necessary, is demonstrated with synthetic functions and with real data. \n\n"}
{"id": "1110.6639", "contents": "Title: On computation of a common mean Abstract: Combining several independent measurements of the same physical quantity is\none of the most important tasks in metrology. Small samples, biased input\nestimates, not always adequate reported uncertainties, and unknown error\ndistribution make a rigorous solution very difficult, if not impossible. For\nthis reason, many methods to compute a common mean and its uncertainty were\nproposed, each with own advantages and shortcomings. Most of them are variants\nof the weighted average (WA) approach with different strategies to compute WA\nand its standard deviation. Median estimate became also increasingly popular\nduring recent years. In this paper, these two methods in most widely used\nmodifications are compared using simulated and real data. To overcome some\nproblems of known approaches to compute the WA uncertainty, a new combined\nestimate has been proposed. It has been shown that the proposed method can help\nto obtain more robust and realistic estimate suitable for both consistent and\ndiscrepant measurements. \n\n"}
{"id": "1111.2074", "contents": "Title: LUXSim: A Component-Centric Approach to Low-Background Simulations Abstract: Geant4 has been used throughout the nuclear and high-energy physics community\nto simulate energy depositions in various detectors and materials. These\nsimulations have mostly been run with a source beam outside the detector. In\nthe case of low-background physics, however, a primary concern is the effect on\nthe detector from radioactivity inherent in the detector parts themselves. From\nthis standpoint, there is no single source or beam, but rather a collection of\nsources with potentially complicated spatial extent. LUXSim is a simulation\nframework used by the LUX collaboration that takes a component-centric approach\nto event generation and recording. A new set of classes allows for multiple\nradioactive sources to be set within any number of components at run time, with\nthe entire collection of sources handled within a single simulation run.\nVarious levels of information can also be recorded from the individual\ncomponents, with these record levels also being set at runtime. This\nflexibility in both source generation and information recording is possible\nwithout the need to recompile, reducing the complexity of code management and\nthe proliferation of versions. Within the code itself, casting geometry objects\nwithin this new set of classes rather than as the default Geant4 classes\nautomatically extends this flexibility to every individual component. No\nadditional work is required on the part of the developer, reducing development\ntime and increasing confidence in the results. We describe the guiding\nprinciples behind LUXSim, detail some of its unique classes and methods, and\ngive examples of usage.\n  * Corresponding author, kareem@llnl.gov \n\n"}
{"id": "1111.2352", "contents": "Title: Using TMine for the Fermi-LAT Event Analysis Abstract: The Large Area Telescope (LAT) event analysis is the final stage in the event\nreconstruction responsible for the creation of high-level variables (e.g.,\nevent energy, incident direction, particle type, etc.). We discuss the\ndevelopment of TMine, a powerful new tool for designing and implementing event\nclassification analyses (e.g., distinguishing photons from charged particles).\nTMine is structured on ROOT, a data analysis framework that is the de-facto\nstandard for current high energy physics experiments; thus, TMine fits\nnaturally into the ROOT-based data processing pipeline of the LAT. TMine\nprovides a visual development environment for the LAT event analysis and\nutilizes advanced multivariate classification algorithms implemented in ROOT.\nWe discuss the application of TMine to the next iteration of the event analysis\n(Pass 8), the LAT charged particle analyses, and the classification of\nunassociated LAT gamma-ray sources. \n\n"}
{"id": "1111.3213", "contents": "Title: A simple energy loss model and its applications for silicon detectors Abstract: The energy loss of charged particles in silicon can be approximated by a\nsimple analytical model. With help of measured charge deposits in individual\nchannels of hit clusters their position and energy can be estimated. Deposits\nbelow threshold and saturated values are treated properly, resulting in a wider\ndynamic range. The proposed method gives improvements on both hit position and\nenergy residuals. The model is successfully applied to track differential\nenergy loss estimation and to detector gain calibration tasks. \n\n"}
{"id": "1111.6453", "contents": "Title: Learning with Submodular Functions: A Convex Optimization Perspective Abstract: Submodular functions are relevant to machine learning for at least two\nreasons: (1) some problems may be expressed directly as the optimization of\nsubmodular functions and (2) the lovasz extension of submodular functions\nprovides a useful set of regularization functions for supervised and\nunsupervised learning. In this monograph, we present the theory of submodular\nfunctions from a convex analysis perspective, presenting tight links between\ncertain polyhedra, combinatorial optimization and convex optimization problems.\nIn particular, we show how submodular function minimization is equivalent to\nsolving a wide variety of convex optimization problems. This allows the\nderivation of new efficient algorithms for approximate and exact submodular\nfunction minimization with theoretical guarantees and good practical\nperformance. By listing many examples of submodular functions, we review\nvarious applications to machine learning, such as clustering, experimental\ndesign, sensor placement, graphical model structure learning or subset\nselection, as well as a family of structured sparsity-inducing norms that can\nbe derived and used from submodular functions. \n\n"}
{"id": "1112.2328", "contents": "Title: Exact Safety Verification of Hybrid Systems Using Sums-Of-Squares\n  Representation Abstract: In this paper we discuss how to generate inductive invariants for safety\nverification of hybrid systems. A hybrid symbolic-numeric method is presented\nto compute inequality inductive invariants of the given systems. A numerical\ninvariant of the given system can be obtained by solving a parameterized\npolynomial optimization problem via sum-of-squares (SOS) relaxation. And a\nmethod based on Gauss-Newton refinement and rational vector recovery is\ndeployed to obtain the invariants with rational coefficients, which exactly\nsatisfy the conditions of invariants. Several examples are given to illustrate\nour algorithm. \n\n"}
{"id": "1112.2803", "contents": "Title: The effect of bias feed profile on spectral properties of noisy\n  Josephson flux flow oscillator Abstract: For creation of a noisy non-stationary spectrometer working in the subTHz\nfrequency range the power and spectral characteristics of a flux flow\noscillator (FFO), based on a long Josephson junction are studied. The effect of\nvarious bias current profiles on the spectral linewidth of long overlap\nJosephson junction are investigated and comparison with the inline junction\ngeometry is outlined. It has been demonstrated that the spectral linewidth can\nbe increased by a factor of three with the moderate reduction of emitted power. \n\n"}
{"id": "1201.0745", "contents": "Title: Communities and bottlenecks: Trees and treelike networks have high\n  modularity Abstract: Much effort has gone into understanding the modular nature of complex\nnetworks. Communities, also known as clusters or modules, are typically\nconsidered to be densely interconnected groups of nodes that are only sparsely\nconnected to other groups in the network. Discovering high quality communities\nis a difficult and important problem in a number of areas. The most popular\napproach is the objective function known as modularity, used both to discover\ncommunities and to measure their strength. To understand the modular structure\nof networks it is then crucial to know how such functions evaluate different\ntopologies, what features they account for, and what implicit assumptions they\nmay make. We show that trees and treelike networks can have unexpectedly and\noften arbitrarily high values of modularity. This is surprising since trees are\nmaximally sparse connected graphs and are not typically considered to possess\nmodular structure, yet the nonlocal null model used by modularity assigns low\nprobabilities, and thus high significance, to the densities of these sparse\ntree communities. We further study the practical performance of popular methods\non model trees and on a genealogical data set and find that the discovered\ncommunities also have very high modularity, often approaching its maximum\nvalue. Statistical tests reveal the communities in trees to be significant, in\ncontrast with known results for partitions of sparse, random graphs. \n\n"}
{"id": "1201.2514", "contents": "Title: Analytical properties of horizontal visibility graphs in the Feigenbaum\n  scenario Abstract: Time series are proficiently converted into graphs via the horizontal\nvisibility (HV) algorithm, which prompts interest in its capability for\ncapturing the nature of different classes of series in a network context. We\nhave recently shown [1] that dynamical systems can be studied from a novel\nperspective via the use of this method. Specifically, the period-doubling and\nband-splitting attractor cascades that characterize unimodal maps transform\ninto families of graphs that turn out to be independent of map nonlinearity or\nother particulars. Here we provide an in depth description of the HV treatment\nof the Feigenbaum scenario, together with analytical derivations that relate to\nthe degree distributions, mean distances, clustering coefficients, etc.,\nassociated to the bifurcation cascades and their accumulation points. We\ndescribe how the resultant families of graphs can be framed into a\nrenormalization group scheme in which fixed-point graphs reveal their scaling\nproperties. These fixed points are then re-derived from an entropy optimization\nprocess defined for the graph sets, confirming a suggested connection between\nrenormalization group and entropy optimization. Finally, we provide analytical\nand numerical results for the graph entropy and show that it emulates the\nLyapunov exponent of the map independently of its sign. \n\n"}
{"id": "1201.4615", "contents": "Title: Augmented L1 and Nuclear-Norm Models with a Globally Linearly Convergent\n  Algorithm Abstract: This paper studies the long-existing idea of adding a nice smooth function to\n\"smooth\" a non-differentiable objective function in the context of sparse\noptimization, in particular, the minimization of\n$||x||_1+1/(2\\alpha)||x||_2^2$, where $x$ is a vector, as well as the\nminimization of $||X||_*+1/(2\\alpha)||X||_F^2$, where $X$ is a matrix and\n$||X||_*$ and $||X||_F$ are the nuclear and Frobenius norms of $X$,\nrespectively. We show that they can efficiently recover sparse vectors and\nlow-rank matrices. In particular, they enjoy exact and stable recovery\nguarantees similar to those known for minimizing $||x||_1$ and $||X||_*$ under\nthe conditions on the sensing operator such as its null-space property,\nrestricted isometry property, spherical section property, or RIPless property.\nTo recover a (nearly) sparse vector $x^0$, minimizing\n$||x||_1+1/(2\\alpha)||x||_2^2$ returns (nearly) the same solution as minimizing\n$||x||_1$ almost whenever $\\alpha\\ge 10||x^0||_\\infty$. The same relation also\nholds between minimizing $||X||_*+1/(2\\alpha)||X||_F^2$ and minimizing\n$||X||_*$ for recovering a (nearly) low-rank matrix $X^0$, if $\\alpha\\ge\n10||X^0||_2$. Furthermore, we show that the linearized Bregman algorithm for\nminimizing $||x||_1+1/(2\\alpha)||x||_2^2$ subject to $Ax=b$ enjoys global\nlinear convergence as long as a nonzero solution exists, and we give an\nexplicit rate of convergence. The convergence property does not require a\nsolution solution or any properties on $A$. To our knowledge, this is the best\nknown global convergence result for first-order sparse optimization algorithms. \n\n"}
{"id": "1202.0425", "contents": "Title: Comparing network covers using mutual information Abstract: In network science, researchers often use mutual information to understand\nthe difference between network partitions produced by community detection\nmethods. Here we extend the use of mutual information to covers, that is, the\ncases where a node can belong to more than one module. In our proposed\nsolution, the underlying stochastic process used to compare partitions is\nextended to deal with covers, and the random variables of the new process are\nsimply fed into the usual definition of mutual information. With partitions,\nour extended process behaves exactly as the conventional approach for\npartitions, and thus, the mutual information values obtained are the same. We\nalso describe how to perform sampling and do error estimation for our extended\nprocess, as both are necessary steps for a practical application of this\nmeasure. The stochastic process that we define here is not only applicable to\nnetworks, but can also be used to compare more general set-to-set binary\nrelations. \n\n"}
{"id": "1203.0637", "contents": "Title: Extension of de Rham decomposition theorem via non-Euclidean development Abstract: In the present paper, we give a necessary and sufficient condition for a\nRiemannian manifold $(M,g)$ to have a reducible action of a hyperbolic analogue\nof the holonomy group. This condition amounts to a decomposition of $(M,g)$ as\na warped product of a special form, in analogy to the classical de Rham\ndecomposition theorem for Riemannian manifolds. As a consequence of these\nresults and Berger's classification of holonomy groups, we obtain a simple\nnecessary and sufficient condition for the complete controllability of the\nsystem of $(M,g)$ rolling against the hyperbolic space. \n\n"}
{"id": "1203.2031", "contents": "Title: Design of modular wireless sensor Abstract: The paper addresses combinatorial approach to design of modular wireless\nsensor as composition of the sensor element from its component alternatives and\naggregation of the obtained solutions into a resultant aggregated solution. A\nhierarchical model is used for the wireless sensor element. The solving process\nconsists of three stages: (i) multicriteria ranking of design alternatives for\nsystem components/parts, (ii) composing the selected design alternatives into\ncomposite solution(s) while taking into account ordinal quality of the design\nalternatives above and their compatibility (this stage is based on Hierarchical\nMorphological Multicriteria Design - HMMD), and (iii) aggregation of the\nobtained composite solutions into a resultant aggregated solution(s). A\nnumerical example describes the problem structuring and solving processes for\nmodular alarm wireless sensor element. \n\n"}
{"id": "1203.3742", "contents": "Title: Path-Following Gradient-Based Decomposition Algorithms For Separable\n  Convex Optimization Abstract: A new decomposition optimization algorithm, called \\textit{path-following\ngradient-based decomposition}, is proposed to solve separable convex\noptimization problems. Unlike path-following Newton methods considered in the\nliterature, this algorithm does not requires any smoothness assumption on the\nobjective function. This allows us to handle more general classes of problems\narising in many real applications than in the path-following Newton methods.\nThe new algorithm is a combination of three techniques, namely smoothing,\nLagrangian decomposition and path-following gradient framework. The algorithm\ndecomposes the original problem into smaller subproblems by using dual\ndecomposition and smoothing via self-concordant barriers, updates the dual\nvariables using a path-following gradient method and allows one to solve the\nsubproblem in parallel. Moreover, the algorithmic parameters are updated\nautomatically without any tuning strategy as in augmented Lagrangian\napproaches. We prove the global convergence of the new algorithm and analyze\nits local convergence rate. Then, we modify the proposed algorithm by applying\nNesterov's accelerating scheme to get a new variant which has a better local\nconvergence rate. Finally, we present preliminary numerical tests that confirm\nthe theory development. \n\n"}
{"id": "1203.4981", "contents": "Title: Imagiro: an implementation of Bayesian iterative unfolding for high\n  energy physics Abstract: Unfolding of reconstructed event properties to identify the true features of\ncollider events is a complementary method to the established practice of\ndetector calibration, and is particularly relevant to large, composite particle\ndetectors such as those at the Large Hadron Collider. The behaviour of the\ndetector is simulated and used to create a mapping between the true properties\nof events and their reconstructed equivalents. Unfolding attempts to invert\nthis mapping for use in correcting measurements.\n  Imagiro is a new software package providing Bayesian iterative unfolding with\nsystematic and statistical error estimation. The software is designed to\nsimplify the user experience with automatic self-testing and the calculation of\noptimal parameters. Methods are provided for loading data and producing plotted\nresults in the widely used ROOT format. \n\n"}
{"id": "1203.6600", "contents": "Title: Beyond It\\^o versus Stratonovich Abstract: Recently, a novel framework to handle stochastic processes has emerged from a\nseries of studies in biology, showing situations beyond 'It\\^o versus\nStratonovich'. Its internal consistency can be demonstrated via the zero mass\nlimit of a generalized Klein-Kramers equation. Moreover, the connection to\nother integrations becomes evident: the obtained Fokker-Planck equation defines\na new type of stochastic calculus that in general differs from the\n{\\alpha}-type interpretation. A unique advantage of this new approach is a\nnatural correspondence between stochastic and deterministic dynamics, which is\nuseful or may even be essential in practice. The core of the framework is a\ntransformation from the usual Langevin equation to a form that contains a\npotential function with two additional dynamical matrices, which reveals an\nunderlying symplectic structure. The framework has a direct physical meaning\nand a straightforward experimental realization. A recent experiment has offered\na first empirical validation of this new stochastic integration. \n\n"}
{"id": "1204.0991", "contents": "Title: Distributed Robust Power System State Estimation Abstract: Deregulation of energy markets, penetration of renewables, advanced metering\ncapabilities, and the urge for situational awareness, all call for system-wide\npower system state estimation (PSSE). Implementing a centralized estimator\nthough is practically infeasible due to the complexity scale of an\ninterconnection, the communication bottleneck in real-time monitoring, regional\ndisclosure policies, and reliability issues. In this context, distributed PSSE\nmethods are treated here under a unified and systematic framework. A novel\nalgorithm is developed based on the alternating direction method of\nmultipliers. It leverages existing PSSE solvers, respects privacy policies,\nexhibits low communication load, and its convergence to the centralized\nestimates is guaranteed even in the absence of local observability. Beyond the\nconventional least-squares based PSSE, the decentralized framework accommodates\na robust state estimator. By exploiting interesting links to the compressive\nsampling advances, the latter jointly estimates the state and identifies\ncorrupted measurements. The novel algorithms are numerically evaluated using\nthe IEEE 14-, 118-bus, and a 4,200-bus benchmarks. Simulations demonstrate that\nthe attainable accuracy can be reached within a few inter-area exchanges, while\nlargest residual tests are outperformed. \n\n"}
{"id": "1204.1018", "contents": "Title: Extension of the Measurement Capabilities of the Quadrupole Resonator Abstract: The Quadrupole Resonator, designed to measure the surface resistance of\nsuperconducting samples at 400 MHz has been refurbished. The accuracy of its\nRF-DC compensation measurement technique is tested by an independent method. It\nis shown that the device enables also measurements at 800 and 1200 MHz and is\ncapable to probe the critical RF magnetic field. The electric and magnetic\nfield configuration of the Quadrupole Resonator are dependent on the excited\nmode. It is shown how this can be used to distinguish between electric and\nmagnetic losses. \n\n"}
{"id": "1204.1220", "contents": "Title: Diagonal and Low-Rank Matrix Decompositions, Correlation Matrices, and\n  Ellipsoid Fitting Abstract: In this paper we establish links between, and new results for, three problems\nthat are not usually considered together. The first is a matrix decomposition\nproblem that arises in areas such as statistical modeling and signal\nprocessing: given a matrix $X$ formed as the sum of an unknown diagonal matrix\nand an unknown low rank positive semidefinite matrix, decompose $X$ into these\nconstituents. The second problem we consider is to determine the facial\nstructure of the set of correlation matrices, a convex set also known as the\nelliptope. This convex body, and particularly its facial structure, plays a\nrole in applications from combinatorial optimization to mathematical finance.\nThe third problem is a basic geometric question: given points\n$v_1,v_2,...,v_n\\in \\R^k$ (where $n > k$) determine whether there is a centered\nellipsoid passing \\emph{exactly} through all of the points.\n  We show that in a precise sense these three problems are equivalent.\nFurthermore we establish a simple sufficient condition on a subspace $U$ that\nensures any positive semidefinite matrix $L$ with column space $U$ can be\nrecovered from $D+L$ for any diagonal matrix $D$ using a convex\noptimization-based heuristic known as minimum trace factor analysis. This\nresult leads to a new understanding of the structure of rank-deficient\ncorrelation matrices and a simple condition on a set of points that ensures\nthere is a centered ellipsoid passing through them. \n\n"}
{"id": "1204.1683", "contents": "Title: Viscosity Solutions for a System of PDEs and Optimal Switching Abstract: In this paper, we study the $m$-states optimal switching problem in finite\nhorizon, when the switching cost functions are arbitrary and can be positive or\nnegative. This has an economic incentive in terms of central evaluation in\ncases where such organizations or state grants or financial assistance to power\nplants that promotes green energy in their production activity or what uses\nless polluting modes in their production. We show existence for optimal\nstrategy via a verification theorem then we show existence and uniqueness of\nthe value processes by using an approximation scheme. In the markovian\nframework we show that the value processes can be characterized in terms of\ndeterministic continuous functions of the state of the process. Those latter\nfunctions are the unique viscosity solutions for a system of $m$ variational\npartial differential inequalities with inter-connected obstacles. \n\n"}
{"id": "1204.1821", "contents": "Title: Permutation Complexity and Coupling Measures in Hidden Markov Models Abstract: In [Haruna, T. and Nakajima, K., 2011. Physica D 240, 1370-1377], the authors\nintroduced the duality between values (words) and orderings (permutations) as a\nbasis to discuss the relationship between information theoretic measures for\nfinite-alphabet stationary stochastic processes and their permutation\nanalogues. It has been used to give a simple proof of the equality between the\nentropy rate and the permutation entropy rate for any finite-alphabet\nstationary stochastic process and show some results on the excess entropy and\nthe transfer entropy for finite-alphabet stationary ergodic Markov processes.\nIn this paper, we extend our previous results to hidden Markov models and show\nthe equalities between various information theoretic complexity and coupling\nmeasures and their permutation analogues. In particular, we show the following\ntwo results within the realm of hidden Markov models with ergodic internal\nprocesses: the two permutation analogues of the transfer entropy, the symbolic\ntransfer entropy and the transfer entropy on rank vectors, are both equivalent\nto the transfer entropy if they are considered as the rates, and the directed\ninformation theory can be captured by the permutation entropy approach. \n\n"}
{"id": "1204.2997", "contents": "Title: Hyperbolicity cones of elementary symmetric polynomials are\n  spectrahedral Abstract: We prove that the hyperbolicity cones of elementary symmetric polynomials are\nspectrahedral, i.e., they are slices of the cone of positive semidefinite\nmatrices. The proof uses the matrix--tree theorem, an idea already present in\nChoe et al. \n\n"}
{"id": "1204.3136", "contents": "Title: Identifying financial crises in real time Abstract: Following the thermodynamic formulation of multifractal measure that was\nshown to be capable of detecting large fluctuations at an early stage, here we\npropose a new index which permits us to distinguish events like financial\ncrisis in real time . We calculate the partition function from where we obtain\nthermodynamic quantities analogous to free energy and specific heat. The index\nis defined as the normalized energy variation and it can be used to study the\nbehavior of stochastic time series, such as financial market daily data. Famous\nfinancial market crashes - Black Thursday (1929), Black Monday (1987) and\nSubprime crisis (2008) - are identified with clear and robust results. The\nmethod is also applied to the market fluctuations of 2011. From these results\nit appears as if the apparent crisis of 2011 is of a different nature from the\nother three. We also show that the analysis has forecasting capabilities. \n\n"}
{"id": "1204.3259", "contents": "Title: Combinatorial Evolution and Forecasting of Communication Protocol ZigBee Abstract: The article addresses combinatorial evolution and forecasting of\ncommunication protocol for wireless sensor networks (ZigBee). Morphological\ntree structure (a version of and-or tree) is used as a hierarchical model for\nthe protocol. Three generations of ZigBee protocol are examined. A set of\nprotocol change operations is generated and described. The change operations\nare used as items for forecasting based on combinatorial problems (e.g.,\nclustering, knapsack problem, multiple choice knapsack problem). Two kinds of\npreliminary forecasts for the examined communication protocol are considered:\n(i) direct expert (expert judgment) based forecast, (ii) computation of the\nforecast(s) (usage of multicriteria decision making and combinatorial\noptimization problems). Finally, aggregation of the obtained preliminary\nforecasts is considered (two aggregation strategies are used). \n\n"}
{"id": "1204.4717", "contents": "Title: Energy-Efficient Building HVAC Control Using Hybrid System LBMPC Abstract: Improving the energy-efficiency of heating, ventilation, and air-conditioning\n(HVAC) systems has the potential to realize large economic and societal\nbenefits. This paper concerns the system identification of a hybrid system\nmodel of a building-wide HVAC system and its subsequent control using a hybrid\nsystem formulation of learning-based model predictive control (LBMPC). Here,\nthe learning refers to model updates to the hybrid system model that\nincorporate the heating effects due to occupancy, solar effects, outside air\ntemperature (OAT), and equipment, in addition to integrator dynamics inherently\npresent in low-level control. Though we make significant modeling\nsimplifications, our corresponding controller that uses this model is able to\nexperimentally achieve a large reduction in energy usage without any\ndegradations in occupant comfort. It is in this way that we justify the\nmodeling simplifications that we have made. We conclude by presenting results\nfrom experiments on our building HVAC testbed, which show an average of 1.5MWh\nof energy savings per day (p = 0.002) with a 95% confidence interval of 1.0MWh\nto 2.1MWh of energy savings. \n\n"}
{"id": "1204.4853", "contents": "Title: WHIZARD @ LCForum 2012: A Status Report Abstract: This is a status report of the WHIZARD Monte Carlo multi-purpose event\ngenerator given at the LCFORUM 2012 at DESY. I review here the development of\nthe WHIZARD generator version 2 with a special emphasis on linear collider\nphysics. In case you use the program, please do cite the official reference(s). \n\n"}
{"id": "1205.0050", "contents": "Title: On the dynamic programming principle for uniformly nondegenerate\n  stochastic differential games in domains and the Isaacs equations Abstract: We prove the dynamic programming principe for uniformly nondegenerate\nstochastic differential games in the framework of time-homogeneous diffusion\nprocesses considered up to the first exit time from a domain. In contrast with\nprevious results established for constant stopping times we allow arbitrary\nstopping times and randomized ones as well. There is no assumption about\nsolvability of the the Isaacs equation in any sense (classical or viscosity).\nThe zeroth-order \"coefficient\" and the \"free\" term are only assumed to be\nmeasurable in the space variable. We also prove that value functions are\nuniquely determined by the functions defining the corresponding Isaacs\nequations and thus stochastic games with the same Isaacs equation have the same\nvalue functions. \n\n"}
{"id": "1205.1863", "contents": "Title: Intrinsic volumes of symmetric cones Abstract: We compute the intrinsic volumes of the cone of positive semidefinite\nmatrices over the real numbers, over the complex numbers, and over the\nquaternions, in terms of integrals related to Mehta's integral. Several\napplications for the probabilistic analysis of semidefinite programming are\ngiven. \n\n"}
{"id": "1205.1925", "contents": "Title: Hamiltonian Annealed Importance Sampling for partition function\n  estimation Abstract: We introduce an extension to annealed importance sampling that uses\nHamiltonian dynamics to rapidly estimate normalization constants. We\ndemonstrate this method by computing log likelihoods in directed and undirected\nprobabilistic image models. We compare the performance of linear generative\nmodels with both Gaussian and Laplace priors, product of experts models with\nLaplace and Student's t experts, the mc-RBM, and a bilinear generative model.\nWe provide code to compare additional models. \n\n"}
{"id": "1205.2081", "contents": "Title: The Computational Complexity of the Restricted Isometry Property, the\n  Nullspace Property, and Related Concepts in Compressed Sensing Abstract: This paper deals with the computational complexity of conditions which\nguarantee that the NP-hard problem of finding the sparsest solution to an\nunderdetermined linear system can be solved by efficient algorithms. In the\nliterature, several such conditions have been introduced. The most well-known\nones are the mutual coherence, the restricted isometry property (RIP), and the\nnullspace property (NSP). While evaluating the mutual coherence of a given\nmatrix is easy, it has been suspected for some time that evaluating RIP and NSP\nis computationally intractable in general. We confirm these conjectures by\nshowing that for a given matrix A and positive integer k, computing the best\nconstants for which the RIP or NSP hold is, in general, NP-hard. These results\nare based on the fact that determining the spark of a matrix is NP-hard, which\nis also established in this paper. Furthermore, we also give several complexity\nstatements about problems related to the above concepts. \n\n"}
{"id": "1205.5950", "contents": "Title: Unique Continuation and Observability Estimates for 2-D Stokes Equations\n  with the Navier Slip Boundary Condition Abstract: This paper presents a unique continuation estimate for 2-D Stokes equations\nwith the Naiver slip boundary condition in a bounded and simply connected\ndomain. Consequently, an observability estimate for this equation from a subset\nof positive measure in time follows from the aforementioned unique continuation\nestimate and the new strategy developed in [16]. Several applications of the\nabove-mentioned observability estimate to control problems of the Stokes\nequations are given. \n\n"}
{"id": "1205.6946", "contents": "Title: Explicit solution of relative entropy weighted control Abstract: We consider the minimization over probability measures of the expected value\nof a random variable, regularized by relative entropy with respect to a given\nprobability distribution. In the general setting we provide a complete\ncharacterization of the situations in which a finite optimal value exists and\nthe situations in which a minimizing probability distribution exists.\nSpecializing to the case where the underlying probability distribution is\nWiener measure, we characterize finite relative entropy changes of measure in\nterms of square integrability of the corresponding change of drift. For the\noptimal change of measure for the relative entropy weighted optimization, an\nexpression involving the Malliavin derivative of the cost random variable is\nderived. The theory is illustrated by its application to several examples,\nincluding the case where the cost variable is the maximum of a standard\nBrownian motion over a finite time horizon. For this example we obtain an exact\noptimal drift, as well as an approximation of the optimal drift through a\nMonte-Carlo algorithm. \n\n"}
{"id": "1206.2245", "contents": "Title: Pippi - painless parsing, post-processing and plotting of posterior and\n  likelihood samples Abstract: Interpreting samples from likelihood or posterior probability density\nfunctions is rarely as straightforward as it seems it should be. Producing\npublication-quality graphics of these distributions is often similarly painful.\nIn this short note I describe pippi, a simple, publicly-available package for\nparsing and post-processing such samples, as well as generating high-quality\nPDF graphics of the results. Pippi is easily and extensively configurable and\ncustomisable, both in its options for parsing and post-processing samples, and\nin the visual aspects of the figures it produces. I illustrate some of these\nusing an existing supersymmetric global fit, performed in the context of a\ngamma-ray search for dark matter. Pippi can be downloaded and followed at\nhttp://github.com/patscott/pippi . \n\n"}
{"id": "1206.2599", "contents": "Title: A tale of two cities. Vulnerabilities of the London and Paris transit\n  networks Abstract: This paper analyses the impact of random failure or attack on the public\ntransit networks of London and Paris in a comparative study. In particular we\nanalyze how the dysfunction or removal of sets of stations or links (rails,\nroads, etc.) affects the connectivity properties within these networks. We show\nhow accumulating dysfunction leads to emergent phenomena that cause the\ntransportation system to break down as a whole. Simulating different directed\nattack strategies, we find minimal strategies with high impact and identify\na-priory criteria that correlate with the resilience of these networks. To\ndemonstrate our approach, we choose the London and Paris public transit\nnetworks. Our quantitative analysis is performed in the frames of the complex\nnetwork theory - a methodological tool that has emerged recently as an\ninterdisciplinary approach joining methods and concepts of the theory of random\ngraphs, percolation, and statistical physics. In conclusion we demonstrate that\ntaking into account cascading effects the network integrity is controlled for\nboth networks by less than 0.5 % of the stations i.e. 19 for Paris and 34 for\nLondon. \n\n"}
{"id": "1206.3123", "contents": "Title: Forecasting cosmological parameter constraints from near-future\n  space-based galaxy surveys Abstract: The next generation of space-based galaxy surveys are expected to measure the\ngrowth rate of structure to about a percent level over a range of redshifts.\nThe rate of growth of structure as a function of redshift depends on the\nbehaviour of dark energy and so can be used to constrain parameters of dark\nenergy models. In this work we investigate how well these future data will be\nable to constrain the time dependence of the dark energy density. We consider\nparameterizations of the dark energy equation of state, such as XCDM and wCDM,\nas well as a consistent physical model of time-evolving scalar field dark\nenergy, \\phi CDM. We show that if the standard, specially-flat cosmological\nmodel is taken as a fiducial model of the Universe, these near-future\nmeasurements of structure growth will be able to constrain the time-dependence\nof scalar field dark energy density to a precision of about 10%, which is\nalmost an order of magnitude better than what can be achieved from a\ncompilation of currently available data sets. \n\n"}
{"id": "1206.5376", "contents": "Title: Note on stochastic control problems related with general fully coupled\n  forward-backward stochastic differential equations Abstract: In this paper we study stochastic optimal control problems of general fully\ncoupled forward-backward stochastic differential equations (FBSDEs). In Li and\nWei [8] the authors studied two cases of diffusion coefficients $\\sigma$ of\nFSDEs, in one case when $\\sigma$\\ depends on the control and does not depend on\nthe second component of the solution $(Y, Z)$ of the BSDE, and in the other\ncase $\\sigma$ depends on $Z$ and doesn't depend on the control. Here we study\nthe general case when $\\sigma$ depends on both $Z$ and the control at the same\ntime. The recursive cost functionals are defined by controlled general fully\ncoupled FBSDEs, then the value functions are given by taking the essential\nsupremum of the cost functionals over all admissible controls. We give the\nformulation of related generalized Hamilton-Jacobi-Bellman (HJB) equations, and\nprove the value function is its viscosity solution. \n\n"}
{"id": "1206.5735", "contents": "Title: Gravity vs radiation model: on the importance of scale and heterogeneity\n  in commuting flows Abstract: We test the recently introduced radiation model against the gravity model for\nthe system composed of England and Wales, both for commuting patterns and for\npublic transportation flows. The analysis is performed both at macroscopic\nscales, i.e. at the national scale, and at microscopic scales, i.e. at the city\nlevel. It is shown that the thermodynamic limit assumption for the original\nradiation model significantly underestimates the commuting flows for large\ncities. We then generalize the radiation model, introducing the correct\nnormalisation factor for finite systems. We show that even if the gravity model\nhas a better overall performance the parameter-free radiation model gives\ncompetitive results, especially for large scales. \n\n"}
{"id": "1206.6670", "contents": "Title: A maximum principle for infinite horizon delay equations Abstract: We prove a maximum principle of optimal control of stochastic delay equations\non infinite horizon. We establish first and second sufficient stochastic\nmaximum principles as well as necessary conditions for that problem. We\nillustrate our results by an application to the optimal consumption rate from\nan economic quantity. \n\n"}
{"id": "1207.0048", "contents": "Title: Economic Dispatch in Unbalanced Distribution Networks via Semidefinite\n  Relaxation Abstract: The economic dispatch problem is considered for unbalanced three-phase power\ndistribution networks entailing both non-deferrable and elastic loads, and\ndistributed generation (DG) units. The objective is to minimize the costs of\npower drawn from the main grid and supplied by the DG units over a given time\nhorizon, while meeting the overall load demand and effecting voltage\nregulation. Similar to optimal power flow counterparts for balanced systems,\nthe resultant optimization problem is nonconvex. Nevertheless, a semidefinite\nprogramming (SDP) relaxation technique is advocated to obtain a (relaxed)\nconvex problem solvable in polynomial-time complexity. To promote a reliable\nyet efficient feeder operation, SDP-compliant constraints on line and neutral\ncurrent magnitudes are accommodated in the optimization formulated, along with\nconstraints on the power factor at the substation and at nodes equipped with\ncapacitor banks. Tests on the IEEE 13-node radial feeder demonstrate the\nability of the proposed method to attain the globally optimal solution of the\noriginal nonconvex problem. \n\n"}
{"id": "1207.0407", "contents": "Title: What is Statistics?; The Answer by Quantum Language Abstract: Since the problem: \"What is statistics?\" is most fundamental in sceince, in\norder to solve this problem, there is every reason to believe that we have to\nstart from the proposal of a worldview. Recently we proposed measurement theory\n(i.e., quantum language, or the linguistic interpretation of quantum\nmechanics), which is characterized as the linguistic turn of the Copenhagen\ninterpretation of quantum mechanics. This turn from physics to language does\nnot only extend quantum theory to classical theory but also yield the quantum\nmechanical world view (i.e., the (quantum) linguistic world view, and thus, a\nform of quantum thinking, in other words, quantum philosophy). Thus, we believe\nthat the quantum lingistic formulation of statistics gives an answer to the\nquestion: \"What is statistics?\". In this paper, this will be done through the\nstudies of inference interval, statistical hypothesis testing, Fisher maximum\nlikelihood method, Bayes method and regression analysis in meaurement theory. \n\n"}
{"id": "1207.1040", "contents": "Title: Comparison between the DSMC and DSBGK Methods Abstract: Recently, the DSBGK method (note: the original name DS-BGK is changed to\nDSBGK for simplicity) was proposed based on the BGK equation to reduce the\nstochastic noise in simulating rarefied gas flows at low velocity, in which the\ndeviation from equilibrium state is small making the traditional DSMC\nsimulation time-consuming due to the dominance of noise in transient results.\nIn both DSMC and DSBGK simulations, the simulated molecules move into and out\nof cells randomly and frequently. Consequently, the transient information of\nmolecules in each particular cell contains significant noise. The DSMC method\nuses the transient values of molecular variables to compute the cell's\nvariables (including number density, flow velocity and temperature) and so the\nstochastic noise in its cell's variables is remarkable particularly in the case\nof low velocity. In the DSBGK simulation, the increments rather than the\ntransient information of molecular variables are used to update the cell's\nvariables based on the mass, momentum and energy conservation principles of\nintermolecular collision process. This updating scheme significantly reduces\nthe noise in cell's variables of DSBGK simulations because the molecular\nvariables are updated smoothly by the extrapolation of acceptance-rejection\nscheme and so their increments contain low noise. The detailed comparisons of\nalgorithms and results between the DSMC and DSBGK methods are given here.\nSeveral benchmark problems are simulated to verify the DSBGK method by\ncomparison with the DSMC method as criterion. \n\n"}
{"id": "1207.1149", "contents": "Title: Graver basis and proximity techniques for block-structured separable\n  convex integer minimization problems Abstract: We consider N-fold 4-block decomposable integer programs, which\nsimultaneously generalize N-fold integer programs and two-stage stochastic\ninteger programs with N scenarios. In previous work [R. Hemmecke, M. Koeppe, R.\nWeismantel, A polynomial-time algorithm for optimizing over N-fold 4-block\ndecomposable integer programs, Proc. IPCO 2010, Lecture Notes in Computer\nScience, vol. 6080, Springer, 2010, pp. 219--229], it was proved that for fixed\nblocks but variable N, these integer programs are polynomial-time solvable for\nany linear objective. We extend this result to the minimization of separable\nconvex objective functions. Our algorithm combines Graver basis techniques with\na proximity result [D.S. Hochbaum and J.G. Shanthikumar, Convex separable\noptimization is not much harder than linear optimization, J. ACM 37 (1990),\n843--862], which allows us to use convex continuous optimization as a\nsubroutine. \n\n"}
{"id": "1207.4592", "contents": "Title: Differentially Private Kalman Filtering Abstract: This paper studies the H2 (Kalman) filtering problem in the situation where a\nsignal estimate must be constructed based on inputs from individual\nparticipants, whose data must remain private. This problem arises in emerging\napplications such as smart grids or intelligent transportation systems, where\nusers continuously send data to third-party aggregators performing global\nmonitoring or control tasks, and require guarantees that this data cannot be\nused to infer additional personal information. To provide strong formal privacy\nguarantees against adversaries with arbitrary side information, we rely on the\nnotion of differential privacy introduced relatively recently in the database\nliterature. This notion is extended to dynamic systems with many participants\ncontributing independent input signals, and mechanisms are then proposed to\nsolve the H2 filtering problem with a differential privacy constraint. A method\nfor mitigating the impact of the privacy-inducing mechanism on the estimation\nperformance is described, which relies on controlling the Hinfinity norm of the\nfilter. Finally, we discuss an application to a privacy-preserving traffic\nmonitoring system. \n\n"}
{"id": "1207.5286", "contents": "Title: Backward stochastic partial differential equations with quadratic growth Abstract: This paper is concerned with the existence and uniqueness of weak solutions\nto the Cauchy-Dirichlet problem of backward stochastic partial differential\nequations (BSPDEs) with nonhomogeneous terms of quadratic growth in both the\ngradient of the first unknown and the second unknown. As an example, we\nconsider a non-Markovian stochastic optimal control problem with cost\nfunctional formulated by a quadratic BSDE, where the corresponding value\nfunction satisfies the above quadratic BSPDE. \n\n"}
{"id": "1207.5451", "contents": "Title: Nonlinear spectral unmixing of hyperspectral images using Gaussian\n  processes Abstract: This paper presents an unsupervised algorithm for nonlinear unmixing of\nhyperspectral images. The proposed model assumes that the pixel reflectances\nresult from a nonlinear function of the abundance vectors associated with the\npure spectral components. We assume that the spectral signatures of the pure\ncomponents and the nonlinear function are unknown. The first step of the\nproposed method consists of the Bayesian estimation of the abundance vectors\nfor all the image pixels and the nonlinear function relating the abundance\nvectors to the observations. The endmembers are subsequently estimated using\nGaussian process regression. The performance of the unmixing strategy is\nevaluated with simulations conducted on synthetic and real data. \n\n"}
{"id": "1208.0529", "contents": "Title: Stochastic maximum principle for infinite dimensional control systems Abstract: The general maximum principle is proved for an infinite dimensional\ncontrolled stochastic evolution system. The control is allowed to take values\nin a nonconvex set and enter into both drift and diffusion terms. The\noperator-valued backward stochastic differential equation, which characterizes\nthe second-order adjoint process, is understood via the concept of \"generalized\nsolution\" proposed by Guatteri and Tessitore [SICON 44 (2006)]. \n\n"}
{"id": "1208.2696", "contents": "Title: Distribution Of Wealth In A Network Model Of The Economy Abstract: We show, analytically and numerically, that wealth distribution in the\nBouchaud-M\\'ezard network model of the economy is described by a\nthree-parameter generalized inverse gamma distribution. In the mean-field limit\nof a network with any two agents linked, it reduces to the inverse gamma\ndistribution. \n\n"}
{"id": "1208.3036", "contents": "Title: Bayesian astrostatistics: a backward look to the future Abstract: This perspective chapter briefly surveys: (1) past growth in the use of\nBayesian methods in astrophysics; (2) current misconceptions about both\nfrequentist and Bayesian statistical inference that hinder wider adoption of\nBayesian methods by astronomers; and (3) multilevel (hierarchical) Bayesian\nmodeling as a major future direction for research in Bayesian astrostatistics,\nexemplified in part by presentations at the first ISI invited session on\nastrostatistics, commemorated in this volume. It closes with an intentionally\nprovocative recommendation for astronomical survey data reporting, motivated by\nthe multilevel Bayesian perspective on modeling cosmic populations: that\nastronomers cease producing catalogs of estimated fluxes and other source\nproperties from surveys. Instead, summaries of likelihood functions (or\nmarginal likelihood functions) for source properties should be reported (not\nposterior probability density functions), including nontrivial summaries (not\nsimply upper limits) for candidate objects that do not pass traditional\ndetection thresholds. \n\n"}
{"id": "1208.3569", "contents": "Title: Asymptotically optimal quantum channel reversal for qudit ensembles and\n  multimode Gaussian states Abstract: We investigate the problem of optimally reversing the action of an arbitrary\nquantum channel C which acts independently on each component of an ensemble of\nn identically prepared d-dimensional quantum systems. In the limit of large\nensembles, we construct the optimal reversing channel R* which has to be\napplied at the output ensemble state, to retrieve a smaller ensemble of m\nsystems prepared in the input state, with the highest possible rate m/n. The\nsolution is found by mapping the problem into the optimal reversal of Gaussian\nchannels on quantum-classical continuous variable systems, which is here solved\nas well. Our general results can be readily applied to improve the\nimplementation of robust long-distance quantum communication. As an example, we\ninvestigate the optimal reversal rate of phase flip channels acting on a\nmulti-qubit register. \n\n"}
{"id": "1208.5393", "contents": "Title: Local controllability of 1D Schr\\\"odinger equations with bilinear\n  control and minimal time Abstract: We consider a linear Schr\\\"odinger equation, on a bounded interval, with\nbilinear control.\n  Beauchard and Laurent proved that, under an appropriate non degeneracy\nassumption, this system is controllable, locally around the ground state, in\narbitrary time. Coron proved that a positive minimal time is required for this\ncontrollability, on a particular degenerate example.\n  In this article, we propose a general context for the local controllability\nto hold in large time, but not in small time. The existence of a positive\nminimal time is closely related to the behaviour of the second order term, in\nthe power series expansion of the solution. \n\n"}
{"id": "1208.6338", "contents": "Title: A Widely Applicable Bayesian Information Criterion Abstract: A statistical model or a learning machine is called regular if the map taking\na parameter to a probability distribution is one-to-one and if its Fisher\ninformation matrix is always positive definite. If otherwise, it is called\nsingular. In regular statistical models, the Bayes free energy, which is\ndefined by the minus logarithm of Bayes marginal likelihood, can be\nasymptotically approximated by the Schwarz Bayes information criterion (BIC),\nwhereas in singular models such approximation does not hold.\n  Recently, it was proved that the Bayes free energy of a singular model is\nasymptotically given by a generalized formula using a birational invariant, the\nreal log canonical threshold (RLCT), instead of half the number of parameters\nin BIC. Theoretical values of RLCTs in several statistical models are now being\ndiscovered based on algebraic geometrical methodology. However, it has been\ndifficult to estimate the Bayes free energy using only training samples,\nbecause an RLCT depends on an unknown true distribution.\n  In the present paper, we define a widely applicable Bayesian information\ncriterion (WBIC) by the average log likelihood function over the posterior\ndistribution with the inverse temperature $1/\\log n$, where $n$ is the number\nof training samples. We mathematically prove that WBIC has the same asymptotic\nexpansion as the Bayes free energy, even if a statistical model is singular for\nand unrealizable by a statistical model. Since WBIC can be numerically\ncalculated without any information about a true distribution, it is a\ngeneralized version of BIC onto singular statistical models. \n\n"}
{"id": "1209.0068", "contents": "Title: Two Newton methods on the manifold of fixed-rank matrices endowed with\n  Riemannian quotient geometries Abstract: We consider two Riemannian geometries for the manifold $\\mathcal{M}(p,m\\times\nn)$ of all $m\\times n$ matrices of rank $p$. The geometries are induced on\n$\\mathcal{M}(p,m\\times n)$ by viewing it as the base manifold of the submersion\n$\\pi:(M,N)\\mapsto MN^T$, selecting an adequate Riemannian metric on the total\nspace, and turning $\\pi$ into a Riemannian submersion. The theory of Riemannian\nsubmersions, an important tool in Riemannian geometry, makes it possible to\nobtain expressions for fundamental geometric objects on $\\mathcal{M}(p,m\\times\nn)$ and to formulate the Riemannian Newton methods on $\\mathcal{M}(p,m\\times\nn)$ induced by these two geometries. The Riemannian Newton methods admit a\nstronger and more streamlined convergence analysis than the Euclidean\ncounterpart, and the computational overhead due to the Riemannian geometric\nmachinery is shown to be mild. Potential applications include low-rank matrix\ncompletion and other low-rank matrix approximation problems. \n\n"}
{"id": "1209.1270", "contents": "Title: A practical recipe to fit discrete power-law distributions Abstract: Power laws pervade statistical physics and complex systems, but,\ntraditionally, researchers in these fields have paid little attention to\nproperly fit these distributions. Who has not seen (or even shown) a log-log\nplot of a completely curved line pretending to be a power law? Recently,\nClauset et al. have proposed a method to decide if a set of values of a\nvariable has a distribution whose tail is a power law. The key of their\nprocedure is the identification of the minimum value of the variable for which\nthe fit holds, which is selected as the value for which the Kolmogorov-Smirnov\ndistance between the empirical distribution and its maximum-likelihood fit is\nminimum. However, it has been shown that this method can reject the power-law\nhypothesis even in the case of power-law simulated data. Here we propose a\nsimpler selection criterion, which is illustrated with the more involving case\nof discrete power-law distributions. \n\n"}
{"id": "1209.1530", "contents": "Title: Hahn's Symmetric Quantum Variational Calculus Abstract: We introduce and develop the Hahn symmetric quantum calculus with\napplications to the calculus of variations. Namely, we obtain a necessary\noptimality condition of Euler-Lagrange type and a sufficient optimality\ncondition for variational problems within the context of Hahn's symmetric\ncalculus. Moreover, we show the effectiveness of Leitmann's direct method when\napplied to Hahn's symmetric variational calculus. Illustrative examples are\nprovided. \n\n"}
{"id": "1209.2678", "contents": "Title: Bad Communities with High Modularity Abstract: In this paper we discuss some problematic aspects of Newman's modularity\nfunction QN. Given a graph G, the modularity of G can be written as QN = Qf\n-Q0, where Qf is the intracluster edge fraction of G and Q0 is the expected\nintracluster edge fraction of the null model, i.e., a randomly connected graph\nwith same expected degree distribution as G. It follows that the maximization\nof QN must accomodate two factors pulling in opposite directions: Qf favors a\nsmall number of clusters and Q0 favors many balanced (i.e., with approximately\nequal degrees) clusters. In certain cases the Q0 term can cause overestimation\nof the true cluster number; this is the opposite of the well-known under\nestimation effect caused by the \"resolution limit\" of modularity. We illustrate\nthe overestimation effect by constructing families of graphs with a \"natural\"\ncommunity structure which, however, does not maximize modularity. In fact, we\nprove that we can always find a graph G with a \"natural clustering\" V of G and\nanother, balanced clustering U of G such that (i) the pair (G; U) has higher\nmodularity than (G; V) and (ii) V and U are arbitrarily different. \n\n"}
{"id": "1209.3600", "contents": "Title: Output Feedback H_2 Model Matching for Decentralized Systems with Delays Abstract: This paper gives a new solution to the output feedback H_2 model matching\nproblem for a large class of delayed information sharing patterns. Existing\nmethods for such problems typically reduce the decentralized problem to a\ncentralized problem of higher state dimension. In contrast, the controller\ngiven in this paper is constructed from the solutions to the centralized\ncontrol and estimation Riccati equations for the original system. The problem\nis solved by decomposing the controller into two components. One is\ncentralized, but delayed, while the other is decentralized with finite impulse\nresponse (FIR). It is then shown that the optimal controller can be constructed\nthrough a combination of centralized spectral factorization and quadratic\nprogramming. \n\n"}
{"id": "1209.3834", "contents": "Title: Low-rank matrix completion by Riemannian optimization---extended version Abstract: The matrix completion problem consists of finding or approximating a low-rank\nmatrix based on a few samples of this matrix. We propose a new algorithm for\nmatrix completion that minimizes the least-square distance on the sampling set\nover the Riemannian manifold of fixed-rank matrices. The algorithm is an\nadaptation of classical non-linear conjugate gradients, developed within the\nframework of retraction-based optimization on manifolds. We describe all the\nnecessary objects from differential geometry necessary to perform optimization\nover this low-rank matrix manifold, seen as a submanifold embedded in the space\nof matrices. In particular, we describe how metric projection can be used as\nretraction and how vector transport lets us obtain the conjugate search\ndirections. Finally, we prove convergence of a regularized version of our\nalgorithm under the assumption that the restricted isometry property holds for\nincoherent matrices throughout the iterations. The numerical experiments\nindicate that our approach scales very well for large-scale problems and\ncompares favorably with the state-of-the-art, while outperforming most existing\nsolvers. \n\n"}
{"id": "1209.5565", "contents": "Title: Algebraic characterization of binary graphs Abstract: One of the fundamental concepts in the statistical mechanics field is that of\nensemble. Ensembles of graphs are collections of graphs, defined according to\ncertain rules. The two most used ensembles in network theory are the\nmicrocanonical and the grandcanonical (whose definitions mimick the classical\nones, originally proposed by Boltzmann and Gibbs), even if the latter is far\nmore used than the former to carry on the analytical calculations. For binary\n(undirected or directed) networks, the grandcanonical ensemble is defined by\nconsidering all the graphs with the same number of vertices and a variable\nnumber of links, ranging from 0 to the maximum: N(N-1)/2 for binary, undirected\ngraphs and N(N-1) for binary, directed graphs. Even if it is commonly used\nalmost exclusively as a tool to calculate the average of some topological\nquantity of interest, its structure is so rich to deserve an analysis on its\nown. In this paper a logic-algebraic characterization of the grandcanonical\nensemble of binary graphs is provided. \n\n"}
{"id": "1209.6315", "contents": "Title: Variational integrators for underactuated mechanical control systems\n  with symmetries Abstract: Optimal control problems for underactuated mechanical systems can be seen as\na higher-order variational problem subject to higher-order constraints (that\nis, when the Lagrangian function and the constraints depend on higher-order\nderivatives such as the acceleration, jerk or jounces). In this paper we\ndiscuss the variational formalism for the class of underactuated mechanical\ncontrol systems when the configuration space is a trivial principal bundle and\nthe construction of variational integrators for such mechanical control\nsystems.\n  An interesting family of geometric integrators can be defined using\ndiscretizations of the Hamilton's principle of critical action. This family of\ngeometric integrators is called variational integrators, being one of their\nmain properties the preservation of geometric features as the symplecticity,\nmomentum preservation and good behavior of the energy. We construct variational\nintegrators for higher-order mechanical systems on trivial principal bundles\nand their extension for higher-order constrained systems and we devote special\nattention to the particular case of underactuated mechanical systems \n\n"}
{"id": "1210.0729", "contents": "Title: An Algorithm to Solve Polyhedral Convex Set Optimization Problems Abstract: An algorithm which computes a solution of a set optimization problem is\nprovided. The graph of the objective map is assumed to be given by finitely\nmany linear inequalities. A solution is understood to be a set of points in the\ndomain satisfying two conditions: the attainment of the infimum and minimality\nwith respect to a set relation. In the first phase of the algorithm, a linear\nvector optimization problem, called the vectorial relaxation, is solved. The\nresulting pre-solution yields the attainment of the infimum but, in general,\nnot minimality. In the second phase of the algorithm, minimality is established\nby solving certain linear programs in combination with vertex enumeration of\nsome values of the objective map. \n\n"}
{"id": "1211.2717", "contents": "Title: Proximal Stochastic Dual Coordinate Ascent Abstract: We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results. \n\n"}
{"id": "1211.3128", "contents": "Title: Non-asymptotic Upper Bounds for Deletion Correcting Codes Abstract: Explicit non-asymptotic upper bounds on the sizes of multiple-deletion\ncorrecting codes are presented. In particular, the largest single-deletion\ncorrecting code for $q$-ary alphabet and string length $n$ is shown to be of\nsize at most $\\frac{q^n-q}{(q-1)(n-1)}$. An improved bound on the asymptotic\nrate function is obtained as a corollary. Upper bounds are also derived on\nsizes of codes for a constrained source that does not necessarily comprise of\nall strings of a particular length, and this idea is demonstrated by\napplication to sets of run-length limited strings.\n  The problem of finding the largest deletion correcting code is modeled as a\nmatching problem on a hypergraph. This problem is formulated as an integer\nlinear program. The upper bound is obtained by the construction of a feasible\npoint for the dual of the linear programming relaxation of this integer linear\nprogram.\n  The non-asymptotic bounds derived imply the known asymptotic bounds of\nLevenshtein and Tenengolts and improve on known non-asymptotic bounds.\nNumerical results support the conjecture that in the binary case, the\nVarshamov-Tenengolts codes are the largest single-deletion correcting codes. \n\n"}
{"id": "1211.4391", "contents": "Title: A Non-Differentiable Quantum Variational Embedding in Presence of Time\n  Delays Abstract: We develop Cresson's non-differentiable embedding to quantum problems of the\ncalculus of variations and optimal control with time delay. Main results show\nthat the dynamics of non-differentiable Lagrangian and Hamiltonian systems with\ntime delays can be determined, in a coherent way, by the least-action\nprinciple. \n\n"}
{"id": "1211.5856", "contents": "Title: Distributed Optimal Power Flow for Smart Microgrids Abstract: Optimal power flow (OPF) is considered for microgrids, with the objective of\nminimizing either the power distribution losses, or, the cost of power drawn\nfrom the substation and supplied by distributed generation (DG) units, while\neffecting voltage regulation. The microgrid is unbalanced, due to unequal loads\nin each phase and non-equilateral conductor spacings on the distribution lines.\nSimilar to OPF formulations for balanced systems, the considered OPF problem is\nnonconvex. Nevertheless, a semidefinite programming (SDP) relaxation technique\nis advocated to obtain a convex problem solvable in polynomial-time complexity.\nEnticingly, numerical tests demonstrate the ability of the proposed method to\nattain the globally optimal solution of the original nonconvex OPF. To ensure\nscalability with respect to the number of nodes, robustness to isolated\ncommunication outages, and data privacy and integrity, the proposed SDP is\nsolved in a distributed fashion by resorting to the alternating direction\nmethod of multipliers. The resulting algorithm entails iterative\nmessage-passing among groups of consumers and guarantees faster convergence\ncompared to competing alternatives \n\n"}
{"id": "1211.7183", "contents": "Title: Bayesian Inference for LISA Pathfinder using Markov Chain Monte Carlo\n  Methods Abstract: We present a parameter estimation procedure based on a Bayesian framework by\napplying a Markov Chain Monte Carlo algorithm to the calibration of the\ndynamical parameters of a space based gravitational wave detector. The method\nis based on the Metropolis-Hastings algorithm and a two-stage annealing\ntreatment in order to ensure an effective exploration of the parameter space at\nthe beginning of the chain. We compare two versions of the algorithm with an\napplication to a LISA Pathfinder data analysis problem. The two algorithms\nshare the same heating strategy but with one moving in coordinate directions\nusing proposals from a multivariate Gaussian distribution, while the other uses\nthe natural logarithm of some parameters and proposes jumps in the eigen-space\nof the Fisher Information matrix. The algorithm proposing jumps in the\neigen-space of the Fisher Information matrix demonstrates a higher acceptance\nrate and a slightly better convergence towards the equilibrium parameter\ndistributions in the application to LISA Pathfinder data . For this experiment,\nwe return parameter values that are all within $\\sim1\\sigma$ of the injected\nvalues. When we analyse the accuracy of our parameter estimation in terms of\nthe effect they have on the force-per-unit test mass noise estimate, we find\nthat the induced errors are three orders of magnitude less than the expected\nexperimental uncertainty in the power spectral density. \n\n"}
{"id": "1212.0578", "contents": "Title: Max-plus algebra models of queueing networks Abstract: A class of queueing networks which may have an arbitrary topology, and\nconsist of single-server fork-join nodes with both infinite and finite buffers\nis examined to derive a representation of the network dynamics in terms of\nmax-plus algebra. For the networks, we present a common dynamic state equation\nwhich relates the departure epochs of customers from the network nodes in an\nexplicit vector form determined by a state transition matrix. It is shown how\nthe matrices inherent in particular networks may be calculated from the service\ntimes of customers. Since, in general, an explicit dynamic equation may not\nexist for a network, related existence conditions are established in terms of\nthe network topology. \n\n"}
{"id": "1212.1269", "contents": "Title: Approximate Dynamic Programming via Sum of Squares Programming Abstract: We describe an approximate dynamic programming method for stochastic control\nproblems on infinite state and input spaces. The optimal value function is\napproximated by a linear combination of basis functions with coefficients as\ndecision variables. By relaxing the Bellman equation to an inequality, one\nobtains a linear program in the basis coefficients with an infinite set of\nconstraints. We show that a recently introduced method, which obtains convex\nquadratic value function approximations, can be extended to higher order\npolynomial approximations via sum of squares programming techniques. An\napproximate value function can then be computed offline by solving a\nsemidefinite program, without having to sample the infinite constraint. The\npolicy is evaluated online by solving a polynomial optimization problem, which\nalso turns out to be convex in some cases. We experimentally validate the\nmethod on an autonomous helicopter testbed using a 10-dimensional helicopter\nmodel. \n\n"}
{"id": "1212.1788", "contents": "Title: Approximate discrete-time schemes for the estimation of diffusion\n  processes from complete observations Abstract: In this paper, a modification of the conventional approximations to the\nquasi-maximum likelihood method is introduced for the parameter estimation of\ndiffusion processes from discrete observations. This is based on a convergent\napproximation to the first two conditional moments of the diffusion process\nthrough discrete-time schemes. It is shown that, for finite samples, the\nresulting approximate estimators converge to the quasi-maximum likelihood one\nwhen the error between the discrete-time approximation and the diffusion\nprocess decreases. For an increasing number of observations, the approximate\nestimators are asymptotically normal distributed and their bias decreases when\nthe mentioned error does it. A simulation study is provided to illustrate the\nperformance of the new estimators. The results show that, with respect to the\nconventional approximate estimators, the new ones significantly enhance the\nparameter estimation of the test equations. The proposed estimators are\nintended for the recurrent practical situation where a nonlinear stochastic\nsystem should be identified from a reduced number of complete observations\ndistant in time. \n\n"}
{"id": "1212.2002", "contents": "Title: A simpler approach to obtaining an O(1/t) convergence rate for the\n  projected stochastic subgradient method Abstract: In this note, we present a new averaging technique for the projected\nstochastic subgradient method. By using a weighted average with a weight of t+1\nfor each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)\nwith both an easy proof and an easy implementation. The new scheme is compared\nempirically to existing techniques, with similar performance behavior. \n\n"}
{"id": "1212.2834", "contents": "Title: Dictionary Subselection Using an Overcomplete Joint Sparsity Model Abstract: Many natural signals exhibit a sparse representation, whenever a suitable\ndescribing model is given. Here, a linear generative model is considered, where\nmany sparsity-based signal processing techniques rely on such a simplified\nmodel. As this model is often unknown for many classes of the signals, we need\nto select such a model based on the domain knowledge or using some exemplar\nsignals. This paper presents a new exemplar based approach for the linear model\n(called the dictionary) selection, for such sparse inverse problems. The\nproblem of dictionary selection, which has also been called the dictionary\nlearning in this setting, is first reformulated as a joint sparsity model. The\njoint sparsity model here differs from the standard joint sparsity model as it\nconsiders an overcompleteness in the representation of each signal, within the\nrange of selected subspaces. The new dictionary selection paradigm is examined\nwith some synthetic and realistic simulations. \n\n"}
{"id": "1212.5142", "contents": "Title: Maxallent: Maximizers of all Entropies and Uncertainty of Uncertainty Abstract: The entropy maximum approach (Maxent) was developed as a minimization of the\nsubjective uncertainty measured by the Boltzmann--Gibbs--Shannon entropy. Many\nnew entropies have been invented in the second half of the 20th century. Now\nthere exists a rich choice of entropies for fitting needs. This diversity of\nentropies gave rise to a Maxent \"anarchism\". Maxent approach is now the\nconditional maximization of an appropriate entropy for the evaluation of the\nprobability distribution when our information is partial and incomplete. The\nrich choice of non-classical entropies causes a new problem: which entropy is\nbetter for a given class of applications? We understand entropy as a measure of\nuncertainty which increases in Markov processes. In this work, we describe the\nmost general ordering of the distribution space, with respect to which all\ncontinuous-time Markov processes are monotonic (the Markov order). For\ninference, this approach results in a set of conditionally \"most random\"\ndistributions. Each distribution from this set is a maximizer of its own\nentropy. This \"uncertainty of uncertainty\" is unavoidable in analysis of\nnon-equilibrium systems. Surprisingly, the constructive description of this set\nof maximizers is possible. Two decomposition theorems for Markov processes\nprovide a tool for this description. \n\n"}
{"id": "1212.5567", "contents": "Title: Zipf's law, power laws, and maximum entropy Abstract: Zipf's law, and power laws in general, have attracted and continue to attract\nconsiderable attention in a wide variety of disciplines - from astronomy to\ndemographics to software structure to economics to linguistics to zoology, and\neven warfare. A recent model of random group formation [RGF] attempts a general\nexplanation of such phenomena based on Jaynes' notion of maximum entropy\napplied to a particular choice of cost function. In the present article I argue\nthat the cost function used in the RGF model is in fact unnecessarily\ncomplicated, and that power laws can be obtained in a much simpler way by\napplying maximum entropy ideas directly to the Shannon entropy subject only to\na single constraint: that the average of the logarithm of the observable\nquantity is specified. \n\n"}
{"id": "1212.6105", "contents": "Title: Information channel capacity in the field theory estimation Abstract: The construction of the information capacity for the vector position\nparameter in the Minkowskian space-time is presented. This lays the statistical\nfoundations of the kinematical term of the Lagrangian of the physical action\nfor many field theory models, derived by the extremal physical information\nmethod of Frieden and Soffer. \n\n"}
{"id": "1301.0091", "contents": "Title: On the Robust Optimal Stopping Problem Abstract: We study a robust optimal stopping problem with respect to a set $\\cP$ of\nmutually singular probabilities. This can be interpreted as a zero-sum\ncontroller-stopper game in which the stopper is trying to maximize its pay-off\nwhile an adverse player wants to minimize this payoff by choosing an evaluation\ncriteria from $\\cP$. We show that the \\emph{upper Snell envelope $\\ol{Z}$} of\nthe reward process $Y$ is a supermartingale with respect to an appropriately\ndefined nonlinear expectation $\\ul{\\sE}$, and $\\ol{Z}$ is further an\n$\\ul{\\sE}-$martingale up to the first time $\\t^*$ when $\\ol{Z}$ meets $Y$.\nConsequently, $\\t^*$ is the optimal stopping time for the robust optimal\nstopping problem and the corresponding zero-sum game has a value. Although the\nresult seems similar to the one obtained in the classical optimal stopping\ntheory, the mutual singularity of probabilities and the game aspect of the\nproblem give rise to major technical hurdles, which we circumvent using some\nnew methods. \n\n"}
{"id": "1301.0790", "contents": "Title: Duality for Sudoku Abstract: We consider a mathematical model for the classical Sudoku puzzle, which we\ncall the primal problem and introduce a corresponding dual problem. Both\nproblems are constraint satisfaction models and a duality relation between them\nis proved. Based on these models, we introduce a primal and a dual optimization\nproblem and show weak and strong duality properties. \n\n"}
{"id": "1301.1054", "contents": "Title: Spectral bounds for the independence ratio and the chromatic number of\n  an operator Abstract: We define the independence ratio and the chromatic number for bounded,\nself-adjoint operators on an L^2-space by extending the definitions for the\nadjacency matrix of finite graphs. In analogy to the Hoffman bounds for finite\ngraphs, we give bounds for these parameters in terms of the numerical range of\nthe operator. This provides a theoretical framework in which many packing and\ncoloring problems for finite and infinite graphs can be conveniently studied\nwith the help of harmonic analysis and convex optimization. The theory is\napplied to infinite geometric graphs on Euclidean space and on the unit sphere. \n\n"}
{"id": "1301.1272", "contents": "Title: Convergence Speed of a Dynamical System for Sparse Recovery Abstract: This paper studies the convergence rate of a continuous-time dynamical system\nfor L1-minimization, known as the Locally Competitive Algorithm (LCA). Solving\nL1-minimization} problems efficiently and rapidly is of great interest to the\nsignal processing community, as these programs have been shown to recover\nsparse solutions to underdetermined systems of linear equations and come with\nstrong performance guarantees. The LCA under study differs from the typical L1\nsolver in that it operates in continuous time: instead of being specified by\ndiscrete iterations, it evolves according to a system of nonlinear ordinary\ndifferential equations. The LCA is constructed from simple components, giving\nit the potential to be implemented as a large-scale analog circuit.\n  The goal of this paper is to give guarantees on the convergence time of the\nLCA system. To do so, we analyze how the LCA evolves as it is recovering a\nsparse signal from underdetermined measurements. We show that under appropriate\nconditions on the measurement matrix and the problem parameters, the path the\nLCA follows can be described as a sequence of linear differential equations,\neach with a small number of active variables. This allows us to relate the\nconvergence time of the system to the restricted isometry constant of the\nmatrix. Interesting parallels to sparse-recovery digital solvers emerge from\nthis study. Our analysis covers both the noisy and noiseless settings and is\nsupported by simulation results. \n\n"}
{"id": "1301.2020", "contents": "Title: Towards the full information chain theory: expected loss and information\n  relevance Abstract: When additional information sources are available, an important question for\nan agent solving a certain problem is how to optimally use the information the\nsources are capable of providing. A framework that relates information accuracy\non the source side to information relevance on the problem side is proposed. An\noptimal information acquisition problem is formulated as that of question\nselection to maximize the loss reduction for the problem solved by the agent. A\nduality relationship between pseudoenergy (accuracy related) quantities on the\nsource side and loss (relevance related) quantities on the problem side is\nobserved. \n\n"}
{"id": "1301.3622", "contents": "Title: On sampling and modeling complex systems Abstract: The study of complex systems is limited by the fact that only few variables\nare accessible for modeling and sampling, which are not necessarily the most\nrelevant ones to explain the systems behavior. In addition, empirical data\ntypically under sample the space of possible states. We study a generic\nframework where a complex system is seen as a system of many interacting\ndegrees of freedom, which are known only in part, that optimize a given\nfunction. We show that the underlying distribution with respect to the known\nvariables has the Boltzmann form, with a temperature that depends on the number\nof unknown variables. In particular, when the unknown part of the objective\nfunction decays faster than exponential, the temperature decreases as the\nnumber of variables increases. We show in the representative case of the\nGaussian distribution, that models are predictable only when the number of\nrelevant variables is less than a critical threshold. As a further consequence,\nwe show that the information that a sample contains on the behavior of the\nsystem is quantified by the entropy of the frequency with which different\nstates occur. This allows us to characterize the properties of maximally\ninformative samples: in the under-sampling regime, the most informative\nfrequency size distributions have power law behavior and Zipf's law emerges at\nthe crossover between the under sampled regime and the regime where the sample\ncontains enough statistics to make inference on the behavior of the system.\nThese ideas are illustrated in some applications, showing that they can be used\nto identify relevant variables or to select most informative representations of\ndata, e.g. in data clustering. \n\n"}
{"id": "1301.4728", "contents": "Title: Relay Augmentation for Lifetime Extension of Wireless Sensor Networks Abstract: We propose a novel relay augmentation strategy for extending the lifetime of\na certain class of wireless sensor networks. In this class sensors are located\nat fixed and pre-determined positions and all communication takes place via\nmulti-hop paths in a fixed routing tree rooted at the base station. It is\nassumed that no accumulation of data takes place along the communication paths\nand that there is no restriction on where additional relays may be located.\nUnder these assumptions the optimal extension of network lifetime is modelled\nas the Euclidean $k$-bottleneck Steiner tree problem. Only two approximation\nalgorithms for this NP-hard problem exist in the literature: a minimum spanning\ntree heuristic (MSTH) with performance ratio 2, and a probabilistic 3-regular\nhypergraph heuristic (3RHH) with performance ratio $\\sqrt{3}+\\epsilon$. We\npresent a new iterative heuristic that incorporates MSTH and show via\nsimulation that our algorithm performs better than MSTH in extending lifetime,\nand outperforms 3RHH in terms of efficiency. \n\n"}
{"id": "1302.0870", "contents": "Title: Centrality-constrained graph embedding Abstract: Visual rendering of graphs is a key task in the mapping of complex network\ndata. Although most graph drawing algorithms emphasize aesthetic appeal,\ncertain applications such as travel-time maps place more importance on\nvisualization of structural network properties. The present paper advocates a\ngraph embedding approach with centrality considerations to comply with node\nhierarchy. The problem is formulated as one of constrained multi-dimensional\nscaling (MDS), and it is solved via block coordinate descent iterations with\nsuccessive approximations and guaranteed convergence to a KKT point. In\naddition, a regularization term enforcing graph smoothness is incorporated with\nthe goal of reducing edge crossings. Experimental results demonstrate that the\nalgorithm converges, and can be used to efficiently embed large graphs on the\norder of thousands of nodes. \n\n"}
{"id": "1303.0542", "contents": "Title: A multidimensional tropical optimization problem with nonlinear\n  objective function and linear constraints Abstract: We examine a multidimensional optimisation problem in the tropical\nmathematics setting. The problem involves the minimisation of a nonlinear\nfunction defined on a finite-dimensional semimodule over an idempotent\nsemifield subject to linear inequality constraints. We start with an overview\nof known tropical optimisation problems with linear and nonlinear objective\nfunctions. A short introduction to tropical algebra is provided to offer a\nformal framework for solving the problem under study. As a preliminary result,\na solution to a linear inequality with an arbitrary matrix is presented. We\ndescribe an example optimisation problem drawn from project scheduling and then\noffer a general representation of the problem. To solve the problem, we\nintroduce an additional variable and reduce the problem to the solving of a\nlinear inequality, in which the variable plays the role of a parameter. A\nnecessary and sufficient condition for the inequality to hold is used to\nevaluate the parameter, whereas the solution to the inequality is considered a\nsolution to the problem. Based on this approach, a complete direct solution in\na compact vector form is derived for the optimisation problem under fairly\ngeneral conditions. Numerical and graphical examples for two-dimensional\nproblems are given to illustrate the obtained results. \n\n"}
{"id": "1303.1707", "contents": "Title: Moment LMI approach to LTV impulsive control Abstract: In the 1960s, a moment approach to linear time varying (LTV) minimal norm\nimpulsive optimal control was developed, as an alternative to direct approaches\n(based on discretization of the equations of motion and linear programming) or\nindirect approaches (based on Pontryagin's maximum principle). This paper\nrevisits these classical results in the light of recent advances in convex\noptimization, in particular the use of measures jointly with hierarchy of\nlinear matrix inequality (LMI) relaxations. Linearity of the dynamics allows us\nto integrate system trajectories and to come up with a simplified LMI hierarchy\nwhere the only unknowns are moments of a vector of control measures of time. In\nparticular, occupation measures of state and control variables do not appear in\nthis formulation. This is in stark contrast with LMI relaxations arising\nusually in polynomial optimal control, where size grows quickly as a function\nof the relaxation order. Jointly with the use of Chebyshev polynomials (as a\nnumerically more stable polynomial basis), this allows LMI relaxations of high\norder (up to a few hundreds) to be solved numerically. \n\n"}
{"id": "1303.2875", "contents": "Title: On the convergence rate improvement of a primal-dual splitting algorithm\n  for solving monotone inclusion problems Abstract: We present two modified versions of the primal-dual splitting algorithm\nrelying on forward-backward splitting proposed in \\cite{vu} for solving\nmonotone inclusion problems. Under strong monotonicity assumptions for some of\nthe operators involved we obtain for the sequences of iterates that approach\nthe solution orders of convergence of O(1/n) and O(\\omega^n), for $\\omega \\in\n(0,1)$, respectively. The investigated primal-dual algorithms are fully\ndecomposable, in the sense that the operators are processed individually at\neach iteration. We also discuss the modified algorithms in the context of\nconvex optimization problems and present numerical experiments in image\nprocessing and support vector machines classification. \n\n"}
{"id": "1303.3049", "contents": "Title: On Optimal Jamming Over an Additive Noise Channel Abstract: This paper considers the problem of optimal zero-delay jamming over an\nadditive noise channel. Early work had already solved this problem for a\nGaussian source and channel. Building on a sequence of recent results on\nconditions for linearity of optimal estimation, and of optimal mappings in\nsource-channel coding, we derive the saddle-point solution to the jamming\nproblem for general sources and channels, without recourse to Gaussian\nassumptions. We show that linearity conditions play a pivotal role in jamming,\nin the sense that the optimal jamming strategy is to effectively force both\ntransmitter and receiver to default to linear mappings, i.e., the jammer\nensures, whenever possible, that the transmitter and receiver cannot benefit\nfrom non-linear strategies. This result is shown to subsume the known result\nfor Gaussian source and channel. We analyze conditions and general settings\nwhere such unbeatable strategy can indeed be achieved by the jammer. Moreover,\nwe provide the procedure to approximate optimal jamming in the remaining\n(source-channel) cases where the jammer cannot impose linearity on the\ntransmitter and the receiver. \n\n"}
{"id": "1303.4882", "contents": "Title: Contextual analysis framework for bursty dynamics Abstract: To understand the origin of bursty dynamics in natural and social processes\nwe provide a general analysis framework, in which the temporal process is\ndecomposed into sub-processes and then the bursts in sub-processes, called\ncontextual bursts, are combined to collective bursts in the original process.\nFor the combination of sub-processes, it is required to consider the\ndistribution of different contexts over the original process. Based on minimal\nassumptions for inter-event time statistics, we present a theoretical analysis\nfor the relationship between contextual and collective inter-event time\ndistributions. Our analysis framework helps to exploit contextual information\navailable in decomposable bursty dynamics. \n\n"}
{"id": "1304.1014", "contents": "Title: A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale\n  SVM Training Abstract: Recently, there has been a renewed interest in the machine learning community\nfor variants of a sparse greedy approximation procedure for concave\noptimization known as {the Frank-Wolfe (FW) method}. In particular, this\nprocedure has been successfully applied to train large-scale instances of\nnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training has\nallowed to obtain efficient algorithms but also important theoretical results,\nincluding convergence analysis of training algorithms and new characterizations\nof model sparsity.\n  In this paper, we present and analyze a novel variant of the FW method based\non a new way to perform away steps, a classic strategy used to accelerate the\nconvergence of the basic FW procedure. Our formulation and analysis is focused\non a general concave maximization problem on the simplex. However, the\nspecialization of our algorithm to quadratic forms is strongly related to some\nclassic methods in computational geometry, namely the Gilbert and MDM\nalgorithms.\n  On the theoretical side, we demonstrate that the method matches the\nguarantees in terms of convergence rate and number of iterations obtained by\nusing classic away steps. In particular, the method enjoys a linear rate of\nconvergence, a result that has been recently proved for MDM on quadratic forms.\n  On the practical side, we provide experiments on several classification\ndatasets, and evaluate the results using statistical tests. Experiments show\nthat our method is faster than the FW method with classic away steps, and works\nwell even in the cases in which classic away steps slow down the algorithm.\nFurthermore, these improvements are obtained without sacrificing the predictive\naccuracy of the obtained SVM model. \n\n"}
{"id": "1304.1768", "contents": "Title: Tidal turbine array optimisation using the adjoint approach Abstract: Oceanic tides have the potential to yield a vast amount of renewable energy.\nTidal stream generators are one of the key technologies for extracting and\nharnessing this potential. In order to extract an economically useful amount of\npower, hundreds of tidal turbines must typically be deployed in an array. This\nnaturally leads to the question of how these turbines should be configured to\nextract the maximum possible power: the positioning and the individual tuning\nof the turbines could significantly influence the extracted power, and hence is\nof major economic interest. However, manual optimisation is difficult due to\nlegal site constraints, nonlinear interactions of the turbine wakes, and the\ncubic dependence of the power on the flow speed. The novel contribution of this\npaper is the formulation of this problem as an optimisation problem constrained\nby a physical model, which is then solved using an efficient gradient-based\noptimisation algorithm. In each optimisation iteration, a two-dimensional\nfinite element shallow water model predicts the flow and the performance of the\ncurrent array configuration. The gradient of the power extracted with respect\nto the turbine positions and their tuning parameters is then computed in a\nfraction of the time taken for a flow solution by solving the associated\nadjoint equations. These equations propagate causality backwards through the\ncomputation, from the power extracted back to the turbine positions and the\ntuning parameters. This yields the gradient at a cost almost independent of the\nnumber of turbines, which is crucial for any practical application. The utility\nof the approach is demonstrated by optimising turbine arrays in four idealised\nscenarios and a more realistic case with up to 256 turbines in the Inner Sound\nof the Pentland Firth, Scotland. \n\n"}
{"id": "1304.2581", "contents": "Title: Stability and performance of stochastic predictive control Abstract: This article is concerned with stability and performance of controlled\nstochastic processes under receding horizon policies. We carry out a systematic\nstudy of methods to guarantee stability under receding horizon policies via\nappropriate selections of cost functions in the underlying finite-horizon\noptimal control problem. We also obtain quantitative bounds on the performance\nof the system under receding horizon policies as measured by the long-run\nexpected average cost. The results are illustrated with the help of several\nsimple examples. \n\n"}
{"id": "1304.5286", "contents": "Title: Elastic demand dynamic network user equilibrium: Formulation, existence\n  and computation Abstract: This paper is concerned with dynamic user equilibrium with elastic travel\ndemand (E-DUE) when the trip demand matrix is determined endogenously. We\npresent an infinite-dimensional variational inequality (VI) formulation that is\nequivalent to the conditions defining a continuous-time E-DUE problem. An\nexistence result for this VI is established by applying a fixed-point existence\ntheorem (Browder, 1968) in an extended Hilbert space. We present three\nalgorithms based on the aforementioned VI and its re-expression as a\ndifferential variational inequality (DVI): a projection method, a self-adaptive\nprojection method, and a proximal point method. Rigorous convergence results\nare provided for these methods, which rely on increasingly relaxed notions of\ngeneralized monotonicity, namely mixed strongly-weakly monotonicity for the\nprojection method; pseudomonotonicity for the self-adaptive projection method,\nand quasimonotonicity for the proximal point method. These three algorithms are\ntested and their solution quality, convergence, and computational efficiency\ncompared. Our convergence results, which transcend the transportation\napplications studied here, apply to a broad family of infinite-dimensional VIs\nand DVIs, and are the weakest reported to date. \n\n"}
{"id": "1304.6800", "contents": "Title: Approximation Hardness of Graphic TSP on Cubic Graphs Abstract: We prove explicit approximation hardness results for the Graphic TSP on cubic\nand subcubic graphs as well as the new inapproximability bounds for the\ncorresponding instances of the (1,2)-TSP. The proof technique uses new modular\nconstructions of simulating gadgets for the restricted cubic and subcubic\ninstances. The modular constructions used in the paper could be also of\nindependent interest. \n\n"}
{"id": "1304.6962", "contents": "Title: Variable projection methods for approximate (greatest) common divisor\n  computations Abstract: We consider the problem of finding for a given $N$-tuple of polynomials (real\nor complex) the closest $N$-tuple that has a common divisor of degree at least\n$d$. Extended weighted Euclidean seminorm of the coefficients is used as a\nmeasure of closeness. Two equivalent representations of the problem are\nconsidered: (i) direct parameterization over the common divisors and quotients\n(image representation), and (ii) Sylvester low-rank approximation (kernel\nrepresentation). We use the duality between least-squares and least-norm\nproblems to show that (i) and (ii) are closely related to mosaic Hankel\nlow-rank approximation. This allows us to apply to the approximate common\ndivisor problem recent results on complexity and accuracy of computations for\nmosaic Hankel low-rank approximation. We develop optimization methods based on\nthe variable projection principle both for image and kernel representation.\nThese methods have linear complexity in the degrees of the polynomials for\nsmall and large $d$. We provide a software implementation of the developed\nmethods, which is based on a software package for structured low-rank\napproximation. \n\n"}
{"id": "1305.0205", "contents": "Title: The effect of the initial network configuration on preferential\n  attachment Abstract: The classical preferential attachment model is sensitive to the choice of the\ninitial configuration of the network. As the number of initial nodes and their\ndegree grow, so does the time needed for an equilibrium degree distribution to\nbe established. We study this phenomenon, provide estimates of the\nequilibration time, and characterize the degree distribution cutoff observed at\nfinite times. When the initial network is dense and exceeds a certain small\nsize, there is no equilibration and a suitable statistical test can always\ndiscern the produced degree distribution from the equilibrium one. As a\nby-product, the weighted Kolmogorov-Smirnov statistic is demonstrated to be\nmore suitable for statistical analysis of power-law distributions with cutoff\nwhen the data is ample. \n\n"}
{"id": "1305.3039", "contents": "Title: Multicanonical MCMC for Sampling Rare Events Abstract: Multicanonical MCMC (Multicanonical Markov Chain Monte Carlo; Multicanonical\nMonte Carlo) is discussed as a method of rare event sampling. Starting from a\nreview of the generic framework of importance sampling, multicanonical MCMC is\nintroduced, followed by applications in random matrices, random graphs, and\nchaotic dynamical systems. Replica exchange MCMC (also known as parallel\ntempering or Metropolis-coupled MCMC) is also explained as an alternative to\nmulticanonical MCMC. In the last section, multicanonical MCMC is applied to\ndata surrogation; a successful implementation in surrogating time series is\nshown. In the appendices, calculation of averages and normalizing constant in\nan exponential family, phase coexistence, simulated tempering, parallelization,\nand multivariate extensions are discussed. \n\n"}
{"id": "1305.4147", "contents": "Title: Dagstuhl Report 13082: Communication Complexity, Linear Optimization,\n  and lower bounds for the nonnegative rank of matrices Abstract: This report documents the program and the outcomes of Dagstuhl Seminar 13082\n\"Communication Complexity, Linear Optimization, and lower bounds for the\nnonnegative rank of matrices\", held in February 2013 at Dagstuhl Castle. \n\n"}
{"id": "1305.5493", "contents": "Title: Information Criteria for Deciding between Normal Regression Models Abstract: Regression models fitted to data can be assessed on their goodness of fit,\nthough models with many parameters should be disfavored to prevent\nover-fitting. Statisticians' tools for this are little known to physical\nscientists. These include the Akaike Information Criterion (AIC), a penalized\ngoodness-of-fit statistic, and the AICc, a variant including a small-sample\ncorrection. They entered the physical sciences through being used by\nastrophysicists to compare cosmological models; e.g., predictions of the\ndistance-redshift relation. The AICc is shown to have been misapplied, being\napplicable only if error variances are unknown. If error bars accompany the\ndata, the AIC should be used instead. Erroneous applications of the AICc are\nlisted in an appendix. It is also shown how the variability of the AIC\ndifference between models with a known error variance can be estimated. This\nyields a significance test that can potentially replace the use of `Akaike\nweights' for deciding between such models. Additionally, the effects of model\nmisspecification are examined. For regression models fitted to data sets\nwithout (rather than with) error bars, they are major: the AICc may be shifted\nby an unknown amount. The extent of this in the fitting of physical models\nremains to be studied. \n\n"}
{"id": "1305.5828", "contents": "Title: An Algorithm for Splitting Parallel Sums of Linearly Composed Monotone\n  Operators, with Applications to Signal Recovery Abstract: We present a new primal-dual splitting algorithm for structured monotone\ninclusions in Hilbert spaces and analyze its asymptotic behavior. A novelty of\nour framework, which is motivated by image recovery applications, is to\nconsider inclusions that combine a variety of monotonicity-preserving\noperations such as sums, linear compositions, parallel sums, and a new notion\nof parallel composition. The special case of minimization problems is studied\nin detail, and applications to signal recovery are discussed. Numerical\nsimulations are provided to illustrate the implementation of the algorithm. \n\n"}
{"id": "1306.0386", "contents": "Title: Improved and Generalized Upper Bounds on the Complexity of Policy\n  Iteration Abstract: Given a Markov Decision Process (MDP) with $n$ states and a totalnumber $m$\nof actions, we study the number of iterations needed byPolicy Iteration (PI)\nalgorithms to converge to the optimal$\\gamma$-discounted policy. We consider\ntwo variations of PI: Howard'sPI that changes the actions in all states with a\npositive advantage,and Simplex-PI that only changes the action in the state\nwith maximaladvantage. We show that Howard's PI terminates after at most\n$O\\left(\\frac{m}{1-\\gamma}\\log\\left(\\frac{1}{1-\\gamma}\\right)\\right)$iterations,\nimproving by a factor $O(\\log n)$ a result by Hansen etal., while Simplex-PI\nterminates after at most\n$O\\left(\\frac{nm}{1-\\gamma}\\log\\left(\\frac{1}{1-\\gamma}\\right)\\right)$iterations,\nimproving by a factor $O(\\log n)$ a result by Ye. Undersome structural\nproperties of the MDP, we then consider bounds thatare independent of the\ndiscount factor~$\\gamma$: quantities ofinterest are bounds $\\tau\\_t$ and\n$\\tau\\_r$---uniform on all states andpolicies---respectively on the\n\\emph{expected time spent in transientstates} and \\emph{the inverse of the\nfrequency of visits in recurrentstates} given that the process starts from the\nuniform distribution.Indeed, we show that Simplex-PI terminates after at most\n$\\tilde O\\left(n^3 m^2 \\tau\\_t \\tau\\_r \\right)$ iterations. This extends\narecent result for deterministic MDPs by Post & Ye, in which $\\tau\\_t\\le 1$ and\n$\\tau\\_r \\le n$, in particular it shows that Simplex-PI isstrongly polynomial\nfor a much larger class of MDPs. We explain whysimilar results seem hard to\nderive for Howard's PI. Finally, underthe additional (restrictive) assumption\nthat the state space ispartitioned in two sets, respectively states that are\ntransient andrecurrent for all policies, we show that both Howard's PI\nandSimplex-PI terminate after at most $\\tilde\nO(m(n^2\\tau\\_t+n\\tau\\_r))$iterations. \n\n"}
{"id": "1306.3184", "contents": "Title: Scalar and vector Slepian functions, spherical signal estimation and\n  spectral analysis Abstract: It is a well-known fact that mathematical functions that are timelimited (or\nspacelimited) cannot be simultaneously bandlimited (in frequency). Yet the\nfinite precision of measurement and computation unavoidably bandlimits our\nobservation and modeling scientific data, and we often only have access to, or\nare only interested in, a study area that is temporally or spatially bounded.\nIn the geosciences we may be interested in spectrally modeling a time series\ndefined only on a certain interval, or we may want to characterize a specific\ngeographical area observed using an effectively bandlimited measurement device.\nIt is clear that analyzing and representing scientific data of this kind will\nbe facilitated if a basis of functions can be found that are \"spatiospectrally\"\nconcentrated, i.e. \"localized\" in both domains at the same time. Here, we give\na theoretical overview of one particular approach to this \"concentration\"\nproblem, as originally proposed for time series by Slepian and coworkers, in\nthe 1960s. We show how this framework leads to practical algorithms and\nstatistically performant methods for the analysis of signals and their power\nspectra in one and two dimensions, and, particularly for applications in the\ngeosciences, for scalar and vectorial signals defined on the surface of a unit\nsphere. \n\n"}
{"id": "1306.3409", "contents": "Title: Constrained fractional set programs and their application in local\n  clustering and community detection Abstract: The (constrained) minimization of a ratio of set functions is a problem\nfrequently occurring in clustering and community detection. As these\noptimization problems are typically NP-hard, one uses convex or spectral\nrelaxations in practice. While these relaxations can be solved globally\noptimally, they are often too loose and thus lead to results far away from the\noptimum. In this paper we show that every constrained minimization problem of a\nratio of non-negative set functions allows a tight relaxation into an\nunconstrained continuous optimization problem. This result leads to a flexible\nframework for solving constrained problems in network analysis. While a\nglobally optimal solution for the resulting non-convex problem cannot be\nguaranteed, we outperform the loose convex or spectral relaxations by a large\nmargin on constrained local clustering problems. \n\n"}
{"id": "1306.4064", "contents": "Title: A surrogate for networks -- How scale-free is my scale-free network? Abstract: Complex networks are now being studied in a wide range of disciplines across\nscience and technology. In this paper we propose a method by which one can\nprobe the properties of experimentally obtained network data. Rather than just\nmeasuring properties of a network inferred from data, we aim to ask how typical\nis that network? What properties of the observed network are typical of all\nsuch scale free networks, and which are peculiar? To do this we propose a\nseries of methods that can be used to generate statistically likely complex\nnetworks which are both similar to the observed data and also consistent with\nan underlying null-hypothesis -- for example a particular degree distribution.\nThere is a direct analogy between the approach we propose here and the\nsurrogate data methods applied to nonlinear time series data. \n\n"}
{"id": "1306.5318", "contents": "Title: Curvature: a variational approach Abstract: The curvature discussed in this paper is a rather far going generalization of\nthe Riemannian sectional curvature. We define it for a wide class of optimal\ncontrol problems: a unified framework including geometric structures such as\nRiemannian, sub-Riemannian, Finsler and sub-Finsler structures; a special\nattention is paid to the sub-Riemannian (or Carnot-Caratheodory) metric spaces.\nOur construction of the curvature is direct and naive, and it is similar to the\noriginal approach of Riemann. Surprisingly, it works in a very general setting\nand, in particular, for all sub-Riemannian spaces. \n\n"}
{"id": "1306.5374", "contents": "Title: Low-Rank Separated Representation Surrogates of High-Dimensional\n  Stochastic Functions: Application in Bayesian Inference Abstract: This study introduces a non-intrusive approach in the context of low-rank\nseparated representation to construct a surrogate of high-dimensional\nstochastic functions, e.g., PDEs/ODEs, in order to decrease the computational\ncost of Markov Chain Monte Carlo simulations in Bayesian inference. The\nsurrogate model is constructed via a regularized alternative least-square\nregression with Tikhonov regularization using a roughening matrix computing the\ngradient of the solution, in conjunction with a perturbation-based error\nindicator to detect optimal model complexities. The model approximates a vector\nof a continuous solution at discrete values of a physical variable. The\nrequired number of random realizations to achieve a successful approximation\nlinearly depends on the function dimensionality. The computational cost of the\nmodel construction is quadratic in the number of random inputs, which\npotentially tackles the curse of dimensionality in high-dimensional stochastic\nfunctions. Furthermore, this vector valued separated representation-based\nmodel, in comparison to the available scalar-valued case, leads to a\nsignificant reduction in the cost of approximation by an order of magnitude\nequal to the vector size. The performance of the method is studied through its\napplication to three numerical examples including a 41-dimensional elliptic PDE\nand a 21-dimensional cavity flow. \n\n"}
{"id": "1307.0141", "contents": "Title: Convergence of the shooting algorithm for singular optimal control\n  problems Abstract: In this article we propose a shooting algorithm for optimal control problems\ngoverned by systems that are affine in one part of the control variable.\nFinitely many equality constraints on the initial and final state are\nconsidered. We recall a second order sufficient condition for weak optimality,\nand show that it guarantees the local quadratic convergence of the algorithm.\nWe show an example and solve it numerically. \n\n"}
{"id": "1307.0445", "contents": "Title: Networked Estimation using Sparsifying Basis Prediction Abstract: We present a framework for networked state estimation, where systems encode\ntheir (possibly high dimensional) state vectors using a mutually agreed basis\nbetween the system and the estimator (in a remote monitoring unit). The basis\nsparsifies the state vectors, i.e., it represents them using vectors with few\nnon-zero components, and as a result, the systems might need to transmit only a\nfraction of the original information to be able to recover the non-zero\ncomponents of the transformed state vector. Hence, the estimator can recover\nthe state vector of the system from an under-determined linear set of\nequations. We use a greedy search algorithm to calculate the sparsifying basis.\nThen, we present an upper bound for the estimation error. Finally, we\ndemonstrate the results on a numerical example. \n\n"}
{"id": "1307.0583", "contents": "Title: Vortex penetration field of the multilayer coating model Abstract: The vortex penetration field of the multilayer coating model with a single\nsuperconductor layer and a single insulator layer formed on a bulk\nsuperconductor are derived. The same formula can be applied to a model with a\nsuperconductor layer formed on a bulk superconductor without an insulator\nlayer. \n\n"}
{"id": "1307.2014", "contents": "Title: On the multifractal effects generated by monofractal signals Abstract: We study quantitatively the level of false multifractal signal one may\nencounter while analyzing multifractal phenomena in time series within\nmultifractal detrended fluctuation analysis (MF-DFA). The investigated effect\nappears as a result of finite length of used data series and is additionally\namplified by the long-term memory the data eventually may contain. We provide\nthe detailed quantitative description of such apparent multifractal background\nsignal as a threshold in spread of generalized Hurst exponent values $\\Delta h$\nor a threshold in the width of multifractal spectrum $\\Delta \\alpha$ below\nwhich multifractal properties of the system are only apparent, i.e. do not\nexist, despite $\\Delta\\alpha\\neq0$ or $\\Delta h\\neq 0$. We find this effect\nquite important for shorter or persistent series and we argue it is linear with\nrespect to autocorrelation exponent $\\gamma$. Its strength decays according to\npower law with respect to the length of time series. The influence of basic\nlinear and nonlinear transformations applied to initial data in finite time\nseries with various level of long memory is also investigated. This provides\nadditional set of semi-analytical results. The obtained formulas are\nsignificant in any interdisciplinary application of multifractality, including\nphysics, financial data analysis or physiology, because they allow to separate\nthe 'true' multifractal phenomena from the apparent (artificial) multifractal\neffects. They should be a helpful tool of the first choice to decide whether we\ndo in particular case with the signal with real multiscaling properties or not. \n\n"}
{"id": "1307.3653", "contents": "Title: Effects of maximal fluctuation moment $q$ and detrending polynomial\n  orders on the observed multifractal features within MFDFA Abstract: We focus on the importance of $q$ moments range used within multifractal\ndetrended fluctuation analysis (MFDFA) to calculate the generalized Hurst\nexponent spread and multifractal properties of signals. Different orders of\ndetrending polynomials are also discussed. In particular, we analyze\nquantitatively the corrections to the spread of generalized Hurst exponent\nprofile $\\Delta h$ allowing to extend the previously found by us formulas for\nlarge $q$, describing the level of artificial multiscaling in finite signals,\nto arbitrary narrower range of $q$ moments used in MFDFA technique in distinct\napplications. \n\n"}
{"id": "1307.4003", "contents": "Title: Information and treatment of unknown correlations in the combination of\n  measurements using the BLUE method Abstract: We discuss the effect of large positive correlations in the combinations of\nseveral measurements of a single physical quantity using the Best Linear\nUnbiased Estimate (BLUE) method. We suggest a new approach for comparing the\nrelative weights of the different measurements in their contributions to the\ncombined knowledge about the unknown parameter, using the well-established\nconcept of Fisher information. We argue, in particular, that one We discuss the\neffect of large positive correlations in the combinations of several\nmeasurements of a single physical quantity using the Best Linear Unbiased\nEstimate (BLUE) method. We suggest a new approach for comparing the relative\nweights of the different measurements in their contributions to the combined\nknowledge about the unknown parameter, using the well-established concept of\nFisher information. We argue, in particular, that one contribution to\ninformation comes from the collective interplay of the measurements through\ntheir correlations and that this contribution cannot be attributed to any of\nthe individual measurements alone. We show that negative coefficients in the\nBLUE weighted average invariably indicate the presence of a regime of high\ncorrelations, where the effect of further increasing some of these correlations\nis that of reducing the error on the combined estimate. In these regimes, we\nstress that assuming fully correlated systematic uncertainties is not a truly\nconservative choice, and that the correlations provided as input to BLUE\ncombinations need to be assessed with extreme care instead. In situations where\nthe precise evaluation of these correlations is impractical, or even\nimpossible, we provide tools to help experimental physicists perform more\nconservative combinations. \n\n"}
{"id": "1307.5001", "contents": "Title: On Lower Complexity Bounds for Large-Scale Smooth Convex Optimization Abstract: We derive lower bounds on the black-box oracle complexity of large-scale\nsmooth convex minimization problems, with emphasis on minimizing smooth (with\nHolder continuous, with a given exponent and constant, gradient) convex\nfunctions over high-dimensional ||.||_p-balls, 1<=p<=\\infty. Our bounds turn\nout to be tight (up to logarithmic in the design dimension factors), and can be\nviewed as a substantial extension of the existing lower complexity bounds for\nlarge-scale convex minimization covering the nonsmooth case and the 'Euclidean'\nsmooth case (minimization of convex functions with Lipschitz continuous\ngradients over Euclidean balls). As a byproduct of our results, we demonstrate\nthat the classical Conditional Gradient algorithm is near-optimal, in the sense\nof Information-Based Complexity Theory, when minimizing smooth convex functions\nover high-dimensional ||.||_\\infty-balls and their matrix analogies -- spectral\nnorm balls in the spaces of square matrices. \n\n"}
{"id": "1307.5235", "contents": "Title: Extremal polynomials in stratified groups Abstract: We introduce a family of extremal polynomials associated with the\nprolongation of a stratified nilpotent Lie algebra. These polynomials are\nrelated to a new algebraic characterization of abnormal subriemannian geodesics\nin stratified nilpotent Lie groups. They satisfy a set of remarkable structure\nrelations that are used to integrate the adjoint equations. \n\n"}
{"id": "1307.6864", "contents": "Title: Convex recovery from interferometric measurements Abstract: This note formulates a deterministic recovery result for vectors $x$ from\nquadratic measurements of the form $(Ax)_i \\overline{(Ax)_j}$ for some\nleft-invertible $A$. Recovery is exact, or stable in the noisy case, when the\ncouples $(i,j)$ are chosen as edges of a well-connected graph. One possible way\nof obtaining the solution is as a feasible point of a simple semidefinite\nprogram. Furthermore, we show how the proportionality constant in the error\nestimate depends on the spectral gap of a data-weighted graph Laplacian. Such\nquadratic measurements have found applications in phase retrieval, angular\nsynchronization, and more recently interferometric waveform inversion. \n\n"}
{"id": "1308.0239", "contents": "Title: Rapid rise and decay in petition signing Abstract: Contemporary collective action, much of which involves social media and other\nInternet-based platforms, leaves a digital imprint which may be harvested to\nbetter understand the dynamics of mobilization. Petition signing is an example\nof collective action which has gained in popularity with rising use of social\nmedia and provides such data for the whole population of petition signatories\nfor a given platform. This paper tracks the growth curves of all 20,000\npetitions to the UK government petitions website\n(http://epetitions.direct.gov.uk) and 1,800 petitions to the US White House\nsite (https://petitions.whitehouse.gov), analyzing the rate of growth and\noutreach mechanism. Previous research has suggested the importance of the first\nday to the ultimate success of a petition, but has not examined early growth\nwithin that day, made possible here through hourly resolution in the data. The\nanalysis shows that the vast majority of petitions do not achieve any measure\nof success; over 99 percent fail to get the 10,000 signatures required for an\nofficial response and only 0.1 percent attain the 100,000 required for a\nparliamentary debate (0.7 percent in the US). We analyze the data through a\nmultiplicative process model framework to explain the heterogeneous growth of\nsignatures at the population level. We define and measure an average outreach\nfactor for petitions and show that it decays very fast (reducing to 0.1 pervent\nafter 10 hours in the UK and 30 hours in the US). After a day or two, a\npetition's fate is virtually set. The findings challenge conventional analyses\nof collective action from economics and political science, where the production\nfunction has been assumed to follow an S-shaped curve. \n\n"}
{"id": "1308.4585", "contents": "Title: Fractional Calculus of Variations of Several Independent Variables Abstract: We prove multidimensional integration by parts formulas for generalized\nfractional derivatives and integrals. The new results allow us to obtain\noptimality conditions for multidimensional fractional variational problems with\nLagrangians depending on generalized partial integrals and derivatives. A\ngeneralized fractional Noether's theorem, a formulation of Dirichlet's\nprinciple and an uniqueness result are given. \n\n"}
{"id": "1308.5000", "contents": "Title: Smoothing and Decomposition for Analysis Sparse Recovery Abstract: We consider algorithms and recovery guarantees for the analysis sparse model\nin which the signal is sparse with respect to a highly coherent frame. We\nconsider the use of a monotone version of the fast iterative shrinkage-\nthresholding algorithm (MFISTA) to solve the analysis sparse recovery problem.\nSince the proximal operator in MFISTA does not have a closed-form solution for\nthe analysis model, it cannot be applied directly. Instead, we examine two\nalternatives based on smoothing and decomposition transformations that relax\nthe original sparse recovery problem, and then implement MFISTA on the relaxed\nformulation. We refer to these two methods as smoothing-based and\ndecomposition-based MFISTA. We analyze the convergence of both algorithms, and\nestablish that smoothing- based MFISTA converges more rapidly when applied to\ngeneral nonsmooth optimization problems. We then derive a performance bound on\nthe reconstruction error using these techniques. The bound proves that our\nmethods can recover a signal sparse in a redundant tight frame when the\nmeasurement matrix satisfies a properly adapted restricted isometry property.\nNumerical examples demonstrate the performance of our methods and show that\nsmoothing-based MFISTA converges faster than the decomposition-based\nalternative in real applications, such as MRI image reconstruction. \n\n"}
{"id": "1308.6774", "contents": "Title: Separable Approximations and Decomposition Methods for the Augmented\n  Lagrangian Abstract: In this paper we study decomposition methods based on separable\napproximations for minimizing the augmented Lagrangian. In particular, we study\nand compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey and\nRuszczy\\'{n}ski and the Parallel Coordinate Descent Method (PCDM) of\nRicht\\'arik and Tak\\'a\\v{c}. We show that the two methods are equivalent for\nfeasibility problems up to the selection of a single step-size parameter.\nFurthermore, we prove an improved complexity bound for PCDM under strong\nconvexity, and show that this bound is at least $8(L'/\\bar{L})(\\omega-1)^2$\ntimes better than the best known bound for DQAM, where $\\omega$ is the degree\nof partial separability and $L'$ and $\\bar{L}$ are the maximum and average of\nthe block Lipschitz constants of the gradient of the quadratic penalty\nappearing in the augmented Lagrangian. \n\n"}
{"id": "1308.6833", "contents": "Title: Stability of Polynomial Differential Equations: Complexity and Converse\n  Lyapunov Questions Abstract: We consider polynomial differential equations and make a number of\ncontributions to the questions of (i) complexity of deciding stability, (ii)\nexistence of polynomial Lyapunov functions, and (iii) existence of sum of\nsquares (sos) Lyapunov functions.\n  (i) We show that deciding local or global asymptotic stability of cubic\nvector fields is strongly NP-hard. Simple variations of our proof are shown to\nimply strong NP-hardness of several other decision problems: testing local\nattractivity of an equilibrium point, stability of an equilibrium point in the\nsense of Lyapunov, invariance of the unit ball, boundedness of trajectories,\nconvergence of all trajectories in a ball to a given equilibrium point,\nexistence of a quadratic Lyapunov function, local collision avoidance, and\nexistence of a stabilizing control law.\n  (ii) We present a simple, explicit example of a globally asymptotically\nstable quadratic vector field on the plane which does not admit a polynomial\nLyapunov function (joint work with M. Krstic). For the subclass of homogeneous\nvector fields, we conjecture that asymptotic stability implies existence of a\npolynomial Lyapunov function, but show that the minimum degree of such a\nLyapunov function can be arbitrarily large even for vector fields in fixed\ndimension and degree. For the same class of vector fields, we further establish\nthat there is no monotonicity in the degree of polynomial Lyapunov functions.\n  (iii) We show via an explicit counterexample that if the degree of the\npolynomial Lyapunov function is fixed, then sos programming may fail to find a\nvalid Lyapunov function even though one exists. On the other hand, if the\ndegree is allowed to increase, we prove that existence of a polynomial Lyapunov\nfunction for a planar or a homogeneous vector field implies existence of a\npolynomial Lyapunov function that is sos and that the negative of its\nderivative is also sos. \n\n"}
{"id": "1309.0474", "contents": "Title: Smooth solutions to portfolio liquidation problems under price-sensitive\n  market impact Abstract: We consider the stochastic control problem of a financial trader that needs\nto unwind a large asset portfolio within a short period of time. The trader can\nsimultaneously submit active orders to a primary market and passive orders to a\ndark pool. Our framework is flexible enough to allow for price-dependent impact\nfunctions describing the trading costs in the primary market and\nprice-dependent adverse selection costs associated with dark pool trading. We\nprove that the value function can be characterized in terms of the unique\nsmooth solution to a PDE with singular terminal value, establish its explicit\nasymptotic behavior at the terminal time, and give the optimal trading strategy\nin feedback form. \n\n"}
{"id": "1309.0531", "contents": "Title: Analysis of multichannel measurements of rare processes with uncertain\n  expected background and acceptance Abstract: A typical experiment in high energy physics is considered. The result of the\nexperiment is assumed to be a histogram consisting of bins or channels with\nnumbers of corresponding registered events. The expected background and\nexpected signal shape or acceptance are measured in separate auxiliary\nexperiments, or calculated by the Monte Carlo method with finite sample size,\nand hence with finite precision. An especially complex situation occurs when\nthe expected background in some of the channels happens to be zero due to\neither a fluctuation of the auxiliary measurement (or simulation) or because it\nis truly zero. Different statistical methods give different confidence\nintervals for the full signal rate and different significances of the\nsignal+background hypothesis versus the pure background hypothesis. Detailed\nanalysis and numerical tests are presented. \n\n"}
{"id": "1309.0790", "contents": "Title: SKYNET: an efficient and robust neural network training tool for machine\n  learning in astronomy Abstract: We present the first public release of our generic neural network training\nalgorithm, called SkyNet. This efficient and robust machine learning tool is\nable to train large and deep feed-forward neural networks, including\nautoencoders, for use in a wide range of supervised and unsupervised learning\napplications, such as regression, classification, density estimation,\nclustering and dimensionality reduction. SkyNet uses a `pre-training' method to\nobtain a set of network parameters that has empirically been shown to be close\nto a good solution, followed by further optimisation using a regularised\nvariant of Newton's method, where the level of regularisation is determined and\nadjusted automatically; the latter uses second-order derivative information to\nimprove convergence, but without the need to evaluate or store the full Hessian\nmatrix, by using a fast approximate method to calculate Hessian-vector\nproducts. This combination of methods allows for the training of complicated\nnetworks that are difficult to optimise using standard backpropagation\ntechniques. SkyNet employs convergence criteria that naturally prevent\noverfitting, and also includes a fast algorithm for estimating the accuracy of\nnetwork outputs. The utility and flexibility of SkyNet are demonstrated by\napplication to a number of toy problems, and to astronomical problems focusing\non the recovery of structure from blurred and noisy images, the identification\nof gamma-ray bursters, and the compression and denoising of galaxy images. The\nSkyNet software, which is implemented in standard ANSI C and fully parallelised\nusing MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/. \n\n"}
{"id": "1309.4039", "contents": "Title: Evolving networks in the human epileptic brain Abstract: Network theory provides novel concepts that promise an improved\ncharacterization of interacting dynamical systems. Within this framework,\nevolving networks can be considered as being composed of nodes, representing\nsystems, and of time-varying edges, representing interactions between these\nsystems. This approach is highly attractive to further our understanding of the\nphysiological and pathophysiological dynamics in human brain networks. Indeed,\nthere is growing evidence that the epileptic process can be regarded as a\nlarge-scale network phenomenon. We here review methodologies for inferring\nnetworks from empirical time series and for a characterization of these\nevolving networks. We summarize recent findings derived from studies that\ninvestigate human epileptic brain networks evolving on timescales ranging from\nfew seconds to weeks. We point to possible pitfalls and open issues, and\ndiscuss future perspectives. \n\n"}
{"id": "1309.5073", "contents": "Title: Non-linear dependences in finance Abstract: The thesis is composed of three parts. Part I introduces the mathematical and\nstatistical tools that are relevant for the study of dependences, as well as\nstatistical tests of Goodness-of-fit for empirical probability distributions. I\npropose two extensions of usual tests when dependence is present in the sample\ndata and when observations have a fat-tailed distribution. The financial\ncontent of the thesis starts in Part II. I present there my studies regarding\nthe \"cross-sectional\" dependences among the time series of daily stock returns,\ni.e. the instantaneous forces that link several stocks together and make them\nbehave somewhat collectively rather than purely independently. A calibration of\na new factor model is presented here, together with a comparison to\nmeasurements on real data. Finally, Part III investigates the temporal\ndependences of single time series, using the same tools and measures of\ncorrelation. I propose two contributions to the study of the origin and\ndescription of \"volatility clustering\": one is a generalization of the\nARCH-like feedback construction where the returns are self-exciting, and the\nother one is a more original description of self-dependences in terms of\ncopulas. The latter can be formulated model-free and is not specific to\nfinancial time series. In fact, I also show here how concepts like recurrences,\nrecords, aftershocks and waiting times, that characterize the dynamics in a\ntime series can be written in the unifying framework of the copula. \n\n"}
{"id": "1309.7367", "contents": "Title: Stochastic Online Shortest Path Routing: The Value of Feedback Abstract: This paper studies online shortest path routing over multi-hop networks. Link\ncosts or delays are time-varying and modeled by independent and identically\ndistributed random processes, whose parameters are initially unknown. The\nparameters, and hence the optimal path, can only be estimated by routing\npackets through the network and observing the realized delays. Our aim is to\nfind a routing policy that minimizes the regret (the cumulative difference of\nexpected delay) between the path chosen by the policy and the unknown optimal\npath. We formulate the problem as a combinatorial bandit optimization problem\nand consider several scenarios that differ in where routing decisions are made\nand in the information available when making the decisions. For each scenario,\nwe derive a tight asymptotic lower bound on the regret that has to be satisfied\nby any online routing policy. These bounds help us to understand the\nperformance improvements we can expect when (i) taking routing decisions at\neach hop rather than at the source only, and (ii) observing per-link delays\nrather than end-to-end path delays. In particular, we show that (i) is of no\nuse while (ii) can have a spectacular impact. Three algorithms, with a\ntrade-off between computational complexity and performance, are proposed. The\nregret upper bounds of these algorithms improve over those of the existing\nalgorithms, and they significantly outperform state-of-the-art algorithms in\nnumerical experiments. \n\n"}
{"id": "1310.0432", "contents": "Title: Online Learning of Dynamic Parameters in Social Networks Abstract: This paper addresses the problem of online learning in a dynamic setting. We\nconsider a social network in which each individual observes a private signal\nabout the underlying state of the world and communicates with her neighbors at\neach time period. Unlike many existing approaches, the underlying state is\ndynamic, and evolves according to a geometric random walk. We view the scenario\nas an optimization problem where agents aim to learn the true state while\nsuffering the smallest possible loss. Based on the decomposition of the global\nloss function, we introduce two update mechanisms, each of which generates an\nestimate of the true state. We establish a tight bound on the rate of change of\nthe underlying state, under which individuals can track the parameter with a\nbounded variance. Then, we characterize explicit expressions for the steady\nstate mean-square deviation(MSD) of the estimates from the truth, per\nindividual. We observe that only one of the estimators recovers the optimal\nMSD, which underscores the impact of the objective function decomposition on\nthe learning quality. Finally, we provide an upper bound on the regret of the\nproposed methods, measured as an average of errors in estimating the parameter\nin a finite time. \n\n"}
{"id": "1310.2446", "contents": "Title: A statistical physics perspective on criticality in financial markets Abstract: Stock markets are complex systems exhibiting collective phenomena and\nparticular features such as synchronization, fluctuations distributed as\npower-laws, non-random structures and similarity to neural networks. Such\nspecific properties suggest that markets operate at a very special point.\nFinancial markets are believed to be critical by analogy to physical systems\nbut few statistically founded evidence have been given. Through a data-based\nmethodology and comparison to simulations inspired by statistical physics of\ncomplex systems, we show that the Dow Jones and indices sets are not rigorously\ncritical. However, financial systems are closer to the criticality in the crash\nneighborhood. \n\n"}
{"id": "1310.3791", "contents": "Title: The Jeffreys-Lindley Paradox and Discovery Criteria in High Energy\n  Physics Abstract: The Jeffreys-Lindley paradox displays how the use of a p-value (or number of\nstandard deviations z) in a frequentist hypothesis test can lead to an\ninference that is radically different from that of a Bayesian hypothesis test\nin the form advocated by Harold Jeffreys in the 1930s and common today. The\nsetting is the test of a well-specified null hypothesis (such as the Standard\nModel of elementary particle physics, possibly with \"nuisance parameters\")\nversus a composite alternative (such as the Standard Model plus a new force of\nnature of unknown strength). The p-value, as well as the ratio of the\nlikelihood under the null hypothesis to the maximized likelihood under the\nalternative, can strongly disfavor the null hypothesis, while the Bayesian\nposterior probability for the null hypothesis can be arbitrarily large. The\nacademic statistics literature contains many impassioned comments on this\nparadox, yet there is no consensus either on its relevance to scientific\ncommunication or on its correct resolution. The paradox is quite relevant to\nfrontier research in high energy physics. This paper is an attempt to explain\nthe situation to both physicists and statisticians, in the hope that further\nprogress can be made. \n\n"}
{"id": "1310.4479", "contents": "Title: Response to comment on theoretical RF field limits of multilayer coating\n  structures of superconducting resonator cavities Abstract: A comment to the authors' SRF Conference pre-print [1] was submitted by A.\nGurevich to the arXiv [2]. In this response, we show that the arguments used in\nthe comment are not valid.\n  [1] arXiv:1309.3239 [2] arXiv:1309.5626 \n\n"}
{"id": "1310.7215", "contents": "Title: Linear and synchrosqueezed time-frequency representations revisited.\n  Part I: Overview, standards of use, related issues and algorithms Abstract: Time-frequency representations (TFRs) of signals, such as the windowed\nFourier transform (WFT), wavelet transform (WT) and their synchrosqueezed\nvariants (SWFT, SWT), provide powerful analysis tools. However, there are many\nimportant issues related to the practical use of TFRs that need to be\nclarified. Here we present a thorough review of these TFRs, summarizing all\ntheoretical, practical and numerical aspects of their use, reconsidering some\nconventions and introducing new concepts and procedures. The purposes of this\nwork are: (i) to provide a consistent overview of the computation, properties,\nand use of the (S)WFT/(S)WT methods; (ii) to establish general standards\nrelated to their use, both theoretical and practical; and (iii) to provide\nclean and optimized algorithms and MatLab codes, appropriate for any window or\nwavelet. \n\n"}
{"id": "1310.8508", "contents": "Title: The distorted mirror of Wikipedia: a quantitative analysis of Wikipedia\n  coverage of academics Abstract: Activity of modern scholarship creates online footprints galore. Along with\ntraditional metrics of research quality, such as citation counts, online images\nof researchers and institutions increasingly matter in evaluating academic\nimpact, decisions about grant allocation, and promotion. We examined 400\nbiographical Wikipedia articles on academics from four scientific fields to\ntest if being featured in the world's largest online encyclopedia is correlated\nwith higher academic notability (assessed through citation counts). We found no\nstatistically significant correlation between Wikipedia articles metrics\n(length, number of edits, number of incoming links from other articles, etc.)\nand academic notability of the mentioned researchers. We also did not find any\nevidence that the scientists with better WP representation are necessarily more\nprominent in their fields. In addition, we inspected the Wikipedia coverage of\nnotable scientists sampled from Thomson Reuters list of \"highly cited\nresearchers\". In each of the examined fields, Wikipedia failed in covering\nnotable scholars properly. Both findings imply that Wikipedia might be\nproducing an inaccurate image of academics on the front end of science. By\nshedding light on how public perception of academic progress is formed, this\nstudy alerts that a subjective element might have been introduced into the\nhitherto structured system of academic evaluation. \n\n"}
{"id": "1311.2296", "contents": "Title: Newton based Stochastic Optimization using q-Gaussian Smoothed\n  Functional Algorithms Abstract: We present the first q-Gaussian smoothed functional (SF) estimator of the\nHessian and the first Newton-based stochastic optimization algorithm that\nestimates both the Hessian and the gradient of the objective function using\nq-Gaussian perturbations. Our algorithm requires only two system simulations\n(regardless of the parameter dimension) and estimates both the gradient and the\nHessian at each update epoch using these. We also present a proof of\nconvergence of the proposed algorithm. In a related recent work (Ghoshdastidar\net al., 2013), we presented gradient SF algorithms based on the q-Gaussian\nperturbations. Our work extends prior work on smoothed functional algorithms by\ngeneralizing the class of perturbation distributions as most distributions\nreported in the literature for which SF algorithms are known to work and turn\nout to be special cases of the q-Gaussian distribution. Besides studying the\nconvergence properties of our algorithm analytically, we also show the results\nof several numerical simulations on a model of a queuing network, that\nillustrate the significance of the proposed method. In particular, we observe\nthat our algorithm performs better in most cases, over a wide range of\nq-values, in comparison to Newton SF algorithms with the Gaussian (Bhatnagar,\n2007) and Cauchy perturbations, as well as the gradient q-Gaussian SF\nalgorithms (Ghoshdastidar et al., 2013). \n\n"}
{"id": "1311.2795", "contents": "Title: Complete solution of a constrained tropical optimization problem with\n  application to location analysis Abstract: We present a multidimensional optimization problem that is formulated and\nsolved in the tropical mathematics setting. The problem consists of minimizing\na nonlinear objective function defined on vectors over an idempotent semifield\nby means of a conjugate transposition operator, subject to constraints in the\nform of linear vector inequalities. A complete direct solution to the problem\nunder fairly general assumptions is given in a compact vector form suitable for\nboth further analysis and practical implementation. We apply the result to\nsolve a multidimensional minimax single facility location problem with\nChebyshev distance and with inequality constraints imposed on the feasible\nlocation area. \n\n"}
{"id": "1311.4291", "contents": "Title: Minimum $n$-Rank Approximation via Iterative Hard Thresholding Abstract: The problem of recovering a low $n$-rank tensor is an extension of sparse\nrecovery problem from the low dimensional space (matrix space) to the high\ndimensional space (tensor space) and has many applications in computer vision\nand graphics such as image inpainting and video inpainting. In this paper, we\nconsider a new tensor recovery model, named as minimum $n$-rank approximation\n(MnRA), and propose an appropriate iterative hard thresholding algorithm with\ngiving the upper bound of the $n$-rank in advance. The convergence analysis of\nthe proposed algorithm is also presented. Particularly, we show that for the\nnoiseless case, the linear convergence with rate $\\frac{1}{2}$ can be obtained\nfor the proposed algorithm under proper conditions. Additionally, combining an\neffective heuristic for determining $n$-rank, we can also apply the proposed\nalgorithm to solve MnRA when $n$-rank is unknown in advance. Some preliminary\nnumerical results on randomly generated and real low $n$-rank tensor completion\nproblems are reported, which show the efficiency of the proposed algorithms. \n\n"}
{"id": "1311.4468", "contents": "Title: Stochastic processes and feedback-linearisation for online\n  identification and Bayesian adaptive control of fully-actuated mechanical\n  systems Abstract: This work proposes a new method for simultaneous probabilistic identification\nand control of an observable, fully-actuated mechanical system. Identification\nis achieved by conditioning stochastic process priors on observations of\nconfigurations and noisy estimates of configuration derivatives. In contrast to\nprevious work that has used stochastic processes for identification, we\nleverage the structural knowledge afforded by Lagrangian mechanics and learn\nthe drift and control input matrix functions of the control-affine system\nseparately. We utilise feedback-linearisation to reduce, in expectation, the\nuncertain nonlinear control problem to one that is easy to regulate in a\ndesired manner. Thereby, our method combines the flexibility of nonparametric\nBayesian learning with epistemological guarantees on the expected closed-loop\ntrajectory. We illustrate our method in the context of torque-actuated pendula\nwhere the dynamics are learned with a combination of normal and log-normal\nprocesses. \n\n"}
{"id": "1311.6107", "contents": "Title: Off-policy reinforcement learning for $ H_\\infty $ control design Abstract: The $H_\\infty$ control design problem is considered for nonlinear systems\nwith unknown internal system model. It is known that the nonlinear $ H_\\infty $\ncontrol problem can be transformed into solving the so-called\nHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial\ndifferential equation that is generally impossible to be solved analytically.\nEven worse, model-based approaches cannot be used for approximately solving HJI\nequation, when the accurate system model is unavailable or costly to obtain in\npractice. To overcome these difficulties, an off-policy reinforcement leaning\n(RL) method is introduced to learn the solution of HJI equation from real\nsystem data instead of mathematical system model, and its convergence is\nproved. In the off-policy RL method, the system data can be generated with\narbitrary policies rather than the evaluating policy, which is extremely\nimportant and promising for practical systems. For implementation purpose, a\nneural network (NN) based actor-critic structure is employed and a least-square\nNN weight update algorithm is derived based on the method of weighted\nresiduals. Finally, the developed NN-based off-policy RL method is tested on a\nlinear F16 aircraft plant, and further applied to a rotational/translational\nactuator system. \n\n"}
{"id": "1311.7174", "contents": "Title: Optimizing gravitational-wave searches for a population of coalescing\n  binaries: Intrinsic parameters Abstract: We revisit the problem of searching for gravitational waves from inspiralling\ncompact binaries in Gaussian coloured noise. For binaries with quasicircular\norbits and non-precessing component spins, considering dominant mode emission\nonly, if the intrinsic parameters of the binary are known then the optimal\nstatistic for a single detector is the well-known two-phase matched filter.\nHowever, the matched filter signal-to-noise ratio is /not/ in general an\noptimal statistic for an astrophysical population of signals, since their\ndistribution over the intrinsic parameters will almost certainly not mirror\nthat of noise events, which is determined by the (Fisher) information metric.\nInstead, the optimal statistic for a given astrophysical distribution will be\nthe Bayes factor, which we approximate using the output of a standard template\nmatched filter search. We then quantify the possible improvement in number of\nsignals detected for various populations of non-spinning binaries: for a\ndistribution of signals uniformly distributed in volume and with component\nmasses distributed uniformly over the range $1\\leq m_{1,2}/M_\\odot\\leq 24$,\n$(m_1+m_2) /M_\\odot\\leq 25$ at fixed expected SNR, we find $\\gtrsim 20\\%$ more\nsignals at a false alarm threshold of $10^{-6}\\,$Hz in a single detector. The\nmethod may easily be generalized to binaries with non-precessing spins. \n\n"}
{"id": "1312.0444", "contents": "Title: A uniform controllability result for the Keller-Segel system Abstract: In this paper we study the controllability of the Keller-Segel system\napproximating its parabolic-elliptic version. We show that this parabolic\nsystem is locally uniform controllable around a constant solution of the\nparabolic-elliptic system when the control is acting on the component of the\nchemical. \n\n"}
{"id": "1312.2638", "contents": "Title: Vertex nomination schemes for membership prediction Abstract: Suppose that a graph is realized from a stochastic block model where one of\nthe blocks is of interest, but many or all of the vertices' block labels are\nunobserved. The task is to order the vertices with unobserved block labels into\na ``nomination list'' such that, with high probability, vertices from the\ninteresting block are concentrated near the list's beginning. We propose\nseveral vertex nomination schemes. Our basic - but principled - setting and\ndevelopment yields a best nomination scheme (which is a Bayes-Optimal\nanalogue), and also a likelihood maximization nomination scheme that is\npractical to implement when there are a thousand vertices, and which is\nempirically near-optimal when the number of vertices is small enough to allow\ncomparison to the best nomination scheme. We then illustrate the robustness of\nthe likelihood maximization nomination scheme to the modeling challenges\ninherent in real data, using examples which include a social network involving\nhuman trafficking, the Enron Graph, a worm brain connectome and a political\nblog network. \n\n"}
{"id": "1312.3039", "contents": "Title: Conic Optimization via Operator Splitting and Homogeneous Self-Dual\n  Embedding Abstract: We introduce a first order method for solving very large convex cone\nprograms. The method uses an operator splitting method, the alternating\ndirections method of multipliers, to solve the homogeneous self-dual embedding,\nan equivalent feasibility problem involving finding a nonzero point in the\nintersection of a subspace and a cone. This approach has several favorable\nproperties. Compared to interior-point methods, first-order methods scale to\nvery large problems, at the cost of requiring more time to reach very high\naccuracy. Compared to other first-order methods for cone programs, our approach\nfinds both primal and dual solutions when available or a certificate of\ninfeasibility or unboundedness otherwise, is parameter-free, and the\nper-iteration cost of the method is the same as applying a splitting method to\nthe primal or dual alone. We discuss efficient implementation of the method in\ndetail, including direct and indirect methods for computing projection onto the\nsubspace, scaling the original problem data, and stopping criteria. We describe\nan open-source implementation, which handles the usual (symmetric)\nnon-negative, second-order, and semidefinite cones as well as the\n(non-self-dual) exponential and power cones and their duals. We report\nnumerical results that show speedups over interior-point cone solvers for large\nproblems, and scaling to very large general cone programs. \n\n"}
{"id": "1312.3917", "contents": "Title: On the Market Viability under Proportional Transaction Costs Abstract: This paper studies the market viability with proportional transaction costs.\nInstead of requiring the existence of strictly consistent price systems (SCPS)\nas in the literature, we show that strictly consistent local martingale systems\n(SCLMS) can successfully serve as the dual elements such that the market\nviability can be verified. We introduce two weaker notions of no arbitrage\nconditions on market models named no unbounded profit with bounded risk (NUPBR)\nand no local arbitrage with bounded portfolios (NLABP). In particular, we show\nthat the NUPBR and NLABP conditions in the robust sense for the smaller bid-ask\nspreads is the equivalent characterization of the existence of SCLMS for\ngeneral market models. We also discuss the implications for the utility\nmaximization problem. \n\n"}
{"id": "1312.4883", "contents": "Title: A Riemannian approach to low-rank algebraic Riccati equations Abstract: We propose a Riemannian optimization approach for computing low-rank\nsolutions of the algebraic Riccati equation. The scheme alternates between\nfixed-rank optimization and rank-one updates. The fixed-rank optimization is on\nthe set of fixed-rank symmetric positive definite matrices which is endowed\nwith a particular Riemannian metric (and geometry) that is tuned to the\nstructure of the objective function. We specifically discuss the implementation\nof a Riemannian trust-region algorithm that is potentially scalable to\nlarge-scale problems. The rank-one update is based on a descent direction that\nensures a monotonic decrease of the cost function. Preliminary numerical\nresults on standard small-scale benchmarks show that we obtain solutions to the\nRiccati equation at lower ranks than the standard approaches. \n\n"}
{"id": "1312.5302", "contents": "Title: Parallel coordinate descent methods for composite minimization:\n  convergence analysis and error bounds Abstract: In this paper we propose a distributed version of a randomized\nblock-coordinate descent method for minimizing the sum of a partially separable\nsmooth convex function and a fully separable non-smooth convex function. Under\nthe assumption of block Lipschitz continuity of the gradient of the smooth\nfunction, this method is shown to have a sublinear convergence rate. Linear\nconvergence rate of the method is obtained for the newly introduced class of\ngeneralized error bound functions. We prove that the new class of generalized\nerror bound functions encompasses both global/local error bound functions and\nsmooth strongly convex functions. We also show that the theoretical estimates\non the convergence rate depend on the number of blocks chosen randomly and a\nnatural measure of separability of the objective function. \n\n"}
{"id": "1312.5434", "contents": "Title: Asynchronous Adaptation and Learning over Networks --- Part I: Modeling\n  and Stability Analysis Abstract: In this work and the supporting Parts II [2] and III [3], we provide a rather\ndetailed analysis of the stability and performance of asynchronous strategies\nfor solving distributed optimization and adaptation problems over networks. We\nexamine asynchronous networks that are subject to fairly general sources of\nuncertainties, such as changing topologies, random link failures, random data\narrival times, and agents turning on and off randomly. Under this model, agents\nin the network may stop updating their solutions or may stop sending or\nreceiving information in a random manner and without coordination with other\nagents. We establish in Part I conditions on the first and second-order moments\nof the relevant parameter distributions to ensure mean-square stable behavior.\nWe derive in Part II expressions that reveal how the various parameters of the\nasynchronous behavior influence network performance. We compare in Part III the\nperformance of asynchronous networks to the performance of both centralized\nsolutions and synchronous networks. One notable conclusion is that the\nmean-square-error performance of asynchronous networks shows a degradation only\nof the order of $O(\\nu)$, where $\\nu$ is a small step-size parameter, while the\nconvergence rate remains largely unaltered. The results provide a solid\njustification for the remarkable resilience of cooperative networks in the face\nof random failures at multiple levels: agents, links, data arrivals, and\ntopology. \n\n"}
{"id": "1312.5638", "contents": "Title: Exploring Multi-Modal Distributions with Nested Sampling Abstract: In performing a Bayesian analysis, two difficult problems often emerge.\nFirst, in estimating the parameters of some model for the data, the resulting\nposterior distribution may be multi-modal or exhibit pronounced (curving)\ndegeneracies. Secondly, in selecting between a set of competing models,\ncalculation of the Bayesian evidence for each model is computationally\nexpensive using existing methods such as thermodynamic integration. Nested\nSampling is a Monte Carlo method targeted at the efficient calculation of the\nevidence, but also produces posterior inferences as a by-product and therefore\nprovides means to carry out parameter estimation as well as model selection.\nThe main challenge in implementing Nested Sampling is to sample from a\nconstrained probability distribution. One possible solution to this problem is\nprovided by the Galilean Monte Carlo (GMC) algorithm. We show results of\napplying Nested Sampling with GMC to some problems which have proven very\ndifficult for standard Markov Chain Monte Carlo (MCMC) and down-hill methods,\ndue to the presence of large number of local minima and/or pronounced (curving)\ndegeneracies between the parameters. We also discuss the use of Nested Sampling\nwith GMC in Bayesian object detection problems, which are inherently\nmulti-modal and require the evaluation of Bayesian evidence for distinguishing\nbetween true and spurious detections. \n\n"}
{"id": "1312.6677", "contents": "Title: Path Finding I :Solving Linear Programs with \\~O(sqrt(rank)) Linear\n  System Solves Abstract: In this paper we present a new algorithm for solving linear programs that\nrequires only $\\tilde{O}(\\sqrt{rank(A)}L)$ iterations to solve a linear program\nwith $m$ constraints, $n$ variables, and constraint matrix $A$, and bit\ncomplexity $L$. Each iteration of our method consists of solving $\\tilde{O}(1)$\nlinear systems and additional nearly linear time computation.\n  Our method improves upon the previous best iteration bound by factor of\n$\\tilde{\\Omega}((m/rank(A))^{1/4})$ for methods with polynomial time computable\niterations and by $\\tilde{\\Omega}((m/rank(A))^{1/2})$ for methods which solve\nat most $\\tilde{O}(1)$ linear systems in each iteration. Our method is\nparallelizable and amenable to linear algebraic techniques for accelerating the\nlinear system solver. As such, up to polylogarithmic factors we either match or\nimprove upon the best previous running times in both depth and work for\ndifferent ratios of $m$ and $rank(A)$.\n  Moreover, our method matches up to polylogarithmic factors a theoretical\nlimit established by Nesterov and Nemirovski in 1994 regarding the use of a\n\"universal barrier\" for interior point methods, thereby resolving a\nlong-standing open question regarding the running time of polynomial time\ninterior point methods for linear programming. \n\n"}
{"id": "1312.7360", "contents": "Title: A state-constrained differential game arising in optimal portfolio\n  liquidation Abstract: We consider $n$ risk-averse agents who compete for liquidity in an\nAlmgren--Chriss market impact model. Mathematically, this situation can be\ndescribed by a Nash equilibrium for a certain linear-quadratic differential\ngame with state constraints. The state constraints enter the problem as\nterminal boundary conditions for finite and infinite time horizons. We prove\nexistence and uniqueness of Nash equilibria and give closed-form solutions in\nsome special cases. We also analyze qualitative properties of the equilibrium\nstrategies and provide corresponding financial interpretations. \n\n"}
{"id": "1312.7853", "contents": "Title: Communication Efficient Distributed Optimization using an Approximate\n  Newton-type Method Abstract: We present a novel Newton-type method for distributed optimization, which is\nparticularly well suited for stochastic optimization and learning problems. For\nquadratic objectives, the method enjoys a linear rate of convergence which\nprovably \\emph{improves} with the data size, requiring an essentially constant\nnumber of iterations under reasonable assumptions. We provide theoretical and\nempirical evidence of the advantages of our method compared to other\napproaches, such as one-shot parameter averaging and ADMM. \n\n"}
{"id": "1401.4786", "contents": "Title: Common Information based Markov Perfect Equilibria for Linear-Gaussian\n  Games with Asymmetric Information Abstract: We consider a class of two-player dynamic stochastic nonzero-sum games where\nthe state transition and observation equations are linear, and the primitive\nrandom variables are Gaussian. Each controller acquires possibly different\ndynamic information about the state process and the other controller's past\nactions and observations. This leads to a dynamic game of asymmetric\ninformation among the controllers. Building on our earlier work on finite games\nwith asymmetric information, we devise an algorithm to compute a Nash\nequilibrium by using the common information among the controllers. We call such\nequilibria common information based Markov perfect equilibria of the game,\nwhich can be viewed as a refinement of Nash equilibrium in games with\nasymmetric information. If the players' cost functions are quadratic, then we\nshow that under certain conditions a unique common information based Markov\nperfect equilibrium exists. Furthermore, this equilibrium can be computed by\nsolving a sequence of linear equations. We also show through an example that\nthere could be other Nash equilibria in a game of asymmetric information, not\ncorresponding to common information based Markov perfect equilibria. \n\n"}
{"id": "1401.4853", "contents": "Title: Intrinsic volumes of set of singular matrices Abstract: We explicitly compute the intrinsic volume of the set of real (and real\nsymmetric) matrices of Frobenius norm one and given corank (the case of\nmatrices with zero determinant as a special case). We give asymptotic formulas\nfor our computations and we discuss several examples and applications. \n\n"}
{"id": "1401.5492", "contents": "Title: Alternating direction method of multipliers for penalized zero-variance\n  discriminant analysis Abstract: We consider the task of classification in the high dimensional setting where\nthe number of features of the given data is significantly greater than the\nnumber of observations. To accomplish this task, we propose a heuristic, called\nsparse zero-variance discriminant analysis (SZVD), for simultaneously\nperforming linear discriminant analysis and feature selection on high\ndimensional data. This method combines classical zero-variance discriminant\nanalysis, where discriminant vectors are identified in the null space of the\nsample within-class covariance matrix, with penalization applied to induce\nsparse structures in the resulting vectors. To approximately solve the\nresulting nonconvex problem, we develop a simple algorithm based on the\nalternating direction method of multipliers. Further, we show that this\nalgorithm is applicable to a larger class of penalized generalized eigenvalue\nproblems, including a particular relaxation of the sparse principal component\nanalysis problem. Finally, we establish theoretical guarantees for convergence\nof our algorithm to stationary points of the original nonconvex problem, and\nempirically demonstrate the effectiveness of our heuristic for classifying\nsimulated data and data drawn from applications in time-series classification. \n\n"}
{"id": "1401.6683", "contents": "Title: Resource Allocation Under Channel Uncertainties for Relay-Aided\n  Device-to-Device Communication Underlaying LTE-A Cellular Networks Abstract: Device-to-device (D2D) communication in cellular networks allows direct\ntransmission between two cellular devices with local communication needs. Due\nto the increasing number of autonomous heterogeneous devices in future mobile\nnetworks, an efficient resource allocation scheme is required to maximize\nnetwork throughput and achieve higher spectral efficiency. In this paper,\nperformance of network-integrated D2D communication under channel uncertainties\nis investigated where D2D traffic is carried through relay nodes. Considering a\nmulti-user and multi-relay network, we propose a robust distributed solution\nfor resource allocation with a view to maximizing network sum-rate when the\ninterference from other relay nodes and the link gains are uncertain. An\noptimization problem is formulated for allocating radio resources at the relays\nto maximize end-to-end rate as well as satisfy the quality-of-service (QoS)\nrequirements for cellular and D2D user equipments under total power constraint.\nEach of the uncertain parameters is modeled by a bounded distance between its\nestimated and bounded values. We show that the robust problem is convex and a\ngradient-aided dual decomposition algorithm is applied to allocate radio\nresources in a distributed manner. Finally, to reduce the cost of robustness\ndefined as the reduction of achievable sum-rate, we utilize the \\textit{chance\nconstraint approach} to achieve a trade-off between robustness and optimality.\nThe numerical results show that there is a distance threshold beyond which\nrelay-aided D2D communication significantly improves network performance when\ncompared to direct communication between D2D peers. \n\n"}
{"id": "1401.7079", "contents": "Title: A Block Successive Upper Bound Minimization Method of Multipliers for\n  Linearly Constrained Convex Optimization Abstract: Consider the problem of minimizing the sum of a smooth convex function and a\nseparable nonsmooth convex function subject to linear coupling constraints.\nProblems of this form arise in many contemporary applications including signal\nprocessing, wireless networking and smart grid provisioning. Motivated by the\nhuge size of these applications, we propose a new class of first order\nprimal-dual algorithms called the block successive upper-bound minimization\nmethod of multipliers (BSUM-M) to solve this family of problems. The BSUM-M\nupdates the primal variable blocks successively by minimizing locally tight\nupper-bounds of the augmented Lagrangian of the original problem, followed by a\ngradient type update for the dual variable in closed form. We show that under\ncertain regularity conditions, and when the primal block variables are updated\nin either a deterministic or a random fashion, the BSUM-M converges to the set\nof optimal solutions. Moreover, in the absence of linear constraints, we show\nthat the BSUM-M, which reduces to the block successive upper-bound minimization\n(BSUM) method, is capable of linear convergence without strong convexity. \n\n"}
{"id": "1401.7279", "contents": "Title: Optimal control and numerical software: an overview Abstract: Optimal Control (OC) is the process of determining control and state\ntrajectories for a dynamic system, over a period of time, in order to optimize\na given performance index. With the increasing of variables and complexity, OC\nproblems can no longer be solved analytically and, consequently, numerical\nmethods are required. For this purpose, direct and indirect methods are used.\nDirect methods consist in the discretization of the OC problem, reducing it to\na nonlinear constrained optimization problem. Indirect methods are based on the\nPontryagin Maximum Principle, which in turn reduces to a boundary value\nproblem. In order to have a more reliable solution, one can solve the same\nproblem through different approaches. Here, as an illustrative example, an\nepidemiological application related to the rubella disease is solved using\nseveral software packages, such as the routine ode45 of Matlab, OC-ODE, DOTcvp\ntoolbox, IPOPT and Snopt, showing the state of the art of numerical software\nfor OC. \n\n"}
{"id": "1402.1125", "contents": "Title: OSGA: A fast subgradient algorithm with optimal complexity Abstract: This paper presents an algorithm for approximately minimizing a convex\nfunction in simple, not necessarily bounded convex domains, assuming only that\nfunction values and subgradients are available. No global information about the\nobjective function is needed apart from a strong convexity parameter (which can\nbe put to zero if only convexity is known).\n  The worst case number of iterations needed to achieve a given accuracy is\nindependent of the dimension (which may be infinite) and - apart from a\nconstant factor - best possible under a variety of smoothness assumptions on\nthe objective function. \n\n"}
{"id": "1402.2467", "contents": "Title: A model problem for Mean Field Games on networks Abstract: In [14], Gueant, Lasry and Lions considered the model problem ``What time\ndoes meeting start?'' as a prototype for a general class of optimization\nproblems with a continuum of players, called Mean Field Games problems. In this\npaper we consider a similar model, but with the dynamics of the agents defined\non a network. We discuss appropriate transition conditions at the vertices\nwhich give a well posed problem and we present some numerical results. \n\n"}
{"id": "1402.4275", "contents": "Title: Robust estimate of dynamo thresholds in the von K\\'arm\\'an sodium\n  experiment using the Extreme Value Theory Abstract: We apply a new threshold detection method based on the extreme value theory\nto the von K\\'arm\\'an sodium (VKS) experiment data. The VKS experiment is a\nsuccessful attempt to get a dynamo magnetic field in a laboratory liquid-metal\nexperiment. We first show that the dynamo threshold is associated to a change\nof the probability density function of the extreme values of the magnetic\nfield. This method does not require the measurement of response functions from\napplied external perturbations, and thus provides a simple threshold estimate.\nWe apply our method to different configurations in the VKS experiment showing\nthat it yields a robust indication of the dynamo threshold as well as evidence\nof hysteretic behaviors. Moreover, for the experimental configurations in which\na dynamo transition is not observed, the method provides a way to extrapolate\nan interval of possible threshold values. \n\n"}
{"id": "1402.4600", "contents": "Title: Ancillary Service to the Grid Using Intelligent Deferrable Loads Abstract: Renewable energy sources such as wind and solar power have a high degree of\nunpredictability and time-variation, which makes balancing demand and supply\nchallenging. One possible way to address this challenge is to harness the\ninherent flexibility in demand of many types of loads. Introduced in this paper\nis a technique for decentralized control for automated demand response that can\nbe used by grid operators as ancillary service for maintaining demand-supply\nbalance.\n  A Markovian Decision Process (MDP) model is introduced for an individual\nload. A randomized control architecture is proposed, motivated by the need for\ndecentralized decision making, and the need to avoid synchronization that can\nlead to large and detrimental spikes in demand. An aggregate model for a large\nnumber of loads is then developed by examining the mean field limit. A key\ninnovation is an LTI-system approximation of the aggregate nonlinear model,\nwith a scalar signal as the input and a measure of the aggregate demand as the\noutput. This makes the approximation particularly convenient for control design\nat the grid level.\n  The second half of the paper contains a detailed application of these results\nto a network of residential pools. Simulations are provided to illustrate the\naccuracy of the approximations and effectiveness of the proposed control\napproach. \n\n"}
{"id": "1402.7030", "contents": "Title: Asymptotic Perron's method and simple Markov Strategies in stochastic\n  games and control Abstract: We introduce a modification of Perron's method, where semi-solutions are\nconsidered in a carefully defined asymptotic sense. With this definition, we\ncan show, in a rather elementary way, that in a zero-sum game or a control\nproblem (with or without model uncertainty), the value function over all\nstrategies coincides with the value function over Markov strategies discretized\nin time. Therefore, there are always discretized Markov $\\varepsilon$-optimal\nstrategies, (uniform with respect to the bounded initial condition). With a\nminor modification, the method produces a value and approximate saddle points\nfor an asymmetric game of feedback strategies vs. counter-strategies. \n\n"}
{"id": "1403.0409", "contents": "Title: L\\'evy flights in inhomogeneous environments and 1/f noise Abstract: Complex dynamical systems which are governed by anomalous diffusion often can\nbe described by Langevin equations driven by L\\'evy stable noise. In this\narticle we generalize nonlinear stochastic differential equations driven by\nGaussian noise and generating signals with 1/f power spectral density by\nreplacing the Gaussian noise with a more general L\\'evy stable noise. The\nequations with the Gaussian noise arise as a special case when the index of\nstability alpha=2. We expect that this generalization may be useful for\ndescribing 1/f fluctuations in the systems subjected to L\\'evy stable noise. \n\n"}
{"id": "1403.1574", "contents": "Title: Consentaneous agent-based and stochastic model of the financial markets Abstract: We are looking for the agent-based treatment of the financial markets\nconsidering necessity to build bridges between microscopic, agent based, and\nmacroscopic, phenomenological modeling. The acknowledgment that agent-based\nmodeling framework, which may provide qualitative and quantitative\nunderstanding of the financial markets, is very ambiguous emphasizes the\nexceptional value of well defined analytically tractable agent systems. Herding\nas one of the behavior peculiarities considered in the behavioral finance is\nthe main property of the agent interactions we deal with in this contribution.\nLooking for the consentaneous agent-based and macroscopic approach we combine\ntwo origins of the noise: exogenous one, related to the information flow, and\nendogenous one, arising form the complex stochastic dynamics of agents. As a\nresult we propose a three state agent-based herding model of the financial\nmarkets. From this agent-based model we derive a set of stochastic differential\nequations, which describes underlying macroscopic dynamics of agent population\nand log price in the financial markets. The obtained solution is then subjected\nto the exogenous noise, which shapes instantaneous return fluctuations. We test\nboth Gaussian and q-Gaussian noise as a source of the short term fluctuations.\nThe resulting model of the return in the financial markets with the same set of\nparameters reproduces empirical probability and spectral densities of absolute\nreturn observed in New York, Warsaw and NASDAQ OMX Vilnius Stock Exchanges. Our\nresult confirms the prevalent idea in behavioral finance that herding\ninteractions may be dominant over agent rationality and contribute towards\nbubble formation. \n\n"}
{"id": "1403.1713", "contents": "Title: The time series forecasting: from the aspect of network Abstract: Forecasting can estimate the statement of events according to the historical\ndata and it is considerably important in many disciplines. At present, time\nseries models have been utilized to solve forecasting problems in various\ndomains. In general, researchers use curve fitting and parameter estimation\nmethods (moment estimation, maximum likelihood estimation and least square\nmethod) to forecast. In this paper, a new sight is given to the forecasting and\na completely different method is proposed to forecast time series. Inspired by\nthe visibility graph and link prediction, this letter converts time series into\nnetwork and then finds the nodes which are mostly likelihood to link with the\npredicted node. Finally, the predicted value will be obtained according to the\nstate of the link. The TAIEX data set is used in the case study to illustrate\nthat the proposed method is effectiveness. Compared with ARIMA model, the\nproposed shows a good forecasting performance when there is a small amount of\ndata. \n\n"}
{"id": "1403.3937", "contents": "Title: Existence of minimizers for generalized Lagrangian functionals and a\n  necessary optimality condition --- Application to fractional variational\n  problems Abstract: We study dynamic minimization problems of the calculus of variations with\ngeneralized Lagrangian functionals that depend on a general linear operator $K$\nand defined on bounded-time intervals. Under assumptions of regularity,\nconvexity and coercivity, we derive sufficient conditions ensuring the\nexistence of a minimizer. Finally, we obtain necessary optimality conditions of\nEuler-Lagrange type. Main results are illustrated with special cases, when $K$\nis a general kernel operator and, in particular, with $K$ the fractional\nintegral of Riemann-Liouville and Hadamard. The application of our results to\nthe recent fractional calculus of variations gives answer to an open question\nposed in [Abstr. Appl. Anal. 2012, Art. ID 871912; doi:10.1155/2012/871912]. \n\n"}
{"id": "1403.4630", "contents": "Title: Penalising model component complexity: A principled, practical approach\n  to constructing priors Abstract: In this paper, we introduce a new concept for constructing prior\ndistributions. We exploit the natural nested structure inherent to many model\ncomponents, which defines the model component to be a flexible extension of a\nbase model. Proper priors are defined to penalise the complexity induced by\ndeviating from the simpler base model and are formulated after the input of a\nuser-defined scaling parameter for that model component, both in the univariate\nand the multivariate case. These priors are invariant to reparameterisations,\nhave a natural connection to Jeffreys' priors, are designed to support Occam's\nrazor and seem to have excellent robustness properties, all which are highly\ndesirable and allow us to use this approach to define default prior\ndistributions. Through examples and theoretical results, we demonstrate the\nappropriateness of this approach and how it can be applied in various\nsituations. \n\n"}
{"id": "1403.5339", "contents": "Title: Spacecraft Position and Attitude Formation Control using Line-of-Sight\n  Observations Abstract: This paper studies formation control of an arbitrary number of spacecraft\nbased on a serial network structure. The leader controls its absolute position\nand absolute attitude with respect to an inertial frame, and the followers\ncontrol its relative position and attitude with respect to another spacecraft\nassigned by the serial network. The unique feature is that both the absolute\nattitude and the relative attitude control systems are developed directly in\nterms of the line-of-sight observations between spacecraft, without need for\nestimating the full absolute and relative attitudes, to improve accuracy and\nefficiency. Control systems are developed on the nonlinear configuration\nmanifold, guaranteeing exponential stability. Numerical examples are presented\nto illustrate the desirable properties of the proposed control system. \n\n"}
{"id": "1403.6281", "contents": "Title: Exponential decay properties of a mathematical model for a certain\n  fluid-structure interaction Abstract: In this work, we derive a result of exponential stability for a coupled\nsystem of partial differential equations (PDEs) which governs a certain\nfluid-structure interaction. In particular, a three-dimensional Stokes flow\ninteracts across a boundary interface with a two-dimensional mechanical plate\nequation. In the case that the PDE plate component is rotational inertia-free,\none will have that solutions of this fluid-structure PDE system exhibit an\nexponential rate of decay. By way of proving this decay, an estimate is\nobtained for the resolvent of the associated semigroup generator, an estimate\nwhich is uniform for frequency domain values along the imaginary axis.\nSubsequently, we proceed to discuss relevant point control and boundary control\nscenarios for this fluid-structure PDE model, with an ultimate view to optimal\ncontrol studies on both finite and infinite horizon. (Because of said\nexponential stability result, optimal control of the PDE on time interval\n$(0,\\infty)$ becomes a reasonable problem for contemplation.) \n\n"}
{"id": "1404.0431", "contents": "Title: Learning Latent Block Structure in Weighted Networks Abstract: Community detection is an important task in network analysis, in which we aim\nto learn a network partition that groups together vertices with similar\ncommunity-level connectivity patterns. By finding such groups of vertices with\nsimilar structural roles, we extract a compact representation of the network's\nlarge-scale structure, which can facilitate its scientific interpretation and\nthe prediction of unknown or future interactions. Popular approaches, including\nthe stochastic block model, assume edges are unweighted, which limits their\nutility by throwing away potentially useful information. We introduce the\n`weighted stochastic block model' (WSBM), which generalizes the stochastic\nblock model to networks with edge weights drawn from any exponential family\ndistribution. This model learns from both the presence and weight of edges,\nallowing it to discover structure that would otherwise be hidden when weights\nare discarded or thresholded. We describe a Bayesian variational algorithm for\nefficiently approximating this model's posterior distribution over latent block\nstructures. We then evaluate the WSBM's performance on both edge-existence and\nedge-weight prediction tasks for a set of real-world weighted networks. In all\ncases, the WSBM performs as well or better than the best alternatives on these\ntasks. \n\n"}
{"id": "1404.2655", "contents": "Title: Open problem: Tightness of maximum likelihood semidefinite relaxations Abstract: We have observed an interesting, yet unexplained, phenomenon: Semidefinite\nprogramming (SDP) based relaxations of maximum likelihood estimators (MLE) tend\nto be tight in recovery problems with noisy data, even when MLE cannot exactly\nrecover the ground truth. Several results establish tightness of SDP based\nrelaxations in the regime where exact recovery from MLE is possible. However,\nto the best of our knowledge, their tightness is not understood beyond this\nregime. As an illustrative example, we focus on the generalized Procrustes\nproblem. \n\n"}
{"id": "1404.5086", "contents": "Title: Algebraic Decompositions of DP Problems with Linear Dynamics Abstract: Inspired by rational canonical forms, we introduce and analyze two\ndecompositions of dynamic programming (DP) problems for systems with linear\ndynamics. Specifically, we consider both finite and infinite horizon DP\nproblems in which the dynamics are linear, the cost function depends only on\nthe state, and the state-space is finite dimensional but defined over an\narbitrary algebraic field. Starting from the natural decomposition of the\nstate-space into the direct sum of subspaces that are invariant under the\nsystem's linear transformation, and assuming that the cost functions exhibit an\nadditive structure compatible with this decomposition, we extract from the\noriginal DP problem two distinct families of smaller DP problems, each\nassociated with a system evolving on an invariant subspace of the original\nstate-space. We propose that each of these families constitutes a decomposition\nof the original problem when the optimal policy and value function of the\noriginal problem can be reconstructed from the optimal policies and value\nfunctions of the individual subproblems in the family. We derive necessary and\nsufficient conditions for these decompositions to exist both in the finite and\ninfinite horizon cases. We also propose a readily verifiable sufficient\ncondition under which the first decomposition exists, and we show that the\nfirst notion of decomposition is generally stronger than the second. \n\n"}
{"id": "1404.5100", "contents": "Title: Convergence of cyclic coordinatewise l1 minimization Abstract: We consider the general problem of minimizing an objective function which is\nthe sum of a convex function (not strictly convex) and absolute values of a\nsubset of variables (or equivalently the l1-norm of the variables). This\nproblem appears exten- sively in modern statistical applications associated\nwith high-dimensional data or \"big data\", and corresponds to optimizing\nl1-regularized likelihoods in the context of model selection. In such\napplications, cyclic coordinatewise minimization (CCM), where the objective\nfunction is sequentially minimized with respect to each individual coordi-\nnate, is often employed as it offers a computationally cheap and effective\noptimization method. Consequently, it is crucial to obtain theoretical\nguarantees of convergence for the sequence of iterates produced by the cyclic\ncoordinatewise minimization in this setting. Moreover, as the objective\ncorresponds to at l1-regularized likelihoods of many variables, it is important\nto obtain convergence of the iterates themselves, and not just the function\nvalues. Previous results in the literature only establish either, (i) that\nevery limit point of the sequence of iterates is a stationary point of the\nobjective function, or (ii) establish convergence under special assumptions, or\n(iii) establish con- vergence for a different minimization approach (which uses\nquadratic approximation based gradient descent followed by an inexact line\nsearch), (iv) establish convergence of only the function values of the sequence\nof iterates produced by random coordinatewise minimization (a variant of CCM).\nIn this paper, a rigorous general proof of convergence for the cyclic\ncoordinatewise minimization algorithm is provided. We demonstrate the\nusefulness of our general results in contemporary applications. \n\n"}
{"id": "1404.7282", "contents": "Title: Formal Proofs for Nonlinear Optimization Abstract: We present a formally verified global optimization framework. Given a\nsemialgebraic or transcendental function $f$ and a compact semialgebraic domain\n$K$, we use the nonlinear maxplus template approximation algorithm to provide a\ncertified lower bound of $f$ over $K$. This method allows to bound in a modular\nway some of the constituents of $f$ by suprema of quadratic forms with a well\nchosen curvature. Thus, we reduce the initial goal to a hierarchy of\nsemialgebraic optimization problems, solved by sums of squares relaxations. Our\nimplementation tool interleaves semialgebraic approximations with sums of\nsquares witnesses to form certificates. It is interfaced with Coq and thus\nbenefits from the trusted arithmetic available inside the proof assistant. This\nfeature is used to produce, from the certificates, both valid underestimators\nand lower bounds for each approximated constituent. The application range for\nsuch a tool is widespread; for instance Hales' proof of Kepler's conjecture\nyields thousands of multivariate transcendental inequalities. We illustrate the\nperformance of our formal framework on some of these inequalities as well as on\nexamples from the global optimization literature. \n\n"}
{"id": "1404.7801", "contents": "Title: Duality results for Iterated Function Systems with a general family of\n  branches Abstract: For $X$, $Y$, $Z$ and $W$ compact metric spaces, consider two uniformly\ncontractive IFS $\\{\\tau_x: Z\\to Z,\\, x\\in x\\}$ and $\\{\\tau_y:W\\to W,\\, y\\in\nY\\}$. For a fixed $\\alpha \\in \\mathcal{P}(X)$ with $supp(\\alpha)=X$ we define\nthe entropy of a holonomic measure $\\pi \\in \\mathcal{P}(X\\times Z)$ relative to\n$\\alpha$, the pressure of a continuous cost function $c(x,z)$ and show that for\n$c$ Lipschitz this pressure coincides with the spectral radius of the\nassociated transfer operator. The same approach can be applied to the pair\n$Y,W$. For fixed probabilities $\\alpha \\in \\mathcal{P}(X)$ and $\\beta \\in\n\\mathcal{P}(Y)$ with $supp(\\alpha)=X,\\,supp(\\beta)=Y$ we denote by\n$H_\\alpha(\\pi), \\pi \\in \\Pi(\\cdot,\\cdot,\\tau)$, the entropy of the\n$(X,Z)-$marginal of $\\pi$ relative to $\\alpha$ and denote by $H_\\beta(\\pi)$,\nthe entropy of the $(Y,W)-$marginal of $\\pi$ relative to $\\beta$. The marginal\npressure of a continuous cost function $c \\in C(X\\times Y \\times Z \\times W)$\nrelative to $(\\alpha,\\beta)$ will be defined by $P^{m}(c) =\n\\sup_{\\pi\\in\\Pi(\\cdot,\\cdot,\\tau)} \\int c\\, d\\pi + H_{\\alpha}(\\pi)\n+H_{\\beta}(\\pi)$ and we will show the following duality result: \\[\\inf_{P^{m}(c\n-\\varphi(x) -\\psi(y))=0} \\int \\varphi(x)\\,d\\mu +\\int \\psi(y)\\,d\\nu =\n\\sup_{\\pi\\in\\Pi(\\mu,\\nu,\\tau)} \\int c\\, d\\pi + H_{\\alpha}(\\pi)\n+H_{\\beta}(\\pi).\\] When $Z$ and $W$ have only one point and the entropy is\nunconsidered this equality can be rewritten as the Kantorovich Duality for\ncompact spaces $X,Y$ and continuous cost $-c$: \\[\\inf_{c -\\varphi(x)\n-\\psi(y)\\leq 0} \\int \\varphi(x)\\,d\\mu +\\int \\psi(y)\\,d\\nu =\n\\sup_{\\pi\\in\\Pi(\\mu,\\nu)} \\int c\\, d\\pi .\\] \n\n"}
{"id": "1405.0042", "contents": "Title: Learning with incremental iterative regularization Abstract: Within a statistical learning setting, we propose and study an iterative\nregularization algorithm for least squares defined by an incremental gradient\nmethod. In particular, we show that, if all other parameters are fixed a\npriori, the number of passes over the data (epochs) acts as a regularization\nparameter, and prove strong universal consistency, i.e. almost sure convergence\nof the risk, as well as sharp finite sample bounds for the iterates. Our\nresults are a step towards understanding the effect of multiple epochs in\nstochastic gradient techniques in machine learning and rely on integrating\nstatistical and optimization results. \n\n"}
{"id": "1405.3402", "contents": "Title: Probability and Statistics for Particle Physicists Abstract: Lectures presented at the 1st CERN Asia-Europe-Pacific School of High-Energy\nPhysics, Fukuoka, Japan, 14-27 October 2012. A pedagogical selection of topics\nin probability and statistics is presented. Choice and emphasis are driven by\nthe author's personal experience, predominantly in the context of physics\nanalyses using experimental data from high-energy physics detectors. \n\n"}
{"id": "1405.4161", "contents": "Title: Long and winding central paths Abstract: We disprove a continuous analogue of the Hirsch conjecture proposed by Deza,\nTerlaky and Zinchenko, by constructing a family of linear programs with $3r+4$\ninequalities in dimension $2r+2$ where the central path has a total curvature\nin $\\Omega(2^r)$. Our method is to tropicalize the central path in linear\nprogramming. The tropical central path is the piecewise-linear limit of the\ncentral paths of parameterized families of classical linear programs viewed\nthrough logarithmic glasses. The lower bound for the classical curvature is\nobtained by developing a combinatorial concept of a tropical angle. \n\n"}
{"id": "1405.4252", "contents": "Title: Stochastic Perron's method for optimal control problems with state\n  constraints Abstract: We apply the stochastic Perron method of Bayraktar and S\\^irbu to a general\ninfinite horizon optimal control problem, where the state $X$ is a controlled\ndiffusion process, and the state constraint is described by a closed set. We\nprove that the value function $v$ is bounded from below (resp., from above) by\na viscosity supersolution (resp., subsolution) of the related state constrained\nproblem for the Hamilton-Jacobi-Bellman equation. In the case of a smooth\ndomain, under some additional assumptions, these estimates allow to identify\n$v$ with a unique continuous constrained viscosity solution of this equation. \n\n"}
{"id": "1405.4454", "contents": "Title: Transposition Method for Backward Stochastic Evolution Equations\n  Revisited, and Its Application Abstract: The main purpose of this paper is to improve our transposition method to\nsolve both vector-valued and operator-valued backward stochastic evolution\nequations with a general filtration. As its application, we obtain a general\nPontryagin-type maximum principle for optimal controls of stochastic evolution\nequations in infinite dimensions. In 1articular, we drop the technical\nassumption appeared in [Q. L\\\"u and X. Zhang, Springer Briefs in\nMathematics,Springer, New York, 2014, Theorem 9.1]. \n\n"}
{"id": "1405.6081", "contents": "Title: An Integer Programming Formulation of the Minimum Common String\n  Partition problem Abstract: We consider the problem of finding a minimum common partition of two strings\n(MCSP). The problem has its application in genome comparison. MCSP problem is\nproved to be NP-hard. In this paper, we develop an Integer Programming (IP)\nformulation for the problem and implement it. The experimental results are\ncompared with the previous state-of-the-art algorithms and are found to be\npromising. \n\n"}
{"id": "1405.6444", "contents": "Title: The role of dimensionality reduction in linear classification Abstract: Dimensionality reduction (DR) is often used as a preprocessing step in\nclassification, but usually one first fixes the DR mapping, possibly using\nlabel information, and then learns a classifier (a filter approach). Best\nperformance would be obtained by optimizing the classification error jointly\nover DR mapping and classifier (a wrapper approach), but this is a difficult\nnonconvex problem, particularly with nonlinear DR. Using the method of\nauxiliary coordinates, we give a simple, efficient algorithm to train a\ncombination of nonlinear DR and a classifier, and apply it to a RBF mapping\nwith a linear SVM. This alternates steps where we train the RBF mapping and a\nlinear SVM as usual regression and classification, respectively, with a\nclosed-form step that coordinates both. The resulting nonlinear low-dimensional\nclassifier achieves classification errors competitive with the state-of-the-art\nbut is fast at training and testing, and allows the user to trade off runtime\nfor classification accuracy easily. We then study the role of nonlinear DR in\nlinear classification, and the interplay between the DR mapping, the number of\nlatent dimensions and the number of classes. When trained jointly, the DR\nmapping takes an extreme role in eliminating variation: it tends to collapse\nclasses in latent space, erasing all manifold structure, and lay out class\ncentroids so they are linearly separable with maximum margin. \n\n"}
{"id": "1406.0533", "contents": "Title: Distributed bargaining in dyadic-exchange networks Abstract: This paper considers dyadic-exchange networks in which individual agents\nautonomously form coalitions of size two and agree on how to split a\ntransferable utility. Valid results for this game include stable (if agents\nhave no unilateral incentive to deviate), balanced (if matched agents obtain\nsimilar benefits from collaborating), or Nash (both stable and balanced)\noutcomes. We design provably-correct continuous-time algorithms to find each of\nthese classes of outcomes in a distributed way. Our algorithmic design to find\nNash bargaining solutions builds on the other two algorithms by having the\ndynamics for finding stable outcomes feeding into the one for finding balanced\nones. Our technical approach to establish convergence and robustness combines\nnotions and tools from optimization, graph theory, nonsmooth analysis, and\nLyapunov stability theory and provides a useful framework for further\nextensions. We illustrate our results in a wireless communication scenario\nwhere single-antenna devices have the possibility of working as 2-antenna\nvirtual devices to improve channel capacity. \n\n"}
{"id": "1406.2075", "contents": "Title: Stochastic Gradient-Push for Strongly Convex Functions on Time-Varying\n  Directed Graphs Abstract: We investigate the convergence rate of the recently proposed subgradient-push\nmethod for distributed optimization over time-varying directed graphs. The\nsubgradient-push method can be implemented in a distributed way without\nrequiring knowledge of either the number of agents or the graph sequence; each\nnode is only required to know its out-degree at each time. Our main result is a\nconvergence rate of $O \\left((\\ln t)/t \\right)$ for strongly convex functions\nwith Lipschitz gradients even if only stochastic gradient samples are\navailable; this is asymptotically faster than the $O \\left((\\ln t)/\\sqrt{t}\n\\right)$ rate previously known for (general) convex functions. \n\n"}
{"id": "1406.2489", "contents": "Title: From human mobility to renewable energies: Big data analysis to approach\n  worldwide multiscale phenomena Abstract: We address and discuss recent trends in the analysis of big data sets, with\nthe emphasis on studying multiscale phenomena. Applications of big data\nanalysis in different scientific fields are described and two particular\nexamples of multiscale phenomena are explored in more detail. The first one\ndeals with wind power production at the scale of single wind turbines, the\nscale of entire wind farms and also at the scale of a whole country. Using open\nsource data we show that the wind power production has an intermittent\ncharacter at all those three scales, with implications for defining adequate\nstrategies for stable energy production. The second example concerns the\ndynamics underlying human mobility, which presents different features at\ndifferent scales. For that end, we analyze $12$-month data of the Eduroam\ndatabase within Portuguese universities, and find that, at the smallest scales,\ntypically within a set of a few adjacent buildings, the characteristic\nexponents of average displacements are different from the ones found at the\nscale of one country or one continent. \n\n"}
{"id": "1406.3228", "contents": "Title: On Existence of $L^1$-solutions for Coupled Boltzmann Transport Equation\n  and Radiation Therapy Treatment Optimization Abstract: The paper considers a linear system of Boltzmann transport equations\nmodelling the evolution of three species of particles, photons, electrons and\npositrons. The system is coupled because of the collision term (an integral\noperator). The model is intended especially for dose calculation (forward\nproblem) in radiation therapy. It, however, does not apply to all relevant\ninteractions in its present form. We show under physically relevant assumptions\nthat the system has a unique solution in appropriate ($L^1$-based) spaces and\nthat the solution is non-negative when the data (internal source and inflow\nboundary source) is non-negative. In order to be self-contained as much as is\npractically possible, many (basic) results and proofs have been reproduced in\nthe paper. Existence, uniqueness and non-negativity of solutions for the\nrelated time-dependent coupled system are also proven. Moreover, we deal with\ninverse radiation treatment planning problem (inverse problem) as an optimal\ncontrol problem both for external and internal therapy (in general\n$L^p$-spaces). Especially, in the case $p=2$ variational equations for an\noptimal control related to an appropriate differentiable convex object function\nare verified. Its solution can be used as an initial point for an actual\n(global) optimization. \n\n"}
{"id": "1406.3665", "contents": "Title: Parallel Successive Convex Approximation for Nonsmooth Nonconvex\n  Optimization Abstract: Consider the problem of minimizing the sum of a smooth (possibly non-convex)\nand a convex (possibly nonsmooth) function involving a large number of\nvariables. A popular approach to solve this problem is the block coordinate\ndescent (BCD) method whereby at each iteration only one variable block is\nupdated while the remaining variables are held fixed. With the recent advances\nin the developments of the multi-core parallel processing technology, it is\ndesirable to parallelize the BCD method by allowing multiple blocks to be\nupdated simultaneously at each iteration of the algorithm. In this work, we\npropose an inexact parallel BCD approach where at each iteration, a subset of\nthe variables is updated in parallel by minimizing convex approximations of the\noriginal objective function. We investigate the convergence of this parallel\nBCD method for both randomized and cyclic variable selection rules. We analyze\nthe asymptotic and non-asymptotic convergence behavior of the algorithm for\nboth convex and non-convex objective functions. The numerical experiments\nsuggest that for a special case of Lasso minimization problem, the cyclic block\nselection rule can outperform the randomized rule. \n\n"}
{"id": "1406.3997", "contents": "Title: Implementation techniques for the SCFO experimental optimization\n  framework Abstract: The material presented in this document is intended as a comprehensive,\nimplementation-oriented supplement to the experimental optimization framework\npresented in a companion document. The issues of physical degradation, unknown\nLipschitz constants, measurement/estimation noise, gradient estimation,\nsufficient excitation, and the handling of soft constraints and/or a numerical\ncost function are all addressed, and a robust, implementable version of the\nsufficient conditions for feasible-side global convergence is proposed. \n\n"}
{"id": "1406.5378", "contents": "Title: Faa di Bruno Hopf Algebra of the Output Feedback Group for Multivariable\n  Fliess Operators Abstract: Given two nonlinear input-output systems written in terms of Chen-Fliess\nfunctional expansions, it is known that the feedback interconnected system is\nalways well defined and in the same class. An explicit formula for the\ngenerating series of a single-input, single-output closed-loop system was\nprovided by the first two authors in earlier work via Hopf algebra methods.\nThis paper is a sequel. It has four main innovations. First, the full\nmultivariable extension of the theory is presented. Next, a major\nsimplification of the basic set up is introduced using a new type of grading\nthat has recently appeared in the literature. This grading also facilitates a\nfully recursive algorithm to compute the antipode of the Hopf algebra of the\noutput feedback group, and thus, the corresponding feedback product can be\ncomputed much more efficiently. The final innovation is an improved convergence\nanalysis of the antipode operation, namely, the radius of convergence of the\nantipode is computed. \n\n"}
{"id": "1406.5839", "contents": "Title: Control of MTDC Transmission Systems under Local Information Abstract: High-voltage direct current (HVDC) is a commonly used technology for\nlong-distance electric power transmission, mainly due to its low resistive\nlosses. In this paper a distributed controller for multi-terminal high-voltage\ndirect current (MTDC) transmission systems is considered. Sufficient conditions\nfor when the proposed controller renders the closed-loop system asymptotically\nstable are provided. Provided that the closed loop system is asymptotically\nstable, it is shown that in steady-state a weighted average of the deviations\nfrom the nominal voltages is zero. Furthermore, a quadratic cost of the current\ninjections is minimized asymptotically. \n\n"}
{"id": "1406.7246", "contents": "Title: Modeling rationality to control self-organization of crowds: An\n  environmental approach Abstract: In this paper we propose a classification of crowd models in built\nenvironments based on the assumed pedestrian ability to foresee the movements\nof other walkers. At the same time, we introduce a new family of macroscopic\nmodels, which make it possible to tune the degree of predictiveness (i.e.,\nrationality) of the individuals. By means of these models we describe both the\nnatural behavior of pedestrians, i.e., their expected behavior according to\ntheir real limited predictive ability, and a target behavior, i.e., a\nparticularly efficient behavior one would like them to assume (for, e.g.,\nlogistic or safety reasons). Then we tackle a challenging shape optimization\nproblem, which consists in controlling the environment in such a way that the\nnatural behavior is as close as possible to the target one, thereby inducing\npedestrians to behave more rationally than what they would naturally do. We\npresent numerical tests which elucidate the role of rational/predictive\nabilities and show some promising results about the shape optimization problem. \n\n"}
{"id": "1406.7779", "contents": "Title: Analytical solution of the weighted Fermat-Torricelli problem for\n  tetrahedra: The case of two pairs of equal weights Abstract: The weighted Fermat-Torricelli problem for four non-collinear and\nnon-coplanar points in the three dimensional Euclidean Space states that: Given\nfour non-collinear and non-coplanar points A1, A2, A3, A4 and a positive real\nnumber (weight) Bi which correspond to each point Ai, for i = 1,2,3,4, find a\nfifth point such that the sum of the weighted distances to these four points is\nminimized. We present an analytical solution for the weighted Fermat-Torricelli\nproblem for tetrahedra in the three dimensional Euclidean Space for the case of\ntwo pairs of equal weights. \n\n"}
{"id": "1407.0214", "contents": "Title: A hybrid proximal-extragradient algorithm with inertial effects Abstract: We incorporate inertial terms in the hybrid proximal-extragradient algorithm\nand investigate the convergence properties of the resulting iterative scheme\ndesigned for finding the zeros of a maximally monotone operator in real Hilbert\nspaces. The convergence analysis relies on extended Fej\\'er monotonicity\ntechniques combined with the celebrated Opial Lemma. We also show that the\nclassical hybrid proximal-extragradient algorithm and the inertial versions of\nthe proximal point, the forward-backward and the forward-backward-forward\nalgorithms can be embedded in the framework of the proposed iterative scheme. \n\n"}
{"id": "1407.0585", "contents": "Title: Gap vectors of real projective varieties Abstract: Let $X\\subseteq \\mathbb{P}^m$ be a totally real, non-degenerate, projective\nvariety and let $\\Gamma\\subseteq X(\\mathbb{R})$ be a generic set of points. Let\n$P$ be the cone of nonnegative quadratic forms on $X$ and let $\\Sigma$ be the\ncone of sums of squares of linear forms. We examine the dimensions of the faces\n$P(\\Gamma)$ and $\\Sigma(\\Gamma)$ consisting of forms in $P$ and $\\Sigma$, which\nvanish on $\\Gamma$. As the cardinality of the set $\\Gamma$ varies in\n$1,\\dots,\\rm{codim}(X)$, the difference between the dimensions of $P(\\Gamma)$\nand $\\Sigma(\\Gamma)$ defines a numerical invariant of $X$, which we call the\ngap vector of X. In this article we begin a systematic study of its fundamental\nproperties. Our main result is a formula relating the components of the gap\nvector of $X$ and the quadratic deficiencies of $X$ and its generic\nprojections. The quadratic deficiency is a fundamental numerical invariant of\nprojective varieties introduced by F. L. Zak. The relationship between\nquadratic deficiency and gap vectors allows us to effectively compute the gap\nvectors of concrete varieties as well as to prove several general properties.\nWe prove that gap vectors are weakly increasing, obtain upper bounds for their\nrate of growth and prove that these upper bounds are eventually achieved for\nall varieties. Moreover, we give a characterization of the varieties with the\nsimplest gap vectors: We prove that the gap vector vanishes identically\nprecisely for varieties of minimal degree, and characterize the varieties whose\ngap vector equals $(0,\\dots, 0,1)$. In particular, our results give a new proof\nof the theorem of Blekherman, Smith and Velasco saying that there are\nnonnegative quadratic forms which are not sums of squares on every variety,\nwhich is not of minimal degree. Finally, we determine the gap vector of all\nVeronese embeddings of $\\mathbb{P}^2$, generalizing work of the first three\nauthors. \n\n"}
{"id": "1407.0748", "contents": "Title: Decay time integrals in neutral meson mixing and their efficient\n  evaluation Abstract: In neutral meson mixing, a certain class of convolution integrals is required\nwhose solution involves the error function $\\mathrm{erf}(z)$ of a complex\nargument $z$. We show the the general shape of the analytic solution of these\nintegrals, and give expressions which allow the normalisation of these\nexpressions for use in probability density functions. Furthermore, we derive\nexpressions which allow a (decay time) acceptance to be included in these\nintegrals, or allow the calculation of moments.\n  We also describe the implementation of numerical routines which allow the\nnumerical evaluation of $w(z)=e^{-z^2}(1-\\mathrm{erf}(-iz))$, sometimes also\ncalled Faddeeva function, in C++. These new routines improve over the old\nCERNLIB routine(s) WWERF/CWERF in terms of both speed and accuracy. These new\nroutines are part of the RooFit package, and have been distributed with it\nsince ROOT version 5.34/08. \n\n"}
{"id": "1407.6490", "contents": "Title: Multi-hop Diffusion LMS for Energy-constrained Distributed Estimation Abstract: We propose a multi-hop diffusion strategy for a sensor network to perform\ndistributed least mean-squares (LMS) estimation under local and network-wide\nenergy constraints. At each iteration of the strategy, each node can combine\nintermediate parameter estimates from nodes other than its physical neighbors\nvia a multi-hop relay path. We propose a rule to select combination weights for\nthe multi-hop neighbors, which can balance between the transient and the\nsteady-state network mean-square deviations (MSDs). We study two classes of\nnetworks: simple networks with a unique transmission path from one node to\nanother, and arbitrary networks utilizing diffusion consultations over at most\ntwo hops. We propose a method to optimize each node's information neighborhood\nsubject to local energy budgets and a network-wide energy budget for each\ndiffusion iteration. This optimization requires the network topology, and the\nnoise and data variance profiles of each node, and is performed offline before\nthe diffusion process. In addition, we develop a fully distributed and adaptive\nalgorithm that approximately optimizes the information neighborhood of each\nnode with only local energy budget constraints in the case where diffusion\nconsultations are performed over at most a predefined number of hops. Numerical\nresults suggest that our proposed multi-hop diffusion strategy achieves the\nsame steady-state MSD as the existing one-hop adapt-then-combine diffusion\nalgorithm but with a lower energy budget. \n\n"}
{"id": "1407.6661", "contents": "Title: Multistep stochastic mirror descent for risk-averse convex stochastic\n  programs based on extended polyhedral risk measures Abstract: We consider risk-averse convex stochastic programs expressed in terms of\nextended polyhedral risk measures. We derive computable confidence intervals on\nthe optimal value of such stochastic programs using the Robust Stochastic\nApproximation and the Stochastic Mirror Descent (SMD) algorithms. When the\nobjective functions are uniformly convex, we also propose a multistep extension\nof the Stochastic Mirror Descent algorithm and obtain confidence intervals on\nboth the optimal values and optimal solutions. Numerical simulations show that\nour confidence intervals are much less conservative and are quicker to compute\nthan previously obtained confidence intervals for SMD and that the multistep\nStochastic Mirror Descent algorithm can obtain a good approximate solution much\nquicker than its nonmultistep counterpart. Our confidence intervals are also\nmore reliable than asymptotic confidence intervals when the sample size is not\nmuch larger than the problem size. \n\n"}
{"id": "1407.7839", "contents": "Title: Semiampleness criteria for divisors on $\\overline M_{0,n}$ Abstract: We develop new characteristic-independent combinatorial criteria for\nsemiampleness of divisors on $\\overline{M}_{0,n}$. As an application, we\nassociate to a cyclic rational quadratic form satisfying a certain balancedness\ncondition an infinite sequence of semiample line bundles. We also give several\nsufficient and effective conditions for a symmetric divisor on\n$\\overline{M}_{0,n}$ to be semiample or nef. \n\n"}
{"id": "1407.8186", "contents": "Title: Exploration vs. Exploitation in the Information Filtering Problem Abstract: We consider information filtering, in which we face a stream of items too\nvoluminous to process by hand (e.g., scientific articles, blog posts, emails),\nand must rely on a computer system to automatically filter out irrelevant\nitems. Such systems face the exploration vs. exploitation tradeoff, in which it\nmay be beneficial to present an item despite a low probability of relevance,\njust to learn about future items with similar content. We present a Bayesian\nsequential decision-making model of this problem, show how it may be solved to\noptimality using a decomposition to a collection of two-armed bandit problems,\nand show structural results for the optimal policy. We show that the resulting\nmethod is especially useful when facing the cold start problem, i.e., when\nfiltering items for new users without a long history of past interactions. We\nthen present an application of this information filtering method to a\nhistorical dataset from the arXiv.org repository of scientific articles. \n\n"}
{"id": "1408.0578", "contents": "Title: A Cyclic Coordinate Descent Algorithm for lq Regularization Abstract: In recent studies on sparse modeling, $l_q$ ($0<q<1$) regularization has\nreceived considerable attention due to its superiorities on sparsity-inducing\nand bias reduction over the $l_1$ regularization.In this paper, we propose a\ncyclic coordinate descent (CCD) algorithm for $l_q$ regularization. Our main\nresult states that the CCD algorithm converges globally to a stationary point\nas long as the stepsize is less than a positive constant. Furthermore, we\ndemonstrate that the CCD algorithm converges to a local minimizer under certain\nadditional conditions. Our numerical experiments demonstrate the efficiency of\nthe CCD algorithm. \n\n"}
{"id": "1408.1434", "contents": "Title: Controlling of clock synchronization in WSNs: structure of optimal\n  solutions Abstract: Energy-saving optimization is very important for various engineering problems\nrelated to modern distributed systems. We consider here a control problem for a\nwireless sensor network with a single time server node and a large number of\nclient nodes. The problem is to minimize a functional which accumulates clock\nsynchronization errors in the clients nodes and the energy consumption of the\nserver over some time interval $[0,T]$. The control function $u=u(t)$, $0\\leq\nu(t)\\leq u_{1}$, corresponds to the power of the server node transmitting\nsynchronization signals to the clients. For all possible parameter values we\nfind the structure of optimal trajectories. We show that for sufficiently large\n$u_{1}$ the solutions contain singular arcs. \n\n"}
{"id": "1408.2043", "contents": "Title: Maxwell Strata and Conjugate Points in the Sub-Riemannian Problem on the\n  Lie Group SH(2) Abstract: We study local and global optimality of geodesics in the left invariant\nsub-Riemannian problem on the Lie group $\\mathrm{SH}(2)$. We obtain the\ncomplete description of the Maxwell points corresponding to the discrete\nsymmetries of the vertical subsystem of the Hamiltonian system. An effective\nupper bound on the cut time is obtained in terms of the first Maxwell times. We\nstudy the local optimality of extremal trajectories and prove the lower and\nupper bounds on the first conjugate times. We also obtain the generic time\ninterval for the $n$-th conjugate time which is important in the study of\nsub-Riemannian wavefront. Based on our results of $n$-th conjugate time and\n$n$-th Maxwell time, we prove a generalization of Rolle's theorem that between\nany two consecutive Maxwell points, there is exactly one conjugate point along\nany geodesic. \n\n"}
{"id": "1408.6372", "contents": "Title: Guaranteed control design under $L_p$-compact constraints on the\n  disturbance Abstract: The paper deals with the problem of optimization of a guaranteed (worst case)\nresult for a control system described by an ordinary differential equation. The\ndisturbances as functions of time are subject to functional constraints\nbelonging to a given family of constraints. The latter family is known to the\nside that forms the control actions. The controlling side uses positional\nfull-memory strategies and does not observe the disturbance. When the\nconstraints family consists of $L_p$-compact sets the optimal guaranteed result\nis non-improvable in the sense that it coincides with that obtained in the\nclass of quasi-strategies -- nonanticipatory transformations of disturbances\ninto controls.\n  In this paper for the effectiveness of implemented control algorithm an\nadditional condition on the system and appropriate ways of constructing an\noptimal strategy are specified. \n\n"}
{"id": "1408.6865", "contents": "Title: Handling uncertainties in background shapes: the discrete profiling\n  method Abstract: A common problem in data analysis is that the functional form, as well as the\nparameter values, of the underlying model which should describe a dataset is\nnot known a priori. In these cases some extra uncertainty must be assigned to\nthe extracted parameters of interest due to lack of exact knowledge of the\nfunctional form of the model. A method for assigning an appropriate error is\npresented. The method is based on considering the choice of functional form as\na discrete nuisance parameter which is profiled in an analogous way to\ncontinuous nuisance parameters. The bias and coverage of this method are shown\nto be good when applied to a realistic example. \n\n"}
{"id": "1409.0507", "contents": "Title: Model reproduces individual, group and collective dynamics of human\n  contact networks Abstract: Empirical data on the dynamics of human face-to-face interactions across a\nvariety of social venues have recently revealed a number of context-independent\nstructural and temporal properties of human contact networks. This universality\nsuggests that some basic mechanisms may be responsible for the unfolding of\nhuman interactions in the physical space. Here we discuss a simple model that\nreproduces the empirical distributions for the individual, group and collective\ndynamics of face-to-face contact networks. The model describes agents that move\nrandomly in a two-dimensional space and tend to stop when meeting \"attractive\"\npeers, and reproduces accurately the empirical distributions. \n\n"}
{"id": "1409.3257", "contents": "Title: Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk\n  Minimization Abstract: We consider a generic convex optimization problem associated with regularized\nempirical risk minimization of linear predictors. The problem structure allows\nus to reformulate it as a convex-concave saddle point problem. We propose a\nstochastic primal-dual coordinate (SPDC) method, which alternates between\nmaximizing over a randomly chosen dual variable and minimizing over the primal\nvariable. An extrapolation step on the primal variable is performed to obtain\naccelerated convergence rate. We also develop a mini-batch version of the SPDC\nmethod which facilitates parallel computing, and an extension with weighted\nsampling probabilities on the dual variables, which has a better complexity\nthan uniform sampling on unnormalized data. Both theoretically and empirically,\nwe show that the SPDC method has comparable or better performance than several\nstate-of-the-art optimization methods. \n\n"}
{"id": "1409.3928", "contents": "Title: Seasonality effects on Dengue: basic reproduction number, sensitivity\n  analysis and optimal control Abstract: Dengue is a vector-borne disease transmitted from an infected human to an\nAedes mosquito, during a blood-meal. Dengue is still a major public health\nproblem. A model for the disease transmission is presented, composed by human\nand mosquitoes compartments. The aim is to simulate the effects of seasonality,\non the vectorial capacity and, consequently, on the disease development. Using\nentomological information about the mosquito behavior under different\ntemperatures and rainfall, simulations are carried out and the repercussions\nanalyzed. The basic reproduction number of the model is given, as well as a\nsensitivity analysis of model's parameters. Finally, an optimal control problem\nis proposed and solved, illustrating the difficulty of making a trade-off\nbetween reduction of infected individuals and costs with insecticide. \n\n"}
{"id": "1409.7915", "contents": "Title: Dengue in Madeira Island Abstract: Dengue is a vector-borne disease and 40% of world population is at risk.\nDengue transcends international borders and can be found in tropical and\nsubtropical regions around the world, predominantly in urban and semi-urban\nareas. A model for dengue disease transmission, composed by mutually-exclusive\ncompartments representing the human and vector dynamics, is presented in this\nstudy. The data is from Madeira, a Portuguese island, where an unprecedented\noutbreak was detected on October 2012. The aim of this work is to simulate the\nrepercussions of the control measures in the fight of the disease. \n\n"}
{"id": "1409.8033", "contents": "Title: On the Convergence of Alternating Direction Lagrangian Methods for\n  Nonconvex Structured Optimization Problems Abstract: Nonconvex and structured optimization problems arise in many engineering\napplications that demand scalable and distributed solution methods. The study\nof the convergence properties of these methods is in general difficult due to\nthe nonconvexity of the problem. In this paper, two distributed solution\nmethods that combine the fast convergence properties of augmented\nLagrangian-based methods with the separability properties of alternating\noptimization are investigated. The first method is adapted from the classic\nquadratic penalty function method and is called the Alternating Direction\nPenalty Method (ADPM). Unlike the original quadratic penalty function method,\nin which single-step optimizations are adopted, ADPM uses an alternating\noptimization, which in turn makes it scalable. The second method is the\nwell-known Alternating Direction Method of Multipliers (ADMM). It is shown that\nADPM for nonconvex problems asymptotically converges to a primal feasible point\nunder mild conditions and an additional condition ensuring that it\nasymptotically reaches the standard first order necessary conditions for local\noptimality are introduced. In the case of the ADMM, novel sufficient conditions\nunder which the algorithm asymptotically reaches the standard first order\nnecessary conditions are established. Based on this, complete convergence of\nADMM for a class of low dimensional problems are characterized. Finally, the\nresults are illustrated by applying ADPM and ADMM to a nonconvex localization\nproblem in wireless sensor networks. \n\n"}
{"id": "1410.1190", "contents": "Title: A General Delta-Nabla Calculus of Variations on Time Scales with\n  Application to Economics Abstract: We consider a general problem of the calculus of variations on time scales\nwith a cost functional that is the composition of a certain scalar function\nwith delta and nabla integrals of a vector valued field. Euler-Lagrange\ndelta-nabla differential equations are proved, which lead to important insights\nin the process of discretization. Application of the obtained results to a firm\nthat wants to program its production and investment policies to reach a given\nproduction rate and to maximize its future market competitiveness is discussed. \n\n"}
{"id": "1410.3898", "contents": "Title: An ADMM Algorithm for Clustering Partially Observed Networks Abstract: Community detection has attracted increasing attention during the past\ndecade, and many algorithms have been proposed to find the underlying community\nstructure in a given network. Many of these algorithms are based on modularity\nmaximization, and these methods suffer from the resolution limit. In order to\ndetect the underlying cluster structure, we propose a new convex formulation to\ndecompose a partially observed adjacency matrix of a network into low-rank and\nsparse components. In such decomposition, the low-rank component encodes the\ncluster structure under certain assumptions. We also devise an alternating\ndirection method of multipliers with increasing penalty sequence to solve this\nproblem; and compare it with Louvain method, which maximizes the modularity, on\nsome synthetic randomly generated networks. Numerical results show that our\nmethod outperforms Louvain method on the randomly generated networks when\nvariance among cluster sizes increases. Moreover, empirical results also\ndemonstrate that our formulation is indeed tighter than the robust PCA\nformulation, and is able to find the true clustering when the robust PCA\nformulation fails. \n\n"}
{"id": "1410.4962", "contents": "Title: Robust Fundamental Theorem for Continuous Processes Abstract: We study a continuous-time financial market with continuous price processes\nunder model uncertainty, modeled via a family $\\mathcal{P}$ of possible\nphysical measures. A robust notion ${\\rm NA}_{1}(\\mathcal{P})$ of no-arbitrage\nof the first kind is introduced; it postulates that a nonnegative, nonvanishing\nclaim cannot be superhedged for free by using simple trading strategies. Our\nfirst main result is a version of the fundamental theorem of asset pricing:\n${\\rm NA}_{1}(\\mathcal{P})$ holds if and only if every $P\\in\\mathcal{P}$ admits\na martingale measure which is equivalent up to a certain lifetime. The second\nmain result provides the existence of optimal superhedging strategies for\ngeneral contingent claims and a representation of the superhedging price in\nterms of martingale measures. \n\n"}
{"id": "1410.5179", "contents": "Title: A surgery result for the spectrum of the Dirichlet Laplacian Abstract: In this paper we give a method to geometrically modify an open set such that\nthe first $k$ eigenvalues of the Dirichlet Laplacian and its perimeter are not\nincreasing, its measure remains constant, and both perimeter and diameter\ndecrease below a certain threshold. The key point of the analysis relies on the\nproperties of the shape subsolutions for the torsion energy. \n\n"}
{"id": "1410.6836", "contents": "Title: Reducing Cascading Failure Risk by Increasing Infrastructure Network\n  Interdependency Abstract: Increased coupling between critical infrastructure networks, such as power\nand communication systems, will have important implications for the reliability\nand security of these systems. To understand the effects of power-communication\ncoupling, several have studied interdependent network models and reported that\nincreased coupling can increase system vulnerability. However, these results\ncome from models that have substantially different mechanisms of cascading,\nrelative to those found in actual power and communication networks. This paper\nreports on two sets of experiments that compare the network vulnerability\nimplications resulting from simple topological models and models that more\naccurately capture the dynamics of cascading in power systems. First, we\ncompare a simple model of topological contagion to a model of cascading in\npower systems and find that the power grid shows a much higher level of\nvulnerability, relative to the contagion model. Second, we compare a model of\ntopological cascades in coupled networks to three different physics-based\nmodels of power grids coupled to communication networks. Again, the more\naccurate models suggest very different conclusions. In all but the most extreme\ncase, the physics-based power grid models indicate that increased\npower-communication coupling decreases vulnerability. This is opposite from\nwhat one would conclude from the coupled topological model, in which zero\ncoupling is optimal. Finally, an extreme case in which communication failures\nimmediately cause grid failures, suggests that if systems are poorly designed,\nincreased coupling can be harmful. Together these results suggest design\nstrategies for reducing the risk of cascades in interdependent infrastructure\nsystems. \n\n"}
{"id": "1411.0416", "contents": "Title: Spatio-Temporal Analysis of Epidemic Phenomena Using the R Package\n  surveillance Abstract: The availability of geocoded health data and the inherent temporal structure\nof communicable diseases have led to an increased interest in statistical\nmodels and software for spatio-temporal data with epidemic features. The open\nsource R package surveillance can handle various levels of aggregation at which\ninfective events have been recorded: individual-level time-stamped\ngeo-referenced data (case reports) in either continuous space or discrete\nspace, as well as counts aggregated by period and region. For each of these\ndata types, the surveillance package implements tools for visualization,\nlikelihoood inference and simulation from recently developed statistical\nregression frameworks capturing endemic and epidemic dynamics. Altogether, this\npaper is a guide to the spatio-temporal modeling of epidemic phenomena,\nexemplified by analyses of public health surveillance data on measles and\ninvasive meningococcal disease. \n\n"}
{"id": "1411.1701", "contents": "Title: The Hierarchy of Circuit Diameters and Transportation Polytopes Abstract: The study of the diameter of the graph of polyhedra is a classical problem in\nthe theory of linear programming. While transportation polytopes are at the\ncore of operations research and statistics it is still open whether the Hirsch\nconjecture is true for general $m{\\times}n$--transportation polytopes. In\nearlier work the first three authors introduced a hierarchy of variations to\nthe notion of graph diameter in polyhedra. The key reason was that this\nhierarchy provides some interesting lower bounds for the usual graph diameter.\n  This paper has three contributions: First, we compare the hierarchy of\ndiameters for the $m{\\times}n$--transportation polytopes. We show that the\nHirsch conjecture bound of $m+n-1$ is actually valid in most of these diameter\nnotions. Second, we prove that for $3{\\times}n$--transportation polytopes the\nHirsch conjecture holds in the classical graph diameter. Third, we show for\n$2{\\times}n$--transportation polytopes that the stronger monotone Hirsch\nconjecture holds and improve earlier bounds on the graph diameter. \n\n"}
{"id": "1411.2003", "contents": "Title: Efficient Estimation of Mutual Information for Strongly Dependent\n  Variables Abstract: We demonstrate that a popular class of nonparametric mutual information (MI)\nestimators based on k-nearest-neighbor graphs requires number of samples that\nscales exponentially with the true MI. Consequently, accurate estimation of MI\nbetween two strongly dependent variables is possible only for prohibitively\nlarge sample size. This important yet overlooked shortcoming of the existing\nestimators is due to their implicit reliance on local uniformity of the\nunderlying joint distribution. We introduce a new estimator that is robust to\nlocal non-uniformity, works well with limited data, and is able to capture\nrelationship strengths over many orders of magnitude. We demonstrate the\nsuperior performance of the proposed estimator on both synthetic and real-world\ndata. \n\n"}
{"id": "1411.2978", "contents": "Title: Universal freezing of quantum correlations within the geometric approach Abstract: Quantum correlations in a composite system can be measured by resorting to a\ngeometric approach, according to which the distance from the state of the\nsystem to a suitable set of classically correlated states is considered. Here\nwe show that all distance functions, which respect natural assumptions of\ninvariance under transposition, convexity, and contractivity under quantum\nchannels, give rise to geometric quantifiers of quantum correlations which\nexhibit the peculiar freezing phenomenon, i.e., remain constant during the\nevolution of a paradigmatic class of states of two qubits each independently\ninteracting with a non-dissipative decohering environment. Our results\ndemonstrate from first principles that freezing of geometric quantum\ncorrelations is independent of the adopted distance and therefore universal.\nThis finding paves the way to a deeper physical interpretation and future\npractical exploitation of the phenomenon for noisy quantum technologies. \n\n"}
{"id": "1411.3224", "contents": "Title: On TD(0) with function approximation: Concentration bounds and a\n  centered variant with exponential convergence Abstract: We provide non-asymptotic bounds for the well-known temporal difference\nlearning algorithm TD(0) with linear function approximators. These include\nhigh-probability bounds as well as bounds in expectation. Our analysis suggests\nthat a step-size inversely proportional to the number of iterations cannot\nguarantee optimal rate of convergence unless we assume (partial) knowledge of\nthe stationary distribution for the Markov chain underlying the policy\nconsidered. We also provide bounds for the iterate averaged TD(0) variant,\nwhich gets rid of the step-size dependency while exhibiting the optimal rate of\nconvergence. Furthermore, we propose a variant of TD(0) with linear\napproximators that incorporates a centering sequence, and establish that it\nexhibits an exponential rate of convergence in expectation. We demonstrate the\nusefulness of our bounds on two synthetic experimental settings. \n\n"}
{"id": "1411.3816", "contents": "Title: Monte Carlo error analyses of Spearman's rank test Abstract: Spearman's rank correlation test is commonly used in astronomy to discern\nwhether a set of two variables are correlated or not. Unlike most other\nquantities quoted in astronomical literature, the Spearman's rank correlation\ncoefficient is generally quoted with no attempt to estimate the errors on its\nvalue. This is a practice that would not be accepted for those other\nquantities, as it is often regarded that an estimate of a quantity without an\nestimate of its associated uncertainties is meaningless. This manuscript\ndescribes a number of easily implemented, Monte Carlo based methods to estimate\nthe uncertainty on the Spearman's rank correlation coefficient, or more\nprecisely to estimate its probability distribution. \n\n"}
{"id": "1411.4031", "contents": "Title: Angular Power Spectra with Finite Counts Abstract: Angular anisotropy techniques for cosmic diffuse radiation maps are powerful\nprobes, even for quite small data sets. A popular observable is the angular\npower spectrum; we present a detailed study applicable to any unbinned source\nskymap S(n) from which N random, independent events are observed. Its exact\nvariance, which is due to the finite statistics, depends only on S(n) and N; we\nalso derive an unbiased estimator of the variance from the data. First-order\neffects agree with previous analytic estimates. Importantly, heretofore\nunidentified higher-order effects are found to contribute to the variance and\nmay cause the uncertainty to be significantly larger than previous analytic\nestimates---potentially orders of magnitude larger. Neglect of these\nhigher-order terms, when significant, may result in a spurious detection of the\npower spectrum. On the other hand, this would indicate the presence of\nhigher-order spatial correlations, such as a large bispectrum, providing new\nclues about the sources. Numerical simulations are shown to support these\nconclusions. Applying the formalism to an ensemble of Gaussian-distributed\nskymaps, the noise-dominated part of the power spectrum uncertainty is\nsignificantly increased at high multipoles by the new, higher-order effects.\nThis work is important for harmonic analyses of the distributions of diffuse\nhigh-energy gamma-rays, neutrinos, and charged cosmic rays, as well as for\npopulations of sparse point sources such as active galactic nuclei. \n\n"}
{"id": "1411.4527", "contents": "Title: Given enough choice, simple local rules percolate discontinuously Abstract: There is still much to discover about the mechanisms and nature of\ndiscontinuous percolation transitions. Much of the past work considers graph\nevolution algorithms known as Achlioptas processes in which a single edge is\nadded to the graph from a set of $k$ randomly chosen candidate edges at each\ntimestep until a giant component emerges. Several Achlioptas processes seem to\nyield a discontinuous percolation transition, but it was proven by Riordan and\nWarnke that the transition must be continuous in the thermodynamic limit.\nHowever, they also proved that if the number $k(n)$ of candidate edges\nincreases with the number of nodes, then the percolation transition may be\ndiscontinuous. Here we attempt to find the simplest such process which yields a\ndiscontinuous transition in the thermodynamic limit. We introduce a process\nwhich considers only the degree of candidate edges and not component size. We\ncalculate the critical point $t_{c}=(1-\\theta(\\frac{1}{k}))n$ and rigorously\nshow that the critical window is of size $O(\\frac{n}{k(n)})$. If $k(n)$ grows\nvery slowly, for example $k(n)=\\log n$, the critical window is barely sublinear\nand hence the phase transition is discontinuous but appears continuous in\nfinite systems. We also present arguments that Achlioptas processes with\nbounded size rules will always have continuous percolation transitions even\nwith infinite choice. \n\n"}
{"id": "1411.6245", "contents": "Title: Statistical criteria for possible indications of new physics in tritium\n  $\\beta$-decay spectrum Abstract: The method of quasi-optimal weights is applied to constructing\n(quasi-)optimal criteria for various anomalous contributions in experimental\nspectra. Anomalies in the spectra could indicate physics beyond the Standard\nModel (additional interactions and neutrino flavours, Lorenz violation etc.).\nIn particular the cumulative tritium $\\beta$-decay spectrum (for instance, in\nTroitsk-$\\nu$-mass, Mainz Neutrino Mass and KATRIN experiments) is analysed\nusing the derived special criteria. Using the power functions we show that the\nderived quasi-optimal criteria are efficient statistical instruments for\ndetecting the anomalous contributions in the spectra. \n\n"}
{"id": "1412.4577", "contents": "Title: Log-log Convexity of Type-Token Growth in Zipf's Systems Abstract: It is traditionally assumed that Zipf's law implies the power-law growth of\nthe number of different elements with the total number of elements in a system\n- the so-called Heaps' law. We show that a careful definition of Zipf's law\nleads to the violation of Heaps' law in random systems, and obtain alternative\ngrowth curves. These curves fulfill universal data collapses that only depend\non the value of the Zipf's exponent. We observe that real books behave very\nmuch in the same way as random systems, despite the presence of burstiness in\nword occurrence. We advance an explanation for this unexpected correspondence. \n\n"}
{"id": "1412.5630", "contents": "Title: Short vs. Long Gamma-Ray Bursts: A Comprehensive Study of Energetics and\n  Prompt Gamma-Ray Correlations Abstract: We present the results of a comprehensive study of the luminosity function,\nenergetics, prompt gamma-ray correlations, and classification methodology of\nshort-hard and long-soft GRBs (SGRBs and LGRBs), based on observational data in\nthe largest catalog of GRBs available to this date: BATSE catalog of 2702 GRBs.\nWe find that: 1. The least-biased classification method of GRBs into short and\nlong, solely based on prompt-emission properties, appears to be the ratio of\nthe observed spectral peak energy to the observed duration ($R=E_p/T_{90}$)\nwith the dividing line at $R\\simeq50[keV~s^{-1}]$. 2. Once data is carefully\ncorrected for the effects of the detection threshold of gamma-ray instruments,\nthe population distribution of SGRBs and LGRBs can be individually well\ndescribed as multivariate log-normal distribution in the $4$--dimensional space\nof the isotropic peak gamma-ray luminosity, total isotropic gamma-ray emission,\nthe intrinsic spectral peak energy, and the intrinsic duration. 3. Relatively\nlarge fractions of SGRBs and LGRBs with moderate-to-low spectral peak energies\nhave been missed by BATSE detectors. 4. Relatively strong and highly\nsignificant intrinsic hardness--brightness and duration--brightness\ncorrelations likely exist in both populations of SGRBs and LGRBs, once data is\ncorrected for selection effects. The strengths of these correlations are very\nsimilar in both populations, implying similar mechanisms at work in both GRB\nclasses, leading to the emergence of these prompt gamma-ray correlations. \n\n"}
{"id": "1412.5691", "contents": "Title: An improved upper bound on the diameters of subset partition graphs Abstract: In 1992, Kalai and Kleitman proved the first subexponential upper bound for\nthe diameters of convex polyhedra. Eisenbrand et al. proved this bound holds\nfor connected layer families, a novel approach to analyzing polytope diameters.\nVery recently, Todd improved the Kalai-Kleitman bound for polyhedra to\n$(n-d)^{1+\\log_2d}$. In this note, we prove an analogous upper bound on the\ndiameters of subset partition graphs satisfying a property related to the\nconnectivity property of connected layer families. \n\n"}
{"id": "1412.7399", "contents": "Title: Do quantum strategies always win? Abstract: In a seminal paper, Meyer [David Meyer, Phys. Rev. Lett. 82, 1052 (1999)]\ndescribed the advantages of quantum game theory by looking at the classical\npenny flip game. A player using a quantum strategy can win against a classical\nplayer almost 100\\% of the time. Here we make a slight modification to the\nquantum game, with the two players sharing an entangled state to begin with. We\nthen analyze two different scenarios, first in which quantum player makes\nunitary transformations to his qubit while the classical player uses a pure\nstrategy of either flipping or not flipping the state of his qubit. In this\ncase the quantum player always wins against the classical player. In the second\nscenario we have the quantum player making similar unitary transformations\nwhile the classical player makes use of a mixed strategy wherein he either\nflips or not with some probability \"p\". We show that in the second scenario,\n100\\% win record of a quantum player is drastically reduced and for a\nparticular probability \"p\" the classical player can even win against the\nquantum player. This is of possible relevance to the field of quantum\ncomputation as we show that in this quantum game of preserving versus\ndestroying entanglement a particular classical algorithm can beat the quantum\nalgorithm. \n\n"}
{"id": "1501.01058", "contents": "Title: Characterizing Real-Valued Multivariate Complex Polynomials and Their\n  Symmetric Tensor Representations Abstract: In this paper we study multivariate polynomial functions in complex variables\nand the corresponding associated symmetric tensor representations. The focus is\non finding conditions under which such complex polynomials/tensors always take\nreal values. We introduce the notion of symmetric conjugate forms and general\nconjugate forms, and present characteristic conditions for such complex\npolynomials to be real-valued. As applications of our results, we discuss the\nrelation between nonnegative polynomials and sums of squares in the context of\ncomplex polynomials. Moreover, new notions of eigenvalues/eigenvectors for\ncomplex tensors are introduced, extending properties from the Hermitian\nmatrices. Finally, we discuss an important property for symmetric tensors,\nwhich states that the largest absolute value of eigenvalue of a symmetric real\ntensor is equal to its largest singular value; the result is known as Banach's\ntheorem. We show that a similar result holds in the complex case as well. \n\n"}
{"id": "1501.01324", "contents": "Title: An Evolution Strategy Method for Optimizing Machining Parameters of\n  Milling Operations Abstract: In this paper, an evolutionary strategy (ES) method is introduced as an\noptimization approach to solve problems in the manufacturing area. The ES\nmethod is applied to a case study for milling operations. The results show that\nit can effectively yield good results. \n\n"}
{"id": "1501.01528", "contents": "Title: The average number of distinct sites visited by a random walker on\n  random graphs Abstract: We study the linear large $n$ behavior of the average number of distinct\nsites $S(n)$ visited by a random walker after $n$ steps on a large random\ngraph. An expression for the graph topology dependent prefactor $B$ in $S(n) =\nBn$ is proposed. We use generating function techniques to relate this prefactor\nto the graph adjacency matrix and then devise message-passing equations to\ncalculate its value. Numerical simulations are performed to evaluate the\nagreement between the message passing predictions and random walk simulations\non random graphs. Scaling with system size and average graph connectivity are\nalso analysed. \n\n"}
{"id": "1501.04352", "contents": "Title: Solving the Infinite-horizon Constrained LQR Problem using Accelerated\n  Dual Proximal Methods Abstract: This work presents an algorithmic scheme for solving the infinite-time\nconstrained linear quadratic regulation problem. We employ an accelerated\nversion of a popular proximal gradient scheme, commonly known as the\nForward-Backward Splitting (FBS), and prove its convergence to the optimal\nsolution in our infinite-dimensional setting. Each iteration of the algorithm\nrequires only finite memory, is computationally cheap, and makes no use of\nterminal invariant sets; hence, the algorithm can be applied to systems of very\nlarge dimensions. The acceleration brings in optimal convergence rates O(1/k^2)\nfor function values and O(1/k) for primal iterates and renders the proposed\nmethod a practical alternative to model predictive control schemes for setpoint\ntracking. In addition, for the case when the true system is subject to\ndisturbances or modelling errors, we propose an efficient warm-starting\nprocedure, which significantly reduces the number of iterations when the\nalgorithm is applied in closed-loop. Numerical examples demonstrate the\napproach. \n\n"}
{"id": "1501.04573", "contents": "Title: On the stability of cycles by delayed feedback control Abstract: We present a delayed feedback control (DFC) mechanism for stabilizing cycles\nof one dimensional discrete time systems. In particular, we consider a delayed\nfeedback control for stabilizing $T$-cycles of a differentiable function $f:\n\\mathbb{R}\\rightarrow\\mathbb{R}$ of the form $$x(k+1) = f(x(k)) + u(k)$$ where\n$$u(k) = (a_1 - 1)f(x(k)) + a_2 f(x(k-T)) + ... + a_N f(x(k-(N-1)T))\\;,$$ with\n$a_1 + ... + a_N = 1$. Following an approach of Morg\\\"ul, we construct a map\n$F: \\mathbb{R}^{T+1} \\rightarrow \\mathbb{R}^{T+1}$ whose fixed points\ncorrespond to $T$-cycles of $f$. We then analyze the local stability of the\nabove DFC mechanism by evaluating the stability of the corresponding equilibrum\npoints of $F$. We associate to each periodic orbit of $f$ an explicit\npolynomial whose Schur stability corresponds to the stability of the DFC on\nthat orbit. An example indicating the efficacy of this method is provided. \n\n"}
{"id": "1501.05923", "contents": "Title: Cheeger $N$-clusters Abstract: In this paper we introduce a Cheeger-type constant defined as a minimization\nof a suitable functional among all the $N$-clusters contained in an open\nbounded set $\\Omega$. Here with $N$-Cluster we mean a family of $N$ sets of\nfinite perimeter, disjoint up to a set of null Lebesgue measure. We call any\n$N$-cluster attaining such a minimum a Cheeger $N$-cluster. Our purpose is to\nprovide a non trivial lower bound on the optimal partition problem for the\nfirst Dirichlet eigenvalue of the Laplacian. Here we discuss the regularity of\nCheeger $N$-clusters in a general ambient space dimension and we give a precise\ndescription of their structure in the the planar case. The last part is devoted\nto the relation between the functional introduced here (namely the $N$-Cheeger\nconstant), the partition problem for the first Dirichlet eigenvalue of the\nLaplacian and the Caffarelli and Lin's conjecture. \n\n"}
{"id": "1501.07668", "contents": "Title: Sloppiness and Emergent Theories in Physics, Biology, and Beyond Abstract: Large scale models of physical phenomena demand the development of new\nstatistical and computational tools in order to be effective. Many such models\nare `sloppy', i.e., exhibit behavior controlled by a relatively small number of\nparameter combinations. We review an information theoretic framework for\nanalyzing sloppy models. This formalism is based on the Fisher Information\nMatrix, which we interpret as a Riemannian metric on a parameterized space of\nmodels. Distance in this space is a measure of how distinguishable two models\nare based on their predictions. Sloppy model manifolds are bounded with a\nhierarchy of widths and extrinsic curvatures. We show how the manifold boundary\napproximation can extract the simple, hidden theory from complicated sloppy\nmodels. We attribute the success of simple effective models in physics as\nlikewise emerging from complicated processes exhibiting a low effective\ndimensionality. We discuss the ramifications and consequences of sloppy models\nfor biochemistry and science more generally. We suggest that the reason our\ncomplex world is understandable is due to the same fundamental reason: simple\ntheories of macroscopic behavior are hidden inside complicated microscopic\nprocesses. \n\n"}
{"id": "1501.07668", "contents": "Title: Sloppiness and Emergent Theories in Physics, Biology, and Beyond Abstract: Large scale models of physical phenomena demand the development of new\nstatistical and computational tools in order to be effective. Many such models\nare `sloppy', i.e., exhibit behavior controlled by a relatively small number of\nparameter combinations. We review an information theoretic framework for\nanalyzing sloppy models. This formalism is based on the Fisher Information\nMatrix, which we interpret as a Riemannian metric on a parameterized space of\nmodels. Distance in this space is a measure of how distinguishable two models\nare based on their predictions. Sloppy model manifolds are bounded with a\nhierarchy of widths and extrinsic curvatures. We show how the manifold boundary\napproximation can extract the simple, hidden theory from complicated sloppy\nmodels. We attribute the success of simple effective models in physics as\nlikewise emerging from complicated processes exhibiting a low effective\ndimensionality. We discuss the ramifications and consequences of sloppy models\nfor biochemistry and science more generally. We suggest that the reason our\ncomplex world is understandable is due to the same fundamental reason: simple\ntheories of macroscopic behavior are hidden inside complicated microscopic\nprocesses. \n\n"}
{"id": "1502.02074", "contents": "Title: Counting real critical points of the distance to orthogonally invariant\n  matrix sets Abstract: Minimizing the Euclidean distance to a set arises frequently in applications.\nWhen the set is algebraic, a measure of complexity of this optimization problem\nis its number of critical points. In this paper we provide a general framework\nto compute and count the real smooth critical points of a data matrix on an\northogonally invariant set of matrices. The technique relies on \"transfer\nprinciples\" that allow calculations to be done in the space of singular values\nof the matrices in the orthogonally invariant set. The calculations often\nsimplify greatly and yield transparent formulas. We illustrate the method on\nseveral examples, and compare our results to the recently introduced notion of\nEuclidean distance degree of an algebraic variety. \n\n"}
{"id": "1502.04846", "contents": "Title: Variational analysis and regularity of the minimum time function for\n  differential inclusions Abstract: We study the time optimal control problem for differential inclusions with a\ngeneral closed target. We first give the representation of the proximal\nhorizontal subgradients of the minimum time function $\\mathcal{T}$ and then,\ntogether with the representation of the proximal subgradients, we obtain some\nrelationships between the normal cones to the sublevel of $\\mathcal{T}$ and the\nnormal cones to its epigraph. The relationships allow us to get the propagation\nof the proximal subdifferential as well as of the proximal horizontal\nsubdifferential of $\\mathcal{T}$ along optimal trajectories. Finally, we show,\nunder suitable assumptions, that the epigraph of $\\mathcal{T}$ is\n$\\varphi$-convex near the target. This is the first nonlinear\n$\\varphi$-convexity result valid in any dimension. \n\n"}
{"id": "1502.07328", "contents": "Title: Combined Top-down and Bottom-up Approach to Multilevel Supervisory\n  Control Abstract: Recently, we have proposed two complementary approaches, top-down and\nbottom-up, to multilevel supervisory control of discrete-event systems. In this\npaper, we compare and combine these approaches. The combined approach has\nstrong features of both approaches, namely, a lower complexity of the top-down\napproach with the generality of the bottom-up approach. We show that, for\nprefix-closed languages, a posteriori supervisors computed in the bottom-up\nmanner do not alter maximal permissiveness within the three-level coordination\ncontrol architecture, that is, the supremal three-level\nconditionally-controllable and conditionally-normal language can always be\ncomputed in a distributed way using multilevel coordination. Moreover, a\ngeneral polynomial-time procedure for non-prefix closed case is proposed based\non coordinators for nonblockingness and a posteriori supervisors. \n\n"}
{"id": "1502.07820", "contents": "Title: Structure Learning and Statistical Estimation in Distribution Networks -\n  Part II Abstract: Part I of this paper discusses the problem of learning the operational\nstructure of the grid from nodal voltage measurements. In this work (Part II),\nthe learning of the operational radial structure is coupled with the problem of\nestimating nodal consumption statistics and inferring the line parameters in\nthe grid. Based on a Linear-Coupled (LC) approximation of AC power flows\nequations, polynomial time algorithms are designed to complete these tasks\nusing the available nodal complex voltage measurements. Then the structure\nlearning algorithm is extended to cases with missing data, where available\nobservations are limited to a fraction of the grid nodes. The efficacy of the\npresented algorithms are demonstrated through simulations on several\ndistribution test cases. \n\n"}
{"id": "1503.00765", "contents": "Title: LDDMM Surface Registration with Atrophy Constraints Abstract: Diffeomorphic registration using optimal control on the diffeomorphism group\nand on shape spaces has become widely used since the development of the Large\nDeformation Diffeomorphic Metric Mapping (LDDMM) algorithm. More recently, a\nseries of algorithms involving sub-riemannian constraints have been introduced,\nin which the velocity fields that control the shapes in the LDDMM framework are\nconstrained in accordance with a specific deformation model. Here, we extend\nthis setting by considering, for the first time, inequality constraints, in\norder to estimate surface deformations that only allow for atrophy, introducing\nfor this purpose an algorithm that uses the augmented lagrangian method. We\nalso provide a version of our approach that uses a weaker constraint in which\nonly the total volume is forced to decrease. These developments are illustrated\nby numerical experiments on brain data. \n\n"}
{"id": "1503.01129", "contents": "Title: Complexity and universality in the long-range order of words Abstract: As is the case of many signals produced by complex systems, language presents\na statistical structure that is balanced between order and disorder. Here we\nreview and extend recent results from quantitative characterisations of the\ndegree of order in linguistic sequences that give insights into two relevant\naspects of language: the presence of statistical universals in word ordering,\nand the link between semantic information and the statistical linguistic\nstructure. We first analyse a measure of relative entropy that assesses how\nmuch the ordering of words contributes to the overall statistical structure of\nlanguage. This measure presents an almost constant value close to 3.5 bits/word\nacross several linguistic families. Then, we show that a direct application of\ninformation theory leads to an entropy measure that can quantify and extract\nsemantic structures from linguistic samples, even without prior knowledge of\nthe underlying language. \n\n"}
{"id": "1503.01457", "contents": "Title: Time Averaged Consensus in a Direct Coupled Distributed Coherent Quantum\n  Observer Abstract: This paper considers the problem of constructing a distributed direct\ncoupling quantum observer for a closed linear quantum system. The proposed\ndistributed observer consists of a network of quantum harmonic oscillators and\nit is shown that the distributed observer converges to a consensus in a time\naveraged sense in which each component of the observer estimates the specified\noutput of the quantum plant. An example and simulations are included to\nillustrate the properties of the distributed observer. \n\n"}
{"id": "1503.01563", "contents": "Title: Convex Optimization for Parallel Energy Minimization Abstract: Energy minimization has been an intensely studied core problem in computer\nvision. With growing image sizes (2D and 3D), it is now highly desirable to run\nenergy minimization algorithms in parallel. But many existing algorithms, in\nparticular, some efficient combinatorial algorithms, are difficult to\npar-allelize. By exploiting results from convex and submodular theory, we\nreformulate the quadratic energy minimization problem as a total variation\ndenoising problem, which, when viewed geometrically, enables the use of\nprojection and reflection based convex methods. The resulting min-cut algorithm\n(and code) is conceptually very simple, and solves a sequence of TV denoising\nproblems. We perform an extensive empirical evaluation comparing\nstate-of-the-art combinatorial algorithms and convex optimization techniques.\nOn small problems the iterative convex methods match the combinatorial max-flow\nalgorithms, while on larger problems they offer other flexibility and important\ngains: (a) their memory footprint is small; (b) their straightforward\nparallelizability fits multi-core platforms; (c) they can easily be\nwarm-started; and (d) they quickly reach approximately good solutions, thereby\nenabling faster \"inexact\" solutions. A key consequence of our approach based on\nsubmodularity and convexity is that it is allows to combine any arbitrary\ncombinatorial or convex methods as subroutines, which allows one to obtain\nhybrid combinatorial and convex optimization algorithms that benefit from the\nstrengths of both. \n\n"}
{"id": "1503.01848", "contents": "Title: SDP-based Joint Sensor and Controller Design for Information-regularized\n  Optimal LQG Control Abstract: We consider a joint sensor and controller design problem for linear Gaussian\nstochastic systems in which a weighted sum of quadratic control cost and the\namount of information acquired by the sensor is minimized. This problem\nformulation is motivated by situations where a control law must be designed in\nthe presence of sensing, communication, and privacy constraints. We show that\nthe optimal joint sensor-controller design is relatively easy when the sensing\npolicy is restricted to be linear. Namely, an explicit form of the optimal\nlinear sensor equation, the Kalman filter, and the certainty equivalence\ncontroller that jointly solves the problem can be efficiently found by\nsemidefinite programming (SDP). Whether the linearity assumption in our design\nis restrictive or not is currently an open problem. \n\n"}
{"id": "1503.02764", "contents": "Title: Minimum Cost Constrained Input-Output and Control Configuration\n  Co-Design Problem: A Structural Systems Approach Abstract: In this paper, we study the minimal cost constrained input-output (I/O) and\ncontrol configuration co-design problem. Given a linear time-invariant plant,\nwhere a collection of possible inputs and outputs is known a priori, we aim to\ndetermine the collection of inputs, outputs and communication among them\nincurring in the minimum cost, such that desired control performance, measured\nin terms of arbitrary pole-placement capability of the closed-loop system, is\nensured. We show that this problem is NP-hard in general (in the size of the\nstate space). However, the subclass of problems, in which the dynamic matrix is\nirreducible, is shown to be polynomially solvable and the corresponding\nalgorithm is presented. In addition, under the same assumption, the same\nalgorithm can be used to solve the minimal cost constrained I/O selection\nproblem, and the minimal cost control configuration selection problem,\nindividually. In order to illustrate the main results of this paper, some\nsimulations are also provided. \n\n"}
{"id": "1503.03231", "contents": "Title: Adaptive-Rate Sparse Signal Reconstruction With Application in\n  Compressive Background Subtraction Abstract: We propose and analyze an online algorithm for reconstructing a sequence of\nsignals from a limited number of linear measurements. The signals are assumed\nsparse, with unknown support, and evolve over time according to a generic\nnonlinear dynamical model. Our algorithm, based on recent theoretical results\nfor $\\ell_1$-$\\ell_1$ minimization, is recursive and computes the number of\nmeasurements to be taken at each time on-the-fly. As an example, we apply the\nalgorithm to compressive video background subtraction, a problem that can be\nstated as follows: given a set of measurements of a sequence of images with a\nstatic background, simultaneously reconstruct each image while separating its\nforeground from the background. The performance of our method is illustrated on\nsequences of real images: we observe that it allows a dramatic reduction in the\nnumber of measurements with respect to state-of-the-art compressive background\nsubtraction schemes. \n\n"}
{"id": "1503.03715", "contents": "Title: Feedback Refinement Relations for the Synthesis of Symbolic Controllers Abstract: We present an abstraction and refinement methodology for the automated\ncontroller synthesis to enforce general predefined specifications. The designed\ncontrollers require quantized (or symbolic) state information only and can be\ninterfaced with the system via a static quantizer. Both features are\nparticularly important with regard to any practical implementation of the\ndesigned controllers and, as we prove, are characterized by the existence of a\nfeedback refinement relation between plant and abstraction. Feedback refinement\nrelations are a novel concept introduced in this paper. Our work builds on a\ngeneral notion of system with set-valued dynamics and possibly\nnon-deterministic quantizers to permit the synthesis of controllers that\nrobustly, and provably, enforce the specification in the presence of various\ntypes of uncertainties and disturbances. We identify a class of abstractions\nthat is canonical in a well-defined sense, and provide a method to efficiently\ncompute canonical abstractions. We demonstrate the practicality of our approach\non two examples. \n\n"}
{"id": "1503.04100", "contents": "Title: Extracting Angular Observables without a Likelihood and Applications to\n  Rare Decays Abstract: Our goal is to obtain a complete set of angular observables arising in a\ngeneric multi-body process. We show how this can be achieved without the need\nto carry out a likelihood fit of the angular distribution to the measured\nevents. Instead, we apply the method of moments that relies both on the\northogonality of angular functions and the estimation of integrals by Monte\nCarlo techniques. The big advantage of this method is that the joint\ndistribution of all observables can be easily extracted, even for very few\nevents. The method of moments is shown to be robust against mismodeling of the\nangular distribution. Our main result is an explicit algorithm that accounts\nfor systematic uncertainties from detector-resolution and acceptance effects.\nFinally, we present the necessary process-dependent formulae needed for direct\napplication of the method to several rare decays of interest. \n\n"}
{"id": "1503.04360", "contents": "Title: Quadratic Multi-Dimensional Signaling Games and Affine Equilibria Abstract: This paper studies the decentralized quadratic cheap talk and signaling game\nproblems when an encoder and a decoder, viewed as two decision makers, have\nmisaligned objective functions. The main contributions of this study are the\nextension of Crawford and Sobel's cheap talk formulation to multi-dimensional\nsources and to noisy channel setups. We consider both (simultaneous) Nash\nequilibria and (sequential) Stackelberg equilibria. We show that for arbitrary\nscalar sources, in the presence of misalignment, the quantized nature of all\nequilibrium policies holds for Nash equilibria in the sense that all Nash\nequilibria are equivalent to those achieved by quantized encoder policies. On\nthe other hand, all Stackelberg equilibria policies are fully informative. For\nmulti-dimensional setups, unlike the scalar case, Nash equilibrium policies may\nbe of non-quantized nature, and even linear. In the noisy setup, a Gaussian\nsource is to be transmitted over an additive Gaussian channel. The goals of the\nencoder and the decoder are misaligned by a bias term and encoder's cost also\nincludes a penalty term on signal power. Conditions for the existence of affine\nNash equilibria as well as general informative equilibria are presented. For\nthe noisy setup, the only Stackelberg equilibrium is the linear equilibrium\nwhen the variables are scalar. Our findings provide further conditions on when\naffine policies may be optimal in decentralized multi-criteria control problems\nand lead to conditions for the presence of active information transmission in\nstrategic environments. \n\n"}
{"id": "1503.05098", "contents": "Title: Randomizing bipartite networks: the case of the World Trade Web Abstract: Within the last fifteen years, network theory has been successfully applied\nboth to natural sciences and to socioeconomic disciplines. In particular,\nbipartite networks have been recognized to provide a particularly insightful\nrepresentation of many systems, ranging from mutualistic networks in ecology to\ntrade networks in economy, whence the need of a pattern detection-oriented\nanalysis in order to identify statistically-significant structural properties.\nSuch an analysis rests upon the definition of suitable null models, i.e. upon\nthe choice of the portion of network structure to be preserved while\nrandomizing everything else. However, quite surprisingly, little work has been\ndone so far to define null models for real bipartite networks. The aim of the\npresent work is to fill this gap, extending a recently-proposed method to\nrandomize monopartite networks to bipartite networks. While the proposed\nformalism is perfectly general, we apply our method to the binary, undirected,\nbipartite representation of the World Trade Web, comparing the observed values\nof a number of structural quantities of interest with the expected ones,\ncalculated via our randomization procedure. Interestingly, the behavior of the\nWorld Trade Web in this new representation is strongly different from the\nmonopartite analogue, showing highly non-trivial patterns of self-organization. \n\n"}
{"id": "1503.06232", "contents": "Title: Universal Ideal Behavior and Macroscopic Work Relation of Linear\n  Irreversible Stochastic Thermodynamics Abstract: We revisit the Ornstein-Uhlenbeck (OU) process as the fundamental\nmathematical description of linear irreversible phenomena, with fluctuations,\nnear an equilibrium. By identifying the underlying circulating dynamics in a\nstationary process as the natural generalization of classical conservative\nmechanics, a bridge between a family of OU processes with equilibrium\nfluctuations and thermodynamics is established through the celebrated Helmholtz\ntheorem. The Helmholtz theorem provides an emergent macroscopic \"equation of\nstate\" of the entire system, which exhibits a universal ideal thermodynamic\nbehavior. Fluctuating macroscopic quantities are studied from the stochastic\nthermodynamic point of view and a non-equilibrium work relation is obtained in\nthe macroscopic picture, which may facilitate experimental study and\napplication of the equalities due to Jarzynski, Crooks, and Hatano and Sasa. \n\n"}
{"id": "1503.07622", "contents": "Title: Practical Statistics for the LHC Abstract: This document is a pedagogical introduction to statistics for particle\nphysics. Emphasis is placed on the terminology, concepts, and methods being\nused at the Large Hadron Collider. The document addresses both the statistical\ntests applied to a model of the data and the modeling itself. \n\n"}
{"id": "1503.08577", "contents": "Title: Sparse Spikes Deconvolution on Thin Grids Abstract: This article analyzes the recovery performance of two popular finite\ndimensional approximations of the sparse spikes deconvolution problem over\nRadon measures. We examine in a unified framework both the L1 regularization\n(often referred to as Lasso or Basis-Pursuit) and the Continuous Basis-Pursuit\n(C-BP) methods. The Lasso is the de-facto standard for the sparse\nregularization of inverse problems in imaging. It performs a nearest neighbor\ninterpolation of the spikes locations on the sampling grid. The C-BP method,\nintroduced by Ekanadham, Tranchina and Simoncelli, uses a linear interpolation\nof the locations to perform a better approximation of the infinite-dimensional\noptimization problem, for positive measures. We show that, in the small noise\nregime, both methods estimate twice the number of spikes as the number of\noriginal spikes. Indeed, we show that they both detect two neighboring spikes\naround the locations of an original spikes. These results for deconvolution\nproblems are based on an abstract analysis of the so-called extended support of\nthe solutions of L1-type problems (including as special cases the Lasso and\nC-BP for deconvolution), which are of an independent interest. They precisely\ncharacterize the support of the solutions when the noise is small and the\nregularization parameter is selected accordingly. We illustrate these findings\nto analyze for the first time the support instability of compressed sensing\nrecovery when the number of measurements is below the critical limit (well\ndocumented in the literature) where the support is provably stable. \n\n"}
{"id": "1503.08985", "contents": "Title: Iterative Regularization for Learning with Convex Loss Functions Abstract: We consider the problem of supervised learning with convex loss functions and\npropose a new form of iterative regularization based on the subgradient method.\nUnlike other regularization approaches, in iterative regularization no\nconstraint or penalization is considered, and generalization is achieved by\n(early) stopping an empirical iteration. We consider a nonparametric setting,\nin the framework of reproducing kernel Hilbert spaces, and prove finite sample\nbounds on the excess risk under general regularity conditions. Our study\nprovides a new class of efficient regularized learning algorithms and gives\ninsights on the interplay between statistics and optimization in machine\nlearning. \n\n"}
{"id": "1504.00057", "contents": "Title: Optimal Power Flow with Weighted Chance Constraints and General Policies\n  for Generation Control Abstract: Due to the increasing amount of electricity generated from renewable sources,\nuncertainty in power system operation will grow. This has implications for\ntools such as Optimal Power Flow (OPF), an optimization problem widely used in\npower system operations and planning, which should be adjusted to account for\nthis uncertainty. One way to handle the uncertainty is to formulate a Chance\nConstrained OPF (CC-OPF) which limits the probability of constraint violation\nto a predefined value. However, existing CC-OPF formulations and solutions are\nnot immune to drawbacks. On one hand, they only consider affine policies for\ngeneration control, which are not always realistic and may be sub-optimal. On\nthe other hand, the standard CC-OPF formulations do not distinguish between\nlarge and small violations, although those might carry significantly different\nrisk. In this paper, we introduce the Weighted CC-OPF (WCC-OPF) that can handle\ngeneral control policies while preserving convexity and allowing for efficient\ncomputation. The weighted chance constraints account for the size of violations\nthrough a weighting function, which assigns a higher risk to a higher\noverloads. We prove that the problem remains convex for any convex weighting\nfunction, and for very general generation control policies. In a case study, we\ncompare the performance of the new WCC-OPF and the standard CC-OPF and\ndemonstrate that WCC-OPF effectively reduces the number of severe overloads.\nFurthermore, we compare an affine generation control policy with a more general\npolicy, and show that the additional flexibility allow for a lower cost while\nmaintaining the same level of risk. \n\n"}
{"id": "1504.00183", "contents": "Title: A Convex Approach to Hydrodynamic Analysis Abstract: We study stability and input-state analysis of three dimensional (3D)\nincompressible, viscous flows with invariance in one direction. By taking\nadvantage of this invariance property, we propose a class of Lyapunov and\nstorage functionals. We then consider exponential stability, induced L2-norms,\nand input-to-state stability (ISS). For streamwise constant flows, we formulate\nconditions based on matrix inequalities. We show that in the case of polynomial\nlaminar flow profiles the matrix inequalities can be checked via convex\noptimization. The proposed method is illustrated by an example of rotating\nCouette flow. \n\n"}
{"id": "1504.00298", "contents": "Title: Bayesian model comparison with un-normalised likelihoods Abstract: Models for which the likelihood function can be evaluated only up to a\nparameter-dependent unknown normalising constant, such as Markov random field\nmodels, are used widely in computer science, statistical physics, spatial\nstatistics, and network analysis. However, Bayesian analysis of these models\nusing standard Monte Carlo methods is not possible due to the intractability of\ntheir likelihood functions. Several methods that permit exact, or close to\nexact, simulation from the posterior distribution have recently been developed.\nHowever, estimating the evidence and Bayes' factors (BFs) for these models\nremains challenging in general. This paper describes new random weight\nimportance sampling and sequential Monte Carlo methods for estimating BFs that\nuse simulation to circumvent the evaluation of the intractable likelihood, and\ncompares them to existing methods. In some cases we observe an advantage in the\nuse of biased weight estimates. An initial investigation into the theoretical\nand empirical properties of this class of methods is presented. Some support\nfor the use of biased estimates is presented, but we advocate caution in the\nuse of such estimates. \n\n"}
{"id": "1504.01032", "contents": "Title: A Three-Operator Splitting Scheme and its Optimization Applications Abstract: Operator splitting schemes have been successfully used in computational\nsciences to reduce complex problems into a series of simpler subproblems. Since\n1950s, these schemes have been widely used to solve problems in PDE and\ncontrol. Recently, large-scale optimization problems in machine learning,\nsignal processing, and imaging have created a resurgence of interest in\noperator-splitting based algorithms because they often have simple\ndescriptions, are easy to code, and have (nearly) state-of-the-art performance\nfor large-scale optimization problems. Although operator splitting techniques\nwere introduced over 60 years ago, their importance has significantly increased\nin the past decade.\n  This paper introduces a new operator-splitting scheme for solving a variety\nof problems that are reduced to a monotone inclusion of three operators, one of\nwhich is cocoercive. Our scheme is very simple, and it does not reduce to any\nexisting splitting schemes. Our scheme recovers the existing forward-backward,\nDouglas-Rachford, and forward-Douglas-Rachford splitting schemes as special\ncases.\n  Our new splitting scheme leads to a set of new and simple algorithms for a\nvariety of other problems, including the 3-set split feasibility problems,\n3-objective minimization problems, and doubly and multiple regularization\nproblems, as well as the simplest extension of the classic ADMM from 2 to 3\nblocks of variables. In addition to the basic scheme, we introduce several\nmodifications and enhancements that can improve the convergence rate in\npractice, including an acceleration that achieves the optimal rate of\nconvergence for strongly monotone inclusions. Finally, we evaluate the\nalgorithm on several applications. \n\n"}
{"id": "1504.01515", "contents": "Title: Simultaneously sparse and low-rank abundance matrix estimation for\n  hyperspectral image unmixing Abstract: In a plethora of applications dealing with inverse problems, e.g. in image\nprocessing, social networks, compressive sensing, biological data processing\netc., the signal of interest is known to be structured in several ways at the\nsame time. This premise has recently guided the research to the innovative and\nmeaningful idea of imposing multiple constraints on the parameters involved in\nthe problem under study. For instance, when dealing with problems whose\nparameters form sparse and low-rank matrices, the adoption of suitably combined\nconstraints imposing sparsity and low-rankness, is expected to yield\nsubstantially enhanced estimation results. In this paper, we address the\nspectral unmixing problem in hyperspectral images. Specifically, two novel\nunmixing algorithms are introduced, in an attempt to exploit both spatial\ncorrelation and sparse representation of pixels lying in homogeneous regions of\nhyperspectral images. To this end, a novel convex mixed penalty term is first\ndefined consisting of the sum of the weighted $\\ell_1$ and the weighted nuclear\nnorm of the abundance matrix corresponding to a small area of the image\ndetermined by a sliding square window. This penalty term is then used to\nregularize a conventional quadratic cost function and impose simultaneously\nsparsity and row-rankness on the abundance matrix. The resulting regularized\ncost function is minimized by a) an incremental proximal sparse and low-rank\nunmixing algorithm and b) an algorithm based on the alternating minimization\nmethod of multipliers (ADMM). The effectiveness of the proposed algorithms is\nillustrated in experiments conducted both on simulated and real data. \n\n"}
{"id": "1504.03584", "contents": "Title: Synergetic and redundant information flow detected by unnormalized\n  Granger causality: application to resting state fMRI Abstract: Objectives: We develop a framework for the analysis of synergy and redundancy\nin the pattern of information flow between subsystems of a complex network.\nMethods: The presence of redundancy and/or synergy in multivariate time series\ndata renders difficult to estimate the neat flow of information from each\ndriver variable to a given target. We show that adopting an unnormalized\ndefinition of Granger causality one may put in evidence redundant multiplets of\nvariables influencing the target by maximizing the total Granger causality to a\ngiven target, over all the possible partitions of the set of driving variables.\nConsequently we introduce a pairwise index of synergy which is zero when two\nindependent sources additively influence the future state of the system,\ndifferently from previous definitions of synergy. Results: We report the\napplication of the proposed approach to resting state fMRI data from the Human\nConnectome Project, showing that redundant pairs of regions arise mainly due to\nspace contiguity and interhemispheric symmetry, whilst synergy occurs mainly\nbetween non-homologous pairs of regions in opposite hemispheres. Conclusions:\nRedundancy and synergy, in healthy resting brains, display characteristic\npatterns, revealed by the proposed approach. Significance: The pairwise synergy\nindex, here introduced, maps the informational character of the system at hand\ninto a weighted complex network: the same approach can be applied to other\ncomplex systems whose normal state corresponds to a balance between redundant\nand synergetic circuits. \n\n"}
{"id": "1504.04920", "contents": "Title: From Weak Learning to Strong Learning in Fictitious Play Type Algorithms Abstract: The paper studies the highly prototypical Fictitious Play (FP) algorithm, as\nwell as a broad class of learning processes based on best-response dynamics,\nthat we refer to as FP-type algorithms. A well-known shortcoming of FP is that,\nwhile players may learn an equilibrium strategy in some abstract sense, there\nare no guarantees that the period-by-period strategies generated by the\nalgorithm actually converge to equilibrium themselves. This issue is\nfundamentally related to the discontinuous nature of the best response\ncorrespondence and is inherited by many FP-type algorithms. Not only does it\ncause problems in the interpretation of such algorithms as a mechanism for\neconomic and social learning, but it also greatly diminishes the practical\nvalue of these algorithms for use in distributed control. We refer to forms of\nlearning in which players learn equilibria in some abstract sense only (to be\ndefined more precisely in the paper) as weak learning, and we refer to forms of\nlearning where players' period-by-period strategies converge to equilibrium as\nstrong learning. An approach is presented for modifying an FP-type algorithm\nthat achieves weak learning in order to construct a variant that achieves\nstrong learning. Theoretical convergence results are proved. \n\n"}
{"id": "1504.05552", "contents": "Title: Category theoretic properties of the A. R\\'enyi and C. Tsallis entropies Abstract: The problem of embedding the Tsallis and R\\'{e}nyi entropies in the framework\nof category theory and their axiomatic foundation is studied. To this end, we\nconstruct a special category MES related to measured spaces. We prove that both\nof the R\\'{e}nyi and Tsallis entropies can be imbedded in the formalism of\ncategory theory by proving that the same basic functional that appears in their\ndefinitions, as well as in the associated Lebesgue space norms, has good\nalgebraic compatibility properties. We prove that this functional is both\nadditive and multiplicative with respect to the direct product and the disjoint\nsum (the coproduct) in the category MES, so it is a natural candidate for the\nmeasure of information or uncertainty. We prove that the category MES can be\nextended to monoidal category, both with respect to the direct product as well\nas to the coproduct. The basic axioms of the original R\\'{e}nyi entropy theory\nare generalized and reformulated in the framework of category MES and we prove\nthat these axioms foresee the existence of an universal exponent having the\nsame values for all the objects of the category MES. In addition, this\nuniversal exponent is the parameter, which appears in the definition of the\nTsallis and R\\'{e}nyi entropies. \n\n"}
{"id": "1504.06068", "contents": "Title: Analysis on Non-negative Factorizations and Applications Abstract: In this work we perform some mathematical analysis on non-negative matrix\nfactorizations (NMF) and apply NMF to some imaging and inverse problems. We\nwill propose a sparse low-rank approximation of big positive data and images in\nterms of tensor products of positive vectors, and investigate its effectiveness\nin terms of the number of tensor products to be used in the approximation. A\nnew concept of multi-level analysis (MLA) framework is also suggested to\nextract major components in the matrix representing structures of different\nresolutions, but still preserving the positivity of the basis and sparsity of\nthe approximation. We will also propose a semi-smooth Newton method based on\nprimal-dual active sets for the non-negative factorization. Numerical results\nare given to demonstrate the effectiveness of the proposed method to capture\nfeatures in images and structures of inverse problems under no a-priori\nassumption on the data structure, as well as to provide a sparse low-rank\nrepresentation of the data. \n\n"}
{"id": "1505.02724", "contents": "Title: From optimal stopping boundaries to Rost's reversed barriers and the\n  Skorokhod embedding Abstract: We provide a new probabilistic proof of the connection between Rost's\nsolution of the Skorokhod embedding problem and a suitable family of optimal\nstopping problems for Brownian motion with finite time-horizon. In particular\nwe use stochastic calculus to show that the time reversal of the optimal\nstopping sets for such problems forms the so-called Rost's reversed barrier. \n\n"}
{"id": "1505.03410", "contents": "Title: Mind the duality gap: safer rules for the Lasso Abstract: Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules. \n\n"}
{"id": "1505.03606", "contents": "Title: Rescaled Pure Greedy Algorithm for Convex Optimization Abstract: We suggest a new greedy strategy for convex optimization in Banach spaces and\nprove its convergent rates under a suitable behavior of the modulus of uniform\nsmoothness of the objective function. \n\n"}
{"id": "1505.04741", "contents": "Title: Untangling the roles of parasites in food webs with generative network\n  models Abstract: Food webs represent the set of consumer-resource interactions among a set of\nspecies that co-occur in a habitat, but most food web studies have omitted\nparasites and their interactions. Recent studies have provided conflicting\nevidence on whether including parasites changes food web structure, with some\nsuggesting that parasitic interactions are structurally distinct from those\namong free-living species while others claim the opposite. Here, we describe a\nprincipled method for understanding food web structure that combines an\nefficient optimization algorithm from statistical physics called parallel\ntempering with a probabilistic generalization of the empirically well-supported\nfood web niche model. This generative model approach allows us to rigorously\nestimate the degree to which interactions that involve parasites are\nstatistically distinguishable from interactions among free-living species,\nwhether parasite niches behave similarly to free-living niches, and the degree\nto which existing hypotheses about food web structure are naturally recovered.\nWe apply this method to the well-studied Flensburg Fjord food web and show that\nwhile predation on parasites, concomitant predation of parasites, and parasitic\nintraguild trophic interactions are largely indistinguishable from free-living\npredation interactions, parasite-host interactions are different. These results\nprovide a powerful new tool for evaluating the impact of classes of species and\ninteractions on food web structure to shed new light on the roles of parasites\nin food webs \n\n"}
{"id": "1505.05118", "contents": "Title: Almost sure convergence of the forward-backward-forward splitting\n  algorithm Abstract: In this paper, we propose a stochastic forward-backward-forward splitting\nalgorithm and prove its almost sure weak convergence in real separable Hilbert\nspaces. Applications to composite monotone inclusion and minimization problems\nare demonstrated. \n\n"}
{"id": "1506.01293", "contents": "Title: An algorithm for discovering Lagrangians automatically from data Abstract: An activity fundamental to science is building mathematical models. These\nmodels are used to both predict the results of future experiments and gain\ninsight into the structure of the system under study. We present an algorithm\nthat automates the model building process in a scientifically principled way.\nThe algorithm can take observed trajectories from a wide variety of mechanical\nsystems and, without any other prior knowledge or tuning of parameters, predict\nthe future evolution of the system. It does this by applying the principle of\nleast action and searching for the simplest Lagrangian that describes the\nsystem's behaviour. By generating this Lagrangian in a human interpretable\nform, it also provides insight into the working of the system. \n\n"}
{"id": "1506.02344", "contents": "Title: Stay on path: PCA along graph paths Abstract: We introduce a variant of (sparse) PCA in which the set of feasible support\nsets is determined by a graph. In particular, we consider the following\nsetting: given a directed acyclic graph $G$ on $p$ vertices corresponding to\nvariables, the non-zero entries of the extracted principal component must\ncoincide with vertices lying along a path in $G$.\n  From a statistical perspective, information on the underlying network may\npotentially reduce the number of observations required to recover the\npopulation principal component. We consider the canonical estimator which\noptimally exploits the prior knowledge by solving a non-convex quadratic\nmaximization on the empirical covariance. We introduce a simple network and\nanalyze the estimator under the spiked covariance model. We show that side\ninformation potentially improves the statistical complexity.\n  We propose two algorithms to approximate the solution of the constrained\nquadratic maximization, and recover a component with the desired properties. We\nempirically evaluate our schemes on synthetic and real datasets. \n\n"}
{"id": "1506.02396", "contents": "Title: ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate\n  Updates Abstract: Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstracts\nmany problems in numerical linear algebra, optimization, and other areas of\nscientific computing. To solve fixed-point problems, we propose ARock, an\nalgorithmic framework in which multiple agents (machines, processors, or cores)\nupdate $x$ in an asynchronous parallel fashion. Asynchrony is crucial to\nparallel computing since it reduces synchronization wait, relaxes communication\nbottleneck, and thus speeds up computing significantly. At each step of ARock,\nan agent updates a randomly selected coordinate $x_i$ based on possibly\nout-of-date information on $x$. The agents share $x$ through either global\nmemory or communication. If writing $x_i$ is atomic, the agents can read and\nwrite $x$ without memory locks.\n  Theoretically, we show that if the nonexpansive operator $T$ has a fixed\npoint, then with probability one, ARock generates a sequence that converges to\na fixed points of $T$. Our conditions on $T$ and step sizes are weaker than\ncomparable work. Linear convergence is also obtained.\n  We propose special cases of ARock for linear systems, convex optimization,\nmachine learning, as well as distributed and decentralized consensus problems.\nNumerical experiments of solving sparse logistic regression problems are\npresented. \n\n"}
{"id": "1506.02444", "contents": "Title: Decomposition Techniques for Bilinear Saddle Point Problems and\n  Variational Inequalities with Affine Monotone Operators on Domains Given by\n  Linear Minimization Oracles Abstract: The majority of First Order methods for large-scale convex-concave saddle\npoint problems and variational inequalities with monotone operators are\nproximal algorithms which at every iteration need to minimize over problem's\ndomain X the sum of a linear form and a strongly convex function. To make such\nan algorithm practical, X should be proximal-friendly -- admit a strongly\nconvex function with easy to minimize linear perturbations. As a byproduct, X\nadmits a computationally cheap Linear Minimization Oracle (LMO) capable to\nminimize over X linear forms. There are, however, important situations where a\ncheap LMO indeed is available, but X is not proximal-friendly, which motivates\nsearch for algorithms based solely on LMO's. For smooth convex minimization,\nthere exists a classical LMO-based algorithm -- Conditional Gradient. In\ncontrast, known to us LMO-based techniques for other problems with convex\nstructure (nonsmooth convex minimization, convex-concave saddle point problems,\neven as simple as bilinear ones, and variational inequalities with monotone\noperators, even as simple as affine) are quite recent and utilize common\napproach based on Fenchel-type representations of the associated\nobjectives/vector fields. The goal of this paper is to develop an alternative\n(and seemingly much simpler) LMO-based decomposition techniques for bilinear\nsaddle point problems and for variational inequalities with affine monotone\noperators. \n\n"}
{"id": "1506.05485", "contents": "Title: On the Convergence Analysis of Asynchronous Distributed Quadratic\n  Programming via Dual Decomposition Abstract: In this paper, we analyze the convergence as well as the rate of convergence\nof asynchronous distributed quadratic programming (QP) with dual decomposition\ntechnique. In general, distributed optimization requires synchronization of\ndata at each iteration step due to the interdependency of data. This\nsynchronization latency may incur a large amount of waiting time caused by an\nidle process during computation. We aim to attack this synchronization penalty\nin distributed QP problems by implementing asynchronous update of dual\nvariable. The price to pay for adopting asynchronous computing algorithms is\nunpredictability of the solution, resulting in a tradeoff between speedup and\naccuracy. Thus, the convergence to an optimal solution is not guaranteed owing\nto the stochastic behavior of asynchrony. In this paper, we employ the switched\nsystem framework as an analysis tool to investigate the convergence of\nasynchronous distributed QP. This switched system will facilitate analysis on\nasynchronous distributed QP with dual decomposition, providing necessary and\nsufficient conditions for the mean square convergence. Also, we provide an\nanalytic expression for the rate of convergence through the switched system,\nwhich enables performance analysis of asynchronous algorithms as compared with\nsynchronous case. To verify the validity of the proposed methods, numerical\nexamples are presented with an implementation of asynchronous parallel QP using\nOpenMP. \n\n"}
{"id": "1506.07165", "contents": "Title: Random walk centrality in interconnected multilayer networks Abstract: Real-world complex systems exhibit multiple levels of relationships. In many\ncases they require to be modeled as interconnected multilayer networks,\ncharacterizing interactions of several types simultaneously. It is of crucial\nimportance in many fields, from economics to biology and from urban planning to\nsocial sciences, to identify the most (or the less) influential nodes in a\nnetwork using centrality measures. However, defining the centrality of actors\nin interconnected complex networks is not trivial. In this paper, we rely on\nthe tensorial formalism recently proposed to characterize and investigate this\nkind of complex topologies, and extend two well known random walk centrality\nmeasures, the random walk betweenness and closeness centrality, to\ninterconnected multilayer networks. For each of the measures we provide\nanalytical expressions that completely agree with numerically results. \n\n"}
{"id": "1506.07249", "contents": "Title: Nodal and spectral minimal partitions -- The state of the art in 2015 -- Abstract: In this article, we propose a state of the art concerning the nodal and\nspectral minimal partitions. First we focus on the nodal partitions and give\nsome examples of Courant sharp cases. Then we are interested in minimal\nspectral partitions. Using the link with the Courant sharp situation, we can\ndetermine the minimal k-partitions for some particular domains. We also recall\nsome results about the topology of regular partitions and Aharonov-Bohm\napproach. The last section deals with the asymptotic behavior of minimal\nk-partition. \n\n"}
{"id": "1506.07719", "contents": "Title: Network Aggregative Games and Distributed Mean Field Control via\n  Consensus Theory Abstract: We consider network aggregative games to model and study multi-agent\npopulations in which each rational agent is influenced by the aggregate\nbehavior of its neighbors, as specified by an underlying network. Specifically,\nwe examine systems where each agent minimizes a quadratic cost function, that\ndepends on its own strategy and on a convex combination of the strategies of\nits neighbors, and is subject to personalized convex constraints. We analyze\nthe best response dynamics and we propose alternative distributed algorithms to\nsteer the strategies of the rational agents to a Nash equilibrium\nconfiguration. The convergence of these schemes is guaranteed under different\nsufficient conditions, depending on the matrices defining the cost and on the\nnetwork. Additionally, we propose an extension to the network aggregative game\nsetting that allows for multiple rounds of communications among the agents, and\nwe illustrate how it can be combined with consensus theory to recover a\nsolution to the mean field control problem in a distributed fashion, that is,\nwithout requiring the presence of a central coordinator. Finally, we apply our\ntheoretical findings to study a novel multi-dimensional, convex-constrained\nmodel of opinion dynamics and a hierarchical demand-response scheme for energy\nmanagement in smart buildings, extending literature results. \n\n"}
{"id": "1506.07940", "contents": "Title: Null boundary controllability of a 1-dimensional heat equation with an\n  internal point mass Abstract: We consider a linear hybrid system composed by two rods of equal length\nconnected by a point mass. We show that the system is null controllable with\nDirichlet and Neumann controls. The results are based on a careful spectral\nspectral analysis together with the moment method. \n\n"}
{"id": "1506.08019", "contents": "Title: Multiobjective approach to optimal control for a dengue transmission\n  model Abstract: During the last decades, the global prevalence of dengue progressed\ndramatically. It is a disease which is now endemic in more than one hundred\ncountries of Africa, America, Asia and the Western Pacific. This study\naddresses a mathematical model for the dengue disease transmission and finding\nthe most effective ways of controlling the disease. The model is described by a\nsystem of ordinary differential equations representing human and vector\ndynamics. Multiobjective optimization is applied to find the optimal control\nstrategies, considering the simultaneous minimization of infected humans and\ncosts due to insecticide application. The obtained results show that\nmultiobjective optimization is an effective tool for finding the optimal\ncontrol. The set of trade-off solutions encompasses a whole range of optimal\nscenarios, providing valuable information about the dynamics of infection\ntransmissions. The results are discussed for different values of model\nparameters. \n\n"}
{"id": "1506.08692", "contents": "Title: Detrended fluctuation analysis made flexible to detect range of\n  cross-correlated fluctuations Abstract: The detrended cross-correlation coefficient $\\rho_{\\rm DCCA}$ has recently\nbeen proposed to quantify the strength of cross-correlations on different\ntemporal scales in bivariate, non-stationary time series. It is based on the\ndetrended cross-correlation and detrended fluctuation analyses (DCCA and DFA,\nrespectively) and can be viewed as an analogue of the Pearson coefficient in\nthe case of the fluctuation analysis. The coefficient $\\rho_{\\rm DCCA}$ works\nwell in many practical situations but by construction its applicability is\nlimited to detection of whether two signals are generally cross-correlated,\nwithout possibility to obtain information on the amplitude of fluctuations that\nare responsible for those cross-correlations. In order to introduce some\nrelated flexibility, here we propose an extension of $\\rho_{\\rm DCCA}$ that\nexploits the multifractal versions of DFA and DCCA: MFDFA and MFCCA,\nrespectively. The resulting new coefficient $\\rho_q$ not only is able to\nquantify the strength of correlations, but also it allows one to identify the\nrange of detrended fluctuation amplitudes that are correlated in two signals\nunder study. We show how the coefficient $\\rho_q$ works in practical situations\nby applying it to stochastic time series representing processes with long\nmemory: autoregressive and multiplicative ones. Such processes are often used\nto model signals recorded from complex systems and complex physical phenomena\nlike turbulence, so we are convinced that this new measure can successfully be\napplied in time series analysis. In particular, we present an example of such\napplication to highly complex empirical data from financial markets. The\npresent formulation can straightforwardly be extended to multivariate data in\nterms of the $q$-dependent counterpart of the correlation matrices and then to\nthe network representation. \n\n"}
{"id": "1507.01716", "contents": "Title: Temporal-varying failures of nodes in networks Abstract: We consider networks in which random walkers are removed because of the\nfailure of specific nodes. We interpret the rate of loss as a measure of the\nimportance of nodes, a notion we denote as failure-centrality. We show that the\ndegree of the node is not sufficient to determine this measure and that, in a\nfirst approximation, the shortest loops through the node have to be taken into\naccount. We propose approximations of the failure-centrality which are valid\nfor temporal-varying failures and we dwell on the possibility of externally\nchanging the relative importance of nodes in a given network, by exploiting the\ninterference between the loops of a node and the cycles of the temporal pattern\nof failures. In the limit of long failure cycles we show analytically that the\nescape in a node is larger than the one estimated from a stochastic failure\nwith the same failure probability. We test our general formalism in two\nreal-world networks (air-transportation and e-mail users) and show how\ncommunities lead to deviations from predictions for failures in hubs. \n\n"}
{"id": "1507.02317", "contents": "Title: A Convex Approach to Sparse H infinity Analysis & Synthesis Abstract: In this paper, we propose a new robust analysis tool motivated by large-scale\nsystems. The H infinity norm of a system measures its robustness by quantifying\nthe worst-case behavior of a system perturbed by a unit-energy disturbance.\nHowever, the disturbance that induces such worst-case behavior requires perfect\ncoordination among all disturbance channels. Given that many systems of\ninterest, such as the power grid, the internet and automated vehicle platoons,\nare large-scale and spatially distributed, such coordination may not be\npossible, and hence the H infinity norm, used as a measure of robustness, may\nbe too conservative. We therefore propose a cardinality constrained variant of\nthe H infinity norm in which an adversarial disturbance can use only a limited\nnumber of channels. As this problem is inherently combinatorial, we present a\nsemidefinite programming (SDP) relaxation based on the l1 norm that yields an\nupper bound on the cardinality constrained robustness problem. We further\npropose a simple rounding heuristic based on the optimal solution of SDP\nrelaxation which provides a lower bound. Motivated by privacy in large-scale\nsystems, we also extend these relaxations to computing the minimum gain of a\nsystem subject to a limited number of inputs. Finally, we also present a SDP\nbased optimal controller synthesis method for minimizing the SDP relaxation of\nour novel robustness measure. The effectiveness of our semidefinite relaxation\nis demonstrated through numerical examples. \n\n"}
{"id": "1507.02528", "contents": "Title: Faster Convex Optimization: Simulated Annealing with an Efficient\n  Universal Barrier Abstract: This paper explores a surprising equivalence between two seemingly-distinct\nconvex optimization methods. We show that simulated annealing, a well-studied\nrandom walk algorithms, is directly equivalent, in a certain sense, to the\ncentral path interior point algorithm for the the entropic universal barrier\nfunction. This connection exhibits several benefits. First, we are able improve\nthe state of the art time complexity for convex optimization under the\nmembership oracle model. We improve the analysis of the randomized algorithm of\nKalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that\nunderly the central path following interior point algorithm. We are able to\ntighten the temperature schedule for simulated annealing which gives an\nimproved running time, reducing by square root of the dimension in certain\ninstances. Second, we get an efficient randomized interior point method with an\nefficiently computable universal barrier for any convex set described by a\nmembership oracle. Previously, efficiently computable barriers were known only\nfor particular convex sets. \n\n"}
{"id": "1507.03103", "contents": "Title: Generalized synchronization of multidimensional chaotic systems in terms\n  of symbolic CTQ-analysis Abstract: A new approach is proposed to the analysis of generalized synchronization of\nmultidimensional chaotic systems. The approach is based on the symbolic\nanalysis of discrete sequences in the basis of a finite T-alphabet. In fact,\nthe symbols of the T-alphabet encode the shape (the geometric structure) of a\ntrajectory of a dynamical system. Investigation of symbolic sequences allows\none to diagnose various regimes of chaos synchronization, including generalized\nsynchronization. The characteristics introduced allow one to detect and study\nthe restructuring and intermittency behavior of attractors in systems (the time\nstructure of synchronization). The measure of T-synchronization proposed is\ngeneralized without restrictions to complex ensembles of strongly nonstationary\nand nonidentical large-dimensional oscillators with arbitrary configuration and\nnetwork (lattice) topology. The main features of the method are illustrated by\nan example. \n\n"}
{"id": "1507.03566", "contents": "Title: Low-rank Solutions of Linear Matrix Equations via Procrustes Flow Abstract: In this paper we study the problem of recovering a low-rank matrix from\nlinear measurements. Our algorithm, which we call Procrustes Flow, starts from\nan initial estimate obtained by a thresholding scheme followed by gradient\ndescent on a non-convex objective. We show that as long as the measurements\nobey a standard restricted isometry property, our algorithm converges to the\nunknown matrix at a geometric rate. In the case of Gaussian measurements, such\nconvergence occurs for a $n_1 \\times n_2$ matrix of rank $r$ when the number of\nmeasurements exceeds a constant times $(n_1+n_2)r$. \n\n"}
{"id": "1507.04374", "contents": "Title: Uniform-Price Mechanism Design for a Large Population of Dynamic Agents Abstract: This paper focuses on the coordination of a large population of dynamic\nagents with private information over multiple periods. Each agent maximizes the\nindividual utility, while the coordinator determines the market rule to achieve\ngroup objectives. The coordination problem is formulated as a dynamic mechanism\ndesign problem. A mechanism is proposed based on the competitive equilibrium of\nthe large population game. We derive the conditions for the general nonlinear\ndynamic systems under which the proposed mechanism is incentive compatible and\ncan implement the social choice function in $\\epsilon$-Nash equilibrium. In\naddition, we show that for linear quadratic problems with bounded parameters,\nthe proposed mechanism can maximize the social welfare subject to a total\nresource constraint in $\\epsilon$-dominant strategy equilibrium. \n\n"}
{"id": "1507.05367", "contents": "Title: Structured Sparsity: Discrete and Convex approaches Abstract: Compressive sensing (CS) exploits sparsity to recover sparse or compressible\nsignals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity\nis also used to enhance interpretability in machine learning and statistics\napplications: While the ambient dimension is vast in modern data analysis\nproblems, the relevant information therein typically resides in a much lower\ndimensional space. However, many solutions proposed nowadays do not leverage\nthe true underlying structure. Recent results in CS extend the simple sparsity\nidea to more sophisticated {\\em structured} sparsity models, which describe the\ninterdependency between the nonzero components of a signal, allowing to\nincrease the interpretability of the results and lead to better recovery\nperformance. In order to better understand the impact of structured sparsity,\nin this chapter we analyze the connections between the discrete models and\ntheir convex relaxations, highlighting their relative advantages. We start with\nthe general group sparse model and then elaborate on two important special\ncases: the dispersive and the hierarchical models. For each, we present the\nmodels in their discrete nature, discuss how to solve the ensuing discrete\nproblems and then describe convex relaxations. We also consider more general\nstructures as defined by set functions and present their convex proxies.\nFurther, we discuss efficient optimization solutions for structured sparsity\nproblems and illustrate structured sparsity in action via three applications. \n\n"}
{"id": "1507.06464", "contents": "Title: A note on the reachability of a Fibonacci control system Abstract: Motivated by applications in robotics, we investigate a discrete control\nsystem related Fibonacci sequence and we characterize its reachable set. \n\n"}
{"id": "1507.06738", "contents": "Title: Linear Contextual Bandits with Knapsacks Abstract: We consider the linear contextual bandit problem with resource consumption,\nin addition to reward generation. In each round, the outcome of pulling an arm\nis a reward as well as a vector of resource consumptions. The expected values\nof these outcomes depend linearly on the context of that arm. The\nbudget/capacity constraints require that the total consumption doesn't exceed\nthe budget for each resource. The objective is once again to maximize the total\nreward. This problem turns out to be a common generalization of classic linear\ncontextual bandits (linContextual), bandits with knapsacks (BwK), and the\nonline stochastic packing problem (OSPP). We present algorithms with\nnear-optimal regret bounds for this problem. Our bounds compare favorably to\nresults on the unstructured version of the problem where the relation between\nthe contexts and the outcomes could be arbitrary, but the algorithm only\ncompetes against a fixed set of policies accessible through an optimization\noracle. We combine techniques from the work on linContextual, BwK, and OSPP in\na nontrivial manner while also tackling new difficulties that are not present\nin any of these special cases. \n\n"}
{"id": "1507.07145", "contents": "Title: Nearly convex sets: fine properties and domains or ranges of\n  subdifferentials of convex functions Abstract: Nearly convex sets play important roles in convex analysis, optimization and\ntheory of monotone operators. We give a systematic study of nearly convex sets,\nand construct examples of subdifferentials of lower semicontinuous convex\nfunctions whose domain or ranges are nonconvex. \n\n"}
{"id": "1507.07595", "contents": "Title: Distributed Stochastic Variance Reduced Gradient Methods and A Lower\n  Bound for Communication Complexity Abstract: We study distributed optimization algorithms for minimizing the average of\nconvex functions. The applications include empirical risk minimization problems\nin statistical machine learning where the datasets are large and have to be\nstored on different machines. We design a distributed stochastic variance\nreduced gradient algorithm that, under certain conditions on the condition\nnumber, simultaneously achieves the optimal parallel runtime, amount of\ncommunication and rounds of communication among all distributed first-order\nmethods up to constant factors. Our method and its accelerated extension also\noutperform existing distributed algorithms in terms of the rounds of\ncommunication as long as the condition number is not too large compared to the\nsize of data in each machine. We also prove a lower bound for the number of\nrounds of communication for a broad class of distributed first-order methods\nincluding the proposed algorithms in this paper. We show that our accelerated\ndistributed stochastic variance reduced gradient algorithm achieves this lower\nbound so that it uses the fewest rounds of communication among all distributed\nfirst-order algorithms. \n\n"}
{"id": "1507.08263", "contents": "Title: Convergence rate for a Gauss collocation method applied to unconstrained\n  optimal control Abstract: A local convergence rate is established for an orthogonal collocation method\nbased on Gauss quadrature applied to an unconstrained optimal control problem.\nIf the continuous problem has a sufficiently smooth solution and the\nHamiltonian satisfies a strong convexity condition, then the discrete problem\npossesses a local minimizer in a neighborhood of the continuous solution, and\nas the number of collocation points increases, the discrete solution\nconvergences exponentially fast in the sup-norm to the continuous solution.\nThis is the first convergence rate result for an orthogonal collocation method\nbased on global polynomials applied to an optimal control problem. \n\n"}
{"id": "1507.08322", "contents": "Title: Distributed Mini-Batch SDCA Abstract: We present an improved analysis of mini-batched stochastic dual coordinate\nascent for regularized empirical loss minimization (i.e. SVM and SVM-type\nobjectives). Our analysis allows for flexible sampling schemes, including where\ndata is distribute across machines, and combines a dependence on the smoothness\nof the loss and/or the data spread (measured through the spectral norm). \n\n"}
{"id": "1507.08366", "contents": "Title: On the matrix square root via geometric optimization Abstract: This paper is triggered by the preprint \"\\emph{Computing Matrix Squareroot\nvia Non Convex Local Search}\" by Jain et al.\n(\\textit{\\textcolor{blue}{arXiv:1507.05854}}), which analyzes gradient-descent\nfor computing the square root of a positive definite matrix. Contrary to claims\nof~\\citet{jain2015}, our experiments reveal that Newton-like methods compute\nmatrix square roots rapidly and reliably, even for highly ill-conditioned\nmatrices and without requiring commutativity. We observe that gradient-descent\nconverges very slowly primarily due to tiny step-sizes and ill-conditioning. We\nderive an alternative first-order method based on geodesic convexity: our\nmethod admits a transparent convergence analysis ($< 1$ page), attains linear\nrate, and displays reliable convergence even for rank deficient problems.\nThough superior to gradient-descent, ultimately our method is also outperformed\nby a well-known scaled Newton method. Nevertheless, the primary value of our\nwork is its conceptual value: it shows that for deriving gradient based methods\nfor the matrix square root, \\emph{the manifold geometric view of positive\ndefinite matrices can be much more advantageous than the Euclidean view}. \n\n"}
{"id": "1508.00193", "contents": "Title: Extended ADMM and BCD for Nonseparable Convex Minimization Models with\n  Quadratic Coupling Terms: Convergence Analysis and Insights Abstract: In this paper, we establish the convergence of the proximal alternating\ndirection method of multipliers (ADMM) and block coordinate descent (BCD) for\nnonseparable minimization models with quadratic coupling terms. The novel\nconvergence results presented in this paper answer several open questions that\nhave been the subject of considerable discussion. We firstly extend the 2-block\nproximal ADMM to linearly constrained convex optimization with a coupled\nquadratic objective function, an area where theoretical understanding is\ncurrently lacking, and prove that the sequence generated by the proximal ADMM\nconverges in point-wise manner to a primal-dual solution pair. Moreover, we\napply randomly permuted ADMM (RPADMM) to nonseparable multi-block convex\noptimization, and prove its expected convergence for a class of nonseparable\nquadratic programming problems. When the linear constraint vanishes, the\n2-block proximal ADMM and RPADMM reduce to the 2-block cyclic proximal BCD\nmethod and randomly permuted BCD (RPBCD). Our study provides the first iterate\nconvergence result for 2-block cyclic proximal BCD without assuming the\nboundedness of the iterates. We also theoretically establish the expected\niterate convergence result concerning multi-block RPBCD for convex quadratic\noptimization. In addition, we demonstrate that RPBCD may have a worse\nconvergence rate than cyclic proximal BCD for 2-block convex quadratic\nminimization problems. Although the results on RPADMM and RPBCD are restricted\nto quadratic minimization models, they provide some interesting insights: 1)\nrandom permutation makes ADMM and BCD more robust for multi-block convex\nminimization problems; 2) cyclic BCD may outperform RPBCD for \"nice\" problems,\nand therefore RPBCD should be applied with caution when solving general convex\noptimization problems. \n\n"}
{"id": "1508.00536", "contents": "Title: Estimating Mutual Information by Local Gaussian Approximation Abstract: Estimating mutual information (MI) from samples is a fundamental problem in\nstatistics, machine learning, and data analysis. Recently it was shown that a\npopular class of non-parametric MI estimators perform very poorly for strongly\ndependent variables and have sample complexity that scales exponentially with\nthe true MI. This undesired behavior was attributed to the reliance of those\nestimators on local uniformity of the underlying (and unknown) probability\ndensity function. Here we present a novel semi-parametric estimator of mutual\ninformation, where at each sample point, densities are {\\em locally}\napproximated by a Gaussians distribution. We demonstrate that the estimator is\nasymptotically unbiased. We also show that the proposed estimator has a\nsuperior performance compared to several baselines, and is able to accurately\nmeasure relationship strengths over many orders of magnitude. \n\n"}
{"id": "1508.01000", "contents": "Title: Uniform Quadratic Optimization and Extensions Abstract: The uniform quadratic optimizatin problem (UQ) is a nonconvex quadratic\nconstrained quadratic programming (QCQP) sharing the same Hessian matrix. Based\non the second-order cone programming (SOCP) relaxation, we establish a new\nsufficient condition to guarantee strong duality for (UQ) and then extend it to\n(QCQP), which not only covers several well-known results in literature but also\npartially gives answers to a few open questions. For convex constrained\nnonconvex (UQ), we propose an improved approximation algorithm based on (SOCP).\nOur approximation bound is dimensional independent. As an application, we\nestablish the first approximation bound for the problem of finding the\nChebyshev center of the intersection of several balls. \n\n"}
{"id": "1508.02324", "contents": "Title: Adaptive Sampling of RF Fingerprints for Fine-grained Indoor\n  Localization Abstract: Indoor localization is a supporting technology for a broadening range of\npervasive wireless applications. One promis- ing approach is to locate users\nwith radio frequency fingerprints. However, its wide adoption in real-world\nsystems is challenged by the time- and manpower-consuming site survey process,\nwhich builds a fingerprint database a priori for localization. To address this\nproblem, we visualize the 3-D RF fingerprint data as a function of locations\n(x-y) and indices of access points (fingerprint), as a tensor and use tensor\nalgebraic methods for an adaptive tubal-sampling of this fingerprint space. In\nparticular using a recently proposed tensor algebraic framework in [1] we\ncapture the complexity of the fingerprint space as a low-dimensional\ntensor-column space. In this formulation the proposed scheme exploits\nadaptivity to identify reference points which are highly informative for\nlearning this low-dimensional space. Further, under certain incoherency\nconditions we prove that the proposed scheme achieves bounded recovery error\nand near-optimal sampling complexity. In contrast to several existing work that\nrely on random sampling, this paper shows that adaptivity in sampling can lead\nto significant improvements in localization accuracy. The approach is validated\non both data generated by the ray-tracing indoor model which accounts for the\nfloor plan and the impact of walls and the real world data. Simulation results\nshow that, while maintaining the same localization accuracy of existing\napproaches, the amount of samples can be cut down by 71% for the high SNR case\nand 55% for the low SNR case. \n\n"}
{"id": "1508.02940", "contents": "Title: Mixed Integer Reformulations of Integer Programs and the Affine\n  TU-dimension of a Matrix Abstract: We study the reformulation of integer linear programs by means of a mixed\ninteger linear program with fewer integer variables. Such reformulations can be\nsolved efficiently with mixed integer linear programming techniques. We exhibit\nexamples that demonstrate how integer programs can be reformulated using far\nfewer integer variables. To this end, we introduce a generalization of total\nunimodularity called the \\emph{affine TU-dimension} of a matrix and study\nrelated theory and algorithms for determining the affine TU-dimension of a\nmatrix. We also present bounds on the number of integer variables needed to\nrepresent certain integer hulls. \n\n"}
{"id": "1508.03571", "contents": "Title: From innovation to diversification: a simple competitive model Abstract: Few attempts have been proposed in order to describe the statistical features\nand historical evolution of the export bipartite matrix countries/products. An\nimportant standpoint is the introduction of a products network, namely a\nhierarchical forest of products that models the formation and the evolution of\ncommodities. In the present article, we propose a simple dynamical model where\ncountries compete with each other to acquire the ability to produce and export\nnew products. Countries will have two possibilities to expand their export:\ninnovating, i.e. introducing new goods, namely new nodes in the product\nnetworks, or copying the productive process of others, i.e. occupying a node\nalready present in the same network. In this way, the topology of the products\nnetwork and the country-product matrix evolve simultaneously, driven by the\ncountries push toward innovation. \n\n"}
{"id": "1508.05156", "contents": "Title: Linear convergence of the generalized PPA and several splitting methods\n  for the composite inclusion problem Abstract: For the inclusion problem involving two maximal monotone operators, under the\nmetric subregularity of the composite operator, we derive the linear\nconvergence of the generalized proximal point algorithm and several splitting\nalgorithms, which include the over-relaxed forward-backward splitting\nalgorithm, the generalized Douglas-Rachford splitting algorithm and Davis'\nthree-operator splitting algorithm. To the best of our knowledge, this linear\nconvergence condition is weaker than the existing ones that almost all require\nthe strong monotonicity of the composite operator. Withal, we give some\nsufficient conditions to ensure the metric subregularity of the composite\noperator. At last, the preliminary numerical performances on some toy examples\nsupport the theoretical results. \n\n"}
{"id": "1508.06182", "contents": "Title: Solving the Optimal Trading Trajectory Problem Using a Quantum Annealer Abstract: We solve a multi-period portfolio optimization problem using D-Wave Systems'\nquantum annealer. We derive a formulation of the problem, discuss several\npossible integer encoding schemes, and present numerical examples that show\nhigh success rates. The formulation incorporates transaction costs (including\npermanent and temporary market impact), and, significantly, the solution does\nnot require the inversion of a covariance matrix. The discrete multi-period\nportfolio optimization problem we solve is significantly harder than the\ncontinuous variable problem. We present insight into how results may be\nimproved using suitable software enhancements, and why current quantum\nannealing technology limits the size of problem that can be successfully solved\ntoday. The formulation presented is specifically designed to be scalable, with\nthe expectation that as quantum annealing technology improves, larger problems\nwill be solvable using the same techniques. \n\n"}
{"id": "1508.07065", "contents": "Title: A dual descent algorithm for node-capacitated multiflow problems and its\n  applications Abstract: In this paper, we develop an $O((m \\log k) {\\rm MSF} (n,m,1))$-time algorithm\nto find a half-integral node-capacitated multiflow of the maximum total\nflow-value in a network with $n$ nodes, $m$ edges, and $k$ terminals, where\n${\\rm MSF} (n',m',\\gamma)$ denotes the time complexity of solving the maximum\nsubmodular flow problem in a network with $n'$ nodes, $m'$ edges, and the\ncomplexity $\\gamma$ of computing the exchange capacity of the submodular\nfunction describing the problem. By using Fujishige-Zhang algorithm for\nsubmodular flow, we can find a maximum half-integral multiflow in $O(m n^3 \\log\nk)$ time. This is the first combinatorial strongly polynomial time algorithm\nfor this problem. Our algorithm is built on a developing theory of discrete\nconvex functions on certain graph structures. Applications include\n\"ellipsoid-free\" combinatorial implementations of a 2-approximation algorithm\nfor the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis. \n\n"}
{"id": "1508.07243", "contents": "Title: Bilevel parameter learning for higher-order total variation\n  regularisation models Abstract: We consider a bilevel optimisation approach for parameter learning in\nhigher-order total variation image reconstruction models. Apart from the least\nsquares cost functional, naturally used in bilevel learning, we propose and\nanalyse an alternative cost, based on a Huber regularised TV-seminorm.\nDifferentiability properties of the solution operator are verified and a\nfirst-order optimality system is derived. Based on the adjoint information, a\nquasi-Newton algorithm is proposed for the numerical solution of the bilevel\nproblems. Numerical experiments are carried out to show the suitability of our\napproach and the improved performance of the new cost functional. Thanks to the\nbilevel optimisation framework, also a detailed comparison between TGV$^2$ and\nICTV is carried out, showing the advantages and shortcomings of both\nregularisers, depending on the structure of the processed images and their\nnoise level. \n\n"}
{"id": "1508.07854", "contents": "Title: Inverse problems for linear parabolic equations using mixed formulations\n  - Part 1 : Theoretical analysis Abstract: We introduce in this document a direct method allowing to solve numerically\ninverse type problems for linear parabolic equations. We consider the\nreconstruction of the full solution of the parabolic equation posed in\n$\\Omega\\times (0,T)$ - $\\Omega$ a bounded subset of $\\mathbb{R}^N$ - from a\npartial distributed observation. We employ a least-squares technique and\nminimize the $L^2$-norm of the distance from the observation to any solution.\nTaking the parabolic equation as the main constraint of the problem, the\noptimality conditions are reduced to a mixed formulation involving both the\nstate to reconstruct and a Lagrange multiplier. The well-posedness of this\nmixed formulation - in particular the inf-sup property - is a consequence of\nclassical energy estimates. We then reproduce the arguments to a linear first\norder system, involving the normal flux, equivalent to the linear parabolic\nequation. The method, valid in any dimension spatial dimension $N$, may also be\nemployed to reconstruct solution for boundary observations.\n  With respect to the hyperbolic situation considered in\n\\cite{NC-AM-InverseProblems} by the first author, the parabolic situation\nrequires - due to regularization properties - the introduction of appropriate\nweights function so as to make the problem numerically stable. \n\n"}
{"id": "1508.07941", "contents": "Title: Optimal Entropy-Transport problems and a new Hellinger-Kantorovich\n  distance between positive measures Abstract: We develop a full theory for the new class of Optimal Entropy-Transport\nproblems between nonnegative and finite Radon measures in general topological\nspaces.\n  They arise quite naturally by relaxing the marginal constraints typical of\nOptimal Transport problems: given a couple of finite measures (with possibly\ndifferent total mass), one looks for minimizers of the sum of a linear\ntransport functional and two convex entropy functionals, that quantify in some\nway the deviation of the marginals of the transport plan from the assigned\nmeasures.\n  As a powerful application of this theory, we study the particular case of\nLogarithmic Entropy-Transport problems and introduce the new\nHellinger-Kantorovich distance between measures in metric spaces.\n  The striking connection between these two seemingly far topics allows for a\ndeep analysis of the geometric properties of the new geodesic distance, which\nlies somehow between the well-known Hellinger-Kakutani and\nKantorovich-Wasserstein distances. \n\n"}
{"id": "1509.01010", "contents": "Title: A method for comparing non-nested models with application to\n  astrophysical searches for new physics Abstract: Searches for unknown physics and decisions between competing astrophysical\nmodels to explain data both rely on statistical hypothesis testing. The usual\napproach in searches for new physical phenomena is based on the statistical\nLikelihood Ratio Test (LRT) and its asymptotic properties. In the common\nsituation, when neither of the two models under comparison is a special case of\nthe other i.e., when the hypotheses are non-nested, this test is not\napplicable. In astrophysics, this problem occurs when two models that reside in\ndifferent parameter spaces are to be compared. An important example is the\nrecently reported excess emission in astrophysical $\\gamma$-rays and the\nquestion whether its origin is known astrophysics or dark matter. We develop\nand study a new, simple, generally applicable, frequentist method and validate\nits statistical properties using a suite of simulations studies. We exemplify\nit on realistic simulated data of the Fermi-LAT $\\gamma$-ray satellite, where\nnon-nested hypotheses testing appears in the search for particle dark matter. \n\n"}
{"id": "1509.01878", "contents": "Title: Measure, Topology and Probabilistic Reasoning in Cosmology Abstract: I explain the difficulty of making various concepts of and relating to\nprobability precise, rigorous and physically significant when attempting to\napply them in reasoning about objects (e.g., spacetimes) living in\ninfinite-dimensional spaces, working through many examples from cosmology. I\nfocus on the relation of topological to measure-theoretic notions of and\nrelating to probability, how they diverge in unpleasant ways in the\ninfinite-dimensional case, and are difficult to work with on their own as well\nin that context. Even in cases where an appropriate family of spacetimes is\nfinite-dimensional, however, and so admits a measure of the relevant sort, it\nis always the case that the family is not a compact topological space, and so\ndoes not admit a physically significant, well behaved probability measure.\nProblems of a different but still deeply troubling sort plague arguments about\nlikelihood in that context, which I also discuss. I conclude that most standard\nforms of argument used in cosmology to estimate the likelihood of the\noccurrence of various properties or behaviors of spacetimes have serious\nmathematical, physical and conceptual problems. \n\n"}
{"id": "1509.02735", "contents": "Title: Containment Problems for Projections of Polyhedra and Spectrahedra Abstract: Spectrahedra are affine sections of the cone of positive semidefinite\nmatrices which form a rich class of convex bodies that properly contains that\nof polyhedra. While the class of polyhedra is closed under linear projections,\nthe class of spectrahedra is not. In this paper we investigate the problem of\ndeciding containment of projections of polyhedra and spectrahedra based on\nprevious works on containment of spectrahedra. The main concern is to study\nthese containment problems by formulating them as polynomial nonnegativity\nproblems. This allows to state hierarchies of (sufficient) semidefinite\nconditions by applying (and proving) sophisticated Positivstellens\\\"atze. We\nalso extend results on a solitary sufficient condition for containment of\nspectrahedra coming from the polyhedral situation as well as connections to the\ntheory of (completely) positive linear maps. \n\n"}
{"id": "1509.03503", "contents": "Title: NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining Abstract: The detection of triadic subgraph motifs is a common methodology in\ncomplex-networks research. The procedure usually applied in order to detect\nmotifs evaluates whether a certain subgraph pattern is overrepresented in a\nnetwork as a whole. However, motifs do not necessarily appear frequently in\nevery region of a graph. For this reason, we recently introduced the framework\nof Node-Specific Pattern Mining (NoSPaM). This work is a manual for an\nimplementation of NoSPaM which can be downloaded from www.mwinkler.eu. \n\n"}
{"id": "1509.05064", "contents": "Title: Exact simultaneous recovery of locations and structure from known\n  orientations and corrupted point correspondences Abstract: Let $t_1,\\ldots,t_{n_l} \\in \\mathbb{R}^d$ and $p_1,\\ldots,p_{n_s} \\in\n\\mathbb{R}^d$ and consider the bipartite location recovery problem: given a\nsubset of pairwise direction observations $\\{(t_i - p_j) / \\|t_i -\np_j\\|_2\\}_{i,j \\in [n_l] \\times [n_s]}$, where a constant fraction of these\nobservations are arbitrarily corrupted, find $\\{t_i\\}_{i \\in [n_ll]}$ and\n$\\{p_j\\}_{j \\in [n_s]}$ up to a global translation and scale. We study the\nrecently introduced ShapeFit algorithm as a method for solving this bipartite\nlocation recovery problem. In this case, ShapeFit consists of a simple convex\nprogram over $d(n_l + n_s)$ real variables. We prove that this program recovers\na set of $n_l+n_s$ i.i.d. Gaussian locations exactly and with high probability\nif the observations are given by a bipartite Erd\\H{o}s-R\\'{e}nyi graph, $d$ is\nlarge enough, and provided that at most a constant fraction of observations\ninvolving any particular location are adversarially corrupted. This recovery\ntheorem is based on a set of deterministic conditions that we prove are\nsufficient for exact recovery. Finally, we propose a modified pipeline for the\nStructure for Motion problem, based on this bipartite location recovery\nproblem. \n\n"}
{"id": "1509.05065", "contents": "Title: Estimating operator norms using covering nets Abstract: We present several polynomial- and quasipolynomial-time approximation schemes\nfor a large class of generalized operator norms. Special cases include the\n$2\\rightarrow q$ norm of matrices for $q>2$, the support function of the set of\nseparable quantum states, finding the least noisy output of\nentanglement-breaking quantum channels, and approximating the injective tensor\nnorm for a map between two Banach spaces whose factorization norm through\n$\\ell_1^n$ is bounded.\n  These reproduce and in some cases improve upon the performance of previous\nalgorithms by Brand\\~ao-Christandl-Yard and followup work, which were based on\nthe Sum-of-Squares hierarchy and whose analysis used techniques from quantum\ninformation such as the monogamy principle of entanglement. Our algorithms, by\ncontrast, are based on brute force enumeration over carefully chosen covering\nnets. These have the advantage of using less memory, having much simpler proofs\nand giving new geometric insights into the problem. Net-based algorithms for\nsimilar problems were also presented by Shi-Wu and Barak-Kelner-Steurer, but in\neach case with a run-time that is exponential in the rank of some matrix. We\nachieve polynomial or quasipolynomial runtimes by using the much smaller nets\nthat exist in $\\ell_1$ spaces. This principle has been used in learning theory,\nwhere it is known as Maurey's empirical method. \n\n"}
{"id": "1509.05586", "contents": "Title: Ear-decompositions and the complexity of the matching polytope Abstract: The complexity of the matching polytope of graphs may be measured with the\nmaximum length $\\beta$ of a starting sequence of odd ears in an\near-decomposition. Indeed, a theorem of Edmonds and Pulleyblank shows that its\nfacets are defined by 2-connected factor-critical graphs, which have an odd\near-decomposition (according to a theorem of Lov\\'asz). In particular,\n$\\beta(G) \\leq 1$ if and only if the matching polytope of the graph $G$ is\ncompletely described by non-negativity, star and odd-circuit inequalities. This\nis essentially equivalent to the h-perfection of the line-graph of $G$, as\nobserved by Cao and Nemhauser.\n  The complexity of computing $\\beta$ is apparently not known. We show that\ndeciding whether $\\beta(G)\\leq 1$ can be executed efficiently by looking at any\near-decomposition starting with an odd circuit and performing basic modulo-2\ncomputations. Such a greedy-approach is surprising in view of the complexity of\nthe problem in more special cases by Bruhn and Schaudt, and it is simpler than\nusing the Parity Minor Algorithm.\n  Our results imply a simple polynomial-time algorithm testing h-perfection in\nline-graphs (deciding h-perfection is open in general). We also generalize our\napproach to binary matroids and show that computing $\\beta$ is a\nFixed-Parameter-Tractable problem (FPT). \n\n"}
{"id": "1509.06582", "contents": "Title: Stability of saddle points via explicit coderivatives of pointwise\n  subdifferentials Abstract: We derive stability criteria for saddle points of a class of nonsmooth\noptimization problems in Hilbert spaces arising in PDE-constrained\noptimization, using metric regularity of infinite-dimensional set-valued\nmappings. A main ingredient is an explicit pointwise characterization of the\nFr\\'echet coderivative of the subdifferential of convex integral functionals.\nThis is applied to several stability properties for parameter identification\nproblems for an elliptic partial differential equation with non-differentiable\ndata fitting terms. \n\n"}
{"id": "1509.07617", "contents": "Title: Optimal frequency regulation in nonlinear power networks including\n  turbine-governor dynamics Abstract: Motivated by an increase of renewable energy sources we propose a distributed\noptimal Load Frequency Control scheme achieving frequency regulation and\neconomic dispatch. Based on an energy function of the power network we derive\nan incremental passivity property for a well known nonlinear structure\npreserving network model, differentiating between generator and load buses.\nExploiting this property we design distributed controllers that adjust the\npower generation. Notably, we explicitly include the turbine-governor dynamics\nwhere first-order and the widely used second-order dynamics are analyzed in a\nunifying way. Due to the non-passive nature of the second-order\nturbine-governor dynamics, incorporating them is challenging and we develop a\nsuitable dissipation inequality for the interconnected generator and\nturbine-governor. This allows us to include the generator side more\nrealistically in the stability analysis of optimal Load Frequency Control than\nwas previously possible. \n\n"}
{"id": "1510.00793", "contents": "Title: Skew-selfadjoint Dirac systems: stability of the procedure of explicit\n  solving the inverse problem Abstract: Procedures to recover explicitly discrete and continuous skew-selfadjoint\nDirac systems on semi-axis from rational Weyl matrix functions are considered.\nTheir stability is shown. Some new facts on asymptotics of pseudo-exponential\npotentials (i.e., of explicit solutions of inverse problems) are proved as\nwell. GBDT version of Backlund-Darboux transformation, methods from system\ntheory and results on algebraic Riccati equations are used for this purpose. \n\n"}
{"id": "1510.02171", "contents": "Title: Hybrid Methods in Solving Alternating-Current Optimal Power Flows Abstract: Many steady-state problems in power systems, including rectangular\npower-voltage formulations of optimal power flows in the alternating-current\nmodel (ACOPF), can be cast as polynomial optimisation problems (POP). For a\nPOP, one can derive strong convex relaxations, or rather hierarchies of ever\nstronger, but ever larger relaxations. We study means of switching from solving\nthe convex relaxation to Newton method working on a non-convex Lagrangian of\nthe POP. \n\n"}
{"id": "1510.02197", "contents": "Title: A characterization of linearizable instances of the quadratic minimum\n  spanning tree problem Abstract: We investigate special cases of the quadratic minimum spanning tree problem\n(QMSTP) on a graph $G=(V,E)$ that can be solved as a linear minimum spanning\ntree problem. Characterization of such problems on graphs with special\nproperties are given. This include complete graphs, complete bipartite graphs,\ncactuses among others. Our characterization can be verified in $O(|E|^2)$ time.\nIn the case of complete graphs and when the cost matrix is given in factored\nform, we show that our characterization can be verified in $O(|E|)$ time.\nRelated open problems are also indicated. \n\n"}
{"id": "1510.05185", "contents": "Title: A Local Perspective on Community Structure in Multilayer Networks Abstract: The analysis of multilayer networks is among the most active areas of network\nscience, and there are now several methods to detect dense \"communities\" of\nnodes in multilayer networks. One way to define a community is as a set of\nnodes that trap a diffusion-like dynamical process (usually a random walk) for\na long time. In this view, communities are sets of nodes that create\nbottlenecks to the spreading of a dynamical process on a network. We analyze\nthe local behavior of different random walks on multiplex networks (which are\nmultilayer networks in which different layers correspond to different types of\nedges) and show that they have very different bottlenecks that hence correspond\nto rather different notions of what it means for a set of nodes to be a good\ncommunity. This has direct implications for the behavior of community-detection\nmethods that are based on these random walks. \n\n"}
{"id": "1510.05654", "contents": "Title: On the universality of interstellar filaments: theory meets simulations\n  and observations Abstract: Filaments are ubiquitous in the universe. Recent observations have revealed\nthat stars and star clusters form preferentially along dense filaments.\nUnderstanding the formation and properties of filaments is therefore a crucial\nstep in understanding star formation. Here we perform three-dimensional\nhigh-resolution magnetohydrodynamical simulations that follow the evolution of\nmolecular clouds and the formation of filaments and stars. We apply a filament\ndetection algorithm and compare simulations with different combinations of\nphysical ingredients: gravity, turbulence, magnetic fields and jet/outflow\nfeedback. We find that gravity-only simulations produce significantly narrower\nfilament profiles than observed, while simulations that include turbulence\nproduce realistic filament properties. For these turbulence simulations, we\nfind a remarkably universal filament width of 0.10 +/- 0.02 pc, which is\nindependent of the star formation history of the clouds. We derive a\ntheoretical model that provides a physical explanation for this characteristic\nfilament width, based on the sonic scale (lambda_sonic) of molecular cloud\nturbulence. Our derivation provides lambda_sonic as a function of the cloud\ndiameter L, the velocity dispersion sigma_v, the gas sound speed c_s, and the\nratio of thermal to magnetic pressure, plasma beta. For typical cloud\nconditions in the Milky Way spiral arms, we find lambda_sonic = 0.04-0.16 pc,\nin excellent agreement with the filament width of 0.05-0.15 pc from\nobservations. Consistent with the theoretical model assumptions, we find that\nthe velocity dispersion inside the filaments is subsonic and supersonic\noutside. We further explain the observed p=2 scaling of the filament density\nprofile, rho ~ r^(-p) with the collision of two planar shocks forming a\nfilament at their intersection. \n\n"}
{"id": "1510.06096", "contents": "Title: When Are Nonconvex Problems Not Scary? Abstract: In this note, we focus on smooth nonconvex optimization problems that obey:\n(1) all local minimizers are also global; and (2) around any saddle point or\nlocal maximizer, the objective has a negative directional curvature. Concrete\napplications such as dictionary learning, generalized phase retrieval, and\northogonal tensor decomposition are known to induce such structures. We\ndescribe a second-order trust-region algorithm that provably converges to a\nglobal minimizer efficiently, without special initializations. Finally we\nhighlight alternatives, and open problems in this direction. \n\n"}
{"id": "1510.07111", "contents": "Title: Dynamic programming approach to principal-agent problems Abstract: We consider a general formulation of the Principal-Agent problem with a\nlump-sum payment on a finite horizon, providing a systematic method for solving\nsuch problems. Our approach is the following: we first find the contract that\nis optimal among those for which the agent's value process allows a dynamic\nprogramming representation, for which the agent's optimal effort is\nstraightforward to find. We then show that the optimization over the restricted\nfamily of contracts represents no loss of generality. As a consequence, we have\nreduced this non-zero sum stochastic differential game to a stochastic control\nproblem which may be addressed by the standard tools of control theory. Our\nproofs rely on the backward stochastic differential equations approach to\nnon-Markovian stochastic control, and more specifically, on the recent\nextensions to the second order case. \n\n"}
{"id": "1510.08570", "contents": "Title: A Semismooth Newton Method for Tensor Eigenvalue Complementarity Problem Abstract: In this paper, we consider the tensor eigenvalue complementarity problem\nwhich is closely related to the optimality conditions for polynomial\noptimization, as well as a class of differential inclusions with nonconvex\nprocesses. By introducing an NCP-function, we reformulate the tensor eigenvalue\ncomplementarity problem as a system of nonlinear equations. We show that this\nfunction is strongly semismooth but not differentiable, in which case the\nclassical smoothing methods cannot apply. Furthermore, we propose a damped\nsemismooth Newton method for tensor eigenvalue complementarity problem. A new\nprocedure to evaluate an element of the generalized Jocobian is given, which\nturns out to be an element of the B-subdifferential under mild assumptions. As\na result, the convergence of the damped semismooth Newton method is guaranteed\nby existing results. The numerical experiments also show that our method is\nefficient and promising. \n\n"}
{"id": "1511.00205", "contents": "Title: Eigenvalue Clustering, Control Energy, and Logarithmic Capacity Abstract: We prove two bounds showing that if the eigenvalues of a matrix are clustered\nin a region of the complex plane then the corresponding discrete-time linear\nsystem requires significant energy to control. A curious feature of one of our\nbounds is that the dependence on the region is via its logarithmic capacity,\nwhich is a measure of how well a unit of mass may be spread out over the region\nto minimize a logarithmic potential. \n\n"}
{"id": "1511.01935", "contents": "Title: A Multiresolution Ensemble Kalman Filter using Wavelet Decomposition Abstract: We present a method of using classical wavelet based multiresolution analysis\nto separate scales in model and observations during data assimilation with the\nensemble Kalman filter. In many applications, the underlying physics of a\nphenomena involve the interaction of features at multiple scales. Blending of\nobservational and model error across scales can result in large forecast\ninaccuracies since large errors at one scale are interpreted as inexact data at\nall scales. Our method uses a transformation of the observation operator in\norder to separate the information from different scales of the observations.\nThis naturally induces a transformation of the observation covariance and we\nput forward several algorithms to efficiently compute the transformed\ncovariance. Another advantage of our multiresolution ensemble Kalman filter is\nthat scales can be weighted independently to adjust each scale's effect on the\nforecast. To demonstrate feasibility we present applications to a one\ndimensional Kuramoto-Sivashinsky (K-S) model with scale dependent observation\nnoise and an application involving the forecasting of solar photospheric flux.\nThe latter example demonstrates the multiresolution ensemble Kalman filter's\nability to account for scale dependent model error. Modeling of photospheric\nmagnetic flux transport is accomplished by the Air Force Data Assimilative\nPhotospheric Transport (ADAPT) model. \n\n"}
{"id": "1511.02124", "contents": "Title: Barrier Frank-Wolfe for Marginal Inference Abstract: We introduce a globally-convergent algorithm for optimizing the\ntree-reweighted (TRW) variational objective over the marginal polytope. The\nalgorithm is based on the conditional gradient method (Frank-Wolfe) and moves\npseudomarginals within the marginal polytope through repeated maximum a\nposteriori (MAP) calls. This modular structure enables us to leverage black-box\nMAP solvers (both exact and approximate) for variational inference, and obtains\nmore accurate results than tree-reweighted algorithms that optimize over the\nlocal consistency relaxation. Theoretically, we bound the sub-optimality for\nthe proposed algorithm despite the TRW objective having unbounded gradients at\nthe boundary of the marginal polytope. Empirically, we demonstrate the\nincreased quality of results found by tightening the relaxation over the\nmarginal polytope as well as the spanning tree polytope on synthetic and\nreal-world instances. \n\n"}
{"id": "1511.02363", "contents": "Title: Frequentist tests for Bayesian models Abstract: Analogues of the frequentist chi-square and F tests are proposed for testing\ngoodness-of-fit and consistency for Bayesian models. Simple examples exhibit\nthese tests' detection of inconsistency between consecutive experiments with\nidentical parameters, when the first experiment provides the prior for the\nsecond. In a related analysis, a quantitative measure is derived for judging\nthe degree of tension between two different experiments with partially\noverlapping parameter vectors. \n\n"}
{"id": "1511.03414", "contents": "Title: A dynamic state transition algorithm with application to sensor network\n  localization Abstract: The sensor network localization (SNL) problem is to reconstruct the positions\nof all the sensors in a network with the given distance between pairs of\nsensors and within the radio range between them. It is proved that the\ncomputational complexity of the SNL problem is NP-hard, and semi-definite\nprogramming or second-order cone programming relaxation methods are only able\nto solve some special problems of this kind. In this study, a stochastic global\noptimization method called the state transition algorithm is introduced to\nsolve the SNL problem without additional assumptions and conditions of the\nproblem structure. To transcend local optimality, a novel dynamic adjustment\nstrategy called \"risk and restoration in probability\" is incorporated into the\nstate transition algorithm. An empirical study is investigated to appropriately\nchoose the \"risk probability\" and \"restoration probability\", yielding the\ndynamic state transition algorithm, which is further improved by gradient-based\nrefinement. The dynamic state transition algorithm with refinement is applied\nto the SNL problem, and satisfactory experimental results have testified the\neffectiveness of the proposed approach. \n\n"}
{"id": "1511.03790", "contents": "Title: Velocity condensation for magnetotactic bacteria Abstract: Magnetotactic swimmers tend to align along magnetic field lines against\nstochastic reorientations. We show that the swimming strategy, e.g. active\nBrownian motion versus run-and-tumble dynamics, strongly affects the\norientation statistics. The latter can exhibit a velocity condensation whereby\nthe alignment probability density diverges. As a consequence, we find that the\nswimming strategy affects the nature of the phase transition to collective\nmotion, indicating that L\\'evy run-and-tumble walks can outperform active\nBrownian processes as strategies to trigger collective behavior. \n\n"}
{"id": "1511.04032", "contents": "Title: Computing Walrasian Equilibria: Fast Algorithms and Structural\n  Properties Abstract: We present the first polynomial time algorithm for computing Walrasian\nequilibrium in an economy with indivisible goods and \\emph{general} buyer\nvaluations having only access to an \\emph{aggregate demand oracle}, i.e., an\noracle that given prices on all goods, returns the aggregated demand over the\nentire population of buyers. For the important special case of gross substitute\nvaluations, our algorithm queries the aggregate demand oracle\n$\\widetilde{O}(n)$ times and takes $\\widetilde{O}(n^3)$ time, where $n$ is the\nnumber of goods. At the heart of our solution is a method for exactly\nminimizing certain convex functions which cannot be evaluated but for which the\nsubgradients can be computed.\n  We also give the fastest known algorithm for computing Walrasian equilibrium\nfor gross substitute valuations in the \\emph{value oracle model}. Our algorithm\nhas running time $\\widetilde{O}((mn + n^3) T_V)$ where $T_V$ is the cost of\nquerying the value oracle. A key technical ingredient is to regularize a convex\nprogramming formulation of the problem in a way that subgradients are cheap to\ncompute. En route, we give necessary and sufficient conditions for the\nexistence of \\emph{robust Walrasian prices}, i.e., prices for which each agent\nhas a unique demanded bundle and the demanded bundles clear the market. When\nsuch prices exist, the market can be perfectly coordinated by solely using\nprices. \n\n"}
{"id": "1511.04516", "contents": "Title: A Realization Method for Transfer Functions of Linear Quantum Stochastic\n  Systems Using Static Networks for Input/Output Processing and Feedback Abstract: The issue of realization of the transfer functions of Linear Quantum\nStochastic Systems (LQSSs) is of fundamental importance for the practical\napplications of such systems, especially as coherent controllers for other\nquantum systems. So far, most works that addressed this problem have used\ncascade realizations. In this work, a new method is proposed, where the\ntransfer function of a LQSS is realized by a series of a pre-processing linear\nstatic network, a reduced LQSS, and a post-processing linear static network.\nThe introduction of the pre- and post-processing static networks leaves an\nintermediate reduced LQSS with a simple input/output structure, that is\nrealized by a concatenation of simple cavities. A feedback connection of the\ncavities through a linear static network is used to produce the correct\ndynamics for the reduced system. The resulting realization provides a nice\nstructural picture of the system. The key mathematical tool that allows for the\nconstruction of this realization, is an SVD-like decomposition for doubled-up\nmatrices in Krein spaces. Illustrative examples are provided for the theory\ndeveloped. \n\n"}
{"id": "1511.06343", "contents": "Title: Online Batch Selection for Faster Training of Neural Networks Abstract: Deep neural networks are commonly trained using stochastic non-convex\noptimization procedures, which are driven by gradient information estimated on\nfractions (batches) of the dataset. While it is commonly accepted that batch\nsize is an important parameter for offline tuning, the benefits of online\nselection of batches remain poorly understood. We investigate online batch\nselection strategies for two state-of-the-art methods of stochastic\ngradient-based optimization, AdaDelta and Adam. As the loss function to be\nminimized for the whole dataset is an aggregation of loss functions of\nindividual datapoints, intuitively, datapoints with the greatest loss should be\nconsidered (selected in a batch) more frequently. However, the limitations of\nthis intuition and the proper control of the selection pressure over time are\nopen questions. We propose a simple strategy where all datapoints are ranked\nw.r.t. their latest known loss value and the probability to be selected decays\nexponentially as a function of rank. Our experimental results on the MNIST\ndataset suggest that selecting batches speeds up both AdaDelta and Adam by a\nfactor of about 5. \n\n"}
{"id": "1511.07086", "contents": "Title: Neighboring Optimal Guidance for Low-Thrust Multi-Burn Orbital Transfers Abstract: This paper presents a novel neighboring extremal approach to establish the\nneighboring optimal guidance (NOG) strategy for fixed-time low-thrust\nmulti-burn orbital transfer problems. Unlike the classical variational methods\nwhich define and solve an accessory minimum problem (AMP) to design the NOG,\nthe core of the proposed method is to construct a parameterized family of\nneighboring extremals around a nominal one. A geometric analysis on the\nprojection behavior of the parameterized neighboring extremals shows that it is\nimpossible to establish the NOG unless not only the typical Jacobi condition\n(JC) between switching times but also a transversal condition (TC) at each\nswitching time is satisfied. According to the theory of field of extremals, the\nJC and the TC, once satisfied, are also sufficient to ensure a multi-burn\nextremal trajectory to be locally optimal. Then, through deriving the\nfirst-order Taylor expansion of the parameterized neighboring extremals, the\nneighboring optimal feedbacks on thrust direction and switching times are\nobtained. Finally, to verify the development of this paper, a fixed-time\nlow-thrust fuel-optimal orbital transfer problem is calculated. \n\n"}
{"id": "1511.07562", "contents": "Title: Efficient Dynamic Compressor Optimization in Natural Gas Transmission\n  Systems Abstract: The growing reliance of electric power systems on gas-fired generation to\nbalance intermittent sources of renewable energy has increased the variation\nand volume of flows through natural gas transmission pipelines. Adapting\npipeline operations to maintain efficiency and security under these new\nconditions requires optimization methods that account for transients and that\ncan quickly compute solutions in reaction to generator re-dispatch. This paper\npresents an efficient scheme to minimize compression costs under dynamic\nconditions where deliveries to customers are described by time-dependent mass\nflow. The optimization scheme relies on a compact representation of gas flow\nphysics, a trapezoidal discretization in time and space, and a two-stage\napproach to minimize energy costs and maximize smoothness. The resulting\nlarge-scale nonlinear programs are solved using a modern interior-point method.\nThe proposed optimization scheme is validated against an integration of dynamic\nequations with adaptive time-stepping, as well as a recently proposed\nstate-of-the-art optimal control method. The comparison shows that the\nsolutions are feasible for the continuous problem and also practical from an\noperational standpoint. The results also indicate that our scheme provides at\nleast an order of magnitude reduction in computation time relative to the\nstate-of-the-art and scales to large gas transmission networks with more than\n6000 kilometers of total pipeline. \n\n"}
{"id": "1511.08974", "contents": "Title: Quantum Weiss-Weinstein bounds for quantum metrology Abstract: Sensing and imaging are among the most important applications of quantum\ninformation science. To investigate their fundamental limits and the\npossibility of quantum enhancements, researchers have for decades relied on the\nquantum Cram\\'er-Rao lower error bounds pioneered by Helstrom. Recent work,\nhowever, has called into question the tightness of those bounds for highly\nnonclassical states in the non-asymptotic regime, and better methods are now\nneeded to assess the attainable quantum limits in reality. Here we propose a\nnew class of quantum bounds called quantum Weiss-Weinstein bounds, which\ninclude Cram\\'er-Rao-type inequalities as special cases but can also be\nsignificantly tighter to the attainable error. We demonstrate the superiority\nof our bounds through the derivation of a Heisenberg limit and phase-estimation\nexamples. \n\n"}
{"id": "1511.09349", "contents": "Title: An analysis of the benefits of signal injection for low-speed sensorless\n  control of induction motors Abstract: We analyze why low-speed sensorless control of the IM is intrinsically\ndifficult, and what is gained by signal injection. The explanation relies on\nthe control-theoretic concept of observability applied to a general model of\nthe saturated IM. We show that the IM is not observable when the stator speed\nis zero in the absence of signal injection, but that observability is restored\nthanks to signal injection and magnetic saturation. The analysis also reveals\nthat existing sensorless algorithms based on signal injection may perform\npoorly for some IMs under particular operating conditions. The approach is\nillustrated by simulations and experimental data. \n\n"}
{"id": "1512.00905", "contents": "Title: Shape-constrained uncertainty quantification in unfolding steeply\n  falling elementary particle spectra Abstract: The high energy physics unfolding problem is an important statistical inverse\nproblem in data analysis at the Large Hadron Collider (LHC) at CERN. The goal\nof unfolding is to make nonparametric inferences about a particle spectrum from\nmeasurements smeared by the finite resolution of the particle detectors.\nPrevious unfolding methods use ad hoc discretization and regularization,\nresulting in confidence intervals that can have significantly lower coverage\nthan their nominal level. Instead of regularizing using a roughness penalty or\nstopping iterative methods early, we impose physically motivated shape\nconstraints: positivity, monotonicity, and convexity. We quantify the\nuncertainty by constructing a nonparametric confidence set for the true\nspectrum, consisting of all those spectra that satisfy the shape constraints\nand that predict the observations within an appropriately calibrated level of\nfit. Projecting that set produces simultaneous confidence intervals for all\nfunctionals of the spectrum, including averages within bins. The confidence\nintervals have guaranteed conservative frequentist finite-sample coverage in\nthe important and challenging class of unfolding problems for steeply falling\nparticle spectra. We demonstrate the method using simulations that mimic\nunfolding the inclusive jet transverse momentum spectrum at the LHC. The\nshape-constrained intervals provide usefully tight conservative inferences,\nwhile the conventional methods suffer from severe undercoverage. \n\n"}
{"id": "1512.02549", "contents": "Title: Facial Reduction and Partial Polyhedrality Abstract: We present FRA-Poly, a facial reduction algorithm (FRA) for conic linear\nprograms that is sensitive to the presence of polyhedral faces in the cone. The\nmain goals of FRA and FRA-Poly are the same, i.e., finding the minimal face\ncontaining the feasible region and detecting infeasibility, but FRA-Poly treats\npolyhedral constraints separately. This idea enables us to reduce the number of\niterations drastically when there are many linear inequality constraints. The\nworst case number of iterations for FRA-poly is written in the terms of a\n\"distance to polyhedrality\" quantity and provides better bounds than FRA under\nmild conditions. In particular, in the case of the doubly nonnegative cone,\nFRA-Poly gives a worst case bound of $n$ whereas the classical FRA is\n$\\mathcal{O}(n^2)$. Of possible independent interest, we prove a variant of\nGordan-Stiemke's Theorem and a proper separation theorem that takes into\naccount partial polyhedrality. We provide a discussion on the optimal facial\nreduction strategy and an instance that forces FRAs to perform many steps. We\nalso present a few applications. In particular, we will use FRA-poly to improve\nthe bounds recently obtained by Liu and Pataki on the dimension of certain\naffine subspaces which appear in weakly infeasible problems. \n\n"}
{"id": "1512.05489", "contents": "Title: Data-driven Inverse Optimization with Imperfect Information Abstract: In data-driven inverse optimization an observer aims to learn the preferences\nof an agent who solves a parametric optimization problem depending on an\nexogenous signal. Thus, the observer seeks the agent's objective function that\nbest explains a historical sequence of signals and corresponding optimal\nactions. We focus here on situations where the observer has imperfect\ninformation, that is, where the agent's true objective function is not\ncontained in the search space of candidate objectives, where the agent suffers\nfrom bounded rationality or implementation errors, or where the observed\nsignal-response pairs are corrupted by measurement noise. We formalize this\ninverse optimization problem as a distributionally robust program minimizing\nthe worst-case risk that the {\\em predicted} decision ({\\em i.e.}, the decision\nimplied by a particular candidate objective) differs from the agent's {\\em\nactual} response to a random signal. We show that our framework offers rigorous\nout-of-sample guarantees for different loss functions used to measure\nprediction errors and that the emerging inverse optimization problems can be\nexactly reformulated as (or safely approximated by) tractable convex programs\nwhen a new suboptimality loss function is used. We show through extensive\nnumerical tests that the proposed distributionally robust approach to inverse\noptimization attains often better out-of-sample performance than the\nstate-of-the-art approaches. \n\n"}
{"id": "1512.05619", "contents": "Title: Modeling Joint Improvisation between Human and Virtual Players in the\n  Mirror Game Abstract: Joint improvisation is observed to emerge spontaneously among humans\nperforming joint action tasks, and has been associated with high levels of\nmovement synchrony and enhanced sense of social bonding. Exploring the\nunderlying cognitive and neural mechanisms behind the emergence of joint\nimprovisation is an open research challenge. This paper investigates the\nemergence of jointly improvised movements between two participants in the\nmirror game, a paradigmatic joint task example. A theoretical model based on\nobservations and analysis of experimental data is proposed to capture the main\nfeatures of their interaction. A set of experiments is carried out to test and\nvalidate the model ability to reproduce the experimental observations. Then,\nthe model is used to drive a computer avatar able to improvise joint motion\nwith a human participant in real time. Finally, a convergence analysis of the\nproposed model is carried out to confirm its ability to reproduce the emergence\nof joint movement between the participants. \n\n"}
{"id": "1512.05878", "contents": "Title: Non-representable hyperbolic matroids Abstract: The generalized Lax conjecture asserts that each hyperbolicity cone is a\nlinear slice of the cone of positive semidefinite matrices. Hyperbolic\npolynomials give rise to a class of (hyperbolic) matroids which properly\ncontains the class of matroids representable over the complex numbers. This\nconnection was used by the second author to construct counterexamples to\nalgebraic (stronger) versions of the generalized Lax conjecture by considering\na non-representable hyperbolic matroid. The V\\'amos matroid and a\ngeneralization of it are, prior to this work, the only known instances of\nnon-representable hyperbolic matroids.\n  We prove that the Non-Pappus and Non-Desargues matroids are non-representable\nhyperbolic matroids by exploiting a connection between Euclidean Jordan\nalgebras and projective geometries. We further identify a large class of\nhyperbolic matroids which contains the V\\'amos matroid and the generalized\nV\\'amos matroids recently studied by Burton, Vinzant and Youm. This proves a\nconjecture of Burton et al. We also prove that many of the matroids considered\nhere are non-representable. The proof of hyperbolicity for the matroids in the\nclass depends on proving nonnegativity of certain symmetric polynomials. In\nparticular we generalize and strengthen several inequalities in the literature,\nsuch as the Laguerre-Tur\\'an inequality and Jensen's inequality. Finally we\nexplore consequences to algebraic versions of the generalized Lax conjecture. \n\n"}
{"id": "1512.08645", "contents": "Title: Continuity argument revisited: geometry of root clustering via symmetric\n  products Abstract: We study the spaces of polynomials stratified into the sets of polynomial\nwith fixed number of roots inside certain semialgebraic region $\\Omega$, on its\nborder, and at the complement to its closure. Presented approach is a\ngeneralisation, unification and development of several classical approaches to\nstability problems in control theory: root clustering ($D$-stability) developed\nby R.E. Kalman, B.R. Barmish, S. Gutman et al., $D$-decomposition(Yu.I.\nNeimark, B.T. Polyak, E.N. Gryazina) and universal parameter space method(A.\nFam, J. Meditch, J.Ackermann).\n  Our approach is based on the interpretation of correspondence between roots\nand coefficients of a polynomial as a symmetric product morphism.\n  We describe the topology of strata up to homotopy equivalence and, for many\nimportant cases, up to homeomorphism. Adjacencies between strata are also\ndescribed. Moreover, we provide an explanation for the special position of\nclassical stability problems: Hurwitz stability, Schur stability,\nhyperbolicity. \n\n"}
{"id": "1601.02239", "contents": "Title: On minimax theorems for lower semicontinuous functions in Hilbert spaces Abstract: We prove minimax theorems for lower semicontinuous functions defined on a\nHilbert space. The main tool is the theory of $\\Phi$-convex functions and\nsufficient and necessary conditions for the minimax equality to hold for\n$\\Phi$-convex functions. These conditions are expressed in terms of abstract\n$\\Phi$-subgradients. \n\n"}
{"id": "1601.04037", "contents": "Title: Funnel Libraries for Real-Time Robust Feedback Motion Planning Abstract: We consider the problem of generating motion plans for a robot that are\nguaranteed to succeed despite uncertainty in the environment, parametric model\nuncertainty, and disturbances. Furthermore, we consider scenarios where these\nplans must be generated in real-time, because constraints such as obstacles in\nthe environment may not be known until they are perceived (with a noisy sensor)\nat runtime. Our approach is to pre-compute a library of \"funnels\" along\ndifferent maneuvers of the system that the state is guaranteed to remain within\n(despite bounded disturbances) when the feedback controller corresponding to\nthe maneuver is executed. We leverage powerful computational machinery from\nconvex optimization (sums-of-squares programming in particular) to compute\nthese funnels. The resulting funnel library is then used to sequentially\ncompose motion plans at runtime while ensuring the safety of the robot. A major\nadvantage of the work presented here is that by explicitly taking into account\nthe effect of uncertainty, the robot can evaluate motion plans based on how\nvulnerable they are to disturbances.\n  We demonstrate and validate our method using extensive hardware experiments\non a small fixed-wing airplane avoiding obstacles at high speed (~12 mph),\nalong with thorough simulation experiments of ground vehicle and quadrotor\nmodels navigating through cluttered environments. To our knowledge, these\ndemonstrations constitute one of the first examples of provably safe and robust\ncontrol for robotic systems with complex nonlinear dynamics that need to plan\nin real-time in environments with complex geometric constraints. \n\n"}
{"id": "1601.05783", "contents": "Title: Uniform observability estimates for linear waves Abstract: In this article, we give a completely constructive proof of the\nobservability/controllability of the wave equation on a compact manifold under\noptimal geometric conditions. This contrasts with the original proof of\nBardos-Lebeau-Rauch, which contains two non-constructive arguments. Our method\nis based on the Dehman-Lebeau Egorov approach to treat the high-frequencies,\nand the optimal unique continuation stability result of the authors for the\nlow-frequencies.\n  As an application, we first give estimates of the blowup of the observability\nconstant when the time tends to the limit geometric control time (for wave\nequations with possibly lower order terms). Second, we provide (on manifolds\nwith or without boundary) with an explicit dependence of the observability\nconstant with respect to the addition of a bounded potential to the equation. \n\n"}
{"id": "1601.06035", "contents": "Title: Recommender systems inspired by the structure of quantum theory Abstract: Physicists use quantum models to describe the behavior of physical systems.\nQuantum models owe their success to their interpretability, to their relation\nto probabilistic models (quantization of classical models) and to their high\npredictive power. Beyond physics, these properties are valuable in general data\nscience. This motivates the use of quantum models to analyze general\nnonphysical datasets. Here we provide both empirical and theoretical insights\ninto the application of quantum models in data science. In the theoretical part\nof this paper, we firstly show that quantum models can be exponentially more\nefficient than probabilistic models because there exist datasets that admit\nlow-dimensional quantum models and only exponentially high-dimensional\nprobabilistic models. Secondly, we explain in what sense quantum models realize\na useful relaxation of compressed probabilistic models. Thirdly, we show that\nsparse datasets admit low-dimensional quantum models and finally, we introduce\na method to compute hierarchical orderings of properties of users (e.g.,\npersonality traits) and items (e.g., genres of movies). In the empirical part\nof the paper, we evaluate quantum models in item recommendation and observe\nthat the predictive power of quantum-inspired recommender systems can compete\nwith state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,\nwe make use of the interpretability of quantum models by computing hierarchical\norderings of properties of users and items. This work establishes a connection\nbetween data science (item recommendation), information theory (communication\ncomplexity), mathematical programming (positive semidefinite factorizations)\nand physics (quantum models). \n\n"}
{"id": "1601.06727", "contents": "Title: Optimal Bounds on Functions of Quantum States under Quantum Channels Abstract: Let $\\rho_1, \\rho_2$ be quantum states and $(\\rho_1,\\rho_2) \\mapsto D(\\rho_1,\n\\rho_2)$ be a scalar function such as the trace norm, the fidelity, and the\nrelative entropy, etc. We determine optimal bounds for $D(\\rho_1,\n\\Phi(\\rho_2))$ for $\\Phi \\in \\mathcal{S}$ for different class of functions\n$D(\\cdot, \\cdot)$, where $\\mathcal{S}$ is the set of unitary quantum channels,\nthe set of mixed unitary channels, the set of unital quantum channels, and the\nset of all quantum channels. \n\n"}
{"id": "1601.07258", "contents": "Title: Fast Integral Image Estimation at 1% measurement rate Abstract: We propose a framework called ReFInE to directly obtain integral image\nestimates from a very small number of spatially multiplexed measurements of the\nscene without iterative reconstruction of any auxiliary image, and demonstrate\ntheir practical utility in visual object tracking. Specifically, we design\nmeasurement matrices which are tailored to facilitate extremely fast estimation\nof the integral image, by using a single-shot linear operation on the measured\nvector. Leveraging a prior model for the images, we formulate a nuclear norm\nminimization problem with second order conic constraints to jointly obtain the\nmeasurement matrix and the linear operator. Through qualitative and\nquantitative experiments, we show that high quality integral image estimates\ncan be obtained using our framework at very low measurement rates. Further, on\na standard dataset of 50 videos, we present object tracking results which are\ncomparable to the state-of-the-art methods, even at an extremely low\nmeasurement rate of 1%. \n\n"}
{"id": "1601.07448", "contents": "Title: A Bayesian Approach for Parameter Estimation with Uncertainty for\n  Dynamic Power Systems Abstract: We address the problem of estimating the uncertainty in the solution of power\ngrid inverse problems within the framework of Bayesian inference. We\ninvestigate two approaches, an adjoint-based method and a stochastic spectral\nmethod. These methods are used to estimate the maximum a posteriori point of\nthe parameters and their variance, which quantifies their uncertainty. Within\nthis framework we estimate several parameters of the dynamic power system, such\nas generator inertias, which are not quantifiable in steady-state models. We\nillustrate the performance of these approaches on a 9-bus power grid example\nand analyze the dependence on measurement frequency, estimation horizon,\nperturbation size, and measurement noise. We assess the computational\nefficiency, and discuss the expected performance when these methods are applied\nto large systems. \n\n"}
{"id": "1602.01379", "contents": "Title: A bi-objective optimization framework for three-dimensional road\n  alignment design Abstract: Optimization of three-dimensional road alignments is a nonlinear non-convex\noptimization problem. The development of models that fully optimize a\nthree-dimensional road alignment problem is challenging due to numerous factors\ninvolved and complexities in the geometric specification of the alignment. In\nthis study, we developed a novel bi-objective optimization approach to solve a\nthree dimensional road alignment problem where the horizontal and vertical\nalignments are optimized simultaneously. Two conflicting cost objective\nfunctions, \\emph{earthwork} cost and the \\emph{utility} cost, are cast in a\nbi-objective optimization problem. We numerically compare several\nmulti-objective optimization solvers, and find that it is possible to determine\nthe Pareto front in a reasonable time. \n\n"}
{"id": "1602.01693", "contents": "Title: Thermodynamic aspects of information transfer in complex dynamical\n  systems Abstract: From the Horowitz-Esposito stochastic thermodynamical description of\ninformation flows in dynamical systems [J. M. Horowitz and M. Esposito, Phys.\nRev. X4, 031015 (2014)], it is known that while the second law of\nthermodynamics is satisfied by a joint system, the entropic balance for the\nsubsystems is adjusted by a term related to the mutual information exchange\nrate between the two subsystems. In this article, we present a quantitative\ndiscussion of the conceptual link between the Horowitz-Esposito analysis and\nthe Liang-Kleeman work on information transfer between dynamical system\ncomponents [X. S. Liang and R. Kleeman, Phys. Rev. Lett. 95, 244101 (2005)]. In\nparticular, the entropic balance arguments employed in the two approaches are\ncompared. Notwithstanding all differences between the two formalisms, our work\nstrengthens the Liang-Kleeman heuristic balance reasoning by showing its formal\nanalogy with the recent Horowitz-Esposito thermodynamic balance arguments. \n\n"}
{"id": "1602.04186", "contents": "Title: An Evolutionary Strategy based on Partial Imitation for Solving\n  Optimization Problems Abstract: In this work we introduce an evolutionary strategy to solve combinatorial\noptimization tasks, i.e. problems characterized by a discrete search space. In\nparticular, we focus on the Traveling Salesman Problem (TSP), i.e. a famous\nproblem whose search space grows exponentially, increasing the number of\ncities, up to becoming NP-hard. The solutions of the TSP can be codified by\narrays of cities, and can be evaluated by fitness, computed according to a cost\nfunction (e.g. the length of a path). Our method is based on the evolution of\nan agent population by means of an imitative mechanism, we define `partial\nimitation'. In particular, agents receive a random solution and then,\ninteracting among themselves, may imitate the solutions of agents with a higher\nfitness. Since the imitation mechanism is only partial, agents copy only one\nentry (randomly chosen) of another array (i.e. solution). In doing so, the\npopulation converges towards a shared solution, behaving like a spin system\nundergoing a cooling process, i.e. driven towards an ordered phase. We\nhighlight that the adopted `partial imitation' mechanism allows the population\nto generate solutions over time, before reaching the final equilibrium. Results\nof numerical simulations show that our method is able to find, in a finite\ntime, both optimal and suboptimal solutions, depending on the size of the\nconsidered search space. \n\n"}
{"id": "1602.04785", "contents": "Title: Approximate solutions of continuous-time stochastic games Abstract: The paper is concerned with a zero-sum continuous-time stochastic\ndifferential game with a dynamics controlled by a Markov process and a terminal\npayoff. The value function of the original game is estimated using the value\nfunction of a model game. The dynamics of the model game differs from the\noriginal one. The general result applied to differential games yields the\napproximation of value function of differential game by the solution of\ncountable system of ODEs. \n\n"}
{"id": "1602.04915", "contents": "Title: Gradient Descent Converges to Minimizers Abstract: We show that gradient descent converges to a local minimizer, almost surely\nwith random initialization. This is proved by applying the Stable Manifold\nTheorem from dynamical systems theory. \n\n"}
{"id": "1602.05105", "contents": "Title: Color of turbulence Abstract: In this paper, we address the problem of how to account for second-order\nstatistics of turbulent flows using low-complexity stochastic dynamical models\nbased on the linearized Navier-Stokes equations. The complexity is quantified\nby the number of degrees of freedom in the linearized evolution model that are\ndirectly influenced by stochastic excitation sources. For the case where only a\nsubset of velocity correlations are known, we develop a framework to complete\nunavailable second-order statistics in a way that is consistent with\nlinearization around turbulent mean velocity. In general, white-in-time\nstochastic forcing is not sufficient to explain turbulent flow statistics. We\ndevelop models for colored-in-time forcing using a maximum entropy formulation\ntogether with a regularization that serves as a proxy for rank minimization. We\nshow that colored-in-time excitation of the Navier-Stokes equations can also be\ninterpreted as a low-rank modification to the generator of the linearized\ndynamics. Our method provides a data-driven refinement of models that originate\nfrom first principles and captures complex dynamics of turbulent flows in a way\nthat is tractable for analysis, optimization, and control design. \n\n"}
{"id": "1602.05130", "contents": "Title: Risk Aversion in Finite Markov Decision Processes Using Total Cost\n  Criteria and Average Value at Risk Abstract: In this paper we present an algorithm to compute risk averse policies in\nMarkov Decision Processes (MDP) when the total cost criterion is used together\nwith the average value at risk (AVaR) metric. Risk averse policies are needed\nwhen large deviations from the expected behavior may have detrimental effects,\nand conventional MDP algorithms usually ignore this aspect. We provide\nconditions for the structure of the underlying MDP ensuring that approximations\nfor the exact problem can be derived and solved efficiently. Our findings are\nnovel inasmuch as average value at risk has not previously been considered in\nassociation with the total cost criterion. Our method is demonstrated in a\nrapid deployment scenario, whereby a robot is tasked with the objective of\nreaching a target location within a temporal deadline where increased speed is\nassociated with increased probability of failure. We demonstrate that the\nproposed algorithm not only produces a risk averse policy reducing the\nprobability of exceeding the expected temporal deadline, but also provides the\nstatistical distribution of costs, thus offering a valuable analysis tool. \n\n"}
{"id": "1602.05642", "contents": "Title: When the Filter Bubble Bursts: Collective Evaluation Dynamics in Online\n  Communities Abstract: We analyze online collective evaluation processes through positive and\nnegative votes in various social media. We find two modes of collective\nevaluations that stem from the existence of filter bubbles. Above a threshold\nof collective attention, negativity grows faster with positivity, as a sign of\nthe burst of a filter bubble when information reaches beyond the local social\ncontext of a user. We analyze how collectively evaluated content can reach\nlarge social contexts and create polarization, showing that emotions expressed\nthrough text play a key role in collective evaluation processes. \n\n"}
{"id": "1602.07592", "contents": "Title: Mean-variance risk-averse optimal control of systems governed by PDEs\n  with random parameter fields using quadratic approximations Abstract: We present a method for optimal control of systems governed by partial\ndifferential equations (PDEs) with uncertain parameter fields. We consider an\nobjective function that involves the mean and variance of the control\nobjective, leading to a risk-averse optimal control problem. To make the\nproblem tractable, we invoke a quadratic Taylor series approximation of the\ncontrol objective with respect to the uncertain parameter. This enables\nderiving explicit expressions for the mean and variance of the control\nobjective in terms of its gradients and Hessians with respect to the uncertain\nparameter. The risk-averse optimal control problem is then formulated as a\nPDE-constrained optimization problem with constraints given by the forward and\nadjoint PDEs defining these gradients and Hessians. The expressions for the\nmean and variance of the control objective under the quadratic approximation\ninvolve the trace of the (preconditioned) Hessian and are thus prohibitive to\nevaluate. To address this, we employ trace estimators that only require a\nmodest number of Hessian-vector products. We illustrate our approach with two\nproblems: the control of a semilinear elliptic PDE with an uncertain boundary\nsource term, and the control of a linear elliptic PDE with an uncertain\ncoefficient field. For the latter problem, we derive adjoint-based expressions\nfor efficient computation of the gradient of the risk-averse objective with\nrespect to the controls. Our method ensures that the cost of computing the\nrisk-averse objective and its gradient with respect to the control, measured in\nthe number of PDE solves, is independent of the (discretized) parameter and\ncontrol dimensions, and depends only on the number of random vectors employed\nin the trace estimation. Finally, we present a comprehensive numerical study of\nan optimal control problem for fluid flow in a porous medium with uncertain\npermeability field. \n\n"}
{"id": "1602.07760", "contents": "Title: Strong mixed-integer formulations for the floor layout problem Abstract: The floor layout problem (FLP) tasks a designer with positioning a collection\nof rectangular boxes on a fixed floor in such a way that minimizes total\ncommunication costs between the components. While several mixed integer\nprogramming (MIP) formulations for this problem have been developed, it remains\nextremely challenging from a computational perspective. This work takes a\nsystematic approach to constructing MIP formulations and valid inequalities for\nthe FLP that unifies and recovers all known formulations for it. In addition,\nthe approach yields new formulations that can provide a significant\ncomputational advantage and can solve previously unsolved instances. While the\nconstruction approach focuses on the FLP, it also exemplifies generic\nformulation techniques that should prove useful for broader classes of\nproblems. \n\n"}
{"id": "1602.08372", "contents": "Title: Explicit Conditions on Existence and Uniqueness of Load-Flow Solutions\n  in Distribution Networks Abstract: We present explicit sufficient conditions that guarantee the existence and\nuniqueness of the feasible load-flow solution for distribution networks with a\ngeneric topology (radial or meshed) modeled with positive sequence equivalents.\nIn the problem, we also account for the presence of shunt elements. The\nconditions have low computational complexity and thus can be efficiently\nverified in a real system. Once the conditions are satisfied, the unique\nload-flow solution can be reached by a given fixed point iteration method of\napproximately linear complexity. Therefore, the proposed approach is of\nparticular interest for modern active distribution network (ADN) setup in the\ncontext of real-time control. The theory has been confirmed through numerical\nexperiments. \n\n"}
{"id": "1603.01681", "contents": "Title: A single-phase, proximal path-following framework Abstract: We propose a new proximal, path-following framework for a class of\nconstrained convex problems. We consider settings where the nonlinear---and\npossibly non-smooth---objective part is endowed with a proximity operator, and\nthe constraint set is equipped with a self-concordant barrier. Our approach\nrelies on the following two main ideas. First, we re-parameterize the\noptimality condition as an auxiliary problem, such that a good initial point is\navailable; by doing so, a family of alternative paths towards the optimum is\ngenerated. Second, we combine the proximal operator with path-following ideas\nto design a single-phase, proximal, path-following algorithm. Our method has\nseveral advantages. First, it allows handling non-smooth objectives via\nproximal operators; this avoids lifting the problem dimension in order to\naccommodate non-smooth components in optimization. Second, it consists of only\na \\emph{single phase}: While the overall convergence rate of classical\npath-following schemes for self-concordant objectives does not suffer from the\ninitialization phase, proximal path-following schemes undergo slow convergence,\nin order to obtain a good starting point \\cite{TranDinh2013e}. In this work, we\nshow how to overcome this limitation in the proximal setting and prove that our\nscheme has the same $\\mathcal{O}(\\sqrt{\\nu}\\log(1/\\varepsilon))$ worst-case\niteration-complexity with standard approaches \\cite{Nesterov2004,Nesterov1994}\nwithout requiring an initial phase, where $\\nu$ is the barrier parameter and\n$\\varepsilon$ is a desired accuracy. Finally, our framework allows errors in\nthe calculation of proximal-Newton directions, without sacrificing the\nworst-case iteration complexity. We demonstrate the merits of our algorithm via\nthree numerical examples, where proximal operators play a key role. \n\n"}
{"id": "1603.02595", "contents": "Title: Connection between MP and DPP for Stochastic Recursive Optimal Control\n  Problems: Viscosity Solution Framework in General Case Abstract: This paper deals with a stochastic recursive optimal control problem, where\nthe diffusion coefficient depends on the control variable and the control\ndomain is not necessarily convex. We focus on the connection between the\ngeneral maximum principle and the dynamic programming principle for such\ncontrol problem without the assumption that the value is smooth enough, the set\ninclusions among the sub- and super-jets of the value function and the\nfirst-order and second-order adjoint processes as well as the generalized\nHamiltonian function are established. Moreover, by comparing these results with\nthe classical ones in Yong and Zhou [{\\em Stochastic Controls: Hamiltonian\nSystems and HJB Equations, Springer-Verlag, New York, 1999}], it is natural to\nobtain the first- and second-order adjoint equations of Hu [{\\em Direct method\non stochastic maximum principle for optimization with recursive utilities,\narXiv:1507.03567v1 [math.OC], 13 Jul. 2015}]. \n\n"}
{"id": "1603.03265", "contents": "Title: Dynamics and optimal control of Ebola transmission Abstract: A major Ebola outbreak occurs in West Africa since March 2014, being the\ndeadliest epidemic in history. As an infectious disease epidemiology, Ebola is\nthe most lethal and is moving faster than in previous outbreaks. On 8 August\n2014, the World Health Organization (WHO) declared the outbreak a public health\nemergency of international concern. Last update on 7 July 2015 by WHO reports\n27 609 cases of Ebola with a total of 11 261 deaths. In this work, we present a\nmathematical description of the spread of Ebola virus based on the SEIR\n(Susceptible-Exposed-Infective-Recovered) model and optimal strategies for\nEbola control. In order to control the propagation of the virus and to predict\nthe impact of vaccine programmes, we investigate several strategies of optimal\ncontrol of the spread of Ebola: control infection by vaccination of\nsusceptible; minimize exposed and infected; reduce Ebola infection by\nvaccination and education. \n\n"}
{"id": "1603.03650", "contents": "Title: A Penalty Function Promoting Sparsity Within and Across Groups Abstract: We introduce a new weakly-convex penalty function for signals with a group\nbehavior. The penalty promotes signals with a few number of active groups,\nwhere within each group, only a few high magnitude coefficients are active. We\nderive the threshold function associated with the proposed penalty and study\nits properties. We discuss how the proposed penalty/threshold function can be\nuseful for signals with isolated non-zeros, such as audio with isolated\nharmonics along the frequency axis, or reflection functions in exploration\nseismology where the non-zeros occur on the boundaries of subsoil layers. We\ndemonstrate the use of the proposed penalty/threshold functions in a convex\ndenoising and a non-convex deconvolution formulation. We provide convergent\nalgorithms for both formulations and compare the performance with\nstate-of-the-art methods. \n\n"}
{"id": "1603.04913", "contents": "Title: Bilateral Boundary Control of One-Dimensional First- and Second-Order\n  PDEs using Infinite-Dimensional Backstepping Abstract: This paper develops an extension of infinite-dimensional backstepping method\nfor parabolic and hyperbolic systems in one spatial dimension with two\nactuators. Typically, PDE backstepping is applied in 1-D domains with an\nactuator at one end. Here, we consider the use of two actuators, one at each\nend of the domain, which we refer to as bilateral control (as opposed to\nunilateral control). Bilateral control laws are derived for linear\nreaction-diffusion, wave and 2X2 hyperbolic 1-D systems (with same speed of\ntransport in both directions). The extension is nontrivial but straightforward\nif the backstepping transformation is adequately posed. The resulting bilateral\ncontrollers are compared with their unilateral counterparts in the\nreaction-diffusion case for constant coefficients, by making use of explicit\nsolutions, showing a reduction in control effort as a tradeoff for the presence\nof two actuators when the system coefficients are large. These results open the\ndoor for more sophisticated designs such as bilateral sensor/actuator output\nfeedback and fault-tolerant designs. \n\n"}
{"id": "1603.04914", "contents": "Title: Boundary Control of Coupled Reaction-Advection-Diffusion Systems with\n  Spatially-Varying Coefficients Abstract: Recently, the problem of boundary stabilization for unstable linear\nconstant-coefficient coupled reaction-diffusion systems was solved by means of\nthe backstepping method. The extension of this result to systems with advection\nterms and spatially-varying coefficients is challenging due to complex boundary\nconditions that appear in the equations verified by the control kernels. In\nthis paper we address this issue by showing that these equations are\nessentially equivalent to those verified by the control kernels for first-order\nhyperbolic coupled systems, which were recently found to be well-posed. The\nresult therefore applies in this case, allowing us to prove H^1 stability for\nthe closed-loop system. It also shows an interesting connection between\nbackstepping kernels for coupled parabolic and hyperbolic problems. \n\n"}
{"id": "1603.05038", "contents": "Title: CoinCalc -- A new R package for quantifying simultaneities of event\n  series Abstract: We present the new R package CoinCalc for performing event coincidence\nanalysis (ECA), a novel statistical method to quantify the simultaneity of\nevents contained in two series of observations, either as simultaneous or\nlagged coincidences within a user-specific temporal tolerance window. The\npackage also provides different analytical as well as surrogate-based\nsignificance tests (valid under different assumptions about the nature of the\nobserved event series) as well as an intuitive visualization of the identified\ncoincidences. We demonstrate the usage of CoinCalc based on two typical\ngeoscientific example problems addressing the relationship between\nmeteorological extremes and plant phenology as well as that between soil\nproperties and land cover. \n\n"}
{"id": "1603.05533", "contents": "Title: Compressed sensing of data with a known distribution Abstract: Compressed sensing is a technique for recovering an unknown sparse signal\nfrom a small number of linear measurements. When the measurement matrix is\nrandom, the number of measurements required for perfect recovery exhibits a\nphase transition: there is a threshold on the number of measurements after\nwhich the probability of exact recovery quickly goes from very small to very\nlarge. In this work we are able to reduce this threshold by incorporating\nstatistical information about the data we wish to recover. Our algorithm works\nby minimizing a suitably weighted $\\ell_1$-norm, where the weights are chosen\nso that the expected statistical dimension of the corresponding descent cone is\nminimized. We also provide new discrete-geometry-based Monte Carlo algorithms\nfor computing intrinsic volumes of such descent cones, allowing us to bound the\nfailure probability of our methods. \n\n"}
{"id": "1603.06544", "contents": "Title: Nearest Points on Toric Varieties Abstract: We determine the Euclidean distance degree of a projective toric variety.\nThis extends the formula of Matsui and Takeuchi for the degree of the\n$A$-discriminant in terms of Euler obstructions. Our primary goal is the\ndevelopment of reliable algorithmic tools for computing the points on a real\ntoric variety that are closest to a given data point. \n\n"}
{"id": "1603.06782", "contents": "Title: Doubly Random Parallel Stochastic Methods for Large Scale Learning Abstract: We consider learning problems over training sets in which both, the number of\ntraining examples and the dimension of the feature vectors, are large. To solve\nthese problems we propose the random parallel stochastic algorithm (RAPSA). We\ncall the algorithm random parallel because it utilizes multiple processors to\noperate in a randomly chosen subset of blocks of the feature vector. We call\nthe algorithm parallel stochastic because processors choose elements of the\ntraining set randomly and independently. Algorithms that are parallel in either\nof these dimensions exist, but RAPSA is the first attempt at a methodology that\nis parallel in both, the selection of blocks and the selection of elements of\nthe training set. In RAPSA, processors utilize the randomly chosen functions to\ncompute the stochastic gradient component associated with a randomly chosen\nblock. The technical contribution of this paper is to show that this minimally\ncoordinated algorithm converges to the optimal classifier when the training\nobjective is convex. In particular, we show that: (i) When using decreasing\nstepsizes, RAPSA converges almost surely over the random choice of blocks and\nfunctions. (ii) When using constant stepsizes, convergence is to a neighborhood\nof optimality with a rate that is linear in expectation. RAPSA is numerically\nevaluated on the MNIST digit recognition problem. \n\n"}
{"id": "1603.07195", "contents": "Title: A Decentralized Quasi-Newton Method for Dual Formulations of Consensus\n  Optimization Abstract: This paper considers consensus optimization problems where each node of a\nnetwork has access to a different summand of an aggregate cost function. Nodes\ntry to minimize the aggregate cost function, while they exchange information\nonly with their neighbors. We modify the dual decomposition method to\nincorporate a curvature correction inspired by the\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dual\nD-BFGS method is a fully decentralized algorithm in which nodes approximate\ncurvature information of themselves and their neighbors through the\nsatisfaction of a secant condition. Dual D-BFGS is of interest in consensus\noptimization problems that are not well conditioned, making first order\ndecentralized methods ineffective, and in which second order information is not\nreadily available, making decentralized second order methods infeasible.\nAsynchronous implementation is discussed and convergence of D-BFGS is\nestablished formally for both synchronous and asynchronous implementations.\nPerformance advantages relative to alternative decentralized algorithms are\nshown numerically. \n\n"}
{"id": "1603.07875", "contents": "Title: Improved Sufficient Conditions for Exact Convex Relaxation of\n  Storage-Concerned ED Abstract: To avoid simultaneous charging and discharging of storages, complementarity\nconstraints are introduced to storage-concerned economic dispatch (ED), which\nmakes the problem non-convex. This letter concerns the conditions under which\nthe convex relaxation of storage-concerned ED with complementarity constraints\nis exact. Two new sufficient conditions are proposed, proved and verified to\nsignificantly reduce the conservatism of recent results [3], [4]. \n\n"}
{"id": "1604.00971", "contents": "Title: Deep Graphs - a general framework to represent and analyze heterogeneous\n  complex systems across scales Abstract: Network theory has proven to be a powerful tool in describing and analyzing\nsystems by modelling the relations between their constituent objects. In recent\nyears great progress has been made by augmenting `traditional' network theory.\nHowever, existing network representations still lack crucial features in order\nto serve as a general data analysis tool. These include, most importantly, an\nexplicit association of information with possibly heterogeneous types of\nobjects and relations, and a conclusive representation of the properties of\ngroups of nodes as well as the interactions between such groups on different\nscales. In this paper, we introduce a collection of definitions resulting in a\nframework that, on the one hand, entails and unifies existing network\nrepresentations (e.g., network of networks, multilayer networks), and on the\nother hand, generalizes and extends them by incorporating the above features.\nTo implement these features, we first specify the nodes and edges of a finite\ngraph as sets of properties. Second, the mathematical concept of partition\nlattices is transferred to network theory in order to demonstrate how\npartitioning the node and edge set of a graph into supernodes and superedges\nallows to aggregate, compute and allocate information on and between arbitrary\ngroups of nodes. The derived partition lattice of a graph, which we denote by\ndeep graph, constitutes a concise, yet comprehensive representation that\nenables the expression and analysis of heterogeneous properties, relations and\ninteractions on all scales of a complex system in a self-contained manner.\nFurthermore, to be able to utilize existing network-based methods and models,\nwe derive different representations of multilayer networks from our framework\nand demonstrate the advantages of our representation. We exemplify an\napplication of deep graphs using a real world dataset of precipitation\nmeasurements. \n\n"}
{"id": "1604.02216", "contents": "Title: A Primal-Dual Type Algorithm with the $O(1/t)$ Convergence Rate for\n  Large Scale Constrained Convex Programs Abstract: This paper considers large scale constrained convex programs, which are\nusually not solvable by interior point methods or other Newton-type methods due\nto the prohibitive computation and storage complexity for Hessians and matrix\ninversions. Instead, large scale constrained convex programs are often solved\nby gradient based methods or decomposition based methods. The conventional\nprimal-dual subgradient method, aka, Arrow-Hurwicz-Uzawa subgradient method, is\na low complexity algorithm with the $O(1/\\sqrt{t})$ convergence rate, where $t$\nis the number of iterations. If the objective and constraint functions are\nseparable, the Lagrangian dual type method can decompose a large scale convex\nprogram into multiple parallel small scale convex programs. The classical dual\ngradient algorithm is an example of Lagrangian dual type methods and has\nconvergence rate $O(1/\\sqrt{t})$. Recently, a new Lagrangian dual type\nalgorithm with faster $O(1/t)$ convergence is proposed in Yu and Neely (2015).\nHowever, if the objective or constraint functions are not separable, each\niteration of the Lagrangian dual type method in Yu and Neely (2015) requires to\nsolve a large scale unconstrained convex program, which can have huge\ncomplexity. This paper proposes a new primal-dual type algorithm, which only\ninvolves simple gradient updates at each iteration and has the $O(1/t)$\nconvergence rate. \n\n"}
{"id": "1604.02872", "contents": "Title: Is this scaling nonlinear? Abstract: One of the most celebrated findings in complex systems in the last decade is\nthat different indexes y (e.g., patents) scale nonlinearly with the\npopulation~x of the cities in which they appear, i.e., $y\\sim x^\\beta, \\beta\n\\neq 1$. More recently, the generality of this finding has been questioned in\nstudies using new databases and different definitions of city boundaries. In\nthis paper we investigate the existence of nonlinear scaling using a\nprobabilistic framework in which fluctuations are accounted explicitly. In\nparticular, we show that this allows not only to (a) estimate $\\beta$ and\nconfidence intervals, but also to (b) quantify the evidence in favor of $\\beta\n\\neq 1$ and (c) test the hypothesis that the observations are compatible with\nthe nonlinear scaling. We employ this framework to compare $5$ different models\nto $15$ different datasets and we find that the answers to points (a)-(c)\ncrucially depend on the fluctuations contained in the data, on how they are\nmodeled, and on the fact that the city sizes are heavy-tailed distributed. \n\n"}
{"id": "1604.04569", "contents": "Title: Kantorovich's theorem on Newton's method for solving strongly regular\n  generalized equation Abstract: In this paper we consider the Newton's method for solving the generalized\nequation of the form $ f(x) +F(x) \\ni 0, $ where $f:{\\Omega}\\to Y$ is a\ncontinuously differentiable mapping, $X$ and $Y$ are Banach spaces,\n$\\Omega\\subseteq X$ an open set and $F:X \\rightrightarrows Y$ be a set-valued\nmapping with nonempty closed graph. We show that, under strong regularity of\nthe generalized equation, concept introduced by S.M.Robinson in [27], and\nstarting point satisfying the Kantorovich's assumptions, the Newton's method is\nquadratically convergent to a solution, which is unique in a suitable\nneighborhood of the starting point. The analysis presented based on Banach\nPerturbation Lemma for generalized equation and the majorant technique, allow\nto unify some results pertaining the Newton's method theory. \n\n"}
{"id": "1604.04603", "contents": "Title: On the Douglas-Rachford algorithm Abstract: The Douglas-Rachford algorithm is a very popular splitting technique for\nfinding a zero of the sum of two maximally monotone operators. However, the\nbehaviour of the algorithm remains mysterious in the general inconsistent case,\ni.e., when the sum problem has no zeros. More than a decade ago, however, it\nwas shown that in the (possibly inconsistent) convex feasibility setting, the\nshadow sequence remains bounded and it is weak cluster points solve a best\napproximation problem.\n  In this paper, we advance the understanding of the inconsistent case\nsignificantly by providing a complete proof of the full weak convergence in the\nconvex feasibility setting. In fact, a more general sufficient condition for\nthe weak convergence in the general case is presented. Several examples\nillustrate the results. \n\n"}
{"id": "1604.06532", "contents": "Title: Application of Bayesian Neural Networks to Energy Reconstruction in EAS\n  Experiments for ground-based TeV Astrophysics Abstract: A toy detector array is designed to detect a shower generated by the\ninteraction between a TeV cosmic ray and the atmosphere. In the present paper,\nthe primary energies of showers detected by the detector array are\nreconstructed with the algorithm of Bayesian neural networks (BNNs) and a\nstandard method like the LHAASO experiment \\cite{lhaaso-ma}, respectively.\nCompared to the standard method, the energy resolutions are significantly\nimproved using the BNNs. And the improvement is more obvious for the high\nenergy showers than the low energy ones. \n\n"}
{"id": "1604.08189", "contents": "Title: Integrating Storage to Power System Management Abstract: Wind integration in power grids is very difficult, essentially because of the\nuncertain nature of wind speed. Forecasting errors on output from wind turbines\nmay have costly consequences. For instance, power might be bought at highest\nprice to meet the load. On the other hand, in case of surplus, power may be\nwasted. Energy storage facility may provide some recourse against the\nuncertainty on wind generation. Because of the sequential nature of power\nscheduling problems, stochastic dynamic programming is often used as solution\nmethod. However, this scheme is limited to very small networks by the so-called\ncurse of dimensionality. To face such limitations, several approximate\napproaches have been proposed. We analyze the management of a network composed\nof conventional power units as well as wind turbines through approximate\ndynamic programming. We consider a general power network model with ramping\nconstraints on the conventional generators. We use generalized linear\nprogramming techniques to linearize the problems. We test the algorithm on\nseveral networks of different sizes and report results about the computation\ntime. We also carry out comparisons with classical dynamic programming on a\nsmall network. The results show the algorithm seems to offer a fair trade-off\nbetween solution time and accuracy. \n\n"}
{"id": "1604.08816", "contents": "Title: Combining complex networks and data mining: why and how Abstract: The increasing power of computer technology does not dispense with the need\nto extract meaningful in- formation out of data sets of ever growing size, and\nindeed typically exacerbates the complexity of this task. To tackle this\ngeneral problem, two methods have emerged, at chronologically different times,\nthat are now commonly used in the scientific community: data mining and complex\nnetwork theory. Not only do complex network analysis and data mining share the\nsame general goal, that of extracting information from complex systems to\nultimately create a new compact quantifiable representation, but they also\noften address similar problems too. In the face of that, a surprisingly low\nnumber of researchers turn out to resort to both methodologies. One may then be\ntempted to conclude that these two fields are either largely redundant or\ntotally antithetic. The starting point of this review is that this state of\naffairs should be put down to contingent rather than conceptual differences,\nand that these two fields can in fact advantageously be used in a synergistic\nmanner. An overview of both fields is first provided, some fundamental concepts\nof which are illustrated. A variety of contexts in which complex network theory\nand data mining have been used in a synergistic manner are then presented.\nContexts in which the appropriate integration of complex network metrics can\nlead to improved classification rates with respect to classical data mining\nalgorithms and, conversely, contexts in which data mining can be used to tackle\nimportant issues in complex network theory applications are illustrated.\nFinally, ways to achieve a tighter integration between complex networks and\ndata mining, and open lines of research are discussed. \n\n"}
{"id": "1605.00042", "contents": "Title: Improved Sparse Low-Rank Matrix Estimation Abstract: We address the problem of estimating a sparse low-rank matrix from its noisy\nobservation. We propose an objective function consisting of a data-fidelity\nterm and two parameterized non-convex penalty functions. Further, we show how\nto set the parameters of the non-convex penalty functions, in order to ensure\nthat the objective function is strictly convex. The proposed objective function\nbetter estimates sparse low-rank matrices than a convex method which utilizes\nthe sum of the nuclear norm and the $\\ell_1$ norm. We derive an algorithm (as\nan instance of ADMM) to solve the proposed problem, and guarantee its\nconvergence provided the scalar augmented Lagrangian parameter is set\nappropriately. We demonstrate the proposed method for denoising an audio signal\nand an adjacency matrix representing protein interactions in the `Escherichia\ncoli' bacteria. \n\n"}
{"id": "1605.01867", "contents": "Title: Statistical mechanics analysis of thresholding 1-bit compressed sensing Abstract: The one-bit compressed sensing framework aims to reconstruct a sparse signal\nby only using the sign information of its linear measurements. To compensate\nfor the loss of scale information, past studies in the area have proposed\nrecovering the signal by imposing an additional constraint on the L2-norm of\nthe signal. Recently, an alternative strategy that captures scale information\nby introducing a threshold parameter to the quantization process was advanced.\nIn this paper, we analyze the typical behavior of the thresholding 1-bit\ncompressed sensing utilizing the replica method of statistical mechanics, so as\nto gain an insight for properly setting the threshold value. Our result shows\nthat, fixing the threshold at a constant value yields better performance than\nvarying it randomly when the constant is optimally tuned, statistically.\nUnfortunately, the optimal threshold value depends on the statistical\nproperties of the target signal, which may not be known in advance. In order to\nhandle this inconvenience, we develop a heuristic that adaptively tunes the\nthreshold parameter based on the frequency of positive (or negative) values in\nthe binary outputs. Numerical experiments show that the heuristic exhibits\nsatisfactory performance while incurring low computational cost. \n\n"}
{"id": "1605.02408", "contents": "Title: Structured Nonconvex and Nonsmooth Optimization: Algorithms and\n  Iteration Complexity Analysis Abstract: Nonconvex and nonsmooth optimization problems are frequently encountered in\nmuch of statistics, business, science and engineering, but they are not yet\nwidely recognized as a technology in the sense of scalability. A reason for\nthis relatively low degree of popularity is the lack of a well developed system\nof theory and algorithms to support the applications, as is the case for its\nconvex counterpart. This paper aims to take one step in the direction of\ndisciplined nonconvex and nonsmooth optimization. In particular, we consider in\nthis paper some constrained nonconvex optimization models in block decision\nvariables, with or without coupled affine constraints. In the case of without\ncoupled constraints, we show a sublinear rate of convergence to an\n$\\epsilon$-stationary solution in the form of variational inequality for a\ngeneralized conditional gradient method, where the convergence rate is shown to\nbe dependent on the H\\\"olderian continuity of the gradient of the smooth part\nof the objective. For the model with coupled affine constraints, we introduce\ncorresponding $\\epsilon$-stationarity conditions, and apply two proximal-type\nvariants of the ADMM to solve such a model, assuming the proximal ADMM updates\ncan be implemented for all the block variables except for the last block, for\nwhich either a gradient step or a majorization-minimization step is\nimplemented. We show an iteration complexity bound of $O(1/\\epsilon^2)$ to\nreach an $\\epsilon$-stationary solution for both algorithms. Moreover, we show\nthat the same iteration complexity of a proximal BCD method follows\nimmediately. Numerical results are provided to illustrate the efficacy of the\nproposed algorithms for tensor robust PCA. \n\n"}
{"id": "1605.03131", "contents": "Title: Topological and geometric measurements of force chain structure Abstract: Developing quantitative methods for characterizing structural properties of\nforce chains in densely packed granular media is an important step toward\nunderstanding or predicting large-scale physical properties of a packing. A\npromising framework in which to develop such methods is network science, which\ncan be used to translate particle locations and force contacts to a graph in\nwhich particles are represented by nodes and forces between particles are\nrepresented by weighted edges. Applying network-based community-detection\ntechniques to extract force chains opens the door to developing statistics of\nforce chain structure, with the goal of identifying shape differences across\npackings, and providing a foundation on which to build predictions of bulk\nmaterial properties from mesoscale network features. Here, we discuss a trio of\nrelated but fundamentally distinct measurements of mesoscale structure of force\nchains in arbitrary 2D packings, including a novel statistic derived using\ntools from algebraic topology, which together provide a tool set for the\nanalysis of force chain architecture. We demonstrate the utility of this tool\nset by detecting variations in force chain architecture with pressure.\nCollectively, these techniques can be generalized to 3D packings, and to the\nassessment of continuous deformations of packings under stress or strain. \n\n"}
{"id": "1605.04131", "contents": "Title: Barzilai-Borwein Step Size for Stochastic Gradient Descent Abstract: One of the major issues in stochastic gradient descent (SGD) methods is how\nto choose an appropriate step size while running the algorithm. Since the\ntraditional line search technique does not apply for stochastic optimization\nalgorithms, the common practice in SGD is either to use a diminishing step\nsize, or to tune a fixed step size by hand, which can be time consuming in\npractice. In this paper, we propose to use the Barzilai-Borwein (BB) method to\nautomatically compute step sizes for SGD and its variant: stochastic variance\nreduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and\nSVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective\nfunctions. As a by-product, we prove the linear convergence result of SVRG with\nOption I proposed in [10], whose convergence result is missing in the\nliterature. Numerical experiments on standard data sets show that the\nperformance of SGD-BB and SVRG-BB is comparable to and sometimes even better\nthan SGD and SVRG with best-tuned step sizes, and is superior to some advanced\nSGD variants. \n\n"}
{"id": "1605.06009", "contents": "Title: Multifractal analysis of three-dimensional grayscale images:\n  Characterization of natural porous structures Abstract: A multifractal analysis (MFA) is performed on three-dimensional grayscale\nimages associated with natural porous structures (soil samples). First,\ncomputed tomography (CT) scans are carried out on the samples to generate 3D\ngrayscale images. Then, a preliminary analysis is conducted to evaluate key\nquantities associated with the porosity, such as void fraction, pore volume,\nconnectivity, and surface area. Finally, the samples are successfully\nidentified and separated into two different structure families by using the\nMFA. A new software (Munari) to carry out the MFA of 3D grayscale images is\nalso presented. \n\n"}
{"id": "1605.06619", "contents": "Title: Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic\n  Gradient Descent Abstract: Asynchronous parallel optimization algorithms for solving large-scale machine\nlearning problems have drawn significant attention from academia to industry\nrecently. This paper proposes a novel algorithm, decoupled asynchronous\nproximal stochastic gradient descent (DAP-SGD), to minimize an objective\nfunction that is the composite of the average of multiple empirical losses and\na regularization term. Unlike the traditional asynchronous proximal stochastic\ngradient descent (TAP-SGD) in which the master carries much of the computation\nload, the proposed algorithm off-loads the majority of computation tasks from\nthe master to workers, and leaves the master to conduct simple addition\noperations. This strategy yields an easy-to-parallelize algorithm, whose\nperformance is justified by theoretical convergence analyses. To be specific,\nDAP-SGD achieves an $O(\\log T/T)$ rate when the step-size is diminishing and an\nergodic $O(1/\\sqrt{T})$ rate when the step-size is constant, where $T$ is the\nnumber of total iterations. \n\n"}
{"id": "1605.08527", "contents": "Title: Stochastic Optimization for Large-scale Optimal Transport Abstract: Optimal transport (OT) defines a powerful framework to compare probability\ndistributions in a geometrically faithful way. However, the practical impact of\nOT is still limited because of its computational burden. We propose a new class\nof stochastic optimization algorithms to cope with large-scale problems\nroutinely encountered in machine learning applications. These methods are able\nto manipulate arbitrary distributions (either discrete or continuous) by simply\nrequiring to be able to draw samples from them, which is the typical setup in\nhigh-dimensional learning problems. This alleviates the need to discretize\nthese densities, while giving access to provably convergent methods that output\nthe correct distance without discretization error. These algorithms rely on two\nmain ideas: (a) the dual OT problem can be re-cast as the maximization of an\nexpectation ; (b) entropic regularization of the primal OT problem results in a\nsmooth dual optimization optimization which can be addressed with algorithms\nthat have a provably faster convergence. We instantiate these ideas in three\ndifferent setups: (i) when comparing a discrete distribution to another, we\nshow that incremental stochastic optimization schemes can beat Sinkhorn's\nalgorithm, the current state-of-the-art finite dimensional OT solver; (ii) when\ncomparing a discrete distribution to a continuous density, a semi-discrete\nreformulation of the dual program is amenable to averaged stochastic gradient\ndescent, leading to better performance than approximately solving the problem\nby discretization ; (iii) when dealing with two continuous densities, we\npropose a stochastic gradient descent over a reproducing kernel Hilbert space\n(RKHS). This is currently the only known method to solve this problem, apart\nfrom computing OT on finite samples. We backup these claims on a set of\ndiscrete, semi-discrete and continuous benchmark problems. \n\n"}
{"id": "1605.08705", "contents": "Title: Manifold boundaries give \"gray-box\" approximations of complex models Abstract: We discuss a method of parameter reduction in complex models known as the\nManifold Boundary Approximation Method (MBAM). This approach, based on a\ngeometric interpretation of statistics, maps the model reduction problem to a\ngeometric approximation problem. It operates iteratively, removing one\nparameter at a time, by approximating a high-dimension, but thin manifold by\nits boundary. Although the method makes no explicit assumption about the\nfunctional form of the model, it does require that the model manifold exhibit a\nhierarchy of boundaries, i.e., faces, edges, corners, hyper-corners, etc. We\nempirically show that a variety of model classes have this curious feature,\nmaking them amenable to MBAM. These model classes include models composed of\nelementary functions (e.g., rational functions, exponentials, and partition\nfunctions), a variety of dynamical system (e.g., chemical and biochemical\nkinetics, Linear Time Invariant (LTI) systems, and compartment models), network\nmodels (e.g., Bayesian networks, Markov chains, artificial neural networks, and\nMarkov random fields), log-linear probability distributions, and models with\nsymmetries. We discuss how MBAM recovers many common approximation methods for\neach model class and discuss potential pitfalls and limitations. \n\n"}
{"id": "1605.08754", "contents": "Title: Faster Eigenvector Computation via Shift-and-Invert Preconditioning Abstract: We give faster algorithms and improved sample complexities for estimating the\ntop eigenvector of a matrix $\\Sigma$ -- i.e. computing a unit vector $x$ such\nthat $x^T \\Sigma x \\ge (1-\\epsilon)\\lambda_1(\\Sigma)$:\n  Offline Eigenvector Estimation: Given an explicit $A \\in \\mathbb{R}^{n \\times\nd}$ with $\\Sigma = A^TA$, we show how to compute an $\\epsilon$ approximate top\neigenvector in time $\\tilde O([nnz(A) + \\frac{d*sr(A)}{gap^2} ]* \\log\n1/\\epsilon )$ and $\\tilde O([\\frac{nnz(A)^{3/4} (d*sr(A))^{1/4}}{\\sqrt{gap}} ]\n* \\log 1/\\epsilon )$. Here $nnz(A)$ is the number of nonzeros in $A$, $sr(A)$\nis the stable rank, $gap$ is the relative eigengap. By separating the $gap$\ndependence from the $nnz(A)$ term, our first runtime improves upon the\nclassical power and Lanczos methods. It also improves prior work using fast\nsubspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], giving\nsignificantly better dependencies on $sr(A)$ and $\\epsilon$. Our second running\ntime improves these further when $nnz(A) \\le \\frac{d*sr(A)}{gap^2}$.\n  Online Eigenvector Estimation: Given a distribution $D$ with covariance\nmatrix $\\Sigma$ and a vector $x_0$ which is an $O(gap)$ approximate top\neigenvector for $\\Sigma$, we show how to refine to an $\\epsilon$ approximation\nusing $ O(\\frac{var(D)}{gap*\\epsilon})$ samples from $D$. Here $var(D)$ is a\nnatural notion of variance. Combining our algorithm with previous work to\ninitialize $x_0$, we obtain improved sample complexity and runtime results\nunder a variety of assumptions on $D$.\n  We achieve our results using a general framework that we believe is of\nindependent interest. We give a robust analysis of the classic method of\nshift-and-invert preconditioning to reduce eigenvector computation to\napproximately solving a sequence of linear systems. We then apply fast\nstochastic variance reduced gradient (SVRG) based system solvers to achieve our\nclaims. \n\n"}
{"id": "1605.09560", "contents": "Title: Gather-and-broadcast frequency control in power systems Abstract: We propose a novel frequency control approach in between centralized and\ndistributed architectures, that is a continuous-time feedback control version\nof the dual decomposition optimization method. Specifically, a convex\ncombination of the frequency measurements is centrally aggregated, followed by\nan integral control and a broadcast signal, which is then optimally allocated\nat local generation units. We show that our gather-and-broadcast control\narchitecture comprises many previously proposed strategies as special cases. We\nprove local asymptotic stability of the closed-loop equilibria of the\nconsidered power system model, which is a nonlinear differential-algebraic\nsystem that includes traditional generators, frequency-responsive devices, as\nwell as passive loads, where the sources are already equipped with primary\ndroop control. Our feedback control is designed such that the closed-loop\nequilibria of the power system solve the optimal economic dispatch problem. \n\n"}
{"id": "1605.09721", "contents": "Title: CYCLADES: Conflict-free Asynchronous Machine Learning Abstract: We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms. \n\n"}
{"id": "1606.00498", "contents": "Title: A Comprehensive Linear Speedup Analysis for Asynchronous Stochastic\n  Parallel Optimization from Zeroth-Order to First-Order Abstract: Asynchronous parallel optimization received substantial successes and\nextensive attention recently. One of core theoretical questions is how much\nspeedup (or benefit) the asynchronous parallelization can bring us. This paper\nprovides a comprehensive and generic analysis to study the speedup property for\na broad range of asynchronous parallel stochastic algorithms from the zeroth\norder to the first order methods. Our result recovers or improves existing\nanalysis on special cases, provides more insights for understanding the\nasynchronous parallel behaviors, and suggests a novel asynchronous parallel\nzeroth order method for the first time. Our experiments provide novel\napplications including model blending problems using the proposed asynchronous\nparallel zeroth order method. \n\n"}
{"id": "1606.01371", "contents": "Title: Assortment optimisation under a general discrete choice model: A tight\n  analysis of revenue-ordered assortments Abstract: The assortment problem in revenue management is the problem of deciding which\nsubset of products to offer to consumers in order to maximise revenue. A simple\nand natural strategy is to select the best assortment out of all those that are\nconstructed by fixing a threshold revenue $\\pi$ and then choosing all products\nwith revenue at least $\\pi$. This is known as the revenue-ordered assortments\nstrategy. In this paper we study the approximation guarantees provided by\nrevenue-ordered assortments when customers are rational in the following sense:\nthe probability of selecting a specific product from the set being offered\ncannot increase if the set is enlarged. This rationality assumption, known as\nregularity, is satisfied by almost all discrete choice models considered in the\nrevenue management and choice theory literature, and in particular by random\nutility models. The bounds we obtain are tight and improve on recent results in\nthat direction, such as for the Mixed Multinomial Logit model by\nRusmevichientong et al. (2014). An appealing feature of our analysis is its\nsimplicity, as it relies only on the regularity condition.\n  We also draw a connection between assortment optimisation and two pricing\nproblems called unit demand envy-free pricing and Stackelberg minimum spanning\ntree: These problems can be restated as assortment problems under discrete\nchoice models satisfying the regularity condition, and moreover revenue-ordered\nassortments correspond then to the well-studied uniform pricing heuristic. When\nspecialised to that setting, the general bounds we establish for\nrevenue-ordered assortments match and unify the best known results on uniform\npricing. \n\n"}
{"id": "1606.02994", "contents": "Title: Evolution of the Wasserstein distance between the marginals of two\n  Markov processes Abstract: In this paper, we are interested in the time derivative of the Wasserstein\ndistance between the marginals of two Markov processes. As recalled in the\nintroduction, the Kantorovich duality leads to a natural candidate for this\nderivative. Up to the sign, it is the sum of the integrals with respect to each\nof the two marginals of the corresponding generator applied to the\ncorresponding Kantorovich potential. For pure jump processes with bounded\nintensity of jumps, we prove that the evolution of the Wasserstein distance is\nactually given by this candidate. In dimension one, we show that this remains\ntrue for Piecewise Deterministic Markov Processes. We apply the formula to\nestimate the exponential decrease rate of the Wasserstein distance between the\nmarginals of two birth and death processes with the same generator in terms of\nthe Wasserstein curvature. \n\n"}
{"id": "1606.03359", "contents": "Title: Viscous corrections of the Time Incremental Minimization Scheme and\n  Visco-Energetic Solutions to Rate-Independent Evolution Problems Abstract: We propose the new notion of Visco-Energetic solutions to rate-independent\nsystems $(X,\\mathcal E,\\mathsf d)$ driven by a time dependent energy $\\mathcal\nE$ and a dissipation quasi-distance $\\mathsf d$ in a general metric-topological\nspace $X$. As for the classic Energetic approach, solutions can be obtained by\nsolving a modified time Incremental Minimization Scheme, where at each step the\ndissipation (quasi-)distance $\\mathsf d$ is incremented by a viscous correction\n$\\delta$ (e.g.~proportional to the square of the distance $\\mathsf d$), which\npenalizes far distance jumps by inducing a localized version of the stability\ncondition.\n  We prove a general convergence result and a typical characterization by\nStability and Energy Balance in a setting comparable to the standard energetic\none, thus capable to cover a wide range of applications. The new refined Energy\nBalance condition compensates the localized stability and provides a careful\ndescription of the jump behavior: at every jump the solution follows an optimal\ntransition, which resembles in a suitable variational sense the discrete scheme\nthat has been implemented for the whole construction. \n\n"}
{"id": "1606.04760", "contents": "Title: Adapting to unknown noise level in sparse deconvolution Abstract: In this paper, we study sparse spike deconvolution over the space of\ncomplex-valued measures when the input measure is a finite sum of Dirac masses.\nWe introduce a modified version of the Beurling Lasso (BLasso), a semi-definite\nprogram that we refer to as the Concomitant Beurling Lasso (CBLasso). This new\nprocedure estimates the target measure and the unknown noise level\nsimultaneously. Contrary to previous estimators in the literature, theory holds\nfor a tuning parameter that depends only on the sample size, so that it can be\nused for unknown noise level problems. Consistent noise level estimation is\nstandardly proved. As for Radon measure estimation, theoretical guarantees\nmatch the previous state-of-the-art results in Super-Resolution regarding\nminimax prediction and localization. The proofs are based on a bound on the\nnoise level given by a new tail estimate of the supremum of a stationary\nnon-Gaussian process through the Rice method. \n\n"}
{"id": "1606.04809", "contents": "Title: ASAGA: Asynchronous Parallel SAGA Abstract: We describe ASAGA, an asynchronous parallel version of the incremental\ngradient algorithm SAGA that enjoys fast linear convergence rates. Through a\nnovel perspective, we revisit and clarify a subtle but important technical\nissue present in a large fraction of the recent convergence rate proofs for\nasynchronous parallel optimization algorithms, and propose a simplification of\nthe recently introduced \"perturbed iterate\" framework that resolves it. We\nthereby prove that ASAGA can obtain a theoretical linear speedup on multi-core\nsystems even without sparsity assumptions. We present results of an\nimplementation on a 40-core architecture illustrating the practical speedup as\nwell as the hardware overhead. \n\n"}
{"id": "1606.05361", "contents": "Title: Impact of storage competition on energy markets Abstract: We study how storage, operating as a price maker within a market environment,\nmay be optimally operated over an extended period of time. The optimality\ncriterion may be the maximisation of the profit of the storage itself, where\nthis profit results from the exploitation of the differences in market clearing\nprices at different times. Alternatively it may be the minimisation of the cost\nof generation, or the maximisation of consumer surplus or social welfare. In\nall cases there is calculated for each successive time-step the cost function\nmeasuring the total impact of whatever action is taken by the storage. The\nsuccession of such cost functions provides the information for the storage to\ndetermine how to behave over time, forming the basis of the appropriate\noptimisation problem. Further, optimal decision making, even over a very long\nor indefinite time period, usually depends on a knowledge of costs over a\nrelatively short running time horizon -- for storage of electrical energy\ntypically of the order of a day or so.\n  We study particularly competition between multiple stores, where the\nobjective of each store is to maximise its own income given the activities of\nthe remainder. We show that, at the Cournot Nash equilibrium, multiple large\nstores collectively erode their own abilities to make profits: essentially each\nstore attempts to increase its own profit over time by overcompeting at the\nexpense of the remainder. We quantify this for linear price functions\n  We give examples throughout based on Great Britain spot-price market data. \n\n"}
{"id": "1606.06709", "contents": "Title: Guaranteed Cost Model Predictive Control-based Driver Assistance System\n  for Vehicle Stabilization Under Tire Parameters Uncertainties Abstract: Road traffic crashes have been the leading cause of death among young people.\nMost of these accidents occur when the driver becomes distracted and a\nloss-of-control situation occurs. Steer-by-Wire systems were recently proposed\nas an alternative to mitigate such accidents. This technology enables the\ndecoupling of the front wheel steering angles from the driver hand wheel angle\nand, consequently, the measurement of road/tire friction limits and the\ndevelopment of novel control systems capable of ensuring vehicle stabilization\nand safety. However, vehicle safety boundaries are highly dependent on tire\ncharacteristics which vary significantly with temperature, wear and the tire\nmanufacturing process. Therefore, design of autonomous vehicle and driver\nassistance controllers cannot assume that these characteristics are constant or\nknown. Thus, this paper proposes a Guaranteed Cost Model Predictive Controller\nDriver Assistance System able to avoid front and rear tire saturation and to\ntrack the drivers intent up to the limits of handling for a vehicle with\nuncertain tire parameters. Simulation results show the performance of the\nproposed approach under time-varying uniformly distributed disturbances. \n\n"}
{"id": "1606.07286", "contents": "Title: Importance sampling strategy for non-convex randomized block-coordinate\n  descent Abstract: As the number of samples and dimensionality of optimization problems related\nto statistics an machine learning explode, block coordinate descent algorithms\nhave gained popularity since they reduce the original problem to several\nsmaller ones. Coordinates to be optimized are usually selected randomly\naccording to a given probability distribution. We introduce an importance\nsampling strategy that helps randomized coordinate descent algorithms to focus\non blocks that are still far from convergence. The framework applies to\nproblems composed of the sum of two possibly non-convex terms, one being\nseparable and non-smooth. We have compared our algorithm to a full gradient\nproximal approach as well as to a randomized block coordinate algorithm that\nconsiders uniform sampling and cyclic block coordinate descent. Experimental\nevidences show the clear benefit of using an importance sampling strategy. \n\n"}
{"id": "1606.07512", "contents": "Title: Constrained fractional variational problems of variable order Abstract: Isoperimetric problems consist in minimizing or maximizing a cost functional\nsubject to an integral constraint. In this work, we present two fractional\nisoperimetric problems where the Lagrangian depends on a combined Caputo\nderivative of variable fractional order and we present a new variational\nproblem subject to a holonomic constraint. We establish necessary optimality\nconditions in order to determine the minimizers of the fractional problems. The\nterminal point in the cost integral, as well the terminal state, are considered\nto be free, and we obtain corresponding natural boundary conditions. \n\n"}
{"id": "1606.08617", "contents": "Title: Sloppy nuclear energy density functionals: effective model reduction Abstract: Concepts from information geometry are used to analyse parameter sensitivity\nfor a nuclear energy density functional, representative of a class of\nsemi-empirical functionals that start from a microscopically motivated ansatz\nfor the density dependence of the energy of a system of protons and neutrons.\nIt is shown that such functionals are sloppy, characterized by an exponential\nrange of sensitivity to parameter variations. Responsive to only a few stiff\nparameter combinations, they exhibit an exponential decrease of sensitivity to\nvariations of the remaining soft parameters. By interpreting the space of model\npredictions as a manifold embedded in the data space, with the parameters of\nthe functional as coordinates on the manifold, it is also shown that the\nexponential distribution of model manifold widths corresponds to the\ndistribution of parameter sensitivity. Using the Manifold Boundary\nApproximation Method, we illustrate how to systematically construct effective\nnuclear density functionals of successively lower dimension in parameter space\nuntil sloppiness is eventually eliminated and the resulting functional contains\nonly stiff combinations of parameters. \n\n"}
{"id": "1606.09365", "contents": "Title: On the worst-case complexity of the gradient method with exact line\n  search for smooth strongly convex functions Abstract: We consider the gradient (or steepest) descent method with exact line search\napplied to a strongly convex function with Lipschitz continuous gradient. We\nestablish the exact worst-case rate of convergence of this scheme, and show\nthat this worst-case behavior is exhibited by a certain convex quadratic\nfunction. We also give the tight worst-case complexity bound for a noisy\nvariant of gradient descent method, where exact line-search is performed in a\nsearch direction that differs from negative gradient by at most a prescribed\nrelative tolerance.\n  The proofs are computer-assisted, and rely on the resolutions of semidefinite\nprogramming performance estimation problems as introduced in the paper [Y.\nDrori and M. Teboulle. Performance of first-order methods for smooth convex\nminimization: a novel approach. Mathematical Programming, 145(1-2):451-482,\n2014]. \n\n"}
{"id": "1606.09552", "contents": "Title: Proximity Operators of Discrete Information Divergences Abstract: Information divergences allow one to assess how close two distributions are\nfrom each other. Among the large panel of available measures, a special\nattention has been paid to convex $\\varphi$-divergences, such as\nKullback-Leibler, Jeffreys-Kullback, Hellinger, Chi-Square, Renyi, and\nI$_{\\alpha}$ divergences. While $\\varphi$-divergences have been extensively\nstudied in convex analysis, their use in optimization problems often remains\nchallenging. In this regard, one of the main shortcomings of existing methods\nis that the minimization of $\\varphi$-divergences is usually performed with\nrespect to one of their arguments, possibly within alternating optimization\ntechniques. In this paper, we overcome this limitation by deriving new\nclosed-form expressions for the proximity operator of such two-variable\nfunctions. This makes it possible to employ standard proximal methods for\nefficiently solving a wide range of convex optimization problems involving\n$\\varphi$-divergences. In addition, we show that these proximity operators are\nuseful to compute the epigraphical projection of several functions of practical\ninterest. The proposed proximal tools are numerically validated in the context\nof optimal query execution within database management systems, where the\nproblem of selectivity estimation plays a central role. Experiments are carried\nout on small to large scale scenarios. \n\n"}
{"id": "1607.00101", "contents": "Title: Randomized block proximal damped Newton method for composite\n  self-concordant minimization Abstract: In this paper we consider the composite self-concordant (CSC) minimization\nproblem, which minimizes the sum of a self-concordant function $f$ and a\n(possibly nonsmooth) proper closed convex function $g$. The CSC minimization is\nthe cornerstone of the path-following interior point methods for solving a\nbroad class of convex optimization problems. It has also found numerous\napplications in machine learning. The proximal damped Newton (PDN) methods have\nbeen well studied in the literature for solving this problem that enjoy a nice\niteration complexity. Given that at each iteration these methods typically\nrequire evaluating or accessing the Hessian of $f$ and also need to solve a\nproximal Newton subproblem, the cost per iteration can be prohibitively high\nwhen applied to large-scale problems. Inspired by the recent success of block\ncoordinate descent methods, we propose a randomized block proximal damped\nNewton (RBPDN) method for solving the CSC minimization. Compared to the PDN\nmethods, the computational cost per iteration of RBPDN is usually significantly\nlower. The computational experiment on a class of regularized logistic\nregression problems demonstrate that RBPDN is indeed promising in solving\nlarge-scale CSC minimization problems. The convergence of RBPDN is also\nanalyzed in the paper. In particular, we show that RBPDN is globally convergent\nwhen $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local\nlinear convergence. Moreover, we show that for a class of $g$ including the\ncase where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear\nconvergence. As a striking consequence, it shows that the classical damped\nNewton methods [22,40] and the PDN [31] for such $g$ are globally linearly\nconvergent, which was previously unknown in the literature. Moreover, this\nresult can be used to sharpen the existing iteration complexity of these\nmethods. \n\n"}
{"id": "1607.00721", "contents": "Title: Recursive utility optimization with concave coefficients Abstract: This paper concerns the recursive utility maximization problem. We assume\nthat the coefficients of the wealth equation and the recursive utility are\nconcave. Then some interesting and important cases with nonlinear and nonsmooth\ncoefficients satisfy our assumption. After given an equivalent backward\nformulation of our problem, we employ the Fenchel-Legendre transform and derive\nthe corresponding variational formulation. By the convex duality method, the\nprimal \"sup-inf\" problem is translated to a dual minimization problem and the\nsaddle point of our problem is derived. Finally, we obtain the optimal terminal\nwealth. To illustrate our results, three cases for investors with ambiguity\naversion are explicitly worked out under some special assumptions. \n\n"}
{"id": "1607.01655", "contents": "Title: L1 penalization of volumetric dose objectives in optimal control of PDEs Abstract: This work is concerned with a class of optimal control problems governed by a\npartial differential equation that are motivated by an application in\nradiotherapy treatment planning, where the primary design objective is to\nminimize the volume where a functional of the state violates a prescribed\nlevel, but prescribing these levels in the form of pointwise state constraints\ncan lead to infeasible problems. We therefore propose an alternative approach\nbased on $L^1$ penalization of the violation. We establish well-posedness of\nthe corresponding optimal control problem, derive first-order optimality\nconditions, and present a semismooth Newton method for the efficient numerical\nsolution of these problems. The performance of this method for a model problem\nis illustrated and contrasted with the alternative approach based on\n(regularized) state constraints. \n\n"}
{"id": "1607.07705", "contents": "Title: Delineating Parameter Unidentifiabilities in Complex Models Abstract: Scientists use mathematical modelling to understand and predict the\nproperties of complex physical systems. In highly parameterised models there\noften exist relationships between parameters over which model predictions are\nidentical, or nearly so. These are known as structural or practical\nunidentifiabilities, respectively. They are hard to diagnose and make reliable\nparameter estimation from data impossible. They furthermore imply the existence\nof an underlying model simplification. We describe a scalable method for\ndetecting unidentifiabilities, and the functional relations defining them, for\ngeneric models. This allows for model simplification, and appreciation of which\nparameters (or functions thereof) cannot be estimated from data. Our algorithm\ncan identify features such as redundant mechanisms and fast timescale\nsubsystems, as well as the regimes in which such approximations are valid. We\nbase our algorithm on a novel quantification of regional parametric\nsensitivity: multiscale sloppiness. Traditionally, the link between parametric\nsensitivity and the conditioning of the parameter estimation problem is made\nlocally, through the Fisher Information Matrix. This is valid in the regime of\ninfinitesimal measurement uncertainty. We demonstrate the duality between\nmultiscale sloppiness and the geometry of confidence regions surrounding\nparameter estimates made where measurement uncertainty is non-negligible.\nFurther theoretical relationships are provided linking multiscale sloppiness to\nthe Likelihood-ratio test. From this, we show that a local sensitivity analysis\n(as typically done) is insufficient for determining the reliability of\nparameter estimation, even with simple (non)linear systems. Our algorithm\nprovides a tractable alternative. We finally apply our methods to a\nlarge-scale, benchmark Systems Biology model of NF-$\\kappa$B, uncovering\npreviously unknown unidentifiabilities. \n\n"}
{"id": "1608.00042", "contents": "Title: Polynomial Time Algorithms and Extended Formulations for Unit Commitment\n  Problems Abstract: Recently increasing penetration of renewable energy generation brings\nchallenges for power system operators to perform efficient power generation\ndaily scheduling, due to the intermittent nature of the renewable generation\nand discrete decisions of each generation unit. Among all aspects to be\nconsidered, a unit commitment polytope is fundamental and embedded in the\nmodels at different stages of power system planning and operations. In this\npaper, we focus on deriving polynomial time algorithms for the unit commitment\nproblems with general convex cost function and piecewise linear cost function\nrespectively. We refine an $\\mathcal{O}(T^3)$ time, where $ T $ represents the\nnumber of time periods, algorithm for the deterministic single-generator unit\ncommitment problem with general convex cost function and accordingly develop an\nextended formulation in a higher dimensional space that can provide an integral\nsolution, in which the physical meanings of the decision variables are\ndescribed. It means the original problem can be solved as a convex program\ninstead of a mixed-integer convex program. Furthermore, for the case in which\nthe cost function is piecewise linear, by exploring the optimality conditions,\nwe derive more efficient algorithms for both deterministic (i.e.,\n$\\mathcal{O}(T)$ time) and stochastic (i.e., $\\mathcal{O}(N)$ time, where $N$\nrepresents the number of nodes in the stochastic scenario tree)\n\\rred{single-generator} unit commitment problems. We also develop the\ncorresponding extended formulations for both deterministic and stochastic\nsingle-generator unit commitment problems that solve the original mixed-integer\nlinear programs as linear programs. Similarly, physical meanings of the\ndecision variables are explored to show the insights of the new modeling\napproach. \n\n"}
{"id": "1608.00663", "contents": "Title: Simulation Optimization of Risk Measures with Adaptive Risk Levels Abstract: Optimizing risk measures such as Value-at-Risk (VaR) and Conditional\nValue-at-Risk (CVaR) of a general loss distribution is usually difficult,\nbecause 1) the loss function might lack structural properties such as convexity\nor differentiability since it is often generated via black-box simulation of a\nstochastic system; 2) evaluation of risk measures often requires rare-event\nsimulation, which is computationally expensive. In this paper, we study the\nextension of the recently proposed gradient-based adaptive stochastic search\n(GASS) to the optimization of risk measures VaR and CVaR. Instead of optimizing\nVaR or CVaR at the target risk level directly, we incorporate an adaptive\nupdating scheme on the risk level, by initializing the algorithm at a small\nrisk level and adaptively increasing it until the target risk level is achieved\nwhile the algorithm converges at the same time. This enables us to adaptively\nreduce the number of samples required to estimate the risk measure at each\niteration, and thus improving the overall efficiency of the algorithm. \n\n"}
{"id": "1608.01728", "contents": "Title: Mean field control hierarchy Abstract: In this paper we model the role of a government of a large population as a\nmean field optimal control problem. Such control problems are constrainted by a\nPDE of continuity-type, governing the dynamics of the probability distribution\nof the agent population. We show the existence of mean field optimal controls\nboth in the stochastic and deterministic setting. We derive rigorously the\nfirst order optimality conditions useful for numerical computation of mean\nfield optimal controls. We introduce a novel approximating hierarchy of\nsub-optimal controls based on a Boltzmann approach, whose computation requires\na very moderate numerical complexity with respect to the one of the optimal\ncontrol. We provide numerical experiments for models in opinion formation\ncomparing the behavior of the control hierarchy. \n\n"}
{"id": "1608.02184", "contents": "Title: Asymptotic Quantum Algorithm for the Toeplitz Systems Abstract: Solving the Toeplitz systems, which is to find the vector $x$ such that $T_nx\n= b$ given an $n\\times n$ Toeplitz matrix $T_n$ and a vector $b$, has a variety\nof applications in mathematics and engineering. In this paper, we present a\nquantum algorithm for solving the linear equations of Toeplitz matrices, in\nwhich the Toeplitz matrices are generated by discretizing a continuous\nfunction. It is shown that our algorithm's complexity is nearly\n$O(\\kappa\\textrm{log}^2 n)$, where $\\kappa$ and $n$ are the condition number\nand the dimension of $T_n$ respectively. This implies our algorithm is\nexponentially faster than the best classical algorithm for the same problem if\n$\\kappa=O(\\textrm{poly}(\\textrm{log}\\,n))$. Since no assumption on the\nsparseness of $T_n$ is demanded in our algorithm, it can serve as an example of\nquantum algorithms for solving non-sparse linear systems. \n\n"}
{"id": "1608.02300", "contents": "Title: A simple preprocessing algorithm for semidefinite programming Abstract: We propose a very simple preprocessing algorithm for semidefinite\nprogramming. Our algorithm inspects the constraints of the problem, deletes\nredundant rows and columns in the constraints, and reduces the size of the\nvariable matrix. It often detects infeasibility. Our algorithm does not rely on\nany optimization solver: the only subroutine it needs is Cholesky\nfactorization, hence it can be implemented with a few lines of code in machine\nprecision. We present computational results on a set of problems arising mostly\nfrom polynomial optimization. \n\n"}
{"id": "1608.03165", "contents": "Title: Linear Programming based Converses for Finite Blocklength Lossy Joint\n  Source-Channel Coding Abstract: A linear programming (LP) based framework is presented for obtaining\nconverses for finite blocklength lossy joint source-channel coding problems.\nThe framework applies for any loss criterion, generalizes certain previously\nknown converses, and also extends to multi-terminal settings. The finite\nblocklength problem is posed equivalently as a nonconvex optimization problem\nand using a lift-and-project-like method, a close but tractable LP relaxation\nof this problem is derived. Lower bounds on the original problem are obtained\nby the construction of feasible points for the dual of the LP relaxation. A\nparticular application of this approach leads to new converses which recover\nand improve on the converses of Kostina and Verdu for finite blocklength lossy\njoint source-channel coding and lossy source coding. For finite blocklength\nchannel coding, the LP relaxation recovers the converse of Polyanskiy, Poor and\nVerdu and leads to a new improvement on the converse of Wolfowitz, showing\nthereby that our LP relaxation is asymptotically tight with increasing\nblocklengths for channel coding, lossless source coding and joint\nsource-channel coding with the excess distortion probability as the loss\ncriterion. Using a duality based argument, a new converse is derived for finite\nblocklength joint source-channel coding for a class of source-channel pairs.\nEmploying this converse, the LP relaxation is also shown to be tight for all\nblocklengths for the minimization of the expected average symbol-wise Hamming\ndistortion of a $q$-ary uniform source over a $q$-ary symmetric memoryless\nchannel for any $q \\in N$. The optimization formulation and the\nlift-and-project method are extended to networked settings and demonstrated by\nobtaining an improvement on a converse of Zhou et al. for the successive\nrefinement problem for successively refinable source-distortion measure\ntriplets. \n\n"}
{"id": "1608.03260", "contents": "Title: Duality Approach to Bilevel Programs with a Convex Lower Level Abstract: Bilevel programs are optimization problems where some variables are solutions\nto optimization problems themselves, and they arise in a variety of control\napplications, including: control of vehicle traffic networks, inverse\nreinforcement learning and inverse optimization, and robust control for\nhuman-automation systems. This paper develops a duality-based approach to\nsolving bilevel programs where the lower level problem is convex. Our approach\nis to use partial dualization to construct a new dual function that is\ndifferentiable, unlike the Lagrangian dual that is only directionally\ndifferentiable. We use our dual to define a duality-based reformulation of\nbilevel programs, prove equivalence of our reformulation with the original\nbilevel program, and then introduce regularization to ensure constraint\nqualification holds. These technical results about our new dual and regularized\nduality-based reformulation are used to provide theoretical justification for\nan algorithm we construct for solving bilevel programs with a convex lower\nlevel, and we conclude by demonstrating the efficacy of our algorithm by\nsolving two practical instances of bilevel programs. \n\n"}
{"id": "1608.03871", "contents": "Title: Application of Polynomial Optimization to Electricity Transmission\n  Networks Abstract: Transmission system operators need to adapt their decision-making tools to\nthe technological evolutions of the twenty first century. A computation\ninherent to most tools seeks to find alternating-current power flows that\nminimize power loss or generation cost. Mathematically, it consists in an\noptimization problem that can be described using only addition and\nmultiplication of complex numbers (i.e. complex polynomial optimization). The\nobjective of this thesis is to find feasible points that are globally optimal.\nOne of the outcomes of this collaborative doctoral project is to transpose the\nLasserre hierarchy to complex numbers. We use it to compute globally optimal\npower flows in the European high-voltage transmission network. It consist in a\nsparse non-convex quadratically-constrained quadratic program with several\nthousand variables and constraints. The complex hierarchy is generally more\ntractable than the Lasserre hierarchy when applied to the optimal power flow\nproblem. \n\n"}
{"id": "1608.04135", "contents": "Title: Simultaneous Input and State Estimation for Linear Time-Varying\n  Continuous-Time Stochastic Systems Abstract: In this paper, we present an optimal filter for linear time-varying\ncontinuous-time stochastic systems that simultaneously estimates the states and\nunknown inputs in an unbiased minimum-variance sense. We first show that the\nunknown inputs cannot be estimated without additional assumptions. Then, we\ndiscuss two complementary variants of the filter: (i) for the case when an\nadditional measurement containing information about the state derivative is\navailable, and (ii) for the case without the additional measurement but the\ninput signals are assumed to be sufficiently smooth and have bounded\nderivatives. Conditions for uniform asymptotic stability and the existence of a\nsteady-state solution for the proposed filter, as well as the convergence rate\nof the state and input estimate biases are given. Moreover, we show that a\nprinciple of separation of estimation and control holds and that the unknown\ninputs may be rejected. Two examples, including a nonlinear vehicle reentry\nexample, are given to illustrate that our filter is applicable even when some\nstrong assumptions do not hold. \n\n"}
{"id": "1608.04137", "contents": "Title: A second order dynamical system with Hessian-driven damping and penalty\n  term associated to variational inequalities Abstract: We consider the minimization of a convex objective function subject to the\nset of minima of another convex function, under the assumption that both\nfunctions are twice continuously differentiable. We approach this optimization\nproblem from a continuous perspective by means of a second order dynamical\nsystem with Hessian-driven damping and a penalty term corresponding to the\nconstrained function. By constructing appropriate energy functionals, we prove\nweak convergence of the trajectories generated by this differential equation to\na minimizer of the optimization problem as well as convergence for the\nobjective function values along the trajectories. The performed investigations\nrely on Lyapunov analysis in combination with the continuous version of the\nOpial Lemma. In case the objective function is strongly convex, we can even\nshow strong convergence of the trajectories. \n\n"}
{"id": "1608.04652", "contents": "Title: Study of movement coordination in human ensembles via a novel\n  computer-based set-up Abstract: Movement coordination in human ensembles has been studied little in the\ncurrent literature. In the existing experimental works, situations where all\nsubjects are connected with each other through direct visual and auditory\ncoupling, and social interaction affects their coordination, have been\ninvestigated. Here, we study coordination in human ensembles via a novel\ncomputer-based set-up that enables individuals to coordinate each other's\nmotion from a distance so as to minimize the influence of social interaction.\nThe proposed platform makes it possible to implement different visual\ninteraction patterns among the players, so that participants take into\nconsideration the motion of a designated subset of the others. This allows the\nevaluation of the exclusive effects on coordination of the structure of\ninterconnections among the players and their own dynamics. Our set-up enables\nalso the deployment of virtual players to investigate dyadic interaction\nbetween a human and a virtual agent, as well as group synchronization in mixed\nteams of human and virtual agents. We use this novel set-up to study\ncoordination both in dyads and in groups over different structures of\ninterconnections, with and without virtual agents. We find that, in dual\ninteraction, virtual players manage to interact with participants in a\nhuman-like fashion, thus confirming findings in previous work. We also observe\nthat, in group interaction, the level of coordination among humans in the\nabsence of direct visual and auditory coupling depends on the structure of\ninterconnections among participants. This confirms, as recently suggested in\nthe literature, that different coordination levels are achieved over diverse\nvisual pairings in the presence and in the absence of social interaction. We\npresent preliminary experimental results on the effect on group coordination of\ndeploying virtual computer agents in the human ensemble. \n\n"}
{"id": "1608.07055", "contents": "Title: PALMA, an improved algorithm for DOSY signal processing Abstract: NMR is a tool of choice for the measure of diffusion coefficients of species\nin solution. The DOSY experiment, a 2D implementation of this measure, has\nproven to be particularly useful for the study of complex mixtures, molecular\ninteractions, polymers, etc. However, DOSY data analysis requires to resort to\ninverse Laplace transform, in particular for polydisperse samples. This is a\nknown difficult numerical task, for which we present here a novel approach. A\nnew algorithm based on a splitting scheme and on the use of proximity operators\nis introduced. Used in conjunction with a Maximum Entropy and $\\ell_1$ hybrid\nregularisation, this algorithm converges rapidly and produces results robust\nagainst experimental noise. This method has been called PALMA. It is able to\nreproduce faithfully monodisperse as well as polydisperse systems, and numerous\nsimulated and experimental examples are presented. It has been implemented on\nthe server http://palma.labo.igbmc.fr where users can have their datasets\nprocessed automatically. \n\n"}
{"id": "1609.01458", "contents": "Title: On a Distributed Computation of Supervisors in Modular Supervisory\n  Control Abstract: In this paper, we discuss a supervisory control problem of modular\ndiscrete-event systems that allows for a distributed computation of\nsupervisors. We provide a characterization and an algorithm to compute the\nsupervisors. If the specification does not satisfy the properties, we make use\nof a relaxation of coordination control to compute a sublanguage of the\nspecification for which the supervisors can be computed in a distributed way. \n\n"}
{"id": "1609.02963", "contents": "Title: Event-triggered second-moment stabilization of linear systems under\n  packet drops Abstract: This paper deals with the stabilization of linear systems with process noise\nunder packet drops between the sensor and the controller. Our aim is to ensure\nexponential convergence of the second moment of the plant state to a given\nbound in finite time. Motivated by considerations about the efficient use of\nthe available resources, we adopt an event-triggering approach to design the\ntransmission policy. In our design, the sensor's decision to transmit or not\nthe state to the controller is based on an online evaluation of the future\nsatisfaction of the control objective. The resulting event-triggering policy is\nhence specifically tailored to the control objective. We formally establish\nthat the proposed event-triggering policy meets the desired objective and\nquantify its efficiency by providing an upper bound on the fraction of expected\nnumber of transmissions in an infinite time interval. Simulations for scalar\nand vector systems illustrate the results. \n\n"}
{"id": "1609.04836", "contents": "Title: On Large-Batch Training for Deep Learning: Generalization Gap and Sharp\n  Minima Abstract: The stochastic gradient descent (SGD) method and its variants are algorithms\nof choice for many Deep Learning tasks. These methods operate in a small-batch\nregime wherein a fraction of the training data, say $32$-$512$ data points, is\nsampled to compute an approximation to the gradient. It has been observed in\npractice that when using a larger batch there is a degradation in the quality\nof the model, as measured by its ability to generalize. We investigate the\ncause for this generalization drop in the large-batch regime and present\nnumerical evidence that supports the view that large-batch methods tend to\nconverge to sharp minimizers of the training and testing functions - and as is\nwell known, sharp minima lead to poorer generalization. In contrast,\nsmall-batch methods consistently converge to flat minimizers, and our\nexperiments support a commonly held view that this is due to the inherent noise\nin the gradient estimation. We discuss several strategies to attempt to help\nlarge-batch methods eliminate this generalization gap. \n\n"}
{"id": "1609.05248", "contents": "Title: Exact and Efficient Hamilton-Jacobi-based Guaranteed Safety Analysis via\n  System Decomposition Abstract: Hamilton-Jacobi (HJ) reachability is a method that provides rigorous analyses\nof the safety properties of dynamical systems. This method has been\nsuccessfully applied to many low-dimensional dynamical system models such as\ncoarse models of aircraft and quadrotors in order to provide safety guarantees\nin potentially dangerous scenarios. These guarantees can be provided by the\ncomputation of a backward reachable set (BRS), which represents the set of\nstates from which the system may be driven into violating safety properties\ndespite the system's best effort to remain safe. Unfortunately, HJ reachability\nis not practical for high-dimensional systems because the complexity of the BRS\ncomputation scales exponentially with the number of state dimensions. Although\nnumerous approximation techniques are able to tractably provide conservative\nestimates of the BRS, they often require restrictive assumptions about system\ndynamics without providing an exact solution. In this paper we propose a\ngeneral method for decomposing dynamical systems. Even when the resulting\nsubsystems are coupled, relatively high-dimensional BRSs that were previously\nintractable or expensive to compute can now be quickly and exactly computed in\nlower-dimensional subspaces. As a result, the curse of dimensionality is\nalleviated to a large degree without sacrificing optimality. We demonstrate our\ntheoretical results through two numerical examples: a 3D Dubins Car model and a\n6D Acrobatic Quadrotor model. \n\n"}
{"id": "1609.05879", "contents": "Title: Online Output-Feedback Parameter and State Estimation for Second Order\n  Linear Systems Abstract: In this paper, a concurrent learning based adaptive observer is developed for\na class of second-order linear time-invariant systems with uncertain system\nmatrices. The developed technique yields an exponentially convergent state\nestimator and an exponentially convergent parameter estimator. As opposed to\npersistent excitation required for parameter convergence in traditional\nadaptive methods, excitation over a finite time-interval is sufficient for the\ndeveloped technique to achieve exponential convergence. Simulation results in\nboth noise-free and noisy environments are presented to validate the design. \n\n"}
{"id": "1609.05920", "contents": "Title: Line Search For Generalized Alternating Projections Abstract: This paper is about line search for the generalized alternating projections\n(GAP) method. This method is a generalization of the von Neumann alternating\nprojections method, where instead of performing alternating projections,\nrelaxed projections are alternated. The method can be interpreted as an\naveraged iteration of a nonexpansive mapping. Therefore, a recently proposed\nline search method for such algorithms is applicable to GAP. We evaluate this\nline search and show situations when the line search can be performed with\nlittle additional cost. We also present a variation of the basic line search\nfor GAP - the projected line search. We prove its convergence and show that the\nline search condition is convex in the step length parameter. We show that\nalmost all convex optimization problems can be solved using this approach and\nnumerical results show superior performance with both the standard and the\nprojected line search, sometimes by several orders of magnitude, compared to\nthe nominal method. \n\n"}
{"id": "1609.06804", "contents": "Title: Decoupled Asynchronous Proximal Stochastic Gradient Descent with\n  Variance Reduction Abstract: In the era of big data, optimizing large scale machine learning problems\nbecomes a challenging task and draws significant attention. Asynchronous\noptimization algorithms come out as a promising solution. Recently, decoupled\nasynchronous proximal stochastic gradient descent (DAP-SGD) is proposed to\nminimize a composite function. It is claimed to be able to off-loads the\ncomputation bottleneck from server to workers by allowing workers to evaluate\nthe proximal operators, therefore, server just need to do element-wise\noperations. However, it still suffers from slow convergence rate because of the\nvariance of stochastic gradient is nonzero. In this paper, we propose a faster\nmethod, decoupled asynchronous proximal stochastic variance reduced gradient\ndescent method (DAP-SVRG). We prove that our method has linear convergence for\nstrongly convex problem. Large-scale experiments are also conducted in this\npaper, and results demonstrate our theoretical analysis. \n\n"}
{"id": "1609.07221", "contents": "Title: Convergence analysis of the direct extension of ADMM for multiple-block\n  separable convex minimization Abstract: Recently, the alternating direction method of multipliers (ADMM) has found\nmany efficient applications in various areas; and it has been shown that the\nconvergence is not guaranteed when it is directly extended to the\nmultiple-block case of separable convex minimization problems where there are\n$m\\ge 3$ functions without coupled variables in the objective. This fact has\ngiven great impetus to investigate various conditions on both the model and the\nalgorithm's parameter that can ensure the convergence of the direct extension\nof ADMM (abbreviated as \"e-ADMM\"). Despite some results under very strong\nconditions (e.g., at least $(m-1)$ functions should be strongly convex) that\nare applicable to the generic case with a general $m$, some others concentrate\non the special case of $m=3$ under the relatively milder condition that only\none function is assumed to be strongly convex. We focus on extending the\nconvergence analysis from the case of $m=3$ to the more general case of\n$m\\ge3$. That is, we show the convergence of e-ADMM for the case of $m\\ge 3$\nwith the assumption of only $(m-2)$ functions being strongly convex; and\nestablish its convergence rates in different scenarios such as the worst-case\nconvergence rates measured by iteration complexity and the asymptotically\nlinear convergence rate under stronger assumptions. Thus the convergence of\ne-ADMM for the general case of $m\\ge 4$ is proved; this result seems to be\nstill unknown even though it is intuitive given the known result of the case of\n$m=3$. Even for the special case of $m=3$, our convergence results turn out to\nbe more general than the exiting results that are derived specifically for the\ncase of $m=3$. \n\n"}
{"id": "1609.07261", "contents": "Title: Existence of tangent lines to Carnot-Carath\\'eodory geodesics Abstract: We show that length minimizing curves in Carnot-Carath\\'eodory spaces possess\nat any point at least one tangent curve (i.e., a blow-up in the nilpotent\napproximation) equal to a straight horizontal line. This is the first\nregularity result for length minimizers that holds with no assumption on either\nthe space (e.g., its rank, step, or analyticity) or the curve, and it is novel\neven in the setting of Carnot groups. \n\n"}
{"id": "1609.08121", "contents": "Title: Improving the Randomization Step in Feasibility Pump Abstract: Feasibility pump (FP) is a successful primal heuristic for mixed-integer\nlinear programs (MILP). The algorithm consists of three main components:\nrounding fractional solution to a mixed-integer one, projection of infeasible\nsolutions to the LP relaxation, and a randomization step used when the\nalgorithm stalls. While many generalizations and improvements to the original\nFeasibility Pump have been proposed, they mainly focus on the rounding and\nprojection steps.\n  We start a more in-depth study of the randomization step in Feasibility Pump.\nFor that, we propose a new randomization step based on the WalkSAT algorithm\nfor solving SAT instances. First, we provide theoretical analyses that show the\npotential of this randomization step; to the best of our knowledge, this is the\nfirst time any theoretical analysis of running-time of Feasibility Pump or its\nvariants has been conducted. Moreover, we also conduct computational\nexperiments incorporating the proposed modification into a state-of-the-art\nFeasibility Pump code that reinforce the practical value of the new\nrandomization step. \n\n"}
{"id": "1610.00652", "contents": "Title: Open research areas in distance geometry Abstract: Distance Geometry is based on the inverse problem that asks to find the\npositions of points, in a Euclidean space of given dimension, that are\ncompatible with a given set of distances. We briefly introduce the field, and\ndiscuss some open and promising research areas. \n\n"}
{"id": "1610.02139", "contents": "Title: Active Fault Tolerant Flight Control System Design Abstract: In this paper we investigate the design of an active fault tolerant control\nsystem applicable to autonomous flight. The system comprises a nonlinear model\npredictive based controller integrated with an unscented Kalman filter for\nfault detection and identification. We apply the fault tolerant control system\ndesign to a generic aircraft model, and simulate a failed engine scenario. The\nresults show that the system correctly identifies the fault within seconds of\noccurrence and updates the nonlinear model predictive controller which is then\nable to reallocate control authority to the healthy actuators based upon up to\ndate fault information. \n\n"}
{"id": "1610.02844", "contents": "Title: Continuous-time Markov decision processes with exponential utility Abstract: In this paper, we consider a continuous-time Markov decision process (CTMDP)\nin Borel spaces, where the certainty equivalent with respect to the exponential\nutility of the total undiscounted cost is to be minimized. The cost rate is\nnonnegative. We establish the optimality equation. Under the\ncompactness-continuity condition, we show the existence of a deterministic\nstationary optimal policy. We reduce the risk-sensitive CTMDP problem to an\nequivalent risk-sensitive discrete-time Markov decision process, which is with\nthe same state and action spaces as the original CTMDP. In particular, the\nvalue iteration algorithm for the CTMDP problem follows from this reduction. We\ndo not need impose any condition on the growth of the transition and cost rate\nin the state, and the controlled process could be explosive. \n\n"}
{"id": "1610.08195", "contents": "Title: On stochastic mirror-prox algorithms for stochastic Cartesian\n  variational inequalities: randomized block coordinate and optimal averaging\n  schemes Abstract: Motivated by multi-user optimization problems and non-cooperative Nash games\nin uncertain regimes, we consider stochastic Cartesian variational inequalities\n(SCVI) where the set is given as the Cartesian product of a collection of\ncomponent sets. First, we consider the case where the number of the component\nsets is large. For solving this type of problems, the classical stochastic\napproximation methods and their prox generalizations are computationally\ninefficient as each iteration becomes very costly. To address this challenge,\nwe develop a randomized block stochastic mirror-prox (B-SMP) algorithm, where\nat each iteration only a randomly selected block coordinate of the solution is\nupdated through implementing two consecutive projection steps. Under standard\nassumptions on the problem and settings of the algorithm, we show that when the\nmapping is strictly pseudo-monotone, the algorithm generates a sequence of\niterates that converges to the solution of the problem almost surely. To derive\nrate statements, we assume that the maps are strongly pseudo-monotone and\nobtain {a non-asymptotic mean squared error\n$\\mathcal{O}\\left(\\frac{d}{k}\\right)$, where $k$ is the iteration number and\n$d$ is the number of component sets. Second, we consider large-scale stochastic\noptimization problems with convex objectives. For this class of problems, we\ndevelop a new averaging scheme for the B-SMP algorithm. Unlike the classical\naveraging stochastic mirror-prox (SMP) method where a decreasing set of weights\nfor the averaging sequence is used, here we consider a different set of weights\nthat are characterized in terms of the stepsizes and a {parameter}. We show\nthat using such weights, the objective values of the averaged sequence\nconverges to the optimal value in the mean sense with the rate\n$\\mathcal{O}\\left(\\frac{\\sqrt{d}}{\\sqrt{k}}\\right)$. \n\n"}
{"id": "1610.09578", "contents": "Title: Maximum Rate of Growth of Enstrophy in Solutions of the Fractional\n  Burgers Equation Abstract: This investigation is a part of a research program aiming to characterize the\nextreme behavior possible in hydrodynamic models by analyzing the maximum\ngrowth of certain fundamental quantities. We consider here the rate of growth\nof the classical and fractional enstrophy in the fractional Burgers equation in\nthe subcritical and supercritical regimes. Since solutions to this equation\nexhibit, respectively, globally well-posed behavior and finite-time blow-up in\nthese two regimes, this makes it a useful model to study the maximum\ninstantaneous growth of enstrophy possible in these two distinct situations.\nFirst, we obtain estimates on the rates of growth and then show that these\nestimates are sharp up to numerical prefactors. This is done by numerically\nsolving suitably defined constrained maximization problems and then\ndemonstrating that for different values of the fractional dissipation exponent\nthe obtained maximizers saturate the upper bounds in the estimates as the\nenstrophy increases. We conclude that the power-law dependence of the enstrophy\nrate of growth on the fractional dissipation exponent has the same global form\nin the subcritical, critical and parts of the supercritical regime. This\nindicates that the maximum enstrophy rate of growth changes smoothly as global\nwell-posedness is lost when the fractional dissipation exponent attains\nsupercritical values. In addition, nontrivial behavior is revealed for the\nmaximum rate of growth of the fractional enstrophy obtained for small values of\nthe fractional dissipation exponents. We also characterize the structure of the\nmaximizers in different cases. \n\n"}
{"id": "1611.00347", "contents": "Title: Surpassing Gradient Descent Provably: A Cyclic Incremental Method with\n  Linear Convergence Rate Abstract: Recently, there has been growing interest in developing optimization methods\nfor solving large-scale machine learning problems. Most of these problems boil\ndown to the problem of minimizing an average of a finite set of smooth and\nstrongly convex functions where the number of functions $n$ is large. Gradient\ndescent method (GD) is successful in minimizing convex problems at a fast\nlinear rate; however, it is not applicable to the considered large-scale\noptimization setting because of the high computational complexity. Incremental\nmethods resolve this drawback of gradient methods by replacing the required\ngradient for the descent direction with an incremental gradient approximation.\nThey operate by evaluating one gradient per iteration and executing the average\nof the $n$ available gradients as a gradient approximate. Although, incremental\nmethods reduce the computational cost of GD, their convergence rates do not\njustify their advantage relative to GD in terms of the total number of gradient\nevaluations until convergence. In this paper, we introduce a Double Incremental\nAggregated Gradient method (DIAG) that computes the gradient of only one\nfunction at each iteration, which is chosen based on a cyclic scheme, and uses\nthe aggregated average gradient of all the functions to approximate the full\ngradient. The iterates of the proposed DIAG method uses averages of both\niterates and gradients in oppose to classic incremental methods that utilize\ngradient averages but do not utilize iterate averages. We prove that not only\nthe proposed DIAG method converges linearly to the optimal solution, but also\nits linear convergence factor justifies the advantage of incremental methods on\nGD. In particular, we prove that the worst case performance of DIAG is better\nthan the worst case performance of GD. \n\n"}
{"id": "1611.00589", "contents": "Title: Stochastic Control and Differential Games with Path-Dependent Influence\n  of Controls on Dynamics and Running Cost Abstract: In this paper, we consider the functional It\\^o calculus framework to find a\npath-dependent version of the Hamilton-Jacobi-Bellman equation for stochastic\ncontrol problems that feature dynamics and running cost that depend on the path\nof the control. We also prove a Dynamic Programming Principle for such\nproblems. We apply our results to path-dependence of the delay type. We further\nstudy Stochastic Differential Games in this context. \n\n"}
{"id": "1611.00607", "contents": "Title: Three Perspectives on Complexity $-$ Entropy, Compression, Subsymmetry Abstract: There is no single universally accepted definition of \"Complexity\". There are\nseveral perspectives on complexity and what constitutes complex behaviour or\ncomplex systems, as opposed to regular, predictable behaviour and simple\nsystems. In this paper, we explore the following perspectives on complexity:\n\"effort-to-describe\" (Shannon entropy $H$, Lempel-Ziv complexity $LZ$),\n\"effort-to-compress\" ($ETC$ complexity) and \"degree-of-order\" (Subsymmetry or\n$SubSym$). While Shannon entropy and $LZ$ are very popular and widely used,\n$ETC$ is a recently proposed measure for time series. In this paper, we also\npropose a novel normalized measure $SubSym$ based on the existing idea of\ncounting the number of subsymmetries or palindromes within a sequence. We\ncompare the performance of these complexity measures on the following tasks: a)\ncharacterizing complexity of short binary sequences of lengths 4 to 16, b)\ndistinguishing periodic and chaotic time series from 1D logistic map and 2D\nH\\'{e}non map, and c) distinguishing between tonic and irregular spiking\npatterns generated from the \"Adaptive exponential integrate-and-fire\" neuron\nmodel. Our study reveals that each perspective has its own advantages and\nuniqueness while also having an overlap with each other. \n\n"}
{"id": "1611.00675", "contents": "Title: emgr - The Empirical Gramian Framework Abstract: System Gramian matrices are a well-known encoding for properties of\ninput-output systems such as controllability, observability or minimality.\nThese so-called system Gramians were developed in linear system theory for\napplications such as model order reduction of control systems. Empirical\nGramian are an extension to the system Gramians for parametric and nonlinear\nsystems as well as a data-driven method of computation. The empirical Gramian\nframework - emgr - implements the empirical Gramians in a uniform and\nconfigurable manner, with applications such as Gramian-based (nonlinear) model\nreduction, decentralized control, sensitivity analysis, parameter\nidentification and combined state and parameter reduction. \n\n"}
{"id": "1611.01046", "contents": "Title: Learning to Pivot with Adversarial Networks Abstract: Several techniques for domain adaptation have been proposed to account for\ndifferences in the distribution of the data used for training and testing. The\nmajority of this work focuses on a binary domain label. Similar problems occur\nin a scientific context where there may be a continuous family of plausible\ndata generation processes associated to the presence of systematic\nuncertainties. Robust inference is possible if it is based on a pivot -- a\nquantity whose distribution does not depend on the unknown values of the\nnuisance parameters that parametrize this family of data generation processes.\nIn this work, we introduce and derive theoretical results for a training\nprocedure based on adversarial networks for enforcing the pivotal property (or,\nequivalently, fairness with respect to continuous attributes) on a predictive\nmodel. The method includes a hyperparameter to control the trade-off between\naccuracy and robustness. We demonstrate the effectiveness of this approach with\na toy example and examples from particle physics. \n\n"}
{"id": "1611.01496", "contents": "Title: Geometry of vectorial martingale optimal transportations and duality Abstract: The theory of Optimal Transport (OT) and Martingale Optimal Transport (MOT)\nwere inspired by problems in economics and finance and have flourished over the\npast decades, making significant advances in theory and practice. MOT considers\nthe problem of pricing and hedging of a financial instrument, referred to as an\noption, assuming its payoff depends on a single asset price. In this paper we\nintroduce Vectorial Martingale Optimal Transport (VMOT) problem, which\nconsiders the more general and realistic situation in which the option payoff\ndepends on multiple asset prices. We address this problem of pricing and\nhedging given market information -- described by vectorial marginal\ndistributions of underlying asset prices -- which is an intimately relevant\nsetup in the robust financial framework.\n  We establish that the VMOT problem, as an infinite-dimensional linear\nprogramming, admits an optimizer for its dual program. Such existence result of\ndual optimizers is significant for several reasons: the dual optimizers\ndescribe how a person who is liable for an option payoff can formulate optimal\nhedging portfolios, and more importantly, they can provide crucial information\non the geometry of primal optimizers, i.e. the VMOTs. As an illustration, we\nshow that multiple martingales given marginals must exhibit an extremal\nconditional correlation structure whenever they jointly optimize the\nexpectation of distance-type cost functions. \n\n"}
{"id": "1611.01828", "contents": "Title: Fast ADMM for homogeneous self-dual embedding of sparse SDPs Abstract: We propose an efficient first-order method, based on the alternating\ndirection method of multipliers (ADMM), to solve the homogeneous self-dual\nembedding problem for a primal-dual pair of semidefinite programs (SDPs) with\nchordal sparsity. Using a series of block eliminations, the per-iteration cost\nof our method is the same as applying a splitting method to the primal or dual\nalone. Moreover, our approach is more efficient than other first-order methods\nfor generic sparse conic programs since we work with smaller semidefinite\ncones. In contrast to previous first-order methods that exploit chordal\nsparsity, our algorithm returns both primal and dual solutions when available,\nand a certificate of infeasibility otherwise. Our techniques are implemented in\nthe open-source MATLAB solver CDCS. Numerical experiments on three sets of\nbenchmark problems from the library SDPLIB show speed-ups compared to some\ncommon state-of-the-art software packages. \n\n"}
{"id": "1611.02338", "contents": "Title: Line failure probability bounds for power grids Abstract: We develop upper bounds for line failure probabilities in power grids, under\nthe DC approximation and assuming Gaussian noise for the power injections. Our\nupper bounds are explicit, and lead to characterization of safe operational\ncapacity regions that are convex and polyhedral, making our tools compatible\nwith existing planning methods. Our probabilistic bounds are derived through\nthe use of powerful concentration inequalities. \n\n"}
{"id": "1611.05475", "contents": "Title: The Bayesian Formulation and Well-Posedness of Fractional Elliptic\n  Inverse Problems Abstract: We study the inverse problem of recovering the order and the diffusion\ncoefficient of an elliptic fractional partial differential equation from a\nfinite number of noisy observations of the solution. We work in a Bayesian\nframework and show conditions under which the posterior distribution is given\nby a change of measure from the prior. Moreover, we show well-posedness of the\ninverse problem, in the sense that small perturbations of the observed solution\nlead to small Hellinger perturbations of the associated posterior measures. We\nthus provide a mathematical foundation to the Bayesian learning of the order\n---and other inputs--- of fractional models. \n\n"}
{"id": "1611.05533", "contents": "Title: Viscosity Solutions to Path-Dependent HJB Equation and Applications Abstract: In this article, the notion of viscosity solution is introduced for the\npath-dependent Hamilton-Jacobi-Bellman (PHJB) equations associated with the\noptimal control problems for path-dependent stochastic differential equations.\nWe identify the value functional of the optimal control problems as unique\nviscosity solution to the associated PHJB equations. Applications to backward\nstochastic Hamilton-Jacobi-Bellman equations are also given. \n\n"}
{"id": "1611.06830", "contents": "Title: Linear quadratic stochastic control problems with stochastic terminal\n  constraint Abstract: We study a linear quadratic optimal control problem with stochastic\ncoefficients and a terminal state constraint, which may be in force merely on a\nset with positive, but not necessarily full probability. Under such a partial\nterminal constraint, the usual approach via a coupled system of a backward\nstochastic Riccati equation and a linear backward equation breaks down. As a\nremedy, we introduce a family of auxiliary problems parametrized by the\nsupersolutions to this Riccati equation alone. The target functional of these\nproblems dominates the original constrained one and allows for an explicit\ndescription of both the optimal control policy and the auxiliary problem's\nvalue in terms of a suitably constructed optimal signal process. This suggests\nthat, for the minimal supersolution of the Riccati equation, the minimizers of\nthe auxiliary problem coincide with those of the original problem, a conjecture\nthat we see confirmed in all examples understood so far. \n\n"}
{"id": "1611.07682", "contents": "Title: Special cases of the quadratic shortest path problem Abstract: The quadratic shortest path problem (QSPP) is \\textcolor{black}{the problem\nof finding a path with prespecified start vertex $s$ and end vertex $t$ in a\ndigraph} such that the sum of weights of arcs and the sum of interaction costs\nover all pairs of arcs on the path is minimized. We first consider a variant of\nthe QSPP known as the adjacent QSPP. It was recently proven that the adjacent\nQSPP on cyclic digraphs cannot be approximated unless P=NP. Here, we give a\nsimple proof for the same result.\n  We also show that if the quadratic cost matrix is a symmetric weak sum matrix\n\\textcolor{black}{ and all $s$-$t$ paths have the same length,} then an optimal\nsolution for the QSPP can be obtained by solving the corresponding instance of\nthe shortest path problem. Similarly, it is shown that the QSPP with a\nsymmetric product cost matrix is solvable in polynomial time.\n  Further, we provide sufficient and necessary conditions for a QSPP instance\non a complete symmetric digraph with four vertices to be linearizable. We also\ncharacterize linearizable QSPP instances on complete symmetric digraphs with\nmore than four vertices. Finally, we derive an algorithm that examines whether\na QSPP instance on the directed grid graph $G_{pq}$ ($p,q\\geq 2$) is\nlinearizable. The complexity of this algorithm is\n${\\mathcal{O}(p^{3}q^{2}+p^{2}q^{3})}$. \n\n"}
{"id": "1611.07943", "contents": "Title: Scheduling of EV Battery Swapping, I: Centralized Solution Abstract: We formulate an optimal scheduling problem for battery swapping that assigns\nto each electric vehicle (EV) a best station to swap its depleted battery based\non its current location and state of charge. The schedule aims to minimize\ntotal travel distance and generation cost over both station assignments and\npower flow variables, subject to EV range constraints, grid operational\nconstraints and AC power flow equations. To deal with the nonconvexity of power\nflow equations and the binary nature of station assignments, we propose a\nsolution based on second-order cone programming (SOCP) relaxation of optimal\npower flow (OPF) and generalized Benders decomposition. When the SOCP\nrelaxation is exact, this approach computes a globally optimal solution. We\nevaluate the performance of the proposed algorithm through simulations. The\nalgorithm requires global information and is suitable for cases where the\ndistribution network, stations, and EVs are managed centrally by the same\noperator. In Part II of the paper, we develop distributed solutions for cases\nwhere they are operated by different organizations that do not share private\ninformation. \n\n"}
{"id": "1611.10191", "contents": "Title: Is Non-Neutrality Profitable for the Stakeholders of the Internet\n  Market? Abstract: Net neutrality on the Internet is perceived as the policy that mandates\nInternet Service Providers (ISPs) to treat all data equally, regardless of the\nsource, destination, or type of transmitted data. In this work, we consider a\nscheme in which the decision makers of the market are two ISPs, one \"big\"\nContent Provider (CP), and a continuum of end-users. One of the ISPs is neutral\nand the other is non-neutral, i.e. she offers a premium quality to a CP in\nexchange for a side-payment. In addition, we assume that the CP can\ndifferentiate between ISPs by controlling the quality of the content she is\noffering on each one. In this part of the paper, we consider a scenario in\nwhich end-users are not locked in with the ISPs and can switch between ISPs\neasily. We formulate a sequential game, and show that there exists a unique\nSub-game Perfect Nash Equilibrium (SPNE) for the game, where the CP pays the\nside-payment to the non-neutral ISP and offers her content with the premium\nquality. In addition, the CP does not offer her content on the neutral ISP.\nThus, driving this ISP out of the market. \n\n"}
{"id": "1612.00150", "contents": "Title: Decentralized Consensus Optimization with Asynchrony and Delays Abstract: We propose an asynchronous, decentralized algorithm for consensus\noptimization. The algorithm runs over a network in which the agents communicate\nwith their neighbors and perform local computation. In the proposed algorithm,\neach agent can compute and communicate independently at different times, for\ndifferent durations, with the information it has even if the latest information\nfrom its neighbors is not yet available. Such an asynchronous algorithm reduces\nthe time that agents would otherwise waste idle because of communication delays\nor because their neighbors are slower. It also eliminates the need for a global\nclock for synchronization. Mathematically, the algorithm involves both primal\nand dual variables, uses fixed step-size parameters, and provably converges to\nthe exact solution under a bounded delay assumption and a random agent\nassumption. When running synchronously, the algorithm performs just as well as\nexisting competitive synchronous algorithms such as PG-EXTRA, which diverges\nwithout synchronization. Numerical experiments confirm the theoretical findings\nand illustrate the performance of the proposed algorithm. \n\n"}
{"id": "1612.03815", "contents": "Title: Gradient-Based Multiobjective Optimization with Uncertainties Abstract: In this article we develop a gradient-based algorithm for the solution of\nmultiobjective optimization problems with uncertainties. To this end, an\nadditional condition is derived for the descent direction in order to account\nfor inaccuracies in the gradients and then incorporated in a subdivison\nalgorithm for the computation of global solutions to multiobjective\noptimization problems. Convergence to a superset of the Pareto set is proved\nand an upper bound for the maximal distance to the set of substationary points\nis given. Besides the applicability to problems with uncertainties, the\nalgorithm is developed with the intention to use it in combination with model\norder reduction techniques in order to efficiently solve PDE-constrained\nmultiobjective optimization problems. \n\n"}
{"id": "1612.04082", "contents": "Title: What Lies Beneath the Surface: Topological-Shape Optimization With the\n  Kernel-Independent Fast Multipole Method Abstract: The paper presents a new method for shape and topology optimization based on\nan efficient and scalable boundary integral formulation for elasticity. To\noptimize topology, our approach uses iterative extraction of isosurfaces of a\ntopological derivative. The numerical solution of the elasticity boundary value\nproblem at every iteration is performed with the boundary element formulation\nand the kernel-independent fast multipole method. Providing excellent single\nnode performance, scalable parallelization and the best available asymptotic\ncomplexity, our method is among the fastest optimization tools available today.\nThe performance of our approach is studied on few illustrative examples,\nincluding the optimization of engineered constructions for the minimum\ncompliance and the optimization of the microstructure of a metamaterial for the\ndesired macroscopic tensor of elasticity. \n\n"}
{"id": "1612.04141", "contents": "Title: Acceleration of saddle-point methods in smooth cases Abstract: In the present paper we propose a novel convergence analysis of the\nAlternating Direction Methods of Multipliers (ADMM), based on its equivalence\nwith the overrelaxed Primal-Dual Hybrid Gradient (oPDHG) algorithm. We consider\nthe smooth case, which correspond to the cas where the objective function can\nbe decomposed into one differentiable with Lipschitz continuous gradient part\nand one strongly convex part. An accelerated variant of the ADMM is also\nproposed, which is shown to converge linearly with same rate as the oPDHG. \n\n"}
{"id": "1612.05812", "contents": "Title: Decentralized Robust Inverter-based Control in Power Systems Abstract: This paper develops a novel framework for power system stability analysis,\nthat allows for the decentralized design of inverter based controllers. The\nmethod requires that each individual inverter satisfies a standard $H^\\infty$\ndesign requirement. Critically each requirement depends only on the dynamics of\nthe components and inverters at each individual bus and the aggregate\nsusceptance of the transmission lines connected to it. The method is both\nrobust to network and delay uncertainties, as well as heterogeneous network\ncomponents, and when no network information is available it reduces to the\nstandard decentralized passivity sufficient condition for stability. We\nillustrate the novelty and strength of our approach by studying the design of\ninverter-based control laws in the presence of delays. \n\n"}
{"id": "1612.06332", "contents": "Title: On Dantzig figures from graded lexicographic orders Abstract: We construct two families of Dantzig figures, which are $(d,2d)$-polytopes\nwith an antipodal vertex pair, from convex hulls of initial subsets for the\ngraded lexicographic (grlex) and graded reverse lexicographic (grevlex) orders\non $\\mathbb{Z}^{d}_{\\geq 0}$. These two polytopes have the same number of\nvertices, $\\mathcal{O}(d^{2})$, and the same number of edges,\n$\\mathcal{O}(d^{3})$, but are not combinatorially equivalent. We provide an\nexplicit description of the vertices and the facets for both families and\ndescribe their graphs along with analyzing their basic properties such as the\nradius, diameter, existence of Hamiltonian circuits, and chromatic number.\nMoreover, we also analyze the edge expansions of these graphs. \n\n"}
{"id": "1612.07335", "contents": "Title: Distributed Dictionary Learning Abstract: The paper studies distributed Dictionary Learning (DL) problems where the\nlearning task is distributed over a multi-agent network with time-varying\n(nonsymmetric) connectivity. This formulation is relevant, for instance, in\nbig-data scenarios where massive amounts of data are collected/stored in\ndifferent spatial locations and it is unfeasible to aggregate and/or process\nall the data in a fusion center, due to resource limitations, communication\noverhead or privacy considerations. We develop a general distributed\nalgorithmic framework for the (nonconvex) DL problem and establish its\nasymptotic convergence. The new method hinges on Successive Convex\nApproximation (SCA) techniques coupled with i) a gradient tracking mechanism\ninstrumental to locally estimate the missing global information; and ii) a\nconsensus step, as a mechanism to distribute the computations among the agents.\nTo the best of our knowledge, this is the first distributed algorithm with\nprovable convergence for the DL problem and, more in general, bi-convex\noptimization problems over (time-varying) directed graphs. \n\n"}
{"id": "1612.08291", "contents": "Title: Effects of Interstitial Oxygen and Carbon on Niobium Superconducting\n  Cavities Abstract: We present results on the effects of interstitial oxygen and carbon on a\nbulk-niobium superconducting radio-frequency cavity. Previous experiments have\nshown that high-temperature (~800 $^\\circ\\text{C}$) nitrogen-doping plays the\ndominant role in the reduction of the electron mean free path in the RF\npenetration layer of niobium that leads to a decrease in microwave surface\nresistance and a suppression the temperature-dependent component of the surface\nresistance with increasing accelerating gradient. In this work, we show that\noxygen and carbon-doping has very similar effects on cavity performance,\ndemonstrating that these effects are not unique to nitrogen. The preparation\nmethod used in the introduction of interstitial oxygen and carbon has the\nadvantage that it is done at lower temperatures than that of high-temperature\nnitrogen-doping and does not require post-treatment electro-polishing. \n\n"}
{"id": "1701.00038", "contents": "Title: Sparsity enabled cluster reduced-order models for control Abstract: Characterizing and controlling nonlinear, multi-scale phenomena play\nimportant roles in science and engineering. Cluster-based reduced-order\nmodeling (CROM) was introduced to exploit the underlying low-dimensional\ndynamics of complex systems. CROM builds a data-driven discretization of the\nPerron-Frobenius operator, resulting in a probabilistic model for ensembles of\ntrajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a\nlinear framework, and uncertainty can be managed with data assimilation. CROM\nis typically computed on high-dimensional data, however, access to and\ncomputations on this full-state data limit the online implementation of CROM\nfor prediction and control. Here, we address this key challenge by identifying\na small subset of critical measurements to learn an efficient CROM, referred to\nas sparsity-enabled CROM. In particular, we leverage compressive measurements\nto faithfully embed the cluster geometry and preserve the probabilistic\ndynamics. Further, we show how to identify fewer optimized sensor locations\ntailored to a specific problem that outperform random measurements. Both of\nthese sparsity-enabled sensing strategies significantly reduce the burden of\ndata acquisition and processing for low-latency in-time estimation and control.\nWe illustrate this unsupervised learning approach on three different\nhigh-dimensional nonlinear dynamical systems from fluids with increasing\ncomplexity, with one application in flow control. Sparsity-enabled CROM is a\ncritical facilitator for real-time implementation on high-dimensional systems\nwhere full-state information may be inaccessible. \n\n"}
{"id": "1701.00196", "contents": "Title: Robust Mean Field Linear-Quadratic-Gaussian Games with Unknown\n  $L^2$-Disturbance Abstract: This paper considers a class of mean field linear-quadratic-Gaussian (LQG)\ngames with model uncertainty. The drift term in the dynamics of the agents\ncontains a common unknown function. We take a robust optimization approach\nwhere a representative agent in the limiting model views the drift uncertainty\nas an adversarial player. By including the mean field dynamics in an augmented\nstate space, we solve two optimal control problems sequentially, which combined\nwith consistent mean field approximations provides a solution to the robust\ngame. A set of decentralized control strategies is derived by use of\nforward-backward stochastic differential equations (FBSDE) and shown to be a\nrobust epsilon-Nash equilibrium. \n\n"}
{"id": "1701.01010", "contents": "Title: Divergence and Sufficiency for Convex Optimization Abstract: Logarithmic score and information divergence appear in information theory,\nstatistics, statistical mechanics, and portfolio theory. We demonstrate that\nall these topics involve some kind of optimization that leads directly to\nregret functions and such regret functions are often given by a Bregman\ndivergence. If the regret function also fulfills a sufficiency condition it\nmust be proportional to information divergence. We will demonstrate that\nsufficiency is equivalent to the apparently weaker notion of locality and it is\nalso equivalent to the apparently stronger notion of monotonicity. These\nsufficiency conditions have quite different relevance in the different areas of\napplication, and often they are not fulfilled. Therefore sufficiency conditions\ncan be used to explain when results from one area can be transferred directly\nto another and when one will experience differences. \n\n"}
{"id": "1701.01722", "contents": "Title: Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU Abstract: The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem. \n\n"}
{"id": "1701.04939", "contents": "Title: Ensemble of Thermostatically Controlled Loads: Statistical Physics\n  Approach Abstract: Thermostatically Controlled Loads (TCL), e.g. air-conditioners and heaters,\nare by far the most wide-spread consumers of electricity. Normally the devices\nare calibrated to provide the so-called bang-bang control of temperature --\nchanging from on to off, and vice versa, depending on temperature. Aggregation\nof a large group of similar devices into a statistical ensemble is considered,\nwhere the devices operate following the same dynamics subject to stochastic\nperturbations and randomized, Poisson on/off switching policy. We analyze,\nusing theoretical and computational tools of statistical physics, how the\nensemble relaxes to a stationary distribution and establish relation between\nthe relaxation and statistics of the probability flux, associated with devices'\ncycling in the mixed (discrete, switch on/off, and continuous, temperature)\nphase space. This allowed us to derive and analyze spectrum of the\nnon-equilibrium (detailed balance broken) statistical system and uncover how\nswitching policy affects oscillatory trend and speed of the relaxation.\nRelaxation of the ensemble is of a practical interest because it describes how\nthe ensemble recovers from significant perturbations, e.g. forceful temporary\nswitching off aimed at utilizing flexibility of the ensemble in providing\n\"demand response\" services relieving consumption temporarily to balance larger\npower grid. We discuss how the statistical analysis can guide further\ndevelopment of the emerging demand response technology. \n\n"}
{"id": "1701.05246", "contents": "Title: Second order dynamical systems with penalty terms associated to monotone\n  inclusions Abstract: In this paper we investigate in a Hilbert space setting a second order\ndynamical system of the form $$\\ddot{x}(t)+\\g(t)\\dot{x}(t)+x(t)-J_{\\lambda(t)\nA}\\big(x(t)-\\lambda(t) D(x(t))-\\lambda(t)\\beta(t)B(x(t))\\big)=0,$$ where\n$A:{\\mathcal H}\\toto{\\mathcal H}$ is a maximal monotone operator,\n$J_{\\lambda(t) A}:{\\mathcal H}\\To{\\mathcal H}$ is the resolvent operator of\n$\\lambda(t)A$ and $D,B: {\\mathcal H}\\rightarrow{\\mathcal H}$ are cocoercive\noperators, and $\\lambda,\\beta :[0,+\\infty)\\rightarrow (0,+\\infty)$, and\n$\\gamma:[0,+\\infty)\\rightarrow (0,+\\infty)$ are step size, penalization and,\nrespectively, damping functions, all depending on time. We show the existence\nand uniqueness of strong global solutions in the framework of the\nCauchy-Lipschitz-Picard Theorem and prove ergodic asymptotic convergence for\nthe generated trajectories to a zero of the operator $A+D+{N}_C,$ where $C=\\zer\nB$ and $N_C$ denotes the normal cone operator of $C$. To this end we use\nLyapunov analysis combined with the celebrated Opial Lemma in its ergodic\ncontinuous version. Furthermore, we show strong convergence for trajectories to\nthe unique zero of $A+D+{N}_C$, provided that $A$ is a strongly monotone\noperator. \n\n"}
{"id": "1701.05566", "contents": "Title: Corral Framework: Trustworthy and Fully Functional Data Intensive\n  Parallel Astronomical Pipelines Abstract: Data processing pipelines represent an important slice of the astronomical\nsoftware library that include chains of processes that transform raw data into\nvaluable information via data reduction and analysis. In this work we present\nCorral, a Python framework for astronomical pipeline generation. Corral\nfeatures a Model-View-Controller design pattern on top of an SQL Relational\nDatabase capable of handling: custom data models; processing stages; and\ncommunication alerts, and also provides automatic quality and structural\nmetrics based on unit testing. The Model-View-Controller provides concept\nseparation between the user logic and the data models, delivering at the same\ntime multi-processing and distributed computing capabilities. Corral represents\nan improvement over commonly found data processing pipelines in Astronomy since\nthe design pattern eases the programmer from dealing with processing flow and\nparallelization issues, allowing them to focus on the specific algorithms\nneeded for the successive data transformations and at the same time provides a\nbroad measure of quality over the created pipeline. Corral and working examples\nof pipelines that use it are available to the community at\nhttps://github.com/toros-astro. \n\n"}
{"id": "1701.07475", "contents": "Title: Projected Primal-Dual Gradient Flow of Augmented Lagrangian with\n  Application to Distributed Maximization of the Algebraic Connectivity of a\n  Network Abstract: In this paper, a projected primal-dual gradient flow of augmented Lagrangian\nis presented to solve convex optimization problems that are not necessarily\nstrictly convex. The optimization variables are restricted by a convex set with\ncomputable projection operation on its tangent cone as well as equality\nconstraints. As a supplement of the analysis in\n\\cite{niederlander2016distributed}, we show that the projected dynamical system\nconverges to one of the saddle points and hence finding an optimal solution.\nMoreover, the problem of distributedly maximizing the algebraic connectivity of\nan undirected network by optimizing the port gains of each nodes (base\nstations) is considered. The original semi-definite programming (SDP) problem\nis relaxed into a nonlinear programming (NP) problem that will be solved by the\naforementioned projected dynamical system. Numerical examples show the\nconvergence of the aforementioned algorithm to one of the optimal solutions.\nThe effect of the relaxation is illustrated empirically with numerical\nexamples. A methodology is presented so that the number of iterations needed to\nreach the equilibrium is suppressed. Complexity per iteration of the algorithm\nis illustrated with numerical examples. \n\n"}
{"id": "1701.07941", "contents": "Title: Computationally Efficient Market Simulation Tool for Future Grid\n  Scenario Analysis Abstract: The paper proposes a computationally efficient electricity market simulation\ntool (MST) suitable for future grid scenario analysis. The market model is\nbased on a unit commitment (UC) problem and takes into account the uptake of\nemerging technologies, like demand response, battery storage, concentrated\nsolar thermal generation, and HVDC transmission lines. To allow for a\nsubsequent stability assessment, the MST requires an explicit representation of\nthe number of online generation units, which affects powers system inertia and\nreactive power support capability. These requirements render a fullfledged UC\nmodel computationally intractable, so we propose unit clustering, rolling\nhorizon approach, and constraint reduction to increase the computational\nefficiency. To showcase the capability of the proposed tool, we use a\nsimplified model of the Australian National Electricity Market with different\npenetrations of renewable generation. The results show that the number of\nonline units resulting from the proposed tool is very close to the binary UC\nrun over a week-long horizon, which is confirmed by the loadability and inertia\nanalysis. That confirms the validity of the approach for long term future grid\nstudies, where one is more interested in finding weak points in the system\nrather than in a detailed analysis of individual operating conditions. \n\n"}
{"id": "1701.08810", "contents": "Title: Reinforcement Learning Algorithm Selection Abstract: This paper formalises the problem of online algorithm selection in the\ncontext of Reinforcement Learning. The setup is as follows: given an episodic\ntask and a finite number of off-policy RL algorithms, a meta-algorithm has to\ndecide which RL algorithm is in control during the next episode so as to\nmaximize the expected return. The article presents a novel meta-algorithm,\ncalled Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is\nto freeze the policy updates at each epoch, and to leave a rebooted stochastic\nbandit in charge of the algorithm selection. Under some assumptions, a thorough\ntheoretical analysis demonstrates its near-optimality considering the\nstructural sampling budget limitations. ESBAS is first empirically evaluated on\na dialogue task where it is shown to outperform each individual algorithm in\nmost configurations. ESBAS is then adapted to a true online setting where\nalgorithms update their policies after each transition, which we call SSBAS.\nSSBAS is evaluated on a fruit collection task where it is shown to adapt the\nstepsize parameter more efficiently than the classical hyperbolic decay, and on\nan Atari game, where it improves the performance by a wide margin. \n\n"}
{"id": "1702.00748", "contents": "Title: QCD-Aware Recursive Neural Networks for Jet Physics Abstract: Recent progress in applying machine learning for jet physics has been built\nupon an analogy between calorimeters and images. In this work, we present a\nnovel class of recursive neural networks built instead upon an analogy between\nQCD and natural languages. In the analogy, four-momenta are like words and the\nclustering history of sequential recombination jet algorithms is like the\nparsing of a sentence. Our approach works directly with the four-momenta of a\nvariable-length set of particles, and the jet-based tree structure varies on an\nevent-by-event basis. Our experiments highlight the flexibility of our method\nfor building task-specific jet embeddings and show that recursive architectures\nare significantly more accurate and data efficient than previous image-based\nnetworks. We extend the analogy from individual jets (sentences) to full events\n(paragraphs), and show for the first time an event-level classifier operating\non all the stable particles produced in an LHC event. \n\n"}
{"id": "1702.00867", "contents": "Title: Error Assessment of Computational Models in Chemistry Abstract: Computational models in chemistry rely on a number of approximations. The\neffect of such approximations on observables derived from them is often\nunpredictable. Therefore, it is challenging to quantify the uncertainty of a\ncomputational result, which, however, is necessary to assess the suitability of\na computational model. Common performance statistics such as the mean absolute\nerror are prone to failure as they do not distinguish the explainable\n(systematic) part of the errors from their unexplainable (random) part. In this\npaper, we discuss problems and solutions for performance assessment of\ncomputational models based on several examples from the quantum chemistry\nliterature. For this purpose, we elucidate the different sources of\nuncertainty, the elimination of systematic errors, and the combination of\nindividual uncertainty components to the uncertainty of a prediction. \n\n"}
{"id": "1702.04939", "contents": "Title: A Bayesian framework for distributed estimation of arrival rates in\n  asynchronous networks Abstract: In this paper we consider a network of agents monitoring a spatially\ndistributed arrival process. Each node measures the number of arrivals seen at\nits monitoring point in a given time-interval with the objective of estimating\nthe unknown local arrival rate. We propose an asynchronous distributed approach\nbased on a Bayesian model with unknown hyperparameter, where each node computes\nthe minimum mean square error (MMSE) estimator of its local arrival rate in a\ndistributed way. As a result, the estimation at each node \"optimally\" fuses the\ninformation from the whole network through a distributed optimization\nalgorithm. Moreover, we propose an ad-hoc distributed estimator, based on a\nconsensus algorithm for time-varying and directed graphs, which exhibits\nreduced complexity and exponential convergence. We analyze the performance of\nthe proposed distributed estimators, showing that they: (i) are reliable even\nin presence of limited local data, and (ii) improve the estimation accuracy\ncompared to the purely decentralized setup. Finally, we provide a statistical\ncharacterization of the proposed estimators. In particular, for the ad-hoc\nestimator, we show that as the number of nodes goes to infinity its mean square\nerror converges to the optimal one. Numerical Monte Carlo simulations confirm\nthe theoretical characterization and highlight the appealing performances of\nthe estimators. \n\n"}
{"id": "1702.05423", "contents": "Title: Accelerated Primal-Dual Proximal Block Coordinate Updating Methods for\n  Constrained Convex Optimization Abstract: Block Coordinate Update (BCU) methods enjoy low per-update computational\ncomplexity because every time only one or a few block variables would need to\nbe updated among possibly a large number of blocks. They are also easily\nparallelized and thus have been particularly popular for solving problems\ninvolving large-scale dataset and/or variables. In this paper, we propose a\nprimal-dual BCU method for solving linearly constrained convex program in\nmulti-block variables. The method is an accelerated version of a primal-dual\nalgorithm proposed by the authors, which applies randomization in selecting\nblock variables to update and establishes an $O(1/t)$ convergence rate under\nweak convexity assumption. We show that the rate can be accelerated to\n$O(1/t^2)$ if the objective is strongly convex. In addition, if one block\nvariable is independent of the others in the objective, we then show that the\nalgorithm can be modified to achieve a linear rate of convergence. The\nnumerical experiments show that the accelerated method performs stably with a\nsingle set of parameters while the original method needs to tune the parameters\nfor different datasets in order to achieve a comparable level of performance. \n\n"}
{"id": "1702.05732", "contents": "Title: Low-dose cryo electron ptychography via non-convex Bayesian optimization Abstract: Electron ptychography has seen a recent surge of interest for phase sensitive\nimaging at atomic or near-atomic resolution. However, applications are so far\nmainly limited to radiation-hard samples because the required doses are too\nhigh for imaging biological samples at high resolution. We propose the use of\nnon-convex, Bayesian optimization to overcome this problem and reduce the dose\nrequired for successful reconstruction by two orders of magnitude compared to\nprevious experiments. We suggest to use this method for imaging single\nbiological macromolecules at cryogenic temperatures and demonstrate 2D\nsingle-particle reconstructions from simulated data with a resolution of 7.9\n\\AA$\\,$ at a dose of 20 $e^- / \\AA^2$. When averaging over only 15 low-dose\ndatasets, a resolution of 4 \\AA$\\,$ is possible for large macromolecular\ncomplexes. With its independence from microscope transfer function, direct\nrecovery of phase contrast and better scaling of signal-to-noise ratio,\ncryo-electron ptychography may become a promising alternative to Zernike\nphase-contrast microscopy. \n\n"}
{"id": "1702.05947", "contents": "Title: Cutting Planes for Families Implying Frankl's Conjecture Abstract: We find previously unknown families of sets which ensure Frankl's conjecture\nholds for all families that contain them using an algorithmic framework. The\nconjecture states that for any nonempty union-closed (UC) family there exists\nan element of the ground set in at least half the sets of the considered UC\nfamily. Poonen's Theorem characterizes the existence of weights which determine\nwhether a given UC family implies the conjecture for all UC families which\ncontain it. We design a cutting-plane method that computes the explicit weights\nwhich imply the existence conditions of Poonen's Theorem. This method enables\nus to answer several open questions regarding structural properties of UC\nfamilies, including the construction of a counterexample to a conjecture of\nMorris from 2006. \n\n"}
{"id": "1702.06175", "contents": "Title: Structured signal recovery from quadratic measurements: Breaking sample\n  complexity barriers via nonconvex optimization Abstract: This paper concerns the problem of recovering an unknown but structured\nsignal $x \\in R^n$ from $m$ quadratic measurements of the form\n$y_r=|<a_r,x>|^2$ for $r=1,2,...,m$. We focus on the under-determined setting\nwhere the number of measurements is significantly smaller than the dimension of\nthe signal ($m<<n$). We formulate the recovery problem as a nonconvex\noptimization problem where prior structural information about the signal is\nenforced through constrains on the optimization variables. We prove that\nprojected gradient descent, when initialized in a neighborhood of the desired\nsignal, converges to the unknown signal at a linear rate. These results hold\nfor any constraint set (convex or nonconvex) providing convergence guarantees\nto the global optimum even when the objective function and constraint set is\nnonconvex. Furthermore, these results hold with a number of measurements that\nis only a constant factor away from the minimal number of measurements required\nto uniquely identify the unknown signal. Our results provide the first provably\ntractable algorithm for this data-poor regime, breaking local sample complexity\nbarriers that have emerged in recent literature. In a companion paper we\ndemonstrate favorable properties for the optimization problem that may enable\nsimilar results to continue to hold more globally (over the entire ambient\nspace). Collectively these two papers utilize and develop powerful tools for\nuniform convergence of empirical processes that may have broader implications\nfor rigorous understanding of constrained nonconvex optimization heuristics.\nThe mathematical results in this paper also pave the way for a new generation\nof data-driven phase-less imaging systems that can utilize prior information to\nsignificantly reduce acquisition time and enhance image reconstruction,\nenabling nano-scale imaging at unprecedented speeds and resolutions. \n\n"}
{"id": "1702.06917", "contents": "Title: Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe Abstract: We consider the problem of bandit optimization, inspired by stochastic\noptimization and online learning problems with bandit feedback. In this\nproblem, the objective is to minimize a global loss function of all the\nactions, not necessarily a cumulative loss. This framework allows us to study a\nvery general class of problems, with applications in statistics, machine\nlearning, and other fields. To solve this problem, we analyze the\nUpper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and\nconvex optimization. We give theoretical guarantees for the performance of this\nalgorithm over various classes of functions, and discuss the optimality of\nthese results. \n\n"}
{"id": "1702.06971", "contents": "Title: Online Ranking with Constraints: A Primal-Dual Algorithm and\n  Applications to Web Traffic-Shaping Abstract: We study the online constrained ranking problem motivated by an application\nto web-traffic shaping: an online stream of sessions arrive in which, within\neach session, we are asked to rank items. The challenge involves optimizing the\nranking in each session so that local vs. global objectives are controlled:\nwithin each session one wishes to maximize a reward (local) while satisfying\ncertain constraints over the entire set of sessions (global). A typical\napplication of this setup is that of page optimization in a web portal. We wish\nto rank items so that not only is user engagement maximized in each session,\nbut also other business constraints (such as the number of views/clicks\ndelivered to various publishing partners) are satisfied.\n  We describe an online algorithm for performing this optimization. A novel\nelement of our approach is the use of linear programming duality and\nconnections to the celebrated Hungarian algorithm. This framework enables us to\ndetermine a set of \\emph{shadow prices} for each traffic-shaping constraint\nthat can then be used directly in the final ranking function to assign\nnear-optimal rankings. The (dual) linear program can be solved off-line\nperiodically to determine the prices. At serving time these prices are used as\nweights to compute weighted rank-scores for the items, and the simplicity of\nthe approach facilitates scalability to web applications. We provide rigorous\ntheoretical guarantees for the performance of our online algorithm and validate\nour approach using numerical experiments on real web-traffic data from a\nprominent internet portal. \n\n"}
{"id": "1702.07173", "contents": "Title: Discretisation and Duality of Optimal Skorokhod Embedding Problems Abstract: We prove a strong duality result for a linear programming problem which has\nthe interpretation of being a discretised optimal Skorokhod embedding problem,\nand we recover this continuous time problem as a limit of the discrete\nproblems. With the discrete setup we show that for a suitably chosen objective\nfunction, the optimiser takes the form of a hitting time for a random walk. In\nthe limiting problem we then reprove the existence of the Root, Rost, and cave\nembedding solutions of the Skorokhod embedding problem.\n  The main strength of this approach is that we can derive properties of the\ndiscrete problem more easily than in continuous time, and then prove that these\nproperties hold in the limit. For example, the strong duality result gives dual\noptimisers, and our limiting arguments can be used to derive properties of the\ncontinuous time dual functions, known to represent a superhedging portfolio. \n\n"}
{"id": "1702.07781", "contents": "Title: On Optimal Portfolios of Dynamic Resource Allocations Abstract: We consider the optimal allocation of generic resources among multiple\ngeneric entities of interest over a finite planning horizon, where each entity\ngenerates stochastic returns as a function of its resource allocation during\neach period. The main objective is to maximize the expected return while at the\nsame time managing risk to an acceptable level for each period. We devise a\ngeneral solution framework and establish how to obtain the optimal dynamic\nresource allocation. \n\n"}
{"id": "1702.08059", "contents": "Title: Observability of a 1D Schr\\\"odinger equation with time-varying\n  boundaries Abstract: We discuss the observability of a one-dimensional Schr\\\"odinger equation on\ncertain time dependent domain. In linear moving case, we give the exact\nboundary and pointwise internal observability for arbitrary time. For the\ngeneral moving, we provide exact boundary observability when the curve\nsatisfies some certain conditions . By duality theory, we establish the\ncontrollability of adjoint system. \n\n"}
{"id": "1703.00272", "contents": "Title: Incremental constraint projection methods for monotone stochastic\n  variational inequalities Abstract: We consider stochastic variational inequalities with monotone operators\ndefined as the expected value of a random operator. We assume the feasible set\nis the intersection of a large family of convex sets. We propose a method that\ncombines stochastic approximation with incremental constraint projections\nmeaning that at each iteration, a step similar to some variant of a\ndeterministic projection method is taken after the random operator is sampled\nand a component of the intersection defining the feasible set is chosen at\nrandom. Such sequential scheme is well suited for applications involving large\ndata sets, online optimization and distributed learning. First, we assume that\nthe variational inequality is weak-sharp. We provide asymptotic convergence,\nfeasibility rate of $O(1/k)$ in terms of the mean squared distance to the\nfeasible set and solvability rate of $O(1/\\sqrt{k})$ (up to first order\nlogarithmic terms) in terms of the mean distance to the solution set for a\nbounded or unbounded feasible set. Then, we assume just monotonicity of the\noperator and introduce an explicit iterative Tykhonov regularization to the\nmethod. We consider Cartesian variational inequalities so as to encompass the\ndistributed solution of stochastic Nash games or multi-agent optimization\nproblems under a limited coordination. We provide asymptotic convergence,\nfeasibility rate of $O(1/k)$ in terms of the mean squared distance to the\nfeasible set and, in the case of a compact set, we provide a near-optimal\nsolvability convergence rate of $O\\left(\\frac{k^\\delta\\ln k}{\\sqrt{k}}\\right)$\nin terms of the mean dual gap-function of the SVI for arbitrarily small\n$\\delta>0$. \n\n"}
{"id": "1703.00405", "contents": "Title: Stability and performance analysis of linear positive systems with\n  delays using input-output methods Abstract: It is known that input-output approaches based on scaled small-gain theorems\nwith constant $D$-scalings and integral linear constraints are non-conservative\nfor the analysis of some classes of linear positive systems interconnected with\nuncertain linear operators. This dramatically contrasts with the case of\ngeneral linear systems with delays where input-output approaches provide, in\ngeneral, sufficient conditions only. Using these results we provide simple\nalternative proofs for many of the existing results on the stability of linear\npositive systems with discrete/distributed/neutral time-invariant/-varying\ndelays and linear difference equations. In particular, we give a simple proof\nfor the characterization of diagonal Riccati stability for systems with\ndiscrete-delays and generalize this equation to other types of delay systems.\nThe fact that all those results can be reproved in a very simple way\ndemonstrates the importance and the efficiency of the input-output framework\nfor the analysis of linear positive systems. The approach is also used to\nderive performance results evaluated in terms of the $L_1$-, $L_2$- and\n$L_\\infty$-gains. It is also flexible enough to be used for design purposes. \n\n"}
{"id": "1703.01685", "contents": "Title: Reliable estimation of prediction uncertainty for physico-chemical\n  property models Abstract: The predictions of parameteric property models and their uncertainties are\nsensitive to systematic errors such as inconsistent reference data, parametric\nmodel assumptions, or inadequate computational methods. Here, we discuss the\ncalibration of property models in the light of bootstrapping, a sampling method\nakin to Bayesian inference that can be employed for identifying systematic\nerrors and for reliable estimation of the prediction uncertainty. We apply\nbootstrapping to assess a linear property model linking the 57Fe Moessbauer\nisomer shift to the contact electron density at the iron nucleus for a diverse\nset of 44 molecular iron compounds. The contact electron density is calculated\nwith twelve density functionals across Jacob's ladder (PWLDA, BP86, BLYP, PW91,\nPBE, M06-L, TPSS, B3LYP, B3PW91, PBE0, M06, TPSSh). We provide systematic-error\ndiagnostics and reliable, locally resolved uncertainties for isomer-shift\npredictions. Pure and hybrid density functionals yield average prediction\nuncertainties of 0.06-0.08 mm/s and 0.04-0.05 mm/s, respectively, the latter\nbeing close to the average experimental uncertainty of 0.02 mm/s. Furthermore,\nwe show that both model parameters and prediction uncertainty depend\nsignificantly on the composition and number of reference data points.\nAccordingly, we suggest that rankings of density functionals based on\nperformance measures (e.g., the coefficient of correlation, r2, or the\nroot-mean-square error, RMSE) should not be inferred from a single data set.\nThis study presents the first statistically rigorous calibration analysis for\ntheoretical Moessbauer spectroscopy, which is of general applicability for\nphysico-chemical property models and not restricted to isomer-shift\npredictions. We provide the statistically meaningful reference data set MIS39\nand a new calibration of the isomer shift based on the PBE0 functional. \n\n"}
{"id": "1703.01942", "contents": "Title: On Time-Consistent Solution to Time-Inconsistent Linear-Quadratic\n  Optimal Control of Discrete-Time Stochastic Systems Abstract: In this paper, we investigate a class of time-inconsistent discrete-time\nstochastic linear-quadratic optimal control problems, whose time-consistent\nsolutions consist of an open-loop equilibrium control and a linear feedback\nequilibrium strategy. The open-loop equilibrium control is defined for a given\ninitial pair, while the linear feedback equilibrium strategy is defined for all\nthe initial pairs. Maximum-principle-type necessary and sufficient conditions\ncontaining stationary and convexity are derived for the existence of these two\ntime-consistent solutions, respectively. Furthermore, for the case where the\nsystem matrices are independent of the initial time, we show that the existence\nof the open-loop equilibrium control for a given initial pair is equivalent to\nthe solvability of a set of nonsymmetric generalized difference Riccati\nequations and a set of linear difference equations. Moreover, the existence of\nlinear feedback equilibrium strategy is equivalent to the solvability of\nanother set of symmetric generalized difference Riccati equations. \n\n"}
{"id": "1703.03669", "contents": "Title: Special cases of pairwise comparisons matrices represented by Toeplitz\n  matrices Abstract: This study presents special cases of inconsistent pairwise comparisons PC\nmatrices and analysis of their eigenvalue-based inconsistency index using\nmathematical methods. All studied special cases of PC matrices are Toeplitz\nmatrices with only three different entries $1$, $x$, and $1/x$. A new type of\ncirculant pairwise comparisons matrix has been introduced. Although this class\nof PC matrices may be perceived as restricted, it is general enough to cover\nnumerous levels of eigenvalue-based inconsistency index from the lowest to the\nhighest. Both exact mathematical expressions and estimations, where the exact\nexpression was impossible to find, are provided \n\n"}
{"id": "1703.06807", "contents": "Title: Guaranteed Sufficient Decrease for Variance Reduced Stochastic Gradient\n  Descent Abstract: In this paper, we propose a novel sufficient decrease technique for variance\nreduced stochastic gradient descent methods such as SAG, SVRG and SAGA. In\norder to make sufficient decrease for stochastic optimization, we design a new\nsufficient decrease criterion, which yields sufficient decrease versions of\nvariance reduction algorithms such as SVRG-SD and SAGA-SD as a byproduct. We\nintroduce a coefficient to scale current iterate and satisfy the sufficient\ndecrease property, which takes the decisions to shrink, expand or move in the\nopposite direction, and then give two specific update rules of the coefficient\nfor Lasso and ridge regression. Moreover, we analyze the convergence properties\nof our algorithms for strongly convex problems, which show that both of our\nalgorithms attain linear convergence rates. We also provide the convergence\nguarantees of our algorithms for non-strongly convex problems. Our experimental\nresults further verify that our algorithms achieve significantly better\nperformance than their counterparts. \n\n"}
{"id": "1703.07409", "contents": "Title: Inference, Prediction, and Control of Networked Epidemics Abstract: We develop a feedback control method for networked epidemic spreading\nprocesses. In contrast to most prior works which consider mean field, open-loop\ncontrol schemes, the present work develops a novel framework for feedback\ncontrol of epidemic processes which leverages incomplete observations of the\nstochastic epidemic process in order to control the exact dynamics of the\nepidemic outbreak. We develop an observation model for the epidemic process,\nand demonstrate that if the set of observed nodes is sufficiently well\nstructured, then the random variables which denote the process' infections are\nconditionally independent given the observations. We then leverage the attained\nconditional independence property to construct tractable mechanisms for the\ninference and prediction of the process state, avoiding the need to use mean\nfield approximations or combinatorial representations. We conclude by\nformulating a one-step lookahead controller for the discrete-time\nSusceptible-Infected-Susceptible (SIS) epidemic process which leverages the\ndeveloped Bayesian inference and prediction mechanisms, and causes the epidemic\nto die out at a chosen rate. \n\n"}
{"id": "1703.07704", "contents": "Title: Formal Methods for Adaptive Control of Dynamical Systems Abstract: We develop a method to control discrete-time systems with constant but\ninitially unknown parameters from linear temporal logic (LTL) specifications.\nWe introduce the notions of (non-deterministic) parametric and adaptive\ntransition systems and show how to use tools from formal methods to compute\nadaptive control strategies for finite systems. For infinite systems, we first\ncompute abstractions in the form of parametric finite quotient transition\nsystems and then apply the techniques for finite systems. Unlike traditional\nadaptive control methods, our approach is correct by design, does not require a\nreference model, and can deal with a much wider range of systems and\nspecifications. Illustrative case studies are included. \n\n"}
{"id": "1703.07761", "contents": "Title: An Active-Set Algorithmic Framework for Non-Convex Optimization Problems\n  over the Simplex Abstract: In this paper, we describe a new active-set algorithmic framework for\nminimizing a non-convex function over the unit simplex. At each iteration, the\nmethod makes use of a rule for identifying active variables (i.e., variables\nthat are zero at a stationary point) and specific directions (that we name\nactive-set gradient related directions) satisfying a new \"nonorthogonality\"\ntype of condition. We prove global convergence to stationary points when using\nan Armijo line search in the given framework. We further describe three\ndifferent examples of active-set gradient related directions that guarantee\nlinear convergence rate (under suitable assumptions). Finally, we report\nnumerical experiments showing the effectiveness of the approach. \n\n"}
{"id": "1703.10797", "contents": "Title: Tunneling estimates and approximate controllability for hypoelliptic\n  equations Abstract: This article is concerned with quantitative unique continuation estimates for\nequations involving a \"sum of squares\" operator $\\mathcal{L}$ on a compact\nmanifold $\\mathcal{M}$ assuming: $(i)$ the Chow-Rashevski-H\\\"ormander condition\nensuring the hypoellipticity of $\\mathcal{L}$, and $(ii)$ the analyticity of\n$\\mathcal{M}$ and the coefficients of $\\mathcal{L}$.\n  The first result is the tunneling estimate $\\|\\varphi\\|_{L^2(\\omega)} \\geq\nCe^{- \\lambda^{\\frac{k}{2}}}$ for normalized eigenfunctions $\\varphi$ of\n$\\mathcal{L}$ from a nonempty open set $\\omega\\subset \\mathcal{M}$, where $k$\nis the hypoellipticity index of $\\mathcal{L}$ and $\\lambda$ the eigenvalue.\n  The main result is a stability estimate for solutions to the hypoelliptic\nwave equation $(\\partial_t^2+\\mathcal{L})u=0$: for $T>2 \\sup_{x \\in\n\\mathcal{M}}(dist(x,\\omega))$ (here, $dist$ is the sub-Riemannian distance),\nthe observation of the solution on $(0,T)\\times \\omega$ determines the data.\nThe constant involved in the estimate is $Ce^{c\\Lambda^k}$ where $\\Lambda$ is\nthe typical frequency of the data.\n  We then prove the approximate controllability of the hypoelliptic heat\nequation $(\\partial_t+\\mathcal{L})v=1_\\omega f$ in any time, with appropriate\n(exponential) cost, depending on $k$. In case $k=2$ (Grushin, Heisenberg...),\nwe further show approximate controllability to trajectories with polynomial\ncost in large time.\n  We also explain how the analyticity assumption can be relaxed, and a boundary\n$\\partial \\mathcal{M}$ can be added in some situations.\n  Most results turn out to be optimal on a family of Grushin-type operators.\n  The main proof relies on the general strategy developed by the authors in\narxiv:1506.04254. \n\n"}
{"id": "1704.00196", "contents": "Title: Faster Subgradient Methods for Functions with H\\\"olderian Growth Abstract: The purpose of this manuscript is to derive new convergence results for\nseveral subgradient methods applied to minimizing nonsmooth convex functions\nwith H\\\"olderian growth. The growth condition is satisfied in many applications\nand includes functions with quadratic growth and weakly sharp minima as special\ncases. To this end there are three main contributions. First, for a constant\nand sufficiently small stepsize, we show that the subgradient method achieves\nlinear convergence up to a certain region including the optimal set, with error\nof the order of the stepsize. Second, if appropriate problem parameters are\nknown, we derive a decaying stepsize which obtains a much faster convergence\nrate than is suggested by the classical $O(1/\\sqrt{k})$ result for the\nsubgradient method. Thirdly we develop a novel \"descending stairs\" stepsize\nwhich obtains this faster convergence rate and also obtains linear convergence\nfor the special case of weakly sharp functions. We also develop an adaptive\nvariant of the \"descending stairs\" stepsize which achieves the same convergence\nrate without requiring an error bound constant which is difficult to estimate\nin practice. \n\n"}
{"id": "1704.00378", "contents": "Title: Learning in anonymous nonatomic games with applications to first-order\n  mean field games Abstract: We introduce a model of anonymous games with the player dependent action\nsets. We propose several learning procedures based on the well-known Fictitious\nPlay and Online Mirror Descent and prove their convergence to equilibrium under\nthe classical monotonicity condition. Typical examples are first-order mean\nfield games. \n\n"}
{"id": "1704.00683", "contents": "Title: Big Holes in Big Data: A Monte Carlo Algorithm for Detecting Large\n  Hyper-rectangles in High Dimensional Data Abstract: We present the first algorithm for finding holes in high dimensional data\nthat runs in polynomial time with respect to the number of dimensions. Previous\nalgorithms are exponential. Finding large empty rectangles or boxes in a set of\npoints in 2D and 3D space has been well studied. Efficient algorithms exist to\nidentify the empty regions in these low-dimensional spaces. Unfortunately such\nefficiency is lacking in higher dimensions where the problem has been shown to\nbe NP-complete when the dimensions are included in the input. Applications for\nalgorithms that find large empty spaces include big data analysis, recommender\nsystems, automated knowledge discovery, and query optimization. Our Monte\nCarlo-based algorithm discovers interesting maximal empty hyper-rectangles in\ncases where dimensionality and input size would otherwise make analysis\nimpractical. The run-time is polynomial in the size of the input and the number\nof dimensions. We apply the algorithm on a 39-dimensional data set for protein\nstructures and discover interesting properties that we think could not be\ninferred otherwise. \n\n"}
{"id": "1704.01524", "contents": "Title: Core of communities in bipartite networks Abstract: We use the information present in a bipartite network to detect cores of\ncommunities of each set of the bipartite system. Cores of communities are found\nby investigating statistically validated projected networks obtained using\ninformation present in the bipartite network. Cores of communities are highly\ninformative and robust with respect to the presence of errors or missing\nentries in the bipartite network. We assess the statistical robustness of cores\nby investigating an artificial benchmark network, the co-authorship network,\nand the actor-movie network. The accuracy and precision of the partition\nobtained with respect to the reference partition are measured in terms of the\nadjusted Rand index and of the adjusted Wallace index respectively. The\ndetection of cores is highly precise although the accuracy of the methodology\ncan be limited in some cases. \n\n"}
{"id": "1704.02689", "contents": "Title: Zero-sum stochastic differential game with risk-sensitive cost Abstract: Zero sum games with risk-sensitive cost criterion are considered with\nunderlying dynamics being given by controlled stochastic differential\nequations. Under the assumption of geometric stability on the dynamics , we\ncompletely characterize all possible saddle point strategies in the class of\nstationary Markov controls. In addition, we also establish existence-uniqueness\nresult for the value function of the Hamilton-Jacobi-Isaacs equation. \n\n"}
{"id": "1704.03184", "contents": "Title: Computing nearest stable matrix pairs Abstract: In this paper, we study the nearest stable matrix pair problem: given a\nsquare matrix pair $(E,A)$, minimize the Frobenius norm of\n$(\\Delta_E,\\Delta_A)$ such that $(E+\\Delta_E,A+\\Delta_A)$ is a stable matrix\npair. We propose a reformulation of the problem with a simpler feasible set by\nintroducing dissipative Hamiltonian (DH) matrix pairs: A matrix pair $(E,A)$ is\nDH if $A=(J-R)Q$ with skew-symmetric $J$, positive semidefinite $R$, and an\ninvertible $Q$ such that $Q^TE$ is positive semidefinite. This reformulation\nhas a convex feasible domain onto which it is easy to project. This allows us\nto employ a fast gradient method to obtain a nearby stable approximation of a\ngiven matrix pair. \n\n"}
{"id": "1704.03252", "contents": "Title: Nonlinear Unknown Input Observability: The General Analytic Solution Abstract: Observability is a fundamental structural property of any dynamic system and\ndescribes the possibility of reconstructing the state that characterizes the\nsystem from observing its inputs and outputs. Despite the huge effort made to\nstudy this property, there is no general analytical criterion to automatically\ncheck the state observability when the dynamics are also driven by unknown\ninputs. Here, we introduce the general analytical solution of this fundamental\nproblem, often called the unknown input observability problem. We provide the\nsystematic procedure, based on automatic computation (differentiation and\nmatrix rank determination), that allows us to automatically check the state\nobservability even in the presence of unknown inputs. The fundamental step to\nobtain this solution is the introduction of the group of invariance of\nobservability. We introduce the group of transformations under which\nobservability is invariant. Based on this group, we introduce new tensor fields\nwith respect to this group of transformations. The analytical solution of the\nunknown input observability problem is expressed in terms of these tensors.\nThis paper provides this solution together with the main analytical steps for\nits derivation and a sketch of the proof of its validity. A complete\nderivation, together with additional important properties, is available in\n[45]. On the other hand, this paper also provides a more general solution than\nthe one presented in [45] by exhaustively dealing with the systems that do not\nbelong to the category of the systems that are canonic with respect to their\nunknown inputs. This solution is also provided in the form of a new algorithm.\nWe illustrate the implementation of the new algorithm by studying the\nobservability properties of a nonlinear system in the framework of\nvisual-inertial sensor fusion. \n\n"}
{"id": "1704.04058", "contents": "Title: Solving ill-posed inverse problems using iterative deep neural networks Abstract: We propose a partially learned approach for the solution of ill posed inverse\nproblems with not necessarily linear forward operators. The method builds on\nideas from classical regularization theory and recent advances in deep learning\nto perform learning while making use of prior information about the inverse\nproblem encoded in the forward operator, noise model and a regularizing\nfunctional. The method results in a gradient-like iterative scheme, where the\n\"gradient\" component is learned using a convolutional network that includes the\ngradients of the data discrepancy and regularizer as input in each iteration.\nWe present results of such a partially learned gradient scheme on a non-linear\ntomographic inversion problem with simulated data from both the Sheep-Logan\nphantom as well as a head CT. The outcome is compared against FBP and TV\nreconstruction and the proposed method provides a 5.4 dB PSNR improvement over\nthe TV reconstruction while being significantly faster, giving reconstructions\nof 512 x 512 volumes in about 0.4 seconds using a single GPU. \n\n"}
{"id": "1704.04073", "contents": "Title: Optimal Spraying in Biological Control of Pests Abstract: We use optimal control theory with the purpose of finding the best spraying\npolicy with the aim of at least to minimize and possibly to eradicate the\nnumber of parasites, i.e., the prey for the spiders living in an\nagroecosystems. Two different optimal control problems are posed and solved,\nand their implications discussed. \n\n"}
{"id": "1704.04342", "contents": "Title: Learning-based Robust Optimization: Procedures and Statistical\n  Guarantees Abstract: Robust optimization (RO) is a common approach to tractably obtain\nsafeguarding solutions for optimization problems with uncertain constraints. In\nthis paper, we study a statistical framework to integrate data into RO, based\non learning a prediction set using (combinations of) geometric shapes that are\ncompatible with established RO tools, and a simple data-splitting validation\nstep that achieves finite-sample nonparametric statistical guarantees on\nfeasibility. We demonstrate how our required sample size to achieve feasibility\nat a given confidence level is independent of the dimensions of both the\ndecision space and the probability space governing the stochasticity, and\ndiscuss some approaches to improve the objective performances while maintaining\nthese dimension-free statistical feasibility guarantees. \n\n"}
{"id": "1704.06209", "contents": "Title: ADMM Penalty Parameter Selection by Residual Balancing Abstract: Appropriate selection of the penalty parameter is crucial to obtaining good\nperformance from the Alternating Direction Method of Multipliers (ADMM). While\nanalytic results for optimal selection of this parameter are very limited,\nthere is a heuristic method that appears to be relatively successful in a\nnumber of different problems. The contribution of this paper is to demonstrate\nthat their is a potentially serious flaw in this heuristic approach, and to\npropose a modification that at least partially addresses it. \n\n"}
{"id": "1704.06368", "contents": "Title: Facially Dual Complete (Nice) cones and lexicographic tangents Abstract: We study the boundary structure of closed convex cones, with a focus on\nfacially dual complete (nice) cones. These cones form a proper subset of\nfacially exposed convex cones, and they behave well in the context of duality\ntheory for convex optimization. Using the well-known and commonly used concept\nof tangent cones in nonlinear optimization, we introduce some new notions for\nexposure of faces of convex sets. Based on these new notions, we obtain a\nnecessary condition and a sufficient condition for a cone to be facially dual\ncomplete. In our sufficient condition, we utilize a new notion called\nlexicographic tangent cones (these are a family of cones obtained from a\nrecursive application of the tangent cone concept). Lexicographic tangent cones\nare related to Nesterov's lexicographic derivatives and to the notion of\nsubtransversality in the context of variational analysis. \n\n"}
{"id": "1704.08623", "contents": "Title: Asymptotic control theory for a closed string Abstract: We develop an asymptotical control theory for one of the simplest distributed\noscillating systems, namely, for a closed string under a bounded load applied\nto a single distinguished point. We find exact classes of string states that\nadmit complete damping and an asymptotically exact value of the required time.\nBy using approximate reachable sets instead of exact ones, we design a\ndry-friction like feedback control, which turns out to be asymptotically\noptimal. We prove the existence of motion under the control using a rather\nexplicit solution of a nonlinear wave equation. Remarkably, the solution is\ndetermined via purely algebraic operations. The main result is a proof of\nasymptotic optimality of the control thus constructed. \n\n"}
{"id": "1705.01079", "contents": "Title: Analysis, simulation and optimal control of a SEIR model for Ebola virus\n  with demographic effects Abstract: Ebola virus is one of the most virulent pathogens for humans. We present a\nmathematical description of different Susceptible-Exposed-Infectious-Recovered\n(SEIR) models. By using mathematical modeling and analysis, the latest major\noutbreak of Ebola virus in West Africa is described. Our aim is to study and\ndiscuss the properties of SEIR models with respect to Ebola virus, the\ninformation they provide, and when the models make sense. We added to the basic\nSEIR model demographic effects in order to analyze the equilibria with vital\ndynamics. Numerical simulations confirm the theoretical analysis. The control\nof the propagation of the virus through vaccination is investigated and the\ncase study of Liberia is discussed in detail. \n\n"}
{"id": "1705.01153", "contents": "Title: The efficient, the intensive, and the productive: insights from urban\n  Kaya scaling Abstract: Urban areas play an unprecedented role in potentially mitigating climate\nchange and supporting sustainable development. In light of the rapid\nurbanisation in many parts on the globe, it is crucial to understand the\nrelationship between settlement size and CO2 emission efficiency of cities.\nRecent literature on urban scaling properties of emissions as a function of\npopulation size have led to contradictory results and more importantly, lacked\nan in-depth investigation of the essential factors and causes explaining such\nscaling properties. Therefore, in analogy to the well-established Kaya\nIdentity, we develop a relation combining the involved exponents. We\ndemonstrate that application of this Urban Kaya Relation will enable a\ncomprehensive understanding about the intrinsic factors determining emission\nefficiencies in large cities by applying it to a global dataset of 61 cities.\nContrary to traditional urban scaling studies which use Ordinary Least Squares\n(OLS) regression, we show that the Reduced Major Axis (RMA) is necessary when\ncomplex relations among scaling exponents are to be investigated. RMA is given\nby the geometric mean of the two OLS slopes obtained by interchanging the\ndependent and independent variable. We discuss the potential of the Urban Kaya\nRelation in main-streaming local actions for climate change mitigation. \n\n"}
{"id": "1705.01440", "contents": "Title: Comparison of Polynomial Chaos and Gaussian Process surrogates for\n  uncertainty quantification and correlation estimation of spatially\n  distributed open-channel steady flows Abstract: Data assimilation is widely used to improve flood forecasting capability,\nespecially through parameter inference requiring statistical information on the\nuncertain input parameters (upstream discharge, friction coefficient) as well\nas on the variability of the water level and its sensitivity with respect to\nthe inputs. For particle filter or ensemble Kalman filter, stochastically\nestimating probability density function and covariance matrices from a Monte\nCarlo random sampling requires a large ensemble of model evaluations, limiting\ntheir use in real-time application. To tackle this issue, fast surrogate models\nbased on Polynomial Chaos and Gaussian Process can be used to represent the\nspatially distributed water level in place of solving the shallow water\nequations. This study investigates the use of these surrogates to estimate\nprobability density functions and covariance matrices at a reduced\ncomputational cost and without the loss of accuracy, in the perspective of\nensemble-based data assimilation. This study focuses on 1-D steady state flow\nsimulated with MASCARET over the Garonne River (South-West France). Results\nshow that both surrogates feature similar performance to the Monte-Carlo random\nsampling, but for a much smaller computational budget; a few MASCARET\nsimulations (on the order of 10-100) are sufficient to accurately retrieve\ncovariance matrices and probability density functions all along the river, even\nwhere the flow dynamic is more complex due to heterogeneous bathymetry. This\npaves the way for the design of surrogate strategies suitable for representing\nunsteady open-channel flows in data assimilation. \n\n"}
{"id": "1705.02766", "contents": "Title: \"Convex Until Proven Guilty\": Dimension-Free Acceleration of Gradient\n  Descent on Non-Convex Functions Abstract: We develop and analyze a variant of Nesterov's accelerated gradient descent\n(AGD) for minimization of smooth non-convex functions. We prove that one of two\ncases occurs: either our AGD variant converges quickly, as if the function was\nconvex, or we produce a certificate that the function is \"guilty\" of being\nnon-convex. This non-convexity certificate allows us to exploit negative\ncurvature and obtain deterministic, dimension-free acceleration of convergence\nfor non-convex functions. For a function $f$ with Lipschitz continuous gradient\nand Hessian, we compute a point $x$ with $\\|\\nabla f(x)\\| \\le \\epsilon$ in\n$O(\\epsilon^{-7/4} \\log(1/ \\epsilon) )$ gradient and function evaluations.\nAssuming additionally that the third derivative is Lipschitz, we require only\n$O(\\epsilon^{-5/3} \\log(1/ \\epsilon) )$ evaluations. \n\n"}
{"id": "1705.03244", "contents": "Title: On Placement of Synthetic Inertia with Explicit Time-Domain Constraints Abstract: Rotational inertia is stabilizing the frequency of electric power systems\nagainst small and large disturbances, but it is also the cause for oscillations\nbetween generators. As more and more conventional generators are replaced by\nrenewable generation with little or no inertia, the dynamics of power systems\nwill change. It has been proposed to add synthetic inertia to the power system\nto counteract these changes. This paper presents an algorithm to compute the\noptimal placement of synthetic inertia and damping in the system with respect\nto explicit time-domain constraints on the rate of change of frequency, the\nfrequency overshoot after a step disturbance, and actuation input. A case study\nhints that the approach delivers reliable results, and it is scalable and\napplicable to realistic power system models. \n\n"}
{"id": "1705.03412", "contents": "Title: Nonconvex Generalization of Alternating Direction Method of Multipliers\n  for Nonlinear Equality Constrained Problems Abstract: The classic Alternating Direction Method of Multipliers (ADMM) is a popular\nframework to solve linear-equality constrained problems. In this paper, we\nextend the ADMM naturally to nonlinear equality-constrained problems, called\nneADMM. The difficulty of neADMM is to solve nonconvex subproblems. We provide\nglobally optimal solutions to them in two important applications. Experiments\non synthetic and real-world datasets demonstrate excellent performance and\nscalability of our proposed neADMM over existing state-of-the-start methods. \n\n"}
{"id": "1705.03803", "contents": "Title: Convergence of inertial dynamics and proximal algorithms governed by\n  maximally monotone operators Abstract: We study the behavior of the trajectories of a second-order differential\nequation with vanishing damping, governed by the Yosida regularization of a\nmaximally monotone operator with time-varying index, along with a new {\\em\nRegularized Inertial Proximal Algorithm} obtained by means of a convenient\nfinite-difference discretization. These systems are the counterpart to\naccelerated forward-backward algorithms in the context of maximally monotone\noperators. A proper tuning of the parameters allows us to prove the weak\nconvergence of the trajectories to zeroes of the operator. Moreover, it is\npossible to estimate the rate at which the speed and acceleration vanish. We\nalso study the effect of perturbations or computational errors that leave the\nconvergence properties unchanged. We also analyze a growth condition under\nwhich strong convergence can be guaranteed. A simple example shows the\ncriticality of the assumptions on the Yosida approximation parameter, and\nallows us to illustrate the behavior of these systems compared with some of\ntheir close relatives. \n\n"}
{"id": "1705.05348", "contents": "Title: Exploiting the Structure via Sketched Gradient Algorithms Abstract: Sketched gradient algorithms have been recently introduced for efficiently\nsolving the large-scale constrained Least-squares regressions. In this paper we\nprovide novel convergence analysis for the basic method {\\it Gradient\nProjection Classical Sketch} (GPCS) to reveal the fast linear convergence rate\nof GPCS towards a vicinity of the solution thanks to the intrinsic\nlow-dimensional geometric structure of the solution prompted by constraint set.\nSimilar to our analysis we observe computational and sketch size trade-offs in\nnumerical experiments. Hence we justify that the combination of gradient\nmethods and the sketching technique is a way of designing efficient algorithms\nwhich can actively exploit the low-dimensional structure to accelerate\ncomputation in large scale data regression and signal processing applications. \n\n"}
{"id": "1705.05982", "contents": "Title: Understanding Quality Factor Degradation in Superconducting Niobium\n  Cavities at Low Microwave Field Amplitudes Abstract: In niobium superconducting radio frequency (SRF) accelerating cavities a\ndecrease of the quality factor at lower fields - a so called \\emph{low field Q\nslope or LFQS} - has been a long-standing unexplained effect. By extending the\nhigh $Q$ measurement techniques to ultralow fields we discover two previously\nunknown features of the effect: i) saturation at rf fields lower than\n$E_\\mathrm{acc} \\sim 0.1$~MV/m; ii) strong degradation enhancement by growing\nthicker niobium pentoxide. Our findings suggest that the LFQS may be caused by\nthe two level systems in the natural niobium oxide on the inner cavity surface,\nthereby identifying a new source of residual resistance and providing guidance\nfor potential non-accelerator low field applications of SRF cavities. \n\n"}
{"id": "1705.06270", "contents": "Title: A Review on Bilevel Optimization: From Classical to Evolutionary\n  Approaches and Applications Abstract: Bilevel optimization is defined as a mathematical program, where an\noptimization problem contains another optimization problem as a constraint.\nThese problems have received significant attention from the mathematical\nprogramming community. Only limited work exists on bilevel problems using\nevolutionary computation techniques; however, recently there has been an\nincreasing interest due to the proliferation of practical applications and the\npotential of evolutionary algorithms in tackling these problems. This paper\nprovides a comprehensive review on bilevel optimization from the basic\nprinciples to solution strategies; both classical and evolutionary. A number of\npotential application problems are also discussed. To offer the readers\ninsights on the prominent developments in the field of bilevel optimization, we\nhave performed an automated text-analysis of an extended list of papers\npublished on bilevel optimization to date. This paper should motivate\nevolutionary computation researchers to pay more attention to this practical\nyet challenging area. \n\n"}
{"id": "1705.06904", "contents": "Title: Evidence for mixed rationalities in preference formation Abstract: Understanding the mechanisms underlying the formation of cultural traits,\nsuch as preferences, opinions and beliefs is an open challenge. Trait formation\nis intimately connected to cultural dynamics, which has been the focus of a\nvariety of quantitative models. Recently, some studies have emphasized the\nimportance of connecting those models to snapshots of cultural dynamics that\nare empirically accessible. By analyzing data obtained from different sources,\nit has been suggested that culture has properties that are universally present,\nand that empirical cultural states differ systematically from randomized\ncounterparts. Hence, a question about the mechanism responsible for the\nobserved patterns naturally arises. This study proposes a stochastic structural\nmodel for generating cultural states that retain those robust, empirical\nproperties. One ingredient of the model, already used in previous work, assumes\nthat every individual's set of traits is partly dictated by one of several,\nuniversal \"rationalities\", informally postulated by several social science\ntheories. The second, new ingredient taken from the same theories assumes that,\napart from a dominant rationality, each individual also has a certain exposure\nto the other rationalities. It is shown that both ingredients are required for\nreproducing the empirical regularities. This key result suggests that the\neffects of cultural dynamics in the real world can be described as an interplay\nof multiple, mixing rationalities, and thus provides indirect evidence for the\nclass of social science theories postulating such mixing. The model should be\nseen as a static, effective description of culture, while a dynamical, more\nfundamental description is left for future research. \n\n"}
{"id": "1705.07576", "contents": "Title: Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk Abstract: We examine the theoretical properties of enforcing priors provided by\ngenerative deep neural networks via empirical risk minimization. In particular\nwe consider two models, one in which the task is to invert a generative neural\nnetwork given access to its last layer and another in which the task is to\ninvert a generative neural network given only compressive linear observations\nof its last layer. We establish that in both cases, in suitable regimes of\nnetwork layer sizes and a randomness assumption on the network weights, that\nthe non-convex objective function given by empirical risk minimization does not\nhave any spurious stationary points. That is, we establish that with high\nprobability, at any point away from small neighborhoods around two scalar\nmultiples of the desired solution, there is a descent direction. Hence, there\nare no local minima, saddle points, or other stationary points outside these\nneighborhoods. These results constitute the first theoretical guarantees which\nestablish the favorable global geometry of these non-convex optimization\nproblems, and they bridge the gap between the empirical success of enforcing\ndeep generative priors and a rigorous understanding of non-linear inverse\nproblems. \n\n"}
{"id": "1705.07957", "contents": "Title: Large Scale Empirical Risk Minimization via Truncated Adaptive Newton\n  Method Abstract: We consider large scale empirical risk minimization (ERM) problems, where\nboth the problem dimension and variable size is large. In these cases, most\nsecond order methods are infeasible due to the high cost in both computing the\nHessian over all samples and computing its inverse in high dimensions. In this\npaper, we propose a novel adaptive sample size second-order method, which\nreduces the cost of computing the Hessian by solving a sequence of ERM problems\ncorresponding to a subset of samples and lowers the cost of computing the\nHessian inverse using a truncated eigenvalue decomposition. We show that while\nwe geometrically increase the size of the training set at each stage, a single\niteration of the truncated Newton method is sufficient to solve the new ERM\nwithin its statistical accuracy. Moreover, for a large number of samples we are\nallowed to double the size of the training set at each stage, and the proposed\nmethod subsequently reaches the statistical accuracy of the full training set\napproximately after two effective passes. In addition to this theoretical\nresult, we show empirically on a number of well known data sets that the\nproposed truncated adaptive sample size algorithm outperforms stochastic\nalternatives for solving ERM problems. \n\n"}
{"id": "1705.08161", "contents": "Title: Computational Methods for Path-based Robust Flows Abstract: Real world networks are often subject to severe uncertainties which need to\nbe addressed by any reliable prescriptive model. In the context of the maximum\nflow problem subject to arc failure, robust models have gained particular\nattention. For a path-based model, the resulting optimization problem is\nassumed to be difficult in the literature, yet the complexity status is widely\nunknown. We present a computational approach to solve the robust flow problem\nto optimality by simultaneous primal and dual separation, the practical\nefficacy of which is shown by a computational study.\n  Furthermore, we introduce a novel model of robust flows which provides a\ncompromise between stochastic and robust optimization by assigning\nprobabilities to groups of scenarios. The new model can be solved by the same\ncomputational techniques as the robust model. A bound on the generalization\nerror is proven for the case that the probabilities are determined empirically.\nThe suggested model as well as the computational approach extend to linear\noptimization problems more general than robust flows. \n\n"}
{"id": "1705.08389", "contents": "Title: A Derandomized Algorithm for RP-ADMM with Symmetric Gauss-Seidel Method Abstract: For multi-block alternating direction method of multipliers(ADMM), where the\nobjective function can be decomposed into multiple block components, we show\nthat with block symmetric Gauss-Seidel iteration, the algorithm will converge\nquickly. The method will apply a block symmetric Gauss-Seidel iteration in the\nprimal update and a linear correction that can be derived in view of Richard\niteration. We also establish the linear convergence rate for linear systems. \n\n"}
{"id": "1705.08494", "contents": "Title: Asynchronous Coordinate Descent under More Realistic Assumptions Abstract: Asynchronous-parallel algorithms have the potential to vastly speed up\nalgorithms by eliminating costly synchronization. However, our understanding to\nthese algorithms is limited because the current convergence of asynchronous\n(block) coordinate descent algorithms are based on somewhat unrealistic\nassumptions. In particular, the age of the shared optimization variables being\nused to update a block is assumed to be independent of the block being updated.\nAlso, it is assumed that the updates are applied to randomly chosen blocks. In\nthis paper, we argue that these assumptions either fail to hold or will imply\nless efficient implementations. We then prove the convergence of\nasynchronous-parallel block coordinate descent under more realistic\nassumptions, in particular, always without the independence assumption. The\nanalysis permits both the deterministic (essentially) cyclic and random rules\nfor block choices. Because a bound on the asynchronous delays may or may not be\navailable, we establish convergence for both bounded delays and unbounded\ndelays. The analysis also covers nonconvex, weakly convex, and strongly convex\nfunctions. We construct Lyapunov functions that directly model both objective\nprogress and delays, so delays are not treated errors or noise. A\ncontinuous-time ODE is provided to explain the construction at a high level. \n\n"}
{"id": "1705.10883", "contents": "Title: Optimization of Tree Ensembles Abstract: Tree ensemble models such as random forests and boosted trees are among the\nmost widely used and practically successful predictive models in applied\nmachine learning and business analytics. Although such models have been used to\nmake predictions based on exogenous, uncontrollable independent variables, they\nare increasingly being used to make predictions where the independent variables\nare controllable and are also decision variables. In this paper, we study the\nproblem of tree ensemble optimization: given a tree ensemble that predicts some\ndependent variable using controllable independent variables, how should we set\nthese variables so as to maximize the predicted value? We formulate the problem\nas a mixed-integer optimization problem. We theoretically examine the strength\nof our formulation, provide a hierarchy of approximate formulations with bounds\non approximation quality and exploit the structure of the problem to develop\ntwo large-scale solution methods, one based on Benders decomposition and one\nbased on iteratively generating tree split constraints. We test our methodology\non real data sets, including two case studies in drug design and customized\npricing, and show that our methodology can efficiently solve large-scale\ninstances to near or full optimality, and outperforms solutions obtained by\nheuristic approaches. In our drug design case, we show how our approach can\nidentify compounds that efficiently trade-off predicted performance and novelty\nwith respect to existing, known compounds. In our customized pricing case, we\nshow how our approach can efficiently determine optimal store-level prices\nunder a random forest model that delivers excellent predictive accuracy. \n\n"}
{"id": "1706.00234", "contents": "Title: Optimality conditions for minimizers at infinity in polynomial\n  programming Abstract: In this paper we study necessary optimality conditions for the optimization\nproblem $$\\textrm{infimum}f_0(x) \\quad \\textrm{ subject to } \\quad x \\in S,$$\nwhere $f_0 \\colon \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a polynomial function\nand $S \\subset \\mathbb{R}^n$ is a set defined by polynomial inequalities.\nAssume that the problem is bounded below and has the Mangasarian--Fromovitz\nproperty at infinity. We first show that if the problem does {\\em not} have an\noptimal solution, then a version at infinity of the Fritz-John optimality\nconditions holds. From this we derive a version at infinity of the\nKarush--Kuhn--Tucker optimality conditions. As applications, we obtain a\nFrank--Wolfe type theorem which states that the optimal solution set of the\nproblem is nonempty provided the objective function $f_0$ is convenient.\nFinally, in the unconstrained case, we show that the optimal value of the\nproblem is the smallest critical value of some polynomial. All the results are\npresented in terms of the Newton polyhedra of the polynomials defining the\nproblem. \n\n"}
{"id": "1706.00908", "contents": "Title: Analyzing Random Permutations for Cyclic Coordinate Descent Abstract: We consider coordinate descent methods on convex quadratic problems, in which\nexact line searches are performed at each iteration. (This algorithm is\nidentical to Gauss-Seidel on the equivalent symmetric positive definite linear\nsystem.) We describe a class of convex quadratic problems for which the\nrandom-permutations version of cyclic coordinate descent (RPCD) outperforms the\nstandard cyclic coordinate descent (CCD) approach, yielding convergence\nbehavior similar to the fully-random variant (RCD). A convergence analysis is\ndeveloped to explain the empirical observations. \n\n"}
{"id": "1706.02602", "contents": "Title: The primal-dual hybrid gradient method reduces to a primal method for\n  linearly constrained optimization problems Abstract: In this work, we show that for linearly constrained optimization problems the\nprimal-dual hybrid gradient algorithm, analyzed by Chambolle and Pock [3], can\nbe written as an entirely primal algorithm. This allows us to prove convergence\nof the iterates even in the degenerate cases when the linear system is\ninconsistent or when the strong duality does not hold. We also obtain new\nconvergence rates which seem to improve existing ones in the literature. For a\ndecentralized distributed optimization we show that the new scheme is much more\nefficient than the original one. \n\n"}
{"id": "1706.02654", "contents": "Title: Derivation and Analysis of the Primal-Dual Method of Multipliers Based\n  on Monotone Operator Theory Abstract: In this paper we present a novel derivation for an existing node-based\nalgorithm for distributed optimisation termed the primal-dual method of\nmultipliers (PDMM). In contrast to its initial derivation, in this work\nmonotone operator theory is used to connect PDMM with other first-order methods\nsuch as Douglas-Rachford splitting and the alternating direction method of\nmultipliers thus providing insight to the operation of the scheme. In\nparticular, we show how PDMM combines a lifted dual form in conjunction with\nPeaceman-Rachford splitting to remove the need for collaboration between nodes\nper iteration. We demonstrate sufficient conditions for strong primal\nconvergence for a general class of functions while under the assumption of\nstrong convexity and functional smoothness, we also introduce a primal\ngeometric convergence bound. Finally we introduce a distributed method of\nparameter selection in the geometric convergent case, requiring only finite\ntransmissions to implement regardless of network topology. \n\n"}
{"id": "1706.04147", "contents": "Title: Layer Communities in Multiplex Networks Abstract: Multiplex networks are a type of multilayer network in which entities are\nconnected to each other via multiple types of connections. We propose a method,\nbased on computing pairwise similarities between layers and then doing\ncommunity detection, for grouping structurally similar layers in multiplex\nnetworks. We illustrate our approach using both synthetic and empirical\nnetworks, and we are able to find meaningful groups of layers in both cases.\nFor example, we find that airlines that are based in similar geographic\nlocations tend to be grouped together in an airline multiplex network and that\nrelated research areas in physics tend to be grouped together in an multiplex\ncollaboration network. \n\n"}
{"id": "1706.06843", "contents": "Title: Optimal control of non-autonomous SEIRS models with vaccination and\n  treatment Abstract: We study an optimal control problem for a non-autonomous SEIRS model with\nincidence given by a general function of the infective, the susceptible and the\ntotal population, and with vaccination and treatment as control variables. We\nprove existence and uniqueness results for our problem and, for the case of\nmass-action incidence, we present some simulation results designed to compare\nan autonomous and corresponding periodic model, as well as the controlled\nversus uncontrolled models. \n\n"}
{"id": "1706.07001", "contents": "Title: Improved Optimization of Finite Sums with Minibatch Stochastic Variance\n  Reduced Proximal Iterations Abstract: We present novel minibatch stochastic optimization methods for empirical risk\nminimization problems, the methods efficiently leverage variance reduced\nfirst-order and sub-sampled higher-order information to accelerate the\nconvergence speed. For quadratic objectives, we prove improved iteration\ncomplexity over state-of-the-art under reasonable assumptions. We also provide\nempirical evidence of the advantages of our method compared to existing\napproaches in the literature. \n\n"}
{"id": "1706.07067", "contents": "Title: Interior-proximal primal-dual methods Abstract: We study preconditioned proximal point methods for a class of saddle point\nproblems, where the preconditioner decouples the overall proximal point method\ninto an alternating primal--dual method. This is akin to the Chambolle--Pock\nmethod or the ADMM. In our work, we replace the squared distance in the dual\nstep by a barrier function on a symmetric cone, while using a standard\n(Euclidean) proximal step for the primal variable. We show that under\nnon-degeneracy and simple linear constraints, such a hybrid primal--dual\nalgorithm can achieve linear convergence on originally strongly convex problems\ninvolving the second-order cone in their saddle point form. On general\nsymmetric cones, we are only able to show an $O(1/N)$ rate. These results are\nbased on estimates of strong convexity of the barrier function, extended with a\npenalty to the boundary of the symmetric cone. \n\n"}
{"id": "1706.07993", "contents": "Title: Behavior of Accelerated Gradient Methods Near Critical Points of\n  Nonconvex Functions Abstract: We examine the behavior of accelerated gradient methods in smooth nonconvex\nunconstrained optimization, focusing in particular on their behavior near\nstrict saddle points. Accelerated methods are iterative methods that typically\nstep along a direction that is a linear combination of the previous step and\nthe gradient of the function evaluated at a point at or near the current\niterate. (The previous step encodes gradient information from earlier stages in\nthe iterative process.) We show by means of the stable manifold theorem that\nthe heavy-ball method method is unlikely to converge to strict saddle points,\nwhich are points at which the gradient of the objective is zero but the Hessian\nhas at least one negative eigenvalue. We then examine the behavior of the\nheavy-ball method and other accelerated gradient methods in the vicinity of a\nstrict saddle point of a nonconvex quadratic function, showing that both\nmethods can diverge from this point more rapidly than the steepest-descent\nmethod. \n\n"}
{"id": "1707.01146", "contents": "Title: Data-driven discovery of Koopman eigenfunctions for control Abstract: Data-driven transformations that reformulate nonlinear systems in a linear\nframework have the potential to enable the prediction, estimation, and control\nof strongly nonlinear dynamics using linear systems theory. The Koopman\noperator has emerged as a principled linear embedding of nonlinear dynamics,\nand its eigenfunctions establish intrinsic coordinates along which the dynamics\nbehave linearly. Previous studies have used finite-dimensional approximations\nof the Koopman operator for model-predictive control approaches. In this work,\nwe illustrate a fundamental closure issue of this approach and argue that it is\nbeneficial to first validate eigenfunctions and then construct reduced-order\nmodels in these validated eigenfunctions. These coordinates form a\nKoopman-invariant subspace by design and, thus, have improved predictive power.\nWe show then how the control can be formulated directly in these intrinsic\ncoordinates and discuss potential benefits and caveats of this perspective. The\nresulting control architecture is termed Koopman Reduced Order Nonlinear\nIdentification and Control (KRONIC). It is demonstrated that these\neigenfunctions can be approximated with data-driven regression and power series\nexpansions, based on the partial differential equation governing the\ninfinitesimal generator of the Koopman operator. Validating discovered\neigenfunctions is crucial and we show that lightly damped eigenfunctions may be\nfaithfully extracted from EDMD or an implicit formulation. These lightly damped\neigenfunctions are particularly relevant for control, as they correspond to\nnearly conserved quantities that are associated with persistent dynamics, such\nas the Hamiltonian. KRONIC is then demonstrated on a number of relevant\nexamples, including 1) a nonlinear system with a known linear embedding, 2) a\nvariety of Hamiltonian systems, and 3) a high-dimensional double-gyre model for\nocean mixing. \n\n"}
{"id": "1707.02428", "contents": "Title: Combinatorial Optimization Problems with Interaction Costs: Complexity\n  and Solvable Cases Abstract: We introduce and study the combinatorial optimization problem with\ninteraction costs (COPIC). COPIC is the problem of finding two combinatorial\nstructures, one from each of two given families, such that the sum of their\nindependent linear costs and the interaction costs between elements of the two\nselected structures is minimized. COPIC generalizes the quadratic assignment\nproblem and many other well studied combinatorial optimization problems, and\nhence covers many real world applications. We show how various topics from\ndifferent areas in the literature can be formulated as special cases of COPIC.\nThe main contributions of this paper are results on the computational\ncomplexity and approximability of COPIC for different families of combinatorial\nstructures (e.g. spanning trees, paths, matroids), and special structures of\nthe interaction costs. More specifically, we analyze the complexity if the\ninteraction cost matrix is parameterized by its rank and if it is a diagonal\nmatrix. Also, we determine the structure of the intersection cost matrix, such\nthat COPIC is equivalent to independently solving linear optimization problems\nfor the two given families of combinatorial structures. \n\n"}
{"id": "1707.03541", "contents": "Title: A Scalable Semidefinite Relaxation Approach to Grid Scheduling Abstract: Determination of the most economic strategies for supply and transmission of\nelectricity is a daunting computational challenge. Due to theoretical barriers,\nso-called NP-hardness, the amount of effort to optimize the schedule of\ngenerating units and route of power, can grow exponentially with the number of\ndecision variables. Practical approaches to this problem involve legacy\napproximations and ad-hoc heuristics that may undermine the efficiency and\nreliability of power system operations, that are ever growing in scale and\ncomplexity. Therefore, the development of powerful optimization methods for\ndetailed power system scheduling is critical to the realization of smart grids\nand has received significant attention recently. In this paper, we propose for\nthe first time a computational method, which is capable of solving large-scale\npower system scheduling problems with thousands of generating units, while\naccurately incorporating the nonlinear equations that govern the flow of\nelectricity on the grid. The utilization of this accurate nonlinear model, as\nopposed to its linear approximations, results in a more efficient and\ntransparent market design, as well as improvements in the reliability of power\nsystem operations. We design a polynomial-time solvable third-order\nsemidefinite programming (TSDP) relaxation, with the aim of finding a near\nglobally optimal solution for the original NP-hard problem. The proposed method\nis demonstrated on the largest available benchmark instances from real-world\nEuropean grid data, for which provably optimal or near-optimal solutions are\nobtained. \n\n"}
{"id": "1707.03686", "contents": "Title: Methodology for Multi-stage, Operations- and Uncertainty-Aware Placement\n  and Sizing of FACTS Devices in a Large Power Transmission System Abstract: We develop new optimization methodology for planning installation of Flexible\nAlternating Current Transmission System (FACTS) devices of the parallel and\nshunt types into large power transmission systems, which allows to delay or\navoid installations of generally much more expensive power lines. Methodology\ntakes as an input projected economic development, expressed through a paced\ngrowth of the system loads, as well as uncertainties, expressed through\nmultiple scenarios of the growth. We price new devices according to their\ncapacities. Installation cost contributes to the optimization objective in\ncombination with the cost of operations integrated over time and averaged over\nthe scenarios. The multi-stage (-time-frame) optimization aims to achieve a\ngradual distribution of new resources in space and time. Constraints on the\ninvestment budget, or equivalently constraint on building capacity, is\nintroduced at each time frame. Our approach adjusts operationally not only\nnewly installed FACTS devices but also other already existing flexible degrees\nof freedom. This complex optimization problem is stated using the most general\nAC Power Flows. Non-linear, non-convex, multiple-scenario and multi-time-frame\noptimization is resolved via efficient heuristics, consisting of a sequence of\nalternating Linear Programmings or Quadratic Programmings (depending on the\ngeneration cost) and AC-PF solution steps designed to maintain operational\nfeasibility for all scenarios. Computational scalability and application of the\nnewly developed approach is illustrated on the example of the 2736-nodes large\nPolish system. One most important advantage of the framework is that the\noptimal capacity of FACTS is build up gradually at each time frame in a limited\nnumber of locations, thus allowing to prepare the system better for possible\ncongestion due to future economic and other uncertainties. \n\n"}
{"id": "1707.05282", "contents": "Title: Certification and Quantification of Multilevel Quantum Coherence Abstract: Quantum coherence, present whenever a quantum system exists in a\nsuperposition of multiple classically distinct states, marks one of the\nfundamental departures from classical physics. Quantum coherence has recently\nbeen investigated rigorously within a resource-theoretic formalism. However,\nthe finer-grained notion of multilevel coherence, which explicitly takes into\naccount the number of superposed classical states, has remained relatively\nunexplored. A comprehensive analysis of multi-level coherence, which acts as\nthe single-party analogue to multi-partite entanglement, is essential for\nunderstanding natural quantum processes as well as for gauging the performance\nof quantum technologies. Here we develop the theoretical and experimental\ngroundwork for characterizing and quantifying multilevel coherence. We prove\nthat non-trivial levels of purity are required for multilevel coherence, as\nthere is a ball of states around the maximally mixed state that do not exhibit\nmultilevel coherence in any basis. We provide a simple necessary and sufficient\nanalytical criterion to verify multilevel coherence, which leads to a complete\nclassification for three-level systems. We present the robustness of multilevel\ncoherence, a bona fide quantifier which we show to be numerically computable\nvia semidefinite programming and experimentally accessible via multilevel\ncoherence witnesses. We further verify and lower-bound the robustness of\nmultilevel coherence by performing a semi-device-independent phase\ndiscrimination task, implemented experimentally with four-level probes in a\nphotonic setup. Our results contribute to understanding the operational\nrelevance of genuine multilevel coherence, also by demonstrating the key role\nit plays in enhanced phase discrimination---a primitive for quantum\ncommunication and metrology---and suggest new ways to reliably test the quantum\nbehaviour of physical systems. \n\n"}
{"id": "1707.06631", "contents": "Title: Two Results on Slime Mold Computations Abstract: We present two results on slime mold computations. In wet-lab experiments\n(Nature'00) by Nakagaki et al. the slime mold Physarum polycephalum\ndemonstrated its ability to solve shortest path problems. Biologists proposed a\nmathematical model, a system of differential equations, for the slime's\nadaption process (J. Theoretical Biology'07). It was shown that the process\nconvergences to the shortest path (J. Theoretical Biology'12) for all graphs.\nWe show that the dynamics actually converges for a much wider class of\nproblems, namely undirected linear programs with a non-negative cost vector.\n  Combinatorial optimization researchers took the dynamics describing slime\nbehavior as an inspiration for an optimization method and showed that its\ndiscretization can $\\varepsilon$-approximately solve linear programs with\npositive cost vector (ITCS'16). Their analysis requires a feasible starting\npoint, a step size depending linearly on $\\varepsilon$, and a number of steps\nwith quartic dependence on $\\mathrm{opt}/(\\varepsilon\\Phi)$, where $\\Phi$ is\nthe difference between the smallest cost of a non-optimal basic feasible\nsolution and the optimal cost ($\\mathrm{opt}$).\n  We give a refined analysis showing that the dynamics initialized with any\nstrongly dominating point converges to the set of optimal solutions. Moreover,\nwe strengthen the convergence rate bounds and prove that the step size is\nindependent of $\\varepsilon$, and the number of steps depends logarithmically\non $1/\\varepsilon$ and quadratically on $\\mathrm{opt}/\\Phi$. \n\n"}
{"id": "1707.07192", "contents": "Title: Investigating Einstein-Podolsky-Rosen steering of continuous variable\n  bipartite states by non-Gaussian pseudospin measurements Abstract: EPR steering is an asymmetric form of correlations which is intermediate\nbetween quantum entanglement and Bell nonlocality, and can be exploited for\nquantum communication with one untrusted party. In particular, steering of\ncontinuous variable Gaussian states has been extensively studied as a\nmanifestation of the EPR paradox. While most of these studies focused on\nquadrature measurements for steering detection, two recent works revealed that\nthere exist Gaussian states which are only steerable by non-Gaussian\nmeasurements. In this paper we perform a systematic investigation of EPR\nsteering of bipartite Gaussian states by pseudospin measurements, complementing\nand extending previous findings. We first derive the density matrix elements of\ntwo-mode squeezed thermal states in the Fock basis, which may be of independent\ninterest. We then use such a representation to investigate steering of these\nstates as detected by a nonlinear criterion, based on second moments of the\npseudospin correlation matrix. This analysis reveals previously unexplored\nregimes where non-Gaussian measurements are more effective than Gaussian ones\nto witness steering of Gaussian states in the presence of local noise. We\nfurther consider an alternative set of pseudospin observables, whose\nexpectation value can be expressed compactly in terms of Wigner functions for\nall two-mode Gaussian states. However, according to the adopted criterion,\nthese observables are found to be always less sensitive than Gaussian\nobservables for steering detection. Finally, we investigate continuous variable\nWerner states, which are non-Gaussian mixtures of Gaussian states, and find\nthat pseudospin measurements are always more effective than Gaussian ones to\nreveal their steerability. Our results provide useful insights on the role of\nnon-Gaussian measurements in characterizing quantum correlations of Gaussian\nand non-Gaussian states. \n\n"}
{"id": "1707.07476", "contents": "Title: About Extensions of the Extremal Principle Abstract: In this article, after recalling and discussing the conventional extremality,\nlocal extremality, stationarity and approximate stationarity properties of\ncollections of sets and the corresponding (extended) extremal principle, we\nfocus on extensions of these properties and the corresponding dual conditions\nwith the goal to refine the main arguments used in this type of results,\nclarify the relationships between different extensions and expand the\napplicability of the generalised separability results. We introduce and study\nnew more universal concepts of relative extremality and stationarity and\nformulate the relative extended extremal principle. Among other things, certain\nstability of the relative approximate stationarity is proved. Some links are\nestablished between the relative extremality and stationarity properties of\ncollections of sets and (the absence of) certain regularity, lower\nsemicontinuity and Lipschitz-like properties of set-valued mappings. \n\n"}
{"id": "1707.09018", "contents": "Title: On a possible fractal relationship between the Hurst exponent and the\n  nonextensive Gutenberg-Richter index Abstract: In the present paper, we analyze the fractal structures in magnitude time\nseries for a set of unprecedented sample extracted from the National Earthquake\nInformation Center (NEIC) catalog corresponding to 12 Circum-Pacific subduction\nzones from Chile to Kermadec. For this end, we used the classical Rescaled\nRange ($R/S$) analysis for estimating the long-term persistence signature\nderived from scaling parameter so-called Hurst exponent, $H$. As a result, we\nmeasured the referred exponent and obtained all values of $H>0.5$, indicating\nthat a long-term memory effect exists. The main contribution of our paper, we\nfound a possible fractal relationship between $H$ and the $b_{s}(q)$-index\nwhich emerges from nonextensive Gutenberg-Richter law as a function of the\nasperity, i.e., we show that the values of $H$ can be associated with the\nmechanism which controls the abundance of magnitude and, therefore, the level\nof activity of earthquakes. Finally, we concluded that dynamics associated with\nfragment-asperity interactions can be emphasized as a self-affine fractal\nphenomenon. \n\n"}
{"id": "1707.09351", "contents": "Title: Nash equilibria for game contingent claims with utility-based hedging Abstract: Game contingent claims (GCCs) generalize American contingent claims by\nallowing the writer to recall the option as long as it is not exercised, at the\nprice of paying some penalty. In incomplete markets, an appealing approach is\nto analyze GCCs like their European and American counterparts by solving option\nholder's and writer's optimal investment problems in the underlying securities.\nBy this, partial hedging opportunities are taken into account. We extend\nresults in the literature by solving the stochastic game corresponding to GCCs\nwith both continuous time stopping and trading. Namely, we construct Nash\nequilibria by rewriting the game as a non-zero-sum stopping game in which\nplayers compare payoffs in terms of their exponential utility indifference\nvalues. As a by-product, we also obtain an existence result for the optimal\nexercise time of an American claim under utility indifference valuation by\nrelating it to the corresponding nonlinear Snell envelope. \n\n"}
{"id": "1707.09726", "contents": "Title: Spectral Compressed Sensing via Projected Gradient Descent Abstract: Let $x\\in\\mathbb{C}^n$ be a spectrally sparse signal consisting of $r$\ncomplex sinusoids with or without damping. We consider the spectral compressed\nsensing problem, which is about reconstructing $x$ from its partial revealed\nentries. By utilizing the low rank structure of the Hankel matrix corresponding\nto $x$, we develop a computationally efficient algorithm for this problem. The\nalgorithm starts from an initial guess computed via one-step hard thresholding\nfollowed by projection, and then proceeds by applying projected gradient\ndescent iterations to a non-convex functional. Based on the sampling with\nreplacement model, we prove that $O(r^2\\log(n))$ observed entries are\nsufficient for our algorithm to achieve the successful recovery of a spectrally\nsparse signal. Moreover, extensive empirical performance comparisons show that\nour algorithm is competitive with other state-of-the-art spectral compressed\nsensing algorithms in terms of phase transitions and overall computational\ntime. \n\n"}
{"id": "1707.09863", "contents": "Title: Dimensionality reduction of SDPs through sketching Abstract: We show how to sketch semidefinite programs (SDPs) using positive maps in\norder to reduce their dimension. More precisely, we use\nJohnson\\hyp{}Lindenstrauss transforms to produce a smaller SDP whose solution\npreserves feasibility or approximates the value of the original problem with\nhigh probability. These techniques allow to improve both complexity and storage\nspace requirements. They apply to problems in which the Schatten 1-norm of the\nmatrices specifying the SDP and also of a solution to the problem is constant\nin the problem size. Furthermore, we provide some results which clarify the\nlimitations of positive, linear sketches in this setting. \n\n"}
{"id": "1708.00745", "contents": "Title: Efficient Inversion of Multiple-Scattering Model for Optical Diffraction\n  Tomography Abstract: Optical diffraction tomography relies on solving an inverse scattering\nproblem governed by the wave equation. Classical reconstruction algorithms are\nbased on linear approximations of the forward model (Born or Rytov), which\nlimits their applicability to thin samples with low refractive-index contrasts.\nMore recent works have shown the benefit of adopting nonlinear models. They\naccount for multiple scattering and reflections, improving the quality of\nreconstruction. To reduce the complexity and memory requirements of these\nmethods, we derive an explicit formula for the Jacobian matrix of the nonlinear\nLippmann-Schwinger model which lends itself to an efficient evaluation of the\ngradient of the data- fidelity term. This allows us to deploy efficient methods\nto solve the corresponding inverse problem subject to sparsity constraints. \n\n"}
{"id": "1708.01334", "contents": "Title: Resonance free regions and non-Hermitian spectral optimization for\n  Schr\\\"odinger point interactions Abstract: Resonances of Schr\\\"odinger Hamiltonians with point interactions are\nconsidered. The main object under the study is the resonance free region under\nthe assumption that the centers, where the point interactions are located, are\nknown and the associated 'strength' parameters are unknown and allowed to bear\nadditional dissipative effects. To this end we consider the boundary of the\nresonance free region as a Pareto optimal frontier and study the corresponding\noptimization problem for resonances. It is shown that upper logarithmic bound\non resonances can be made uniform with respect to the strength parameters. The\nnecessary conditions on optimality are obtained in terms of first principal\nminors of the characteristic determinant. We demonstrate the applicability of\nthese optimality conditions on the case of 4 equidistant centers by computing\nexplicitly the resonances of minimal decay for all frequencies. This example\nshows that a resonance of minimal decay is not necessarily simple, and in some\ncases it is generated by an infinite family of feasible resonators. \n\n"}
{"id": "1708.04527", "contents": "Title: The Trimmed Lasso: Sparsity and Robustness Abstract: Nonconvex penalty methods for sparse modeling in linear regression have been\na topic of fervent interest in recent years. Herein, we study a family of\nnonconvex penalty functions that we call the trimmed Lasso and that offers\nexact control over the desired level of sparsity of estimators. We analyze its\nstructural properties and in doing so show the following:\n  1) Drawing parallels between robust statistics and robust optimization, we\nshow that the trimmed-Lasso-regularized least squares problem can be viewed as\na generalized form of total least squares under a specific model of\nuncertainty. In contrast, this same model of uncertainty, viewed instead\nthrough a robust optimization lens, leads to the convex SLOPE (or OWL) penalty.\n  2) Further, in relating the trimmed Lasso to commonly used sparsity-inducing\npenalty functions, we provide a succinct characterization of the connection\nbetween trimmed-Lasso- like approaches and penalty functions that are\ncoordinate-wise separable, showing that the trimmed penalties subsume existing\ncoordinate-wise separable penalties, with strict containment in general.\n  3) Finally, we describe a variety of exact and heuristic algorithms, both\nexisting and new, for trimmed Lasso regularized estimation problems. We include\na comparison between the different approaches and an accompanying\nimplementation of the algorithms. \n\n"}
{"id": "1708.04783", "contents": "Title: Non-convex Conditional Gradient Sliding Abstract: We investigate a projection free method, namely conditional gradient sliding\non batched, stochastic and finite-sum non-convex problem. CGS is a smart\ncombination of Nesterov's accelerated gradient method and Frank-Wolfe (FW)\nmethod, and outperforms FW in the convex setting by saving gradient\ncomputations. However, the study of CGS in the non-convex setting is limited.\nIn this paper, we propose the non-convex conditional gradient sliding (NCGS)\nwhich surpasses the non-convex Frank-Wolfe method in batched, stochastic and\nfinite-sum setting. \n\n"}
{"id": "1708.05136", "contents": "Title: More Iterations per Second, Same Quality -- Why Asynchronous Algorithms\n  may Drastically Outperform Traditional Ones Abstract: In this paper, we consider the convergence of a very general\nasynchronous-parallel algorithm called ARock, that takes many well-known\nasynchronous algorithms as special cases (gradient descent, proximal gradient,\nDouglas Rachford, ADMM, etc.). In asynchronous-parallel algorithms, the\ncomputing nodes simply use the most recent information that they have access\nto, instead of waiting for a full update from all nodes in the system. This\nmeans that nodes do not have to waste time waiting for information, which can\nbe a major bottleneck, especially in distributed systems. When the system has\n$p$ nodes, asynchronous algorithms may complete $\\Theta(\\ln(p))$ more\niterations than synchronous algorithms in a given time period (\"more iterations\nper second\").\n  Although asynchronous algorithms may compute more iterations per second,\nthere is error associated with using outdated information. How many more\niterations in total are needed to compensate for this error is still an open\nquestion. The main results of this paper aim to answer this question. We prove,\nloosely, that as the size of the problem becomes large, the number of\nadditional iterations that asynchronous algorithms need becomes negligible\ncompared to the total number (\"same quality\" of the iterations). Taking these\nfacts together, our results provide solid evidence of the potential of\nasynchronous algorithms to vastly speed up certain distributed computations. \n\n"}
{"id": "1708.07311", "contents": "Title: Generalized maximum entropy estimation Abstract: We consider the problem of estimating a probability distribution that\nmaximizes the entropy while satisfying a finite number of moment constraints,\npossibly corrupted by noise. Based on duality of convex programming, we present\na novel approximation scheme using a smoothed fast gradient method that is\nequipped with explicit bounds on the approximation error. We further\ndemonstrate how the presented scheme can be used for approximating the chemical\nmaster equation through the zero-information moment closure method, and for an\napproximate dynamic programming approach in the context of constrained Markov\ndecision processes with uncountable state and action spaces. \n\n"}
{"id": "1708.07620", "contents": "Title: Fenchel Dual Gradient Methods for Distributed Convex Optimization over\n  Time-varying Networks Abstract: In the large collection of existing distributed algorithms for convex\nmulti-agent optimization, only a handful of them provide convergence rate\nguarantees on agent networks with time-varying topologies, which, however,\nrestrict the problem to be unconstrained. Motivated by this, we develop a\nfamily of distributed Fenchel dual gradient methods for solving constrained,\nstrongly convex but not necessarily smooth multi-agent optimization problems\nover time-varying undirected networks. The proposed algorithms are constructed\nbased on the application of weighted gradient methods to the Fenchel dual of\nthe multi-agent optimization problem, and can be implemented in a fully\ndecentralized fashion. We show that the proposed algorithms drive all the\nagents to both primal and dual optimality asymptotically under a minimal\nconnectivity condition and at sublinear rates under a standard connectivity\ncondition. Finally, the competent convergence performance of the distributed\nFenchel dual gradient methods is demonstrated via simulations. \n\n"}
{"id": "1709.01168", "contents": "Title: Factor Models with Real Data: a Robust Estimation of the Number of\n  Factors Abstract: Factor models are a very efficient way to describe high dimensional vectors\nof data in terms of a small number of common relevant factors. This problem,\nwhich is of fundamental importance in many disciplines, is usually reformulated\nin mathematical terms as follows. We are given the covariance matrix Sigma of\nthe available data. Sigma must be additively decomposed as the sum of two\npositive semidefinite matrices D and L: D | that accounts for the idiosyncratic\nnoise affecting the knowledge of each component of the available vector of data\n| must be diagonal and L must have the smallest possible rank in order to\ndescribe the available data in terms of the smallest possible number of\nindependent factors.\n  In practice, however, the matrix Sigma is never known and therefore it must\nbe estimated from the data so that only an approximation of Sigma is actually\navailable. This paper discusses the issues that arise from this uncertainty and\nprovides a strategy to deal with the problem of robustly estimating the number\nof factors. \n\n"}
{"id": "1709.03528", "contents": "Title: GIANT: Globally Improved Approximate Newton Method for Distributed\n  Optimization Abstract: For distributed computing environment, we consider the empirical risk\nminimization problem and propose a distributed and communication-efficient\nNewton-type optimization method. At every iteration, each worker locally finds\nan Approximate NewTon (ANT) direction, which is sent to the main driver. The\nmain driver, then, averages all the ANT directions received from workers to\nform a {\\it Globally Improved ANT} (GIANT) direction. GIANT is highly\ncommunication efficient and naturally exploits the trade-offs between local\ncomputations and global communications in that more local computations result\nin fewer overall rounds of communications. Theoretically, we show that GIANT\nenjoys an improved convergence rate as compared with first-order methods and\nexisting distributed Newton-type methods. Further, and in sharp contrast with\nmany existing distributed Newton-type methods, as well as popular first-order\nmethods, a highly advantageous practical feature of GIANT is that it only\ninvolves one tuning parameter. We conduct large-scale experiments on a computer\ncluster and, empirically, demonstrate the superior performance of GIANT. \n\n"}
{"id": "1709.03668", "contents": "Title: Branch-and-bound for biobjective mixed-integer linear programming Abstract: We present a generic branch-and-bound algorithm for finding all the Pareto\nsolutions of a biobjective mixed-integer linear program. The main contributions\nare new algorithms for obtaining dual bounds at a node, for checking node\nfathoming, presolve and duality gap measurement. Our branch-and-bound is\npredominantly a decision space search method since the branching is performed\non the decision variables, akin to single objective problems, although we also\nsometimes split gaps and branch in the objective space. The various algorithms\nare implemented using a data structure for storing Pareto sets. Computational\nexperiments are carried out on literature instances and also on a new set of\ninstances that we generate using the MIPLIB benchmark library for single\nobjective problems. We also perform comparisons against the triangle splitting\nmethod from literature, which is an objective space search algorithm. \n\n"}
{"id": "1709.04215", "contents": "Title: Long time behavior of the master equation in mean-field game theory Abstract: Mean Field Game (MFG) systems describe equilibrium configurations in games\nwith infinitely many interacting controllers. We are interested in the behavior\nof this system as the horizon becomes large, or as the discount factor tends to\n$0$. We show that, in the two cases, the asymptotic behavior of the Mean Field\nGame system is strongly related with the long time behavior of the so-called\nmaster equation and with the vanishing discount limit of the discounted master\nequation, respectively. Both equations are nonlinear transport equations in the\nspace of measures. We prove the existence of a solution to an ergodic master\nequation, towards which the time-dependent master equation converges as the\nhorizon becomes large, and towards which the discounted master equation\nconverges as the discount factor tends to $0$. The whole analysis is based on\nthe obtention of new estimates for the exponential rates of convergence of the\ntime-dependent MFG system and the discounted MFG system. \n\n"}
{"id": "1709.05071", "contents": "Title: A perturbation analysis of stochastic matrix Riccati diffusions Abstract: Matrix differential Riccati equations are central in filtering and optimal\ncontrol theory. The purpose of this article is to develop a perturbation theory\nfor a class of stochastic matrix Riccati diffusions. Diffusions of this type\narise, for example, in the analysis of ensemble Kalman-Bucy filters since they\ndescribe the flow of certain sample covariance estimates. In this context, the\nrandom perturbations come from the fluctuations of a mean field particle\ninterpretation of a class of nonlinear diffusions equipped with an interacting\nsample covariance matrix functional. The main purpose of this article is to\nderive non-asymptotic Taylor-type expansions of stochastic matrix Riccati flows\nwith respect to some perturbation parameter. These expansions rely on an\noriginal combination of stochastic differential analysis and nonlinear\nsemigroup techniques on matrix spaces. The results here quantify the\nfluctuation of the stochastic flow around the limiting deterministic Riccati\nequation, at any order. The convergence of the interacting sample covariance\nmatrices to the deterministic Riccati flow is proven as the number of particles\ntends to infinity. Also presented are refined moment estimates and sharp bias\nand variance estimates. These expansions are also used to deduce a functional\ncentral limit theorem at the level of the diffusion process in matrix spaces. \n\n"}
{"id": "1709.05150", "contents": "Title: Modeling correlated bursts by the bursty-get-burstier mechanism Abstract: Temporal correlations of time series or event sequences in natural and social\nphenomena have been characterized by power-law decaying autocorrelation\nfunctions with decaying exponent $\\gamma$. Such temporal correlations can be\nunderstood in terms of power-law distributed interevent times with exponent\n$\\alpha$, and/or correlations between interevent times. The latter, often\ncalled correlated bursts, has recently been studied by measuring power-law\ndistributed bursty trains with exponent $\\beta$. A scaling relation between\n$\\alpha$ and $\\gamma$ has been established for the uncorrelated interevent\ntimes, while little is known about the effects of correlated interevent times\non temporal correlations. In order to study these effects, we devise the\nbursty-get-burstier model for correlated bursts, by which one can tune the\ndegree of correlations between interevent times, while keeping the same\ninterevent time distribution. We numerically find that sufficiently strong\ncorrelations between interevent times could violate the scaling relation\nbetween $\\alpha$ and $\\gamma$ for the uncorrelated case. A non-trivial\ndependence of $\\gamma$ on $\\beta$ is also found for some range of $\\alpha$. The\nimplication of our results is discussed in terms of the hierarchical\norganization of bursty trains at various timescales. \n\n"}
{"id": "1709.05758", "contents": "Title: A Study of Piecewise Linear-Quadratic Programs Abstract: Motivated by a growing list of nontraditional statistical estimation problems\nof the piecewise kind, this paper provides a survey of known results\nsupplemented with new results for the class of piecewise linear-quadratic\nprograms. These are linearly constrained optimization problems with piecewise\nlinear-quadratic (PLQ) objective functions. Starting from a study of the\nrepresentation of such a function in terms of a family of elementary functions\nconsisting of squared affine functions, squared plus-composite-affine\nfunctions, and affine functions themselves, we summarize some local properties\nof a PLQ function in terms of their first and second-order directional\nderivatives. We extend some well-known necessary and sufficient second-order\nconditions for local optimality of a quadratic program to a PLQ program and\nprovide a dozen such equivalent conditions for strong, strict, and isolated\nlocal optimality, showing in particular that a PLQ program has the same\ncharacterizations for local minimality as a standard quadratic program. As a\nconsequence of one such condition, we show that the number of strong, strict,\nor isolated local minima of a PLQ program is finite; this result supplements a\nrecent result about the finite number of directional stationary objective\nvalues. Interestingly, these finiteness results can be uncovered by invoking a\nvery powerful property of subanalytic functions; our proof is fairly\nelementary, however. We discuss applications of PLQ programs in some modern\nstatistical estimation problems. These problems lead to a special class of\nunconstrained composite programs involving the non-differentiable\n$\\ell_1$-function, for which we show that the task of verifying the\nsecond-order stationary condition can be converted to the problem of checking\nthe copositivity of certain Schur complement on the nonnegative orthant. \n\n"}
{"id": "1709.05907", "contents": "Title: A Generalized Framework for Kullback-Leibler Markov Aggregation Abstract: This paper proposes an information-theoretic cost function for aggregating a\nMarkov chain via a (possibly stochastic) mapping. The cost function is\nmotivated by two objectives: 1) The process obtained by observing the Markov\nchain through the mapping should be close to a Markov chain, and 2) the\naggregated Markov chain should retain as much of the temporal dependence\nstructure of the original Markov chain as possible. We discuss properties of\nthis parameterized cost function and show that it contains the cost functions\npreviously proposed by Deng et al., Xu et al., and Geiger et al. as special\ncases. We moreover discuss these special cases providing a better understanding\nand highlighting potential shortcomings: For example, the cost function\nproposed by Geiger et al. is tightly connected to approximate probabilistic\nbisimulation, but leads to trivial solutions if optimized without\nregularization. We furthermore propose a simple heuristic to optimize our cost\nfunction for deterministic aggregations and illustrate its performance on a set\nof synthetic examples. \n\n"}
{"id": "1709.08740", "contents": "Title: On the error of a priori sampling: zero forcing sets and propagation\n  time Abstract: Zero forcing is an iterative process on a graph used to bound the maximum\nnullity. The process begins with select vertices as colored, and the remaining\nvertices can become colored under a specific color change rule. The goal is to\nfind a minimum set of vertices such that after iteratively applying the rule,\nall of the vertices become colored (i.e., a minimum zero forcing set). Of\nparticular interest is the propagation time of a chosen set which is the number\nof steps the rule must be applied in order to color all the vertices of a\ngraph.\n  We give a purely linear algebraic interpretation of zero forcing: Find a set\nof vertices $S$ such that for any weighted adjacency matrix $\\mathbf{A}$,\nwhenever $\\mathbf{Ax} = \\mathbf{0}$, the entirety of of $\\mathbf{x}$ can be\nrecovered using only $\\mathbf{x}_S$, the entries corresponding to $S$. The key\nhere is that $S$ must be chosen before $\\mathbf{A}$. In this light, we are able\nto give a linear algebraic interpretation of the propagation time: Any error in\n$\\mathbf{x}_S$ effects the error of $\\mathbf{x}$ exponentially in the\npropagation time. This error can be quantitatively measured using newly defined\nzero forcing-related parameters, the error polynomial vector and the variance\npolynomial vector. In this sense, the quality of two zero forcing sets can\nobjectively be compared even if the sets are the same size and their\npropagation time is the same. Examples and constructions are given. \n\n"}
{"id": "1709.10029", "contents": "Title: Sparse High-Dimensional Regression: Exact Scalable Algorithms and Phase\n  Transitions Abstract: We present a novel binary convex reformulation of the sparse regression\nproblem that constitutes a new duality perspective. We devise a new cutting\nplane method and provide evidence that it can solve to provable optimality the\nsparse regression problem for sample sizes n and number of regressors p in the\n100,000s, that is two orders of magnitude better than the current state of the\nart, in seconds. The ability to solve the problem for very high dimensions\nallows us to observe new phase transition phenomena. Contrary to traditional\ncomplexity theory which suggests that the difficulty of a problem increases\nwith problem size, the sparse regression problem has the property that as the\nnumber of samples $n$ increases the problem becomes easier in that the solution\nrecovers 100% of the true signal, and our approach solves the problem extremely\nfast (in fact faster than Lasso), while for small number of samples n, our\napproach takes a larger amount of time to solve the problem, but importantly\nthe optimal solution provides a statistically more relevant regressor. We argue\nthat our exact sparse regression approach presents a superior alternative over\nheuristic methods available at present. \n\n"}
{"id": "1709.10485", "contents": "Title: Designing Real-Time Prices to Reduce Load Variability with HVAC Abstract: Utilities use demand response to shift or reduce electricity usage of\nflexible loads, to better match electricity demand to power generation. A\ncommon mechanism is peak pricing (PP), where consumers pay reduced (increased)\nprices for electricity during periods of low (high) demand, and its simplicity\nallows consumers to understand how their consumption affects costs. However,\nnew consumer technologies like internet-connected smart thermostats simplify\nreal-time pricing (RP), because such devices can automate the tradeoff between\ncosts and consumption. These devices enable consumer choice under RP by\nabstracting this tradeoff into a question of quality of service (e.g., comfort)\nversus price. This paper uses a principal-agent framework to design PP and RP\nrates for heating, ventilation, and air-conditioning (HVAC) to address adverse\nselection due to variations in consumer comfort preferences. We formulate the\npricing problem as a stochastic bilevel program, and numerically solve it by\nreformulation as a mixed integer program (MIP). Last, we compare the\neffectiveness of different pricing schemes on reductions of peak load or load\nvariability. We find that PP pricing induces HVAC consumption to spike high\n(before), spike low (during), and spike high (after) the PP event, whereas RP\nachieves reductions in peak loads and load variability while preventing large\nspikes in electricity usage. \n\n"}
{"id": "1710.01580", "contents": "Title: On the minimization of quantum entropies under local constraints Abstract: This work is concerned with the minimization of quantum entropies under local\nconstraints of density, current, and energy. The problem arises in the work of\nDegond and Ringhofer about the derivation of quantum hydrodynamical models from\nfirst principles, and is an adaptation to the quantum setting of the moment\nclosure strategy by entropy minimization encountered in kinetic equations. The\nmain mathematical difficulty is the lack of compactness needed to recover the\nenergy constraint. We circumvent this issue by a monotonicity argument\ninvolving energy, temperature and entropy, that is inspired by some\nthermodynamical considerations. \n\n"}
{"id": "1710.03667", "contents": "Title: High-dimensional dynamics of generalization error in neural networks Abstract: We perform an average case analysis of the generalization dynamics of large\nneural networks trained using gradient descent. We study the\npractically-relevant \"high-dimensional\" regime where the number of free\nparameters in the network is on the order of or even larger than the number of\nexamples in the dataset. Using random matrix theory and exact solutions in\nlinear models, we derive the generalization error and training error dynamics\nof learning and analyze how they depend on the dimensionality of data and\nsignal to noise ratio of the learning problem. We find that the dynamics of\ngradient descent learning naturally protect against overtraining and\noverfitting in large networks. Overtraining is worst at intermediate network\nsizes, when the effective number of free parameters equals the number of\nsamples, and thus can be reduced by making a network smaller or larger.\nAdditionally, in the high-dimensional regime, low generalization error requires\nstarting with small initial weights. We then turn to non-linear neural\nnetworks, and show that making networks very large does not harm their\ngeneralization performance. On the contrary, it can in fact reduce\novertraining, even without early stopping or regularization of any sort. We\nidentify two novel phenomena underlying this behavior in overcomplete models:\nfirst, there is a frozen subspace of the weights in which no learning occurs\nunder gradient descent; and second, the statistical properties of the\nhigh-dimensional regime yield better-conditioned input correlations which\nprotect against overtraining. We demonstrate that naive application of\nworst-case theories such as Rademacher complexity are inaccurate in predicting\nthe generalization performance of deep neural networks, and derive an\nalternative bound which incorporates the frozen subspace and conditioning\neffects and qualitatively matches the behavior observed in simulation. \n\n"}
{"id": "1710.03695", "contents": "Title: Fast and Safe: Accelerated gradient methods with optimality certificates\n  and underestimate sequences Abstract: In this work we introduce the concept of an Underestimate Sequence (UES),\nwhich is motivated by Nesterov's estimate sequence. Our definition of a UES\nutilizes three sequences, one of which is a lower bound (or under-estimator) of\nthe objective function. The question of how to construct an appropriate\nsequence of lower bounds is addressed, and we present lower bounds for strongly\nconvex smooth functions and for strongly convex composite functions, which\nadhere to the UES framework. Further, we propose several first order methods\nfor minimizing strongly convex functions in both the smooth and composite\ncases. The algorithms, based on efficiently updating lower bounds on the\nobjective functions, have natural stopping conditions that provide the user\nwith a certificate of optimality. Convergence of all algorithms is guaranteed\nthrough the UES framework, and we show that all presented algorithms converge\nlinearly, with the accelerated variants enjoying the optimal linear rate of\nconvergence. \n\n"}
{"id": "1710.04062", "contents": "Title: Decentralized Online Learning with Kernels Abstract: We consider multi-agent stochastic optimization problems over reproducing\nkernel Hilbert spaces (RKHS). In this setting, a network of interconnected\nagents aims to learn decision functions, i.e., nonlinear statistical models,\nthat are optimal in terms of a global convex functional that aggregates data\nacross the network, with only access to locally and sequentially observed\nsamples. We propose solving this problem by allowing each agent to learn a\nlocal regression function while enforcing consensus constraints. We use a\npenalized variant of functional stochastic gradient descent operating\nsimultaneously with low-dimensional subspace projections. These subspaces are\nconstructed greedily by applying orthogonal matching pursuit to the sequence of\nkernel dictionaries and weights. By tuning the projection-induced bias, we\npropose an algorithm that allows for each individual agent to learn, based upon\nits locally observed data stream and message passing with its neighbors only, a\nregression function that is close to the globally optimal regression function.\nThat is, we establish that with constant step-size selections agents' functions\nconverge to a neighborhood of the globally optimal one while satisfying the\nconsensus constraints as the penalty parameter is increased. Moreover, the\ncomplexity of the learned regression functions is guaranteed to remain finite.\nOn both multi-class kernel logistic regression and multi-class kernel support\nvector classification with data generated from class-dependent Gaussian mixture\nmodels, we observe stable function estimation and state of the art performance\nfor distributed online multi-class classification. Experiments on the Brodatz\ntextures further substantiate the empirical validity of this approach. \n\n"}
{"id": "1710.04073", "contents": "Title: Stream Graphs and Link Streams for the Modeling of Interactions over\n  Time Abstract: Graph theory provides a language for studying the structure of relations, and\nit is often used to study interactions over time too. However, it poorly\ncaptures the both temporal and structural nature of interactions, that calls\nfor a dedicated formalism. In this paper, we generalize graph concepts in order\nto cope with both aspects in a consistent way. We start with elementary\nconcepts like density, clusters, or paths, and derive from them more advanced\nconcepts like cliques, degrees, clustering coefficients, or connected\ncomponents. We obtain a language to directly deal with interactions over time,\nsimilar to the language provided by graphs to deal with relations. This\nformalism is self-consistent: usual relations between different concepts are\npreserved. It is also consistent with graph theory: graph concepts are special\ncases of the ones we introduce. This makes it easy to generalize higher-level\nobjects such as quotient graphs, line graphs, k-cores, and centralities. This\npaper also considers discrete versus continuous time assumptions, instantaneous\nlinks, and extensions to more complex cases. \n\n"}
{"id": "1710.05133", "contents": "Title: Tracking Moving Agents via Inexact Online Gradient Descent Algorithm Abstract: Multi-agent systems are being increasingly deployed in challenging\nenvironments for performing complex tasks such as multi-target tracking,\nsearch-and-rescue, and intrusion detection. Notwithstanding the computational\nlimitations of individual robots, such systems rely on collaboration to sense\nand react to the environment. This paper formulates the generic target tracking\nproblem as a time-varying optimization problem and puts forth an inexact online\ngradient descent method for solving it sequentially. The performance of the\nproposed algorithm is studied by characterizing its dynamic regret, a notion\ncommon to the online learning literature. Building upon the existing results,\nwe provide improved regret rates that not only allow non-strongly convex costs\nbut also explicating the role of the cumulative gradient error. Two distinct\nclasses of problems are considered: one in which the objective function adheres\nto a quadratic growth condition, and another where the objective function is\nconvex but the variable belongs to a compact domain. For both cases, results\nare developed while allowing the error to be either adversarial or arising from\na white noise process. Further, the generality of the proposed framework is\ndemonstrated by developing online variants of existing stochastic gradient\nalgorithms and interpreting them as special cases of the proposed inexact\ngradient method. The efficacy of the proposed inexact gradient framework is\nestablished on a multi-agent multi-target tracking problem, while its\nflexibility is exemplified by generating online movie recommendations for\nMovielens $10$M dataset. \n\n"}
{"id": "1710.05521", "contents": "Title: On the Hybrid Minimum Principle Abstract: The Hybrid Minimum Principle (HMP) is established for the optimal control of\ndeterministic hybrid systems with both autonomous and controlled switchings and\njumps where state jumps at the switching instants are permitted to be\naccompanied by changes in the dimension of the state space. First order\nvariational analysis is performed via the needle variation methodology and the\nnecessary optimality conditions are established in the form of the HMP. A\nfeature of special interest in this work is the explicit presentations of\nboundary conditions on the Hamiltonians and the adjoint processes before and\nafter switchings and jumps. In addition to an analytic example, the HMP results\nare illustrated for the optimal control of an electric vehicle with\ntransmission, where the modelling of the powertrain requires the consideration\nof both autonomous and controlled switchings accompanied by dimension changes. \n\n"}
{"id": "1710.05533", "contents": "Title: Decentralized Charging Control of Electric Vehicles in Residential\n  Distribution Networks Abstract: Electric vehicle (EV) charging can negatively impact electric distribution\nnetworks by exceeding equipment thermal ratings and causing voltages to drop\nbelow standard ranges. In this paper, we develop a decentralized EV charging\ncontrol scheme to achieve \"valley-filling\" (i.e., flattening demand profile\nduring overnight charging), meanwhile meeting heterogeneous individual charging\nrequirements and satisfying distribution network constraints. The formulated\nproblem is an optimization problem with a non-separable objective function and\nstrongly coupled inequality constraints. We propose a novel shrunken\nprimal-dual subgradient (SPDS) algorithm to support the decentralized control\nscheme, derive conditions guaranteeing its convergence, and verify its efficacy\nand convergence with a representative distribution network model. \n\n"}
{"id": "1710.05787", "contents": "Title: Information loss under coarse-graining: a geometric approach Abstract: We use information geometry, in which the local distance between models\nmeasures their distinguishability from data, to quantify the flow of\ninformation under the renormalization group. We show that information about\nrelevant parameters is preserved, with distances along relevant directions\nmaintained under flow. By contrast, irrelevant parameters become less\ndistinguishable under the flow, with distances along irrelevant directions\ncontracting according to renormalization group exponents. We develop a\ncovariant formalism to understand the contraction of the model manifold. We\nthen apply our tools to understand the emergence of the diffusion equation and\nmore general statistical systems described by a free energy. Our results give\nan information-theoretic justification of universality in terms of the flow of\nthe model manifold under coarse-graining. \n\n"}
{"id": "1710.06068", "contents": "Title: Data analysis recipes: Using Markov Chain Monte Carlo Abstract: Markov Chain Monte Carlo (MCMC) methods for sampling probability density\nfunctions (combined with abundant computational resources) have transformed the\nsciences, especially in performing probabilistic inferences, or fitting models\nto data. In this primarily pedagogical contribution, we give a brief overview\nof the most basic MCMC method and some practical advice for the use of MCMC in\nreal inference problems. We give advice on method choice, tuning for\nperformance, methods for initialization, tests of convergence, troubleshooting,\nand use of the chain output to produce or report parameter estimates with\nassociated uncertainties. We argue that autocorrelation time is the most\nimportant test for convergence, as it directly connects to the uncertainty on\nthe sampling estimate of any quantity of interest. We emphasize that sampling\nis a method for doing integrals; this guides our thinking about how MCMC output\nis best used. \n\n"}
{"id": "1710.06185", "contents": "Title: Accretion-induced spin-wandering effects on the neutron star in Scorpius\n  X-1: Implications for continuous gravitational wave searches Abstract: The LIGO's discovery of binary black hole mergers has opened up a new era of\ntransient gravitational wave astronomy. The potential detection of\ngravitational radiation from another class of astronomical objects, rapidly\nspinning non-axisymmetric neutron stars, would constitute a new area of\ngravitational wave astronomy. Scorpius X-1 (Sco X-1) is one of the most\npromising sources of continuous gravitational radiation to be detected with\npresent-generation ground-based gravitational wave detectors, such as Advanced\nLIGO and Advanced Virgo. As the sensitivity of these detectors improve in the\ncoming years, so will power of the search algorithms being used to find\ngravitational wave signals. Those searches will still require integation over\nnearly year long observational spans to detect the incredibly weak signals from\nrotating neutron stars. For low mass X-ray binaries such as Sco X-1 this\ndifficult task is compounded by neutron star \"spin wandering\" caused by\nstochastic accretion fluctuations. In this paper, we analyze X-ray data from\nthe RXTE satellite to infer the fluctuating torque on the neutron star in Sco\nX-1. We then perform a large-scale simulation to quantify the statistical\nproperties of spin-wandering effects on the gravitational wave signal frequency\nand phase evolution. We find that there are a broad range of expected maximum\nlevels of frequency wandering corresponding to maximum drifts of between 0.3-50\n{\\mu}Hz/sec over a year at 99% confidence. These results can be cast in terms\nof the maximum allowed length of a coherent signal model neglecting\nspin-wandering effects as ranging between 5-80 days. This study is designed to\nguide the development and evaluation of Sco X-1 search algorithms. \n\n"}
{"id": "1710.07410", "contents": "Title: Complete Facial Reduction in One Step for Spectrahedra Abstract: A spectrahedron is the feasible set of a semidefinite program, SDP, i.e., the\nintersection of an affine set with the positive semidefinite cone. While strict\nfeasibility is a generic property for random problems, there are many classes\nof problems where strict feasibility fails and this means that strong duality\ncan fail as well. If the minimal face containing the spectrahedron is known,\nthe SDPcan easily be transformed into an equivalent problem where strict\nfeasibility holds and thus strong duality follows as well. The minimal face is\nfully characterized by the range or nullspace of any of the matrices in its\nrelative interior. Obtaining such a matrix may require many facial reduction\nsteps and is currently not known to be a tractable problem for spectrahedra\nwith singularity degree greater than one. We propose a single parametric\noptimization problem with a resulting type of central path and prove that the\noptimal solution is unique and in the relative interior of the spectrahedron.\nNumerical tests illustrate the efficacy of our approach and its usefulness in\nregularizing SDPs. \n\n"}
{"id": "1710.07737", "contents": "Title: Dynamic mode decomposition for compressive system identification Abstract: Dynamic mode decomposition has emerged as a leading technique to identify\nspatiotemporal coherent structures from high-dimensional data, benefiting from\na strong connection to nonlinear dynamical systems via the Koopman operator. In\nthis work, we integrate and unify two recent innovations that extend DMD to\nsystems with actuation [Proctor et al., 2016] and systems with heavily\nsubsampled measurements [Brunton et al., 2015]. When combined, these methods\nyield a novel framework for compressive system identification [code is publicly\navailable at: https://github.com/zhbai/cDMDc]. It is possible to identify a\nlow-order model from limited input-output data and reconstruct the associated\nfull-state dynamic modes with compressed sensing, adding interpretability to\nthe state of the reduced-order model. Moreover, when full-state data is\navailable, it is possible to dramatically accelerate downstream computations by\nfirst compressing the data. We demonstrate this unified framework on two model\nsystems, investigating the effects of sensor noise, different types of\nmeasurements (e.g., point sensors, Gaussian random projections, etc.),\ncompression ratios, and different choices of actuation (e.g., localized,\nbroadband, etc.). In the first example, we explore this architecture on a test\nsystem with known low-rank dynamics and an artificially inflated state\ndimension. The second example consists of a real-world engineering application\ngiven by the fluid flow past a pitching airfoil at low Reynolds number. This\nexample provides a challenging and realistic test-case for the proposed method,\nand results demonstrate that the dominant coherent structures are well\ncharacterized despite actuation and heavily subsampled data. \n\n"}
{"id": "1710.08005", "contents": "Title: Smart \"Predict, then Optimize\" Abstract: Many real-world analytics problems involve two significant challenges:\nprediction and optimization. Due to the typically complex nature of each\nchallenge, the standard paradigm is predict-then-optimize. By and large,\nmachine learning tools are intended to minimize prediction error and do not\naccount for how the predictions will be used in the downstream optimization\nproblem. In contrast, we propose a new and very general framework, called Smart\n\"Predict, then Optimize\" (SPO), which directly leverages the optimization\nproblem structure, i.e., its objective and constraints, for designing better\nprediction models. A key component of our framework is the SPO loss function\nwhich measures the decision error induced by a prediction.\n  Training a prediction model with respect to the SPO loss is computationally\nchallenging, and thus we derive, using duality theory, a convex surrogate loss\nfunction which we call the SPO+ loss. Most importantly, we prove that the SPO+\nloss is statistically consistent with respect to the SPO loss under mild\nconditions. Our SPO+ loss function can tractably handle any polyhedral, convex,\nor even mixed-integer optimization problem with a linear objective. Numerical\nexperiments on shortest path and portfolio optimization problems show that the\nSPO framework can lead to significant improvement under the\npredict-then-optimize paradigm, in particular when the prediction model being\ntrained is misspecified. We find that linear models trained using SPO+ loss\ntend to dominate random forest algorithms, even when the ground truth is highly\nnonlinear. \n\n"}
{"id": "1710.09351", "contents": "Title: A scaling law from discrete to continuous solutions of channel capacity\n  problems in the low-noise limit Abstract: An analog communication channel typically achieves its full capacity when the\ndistribution of inputs is discrete, composed of just K symbols, such as voltage\nlevels or wavelengths. As the effective noise level goes to zero, for example\nby sending the same message multiple times, it is known that optimal codes\nbecome continuous. Here we derive a scaling law for the optimal number of\nsymbols in this limit, finding a novel rational scaling exponent. The number of\nsymbols in the optimal code grows as $\\log K \\sim I^{4/3}$, where the channel\ncapacity I increases with decreasing noise. The same scaling applies to other\nproblems equivalent to maximizing channel capacity over a continuous\ndistribution. \n\n"}
{"id": "1710.09876", "contents": "Title: Computing the Line Index of Balance Using Integer Programming\n  Optimisation Abstract: An important measure of signed graphs is the line index of balance which has\nseveral applications in many fields. However, this graph-theoretic measure was\nunderused for decades because of the inherent complexity in its computation\nwhich is closely related to solving NP-hard graph optimisation problems like\nMAXCUT. We develop new quadratic and linear programming models to compute the\nline index of balance exactly. Using the Gurobi integer programming\noptimisation solver, we evaluate the line index of balance on real-world and\nsynthetic datasets. The synthetic data involves Erd\\H{o}s-R\\'{e}nyi graphs,\nBarab\\'{a}si-Albert graphs, and specially structured random graphs. We also use\nwell known datasets from the sociology literature, such as signed graphs\ninferred from students' choice and rejection as well as datasets from the\nbiology literature including gene regulatory networks. The results show that\nexact values of the line index of balance in relatively large signed graphs can\nbe efficiently computed using our suggested optimisation models. We find that\nmost real-world social networks and some biological networks have small line\nindex of balance which indicates that they are close to balanced. \n\n"}
{"id": "1710.09917", "contents": "Title: A De Giorgi Iteration-based Approach for the Establishment of ISS\n  Properties of a Class of Semi-linear Parabolic PDEs with Boundary and\n  In-domain Disturbances Abstract: This paper addresses input-to-state stability (ISS) properties with respect\nto boundary and in-domain disturbances for a class of semi-linear partial\ndifferential equations (PDEs) subject to Dirichlet boundary conditions. The\ndeveloped approach is a combination of the method of De Giorgi iteration and\nthe technique of Lyapunov functionals by adequately splitting the original\nproblem into two subsystems. The ISS in $L^\\infty$-norm for a 1-$D$ transport\nequation and the ISS in $L^2$-norm for Burgers' equation have been established\nusing this method. As an application of the main result for the 1-D transport\nequation, a study on ISS properties in $L^\\infty$-norm of a 1-D transport\nequation with anti-stable term under boundary feedback control is carried out.\nThis is the first time that the method of De Giorgi iteration is introduced in\nthe ISS theory for infinite dimensional systems, and the developed method can\nbe generalized for tackling some problems on multidimensional spatial domains\nand be applied to a wider class of nonlinear parabolic PDEs. \n\n"}
{"id": "1710.09997", "contents": "Title: Zeroth Order Nonconvex Multi-Agent Optimization over Networks Abstract: In this paper, we consider distributed optimization problems over a\nmulti-agent network, where each agent can only partially evaluate the objective\nfunction, and it is allowed to exchange messages with its immediate neighbors.\nDifferently from all existing works on distributed optimization, our focus is\ngiven to optimizing a class of non-convex problems, and under the challenging\nsetting where each agent can only access the zeroth-order information (i.e.,\nthe functional values) of its local functions. For different types of network\ntopologies such as undirected connected networks or star networks, we develop\nefficient distributed algorithms and rigorously analyze their convergence and\nrate of convergence (to the set of stationary solutions). Numerical results are\nprovided to demonstrate the efficiency of the proposed algorithms. \n\n"}
{"id": "1710.10016", "contents": "Title: Regularization via Mass Transportation Abstract: The goal of regression and classification methods in supervised learning is\nto minimize the empirical risk, that is, the expectation of some loss function\nquantifying the prediction error under the empirical distribution. When facing\nscarce training data, overfitting is typically mitigated by adding\nregularization terms to the objective that penalize hypothesis complexity. In\nthis paper we introduce new regularization techniques using ideas from\ndistributionally robust optimization, and we give new probabilistic\ninterpretations to existing techniques. Specifically, we propose to minimize\nthe worst-case expected loss, where the worst case is taken over the ball of\nall (continuous or discrete) distributions that have a bounded transportation\ndistance from the (discrete) empirical distribution. By choosing the radius of\nthis ball judiciously, we can guarantee that the worst-case expected loss\nprovides an upper confidence bound on the loss on test data, thus offering new\ngeneralization bounds. We prove that the resulting regularized learning\nproblems are tractable and can be tractably kernelized for many popular loss\nfunctions. We validate our theoretical out-of-sample guarantees through\nsimulated and empirical experiments. \n\n"}
{"id": "1710.10544", "contents": "Title: Hierarchical and Distributed Monitoring of Voltage Stability in\n  Distribution Networks Abstract: We consider the problem of quantifying and assessing the steady-state voltage\nstability in radial distribution networks. Our approach to the voltage\nstability problem is based on a local, approximate, and yet highly accurate\ncharacterization of the determinant of the Jacobian of the power flow equations\nparameterized according to the branch-flow model. The proposed determinant\napproximation allows us to construct a voltage stability index that can be\ncomputed in a fully distributed or in a hierarchical fashion, resulting in a\nscalable approach to the assessment of steady-state voltage stability. Finally,\nwe provide upper bounds for the approximation error and we numerically validate\nthe quality and the robustness of the proposed approximation with the IEEE\n123-bus test feeder. \n\n"}
{"id": "1710.11295", "contents": "Title: Optimal Control of Connected and Automated Vehicles at Roundabouts: An\n  Investigation in a Mixed-Traffic Environment Abstract: Connectivity and automation in vehicles provide the most intriguing\nopportunity for enabling users to better monitor transportation network\nconditions and make better operating decisions to improve safety and reduce\npollution, energy consumption, and travel delays. This paper investigates the\nimplications of optimally coordinating vehicles that are wirelessly connected\nto each other in roundabouts to achieve a smooth traffic flow. We apply an\noptimization framework and an analytical solution that allows optimal\ncoordination of vehicles for merging in such traffic scenario. The\neffectiveness of the proposed approach is validated through simulation and it\nis shown that fully coordinated vehicles reduce total travel time by 51% and\nfuel consumption by 35%. Furthermore, we study the influence of vehicle\ncoordination in a mixed-traffic environment and compare the network performance\nunder different market penetration rates of connected and automated vehicles\n(CAVs). For this particular study with near-capacity demand, due to extremely\nunstable traffic, the results show that even with high penetration of CAVs\n(e.g., 80%), travel time and fuel savings are much less than a network of CAVs. \n\n"}
{"id": "1710.11302", "contents": "Title: Stochastic Linear Quadratic Optimal Control with General Control Domain Abstract: This paper considers the stochastic linear quadratic optimal control problem\nin which the control domain is nonconvex. By the functional analysis and convex\nperturbation methods, we establish a novel maximum principle. The application\nof the proposed maximum principle is illustrated through a work-out example. \n\n"}
{"id": "1711.00946", "contents": "Title: Learning Linear Dynamical Systems via Spectral Filtering Abstract: We present an efficient and practical algorithm for the online prediction of\ndiscrete-time linear dynamical systems with a symmetric transition matrix. We\ncircumvent the non-convex optimization problem using improper learning:\ncarefully overparameterize the class of LDSs by a polylogarithmic factor, in\nexchange for convexity of the loss functions. From this arises a\npolynomial-time algorithm with a near-optimal regret guarantee, with an\nanalogous sample complexity bound for agnostic learning. Our algorithm is based\non a novel filtering technique, which may be of independent interest: we\nconvolve the time series with the eigenvectors of a certain Hankel matrix. \n\n"}
{"id": "1711.01330", "contents": "Title: Asymptotics of extreme statistics of escape time in 1,2 and\n  3-dimensional diffusions Abstract: The first of $N$ identical independently distributed (i.i.d.) Brownian\ntrajectories that arrives to a small target, sets the time scale of activation,\nwhich in general is much faster than the arrival to the target of only a single\ntrajectory. Analytical asymptotic expressions for the minimal time is\nnotoriously difficult to compute in general geometries. We derive here\nasymptotic laws for the probability density function of the first and second\narrival times of a large number of i.i.d. Brownian trajectories to a small\ntarget in 1,2, and 3 dimensions and study their range of validity by stochastic\nsimulations. The results are applied to activation of biochemical pathways in\ncellular transduction. \n\n"}
{"id": "1711.01761", "contents": "Title: AdaBatch: Efficient Gradient Aggregation Rules for Sequential and\n  Parallel Stochastic Gradient Methods Abstract: We study a new aggregation operator for gradients coming from a mini-batch\nfor stochastic gradient (SG) methods that allows a significant speed-up in the\ncase of sparse optimization problems. We call this method AdaBatch and it only\nrequires a few lines of code change compared to regular mini-batch SGD\nalgorithms. We provide a theoretical insight to understand how this new class\nof algorithms is performing and show that it is equivalent to an implicit\nper-coordinate rescaling of the gradients, similarly to what Adagrad methods\ncan do. In theory and in practice, this new aggregation allows to keep the same\nsample efficiency of SG methods while increasing the batch size.\nExperimentally, we also show that in the case of smooth convex optimization,\nour procedure can even obtain a better loss when increasing the batch size for\na fixed number of samples. We then apply this new algorithm to obtain a\nparallelizable stochastic gradient method that is synchronous but allows\nspeed-up on par with Hogwild! methods as convergence does not deteriorate with\nthe increase of the batch size. The same approach can be used to make\nmini-batch provably efficient for variance-reduced SG methods such as SVRG. \n\n"}
{"id": "1711.02798", "contents": "Title: Spatiotemporal pattern extraction by spectral analysis of vector-valued\n  observables Abstract: We present a data-driven framework for extracting complex spatiotemporal\npatterns generated by ergodic dynamical systems. Our approach, called\nVector-valued Spectral Analysis (VSA), is based on an eigendecomposition of a\nkernel integral operator acting on a Hilbert space of vector-valued observables\nof the system, taking values in a space of functions (scalar fields) on a\nspatial domain. This operator is constructed by combining aspects of the theory\nof operator-valued kernels for machine learning with delay-coordinate maps of\ndynamical systems. In contrast to conventional eigendecomposition techniques,\nwhich decompose the input data into pairs of temporal and spatial modes with a\nseparable, tensor product structure, the patterns recovered by VSA can be\nmanifestly non-separable, requiring only a modest number of modes to represent\nsignals with intermittency in both space and time. Moreover, the kernel\nconstruction naturally quotients out dynamical symmetries in the data, and\nexhibits an asymptotic commutativity property with the Koopman evolution\noperator of the system, enabling decomposition of multiscale signals into\ndynamically intrinsic patterns. Application of VSA to the Kuramoto-Sivashinsky\nmodel demonstrates significant performance gains in efficient and meaningful\ndecomposition over eigendecomposition techniques utilizing scalar-valued\nkernels. \n\n"}
{"id": "1711.03070", "contents": "Title: Curing Epidemics on Networks using a Polya Contagion Model Abstract: We study the curing of epidemics of a network contagion, which is modelled\nusing a variation of the classical Polya urn process that takes into account\nspatial infection among neighbouring nodes. We introduce several quantities for\nmeasuring the overall infection in the network and use them to formulate an\noptimal control problem for minimizing the average infection rate using limited\ncuring resources. We prove the feasibility of this problem under high curing\nbudgets by deriving conservative lower bounds on the amount of curing per node\nthat turns our measures of network infection into supermartingales. We also\nprovide a provably convergent gradient descent algorithm to find the allocation\nof curing under limited budgets. Motivated by the fact that this strategy is\ncomputationally expensive, we design a suit of heuristic methods that are\nlocally implementable and nearly as effective. Extensive simulations run on\nlargescale networks demonstrate the effectiveness of our proposed strategies. \n\n"}
{"id": "1711.03247", "contents": "Title: The nonsmooth landscape of phase retrieval Abstract: We consider a popular nonsmooth formulation of the real phase retrieval\nproblem. We show that under standard statistical assumptions, a simple\nsubgradient method converges linearly when initialized within a constant\nrelative distance of an optimal solution. Seeking to understand the\ndistribution of the stationary points of the problem, we complete the paper by\nproving that as the number of Gaussian measurements increases, the stationary\npoints converge to a codimension two set, at a controlled rate. Experiments on\nimage recovery problems illustrate the developed algorithm and theory. \n\n"}
{"id": "1711.03314", "contents": "Title: Perspectives on characteristics based curse-of-dimensionality-free\n  numerical approaches for solving Hamilton-Jacobi equations Abstract: This paper extends the considerations of the works [1, 2] regarding\ncurse-of-dimensionality-free numerical approaches to solve certain types of\nHamilton-Jacobi equations arising in optimal control problems, differential\ngames and elsewhere. A rigorous formulation and justification for the extended\nHopf-Lax formula of [2] is provided together with novel theoretical and\npractical discussions including useful recommendations. By using the method of\ncharacteristics, the solutions of some problem classes under\nconvexity/concavity conditions on Hamiltonians (in particular, the solutions of\nHamilton-Jacobi-Bellman equations in optimal control problems) are evaluated\nseparately at different initial positions. This allows for the avoidance of the\ncurse of dimensionality, as well as for choosing arbitrary computational\nregions. The corresponding feedback control strategies are obtained at selected\npositions without approximating the partial derivatives of the solutions. The\nresults of numerical simulations demonstrate the high potential of the proposed\ntechniques. It is also pointed out that, despite the indicated advantages, the\nrelated approaches still have a limited range of applicability, and their\nextensions to Hamilton-Jacobi-Isaacs equations in zero-sum two-player\ndifferential games are currently developed only for sufficiently narrow classes\nof control systems. That is why further extensions are worth investigating. \n\n"}
{"id": "1711.03573", "contents": "Title: Deep Neural Networks for Physics Analysis on low-level whole-detector\n  data at the LHC Abstract: There has been considerable recent activity applying deep convolutional\nneural nets (CNNs) to data from particle physics experiments. Current\napproaches on ATLAS/CMS have largely focussed on a subset of the calorimeter,\nand for identifying objects or particular particle types. We explore approaches\nthat use the entire calorimeter, combined with track information, for directly\nconducting physics analyses: i.e. classifying events as known-physics\nbackground or new-physics signals.\n  We use an existing RPV-Supersymmetry analysis as a case study and explore\nCNNs on multi-channel, high-resolution sparse images: applied on GPU and\nmulti-node CPU architectures (including Knights Landing (KNL) Xeon Phi nodes)\non the Cori supercomputer at NERSC. \n\n"}
{"id": "1711.03741", "contents": "Title: On a Class of Singular Stochastic Control Problems for Reflected\n  Diffusions Abstract: Reflected diffusions naturally arise in many problems from applications\nranging from economics and mathematical biology to queueing theory. In this\npaper we consider a class of infinite time-horizon singular stochastic control\nproblems for a general one-dimensional diffusion that is reflected at zero. We\nassume that exerting control leads to a state-dependent instantaneous reward,\nwhereas reflecting the diffusion at zero gives rise to a proportional cost with\nconstant marginal value. The aim is to maximize the total expected reward,\nminus the total expected cost of reflection. We show that depending on the\nproperties of the state-dependent instantaneous reward we can have\nqualitatively different kinds of optimal strategies. The techniques employed\nare those of stochastic control and of the theory of linear diffusions. \n\n"}
{"id": "1711.03963", "contents": "Title: Asynchronous Schemes for Stochastic and Misspecified Potential Games and\n  Nonconvex Optimization Abstract: The distributed computation of equilibria and optima has seen growing\ninterest in a broad collection of networked problems. We consider the\ncomputation of equilibria of convex stochastic Nash games characterized by a\npossibly nonconvex potential function. Our focus is on two classes of\nstochastic Nash games: (P1): A potential stochastic Nash game, in which each\nplayer solves a parameterized stochastic convex program; and (P2): A\nmisspecified generalization, where the player-specific stochastic program is\ncomplicated by a parametric misspecification. In both settings, exact proximal\nBR solutions are generally unavailable in finite time since they necessitate\nsolving parameterized stochastic programs. Consequently, we design two\nasynchronous inexact proximal BR schemes to solve the problems, where in each\niteration a single player is randomly chosen to compute an inexact proximal BR\nsolution with rivals' possibly outdated information. Yet, in the misspecified\nregime (P2), each player possesses an extra estimate of the misspecified\nparameter and updates its estimate by a projected stochastic gradient (SG)\nalgorithm. By Since any stationary point of the potential function is a Nash\nequilibrium of the associated game, we believe this paper is amongst the first\nones for stochastic nonconvex (but block convex) optimization problems equipped\nwith almost-sure convergence guarantees. These statements can be extended to\nallow for accommodating weighted potential games and generalized potential\ngames. Finally, we present preliminary numerics based on applying the proposed\nschemes to congestion control and Nash-Cournot games. \n\n"}
{"id": "1711.04734", "contents": "Title: Sample average approximation with heavier tails II: localization in\n  stochastic convex optimization and persistence results for the Lasso Abstract: ``Localization'' has proven to be a valuable tool in the Statistical Learning\nliterature as it allows sharp risk bounds in terms of the problem geometry.\nLocalized bounds seem to be much less exploited in the Stochastic Optimization\nliterature. In addition, there is an obvious interest in both communities in\nobtaining risk bounds that require weak moment assumptions or\n``heavier-tails''. In this work we use a localization toolbox to derive risk\nbounds in two specific applications. The first is in portfolio risk\nminimization with conditional value-at-risk constraints. We consider a setting\nwhere, among all assets with high returns, there is a portion of dimension $g$,\nunknown to the investor, that has significant less risk than the other\nremaining portion. Our rates for the SAA problem show that ``risk inflation'',\ncaused by a multiplicative factor, affects the statistical rate only via a term\nproportional to $g$. As the ``normalized risk'' increases, the contribution in\nthe rate from the extrinsic dimension diminishes while the dependence on $g$ is\nkept fixed. Localization is a key tool to show this property. As a second\napplication of our localization toolbox, we obtain sharp oracle inequalities\nfor least-squares estimators with a Lasso-type constraint under weak moment\nassumptions. One main consequence of these inequalities is to obtain\n\\emph{persistence}, as posed by Greenshtein and Ritov, with covariates having\nheavier tails. This gives improvements in prior work of Bartlett, Mendelson and\nNeeman. \n\n"}
{"id": "1711.05305", "contents": "Title: An Accelerated Communication-Efficient Primal-Dual Optimization\n  Framework for Structured Machine Learning Abstract: Distributed optimization algorithms are essential for training machine\nlearning models on very large-scale datasets. However, they often suffer from\ncommunication bottlenecks. Confronting this issue, a communication-efficient\nprimal-dual coordinate ascent framework (CoCoA) and its improved variant CoCoA+\nhave been proposed, achieving a convergence rate of $\\mathcal{O}(1/t)$ for\nsolving empirical risk minimization problems with Lipschitz continuous losses.\nIn this paper, an accelerated variant of CoCoA+ is proposed and shown to\npossess a convergence rate of $\\mathcal{O}(1/t^2)$ in terms of reducing\nsuboptimality. The analysis of this rate is also notable in that the\nconvergence rate bounds involve constants that, except in extreme cases, are\nsignificantly reduced compared to those previously provided for CoCoA+. The\nresults of numerical experiments are provided to show that acceleration can\nlead to significant performance gains. \n\n"}
{"id": "1711.06287", "contents": "Title: Accuracy of inference on the physics of binary evolution from\n  gravitational-wave observations Abstract: The properties of the population of merging binary black holes encode some of\nthe uncertain physics of the evolution of massive stars in binaries. The binary\nblack hole merger rate and chirp mass distribution are being measured by\nground-based gravitational-wave detectors. We consider isolated binary\nevolution and explore how accurately the physical model can be constrained with\nsuch observations by applying the Fisher information matrix to the merging\nblack hole population simulated with the rapid binary population synthesis code\nCOMPAS. We investigate variations in four COMPAS parameters: common envelope\nefficiency, kick velocity dispersion, and mass loss rates during the luminous\nblue variable and Wolf--Rayet stellar evolutionary phases. We find that 1000\nobservations would constrain these model parameters to a fractional accuracy of\na few percent. Given the empirically determined binary black hole merger rate,\nwe can expect gravitational-wave observations alone to place strong constraints\non the physics of stellar and binary evolution within a few years. \n\n"}
{"id": "1711.06952", "contents": "Title: Approximating geodesics via random points Abstract: Given a `cost' functional $F$ on paths $\\gamma$ in a domain\n$D\\subset\\mathbb{R}^d$, in the form $F(\\gamma) = \\int_0^1\nf(\\gamma(t),\\dot\\gamma(t))dt$, it is of interest to approximate its minimum\ncost and geodesic paths. Let $X_1,\\ldots, X_n$ be points drawn independently\nfrom $D$ according to a distribution with a density. Form a random geometric\ngraph on the points where $X_i$ and $X_j$ are connected when $0<|X_i -\nX_j|<\\epsilon$, and the length scale $\\epsilon=\\epsilon_n$ vanishes at a\nsuitable rate.\n  For a general class of functionals $F$, associated to Finsler and other\ndistances on $D$, using a probabilistic form of Gamma convergence, we show that\nthe minimum costs and geodesic paths, with respect to types of approximating\ndiscrete `cost' functionals, built from the random geometric graph, converge\nalmost surely in various senses to those corresponding to the continuum cost\n$F$, as the number of sample points diverges. In particular, the geodesic path\nconvergence shown appears to be among the first results of its kind. \n\n"}
{"id": "1711.06963", "contents": "Title: The Strength of Multi-row Aggregation Cuts for Sign-pattern Integer\n  Programs Abstract: In this paper, we study the strength of aggregation cuts for sign-pattern\ninteger programs (IPs). Sign-pattern IPs are a generalization of packing IPs\nand are of the form $\\{x\\in \\mathbb{Z}^n_+\\ | \\ Ax\\le b\\}$ where for a given\ncolumn $j$, $A_{ij}$ is either non-negative for all $i$ or non-positive for all\n$i$. Our first result is that the aggregation closure for such sign-pattern IPs\ncan be 2-approximated by the original 1-row closure. This generalizes a result\nfor packing IPs. On the other hand, unlike in the case of packing IPs, we show\nthat the multi-row aggregation closure cannot be well approximated by the\noriginal multi-row closure. Therefore for these classes of integer programs\ngeneral aggregated multi-row cutting planes can perform significantly better\nthan just looking at cuts from multiple original constraints. \n\n"}
{"id": "1711.07564", "contents": "Title: Unbiased Simulation for Optimizing Stochastic Function Compositions Abstract: In this paper, we introduce an unbiased gradient simulation algorithms for\nsolving convex optimization problem with stochastic function compositions. We\nshow that the unbiased gradient generated from the algorithm has finite\nvariance and finite expected computation cost. We then combined the unbiased\ngradient simulation with two variance reduced algorithms (namely SVRG and SCSG)\nand showed that the proposed optimization algorithms based on unbiased gradient\nsimulations exhibit satisfactory convergence properties. Specifically, in the\nSVRG case, the algorithm with simulated gradient can be shown to converge\nlinearly to optima in expectation and almost surely under strong convexity.\nFinally, for the numerical experiment,we applied the algorithms to two\nimportant cases of stochastic function compositions optimization: maximizing\nthe Cox's partial likelihood model and training conditional random fields. \n\n"}
{"id": "1711.08428", "contents": "Title: Computing return times or return periods with rare event algorithms Abstract: The average time between two occurrences of the same event, referred to as\nits return time (or return period), is a useful statistical concept for\npractical applications. For instance insurances or public agency may be\ninterested by the return time of a 10m flood of the Seine river in Paris.\nHowever, due to their scarcity, reliably estimating return times for rare\nevents is very difficult using either observational data or direct numerical\nsimulations. For rare events, an estimator for return times can be built from\nthe extrema of the observable on trajectory blocks. Here, we show that this\nestimator can be improved to remain accurate for return times of the order of\nthe block size. More importantly, we show that this approach can be generalised\nto estimate return times from numerical algorithms specifically designed to\nsample rare events. So far those algorithms often compute probabilities, rather\nthan return times. The approach we propose provides a computationally extremely\nefficient way to estimate numerically the return times of rare events for a\ndynamical system, gaining several orders of magnitude of computational costs.\nWe illustrate the method on two kinds of observables, instantaneous and\ntime-averaged, using two different rare event algorithms, for a simple\nstochastic process, the Ornstein-Uhlenbeck process. As an example of realistic\napplications to complex systems, we finally discuss extreme values of the drag\non an object in a turbulent flow. \n\n"}
{"id": "1711.09953", "contents": "Title: Online Stochastic Optimization of Networked Distributed Energy Resources Abstract: This paper investigates distributed control and incentive mechanisms to\ncoordinate distributed energy resources (DERs) with both continuous and\ndiscrete decision variables as well as device dynamics in distribution grids.\nWe formulate a multi-period social welfare maximization problem, and based on\nits convex relaxation propose a distributed stochastic dual gradient algorithm\nfor managing DERs. We further extend it to an online realtime setting with\ntime-varying operating conditions, asynchronous updates by devices, and\nfeedback being leveraged to account for nonlinear power flows as well as reduce\ncommunication overhead. The resulting algorithm provides a general online\nstochastic optimization algorithm for coordinating networked DERs with discrete\npower setpoints and dynamics to meet operational and economic objectives and\nconstraints. We characterize the convergence of the algorithm analytically and\nevaluate its performance numerically. \n\n"}
{"id": "1711.11459", "contents": "Title: A method to define the energy threshold depending on noise level for\n  rare event searches Abstract: Solid state detectors and cryogenic detectors are widely employed in rare\nevent searches, such as direct Dark Matter detection or Coherent Neutrino\nNucleus Scattering experiments. The excellent sensitivity and, consequently,\ntheir low energy thresholds are among the most appealing features of such\ndetectors. We present a method to quantify the lowest trigger threshold\nachievable as a function of the acceptable amount of noise events triggered for\nthe physics case under investigation. We then apply this novel method to\nexisting experimental and simulated data to validate the model we presented. \n\n"}
{"id": "1712.01536", "contents": "Title: Robust Optimization Approaches for the Design of an Electric Machine Abstract: In this paper two formulations for the robust optimization of the size of the\npermanent magnet in a synchronous machine are discussed. The optimization is\nconstrained by a partial differential equation to describe the electromagnetic\nbehavior of the machine. The need for a robust optimization procedure\noriginates from the fact that optimization parameters have deviations. The\nfirst approach, i.e., \\textcolor{red}{worst-case} optimization, makes use of\nlocal sensitivities. The second approach takes into account expectation values\nand standard deviations. The latter are associated with global sensitivities.\nThe geometry parametrization is elegantly handled thanks to the introduction of\nan affine decomposition. Since the stochastic quantities are determined by\ntools from uncertainty quantification (UQ) and thus require a lot of finite\nelement evaluations, model order reduction is used in order to increase the\nefficiency of the procedure. It is shown that both approaches are equivalent if\na linearization is carried out. \\textcolor{blue}{This finding is supported by\nthe application on an electric machine. The optimization algorithms used are\nsequential quadratic programming, particle swarm optimization and genetic\nalgorithm}. While both formulations reduce the size of the magnets, the UQ\nbased optimization approach is less pessimistic with respect to deviations and\nyields smaller magnets. \n\n"}
{"id": "1712.02426", "contents": "Title: Compressive Phase Retrieval via Reweighted Amplitude Flow Abstract: The problem of reconstructing a sparse signal vector from magnitude-only\nmeasurements (a.k.a., compressive phase retrieval), emerges naturally in\ndiverse applications, but it is NP-hard in general. Building on recent advances\nin nonconvex optimization, this paper puts forth a new algorithm that is termed\ncompressive reweighted amplitude flow and abbreviated as CRAF, for compressive\nphase retrieval. Specifically, CRAF operates in two stages. The first stage\nseeks a sparse initial guess via a new spectral procedure. In the second stage,\nCRAF implements a few hard thresholding based iterations using reweighted\ngradients. When there are sufficient measurements, CRAF provably recovers the\nunderlying signal vector exactly with high probability under suitable\nconditions. Moreover, its sample complexity coincides with that of the\nstate-of-the-art procedures. Finally, substantial simulated tests showcase\nremarkable performance of the new spectral initialization, as well as improved\nexact recovery relative to competing alternatives. \n\n"}
{"id": "1712.03888", "contents": "Title: A semi-implicit scheme based on Arrow-Hurwicz method for saddle point\n  problems Abstract: We search saddle points for a large class of convex-concave Lagrangian. A\ngeneralized explicit iterative scheme based on Arrow-Hurwicz method converges\nto a saddle point of the problem. We also propose in this work, a convergent\nsemi-implicit scheme in order to accelerate the convergence of the iterative\nprocess. Numerical experiments are provided for a nontrivial numerical problem\nmodeling an optimal shape problem of thin torsion rods. This semi-implicit\nscheme is figured out in practice robustly efficient in comparison with the\nexplicit one. \n\n"}
{"id": "1712.03913", "contents": "Title: A Non-Cooperative Game Approach to Autonomous Racing Abstract: We consider autonomous racing of two cars and present an approach to\nformulate racing decisions as a non-cooperative non-zero-sum game. We design\nthree different games where the players aim to fulfill static track constraints\nas well as avoid collision with each other; the latter constraint depends on\nthe combined actions of the two players. The difference between the games are\nthe collision constraints and the payoff. In the first game collision avoidance\nis only considered by the follower, and each player maximizes their own\nprogress towards the finish line. We show that, thanks to the sequential\nstructure of this game, equilibria can be computed through an efficient\nsequential maximization approach. Further, we show these actions, if feasible,\nare also a Stackelberg and Nash equilibrium in pure strategies of our second\ngame where both players consider the collision constraints. The payoff of our\nthird game is designed to promote blocking, by additionally rewarding the cars\nfor staying ahead at the end of the horizon. We show that this changes the\nStackelberg equilibrium, but has a minor influence on the Nash equilibria. For\nonline implementation, we propose to play the games in a moving horizon\nfashion, and discuss two methods for guaranteeing feasibility of the resulting\ncoupled repeated games. Finally, we study the performance of the proposed\napproaches in simulation for a set-up that replicates the miniature race car\ntested at the Automatic Control Laboratory of ETH Zurich. The simulation study\nshows that the presented games can successfully model different racing\nbehaviors and generate interesting racing situations. \n\n"}
{"id": "1712.04900", "contents": "Title: Approximate controllability for Navier--Stokes equations in\n  $\\mathrm{3D}$ rectangles under Lions boundary conditions Abstract: The $\\mathrm{3D}$ Navier--Stokes system, under Lions boundary conditions, is\nproven to be approximately controllable provided a suitable saturating set does\nexist. An explicit saturating set for $\\mathrm{3D}$ rectangles is given. \n\n"}
{"id": "1712.05654", "contents": "Title: Catalyst Acceleration for First-order Convex Optimization: from Theory\n  to Practice Abstract: We introduce a generic scheme for accelerating gradient-based optimization\nmethods in the sense of Nesterov. The approach, called Catalyst, builds upon\nthe inexact accelerated proximal point algorithm for minimizing a convex\nobjective function, and consists of approximately solving a sequence of\nwell-chosen auxiliary problems, leading to faster convergence. One of the keys\nto achieve acceleration in theory and in practice is to solve these\nsub-problems with appropriate accuracy by using the right stopping criterion\nand the right warm-start strategy. We give practical guidelines to use Catalyst\nand present a comprehensive analysis of its global complexity. We show that\nCatalyst applies to a large class of algorithms, including gradient descent,\nblock coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG,\nMISO/Finito, and their proximal variants. For all of these methods, we\nestablish faster rates using the Catalyst acceleration, for strongly convex and\nnon-strongly convex objectives. We conclude with extensive experiments showing\nthat acceleration is useful in practice, especially for ill-conditioned\nproblems. \n\n"}
{"id": "1712.07411", "contents": "Title: Optimization of stochastic lossy transport networks and applications to\n  power grids Abstract: Motivated by developments in renewable energy and smart grids, we formulate a\nstylized mathematical model of a transport network with stochastic load\nfluctuations. Using an affine control rule, we explore the trade-off between\nthe number of controllable resources in a lossy transport network and the\nperformance gain they yield in terms of expected power losses. Our results are\nexplicit and reveal the interaction between the level of flexibility, the\nintrinsic load uncertainty and the network structure. \n\n"}
{"id": "1712.08226", "contents": "Title: A Primal-Dual Method for Optimal Control and Trajectory Generation in\n  High-Dimensional Systems Abstract: Presented is a method for efficient computation of the Hamilton-Jacobi (HJ)\nequation for time-optimal control problems using the generalized Hopf formula.\nTypically, numerical methods to solve the HJ equation rely on a discrete grid\nof the solution space and exhibit exponential scaling with dimension. The\ngeneralized Hopf formula avoids the use of grids and numerical gradients by\nformulating an unconstrained convex optimization problem. The solution at each\npoint is completely independent, and allows a massively parallel implementation\nif solutions at multiple points are desired. This work presents a primal-dual\nmethod for efficient numeric solution and presents how the resulting optimal\ntrajectory can be generated directly from the solution of the Hopf formula,\nwithout further optimization. Examples presented have execution times on the\norder of milliseconds and experiments show computation scales approximately\npolynomial in dimension with very small high-order coefficients. \n\n"}
{"id": "1712.08559", "contents": "Title: An Approximate Shapley-Folkman Theorem Abstract: The Shapley-Folkman theorem shows that Minkowski averages of uniformly\nbounded sets tend to be convex when the number of terms in the sum becomes much\nlarger than the ambient dimension. In optimization, Aubin and Ekeland [1976]\nshow that this produces an a priori bound on the duality gap of separable\nnonconvex optimization problems involving finite sums. This bound is highly\nconservative and depends on unstable quantities, and we relax it in several\ndirections to show that non convexity can have a much milder impact on finite\nsum minimization problems such as empirical risk minimization and multi-task\nclassification. As a byproduct, we show a new version of Maurey's classical\napproximate Carath\\'eodory lemma where we sample a significant fraction of the\ncoefficients, without replacement, as well as a result on sampling constraints\nusing an approximate Helly theorem, both of independent interest. \n\n"}
{"id": "1712.09163", "contents": "Title: Computer Algebra Methods in Control Systems Abstract: As dynamic and control systems become more complex, relying purely on\nnumerical computations for systems analysis and design might become extremely\nexpensive or totally infeasible. Computer algebra can act as an enabler for\nanalysis and design of such complex systems. It also provides means for\ncharacterization of all solutions and studying them before realizing a\nparticular solution. This note provides a brief survey on some of the\napplications of symbolic computations in control systems analysis and design. \n\n"}
{"id": "1712.09498", "contents": "Title: A single potential governing convergence of conjugate gradient,\n  accelerated gradient and geometric descent Abstract: Nesterov's accelerated gradient (AG) method for minimizing a smooth strongly\nconvex function $f$ is known to reduce $f({\\bf x}_k)-f({\\bf x}^*)$ by a factor\nof $\\epsilon\\in(0,1)$ after $k=O(\\sqrt{L/\\ell}\\log(1/\\epsilon))$ iterations,\nwhere $\\ell,L$ are the two parameters of smooth strong convexity. Furthermore,\nit is known that this is the best possible complexity in the function-gradient\noracle model of computation. Modulo a line search, the geometric descent (GD)\nmethod of Bubeck, Lee and Singh has the same bound for this class of functions.\nThe method of linear conjugate gradients (CG) also satisfies the same\ncomplexity bound in the special case of strongly convex quadratic functions,\nbut in this special case it can be faster than the AG and GD methods.\n  Despite similarities in the algorithms and their asymptotic convergence\nrates, the conventional analysis of the running time of CG is mostly disjoint\nfrom that of AG and GD. The analyses of the AG and GD methods are also rather\ndistinct.\n  Our main result is analyses of the three methods that share several common\nthreads: all three analyses show a relationship to a certain \"idealized\nalgorithm\", all three establish the convergence rate through the use of the\nBubeck-Lee-Singh geometric lemma, and all three have the same potential that is\ncomputable at run-time and exhibits decrease by a factor of $1-\\sqrt{\\ell/L}$\nor better per iteration.\n  One application of these analyses is that they open the possibility of hybrid\nor intermediate algorithms. One such algorithm is proposed herein and is shown\nto perform well in computational tests. \n\n"}
{"id": "1712.09677", "contents": "Title: Momentum and Stochastic Momentum for Stochastic Gradient, Newton,\n  Proximal Point and Subspace Descent Methods Abstract: In this paper we study several classes of stochastic optimization algorithms\nenriched with heavy ball momentum. Among the methods studied are: stochastic\ngradient descent, stochastic Newton, stochastic proximal point and stochastic\ndual subspace ascent. This is the first time momentum variants of several of\nthese methods are studied. We choose to perform our analysis in a setting in\nwhich all of the above methods are equivalent. We prove global nonassymptotic\nlinear convergence rates for all methods and various measures of success,\nincluding primal function values, primal iterates (in L2 sense), and dual\nfunction values. We also show that the primal iterates converge at an\naccelerated linear rate in the L1 sense. This is the first time a linear rate\nis shown for the stochastic heavy ball method (i.e., stochastic gradient\ndescent method with momentum). Under somewhat weaker conditions, we establish a\nsublinear convergence rate for Cesaro averages of primal iterates. Moreover, we\npropose a novel concept, which we call stochastic momentum, aimed at decreasing\nthe cost of performing the momentum step. We prove linear convergence of\nseveral stochastic methods with stochastic momentum, and show that in some\nsparse data regimes and for sufficiently small momentum parameters, these\nmethods enjoy better overall complexity than methods with deterministic\nmomentum. Finally, we perform extensive numerical testing on artificial and\nreal datasets, including data coming from average consensus problems. \n\n"}
{"id": "1801.00480", "contents": "Title: The Cyclic Douglas-Rachford Algorithm with r-sets-Douglas-Rachford\n  Operators Abstract: The Douglas-Rachford (DR) algorithm is an iterative procedure that uses\nsequential reflections onto convex sets and which has become popular for convex\nfeasibility problems. In this paper we propose a structural generalization that\nallows to use $r$-sets-DR operators in a cyclic fashion. We prove convergence\nand present numerical illustrations of the potential advantage of such\noperators with $r>2$ over the classical $2$-sets-DR operators in a cyclic\nalgorithm. \n\n"}
{"id": "1801.01088", "contents": "Title: Convergence rates of Forward--Douglas--Rachford splitting method Abstract: Over the past years, operator splitting methods have become ubiquitous for\nnon-smooth optimization owing to their simplicity and efficiency. In this\npaper, we consider the Forward--Douglas--Rachford splitting method (FDR)\n[10,40], and study both global and local convergence rates of this method. For\nthe global rate, we establish an $o(1/k)$ convergence rate in terms of a\nBregman divergence suitably designed for the objective function. Moreover, when\nspecializing to the case of Forward--Backward splitting method, we show that\nconvergence rate of the objective function of the method is actually $o(1/k)$\nfor a large choice of the descent step-size. Then locally, based on the\nassumption that the non-smooth part of the optimization problem is partly\nsmooth, we establish local linear convergence of the method. More precisely, we\nshow that the sequence generated by FDR method first (i) identifies a smooth\nmanifold in a finite number of iteration, and then (ii) enters a local linear\nconvergence regime, which is for instance characterized in terms of the\nstructure of the underlying active smooth manifold. To exemplify the usefulness\nof the obtained result, we consider several concrete numerical experiments\narising from applicative fields including, for instance, signal/image\nprocessing, inverse problems and machine learning. \n\n"}
{"id": "1801.01221", "contents": "Title: Bounded-Velocity Stochastic Control for Dynamic Resource Allocation Abstract: We consider a general class of dynamic resource allocation problems within a\nstochastic optimal control framework. This class of problems arises in a wide\nvariety of applications, each of which intrinsically involves resources of\ndifferent types and demand with uncertainty and/or variability. The goal\ninvolves dynamically allocating capacity for every resource type in order to\nserve the uncertain/variable demand, modeled as Brownian motion, and maximize\nthe discounted expected net-benefit over an infinite time horizon based on the\nrewards and costs associated with the different resource types, subject to\nflexibility constraints on the rate of change of each type of resource\ncapacity. We derive the optimal control policy within a bounded-velocity\nstochastic control setting, which includes efficient and easily implementable\nalgorithms for governing the dynamic adjustments to resource allocation\ncapacities over time. Computational experiments investigate various issues of\nboth theoretical and practical interest, quantifying the benefits of our\napproach over recent alternative optimization approaches. \n\n"}
{"id": "1801.01780", "contents": "Title: Probabilistic max-plus schemes for solving Hamilton-Jacobi-Bellman\n  equations Abstract: We consider fully nonlinear Hamilton-Jacobi-Bellman equations associated to\ndiffusion control problems involving a finite set-valued (or switching) control\nand possibly a continuum-valued control. In previous works (Akian, Fodjo, 2016\nand 2017), we introduced a lower complexity probabilistic numerical algorithm\nfor such equations by combining max-plus and numerical probabilistic\napproaches. The max-plus approach is in the spirit of the one of McEneaney,\nKaise and Han (2011), and is based on the distributivity of monotone operators\nwith respect to suprema. The numerical probabilistic approach is in the spirit\nof the one proposed by Fahim, Touzi and Warin (2011). A difficulty of the\nlatter algorithm was in the critical constraints imposed on the Hamiltonian to\nensure the monotonicity of the scheme, hence the convergence of the algorithm.\nHere, we present new probabilistic schemes which are monotone under rather weak\nassumptions, and show error estimates for these schemes. These estimates will\nbe used in further works to study the probabilistic max-plus method. \n\n"}
{"id": "1801.03203", "contents": "Title: A global maximum principle for optimal control of general mean-field\n  forward-backward stochastic systems with jumps Abstract: In this paper we prove a necessary condition of the optimal control problem\nfor a class of general mean-field forward-backward stochastic systems with\njumps in the case where the diffusion coefficients depend on control, the\ncontrol set does not need to be convex, the coefficients of jump terms are\nindependent of control as well as the coefficients of mean-field backward\nstochastic differential equations depend on the joint law of $(X(t),Y(t))$. Two\nnew adjoint equations are brought in as well as several new generic estimates\nof their solutions are investigated for analysing the higher terms, especially,\nthose involving the expectation which come from the derivatives of the\ncoefficients with respect to the measure. Utilizing these subtle estimates, the\nsecond-order expansion of the cost functional, which is the key point to\nanalyse the necessary condition, is obtained, and whereafter the stochastic\nmaximum principle. \n\n"}
{"id": "1801.03520", "contents": "Title: Communication-Constrained Expansion Planning for Resilient Distribution\n  Systems Abstract: Distributed generation and remotely controlled switches have emerged as\nimportant technologies to improve the resiliency of distribution grids against\nextreme weather-related disturbances. Therefore it becomes impor- tant to study\nhow best to place them on the grid in order to meet a resiliency criteria,\nwhile minimizing costs and capturing their dependencies on the associated\ncommunication systems that sustains their distributed operations. This paper\nintroduces the Optimal Resilient Design Problem for Distribution and Communi-\ncation Systems (ORDPDC) to address this need. The ORDPDC is formulated as a\ntwo-stage stochastic mixed-integer program that captures the physical laws of\ndistribution systems, the communication connec- tivity of the smart grid\ncomponents, and a set of scenarios which specifies which components are\naffected by potential disasters. The paper proposes an exact branch-and-price\nalgorithm for the ORDPDC which features a strong lower bound and a variety of\nacceleration schemes to address degeneracy. The ORDPDC model and\nbranch-and-price algorithm were evaluated on a variety of test cases with\nvarying disaster inten- sities and network topologies. The results demonstrate\nthe significant impact of the network topologies on the expansion plans and\ncosts, as well as the computational benefits of the proposed approach. \n\n"}
{"id": "1801.03663", "contents": "Title: From Uncertainty Data to Robust Policies for Temporal Logic Planning Abstract: We consider the problem of synthesizing robust disturbance feedback policies\nfor systems performing complex tasks. We formulate the tasks as linear temporal\nlogic specifications and encode them into an optimization framework via\nmixed-integer constraints. Both the system dynamics and the specifications are\nknown but affected by uncertainty. The distribution of the uncertainty is\nunknown, however realizations can be obtained. We introduce a data-driven\napproach where the constraints are fulfilled for a set of realizations and\nprovide probabilistic generalization guarantees as a function of the number of\nconsidered realizations. We use separate chance constraints for the\nsatisfaction of the specification and operational constraints. This allows us\nto quantify their violation probabilities independently. We compute disturbance\nfeedback policies as solutions of mixed-integer linear or quadratic\noptimization problems. By using feedback we can exploit information of past\nrealizations and provide feasibility for a wider range of situations compared\nto static input sequences. We demonstrate the proposed method on two robust\nmotion-planning case studies for autonomous driving. \n\n"}
{"id": "1801.06366", "contents": "Title: Lyapunov stability of differential inclusions with Lipschitz Cusco\n  perturbations of maximal monotone operators Abstract: We give criteria for weak and strong invariant closed sets for differential\ninclusions given in $\\mathbb{R}^{n}$ and governed by Lipschitz Cusco\nperturbations of maximal monotone operators. Correspondingly, we provide\ndifferent characterizations for the\\ associated strong Lyapunov functions. The\nresulting conditions only depend on the data of the system. \n\n"}
{"id": "1801.06719", "contents": "Title: A Tutorial on Modeling and Analysis of Dynamic Social Networks. Part II Abstract: Recent years have witnessed a significant trend towards filling the gap\nbetween Social Network Analysis (SNA) and control theory. This trend was\nenabled by the introduction of new mathematical models describing dynamics of\nsocial groups, the development of algorithms and software for data analysis and\nthe tremendous progress in understanding complex networks and multi-agent\nsystems (MAS) dynamics. The aim of this tutorial is to highlight a novel\nchapter of control theory, dealing with dynamic models of social networks and\nprocesses over them, to the attention of the broad research community. In its\nfirst part [1], we have considered the most classical models of social\ndynamics, which have anticipated and to a great extent inspired the recent\nextensive studies on MAS and complex networks. This paper is the second part of\nthe tutorial, and it is focused on more recent models of social processes that\nhave been developed concurrently with MAS theory. Future perspectives of\ncontrol in social and techno-social systems are also discussed. \n\n"}
{"id": "1801.06768", "contents": "Title: Embedded Model Error Representation for Bayesian Model Calibration Abstract: Model error estimation remains one of the key challenges in uncertainty\nquantification and predictive science. For computational models of complex\nphysical systems, model error, also known as structural error or model\ninadequacy, is often the largest contributor to the overall predictive\nuncertainty. This work builds on a recently developed framework of embedded,\ninternal model correction, in order to represent and quantify structural\nerrors, together with model parameters, within a Bayesian inference context. We\nfocus specifically on a Polynomial Chaos representation with additive\nmodification of existing model parameters, enabling a non-intrusive procedure\nfor efficient approximate likelihood construction, model error estimation, and\ndisambiguation of model and data errors' contributions to predictive\nuncertainty. The framework is demonstrated on several synthetic examples, as\nwell as on a chemical ignition problem. \n\n"}
{"id": "1801.08413", "contents": "Title: Mean-field risk sensitive control and zero-sum games for Markov chains Abstract: We establish existence of controlled Markov chain of mean-field type with\nunbounded jump intensities by means of a fixed point argument using the\nWasserstein distance. Using a Markov chain entropic backward SDE approach, we\nfurther suggest conditions for existence of an optimal control and a\nsaddle-point for respectively a control problem and a zero-sum differential\ngame associated with risk sensitive payoff functionals of mean-field type. \n\n"}
{"id": "1801.08946", "contents": "Title: Expected Precision of Europa Clipper Gravity Measurements Abstract: The primary gravity science objective of NASA's Clipper mission to Europa is\nto confirm the presence or absence of a global subsurface ocean beneath\nEuropa's Icy crust. Gravity field measurements obtained with a radio science\ninvestigation can reveal much about Europa's interior structure. Here, we\nconduct extensive simulations of the radio science measurements with the\nanticipated spacecraft trajectory and attitude (17F12v2) and assets on the\nspacecraft and the ground, including antenna orientations and beam patterns,\ntransmitter characteristics, and receiver noise figures. In addition to two-way\nDoppler measurements, we also include radar altimeter crossover range\nmeasurements. We concentrate on +/-2 hour intervals centered on the closest\napproach of each of the 46 flybys. Our covariance analyses reveal the precision\nwith which the tidal Love number k2, second-degree gravity coefficients C20 and\nC22, and higher-order gravity coefficients can be determined. The results\ndepend on the Deep Space Network (DSN) assets that are deployed to track the\nspacecraft. We find that some DSN allocations are sufficient to conclusively\nconfirm the presence or absence of a global ocean. Given adequate crossover\nrange performance, it is also possible to evaluate whether the ice shell is\nhydrostatic. \n\n"}
{"id": "1801.10218", "contents": "Title: A Framework for the Dynamic Programming Principle and\n  Martingale-generated Control Correspondences Abstract: We construct an abstract framework in which the dynamic programming principle\n(DPP) can be readily proven. It encompasses a broad range of common stochastic\ncontrol problems in the weak formulation, and deals with problems in the\n\"martingale formulation\" with particular ease. We give two illustrations;\nfirst, we establish the DPP for general controlled diffusions and show that\ntheir value functions are viscosity solutions of the associated\nHamilton-Jacobi-Bellman equations under minimal conditions. After that, we show\nhow to treat singular control on the example of the classical monotone-follower\nproblem. \n\n"}
{"id": "1802.00538", "contents": "Title: Decentralized Control of Stochastically Switched Linear System with\n  Unreliable Communication Abstract: We consider a networked control system (NCS) consisting of two plants, a\nglobal plant and a local plant, and two controllers, a global controller and a\nlocal controller. The global (resp. local) plant follows discrete-time\nstochastically switched linear dynamics with a continuous global (resp. local)\nstate and a discrete global (resp. local) mode. We assume that the state and\nmode of the global plant are observed by both controllers while the state and\nmode of the local plant are only observed by the local controller. The local\ncontroller can inform the global controller of the local plant's state and mode\nthrough an unreliable TCP-like communication channel where successful\ntransmissions are acknowledged. The objective of the controllers is to\ncooperatively minimize a modes-dependent quadratic cost over a finite time\nhorizon. Following the method developed in [1] and [2], we construct a dynamic\nprogram based on common information and a decomposition of strategies, and use\nit to obtain explicit optimal strategies for the controllers. In the optimal\nstrategies, both controllers compute a common estimate of the local plant's\nstate. The global controller's action is linear in the state of the global\nplant and the common estimated state, and the local controller's action is\nlinear in the actual states of both plants and the common estimated state.\nFurthermore, the gain matrices for the global controller depend on the global\nmode and its observation about the local mode, while the gain matrices for the\nlocal controller depend on the actual modes of both plants and the global\ncontroller's observation about the local mode. \n\n"}
{"id": "1802.00615", "contents": "Title: Sparse control of Hegselmann-Krause models: Black hole and declustering Abstract: This paper elaborates control strategies to prevent clustering effects in\nopinion formation models. This is the exact opposite of numerous situations\nencountered in the literature where, on the contrary, one seeks controls\npromoting consensus. In order to promote declustering, instead of using the\nclassical variance that does not capture well the phenomenon of dispersion, we\nintroduce an entropy-type functional that is adapted to measuring pairwise\ndistances between agents. We then focus on a Hegselmann-Krause-type system and\ndesign declustering sparse controls both in finite-dimensional and kinetic\nmodels. We provide general conditions characterizing whether clustering can be\navoided as function of the initial data. Such results include the description\nof black holes (where complete collapse to consensus is not avoidable), safety\nzones (where the control can keep the system far from clustering), basins of\nattraction (attractive zones around the clustering set) and collapse prevention\n(when convergence to the clustering set can be avoided). \n\n"}
{"id": "1802.01499", "contents": "Title: An extreme function which is nonnegative and discontinuous everywhere Abstract: We consider Gomory and Johnson's infinite group model with a single row.\nValid inequalities for this model are expressed by valid functions and it has\nbeen recently shown that any valid function is dominated by some nonnegative\nvalid function, modulo the affine hull of the model. Within the set of\nnonnegative valid functions, extreme functions are the ones that cannot be\nexpressed as convex combinations of two distinct valid functions. In this paper\nwe construct an extreme function $\\pi:\\mathbb{R} \\to [0,1]$ whose graph is\ndense in $\\mathbb{R} \\times [0,1]$. Therefore, $\\pi$ is discontinuous\neverywhere. \n\n"}
{"id": "1802.01989", "contents": "Title: Tropical implementation of the Analytical Hierarchy Process decision\n  method Abstract: We apply methods and techniques of tropical optimization to develop a new\ntheoretical and computational framework for the implementation of the Analytic\nHierarchy Process in multi-criteria problems of rating alternatives from\npairwise comparison data. In this framework, we first consider the minimax\nChebyshev approximation of pairwise comparison matrices by consistent matrices\nin the logarithmic scale. Recasting this approximation problem as a problem of\ntropical pseudo-quadratic programming we then write out a closed-form solution\nto it. This solution might be either a unique score vector (up to a positive\nfactor) or a set of different score vectors. To handle the problem when the\nsolution is not unique, we develop tropical optimization techniques of\nmaximizing and minimizing the Hilbert seminorm to find those vectors from the\nsolution set that are the most and least differentiating between the\nalternatives with the highest and lowest scores, and thus are well\nrepresentative of the entire solution set. \n\n"}
{"id": "1802.03347", "contents": "Title: Acceleration and global convergence of a first-order primal--dual method\n  for nonconvex problems Abstract: The primal--dual hybrid gradient method (PDHGM, also known as the\nChambolle--Pock method) has proved very successful for convex optimization\nproblems involving linear operators arising in image processing and inverse\nproblems. In this paper, we analyze an extension to nonconvex problems that\narise if the operator is nonlinear. Based on the idea of testing, we derive new\nstep length parameter conditions for the convergence in infinite-dimensional\nHilbert spaces and provide acceleration rules for suitably (locally and/or\npartially) monotone problems. Importantly, we prove linear convergence rates as\nwell as global convergence in certain cases. We demonstrate the efficacy of\nthese step length rules for PDE-constrained optimization problems. \n\n"}
{"id": "1802.03966", "contents": "Title: From the difference of structures to the structure of the difference Abstract: When dealing with evolving or multi-dimensional complex systems, network\ntheory provides with elegant ways of describing their constituting components,\nthrough respectively time-varying and multi-layer complex networks.\nNevertheless, the analysis of how these components are related is still an open\nproblem. We here propose a framework for analysing the evolution of a (complex)\nsystem, by describing the structure created by the difference between multiple\nnetworks by means of the Information Content metric. As opposed to other\napproaches, as for instance the use of global overlap or entropies, the\nproposed one allows to understand if the observed changes are due to random\nnoise, or to structural (targeted) modifications. We validate the framework by\nmeans of sets of synthetic networks, as well as networks representing real\ntechnological, social and biological evolving systems. We further propose a way\nof reconstructing network correlograms, which allow to convert the system's\nevolution to the frequency domain. \n\n"}
{"id": "1802.04644", "contents": "Title: Price of Anarchy for Mean Field Games Abstract: The price of anarchy, originally introduced to quantify the inefficiency of\nselfish behavior in routing games, is extended to mean field games. The price\nof anarchy is defined as the ratio of a worst case social cost computed for a\nmean field game equilibrium to the optimal social cost as computed by a central\nplanner. We illustrate properties of such a price of anarchy on linear\nquadratic extended mean field games, for which explicit computations are\npossible. Various asymptotic behaviors of the price of anarchy are proved for\nlimiting behaviors of the coefficients in the model and numerics are presented. \n\n"}
{"id": "1802.04670", "contents": "Title: Equilibrium solutions of three player Kuhn poker with $N>3$ cards: A new\n  numerical method using regularization and arc-length continuation Abstract: We study the equilibrium solutions of three player Kuhn poker with $N>3$\ncards. We compute these solutions as a function of the initial pot size, $P$,\nusing a novel method based on regularizing the system of polynomial equations\nand inequalities that defines the solutions, and solving the resulting system\nof nonlinear, algebraic equations using a combination of Newton's method and\narc-length continuation. We find that the structure of the equilibrium solution\ncurve is very complex, even for games with a small number of cards. Standard\nthree player Kuhn poker, which is played with $N=4$ cards, is qualitatively\ndifferent from the game with $N>4$ cards because of the simplicity of the\nstructure of the value betting and bluffing ranges of each player. When $N>5$,\nwe find that there is a new type of equilibrium bet with midrange cards that\nacts as a bluff against one player and a value bet against the other. \n\n"}
{"id": "1802.05754", "contents": "Title: Extended Mean Field Control Problems: stochastic maximum principle and\n  transport perspective Abstract: We study Mean Field stochastic control problems where the cost function and\nthe state dynamics depend upon the joint distribution of the controlled state\nand the control process. We prove suitable versions of the Pontryagin\nstochastic maximum principle, both in necessary and in sufficient form, which\nextend the known conditions to this general framework. Furthermore, we suggest\na variational approach to study a weak formulation of these control problems.\nWe show a natural connection between this weak formulation and optimal\ntransport on path space, which inspires a novel discretization scheme. \n\n"}
{"id": "1802.06864", "contents": "Title: The NWRA Classification Infrastructure: Description and Extension to the\n  Discriminant Analysis Flare Forecasting System (DAFFS) Abstract: A classification infrastructure built upon Discriminant Analysis has been\ndeveloped at NorthWest Research Associates for examining the statistical\ndifferences between samples of two known populations. Originating to examine\nthe physical differences between flare-quiet and flare-imminent solar active\nregions, we describe herein some details of the infrastructure including:\nparametrization of large datasets, schemes for handling \"null\" and \"bad\" data\nin multi-parameter analysis, application of non-parametric multi-dimensional\nDiscriminant Analysis, an extension through Bayes' theorem to probabilistic\nclassification, and methods invoked for evaluating classifier success. The\nclassifier infrastructure is applicable to a wide range of scientific questions\nin solar physics. We demonstrate its application to the question of\ndistinguishing flare-imminent from flare-quiet solar active regions, updating\nresults from the original publications that were based on different data and\nmuch smaller sample sizes. Finally, as a demonstration of \"Research to\nOperations\" efforts in the space-weather forecasting context, we present the\nDiscriminant Analysis Flare Forecasting System (DAFFS), a near-real-time\noperationally-running solar flare forecasting tool that was developed from the\nresearch-directed infrastructure. \n\n"}
{"id": "1802.07712", "contents": "Title: Condition numbers of stochastic mean payoff games and what they say\n  about nonarchimedean semidefinite programming Abstract: Semidefinite programming can be considered over any real closed field,\nincluding fields of Puiseux series equipped with their nonarchimedean\nvaluation. Nonarchimedean semidefinite programs encode parametric families of\nclassical semidefinite programs, for sufficiently large values of the\nparameter. Recently, a correspondence has been established between\nnonarchimedean semidefinite programs and stochastic mean payoff games with\nperfect information. This correspondence relies on tropical geometry. It allows\none to solve generic nonarchimedean semidefinite feasibility problems, of large\nscale, by means of stochastic game algorithms. In this paper, we show that the\nmean payoff of these games can be interpreted as a condition number for the\ncorresponding nonarchimedean feasibility problems. This number measures how\nclose a feasible instance is from being infeasible, and vice versa. We show\nthat it coincides with the maximal radius of a ball in Hilbert's projective\nmetric, that is included in the feasible set. The geometric interpretation of\nthe condition number relies in particular on a duality theorem for tropical\nsemidefinite feasibility programs. Then, we bound the complexity of the\nfeasibility problem in terms of the condition number. We finally give explicit\nbounds for this condition number, in terms of the characteristics of the\nstochastic game. As a consequence, we show that the simplest algorithm to\ndecide whether a stochastic mean payoff game is winning, namely value\niteration, has a pseudopolynomial complexity when the number of random\npositions is fixed. \n\n"}
{"id": "1802.08376", "contents": "Title: LQG Control and Sensing Co-Design Abstract: We investigate a Linear-Quadratic-Gaussian (LQG) control and sensing\nco-design problem, where one jointly designs sensing and control policies. We\nfocus on the realistic case where the sensing design is selected among a finite\nset of available sensors, where each sensor is associated with a different cost\n(e.g., power consumption). We consider two dual problem instances:\nsensing-constrained LQG control, where one maximizes control performance\nsubject to a sensor cost budget, and minimum-sensing LQG control, where one\nminimizes sensor cost subject to performance constraints. We prove no\npolynomial time algorithm guarantees across all problem instances a constant\napproximation factor from the optimal. Nonetheless, we present the first\npolynomial time algorithms with per-instance suboptimality guarantees. To this\nend, we leverage a separation principle, that partially decouples the design of\nsensing and control. Then, we frame LQG co-design as the optimization of\napproximately supermodular set functions; we develop novel algorithms to solve\nthe problems; and we prove original results on the performance of the\nalgorithms, and establish connections between their suboptimality and\ncontrol-theoretic quantities. We conclude the paper by discussing two\napplications, namely, sensing-constrained formation control and\nresource-constrained robot navigation. \n\n"}
{"id": "1802.10432", "contents": "Title: Talking about Probability, Inference and Decisions. Part 1: The Witches\n  of Bayes Abstract: In October 2017 the Italian National Institute of Statistics (ISTAT), Italy's\nbody for official statistics, has published the book of fairy tales Le streghe\ndi Bayes (The witches of Bayes) written by ISTAT staff members with the\ncommendable aim of introducing statistical and probabilistic reasoning to\nchildren. In this paper the fairy tale which gives the name to the book is\nanalyzed in a dialog between three teachers with different background and\nexpertise. The outcomes are definitively discouraging, especially when the\nstory is compared to the appendix of the book, in which the teaching power of\nevery story is indeed explained (as a matter of fact, without the appendix the\nfairy tale of the witches seemed to be written with the purpose of make the\n'Bayesians', meant as the villagers from 'Bayes', ridiculous). In fact the\nfairy tale of the witches does not contain any Bayesian reasoning, the\nsuggested decision strategy is simply wrong and the story does not even seem to\nbe easily modifiable (besides the trivial correction of the decision strategy)\nin order to make it usable as a teaching tool. As it happens in real dialogues,\nbesides the fairy tale in question, the dialogue touches several issues somehow\nrelated to the story and concerning probability, inference, prediction and\ndecision making. The present paper is an indirect response to the invitation by\nthe ISBA bulletin to comment on the fairy tale. \n\n"}
{"id": "1803.00225", "contents": "Title: Global Convergence of Block Coordinate Descent in Deep Learning Abstract: Deep learning has aroused extensive attention due to its great empirical\nsuccess. The efficiency of the block coordinate descent (BCD) methods has been\nrecently demonstrated in deep neural network (DNN) training. However,\ntheoretical studies on their convergence properties are limited due to the\nhighly nonconvex nature of DNN training. In this paper, we aim at providing a\ngeneral methodology for provable convergence guarantees for this type of\nmethods. In particular, for most of the commonly used DNN training models\ninvolving both two- and three-splitting schemes, we establish the global\nconvergence to a critical point at a rate of ${\\cal O}(1/k)$, where $k$ is the\nnumber of iterations. The results extend to general loss functions which have\nLipschitz continuous gradients and deep residual networks (ResNets). Our key\ndevelopment adds several new elements to the Kurdyka-{\\L}ojasiewicz inequality\nframework that enables us to carry out the global convergence analysis of BCD\nin the general scenario of deep learning. \n\n"}
{"id": "1803.00301", "contents": "Title: (Sub)Optimal feedback control of mean field multi-population dynamics Abstract: We study a multiscale approach for the control of agent-based, two-population\nmodels. The control variable acts over one population of leaders, which\ninfluence the population of followers via the coupling generated by their\ninteraction. We cast a quadratic optimal control problem for the large-scale\nmicroscale model, which is approximated via a Boltzmann approach. By sampling\nsolutions of the optimal control problem associated to binary two-population\ndynamics, we generate sub-optimal control laws for the kinetic limit of the\nmulti-population model. We present numerical experiments related to opinion\ndynamics assessing the performance of the proposed control design. \n\n"}
{"id": "1803.01078", "contents": "Title: Learning in Wireless Control Systems over Non-Stationary Channels Abstract: This paper considers a set of multiple independent control systems that are\neach connected over a non-stationary wireless channel. The goal is to maximize\ncontrol performance over all the systems through the allocation of transmitting\npower within a fixed budget. This can be formulated as a constrained\noptimization problem examined using Lagrangian duality. By taking samples of\nthe unknown wireless channel at every time instance, the resulting problem\ntakes on the form of empirical risk minimization, a well-studied problem in\nmachine learning. Due to the non-stationarity of wireless channels, optimal\nallocations must be continuously learned and updated as the channel evolves.\nThe quadratic convergence property of Newton's method motivates its use in\nlearning approximately optimal power allocation policies over the sampled dual\nfunction as the channel evolves over time. Conditions are established under\nwhich Newton's method learns approximate solutions with a single update, and\nthe subsequent sub-optimality of the control problem is further characterized.\nNumerical simulations illustrate the near-optimal performance of the method and\nresulting stability on a wireless control problem. \n\n"}
{"id": "1803.01329", "contents": "Title: One Mirror Descent Algorithm for Convex Constrained Optimization\n  Problems with non-standard growth properties Abstract: The paper is devoted to a special Mirror Descent algorithm for problems of\nconvex minimization with functional constraints. The objective function may not\nsatisfy the Lipschitz condition, but it must necessarily have the\nLipshitz-continuous gradient. We assume, that the functional constraint can be\nnon-smooth, but satisfying the Lipschitz condition. In particular, such\nfunctionals appear in the well-known Truss Topology Design problem. Also we\nhave applied the technique of restarts in the mentioned version of Mirror\nDescent for strongly convex problems. Some estimations for a rate of\nconvergence are investigated for considered Mirror Descent algorithms. \n\n"}
{"id": "1803.02896", "contents": "Title: A Robustness Measure of Transient Stability under Operational\n  Constraints in Power Systems Abstract: The aggressive integration of distributed renewable sources is changing the\ndynamics of the electric power grid in an unexpected manner. As a result,\nmaintaining conventional performance specifications, such as transient\nstability, may not be sufficient to ensure its reliable operation in stressed\nconditions. In this paper, we introduce a novel criteria in transient stability\nwith consideration of operational constraints over frequency deviation and\nangular separation. In addition, we provide a robustness measure of the region\nof attraction, which can quantify the ability of the post-fault system to\nremain synchronized even under disturbances. To assess this new stability\nspecification, we adopt the notion of Input-to-State Stability (ISS) to the\ncontext of power systems and introduce a new class of convex Lyapunov\nfunctions, which will result in tractable convex-optimization-based stability\ncertificates. As a result, we are able to quantify the level of disturbance a\npower system can withstand while maintaining its safe operation. We illustrate\nthe introduced stability specification and certificate on the IEEE 9 bus\nsystem. \n\n"}
{"id": "1803.04144", "contents": "Title: Solving Markov decision processes for network-level post-hazard recovery\n  via simulation optimization and rollout Abstract: Computation of optimal recovery decisions for community resilience assurance\npost-hazard is a combinatorial decision-making problem under uncertainty. It\ninvolves solving a large-scale optimization problem, which is significantly\naggravated by the introduction of uncertainty. In this paper, we draw upon\nestablished tools from multiple research communities to provide an effective\nsolution to this challenging problem. We provide a stochastic model of damage\nto the water network (WN) within a testbed community following a severe\nearthquake and compute near-optimal recovery actions for restoration of the\nwater network. We formulate this stochastic decision-making problem as a Markov\nDecision Process (MDP), and solve it using a popular class of heuristic\nalgorithms known as rollout. A simulation-based representation of MDPs is\nutilized in conjunction with rollout and the Optimal Computing Budget\nAllocation (OCBA) algorithm to address the resulting stochastic simulation\noptimization problem. Our method employs non-myopic planning with efficient use\nof simulation budget. We show, through simulation results, that rollout fused\nwith OCBA performs competitively with respect to rollout with total equal\nallocation (TEA) at a meagre simulation budget of 5-10% of rollout with TEA,\nwhich is a crucial step towards addressing large-scale community recovery\nproblems following natural disasters. \n\n"}
{"id": "1803.04493", "contents": "Title: Particle Identification In Camera Image Sensors Using Computer Vision Abstract: We present a deep learning, computer vision algorithm constructed for the\npurposes of identifying and classifying charged particles in camera image\nsensors. We apply our algorithm to data collected by the Distributed Electronic\nCosmic-ray Observatory (DECO), a global network of smartphones that monitors\ncamera image sensors for the signatures of cosmic rays and other energetic\nparticles, such as those produced by radioactive decays. The algorithm, whose\ncore component is a convolutional neural network, achieves classification\nperformance comparable to human quality across four distinct DECO event\ntopologies. We apply our model to the entire DECO data set and determine a\nselection that achieves $\\ge90\\%$ purity for all event types. In particular, we\nestimate a purity of $95\\%$ when applied to cosmic-ray muons. The automated\nclassification is run on the public DECO data set in real time in order to\nprovide classified particle interaction images to users of the app and other\ninterested members of the public. \n\n"}
{"id": "1803.04577", "contents": "Title: Bayesian optimization for computationally extensive probability\n  distributions Abstract: An efficient method for finding a better maximizer of computationally\nextensive probability distributions is proposed on the basis of a Bayesian\noptimization technique. A key idea of the proposed method is to use extreme\nvalues of acquisition functions by Gaussian processes for the next training\nphase, which should be located near a local maximum or a global maximum of the\nprobability distribution. Our Bayesian optimization technique is applied to the\nposterior distribution in the effective physical model estimation, which is a\ncomputationally extensive probability distribution. Even when the number of\nsampling points on the posterior distributions is fixed to be small, the\nBayesian optimization provides a better maximizer of the posterior\ndistributions in comparison to those by the random search method, the steepest\ndescent method, or the Monte Carlo method. Furthermore, the Bayesian\noptimization improves the results efficiently by combining the steepest descent\nmethod and thus it is a powerful tool to search for a better maximizer of\ncomputationally extensive probability distributions. \n\n"}
{"id": "1803.05591", "contents": "Title: On the insufficiency of existing momentum schemes for Stochastic\n  Optimization Abstract: Momentum based stochastic gradient methods such as heavy ball (HB) and\nNesterov's accelerated gradient descent (NAG) method are widely used in\npractice for training deep networks and other supervised learning models, as\nthey often provide significant improvements over stochastic gradient descent\n(SGD). Rigorously speaking, \"fast gradient\" methods have provable improvements\nover gradient descent only for the deterministic case, where the gradients are\nexact. In the stochastic case, the popular explanations for their wide\napplicability is that when these fast gradient methods are applied in the\nstochastic case, they partially mimic their exact gradient counterparts,\nresulting in some practical gain. This work provides a counterpoint to this\nbelief by proving that there exist simple problem instances where these methods\ncannot outperform SGD despite the best setting of its parameters. These\nnegative problem instances are, in an informal sense, generic; they do not look\nlike carefully constructed pathological instances. These results suggest (along\nwith empirical evidence) that HB or NAG's practical performance gains are a\nby-product of mini-batching.\n  Furthermore, this work provides a viable (and provable) alternative, which,\non the same set of problem instances, significantly improves over HB, NAG, and\nSGD's performance. This algorithm, referred to as Accelerated Stochastic\nGradient Descent (ASGD), is a simple to implement stochastic algorithm, based\non a relatively less popular variant of Nesterov's Acceleration. Extensive\nempirical results in this paper show that ASGD has performance gains over HB,\nNAG, and SGD. \n\n"}
{"id": "1803.05804", "contents": "Title: Stability analysis by dynamic dissipation inequalities: On merging\n  frequency-domain techniques with time-domain conditions Abstract: In this paper we provide a complete link between dissipation theory and a\ncelebrated result on stability analysis with integral quadratic constraints.\nThis is achieved with a new stability characterization for feedback\ninterconnections based on the notion of finite-horizon integral quadratic\nconstraints with a terminal cost. As the main benefit, this opens up\nopportunities for guaranteeing constraints on the transient responses of\ntrajectories in feedback loops within absolute stability theory. For parametric\nrobustness, we show how to generate tight robustly invariant ellipsoids on the\nbasis of a classical frequency-domain stability test, with illustrations by a\nnumerical example. \n\n"}
{"id": "1803.06531", "contents": "Title: Topology Estimation using Graphical Models in Multi-Phase Power\n  Distribution Grids Abstract: Distribution grid is the medium and low voltage part of a large power system.\nStructurally, the majority of distribution networks operate radially, such that\nenergized lines form a collection of trees, i.e. forest, with a substation\nbeing at the root of any tree. The operational topology/forest may change from\ntime to time, however tracking these changes, even though important for the\ndistribution grid operation and control, is hindered by limited real-time\nmonitoring. This paper develops a learning framework to reconstruct radial\noperational structure of the distribution grid from synchronized voltage\nmeasurements in the grid subject to the exogenous fluctuations in nodal power\nconsumption. To detect operational lines our learning algorithm uses\nconditional independence tests for continuous random variables that is\napplicable to a wide class of probability distributions of the nodal\nconsumption and Gaussian injections in particular. Moreover, our algorithm\napplies to the practical case of unbalanced three-phase power flow. Algorithm\nperformance is validated on AC power flow simulations over IEEE distribution\ngrid test cases. \n\n"}
{"id": "1803.07725", "contents": "Title: Semidefinite Outer Approximation of the Backward Reachable Set of\n  Discrete-time Autonomous Polynomial Systems Abstract: We approximate the backward reachable set of discrete-time autonomous\npolynomial systems using the recently developed occupation measure approach. We\nformulate the problem as an infinite-dimensional linear programming (LP)\nproblem on measures and its dual on continuous functions. Then we approximate\nthe LP by a hierarchy of finite-dimensional semidefinite programming (SDP)\nprograms on moments of measures and their duals on sums-of-squares polynomials.\nFinally we solve the SDP's and obtain a sequence of outer approximations of the\nbackward reachable set. We demonstrate our approach on three dynamical systems.\nAs a special case, we also show how to approximate the preimage of a compact\nsemi-algebraic set under a polynomial map. \n\n"}
{"id": "1803.08711", "contents": "Title: The Price of Uncertainty: Chance-constrained OPF vs. In-hindsight OPF Abstract: The operation of power systems has become more challenging due to feed-in of\nvolatile renewable energy sources. Chance-constrained optimal power flow\n(ccOPF) is one possibility to explicitly consider volatility via probabilistic\nuncertainties resulting in mean-optimal feedback policies. These policies are\ncomputed before knowledge of the realization of the uncertainty is available.\nOn the other hand, the hypothetical case of computing the power injections\nknowing every realization beforehand---called in-hindsight OPF(hOPF)---cannot\nbe outperformed w.r.t. costs and constraint satisfaction. In this paper, we\ninvestigate how ccOPF feedback relates to the full-information hOPF. To this\nend, we introduce different dimensions of the price of uncertainty. Using mild\nassumptions on the uncertainty we present sufficient conditions when ccOPF is\nidentical to hOPF. We suggest using the total variational distance of\nprobability densities to quantify the performance gap of hOPF and ccOPF.\nFinally, we draw upon a tutorial example to illustrate our results. \n\n"}
{"id": "1803.08832", "contents": "Title: Golden Ratio Algorithms for Variational Inequalities Abstract: The paper presents a fully explicit algorithm for monotone variational\ninequalities. The method uses variable stepsizes that are computed using two\nprevious iterates as an approximation of the local Lipschitz constant without\nrunning a linesearch. Thus, each iteration of the method requires only one\nevaluation of a monotone operator $F$ and a proximal mapping $g$. The operator\n$F$ need not be Lipschitz-continuous, which also makes the algorithm\ninteresting in the area of composite minimization where one cannot use the\ndescent lemma. The method exhibits an ergodic $O(1/k)$ convergence rate and\n$R$-linear rate, if $F, g$ satisfy the error bound condition. We discuss\npossible applications of the method to fixed point problems. We discuss\npossible applications of the method to fixed point problems as well as its\ndifferent generalizations. \n\n"}
{"id": "1803.10350", "contents": "Title: Sobolev spaces with non-Muckenhoupt weights, fractional elliptic\n  operators, and applications Abstract: We propose a new variational model in weighted Sobolev spaces with\nnon-standard weights and applications to image processing. We show that these\nweights are, in general, not of Muckenhoupt type and therefore the classical\nanalysis tools may not apply. For special cases of the weights, the resulting\nvariational problem is known to be equivalent to the fractional Poisson\nproblem. The trace space for the weighted Sobolev space is identified to be\nembedded in a weighted $L^2$ space. We propose a finite element scheme to solve\nthe Euler-Lagrange equations, and for the image denoising application we\npropose an algorithm to identify the unknown weights. The approach is\nillustrated on several test problems and it yields better results when compared\nto the existing total variation techniques. \n\n"}
{"id": "1803.10653", "contents": "Title: QuipuNet: convolutional neural network for single-molecule nanopore\n  sensing Abstract: Nanopore sensing is a versatile technique for the analysis of molecules on\nthe single-molecule level. However, extracting information from data with\nestablished algorithms usually requires time-consuming checks by an experienced\nresearcher due to inherent variability of solid-state nanopores. Here, we\ndevelop a convolutional neural network (CNN) for the fully automated extraction\nof information from the time-series signals obtained by nanopore sensors. In\nour demonstration, we use a previously published dataset on multiplexed\nsingle-molecule protein sensing. The neural network learns to classify\ntranslocation events with greater accuracy than previously possible, while also\nincreasing the number of analysable events by a factor of five. Our results\ndemonstrate that deep learning can achieve significant improvements in single\nmolecule nanopore detection with potential applications in rapid diagnostics. \n\n"}
{"id": "1803.10856", "contents": "Title: Glassy Phase of Optimal Quantum Control Abstract: We study the problem of preparing a quantum many-body system from an initial\nto a target state by optimizing the fidelity over the family of bang-bang\nprotocols. We present compelling numerical evidence for a universal\nspin-glass-like transition controlled by the protocol time duration. The glassy\ncritical point is marked by a proliferation of protocols with close-to-optimal\nfidelity and with a true optimum that appears exponentially difficult to\nlocate. Using a machine learning (ML) inspired framework based on the manifold\nlearning algorithm t-SNE, we are able to visualize the geometry of the\nhigh-dimensional control landscape in an effective low-dimensional\nrepresentation. Across the transition, the control landscape features an\nexponential number of clusters separated by extensive barriers, which bears a\nstrong resemblance with replica symmetry breaking in spin glasses and random\nsatisfiability problems. We further show that the quantum control landscape\nmaps onto a disorder-free classical Ising model with frustrated nonlocal,\nmultibody interactions. Our work highlights an intricate but unexpected\nconnection between optimal quantum control and spin glass physics, and shows\nhow tools from ML can be used to visualize and understand glassy optimization\nlandscapes. \n\n"}
{"id": "1804.01189", "contents": "Title: Real-Time Prediction of the Duration of Distribution System Outages Abstract: This paper addresses the problem of predicting duration of unplanned power\noutages, using historical outage records to train a series of neural network\npredictors. The initial duration prediction is made based on environmental\nfactors, and it is updated based on incoming field reports using natural\nlanguage processing to automatically analyze the text. Experiments using 15\nyears of outage records show good initial results and improved performance\nleveraging text. Case studies show that the language processing identifies\nphrases that point to outage causes and repair steps. \n\n"}
{"id": "1804.02008", "contents": "Title: Deterministic guarantees for Burer-Monteiro factorizations of smooth\n  semidefinite programs Abstract: We consider semidefinite programs (SDPs) with equality constraints. The\nvariable to be optimized is a positive semidefinite matrix $X$ of size $n$.\nFollowing the Burer--Monteiro approach, we optimize a factor $Y$ of size $n\n\\times p$ instead, such that $X = YY^T$. This ensures positive semidefiniteness\nat no cost and can reduce the dimension of the problem if $p$ is small, but\nresults in a non-convex optimization problem with a quadratic cost function and\nquadratic equality constraints in $Y$. In this paper, we show that if the set\nof constraints on $Y$ regularly defines a smooth manifold, then, despite\nnon-convexity, first- and second-order necessary optimality conditions are also\nsufficient, provided $p$ is large enough. For smaller values of $p$, we show a\nsimilar result holds for almost all (linear) cost functions. Under those\nconditions, a global optimum $Y$ maps to a global optimum $X = YY^T$ of the\nSDP. We deduce old and new consequences for SDP relaxations of the generalized\neigenvector problem, the trust-region subproblem and quadratic optimization\nover several spheres, as well as for the Max-Cut and Orthogonal-Cut SDPs which\nare common relaxations in stochastic block modeling and synchronization of\nrotations. \n\n"}
{"id": "1804.02080", "contents": "Title: Optimal Voltage Phasor Regulation for Switching Actions in Unbalanced\n  Distribution Systems Abstract: The proliferation of phasor measurement units (PMUs) into electric power\ndistribution grids presents new opportunities for utility operators to manage\ndistribution systems more effectively. One potential application of PMU\nmeasurements is to facilitate distribution grid re-configuration. Given the\nincreasing amount of Distributed Energy Resource (DER) penetration into\ndistribution grids, in this work we formulate an Optimal Power Flow (OPF)\napproach that manages DER power injections to minimize the voltage phasor\ndifference between two nodes on a distribution network to enable efficient\nnetwork reconfiguration. In order to accomplish this, we develop a linear model\nthat relates voltage phase angles to real and reactive power flows in\nunbalanced distribution systems. Used in conjunction with existing\nlinearizations relating voltage magnitude differences to power flows, we\nformulate an OPF capable of minimizing voltage phasor differences across\ndifferent points in the network. In simulations, we explore the use of the\ndeveloped approach to minimize the phasor difference across switches to be\nopened or closed, thereby providing an opportunity to automate and increase the\nspeed of reconfigurations in unbalanced distribution grids. \n\n"}
{"id": "1804.02711", "contents": "Title: Decomposition and Completion of Sum-of-Squares Matrices Abstract: This paper introduces a notion of decomposition and completion of\nsum-of-squares (SOS) matrices. We show that a subset of sparse SOS matrices\nwith chordal sparsity patterns can be equivalently decomposed into a sum of\nmultiple SOS matrices that are nonzero only on a principal submatrix. Also, the\ncompletion of an SOS matrix is equivalent to a set of SOS conditions on its\nprincipal submatrices and a consistency condition on the Gram representation of\nthe principal submatrices. These results are partial extensions of chordal\ndecomposition and completion of scalar matrices to matrices with polynomial\nentries. We apply the SOS decomposition result to exploit sparsity in\nmatrix-valued SOS programs. Numerical results demonstrate the high potential of\nthis approach for solving large-scale sparse matrix-valued SOS programs. \n\n"}
{"id": "1804.02829", "contents": "Title: Optimal Covariance Control for Stochastic Systems Under Chance\n  Constraints Abstract: This work addresses the optimal covariance control problem for stochastic\ndiscrete-time linear time-varying systems subject to chance constraints.\nCovariance steering is a stochastic control problem to steer the system state\nGaussian distribution to another Gaussian distribution while minimizing a cost\nfunction. To the best of our knowledge, covariance steering problems have never\nbeen discussed with probabilistic chance constraints although it is a natural\nextension. In this work, first we show that, unlike the case with no chance\nconstraints, the covariance steering with chance constraints problem cannot\ndecouple the mean and covariance steering sub-problems. Then we propose an\napproach to solve the covariance steering with chance constraints problem by\nconverting it to a semidefinite programming problem. The proposed algorithm is\nverified using two simple numerical simulations. \n\n"}
{"id": "1804.03986", "contents": "Title: Dynamic Sensor Subset Selection for Centralized Tracking a Time-Varying\n  Stochastic Process Abstract: Motivated by the Internet-of-things and sensor networks for cyberphysical\nsystems, the problem of dynamic sensor activation for the centralized tracking\nof an i.i.d. time-varying process is examined. The tradeoff is between energy\nefficiency, which decreases with the number of active sensors, and fidelity,\nwhich increases with the number of active sensors. The problem of minimizing\nthe time-averaged mean-squared error over infinite horizon is examined under\nthe constraint of the mean number of active sensors. The proposed methods\nartfully combine Gibbs sampling and stochastic approximation for learning, in\norder to create a high performance, energy efficient tracking mechanisms with\nactive sensor selection. Centralized tracking of i.i.d. process with known\ndistribution as well as an unknown parametric distribution are considered. For\nan i.i.d. process with known distribution, convergence to the global optimal\nsolution with high probability is proved. The main challenge of the i.i.d. case\nis that the process has a distribution parameterized by a known or unknown\nparameter which must be learned; one key theoretical result proves that the\nproposed algorithm for tracking an i.i.d. process with unknown parametric\ndistribution converges to local optima. Numerical results show the efficacy of\nthe proposed algorithms and also suggest that global optimality is in fact\nachieved in some cases. \n\n"}
{"id": "1804.04432", "contents": "Title: Numerical approximation of the data-rate limit for state estimation\n  under communication constraints Abstract: In networked control, a fundamental problem is to determine the smallest\ncapacity of a communication channel between a dynamical system and a controller\nabove which a prescribed control objective can be achieved. Often, a\npreliminary task of the controller, before selecting the control input, is to\nestimate the state with a sufficient accuracy. For time-invariant systems, it\nhas been shown that the smallest channel capacity $C_0$ above which the state\ncan be estimated with an arbitrarily small error, depending on the precise\nformulation of the estimation objective, is given by the topological entropy or\na quantity named restoration entropy, respectively. In this paper, we propose\nan algorithm that computes rigorous upper bounds of $C_0$, based on previous\nanalytical estimates. \n\n"}
{"id": "1804.04688", "contents": "Title: Topological data analysis and diagnostics of compressible MHD turbulence Abstract: The predictions of mean-field electrodynamics can now be probed using direct\nnumerical simulations of random flows and magnetic fields. When modelling\nastrophysical MHD, it is important to verify that such simulations are in\nagreement with observations. One of the main challenges in this area is to\nidentify robust \\it{quantitative} measures to compare structures found in\nsimulations with those inferred from astrophysical observations. A similar\nchallenge is to compare quantitatively results from different simulations.\nTopological data analysis offers a range of techniques, including the Betti\nnumbers and persistence diagrams, that can be used to facilitate such a\ncomparison. After describing these tools, we first apply them to synthetic\nrandom fields and demonstrate that, when the data are standardized in a\nstraightforward manner, some topological measures are insensitive to either\nlarge-scale trends or the resolution of the data. Focusing upon one particular\nastrophysical example, we apply topological data analysis to HI observations of\nthe turbulent interstellar medium (ISM) in the Milky Way and to recent MHD\nsimulations of the random, strongly compressible ISM. We stress that these\ntopological techniques are generic and could be applied to any complex,\nmulti-dimensional random field. \n\n"}
{"id": "1804.05201", "contents": "Title: Statistical and systematical errors in analyses of separate experimental\n  data sets in high energy physics Abstract: Different ways of extracting parameters of interest from combined data sets\nof separate experiments are investigated accounting for the systematic errors.\nIt is shown, that the frequentist approach may yield larger $\\chi^2$ values\nwhen compared to the Bayesian approach, where the systematic errors have a\nGaussian distributed prior calculated in quadrature. The former leads to a\nbetter estimation of the parameters. A maximum-likelihood method, applied to\ndifferent \"gedanken\" and real LHC data, is presented. The results allow to\nchoose an optimal approach for obtaining the fit based model parameters. \n\n"}
{"id": "1804.06724", "contents": "Title: Using Convex Optimization of Autocorrelation with Constrained Support\n  and Windowing for Improved Phase Retrieval Accuracy Abstract: In imaging modalities recording diffraction data, the original image can be\nreconstructed assuming known phases. When phases are unknown, oversampling and\na constraint on the support region in the original object can be used to solve\na non-convex optimization problem.\n  Such schemes are ill-suited to find the optimum solution for sparse data,\nsince the recorded image does not correspond exactly to the original wave\nfunction. We construct a convex optimization problem using a relaxed support\nconstraint and a maximum-likelihood treatment of the recorded data as a sample\nfrom the underlying wave function. We also stress the need to use relevant\nwindowing techniques to account for the sampled pattern being finite.\n  On simulated data, we demonstrate the benefits of our approach in terms of\nvisual quality and an improvement in the crystallographic R-factor from .4 to\n.1 for highly noisy data. \n\n"}
{"id": "1804.07565", "contents": "Title: Moments and convex optimization for analysis and control of nonlinear\n  partial differential equations Abstract: This work presents a convex-optimization-based framework for analysis and\ncontrol of nonlinear partial differential equations. The approach uses a\nparticular weak embedding of the nonlinear PDE, resulting in a linear equation\nin the space of Borel measures. This equation is then used as a constraint of\nan infinite-dimensional linear programming problem (LP). This LP is then\napproximated by a hierarchy of convex, finite-dimensional, semidefinite\nprogramming problems (SDPs). In the case of analysis of uncontrolled PDEs, the\nsolutions to these SDPs provide bounds on a specified, possibly nonlinear,\nfunctional of the solutions to the PDE; in the case of PDE control, the\nsolutions to these SDPs provide bounds on the optimal value of a given optimal\ncontrol problem as well as suboptimal feedback controllers. The entire approach\nis based purely on convex optimization and does not rely on spatio-temporal\ngridding, even though the PDE addressed can be fully nonlinear. The approach is\napplicable to a very broad class nonlinear PDEs with polynomial data.\nComputational complexity is analyzed and several complexity reduction\nprocedures are described. Numerical examples demonstrate the approach. \n\n"}
{"id": "1804.07795", "contents": "Title: Stochastic subgradient method converges on tame functions Abstract: This work considers the question: what convergence guarantees does the\nstochastic subgradient method have in the absence of smoothness and convexity?\nWe prove that the stochastic subgradient method, on any semialgebraic locally\nLipschitz function, produces limit points that are all first-order stationary.\nMore generally, our result applies to any function with a Whitney stratifiable\ngraph. In particular, this work endows the stochastic subgradient method, and\nits proximal extension, with rigorous convergence guarantees for a wide class\nof problems arising in data science---including all popular deep learning\narchitectures. \n\n"}
{"id": "1804.07884", "contents": "Title: Neural-inspired sensors enable sparse, efficient classification of\n  spatiotemporal data Abstract: Sparse sensor placement is a central challenge in the efficient\ncharacterization of complex systems when the cost of acquiring and processing\ndata is high. Leading sparse sensing methods typically exploit either spatial\nor temporal correlations, but rarely both. This work introduces a new sparse\nsensor optimization that is designed to leverage the rich spatiotemporal\ncoherence exhibited by many systems. Our approach is inspired by the remarkable\nperformance of flying insects, which use a few embedded strain-sensitive\nneurons to achieve rapid and robust flight control despite large gust\ndisturbances. Specifically, we draw on nature to identify targeted\nneural-inspired sensors on a flapping wing to detect body rotation. This task\nis particularly challenging as the rotational twisting mode is three\norders-of-magnitude smaller than the flapping modes. We show that nonlinear\nfiltering in time, built to mimic strain-sensitive neurons, is essential to\ndetect rotation, whereas instantaneous measurements fail. Optimized sparse\nsensor placement results in efficient classification with approximately ten\nsensors, achieving the same accuracy and noise robustness as full measurements\nconsisting of hundreds of sensors. Sparse sensing with neural inspired encoding\nestablishes a new paradigm in hyper-efficient, embodied sensing of\nspatiotemporal data and sheds light on principles of biological sensing for\nagile flight control. \n\n"}
{"id": "1804.09559", "contents": "Title: Feedback Synthesis For Underactuated Systems Using Sequential\n  Second-Order Needle Variations Abstract: This paper derives nonlinear feedback control synthesis for general control\naffine systems using second-order actions---the second-order needle variations\nof optimal control---as the basis for choosing each control response to the\ncurrent state. A second result of the paper is that the method provably\nexploits the nonlinear controllability of a system by virtue of an explicit\ndependence of the second-order needle variation on the Lie bracket between\nvector fields. As a result, each control decision necessarily decreases the\nobjective when the system is nonlinearly controllable using first-order Lie\nbrackets. Simulation results using a differential drive cart, an underactuated\nkinematic vehicle in three dimensions, and an underactuated dynamic model of an\nunderwater vehicle demonstrate that the method finds control solutions when the\nfirst-order analysis is singular. Lastly, the underactuated dynamic underwater\nvehicle model demonstrates convergence even in the presence of a velocity\nfield. \n\n"}
{"id": "1804.10483", "contents": "Title: A Graph-Theoretic Approach to the $\\mathcal{H}_{\\infty}$ Performance of\n  Dynamical Systems on Directed and Undirected Networks Abstract: We study a graph-theoretic approach to the $\\mathcal{H}_{\\infty}$ performance\nof leader following consensus dynamics on directed and undirected graphs. We\nfirst provide graph-theoretic bounds on the system $\\mathcal{H}_{\\infty}$ norm\nof the leader following dynamics and show the tightness of the proposed bounds.\nThen, we discuss the relation between the system $\\mathcal{H}_{\\infty}$ norm\nfor directed and undirected networks for specific classes of graphs, i.e.,\nbalanced digraphs and directed trees. Moreover, we investigate the effects of\nadding directed edges to a directed tree on the resulting system\n$\\mathcal{H}_{\\infty}$ norm. In the end, we apply these theoretical results to\na reference velocity tracking problem in a platoon of connected vehicles and\ndiscuss the effect of the location of the leading vehicle on the overall\n$\\mathcal{H}_{\\infty}$ performance of the system. \n\n"}
{"id": "1804.10554", "contents": "Title: Random Asynchronous Iterations in Distributed Coordination Algorithms Abstract: Distributed coordination algorithms (DCA) carry out information processing\nprocesses among a group of networked agents without centralized information\nfusion. Though it is well known that DCA characterized by an SIA (stochastic,\nindecomposable, aperiodic) matrix generate consensus asymptotically via\nsynchronous iterations, the dynamics of DCA with asynchronous iterations have\nnot been studied extensively, especially when viewed as stochastic processes.\nThis paper aims to show that for any given irreducible stochastic matrix, even\nnon-SIA, the corresponding DCA lead to consensus successfully via random\nasynchronous iterations under a wide range of conditions on the transition\nprobability. Particularly, the transition probability is neither required to be\nindependent and identically distributed, nor characterized by a Markov chain. \n\n"}
{"id": "1805.00164", "contents": "Title: Affine Multiplexing Networks: System Analysis, Learning, and Computation Abstract: We introduce a novel architecture and computational framework for formal,\nautomated analysis of systems with a broad set of nonlinearities in the\nfeedback loop, such as neural networks, vision controllers, switched systems,\nand even simple programs. We call this computational structure an affine\nmultiplexing network (AMN). The architecture is based on interconnections of\ntwo basic conceptual building blocks: multiplexers ($\\mu$), and affine\ntransformations ($\\alpha$). When attached together appropriately, these\nbuilding blocks translate to conjunctions and disjunctions of affine\nstatements, resulting in an encoding of the network into satisfiability modulo\ntheory (SMT), mixed integer programming, and sequential convex optimization\nsolvers. We show how to formulate and verify system properties like stability\nand robustness, how to compute margins, and how to verify performance through a\nsequence of SMT queries. As illustration, we use the framework to verify closed\nloop, possibly nonlinear dynamical systems that contain neural networks in the\nloop, and hint at a number of extensions that can make AMNs a potent playground\nfor interfacing between machine learning, control, convex and nonconvex\noptimization, and formal methods. \n\n"}
{"id": "1805.01663", "contents": "Title: Dynamic Power Allocation for Smart Grids via ADMM Abstract: Electric power distribution systems will encounter fluctuations in supply due\nto the introduction of renewable sources with high variability in generation\ncapacity. It is therefore necessary to provide algorithms that are capable of\ndynamically finding approximate solutions. We propose two semi-distributed\nalgorithms based on ADMM and discuss their advantages and disadvantages. One of\nthe algorithms computes a feasible approximate of the optimal power allocation\nat each instance. We require coordination between the nodes to guarantee\nfeasibility of each of the iterates. We bound the distance from the approximate\nsolutions to the optimal solution as a function of the variation in optimal\npower allocation. Finally, we verify our results via experiments. \n\n"}
{"id": "1805.02158", "contents": "Title: Acceleration of RED via Vector Extrapolation Abstract: Models play an important role in inverse problems, serving as the prior for\nrepresenting the original signal to be recovered. REgularization by Denoising\n(RED) is a recently introduced general framework for constructing such priors\nusing state-of-the-art denoising algorithms. Using RED, solving inverse\nproblems is shown to amount to an iterated denoising process. However, as the\ncomplexity of denoising algorithms is generally high, this might lead to an\noverall slow algorithm. In this paper, we suggest an accelerated technique\nbased on vector extrapolation (VE) to speed-up existing RED solvers. Numerical\nexperiments validate the obtained gain by VE, leading to a substantial savings\nin computations compared with the original fixed-point method. \n\n"}
{"id": "1805.03884", "contents": "Title: Modified Skellam, Poisson and Gaussian distributions in semi-open\n  systems at charge-like conservation law Abstract: A modification of the Skellam and Poisson distributions is proposed for\nsubsystems when the constraints imposed by the charge conservation law in the\ncomplete system are taken into account. Such distributions can be applied, for\nexample, for an analysis of the fluctuations of baryon and net baryon numbers\nin certain pseudo-rapidity interval in $A+A$ and $p+p$ collisions with high\nmultiplicities. The presented modified Skellam, Poisson and Gaussian\ndistributions can be utilized also in various branches of science, when one\nstudies the fluctuations of the two variables related to a subsystem, as well\nas the distribution of the difference of these variables, while the mentioned\ndifference in the total system is fixed. \n\n"}
{"id": "1805.03961", "contents": "Title: Study of constraint and impact of a nuisance parameter in maximum\n  likelihood method Abstract: Maximum likelihood method is widely used for parameter estimation in high\nenergy physics. To consider various systematic uncertainties, tens of or even\nhundreds of nuisance parameters (NP) are introduced in a likelihood fit. The\nconstraint of a nuisance parameter and its impact on the parameter of interest\n(POI) will be the main concerns for a precise measurement. A fit involving many\nparameters is usually slow and it is even more time-consuming to investigate\nwhy a parameter is over-constrained or has a large impact. In this paper, we\nare trying to understand the reasons behind and provide simple formulae to\nestimate the constraint and impact directly. \n\n"}
{"id": "1805.04800", "contents": "Title: Canonical tensor model through data analysis -- Dimensions, topologies,\n  and geometries -- Abstract: The canonical tensor model (CTM) is a tensor model in Hamilton formalism and\nis studied as a model for gravity in both classical and quantum frameworks. Its\ndynamical variables are a canonical conjugate pair of real symmetric\nthree-index tensors, and a question in this model was how to extract spacetime\npictures from the tensors. We give such an extraction procedure by using two\ntechniques widely known in data analysis. One is the tensor-rank (or CP, etc.)\ndecomposition, which is a certain generalization of the singular value\ndecomposition of a matrix and decomposes a tensor into a number of vectors. By\nregarding the vectors as points forming a space, topological properties can be\nextracted by using the other data analysis technique called persistent\nhomology, and geometries by virtual diffusion processes over points. Thus, time\nevolutions of the tensors in the CTM can be interpreted as topological and\ngeometric evolutions of spaces. We have performed some initial investigations\nof the classical equation of motion of the CTM in terms of these techniques for\na homogeneous fuzzy circle and homogeneous two- and three-dimensional fuzzy\nspheres as spaces, and have obtained agreement with the general relativistic\nsystem obtained previously in a formal continuum limit of the CTM. It is also\ndemonstrated by some concrete examples that the procedure is general for any\ndimensions and topologies, showing the generality of the CTM. \n\n"}
{"id": "1805.05565", "contents": "Title: A Cubic Regularized Newton's Method over Riemannian Manifolds Abstract: In this paper we present a cubic regularized Newton's method to minimize a\nsmooth function over a Riemannian manifold. The proposed algorithm is shown to\nreach a second-order $\\epsilon$-stationary point within\n$\\mathcal{O}(1/\\epsilon^{\\frac{3}{2}})$ iterations, under the condition that\nthe pullbacks are locally Lipschitz continuous, a condition that is shown to be\nsatisfied if the manifold is compact. Furthermore, we present a local\nsuperlinear convergence result under some additional conditions. \n\n"}
{"id": "1805.05789", "contents": "Title: Extended finite element methods for optimal control problems governed by\n  Poisson equation in non-convex domains Abstract: This paper analyzes two eXtended finite element methods (XFEMs) for linear\nquadratic optimal control problems governed by Poisson equation in non-convex\ndomains. We follow the variational discretization concept to discretize the\ncontinuous problems, and apply an XFEM with a cut-off function and a classic\nXFEM with a fixed enrichment area to discretize the state and co-state\nequations. Optimal error estimates are derived for the state, co-state and\ncontrol. Numerical results confirm our theoretical results. \n\n"}
{"id": "1805.05935", "contents": "Title: Feedback-Based Tree Search for Reinforcement Learning Abstract: Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of\nartificial intelligence (AI) application domains, we propose a model-based\nreinforcement learning (RL) technique that iteratively applies MCTS on batches\nof small, finite-horizon versions of the original infinite-horizon Markov\ndecision process. The terminal condition of the finite-horizon problems, or the\nleaf-node evaluator of the decision tree generated by MCTS, is specified using\na combination of an estimated value function and an estimated policy function.\nThe recommendations generated by the MCTS procedure are then provided as\nfeedback in order to refine, through classification and regression, the\nleaf-node evaluator for the next iteration. We provide the first sample\ncomplexity bounds for a tree search-based RL algorithm. In addition, we show\nthat a deep neural network implementation of the technique can create a\ncompetitive AI agent for the popular multi-player online battle arena (MOBA)\ngame King of Glory. \n\n"}
{"id": "1805.06498", "contents": "Title: Utility maximization with proportional transaction costs under model\n  uncertainty Abstract: We consider a discrete time financial market with proportional transaction\ncosts under model uncertainty, and study a num\\'eraire-based semi-static\nutility maximization problem with an exponential utility preference. The\nrandomization techniques recently developed in \\cite{BDT17} allow us to\ntransform the original problem into a frictionless counterpart on an enlarged\nspace. By suggesting a different dynamic programming argument than in\n\\cite{bartl2016exponential}, we are able to prove the existence of the optimal\nstrategy and the convex duality theorem in our context with transaction costs.\nIn the frictionless framework, this alternative dynamic programming argument\nalso allows us to generalize the main results in \\cite{bartl2016exponential} to\na weaker market condition. Moreover, as an application of the duality\nrepresentation, some basic features of utility indifference prices are\ninvestigated in our robust setting with transaction costs. \n\n"}
{"id": "1805.07038", "contents": "Title: Fundamental Tradeoffs in Communication and Trajectory Design for\n  UAV-Enabled Wireless Network Abstract: The use of unmanned aerial vehicles (UAVs) as aerial communication platforms\nis of high practical value for future wireless systems such as 5G, especially\nfor swift and on-demand deployment in temporary events and emergency\nsituations. Compared to traditional terrestrial base stations (BSs) in cellular\nnetwork, UAV-mounted aerial BSs possess stronger line-of-sight (LoS) links with\nthe ground users due to their high altitude as well as high and flexible\nmobility in three-dimensional (3D) space, which can be exploited to enhance the\ncommunication performance. On the other hand, unlike terrestrial BSs that have\nreliable power supply, aerial BSs in practice have limited on-board energy, but\nrequire significant propulsion energy to stay airborne and support high\nmobility. Motivated by the above new considerations, this article aims to\nrevisit some fundamental tradeoffs in UAV-enabled communication and trajectory\ndesign. Specifically, it is shown that communication throughput, delay, and\n(propulsion) energy consumption can be traded off among each other by adopting\ndifferent UAV trajectory designs, which sheds new light on their traditional\ntradeoffs in terrestrial communication. Promising directions for future\nresearch are also discussed. \n\n"}
{"id": "1805.07746", "contents": "Title: Network Reconstruction and Controlling Based on Structural Regularity\n  Analysis Abstract: From the perspective of network analysis, the ubiquitous networks are\ncomprised of regular and irregular components, which makes uncovering the\ncomplexity of network structures to be a fundamental challenge. Exploring the\nregular information and identifying the roles of microscopic elements in\nnetwork data can help us recognize the principle of network organization and\ncontribute to network data utilization. However, the intrinsic structural\nproperties of networks remain so far inadequately explored and theorised. With\nthe realistic assumption that there are consistent features across the local\nstructures of networks, we propose a low-rank pursuit based self-representation\nnetwork model, in which the principle of network organization can be uncovered\nby a representation matrix. According to this model, original true networks can\nbe reconstructed based on the observed unreliable network topology. In\nparticular, the proposed model enables us to estimate the extent to which the\nnetworks are regulable, i.e., measuring the reconstructability of networks. In\naddition, the model is capable of measuring the importance of microscopic\nnetwork elements, i.e., nodes and links, in terms of network regularity thereby\nallowing us to regulate the reconstructability of networks based on them.\nExtensive experiments on disparate real-world networks demonstrate the\neffectiveness of the proposed network reconstruction and regulation algorithm.\nSpecifically, the network regularity metric can reflect the reconstructability\nof networks, and the reconstruction accuracy can be improved by removing\nirregular network links. Lastly, our approach provides an unique and novel\ninsight into the organization exploring of complex networks. \n\n"}
{"id": "1805.09185", "contents": "Title: Alternating Randomized Block Coordinate Descent Abstract: Block-coordinate descent algorithms and alternating minimization methods are\nfundamental optimization algorithms and an important primitive in large-scale\noptimization and machine learning. While various block-coordinate-descent-type\nmethods have been studied extensively, only alternating minimization -- which\napplies to the setting of only two blocks -- is known to have convergence time\nthat scales independently of the least smooth block. A natural question is\nthen: is the setting of two blocks special?\n  We show that the answer is \"no\" as long as the least smooth block can be\noptimized exactly -- an assumption that is also needed in the setting of\nalternating minimization. We do so by introducing a novel algorithm AR-BCD,\nwhose convergence time scales independently of the least smooth (possibly\nnon-smooth) block. The basic algorithm generalizes both alternating\nminimization and randomized block coordinate (gradient) descent, and we also\nprovide its accelerated version -- AAR-BCD. As a special case of AAR-BCD, we\nobtain the first nontrivial accelerated alternating minimization algorithm. \n\n"}
{"id": "1805.09877", "contents": "Title: Online Optimization as a Feedback Controller: Stability and Tracking Abstract: This paper develops and analyzes feedback-based online optimization methods\nto regulate the output of a linear time-invariant (LTI) dynamical system to the\noptimal solution of a time-varying convex optimization problem. The design of\nthe algorithm is based on continuous-time primal-dual dynamics, properly\nmodified to incorporate feedback from the LTI dynamical system, applied to a\nproximal augmented Lagrangian function. The resultant closed-loop algorithm\ntracks the solution of the time-varying optimization problem without requiring\nknowledge of (time-varying) disturbances in the dynamical system. The analysis\nleverages integral quadratic constraints to provide linear matrix inequality\n(LMI) conditions that guarantee global exponential stability and bounded\ntracking error. Analytical results show that, under a sufficient time-scale\nseparation between the dynamics of the LTI dynamical system and the algorithm,\nthe LMI conditions can be always satisfied. The paper further proposes a\nmodified algorithm that can track an approximate solution trajectory of the\nconstrained optimization problem under less restrictive assumptions. As an\nillustrative example, the proposed algorithms are showcased for power\ntransmission systems, to compress the time scales between secondary and\ntertiary control, and allow to simultaneously power re-balancing and tracking\nof DC optimal power flow points. \n\n"}
{"id": "1805.09965", "contents": "Title: LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed\n  Learning Abstract: This paper presents a new class of gradient methods for distributed machine\nlearning that adaptively skip the gradient calculations to learn with reduced\ncommunication and computation. Simple rules are designed to detect\nslowly-varying gradients and, therefore, trigger the reuse of outdated\ngradients. The resultant gradient-based algorithms are termed Lazily Aggregated\nGradient --- justifying our acronym LAG used henceforth. Theoretically, the\nmerits of this contribution are: i) the convergence rate is the same as batch\ngradient descent in strongly-convex, convex, and nonconvex smooth cases; and,\nii) if the distributed datasets are heterogeneous (quantified by certain\nmeasurable constants), the communication rounds needed to achieve a targeted\naccuracy are reduced thanks to the adaptive reuse of lagged gradients.\nNumerical experiments on both synthetic and real data corroborate a significant\ncommunication reduction compared to alternatives. \n\n"}
{"id": "1805.10786", "contents": "Title: Phase portrait control for 1D monostable and bistable reaction-diffusion\n  equations Abstract: We consider the problem of controlling parabolic semilinear equations arising\nin population dynamics, either in finite time or infinite time. These are the\nmonostable and bistable equations on $(0,L)$ for a density of individuals $0\n\\leq y(t,x) \\leq 1$, with Dirichlet controls taking their values in $[0,1]$. We\nprove that the system can never be steered to extinction (steady state $0$) or\ninvasion (steady state $1$) in finite time, but is asymptotically controllable\nto $1$ independently of the size $L$, and to $0$ if the length $L$ of the\ninterval domain is less than some threshold value $L^\\star$, which can be\ncomputed from transcendental integrals. In the bistable case, controlling to\nthe other homogeneous steady state $0 <\\theta< 1$ is much more intricate. We\nrely on a staircase control strategy to prove that $\\theta$ can be reached in\nfinite time if and only if $L< L^\\star$. The phase plane analysis of those\nequations is instrumental in the whole process. It allows us to read obstacles\nto controllability, compute the threshold value for domain size as well as\ndesign the path of steady states for the control strategy. \n\n"}
{"id": "1805.12052", "contents": "Title: Unwinding the model manifold: choosing similarity measures to remove\n  local minima in sloppy dynamical systems Abstract: In this paper, we consider the problem of parameter sensitivity in models of\ncomplex dynamical systems through the lens of information geometry. We\ncalculate the sensitivity of model behavior to variations in parameters. In\nmost cases, models are sloppy, that is, exhibit an exponential hierarchy of\nparameter sensitivities. We propose a parameter classification scheme based on\nhow the sensitivities scale at long observation times. We show that for\noscillatory models, either with a limit cycle or a strange attractor,\nsensitivities can become arbitrarily large, which implies a high\neffective-dimensionality on the model manifold. Sloppy models with a single\nfixed point have model manifolds with low effective-dimensionality, previously\ndescribed as a \"hyper-ribbon\". In contrast, models with high effective\ndimensionality translate into multimodal fitting problems. We define a measure\nof curvature on the model manifold which we call the \\emph{winding frequency}\nthat estimates the linear density of local minima in the model's parameter\nspace. We then show how alternative choices of fitting metrics can \"unwind\" the\nmodel manifold and give low winding frequencies. This prescription translates\nthe model manifold from one of high effective-dimensionality into the\n\"hyper-ribbon\" structures observed elsewhere. This translation opens the door\nfor applications of sloppy model analysis and model reduction methods developed\nfor models with low effective-dimensionality. \n\n"}
{"id": "1805.12244", "contents": "Title: Mining gold from implicit models to improve likelihood-free inference Abstract: Simulators often provide the best description of real-world phenomena.\nHowever, they also lead to challenging inverse problems because the density\nthey implicitly define is often intractable. We present a new suite of\nsimulation-based inference techniques that go beyond the traditional\nApproximate Bayesian Computation approach, which struggles in a\nhigh-dimensional setting, and extend methods that use surrogate models based on\nneural networks. We show that additional information, such as the joint\nlikelihood ratio and the joint score, can often be extracted from simulators\nand used to augment the training data for these surrogate models. Finally, we\ndemonstrate that these new techniques are more sample efficient and provide\nhigher-fidelity inference than traditional methods. \n\n"}
{"id": "1806.00992", "contents": "Title: Integrality of Subgradients and Biconjugates of Integrally Convex\n  Functions Abstract: Integrally convex functions constitute a fundamental function class in\ndiscrete convex analysis. This paper shows that an integer-valued integrally\nconvex function admits an integral subgradient and that the integral\nbiconjugate of an integer-valued integrally convex function coincides with\nitself. The proof is based on the Fourier-Motzkin elimination. The latter\nresult provides a unified proof of integral biconjugacy for various classes of\ninteger-valued discrete convex functions, including L-convex, M-convex,\nL$_{2}$-convex, M$_{2}$-convex, BS-convex, and UJ-convex functions as well as\nmultimodular functions. Our results of integral subdifferentiability and\nintegral biconjugacy make it possible to extend the theory of discrete DC\n(difference of convex) functions developed for L- and M-convex functions to\nthat for integrally convex functions, including an analogue of the\nToland--Singer duality for integrally convex functions. \n\n"}
{"id": "1806.03167", "contents": "Title: A spatial likelihood analysis for MAGIC telescope data Abstract: Context. The increase in sensitivity of Imaging Atmospheric Cherenkov\nTelescopes (IACTs) has lead to numerous detections of extended $\\gamma$-ray\nsources at TeV energies, sometimes of sizes comparable to the instrument's\nfield of view (FoV). This creates a demand for advanced and flexible data\nanalysis methods, able to extract source information by utilising the photon\ncounts in the entire FoV.\n  Aims. We present a new software package, \"SkyPrism\", aimed at performing 2D\n(3D if energy is considered) fits of IACT data, possibly containing multiple\nand extended sources, based on sky images binned in energy. Though the\ndevelopment of this package was focused on the analysis of data collected with\nthe MAGIC telescopes, it can further be adapted to other instruments, such as\nthe future Cherenkov Telescope Array (CTA).\n  Methods. We have developed a set of tools that, apart from sky images (count\nmaps), compute the instrument response functions (IRFs) of MAGIC (effective\nexposure throughout the FoV, point spread function (PSF), energy resolution and\nbackground shape), based on the input data, Monte-Carlo simulations and the\npointing track of the telescopes. With this information, the presented package\ncan perform a simultaneous maximum likelihood fit of source models of arbitrary\nmorphology to the sky images providing energy spectra, detection significances,\nand upper limits.\n  Results. We demonstrate that the SkyPrism tool accurately reconstructs the\nMAGIC PSF, on and off-axis performance as well as the underlying background. We\nfurther show that for a point source analysis with MAGIC's default\nobservational settings, SkyPrism gives results compatible with those of the\nstandard tools while being more flexible and widely applicable. \n\n"}
{"id": "1806.03475", "contents": "Title: Input Matrix Construction and Approximation Using a Graphic Approach Abstract: Given a state transition matrix (STM), we reinvestigate the problem of\nconstructing the sparest input matrix with a fixed number of inputs to\nguarantee controllability. We give a new and simple graph theoretic\ncharacterization for the sparsity pattern of input matrices to guarantee\ncontrollability for a general STM admitting multiple eigenvalues, and provide a\ndeterministic procedure with polynomial time complexity to construct real\nvalued input matrices with arbi- trarily prescribed sparsity pattern satisfying\ncontrollability. Based on this criterion, some novel results on sparsely\ncontrolling a system are obtained. It is proven that the minimal number of\ninputs to guarantee controllability equals to the maximum geometric\nmultiplicity of the STM under the constraint that some states are\nactuated-forbidden, extending the results of [28]. The minimal sparsity of\ninput matrices with a fixed number of inputs is not necessarily equal to the\nminimal number of actuated states to ensure controllability. Furthermore, a\ngraphic sub- modular function is built, leading to a greedy algorithm to\nefficiently approximate the minimal actuated states to assure controllability\nfor general STMs. For the problem of approximating the sparsest input matrices\nwith a fixed number of inputs, we propose a simple greedy algo- rithm\n(non-submodular) and a two-stage algorithm, and demonstrate that the latter\nalgorithm, inspired from techniques in dynamic coloring, has a provable\napproximation guarantee. Finally, we present numerical results to show the\nefficiency and effectiveness of our approaches. \n\n"}
{"id": "1806.03975", "contents": "Title: Efficient global optimization of constrained mixed variable problems Abstract: Due to the increasing demand for high performance and cost reduction within\nthe framework of complex system design, numerical optimization of\ncomputationally costly problems is an increasingly popular topic in most\nengineering fields. In this paper, several variants of the Efficient Global\nOptimization algorithm for costly constrained problems depending simultaneously\non continuous decision variables as well as on quantitative and/or qualitative\ndiscrete design parameters are proposed. The adaptation that is considered is\nbased on a redefinition of the Gaussian Process kernel as a product between the\nstandard continuous kernel and a second kernel representing the covariance\nbetween the discrete variable values. Several parameterizations of this\ndiscrete kernel, with their respective strengths and weaknesses, are discussed\nin this paper. The novel algorithms are tested on a number of analytical\ntest-cases and an aerospace related design problem, and it is shown that they\nrequire fewer function evaluations in order to converge towards the\nneighborhoods of the problem optima when compared to more commonly used\noptimization algorithms. \n\n"}
{"id": "1806.04781", "contents": "Title: On the Convergence Rate of Stochastic Mirror Descent for Nonsmooth\n  Nonconvex Optimization Abstract: In this paper, we investigate the non-asymptotic stationary convergence\nbehavior of Stochastic Mirror Descent (SMD) for nonconvex optimization. We\nfocus on a general class of nonconvex nonsmooth stochastic optimization\nproblems, in which the objective can be decomposed into a relatively weakly\nconvex function (possibly non-Lipschitz) and a simple non-smooth convex\nregularizer. We prove that SMD, without the use of mini-batch, is guaranteed to\nconverge to a stationary point in a convergence rate of $\n\\mathcal{O}(1/\\sqrt{t}) $. The efficiency estimate matches with existing\nresults for stochastic subgradient method, but is evaluated under a stronger\nstationarity measure. Our convergence analysis applies to both the original SMD\nand its proximal version, as well as the deterministic variants, for solving\nrelatively weakly convex problems. \n\n"}
{"id": "1806.05140", "contents": "Title: Generalized Mirror Prox for Monotone Variational Inequalities:\n  Universality and Inexact Oracle Abstract: We introduce an inexact oracle model for variational inequalities (VI) with\nmonotone operator, propose a numerical method which solves such VI's and\nanalyze its convergence rate. As a particular case, we consider VI's with\nH\\\"older-continuous operator and show that our algorithm is universal. This\nmeans that without knowing the H\\\"older parameter $\\nu$ and H\\\"older constant\n$L_{\\nu}$ it has the best possible complexity for this class of VI's, namely\nour algorithm has complexity $O\\left(\n\\inf_{\\nu\\in[0,1]}\\left(\\frac{L_{\\nu}}{\\varepsilon}\n\\right)^{\\frac{2}{1+\\nu}}R^2 \\right)$, where $R$ is the size of the feasible\nset and $\\varepsilon$ is the desired accuracy of the solution. We also consider\nthe case of VI's with strongly monotone operator and generalize our method for\nVI's with inexact oracle and our universal method for this class of problems.\nFinally, we show, how our method can be applied to convex-concave saddle point\nproblems with H\\\"older-continuous partial subgradients. \n\n"}
{"id": "1806.05977", "contents": "Title: Understanding Complex Systems: From Networks to Optimal Higher-Order\n  Models Abstract: To better understand the structure and function of complex systems,\nresearchers often represent direct interactions between components in complex\nsystems with networks, assuming that indirect influence between distant\ncomponents can be modelled by paths. Such network models assume that actual\npaths are memoryless. That is, the way a path continues as it passes through a\nnode does not depend on where it came from. Recent studies of data on actual\npaths in complex systems question this assumption and instead indicate that\nmemory in paths does have considerable impact on central methods in network\nscience. A growing research community working with so-called higher-order\nnetwork models addresses this issue, seeking to take advantage of information\nthat conventional network representations disregard. Here we summarise the\nprogress in this area and outline remaining challenges calling for more\nresearch. \n\n"}
{"id": "1806.06744", "contents": "Title: Sustainable Inventory with Robust Periodic-Affine Policies and\n  Application to Medical Supply Chains Abstract: We introduce a new class of adaptive policies called periodic-affine\npolicies, that allows a decision maker to optimally manage and control\nlarge-scale newsvendor networks in the presence of uncertain demand without\ndistributional assumptions. These policies are data-driven and model many\nfeatures of the demand such as correlation, and remain robust to parameter\nmis-specification. We present a model that can be generalized to multi-product\nsettings and extended to multi-period problems. This is accomplished by\nmodeling the uncertain demand via sets. In this way, it offers a natural\nframework to study competing policies such as base-stock, affine, and\napproximative approaches with respect to their profit, sensitivity to\nparameters and assumptions, and computational scalability. We show that the\nperiodic-affine policies are sustainable, i.e. time consistent, because they\nwarrant optimality both within subperiods and over the entire planning horizon.\nThis approach is tractable and free of distributional assumptions, and hence,\nsuited for real-world applications. We provide efficient algorithms to obtain\nthe optimal periodic-affine policies and demonstrate their advantages on the\nsales data from one of India's largest pharmacy retailers. \n\n"}
{"id": "1806.08365", "contents": "Title: Mining Gravitational-wave Catalogs To Understand Binary Stellar\n  Evolution: A New Hierarchical Bayesian Framework Abstract: Catalogs of stellar-mass compact binary systems detected by ground-based\ngravitational-wave instruments (such as Advanced LIGO and Advanced Virgo) will\noffer insights into the demographics of progenitor systems and the physics\nguiding stellar evolution. Existing techniques approach this through\nphenomenological modeling, discrete model selection, or model mixtures.\nInstead, we explore a novel technique that mines gravitational-wave catalogs to\ndirectly infer posterior probability distributions of the hyper-parameters\ndescribing formation and evolutionary scenarios (e.g. progenitor metallicity,\nkick parameters, and common-envelope efficiency). We use a bank of\ncompact-binary population synthesis simulations to train a Gaussian-process\nemulator that acts as a prior on observed parameter distributions (e.g. chirp\nmass, redshift, rate). This emulator slots into a hierarchical population\ninference framework to extract the underlying astrophysical origins of systems\ndetected by Advanced LIGO and Advanced Virgo. Our method is fast, easily\nexpanded with additional simulations, and can be adapted for training on\narbitrary population synthesis codes, as well as different detectors like LISA. \n\n"}
{"id": "1806.09893", "contents": "Title: A parametric approach to information filtering in complex networks: The\n  P\\'olya filter Abstract: The ever increasing availability of data demands for techniques to extract\nrelevant information from complex interacting systems, which can often be\nrepresented as weighted networks. In recent years, a number of approaches have\nbeen proposed to extract network backbones by assessing the statistical\nsignificance of links against null hypotheses of random interaction. Yet, it is\nwell known that the growth of most real-world networks is highly non-random, as\npast interactions between nodes typically increase the likelihood of further\ninteraction. Here, we propose a network filtering methodology based on a family\nof null hypotheses that can be calibrated to assess which links are\nstatistically significant with respect to a given network's own heterogeneity.\nWe design such family of null hypotheses by adapting the P\\'olya urn, a simple\none-parameter combinatorial model driven by a self-reinforcement mechanism, to\na network setting. We provide a full analytical description of our filter, and\nshow that it retains or discards links based on a non-trivial interplay between\ntheir own local importance and the importance of the nodes they belong to. We\nprove that the widely used disparity filter can be recovered as a\nlarge-strength approximation for a specific value of the P\\'olya filter's\nparameter, and illustrate our findings by applying the filter to two large\nnetwork datasets. \n\n"}
{"id": "1806.11449", "contents": "Title: A distributed scheme for secondary frequency control with stability\n  guarantees and optimal power allocation Abstract: We consider the problem of distributed secondary frequency regulation in\npower networks such that stability and an optimal power allocation are\nattained. This is a problem that has been widely studied in the literature, and\ntwo main control schemes have been proposed, usually referred to as\n'primal-dual' and 'distributed averaging proportional-integral (DAPI)'\nrespectively. However, each has its limitations, with the former requiring\nknowledge of uncontrollable demand, which can be difficult to obtain in real\ntime, and with the existing literature on the latter being based on static\nmodels for generation and demand. We propose a novel control scheme that\novercomes these issues by making use of generation measurements in the control\npolicy. In particular, our analysis allows distributed stability and optimality\nguarantees to be deduced with practical measurement requirements and permits a\nbroad range of linear generation dynamics, that can be of higher order, to be\nincorporated in the power network. We show how the controller parameters can be\nselected in a computationally efficient way by solving appropriate linear\nmatrix inequalities (LMIs). Furthermore, we demonstrate how the proposed\nanalysis applies to several examples of turbine governor models. The\npracticality of our analysis is demonstrated with simulations on the Northeast\nPower Coordinating Council (NPCC) 140-bus system that verify that our proposed\ncontroller achieves convergence to the nominal frequency and an economically\noptimal power allocation. \n\n"}
{"id": "1807.00937", "contents": "Title: Constrained dynamical optimal transport and its Lagrangian formulation Abstract: We propose dynamical optimal transport (OT) problems constrained in a\nparameterized probability subset. In application problems such as deep\nlearning, the probability distribution is often generated by a parameterized\nmapping function. In this case, we derive a simple formulation for the\nconstrained dynamical OT. \n\n"}
{"id": "1807.04261", "contents": "Title: Phase Retrieval Under a Generative Prior Abstract: The phase retrieval problem asks to recover a natural signal $y_0 \\in\n\\mathbb{R}^n$ from $m$ quadratic observations, where $m$ is to be minimized. As\nis common in many imaging problems, natural signals are considered sparse with\nrespect to a known basis, and the generic sparsity prior is enforced via\n$\\ell_1$ regularization. While successful in the realm of linear inverse\nproblems, such $\\ell_1$ methods have encountered possibly fundamental\nlimitations, as no computationally efficient algorithm for phase retrieval of a\n$k$-sparse signal has been proven to succeed with fewer than $O(k^2\\log n)$\ngeneric measurements, exceeding the theoretical optimum of $O(k \\log n)$. In\nthis paper, we propose a novel framework for phase retrieval by 1) modeling\nnatural signals as being in the range of a deep generative neural network $G :\n\\mathbb{R}^k \\rightarrow \\mathbb{R}^n$ and 2) enforcing this prior directly by\noptimizing an empirical risk objective over the domain of the generator. Our\nformulation has provably favorable global geometry for gradient methods, as\nsoon as $m = O(kd^2\\log n)$, where $d$ is the depth of the network.\nSpecifically, when suitable deterministic conditions on the generator and\nmeasurement matrix are met, we construct a descent direction for any point\noutside of a small neighborhood around the unique global minimizer and its\nnegative multiple, and show that such conditions hold with high probability\nunder Gaussian ensembles of multilayer fully-connected generator networks and\nmeasurement matrices. This formulation for structured phase retrieval thus has\ntwo advantages over sparsity based methods: 1) deep generative priors can more\ntightly represent natural signals and 2) information theoretically optimal\nsample complexity. We corroborate these results with experiments showing that\nexploiting generative models in phase retrieval tasks outperforms sparse phase\nretrieval methods. \n\n"}
{"id": "1807.05346", "contents": "Title: Quantitative analysis of finite-difference approximations of\n  free-discontinuity problems Abstract: Motivated by applications to image reconstruction, in this paper we analyse a\n\\emph{finite-difference discretisation} of the Ambrosio-Tortorelli functional.\nDenoted by $\\varepsilon$ the elliptic-approximation parameter and by $\\delta$\nthe discretisation step-size, we fully describe the relative impact of\n$\\varepsilon$ and $\\delta$ in terms of $\\Gamma$-limits for the corresponding\ndiscrete functionals, in the three possible scaling regimes. We show, in\nparticular, that when $\\varepsilon$ and $\\delta$ are of the same order, the\nunderlying lattice structure affects the $\\Gamma$-limit which turns out to be\nan anisotropic free-discontinuity functional. \n\n"}
{"id": "1807.06240", "contents": "Title: Definition of the moist-air exergy norm: a comparison with existing\n  \"moist energy norms\" Abstract: This study presents a new formulation for the norms and scalar products used\nin tangent linear or adjoint models to determine forecast errors and\nsensitivity to observations and to calculate singular vectors. The new norm is\nderived from the concept of moist-air available enthalpy, which is one of the\navailability functions referred to as exergy in general thermodynamics. It is\nshown that the sum of the kinetic energy and the moist-air available enthalpy\ncan be used to define a new moist-air squared norm which is quadratic in: 1)\nwind components; 2) temperature; 3) surface pressure; and 4) water vapor\ncontent. Preliminary numerical applications are performed to show that the new\nweighting factors for temperature and water vapor are significantly different\nfrom those used in observation impact studies, and are in better agreement with\nobserved analysis increments. These numerical applications confirm that the\nweighting factors for water vapor and temperature exhibit a large increase with\nheight (by several orders of magnitude) and a minimum in the middle\ntroposphere, respectively. \n\n"}
{"id": "1807.06629", "contents": "Title: Parallel Restarted SGD with Faster Convergence and Less Communication:\n  Demystifying Why Model Averaging Works for Deep Learning Abstract: In distributed training of deep neural networks, parallel mini-batch SGD is\nwidely used to speed up the training process by using multiple workers. It uses\nmultiple workers to sample local stochastic gradient in parallel, aggregates\nall gradients in a single server to obtain the average, and update each\nworker's local model using a SGD update with the averaged gradient. Ideally,\nparallel mini-batch SGD can achieve a linear speed-up of the training time\n(with respect to the number of workers) compared with SGD over a single worker.\nHowever, such linear scalability in practice is significantly limited by the\ngrowing demand for gradient communication as more workers are involved. Model\naveraging, which periodically averages individual models trained over parallel\nworkers, is another common practice used for distributed training of deep\nneural networks since (Zinkevich et al. 2010) (McDonald, Hall, and Mann 2010).\nCompared with parallel mini-batch SGD, the communication overhead of model\naveraging is significantly reduced. Impressively, tremendous experimental works\nhave verified that model averaging can still achieve a good speed-up of the\ntraining time as long as the averaging interval is carefully controlled.\nHowever, it remains a mystery in theory why such a simple heuristic works so\nwell. This paper provides a thorough and rigorous theoretical study on why\nmodel averaging can work as well as parallel mini-batch SGD with significantly\nless communication overhead. \n\n"}
{"id": "1807.07032", "contents": "Title: Time-Varying Optimization: Algorithms and Engineering Applications Abstract: This is the write-up of the talk I gave at the 23rd International Symposium\non Mathematical Programming (ISMP) in Bordeaux, France, July 6th, 2018. The\ntalk was a general overview of the state of the art of time-varying, mainly\nconvex, optimization, with special emphasis on discrete-time algorithms and\napplications in energy and transportation. This write-up is mathematically\ncorrect, while its style is somewhat less formal than a standard paper. \n\n"}
{"id": "1807.07227", "contents": "Title: Convex Relaxations in Power System Optimization: A Brief Introduction Abstract: Convex relaxations of the AC power flow equations have attracted significant\ninterest in the power systems research community in recent years. The following\ncollection of video lectures provides a brief introduction to the mathematics\nof AC power systems, continuous nonlinear optimization, and relaxations of the\npower flow equations. The aim of the videos is to provide the high level ideas\nof convex relaxations and their applications in power system optimization, and\ncould be used as a starting point for researchers who want to study, use or\ndevelop new convex relaxations for use in their own research. The videos do not\naim to provide an in-depth tutorial about specific convex relaxations, but\nrather focus on ideas that are common to all convex relaxations of the AC\noptimal power flow problem. \n\n"}
{"id": "1807.07432", "contents": "Title: Signal Alignment for Humanoid Skeletons via the Globally Optimal\n  Reparameterization Algorithm Abstract: The general ability to analyze and classify the 3D kinematics of the human\nform is an essential step in the development of socially adept humanoid robots.\nA variety of different types of signals can be used by machines to represent\nand characterize actions such as RGB videos, infrared maps, and optical flow.\nIn particular, skeleton sequences provide a natural 3D kinematic description of\nhuman motions and can be acquired in real time using RGB+D cameras. Moreover,\nskeleton sequences are generalizable to characterize the motions of both humans\nand humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA)\nis a novel, recently proposed algorithm for signal alignment in which signals\nare reparameterized to a globally optimal universal standard timescale (UST).\nHere, we introduce a variant of GORA for humanoid action recognition with\nskeleton sequences, which we call GORA-S. We briefly review the algorithm's\nmathematical foundations and contextualize them in the problem of action\nrecognition with skeleton sequences. Subsequently, we introduce GORA-S and\ndiscuss parameters and numerical techniques for its effective implementation.\nWe then compare its performance with that of the DTW and FastDTW algorithms, in\nterms of computational efficiency and accuracy in matching skeletons. Our\nresults show that GORA-S attains a complexity that is significantly less than\nthat of any tested DTW method. In addition, it displays a favorable balance\nbetween speed and accuracy that remains invariant under changes in skeleton\nsampling frequency, lending it a degree of versatility that could make it\nwell-suited for a variety of action recognition tasks. \n\n"}
{"id": "1807.07681", "contents": "Title: Optimal Co-design of Industrial Networked Control Systems with\n  State-dependent Correlated Fading Channels Abstract: This paper examines a co-design problem for industrial networked control\nsystems (NCS) whereby physical systems are controlled over wireless fading\nchannels. In particular, the considered wireless channels are also\nstochastically dependent on the physical states of moving machineries in an\nindustrial working space. In this paper, the moving machineries are modeled as\nMarkov decision processes whereas the characteristics of the correlated fading\nchannels are modeled as a binary random process whose probability measure is a\nfunction of both the moving machineries' physical states and the channels'\ntransmission power. Under such state-dependent fading channel models,\nsufficient conditions which ensure the stochastic safety of the NCS are first\nderived. Using the derived safety conditions, the co-design problem is then\nformulated as a constrained joint optimization problem that seeks for optimal\ncontrol and transmission power policies which simultaneously minimize both the\ncommunication and control costs in an infinite time horizon. This paper shows\nthat such optimal co-design policies can be obtained in an efficient manner\nfrom the solution of convex programs. Simulation results from industrial NCS\nmodels consisting forklift truck and networked DC motor systems are also\npresented to illustrate and verify both the advantages and efficacy of the\nproposed co-design solution framework. \n\n"}
{"id": "1807.07706", "contents": "Title: Efficient Probabilistic Inference in the Quest for Physics Beyond the\n  Standard Model Abstract: We present a novel probabilistic programming framework that couples directly\nto existing large-scale simulators through a cross-platform probabilistic\nexecution protocol, which allows general-purpose inference engines to record\nand control random number draws within simulators in a language-agnostic way.\nThe execution of existing simulators as probabilistic programs enables highly\ninterpretable posterior inference in the structured model defined by the\nsimulator code base. We demonstrate the technique in particle physics, on a\nscientifically accurate simulation of the tau lepton decay, which is a key\ningredient in establishing the properties of the Higgs boson. Inference\nefficiency is achieved via inference compilation where a deep recurrent neural\nnetwork is trained to parameterize proposal distributions and control the\nstochastic simulator in a sequential importance sampling scheme, at a fraction\nof the computational cost of a Markov chain Monte Carlo baseline. \n\n"}
{"id": "1807.07911", "contents": "Title: Application of the Iterated Weighted Least-Squares Fit to counting\n  experiments Abstract: Least-squares fits are an important tool in many data analysis applications.\nIn this paper, we review theoretical results, which are relevant for their\napplication to data from counting experiments. Using a simple example, we\nillustrate the well known fact that commonly used variants of the least-squares\nfit applied to Poisson-distributed data produce biased estimates. The bias can\nbe overcome with an iterated weighted least-squares method, which produces\nresults identical to the maximum-likelihood method. For linear models, the\niterated weighted least-squares method converges faster than the equivalent\nmaximum-likelihood method, and does not require problem-specific starting\nvalues, which may be a practical advantage. The equivalence of both methods\nalso holds for binomially distributed data. We further show that the unbinned\nmaximum-likelihood method can be derived as a limiting case of the iterated\nleast-squares fit when the bin width goes to zero, which demonstrates a deep\nconnection between the two methods. \n\n"}
{"id": "1807.07968", "contents": "Title: Scaling in the eigenvalue fluctuations of the empirical correlation\n  matrices Abstract: The spectra of empirical correlation matrices, constructed from multivariate\ndata, are widely used in many areas of sciences, engineering and social\nsciences as a tool to understand the information contained in typically large\ndatasets. In the last two decades, random matrix theory-based tools such as the\nnearest neighbour eigenvalue spacing and eigenvector distributions have been\nemployed to extract the significant modes of variability present in such\nempirical correlations. In this work, we present an alternative analysis in\nterms of the recently introduced spacing ratios, which does not require the\ncumbersome unfolding process. It is shown that the higher order spacing ratio\ndistributions for the Wishart ensemble of random matrices, characterized by the\nDyson index $\\beta$, is related to the first order spacing ratio distribution\nwith a modified value of co-dimension $\\beta'$. This scaling is demonstrated\nfor Wishart ensemble and also for the spectra of empirical correlation matrices\ndrawn from the observed stock market and atmospheric pressure data. Using a\ncombination of analytical and numerics, such scalings in spacing distributions\nare also discussed. \n\n"}
{"id": "1807.08684", "contents": "Title: Primal-Dual Gradient Flow Algorithm for Distributed Support Vector\n  Machines Abstract: In this paper, a primal-dual gradient flow algorithm for distributed support\nvector machines (DSVM) is proposed. A network of computing nodes, each carrying\na subset of horizontally partitioned large dataset is considered. The nodes are\nrepresented as dynamical systems with Arrow-Hurwicz-Uzawa gradient flow\ndynamics, derived from the Lagrangian function of the DSVM problem. It is first\nproved that the nodes are passive dynamical systems. Then, by employing the\nKrasovskii type candidate Lyapunov functions, it is proved that the computing\nnodes asymptotically converge to the optimal primal-dual solution. \n\n"}
{"id": "1807.09078", "contents": "Title: An entropy minimization approach to second-order variational mean-field\n  games Abstract: We propose a new viewpoint on variational mean-field games with diffusion and\nquadratic Hamiltonian. We show the equivalence of such mean-field games with a\nrelative entropy minimization at the level of probabilities on curves. We also\naddress the time-discretization of such problems, establish\n$\\Gamma$-convergence results as the time step vanishes and propose an efficient\nalgorithm relying on this entropic interpretation as well as on the Sinkhorn\nscaling algorithm. \n\n"}
{"id": "1807.10536", "contents": "Title: Internal observability of the wave equation in a triangular domain Abstract: We investigate the internal observability of the wave equation with Dirichlet\nboundary conditions in a triangular domain. More precisely, the domain taken\ninto exam is the half of the equilateral triangle. Our approach is based on\nFourier analysis and on tessellation theory: by means of a suitable tiling of\nthe rectangle, we extend earlier observability results in the rectangle to the\ncase of a triangular domain. The paper includes a general result relating\nproblems in general domains to their tiles, and a discussion of the triangular\ncase. As an application, we provide an estimation of the observation time when\nthe observed domain is composed by three strips with a common side to the edges\nof the triangle. \n\n"}
{"id": "1808.00235", "contents": "Title: On the stability of matrix-valued Riccati diffusions Abstract: The stability properties of matrix-valued Riccati diffusions are\ninvestigated. The matrix-valued Riccati diffusion processes considered in this\nwork are of interest in their own right, as a rather prototypical model of a\nmatrix-valued quadratic stochastic process. Under rather natural observability\nand controllability conditions, we derive time-uniform moment and fluctuation\nestimates and exponential contraction inequalities. Our approach combines\nspectral theory with nonlinear semigroup methods and stochastic matrix\ncalculus. This analysis seem to be the first of its kind for this class of\nmatrix-valued stochastic differential equation. This class of stochastic models\narise in signal processing and data assimilation, and more particularly in\nensemble Kalman-Bucy filtering theory. In this context, the Riccati diffusion\nrepresents the flow of the sample covariance matrices associated with\nMcKean-Vlasov-type interacting Kalman-Bucy filters. The analysis developed here\napplies to filtering problems with unstable signals. \n\n"}
{"id": "1808.00862", "contents": "Title: A Geometric Obstruction to Almost Global Synchronization on Riemannian\n  Manifolds Abstract: Multi-agent systems on nonlinear spaces sometimes fail to synchronize. This\nis usually attributed to the initial configuration of the agents being too\nspread out, the graph topology having certain undesired symmetries, or both.\nBesides nonlinearity, the role played by the geometry and topology of the\nnonlinear space is often overlooked. This paper concerns two gradient descent\nflows of quadratic disagreement functions on general Riemannian manifolds. One\nsystem is intrinsic while the other is extrinsic. We derive necessary\nconditions for the agents to synchronize from almost all initial conditions\nwhen the graph used to model the network is connected. If a Riemannian manifold\ncontains a closed curve of locally minimum length, then there is a connected\ngraph and a dense set of initial conditions from which the intrinsic system\nfails to synchronize. The extrinsic system fails to synchronize if the manifold\nis multiply connected. The extrinsic system appears in the Kuramoto model on\n$\\smash{\\mathcal{S}^1}$, rigid-body attitude synchronization on\n$\\mathsf{SO}(3)$, the Lohe model of quantum synchronization on the $n$-sphere,\nand the Lohe model on $\\mathsf{U}(n)$. Except for the Lohe model on the\n$n$-sphere where $n\\in\\mathbb{N}\\backslash\\{1\\}$, there are dense sets of\ninitial conditions on which these systems fail to synchronize. The reason for\nthis difference is that the $n$-sphere is simply connected for all\n$n\\in\\mathbb{N}\\backslash\\{1\\}$ whereas the other manifolds are multiply\nconnected. \n\n"}
{"id": "1808.00911", "contents": "Title: Detector monitoring with artificial neural networks at the CMS\n  experiment at the CERN Large Hadron Collider Abstract: Reliable data quality monitoring is a key asset in delivering collision data\nsuitable for physics analysis in any modern large-scale High Energy Physics\nexperiment. This paper focuses on the use of artificial neural networks for\nsupervised and semi-supervised problems related to the identification of\nanomalies in the data collected by the CMS muon detectors. We use deep neural\nnetworks to analyze LHC collision data, represented as images organized\ngeographically. We train a classifier capable of detecting the known anomalous\nbehaviors with unprecedented efficiency and explore the usage of convolutional\nautoencoders to extend anomaly detection capabilities to unforeseen failure\nmodes. A generalization of this strategy could pave the way to the automation\nof the data quality assessment process for present and future high-energy\nphysics experiments. \n\n"}
{"id": "1808.01181", "contents": "Title: Robust Spectral Filtering and Anomaly Detection Abstract: We consider a setting, where the output of a linear dynamical system (LDS)\nis, with an unknown but fixed probability, replaced by noise. There, we present\na robust method for the prediction of the outputs of the LDS and identification\nof the samples of noise, and prove guarantees on its statistical performance.\nOne application lies in anomaly detection: the samples of noise, unlikely to\nhave been generated by the dynamics, can be flagged to operators of the system\nfor further study. \n\n"}
{"id": "1808.03794", "contents": "Title: Magnetic microstructure machine learning analysis Abstract: We use a machine learning approach to identify the importance of\nmicrostructure characteristics in causing magnetization reversal in ideally\nstructured large-grained Nd$_2$Fe$_{14}$B permanent magnets. The embedded\nStoner-Wohlfarth method is used as a reduced order model for determining local\nswitching field maps which guide the data-driven learning procedure. The\npredictor model is a random forest classifier which we validate by comparing\nwith full micromagnetic simulations in the case of small granular test\nstructures. In the course of the machine learning microstructure analysis the\nmost important features explaining magnetization reversal were found to be the\nmisorientation and the position of the grain within the magnet. The lowest\nswitching fields occur near the top and bottom edges of the magnet. While the\ndependence of the local switching field on the grain orientation is known from\ntheory, the influence of the position of the grain on the local coercive field\nstrength is less obvious. As a direct result of our findings of the machine\nlearning analysis we show that edge hardening via Dy-diffusion leads to higher\ncoercive fields. \n\n"}
{"id": "1808.05156", "contents": "Title: An Analysis of Asynchronous Stochastic Accelerated Coordinate Descent Abstract: Gradient descent, and coordinate descent in particular, are core tools in\nmachine learning and elsewhere. Large problem instances are common. To help\nsolve them, two orthogonal approaches are known: acceleration and parallelism.\nIn this work, we ask whether they can be used simultaneously. The answer is\n\"yes\".\n  More specifically, we consider an asynchronous parallel version of the\naccelerated coordinate descent algorithm proposed and analyzed by Lin, Liu and\nXiao (SIOPT'15). We give an analysis based on the efficient implementation of\nthis algorithm. The only constraint is a standard bounded asynchrony\nassumption, namely that each update can overlap with at most q others. (q is at\nmost the number of processors times the ratio in the lengths of the longest and\nshortest updates.) We obtain the following three results:\n  1. A linear speedup for strongly convex functions so long as q is not too\nlarge.\n  2. A substantial, albeit sublinear, speedup for strongly convex functions for\nlarger q.\n  3. A substantial, albeit sublinear, speedup for convex functions. \n\n"}
{"id": "1808.07857", "contents": "Title: Probabilistic Multilayer Networks Abstract: Here we introduce probabilistic weighted and unweighted multilayer networks\nas derived from information theoretical correlation measures on large\nmultidimensional datasets. We present the fundamentals of the formal\napplication of probabilistic inference on problems embedded in multilayered\nenvironments, providing examples taken from the analysis of biological and\nsocial systems: cancer genomics and drug-related violence. \n\n"}
{"id": "1808.09807", "contents": "Title: Continuous-time Duality for Super-replication with Transient Price\n  Impact Abstract: We establish a super-replication duality in a continuous-time financial model\nwhere an investor's trades adversely affect bid- and ask-prices for a risky\nasset and where market resilience drives the resulting spread back towards zero\nat an exponential rate. Similar to the literature on models with a constant\nspread, our dual description of super-replication prices involves the\nconstruction of suitable absolutely continuous measures with martingales close\nto the unaffected reference price. A novel feature in our duality is a\nliquidity weighted $L^2$-norm that enters as a measurement of this closeness\nand that accounts for strategy dependent spreads. As applications, we establish\noptimality of buy-and-hold strategies for the super-replication of call options\nand we prove a verification theorem for utility maximizing investment\nstrategies. \n\n"}
{"id": "1809.01106", "contents": "Title: Distributed Nonconvex Constrained Optimization over Time-Varying\n  Digraphs Abstract: This paper considers nonconvex distributed constrained optimization over\nnetworks, modeled as directed (possibly time-varying) graphs. We introduce the\nfirst algorithmic framework for the minimization of the sum of a smooth\nnonconvex (nonseparable) function--the agent's sum-utility--plus a\nDifference-of-Convex (DC) function (with nonsmooth convex part). This general\nformulation arises in many applications, from statistical machine learning to\nengineering. The proposed distributed method combines successive convex\napproximation techniques with a judiciously designed perturbed push-sum\nconsensus mechanism that aims to track locally the gradient of the (smooth part\nof the) sum-utility. Sublinear convergence rate is proved when a fixed\nstep-size (possibly different among the agents) is employed whereas asymptotic\nconvergence to stationary solutions is proved using a diminishing step-size.\nNumerical results show that our algorithms compare favorably with current\nschemes on both convex and nonconvex problems. \n\n"}
{"id": "1809.01218", "contents": "Title: The Saddle Point Problem of Polynomials Abstract: This paper studies the saddle point problem of polynomials. We give an\nalgorithm for computing saddle points. It is based on solving Lasserre's\nhierarchy of semidefinite relaxations. Under some genericity assumptions on\ndefining polynomials, we show that: i) if there exists a saddle point, our\nalgorithm can get one by solving a finite number of Lasserre type semidefinite\nrelaxations; ii) if there is no saddle point, our algorithm can detect its\nnonexistence. \n\n"}
{"id": "1809.01275", "contents": "Title: Solving Non-smooth Constrained Programs with Lower Complexity than\n  $\\mathcal{O}(1/\\varepsilon)$: A Primal-Dual Homotopy Smoothing Approach Abstract: We propose a new primal-dual homotopy smoothing algorithm for a linearly\nconstrained convex program, where neither the primal nor the dual function has\nto be smooth or strongly convex. The best known iteration complexity solving\nsuch a non-smooth problem is $\\mathcal{O}(\\varepsilon^{-1})$. In this paper, we\nshow that by leveraging a local error bound condition on the dual function, the\nproposed algorithm can achieve a better primal convergence time of\n$\\mathcal{O}\\left(\\varepsilon^{-2/(2+\\beta)}\\log_2(\\varepsilon^{-1})\\right)$,\nwhere $\\beta\\in(0,1]$ is a local error bound parameter. As an example\napplication of the general algorithm, we show that the distributed geometric\nmedian problem, which can be formulated as a constrained convex program, has\nits dual function non-smooth but satisfying the aforementioned local error\nbound condition with $\\beta=1/2$, therefore enjoying a convergence time of\n$\\mathcal{O}\\left(\\varepsilon^{-4/5}\\log_2(\\varepsilon^{-1})\\right)$. This\nresult improves upon the $\\mathcal{O}(\\varepsilon^{-1})$ convergence time bound\nachieved by existing distributed optimization algorithms. Simulation\nexperiments also demonstrate the performance of our proposed algorithm. \n\n"}
{"id": "1809.01731", "contents": "Title: Quantum algorithms and lower bounds for convex optimization Abstract: While recent work suggests that quantum computers can speed up the solution\nof semidefinite programs, little is known about the quantum complexity of more\ngeneral convex optimization. We present a quantum algorithm that can optimize a\nconvex function over an $n$-dimensional convex body using $\\tilde{O}(n)$\nqueries to oracles that evaluate the objective function and determine\nmembership in the convex body. This represents a quadratic improvement over the\nbest-known classical algorithm. We also study limitations on the power of\nquantum computers for general convex optimization, showing that it requires\n$\\tilde{\\Omega}(\\sqrt n)$ evaluation queries and $\\Omega(\\sqrt{n})$ membership\nqueries. \n\n"}
{"id": "1809.04198", "contents": "Title: Optimization with Non-Differentiable Constraints with Applications to\n  Fairness, Recall, Churn, and Other Goals Abstract: We show that many machine learning goals, such as improved fairness metrics,\ncan be expressed as constraints on the model's predictions, which we call rate\nconstraints. We study the problem of training non-convex models subject to\nthese rate constraints (or any non-convex and non-differentiable constraints).\nIn the non-convex setting, the standard approach of Lagrange multipliers may\nfail. Furthermore, if the constraints are non-differentiable, then one cannot\noptimize the Lagrangian with gradient-based methods. To solve these issues, we\nintroduce the proxy-Lagrangian formulation. This new formulation leads to an\nalgorithm that produces a stochastic classifier by playing a two-player\nnon-zero-sum game solving for what we call a semi-coarse correlated\nequilibrium, which in turn corresponds to an approximately optimal and feasible\nsolution to the constrained optimization problem. We then give a procedure\nwhich shrinks the randomized solution down to one that is a mixture of at most\n$m+1$ deterministic solutions, given $m$ constraints. This culminates in\nalgorithms that can solve non-convex constrained optimization problems with\npossibly non-differentiable and non-convex constraints with theoretical\nguarantees. We provide extensive experimental results enforcing a wide range of\npolicy goals including different fairness metrics, and other goals on accuracy,\ncoverage, recall, and churn. \n\n"}
{"id": "1809.04618", "contents": "Title: Global Convergence of Stochastic Gradient Hamiltonian Monte Carlo for\n  Non-Convex Stochastic Optimization: Non-Asymptotic Performance Bounds and\n  Momentum-Based Acceleration Abstract: Stochastic gradient Hamiltonian Monte Carlo (SGHMC) is a variant of\nstochastic gradient with momentum where a controlled and properly scaled\nGaussian noise is added to the stochastic gradients to steer the iterates\ntowards a global minimum. Many works reported its empirical success in practice\nfor solving stochastic non-convex optimization problems, in particular it has\nbeen observed to outperform overdamped Langevin Monte Carlo-based methods such\nas stochastic gradient Langevin dynamics (SGLD) in many applications. Although\nasymptotic global convergence properties of SGHMC are well known, its\nfinite-time performance is not well-understood. In this work, we study two\nvariants of SGHMC based on two alternative discretizations of the underdamped\nLangevin diffusion. We provide finite-time performance bounds for the global\nconvergence of both SGHMC variants for solving stochastic non-convex\noptimization problems with explicit constants. Our results lead to\nnon-asymptotic guarantees for both population and empirical risk minimization\nproblems. For a fixed target accuracy level, on a class of non-convex problems,\nwe obtain complexity bounds for SGHMC that can be tighter than those for SGLD.\nThese results show that acceleration with momentum is possible in the context\nof global non-convex optimization. \n\n"}
{"id": "1809.04962", "contents": "Title: Effect of centrality bin width corrections on two-particle number and\n  transverse momentum differential correlation functions Abstract: Two-particle number and transverse momentum differential correlation\nfunctions are powerful tools for unveiling the detailed dynamics and particle\nproduction mechanisms involved in relativistic heavy-ion collisions.\nMeasurements of transverse momentum correlators $P_2$ and $G_2$, in particular,\nprovide added information not readily accessible with better known number\ncorrelation functions $R_2$. However, it is found that the $R_2$ and $G_2$\ncorrelators are somewhat sensitive to the details of the experimental procedure\nused to measure them. They exhibit, in particular, a dependence on the\ncollision centrality bin width, which may have a rather detrimental impact on\ntheir physical interpretation. A technique to correct these correlators for\ncollision centrality bin-width averaging is presented. The technique is based\non the hypothesis that the shape of single- and pair- probability densities\nvary slower with collision centrality than the corresponding integrated yields.\nThe technique is tested with Pb-Pb simulations based on the HIJING and\nultrarelativistic quantum molecular dynamics models and shown to enable a\nprecision better than 1% for particles in the kinematic range $0.2 \\leq p_{\\rm\nT} \\leq 2.0$ GeV/$c$. \n\n"}
{"id": "1809.05583", "contents": "Title: Image registration and super resolution from first principles Abstract: Image registration is the inference of transformations relating noisy and\ndistorted images. It is fundamental in computer vision, experimental physics,\nand medical imaging. Many algorithms and analyses exist for inferring shift,\nrotation, and nonlinear transformations between image coordinates. Even in the\nsimplest case of translation, however, all known algorithms are biased and none\nhave achieved the precision limit of the Cramer Rao bound (CRB). Following\nBayesian inference, we prove that the standard method of shifting one image to\nmatch another cannot reach the CRB. We show that the bias can be cured and the\nCRB reached if, instead, we use Super Registration: learning an optimal model\nfor the underlying image and shifting that to match the data. Our theory shows\nthat coarse-graining oversampled images can improve registration precision of\nthe standard method. For oversampled data, our method does not yield striking\nimprovements as measured by eye. In these cases, however, we show our new\nregistration method can lead to dramatic improvements in extractable\ninformation, for example, inferring $10\\times$ more precise particle positions. \n\n"}
{"id": "1809.05905", "contents": "Title: From Integrable to Chaotic Systems: Universal Local Statistics of\n  Lyapunov exponents Abstract: Systems where time evolution follows a multiplicative process are ubiquitous\nin physics. We study a toy model for such systems where each time step is given\nby multiplication with an independent random $N\\times N$ matrix with complex\nGaussian elements, the complex Ginibre ensemble. This model allows to\nexplicitly compute the Lyapunov exponents and local correlations amongst them,\nwhen the number of factors $M$ becomes large. While the smallest eigenvalues\nalways remain deterministic, which is also the case for many chaotic quantum\nsystems, we identify a critical double scaling limit $N\\sim M$ for the rest of\nthe spectrum. It interpolates between the known deterministic behaviour of the\nLyapunov exponents for $M\\gg N$ (or $N$ fixed) and universal random matrix\nstatistics for $M\\ll N$ (or $M$ fixed), characterising chaotic behaviour. After\nunfolding this agrees with Dyson's Brownian Motion starting from equidistant\npositions in the bulk and at the soft edge of the spectrum. This universality\nstatement is further corroborated by numerical experiments, multiplying\ndifferent kinds of random matrices. It leads us to conjecture a much wider\napplicability in complex systems, that display a transition from deterministic\nto chaotic behaviour. \n\n"}
{"id": "1809.05933", "contents": "Title: Day-to-day dynamic traffic assignment model with variable message signs\n  and endogenous user compliance Abstract: This paper proposes a dual-time-scale, day-to-day dynamic traffic assignment\nmodel that takes into account variable message signs (VMS) and its interactions\nwith drivers' travel choices and adaptive learning processes. The within-day\ndynamic is captured by a dynamic network loading problem with en route update\nof path choices influenced by the VMS; the day-to-day dynamic is captured by a\nsimultaneous route-and-departure-time adjustment process that employs bounded\nuser rationality. Moreover, we describe the evolution of the VMS compliance\nrate by modeling drivers' learning processes. We endogenize traffic dynamics,\nroute and departure time choices, travel delays, and VMS compliance, and\nthereby captur their interactions and interdependencies in a holistic manner. A\ncase study in the west end of Glasgow is carried out to understand the impact\nof VMS has on road congestion and route choices in both the short and long run.\nOur main findings include an adverse effect of the VMS on the network\nperformance in the long run (the 'rebound' effect), and existence of an\nequilibrium state where both traffic and VMS compliance are stabilized. \n\n"}
{"id": "1809.06966", "contents": "Title: Astrophysical S-factors, thermonuclear rates, and electron screening\n  potential for the $^3$He(d,p)$^{4}$He Big Bang reaction via a hierarchical\n  Bayesian model Abstract: We developed a hierarchical Bayesian framework to estimate S-factors and\nthermonuclear rates for the $^3$He(d,p)$^{4}$He reaction, which impacts the\nprimordial abundances of $^3$He and $^7$Li. The available data are evaluated\nand all direct measurements are taken into account in our analysis for which we\ncan estimate separate uncertainties for systematic and statistical effects. For\nthe nuclear reaction model, we adopt a single-level, two-channel approximation\nof R-matrix theory, suitably modified to take the effects of electron screening\nat lower energies into account. Apart from the usual resonance parameters\n(resonance location and reduced widths for the incoming and outgoing reaction\nchannel), we include for the first time the channel radii and boundary\ncondition in the fitting process. Our new analysis of the $^3$He(d,p)$^{4}$He\nS-factor data results in improved estimates for the thermonuclear rates. This\nwork represents the first nuclear rate evaluation using the R-matrix theory\nembedded into a hierarchical Bayesian framework, properly accounting for all\nknown sources of uncertainty. Therefore, it provides a test bed for future\nstudies of more complex reactions. \n\n"}
{"id": "1809.07288", "contents": "Title: Time-varying Projected Dynamical Systems with Applications to Feedback\n  Optimization of Power Systems Abstract: This paper is concerned with the study of continuous-time, non-smooth\ndynamical systems which arise in the context of time-varying non-convex\noptimization problems, as for example the feedback-based optimization of power\nsystems. We generalize the notion of projected dynamical systems to\ntime-varying, possibly non-regular, domains and derive conditions for the\nexistence of so-called Krasovskii solutions. The key insight is that for\ntrajectories to exist, informally, the time-varying domain can only contract at\na bounded rate whereas it may expand discontinuously. This condition is met, in\nparticular, by feasible sets delimited via piecewise differentiable functions\nunder appropriate constraint qualifications. To illustrate the necessity and\nusefulness of such a general framework, we consider a simple yet insightful\npower system example, and we discuss the implications of the proposed\nconditions for the design of feedback optimization schemes. \n\n"}
{"id": "1809.07376", "contents": "Title: Decentralized Resource Allocation via Dual Consensus ADMM Abstract: We consider a resource allocation problem over an undirected network of\nagents, where edges of the network define communication links. The goal is to\nminimize the sum of agent-specific convex objective functions, while the\nagents' decisions are coupled via a convex conic constraint. We derive two\nmethods by applying the alternating direction method of multipliers (ADMM) for\ndecentralized consensus optimization to the dual of our resource allocation\nproblem. Both methods are fully parallelizable and decentralized in the sense\nthat each agent exchanges information only with its neighbors in the network\nand requires only its own data for updating its decision. We prove convergence\nof the proposed methods and demonstrate their effectiveness with a numerical\nexample. \n\n"}
{"id": "1809.07383", "contents": "Title: Geometric Convergence of Gradient Play Algorithms for Distributed Nash\n  Equilibrium Seeking Abstract: We study distributed algorithms for seeking a Nash equilibrium in a class of\nnon-cooperative convex games with strongly monotone mappings. Each player has\naccess to her own smooth local cost function and can communicate to her\nneighbors in some undirected graph. To deal with fast distributed learning of\nNash equilibria under such settings, we introduce a so called augmented game\nmapping and provide conditions under which this mapping is strongly monotone.\nWe consider a distributed gradient play algorithm for determining a Nash\nequilibrium (GRANE). The algorithm involves every player performing a gradient\nstep to minimize her own cost function while sharing and retrieving information\nlocally among her neighbors in the network. Using the reformulation of the Nash\nequilibrium problem based on the strong monotone augmented game mapping, we\nprove the convergence of this algorithm to a Nash equilibrium with a geometric\nrate. Further, we introduce the Nesterov type acceleration for the gradient\nplay algorithm. We demonstrate that, similarly to the accelerated algorithms in\ncentralized optimization and variational inequality problems, our accelerated\nalgorithm outperforms GRANE in the convergence rate. Moreover, to relax\nassumptions required to guarantee the strongly monotone augmented mapping, we\nanalyze the restricted strongly monotone property of this mapping and prove\ngeometric convergence of the distributed gradient play under milder\nassumptions. \n\n"}
{"id": "1809.08843", "contents": "Title: Inference of the Kinetic Ising Model with Heterogeneous Missing Data Abstract: We consider the problem of inferring a causality structure from multiple\nbinary time series by using the Kinetic Ising Model in datasets where a\nfraction of observations is missing. We take our steps from a recent work on\nMean Field methods for the inference of the model with hidden spins and develop\na pseudo-Expectation-Maximization algorithm that is able to work even in\nconditions of severe data sparsity. The methodology relies on the\nMartin-Siggia-Rose path integral method with second order saddle-point solution\nto make it possible to calculate the log-likelihood in polynomial time, giving\nas output a maximum likelihood estimate of the couplings matrix and of the\nmissing observations. We also propose a recursive version of the algorithm,\nwhere at every iteration some missing values are substituted by their maximum\nlikelihood estimate, showing that the method can be used together with\nsparsification schemes like LASSO regularization or decimation. We test the\nperformance of the algorithm on synthetic data and find interesting properties\nwhen it comes to the dependency on heterogeneity of the observation frequency\nof spins and when some of the hypotheses that are necessary to the saddle-point\napproximation are violated, such as the small couplings limit and the\nassumption of statistical independence between couplings. \n\n"}
{"id": "1809.10482", "contents": "Title: Budgeted Multi-Objective Optimization with a Focus on the Central Part\n  of the Pareto Front -- Extended Version Abstract: Optimizing nonlinear systems involving expensive computer experiments with\nregard to conflicting objectives is a common challenge. When the number of\nexperiments is severely restricted and/or when the number of objectives\nincreases, uncovering the whole set of Pareto optimal solutions is out of\nreach, even for surrogate-based approaches: the proposed solutions are\nsub-optimal or do not cover the front well. As non-compromising optimal\nsolutions have usually little point in applications, this work restricts the\nsearch to solutions that are close to the Pareto front center. The article\nstarts by characterizing this center, which is defined for any type of front.\nNext, a Bayesian multi-objective optimization method for directing the search\ntowards it is proposed. Targeting a subset of the Pareto front allows an\nimproved optimality of the solutions and a better coverage of this zone, which\nis our main concern. A criterion for detecting convergence to the center is\ndescribed. If the criterion is triggered, a widened central part of the Pareto\nfront is targeted such that sufficiently accurate convergence to it is\nforecasted within the remaining budget. Numerical experiments show how the\nresulting algorithm, C-EHI, better locates the central part of the Pareto front\nwhen compared to state-of-the-art Bayesian algorithms. \n\n"}
{"id": "1810.00089", "contents": "Title: Feedback Stabilization Using Koopman Operator Abstract: In this paper, we provide a systematic approach for the design of stabilizing\nfeedback controllers for nonlinear control systems using the Koopman operator\nframework. The Koopman operator approach provides a linear representation for a\nnonlinear dynamical system and a bilinear representation for a nonlinear\ncontrol system. The problem of feedback stabilization of a nonlinear control\nsystem is then transformed to the stabilization of a bilinear control system.\nWe propose a control Lyapunov function (CLF)-based approach for the design of\nstabilizing feedback controllers for the bilinear system. The search for\nfinding a CLF for the bilinear control system is formulated as a convex\noptimization problem. This leads to a schematic procedure for designing\nCLF-based stabilizing feedback controllers for the bilinear system and hence\nthe original nonlinear system. Another advantage of the proposed controller\ndesign approach outlined in this paper is that it does not require explicit\nknowledge of system dynamics. In particular, the bilinear representation of a\nnonlinear control system in the Koopman eigenfunction space can be obtained\nfrom time-series data. Simulation results are presented to verify the main\nresults on the design of stabilizing feedback controllers and the data-driven\naspect of the proposed approach. \n\n"}
{"id": "1810.01133", "contents": "Title: A Gini approach to spatial CO2 emissions Abstract: Combining global gridded population and fossil fuel based CO2 emission data\nat 1km scale, we investigate the spatial origin of CO2 emissions in relation to\nthe population distribution within countries. We depict the correlations\nbetween these two datasets by a quasi-Lorenz curve which enables us to discern\nthe individual contributions of densely and sparsely populated regions to the\nnational CO2 emissions. We observe pronounced country-specific characteristics\nand quantify them using an indicator resembling the Gini-index. Relating these\nindices with the degree of socio-economic development, we find that in\ndeveloping countries locations with large population tend to emit relatively\nmore CO2 and in developed countries the opposite tends to be the case. Based on\nthe relation to urban scaling we discuss the connection with CO2 emissions from\ncities. Our results show that general statements with regard to the\n(in)efficiency of large cities should be avoided as it is subject to the\nsocio-economic development of respective countries. Concerning the political\nrelevance, our results suggest a differentiated spatial prioritization in\ndeploying climate change mitigation measures in cities for developed and\ndeveloping countries. \n\n"}
{"id": "1810.01488", "contents": "Title: Using Machine Learning to Discern Eruption in Noisy Environments: A Case\n  Study using CO2-driven Cold-Water Geyser in Chimayo, New Mexico Abstract: We present an approach based on machine learning (ML) to distinguish eruption\nand precursory signals of Chimay\\'{o} geyser (New Mexico, USA) under noisy\nenvironments. This geyser can be considered as a natural analog of\n$\\mathrm{CO}_2$ intrusion into shallow water aquifers. By studying this geyser,\nwe can understand upwelling of $\\mathrm{CO}_2$-rich fluids from depth, which\nhas relevance to leak monitoring in a $\\mathrm{CO}_2$ sequestration project. ML\nmethods such as Random Forests (RF) are known to be robust multi-class\nclassifiers and perform well under unfavorable noisy conditions. However, the\nextent of the RF method's accuracy is poorly understood for this\n$\\mathrm{CO}_2$-driven geysering application. The current study aims to\nquantify the performance of RF-classifiers to discern the geyser state. Towards\nthis goal, we first present the data collected from the seismometer that is\ninstalled near the Chimay\\'{o} geyser. The seismic signals collected at this\nsite contain different types of noises such as daily temperature variations,\nseasonal trends, animal movement near the geyser, and human activity. First, we\nfilter the signals from these noises by combining the Butterworth-Highpass\nfilter and an Autoregressive method in a multi-level fashion. We show that by\ncombining these filtering techniques, in a hierarchical fashion, leads to\nreduction in the noise in the seismic data without removing the precursors and\neruption event signals. We then use RF on the filtered data to classify the\nstate of geyser into three classes -- remnant noise, precursor, and eruption\nstates. We show that the classification accuracy using RF on the filtered data\nis greater than 90\\%.These aspects make the proposed ML framework attractive\nfor event discrimination and signal enhancement under noisy conditions, with\nstrong potential for application to monitoring leaks in $\\mathrm{CO}_2$\nsequestration. \n\n"}
{"id": "1810.02429", "contents": "Title: Restarting Frank-Wolfe: Faster Rates Under H\\\"olderian Error Bounds Abstract: Conditional Gradient algorithms (aka Frank-Wolfe algorithms) form a classical\nset of methods for constrained smooth convex minimization due to their\nsimplicity, the absence of projection steps, and competitive numerical\nperformance. While the vanilla Frank-Wolfe algorithm only ensures a worst-case\nrate of $\\mathcal{O}(1/\\epsilon)$, various recent results have shown that for\nstrongly convex functions on polytopes, the method can be slightly modified to\nachieve linear convergence. However, this still leaves a huge gap between\nsublinear $\\mathcal{O}(1/\\epsilon)$ convergence and linear $\\mathcal{O}(\\log\n1/\\epsilon)$ convergence to reach an $\\epsilon$-approximate solution. Here, we\npresent a new variant of Conditional Gradient algorithms, that can dynamically\nadapt to the function's geometric properties using restarts and smoothly\ninterpolates between the sublinear and linear regimes. These interpolated\nconvergence rates are obtained when the optimization problem satisfies a new\ntype of error bounds, which we call \\textit{strong Wolfe primal bounds}. They\ncombine geometric information on the constraint set with H\\\"olderian Error\nBounds on the objective function. \n\n"}
{"id": "1810.03130", "contents": "Title: Nonlinear Stochastic Attitude Filters on the Special Orthogonal Group 3:\n  Ito and Stratonovich Abstract: Two nonlinear stochastic complimentary filters are developed on SO(3). They\nguarantee that errors in the Rodriguez vector and estimates are semi-globally\nuniformly ultimately bounded in mean square, and they converge to a small\nneighborhood of the origin. Simulation results are presented to illustrate the\neffectiveness of the proposed filters considering high level of uncertainties\nin angular velocity as well as body-frame vector measurements. Keywords:\nAttitude estimate, Attitude estimator, Attitude observer, Attitude filter,\nNonlinear stochastic filter, stochastic differential equations, Brownian motion\nprocess, Ito, Stratonovich, Wong Zakai, Rodriguez vector, unit-quaternion,\nspecial orthogonal group 3, Euclidean, Euler angles, Angle-axis, Mapping,\nParameterization, Representation, Robust, Invariant, Kalman Filter, Extended\nKalman Filter, Multiplicative Extended Kalman Filter, Unscented Kalman Filter,\nParticle Filter, KF, EKF, MEKF, IEKF, first, second, Partial derivative,\noperator, probability, small, error, dynamics, kinematics, equilibrium,\nasymptotic, covariance, mean square, expected value, zero, unknown,\ntime-varying, global, semi-global, stable, stability, uncertain, white noise,\nGaussian noise, colored noise, bias, vectorial, vector measurement, angular\nvelocity, singular value decomposition, bounded, rotational matrix, identity,\ndeterministic, orientation, body frame, comparison, inertial frame, rigid body,\nthree dimensional, 3D, space, Attitude Control, Lie algebra, Lie group,\nprojection, Gyroscope, Inertial measurement units, micro electromechanical\nsystems, sensor, IMUs, MEMS, Roll, Pitch, Yaw, UAVs, QUAV, SVD, Fixed, Moving,\nVehicles, Robot, Robotic System, Spacecraft, submarine, Underwater vehicle,\nProblem, advantage, integral, integration, passive complementary filter,\nDisadvantage, autonomous, Review, Overview, Survey, comparative study, pose,\nSDEs, SE(3), SO(3). \n\n"}
{"id": "1810.03358", "contents": "Title: Algorithms for local optimization of OPLS energy for large protein\n  structures Abstract: Many problems arise in computational biology can be reduced to the\nminimization of energy function, that determines on the geometry of considered\nmolecule. The solution of this problem allows in particular to solve folding\nand docking problems in structural biology. For the small molecules this\nproblem is well solved. But for the large molecules ($10^4$ atoms and more)\nthis is still an open problem. In this work we consider energy minimization\nproblem (OPLS force field) for the large molecules but with good enough initial\n(starting) point. In the paper one can find a biological explanation of this\nassumption. Due to this assumption we reduce the global optimization problem to\nthe local one. We compare different methods: gradient-free methods, gradient\ntype methods (gradient method, fast gradient method, conjugate gradients (CG),\nLBFGS), high-order (tensor) methods. We observe that the most convenient ones\nin GPU realization are fast gradient descent with special line-search and CG\n(Polak--Ribiere--Polyak), LBFGS (memory = 3 iteration). Finally, we demonstrate\nhow all these method work on real data set provided by BIOCAD. \n\n"}
{"id": "1810.03433", "contents": "Title: Characterization of minimizable Lagrangian action functionals and a dual\n  Mather theorem Abstract: We show that a necessary and sufficient condition for a smooth function on\nthe tangent bundle of a manifold to be a Lagrangian density whose action can be\nminimized is, roughly speaking, that it be the sum of a constant, a nonnegative\nfunction vanishing on the support of the minimizers, and an exact form.\n  We show that this exact form corresponds to the differential of a Lipschitz\nfunction on the manifold that is differentiable on the projection of the\nsupport of the minimizers, and its derivative there is Lipschitz. This function\ngeneralizes the notion of subsolution of the Hamilton-Jacobi equation that\nappears in weak KAM theory, and the Lipschitzity result allows for the recovery\nof Mather's celebrated 1991 result as a special case. We also show that our\nresult is sharp with several examples.\n  Finally, we apply the same type of reasoning to an example of a finite\nhorizon Legendre problem in optimal control, and together with the Lipschitzity\nresult we obtain the Hamilton-Jacobi-Bellman equation and the Maximum\nPrinciple.\n  This version contains errata correcting an issue in the published version. \n\n"}
{"id": "1810.03724", "contents": "Title: Optimal Steady-State Control for Linear Time-Invariant Systems Abstract: We consider the problem of designing a feedback controller that guides the\ninput and output of a linear time-invariant system to a minimizer of a convex\noptimization problem. The system is subject to an unknown disturbance that\ndetermines the feasible set defined by the system equilibrium constraints. Our\nproposed design enforces the Karush-Kuhn-Tucker optimality conditions in\nsteady-state without incorporating dual variables into the controller. We prove\nthat the input and output variables achieve optimality in equilibrium and\noutline two procedures for designing controllers that stabilize the closed-loop\nsystem. We explore key ideas through simple examples and simulations. \n\n"}
{"id": "1810.05217", "contents": "Title: Stochastic reachability of a target tube: Theory and computation Abstract: Probabilistic guarantees of safety and performance are important in\nconstrained dynamical systems with stochastic uncertainty. We consider the\nstochastic reachability problem, which maximizes the probability that the state\nremains within time-varying state constraints (i.e., a ``target tube''),\ndespite bounded control authority. This problem subsumes the stochastic\nviability and terminal hitting-time stochastic reach-avoid problems. Of special\ninterest is the stochastic reach set, the set of all initial states from which\nit is possible to stay in the target tube with a probability above a desired\nthreshold. We provide sufficient conditions under which the stochastic reach\nset is closed, compact, and convex, and provide an underapproximative\ninterpolation technique for stochastic reach sets. Utilizing convex\noptimization, we propose a scalable and grid-free algorithm that computes a\npolytopic underapproximation of the stochastic reach set and synthesizes an\nopen-loop controller. This algorithm is anytime, i.e., it produces a valid\noutput even on early termination. We demonstrate the efficacy and scalability\nof our approach on several numerical examples, and show that our algorithm\noutperforms existing software tools for verification of linear systems. \n\n"}
{"id": "1810.05231", "contents": "Title: Exploiting Low-Rank Structure in Semidefinite Programming by Approximate\n  Operator Splitting Abstract: In contrast with many other convex optimization classes, state-of-the-art\nsemidefinite programming solvers are yet unable to efficiently solve large\nscale instances. This work aims to reduce this scalability gap by proposing a\nnovel proximal algorithm for solving general semidefinite programming problems.\nThe proposed methodology, based on the primal-dual hybrid gradient method,\nallows the presence of linear inequalities without the need of adding extra\nslack variables and avoids solving a linear system at each iteration. More\nimportantly, it does simultaneously compute the dual variables associated with\nthe linear constraints. The main contribution of this work is to achieve a\nsubstantial speedup by effectively adjusting the proposed algorithm in order to\nexploit the low-rank property inherent to several semidefinite programming\nproblems. This proposed modification is the key element that allows the\noperator splitting method to efficiently scale to larger instances. Convergence\nguarantees are presented along with an intuitive interpretation of the\nalgorithm. Additionally, an open source semidefinite programming solver, called\nProxSDP, is made available and implementation details are discussed. Case\nstudies are presented in order to evaluate the performance of the proposed\nmethodology. \n\n"}
{"id": "1810.05876", "contents": "Title: A space-time pseudospectral discretization method for solving diffusion\n  optimal control problems with two-sided fractional derivatives Abstract: We propose a direct numerical method for the solution of an optimal control\nproblem governed by a two-side space-fractional diffusion equation. The\npresented method contains two main steps. In the first step, the space variable\nis discretized by using the Jacobi-Gauss pseudospectral discretization and, in\nthis way, the original problem is transformed into a classical integer-order\noptimal control problem. The main challenge, which we faced in this step, is to\nderive the left and right fractional differentiation matrices. In this respect,\nnovel techniques for derivation of these matrices are presented. In the second\nstep, the Legendre-Gauss-Radau pseudospectral method is employed. With these\ntwo steps, the original problem is converted into a convex quadratic\noptimization problem, which can be solved efficiently by available methods. Our\napproach can be easily implemented and extended to cover fractional optimal\ncontrol problems with state constraints. Five test examples are provided to\ndemonstrate the efficiency and validity of the presented method. The results\nshow that our method reaches the solutions with good accuracy and a low CPU\ntime. \n\n"}
{"id": "1810.06108", "contents": "Title: A sharp estimate for the first Robin-Laplacian eigenvalue with negative\n  boundary parameter Abstract: In this paper we prove that the ball maximizes the first eigenvalue of the\nRobin Laplacian operator with negative boundary parameter, among all convex\nsets of \\mathbb{R}^n with prescribed perimeter. The key of the proof is a\ndearrangement procedure of the first eigenfunction of the ball on the level\nsets of the distance function to the boundary of the convex set, which controls\nthe boundary and the volume energies of the Rayleigh quotient. \n\n"}
{"id": "1810.06256", "contents": "Title: A Polynomial-Time Method for Testing Admissibility of Uncertain Power\n  Injections in Microgrids Abstract: We study the admissibility of power injections in single-phase microgrids,\nwhere the electrical state is represented by complex nodal voltages and\ncontrolled by nodal power injections. Assume that (i) there is an initial\nelectrical state that satisfies security constraints and the non-singularity of\nload-flow Jacobian, and (ii) power injections reside in some uncertainty set.\nWe say that the uncertainty set is admissible for the initial electrical state\nif any continuous trajectory of the electrical state is ensured to be secured\nand non-singular as long as power injections remain in the uncertainty set. We\nuse the recently proposed V-control and show two new results. First, if a\ncomplex nodal voltage set V is convex and every element in V is nonsingular,\nthen V is a domain of uniqueness. Second, we give sufficient conditions to\nguarantee that every element in some power injection set S has a load-flow\nsolution in V, based on impossibility of obtaining load-flow solutions at the\nboundary of V. By these results, we develop a framework for the\nadmissibility-test method; this framework is extensible to multi-phase grids.\nWithin the framework, we establish a polynomial-time method, using the\ninfeasibility check of convex optimizations. The method is evaluated\nnumerically. \n\n"}
{"id": "1810.06713", "contents": "Title: A Chebyshev-Accelerated Primal-Dual Method for Distributed Optimization Abstract: We consider a distributed optimization problem over a network of agents\naiming to minimize a global objective function that is the sum of local convex\nand composite cost functions. To this end, we propose a distributed\nChebyshev-accelerated primal-dual algorithm to achieve faster ergodic\nconvergence rates. In standard distributed primal-dual algorithms, the speed of\nconvergence towards a global optimum (i.e., a saddle point in the corresponding\nLagrangian function) is directly influenced by the eigenvalues of the Laplacian\nmatrix representing the communication graph. In this paper, we use Chebyshev\nmatrix polynomials to generate gossip matrices whose spectral properties result\nin faster convergence speeds, while allowing for a fully distributed\nimplementation. As a result, the proposed algorithm requires fewer gradient\nupdates at the cost of additional rounds of communications between agents. We\nillustrate the performance of the proposed algorithm in a distributed signal\nrecovery problem. Our simulations show how the use of Chebyshev matrix\npolynomials can be used to improve the convergence speed of a primal-dual\nalgorithm over communication networks, especially in networks with poor\nspectral properties, by trading local computation by communication rounds. \n\n"}
{"id": "1810.10690", "contents": "Title: SpiderBoost and Momentum: Faster Stochastic Variance Reduction\n  Algorithms Abstract: SARAH and SPIDER are two recently developed stochastic variance-reduced\nalgorithms, and SPIDER has been shown to achieve a near-optimal first-order\noracle complexity in smooth nonconvex optimization. However, SPIDER uses an\naccuracy-dependent stepsize that slows down the convergence in practice, and\ncannot handle objective functions that involve nonsmooth regularizers. In this\npaper, we propose SpiderBoost as an improved scheme, which allows to use a much\nlarger constant-level stepsize while maintaining the same near-optimal oracle\ncomplexity, and can be extended with proximal mapping to handle composite\noptimization (which is nonsmooth and nonconvex) with provable convergence\nguarantee. In particular, we show that proximal SpiderBoost achieves an oracle\ncomplexity of $\\mathcal{O}(\\min\\{n^{1/2}\\epsilon^{-2},\\epsilon^{-3}\\})$ in\ncomposite nonconvex optimization, improving the state-of-the-art result by a\nfactor of $\\mathcal{O}(\\min\\{n^{1/6},\\epsilon^{-1/3}\\})$. We further develop a\nnovel momentum scheme to accelerate SpiderBoost for composite optimization,\nwhich achieves the near-optimal oracle complexity in theory and substantial\nimprovement in experiments. \n\n"}
{"id": "1810.11130", "contents": "Title: Robust Importance Sampling with Adaptive Winsorization Abstract: Importance sampling is a widely used technique to estimate properties of a\ndistribution. This paper investigates trading-off some bias for variance by\nadaptively winsorizing the importance sampling estimator. The novel winsorizing\nprocedure, based on the Balancing Principle (or Lepskii's Method), chooses a\nthreshold level among a pre-defined set by roughly balancing the bias and\nvariance of the estimator when winsorized at different levels. As a\nconsequence, it provides a principled way to perform winsorization with\nfinite-sample optimality guarantees under minimal assumptions. In various\nexamples, the proposed estimator is shown to have smaller mean squared error\nand mean absolute deviation than leading alternatives. \n\n"}
{"id": "1810.11229", "contents": "Title: Null-controllability and control cost estimates for the heat equation on\n  unbounded and large bounded domains Abstract: We survey recent results on the control problem for the heat equation on\nunbounded and large bounded domains. First we formulate new uncertainty\nrelations, respectively spectral inequalities. Then we present an abstract\ncontrol cost estimate which improves upon earlier results. It is particularly\ninteresting when combined with the earlier mentioned spectral inequalities\nsince it yields sharp control cost bounds in several asymptotic regimes. We\nalso show that control problems on unbounded domains can be approximated by\ncorresponding problems on a sequence of bounded domains forming an exhaustion.\nOur results apply also for the generalized heat equation associated with a\nSchr\\\"odinger semigroup. \n\n"}
{"id": "1810.11235", "contents": "Title: Rational Quartic Spectrahedra Abstract: Rational quartic spectrahedra in 3-space are semialgebraic convex subsets in\n$\\mathbb{R}^3$ of semidefinite, real symmetric $(4 \\times 4)$-matrices, whose\nboundary admits a rational parameterization. The Zariski closure in\n$\\mathbb{C}\\mathbb{P}^3$ of the boundary of a rational spectrahedron is a\nrational complex symmetroid. We give necessary conditions on the configurations\nof singularities of the corresponding real symmetroids in\n$\\mathbb{R}\\mathbb{P}^3$ of rational quartic spectrahedra. We provide an almost\nexhaustive list of examples realizing the configurations, and conjecture that\nthe missing example does not occur. \n\n"}
{"id": "1810.12036", "contents": "Title: Small noise limit and convexity for generalized incompressible flows,\n  Schr\\\"odinger problems, and optimal transport Abstract: This paper is concerned with six variational problems and their mutual\nconnections: The quadratic Monge-Kantorovich optimal transport, the\nSchr\\\"odinger problem, Brenier's relaxed model for incompressible fluids, the\nso-called Br\\\"odinger problem recently introduced by M. Arnaudon & al. [3], the\nmultiphase Brenier model, and the multiphase Br\\\"odinger problem. All of them\ninvolve the minimization of a kinetic action and/or a relative entropy of some\npath measures with respect to the reversible Brownian motion. As the viscosity\nparameter $\\nu\\to 0$ we establish Gamma-convergence relations between the\ncorresponding problems, and prove the convergence of the associated pressures\narising from the incompressibility constraints. We also present new results on\nthe time-convexity of the entropy for some of the dynamical interpolations.\nAlong the way we extend previous results by H. Lavenant [30] and J-D. Benamou &\nal. [10]. \n\n"}
{"id": "1810.12997", "contents": "Title: An Online-Learning Approach to Inverse Optimization Abstract: In this paper, we demonstrate how to learn the objective function of a\ndecision-maker while only observing the problem input data and the\ndecision-maker's corresponding decisions over multiple rounds. We present exact\nalgorithms for this online version of inverse optimization which converge at a\nrate of $ \\mathcal{O}(1/\\sqrt{T}) $ in the number of observations~$T$ and\ncompare their further properties. Especially, they all allow taking decisions\nwhich are essentially as good as those of the observed decision-maker already\nafter relatively few iterations, but are suited best for different settings\neach. Our approach is based on online learning and works for linear objectives\nover arbitrary feasible sets for which we have a linear optimization oracle. As\nsuch, it generalizes previous approaches based on KKT-system decomposition and\ndualization. We also introduce several generalizations, such as the approximate\nlearning of non-linear objective functions, dynamically changing as well as\nparameterized objectives and the case of suboptimal observed decisions. When\napplied to the stochastic offline case, our algorithms are able to give\nguarantees on the quality of the learned objectives in expectation. Finally, we\nshow the effectiveness and possible applications of our methods in indicative\ncomputational experiments. \n\n"}
{"id": "1810.13245", "contents": "Title: Fast Convergence Rates of Distributed Subgradient Methods with Adaptive\n  Quantization Abstract: We study distributed optimization problems over a network when the\ncommunication between the nodes is constrained, and so information that is\nexchanged between the nodes must be quantized. Recent advances using the\ndistributed gradient algorithm with a quantization scheme at a fixed resolution\nhave established convergence, but at rates significantly slower than when the\ncommunications are unquantized.\n  In this paper, we introduce a novel quantization method, which we refer to as\nadaptive quantization, that allows us to match the convergence rates under\nperfect communications. Our approach adjusts the quantization scheme used by\neach node as the algorithm progresses: as we approach the solution, we become\nmore certain about where the state variables are localized, and adapt the\nquantizer codebook accordingly.\n  We bound the convergence rates of the proposed method as a function of the\ncommunication bandwidth, the underlying network topology, and structural\nproperties of the constituent objective functions. In particular, we show that\nif the objective functions are convex or strongly convex, then using adaptive\nquantization does not affect the rate of convergence of the distributed\nsubgradient methods when the communications are quantized, except for a\nconstant that depends on the resolution of the quantizer. To the best of our\nknowledge, the rates achieved in this paper are better than any existing work\nin the literature for distributed gradient methods under finite communication\nbandwidths. We also provide numerical simulations that compare convergence\nproperties of the distributed gradient methods with and without quantization\nfor solving distributed regression problems for both quadratic and absolute\nloss functions. \n\n"}
{"id": "1811.01182", "contents": "Title: Stochastic Primal-Dual Method for Empirical Risk Minimization with\n  $\\mathcal{O}(1)$ Per-Iteration Complexity Abstract: Regularized empirical risk minimization problem with linear predictor appears\nfrequently in machine learning. In this paper, we propose a new stochastic\nprimal-dual method to solve this class of problems. Different from existing\nmethods, our proposed methods only require O(1) operations in each iteration.\nWe also develop a variance-reduction variant of the algorithm that converges\nlinearly. Numerical experiments suggest that our methods are faster than\nexisting ones such as proximal SGD, SVRG and SAGA on high-dimensional problems. \n\n"}
{"id": "1811.01372", "contents": "Title: Transient Stability Analysis of Power Systems via Occupation Measures Abstract: We propose the application of occupation measure theory to the classical\nproblem of transient stability analysis for power systems. This enables the\ncomputation of certified inner and outer approximations for the region of\nattraction of a nominal operating point. In order to determine whether a\npost-disturbance point requires corrective actions to ensure stability, one\nwould then simply need to check the sign of a polynomial evaluated at that\npoint. Thus, computationally expensive dynamical simulations are only required\nfor post-disturbance points in the region between the inner and outer\napproximations. We focus on the nonlinear swing equations but voltage dynamics\ncould also be included. The proposed approach is formulated as a hierarchy of\nsemidefinite programs stemming from an infinite-dimensional linear program in a\nmeasure space, with a natural dual sum-of-squares perspective. On the\ntheoretical side, this paper lays the groundwork for exploiting the oscillatory\nstructure of power systems by using Hermitian (instead of real) sums-of-squares\nand connects the proposed approach to recent results from algebraic geometry. \n\n"}
{"id": "1811.01988", "contents": "Title: Strong mixed-integer programming formulations for trained neural\n  networks Abstract: We present strong mixed-integer programming (MIP) formulations for\nhigh-dimensional piecewise linear functions that correspond to trained neural\nnetworks. These formulations can be used for a number of important tasks, such\nas verifying that an image classification network is robust to adversarial\ninputs, or solving decision problems where the objective function is a machine\nlearning model. We present a generic framework, which may be of independent\ninterest, that provides a way to construct sharp or ideal formulations for the\nmaximum of d affine functions over arbitrary polyhedral input domains. We apply\nthis result to derive MIP formulations for a number of the most popular\nnonlinear operations (e.g. ReLU and max pooling) that are strictly stronger\nthan other approaches from the literature. We corroborate this computationally,\nshowing that our formulations are able to offer substantial improvements in\nsolve time on verification tasks for image classification networks. \n\n"}
{"id": "1811.02157", "contents": "Title: Solution Refinement at Regular Points of Conic Problems Abstract: Most numerical methods for conic problems use the homogenous primal-dual\nembedding, which yields a primal-dual solution or a certificate establishing\nprimal or dual infeasibility. Following Patrinos (and others, 2018), we express\nthe embedding as the problem of finding a zero of a mapping containing a\nskew-symmetric linear function and projections onto cones and their duals. We\nfocus on the special case when this mapping is regular, i.e., differentiable\nwith nonsingular derivative matrix, at a solution point. While this is not\nalways the case, it is a very common occurrence in practice. We propose a\nsimple method that uses LSQR, a variant of conjugate gradients for least\nsquares problems, and the derivative of the residual mapping to refine an\napproximate solution, i.e., to increase its accuracy. LSQR is a matrix-free\nmethod, i.e., requires only the evaluation of the derivative mapping and its\nadjoint, and so avoids forming or storing large matrices, which makes it\nefficient even for cone problems in which the data matrices are given and\ndense, and also allows the method to extend to cone programs in which the data\nare given as abstract linear operators. Numerical examples show that the method\nalmost always improves an approximate solution of a conic program, and often\ndramatically, at a computational cost that is typically small compared to the\ncost of obtaining the original approximate solution. For completeness we\ndescribe methods for computing the derivative of the projection onto the cones\ncommonly used in practice: nonnegative, second-order, semidefinite, and\nexponential cones. The paper is accompanied by an open source implementation. \n\n"}
{"id": "1811.05765", "contents": "Title: Koopman-Based Approach to Non-intrusive Projection-Based Reduced-Order\n  Modeling with Black-Box High-Fidelity Models. Part II: Application Abstract: A methodology for non-intrusive, projection-based non-linear model reduction\noriginally presented by Renganathan et. al.\n(2018)~\\cite{renganathan2018koopman} is further extended towards parametric\nsystems with focus on application to aerospace design. Specifically, we extend\nthe method for static systems with parametric geometry (that deforms the mesh),\nin addition to parametric boundary conditions. The main idea is to first\nperform a transformation on the governing equations such that it is lifted to a\nhigher dimensional but linear under-determined system. This enables one to\nextract the system matrices easily compared to that of the original non-linear\nsystem. The under-determined system is closed with a set of model-dependent\nnon-linear constraints upon which the model reduction is finally performed. The\nmethodology is validated on the subsonic and transonic inviscid flow past the\nNACA0012 and the RAE2822 airfoils. We further demonstrate the utility of the\napproach by applying it to two common problems in aerospace design namely,\nderivative-free global optimization and parametric uncertainty quantification\nwith Monte Carlo sampling. Overall, the methodology is shown to achieve\naccuracy upto 5\\% and computational speed-up of 2-3 orders of magnitude as that\nof the full-order model. Comparison against another non-intrusive model\nreduction method revealed that the proposed approach is more robust, accurate\nand retains the consistency between the state variables. \n\n"}
{"id": "1811.06572", "contents": "Title: A General Economic Dispatch Problem with Marginal Losses Abstract: Standard economic dispatch problems that consider line losses are linear\napproximations of a non-convex economic dispatch problem formulated by fixing\nvoltage magnitudes and assuming the decoupling of real and reactive power. This\npaper formulates and analyzes the general non-convex economic dispatch problem,\nincorporating and generalizing the Fictitious Nodal Demand (FND) model,\nresulting in a slack bus independent formulation that provides insight into\nstandard formulations by pointing out commonly used but unnecessary assumptions\nand by deriving proper choices of \"tuning parameters.\" The proper choice of\nloss allocation is derived to assign half of the losses of each transmission\nline to adjacent buses, justifying approaches in the literature. Line\nconstraints are proposed in the form of voltage angle difference limits and are\nproven equivalent to various other line limits including current magnitude\nlimits and mid-line power flow limits. The formulated general economic dispatch\nproblem with marginal losses consistently models flows and loss approximation,\nresults in approximately correct outcomes and is proven to be reference bus\nindependent. Various approximations of this problem are compared using\nrealistically large transmission network test cases. \n\n"}
{"id": "1811.07057", "contents": "Title: Universal regularization methods - varying the power, the smoothness and\n  the accuracy Abstract: Adaptive cubic regularization methods have emerged as a credible alternative\nto linesearch and trust-region for smooth nonconvex optimization, with optimal\ncomplexity amongst second-order methods. Here we consider a general/new class\nof adaptive regularization methods, that use first- or higher-order local\nTaylor models of the objective regularized by a(ny) power of the step size and\napplied to convexly-constrained optimization problems. We investigate the\nworst-case evaluation complexity/global rate of convergence of these\nalgorithms, when the level of sufficient smoothness of the objective may be\nunknown or may even be absent. We find that the methods accurately reflect in\ntheir complexity the degree of smoothness of the objective and satisfy\nincreasingly better bounds with improving accuracy of the models. The bounds\nvary continuously and robustly with respect to the regularization power and\naccuracy of the model and the degree of smoothness of the objective. \n\n"}
{"id": "1811.07659", "contents": "Title: Synthesis of Spatial Charging/Discharging Patterns of In-Vehicle\n  Batteries for Provision of Ancillary Service and Mitigation of Voltage Impact Abstract: We develop an algorithm for synthesizing a spatial pattern of\ncharging/discharging operations of in-vehicle batteries for provision of\nAncillary Service (AS) in power distribution grids. The algorithm is based on\nthe ODE (Ordinary Differential Equation) model of distribution voltage that has\nbeen recently introduced. In this paper, firstly, we derive analytical\nsolutions of the ODE model for a single straight-line feeder through a partial\nlinearization, thereby providing a physical insight to the impact of spatial EV\ncharging/discharging to the distribution voltage. Second, based on the\nanalytical solutions, we propose an algorithm for determining the values of\ncharging/discharging power (active and reactive) by in-vehicle batteries in the\nsingle feeder grid, so that the power demanded as AS (e.g. a regulation signal\nto distribution system operator for primary frequency control reserve) is\nprovided by EVs, and the deviation of distribution voltage from a nominal value\nis reduced in the grid. Effectiveness of the algorithm is established with\nnumerical simulations on the single feeder grid and on a realistic feeder grid\nwith multiple bifurcations. \n\n"}
{"id": "1811.10130", "contents": "Title: Canonical Duality Theory and Algorithm for Solving Bilevel Knapsack\n  Problems with Applications Abstract: A novel canonical duality theory (CDT) is presented for solving general\nbilevel mixed integer nonlinear optimization governed by linear and quadratic\nknapsack problems. It shows that the challenging knapsack problems can be\nsolved analytically in term of their canonical dual solutions. The existence\nand uniqueness of these analytical solutions are proved. NP-Hardness of the\nknapsack problems is discussed. A powerful CDT algorithm combined with an\nalternative iteration and a volume reduction method is proposed for solving the\nNP-hard bilevel knapsack problems. Application is illustrated by a benchmark\nproblem in optimal topology design. The performance and novelty of the proposed\nmethod are compared with the popular commercial codes. \n\n"}
{"id": "1811.10793", "contents": "Title: Anatomy of a six-parameter fit to the $b\\to s \\ell^+\\ell^-$ anomalies Abstract: Discrepancies between measurements of decay modes with an underlying quark\nlevel transition $b\\to s \\ell^+\\ell^-$ and standard model (SM) predictions have\npersisted for several years, particularly for the muon channels. The inadequacy\nof the SM becomes more compelling in a global fit. For example, Ref. [1]\ndescribed 175 observables by six parameters encoding new physics and quantified\nthe disagreement with the SM at about the $5\\sigma$ level. While certain one\nand two parameter fits have previously been considered in detail, we establish\na framework for the detailed discussion of the full 6d fit. We visualize and\nquantify the 6d $1\\sigma$ region around the best fit point and define fit\nuncertainties for both current and future observables. We then define metrics\nquantifying the deviations between measurements and both SM and best fit\npredictions. These metrics relate observables to directions in parameter space,\nrevealing their precise role in the fit, thus providing guidance for future\ntheoretical and experimental work. Some metrics further quantify the role of\ncorrelated uncertainties, which turns out to be significant. For example the\nrelevance of angular observables such as $P_5^\\prime$ is reduced in this\ncontext. Finally, studying the space of observables allows us to discuss the\ninternal tensions in the fit. \n\n"}
{"id": "1811.12710", "contents": "Title: Non-coercive first order Mean Field Games Abstract: We study first order evolutive Mean Field Games where the Hamiltonian is\nnon-coercive. This situation occurs, for instance, when some directions are\n\"forbidden\" to the generic player at some points. We establish the existence of\na weak solution of the system via a vanishing viscosity method and, mainly, we\nprove that the evolution of the population's density is the push-forward of the\ninitial density through the flow characterized almost everywhere by the optimal\ntrajectories of the control problem underlying the Hamilton-Jacobi equation. As\npreliminary steps, we need that the optimal trajectories for the control\nproblem are unique (at least for a.e. starting points) and that the optimal\ncontrols can be expressed in terms of the horizontal gradient of the value\nfunction. \n\n"}
{"id": "1812.00445", "contents": "Title: Distributed averaging integral Nash equilibrium seeking on networks Abstract: Continuous-time gradient-based Nash equilibrium seeking algorithms enjoy a\npassivity property under a suitable monotonicity assumption. This feature has\nbeen exploited to design distributed algorithms that converge to Nash\nequilibria and use local information only. We further exploit the passivity\nproperty to interconnect the algorithms with distributed averaging integral\ncontrollers that tune on-line the weights of the communication graph. The main\nadvantage is to guarantee convergence to a Nash equilibrium without requiring a\nstrong coupling condition on the algebraic connectivity of the communication\ngraph over which the players exchange information, nor a global high-gain. \n\n"}
{"id": "1812.03759", "contents": "Title: A parameterized proximal point algorithm for separable convex\n  optimization Abstract: In this paper, we develop a parameterized proximal point algorithm (P-PPA)\nfor solving a class of separable convex programming problems subject to linear\nand convex constraints. The proposed algorithm is provable to be globally\nconvergent with a worst-case O(1/t) convergence rate, wheret denotes the\niteration number. By properly choosing the algorithm parameters, numerical\nexperiments on solving a sparse optimization problem arising from statistical\nlearning show that our P-PPA could perform significantly better than other\nstate-of-the-art methods, such as the alternating direction method of\nmultipliers and the relaxed proximal point algorithm. \n\n"}
{"id": "1812.03769", "contents": "Title: Generalized Symmetric ADMM for Separable Convex Optimization Abstract: The Alternating Direction Method of Multipliers (ADMM) has been proved to be\neffective for solving separable convex optimization subject to linear\nconstraints. In this paper, we propose a Generalized Symmetric ADMM (GS-ADMM),\nwhich updates the Lagrange multiplier twice with suitable stepsizes, to solve\nthe multi-block separable convex programming. This GS-ADMM partitions the data\ninto two group variables so that one group consists of $p$ block variables\nwhile the other has $q$ block variables, where $p \\ge 1$ and $q \\ge 1$ are two\nintegers. The two grouped variables are updated in a {\\it Gauss-Seidel} scheme,\nwhile the variables within each group are updated in a {\\it Jacobi} scheme,\nwhich would make it very attractive for a big data setting. By adding proper\nproximal terms to the subproblems, we specify the domain of the stepsizes to\nguarantee that GS-ADMM is globally convergent with a worst-case $O(1/t)$\nergodic convergence rate. It turns out that our convergence domain of the\nstepsizes is significantly larger than other convergence domains in the\nliterature. Hence, the GS-ADMM is more flexible and attractive on choosing and\nusing larger stepsizes of the dual variable. Besides, two special cases of\nGS-ADMM, which allows using zero penalty terms, are also discussed and\nanalyzed. Compared with several state-of-the-art methods, preliminary numerical\nexperiments on solving a sparse matrix minimization problem in the statistical\nlearning show that our proposed method is effective and promising. \n\n"}
{"id": "1812.04370", "contents": "Title: Sparse component separation from Poisson measurements Abstract: Blind source separation (BSS) aims at recovering signals from mixtures. This\nproblem has been extensively studied in cases where the mixtures are\ncontaminated with additive Gaussian noise. However, it is not well suited to\ndescribe data that are corrupted with Poisson measurements such as in low\nphoton count optics or in high-energy astronomical imaging (e.g. observations\nfrom the Chandra or Fermi telescopes). To that purpose, we propose a novel BSS\nalgorithm coined pGMCA that specifically tackles the blind separation of sparse\nsources from Poisson measurements. \n\n"}
{"id": "1812.04517", "contents": "Title: Some Analogue of Quadratic Interpolation for a Special Class of\n  Non-Smooth Functionals and One Application to Adaptive Mirror Descent for\n  Constrained Optimization Problems Abstract: Theoretical estimates of the convergence rate of many well-known\ngradient-type optimization methods are based on quadratic interpolation,\nprovided that the Lipschitz condition for the gradient is satisfied. In this\narticle we obtain a possibility of constructing an analogue of such\ninterpolation in the class of locally Lipschitz quasi-convex functionals with\nthe special conditions of non-smoothness (Lipshitz-continuous subgradient)\nintroduced in this paper. As an application, estimates are obtained for the\nrate of convergence of the previously proposed adaptive mirror descent method\nfor the problems of minimizing a quasi-convex locally Lipschitz functional with\nseveral convex functional constraints. \n\n"}
{"id": "1812.04658", "contents": "Title: Two different origins of the Q-slope problem in superconducting niobium\n  film cavities for a heavy ion accelerator at CERN Abstract: Superconducting niobium film cavities deposited on copper substrates (Nb/Cu)\nhave suffered from strong field-dependent surface resistance, often referred to\nas the Q-slope problem, since their invention. We argue that the Q-slope may\nnot be an intrinsic problem, but rather originates from a combination of\nfactors which can be revealed in appropriate environmental conditions. In this\nstudy, extrinsic effects were carefully minimized in a series of experiments on\na seamless cavity. The origin of the Q-slope in low frequency cavities is\ntraced back to two contributions with different temperature and magnetic field\ndependences. The first component of Q-slope, affecting the residual resistance,\nis caused by trapped magnetic flux which is normally suppressed by a magnetic\nshield for bulk niobium cavities. The second, temperature dependent component\nof Q-slope, is similar to the medium-field Q-slope which is well known in bulk\nniobium cavities. These results are compared with theoretical models and\npossible future studies are proposed. \n\n"}
{"id": "1812.04941", "contents": "Title: A semi-proximal augmented Lagrangian based decomposition method for\n  primal block angular convex composite quadratic conic programming problems Abstract: We propose a semi-proximal augmented Lagrangian based decomposition method\nfor convex composite quadratic conic programming problems with primal block\nangular structures. Using our algorithmic framework, we are able to naturally\nderive several well known augmented Lagrangian based decomposition methods for\nstochastic programming such as the diagonal quadratic approximation method of\nMulvey and Ruszczy\\'{n}ski. Moreover, we are able to derive novel enhancements\nand generalizations of these well known methods. We also propose a\nsemi-proximal symmetric Gauss-Seidel based alternating direction method of\nmultipliers for solving the corresponding dual problem. Numerical results show\nthat our algorithms can perform well even for very large instances of primal\nblock angular convex QP problems. For example, one instance with more than\n$300,000$ linear constraints and $12,500,000$ nonnegative variables is solved\nin less than a minute whereas Gurobi took more than 3 hours, and another\ninstance {\\tt qp-gridgen1} with more than $331,000$ linear constraints and\n$986,000$ nonnegative variables is solved in about 5 minutes whereas Gurobi\ntook more than 35 minutes. \n\n"}
{"id": "1812.05243", "contents": "Title: A New Homotopy Proximal Variable-Metric Framework for Composite Convex\n  Minimization Abstract: This paper suggests two novel ideas to develop new proximal variable-metric\nmethods for solving a class of composite convex optimization problems. The\nfirst idea is a new parameterization of the optimality condition which allows\nus to develop a class of homotopy proximal variable-metric methods. We show\nthat under appropriate assumptions such as strong convexity-type and\nsmoothness, or self-concordance, our new schemes can achieve finite global\niteration-complexity bounds. Our second idea is a primal-dual-primal framework\nfor proximal-Newton methods which can lead to some useful computational\nfeatures for a subclass of nonsmooth composite convex optimization problems.\nStarting from the primal problem, we formulate its dual problem, and use our\nhomotopy proximal Newton method to solve this dual problem. Instead of solving\nthe subproblem directly in the dual space, we suggest to dualize this\nsubproblem to go back to the primal space. The resulting subproblem shares some\nsimilarity promoted by the regularizer of the original problem and leads to\nsome computational advantages. As a byproduct, we specialize the proposed\nalgorithm to solve covariance estimation problems. Surprisingly, our new\nalgorithm does not require any matrix inversion or Cholesky factorization, and\nfunction evaluation, while it works in the primal space with sparsity\nstructures that are promoted by the regularizer. Numerical examples on several\napplications are given to illustrate our theoretical development and to compare\nwith state-of-the-arts. \n\n"}
{"id": "1812.05325", "contents": "Title: Probing high order dependencies with information theory Abstract: Information theoretic measures (entropies, entropy rates, mutual information)\nare nowadays commonly used in statistical signal processing for real-world data\nanalysis. The present work proposes the use of Auto Mutual Information (Mutual\nInformation between subsets of the same signal) and entropy rate as powerful\ntools to assess refined dependencies of any order in signal temporal dynamics.\nNotably, it is shown how two-point Auto Mutual Information and entropy rate\nunveil information conveyed by higher order statistic and thus capture details\nof temporal dynamics that are overlooked by the (two-point) correlation\nfunction. Statistical performance of relevant estimators for Auto Mutual\nInformation and entropy rate are studied numerically, by means of Monte Carlo\nsimulations, as functions of sample size, dependence structures and hyper\nparameters that enter their definition. Further, it is shown how Auto Mutual\nInformation permits to discriminate between several different non Gaussian\nprocesses, having exactly the same marginal distribution and covariance\nfunction. Assessing higher order statistics via multipoint Auto Mutual\nInformation is also shown to unveil the global dependence structure fo these\nprocesses, indicating that one of the non Gaussian actually has temporal\ndynamics that ressembles that of a Gaussian process with same covariance while\nthe other does not. \n\n"}
{"id": "1812.07545", "contents": "Title: A class of robust consensus algorithms with predefined-time convergence\n  under switching topologies Abstract: This paper addresses the robust consensus problem under switching topologies.\nContrary to existing methods, the proposed approach provides decentralized\nprotocols that achieve consensus for networked multi-agent systems in a\npredefined time. Namely, the protocol design provides a tuning parameter that\nallows setting the convergence time of the agents to a consensus state. An\nappropriate Lyapunov analysis exposes the capability of the current proposal to\nachieve predefined-time consensus over switching topologies despite the\npresence of bounded perturbations. Finally, the paper presents a comparison\nshowing that the suggested approach subsumes existing fixed-time consensus\nalgorithms and provides extra degrees of freedom to obtain predefined-time\nconsensus protocols that are less over-engineered, i.e., the difference between\nthe estimated convergence time and its actual value is lower in our approach.\nNumerical results are given to illustrate the effectiveness and advantages of\nthe proposed approach. \n\n"}
{"id": "1812.07745", "contents": "Title: Observability Robustness under Sensor Failures: a Computational\n  Perspective Abstract: This paper studies the robustness of observability of a linear time-invariant\nsystem under sensor failures from a computational perspective. To be precise,\nthe problem of determining the minimum number of sensors whose removal can\ndestroy system observability, as well as the problem of determining the minimum\nnumber of state variables that need to be prevented from being directly\nmeasured by the existing sensors to destroy observability, is investigated. The\nfirst one is closely related to the ability of unique state reconstruction of a\nsystem under adversarial sensor attacks, and the dual of both problems are in\nthe opposite direction of the well-studied minimal controllability problems. It\nis proven that all these problems are NP-hard, both for a numerical system and\na structured system, even restricted to some special cases. It is also shown\nthat the first problems both for a numerical system and a structured one share\na cardinality-constrained submodular minimization structure, for which there is\nno known constant or logarithmic factor approximation in general. On the other\nhand, for the first two problems, under a reasonable assumption often met by\npractical systems, that the eigenvalue geometric multiplicities of the\nnumerical systems or the matching deficiencies of the structured systems are\nbounded by a constant, by levering the rank-one update property of the involved\nrank function, it is possible to obtain the corresponding optimal solutions by\ntraversing a subset of the feasible solutions. We show such a method has\npolynomial time complexity in the system dimensions and the number of sensors. \n\n"}
{"id": "1812.09808", "contents": "Title: Wasserstein Distributionally Robust Stochastic Control: A Data-Driven\n  Approach Abstract: Standard stochastic control methods assume that the probability distribution\nof uncertain variables is available. Unfortunately, in practice, obtaining\naccurate distribution information is a challenging task. To resolve this issue,\nwe investigate the problem of designing a control policy that is robust against\nerrors in the empirical distribution obtained from data. This problem can be\nformulated as a two-player zero-sum dynamic game problem, where the action\nspace of the adversarial player is a Wasserstein ball centered at the empirical\ndistribution. We propose computationally tractable value and policy iteration\nalgorithms with explicit estimates of the number of iterations required for\nconstructing an $\\epsilon$-optimal policy. We show that the contraction\nproperty of associated Bellman operators extends a single-stage out-of-sample\nperformance guarantee, obtained using a measure concentration inequality, to\nthe corresponding multi-stage guarantee without any degradation in the\nconfidence level. In addition, we characterize an explicit form of the optimal\ndistributionally robust control policy and the worst-case distribution policy\nfor linear-quadratic problems with Wasserstein penalty. Our study indicates\nthat dynamic programming and Kantorovich duality play a critical role in\nsolving and analyzing the Wasserstein distributionally robust stochastic\ncontrol problems. \n\n"}
{"id": "1901.01624", "contents": "Title: Composite optimization for robust blind deconvolution Abstract: The blind deconvolution problem seeks to recover a pair of vectors from a set\nof rank one bilinear measurements. We consider a natural nonsmooth formulation\nof the problem and show that under standard statistical assumptions, its moduli\nof weak convexity, sharpness, and Lipschitz continuity are all dimension\nindependent. This phenomenon persists even when up to half of the measurements\nare corrupted by noise. Consequently, standard algorithms, such as the\nsubgradient and prox-linear methods, converge at a rapid dimension-independent\nrate when initialized within constant relative error of the solution. We then\ncomplete the paper with a new initialization strategy, complementing the local\nsearch algorithms. The initialization procedure is both provably efficient and\nrobust to outlying measurements. Numerical experiments, on both simulated and\nreal data, illustrate the developed theory and methods. \n\n"}
{"id": "1901.01872", "contents": "Title: A Fast Distributed Asynchronous Newton-Based Optimization Algorithm Abstract: One of the most important problems in the field of distributed optimization\nis the problem of minimizing a sum of local convex objective functions over a\nnetworked system. Most of the existing work in this area focus on developing\ndistributed algorithms in a synchronous setting under the presence of a central\nclock, where the agents need to wait for the slowest one to finish the update,\nbefore proceeding to the next iterate. Asynchronous distributed algorithms\nremove the need for a central coordinator, reduce the synchronization wait, and\nallow some agents to compute faster and execute more iterations. In the\nasynchronous setting, the only known algorithms for solving this problem could\nachieve either linear or sublinear rate of convergence. In this work, we built\nupon the existing literature to develop and analyze an asynchronous\nNewton-based method to solve a penalized version of the problem. We show that\nthis algorithm guarantees almost sure convergence with global linear and local\nquadratic rate in expectation. Numerical studies confirm superior performance\nof our algorithm against other asynchronous methods. \n\n"}
{"id": "1901.03747", "contents": "Title: Small gain theorems for general networks of heterogeneous\n  infinite-dimensional systems Abstract: We prove a small-gain theorem for interconnections of $n$ nonlinear\nheterogeneous input-to-state stable (ISS) control systems of a general nature,\ncovering partial, delay and ordinary differential equations. Furthermore, for\nthe same class of control systems, we derive small-gain theorems for asymptotic\ngain, uniform global stability and weak input-to-state stability properties. We\nshow that our technique is applicable for different formulations of ISS\nproperty (summation, maximum, semimaximum) and discuss tightness of achieved\nsmall-gain theorems. Finally, we introduce variations of uniform asymptotic\ngain and uniform limit properties, which are particularly useful for small-gain\narguments and characterize ISS in terms of these notions. \n\n"}
{"id": "1901.04122", "contents": "Title: Data-driven inference of hidden nodes in networks Abstract: The explosion of activity in finding interactions in complex systems is\ndriven by availability of copious observations of complex natural systems.\nHowever, such systems, e.g. the human brain, are rarely completely observable.\nInteraction network inference must then contend with hidden variables affecting\nthe behavior of the observed parts of the system. We present a novel\ndata-driven approach for model inference with hidden variables. From\nconfigurations of observed variables, we identify the observed-to-observed,\nhidden-to-observed, observed-to-hidden, and hidden-to-hidden interactions, the\nconfigurations of hidden variables, and the number of hidden variables. We\ndemonstrate the performance of our method by simulating a kinetic Ising model,\nand show that our method outperforms existing methods. Turning to real data, we\ninfer the hidden nodes in a neuronal network in the salamander retina and a\nstock market network. We show that predictive modeling with hidden variables is\nsignificantly more accurate than that without hidden variables. Finally, an\nimportant hidden variable problem is to find the number of clusters in a\ndataset. We apply our method to classify MNIST handwritten digits. We find that\nthere are about 60 clusters which are roughly equally distributed amongst the\ndigits. \n\n"}
{"id": "1901.05583", "contents": "Title: A Multilevel Approach for Stochastic Nonlinear Optimal Control Abstract: We consider a class of finite time horizon nonlinear stochastic optimal\ncontrol problem, where the control acts additively on the dynamics and the\ncontrol cost is quadratic. This framework is flexible and has found\napplications in many domains. Although the optimal control admits a path\nintegral representation for this class of control problems, efficient\ncomputation of the associated path integrals remains a challenging Monte Carlo\ntask. The focus of this article is to propose a new Monte Carlo approach that\nsignificantly improves upon existing methodology. Our proposed methodology\nfirst tackles the issue of exponential growth in variance with the time horizon\nby casting optimal control estimation as a smoothing problem for a state space\nmodel associated with the control problem, and applying smoothing algorithms\nbased on particle Markov chain Monte Carlo. To further reduce computational\ncost, we then develop a multilevel Monte Carlo method which allows us to obtain\nan estimator of the optimal control with $\\mathcal{O}(\\epsilon^2)$ mean squared\nerror with a computational cost of\n$\\mathcal{O}(\\epsilon^{-2}\\log(\\epsilon)^2)$. In contrast, a computational cost\nof $\\mathcal{O}(\\epsilon^{-3})$ is required for existing methodology to achieve\nthe same mean squared error. Our approach is illustrated on two numerical\nexamples, which validate our theory. \n\n"}
{"id": "1901.06074", "contents": "Title: Exact Controllability for a Refined Stochastic Wave Equation Abstract: A widely used stochastic wave equation is the classical wave equation\nperturbed by a term of It\\^o's integral. We show that this equation is not\nexactly controllable even if the controls are effective everywhere in both the\ndrift and the diffusion terms and also on the boundary. In some sense this\nmeans that some key feature has been ignored in this model. Then, based on a\nstochastic Newton's law, we propose a refined stochastic wave equation. By\nmeans of a new global Carleman estimate, we establish the exact controllability\nof our stochastic wave equation with three controls. Moreover, we give a result\nabout the lack of exact controllability, which shows that the action of three\ncontrols is necessary. Our analysis indicates that, at least from the point of\nview of control theory, the new stochastic wave equation introduced in this\npaper is a more reasonable model than that in the existing literatures. \n\n"}
{"id": "1901.06594", "contents": "Title: Molecular Force Fields with Gradient-Domain Machine Learning:\n  Construction and Application to Dynamics of Small Molecules with Coupled\n  Cluster Forces Abstract: We present the construction of molecular force fields for small molecules\n(less than 25 atoms) using the recently developed symmetrized gradient-domain\nmachine learning (sGDML) approach [Chmiela et al., Nat. Commun. 9, 3887 (2018);\nSci. Adv. 3, e1603015 (2017)]. This approach is able to accurately reconstruct\ncomplex high-dimensional potential-energy surfaces from just a few 100s of\nmolecular conformations extracted from ab initio molecular dynamics\ntrajectories. The data efficiency of the sGDML approach implies that atomic\nforces for these conformations can be computed with high-level\nwavefunction-based approaches, such as the \"gold standard\" CCSD(T) method. We\ndemonstrate that the flexible nature of the sGDML model recovers local and\nnon-local electronic interactions (e.g. H-bonding, proton transfer, lone pairs,\nchanges in hybridization states, steric repulsion and $n\\to\\pi^*$ interactions)\nwithout imposing any restriction on the nature of interatomic potentials. The\nanalysis of sGDML molecular dynamics trajectories yields new qualitative\ninsights into dynamics and spectroscopy of small molecules close to\nspectroscopic accuracy. \n\n"}
{"id": "1901.06995", "contents": "Title: Distributed Nesterov gradient methods over arbitrary graphs Abstract: In this letter, we introduce a distributed Nesterov method, termed as\n$\\mathcal{ABN}$, that does not require doubly-stochastic weight matrices.\nInstead, the implementation is based on a simultaneous application of both row-\nand column-stochastic weights that makes this method applicable to arbitrary\n(strongly-connected) graphs. Since constructing column-stochastic weights needs\nadditional information (the number of outgoing neighbors at each agent), not\navailable in certain communication protocols, we derive a variation, termed as\nFROZEN, that only requires row-stochastic weights but at the expense of\nadditional iterations for eigenvector learning. We numerically study these\nalgorithms for various objective functions and network parameters and show that\nthe proposed distributed Nesterov methods achieve acceleration compared to the\ncurrent state-of-the-art methods for distributed optimization. \n\n"}
{"id": "1901.08022", "contents": "Title: A Universally Optimal Multistage Accelerated Stochastic Gradient Method Abstract: We study the problem of minimizing a strongly convex, smooth function when we\nhave noisy estimates of its gradient. We propose a novel multistage accelerated\nalgorithm that is universally optimal in the sense that it achieves the optimal\nrate both in the deterministic and stochastic case and operates without\nknowledge of noise characteristics. The algorithm consists of stages that use a\nstochastic version of Nesterov's method with a specific restart and parameters\nselected to achieve the fastest reduction in the bias-variance terms in the\nconvergence rate bounds. \n\n"}
{"id": "1901.08497", "contents": "Title: Modelling the Demand and Uncertainty of Low Voltage Networks and the\n  Effect of non-Domestic Consumers Abstract: The increasing use and spread of low carbon technologies are expected to\ncause new patterns in electric demand and set novel challenges to a\ndistribution network operator (DNO). In this study, we build upon a recently\nintroduced method, called \"buddying\", which simulates low voltage (LV) networks\nof both residential and non-domestic (e.g. shops, offices, schools, hospitals,\netc.) customers through optimization (via a genetic algorithm) of demands based\non limited monitored and customer data. The algorithm assigns a limited but\ndiverse number of monitored households (the \"buddies\") to the unmonitored\ncustomers on a network. We study and compare two algorithms, one where\nsubstation monitoring data is available and a second where no substation\ninformation is used. Despite the roll out of monitoring equipment at domestic\nproperties and/or substations, less data is available for commercial customers.\nThis study focuses on substations with commercial customers most of which have\nno monitored `buddy', in which case a profile must be created. Due to the\nvolatile nature of the low voltage networks, uncertainty bounds are crucial for\noperational purposes. We introduce and demonstrate two techniques for modelling\nthe confidence bounds on the modelled LV networks. The first method uses\nprobabilistic forecast methods based on substation monitoring; the second only\nuses a simple bootstrap of the sample of monitored customers but has the\nadvantage of not requiring monitoring at the substation. These modelling tools,\nbuddying and uncertainty bounds, can give further insight to a DNO to better\nplan and manage the network when limited information is available. \n\n"}
{"id": "1901.08663", "contents": "Title: New nonasymptotic convergence rates of stochastic proximal\n  pointalgorithm for convex optimization problems Abstract: Large sectors of the recent optimization literature focused in the last\ndecade on the development of optimal stochastic first order schemes for\nconstrained convex models under progressively relaxed assumptions. Stochastic\nproximal point is an iterative scheme born from the adaptation of proximal\npoint algorithm to noisy stochastic optimization, with a resulting iteration\nrelated to stochastic alternating projections. Inspired by the scalability of\nalternating projection methods, we start from the (linear) regularity\nassumption, typically used in convex feasiblity problems to guarantee the\nlinear convergence of stochastic alternating projection methods, and analyze a\ngeneral weak linear regularity condition which facilitates convergence rate\nboosts in stochastic proximal point schemes. Our applications include many\nnon-strongly convex functions classes often used in machine learning and\nstatistics. Moreover, under weak linear regularity assumption we guarantee\n$\\mathcal{O}\\left(\\frac{1}{k}\\right)$ convergence rate for SPP, in terms of the\ndistance to the optimal set, using only projections onto a simple component\nset. Linear convergence is obtained for interpolation setting, when the optimal\nset of the expected cost is included into the optimal sets of each functional\ncomponent. \n\n"}
{"id": "1901.09109", "contents": "Title: DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online\n  Optimization Abstract: Adaptive gradient-based optimization methods such as \\textsc{Adagrad},\n\\textsc{Rmsprop}, and \\textsc{Adam} are widely used in solving large-scale\nmachine learning problems including deep learning. A number of schemes have\nbeen proposed in the literature aiming at parallelizing them, based on\ncommunications of peripheral nodes with a central node, but incur high\ncommunications cost. To address this issue, we develop a novel consensus-based\ndistributed adaptive moment estimation method (\\textsc{Dadam}) for online\noptimization over a decentralized network that enables data parallelization, as\nwell as decentralized computation. The method is particularly useful, since it\ncan accommodate settings where access to local data is allowed. Further, as\nestablished theoretically in this work, it can outperform centralized adaptive\nalgorithms, for certain classes of loss functions used in applications. We\nanalyze the convergence properties of the proposed algorithm and provide a\ndynamic regret bound on the convergence rate of adaptive moment estimation\nmethods in both stochastic and deterministic settings. Empirical results\ndemonstrate that \\textsc{Dadam} works also well in practice and compares\nfavorably to competing online optimization methods. \n\n"}
{"id": "1901.09252", "contents": "Title: Asynchronous Distributed Optimization over Lossy Networks via Relaxed\n  ADMM: Stability and Linear Convergence Abstract: In this work we focus on the problem of minimizing the sum of convex cost\nfunctions in a distributed fashion over a peer-to-peer network. In particular,\nwe are interested in the case in which communications between nodes are prone\nto failures and the agents are not synchronized among themselves. We address\nthe problem proposing a modified version of the relaxed ADMM, which corresponds\nto the Peaceman-Rachford splitting method applied to the dual. By exploiting\nresults from operator theory, we are able to prove the almost sure convergence\nof the proposed algorithm under general assumptions on the distribution of\ncommunication loss and node activation events. By further assuming the cost\nfunctions to be strongly convex, we prove the linear convergence of the\nalgorithm in mean to a neighborhood of the optimal solution, and provide an\nupper bound to the convergence rate. Finally, we present numerical results\ntesting the proposed method in different scenarios. \n\n"}
{"id": "1901.09253", "contents": "Title: On Deriving Probabilistic Models for Adsorption Energy on Transition\n  Metals using Multi-level Ab initio and Experimental Data Abstract: In this paper, we apply multi-task Gaussian Process (MT-GP) to show that the\nadsorption energy of small adsorbates on transition metal surfaces can be\nmodeled to a high level of fidelity using data from multiple sources, taking\nadvantage of the relatively abundant ''low fidelity\" data (such as from density\nfunctional theory computations) and small amounts of ''high fidelity\"\ncomputational (e.g. using the random phase approximation) or experimental data.\nTo fully explore the performance of MT-GP, we perform two case studies - one\nusing purely computational datasets and the other using a combination of\nexperimental and computational datasets. In both cases, the performance of\nMT-GPs is significantly better than single-task models built on a single data\nsource. This method can be used to learn improved models from fused datasets,\nand thereby build accurate models under tight computational and experimental\nbudget. \n\n"}
{"id": "1901.09506", "contents": "Title: An iterative regularized mirror descent method for ill-posed\n  nondifferentiable stochastic optimization Abstract: A wide range of applications arising in machine learning and signal\nprocessing can be cast as convex optimization problems. These problems are\noften ill-posed, i.e., the optimal solution lacks a desired property such as\nuniqueness or sparsity. In the literature, to address ill-posedness, a bilevel\noptimization problem is considered where the goal is to find among optimal\nsolutions of the inner level optimization problem, a solution that minimizes a\nsecondary metric, i.e., the outer level objective function. In addressing the\nresulting bilevel model, the convergence analysis of most existing methods is\nlimited to the case where both inner and outer level objectives are\ndifferentiable deterministic functions. While these assumptions may not hold in\nbig data applications, to the best of our knowledge, no solution method\nequipped with complexity analysis exists to address presence of uncertainty and\nnondifferentiability in both levels in this class of problems. Motivated by\nthis gap, we develop a first-order method called Iterative Regularized\nStochastic Mirror Descent (IR-SMD). We establish the global convergence of the\niterate generated by the algorithm to the optimal solution of the bilevel\nproblem in an almost sure and a mean sense. We derive a convergence rate of\n${\\cal O}\\left(1/N^{0.5-\\delta}\\right)$ for the inner level problem, where\n$\\delta>0$ is an arbitrary small scalar. Numerical experiments for solving two\nclasses of bilevel problems, including a large scale binary text classification\napplication, are presented. \n\n"}
{"id": "1901.10348", "contents": "Title: Stochastic Frank-Wolfe for Composite Convex Minimization Abstract: A broad class of convex optimization problems can be formulated as a\nsemidefinite program (SDP), minimization of a convex function over the\npositive-semidefinite cone subject to some affine constraints. The majority of\nclassical SDP solvers are designed for the deterministic setting where problem\ndata is readily available. In this setting, generalized conditional gradient\nmethods (aka Frank-Wolfe-type methods) provide scalable solutions by leveraging\nthe so-called linear minimization oracle instead of the projection onto the\nsemidefinite cone. Most problems in machine learning and modern engineering\napplications, however, contain some degree of stochasticity. In this work, we\npropose the first conditional-gradient-type method for solving stochastic\noptimization problems under affine constraints. Our method guarantees\n$\\mathcal{O}(k^{-1/3})$ convergence rate in expectation on the objective\nresidual and $\\mathcal{O}(k^{-5/12})$ on the feasibility gap. \n\n"}
{"id": "1901.11224", "contents": "Title: Lower Bounds for Smooth Nonconvex Finite-Sum Optimization Abstract: Smooth finite-sum optimization has been widely studied in both convex and\nnonconvex settings. However, existing lower bounds for finite-sum optimization\nare mostly limited to the setting where each component function is (strongly)\nconvex, while the lower bounds for nonconvex finite-sum optimization remain\nlargely unsolved. In this paper, we study the lower bounds for smooth nonconvex\nfinite-sum optimization, where the objective function is the average of $n$\nnonconvex component functions. We prove tight lower bounds for the complexity\nof finding $\\epsilon$-suboptimal point and $\\epsilon$-approximate stationary\npoint in different settings, for a wide regime of the smallest eigenvalue of\nthe Hessian of the objective function (or each component function). Given our\nlower bounds, we can show that existing algorithms including KatyushaX\n(Allen-Zhu, 2018), Natasha (Allen-Zhu, 2017), RapGrad (Lan and Yang, 2018) and\nStagewiseKatyusha (Chen and Yang, 2018) have achieved optimal Incremental\nFirst-order Oracle (IFO) complexity (i.e., number of IFO calls) up to logarithm\nfactors for nonconvex finite-sum optimization. We also point out potential ways\nto further improve these complexity results, in terms of making stronger\nassumptions or by a different convergence analysis. \n\n"}
{"id": "astro-ph/0005101", "contents": "Title: Data Streams from the Low Frequency Instrument On-Board the Planck\n  Satellite: Statistical Analysis and Compression Efficiency Abstract: The expected data rate produced by the Low Frequency Instrument (LFI) planned\nto fly on the ESA Planck mission in 2007, is over a factor 8 larger than the\nbandwidth allowed by the spacecraft transmission system to download the LFI\ndata. We discuss the application of lossless compression to Planck/LFI data\nstreams in order to reduce the overall data flow. We perform both theoretical\nanalysis and experimental tests using realistically simulated data streams in\norder to fix the statistical properties of the signal and the maximal\ncompression rate allowed by several lossless compression algorithms. We studied\nthe influence of signal composition and of acquisition parameters on the\ncompression rate Cr and develop a semiempirical formalism to account for it.\nThe best performing compressor tested up to now is the arithmetic compression\nof order 1, designed for optimizing the compression of white noise like\nsignals, which allows an overall compression rate <Cr> = 2.65 +/- 0.02. We find\nthat such result is not improved by other lossless compressors, being the\nsignal almost white noise dominated. Lossless compression algorithms alone will\nnot solve the bandwidth problem but needs to be combined with other techniques. \n\n"}
{"id": "astro-ph/0011397", "contents": "Title: A Measurement of the Three-Dimensional Clustering of C IV\n  Absorption-Line Systems on Scales of 5 to 300 Mpc Abstract: We examine the three-dimensional clustering of C IV absorption-line systems,\nusing an extensive catalog of QSO heavy-element absorbers drawn from the\nliterature. We measure clustering by a volume-weighted integral of the\ncorrelation function called the reduced second-moment measure, and include\ninformation from both along and across QSO lines of sight, thus enabling a full\ndetermination of the three-dimensional clustering of absorbers, as well as a\ncomparison of line- and cross-line-of-sight clustering properties. Here we\npresent the three-dimensional reduced second-moment estimator for a\nthree-dimensional point process probed by one-dimensional lines of sight, and\napply our algorithm to a sample of 345 C IV absorbers with median redshift of\n2.2, drawn from the spectra of 276 QSOs. We confirm the existence of\nsignificant clustering on comoving scales up to 100 Mpc, and find that the\nadditional cross-line-of-sight information strengthens the evidence for\nclustering on scales from 100 Mpc to 150 Mpc. There is no evidence of absorber\nclustering along or across lines of sight for scales from 150 Mpc to 300 Mpc.\nWe show that with a 300-times larger catalog, such as that to be compiled by\nthe Sloan Digital Sky Survey (100,000 QSOs), use of the full three-dimensional\nestimator and cross-line-of-sight information will substantially increase\nclustering sensitivity. We find that standard errors are reduced by an\nadditional factor of 2 to 20 on scales of 30 to 200 Mpc, effectively increasing\nthe sample size by an extra factor of 4 to 400 at large distances. \n\n"}
{"id": "astro-ph/0206431", "contents": "Title: Search for correlation between GRB's detected by BeppoSAX and\n  gravitational wave detectors EXPLORER and NAUTILUS Abstract: Data obtained during five months of 2001 with the gravitational wave (GW)\ndetectors EXPLORER and NAUTILUS were studied in correlation with the gamma ray\nburst data (GRB) obtained with the BeppoSAX satellite. During this period\nBeppoSAX was the only GRB satellite in operation, while EXPLORER and NAUTILUS\nwere the only GW detectors in operation.\n  No correlation between the GW data and the GRB bursts was found. The\nanalysis, performed over 47 GRB's, excludes the presence of signals of\namplitude h >=1.2 * 10^{-18}, with 95 % probability, if we allow a time delay\nbetween GW bursts and GRB within +-400 s, and h >= 6.5 * 10^{-19}, if the time\ndelay is within +- 5 s. The result is also provided in form of scaled\nlikelihood for unbiased interpretation and easier use for further analysis. \n\n"}
{"id": "cond-mat/0106096", "contents": "Title: Statistical mechanics of complex networks Abstract: Complex networks describe a wide range of systems in nature and society, much\nquoted examples including the cell, a network of chemicals linked by chemical\nreactions, or the Internet, a network of routers and computers connected by\nphysical links. While traditionally these systems were modeled as random\ngraphs, it is increasingly recognized that the topology and evolution of real\nnetworks is governed by robust organizing principles. Here we review the recent\nadvances in the field of complex networks, focusing on the statistical\nmechanics of network topology and dynamics. After reviewing the empirical data\nthat motivated the recent interest in networks, we discuss the main models and\nanalytical tools, covering random graphs, small-world and scale-free networks,\nas well as the interplay between topology and the network's robustness against\nfailures and attacks. \n\n"}
{"id": "cond-mat/0401124", "contents": "Title: Thermodynamics and Phase Diagram of High Temperature Superconductors Abstract: Thermodynamic quantities are derived for superconducting and pseudogap\nregimes by taking into account both amplitude and phase fluctuations of the\npairing field. In the normal (pseudogap) state of the underdoped cuprates, two\ndomains have to be distinguished: near the superconducting region, phase\ncorrelations are important up to the temperature $T_\\phi$. Above $T_\\phi$, the\npseudogap region is only determined by amplitudes, and phases are uncorrelated.\n  Our calculations show excellent quantitative agreement with specific heat and\nmagnetic susceptibility experiments on cuprates. We find that the mean field\ntemperature $T_0$ has a similar doping dependence as the pseudogap temperature\n$T^*$, whereas the pseudogap energy scale is given by the average amplitude\nabove $T_c$. \n\n"}
{"id": "cond-mat/0408507", "contents": "Title: Stochastic nonlinear differential equation generating 1/f noise Abstract: Starting from the simple point process model of 1/f noise we derive a\nstochastic nonlinear differential equation for the signal exhibiting 1/f noise\nin any desirably wide range of frequency. A stochastic differential equation\n(the general Langevin equation with a multiplicative noise) that gives 1/f\nnoise is derived for the first time. The solution of the equation exhibits the\npower-law distribution. The process with 1/f noise is demonstrated by the\nnumerical solution of the derived equation with the appropriate restriction of\nthe diffusion of the signal in some finite interval. \n\n"}
{"id": "cond-mat/0604409", "contents": "Title: An ensemble approach to the analysis of weighted networks Abstract: We present a new approach to the calculation of measures in weighted\nnetworks, based on the translation of a weighted network into an ensemble of\nedges. This leads to a straightforward generalization of any measure defined on\nunweighted networks, such as the average degree of the nearest neighbours, the\nclustering coefficient, the `betweenness', the distance between two nodes and\nthe diameter of a network. All these measures are well established for\nunweighted networks but have hitherto proven difficult to define for weighted\nnetworks. Further to introducing this approach we demonstrate its advantages by\napplying the clustering coefficient constructed in this way to two real-world\nweighted networks. \n\n"}
{"id": "hep-ex/0011069", "contents": "Title: The Power of Confidence Intervals Abstract: We consider the power to reject false values of the parameter in Frequentist\nmethods for the calculation of confidence intervals. We connect the power with\nthe physical significance (reliability) of confidence intervals for a parameter\nbounded to be non-negative. We show that the confidence intervals (upper\nlimits) obtained with a (biased) method that near the boundary has large power\nin testing the parameter against larger alternatives and small power in testing\nthe parameter against smaller alternatives are physically more significant.\nConsidering the recently proposed methods with correct coverage, we show that\nthe physical significance of upper limits is smallest in the Unified Approach\nand highest in the Maximum Likelihood Estimator method. We illustrate our\narguments in the specific cases of a bounded Gaussian distribution and a\nPoisson distribution with known background. \n\n"}
{"id": "hep-ex/9902006", "contents": "Title: Confidence Level Computation for Combining Searches with Small\n  Statistics Abstract: This article describes an efficient procedure for computing approximate\nconfidence levels for searches for new particles where the expected signal and\nbackground levels are small enough to require the use of Poisson statistics.\nThe results of many independent searches for the same particle may be combined\neasily, regardless of the discriminating variables which may be measured for\nthe candidate events. The effects of systematic uncertainty in the signal and\nbackground models are incorporated in the confidence levels. The procedure\ndescribed allows efficient computation of expected confidence levels. \n\n"}
{"id": "hep-ph/0107067", "contents": "Title: Inferring $\\rhobar$ and $\\etabar$ of the CKM matrix - A simplified,\n  intuitive approach Abstract: This analysis is based on the same ideas and numerical inputs of the recent\npaper by Ciuchini et al. on the subject. Some approximations are applied, which\nmake analytical calculations applicable in most of the work, thus avoiding\nMonte Carlo integration. The final result is practically identical to the one\nobtained by the more detailed numerical analysis. \n\n"}
{"id": "hep-ph/0201040", "contents": "Title: A Statistical Analysis of Hadron Spectrum: Quantum Chaos in Hadrons Abstract: The nearest-neighbor mass-spacing distribution of the meson and baryon\nspectrum (up to 2.5 GeV) is described by the Wigner surmise corresponding to\nthe statistics of the Gaussian orthogonal ensemble of random matrix theory.\nThis can be viewed as a manifestation of quantum chaos in hadrons. \n\n"}
{"id": "hep-ph/0609001", "contents": "Title: Catfish: A Monte Carlo simulator for black holes at the LHC Abstract: We present a new Fortran Monte Carlo generator to simulate black hole events\nat CERN's Large Hadron Collider. The generator interfaces to the PYTHIA Monte\nCarlo fragmentation code. The physics of the BH generator includes, but not\nlimited to, inelasticity effects, exact field emissivities, corrections to\nsemiclassical black hole evaporation and gravitational energy loss at\nformation. These features are essential to realistically reconstruct the\ndetector response and test different models of black hole formation and decay\nat the LHC. \n\n"}
{"id": "math-ph/0401038", "contents": "Title: Eigenvalue Density of Correlated Complex Random Wishart Matrices Abstract: Using a character expansion method, we calculate exactly the eigenvalue\ndensity of random matrices of the form M^\\dagger M where M is a complex matrix\ndrawn from a normalized distribution P(M) ~ exp(-\\Tr(A M B M^\\dagger) with A\nand B positive definite (square) matrices of arbitrary dimensions. Such\nso-called ``correlated Wishart matrices'' occur in many fields ranging from\ninformation theory to multivariate analysis. \n\n"}
{"id": "math/0206230", "contents": "Title: Caratheodory-Equivalence, Noether Theorems, and Tonelli Full-Regularity\n  in the Calculus of Variations and Optimal Control Abstract: We study, in a unified way, the following questions related to the properties\nof Pontryagin extremals for optimal control problems with unrestricted\ncontrols: i) How the transformations, which define the equivalence of two\nproblems, transform the extremals? ii) How to obtain quantities which are\nconserved along any extremal? iii) How to assure that the set of extremals\ninclude the minimizers predicted by the existence theory? These questions are\nconnected to: i) the Caratheodory method which establishes a correspondence\nbetween the minimizing curves of equivalent problems; ii) the interplay between\nthe concept of invariance and the theory of optimality conditions in optimal\ncontrol, which are the concern of the theorems of Noether; iii) regularity\nconditions for the minimizers and the work pioneered by Tonelli. \n\n"}
{"id": "math/0211450", "contents": "Title: Symmetry groups, semidefinite programs, and sums of squares Abstract: We investigate the representation of symmetric polynomials as a sum of\nsquares. Since this task is solved using semidefinite programming tools we\nexplore the geometric, algebraic, and computational implications of the\npresence of discrete symmetries in semidefinite programs. It is shown that\nsymmetry exploitation allows a significant reduction in both matrix size and\nnumber of decision variables. This result is applied to semidefinite programs\narising from the computation of sum of squares decompositions for multivariate\npolynomials. The results, reinterpreted from an invariant-theoretic viewpoint,\nprovide a novel representation of a class of nonnegative symmetric polynomials.\nThe main theorem states that an invariant sum of squares polynomial is a sum of\ninner products of pairs of matrices, whose entries are invariant polynomials.\nIn these pairs, one of the matrices is computed based on the real irreducible\nrepresentations of the group, and the other is a sum of squares matrix. The\nreduction techniques enable the numerical solution of large-scale instances,\notherwise computationally infeasible to solve. \n\n"}
{"id": "math/0406284", "contents": "Title: Barvinok's Rational Functions: Algorithms and Applications to\n  Optimization, Statistics, and Algebra Abstract: The main theme of this dissertation is the study of the lattice points in a\nrational convex polyhedron and their encoding in terms of Barvinok's short\nrational functions. The first part of this thesis looks into theoretical\napplications of these rational functions to Optimization, Statistics, and\nComputational Algebra. The main theorem on Chapter 2 concerns the computation\nof the \\emph{toric ideal} $I_A$ of an integral $n \\times d$ matrix $A$. We\nencode the binomials belonging to the toric ideal $I_A$ associated with $A$\nusing Barvinok's rational functions. If we fix $d$ and $n$, this representation\nallows us to compute a universal Gr\\\"obner basis and the reduced Gr\\\"obner\nbasis of the ideal $I_A$, with respect to any term order, in polynomial time.\nWe derive a polynomial time algorithm for normal form computations which\nreplaces in this new encoding the usual reductions of the division algorithm.\nChapter 3 presents three ways to use Barvinok's rational functions to solve\nInteger Programs.\n  The second part of the thesis is experimental and consists mainly of the\nsoftware package {\\tt LattE}, the first implementation of Barvinok's algorithm.\nWe report on experiments with families of well-known rational polytopes:\nmultiway contingency tables, knapsack type problems, and rational polygons. We\nalso developed a new algorithm, {\\em the homogenized Barvinok's algorithm} to\ncompute the generating function for a rational polytope. We showed that it runs\nin polynomial time in fixed dimension. With the homogenized Barvinok's\nalgorithm, we obtained new combinatorial formulas: the generating function for\nthe number of $5\\times 5$ magic squares and the generating function for the\nnumber of $3\\times 3 \\times 3 \\times 3$ magic cubes as rational functions. \n\n"}
{"id": "math/0408270", "contents": "Title: Solving the Likelihood Equations Abstract: Given a model in algebraic statistics and some data, the likelihood function\nis a rational function on a projective variety. Algebraic algorithms are\npresented for computing all critical points of this function, with the aim of\nidentifying the local maxima in the probability simplex. Applications include\nmodels specified by rank conditions on matrices and the Jukes-Cantor models of\nphylogenetics. The maximum likelihood degree of a generic complete intersection\nis also determined. \n\n"}
{"id": "math/0510333", "contents": "Title: Optimal Bond Portfolios Abstract: We aim to construct a general framework for portfolio management in\ncontinuous time, encompassing both stocks and bonds. In these lecture notes we\ngive an overview of the state of the art of optimal bond portfolios and we\nre-visit main results and mathematical constructions introduced in our previous\npublications (Ann. Appl. Probab. \\textbf{15}, 1260--1305 (2005) and Fin. Stoch.\n{\\bf9}, 429--452 (2005)).\n  A solution of the optimal bond portfolio problem is given for general utility\nfunctions and volatility operator processes, provided that the market price of\nrisk process has certain Malliavin differentiability properties or is finite\ndimensional.\n  The text is essentially self-contained. \n\n"}
{"id": "math/0605572", "contents": "Title: Systems with distributions and viability theorem Abstract: The approach to the consideration of the ordinary differential equations with\ndistributions in the classical space $\\mathcal D'$ of distributions with\ncontinuous test functions has certain insufficiencies: the notations are\nincorrect from the point of view of distribution theory, the right-hand side\nhas to satisfy the restrictive conditions of equality type. In the present\npaper we consider an initial value problem for the ordinary differential\nequation with distributions in the space of distributions with dynamic test\nfunctions $\\mathcal T'$, where the continuous operation of multiplication of\ndistributions by discontinuous functions is defined, and show that this\napproach does not have the aforementioned insufficiencies.\n  We provide the sufficient conditions for viability of solutions of the\nordinary differential equations with distributions (a generalization of the\nNagumo Theorem), and show that the consideration of the distributional\n(impulse) controls in the problem for avoidance of encounters with the set (the\nmaximal viability time problem) allows us to provide the existence of solution,\nwhich may not exist for the ordinary controls. \n\n"}
{"id": "math/0608381", "contents": "Title: Absolute Extrema of Invariant Optimal Control Problems Abstract: Optimal control problems are usually addressed with the help of the famous\nPontryagin Maximum Principle (PMP) which gives a generalization of the\nclassical Euler-Lagrange and Weierstrass necessary optimality conditions of the\ncalculus of variations. Success in applying the PMP permits to obtain\ncandidates for a local minimum. In 1967 a direct method, which permits to\nobtain global minimizers directly, without using necessary conditions, was\nintroduced by Leitmann. Leitmann's approach is connected, as showed by Carlson\nin 2002, with \"Caratheodory's royal road of the Calculus of variations\". Here\nwe propose a related but different direct approach to problems of the calculus\nof variations and optimal control, which permit to obtain global minima\ndirectly, without recourse to needle variations and necessary conditions. Our\nmethod is inspired by the classical Noether's theorem and its recent extensions\nto optimal control. We make use of the variational symmetries of the problem,\nconsidering parameter-invariance transformations and substituting the original\nproblem by a parameter-family of optimal control problems. Parameters are then\nfixed in order to make the problem trivial, in some sense. Finally, by applying\nthe inverse of the chosen invariance-transformation, we get the global\nminimizer for the original problem. The proposed method is illustrated, by\nsolving concrete problems, and compared with Leitmann's approach. \n\n"}
{"id": "nlin/0409024", "contents": "Title: Quantifying Self-Organization with Optimal Predictors Abstract: Despite broad interest in self-organizing systems, there are few\nquantitative, experimentally-applicable criteria for self-organization. The\nexisting criteria all give counter-intuitive results for important cases. In\nthis Letter, we propose a new criterion, namely an internally-generated\nincrease in the statistical complexity, the amount of information required for\noptimal prediction of the system's dynamics. We precisely define this\ncomplexity for spatially-extended dynamical systems, using the probabilistic\nideas of mutual information and minimal sufficient statistics. This leads to a\ngeneral method for predicting such systems, and a simple algorithm for\nestimating statistical complexity. The results of applying this algorithm to a\nclass of models of excitable media (cyclic cellular automata) strongly support\nour proposal. \n\n"}
{"id": "nlin/0507037", "contents": "Title: Forecasting non-stationary financial time series through genetic\n  algorithm Abstract: We utilize a recently developed genetic algorithm, in conjunction with\ndiscrete wavelets, for carrying out successful forecasts of the trend in\nfinancial time series, that includes the NASDAQ composite index. Discrete\nwavelets isolate the local, small scale variations in these non-stationary time\nseries, after which the genetic algorithm's predictions are found to be quite\naccurate. The power law behavior in Fourier domain reveals an underlying\nself-affine dynamical behavior, well captured by the algorithm, in the form of\nan analytic equation. Remarkably, the same equation captures the trend of the\nBombay stock exchange composite index quite well. \n\n"}
{"id": "nucl-th/0701096", "contents": "Title: A Global Model of $\\beta^-$-Decay Half-Lives Using Neural Networks Abstract: Statistical modeling of nuclear data using artificial neural networks (ANNs)\nand, more recently, support vector machines (SVMs), is providing novel\napproaches to systematics that are complementary to phenomenological and\nsemi-microscopic theories. We present a global model of $\\beta^-$-decay\nhalflives of the class of nuclei that decay 100% by $\\beta^-$ mode in their\nground states. A fully-connected multilayered feed forward network has been\ntrained using the Levenberg-Marquardt algorithm, Bayesian regularization, and\ncross-validation. The halflife estimates generated by the model are discussed\nand compared with the available experimental data, with previous results\nobtained with neural networks, and with estimates coming from traditional\nglobal nuclear models. Predictions of the new neural-network model are given\nfor nuclei far from stability, with particular attention to those involved in\nr-process nucleosynthesis. This study demonstrates that in the framework of the\n$\\beta^-$-decay problem considered here, global models based on ANNs can at\nleast match the predictive performance of the best conventional global models\nrooted in nuclear theory. Accordingly, such statistical models can provide a\nvaluable tool for further mapping of the nuclidic chart. \n\n"}
{"id": "physics/0009032", "contents": "Title: Information theory and learning: a physical approach Abstract: We try to establish a unified information theoretic approach to learning and\nto explore some of its applications. First, we define {\\em predictive\ninformation} as the mutual information between the past and the future of a\ntime series, discuss its behavior as a function of the length of the series,\nand explain how other quantities of interest studied previously in learning\ntheory - as well as in dynamical systems and statistical mechanics - emerge\nfrom this universally definable concept. We then prove that predictive\ninformation provides the {\\em unique measure for the complexity} of dynamics\nunderlying the time series and show that there are classes of models\ncharacterized by {\\em power-law growth of the predictive information} that are\nqualitatively more complex than any of the systems that have been investigated\nbefore. Further, we investigate numerically the learning of a nonparametric\nprobability density, which is an example of a problem with power-law\ncomplexity, and show that the proper Bayesian formulation of this problem\nprovides for the `Occam' factors that punish overly complex models and thus\nallow one {\\em to learn not only a solution within a specific model class, but\nalso the class itself} using the data only and with very few a priori\nassumptions. We study a possible {\\em information theoretic method} that\nregularizes the learning of an undersampled discrete variable, and show that\nlearning in such a setup goes through stages of very different complexities.\nFinally, we discuss how all of these ideas may be useful in various problems in\nphysics, statistics, and, most importantly, biology. \n\n"}
{"id": "physics/0009064", "contents": "Title: Confidence intervals for the parameter of Poisson distribution in\n  presence of background Abstract: A results of numerical procedure for construction of confidence intervals for\nparameter of Poisson distribution for signal in the presence of background\nwhich has Poisson distribution with known value of parameter are presented. It\nis shown that the described procedure has both Bayesian and frequentist\ninterpretations. \n\n"}
{"id": "physics/0010063", "contents": "Title: Optimal Recovery of Local Truth Abstract: Probability mass curves the data space with horizons. Let f be a multivariate\nprobability density function with continuous second order partial derivatives.\nConsider the problem of estimating the true value of f(z) > 0 at a single point\nz, from n independent observations. It is shown that, the fastest possible\nestimators (like the k-nearest neighbor and kernel) have minimum asymptotic\nmean square errors when the space of observations is thought as conformally\ncurved. The optimal metric is shown to be generated by the Hessian of f in the\nregions where the Hessian is definite. Thus, the peaks and valleys of f are\nsurrounded by singular horizons when the Hessian changes signature from\nRiemannian to pseudo-Riemannian. Adaptive estimators based on the optimal\nvariable metric show considerable theoretical and practical improvements over\ntraditional methods. The formulas simplify dramatically when the dimension of\nthe data space is 4. The similarities with General Relativity are striking but\npossibly illusory at this point. However, these results suggest that\nnonparametric density estimation may have something new to say about current\nphysical theory. \n\n"}
{"id": "physics/0108030", "contents": "Title: Quasi-optimal observables: Attaining the quality of maximal likelihood\n  in parameter estimation when only a MC event generator is available Abstract: A new method of quasi-optimal observables allows one to approach the quality\nof data processing usually associated with the method of maximal likelihood\nwithin the simpler algorithmic context of generalized moments. \n\n"}
{"id": "physics/0201016", "contents": "Title: Entropic Priors for Discrete Probabilistic Networks and for Mixtures of\n  Gaussians Models Abstract: The ongoing unprecedented exponential explosion of available computing power,\nhas radically transformed the methods of statistical inference. What used to be\na small minority of statisticians advocating for the use of priors and a strict\nadherence to bayes theorem, it is now becoming the norm across disciplines. The\nevolutionary direction is now clear. The trend is towards more realistic,\nflexible and complex likelihoods characterized by an ever increasing number of\nparameters. This makes the old question of: What should the prior be? to\nacquire a new central importance in the modern bayesian theory of inference.\nEntropic priors provide one answer to the problem of prior selection. The\ngeneral definition of an entropic prior has existed since 1988, but it was not\nuntil 1998 that it was found that they provide a new notion of complete\nignorance. This paper re-introduces the family of entropic priors as minimizers\nof mutual information between the data and the parameters, as in\n[rodriguez98b], but with a small change and a correction. The general formalism\nis then applied to two large classes of models: Discrete probabilistic networks\nand univariate finite mixtures of gaussians. It is also shown how to perform\ninference by efficiently sampling the corresponding posterior distributions. \n\n"}
{"id": "physics/0203081", "contents": "Title: On the limits of spectral methods for frequency estimation Abstract: An algorithm is presented which generates pairs of oscillatory random time\nseries which have identical periodograms but differ in the number of\noscillations. This result indicate the intrinsic limitations of spectral\nmethods when it comes to the task of measuring frequencies. Other examples, one\nfrom medicine and one from bifurcation theory, are given, which also exhibit\nthese limitations of spectral methods. For two methods of spectral estimation\nit is verified that the particular way end points are treated, which is\nspecific to each method, is, for long enough time series, not relevant for the\nmain result. \n\n"}
{"id": "physics/0205053", "contents": "Title: A Quantum Approach to Stock Price Fluctuations Abstract: A simple quantum model explains the Levy-unstable distributions for\nindividual stock returns observed by ref.[1]. The probability density function\nof the returns is written as the squared modulus of an amplitude. For short\ntime intervals this amplitude is proportional to a Cauchy-distribution and\nsatisfies the Schroedinger equation with a non-hermitian Hamiltonian. The\nobserved power law tails of the return fluctuations imply that the \"decay\nrate\", $\\gamma(q)$ asymptotically is proportional to $|q|$, for large $|q|$.\nThe wave number, the Fourier-conjugate variable to the return, is interpreted\nas a quantitative measure of \"market sentiment\". On a time scale of less than a\nfew weeks, the distribution of returns in this quantum model is shape stable\nand scales. The model quantitatively reproduces the observed cumulative\ndistribution for the short-term normalized returns over 7 orders of magnitude\nwithout adjustable parameters. The return fluctuations over large time periods\nultimately become Gaussian if $\\gamma(q\\sim 0)\\propto q^2$. The ansatz\n$\\gamma(q)=b_T\\sqrt{m^2+q^2}$ is found to describe the positive part of the\nobserved historic probability of normalized returns for time periods between\nT=5 min and $T\\sim 4$ years over more than 4 orders of magnitude in terms of\none adjustable parameter $s_T=m b_T\\propto T$. The Sharpe ratio of a stock in\nthis model has a finite limit as the investment horizon $T\\to 0$. Implications\nfor short-term investments are discussed. \n\n"}
{"id": "physics/0210025", "contents": "Title: Pricing European Options in Realistic Markets Abstract: We investigate the relation between the fair price for European-style vanilla\noptions and the distribution of short-term returns on the underlying asset\nignoring transaction and other costs. We compute the risk-neutral probability\ndensity conditional on the total variance of the asset's returns when the\noption expires. If the asset's future price has finite expectation, the\noption's fair value satisfies a parabolic partial differential equation of the\nBlack-Scholes type in which the variance of the asset's returns rather than a\ntrading time is the evolution parameter. By immunizing the portfolio against\nlarge-scale price fluctuations of the asset, the valuation of options is\nextended to the realistic case\\cite{St99} of assets whose short-term returns\nhave finite variance but very large, or even infinite, higher moments. A\ndynamic Delta-hedged portfolio that is statically insured against exceptionally\nlarge fluctuations includes at least two different options on the asset. The\nfair value of an option in this case is determined by a universal drift\nfunction that is common to all options on the asset. This drift is interpreted\nas the premium for an investment exposed to risk due to exceptionally large\nvariations of the asset's price. It affects the option valuation like an\neffective cost-of-carry for the underlying in the Black-Scholes world would.\nThe derived pricing formula for options in realistic markets is arbitrage free\nby construction. A simple model with constant drift qualitatively reproduces\nthe often observed volatility -skew and -term structure. \n\n"}
{"id": "physics/0302032", "contents": "Title: Distance, dissimilarity index, and network community structure Abstract: We address the question of finding the community structure of a complex\nnetwork. In an earlier effort [H. Zhou, {\\em Phys. Rev. E} (2003)], the concept\nof network random walking is introduced and a distance measure defined. Here we\ncalculate, based on this distance measure, the dissimilarity index between\nnearest-neighboring vertices of a network and design an algorithm to partition\nthese vertices into communities that are hierarchically organized. Each\ncommunity is characterized by an upper and a lower dissimilarity threshold. The\nalgorithm is applied to several artificial and real-world networks, and\nexcellent results are obtained. In the case of artificially generated random\nmodular networks, this method outperforms the algorithm based on the concept of\nedge betweenness centrality. For yeast's protein-protein interaction network,\nwe are able to identify many clusters that have well defined biological\nfunctions. \n\n"}
{"id": "physics/0303011", "contents": "Title: How many clusters? An information theoretic perspective Abstract: Clustering provides a common means of identifying structure in complex data,\nand there is renewed interest in clustering as a tool for the analysis of large\ndata sets in many fields. A natural question is how many clusters are\nappropriate for the description of a given system. Traditional approaches to\nthis problem are based either on a framework in which clusters of a particular\nshape are assumed as a model of the system or on a two-step procedure in which\na clustering criterion determines the optimal assignments for a given number of\nclusters and a separate criterion measures the goodness of the classification\nto determine the number of clusters. In a statistical mechanics approach,\nclustering can be seen as a trade--off between energy-- and entropy--like\nterms, with lower temperature driving the proliferation of clusters to provide\na more detailed description of the data. For finite data sets, we expect that\nthere is a limit to the meaningful structure that can be resolved and therefore\na minimum temperature beyond which we will capture sampling noise. This\nsuggests that correcting the clustering criterion for the bias which arises due\nto sampling errors will allow us to find a clustering solution at a temperature\nwhich is optimal in the sense that we capture maximal meaningful structure --\nwithout having to define an external criterion for the goodness or stability of\nthe clustering. We show that, in a general information theoretic framework, the\nfinite size of a data set determines an optimal temperature, and we introduce a\nmethod for finding the maximal number of clusters which can be resolved from\nthe data in the hard clustering limit. \n\n"}
{"id": "physics/0310148", "contents": "Title: Random Walker Ranking for NCAA Division I-A Football Abstract: Each December, college football fans and pundits across America debate which\ntwo teams should meet in the NCAA Division I-A National Championship game. The\nBowl Championship Series (BCS) standings employed to select the teams invited\nto this game are intended to provide an unequivocal #1 v. #2 game for the\nchampionship; however, this selection process has itself been highly\ncontroversial in recent years. The computer algorithms that constitute one part\nof the BCS standings often act as lightning rods for the controversy, in part\nbecause they are inadequately explained to the public. We present an\nalternative algorithm that is simply explained yet remains effective at ranking\nthe best teams. We define a ranking in terms of biased random walkers on the\ngraph formed by the schedule of games played, with two teams (vertices)\nconnected by an edge if they played each other. Each random walker moves from\nteam to team by selecting a game and \"voting\" for its winner with probability\np, tracing out a never-ending path motivated by the \"my team beat your team\"\nargument. We study the statistical properties of a collection of such walkers,\nrelate the rankings to the community structure of the underlying network, and\ndemonstrate the results for recent NCAA Division I-A seasons. We also discuss\nthe algorithm's asymptotic behavior, illustrated with some analytically\ntractable cases for round-robin tournaments, and discuss possible\ngeneralizations. \n\n"}
{"id": "physics/0402039", "contents": "Title: Pattern Recognition and Event Reconstruction in Particle Physics\n  Experiments Abstract: This report reviews methods of pattern recognition and event reconstruction\nused in modern high energy physics experiments. After a brief introduction into\ngeneral concepts of particle detectors and statistical evaluation, different\napproaches in global and local methods of track pattern recognition are\nreviewed with their typical strengths and shortcomings. The emphasis is then\nmoved to methods which estimate the particle properties from the signals which\npattern recognition has associated. Finally, the global reconstruction of the\nevent is briefly addressed. \n\n"}
{"id": "physics/0403086", "contents": "Title: Asymmetric Uncertainties: Sources, Treatment and Potential Dangers Abstract: The issue of asymmetric uncertainties resulting from fits, nonlinear\npropagation and systematic effects is reviewed. It is shown that, in all cases,\nwhenever a published result is given with asymmetric uncertainties, the value\nof the physical quantity of interest is biased with respect to what would be\nobtained using at best all experimental and theoretical information that\ncontribute to evaluate the combined uncertainty. The probabilistic solution to\nthe problem is provided both in exact and in approximated forms. \n\n"}
{"id": "physics/0404015", "contents": "Title: Stochastic analysis of different rough surfaces Abstract: This paper shows in detail the application of a new stochastic approach for\nthe characterization of surface height profiles, which is based on the theory\nof Markov processes. With this analysis we achieve a characterization of the\nscale dependent complexity of surface roughness by means of a Fokker-Planck or\nLangevin equation, providing the complete stochastic information of multiscale\njoint probabilities. The method is applied to several surfaces with different\nproperties, for the purpose of showing the utility of this method in more\ndetails. In particular we show the evidence of Markov properties, and we\nestimate the parameters of the Fokker-Planck equation by pure, parameter-free\ndata analysis. The resulting Fokker-Planck equations are verified by numerical\nreconstruction of conditional probability density functions. The results are\ncompared with those from the analysis of multi-affine and extended multi-affine\nscaling properties which is often used for surface topographies. The different\nsurface structures analysed here show in details advantages and disadvantages\nof these methods. \n\n"}
{"id": "physics/0404021", "contents": "Title: Increment definitions for scale dependent analysis of stochastic data Abstract: It is common for scale-dependent analysis of stochastic data to use the\nincrement $\\Delta(t,r) = \\xi(t+r) - \\xi(t)$ of a data set $\\xi(t)$ as a\nstochastic measure, where $r$ denotes the scale. For joint statistics of\n$\\Delta(t,r)$ and $\\Delta(t,r')$ the question how to nest the increments on\ndifferent scales $r,r'$ is investigated. Here we show that in some cases\nspurious correlations between scales can be introduced by the common\nleft-justified definition. The consequences for a Markov process are discussed.\nThese spurious correlations can be avoided by an appropriate nesting of\nincrements. We demonstrate this effect for different data sets and show how it\ncan be detected and quantified. The problem allows to propose a unique method\nto distinguish between experimental data generated by a noiselike or a\nLangevin-like random-walk process, respectively. \n\n"}
{"id": "physics/0405008", "contents": "Title: Goodness-of-fit tests in many dimensions Abstract: A method is presented to construct goodness-of-fit statistics in many\ndimensions for which the distribution of all possible test results in the limit\nof an infinite number of data becomes Gaussian if also the number of dimensions\nbecomes infinite. Furthermore, an explicit example is presented, for which this\ndistribution as good as only depends on the expectation value and the variance\nof the statistic for any dimension larger than one. \n\n"}
{"id": "physics/0405044", "contents": "Title: Least Dependent Component Analysis Based on Mutual Information Abstract: We propose to use precise estimators of mutual information (MI) to find least\ndependent components in a linearly mixed signal. On the one hand this seems to\nlead to better blind source separation than with any other presently available\nalgorithm. On the other hand it has the advantage, compared to other\nimplementations of `independent' component analysis (ICA) some of which are\nbased on crude approximations for MI, that the numerical values of the MI can\nbe used for:\n  (i) estimating residual dependencies between the output components;\n  (ii) estimating the reliability of the output, by comparing the pairwise MIs\nwith those of re-mixed components;\n  (iii) clustering the output according to the residual interdependencies.\n  For the MI estimator we use a recently proposed k-nearest neighbor based\nalgorithm. For time sequences we combine this with delay embedding, in order to\ntake into account non-trivial time correlations. After several tests with\nartificial data, we apply the resulting MILCA (Mutual Information based Least\ndependent Component Analysis) algorithm to a real-world dataset, the ECG of a\npregnant woman.\n  The software implementation of the MILCA algorithm is freely available at\nhttp://www.fz-juelich.de/nic/cs/software \n\n"}
{"id": "physics/0408039", "contents": "Title: On Bayesian Treatment of Systematic Uncertainties in Confidence Interval\n  Calculation Abstract: In high energy physics, a widely used method to treat systematic\nuncertainties in confidence interval calculations is based on combining a\nfrequentist construction of confidence belts with a Bayesian treatment of\nsystematic uncertainties. In this note we present a study of the coverage of\nthis method for the standard Likelihood Ratio (aka Feldman & Cousins)\nconstruction for a Poisson process with known background and Gaussian or\nlog-Normal distributed uncertainties in the background or signal efficiency.\nFor uncertainties in the signal efficiency of upto 40 % we find over-coverage\non the level of 2 to 4 % depending on the size of uncertainties and the region\nin signal space. Uncertainties in the background generally have smaller effect\non the coverage. A considerable smoothing of the coverage curves is observed. A\nsoftware package is presented which allows fast calculation of the confidence\nintervals for a variety of assumptions on shape and size of systematic\nuncertainties for different nuisance parameters. The calculation speed allows\nexperimenters to test the coverage for their specific conditions. \n\n"}
{"id": "physics/0410226", "contents": "Title: THE CAVES Project - Collaborative Analysis Versioning Environment\n  System; THE CODESH Project - Collaborative Development Shell Abstract: A key feature of collaboration in science and software development is to have\na {\\em log} of what and how is being done - for private use and reuse and for\nsharing selected parts with collaborators, which most often today are\ndistributed geographically on an ever larger scale. Even better if this log is\n{\\em automatic}, created on the fly while a scientist or software developer is\nworking in a habitual way, without the need for extra efforts. The {\\tt CAVES}\nand {\\tt CODESH} projects address this problem in a novel way, building on the\nconcepts of {\\em virtual state} and {\\em virtual transition} to provide an\nautomatic persistent logbook for sessions of data analysis or software\ndevelopment in a collaborating group. A repository of sessions can be\nconfigured dynamically to record and make available the knowledge accumulated\nin the course of a scientific or software endeavor. Access can be controlled to\ndefine logbooks of private sessions and sessions shared within or between\ncollaborating groups. \n\n"}
{"id": "physics/0410255", "contents": "Title: X-ray properties of the transient pulsar 3A 0535+262 in quiescence Abstract: We present the timing and spectral properties of the transient Be/X-ray\nbinary pulsar 3A 0535+262 during quiescence using three observations with the\nnarrow field imaging instruments (NFI) of BeppoSAX. Assuming a distance of 2\nkpc for this system, the 2-10 keV X-ray luminosities measured from the three\nobservations are in the range of 1.5-4.0 $\\times$ 10$^{33}$ erg s$^{-1}$,\nindicating a very low rate of accretion. We report the detection of pulsations\nat a very low luminosity of 2 $\\times$ 10$^{33}$ erg s$^{-1}$ during one of the\nthree observations, though at this accretion rate the system is expected to be\nin the centrifugally inhibited regime. The X-ray spectra for the unpulsed\nobservations are best modeled as power law type while a combined model of power\nlaw and black-body is required to fit the pulsed spectrum. \n\n"}
{"id": "physics/0410274", "contents": "Title: Recurrence intervals between earthquakes strongly depend on history Abstract: We study the statistics of the recurrence times between earthquakes above a\ncertain magnitude M$ in California. We find that the distribution of the\nrecurrence times strongly depends on the previous recurrence time $\\tau_0$. As\na consequence, the conditional mean recurrence time $\\hat \\tau(\\tau_0)$ between\ntwo events increases monotonically with $\\tau_0$. For $\\tau_0$ well below the\naverage recurrence time $\\ov{\\tau}, \\hat\\tau(\\tau_0)$ is smaller than\n$\\ov{\\tau}$, while for $\\tau_0>\\ov{\\tau}$, $\\hat\\tau(\\tau_0)$ is greater than\n$\\ov{\\tau}$. Also the mean residual time until the next earthquake does not\ndepend only on the elapsed time, but also strongly on $\\tau_0$. The larger\n$\\tau_0$ is, the larger is the mean residual time. The above features should be\ntaken into account in any earthquake prognosis. \n\n"}
{"id": "physics/0502048", "contents": "Title: The occupation of a box as a toy model for the seismic cycle of a fault Abstract: We illustrate how a simple statistical model can describe the quasiperiodic\noccurrence of large earthquakes. The model idealizes the loading of elastic\nenergy in a seismic fault by the stochastic filling of a box. The emptying of\nthe box after it is full is analogous to the generation of a large earthquake\nin which the fault relaxes after having been loaded to its failure threshold.\nThe duration of the filling process is analogous to the seismic cycle, the time\ninterval between two successive large earthquakes in a particular fault. The\nsimplicity of the model enables us to derive the statistical distribution of\nits seismic cycle. We use this distribution to fit the series of earthquakes\nwith magnitude around 6 that occurred at the Parkfield segment of the San\nAndreas fault in California. Using this fit, we estimate the probability of the\nnext large earthquake at Parkfield and devise a simple forecasting strategy. \n\n"}
{"id": "physics/0504158", "contents": "Title: Detecting subtle effects of persistence in the stock market dynamics Abstract: The conventional formal tool to detect effects of the financial persistence\nis in terms of the Hurst exponent. A typical corresponding result is that its\nvalue comes out close to 0.5, as characteristic for geometric Brownian motion,\nwith at most small departures from this value in either direction depending on\nthe market and on the time scales involved. We study the high frequency price\nchanges on the American and on the German stock markets. For both corresponding\nindices, the Dow Jones and the DAX respectively, the Hurst exponent analysis\nresults in values close to 0.5. However, by decomposing the market dynamics\ninto pairs of steps such that an elementary move up (down) is followed by\nanother move up (down) and explicitly counting the resulting conditional\nprobabilities we find values typically close to 60%. This effect of persistence\nis particularly visible on the short time scales ranging from 1 up to 3\nminutes, decreasing gradually to 50% and even significantly below this value on\nthe larger time scales. We also detect some asymmetry in persistence related to\nthe moves up and down, respectively. This indicates a subtle nature of the\nfinancial persistence whose characteristics escape detection within the\nconventional Hurst exponent formalism. \n\n"}
{"id": "physics/0508190", "contents": "Title: Maximum-likelihood estimation prevents unphysical Mueller matrices Abstract: We show that the method of maximum-likelihood estimation, recently introduced\nin the context of quantum process tomography, can be applied to the\ndetermination of Mueller matrices characterizing the polarization properties of\nclassical optical systems. Contrary to linear reconstruction algorithms, the\nproposed method yields physically acceptable Mueller matrices even in presence\nof uncontrolled experimental errors. We illustrate the method on the case of an\nunphysical measured Mueller matrix taken from the literature. \n\n"}
{"id": "physics/0509045", "contents": "Title: Wealth distribution and Pareto's law in the Hungarian medieval society Abstract: The distribution of wealth in the Hungarian medieval aristocratic society is\nreported and studied. The number of serf families belonging to a noble is taken\nas a measure of the corresponding wealth. Our results reveal the power-law\nnature of this distribution function, confirming the validity of the Pareto law\nfor such a society. The obtained Pareto index $\\alpha=0.92$ is however smaller\nthan the values currently reported in the literature. We argue that the value\nclose to 1, of the Pareto index is a consequence of the absence of a relevant\neconomic life in the targeted society, in agreement with the prediction of\nexisting wealth distribution models for the idealized case of independently\nacting agents. Models developed to explain city populations may also be adapted\nto justify our results. \n\n"}
{"id": "physics/0509257", "contents": "Title: Small scale behavior of financial data Abstract: A new approach is presented to describe the change in the statistics of the\nlog return distribution of financial data as a function of the timescale. To\nthis purpose a measure is introduced, which quantifies the distance of a\nconsidered distribution to a reference distribution. The existence of a small\ntimescale regime is demonstrated, which exhibits different properties compared\nto the normal timescale regime. This regime seems to be universal for\nindividual stocks. It is shown that the existence of this small timescale\nregime is not dependent on the special choice of the distance measure or the\nreference distribution. These findings have important implications for risk\nanalysis, in particular for the probability of extreme events. \n\n"}
{"id": "physics/0512068", "contents": "Title: Modeling of flows with the power-law spectral densities and power-law\n  distributions of flow's intensities Abstract: We present analytical and numerical results of modeling of flows represented\nas the correlated non-Poissonian point process and as the Poissonian sequence\nof pulses of the different size. Both models may generate signals with the\npower-law distributions of the intensity of the flow and the power-law spectral\ndensity. Furthermore, different distributions of the interevent time of the\npoint process and different statistics of the size of pulses may result in\n$1/f^{\\beta}$ noise (one-over-f noise, 1-f noise) with\n$0.5\\lesssim\\beta\\lesssim2$. Combination of the models is applied for modeling\nof the Internet traffic. \n\n"}
{"id": "physics/0512221", "contents": "Title: Escape of a Uniform Random Walk from an Interval Abstract: We study the first-passage properties of a random walk in the unit interval\nin which the length of a single step is uniformly distributed over the finite\nrange [-a,a]. For a of the order of one, the exit probabilities to each edge of\nthe interval and the exit time from the interval exhibit anomalous properties\nstemming from the change in the minimum number of steps to escape the interval\nas a function of the starting point. As a decreases, first-passage properties\napproach those of continuum diffusion, but non-diffusive effects remain because\nof residual discreteness effects \n\n"}
{"id": "physics/0602023", "contents": "Title: sPlot: A Quick Introduction Abstract: The paper advocates the use of a statistical tool dedicated to the\nexploration of data samples populated by several sources of events. This new\ntechnique, called sPlot, is able to unfold the contributions of the different\nsources to the distribution of a data sample in a given variable. The sPlot\ntool applies in the context of a Likelihood fit which is performed on the data\nsample to determine the yields of the various sources. \n\n"}
{"id": "physics/0604193", "contents": "Title: Exploring Complex Networks through Random Walks Abstract: Most real complex networks -- such as protein interactions, social contacts,\nthe internet -- are only partially known and available to us. While the process\nof exploring such networks in many cases resembles a random walk, it becomes a\nkey issue to investigate and characterize how effectively the nodes and edges\nof such networks can be covered by different strategies. At the same time, it\nis critically important to infer how well can topological measurements such as\nthe average node degree and average clustering coefficient be estimated during\nsuch network explorations. The present article addresses these problems by\nconsidering random and Barab\\'asi-Albert (BA) network models with varying\nconnectivity explored by three types of random walks: traditional, preferential\nto untracked edges, and preferential to unvisited nodes. A series of relevant\nresults are obtained, including the fact that random and BA models with the\nsame size and average node degree allow similar node and edge coverage\nefficiency, the identification of linear scaling with the size of the network\nof the random walk step at which a given percentage of the nodes/edges is\ncovered, and the critical result that the estimation of the averaged node\ndegree and clustering coefficient by random walks on BA networks often leads to\nheavily biased results. Many are the theoretical and practical implications of\nsuch results. \n\n"}
{"id": "physics/0605251", "contents": "Title: Correlation based networks of equity returns sampled at different time\n  horizons Abstract: We investigate the planar maximally filtered graphs of the portfolio of the\n300 most capitalized stocks traded at the New York Stock Exchange during the\ntime period 2001-2003. Topological properties such as the average length of\nshortest paths, the betweenness and the degree are computed on different planar\nmaximally filtered graphs generated by sampling the returns at different time\nhorizons ranging from 5 min up to one trading day. This analysis confirms that\nthe selected stocks compose a hierarchical system progressively structuring as\nthe sampling time horizon increases. Finally, a cluster formation, associated\nto economic sectors, is quantitatively investigated. \n\n"}
{"id": "physics/0606006", "contents": "Title: A Test for the Presence of a Signal Abstract: We describe a statistical hypothesis test for the presence of a signal based\non the likelihood ratio statistic. We derive the test for a case of interest\nand also show that for that case the test works very well, even far out in the\ntails of the distribution. We also study extensions of the test to cases where\nthere are multiple channels. \n\n"}
{"id": "physics/0606088", "contents": "Title: Relationship Between Structural Characters and Synchronizability of\n  Scale-free Networks Abstract: Using Memory Tabu Search(MTS) algorithm, we investigate the relationship\nbetween structural characters and synchronizability of scale-free networks by\nmaximizing and minimizing the ratio $Q$ of the eigenvalues of the coupling\nmatrix by edge-intercrossing procedures. The numerical results indicate that\nclustering coefficient $C$, maximal betweenness $B_{max}$ are two most\nimportant factors to scale-free network synchronizability, and assortative\ncoefficient $r$ and average distance $D$ are the secondary ones. Moreover, the\naverage degree $<k>$ affects the relationship between above structural\ncharacters and synchronizability of scale-free networks, and the minimal $Q$\ndecreases when $<k>$ increases. \n\n"}
{"id": "physics/0607043", "contents": "Title: Statistical analysis of time-resolved emission from ensembles of\n  semiconductor quantum dots: interpretation of exponential decay models Abstract: We present a statistical analysis of time-resolved spontaneous emission decay\ncurves from ensembles of emitters, such as semiconductor quantum dots, with the\naim to interpret ubiquitous non-single-exponential decay. Contrary to what is\nwidely assumed, the density of excited emitters and the intensity in an\nemission decay curve are not proportional, but the density is a time-integral\nof the intensity. The integral relation is crucial to correctly interpret\nnon-single-exponential decay. We derive the proper normalization for both a\ndiscrete, and a continuous distribution of rates, where every decay component\nis multiplied with its radiative decay rate. A central result of our paper is\nthe derivation of the emission decay curve in case that both radiative and\nnon-radiative decays are independently distributed. In this case, the\nwell-known emission quantum efficiency can not be expressed by a single number\nanymore, but it is also distributed. We derive a practical description of\nnon-single-exponential emission decay curves in terms of a single distribution\nof decay rates; the resulting distribution is identified as the distribution of\ntotal decay rates weighted with the radiative rates. We apply our analysis to\nrecent examples of colloidal quantum dot emission in suspensions and in\nphotonic crystals, and we find that this important class of emitters is well\ndescribed by a log-normal distribution of decay rates with a narrow and a broad\ndistribution, respectively. Finally, we briefly discuss the Kohlrausch\nstretched-exponential model, and find that its normalization is ill-defined for\nemitters with a realistic quantum efficiency of less than 100 %. \n\n"}
{"id": "physics/0607272", "contents": "Title: Beyond the average: detecting global singular nodes from local features\n  in complex networks Abstract: Deviations from the average can provide valuable insights about the\norganization of natural systems. This article extends this important principle\nto the more systematic identification and analysis of singular local\nconnectivity patterns in complex networks. Four measurements quantifying\ndifferent and complementary features of the connectivity around each node are\ncalculated and multivariate statistical methods are then applied in order to\nidentify outliers. The potential of the presented concepts and methodology is\nillustrated with respect to a word association network. \n\n"}
{"id": "physics/0608081", "contents": "Title: Performance of the CDF Calorimeter Simulation in Tevatron Run II Abstract: The CDF experiment is successfully collecting data from ppbar collisions at\nthe Tevatron in Run II. As the data samples are getting larger, systematic\nuncertainties due to the measurement of the jet energy scale assessed using the\ncalorimeter simulation have become increasingly important. In many years of\noperation, the collaboration has gained experience with GFLASH, a fast\nparametrization of electromagnetic and hadronic showers used for the\ncalorimeter simulation. We present the performance of the calorimeter\nsimulation and report on recent improvements based on a refined in situ tuning\ntechnique. The central calorimeter response is reproduced with a precision of\n1-2%. \n\n"}
{"id": "physics/0610211", "contents": "Title: Probability of stochastic processes and spacetime geometry Abstract: We made a first attempt to associate a probabilistic description of\nstochastic processes like birth-death processes with spacetime geometry in the\nSchwarzschild metrics on distance scales from the macro- to the micro-domains.\nWe idealize an ergodic system in which system states communicate through a\ncurved path composed of transition arrows where each arrow corresponds to a\npositive, analogous birth or death rate. \n\n"}
{"id": "physics/0612231", "contents": "Title: A mechanism to derive multi-power law functions: an application in the\n  econophysics framework Abstract: It is generally recognized that economical systems, and more in general\ncomplex systems, are characterized by power law distributions. Sometime, these\ndistributions show a changing of the slope in the tail so that, more\nappropriately, they show a multi-power law behavior. We present a method to\nderive analytically a two-power law distribution starting from a single power\nlaw function recently obtained, in the frameworks of the generalized\nstatistical mechanics based on the Sharma-Taneja-Mittal information measure. In\norder to test the method, we fit the cumulative distribution of personal income\nand gross domestic production of several countries, obtaining a good agreement\nfor a wide range of data. \n\n"}
{"id": "physics/0701348", "contents": "Title: Deterministic Modularity Optimization Abstract: We study community structure of networks. We have developed a scheme for\nmaximizing the modularity Q based on mean field methods. Further, we have\ndefined a simple family of random networks with community structure; we\nunderstand the behavior of these networks analytically. Using these networks,\nwe show how the mean field methods display better performance than previously\nknown deterministic methods for optimization of Q. \n\n"}
{"id": "physics/0703039", "contents": "Title: TMVA - Toolkit for Multivariate Data Analysis Abstract: In high-energy physics, with the search for ever smaller signals in ever\nlarger data sets, it has become essential to extract a maximum of the available\ninformation from the data. Multivariate classification methods based on machine\nlearning techniques have become a fundamental ingredient to most analyses. Also\nthe multivariate classifiers themselves have significantly evolved in recent\nyears. Statisticians have found new ways to tune and to combine classifiers to\nfurther gain in performance. Integrated into the analysis framework ROOT, TMVA\nis a toolkit which hosts a large variety of multivariate classification\nalgorithms. Training, testing, performance evaluation and application of all\navailable classifiers is carried out simultaneously via user-friendly\ninterfaces. With version 4, TMVA has been extended to multivariate regression\nof a real-valued target vector. Regression is invoked through the same user\ninterfaces as classification. TMVA 4 also features more flexible data handling\nallowing one to arbitrarily form combined MVA methods. A generalised boosting\nmethod is the first realisation benefiting from the new framework. \n\n"}
{"id": "physics/9808009", "contents": "Title: Are We Cruising a Hypothesis Space? Abstract: This paper is about Information Geometry, a relatively new subject within\nmathematical statistics that attempts to study the problem of inference by\nusing tools from modern differential geometry. This paper provides an overview\nof some of the achievements and possible future applications of this subject to\nphysics. \n\n"}
{"id": "physics/9809037", "contents": "Title: On Observability of Signal over Background Abstract: Several statistics used by physicists to declare the signal observability\nover the background are compared. It is shown that the frequentist method of\ntesting a precise hypothesis allows one to estimate the power value of criteria\nwith specified level of significance for the considered statistics by Monte\nCarlo calculations. The application of this approach for the analysis of\ndiscovery potential of experiments is discussed. \n\n"}
{"id": "physics/9812016", "contents": "Title: Lifetimes of agents under external stress Abstract: An exact formula for the distribution of lifetimes in coherent-noise models\nand related models is derived. For certain stress distributions, this formula\ncan be analytically evaluated and yields simple closed expressions. For those\ntypes of stress for which a closed expression is not available, a numerical\nevaluation can be done in a straightforward way. All results obtained are in\nperfect agreement with numerical experiments. The implications for the\ncoherent-noise models' application to macroevolution are discussed. \n\n"}
{"id": "physics/9812036", "contents": "Title: Improved Probability Method for Estimating Signal in the Presence of\n  Background Abstract: A suggestion is made for improving the Feldman Cousins method of estimating\nsignal counts in the presence of background. The method concentrates on finding\nessential information about the signal and ignoring extraneous information\nabout background. An appropriate method is found which uses the condition that\nthe number of background events obtained does not exceed the total number of\nevents obtained. Several alternative approaches are explored. \n\n"}
{"id": "physics/9906035", "contents": "Title: Time Series Forecasting: A Nonlinear Dynamics Approach Abstract: The problem of prediction of a given time series is examined on the basis of\nrecent nonlinear dynamics theories. Particular attention is devoted to forecast\nthe amplitude and phase of one of the most common solar indicator activity, the\ninternational monthly smoothed sunspot number. It is well known that the solar\ncycle is very difficult to predict due to the intrinsic complexity of the\nrelated time behaviour and to the lack of a succesful quantitative theoretical\nmodel of the Sun magnetic cycle. Starting from a previous recent work, we\nchecked the reliability and accuracy of a forecasting model based on concepts\nof nonlinear dynamical systems applied to experimental time series, such as\nembedding phase space, Lyapunov spectrum, chaotic behaviour. The model is based\non a local hypothesis of the behaviour on the embedding space, utilizing an\noptimal number k of neighbour vectors to predict the future evolution of the\ncurrent point with the set of characteristic parameters determined by several\nprevious parametric computations. The performances of this method suggest its\nvaluable insertion in the set of the so-called statistical-numerical prediction\ntechniques, like Fourier analyses, curve fitting, neural networks,\nclimatological, etc. The main task is to set up and to compare a promising\nnumerical nonlinear prediction technique, essentially based on an inverse\nproblem, with the most accurate predictive methods like the so-called\n\"precursor methods\" which appear now reasonably accurate in predicting \"long\nterm\" Sun activity, with particular reference to the \"solar\" precursor methods\nbased on a solar dynamo theory. \n\n"}
{"id": "physics/9911077", "contents": "Title: Mixtures of Gaussian process priors Abstract: Nonparametric Bayesian approaches based on Gaussian processes have recently\nbecome popular in the empirical learning community. They encompass many\nclassical methods of statistics, like Radial Basis Functions or various\nsplines, and are technically convenient because Gaussian integrals can be\ncalculated analytically. Restricting to Gaussian processes, however, forbids\nfor example the implemention of genuine nonconcave priors. Mixtures of Gaussian\nprocess priors, on the other hand, allow the flexible implementation of complex\nand situation specific, also nonconcave \"a priori\" information. This is\nessential for tasks with, compared to their complexity, a small number of\navailable training data. The paper concentrates on the formalism for Gaussian\nregression problems where prior mixture models provide a generalisation of\nclassical quadratic, typically smoothness related, regularisation approaches\nbeing more flexible without having a much larger computational complexity. \n\n"}
{"id": "quant-ph/0409156", "contents": "Title: Optimizing linear optics quantum gates Abstract: In this paper, the problem of finding optimal success probabilities of static\nlinear optics quantum gates is linked to the theory of convex optimization. It\nis shown that by exploiting this link, upper bounds for the success probability\nof networks realizing single-mode gates can be derived, which hold in\ngenerality for linear optical networks followed by postselection, i.e., for\nnetworks of arbitrary size, any number of auxiliary modes, and arbitrary photon\nnumbers. As a corollary, the previously formulated conjecture is proven that\nthe optimal success probability of a postselected non-linear sign shift without\nfeed-forward is 1/4, a gate playing the central role in the scheme of\nKnill-Laflamme-Milburn for quantum computation with linear optics. The concept\nof Lagrange duality is shown to be applicable to provide rigorous proofs for\nsuch bounds for elementary gates, although the original problem is a difficult\nnon-convex problem in infinitely many objective variables. The versatility of\nthis approach to identify other optimal linear optical schemes is demonstrated. \n\n"}
