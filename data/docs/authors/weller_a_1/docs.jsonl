{"id": "0704.0386", "contents": "Title: Quantum non-local effects with Bose-Einstein condensates Abstract: We study theoretically the properties of two Bose-Einstein condensates in\ndifferent spin states, represented by a double Fock state. Individual\nmeasurements of the spins of the particles are performed in transverse\ndirections, giving access to the relative phase of the condensates. Initially,\nthis phase is completely undefined, and the first measurements provide random\nresults. But a fixed value of this phase rapidly emerges under the effect of\nthe successive quantum measurements, giving rise to a quasi-classical situation\nwhere all spins have parallel transverse orientations. If the number of\nmeasurements reaches its maximum (the number of particles), quantum effects\nshow up again, giving rise to violations of Bell type inequalities. The\nviolation of BCHSH inequalities with an arbitrarily large number of spins may\nbe comparable (or even equal) to that obtained with two spins. \n\n"}
{"id": "0704.1580", "contents": "Title: Optical implementation and entanglement distribution in Gaussian valence\n  bond states Abstract: We study Gaussian valence bond states of continuous variable systems,\nobtained as the outputs of projection operations from an ancillary space of M\ninfinitely entangled bonds connecting neighboring sites, applied at each of $N$\nsites of an harmonic chain. The entanglement distribution in Gaussian valence\nbond states can be controlled by varying the input amount of entanglement\nengineered in a (2M+1)-mode Gaussian state known as the building block, which\nis isomorphic to the projector applied at a given site. We show how this\nmechanism can be interpreted in terms of multiple entanglement swapping from\nthe chain of ancillary bonds, through the building blocks. We provide optical\nschemes to produce bisymmetric three-mode Gaussian building blocks (which\ncorrespond to a single bond, M=1), and study the entanglement structure in the\noutput Gaussian valence bond states. The usefulness of such states for quantum\ncommunication protocols with continuous variables, like telecloning and\nteleportation networks, is finally discussed. \n\n"}
{"id": "0704.2575", "contents": "Title: Strong photon non-linearities and photonic Mott insulators Abstract: We show, that photon non-linearities in electromagnetically induced\ntransparency can be at least one order of magnitude larger than predicted in\nall previous approaches. As an application we demonstrate that, in this regime\nthey give rise to very strong photon - photon interactions which are strong\nenough to make an experimental realization of a photonic Mott insulator state\nfeasible in arrays of coupled ultra high-Q micro-cavities. \n\n"}
{"id": "0706.2094", "contents": "Title: Frustration, Area Law, and Interference in Quantum Spin Models Abstract: We study frustrated quantum systems from a quantum information perspective.\nWithin this approach, we find that highly frustrated systems do not follow any\ngeneral ''area law'' of block entanglement, while weakly frustrated ones have\narea laws similar to those of nonfrustrated systems away from criticality. To\ncalculate the block entanglement in systems with degenerate ground states,\ntypical in frustrated systems, we define a ''cooling'' procedure of the ground\nstate manifold, and propose a frustration degree and a method to quantify\nconstructive and destructive interference effects of entanglement. \n\n"}
{"id": "0706.2432", "contents": "Title: Toolbox for entanglement detection and fidelity estimation Abstract: The determination of the state fidelity and the detection of entanglement are\nfundamental problems in quantum information experiments. We investigate how\nthese goals can be achieved with a minimal effort. We show that the fidelity of\nGHZ and W states can be determined with an effort increasing only linearly with\nthe number of qubits. We also present simple and robust methods for other\nstates, such as cluster states and states in decoherence-free subspaces. \n\n"}
{"id": "0706.3051", "contents": "Title: Molecular Dipolar Crystals as High Fidelity Quantum Memory for Hybrid\n  Quantum Computing Abstract: We study collective excitations of rotational and spin states of an ensemble\nof polar molecules, which are prepared in a dipolar crystalline phase, as a\ncandidate for a high fidelity quantum memory. While dipolar crystals are formed\nin the high density limit of cold clouds of polar molecules under 1D and 2D\ntrapping conditions, the crystalline structure protects the molecular qubits\nfrom detrimental effects of short range collisions. We calculate the lifetime\nof the quantum memory by identifying the dominant decoherence mechanisms, and\nestimate their effects on gate operations, when a molecular ensemble qubit is\ntransferred to a superconducting strip line cavity (circuit QED). In the case\nrotational excitations coupled by dipole-dipole interactions we identify\nphonons as the main limitation of the life time of qubits. We study specific\nsetups and conditions, where the coupling to the phonon modes is minimized.\nDetailed results are presented for a 1D dipolar chain. \n\n"}
{"id": "0706.3411", "contents": "Title: Cavity QED with a Bose-Einstein condensate Abstract: Cavity quantum electrodynamics (cavity QED) describes the coherent\ninteraction between matter and an electromagnetic field confined within a\nresonator structure, and is providing a useful platform for developing concepts\nin quantum information processing. By using high-quality resonators, a strong\ncoupling regime can be reached experimentally in which atoms coherently\nexchange a photon with a single light-field mode many times before dissipation\nsets in. This has led to fundamental studies with both microwave and optical\nresonators. To meet the challenges posed by quantum state engineering and\nquantum information processing, recent experiments have focused on laser\ncooling and trapping of atoms inside an optical cavity. However, the tremendous\ndegree of control over atomic gases achieved with Bose-Einstein condensation\nhas so far not been used for cavity QED. Here we achieve the strong coupling of\na Bose-Einstein condensate to the quantized field of an ultrahigh-finesse\noptical cavity and present a measurement of its eigenenergy spectrum. This is a\nconceptually new regime of cavity QED, in which all atoms occupy a single mode\nof a matter-wave field and couple identically to the light field, sharing a\nsingle excitation. This opens possibilities ranging from quantum communication\nto a wealth of new phenomena that can be expected in the many-body physics of\nquantum gases with cavity-mediated interactions. \n\n"}
{"id": "0707.0741", "contents": "Title: Realization of quantum walks with negligible decoherence in waveguide\n  lattices Abstract: Quantum random walks are the quantum counterpart of classical random walks,\nand were recently studied in the context of quantum computation. A quantum\nrandom walker is subject to self interference, leading to a remarkably\ndifferent behavior than that of classical random walks such as ballistic\npropagation or localization due to disorder. Physical implementations of\nquantum walks have only been made in very small scale systems severely limited\nby decoherence. Here we show that the propagation of photons in waveguide\nlattices, which have been studied extensively in recent years, are essentially\nan implementation of quantum walks. Since waveguide lattices are easily\nconstructed at large scales and display negligible decoherence, they can serve\nas an ideal and versatile experimental playground for the study of quantum\nwalks and quantum algorithms. We experimentally observe quantum walks in large\nsystems (~100 sites) and confirm quantum walks effects which were studied\ntheoretically, including ballistic propagation, disorder and boundary related\neffects. \n\n"}
{"id": "0707.1457", "contents": "Title: Visibility Fringe Reduction Due to Noise-Induced Effects: Microscopic\n  Approach to Interference Experiments Abstract: Decoherence is the main process behind the quantum to classical transition.\nIt is a purely quantum mechanical effect by which the system looses its ability\nto exhibit coherent behavior. The recent experimental observation of\ndiffraction and interference patterns for large molecules raises some\ninteresting questions. In this context, we identify possible agents of\ndecoherence to take into account when modeling these experiments and study\ntheirs visible (or not) effects on the interference pattern. Thereby, we\npresent an analysis of matter wave interferometry in the presence of a dynamic\nquantum environment and study how much the visibility fringe is reduced and in\nwhich timescale the decoherence effects destroy the interference of massive\nobjects. Finally, we apply our results to the experimental data reported on\nfullerenes and cold neutrons. \n\n"}
{"id": "0707.1645", "contents": "Title: Decoherence in a Two Slit Diffraction Experiment with Massive Particles Abstract: Matter-wave interferometry has been largely studied in the last few years.\nUsually, the main problem in the analysis of the diffraction experiments is to\nestablish the causes for the loss of coherence observed in the interference\npattern. In this work, we use different type of environmental couplings to\nmodel a two slit diffraction experiment with massive particles. For each model,\nwe study the effects of decoherence on the interference pattern and define a\nvisibility function that measures the loss of contrast of the interference\nfringes on a distant screen. Finally, we apply our results to the experimental\nreported data on massive particles $C_{70}$. \n\n"}
{"id": "0709.3463", "contents": "Title: Quantum Ratchets for Quantum Communication with Optical Superlattices Abstract: We propose to use a quantum ratchet to transport quantum information in a\nchain of atoms trapped in an optical superlattice. The quantum ratchet is\ncreated by a continuous modulation of the optical superlattice which is\nperiodic in time and in space. Though there is zero average force acting on the\natoms, we show that indeed the ratchet effect permits atoms on even and odd\nsites to move along opposite directions. By loading the optical lattice with\ntwo-level bosonic atoms, this scheme permits to perfectly transport a qubit or\nentangled state imprinted in one or more atoms to any desired position in the\nlattice. From the quantum computation point of view, the transport is achieved\nby a smooth concatenation of perfect swap gates. We analyze setups with\nnoninteracting and interacting particles and in the latter case we use the\ntools of optimal control to design optimal modulations. We also discuss the\nfeasibility of this method in current experiments. \n\n"}
{"id": "0711.3562", "contents": "Title: EPR argument and Bell inequalities for Bose-Einstein spin condensates Abstract: We discuss the properties of two Bose-Einstein condensates in different spin\nstates, represented quantum mechanically by a double Fock state. Individual\nmeasurements of the spins of the particles are performed in transverse\ndirections (perpendicular to the spin quantization axis), giving access to the\nrelative phase of the two macroscopically occupied states. Before the first\nspin measurement, the phase is completely undetermined; after a few\nmeasurements, a more and more precise knowledge of its value emerges under the\neffect of the quantum measurement process. This naturally leads to the usual\nnotion of a quasi-classical phase (Anderson phase) and to an interesting\ntransposition of the EPR (Einstein-Podolsky-Rosen) argument to macroscopic\nphysical quantities. The purpose of this article is to discuss this\ntransposition, as well as situations where the notion of a quasi-classical\nphase is no longer sufficient to account for the quantum results, and where\nsignificant violations of Bell type inequalities are predicted. \n\n"}
{"id": "0801.4771", "contents": "Title: Self-organization of a Bose-Einstein condensate in an optical cavity Abstract: The spatial self-organization of a Bose-Einstein condensate (BEC) in a\nhigh-finesse linear optical cavity is discussed. The condensate atoms are\nlaser-driven from the side and scatter photons into the cavity. Above a\ncritical pump intensity the homogeneous condensate evolves into a stable\npattern bound by the cavity field. The transition point is determined\nanalytically from a mean-field theory. We calculate the lowest lying Bogoliubov\nexcitations of the coupled BEC-cavity system and the quantum depletion due to\nthe atom-field coupling. \n\n"}
{"id": "0802.0577", "contents": "Title: Chirality Quantum Phase Transition in the Dirac oscillator Abstract: We study a relativistic spin-1/2 fermion subjected to a Dirac oscillator\ncoupling and a constant magnetic field. An interplay between opposed chirality\ninteractions culminates in the appearance of a relativistic quantum phase\ntransition, which can be fully characterized. We obtain analytical expressions\nfor the energy gap, order parameter, and canonical quantum fluctuations across\nthe critical point. Moreover, we also discuss the effect of this phase\ntransition on the statistics of the chiral bosonic ensemble, where its super-\nor sub-Poissonian nature can be controled by means of external parameters.\nFinally, we study the entanglement properties between the degrees of freedom in\nthe relativistic ground state, where an interesting transition between a\nbi-separable and a genuinely tripartite entangled state occurs. \n\n"}
{"id": "0802.3365", "contents": "Title: Simulation of high-spin Heisenberg models in coupled cavities Abstract: We propose a scheme to realize the Heisenberg model of any spin in an\narbitrary array of coupled cavities. Our scheme is based on a fixed number of\natoms confined in each cavity and collectively applied constant laser fields,\nand is in a regime where both atomic and cavity excitations are suppressed. It\nis shown that as well as optically controlling the effective spin Hamiltonian,\nit is also possible to engineer the magnitude of the spin. Our scheme would\nopen up an unprecedented way to simulate otherwise intractable high-spin\nproblems in many-body physics. \n\n"}
{"id": "0803.0760", "contents": "Title: Phase transitions, entanglement and quantum noise interferometry in cold\n  atoms Abstract: We show that entanglement monotones can characterize the pronounced\nenhancement of entanglement at a quantum phase transition if they are sensitive\nto long-range high order correlations. These monotones are found to develop a\nsharp peak at the critical point and to exhibit universal scaling. We\ndemonstrate that similar features are shared by noise correlations and verify\nthat these experimentally accessible quantities indeed encode entanglement\ninformation and probe separability. \n\n"}
{"id": "0803.1463", "contents": "Title: Preparation of Entangled States by Quantum Markov Processes Abstract: We investigate the possibility of using a dissipative process to prepare a\nquantum system in a desired state. We derive for any multipartite pure state a\ndissipative process for which this state is the unique stationary state and\nsolve the corresponding master equation analytically. For certain states, like\nthe Cluster states, we use this process to show that the jump operators can be\nchosen quasi-locally, i.e. they act non-trivially only on a few, neighboring\nqubits. Furthermore, the relaxation time of this dissipative process is\nindependent of the number of subsystems. We demonstrate the general formalism\nby considering arbitrary MPS-PEPS states. In particular, we show that the\nground state of the AKLT-model can be prepared employing a quasi--local\ndissipative process. \n\n"}
{"id": "0803.1760", "contents": "Title: Entanglement of two distant Bose-Einstein condensates by detection of\n  Bragg-scattered photons Abstract: We show that it is possible to generate entanglement between two distant\nBose-Einstein condensates by detection of Hanbury Brown-Twiss type correlations\nin photons Bragg-scattered by the condensates. Upon coincident detection of two\nphotons by two detectors, the projected joint state of two condensates is shown\nto be non-Gaussian. We verify the existence of entanglement by showing that the\npartially transposed state is negative. Further we use the inequality in terms\nof higher order moments to confirm entanglement. Our proposed scheme can be\ngeneralized for multiple condensates and also for spinor condensates with Bragg\nscattering of polarized light with the latter capable of producing hyper\nentanglement. \n\n"}
{"id": "0804.1969", "contents": "Title: Quantum systems under the influence of external conditions: fluctuations\n  and decoherence Abstract: In this Thesis we study the quantum to classical transition process in the\ncontext of quantum mechanics and quantum field theory. We shall analyze the\neffects that general environments, namely ohmic and non-ohmic, at zero and high\ntemperature induce over a quantum Brownian particle. We state that the\nevolution of the system can be summarized in terms of two main environmental\ninduced physical phenomena: decoherence and energy activation. In this Thesis,\nwe shall show that the latter is a post-decoherence phenomenon. As the energy\nis an observable, the excitation process can be consider a direct indicator of\nthe system-environment entanglement particularly useful at zero temperature.\n  From other point of view, we shall study different attempts to show the\ndecoherence process in double-slit-like experiments both for charged particles\n(electrons) and for neutral particles with permanent dipole moments. In this\ncontext, we shall show that the interaction between the particles and\ntime-dependent fields induces a time-varying Aharonov phase. In this context,\nwe shall apply our results to a real matter wave interferometry experiment. We\nshall also show under which general conditions the geometry phase of a quantum\nopen system can be observed.Finally, we shall study the decoherence process\nduring a quantum phase transition. In this framework, we shall show that it can\nbe phrased easily in terms of the decoherence functional, without having to use\nthe master equation. To demonstrate this, we shall consider the decohering\neffects due to the displacement of domain boundaries, with implications for the\ndisplacement of defects, in general. We shall see that decoherence arises so\nquickly in this event, that it is negligible in comparison to decoherence due\nto field fluctuations in the way defined in previous papers. \n\n"}
{"id": "0804.4406", "contents": "Title: Evolution of spin entanglement and an entanglement witness in\n  multiple-quantum NMR experiments Abstract: We investigate the evolution of entanglement in multiple-quantum (MQ) NMR\nexperiments in crystals with pairs of close nuclear spins-1/2. The initial\nthermodynamic equilibrium state of the system in a strong external magnetic\nfield evolves under the non-secular part of the dipolar Hamiltonian. As a\nresult, MQ coherences of the zeroth and plus/minus second orders appear. A\nsimple condition for the emergence of entanglement is obtained. We show that\nthe measure of the spin pair entanglement, concurrence, coincides qualitatively\nwith the intensity of MQ coherences of the plus/minus second order and hence\nthe entanglement can be studied with MQ NMR methods. We introduce an\nEntanglement Witness using MQ NMR coherences of the plus/minus second order. \n\n"}
{"id": "0805.1986", "contents": "Title: Cooper Pair Boxes Weakly Coupled to External Environments Abstract: We study the behaviour of charge oscillations in Superconducting Cooper Pair\nBoxes weakly interacting with an environment. We found that, due to the noise\nand dissipation induced by the environment, the stability properties of these\nnanodevices differ according to whether the charge oscillations are interpreted\nas an effect of macroscopic quantum coherence, or semiclassically in terms of\nthe Gross-Pitaevskii equation. More specifically, occupation number states,\nused in the quantum interpretation of the oscillations, are found to be much\nmore unstable than coherent ones, typical of the semiclassical explanation. \n\n"}
{"id": "0807.1777", "contents": "Title: Mean-field dynamics of a non-Hermitian Bose-Hubbard dimer Abstract: We investigate an $N$-particle Bose-Hubbard dimer with an additional\neffective decay term in one of the sites. A mean-field approximation for this\nnon-Hermitian many-particle system is derived, based on a coherent state\napproximation. The resulting nonlinear, non-Hermitian two-level dynamics, in\nparticular the fixed point structures showing characteristic modifications of\nthe self-trapping transition, are analyzed. The mean-field dynamics is found to\nbe in reasonable agreement with the full many-particle evolution. \n\n"}
{"id": "0807.3585", "contents": "Title: Dynamical Backaction of Microwave Fields on a Nanomechanical Oscillator Abstract: We measure the response and thermal motion of a high-Q nanomechanical\noscillator coupled to a superconducting microwave cavity in the\nresolved-sideband regime where the oscillator's resonance frequency exceeds the\ncavity's linewidth. The coupling between the microwave field and mechanical\nmotion is strong enough for radiation pressure to overwhelm the intrinsic\nmechanical damping. This radiation-pressure damping cools the fundamental\nmechanical mode by a factor of 5 below the thermal equilibrium temperature in a\ndilution refrigerator to a phonon occupancy of 140 quanta. \n\n"}
{"id": "0808.3581", "contents": "Title: Supersonic quantum communication Abstract: When locally exciting a quantum lattice model, the excitation will propagate\nthrough the lattice. The effect is responsible for a wealth of non-equilibrium\nphenomena, and has been exploited to transmit quantum information through spin\nchains. It is a commonly expressed belief that for local Hamiltonians, any such\npropagation happens at a finite \"speed of sound\". Indeed, the Lieb-Robinson\ntheorem states that in spin models, all effects caused by a perturbation are\nlimited to a causal cone defined by a constant speed, up to exponentially small\ncorrections. In this work we show that for translationally invariant bosonic\nmodels with nearest-neighbor interactions, this belief is incorrect: We prove\nthat one can encounter excitations which accelerate under the natural dynamics\nof the lattice and allow for reliable transmission of information faster than\nany finite speed of sound. The effect is only limited by the model's range of\nvalidity (eventually by relativity). It also implies that in non-equilibrium\ndynamics of strongly correlated bosonic models far-away regions may become\nquickly entangled, suggesting that their simulation may be much harder than\nthat of spin chains even in the low energy sector. \n\n"}
{"id": "0809.3170", "contents": "Title: A New Framework of Multistage Hypothesis Tests Abstract: In this paper, we have established a general framework of multistage\nhypothesis tests which applies to arbitrarily many mutually exclusive and\nexhaustive composite hypotheses. Within the new framework, we have constructed\nspecific multistage tests which rigorously control the risk of committing\ndecision errors and are more efficient than previous tests in terms of average\nsample number and the number of sampling operations. Without truncation, the\nsample numbers of our testing plans are absolutely bounded. \n\n"}
{"id": "0810.1556", "contents": "Title: Generic quantum walk using a coin-embedded shift operator Abstract: The study of quantum walk processes has been widely divided into two standard\nvariants, the discrete-time quantum walk (DTQW) and the continuous-time quantum\nwalk (CTQW). The connection between the two variants has been established by\nconsidering the limiting value of the coin operation parameter in the DTQW, and\nthe coin degree of freedom was shown to be unnecessary [26]. But the coin\ndegree of freedom is an additional resource which can be exploited to control\nthe dynamics of the QW process. In this paper we present a generic quantum walk\nmodel using a quantum coin-embedded unitary shift operation $U_{C}$. The\nstandard version of the DTQW and the CTQW can be conveniently retrieved from\nthis generic model, retaining the features of the coin degree of freedom in\nboth variants. \n\n"}
{"id": "0811.2030", "contents": "Title: A comparative study of dynamical simulation methods for the dissociation\n  of molecular Bose-Einstein condensates Abstract: We describe a pairing mean-field theory related to the\nHartree-Fock-Bogoliubov approach, and apply it to the dynamics of dissociation\nof a molecular Bose-Einstein condensate (BEC) into correlated bosonic atom\npairs. We also perform the same simulation using two stochastic phase-space\ntechniques for quantum dynamics -- the positive P-representation method and the\ntruncated Wigner method. By comparing the results of our calculations we are\nable to assess the relative strength of these theoretical techniques in\ndescribing molecular dissociation in one spatial dimension. An important aspect\nof our analysis is the inclusion of atom-atom interactions which can be\nproblematic for the positive-P method. We find that the truncated Wigner method\nmostly agrees with the positive-P simulations, but can be simulated for\nsignificantly longer times. The pairing mean-field theory results diverge from\nthe quantum dynamical methods after relatively short times. \n\n"}
{"id": "0812.0743", "contents": "Title: A Novel Clustering Algorithm Based on Quantum Games Abstract: Enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum game with the problem of data\nclustering, and then develop a quantum-game-based clustering algorithm, in\nwhich data points in a dataset are considered as players who can make decisions\nand implement quantum strategies in quantum games. After each round of a\nquantum game, each player's expected payoff is calculated. Later, he uses a\nlink-removing-and-rewiring (LRR) function to change his neighbors and adjust\nthe strength of links connecting to them in order to maximize his payoff.\nFurther, algorithms are discussed and analyzed in two cases of strategies, two\npayoff matrixes and two LRR functions. Consequently, the simulation results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms have fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach. \n\n"}
{"id": "0812.3034", "contents": "Title: Quantum transients Abstract: Quantum transients are temporary features of matter waves before they reach a\nstationary regime. Transients may arise after the preparation of an unstable\ninitial state or due to a sudden interaction or a change in the boundary\nconditions. Examples are diffraction in time, buildup processes, decay,\ntrapping, forerunners or pulse formation, as well as other phenomena recently\ndiscovered, such as the simultaneous arrival of a wave peak at arbitrarily\ndistant observers. The interest on these transients is nowadays enhanced by new\ntechnological possibilities to control, manipulate and measure matter waves. \n\n"}
{"id": "0901.1345", "contents": "Title: Simulations of quantum double models Abstract: We demonstrate how to build a simulation of two dimensional physical theories\ndescribing topologically ordered systems whose excitations are in one to one\ncorrespondence with irreducible representations of a Hopf algebra, D(G), the\nquantum double of a finite group G. Our simulation uses a digital sequence of\noperations on a spin lattice to prepare a ground \"vacuum\" state and to create,\nbraid and fuse anyonic excitations. The simulation works with or without the\npresence of a background Hamiltonian though only in the latter case is the\nsystem topologically protected. We describe a physical realization of a\nsimulation of the simplest non-Abelian model, D(S_3), using trapped neutral\natoms in a two dimensional optical lattice and provide a sequence of steps to\nperform universal quantum computation with anyons. The use of ancillary spin\ndegrees of freedom figures prominently in our construction and provides a novel\ntechnique to prepare and probe these systems. \n\n"}
{"id": "0901.3335", "contents": "Title: Quantum Optics with Quantum Gases Abstract: Quantum optics with quantum gases represents a new field, where the quantum\nnature of both light and ultracold matter plays equally important role. Only\nvery recently this ultimate quantum limit of light-matter interaction became\nfeasible experimentally. In traditional quantum optics, the cold atoms are\nconsidered classically, whereas, in quantum atom optics, the light is used as\nan essentially classical axillary tool. On the one hand, the quantization of\noptical trapping potentials can significantly modify many-body dynamics of\natoms, which is well-known only for classical potentials. On the other hand,\natomic fluctuations can modify the properties of the scattered light. \n\n"}
{"id": "0902.0387", "contents": "Title: Effective Abelian and non-Abelian gauge potentials in cavity QED Abstract: Cavity QED models are analyzed in terms of field quadrature operators. We\ndemonstrate that in such representation, the problem can be formulated in terms\nof effective gauge potentials. In this respect, it presents a completely new\nsystem in which gauge fields arise, possessing the advantages of purity, high\ncontrol of system parameters as well as preparation and detection methods.\nStudying three well known models, it is shown that either Abelian or\nnon-Abelian gauge potentials can be constructed. The non-Abelian\ncharacteristics are evidenced via numerical simulations utilizing experimental\nparameters. \n\n"}
{"id": "0902.4193", "contents": "Title: Optimal Control at the Quantum Speed Limit Abstract: Optimal control theory is a promising candidate for a drastic improvement of\nthe performance of quantum information tasks. We explore its ultimate limit in\nparadigmatic cases, and demonstrate that it coincides with the maximum speed\nlimit allowed by quantum evolution (the quantum speed limit). \n\n"}
{"id": "0906.2706", "contents": "Title: Extracavity quantum vacuum radiation from a single qubit Abstract: We present a theory of the quantum vacuum radiation that is generated by a\nfast modulation of the vacuum Rabi frequency of a single two-level system\nstrongly coupled to a single cavity mode. The dissipative dynamics of the\nJaynes-Cummings model in the presence of anti-rotating wave terms is described\nby a generalized master equation including non-Markovian terms. Peculiar\nspectral properties and significant extracavity quantum vacuum radiation output\nare predicted for state-of-the-art circuit cavity quantum electrodynamics\nsystems with superconducting qubits. \n\n"}
{"id": "0909.1868", "contents": "Title: Exchange interaction radically changes behavior of a quantum particle in\n  a classically forbidden region: simple model Abstract: Exchange interaction strongly influences the long-range behavior of localized\nelectron orbitals and quantum tunneling amplitudes.In the Hartree-Fock\napproximation the exchange produces a power-law decay instead of the usual\nexponential decrease at large distances. To show that this effect is real (i.e.\nnot a result of the approximation) we consider a simple model where different\neffects may be accurately analyzed. Applications include huge enhancement of\ninner electron ionization by a static electric field or laser field considered\nin Ref. M. Ya. Amusia, arxiv:0904.4395 \n\n"}
{"id": "0909.2122", "contents": "Title: Multiple atomic dark solitons in cigar-shaped Bose-Einstein condensates Abstract: We consider the stability and dynamics of multiple dark solitons in\ncigar-shaped Bose-Einstein condensates (BECs). Our study is motivated by the\nfact that multiple matter-wave dark solitons may naturally form in such\nsettings as per our recent work [Phys. Rev. Lett. 101, 130401 (2008)]. First,\nwe study the dark soliton interactions and show that the dynamics of\nwell-separated solitons (i.e., ones that undergo a collision with relatively\nlow velocities) can be analyzed by means of particle-like equations of motion.\nThe latter take into regard the repulsion between solitons (via an effective\nrepulsive potential) and the confinement and dimensionality of the system (via\nan effective parabolic trap for each soliton). Next, based on the fact that\nstationary, well-separated dark multi-soliton states emerge as a nonlinear\ncontinuation of the appropriate excited eigensates of the quantum harmonic\noscillator, we use a Bogoliubov-de Gennes analysis to systematically study the\nstability of such structures. We find that for a sufficiently large number of\natoms, multiple soliton states may be dynamically stable, while for a small\nnumber of atoms, we predict a dynamical instability emerging from resonance\neffects between the eigenfrequencies of the soliton modes and the intrinsic\nexcitation frequencies of the condensate. Finally we present experimental\nrealizations of multi-soliton states including a three-soliton state consisting\nof two solitons oscillating around a stationary one. \n\n"}
{"id": "0910.3133", "contents": "Title: Temperature dependence of the magnetic Casimir-Polder interaction Abstract: We analyze the magnetic dipole contribution to atom-surface dispersion\nforces. Unlike its electrical counterpart, it involves small transition\nfrequencies that are comparable to thermal energy scales. A significant\ntemperature dependence is found near surfaces with a nonzero DC conductivity,\nleading to a strong suppression of the dispersion force at T > 0. We use\nthermal response theory for the surface material and discuss both normal metals\nand superconductors. The asymptotes of the free energy of interaction and of\nthe entropy are calculated analytically over a large range of distances. Near a\nsuperconductor, the onset of dissipation at the phase transition strongly\nchanges the interaction, including a discontinuous entropy. We discuss the\nsimilarities with the Casimir interaction beween two surfaces and suggest that\nprecision measurements of the atom-surface interaction may shed new light upon\nopen questions around the temperature dependence of dispersion forces between\nlossy media. \n\n"}
{"id": "0910.4627", "contents": "Title: Self-concordant analysis for logistic regression Abstract: Most of the non-asymptotic theoretical work in regression is carried out for\nthe square loss, where estimators can be obtained through closed-form\nexpressions. In this paper, we use and extend tools from the convex\noptimization literature, namely self-concordant functions, to provide simple\nextensions of theoretical results for the square loss to the logistic loss. We\napply the extension techniques to logistic regression with regularization by\nthe $\\ell_2$-norm and regularization by the $\\ell_1$-norm, showing that new\nresults for binary classification through logistic regression can be easily\nderived from corresponding results for least-squares regression. \n\n"}
{"id": "0911.2398", "contents": "Title: High fidelity quantum memory via dynamical decoupling: theory and\n  experiment Abstract: Quantum information processing requires overcoming decoherence---the loss of\n\"quantumness\" due to the inevitable interaction between the quantum system and\nits environment. One approach towards a solution is quantum dynamical\ndecoupling---a method employing strong and frequent pulses applied to the\nqubits. Here we report on the first experimental test of the concatenated\ndynamical decoupling (CDD) scheme, which invokes recursively constructed pulse\nsequences. Using nuclear magnetic resonance, we demonstrate a near order of\nmagnitude improvement in the decay time of stored quantum states. In\nconjunction with recent results on high fidelity quantum gates using CDD, our\nresults suggest that quantum dynamical decoupling should be used as a first\nlayer of defense against decoherence in quantum information processing\nimplementations, and can be a stand-alone solution in the right parameter\nregime. \n\n"}
{"id": "0911.4046", "contents": "Title: Super-Linear Convergence of Dual Augmented-Lagrangian Algorithm for\n  Sparsity Regularized Estimation Abstract: We analyze the convergence behaviour of a recently proposed algorithm for\nregularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is\nbased on a new interpretation of DAL as a proximal minimization algorithm. We\ntheoretically show under some conditions that DAL converges super-linearly in a\nnon-asymptotic and global sense. Due to a special modelling of sparse\nestimation problems in the context of machine learning, the assumptions we make\nare milder and more natural than those made in conventional analysis of\naugmented Lagrangian algorithms. In addition, the new interpretation enables us\nto generalize DAL to wide varieties of sparse estimation problems. We\nexperimentally confirm our analysis in a large scale $\\ell_1$-regularized\nlogistic regression problem and extensively compare the efficiency of DAL\nalgorithm to previously proposed algorithms on both synthetic and benchmark\ndatasets. \n\n"}
{"id": "0912.1804", "contents": "Title: Dressed Qubits in Nuclear Spin Baths Abstract: We present a method to encode a \\textit{dressed} qubit into the product state\nof an electron spin localized in quantum dot and its surrounding nuclear spins\nvia a dressing transformation. In this scheme, the hyperfine coupling and a\nportion of nuclear dipole dipole interaction become logic gates, while they are\nthe sources of decoherence in electron spin qubit proposals. We discuss errors\nand corrections for the dressed qubits. Interestingly, the effective\nHamiltonian of nuclear spins is equivalent to a pairing Hamiltonian, which\nprovides the microscopic mechanism to protect dressed qubits against\ndecoherence. \n\n"}
{"id": "1002.0537", "contents": "Title: Resources Required for Topological Quantum Factoring Abstract: We consider a hypothetical topological quantum computer where the qubits are\ncomprised of either Ising or Fibonacci anyons. For each case, we calculate the\ntime and number of qubits (space) necessary to execute the most computationally\nexpensive step of Shor's algorithm, modular exponentiation. For Ising anyons,\nwe apply Bravyi's distillation method [S. Bravyi, Phys. Rev. A 73, 042313\n(2006)] which combines topological and non-topological operations to allow for\nuniversal quantum computation. With reasonable restrictions on the physical\nparameters we find that factoring a 128 bit number requires approximately 10^3\nFibonacci anyons versus at least 3 x 10^9 Ising anyons. Other distillation\nalgorithms could reduce the resources for Ising anyons substantially. \n\n"}
{"id": "1002.0601", "contents": "Title: Quasi-Fibonacci oscillators Abstract: We study the properties of sequences of the energy eigenvalues for some\ngeneralizations of q-deformed oscillators including the p,q-oscillator, the 3-,\n4- and 5-parameter deformed oscillators given in the literature. It is shown\nthat most of the considered models belong to the class of so-called Fibonacci\noscillators for which any three consequtive energy levels satisfy the relation\nE_{n+1}=\\lambda E_n+\\rho E_{n-1} with real constants \\lambda, \\rho. On the\nother hand, for certain \\mu-oscillator known from 1993 we prove the fact of its\nnon-Fibonacci nature. Possible generalizations of the three-term Fibonacci\nrelation are discussed among which we choose, as most adequate for the\n\\mu$-oscillator, the so-called quasi-Fibonacci (or local Fibonacci) property of\nthe energy levels. The property is encoded in the three-term quasi-Fibonacci\n(QF) relation with non-constant, n-dependent coefficients \\lambda and \\rho.\nVarious aspects of the QF relation are elaborated for the \\mu-oscillator and\nsome of its extensions. \n\n"}
{"id": "1002.4658", "contents": "Title: Principal Component Analysis with Contaminated Data: The High\n  Dimensional Case Abstract: We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero. \n\n"}
{"id": "1003.2087", "contents": "Title: The role of chaos in quantum communication through a dynamical dephasing\n  channel Abstract: In this article we treat the subject of chaotic environments with few degrees\nof freedom in quantum communication by investigating a conservative dynamical\nmap as a model of a dephasing quantum channel. When the channel's dynamics is\nchaotic, we investigate the model's semi-classical limit and show that the\nentropy exchange grows at a constant rate which depends on a single parameter\n(the interaction strength), analogous to stochastic models of dephasing\nchannels. We analyze memory effects in the channel and present strong physical\narguments to support that the present model is forgetful in the chaotic regime\nwhile memory effects in general cannot be ignored when channel dynamics is\nregular. In order to render the non-chaotic channel forgetful, it becomes\nnecessary to apply a reset to the channel and this reset can efficiently be\nmodeled by application of a chaotic map. We may then refer to encoding theorems\n(valid in the case of forgetful channels) to present evidence of a transition\nfrom noiseless to noisy channel due to the environment's transition from\nregular to chaotic dynamics. \n\n"}
{"id": "1004.0081", "contents": "Title: Limitations of quantum computing with Gaussian cluster states Abstract: We discuss the potential and limitations of Gaussian cluster states for\nmeasurement-based quantum computing. Using a framework of Gaussian projected\nentangled pair states (GPEPS), we show that no matter what Gaussian local\nmeasurements are performed on systems distributed on a general graph, transport\nand processing of quantum information is not possible beyond a certain\ninfluence region, except for exponentially suppressed corrections. We also\ndemonstrate that even under arbitrary non-Gaussian local measurements, slabs of\nGaussian cluster states of a finite width cannot carry logical quantum\ninformation, even if sophisticated encodings of qubits in continuous-variable\n(CV) systems are allowed for. This is proven by suitably contracting tensor\nnetworks representing infinite-dimensional quantum systems. The result can be\nseen as sharpening the requirements for quantum error correction and fault\ntolerance for Gaussian cluster states, and points towards the necessity of\nnon-Gaussian resource states for measurement-based quantum computing. The\nresults can equally be viewed as referring to Gaussian quantum repeater\nnetworks. \n\n"}
{"id": "1004.0509", "contents": "Title: Intrinsic geometry of quantum adiabatic evolution and quantum phase\n  transitions Abstract: We elucidate the geometry of quantum adiabatic evolution. By minimizing the\ndeviation from adiabaticity we find a Riemannian metric tensor underlying\nadiabatic evolution. Equipped with this tensor, we identify a unified geometric\ndescription of quantum adiabatic evolution and quantum phase transitions, which\ngeneralizes previous treatments to allow for degeneracy. The same structure is\nrelevant for applications in quantum information processing, including\nadiabatic and holonomic quantum computing, where geodesics over the manifold of\ncontrol parameters correspond to paths which minimize errors. We illustrate\nthis geometric structure with examples, for which we explicitly find adiabatic\ngeodesics. By solving the geodesic equations in the vicinity of a quantum\ncritical point, we identify universal characteristics of optimal adiabatic\npassage through a quantum phase transition. In particular, we show that in the\nvicinity of a critical point describing a second order quantum phase\ntransition, the geodesic exhibits power-law scaling with an exponent given by\ntwice the inverse of the product of the spatial and scaling dimensions. \n\n"}
{"id": "1004.5041", "contents": "Title: Circuit QED scheme for realization of the Lipkin-Meshkov-Glick model Abstract: We propose a scheme in which the Lipkin-Meshkov-Glick model is realized\nwithin a circuit QED system. An array of N superconducting qubits interacts\nwith a driven cavity mode. In the dispersive regime, the cavity mode is\nadiabatically eliminated generating an effective model for the qubits alone.\nThe characteristic long-range order of the Lipkin-Meshkov-Glick model is here\nmediated by the cavity field. For a closed qubit system, the inherent second\norder phase transition of the qubits is reflected in the intensity of the\noutput cavity field. In the broken symmetry phase, the many-body ground state\nis highly entangled. Relaxation of the qubits is analyzed within a mean-field\ntreatment. The second order phase transition is lost, while new bistable\nregimes occur. \n\n"}
{"id": "1005.2638", "contents": "Title: Hierarchical Clustering for Finding Symmetries and Other Patterns in\n  Massive, High Dimensional Datasets Abstract: Data analysis and data mining are concerned with unsupervised pattern finding\nand structure determination in data sets. \"Structure\" can be understood as\nsymmetry and a range of symmetries are expressed by hierarchy. Such symmetries\ndirectly point to invariants, that pinpoint intrinsic properties of the data\nand of the background empirical domain of interest. We review many aspects of\nhierarchy here, including ultrametric topology, generalized ultrametric,\nlinkages with lattices and other discrete algebraic structures and with p-adic\nnumber representations. By focusing on symmetries in data we have a powerful\nmeans of structuring and analyzing massive, high dimensional data stores. We\nillustrate the powerfulness of hierarchical clustering in case studies in\nchemistry and finance, and we provide pointers to other published case studies. \n\n"}
{"id": "1007.3622", "contents": "Title: A generalized risk approach to path inference based on hidden Markov\n  models Abstract: Motivated by the unceasing interest in hidden Markov models (HMMs), this\npaper re-examines hidden path inference in these models, using primarily a\nrisk-based framework. While the most common maximum a posteriori (MAP), or\nViterbi, path estimator and the minimum error, or Posterior Decoder (PD), have\nlong been around, other path estimators, or decoders, have been either only\nhinted at or applied more recently and in dedicated applications generally\nunfamiliar to the statistical learning community. Over a decade ago, however, a\nfamily of algorithmically defined decoders aiming to hybridize the two standard\nones was proposed (Brushe et al., 1998). The present paper gives a careful\nanalysis of this hybridization approach, identifies several problems and issues\nwith it and other previously proposed approaches, and proposes practical\nresolutions of those. Furthermore, simple modifications of the classical\ncriteria for hidden path recognition are shown to lead to a new class of\ndecoders. Dynamic programming algorithms to compute these decoders in the usual\nforward-backward manner are presented. A particularly interesting subclass of\nsuch estimators can be also viewed as hybrids of the MAP and PD estimators.\nSimilar to previously proposed MAP-PD hybrids, the new class is parameterized\nby a small number of tunable parameters. Unlike their algorithmic predecessors,\nthe new risk-based decoders are more clearly interpretable, and, most\nimportantly, work \"out of the box\" in practice, which is demonstrated on some\nreal bioinformatics tasks and data. Some further generalizations and\napplications are discussed in conclusion. \n\n"}
{"id": "1007.3957", "contents": "Title: Strong and weak thermalization of infinite non-integrable quantum\n  systems Abstract: When a non-integrable system evolves out of equilibrium for a long time,\nlocal observables are expected to attain stationary expectation values,\nindependent of the details of the initial state. However, intriguing\nexperimental results with ultracold gases have shown no thermalization in\nnon-integrable settings, triggering an intense theoretical effort to decide the\nquestion. Here we show that the phenomenology of thermalization in a quantum\nsystem is much richer than its classical counterpart. Using a new numerical\ntechnique, we identify two distinct thermalization regimes, strong and weak,\noccurring for different initial states. Strong thermalization, intrinsically\nquantum, happens when instantaneous local expectation values converge to the\nthermal ones. Weak thermalization, well-known in classical systems, happens\nwhen local expectation values converge to the thermal ones only after time\naveraging. Remarkably, we find a third group of states showing no\nthermalization, neither strong nor weak, to the time scales one can reliably\nsimulate. \n\n"}
{"id": "1008.3640", "contents": "Title: Progress in Experimental Measurements of the Surface-Surface Casimir\n  Force: Electrostatic Calibrations and Limitations to Accuracy Abstract: Several new experiments have extended studies of the Casimir force into new\nand interesting regimes. This recent work will be briefly reviewed. With this\nrecent progress, new issues with background electrostatic effects have been\nuncovered. The myriad of problems associated with both patch potentials and\nelectrostatic calibrations are discussed and the remaining open questions are\nbrought forward. \n\n"}
{"id": "1010.0442", "contents": "Title: Measurement of damping and temperature: Precision bounds in Gaussian\n  dissipative channels Abstract: We present a comprehensive analysis of the performance of different classes\nof Gaussian states in the estimation of Gaussian phase-insensitive dissipative\nchannels. In particular, we investigate the optimal estimation of the damping\nconstant and reservoir temperature. We show that, for two-mode squeezed vacuum\nprobe states, the quantum-limited accuracy of both parameters can be achieved\nsimultaneously. Moreover, we show that for both parameters two-mode squeezed\nvacuum states are more efficient than either coherent, thermal or single-mode\nsqueezed states. This suggests that at high energy regimes two-mode squeezed\nvacuum states are optimal within the Gaussian setup. This optimality result\nindicates a stronger form of compatibility for the estimation of the two\nparameters. Indeed, not only the minimum variance can be achieved at fixed\nprobe states, but also the optimal state is common to both parameters.\nAdditionally, we explore numerically the performance of non-Gaussian states for\nparticular parameter values to find that maximally entangled states within\nD-dimensional cutoff subspaces perform better than any randomly sampled states\nwith similar energy. However, we also find that states with very similar\nperformance and energy exist with much less entanglement than the maximally\nentangled ones. \n\n"}
{"id": "1010.1192", "contents": "Title: Spin dynamics in the optical cycle of single nitrogen-vacancy centres in\n  diamond Abstract: We investigate spin-dependent decay and intersystem crossing in the optical\ncycle of single negatively-charged nitrogen-vacancy (NV) centres in diamond. We\nuse spin control and pulsed optical excitation to extract both the\nspin-resolved lifetimes of the excited states and the degree of\noptically-induced spin polarization. By optically exciting the centre with a\nseries of picosecond pulses, we determine the spin-flip probabilities per\noptical cycle, as well as the spin-dependent probability for intersystem\ncrossing. This information, together with the indepedently measured decay rate\nof singlet population provides a full description of spin dynamics in the\noptical cycle of NV centres. The temperature dependence of the singlet\npopulation decay rate provides information on the number of singlet states\ninvolved in the optical cycle. \n\n"}
{"id": "1010.6287", "contents": "Title: Fabrication and characterization of high quality factor silicon nitride\n  nanobeam cavities Abstract: Si3N4 is an excellent material for applications of nanophotonics at visible\nwavelengths due to its wide bandgap and moderately large refractive index (n\n$\\approx$ 2.0). We present the fabrication and characterization of Si3N4\nphotonic crystal nanobeam cavities for coupling to diamond nanocrystals and\nNitrogen-Vacancy centers in a cavity QED system. Confocal\nmicro-photoluminescence analysis of the nanobeam cavities demonstrates quality\nfactors up to Q ~ 55,000, which is limited by the resolution of our\nspectrometer. We also demonstrate coarse tuning of cavity resonances across the\n600-700nm range by lithographically scaling the size of fabricated devices.\nThis is an order of magnitude improvement over previous SiNx cavities at this\nimportant wavelength range. \n\n"}
{"id": "1012.0859", "contents": "Title: Local non-CSS quantum error correcting code on a three-dimensional\n  lattice Abstract: We present a family of non-CSS quantum error-correcting code consisting of\ngeometrically local stabilizer generators on a 3D lattice. We study the\nHamiltonian constructed from ferromagnetic interaction of overcomplete set of\nlocal stabilizer generators. The degenerate ground state of the system is\ncharacterized by a quantum error-correcting code whose number of encoded qubits\nare equal to the second Betti number of the manifold. These models (i) have\nsolely local interactions; (ii) admit a strong-weak duality relation with an\nIsing model on a dual lattice; (iii) have topological order in the ground\nstate, some of which survive at finite temperature; and (iv) behave as\nclassical memory at finite temperature. \n\n"}
{"id": "1012.5183", "contents": "Title: Casimir-Lifshitz force out of thermal equilibrium and heat transfer\n  between arbitrary bodies Abstract: We study the Casimir-Lifshitz force and the radiative heat transfer occurring\nbetween two arbitrary bodies, each one held at a given temperature, surrounded\nby environmental radiation at a third temperature. The system, in stationary\nconfiguration out of thermal equilibrium, is characterized by a force and a\nheat transfer depending on the three temperatures, and explicitly expressed in\nterms of the scattering operators of each body. We find a closed-form analytic\nexpression valid for bodies of any geometry and dielectric properties. As an\nexample, the force between two parallel slabs of finite thickness is\ncalculated, showing the importance of the environmental temperature as well as\nthe occurrence of a repulsive interaction. An analytic expression is also\nprovided for the force acting on an atom in front of a slab. Our predictions\ncan be relevant for experimental and technological purposes. \n\n"}
{"id": "1101.4439", "contents": "Title: Reproducing Kernel Banach Spaces with the l1 Norm II: Error Analysis for\n  Regularized Least Square Regression Abstract: A typical approach in estimating the learning rate of a regularized learning\nscheme is to bound the approximation error by the sum of the sampling error,\nthe hypothesis error and the regularization error. Using a reproducing kernel\nspace that satisfies the linear representer theorem brings the advantage of\ndiscarding the hypothesis error from the sum automatically. Following this\ndirection, we illustrate how reproducing kernel Banach spaces with the l1 norm\ncan be applied to improve the learning rate estimate of l1-regularization in\nmachine learning. \n\n"}
{"id": "1101.5258", "contents": "Title: Casimir interaction between a dielectric nanosphere and a metallic plane Abstract: We study the Casimir interaction between a dielectric nanosphere and a\nmetallic plane, using the multiple scattering theory. Exact results are\nobtained with the dielectric described by a Sellmeier model and the metal by a\nDrude model. Asymptotic forms are discussed for small spheres, large or small\ndistances. The well-known Casimir-Polder formula is recovered at the limit of\nvanishingly small spheres, while an expression better behaved at small\ndistances is found for any finite value of the radius. The exact results are of\nparticular interest for the study of quantum states of nanospheres in the\nvicinity of surfaces. \n\n"}
{"id": "1102.0316", "contents": "Title: Partition Functions of Normal Factor Graphs Abstract: One of the most common types of functions in mathematics, physics, and\nengineering is a sum of products, sometimes called a partition function. After\n\"normalization,\" a sum of products has a natural graphical representation,\ncalled a normal factor graph (NFG), in which vertices represent factors, edges\nrepresent internal variables, and half-edges represent the external variables\nof the partition function. In physics, so-called trace diagrams share similar\nfeatures. We believe that the conceptual framework of representing sums of\nproducts as partition functions of NFGs is an important and intuitive paradigm\nthat, surprisingly, does not seem to have been introduced explicitly in the\nprevious factor graph literature. Of particular interest are NFG modifications\nthat leave the partition function invariant. A simple subclass of such NFG\nmodifications offers a unifying view of the Fourier transform, tree-based\nreparameterization, loop calculus, and the Legendre transform. \n\n"}
{"id": "1102.3821", "contents": "Title: Quantitative entanglement witnesses of Isotropic- and Werner-class via\n  local measurements Abstract: Quantitative entanglement witnesses allow one to bound the entanglement\npresent in a system by acquiring a single expectation value. In this paper we\nanalyze a special class of such observables which are associated with\n(generalized) Werner and Isotropic states. For them the optimal bounding\nfunctions can be easily derived by exploiting known results on twirling\ntransformations. By focusing on an explicit local decomposition for these\nobservables we then show how simple classical post-processing of the measured\ndata can tighten the entanglement bounds. Quantum optics implementations based\non hyper-entanglement generation schemes are analyzed. \n\n"}
{"id": "1102.4403", "contents": "Title: Effect of control procedures on the evolution of entanglement in open\n  quantum systems Abstract: The effect of a number of mechanisms designed to suppress decoherence in open\nquantum systems are studied with respect to their effectiveness at slowing down\nthe loss of entanglement. The effect of photonic band-gap materials and\nfrequency modulation of the system-bath coupling are along expected lines in\nthis regard. However, other control schemes, like resonance fluorescence,\nachieve quite the contrary: increasing the strength of the control kills\nentanglement off faster. The effect of dynamic decoupling schemes on two\nqualitatively different system-bath interactions are studied in depth. Dynamic\ndecoupling control has the expected effect of slowing down the decay of\nentanglement in a two-qubit system coupled to a harmonic oscillator bath under\nnon-demolition interaction. However, non-trivial phenomena are observed when a\nJosephson charge qubit, strongly coupled to a random telegraph noise bath, is\nsubject to decoupling pulses. The most striking of these reflects the resonance\nfluorescence scenario in that an increase in the pulse strength decreases\ndecoherence but also speeds up the sudden death of entanglement. This\ndemonstrates that the behaviour of decoherence and entanglement in time can be\nqualitatively different in the strong-coupling non-Markovian regime. \n\n"}
{"id": "1102.5748", "contents": "Title: A Spinning Particle in a Mobius Strip Abstract: We develop the classical and quantum theory of a spinning particle moving in\na Mobius strip. We first propose a Lagrangian for such a system and then we\nproceed to quantize the system via the constraint Hamiltonian system formalism.\nOur results may be of particular interest in several physical scenarios,\nincluding solid state physics and optics. In fact, the present work may shed\nsome new light on the recent discoveries on condensed matter concerning\ntopological insulators. \n\n"}
{"id": "1103.1417", "contents": "Title: Localization from Incomplete Noisy Distance Measurements Abstract: We consider the problem of positioning a cloud of points in the Euclidean\nspace $\\mathbb{R}^d$, using noisy measurements of a subset of pairwise\ndistances. This task has applications in various areas, such as sensor network\nlocalization and reconstruction of protein conformations from NMR measurements.\nAlso, it is closely related to dimensionality reduction problems and manifold\nlearning, where the goal is to learn the underlying global geometry of a data\nset using local (or partial) metric information. Here we propose a\nreconstruction algorithm based on semidefinite programming. For a random\ngeometric graph model and uniformly bounded noise, we provide a precise\ncharacterization of the algorithm's performance: In the noiseless case, we find\na radius $r_0$ beyond which the algorithm reconstructs the exact positions (up\nto rigid transformations). In the presence of noise, we obtain upper and lower\nbounds on the reconstruction error that match up to a factor that depends only\non the dimension $d$, and the average degree of the nodes in the graph. \n\n"}
{"id": "1103.2272", "contents": "Title: The Impact of Giulio Racah on Crystal- and Ligand-field Theories Abstract: This paper focuses on the impact of Racah on crystal- and ligand-field\ntheories, two branches of molecular physics and condensed matter physics\n(dealing with ions embedded in aggregates of finite symmetry). The role of\nRacah and some of his students in developing a symmetry-adapted weak-field\nmodel for crystal-field theory is examined. Then, we discuss the extension of\nthis model to a generalized symmetry-adapted weak-field model for ligand-field\ntheory. Symmetry considerations via the use of the Wigner-Racah algebra for\nchains of type SU(2) > G is essential for these weak-field models. Therefore,\nthe basic ingredients for the Wigner-Racah algebra of a finite or compact group\nare reviewed with a special attention paid to the SU(2) group in a SU(2) > G\nbasis. Finally, as an unexpected application of nonstandard SU(2) bases, it is\nshown how SU(2) bases adapted to the cyclic group allow to build bases of\nrelevance in quantum information. \n\n"}
{"id": "1104.3081", "contents": "Title: Digital Quantum Simulation with Rydberg Atoms Abstract: We discuss in detail the implementation of an open-system quantum simulator\nwith Rydberg states of neutral atoms held in an optical lattice. Our scheme\nallows one to realize both coherent as well as dissipative dynamics of complex\nspin models involving many-body interactions and constraints. The central\nbuilding block of the simulation scheme is constituted by a mesoscopic Rydberg\ngate that permits the entanglement of several atoms in an efficient, robust and\nquick protocol. In addition, optical pumping on ancillary atoms provides the\ndissipative ingredient for engineering the coupling between the system and a\ntailored environment. As an illustration, we discuss how the simulator enables\nthe simulation of coherent evolution of quantum spin models such as the\ntwo-dimensional Heisenberg model and Kitaev's toric code, which involves\nfour-body spin interactions. We moreover show that in principle also the\nsimulation of lattice fermions can be achieved. As an example for controlled\ndissipative dynamics, we discuss ground state cooling of frustration-free spin\nHamiltonians. \n\n"}
{"id": "1104.4150", "contents": "Title: Coherent spectroscopy of rare-earth-ion doped whispering-gallery mode\n  resonators Abstract: We perform an investigation into the properties of Pr3+:Y2SiO5 whispering\ngallery mode resonators as a first step towards achieving the strong coupling\nregime of cavity QED with rare-earth-ion doped crystals. Direct measurement of\ncavity QED parameters are made using photon echoes, giving good agreement with\ntheoretical predictions. By comparing the ions at the surface of the resonator\nto those in the center it is determined that the physical process of making the\nresonator does not negatively affect the properties of the ions. Coupling\nbetween the ions and resonator is analyzed through the observation of optical\nbistability and normal-mode splitting. \n\n"}
{"id": "1105.0540", "contents": "Title: Pruning nearest neighbor cluster trees Abstract: Nearest neighbor (k-NN) graphs are widely used in machine learning and data\nmining applications, and our aim is to better understand what they reveal about\nthe cluster structure of the unknown underlying distribution of points.\nMoreover, is it possible to identify spurious structures that might arise due\nto sampling variability?\n  Our first contribution is a statistical analysis that reveals how certain\nsubgraphs of a k-NN graph form a consistent estimator of the cluster tree of\nthe underlying distribution of points. Our second and perhaps most important\ncontribution is the following finite sample guarantee. We carefully work out\nthe tradeoff between aggressive and conservative pruning and are able to\nguarantee the removal of all spurious cluster structures at all levels of the\ntree while at the same time guaranteeing the recovery of salient clusters. This\nis the first such finite sample result in the context of clustering. \n\n"}
{"id": "1107.3828", "contents": "Title: A micropillar for cavity optomechanics Abstract: We present a new micromechanical resonator designed for cavity optomechanics.\nWe have used a micropillar geometry to obtain a high-frequency mechanical\nresonance with a low effective mass and a very high quality factor. We have\ncoated a 60-$\\mu$m diameter low-loss dielectric mirror on top of the pillar and\nare planning to use this micromirror as part of a high-finesse Fabry-Perot\ncavity, to laser cool the resonator down to its quantum ground state and to\nmonitor its quantum position fluctuations by quantum-limited optical\ninterferometry. \n\n"}
{"id": "1107.4297", "contents": "Title: Two-fermion composite quasi-bosons and deformed oscillators Abstract: The concept of quasi-bosons or composite bosons (like mesons, excitons etc.)\nhas a wide range of potential physical applications. Even composed of two pure\nfermions, the quasi-boson creation and annihilation operators satisfy\nnon-standard commutation relations. It is natural to try to realize the\nquasi-boson operators by the operators of deformed (nonlinear) oscillator, the\nlatter constituting widely studied field of modern quantum physics. In this\npaper, it is proved that the deformed oscillators which realize quasi-boson\noperators in a consistent way really exist. The conditions for such realization\nare derived, and the uniqueness of the family of deformations under\nconsideration is shown. \n\n"}
{"id": "1108.2416", "contents": "Title: Phonon-induced entanglement dynamics of two donor-based charge quantum\n  bits Abstract: The entanglement dynamics of a pair of donor-based charge qubits is obtained\nin analytical form. The disentanglement is induced by off resonant scattering\nof acoustical phonons in the semiconductor host. According to our results a\nrather unusual recovery of entanglement occurs that depends on the geometrical\nconfiguration of the qubits. In addition, for large times a non-vanishing\nstationary entanglement is predicted. For the cases of one and two initial\nexcitations a simple kinetic interpretation allows for an adequate analysis of\nthe observed dynamics. Our results also reveal a direct relation between the\ndisentanglement rate and the inter-donor decoherence rates. \n\n"}
{"id": "1109.1199", "contents": "Title: Two-Frequency Jahn-Teller Systems in Circuit QED Abstract: We investigate the simulation of Jahn-Teller models with two non-degenerate\nvibrational modes using a circuit QED architecture. Typical Jahn-Teller systems\nare anisotropic and require at least a two-frequency description. The proposed\nsimulator consists of two superconducting lumped-element resonators interacting\nwith a common flux qubit in the ultrastrong coupling regime. We translate the\ncircuit QED model of the system to a two-frequency Jahn-Teller Hamiltonian and\ncalculate its energy eigenvalues and the emission spectrum of the cavities. It\nis shown that the system can be systematically tuned to an effective single\nmode Hamiltonian from the two-mode model by varying the coupling strength\nbetween the resonators. The flexibility in manipulating the parameters of the\ncircuit QED simulator permits isolating the effective single frequency and pure\ntwo-frequency effects in the spectral response of Jahn-Teller systems. \n\n"}
{"id": "1111.5479", "contents": "Title: The Graphical Lasso: New Insights and Alternatives Abstract: The graphical lasso \\citep{FHT2007a} is an algorithm for learning the\nstructure in an undirected Gaussian graphical model, using $\\ell_1$\nregularization to control the number of zeros in the precision matrix\n${\\B\\Theta}={\\B\\Sigma}^{-1}$ \\citep{BGA2008,yuan_lin_07}. The {\\texttt R}\npackage \\GL\\ \\citep{FHT2007a} is popular, fast, and allows one to efficiently\nbuild a path of models for different values of the tuning parameter.\nConvergence of \\GL\\ can be tricky; the converged precision matrix might not be\nthe inverse of the estimated covariance, and occasionally it fails to converge\nwith warm starts. In this paper we explain this behavior, and propose new\nalgorithms that appear to outperform \\GL.\n  By studying the \"normal equations\" we see that, \\GL\\ is solving the {\\em\ndual} of the graphical lasso penalized likelihood, by block coordinate ascent;\na result which can also be found in \\cite{BGA2008}.\n  In this dual, the target of estimation is $\\B\\Sigma$, the covariance matrix,\nrather than the precision matrix $\\B\\Theta$. We propose similar primal\nalgorithms \\PGL\\ and \\DPGL, that also operate by block-coordinate descent,\nwhere $\\B\\Theta$ is the optimization target. We study all of these algorithms,\nand in particular different approaches to solving their coordinate\nsub-problems. We conclude that \\DPGL\\ is superior from several points of view. \n\n"}
{"id": "1112.5731", "contents": "Title: Time-dependent density functional theory for open spin chains Abstract: The application of methods of time-dependent density functional theory\n(TDDFT) to systems of qubits provided the interesting possibility of simulating\nan assigned Hamiltonian evolution by means of an auxiliary Hamiltonian having\ndifferent two-qubit interactions and hence a possibly simpler wave function\nevolution. In this note we extend these methods to some instances of Lindblad\nevolution of a spin chain. \n\n"}
{"id": "1112.6171", "contents": "Title: Transverse Ising Chain under Periodic Instantaneous Quenches: Dynamical\n  Many-Body Freezing and Emergence of Solitary Oscillation Abstract: We study the real-time dynamics of a quantum Ising chain driven periodically\nby instantaneous quenches of the transverse field (the transverse field varying\nas rectangular wave symmetric about zero). Two interesting phenomena are\nreported and analyzed: (1) We observe dynamical many-body freezing or DMF\n(Phys. Rev. B, vol. 82, 172402, 2010), i.e. strongly non-monotonic freezing of\nthe response (transverse magnetization) with respect to the driving parameters\n(pulse width and height) resulting from equivocal freezing behavior of all the\nmany-body modes. The freezing occurs due to coherent suppression of dynamics of\nthe many-body modes. For certain combination of the pulse height and period,\nmaximal freezing (freezing peaks) are observed. For those parameter values, a\nmassive collapse of the entire Floquet spectrum occurs. (2) Secondly, we\nobserve emergence of a distinct solitary oscillation with a single frequency,\nwhich can be much lower than the driving frequency. This slow oscillation,\ninvolving many high-energy modes, dominates the response remarkably in the\nlimit of long observation time. We identify this slow oscillation as the unique\nsurvivor of destructive quantum interference between the many-body modes. The\noscillation is found to decay algebraically with time to a constant value. All\nthe key features are demonstrated analytically with numerical evaluations for\nspecific results. \n\n"}
{"id": "1201.0794", "contents": "Title: Sparse Nonparametric Graphical Models Abstract: We present some nonparametric methods for graphical modeling. In the discrete\ncase, where the data are binary or drawn from a finite alphabet, Markov random\nfields are already essentially nonparametric, since the cliques can take only a\nfinite number of values. Continuous data are different. The Gaussian graphical\nmodel is the standard parametric model for continuous data, but it makes\ndistributional assumptions that are often unrealistic. We discuss two\napproaches to building more flexible graphical models. One allows arbitrary\ngraphs and a nonparametric extension of the Gaussian; the other uses kernel\ndensity estimation and restricts the graphs to trees and forests. Examples of\nboth methods are presented. We also discuss possible future research directions\nfor nonparametric graphical modeling. \n\n"}
{"id": "1201.1932", "contents": "Title: A quantum phase transition in a quantum external field: Superposing two\n  magnetic phases Abstract: We study an Ising chain undergoing a quantum phase transition in a quantum\nmagnetic field. Such a field can be emulated by coupling the chain to a central\nspin initially in a superposition state. We show that - by adiabatically\ndriving such a system - one can prepare a quantum superposition of any two\nground states of the Ising chain. In particular, one can end up with the Ising\nchain in a superposition of ferromagnetic and paramagnetic phases -- a scenario\nwith no analogue in prior studies of quantum phase transitions. Remarkably, the\nresulting magnetization of the chain encodes the position of the critical point\nand universal critical exponents, as well as the ground state fidelity. \n\n"}
{"id": "1202.5872", "contents": "Title: Speed limits for quantum gates in multi-qubit systems Abstract: We use analytical and numerical calculations to obtain speed limits for\nvarious unitary quantum operations in multiqubit systems under typical\nexperimental conditions. The operations that we consider include single-, two-,\nand three-qubit gates, as well as quantum-state transfer in a chain of qubits.\nWe find in particular that simple methods for implementing two-qubit gates\ngenerally provide the fastest possible implementations of these gates. We also\nfind that the three-qubit Toffoli gate time varies greatly depending on the\ntype of interactions and the system's geometry, taking only slightly longer\nthan a two-qubit controlled-NOT (CNOT) gate for a triangle geometry. The speed\nlimit for quantum-state transfer across a qubit chain is set by the maximum\nspin-wave speed in the chain. \n\n"}
{"id": "1203.0453", "contents": "Title: Change-Point Detection in Time-Series Data by Relative Density-Ratio\n  Estimation Abstract: The objective of change-point detection is to discover abrupt property\nchanges lying behind time-series data. In this paper, we present a novel\nstatistical change-point detection algorithm based on non-parametric divergence\nestimation between time-series samples from two retrospective segments. Our\nmethod uses the relative Pearson divergence as a divergence measure, and it is\naccurately and efficiently estimated by a method of direct density-ratio\nestimation. Through experiments on artificial and real-world datasets including\nhuman-activity sensing, speech, and Twitter messages, we demonstrate the\nusefulness of the proposed method. \n\n"}
{"id": "1203.2149", "contents": "Title: Cooperative effects in nuclear excitation with coherent x-ray light Abstract: The interaction between super-intense coherent x-ray light and nuclei is\nstudied theoretically. One of the main difficulties with driving nuclear\ntransitions arises from the very narrow nuclear excited state widths which\nlimit the coupling between laser and nuclei. In the context of direct\nlaser-nucleus interaction, we consider the nuclear width broadening that occurs\nwhen in solid targets, the excitation caused by a single photon is shared by a\nlarge number of nuclei, forming a collective excited state. Our results show\nthat for certain isotopes, cooperative effects may lead to an enhancement of\nthe nuclear excited state population by almost two orders of magnitude.\nAdditionally, an update of previous estimates for nuclear excited state\npopulation and signal photons taking into account the experimental advances of\nthe x-ray coherent light sources is given. The presented values are an\nimprovement by orders of magnitude and are encouraging for the future prospects\nof nuclear quantum optics. \n\n"}
{"id": "1203.4523", "contents": "Title: On the Equivalence between Herding and Conditional Gradient Algorithms Abstract: We show that the herding procedure of Welling (2009) takes exactly the form\nof a standard convex optimization algorithm--namely a conditional gradient\nalgorithm minimizing a quadratic moment discrepancy. This link enables us to\ninvoke convergence results from convex optimization and to consider faster\nalternatives for the task of approximating integrals in a reproducing kernel\nHilbert space. We study the behavior of the different variants through\nnumerical simulations. The experiments indicate that while we can improve over\nherding on the task of approximating integrals, the original herding algorithm\ntends to approach more often the maximum entropy distribution, shedding more\nlight on the learning bias behind herding. \n\n"}
{"id": "1203.5326", "contents": "Title: A General Approach to Casimir Force Problems Based on Local Reflection\n  Amplitudes and Huygen's Principle Abstract: In this paper we describe an approach to Casimir Force problems that is\nultimately generalizable to all fields, boundary conditions, and cavity\ngeometries. This approach utilizes locally defined reflection amplitudes to\nexpress the energy per unit area of any Casimir interaction. To demonstrate\nthis approach we solve a number of Casimir Force problems including the case of\nuniaxial boundary conditions in a parallel-plate cavity. \n\n"}
{"id": "1205.1240", "contents": "Title: Convex Relaxation for Combinatorial Penalties Abstract: In this paper, we propose an unifying view of several recently proposed\nstructured sparsity-inducing norms. We consider the situation of a model\nsimultaneously (a) penalized by a set- function de ned on the support of the\nunknown parameter vector which represents prior knowledge on supports, and (b)\nregularized in Lp-norm. We show that the natural combinatorial optimization\nproblems obtained may be relaxed into convex optimization problems and\nintroduce a notion, the lower combinatorial envelope of a set-function, that\ncharacterizes the tightness of our relaxations. We moreover establish links\nwith norms based on latent representations including the latent group Lasso and\nblock-coding, and with norms obtained from submodular functions. \n\n"}
{"id": "1205.4349", "contents": "Title: From Exact Learning to Computing Boolean Functions and Back Again Abstract: The goal of the paper is to relate complexity measures associated with the\nevaluation of Boolean functions (certificate complexity, decision tree\ncomplexity) and learning dimensions used to characterize exact learning\n(teaching dimension, extended teaching dimension). The high level motivation is\nto discover non-trivial relations between exact learning of an unknown concept\nand testing whether an unknown concept is part of a concept class or not.\nConcretely, the goal is to provide lower and upper bounds of complexity\nmeasures for one problem type in terms of the other. \n\n"}
{"id": "1205.5078", "contents": "Title: The driven Harper model Abstract: We analyze the driven Harper model, which appears in the problem of\ntight-binding electrons in the Hall configuration (normal to the lattice plane\nmagnetic field plus in-plane electric field). The presence of an electric field\nextends the celebrated Harper model, which is parametrized by the Peierls\nphase, into the driven Harper model, which is additionally parametrized by two\nBloch frequencies associated with the two components of the electric field. We\nshow that the eigenstates of the driven Harper model are either extended or\nlocalized, depending on the commensurability of Bloch frequencies. This results\nholds for both rational and irrational values of the Peierls phase. In the case\nof incommensurate Bloch frequencies we provide an estimate for the\nwave-function localization length. \n\n"}
{"id": "1206.1088", "contents": "Title: Bayesian Structure Learning for Markov Random Fields with a Spike and\n  Slab Prior Abstract: In recent years a number of methods have been developed for automatically\nlearning the (sparse) connectivity structure of Markov Random Fields. These\nmethods are mostly based on L1-regularized optimization which has a number of\ndisadvantages such as the inability to assess model uncertainty and expensive\ncross-validation to find the optimal regularization parameter. Moreover, the\nmodel's predictive performance may degrade dramatically with a suboptimal value\nof the regularization parameter (which is sometimes desirable to induce\nsparseness). We propose a fully Bayesian approach based on a \"spike and slab\"\nprior (similar to L0 regularization) that does not suffer from these\nshortcomings. We develop an approximate MCMC method combining Langevin dynamics\nand reversible jump MCMC to conduct inference in this model. Experiments show\nthat the proposed model learns a good combination of the structure and\nparameter values without the need for separate hyper-parameter tuning.\nMoreover, the model's predictive performance is much more robust than L1-based\nmethods with hyper-parameter settings that induce highly sparse model\nstructures. \n\n"}
{"id": "1206.1106", "contents": "Title: No More Pesky Learning Rates Abstract: The performance of stochastic gradient descent (SGD) depends critically on\nhow learning rates are tuned and decreased over time. We propose a method to\nautomatically adjust multiple learning rates so as to minimize the expected\nerror at any one time. The method relies on local gradient variations across\nsamples. In our approach, learning rates can increase as well as decrease,\nmaking it suitable for non-stationary problems. Using a number of convex and\nnon-convex learning tasks, we show that the resulting algorithm matches the\nperformance of SGD or other adaptive approaches with their best settings\nobtained through systematic search, and effectively removes the need for\nlearning rate tuning. \n\n"}
{"id": "1207.5000", "contents": "Title: Transport properties of anyons in random topological environments Abstract: The quasi one-dimensional transport of Abelian and non-Abelian anyons is\nstudied in the presence of a random topological background. In particular, we\nconsider the quantum walk of an anyon that braids around islands of randomly\nfilled static anyons of the same type. Two distinct behaviours are identified.\nWe analytically demonstrate that all types of Abelian anyons localise purely\ndue to the statistical phases induced by their random anyonic environment. In\ncontrast, we numerically show that non-Abelian Ising anyons do not localise.\nThis is due to their entanglement with the anyonic environment that effectively\ninduces dephasing. Our study demonstrates that localisation properties strongly\ndepend on non-local topological interactions and it provides a clear\ndistinction in the transport properties of Abelian and non-Abelian statistics. \n\n"}
{"id": "1207.5880", "contents": "Title: Analysis of the quantum Zeno effect for quantum control and computation Abstract: Within quantum information, many methods have been proposed to avoid or\ncorrect the deleterious effects of the environment on a system of interest. In\nthis work, expanding on our earlier paper [G. A. Paz-Silva et al., Phys. Rev.\nLett. 108, 080501 (2012), arXiv:1104.5507], we evaluate the applicability of\nthe quantum Zeno effect as one such method. Using the algebraic structure of\nstabilizer quantum error correction codes as a unifying framework, two\nopen-loop protocols are described which involve frequent non-projective (i.e.,\nweak) measurement of either the full stabilizer group or a minimal generating\nset thereof. The effectiveness of the protocols is measured by the distance\nbetween the final state under the protocol and the final state of an idealized\nevolution in which system and environment do not interact. Rigorous bounds on\nthis metric are derived which demonstrate that, under certain assumptions, a\nZeno effect may be realized with arbitrarily weak measurements, and that this\neffect can protect an arbitrary, unknown encoded state against the environment\narbitrarily well. \n\n"}
{"id": "1208.1056", "contents": "Title: Sequential Estimation Methods from Inclusion Principle Abstract: In this paper, we propose new sequential estimation methods based on\ninclusion principle. The main idea is to reformulate the estimation problems as\nconstructing sequential random intervals and use confidence sequences to\ncontrol the associated coverage probabilities. In contrast to existing\nasymptotic sequential methods, our estimation procedures rigorously guarantee\nthe pre-specified levels of confidence. \n\n"}
{"id": "1208.1237", "contents": "Title: Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix\n  Factorization Abstract: In this paper, we study the nonnegative matrix factorization problem under\nthe separability assumption (that is, there exists a cone spanned by a small\nsubset of the columns of the input nonnegative data matrix containing all\ncolumns), which is equivalent to the hyperspectral unmixing problem under the\nlinear mixing model and the pure-pixel assumption. We present a family of fast\nrecursive algorithms, and prove they are robust under any small perturbations\nof the input data matrix. This family generalizes several existing\nhyperspectral unmixing algorithms and hence provides for the first time a\ntheoretical justification of their better practical performance. \n\n"}
{"id": "1208.3728", "contents": "Title: Online Learning with Predictable Sequences Abstract: We present methods for online linear optimization that take advantage of\nbenign (as opposed to worst-case) sequences. Specifically if the sequence\nencountered by the learner is described well by a known \"predictable process\",\nthe algorithms presented enjoy tighter bounds as compared to the typical worst\ncase bounds. Additionally, the methods achieve the usual worst-case regret\nbounds if the sequence is not benign. Our approach can be seen as a way of\nadding prior knowledge about the sequence within the paradigm of online\nlearning. The setting is shown to encompass partial and side information.\nVariance and path-length bounds can be seen as particular examples of online\nlearning with simple predictable sequences.\n  We further extend our methods and results to include competing with a set of\npossible predictable processes (models), that is \"learning\" the predictable\nprocess itself concurrently with using it to obtain better regret guarantees.\nWe show that such model selection is possible under various assumptions on the\navailable feedback. Our results suggest a promising direction of further\nresearch with potential applications to stock market and time series\nprediction. \n\n"}
{"id": "1208.5062", "contents": "Title: Changepoint detection for high-dimensional time series with missing data Abstract: This paper describes a novel approach to change-point detection when the\nobserved high-dimensional data may have missing elements. The performance of\nclassical methods for change-point detection typically scales poorly with the\ndimensionality of the data, so that a large number of observations are\ncollected after the true change-point before it can be reliably detected.\nFurthermore, missing components in the observed data handicap conventional\napproaches. The proposed method addresses these challenges by modeling the\ndynamic distribution underlying the data as lying close to a time-varying\nlow-dimensional submanifold embedded within the ambient observation space.\nSpecifically, streaming data is used to track a submanifold approximation,\nmeasure deviations from this approximation, and calculate a series of\nstatistics of the deviations for detecting when the underlying manifold has\nchanged in a sharp or unexpected manner. The approach described in this paper\nleverages several recent results in the field of high-dimensional data\nanalysis, including subspace tracking with missing data, multiscale analysis\ntechniques for point clouds, online optimization, and change-point detection\nperformance analysis. Simulations and experiments highlight the robustness and\nefficacy of the proposed approach in detecting an abrupt change in an otherwise\nslowly varying low-dimensional manifold. \n\n"}
{"id": "1209.1873", "contents": "Title: Stochastic Dual Coordinate Ascent Methods for Regularized Loss\n  Minimization Abstract: Stochastic Gradient Descent (SGD) has become popular for solving large scale\nsupervised machine learning optimization problems such as SVM, due to their\nstrong theoretical guarantees. While the closely related Dual Coordinate Ascent\n(DCA) method has been implemented in various software packages, it has so far\nlacked good convergence analysis. This paper presents a new analysis of\nStochastic Dual Coordinate Ascent (SDCA) showing that this class of methods\nenjoy strong theoretical guarantees that are comparable or better than SGD.\nThis analysis justifies the effectiveness of SDCA for practical applications. \n\n"}
{"id": "1209.5350", "contents": "Title: Learning Topic Models and Latent Bayesian Networks Under Expansion\n  Constraints Abstract: Unsupervised estimation of latent variable models is a fundamental problem\ncentral to numerous applications of machine learning and statistics. This work\npresents a principled approach for estimating broad classes of such models,\nincluding probabilistic topic models and latent linear Bayesian networks, using\nonly second-order observed moments. The sufficient conditions for\nidentifiability of these models are primarily based on weak expansion\nconstraints on the topic-word matrix, for topic models, and on the directed\nacyclic graph, for Bayesian networks. Because no assumptions are made on the\ndistribution among the latent variables, the approach can handle arbitrary\ncorrelations among the topics or latent factors. In addition, a tractable\nlearning method via $\\ell_1$ optimization is proposed and studied in numerical\nexperiments. \n\n"}
{"id": "1210.1461", "contents": "Title: A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and\n  Tighter Bound Abstract: The CUR matrix decomposition is an important extension of Nystr\\\"{o}m\napproximation to a general matrix. It approximates any data matrix in terms of\na small number of its columns and rows. In this paper we propose a novel\nrandomized CUR algorithm with an expected relative-error bound. The proposed\nalgorithm has the advantages over the existing relative-error CUR algorithms\nthat it possesses tighter theoretical bound and lower time complexity, and that\nit can avoid maintaining the whole data matrix in main memory. Finally,\nexperiments on several real-world datasets demonstrate significant improvement\nover the existing relative-error algorithms. \n\n"}
{"id": "1210.5474", "contents": "Title: Disentangling Factors of Variation via Generative Entangling Abstract: Here we propose a novel model family with the objective of learning to\ndisentangle the factors of variation in data. Our approach is based on the\nspike-and-slab restricted Boltzmann machine which we generalize to include\nhigher-order interactions among multiple latent variables. Seen from a\ngenerative perspective, the multiplicative interactions emulates the entangling\nof factors of variation. Inference in the model can be seen as disentangling\nthese generative factors. Unlike previous attempts at disentangling latent\nfactors, the proposed model is trained using no supervised information\nregarding the latent factors. We apply our model to the task of facial\nexpression classification. \n\n"}
{"id": "1210.6928", "contents": "Title: Dynamics of the rotated Dicke model Abstract: We study quantum dynamics of the rotationally driven Dicke model where the\ncollective spin is rotated around the z axis with a finite velocity. In the\nabsence of the rotating wave approximation we observe that for several\nphysically relevant initial states the position of the quantum critical point\nis shifted by the amount given by the applied rotation velocity. This allows us\nto probe the quantum criticality \"from a distance\" in parameter space without\nactual crossing of the quantum critical surface but instead by encircling it in\nthe parameter space. This may provide a useful experimental hint since the\nquantum state is not destroyed by this protocol. Moreover, for the coherent\ninitial state we observe an interesting non-equilibrium reentrant phenomenon of\nquantum critical behavior as a function of the driving velocity and construct a\nnon-equilibrium phase diagram of the driven model. \n\n"}
{"id": "1211.2717", "contents": "Title: Proximal Stochastic Dual Coordinate Ascent Abstract: We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results. \n\n"}
{"id": "1211.4116", "contents": "Title: The Algebraic Combinatorial Approach for Low-Rank Matrix Completion Abstract: We present a novel algebraic combinatorial view on low-rank matrix completion\nbased on studying relations between a few entries with tools from algebraic\ngeometry and matroid theory. The intrinsic locality of the approach allows for\nthe treatment of single entries in a closed theoretical and practical\nframework. More specifically, apart from introducing an algebraic combinatorial\ntheory of low-rank matrix completion, we present probability-one algorithms to\ndecide whether a particular entry of the matrix can be completed. We also\ndescribe methods to complete that entry from a few others, and to estimate the\nerror which is incurred by any method completing that entry. Furthermore, we\nshow how known results on matrix completion and their sampling assumptions can\nbe related to our new perspective and interpreted in terms of a completability\nphase transition. \n\n"}
{"id": "1211.6687", "contents": "Title: Robustness Analysis of Hottopixx, a Linear Programming Model for\n  Factoring Nonnegative Matrices Abstract: Although nonnegative matrix factorization (NMF) is NP-hard in general, it has\nbeen shown very recently that it is tractable under the assumption that the\ninput nonnegative data matrix is close to being separable (separability\nrequires that all columns of the input matrix belongs to the cone spanned by a\nsmall subset of these columns). Since then, several algorithms have been\ndesigned to handle this subclass of NMF problems. In particular, Bittorf,\nRecht, R\\'e and Tropp (`Factoring nonnegative matrices with linear programs',\nNIPS 2012) proposed a linear programming model, referred to as Hottopixx. In\nthis paper, we provide a new and more general robustness analysis of their\nmethod. In particular, we design a provably more robust variant using a\npost-processing strategy which allows us to deal with duplicates and near\nduplicates in the dataset. \n\n"}
{"id": "1301.5088", "contents": "Title: Piecewise Linear Multilayer Perceptrons and Dropout Abstract: We propose a new type of hidden layer for a multilayer perceptron, and\ndemonstrate that it obtains the best reported performance for an MLP on the\nMNIST dataset. \n\n"}
{"id": "1301.5220", "contents": "Title: Properties of the Least Squares Temporal Difference learning algorithm Abstract: This paper presents four different ways of looking at the well-known Least\nSquares Temporal Differences (LSTD) algorithm for computing the value function\nof a Markov Reward Process, each of them leading to different insights: the\noperator-theory approach via the Galerkin method, the statistical approach via\ninstrumental variables, the linear dynamical system view as well as the limit\nof the TD iteration. We also give a geometric view of the algorithm as an\noblique projection. Furthermore, there is an extensive comparison of the\noptimization problem solved by LSTD as compared to Bellman Residual\nMinimization (BRM). We then review several schemes for the regularization of\nthe LSTD solution. We then proceed to treat the modification of LSTD for the\ncase of episodic Markov Reward Processes. \n\n"}
{"id": "1302.0082", "contents": "Title: Distribution-Free Distribution Regression Abstract: `Distribution regression' refers to the situation where a response Y depends\non a covariate P where P is a probability distribution. The model is Y=f(P) +\nmu where f is an unknown regression function and mu is a random error.\nTypically, we do not observe P directly, but rather, we observe a sample from\nP. In this paper we develop theory and methods for distribution-free versions\nof distribution regression. This means that we do not make distributional\nassumptions about the error term mu and covariate P. We prove that when the\neffective dimension is small enough (as measured by the doubling dimension),\nthen the excess prediction risk converges to zero with a polynomial rate. \n\n"}
{"id": "1302.3639", "contents": "Title: A Latent Source Model for Nonparametric Time Series Classification Abstract: For classifying time series, a nearest-neighbor approach is widely used in\npractice with performance often competitive with or better than more elaborate\nmethods such as neural networks, decision trees, and support vector machines.\nWe develop theoretical justification for the effectiveness of\nnearest-neighbor-like classification of time series. Our guiding hypothesis is\nthat in many applications, such as forecasting which topics will become trends\non Twitter, there aren't actually that many prototypical time series to begin\nwith, relative to the number of time series we have access to, e.g., topics\nbecome trends on Twitter only in a few distinct manners whereas we can collect\nmassive amounts of Twitter data. To operationalize this hypothesis, we propose\na latent source model for time series, which naturally leads to a \"weighted\nmajority voting\" classification rule that can be approximated by a\nnearest-neighbor classifier. We establish nonasymptotic performance guarantees\nof both weighted majority voting and nearest-neighbor classification under our\nmodel accounting for how much of the time series we observe and the model\ncomplexity. Experimental results on synthetic data show weighted majority\nvoting achieving the same misclassification rate as nearest-neighbor\nclassification while observing less of the time series. We then use weighted\nmajority to forecast which news topics on Twitter become trends, where we are\nable to detect such \"trending topics\" in advance of Twitter 79% of the time,\nwith a mean early advantage of 1 hour and 26 minutes, a true positive rate of\n95%, and a false positive rate of 4%. \n\n"}
{"id": "1302.4389", "contents": "Title: Maxout Networks Abstract: We consider the problem of designing models to leverage a recently introduced\napproximate model averaging technique called dropout. We define a simple new\nmodel called maxout (so named because its output is the max of a set of inputs,\nand because it is a natural companion to dropout) designed to both facilitate\noptimization by dropout and improve the accuracy of dropout's fast approximate\nmodel averaging technique. We empirically verify that the model successfully\naccomplishes both of these tasks. We use maxout and dropout to demonstrate\nstate of the art classification performance on four benchmark datasets: MNIST,\nCIFAR-10, CIFAR-100, and SVHN. \n\n"}
{"id": "1303.1208", "contents": "Title: Classification with Asymmetric Label Noise: Consistency and Maximal\n  Denoising Abstract: In many real-world classification problems, the labels of training examples\nare randomly corrupted. Most previous theoretical work on classification with\nlabel noise assumes that the two classes are separable, that the label noise is\nindependent of the true class label, or that the noise proportions for each\nclass are known. In this work, we give conditions that are necessary and\nsufficient for the true class-conditional distributions to be identifiable.\nThese conditions are weaker than those analyzed previously, and allow for the\nclasses to be nonseparable and the noise levels to be asymmetric and unknown.\nThe conditions essentially state that a majority of the observed labels are\ncorrect and that the true class-conditional distributions are \"mutually\nirreducible,\" a concept we introduce that limits the similarity of the two\ndistributions. For any label noise problem, there is a unique pair of true\nclass-conditional distributions satisfying the proposed conditions, and we\nargue that this pair corresponds in a certain sense to maximal denoising of the\nobserved distributions.\n  Our results are facilitated by a connection to \"mixture proportion\nestimation,\" which is the problem of estimating the maximal proportion of one\ndistribution that is present in another. We establish a novel rate of\nconvergence result for mixture proportion estimation, and apply this to obtain\nconsistency of a discrimination rule based on surrogate loss minimization.\nExperimental results on benchmark data and a nuclear particle classification\nproblem demonstrate the efficacy of our approach. \n\n"}
{"id": "1304.2994", "contents": "Title: A Generalized Online Mirror Descent with Applications to Classification\n  and Regression Abstract: Online learning algorithms are fast, memory-efficient, easy to implement, and\napplicable to many prediction problems, including classification, regression,\nand ranking. Several online algorithms were proposed in the past few decades,\nsome based on additive updates, like the Perceptron, and some on multiplicative\nupdates, like Winnow. A unifying perspective on the design and the analysis of\nonline algorithms is provided by online mirror descent, a general prediction\nstrategy from which most first-order algorithms can be obtained as special\ncases. We generalize online mirror descent to time-varying regularizers with\ngeneric updates. Unlike standard mirror descent, our more general formulation\nalso captures second order algorithms, algorithms for composite losses and\nalgorithms for adaptive filtering. Moreover, we recover, and sometimes improve,\nknown regret bounds as special cases of our analysis using specific\nregularizers. Finally, we show the power of our approach by deriving a new\nsecond order algorithm with a regret bound invariant with respect to arbitrary\nrescalings of individual features. \n\n"}
{"id": "1304.3582", "contents": "Title: Chaos in circuit QED: decoherence, localization, and nonclassicality Abstract: We study the open system dynamics of a circuit QED model operating in the\nultrastrong coupling regime. If the resonator is pumped periodically in time\nthe underlying classical system is chaotic. Indeed, the periodically driven\nJaynes-Cummings model in the Born-Oppenheimer approximation resembles a Duffing\noscillator which in the classical limit is a well-known example of a chaotic\nsystem. Detection of the field quadrature of the output field acts as an\neffective position measurement of the oscillator. We address how such detection\naffects the quantum chaotic evolution in this bipartite system. We\ndifferentiate between single measurement realizations and ensembles of repeated\nmeasurements. In the former case a measurement/decoherence induced localization\neffect is encountered, while in the latter this localization is almost\ncompletely absent. This is in marked contrast to numerous earlier works\ndiscussing the quantum-classical correspondence in measured chaotic systems.\nThis lack of a classical correspondence under relatively strong measurement\ninduced decoherence is attributed to the inherent quantum nature of the qubit\nsubsystem and in particular to the quantum correlations between the qubit and\nthe field which persist despite the decoherence. \n\n"}
{"id": "1304.7195", "contents": "Title: Noise-resistant optimal spin squeezing via quantum control Abstract: Entangled atomic states, such as spin squeezed states, represent a promising\nresource for a new generation of quantum sensors and atomic clocks. We\ndemonstrate that optimal control techniques can be used to substantially\nenhance the degree of spin squeezing in strongly interacting many-body systems,\neven in the presence of noise and imperfections. Specifically, we present a\nprotocol that is robust to noise which outperforms conventional methods.\nPotential experimental implementations are discussed. \n\n"}
{"id": "1306.1610", "contents": "Title: Entanglement dynamics of a two-qubit system coupled individually to\n  Ohmic baths Abstract: Developed originally for the Holstein polaron, the Davydov D1 ansatz is an\nefficient, yet extremely accurate trial state for time-dependent variation of\nthe spin-boson model [J. Chem. Phys. 138, 084111 (2013)]. In this work, the\nDirac-Frenkel time-dependent variational procedure utilizing the Davydov D1\nansatz is implemented to study entanglement dynamics of two qubits under the\ninfluence of two independent baths. The Ohmic spectral density is used without\nthe Born-Markov approximation or the rotating-wave approximation. In the strong\ncoupling regime finite-time disentanglement is always found to exist, while at\nthe intermediate coupling regime, the entanglement dynamics calculated by\nDavydov D1 ansatz displays oscillatory behavior in addition to entanglement\ndisappearance and revival. \n\n"}
{"id": "1306.2035", "contents": "Title: Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean\n  Separation Abstract: While several papers have investigated computationally and statistically\nefficient methods for learning Gaussian mixtures, precise minimax bounds for\ntheir statistical performance as well as fundamental limits in high-dimensional\nsettings are not well-understood. In this paper, we provide precise information\ntheoretic bounds on the clustering accuracy and sample complexity of learning a\nmixture of two isotropic Gaussians in high dimensions under small mean\nseparation. If there is a sparse subset of relevant dimensions that determine\nthe mean separation, then the sample complexity only depends on the number of\nrelevant dimensions and mean separation, and can be achieved by a simple\ncomputationally efficient procedure. Our results provide the first step of a\ntheoretical basis for recent methods that combine feature selection and\nclustering. \n\n"}
{"id": "1306.2620", "contents": "Title: Decay of spin coherences in one-dimensional spin systems Abstract: Strategies to protect multi-qubit states against decoherence are difficult to\nformulate because of their complex many-body dynamics. A better knowledge of\nthe decay dynamics would help in the construction of decoupling control\nschemes. Here we use solid-state nuclear magnetic resonance techniques to\nexperimentally investigate the decay of coherent multi-spin states in linear\nspin chains. Leveraging on the quasi-one-dimensional geometry of Fluorapatite\ncrystal spin systems, we can gain a deeper insight on the multi-spin states\ncreated by the coherent evolution, and their subsequent decay, than it is\npossible in 3D systems. We are then able to formulate an analytical model that\ncaptures the key features of the decay. We can thus compare the decoherence\nbehavior for different initial states of the spin chain and link their decay\nrate to the state characteristics, in particular their coherence and long-range\ncorrelation among spins. Our experimental and theoretical study shows that the\nspin chains undergo a rich dynamics, with a slower decay rate than for the 3D\ncase, and thus might be more amenable to decoupling techniques. \n\n"}
{"id": "1306.4410", "contents": "Title: Joint estimation of sparse multivariate regression and conditional\n  graphical models Abstract: Multivariate regression model is a natural generalization of the classical\nunivari- ate regression model for fitting multiple responses. In this paper, we\npropose a high- dimensional multivariate conditional regression model for\nconstructing sparse estimates of the multivariate regression coefficient matrix\nthat accounts for the dependency struc- ture among the multiple responses. The\nproposed method decomposes the multivariate regression problem into a series of\npenalized conditional log-likelihood of each response conditioned on the\ncovariates and other responses. It allows simultaneous estimation of the sparse\nregression coefficient matrix and the sparse inverse covariance matrix. The\nasymptotic selection consistency and normality are established for the\ndiverging dimension of the covariates and number of responses. The\neffectiveness of the pro- posed method is also demonstrated in a variety of\nsimulated examples as well as an application to the Glioblastoma multiforme\ncancer data. \n\n"}
{"id": "1306.5362", "contents": "Title: A Statistical Perspective on Algorithmic Leveraging Abstract: One popular method for dealing with large-scale data sets is sampling. For\nexample, by using the empirical statistical leverage scores as an importance\nsampling distribution, the method of algorithmic leveraging samples and\nrescales rows/columns of data matrices to reduce the data size before\nperforming computations on the subproblem. This method has been successful in\nimproving computational efficiency of algorithms for matrix problems such as\nleast-squares approximation, least absolute deviations approximation, and\nlow-rank matrix approximation. Existing work has focused on algorithmic issues\nsuch as worst-case running times and numerical issues associated with providing\nhigh-quality implementations, but none of it addresses statistical aspects of\nthis method.\n  In this paper, we provide a simple yet effective framework to evaluate the\nstatistical properties of algorithmic leveraging in the context of estimating\nparameters in a linear regression model with a fixed number of predictors. We\nshow that from the statistical perspective of bias and variance, neither\nleverage-based sampling nor uniform sampling dominates the other. This result\nis particularly striking, given the well-known result that, from the\nalgorithmic perspective of worst-case analysis, leverage-based sampling\nprovides uniformly superior worst-case algorithmic results, when compared with\nuniform sampling. Based on these theoretical results, we propose and analyze\ntwo new leveraging algorithms. A detailed empirical evaluation of existing\nleverage-based methods as well as these two new methods is carried out on both\nsynthetic and real data sets. The empirical results indicate that our theory is\na good predictor of practical performance of existing and new leverage-based\nalgorithms and that the new algorithms achieve improved performance. \n\n"}
{"id": "1307.2579", "contents": "Title: Tuned Models of Peer Assessment in MOOCs Abstract: In massive open online courses (MOOCs), peer grading serves as a critical\ntool for scaling the grading of complex, open-ended assignments to courses with\ntens or hundreds of thousands of students. But despite promising initial\ntrials, it does not always deliver accurate results compared to human experts.\nIn this paper, we develop algorithms for estimating and correcting for grader\nbiases and reliabilities, showing significant improvement in peer grading\naccuracy on real data with 63,199 peer grades from Coursera's HCI course\nofferings --- the largest peer grading networks analysed to date. We relate\ngrader biases and reliabilities to other student factors such as student\nengagement, performance as well as commenting style. We also show that our\nmodel can lead to more intelligent assignment of graders to gradees. \n\n"}
{"id": "1307.6134", "contents": "Title: Modeling Human Decision-making in Generalized Gaussian Multi-armed\n  Bandits Abstract: We present a formal model of human decision-making in explore-exploit tasks\nusing the context of multi-armed bandit problems, where the decision-maker must\nchoose among multiple options with uncertain rewards. We address the standard\nmulti-armed bandit problem, the multi-armed bandit problem with transition\ncosts, and the multi-armed bandit problem on graphs. We focus on the case of\nGaussian rewards in a setting where the decision-maker uses Bayesian inference\nto estimate the reward values. We model the decision-maker's prior knowledge\nwith the Bayesian prior on the mean reward. We develop the upper credible limit\n(UCL) algorithm for the standard multi-armed bandit problem and show that this\ndeterministic algorithm achieves logarithmic cumulative expected regret, which\nis optimal performance for uninformative priors. We show how good priors and\ngood assumptions on the correlation structure among arms can greatly enhance\ndecision-making performance, even over short time horizons. We extend to the\nstochastic UCL algorithm and draw several connections to human decision-making\nbehavior. We present empirical data from human experiments and show that human\nperformance is efficiently captured by the stochastic UCL algorithm with\nappropriate parameters. For the multi-armed bandit problem with transition\ncosts and the multi-armed bandit problem on graphs, we generalize the UCL\nalgorithm to the block UCL algorithm and the graphical block UCL algorithm,\nrespectively. We show that these algorithms also achieve logarithmic cumulative\nexpected regret and require a sub-logarithmic expected number of transitions\namong arms. We further illustrate the performance of these algorithms with\nnumerical examples. NB: Appendix G included in this version details minor\nmodifications that correct for an oversight in the previously-published proofs.\nThe remainder of the text reflects the published work. \n\n"}
{"id": "1308.4615", "contents": "Title: Quantum Computing Gates via Optimal Control Abstract: We demonstrate the use of optimal control to design two entropy-manipulating\nquantum gates which are more complex than the corresponding, commonly used,\ngates, such as CNOT and Toffoli (CCNOT): A 2-qubit gate called PE (polarization\nexchange) and a 3-qubit gate called COMP (polarization compression) were\ndesigned using GRAPE, an optimal control algorithm. Both gates were designed\nfor a three-spin system. Our design provided efficient and robust NMR radio\nfrequency (RF) pulses for 13C2-trichloroethylene (TCE), our chosen three-spin\nsystem. We then experimentally applied these two quantum gates onto TCE at the\nNMR lab. Such design of these gates and others could be relevant for\nnear-future applications of quantum computing devices. \n\n"}
{"id": "1308.5546", "contents": "Title: Sparse and Non-Negative BSS for Noisy Data Abstract: Non-negative blind source separation (BSS) has raised interest in various\nfields of research, as testified by the wide literature on the topic of\nnon-negative matrix factorization (NMF). In this context, it is fundamental\nthat the sources to be estimated present some diversity in order to be\nefficiently retrieved. Sparsity is known to enhance such contrast between the\nsources while producing very robust approaches, especially to noise. In this\npaper we introduce a new algorithm in order to tackle the blind separation of\nnon-negative sparse sources from noisy measurements. We first show that\nsparsity and non-negativity constraints have to be carefully applied on the\nsought-after solution. In fact, improperly constrained solutions are unlikely\nto be stable and are therefore sub-optimal. The proposed algorithm, named nGMCA\n(non-negative Generalized Morphological Component Analysis), makes use of\nproximal calculus techniques to provide properly constrained solutions. The\nperformance of nGMCA compared to other state-of-the-art algorithms is\ndemonstrated by numerical experiments encompassing a wide variety of settings,\nwith negligible parameter tuning. In particular, nGMCA is shown to provide\nrobustness to noise and performs well on synthetic mixtures of real NMR\nspectra. \n\n"}
{"id": "1308.6253", "contents": "Title: Quantum Simulation Abstract: Simulating quantum mechanics is known to be a difficult computational\nproblem, especially when dealing with large systems. However, this difficulty\nmay be overcome by using some controllable quantum system to study another less\ncontrollable or accessible quantum system, i.e., quantum simulation. Quantum\nsimulation promises to have applications in the study of many problems in,\ne.g., condensed-matter physics, high-energy physics, atomic physics, quantum\nchemistry and cosmology. Quantum simulation could be implemented using quantum\ncomputers, but also with simpler, analog devices that would require less\ncontrol, and therefore, would be easier to construct. A number of quantum\nsystems such as neutral atoms, ions, polar molecules, electrons in\nsemiconductors, superconducting circuits, nuclear spins and photons have been\nproposed as quantum simulators. This review outlines the main theoretical and\nexperimental aspects of quantum simulation and emphasizes some of the\nchallenges and promises of this fast-growing field. \n\n"}
{"id": "1308.6339", "contents": "Title: New bounds for circulant Johnson-Lindenstrauss embeddings Abstract: This paper analyzes circulant Johnson-Lindenstrauss (JL) embeddings which, as\nan important class of structured random JL embeddings, are formed by\nrandomizing the column signs of a circulant matrix generated by a random\nvector. With the help of recent decoupling techniques and matrix-valued\nBernstein inequalities, we obtain a new bound\n$k=O(\\epsilon^{-2}\\log^{(1+\\delta)} (n))$ for Gaussian circulant JL embeddings.\nMoreover, by using the Laplace transform technique (also called Bernstein's\ntrick), we extend the result to subgaussian case. The bounds in this paper\noffer a small improvement over the current best bounds for Gaussian circulant\nJL embeddings for certain parameter regimes and are derived using more direct\nmethods. \n\n"}
{"id": "1309.2308", "contents": "Title: Breakdown of quasilocality in long-range quantum lattice models Abstract: We study the non-equilibrium dynamics of correlations in quantum lattice\nmodels in the presence of long-range interactions decaying asymptotically as a\npower law. For exponents larger than the lattice dimensionality, a\nLieb-Robinson-type bound effectively restricts the spreading of correlations to\na causal region, but allows supersonic propagation. We show that this decay is\nnot only sufficient but also necessary. Using tools of quantum metrology, for\nany exponents smaller than the lattice dimension, we construct Hamiltonians\ngiving rise to quantum channels with capacities not restricted to a causal\nregion. An analytical analysis of long-range Ising models illustrates the\ndisappearance of the causal region and the creation of correlations becoming\ndistance-independent. Numerical results obtained using matrix product state\nmethods for the XXZ spin chain reveal the presence of a sound cone for large\nexponents, and supersonic propagation for small ones. In all models we analyzed\nthe fast spreading of correlations follows a power law, but not the exponential\nincrease of the long-range Lieb-Robinson bound. \n\n"}
{"id": "1310.1415", "contents": "Title: Narrowing the Gap: Random Forests In Theory and In Practice Abstract: Despite widespread interest and practical use, the theoretical properties of\nrandom forests are still not well understood. In this paper we contribute to\nthis understanding in two ways. We present a new theoretically tractable\nvariant of random regression forests and prove that our algorithm is\nconsistent. We also provide an empirical evaluation, comparing our algorithm\nand other theoretically tractable random forest models to the random forest\nalgorithm used in practice. Our experiments provide insight into the relative\nimportance of different simplifications that theoreticians have made to obtain\ntractable models for analysis. \n\n"}
{"id": "1311.1619", "contents": "Title: Transfer Matrices as Non-Unitary S-Matrices, Multimode Unidirectional\n  Invisibility, and Perturbative Inverse Scattering Abstract: We show that in one dimension the transfer matrix M of any scattering\npotential v coincides with the S-matrix of an associated time-dependent\nnon-Hermitian 2 x 2 matrix Hamiltonian H(\\tau). If v is real-valued, H(\\tau) is\npseudo-Hermitian and its exceptional points correspond to the classical turning\npoints of v. Applying time-dependent perturbation theory to H(\\tau) we obtain a\nperturbative series expansion for M and use it to study the phenomenon of\nunidirectional invisibility. In particular, we establish the possibility of\nhaving multimode unidirectional invisibility with wavelength-dependent\ndirection of invisibility and construct various physically realizable optical\npotentials possessing this property. We also offer a simple demonstration of\nthe fact that the off-diagonal entries of the first Born approximation for M\ndetermine the form of the potential. This gives rise to a perturbative inverse\nscattering scheme that is particularly suitable for optical design. As a simple\napplication of this scheme, we construct an infinite-range unidirectionally\ninvisible potential. \n\n"}
{"id": "1311.7385", "contents": "Title: Algorithmic Identification of Probabilities Abstract: TThe problem is to identify a probability associated with a set of natural\nnumbers, given an infinite data sequence of elements from the set. If the given\nsequence is drawn i.i.d. and the probability mass function involved (the\ntarget) belongs to a computably enumerable (c.e.) or co-computably enumerable\n(co-c.e.) set of computable probability mass functions, then there is an\nalgorithm to almost surely identify the target in the limit. The technical tool\nis the strong law of large numbers. If the set is finite and the elements of\nthe sequence are dependent while the sequence is typical in the sense of\nMartin-L\\\"of for at least one measure belonging to a c.e. or co-c.e. set of\ncomputable measures, then there is an algorithm to identify in the limit a\ncomputable measure for which the sequence is typical (there may be more than\none such measure). The technical tool is the theory of Kolmogorov complexity.\nWe give the algorithms and consider the associated predictions. \n\n"}
{"id": "1312.1666", "contents": "Title: Semi-Stochastic Gradient Descent Methods Abstract: In this paper we study the problem of minimizing the average of a large\nnumber ($n$) of smooth convex loss functions. We propose a new method, S2GD\n(Semi-Stochastic Gradient Descent), which runs for one or several epochs in\neach of which a single full gradient and a random number of stochastic\ngradients is computed, following a geometric law. The total work needed for the\nmethod to output an $\\varepsilon$-accurate solution in expectation, measured in\nthe number of passes over data, or equivalently, in units equivalent to the\ncomputation of a single gradient of the loss, is\n$O((\\kappa/n)\\log(1/\\varepsilon))$, where $\\kappa$ is the condition number.\nThis is achieved by running the method for $O(\\log(1/\\varepsilon))$ epochs,\nwith a single gradient evaluation and $O(\\kappa)$ stochastic gradient\nevaluations in each. The SVRG method of Johnson and Zhang arises as a special\ncase. If our method is limited to a single epoch only, it needs to evaluate at\nmost $O((\\kappa/\\varepsilon)\\log(1/\\varepsilon))$ stochastic gradients. In\ncontrast, SVRG requires $O(\\kappa/\\varepsilon^2)$ stochastic gradients. To\nillustrate our theoretical results, S2GD only needs the workload equivalent to\nabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for\na problem with $n=10^9$ and $\\kappa=10^3$. \n\n"}
{"id": "1312.1847", "contents": "Title: Understanding Deep Architectures using a Recursive Convolutional Network Abstract: A key challenge in designing convolutional network models is sizing them\nappropriately. Many factors are involved in these decisions, including number\nof layers, feature maps, kernel sizes, etc. Complicating this further is the\nfact that each of these influence not only the numbers and dimensions of the\nactivation units, but also the total number of parameters. In this paper we\nfocus on assessing the independent contributions of three of these linked\nvariables: The numbers of layers, feature maps, and parameters. To accomplish\nthis, we employ a recursive convolutional network whose weights are tied\nbetween layers; this allows us to vary each of the three factors in a\ncontrolled setting. We find that while increasing the numbers of layers and\nparameters each have clear benefit, the number of feature maps (and hence\ndimensionality of the representation) appears ancillary, and finds most of its\nbenefit through the introduction of more weights. Our results (i) empirically\nconfirm the notion that adding layers alone increases computational power,\nwithin the context of convolutional layers, and (ii) suggest that precise\nsizing of convolutional feature map dimensions is itself of little concern;\nmore attention should be paid to the number of parameters in these layers\ninstead. \n\n"}
{"id": "1312.4426", "contents": "Title: Optimization for Compressed Sensing: the Simplex Method and Kronecker\n  Sparsification Abstract: In this paper we present two new approaches to efficiently solve large-scale\ncompressed sensing problems. These two ideas are independent of each other and\ncan therefore be used either separately or together. We consider all\npossibilities.\n  For the first approach, we note that the zero vector can be taken as the\ninitial basic (infeasible) solution for the linear programming problem and\ntherefore, if the true signal is very sparse, some variants of the simplex\nmethod can be expected to take only a small number of pivots to arrive at a\nsolution. We implemented one such variant and demonstrate a dramatic\nimprovement in computation time on very sparse signals.\n  The second approach requires a redesigned sensing mechanism in which the\nvector signal is stacked into a matrix. This allows us to exploit the Kronecker\ncompressed sensing (KCS) mechanism. We show that the Kronecker sensing requires\nstronger conditions for perfect recovery compared to the original vector\nproblem. However, the Kronecker sensing, modeled correctly, is a much sparser\nlinear optimization problem. Hence, algorithms that benefit from sparse problem\nrepresentation, such as interior-point methods, can solve the Kronecker sensing\nproblems much faster than the corresponding vector problem. In our numerical\nstudies, we demonstrate a ten-fold improvement in the computation time. \n\n"}
{"id": "1312.5734", "contents": "Title: Time-varying Learning and Content Analytics via Sparse Factor Analysis Abstract: We propose SPARFA-Trace, a new machine learning-based framework for\ntime-varying learning and content analytics for education applications. We\ndevelop a novel message passing-based, blind, approximate Kalman filter for\nsparse factor analysis (SPARFA), that jointly (i) traces learner concept\nknowledge over time, (ii) analyzes learner concept knowledge state transitions\n(induced by interacting with learning resources, such as textbook sections,\nlecture videos, etc, or the forgetting effect), and (iii) estimates the content\norganization and intrinsic difficulty of the assessment questions. These\nquantities are estimated solely from binary-valued (correct/incorrect) graded\nlearner response data and a summary of the specific actions each learner\nperforms (e.g., answering a question or studying a learning resource) at each\ntime instance. Experimental results on two online course datasets demonstrate\nthat SPARFA-Trace is capable of tracing each learner's concept knowledge\nevolution over time, as well as analyzing the quality and content organization\nof learning resources, the question-concept associations, and the question\nintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparable\nor better performance in predicting unobserved learner responses than existing\ncollaborative filtering and knowledge tracing approaches for personalized\neducation. \n\n"}
{"id": "1312.5847", "contents": "Title: Deep learning for neuroimaging: a validation study Abstract: Deep learning methods have recently made notable advances in the tasks of\nclassification and representation learning. These tasks are important for brain\nimaging and neuroscience discovery, making the methods attractive for porting\nto a neuroimager's toolbox. Success of these methods is, in part, explained by\nthe flexibility of deep learning models. However, this flexibility makes the\nprocess of porting to new areas a difficult parameter optimization problem. In\nthis work we demonstrate our results (and feasible parameter ranges) in\napplication of deep learning methods to structural and functional brain imaging\ndata. We also describe a novel constraint-based approach to visualizing high\ndimensional data. We use it to analyze the effect of parameter choices on data\ntransformations. Our results show that deep learning methods are able to learn\nphysiologically important representations and detect latent relations in\nneuroimaging data. \n\n"}
{"id": "1312.6114", "contents": "Title: Auto-Encoding Variational Bayes Abstract: How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions are two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results. \n\n"}
{"id": "1401.2304", "contents": "Title: Lasso and equivalent quadratic penalized models Abstract: The least absolute shrinkage and selection operator (lasso) and ridge\nregression produce usually different estimates although input, loss function\nand parameterization of the penalty are identical. In this paper we look for\nridge and lasso models with identical solution set.\n  It turns out, that the lasso model with shrink vector $\\lambda$ and a\nquadratic penalized model with shrink matrix as outer product of $\\lambda$ with\nitself are equivalent, in the sense that they have equal solutions. To achieve\nthis, we have to restrict the estimates to be positive. This doesn't limit the\narea of application since we can easily decompose every estimate in a positive\nand negative part. The resulting problem can be solved with a non negative\nleast square algorithm.\n  Beside this quadratic penalized model, an augmented regression model with\npositive bounded estimates is developed which is also equivalent to the lasso\nmodel, but is probably faster to solve. \n\n"}
{"id": "1401.2753", "contents": "Title: Stochastic Optimization with Importance Sampling Abstract: Uniform sampling of training data has been commonly used in traditional\nstochastic optimization algorithms such as Proximal Stochastic Gradient Descent\n(prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although\nuniform sampling can guarantee that the sampled stochastic quantity is an\nunbiased estimate of the corresponding true quantity, the resulting estimator\nmay have a rather high variance, which negatively affects the convergence of\nthe underlying optimization procedure. In this paper we study stochastic\noptimization with importance sampling, which improves the convergence rate by\nreducing the stochastic variance. Specifically, we study prox-SGD (actually,\nstochastic mirror descent) with importance sampling and prox-SDCA with\nimportance sampling. For prox-SGD, instead of adopting uniform sampling\nthroughout the training process, the proposed algorithm employs importance\nsampling to minimize the variance of the stochastic gradient. For prox-SDCA,\nthe proposed importance sampling scheme aims to achieve higher expected dual\nvalue at each dual coordinate ascent step. We provide extensive theoretical\nanalysis to show that the convergence rates with the proposed importance\nsampling methods can be significantly improved under suitable conditions both\nfor prox-SGD and for prox-SDCA. Experiments are provided to verify the\ntheoretical analysis. \n\n"}
{"id": "1401.3632", "contents": "Title: Bayesian Conditional Density Filtering Abstract: We propose a Conditional Density Filtering (C-DF) algorithm for efficient\nonline Bayesian inference. C-DF adapts MCMC sampling to the online setting,\nsampling from approximations to conditional posterior distributions obtained by\npropagating surrogate conditional sufficient statistics (a function of data and\nparameter estimates) as new data arrive. These quantities eliminate the need to\nstore or process the entire dataset simultaneously and offer a number of\ndesirable features. Often, these include a reduction in memory requirements and\nruntime and improved mixing, along with state-of-the-art parameter inference\nand prediction. These improvements are demonstrated through several\nillustrative examples including an application to high dimensional compressed\nregression. Finally, we show that C-DF samples converge to the target posterior\ndistribution asymptotically as sampling proceeds and more data arrives. \n\n"}
{"id": "1401.3737", "contents": "Title: Coordinate Descent with Online Adaptation of Coordinate Frequencies Abstract: Coordinate descent (CD) algorithms have become the method of choice for\nsolving a number of optimization problems in machine learning. They are\nparticularly popular for training linear models, including linear support\nvector machine classification, LASSO regression, and logistic regression.\n  We consider general CD with non-uniform selection of coordinates. Instead of\nfixing selection frequencies beforehand we propose an online adaptation\nmechanism for this important parameter, called the adaptive coordinate\nfrequencies (ACF) method. This mechanism removes the need to estimate optimal\ncoordinate frequencies beforehand, and it automatically reacts to changing\nrequirements during an optimization run.\n  We demonstrate the usefulness of our ACF-CD approach for a variety of\noptimization problems arising in machine learning contexts. Our algorithm\noffers significant speed-ups over state-of-the-art training methods. \n\n"}
{"id": "1401.5899", "contents": "Title: Kernel Least Mean Square with Adaptive Kernel Size Abstract: Kernel adaptive filters (KAF) are a class of powerful nonlinear filters\ndeveloped in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel is\nusually the default kernel in KAF algorithms, but selecting the proper kernel\nsize (bandwidth) is still an open important issue especially for learning with\nsmall sample sizes. In previous research, the kernel size was set manually or\nestimated in advance by Silvermans rule based on the sample distribution. This\nstudy aims to develop an online technique for optimizing the kernel size of the\nkernel least mean square (KLMS) algorithm. A sequential optimization strategy\nis proposed, and a new algorithm is developed, in which the filter weights and\nthe kernel size are both sequentially updated by stochastic gradient algorithms\nthat minimize the mean square error (MSE). Theoretical results on convergence\nare also presented. The excellent performance of the new algorithm is confirmed\nby simulations on static function estimation and short term chaotic time series\nprediction. \n\n"}
{"id": "1402.0119", "contents": "Title: Randomized Nonlinear Component Analysis Abstract: Classical methods such as Principal Component Analysis (PCA) and Canonical\nCorrelation Analysis (CCA) are ubiquitous in statistics. However, these\ntechniques are only able to reveal linear relationships in data. Although\nnonlinear variants of PCA and CCA have been proposed, these are computationally\nprohibitive in the large scale.\n  In a separate strand of recent research, randomized methods have been\nproposed to construct features that help reveal nonlinear patterns in data. For\nbasic tasks such as regression or classification, random features exhibit\nlittle or no loss in performance, while achieving drastic savings in\ncomputational requirements.\n  In this paper we leverage randomness to design scalable new variants of\nnonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such\nas spectral clustering or LDA. We demonstrate our algorithms through\nexperiments on real-world data, on which we compare against the\nstate-of-the-art. A simple R implementation of the presented algorithms is\nprovided. \n\n"}
{"id": "1402.2447", "contents": "Title: A comparison of linear and non-linear calibrations for speaker\n  recognition Abstract: In recent work on both generative and discriminative score to\nlog-likelihood-ratio calibration, it was shown that linear transforms give good\naccuracy only for a limited range of operating points. Moreover, these methods\nrequired tailoring of the calibration training objective functions in order to\ntarget the desired region of best accuracy. Here, we generalize the linear\nrecipes to non-linear ones. We experiment with a non-linear, non-parametric,\ndiscriminative PAV solution, as well as parametric, generative,\nmaximum-likelihood solutions that use Gaussian, Student's T and\nnormal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scores\nsuggest that the non-linear methods provide wider ranges of optimal accuracy\nand can be trained without having to resort to objective function tailoring. \n\n"}
{"id": "1402.4552", "contents": "Title: Dark State Adiabatic Passage with spin-one particles Abstract: Adiabatic transport of information is a widely invoked resource in connection\nwith quantum information processing and distribution. The study of adiabatic\ntransport via spin-half chains or clusters is standard in the literature, while\nin practice the true realisation of a completely isolated two-level quantum\nsystem is not achievable. We explore here, theoretically, the extension of\nspin-half chain models to higher spins. Considering arrangements of three\nspin-one particles, we show that adiabatic transport, specifically a\ngeneralisation of the Dark State Adiabatic Passage procedure, is applicable to\nspin-one systems. We thus demonstrate a qutrit state transfer protocol. We\ndiscuss possible ways to physically implement this protocol, considering\nquantum dot and nitrogen-vacancy implementations. \n\n"}
{"id": "1402.4862", "contents": "Title: Learning the Parameters of Determinantal Point Process Kernels Abstract: Determinantal point processes (DPPs) are well-suited for modeling repulsion\nand have proven useful in many applications where diversity is desired. While\nDPPs have many appealing properties, such as efficient sampling, learning the\nparameters of a DPP is still considered a difficult problem due to the\nnon-convex nature of the likelihood function. In this paper, we propose using\nBayesian methods to learn the DPP kernel parameters. These methods are\napplicable in large-scale and continuous DPP settings even when the exact form\nof the eigendecomposition is unknown. We demonstrate the utility of our DPP\nlearning methods in studying the progression of diabetic neuropathy based on\nspatial distribution of nerve fibers, and in studying human perception of\ndiversity in images. \n\n"}
{"id": "1402.5731", "contents": "Title: Information-Theoretic Bounds for Adaptive Sparse Recovery Abstract: We derive an information-theoretic lower bound for sample complexity in\nsparse recovery problems where inputs can be chosen sequentially and\nadaptively. This lower bound is in terms of a simple mutual information\nexpression and unifies many different linear and nonlinear observation models.\nUsing this formula we derive bounds for adaptive compressive sensing (CS),\ngroup testing and 1-bit CS problems. We show that adaptivity cannot decrease\nsample complexity in group testing, 1-bit CS and CS with linear sparsity. In\ncontrast, we show there might be mild performance gains for CS in the sublinear\nregime. Our unified analysis also allows characterization of gains due to\nadaptivity from a wider perspective on sparse problems. \n\n"}
{"id": "1402.7005", "contents": "Title: Bayesian Multi-Scale Optimistic Optimization Abstract: Bayesian optimization is a powerful global optimization technique for\nexpensive black-box functions. One of its shortcomings is that it requires\nauxiliary optimization of an acquisition function at each iteration. This\nauxiliary optimization can be costly and very hard to carry out in practice.\nMoreover, it creates serious theoretical concerns, as most of the convergence\nresults assume that the exact optimum of the acquisition function can be found.\nIn this paper, we introduce a new technique for efficient global optimization\nthat combines Gaussian process confidence bounds and treed simultaneous\noptimistic optimization to eliminate the need for auxiliary optimization of\nacquisition functions. The experiments with global optimization benchmarks and\na novel application to automatic information extraction demonstrate that the\nresulting technique is more efficient than the two approaches from which it\ndraws inspiration. Unlike most theoretical analyses of Bayesian optimization\nwith Gaussian processes, our finite-time convergence rate proofs do not require\nexact optimization of an acquisition function. That is, our approach eliminates\nthe unsatisfactory assumption that a difficult, potentially NP-hard, problem\nhas to be solved in order to obtain vanishing regret rates. \n\n"}
{"id": "1403.3485", "contents": "Title: A Bright Solitonic Matter-Wave Interferometer Abstract: We present the first realisation of a solitonic atom interferometer. A\nBose-Einstein condensate of $1\\times10^4$ atoms of rubidium-85 is loaded into a\nhorizontal optical waveguide. Through the use of a Feshbach resonance, the\n$s$-wave scattering length of the $^{85}$Rb atoms is tuned to a small negative\nvalue. This attractive atomic interaction then balances the inherent\nmatter-wave dispersion, creating a bright solitonic matter wave. A Mach-Zehnder\ninterferometer is constructed by driving Bragg transitions with the use of an\noptical lattice co-linear with the waveguide. Matter wave propagation and\ninterferometric fringe visibility are compared across a range of $s$-wave\nscattering values including repulsive, attractive and non-interacting values.\nThe solitonic matter wave is found to significantly increase fringe visibility\neven compared with a non-interacting cloud. \n\n"}
{"id": "1403.5607", "contents": "Title: Bayesian Optimization with Unknown Constraints Abstract: Recent work on Bayesian optimization has shown its effectiveness in global\noptimization of difficult black-box objective functions. Many real-world\noptimization problems of interest also have constraints which are unknown a\npriori. In this paper, we study Bayesian optimization for constrained problems\nin the general case that noise may be present in the constraint functions, and\nthe objective and constraints may be evaluated independently. We provide\nmotivating practical examples, and present a general framework to solve such\nproblems. We demonstrate the effectiveness of our approach on optimizing the\nperformance of online latent Dirichlet allocation subject to topic sparsity\nconstraints, tuning a neural network given test-time memory constraints, and\noptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed\ntime, subject to passing standard convergence diagnostics. \n\n"}
{"id": "1403.5997", "contents": "Title: Bayesian calibration for forensic evidence reporting Abstract: We introduce a Bayesian solution for the problem in forensic speaker\nrecognition, where there may be very little background material for estimating\nscore calibration parameters. We work within the Bayesian paradigm of evidence\nreporting and develop a principled probabilistic treatment of the problem,\nwhich results in a Bayesian likelihood-ratio as the vehicle for reporting\nweight of evidence. We show in contrast, that reporting a likelihood-ratio\ndistribution does not solve this problem. Our solution is experimentally\nexercised on a simulated forensic scenario, using NIST SRE'12 scores, which\ndemonstrates a clear advantage for the proposed method compared to the\ntraditional plugin calibration recipe. \n\n"}
{"id": "1404.3634", "contents": "Title: Optimal Quench for Distance-Independent Entanglement and Maximal Block\n  Entropy Abstract: We optimize a quantum walk of multiple fermions following a quench in a spin\nchain to generate near ideal resources for quantum networking. We first prove\nan useful theorem mapping the correlations evolved from specific quenches to\nthe apparently unrelated problem of quantum state transfer between distinct\nspins. This mapping is then exploited to optimize the dynamics and produce\nlarge amounts of entanglement distributed in very special ways. Two\napplications are considered: the simultaneous generation of many Bell states\nbetween pairs of distant spins (maximal block entropy), or high entanglement\nbetween the ends of an arbitrarily long chain (distance-independent\nentanglement). Thanks to the generality of the result, we study its\nimplementation in different experimental setups using present technology: NMR,\nion traps and ultracold atoms in optical lattices. \n\n"}
{"id": "1405.3536", "contents": "Title: Improving offline evaluation of contextual bandit algorithms via\n  bootstrapping techniques Abstract: In many recommendation applications such as news recommendation, the items\nthat can be rec- ommended come and go at a very fast pace. This is a challenge\nfor recommender systems (RS) to face this setting. Online learning algorithms\nseem to be the most straight forward solution. The contextual bandit framework\nwas introduced for that very purpose. In general the evaluation of a RS is a\ncritical issue. Live evaluation is of- ten avoided due to the potential loss of\nrevenue, hence the need for offline evaluation methods. Two options are\navailable. Model based meth- ods are biased by nature and are thus difficult to\ntrust when used alone. Data driven methods are therefore what we consider here.\nEvaluat- ing online learning algorithms with past data is not simple but some\nmethods exist in the litera- ture. Nonetheless their accuracy is not satisfac-\ntory mainly due to their mechanism of data re- jection that only allow the\nexploitation of a small fraction of the data. We precisely address this issue\nin this paper. After highlighting the limita- tions of the previous methods, we\npresent a new method, based on bootstrapping techniques. This new method comes\nwith two important improve- ments: it is much more accurate and it provides a\nmeasure of quality of its estimation. The latter is a highly desirable property\nin order to minimize the risks entailed by putting online a RS for the first\ntime. We provide both theoretical and ex- perimental proofs of its superiority\ncompared to state-of-the-art methods, as well as an analysis of the convergence\nof the measure of quality. \n\n"}
{"id": "1405.7778", "contents": "Title: Universal Quantum Computation with Metaplectic Anyons Abstract: We show that braidings of the metaplectic anyons $X_\\epsilon$ in\n$SO(3)_2=SU(2)_4$ with their total charge equal to the metaplectic mode $Y$\nsupplemented with measurements of the total charge of two metaplectic anyons\nare universal for quantum computation. We conjecture that similar universal\ncomputing models can be constructed for all metaplectic anyon systems $SO(p)_2$\nfor any odd prime $p\\geq 5$. In order to prove universality, we find new\nconceptually appealing universal gate sets for qutrits and qupits. \n\n"}
{"id": "1406.2338", "contents": "Title: Cellular-automaton decoders for topological quantum memories Abstract: We introduce a new framework for constructing topological quantum memories,\nby recasting error recovery as a dynamical process on a field generating\ncellular automaton. We envisage quantum systems controlled by a classical\nhardware composed of small local memories, communicating with neighbours, and\nrepeatedly performing identical simple update rules. This approach does not\nrequire any global operations or complex decoding algorithms. Our cellular\nautomata draw inspiration from classical field theories, with a Coulomb-like\npotential naturally emerging from the local dynamics. For a 3D automaton\ncoupled to a 2D toric code, we present evidence of an error correction\nthreshold above 6.1% for uncorrelated noise. A 2D automaton equipped with a\nmore complex update rule yields a threshold above 8.2%. Our framework provides\ndecisive new tools in the quest for realising a passive dissipative quantum\nmemory. \n\n"}
{"id": "1406.2582", "contents": "Title: Probabilistic ODE Solvers with Runge-Kutta Means Abstract: Runge-Kutta methods are the classic family of solvers for ordinary\ndifferential equations (ODEs), and the basis for the state of the art. Like\nmost numerical methods, they return point estimates. We construct a family of\nprobabilistic numerical methods that instead return a Gauss-Markov process\ndefining a probability distribution over the ODE solution. In contrast to prior\nwork, we construct this family such that posterior means match the outputs of\nthe Runge-Kutta family exactly, thus inheriting their proven good properties.\nRemaining degrees of freedom not identified by the match to Runge-Kutta are\nchosen such that the posterior probability measure fits the observed structure\nof the ODE. Our results shed light on the structure of Runge-Kutta solvers from\na new direction, provide a richer, probabilistic output, have low computational\ncost, and raise new research questions. \n\n"}
{"id": "1406.5286", "contents": "Title: Enhancing Pure-Pixel Identification Performance via Preconditioning Abstract: In this paper, we analyze different preconditionings designed to enhance\nrobustness of pure-pixel search algorithms, which are used for blind\nhyperspectral unmixing and which are equivalent to near-separable nonnegative\nmatrix factorization algorithms. Our analysis focuses on the successive\nprojection algorithm (SPA), a simple, efficient and provably robust algorithm\nin the pure-pixel algorithm class. Recently, a provably robust preconditioning\nwas proposed by Gillis and Vavasis (arXiv:1310.2273) which requires the\nresolution of a semidefinite program (SDP) to find a data points-enclosing\nminimum volume ellipsoid. Since solving the SDP in high precisions can be time\nconsuming, we generalize the robustness analysis to approximate solutions of\nthe SDP, that is, solutions whose objective function values are some\nmultiplicative factors away from the optimal value. It is shown that a high\naccuracy solution is not crucial for robustness, which paves the way for faster\npreconditionings (e.g., based on first-order optimization methods). This first\ncontribution also allows us to provide a robustness analysis for two other\npreconditionings. The first one is pre-whitening, which can be interpreted as\nan optimal solution of the same SDP with additional constraints. We analyze\nrobustness of pre-whitening which allows us to characterize situations in which\nit performs competitively with the SDP-based preconditioning. The second one is\nbased on SPA itself and can be interpreted as an optimal solution of a\nrelaxation of the SDP. It is extremely fast while competing with the SDP-based\npreconditioning on several synthetic data sets. \n\n"}
{"id": "1406.5736", "contents": "Title: Convex Optimization Learning of Faithful Euclidean Distance\n  Representations in Nonlinear Dimensionality Reduction Abstract: Classical multidimensional scaling only works well when the noisy distances\nobserved in a high dimensional space can be faithfully represented by Euclidean\ndistances in a low dimensional space. Advanced models such as Maximum Variance\nUnfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-Definite\nProgramming (SDP) to reconstruct such faithful representations. While those SDP\nmodels are capable of producing high quality configuration numerically, they\nsuffer two major drawbacks. One is that there exist no theoretically guaranteed\nbounds on the quality of the configuration. The other is that they are slow in\ncomputation when the data points are beyond moderate size. In this paper, we\npropose a convex optimization model of Euclidean distance matrices. We\nestablish a non-asymptotic error bound for the random graph model with\nsub-Gaussian noise, and prove that our model produces a matrix estimator of\nhigh accuracy when the order of the uniform sample size is roughly the degree\nof freedom of a low-rank matrix up to a logarithmic factor. Our results\npartially explain why MVU and MVE often work well. Moreover, we develop a fast\ninexact accelerated proximal gradient method. Numerical experiments show that\nthe model can produce configurations of high quality on large data points that\nthe SDP approach would struggle to cope with. \n\n"}
{"id": "1407.3289", "contents": "Title: Altitude Training: Strong Bounds for Single-Layer Dropout Abstract: Dropout training, originally designed for deep neural networks, has been\nsuccessful on high-dimensional single-layer natural language tasks. This paper\nproposes a theoretical explanation for this phenomenon: we show that, under a\ngenerative Poisson topic model with long documents, dropout training improves\nthe exponent in the generalization bound for empirical risk minimization.\nDropout achieves this gain much like a marathon runner who practices at\naltitude: once a classifier learns to perform reasonably well on training\nexamples that have been artificially corrupted by dropout, it will do very well\non the uncorrupted test set. We also show that, under similar conditions,\ndropout preserves the Bayes decision boundary and should therefore induce\nminimal bias in high dimensions. \n\n"}
{"id": "1407.5978", "contents": "Title: Sequential Changepoint Approach for Online Community Detection Abstract: We present new algorithms for detecting the emergence of a community in large\nnetworks from sequential observations. The networks are modeled using\nErdos-Renyi random graphs with edges forming between nodes in the community\nwith higher probability. Based on statistical changepoint detection\nmethodology, we develop three algorithms: the Exhaustive Search (ES), the\nmixture, and the Hierarchical Mixture (H-Mix) methods. Performance of these\nmethods is evaluated by the average run length (ARL), which captures the\nfrequency of false alarms, and the detection delay. Numerical comparisons show\nthat the ES method performs the best; however, it is exponentially complex. The\nmixture method is polynomially complex by exploiting the fact that the size of\nthe community is typically small in a large network. However, it may react to a\ngroup of active edges that do not form a community. This issue is resolved by\nthe H-Mix method, which is based on a dendrogram decomposition of the network.\nWe present an asymptotic analytical expression for ARL of the mixture method\nwhen the threshold is large. Numerical simulation verifies that our\napproximation is accurate even in the non-asymptotic regime. Hence, it can be\nused to determine a desired threshold efficiently. Finally, numerical examples\nshow that the mixture and the H-Mix methods can both detect a community quickly\nwith a lower complexity than the ES method. \n\n"}
{"id": "1408.4616", "contents": "Title: Exploring quantum phases by driven dissipation Abstract: Ever since the insight spreaded that tailored dissipation can be employed to\ncontrol quantum systems and drive them towards pure states, the field of\nnon-equilibrium quantum mechanics gained remarkable momentum. So far research\nfocussed on emergent phenomena caused by the interplay and competition of\nunitary Hamiltonian and dissipative Markovian dynamics. In this manuscript we\nzero in on a so far rather understudied aspect of open quantum systems and\nnon-equilibrium physics, namely the utilization of purely dissipative couplings\nto explore pure quantum phases and non-equilibrium phase transitions. To\nillustrate this concept, we introduce and scrutinize purely dissipative\ncounterparts of (1) the paradigmatic transverse field Ising model and (2) the\nconsiderably more complex $\\mathbb{Z}_2$ lattice gauge theory with coupled\nmatter field. We show that, in mean field approximation, the non-equilibrium\nphase diagrams parallel the (thermal) phase diagrams of the Hamiltonian \"blue\nprint\" theories qualitatively. \n\n"}
{"id": "1409.5209", "contents": "Title: Pedestrian Detection with Spatially Pooled Features and Structured\n  Ensemble Learning Abstract: Many typical applications of object detection operate within a prescribed\nfalse-positive range. In this situation the performance of a detector should be\nassessed on the basis of the area under the ROC curve over that range, rather\nthan over the full curve, as the performance outside the range is irrelevant.\nThis measure is labelled as the partial area under the ROC curve (pAUC). We\npropose a novel ensemble learning method which achieves a maximal detection\nrate at a user-defined range of false positive rates by directly optimizing the\npartial AUC using structured learning.\n  In order to achieve a high object detection performance, we propose a new\napproach to extract low-level visual features based on spatial pooling.\nIncorporating spatial pooling improves the translational invariance and thus\nthe robustness of the detection process. Experimental results on both synthetic\nand real-world data sets demonstrate the effectiveness of our approach, and we\nshow that it is possible to train state-of-the-art pedestrian detectors using\nthe proposed structured ensemble learning method with spatially pooled\nfeatures. The result is the current best reported performance on the\nCaltech-USA pedestrian detection dataset. \n\n"}
{"id": "1410.0059", "contents": "Title: Counterdiabatic driving of the quantum Ising model Abstract: The system undergoes adiabatic evolution when its population in the\ninstantaneous eigenbasis of its time-dependent Hamiltonian changes only\nnegligibly. Realization of such dynamics requires slow-enough changes of the\nparameters of the Hamiltonian, a task that can be hard to achieve near quantum\ncritical points. A powerful alternative is provided by the counterdiabatic\nmodification of the Hamiltonian allowing for an arbitrarily quick\nimplementation of the adiabatic dynamics. Such a counterdiabatic driving\nprotocol has been recently proposed for the quantum Ising model [A. del Campo\net al., Phys. Rev. Lett. 109, 115703 (2012)]. We derive an exact closed-form\nexpression for all the coefficients of the counterdiabatic Ising Hamiltonian.\nWe also discuss two approximations to the exact counterdiabatic Ising\nHamiltonian quantifying their efficiency of the dynamical preparation of the\ndesired ground state. In particular, these studies show how quantum criticality\nenhances finite-size effects in the counterdiabatic dynamics. \n\n"}
{"id": "1410.5444", "contents": "Title: Simulating long-distance entanglement in quantum spin chains by\n  superconducting flux qubits Abstract: We investigate the performance of superconducting flux qubits for the\nadiabatic quantum simulation of long distance entanglement (LDE), namely a\nfinite ground-state entanglement between the end spins of a quantum spin chain\nwith open boundary conditions. As such, LDE can be considered an elementary\nprecursor of edge modes and topological order. We discuss two possible\nimplementations which simulate open chains with uniform bulk and weak end\nbonds, either with Ising or with XX nearest-neighbor interactions. In both\ncases we discuss a suitable protocol for the adiabatic preparation of the\nground state in the physical regimes featuring LDE. In the first case the\nadiabatic manipulation and the Ising interactions are realized using dc\ncurrents, while in the second case microwaves fields are used to control the\nsmoothness of the transformation and to realize the effective XX interactions.\nWe demonstrate the adiabatic preparation of the end-to-end entanglement in\nchains of four qubits with realistic parameters and on a relatively fast time\nscale. \n\n"}
{"id": "1410.5920", "contents": "Title: Active Regression by Stratification Abstract: We propose a new active learning algorithm for parametric linear regression\nwith random design. We provide finite sample convergence guarantees for general\ndistributions in the misspecified model. This is the first active learner for\nthis setting that provably can improve over passive learning. Unlike other\nlearning settings (such as classification), in regression the passive learning\nrate of $O(1/\\epsilon)$ cannot in general be improved upon. Nonetheless, the\nso-called `constant' in the rate of convergence, which is characterized by a\ndistribution-dependent risk, can be improved in many cases. For a given\ndistribution, achieving the optimal risk requires prior knowledge of the\ndistribution. Following the stratification technique advocated in Monte-Carlo\nfunction integration, our active learner approaches the optimal risk using\npiecewise constant approximations. \n\n"}
{"id": "1411.2337", "contents": "Title: Multi-Task Metric Learning on Network Data Abstract: Multi-task learning (MTL) improves prediction performance in different\ncontexts by learning models jointly on multiple different, but related tasks.\nNetwork data, which are a priori data with a rich relational structure, provide\nan important context for applying MTL. In particular, the explicit relational\nstructure implies that network data is not i.i.d. data. Network data also often\ncomes with significant metadata (i.e., attributes) associated with each entity\n(node). Moreover, due to the diversity and variation in network data (e.g.,\nmulti-relational links or multi-category entities), various tasks can be\nperformed and often a rich correlation exists between them. Learning algorithms\nshould exploit all of these additional sources of information for better\nperformance. In this work we take a metric-learning point of view for the MTL\nproblem in the network context. Our approach builds on structure preserving\nmetric learning (SPML). In particular SPML learns a Mahalanobis distance metric\nfor node attributes using network structure as supervision, so that the learned\ndistance function encodes the structure and can be used to predict link\npatterns from attributes. SPML is described for single-task learning on single\nnetwork. Herein, we propose a multi-task version of SPML, abbreviated as\nMT-SPML, which is able to learn across multiple related tasks on multiple\nnetworks via shared intermediate parametrization. MT-SPML learns a specific\nmetric for each task and a common metric for all tasks. The task correlation is\ncarried through the common metric and the individual metrics encode task\nspecific information. When combined together, they are structure-preserving\nwith respect to individual tasks. MT-SPML works on general networks, thus is\nsuitable for a wide variety of problems. In experiments, we challenge MT-SPML\non two real-word problems, where MT-SPML achieves significant improvement. \n\n"}
{"id": "1411.3436", "contents": "Title: SelfieBoost: A Boosting Algorithm for Deep Learning Abstract: We describe and analyze a new boosting algorithm for deep learning called\nSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct\nensembles of classifiers, SelfieBoost boosts the accuracy of a single network.\nWe prove a $\\log(1/\\epsilon)$ convergence rate for SelfieBoost under some \"SGD\nsuccess\" assumption which seems to hold in practice. \n\n"}
{"id": "1411.7783", "contents": "Title: From neural PCA to deep unsupervised learning Abstract: A network supporting deep unsupervised learning is presented. The network is\nan autoencoder with lateral shortcut connections from the encoder to decoder at\neach level of the hierarchy. The lateral shortcut connections allow the higher\nlevels of the hierarchy to focus on abstract invariant features. While standard\nautoencoders are analogous to latent variable models with a single layer of\nstochastic variables, the proposed network is analogous to hierarchical latent\nvariables models. Learning combines denoising autoencoder and denoising sources\nseparation frameworks. Each layer of the network contributes to the cost\nfunction a term which measures the distance of the representations produced by\nthe encoder and the decoder. Since training signals originate from all levels\nof the network, all layers can learn efficiently even in deep networks. The\nspeedup offered by cost terms from higher levels of the hierarchy and the\nability to learn invariant features are demonstrated in experiments. \n\n"}
{"id": "1412.2858", "contents": "Title: Thermalization time bounds for Pauli stabilizer Hamiltonians Abstract: We prove a general lower bound to the spectral gap of the Davies generator\nfor Hamiltonians that can be written as the sum of commuting Pauli operators.\nThese Hamiltonians, defined on the Hilbert space of $N$-qubits, serve as one of\nthe most frequently considered candidates for a self-correcting quantum memory.\nA spectral gap bound on the Davies generator establishes an upper limit on the\nlife time of such a quantum memory and can be used to estimate the time until\nthe system relaxes to thermal equilibrium when brought into contact with a\nthermal heat bath. The bound can be shown to behave as $\\lambda \\geq {\\cal\nO}(N^{-1}\\exp(-2\\beta \\, \\overline{\\epsilon}))$, where $\\overline{\\epsilon}$ is\na generalization of the well known energy barrier for logical operators.\nParticularly in the low temperature regime we expect this bound to provide the\ncorrect asymptotic scaling of the gap with the system size up to a factor of\n$N^{-1}$. Furthermore, we discuss conditions and provide scenarios where this\nfactor can be removed and a constant lower bound can be proven \n\n"}
{"id": "1412.4864", "contents": "Title: Learning with Pseudo-Ensembles Abstract: We formalize the notion of a pseudo-ensemble, a (possibly infinite)\ncollection of child models spawned from a parent model by perturbing it\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\nneural network trains a pseudo-ensemble of child subnetworks generated by\nrandomly masking nodes in the parent network. We present a novel regularizer\nbased on making the behavior of a pseudo-ensemble robust with respect to the\nnoise process generating it. In the fully-supervised setting, our regularizer\nmatches the performance of dropout. But, unlike dropout, our regularizer\nnaturally extends to the semi-supervised setting, where it produces\nstate-of-the-art results. We provide a case study in which we transform the\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\npseudo-ensemble, which significantly improves its performance on a real-world\nsentiment analysis benchmark. \n\n"}
{"id": "1412.6039", "contents": "Title: Generative Deep Deconvolutional Learning Abstract: A generative Bayesian model is developed for deep (multi-layer) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up and top-down probabilistic\nlearning. After learning the deep convolutional dictionary, testing is\nimplemented via deconvolutional inference. To speed up this inference, a new\nstatistical approach is proposed to project the top-layer dictionary elements\nto the data level. Following this, only one layer of deconvolution is required\nduring testing. Experimental results demonstrate powerful capabilities of the\nmodel to learn multi-layer features from images. Excellent classification\nresults are obtained on both the MNIST and Caltech 101 datasets. \n\n"}
{"id": "1412.6606", "contents": "Title: Competing with the Empirical Risk Minimizer in a Single Pass Abstract: In many estimation problems, e.g. linear and logistic regression, we wish to\nminimize an unknown objective given only unbiased samples of the objective\nfunction. Furthermore, we aim to achieve this using as few samples as possible.\nIn the absence of computational constraints, the minimizer of a sample average\nof observed data -- commonly referred to as either the empirical risk minimizer\n(ERM) or the $M$-estimator -- is widely regarded as the estimation strategy of\nchoice due to its desirable statistical convergence properties. Our goal in\nthis work is to perform as well as the ERM, on every problem, while minimizing\nthe use of computational resources such as running time and space usage.\n  We provide a simple streaming algorithm which, under standard regularity\nassumptions on the underlying problem, enjoys the following properties:\n  * The algorithm can be implemented in linear time with a single pass of the\nobserved data, using space linear in the size of a single sample.\n  * The algorithm achieves the same statistical rate of convergence as the\nempirical risk minimizer on every problem, even considering constant factors.\n  * The algorithm's performance depends on the initial error at a rate that\ndecreases super-polynomially.\n  * The algorithm is easily parallelizable.\n  Moreover, we quantify the (finite-sample) rate at which the algorithm becomes\ncompetitive with the ERM. \n\n"}
{"id": "1412.6622", "contents": "Title: Deep metric learning using Triplet network Abstract: Deep learning has proven itself as a successful set of models for learning\nuseful semantic representations of data. These, however, are mostly implicitly\nlearned as part of a classification task. In this paper we propose the triplet\nnetwork model, which aims to learn useful representations by distance\ncomparisons. A similar model was defined by Wang et al. (2014), tailor made for\nlearning a ranking for image information retrieval. Here we demonstrate using\nvarious datasets that our model learns a better representation than that of its\nimmediate competitor, the Siamese network. We also discuss future possible\nusage as a framework for unsupervised learning. \n\n"}
{"id": "1501.00375", "contents": "Title: Passing Expectation Propagation Messages with Kernel Methods Abstract: We propose to learn a kernel-based message operator which takes as input all\nexpectation propagation (EP) incoming messages to a factor node and produces an\noutgoing message. In ordinary EP, computing an outgoing message involves\nestimating a multivariate integral which may not have an analytic expression.\nLearning such an operator allows one to bypass the expensive computation of the\nintegral during inference by directly mapping all incoming messages into an\noutgoing message. The operator can be learned from training data (examples of\ninput and output messages) which allows automated inference to be made on any\nkind of factor that can be sampled. \n\n"}
{"id": "1501.05590", "contents": "Title: Sketch and Validate for Big Data Clustering Abstract: In response to the need for learning tools tuned to big data analytics, the\npresent paper introduces a framework for efficient clustering of huge sets of\n(possibly high-dimensional) data. Building on random sampling and consensus\n(RANSAC) ideas pursued earlier in a different (computer vision) context for\nrobust regression, a suite of novel dimensionality and set-reduction algorithms\nis developed. The advocated sketch-and-validate (SkeVa) family includes two\nalgorithms that rely on K-means clustering per iteration on reduced number of\ndimensions and/or feature vectors: The first operates in a batch fashion, while\nthe second sequential one offers computational efficiency and suitability with\nstreaming modes of operation. For clustering even nonlinearly separable\nvectors, the SkeVa family offers also a member based on user-selected kernel\nfunctions. Further trading off performance for reduced complexity, a fourth\nmember of the SkeVa family is based on a divergence criterion for selecting\nproper minimal subsets of feature variables and vectors, thus bypassing the\nneed for K-means clustering per iteration. Extensive numerical tests on\nsynthetic and real data sets highlight the potential of the proposed\nalgorithms, and demonstrate their competitive performance relative to\nstate-of-the-art random projection alternatives. \n\n"}
{"id": "1501.07140", "contents": "Title: Dynamic spectral aspects of interparticle correlation Abstract: Time-dependent quantities are calculated in the linear response limit for a\ncorrelated one dimensional model atom driven by an external quadrupolar\ntime-dependent field. Besides the analysis of the time-evolving energy change\nin the correlated two-particle system, and orthogonality of initial and final\nstates, Mehler's formula is applied in order to derive a point-wise\ndecomposition of the time-dependent one-matrix in terms of time-dependent\noccupation numbers and time-dependent orthonormal, natural orbitals. Based on\nsuch exact spectral decomposition on the time domain, R\\'enyi's entropy is also\ninvestigated. Considering the structure of the exact time-dependent one-matrix,\nan independent-particle model is defined from it which contains exact\ninformation on the single-particle probability density and probability current.\nThe resulting noninteracting auxiliary state is used to construct an effective\npotential and discuss its applicability. \n\n"}
{"id": "1502.00186", "contents": "Title: Advanced Mean Field Theory of Restricted Boltzmann Machine Abstract: Learning in restricted Boltzmann machine is typically hard due to the\ncomputation of gradients of log-likelihood function. To describe the network\nstate statistics of the restricted Boltzmann machine, we develop an advanced\nmean field theory based on the Bethe approximation. Our theory provides an\nefficient message passing based method that evaluates not only the partition\nfunction (free energy) but also its gradients without requiring statistical\nsampling. The results are compared with those obtained by the computationally\nexpensive sampling based method. \n\n"}
{"id": "1502.02458", "contents": "Title: Transfer of arbitrary two qubit states via a spin chain Abstract: We investigate the fidelity of the quantum state transfer (QST) of two qubits\nby means of an arbitrary spin-1/2 network, on a lattice of any dimensionality.\nUnder the assumptions that the network Hamiltonian preserves the magnetization\nand that a fully polarized initial state is taken for the lattice, we obtain a\ngeneral formula for the average fidelity of the two qubits QST, linking it to\nthe one- and two-particle transfer amplitudes of the spin-excitations among the\nsites of the lattice. We then apply this formalism to a 1D spin chain with\nXX-Heisenberg type nearest-neighbour interactions adopting a protocol that is a\ngeneralization of the single qubit one proposed in Ref. [Phys. Rev. A 87,\n062309 (2013)]. We find that a high-quality two qubit QST can be achieved\nprovided one can control the local fields at sites near the sender and\nreceiver. Under such conditions, we obtain an almost perfect transfer in a time\nthat scales either linearly or, depending on the spin number, quadratically\nwith the length of the chain. \n\n"}
{"id": "1502.03492", "contents": "Title: Gradient-based Hyperparameter Optimization through Reversible Learning Abstract: Tuning hyperparameters of learning algorithms is hard because gradients are\nusually unavailable. We compute exact gradients of cross-validation performance\nwith respect to all hyperparameters by chaining derivatives backwards through\nthe entire training procedure. These gradients allow us to optimize thousands\nof hyperparameters, including step-size and momentum schedules, weight\ninitialization distributions, richly parameterized regularization schemes, and\nneural network architectures. We compute hyperparameter gradients by exactly\nreversing the dynamics of stochastic gradient descent with momentum. \n\n"}
{"id": "1502.03630", "contents": "Title: Ordering-sensitive and Semantic-aware Topic Modeling Abstract: Topic modeling of textual corpora is an important and challenging problem. In\nmost previous work, the \"bag-of-words\" assumption is usually made which ignores\nthe ordering of words. This assumption simplifies the computation, but it\nunrealistically loses the ordering information and the semantic of words in the\ncontext. In this paper, we present a Gaussian Mixture Neural Topic Model\n(GMNTM) which incorporates both the ordering of words and the semantic meaning\nof sentences into topic modeling. Specifically, we represent each topic as a\ncluster of multi-dimensional vectors and embed the corpus into a collection of\nvectors generated by the Gaussian mixture model. Each word is affected not only\nby its topic, but also by the embedding vector of its surrounding words and the\ncontext. The Gaussian mixture components and the topic of documents, sentences\nand words can be learnt jointly. Extensive experiments show that our model can\nlearn better topics and more accurate word distributions for each topic.\nQuantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM\nobtains significantly better performance in terms of perplexity, retrieval\naccuracy and classification accuracy. \n\n"}
{"id": "1502.03919", "contents": "Title: Policy Gradient for Coherent Risk Measures Abstract: Several authors have recently developed risk-sensitive policy gradient\nmethods that augment the standard expected cost minimization problem with a\nmeasure of variability in cost. These studies have focused on specific\nrisk-measures, such as the variance or conditional value at risk (CVaR). In\nthis work, we extend the policy gradient method to the whole class of coherent\nrisk measures, which is widely accepted in finance and operations research,\namong other fields. We consider both static and time-consistent dynamic risk\nmeasures. For static risk measures, our approach is in the spirit of policy\ngradient algorithms and combines a standard sampling approach with convex\nprogramming. For dynamic risk measures, our approach is actor-critic style and\ninvolves explicit approximation of value function. Most importantly, our\ncontribution presents a unified approach to risk-sensitive reinforcement\nlearning that generalizes and extends previous results. \n\n"}
{"id": "1502.04135", "contents": "Title: Undecidability of the Spectral Gap (short version) Abstract: The spectral gap - the energy difference between the ground state and first\nexcited state - is central to quantum many-body physics. Many challenging open\nproblems, such as the Haldane conjecture, existence of gapped topological spin\nliquid phases, or the Yang-Mills gap conjecture, concern spectral gaps. These\nand other problems are particular cases of the general spectral gap problem:\ngiven a quantum many-body Hamiltonian, is it gapped or gapless? Here we prove\nthat this is an undecidable problem. We construct families of quantum spin\nsystems on a 2D lattice with translationally-invariant, nearest-neighbour\ninteractions for which the spectral gap problem is undecidable. This result\nextends to undecidability of other low energy properties, such as existence of\nalgebraically decaying ground-state correlations. The proof combines\nHamiltonian complexity techniques with aperiodic tilings, to construct a\nHamiltonian whose ground state encodes the evolution of a quantum\nphase-estimation algorithm followed by a universal Turing Machine. The spectral\ngap depends on the outcome of the corresponding Halting Problem. Our result\nimplies that there exists no algorithm to determine whether an arbitrary model\nis gapped or gapless. It also implies that there exist models for which the\npresence or absence of a spectral gap is independent of the axioms of\nmathematics. \n\n"}
{"id": "1502.04434", "contents": "Title: Invariant backpropagation: how to train a transformation-invariant\n  neural network Abstract: In many classification problems a classifier should be robust to small\nvariations in the input vector. This is a desired property not only for\nparticular transformations, such as translation and rotation in image\nclassification problems, but also for all others for which the change is small\nenough to retain the object perceptually indistinguishable. We propose two\nextensions of the backpropagation algorithm that train a neural network to be\nrobust to variations in the feature vector. While the first of them enforces\nrobustness of the loss function to all variations, the second method trains the\npredictions to be robust to a particular variation which changes the loss\nfunction the most. The second methods demonstrates better results, but is\nslightly slower. We analytically compare the proposed algorithm with two the\nmost similar approaches (Tangent BP and Adversarial Training), and propose\ntheir fast versions. In the experimental part we perform comparison of all\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\nproposed algorithm depends on the dataset size and data augmentation. \n\n"}
{"id": "1502.06055", "contents": "Title: Synchronization of Interacting Quantum Dipoles Abstract: Macroscopic ensembles of radiating dipoles are ubiquitous in the physical and\nnatural sciences. In the classical limit the dipoles can be described as\ndamped-driven oscillators, which are able to spontaneously synchronize and\ncollectively lock their phases. Here we investigate the correspond- ing\nphenomenon in the quantum regime with arrays of quantized two-level systems\ncoupled via long-range and anisotropic dipolar interactions. Our calculations\ndemonstrate that the dipoles may overcome the decoherence induced by quantum\nfluctuations and inhomogeneous couplings and evolve to a synchronized\nsteady-state. This steady-state bears much similarity to that observed in\nclassical systems, and yet also exhibits genuine quantum properties such as\nquantum correlations and quan- tum phase diffusion (reminiscent of lasing). Our\npredictions could be relevant for the development of better atomic clocks and a\nvariety of noise tolerant quantum devices. \n\n"}
{"id": "1503.00189", "contents": "Title: Microwave Quantum Illumination Abstract: Quantum illumination is a quantum-optical sensing technique in which an\nentangled source is exploited to improve the detection of a low-reflectivity\nobject that is immersed in a bright thermal background. Here we describe and\nanalyze a system for applying this technique at microwave frequencies, a more\nappropriate spectral region for target detection than the optical, due to the\nnaturally-occurring bright thermal background in the microwave regime. We use\nan electro-optomechanical converter to entangle microwave signal and optical\nidler fields, with the former being sent to probe the target region and the\nlatter being retained at the source. The microwave radiation collected from the\ntarget region is then phase conjugated and upconverted into an optical field\nthat is combined with the retained idler in a joint-detection quantum\nmeasurement. The error probability of this microwave quantum-illumination\nsystem, or quantum radar, is shown to be superior to that of any classical\nmicrowave radar of equal transmitted energy. \n\n"}
{"id": "1503.01445", "contents": "Title: Toxicity Prediction using Deep Learning Abstract: Everyday we are exposed to various chemicals via food additives, cleaning and\ncosmetic products and medicines -- and some of them might be toxic. However\ntesting the toxicity of all existing compounds by biological experiments is\nneither financially nor logistically feasible. Therefore the government\nagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the\n\"Toxicology in the 21st Century\" (Tox21) initiative. The goal of this challenge\nwas to assess the performance of computational methods in predicting the\ntoxicity of chemical compounds. State of the art toxicity prediction methods\nbuild upon specifically-designed chemical descriptors developed over decades.\nThough Deep Learning is new to the field and was never applied to toxicity\nprediction before, it clearly outperformed all other participating methods. In\nthis application paper we show that deep nets automatically learn features\nresembling well-established toxicophores. In total, our Deep Learning approach\nwon both of the panel-challenges (nuclear receptors and stress response) as\nwell as the overall Grand Challenge, and thereby sets a new standard in tox\nprediction. \n\n"}
{"id": "1503.02551", "contents": "Title: Kernel-Based Just-In-Time Learning for Passing Expectation Propagation\n  Messages Abstract: We propose an efficient nonparametric strategy for learning a message\noperator in expectation propagation (EP), which takes as input the set of\nincoming messages to a factor node, and produces an outgoing message as output.\nThis learned operator replaces the multivariate integral required in classical\nEP, which may not have an analytic expression. We use kernel-based regression,\nwhich is trained on a set of probability distributions representing the\nincoming messages, and the associated outgoing messages. The kernel approach\nhas two main advantages: first, it is fast, as it is implemented using a novel\ntwo-layer random feature representation of the input message distributions;\nsecond, it has principled uncertainty estimates, and can be cheaply updated\nonline, meaning it can request and incorporate new training data when it\nencounters inputs on which it is uncertain. In experiments, our approach is\nable to solve learning problems where a single message operator is required for\nmultiple, substantially different data sets (logistic regression for a variety\nof classification problems), where it is essential to accurately assess\nuncertainty and to efficiently and robustly update the message operator. \n\n"}
{"id": "1503.04613", "contents": "Title: Isolelectronic apparatus to probe the thermal Casimir force Abstract: Isoelectronic differential force measurements provide a unique opportunity to\nprobe controversial features of the thermal Casimir effect, that are still much\ndebated in the current literature. Isolectronic setups offer two major\nadvantages over conventional Casimir setups. On one hand they are immune from\nelectrostatic forces caused by potential patches on the plates surfaces, that\nplague present Casimir experiments especially for separations in the micron\nrange. On the other hand they can strongly enhance the discrepancy between\nalternative theoretical models that have been proposed to estimate the thermal\nCasimir force for metallic and magnetic surfaces. Thanks to these two features,\nisoelectronic differential experiments should allow to establish conclusively\nwhich among these models correctly describes the thermal Casimir force. \n\n"}
{"id": "1503.08767", "contents": "Title: Decoherence in adiabatic quantum computation Abstract: Recent experiments with increasingly larger numbers of qubits have sparked\nrenewed interest in adiabatic quantum computation, and in particular quantum\nannealing. A central question that is repeatedly asked is whether quantum\nfeatures of the evolution can survive over the long time-scales used for\nquantum annealing relative to standard measures of the decoherence time. We\nreconsider the role of decoherence in adiabatic quantum computation and quantum\nannealing using the adiabatic quantum master equation formalism. We restrict\nourselves to the weak-coupling and singular-coupling limits, which correspond\nto decoherence in the energy eigenbasis and in the computational basis,\nrespectively. We demonstrate that decoherence in the instantaneous energy\neigenbasis does not necessarily detrimentally affect adiabatic quantum\ncomputation, and in particular that a short single-qubit $T_2$ time need not\nimply adverse consequences for the success of the quantum adiabatic algorithm.\nWe further demonstrate that boundary cancellation methods, designed to improve\nthe fidelity of adiabatic quantum computing in the closed system setting,\nremain beneficial in the open system setting. To address the high computational\ncost of master equation simulations, we also demonstrate that a quantum Monte\nCarlo algorithm that explicitly accounts for a thermal bosonic bath can be used\nto interpolate between classical and quantum annealing. Our study highlights\nand clarifies the significantly different role played by decoherence in the\nadiabatic and circuit models of quantum computing. \n\n"}
{"id": "1504.01344", "contents": "Title: Early Stopping is Nonparametric Variational Inference Abstract: We show that unconverged stochastic gradient descent can be interpreted as a\nprocedure that samples from a nonparametric variational approximate posterior\ndistribution. This distribution is implicitly defined as the transformation of\nan initial distribution by a sequence of optimization updates. By tracking the\nchange in entropy over this sequence of transformations during optimization, we\nform a scalable, unbiased estimate of the variational lower bound on the log\nmarginal likelihood. We can use this bound to optimize hyperparameters instead\nof using cross-validation. This Bayesian interpretation of SGD suggests\nimproved, overfitting-resistant optimization procedures, and gives a\ntheoretical foundation for popular tricks such as early stopping and\nensembling. We investigate the properties of this marginal likelihood estimator\non neural network models. \n\n"}
{"id": "1504.01446", "contents": "Title: Totally Corrective Boosting with Cardinality Penalization Abstract: We propose a totally corrective boosting algorithm with explicit cardinality\nregularization. The resulting combinatorial optimization problems are not known\nto be efficiently solvable with existing classical methods, but emerging\nquantum optimization technology gives hope for achieving sparser models in\npractice. In order to demonstrate the utility of our algorithm, we use a\ndistributed classical heuristic optimizer as a stand-in for quantum hardware.\nEven though this evaluation methodology incurs large time and resource costs on\nclassical computing machinery, it allows us to gauge the potential gains in\ngeneralization performance and sparsity of the resulting boosted ensembles. Our\nexperimental results on public data sets commonly used for benchmarking of\nboosting algorithms decidedly demonstrate the existence of such advantages. If\nactual quantum optimization were to be used with this algorithm in the future,\nwe would expect equivalent or superior results at much smaller time and energy\ncosts during training. Moreover, studying cardinality-penalized boosting also\nsheds light on why unregularized boosting algorithms with early stopping often\nyield better results than their counterparts with explicit convex\nregularization: Early stopping performs suboptimal cardinality regularization.\nThe results that we present here indicate it is beneficial to explicitly solve\nthe combinatorial problem still left open at early termination. \n\n"}
{"id": "1504.05035", "contents": "Title: F-SVM: Combination of Feature Transformation and SVM Learning via Convex\n  Relaxation Abstract: The generalization error bound of support vector machine (SVM) depends on the\nratio of radius and margin, while standard SVM only considers the maximization\nof the margin but ignores the minimization of the radius. Several approaches\nhave been proposed to integrate radius and margin for joint learning of feature\ntransformation and SVM classifier. However, most of them either require the\nform of the transformation matrix to be diagonal, or are non-convex and\ncomputationally expensive. In this paper, we suggest a novel approximation for\nthe radius of minimum enclosing ball (MEB) in feature space, and then propose a\nconvex radius-margin based SVM model for joint learning of feature\ntransformation and SVM classifier, i.e., F-SVM. An alternating minimization\nmethod is adopted to solve the F-SVM model, where the feature transformation is\nupdatedvia gradient descent and the classifier is updated by employing the\nexisting SVM solver. By incorporating with kernel principal component analysis,\nF-SVM is further extended for joint learning of nonlinear transformation and\nclassifier. Experimental results on the UCI machine learning datasets and the\nLFW face datasets show that F-SVM outperforms the standard SVM and the existing\nradius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\\mu}. \n\n"}
{"id": "1504.05823", "contents": "Title: Normal Bandits of Unknown Means and Variances: Asymptotic Optimality,\n  Finite Horizon Regret Bounds, and a Solution to an Open Problem Abstract: Consider the problem of sampling sequentially from a finite number of $N \\geq\n2$ populations, specified by random variables $X^i_k$, $ i = 1,\\ldots , N,$ and\n$k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from population $i$ the\n$k^{th}$ time it is sampled. It is assumed that for each fixed $i$,\n  $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. normal random variables,\nwith unknown mean $\\mu_i$ and unknown variance $\\sigma_i^2$.\n  The objective is to have a policy $\\pi$ for deciding from which of the $N$\npopulations to sample form at any time $n=1,2,\\ldots$ so as to maximize the\nexpected sum of outcomes of $n$ samples or equivalently to minimize the regret\ndue to lack on information of the parameters $\\mu_i$ and $\\sigma_i^2$. In this\npaper, we present a simple inflated sample mean (ISM) index policy that is\nasymptotically optimal in the sense of Theorem 4 below. This resolves a\nstanding open problem from Burnetas and Katehakis (1996). Additionally, finite\nhorizon regret bounds are given. \n\n"}
{"id": "1504.06544", "contents": "Title: Sampling Correctors Abstract: In many situations, sample data is obtained from a noisy or imperfect source.\nIn order to address such corruptions, this paper introduces the concept of a\nsampling corrector. Such algorithms use structure that the distribution is\npurported to have, in order to allow one to make \"on-the-fly\" corrections to\nsamples drawn from probability distributions. These algorithms then act as\nfilters between the noisy data and the end user.\n  We show connections between sampling correctors, distribution learning\nalgorithms, and distribution property testing algorithms. We show that these\nconnections can be utilized to expand the applicability of known distribution\nlearning and property testing algorithms as well as to achieve improved\nalgorithms for those tasks.\n  As a first step, we show how to design sampling correctors using proper\nlearning algorithms. We then focus on the question of whether algorithms for\nsampling correctors can be more efficient in terms of sample complexity than\nlearning algorithms for the analogous families of distributions. When\ncorrecting monotonicity, we show that this is indeed the case when also granted\nquery access to the cumulative distribution function. We also obtain sampling\ncorrectors for monotonicity without this stronger type of access, provided that\nthe distribution be originally very close to monotone (namely, at a distance\n$O(1/\\log^2 n)$). In addition to that, we consider a restricted error model\nthat aims at capturing \"missing data\" corruptions. In this model, we show that\ndistributions that are close to monotone have sampling correctors that are\nsignificantly more efficient than achievable by the learning approach.\n  We also consider the question of whether an additional source of independent\nrandom bits is required by sampling correctors to implement the correction\nprocess. \n\n"}
{"id": "1505.00290", "contents": "Title: Algorithms for Lipschitz Learning on Graphs Abstract: We develop fast algorithms for solving regression problems on graphs where\none is given the value of a function at some vertices, and must find its\nsmoothest possible extension to all vertices. The extension we compute is the\nabsolutely minimal Lipschitz extension, and is the limit for large $p$ of\n$p$-Laplacian regularization. We present an algorithm that computes a minimal\nLipschitz extension in expected linear time, and an algorithm that computes an\nabsolutely minimal Lipschitz extension in expected time $\\widetilde{O} (m n)$.\nThe latter algorithm has variants that seem to run much faster in practice.\nThese extensions are particularly amenable to regularization: we can perform\n$l_{0}$-regularization on the given values in polynomial time and\n$l_{1}$-regularization on the initial function values and on graph edge weights\nin time $\\widetilde{O} (m^{3/2})$. \n\n"}
{"id": "1505.00308", "contents": "Title: Multi-Object Classification and Unsupervised Scene Understanding Using\n  Deep Learning Features and Latent Tree Probabilistic Models Abstract: Deep learning has shown state-of-art classification performance on datasets\nsuch as ImageNet, which contain a single object in each image. However,\nmulti-object classification is far more challenging. We present a unified\nframework which leverages the strengths of multiple machine learning methods,\nviz deep learning, probabilistic models and kernel methods to obtain\nstate-of-art performance on Microsoft COCO, consisting of non-iconic images. We\nincorporate contextual information in natural images through a conditional\nlatent tree probabilistic model (CLTM), where the object co-occurrences are\nconditioned on the extracted fc7 features from pre-trained Imagenet CNN as\ninput. We learn the CLTM tree structure using conditional pairwise\nprobabilities for object co-occurrences, estimated through kernel methods, and\nwe learn its node and edge potentials by training a new 3-layer neural network,\nwhich takes fc7 features as input. Object classification is carried out via\ninference on the learnt conditional tree model, and we obtain significant gain\nin precision-recall and F-measures on MS-COCO, especially for difficult object\ncategories. Moreover, the latent variables in the CLTM capture scene\ninformation: the images with top activations for a latent node have common\nthemes such as being a grasslands or a food scene, and on on. In addition, we\nshow that a simple k-means clustering of the inferred latent nodes alone\nsignificantly improves scene classification performance on the MIT-Indoor\ndataset, without the need for any retraining, and without using scene labels\nduring training. Thus, we present a unified framework for multi-object\nclassification and unsupervised scene understanding. \n\n"}
{"id": "1505.00553", "contents": "Title: On Regret-Optimal Learning in Decentralized Multi-player Multi-armed\n  Bandits Abstract: We consider the problem of learning in single-player and multiplayer\nmultiarmed bandit models. Bandit problems are classes of online learning\nproblems that capture exploration versus exploitation tradeoffs. In a\nmultiarmed bandit model, players can pick among many arms, and each play of an\narm generates an i.i.d. reward from an unknown distribution. The objective is\nto design a policy that maximizes the expected reward over a time horizon for a\nsingle player setting and the sum of expected rewards for the multiplayer\nsetting. In the multiplayer setting, arms may give different rewards to\ndifferent players. There is no separate channel for coordination among the\nplayers. Any attempt at communication is costly and adds to regret. We propose\ntwo decentralizable policies, $\\tt E^3$ ($\\tt E$-$\\tt cubed$) and $\\tt\nE^3$-$\\tt TS$, that can be used in both single player and multiplayer settings.\nThese policies are shown to yield expected regret that grows at most as\nO($\\log^{1+\\epsilon} T$). It is well known that $\\log T$ is the lower bound on\nthe rate of growth of regret even in a centralized case. The proposed\nalgorithms improve on prior work where regret grew at O($\\log^2 T$). More\nfundamentally, these policies address the question of additional cost incurred\nin decentralized online learning, suggesting that there is at most an\n$\\epsilon$-factor cost in terms of order of regret. This solves a problem of\nrelevance in many domains and had been open for a while. \n\n"}
{"id": "1505.01658", "contents": "Title: A Survey of Predictive Modelling under Imbalanced Distributions Abstract: Many real world data mining applications involve obtaining predictive models\nusing data sets with strongly imbalanced distributions of the target variable.\nFrequently, the least common values of this target variable are associated with\nevents that are highly relevant for end users (e.g. fraud detection, unusual\nreturns on stock markets, anticipation of catastrophes, etc.). Moreover, the\nevents may have different costs and benefits, which when associated with the\nrarity of some of them on the available training data creates serious problems\nto predictive modelling techniques. This paper presents a survey of existing\ntechniques for handling these important applications of predictive analytics.\nAlthough most of the existing work addresses classification tasks (nominal\ntarget variables), we also describe methods designed to handle similar problems\nwithin regression tasks (numeric target variables). In this survey we discuss\nthe main challenges raised by imbalanced distributions, describe the main\napproaches to these problems, propose a taxonomy of these methods and refer to\nsome related problems within predictive modelling. \n\n"}
{"id": "1505.01918", "contents": "Title: An Asymptotically Optimal Policy for Uniform Bandits of Unknown Support Abstract: Consider the problem of a controller sampling sequentially from a finite\nnumber of $N \\geq 2$ populations, specified by random variables $X^i_k$, $ i =\n1,\\ldots , N,$ and $k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from\npopulation $i$ the $k^{th}$ time it is sampled. It is assumed that for each\nfixed $i$, $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. uniform random\nvariables over some interval $[a_i, b_i]$, with the support (i.e., $a_i, b_i$)\nunknown to the controller. The objective is to have a policy $\\pi$ for\ndeciding, based on available data, from which of the $N$ populations to sample\nfrom at any time $n=1,2,\\ldots$ so as to maximize the expected sum of outcomes\nof $n$ samples or equivalently to minimize the regret due to lack on\ninformation of the parameters $\\{ a_i \\}$ and $\\{ b_i \\}$. In this paper, we\npresent a simple inflated sample mean (ISM) type policy that is asymptotically\noptimal in the sense of its regret achieving the asymptotic lower bound of\nBurnetas and Katehakis (1996). Additionally, finite horizon regret bounds are\ngiven. \n\n"}
{"id": "1505.03410", "contents": "Title: Mind the duality gap: safer rules for the Lasso Abstract: Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules. \n\n"}
{"id": "1505.04650", "contents": "Title: Compressed Nonnegative Matrix Factorization is Fast and Accurate Abstract: Nonnegative matrix factorization (NMF) has an established reputation as a\nuseful data analysis technique in numerous applications. However, its usage in\npractical situations is undergoing challenges in recent years. The fundamental\nfactor to this is the increasingly growing size of the datasets available and\nneeded in the information sciences. To address this, in this work we propose to\nuse structured random compression, that is, random projections that exploit the\ndata structure, for two NMF variants: classical and separable. In separable NMF\n(SNMF) the left factors are a subset of the columns of the input matrix. We\npresent suitable formulations for each problem, dealing with different\nrepresentative algorithms within each one. We show that the resulting\ncompressed techniques are faster than their uncompressed variants, vastly\nreduce memory demands, and do not encompass any significant deterioration in\nperformance. The proposed structured random projections for SNMF allow to deal\nwith arbitrarily shaped large matrices, beyond the standard limit of\ntall-and-skinny matrices, granting access to very efficient computations in\nthis general setting. We accompany the algorithmic presentation with\ntheoretical foundations and numerous and diverse examples, showing the\nsuitability of the proposed approaches. \n\n"}
{"id": "1505.06798", "contents": "Title: Accelerating Very Deep Convolutional Networks for Classification and\n  Detection Abstract: This paper aims to accelerate the test-time computation of convolutional\nneural networks (CNNs), especially very deep CNNs that have substantially\nimpacted the computer vision community. Unlike previous methods that are\ndesigned for approximating linear filters or linear responses, our method takes\nthe nonlinear units into account. We develop an effective solution to the\nresulting nonlinear optimization problem without the need of stochastic\ngradient descent (SGD). More importantly, while previous methods mainly focus\non optimizing one or two layers, our nonlinear method enables an asymmetric\nreconstruction that reduces the rapidly accumulated error when multiple (e.g.,\n>=10) layers are approximated. For the widely used very deep VGG-16 model, our\nmethod achieves a whole-model speedup of 4x with merely a 0.3% increase of\ntop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also\nshows a graceful accuracy degradation for object detection when plugged into\nthe Fast R-CNN detector. \n\n"}
{"id": "1505.06813", "contents": "Title: Surrogate Functions for Maximizing Precision at the Top Abstract: The problem of maximizing precision at the top of a ranked list, often dubbed\nPrecision@k (prec@k), finds relevance in myriad learning applications such as\nranking, multi-label classification, and learning with severe label imbalance.\nHowever, despite its popularity, there exist significant gaps in our\nunderstanding of this problem and its associated performance measure.\n  The most notable of these is the lack of a convex upper bounding surrogate\nfor prec@k. We also lack scalable perceptron and stochastic gradient descent\nalgorithms for optimizing this performance measure. In this paper we make key\ncontributions in these directions. At the heart of our results is a family of\ntruly upper bounding surrogates for prec@k. These surrogates are motivated in a\nprincipled manner and enjoy attractive properties such as consistency to prec@k\nunder various natural margin/noise conditions.\n  These surrogates are then used to design a class of novel perceptron\nalgorithms for optimizing prec@k with provable mistake bounds. We also devise\nscalable stochastic gradient descent style methods for this problem with\nprovable convergence bounds. Our proofs rely on novel uniform convergence\nbounds which require an in-depth analysis of the structural properties of\nprec@k and its surrogates. We conclude with experimental results comparing our\nalgorithms with state-of-the-art cutting plane and stochastic gradient\nalgorithms for maximizing prec@k. \n\n"}
{"id": "1505.07067", "contents": "Title: Belief Flows of Robust Online Learning Abstract: This paper introduces a new probabilistic model for online learning which\ndynamically incorporates information from stochastic gradients of an arbitrary\nloss function. Similar to probabilistic filtering, the model maintains a\nGaussian belief over the optimal weight parameters. Unlike traditional Bayesian\nupdates, the model incorporates a small number of gradient evaluations at\nlocations chosen using Thompson sampling, making it computationally tractable.\nThe belief is then transformed via a linear flow field which optimally updates\nthe belief distribution using rules derived from information theoretic\nprinciples. Several versions of the algorithm are shown using different\nconstraints on the flow field and compared with conventional online learning\nalgorithms. Results are given for several classification tasks including\nlogistic regression and multilayer neural networks. \n\n"}
{"id": "1505.07457", "contents": "Title: Non-Markovian Reactivation of Quantum Relays Abstract: We consider a quantum relay which is used by two parties to perform several\ncontinuous-variable protocols: Entanglement swapping, distillation, quantum\nteleportation, and quantum key distribution. The theory of these protocols is\nextended to a non-Markovian model of decoherence characterized by correlated\nGaussian noise. Even if bipartite entanglement is completely lost at the relay,\nwe show that the various protocols can progressively be reactivated by the\nseparable noise-correlations of the environment. In fact, above a critical\namount, these correlations are able to restore the distribution of\nquadripartite entanglement, which can be localized into an exploitable\nbipartite form by the action of the relay. Our findings are confirmed by a\nproof-of-principle experiment and show the potential advantages of\nnon-Markovian effects in a quantum network architecture. \n\n"}
{"id": "1506.00935", "contents": "Title: Discovering Valuable Items from Massive Data Abstract: Suppose there is a large collection of items, each with an associated cost\nand an inherent utility that is revealed only once we commit to selecting it.\nGiven a budget on the cumulative cost of the selected items, how can we pick a\nsubset of maximal value? This task generalizes several important problems such\nas multi-arm bandits, active search and the knapsack problem. We present an\nalgorithm, GP-Select, which utilizes prior knowledge about similarity be- tween\nitems, expressed as a kernel function. GP-Select uses Gaussian process\nprediction to balance exploration (estimating the unknown value of items) and\nexploitation (selecting items of high value). We extend GP-Select to be able to\ndiscover sets that simultaneously have high utility and are diverse. Our\npreference for diversity can be specified as an arbitrary monotone submodular\nfunction that quantifies the diminishing returns obtained when selecting\nsimilar items. Furthermore, we exploit the structure of the model updates to\nachieve an order of magnitude (up to 40X) speedup in our experiments without\nresorting to approximations. We provide strong guarantees on the performance of\nGP-Select and apply it to three real-world case studies of industrial\nrelevance: (1) Refreshing a repository of prices in a Global Distribution\nSystem for the travel industry, (2) Identifying diverse, binding-affine\npeptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale\nrecommender system by recommending items to users. \n\n"}
{"id": "1506.02158", "contents": "Title: Bayesian Convolutional Neural Networks with Bernoulli Approximate\n  Variational Inference Abstract: Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10. \n\n"}
{"id": "1506.02550", "contents": "Title: Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem Abstract: We study the $K$-armed dueling bandit problem, a variation of the standard\nstochastic bandit problem where the feedback is limited to relative comparisons\nof a pair of arms. We introduce a tight asymptotic regret lower bound that is\nbased on the information divergence. An algorithm that is inspired by the\nDeterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)\nis proposed, and its regret is analyzed. The proposed algorithm is found to be\nthe first one with a regret upper bound that matches the lower bound.\nExperimental comparisons of dueling bandit algorithms show that the proposed\nalgorithm significantly outperforms existing ones. \n\n"}
{"id": "1506.03016", "contents": "Title: Accelerated Stochastic Gradient Descent for Minimizing Finite Sums Abstract: We propose an optimization method for minimizing the finite sums of smooth\nconvex functions. Our method incorporates an accelerated gradient descent (AGD)\nand a stochastic variance reduction gradient (SVRG) in a mini-batch setting.\nUnlike SVRG, our method can be directly applied to non-strongly and strongly\nconvex problems. We show that our method achieves a lower overall complexity\nthan the recently proposed methods that supports non-strongly convex problems.\nMoreover, this method has a fast rate of convergence for strongly convex\nproblems. Our experiments show the effectiveness of our method. \n\n"}
{"id": "1506.08700", "contents": "Title: Dropout as data augmentation Abstract: Dropout is typically interpreted as bagging a large number of models sharing\nparameters. We show that using dropout in a network can also be interpreted as\na kind of data augmentation in the input space without domain knowledge. We\npresent an approach to projecting the dropout noise within a network back into\nthe input space, thereby generating augmented versions of the training data,\nand we show that training a deterministic network on the augmented samples\nyields similar results. Finally, we propose a new dropout noise scheme based on\nour observations and show that it improves dropout results without adding\nsignificant computational cost. \n\n"}
{"id": "1507.00300", "contents": "Title: Bootstrapped Thompson Sampling and Deep Exploration Abstract: This technical note presents a new approach to carrying out the kind of\nexploration achieved by Thompson sampling, but without explicitly maintaining\nor sampling from posterior distributions. The approach is based on a bootstrap\ntechnique that uses a combination of observed and artificially generated data.\nThe latter serves to induce a prior distribution which, as we will demonstrate,\nis critical to effective exploration. We explain how the approach can be\napplied to multi-armed bandit and reinforcement learning problems and how it\nrelates to Thompson sampling. The approach is particularly well-suited for\ncontexts in which exploration is coupled with deep learning, since in these\nsettings, maintaining or generating samples from a posterior distribution\nbecomes computationally infeasible. \n\n"}
{"id": "1507.01073", "contents": "Title: Convex Factorization Machine for Regression Abstract: We propose the convex factorization machine (CFM), which is a convex variant\nof the widely used Factorization Machines (FMs). Specifically, we employ a\nlinear+quadratic model and regularize the linear term with the\n$\\ell_2$-regularizer and the quadratic term with the trace norm regularizer.\nThen, we formulate the CFM optimization as a semidefinite programming problem\nand propose an efficient optimization procedure with Hazan's algorithm. A key\nadvantage of CFM over existing FMs is that it can find a globally optimal\nsolution, while FMs may get a poor locally optimal solution since the objective\nfunction of FMs is non-convex. In addition, the proposed algorithm is simple\nyet effective and can be implemented easily. Finally, CFM is a general\nfactorization method and can also be used for other factorization problems\nincluding including multi-view matrix factorization and tensor completion\nproblems. Through synthetic and movielens datasets, we first show that the\nproposed CFM achieves results competitive to FMs. Furthermore, in a\ntoxicogenomics prediction task, we show that CFM outperforms a state-of-the-art\ntensor factorization method. \n\n"}
{"id": "1507.01549", "contents": "Title: Special features of the thermal Casimir effect across a uniaxial\n  anisotropic film Abstract: We investigate the thermal Casimir force between two parallel plates made of\ndifferent isotropic materials which are separated by a uniaxial anisotropic\nfilm. Numerical computations of the Casimir pressure at T=300K are performed\nusing the complete Lifshitz formula adapted for an anisotropic intervening\nlayer and in the nonrelativistic limit. It is shown that the standard\n(nonrelativistic) theory of the van der Waals force is not applicable in this\ncase, because the effects of retardation contribute significantly even for film\nthicknesses of a few nanometers. We have also obtained simple analytic\nexpressions for the classical Casimir free energy and pressure for large film\nthicknesses (high temperatures). Unlike the case of isotropic intervening\nfilms, for two metallic plates the classical Casimir free energy and pressure\nare shown to depend on the static dielectric permittivities of an anisotropic\nfilm. One further interesting feature is that the classical limit is achieved\nat much shorter separations between the plates than for a vacuum gap. Possible\napplications of the obtained results are discussed. \n\n"}
{"id": "1507.01569", "contents": "Title: Emphatic Temporal-Difference Learning Abstract: Emphatic algorithms are temporal-difference learning algorithms that change\ntheir effective state distribution by selectively emphasizing and\nde-emphasizing their updates on different time steps. Recent works by Sutton,\nMahmood and White (2015), and Yu (2015) show that by varying the emphasis in a\nparticular way, these algorithms become stable and convergent under off-policy\ntraining with linear function approximation. This paper serves as a unified\nsummary of the available results from both works. In addition, we demonstrate\nthe empirical benefits from the flexibility of emphatic algorithms, including\nstate-dependent discounting, state-dependent bootstrapping, and the\nuser-specified allocation of function approximation resources. \n\n"}
{"id": "1507.02528", "contents": "Title: Faster Convex Optimization: Simulated Annealing with an Efficient\n  Universal Barrier Abstract: This paper explores a surprising equivalence between two seemingly-distinct\nconvex optimization methods. We show that simulated annealing, a well-studied\nrandom walk algorithms, is directly equivalent, in a certain sense, to the\ncentral path interior point algorithm for the the entropic universal barrier\nfunction. This connection exhibits several benefits. First, we are able improve\nthe state of the art time complexity for convex optimization under the\nmembership oracle model. We improve the analysis of the randomized algorithm of\nKalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that\nunderly the central path following interior point algorithm. We are able to\ntighten the temperature schedule for simulated annealing which gives an\nimproved running time, reducing by square root of the dimension in certain\ninstances. Second, we get an efficient randomized interior point method with an\nefficiently computable universal barrier for any convex set described by a\nmembership oracle. Previously, efficiently computable barriers were known only\nfor particular convex sets. \n\n"}
{"id": "1507.04777", "contents": "Title: Sparse Probit Linear Mixed Model Abstract: Linear Mixed Models (LMMs) are important tools in statistical genetics. When\nused for feature selection, they allow to find a sparse set of genetic traits\nthat best predict a continuous phenotype of interest, while simultaneously\ncorrecting for various confounding factors such as age, ethnicity and\npopulation structure. Formulated as models for linear regression, LMMs have\nbeen restricted to continuous phenotypes. We introduce the Sparse Probit Linear\nMixed Model (Probit-LMM), where we generalize the LMM modeling paradigm to\nbinary phenotypes. As a technical challenge, the model no longer possesses a\nclosed-form likelihood function. In this paper, we present a scalable\napproximate inference algorithm that lets us fit the model to high-dimensional\ndata sets. We show on three real-world examples from different domains that in\nthe setup of binary labels, our algorithm leads to better prediction accuracies\nand also selects features which show less correlation with the confounding\nfactors. \n\n"}
{"id": "1507.05444", "contents": "Title: Canonical Correlation Forests Abstract: We introduce canonical correlation forests (CCFs), a new decision tree\nensemble method for classification and regression. Individual canonical\ncorrelation trees are binary decision trees with hyperplane splits based on\nlocal canonical correlation coefficients calculated during training. Unlike\naxis-aligned alternatives, the decision surfaces of CCFs are not restricted to\nthe coordinate system of the inputs features and therefore more naturally\nrepresent data with correlated inputs. CCFs naturally accommodate multiple\noutputs, provide a similar computational complexity to random forests, and\ninherit their impressive robustness to the choice of input parameters. As part\nof the CCF training algorithm, we also introduce projection bootstrapping, a\nnovel alternative to bagging for oblique decision tree ensembles which\nmaintains use of the full dataset in selecting split points, often leading to\nimprovements in predictive accuracy. Our experiments show that, even without\nparameter tuning, CCFs out-perform axis-aligned random forests and other\nstate-of-the-art tree ensemble methods on both classification and regression\nproblems, delivering both improved predictive accuracy and faster training\ntimes. We further show that they outperform all of the 179 classifiers\nconsidered in a recent extensive survey. \n\n"}
{"id": "1507.06550", "contents": "Title: Human Pose Estimation with Iterative Error Feedback Abstract: Hierarchical feature extractors such as Convolutional Networks (ConvNets)\nhave achieved impressive performance on a variety of classification tasks using\npurely feedforward processing. Feedforward architectures can learn rich\nrepresentations of the input space but do not explicitly model dependencies in\nthe output spaces, that are quite structured for tasks such as articulated\nhuman pose estimation or object segmentation. Here we propose a framework that\nexpands the expressive power of hierarchical feature extractors to encompass\nboth input and output spaces, by introducing top-down feedback. Instead of\ndirectly predicting the outputs in one go, we use a self-correcting model that\nprogressively changes an initial solution by feeding back error predictions, in\na process we call Iterative Error Feedback (IEF). IEF shows excellent\nperformance on the task of articulated pose estimation in the challenging MPII\nand LSP benchmarks, matching the state-of-the-art without requiring ground\ntruth scale annotation. \n\n"}
{"id": "1507.07374", "contents": "Title: A genetic algorithm for autonomous navigation in partially observable\n  domain Abstract: The problem of autonomous navigation is one of the basic problems for\nrobotics. Although, in general, it may be challenging when an autonomous\nvehicle is placed into partially observable domain. In this paper we consider\nsimplistic environment model and introduce a navigation algorithm based on\nLearning Classifier System. \n\n"}
{"id": "1508.00038", "contents": "Title: Quantum Walks and discrete Gauge Theories Abstract: A particular example is produced to prove that quantum walks can be used to\nsimulate full-fledged discrete gauge theories. A new family of $2D$ walks is\nintroduced and its continuous limit is shown to coincide with the dynamics of a\nDirac fermion coupled to arbitrary electromagnetic fields. The electromagnetic\ninterpretation is extended beyond the continuous limit by proving that these\nDTQWs exhibit an exact discrete local $U(1)$ gauge invariance and possess a\ndiscrete gauge-invariant conserved current. A discrete gauge-invariant\nelectromagnetic field is also constructed and that field is coupled to the\nconserved current by a discrete generalization of Maxwell equations. The\ndynamics of the DTQWs under crossed electric and magnetic fields is finally\nexplored outside the continuous limit by numerical simulations. Bloch\noscillations and the so-called ${\\bf E} \\times {\\bf B}$ drift are recovered in\nthe weak-field limit. Localization is observed for some values of the gauge\nfields. \n\n"}
{"id": "1508.01774", "contents": "Title: An End-to-End Neural Network for Polyphonic Piano Music Transcription Abstract: We present a supervised neural network model for polyphonic piano music\ntranscription. The architecture of the proposed model is analogous to speech\nrecognition systems and comprises an acoustic model and a music language model.\nThe acoustic model is a neural network used for estimating the probabilities of\npitches in a frame of audio. The language model is a recurrent neural network\nthat models the correlations between pitch combinations over time. The proposed\nmodel is general and can be used to transcribe polyphonic music without\nimposing any constraints on the polyphony. The acoustic and language model\npredictions are combined using a probabilistic graphical model. Inference over\nthe output variables is performed using the beam search algorithm. We perform\ntwo sets of experiments. We investigate various neural network architectures\nfor the acoustic models and also investigate the effect of combining acoustic\nand music language model predictions using the proposed architecture. We\ncompare performance of the neural network based acoustic models with two\npopular unsupervised acoustic models. Results show that convolutional neural\nnetwork acoustic models yields the best performance across all evaluation\nmetrics. We also observe improved performance with the application of the music\nlanguage models. Finally, we present an efficient variant of beam search that\nimproves performance and reduces run-times by an order of magnitude, making the\nmodel suitable for real-time applications. \n\n"}
{"id": "1508.04211", "contents": "Title: Scalable Bayesian Non-Negative Tensor Factorization for Massive Count\n  Data Abstract: We present a Bayesian non-negative tensor factorization model for\ncount-valued tensor data, and develop scalable inference algorithms (both batch\nand online) for dealing with massive tensors. Our generative model can handle\noverdispersed counts as well as infer the rank of the decomposition. Moreover,\nleveraging a reparameterization of the Poisson distribution as a multinomial\nfacilitates conjugacy in the model and enables simple and efficient Gibbs\nsampling and variational Bayes (VB) inference updates, with a computational\ncost that only depends on the number of nonzeros in the tensor. The model also\nprovides a nice interpretability for the factors; in our model, each factor\ncorresponds to a \"topic\". We develop a set of online inference algorithms that\nallow further scaling up the model to massive tensors, for which batch\ninference methods may be infeasible. We apply our framework on diverse\nreal-world applications, such as \\emph{multiway} topic modeling on a scientific\npublications database, analyzing a political science data set, and analyzing a\nmassive household transactions data set. \n\n"}
{"id": "1508.05003", "contents": "Title: AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization Abstract: We study distributed stochastic convex optimization under the delayed\ngradient model where the server nodes perform parameter updates, while the\nworker nodes compute stochastic gradients. We discuss, analyze, and experiment\nwith a setup motivated by the behavior of real-world distributed computation\nnetworks, where the machines are differently slow at different time. Therefore,\nwe allow the parameter updates to be sensitive to the actual delays\nexperienced, rather than to worst-case bounds on the maximum delay. This\nsensitivity leads to larger stepsizes, that can help gain rapid initial\nconvergence without having to wait too long for slower machines, while\nmaintaining the same asymptotic complexity. We obtain encouraging improvements\nto overall convergence for distributed experiments on real datasets with up to\nbillions of examples and features. \n\n"}
{"id": "1508.06091", "contents": "Title: AUC Optimisation and Collaborative Filtering Abstract: In recommendation systems, one is interested in the ranking of the predicted\nitems as opposed to other losses such as the mean squared error. Although a\nvariety of ways to evaluate rankings exist in the literature, here we focus on\nthe Area Under the ROC Curve (AUC) as it widely used and has a strong\ntheoretical underpinning. In practical recommendation, only items at the top of\nthe ranked list are presented to the users. With this in mind, we propose a\nclass of objective functions over matrix factorisations which primarily\nrepresent a smooth surrogate for the real AUC, and in a special case we show\nhow to prioritise the top of the list. The objectives are differentiable and\noptimised through a carefully designed stochastic gradient-descent-based\nalgorithm which scales linearly with the size of the data. In the special case\nof square loss we show how to improve computational complexity by leveraging\npreviously computed measures. To understand theoretically the underlying matrix\nfactorisation approaches we study both the consistency of the loss functions\nwith respect to AUC, and generalisation using Rademacher theory. The resulting\ngeneralisation analysis gives strong motivation for the optimisation under\nstudy. Finally, we provide computation results as to the efficacy of the\nproposed method using synthetic and real data. \n\n"}
{"id": "1508.06477", "contents": "Title: Greedy methods, randomization approaches and multi-arm bandit algorithms\n  for efficient sparsity-constrained optimization Abstract: Several sparsity-constrained algorithms such as Orthogonal Matching Pursuit\nor the Frank-Wolfe algorithm with sparsity constraints work by iteratively\nselecting a novel atom to add to the current non-zero set of variables. This\nselection step is usually performed by computing the gradient and then by\nlooking for the gradient component with maximal absolute entry. This step can\nbe computationally expensive especially for large-scale and high-dimensional\ndata. In this work, we aim at accelerating these sparsity-constrained\noptimization algorithms by exploiting the key observation that, for these\nalgorithms to work, one only needs the coordinate of the gradient's top entry.\nHence, we introduce algorithms based on greedy methods and randomization\napproaches that aim at cheaply estimating the gradient and its top entry.\nAnother of our contribution is to cast the problem of finding the best gradient\nentry as a best arm identification in a multi-armed bandit problem. Owing to\nthis novel insight, we are able to provide a bandit-based algorithm that\ndirectly estimates the top entry in a very efficient way. Theoretical\nobservations stating that the resulting inexact Frank-Wolfe or Orthogonal\nMatching Pursuit algorithms act, with high probability, similarly to their\nexact versions are also given. We have carried out several experiments showing\nthat the greedy deterministic and the bandit approaches we propose can achieve\nan acceleration of an order of magnitude while being as efficient as the exact\ngradient when used in algorithms such as OMP, Frank-Wolfe or CoSaMP. \n\n"}
{"id": "1508.07091", "contents": "Title: Multi-armed Bandit Problem with Known Trend Abstract: We consider a variant of the multi-armed bandit model, which we call\nmulti-armed bandit problem with known trend, where the gambler knows the shape\nof the reward function of each arm but not its distribution. This new problem\nis motivated by different online problems like active learning, music and\ninterface recommendation applications, where when an arm is sampled by the\nmodel the received reward change according to a known trend. By adapting the\nstandard multi-armed bandit algorithm UCB1 to take advantage of this setting,\nwe propose the new algorithm named A-UCB that assumes a stochastic model. We\nprovide upper bounds of the regret which compare favourably with the ones of\nUCB1. We also confirm that experimentally with different simulations \n\n"}
{"id": "1509.00114", "contents": "Title: Multi-Sensor Slope Change Detection Abstract: We develop a mixture procedure for multi-sensor systems to monitor data\nstreams for a change-point that causes a gradual degradation to a subset of the\nstreams. Observations are assumed to be initially normal random variables with\nknown constant means and variances. After the change-point, observations in the\nsubset will have increasing or decreasing means. The subset and the\nrate-of-changes are unknown. Our procedure uses a mixture statistics, which\nassumes that each sensor is affected by the change-point with probability\n$p_0$. Analytic expressions are obtained for the average run length (ARL) and\nthe expected detection delay (EDD) of the mixture procedure, which are\ndemonstrated to be quite accurate numerically. We establish the asymptotic\noptimality of the mixture procedure. Numerical examples demonstrate the good\nperformance of the proposed procedure. We also discuss an adaptive mixture\nprocedure using empirical Bayes. This paper extends our earlier work on\ndetecting an abrupt change-point that causes a mean-shift, by tackling the\nchallenges posed by the non-stationarity of the slope-change problem. \n\n"}
{"id": "1509.01561", "contents": "Title: Universality of Generalized Bunching and Efficient Assessment of Boson\n  Sampling Abstract: It is found that identical bosons (fermions) show generalized bunching\n(antibunching) property in linear networks: The absolute maximum (minimum) of\nprobability that all $N$ input particles are detected in a subset of\n$\\mathcal{K}$ output modes of any nontrivial linear $M$-mode network is\nattained \\textit{only} by completely indistinguishable bosons (fermions). For\nfermions $\\mathcal{K}$ is arbitrary, for bosons it is either ($i$) arbitrary\nfor only classically correlated bosons or ($ii$) satisfies $\\mathcal{K}\\ge N$\n(or $\\mathcal{K}=1$) for arbitrary input states of $N$ particles. The\ngeneralized bunching allows to certify in a \\textit{polynomial} in $N$ number\nof runs that a physical device realizing Boson Sampling with \\textit{an\narbitrary} network operates in the regime of full quantum coherence compatible\n\\textit{only} with completely indistinguishable bosons. The protocol needs\n\\textit{only polynomial} classical computations for the standard Boson\nSampling, whereas an \\textit{analytic formula} is available for the scattershot\nversion. \n\n"}
{"id": "1509.02971", "contents": "Title: Continuous control with deep reinforcement learning Abstract: We adapt the ideas underlying the success of Deep Q-Learning to the\ncontinuous action domain. We present an actor-critic, model-free algorithm\nbased on the deterministic policy gradient that can operate over continuous\naction spaces. Using the same learning algorithm, network architecture and\nhyper-parameters, our algorithm robustly solves more than 20 simulated physics\ntasks, including classic problems such as cartpole swing-up, dexterous\nmanipulation, legged locomotion and car driving. Our algorithm is able to find\npolicies whose performance is competitive with those found by a planning\nalgorithm with full access to the dynamics of the domain and its derivatives.\nWe further demonstrate that for many of the tasks the algorithm can learn\npolicies end-to-end: directly from raw pixel inputs. \n\n"}
{"id": "1509.04397", "contents": "Title: Exponential Family Matrix Completion under Structural Constraints Abstract: We consider the matrix completion problem of recovering a structured matrix\nfrom noisy and partial measurements. Recent works have proposed tractable\nestimators with strong statistical guarantees for the case where the underlying\nmatrix is low--rank, and the measurements consist of a subset, either of the\nexact individual entries, or of the entries perturbed by additive Gaussian\nnoise, which is thus implicitly suited for thin--tailed continuous data.\nArguably, common applications of matrix completion require estimators for (a)\nheterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)\nfor heterogeneous noise models (beyond Gaussian), which capture varied\nuncertainty in the measurements, and (c) heterogeneous structural constraints\nbeyond low--rank, such as block--sparsity, or a superposition structure of\nlow--rank plus elementwise sparseness, among others. In this paper, we provide\na vastly unified framework for generalized matrix completion by considering a\nmatrix completion setting wherein the matrix entries are sampled from any\nmember of the rich family of exponential family distributions; and impose\ngeneral structural constraints on the underlying matrix, as captured by a\ngeneral regularizer $\\mathcal{R}(.)$. We propose a simple convex regularized\n$M$--estimator for the generalized framework, and provide a unified and novel\nstatistical analysis for this general class of estimators. We finally\ncorroborate our theoretical results on simulated datasets. \n\n"}
{"id": "1509.06569", "contents": "Title: Tensorizing Neural Networks Abstract: Deep neural networks currently demonstrate state-of-the-art performance in\nseveral domains. At the same time, models of this class are very demanding in\nterms of computational resources. In particular, a large amount of memory is\nrequired by commonly used fully-connected layers, making it hard to use the\nmodels on low-end devices and stopping the further increase of the model size.\nIn this paper we convert the dense weight matrices of the fully-connected\nlayers to the Tensor Train format such that the number of parameters is reduced\nby a huge factor and at the same time the expressive power of the layer is\npreserved. In particular, for the Very Deep VGG networks we report the\ncompression factor of the dense weight matrix of a fully-connected layer up to\n200000 times leading to the compression factor of the whole network up to 7\ntimes. \n\n"}
{"id": "1509.08360", "contents": "Title: Compressive spectral embedding: sidestepping the SVD Abstract: Spectral embedding based on the Singular Value Decomposition (SVD) is a\nwidely used \"preprocessing\" step in many learning tasks, typically leading to\ndimensionality reduction by projecting onto a number of dominant singular\nvectors and rescaling the coordinate axes (by a predefined function of the\nsingular value). However, the number of such vectors required to capture\nproblem structure grows with problem size, and even partial SVD computation\nbecomes a bottleneck. In this paper, we propose a low-complexity it compressive\nspectral embedding algorithm, which employs random projections and finite order\npolynomial expansions to compute approximations to SVD-based embedding. For an\nm times n matrix with T non-zeros, its time complexity is O((T+m+n)log(m+n)),\nand the embedding dimension is O(log(m+n)), both of which are independent of\nthe number of singular vectors whose effect we wish to capture. To the best of\nour knowledge, this is the first work to circumvent this dependence on the\nnumber of singular vectors for general SVD-based embeddings. The key to\nsidestepping the SVD is the observation that, for downstream inference tasks\nsuch as clustering and classification, we are only interested in using the\nresulting embedding to evaluate pairwise similarity metrics derived from the\neuclidean norm, rather than capturing the effect of the underlying matrix on\narbitrary vectors as a partial SVD tries to do. Our numerical results on\nnetwork datasets demonstrate the efficacy of the proposed method, and motivate\nfurther exploration of its application to large-scale inference tasks. \n\n"}
{"id": "1510.00149", "contents": "Title: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding Abstract: Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency. \n\n"}
{"id": "1510.01064", "contents": "Title: Boosting in the presence of outliers: adaptive classification with\n  non-convex loss functions Abstract: This paper examines the role and efficiency of the non-convex loss functions\nfor binary classification problems. In particular, we investigate how to design\na simple and effective boosting algorithm that is robust to the outliers in the\ndata. The analysis of the role of a particular non-convex loss for prediction\naccuracy varies depending on the diminishing tail properties of the gradient of\nthe loss -- the ability of the loss to efficiently adapt to the outlying data,\nthe local convex properties of the loss and the proportion of the contaminated\ndata. In order to use these properties efficiently, we propose a new family of\nnon-convex losses named $\\gamma$-robust losses. Moreover, we present a new\nboosting framework, {\\it Arch Boost}, designed for augmenting the existing work\nsuch that its corresponding classification algorithm is significantly more\nadaptable to the unknown data contamination. Along with the Arch Boosting\nframework, the non-convex losses lead to the new class of boosting algorithms,\nnamed adaptive, robust, boosting (ARB). Furthermore, we present theoretical\nexamples that demonstrate the robustness properties of the proposed algorithms.\nIn particular, we develop a new breakdown point analysis and a new influence\nfunction analysis that demonstrate gains in robustness. Moreover, we present\nnew theoretical results, based only on local curvatures, which may be used to\nestablish statistical and optimization properties of the proposed Arch boosting\nalgorithms with highly non-convex loss functions. Extensive numerical\ncalculations are used to illustrate these theoretical properties and reveal\nadvantages over the existing boosting methods when data exhibits a number of\noutliers. \n\n"}
{"id": "1510.01171", "contents": "Title: On the Online Frank-Wolfe Algorithms for Convex and Non-convex\n  Optimizations Abstract: In this paper, the online variants of the classical Frank-Wolfe algorithm are\nconsidered. We consider minimizing the regret with a stochastic cost. The\nonline algorithms only require simple iterative updates and a non-adaptive step\nsize rule, in contrast to the hybrid schemes commonly considered in the\nliterature. Several new results are derived for convex and non-convex losses.\nWith a strongly convex stochastic cost and when the optimal solution lies in\nthe interior of the constraint set or the constraint set is a polytope, the\nregret bound and anytime optimality are shown to be ${\\cal O}( \\log^3 T / T )$\nand ${\\cal O}( \\log^2 T / T)$, respectively, where $T$ is the number of rounds\nplayed. These results are based on an improved analysis on the stochastic\nFrank-Wolfe algorithms. Moreover, the online algorithms are shown to converge\neven when the loss is non-convex, i.e., the algorithms find a stationary point\nto the time-varying/stochastic loss at a rate of ${\\cal O}(\\sqrt{1/T})$.\nNumerical experiments on realistic data sets are presented to support our\ntheoretical claims. \n\n"}
{"id": "1510.01799", "contents": "Title: Efficient Per-Example Gradient Computations Abstract: This technical report describes an efficient technique for computing the norm\nof the gradient of the loss function for a neural network with respect to its\nparameters. This gradient norm can be computed efficiently for every example. \n\n"}
{"id": "1510.04189", "contents": "Title: Improving Back-Propagation by Adding an Adversarial Gradient Abstract: The back-propagation algorithm is widely used for learning in artificial\nneural networks. A challenge in machine learning is to create models that\ngeneralize to new data samples not seen in the training data. Recently, a\ncommon flaw in several machine learning algorithms was discovered: small\nperturbations added to the input data lead to consistent misclassification of\ndata samples. Samples that easily mislead the model are called adversarial\nexamples. Training a \"maxout\" network on adversarial examples has shown to\ndecrease this vulnerability, but also increase classification performance. This\npaper shows that adversarial training has a regularizing effect also in\nnetworks with logistic, hyperbolic tangent and rectified linear units. A simple\nextension to the back-propagation method is proposed, that adds an adversarial\ngradient to the training. The extension requires an additional forward and\nbackward pass to calculate a modified input sample, or mini batch, used as\ninput for standard back-propagation learning. The first experimental results on\nMNIST show that the \"adversarial back-propagation\" method increases the\nresistance to adversarial examples and boosts the classification performance.\nThe extension reduces the classification error on the permutation invariant\nMNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a\nnetwork with rectified linear units. Results on CIFAR-10 indicate that the\nmethod has a regularizing effect similar to dropout in fully connected\nnetworks. Based on these promising results, adversarial back-propagation is\nproposed as a stand-alone regularizing method that should be further\ninvestigated. \n\n"}
{"id": "1510.04822", "contents": "Title: SGD with Variance Reduction beyond Empirical Risk Minimization Abstract: We introduce a doubly stochastic proximal gradient algorithm for optimizing a\nfinite average of smooth convex functions, whose gradients depend on\nnumerically expensive expectations. Our main motivation is the acceleration of\nthe optimization of the regularized Cox partial-likelihood (the core model used\nin survival analysis), but our algorithm can be used in different settings as\nwell. The proposed algorithm is doubly stochastic in the sense that gradient\nsteps are done using stochastic gradient descent (SGD) with variance reduction,\nwhere the inner expectations are approximated by a Monte-Carlo Markov-Chain\n(MCMC) algorithm. We derive conditions on the MCMC number of iterations\nguaranteeing convergence, and obtain a linear rate of convergence under strong\nconvexity and a sublinear rate without this assumption. We illustrate the fact\nthat our algorithm improves the state-of-the-art solver for regularized Cox\npartial-likelihood on several datasets from survival analysis. \n\n"}
{"id": "1510.07471", "contents": "Title: A Parallel algorithm for $\\mathcal{X}$-Armed bandits Abstract: The target of $\\mathcal{X}$-armed bandit problem is to find the global\nmaximum of an unknown stochastic function $f$, given a finite budget of $n$\nevaluations. Recently, $\\mathcal{X}$-armed bandits have been widely used in\nmany situations. Many of these applications need to deal with large-scale data\nsets. To deal with these large-scale data sets, we study a distributed setting\nof $\\mathcal{X}$-armed bandits, where $m$ players collaborate to find the\nmaximum of the unknown function. We develop a novel anytime distributed\n$\\mathcal{X}$-armed bandit algorithm. Compared with prior work on\n$\\mathcal{X}$-armed bandits, our algorithm uses a quite different searching\nstrategy so as to fit distributed learning scenarios. Our theoretical analysis\nshows that our distributed algorithm is $m$ times faster than the classical\nsingle-player algorithm. Moreover, the number of communication rounds of our\nalgorithm is only logarithmic in $mn$. The numerical results show that our\nmethod can make effective use of every players to minimize the loss. Thus, our\ndistributed approach is attractive and useful. \n\n"}
{"id": "1510.08692", "contents": "Title: Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale\n  Bayesian Sampling Abstract: Monte Carlo sampling for Bayesian posterior inference is a common approach\nused in machine learning. The Markov Chain Monte Carlo procedures that are used\nare often discrete-time analogues of associated stochastic differential\nequations (SDEs). These SDEs are guaranteed to leave invariant the required\nposterior distribution. An area of current research addresses the computational\nbenefits of stochastic gradient methods in this setting. Existing techniques\nrely on estimating the variance or covariance of the subsampling error, and\ntypically assume constant variance. In this article, we propose a\ncovariance-controlled adaptive Langevin thermostat that can effectively\ndissipate parameter-dependent noise while maintaining a desired target\ndistribution. The proposed method achieves a substantial speedup over popular\nalternative schemes for large-scale machine learning applications. \n\n"}
{"id": "1510.08896", "contents": "Title: Robust Shift-and-Invert Preconditioning: Faster and More Sample\n  Efficient Algorithms for Eigenvector Computation Abstract: We provide faster algorithms and improved sample complexities for\napproximating the top eigenvector of a matrix.\n  Offline Setting: Given an $n \\times d$ matrix $A$, we show how to compute an\n$\\epsilon$ approximate top eigenvector in time $\\tilde O ( [nnz(A) + \\frac{d\n\\cdot sr(A)}{gap^2}]\\cdot \\log 1/\\epsilon )$ and $\\tilde O([\\frac{nnz(A)^{3/4}\n(d \\cdot sr(A))^{1/4}}{\\sqrt{gap}}]\\cdot \\log1/\\epsilon )$. Here $sr(A)$ is the\nstable rank and $gap$ is the multiplicative eigenvalue gap. By separating the\n$gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczos\nmethods. We also improve prior work using fast subspace embeddings and\nstochastic optimization, giving significantly improved dependencies on $sr(A)$\nand $\\epsilon$. Our second running time improves this further when $nnz(A) \\le\n\\frac{d\\cdot sr(A)}{gap^2}$.\n  Online Setting: Given a distribution $D$ with covariance matrix $\\Sigma$ and\na vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\\Sigma$,\nwe show how to refine to an $\\epsilon$ approximation using $\\tilde\nO(\\frac{v(D)}{gap^2} + \\frac{v(D)}{gap \\cdot \\epsilon})$ samples from $D$. Here\n$v(D)$ is a natural variance measure. Combining our algorithm with previous\nwork to initialize $x_0$, we obtain a number of improved sample complexity and\nruntime results. For general distributions, we achieve asymptotically optimal\naccuracy as a function of sample size as the number of samples grows large.\n  Our results center around a robust analysis of the classic method of\nshift-and-invert preconditioning to reduce eigenvector computation to\napproximately solving a sequence of linear systems. We then apply fast SVRG\nbased approximate system solvers to achieve our claims. We believe our results\nsuggest the general effectiveness of shift-and-invert based approaches and\nimply that further computational gains may be reaped in practice. \n\n"}
{"id": "1511.06592", "contents": "Title: Quantum memories with zero-energy Majorana modes and experimental\n  constraints Abstract: In this work we address the problem of realizing a reliable quantum memory\nbased on zero-energy Majorana modes in the presence of experimental constraints\non the operations aimed at recovering the information. In particular, we\ncharacterize the best recovery operation acting only on the zero-energy\nMajorana modes and the memory fidelity that can be therewith achieved. In order\nto understand the effect of such restriction, we discuss two examples of noise\nmodels acting on the topological system and compare the amount of information\nthat can be recovered by accessing either the whole system, or the zero-modes\nonly, with particular attention to the scaling with the size of the system and\nthe energy gap. We explicitly discuss the case of a thermal bosonic environment\ninducing a parity-preserving Markovian dynamics in which the introduced memory\nfidelity decays exponentially in time, independent from system size, thus\nshowing the impossibility to retrieve the information by acting on the\nzero-modes only. We argue, however, that even in the presence of experimental\nlimitations, the Hamiltonian gap is still beneficial to the storage of\ninformation. \n\n"}
{"id": "1511.06881", "contents": "Title: Zoom Better to See Clearer: Human and Object Parsing with Hierarchical\n  Auto-Zoom Net Abstract: Parsing articulated objects, e.g. humans and animals, into semantic parts\n(e.g. body, head and arms, etc.) from natural images is a challenging and\nfundamental problem for computer vision. A big difficulty is the large\nvariability of scale and location for objects and their corresponding parts.\nEven limited mistakes in estimating scale and location will degrade the parsing\noutput and cause errors in boundary details. To tackle these difficulties, we\npropose a \"Hierarchical Auto-Zoom Net\" (HAZN) for object part parsing which\nadapts to the local scales of objects and parts. HAZN is a sequence of two\n\"Auto-Zoom Net\" (AZNs), each employing fully convolutional networks that\nperform two tasks: (1) predict the locations and scales of object instances\n(the first AZN) or their parts (the second AZN); (2) estimate the part scores\nfor predicted object instance or part regions. Our model can adaptively \"zoom\"\n(resize) predicted image regions into their proper scales to refine the\nparsing.\n  We conduct extensive experiments over the PASCAL part datasets on humans,\nhorses, and cows. For humans, our approach significantly outperforms the\nstate-of-the-arts by 5% mIOU and is especially better at segmenting small\ninstances and small parts. We obtain similar improvements for parsing cows and\nhorses over alternative methods. In summary, our strategy of first zooming into\nobjects and then zooming into parts is very effective. It also enables us to\nprocess different regions of the image at different scales adaptively so that,\nfor example, we do not need to waste computational resources scaling the entire\nimage. \n\n"}
{"id": "1511.07371", "contents": "Title: Dynamical Casimir effect in superconducting circuits: a numerical\n  approach Abstract: We present a numerical analysis of the particle creation for a quantum field\nin the presence of time dependent boundary conditions. Having in mind recent\nexperiments involving superconducting circuits, we consider their description\nin terms of a scalar field in a one dimensional cavity satisfying generalized\nboundary conditions that involve a time-dependent linear combination of the\nfield and its spatial and time derivatives. We evaluate numerically the\nBogoliubov transformation between {\\it in} and {\\it out}-states and find that\nthe rate of particle production strongly depends on whether the spectrum of the\nunperturbed cavity is equidistant or not, and also on the amplitude of the\ntemporal oscillations of the boundary conditions. We provide analytic\njustifications for the different regimes found numerically. \n\n"}
{"id": "1511.07953", "contents": "Title: Exploring Correlation between Labels to improve Multi-Label\n  Classification Abstract: This paper attempts multi-label classification by extending the idea of\nindependent binary classification models for each output label, and exploring\nhow the inherent correlation between output labels can be used to improve\npredictions. Logistic Regression, Naive Bayes, Random Forest, and SVM models\nwere constructed, with SVM giving the best results: an improvement of 12.9\\%\nover binary models was achieved for hold out cross validation by augmenting\nwith pairwise correlation probabilities of the labels. \n\n"}
{"id": "1512.02468", "contents": "Title: The nine ways of four qubit entanglement and their threetangle Abstract: I calculate the mixed threetangle $\\tau_3[\\rho]$ for the reduced density\nmatrices of the four-qubit representant states found in Phys. Rev. A {\\bf 65},\n052112 (2002). In most of the cases, the convex roof is obtained, except for\none class, where I provide with a new upper bound, which is assumed to be very\nclose to the convex roof. I compare with results published in Phys. Rev. Lett.\n{\\bf 113}, 110501 (2014). Since the method applied there usually results in\nhigher values for the upper bound, in certain cases it can be understood that\nthe convex roof is obtained exactly, namely when the zero-polytope where\n$\\tau_3$ vanishes shrinks to a single point. \n\n"}
{"id": "1512.03375", "contents": "Title: Convolutional Monte Carlo Rollouts in Go Abstract: In this work, we present a MCTS-based Go-playing program which uses\nconvolutional networks in all parts. Our method performs MCTS in batches,\nexplores the Monte Carlo search tree using Thompson sampling and a\nconvolutional network, and evaluates convnet-based rollouts on the GPU. We\nachieve strong win rates against open source Go programs and attain competitive\nresults against state of the art convolutional net-based Go-playing programs. \n\n"}
{"id": "1512.03880", "contents": "Title: Active Sampler: Light-weight Accelerator for Complex Data Analytics at\n  Scale Abstract: Recent years have witnessed amazing outcomes from \"Big Models\" trained by\n\"Big Data\". Most popular algorithms for model training are iterative. Due to\nthe surging volumes of data, we can usually afford to process only a fraction\nof the training data in each iteration. Typically, the data are either\nuniformly sampled or sequentially accessed.\n  In this paper, we study how the data access pattern can affect model\ntraining. We propose an Active Sampler algorithm, where training data with more\n\"learning value\" to the model are sampled more frequently. The goal is to focus\ntraining effort on valuable instances near the classification boundaries,\nrather than evident cases, noisy data or outliers. We show the correctness and\noptimality of Active Sampler in theory, and then develop a light-weight\nvectorized implementation. Active Sampler is orthogonal to most approaches\noptimizing the efficiency of large-scale data analytics, and can be applied to\nmost analytics models trained by stochastic gradient descent (SGD) algorithm.\nExtensive experimental evaluations demonstrate that Active Sampler can speed up\nthe training procedure of SVM, feature selection and deep learning, for\ncomparable training quality by 1.6-2.2x. \n\n"}
{"id": "1512.04945", "contents": "Title: General Benchmarks for Quantum Repeaters Abstract: Using a technique based on quantum teleportation, we simplify the most\ngeneral adaptive protocols for key distribution, entanglement distillation and\nquantum communication over a wide class of quantum channels in arbitrary\ndimension. Thanks to this method, we bound the ultimate rates for secret key\ngeneration and quantum communication through single-mode Gaussian channels and\nseveral discrete-variable channels. In particular, we derive exact formulas for\nthe two-way assisted capacities of the bosonic quantum-limited amplifier and\nthe dephasing channel in arbitrary dimension, as well as the secret key\ncapacity of the qubit erasure channel. Our results establish the limits of\nquantum communication with arbitrary systems and set the most general and\nprecise benchmarks for testing quantum repeaters in both discrete- and\ncontinuous-variable settings. \n\n"}
{"id": "1512.07797", "contents": "Title: The Lov\\'asz Hinge: A Novel Convex Surrogate for Submodular Losses Abstract: Learning with non-modular losses is an important problem when sets of\npredictions are made simultaneously. The main tools for constructing convex\nsurrogate loss functions for set prediction are margin rescaling and slack\nrescaling. In this work, we show that these strategies lead to tight convex\nsurrogates iff the underlying loss function is increasing in the number of\nincorrect predictions. However, gradient or cutting-plane computation for these\nfunctions is NP-hard for non-supermodular loss functions. We propose instead a\nnovel surrogate loss function for submodular losses, the Lov\\'asz hinge, which\nleads to O(p log p) complexity with O(p) oracle accesses to the loss function\nto compute a gradient or cutting-plane. We prove that the Lov\\'asz hinge is\nconvex and yields an extension. As a result, we have developed the first\ntractable convex surrogates in the literature for submodular losses. We\ndemonstrate the utility of this novel convex surrogate through several set\nprediction tasks, including on the PASCAL VOC and Microsoft COCO datasets. \n\n"}
{"id": "1512.08787", "contents": "Title: Matrix Completion Under Monotonic Single Index Models Abstract: Most recent results in matrix completion assume that the matrix under\nconsideration is low-rank or that the columns are in a union of low-rank\nsubspaces. In real-world settings, however, the linear structure underlying\nthese models is distorted by a (typically unknown) nonlinear transformation.\nThis paper addresses the challenge of matrix completion in the face of such\nnonlinearities. Given a few observations of a matrix that are obtained by\napplying a Lipschitz, monotonic function to a low rank matrix, our task is to\nestimate the remaining unobserved entries. We propose a novel matrix completion\nmethod that alternates between low-rank matrix estimation and monotonic\nfunction estimation to estimate the missing matrix elements. Mean squared error\nbounds provide insight into how well the matrix can be estimated based on the\nsize, rank of the matrix and properties of the nonlinear transformation.\nEmpirical results on synthetic and real-world datasets demonstrate the\ncompetitiveness of the proposed approach. \n\n"}
{"id": "1601.00966", "contents": "Title: Capacities of repeater-assisted quantum communications Abstract: We consider quantum and private communications assisted by repeaters, from\nthe basic scenario of a single repeater chain to the general case of an\narbitrarily-complex quantum network, where systems may be routed through single\nor multiple paths. In this context, we investigate the ultimate rates at which\ntwo end-parties may transmit quantum information, distribute entanglement, or\ngenerate secret keys. These end-to-end capacities are defined by optimizing\nover the most general adaptive protocols that are allowed by quantum mechanics.\nCombining techniques from quantum information and classical network theory, we\nderive single-letter upper bounds for the end-to-end capacities in repeater\nchains and quantum networks connected by arbitrary quantum channels,\nestablishing exact formulas under basic decoherence models, including bosonic\nlossy channels, quantum-limited amplifiers, dephasing and erasure channels. For\nthe converse part, we adopt a teleportation-inspired simulation of a quantum\nnetwork which leads to upper bounds in terms of the relative entropy of\nentanglement. For the lower bounds we combine point-to-point quantum protocols\nwith classical network algorithms. Depending on the type of routing (single or\nmultiple), optimal strategies corresponds to finding the widest path or the\nmaximum flow in the quantum network. Our theory can also be extended to\nsimultaneous quantum communication between multiple senders and receivers. \n\n"}
{"id": "1601.03642", "contents": "Title: Creativity in Machine Learning Abstract: Recent machine learning techniques can be modified to produce creative\nresults. Those results did not exist before; it is not a trivial combination of\nthe data which was fed into the machine learning system. The obtained results\ncome in multiple forms: As images, as text and as audio.\n  This paper gives a high level overview of how they are created and gives some\nexamples. It is meant to be a summary of the current work and give people who\nare new to machine learning some starting points. \n\n"}
{"id": "1601.03782", "contents": "Title: Robustness of asymmetry and coherence of quantum states Abstract: Quantum states may exhibit asymmetry with respect to the action of a given\ngroup. Such an asymmetry of states can be considered as a resource in\napplications such as quantum metrology, and it is a concept that encompasses\nquantum coherence as a special case. We introduce explicitly and study the\nrobustness of asymmetry, a quantifier of asymmetry of states that we prove to\nhave many attractive properties, including efficient numerical computability\nvia semidefinite programming, and an operational interpretation in a channel\ndiscrimination context. We also introduce the notion of asymmetry witnesses,\nwhose measurement in a laboratory detects the presence of asymmetry. We prove\nthat properly constrained asymmetry witnesses provide lower bounds to the\nrobustness of asymmetry, which is shown to be a directly measurable quantity\nitself. We then focus our attention on coherence witnesses and the robustness\nof coherence, for which we prove a number of additional results; these include\nan analysis of its specific relevance in phase discrimination and quantum\nmetrology, an analytical calculation of its value for a relevant class of\nquantum states, and tight bounds that relate it to another previously defined\ncoherence monotone. \n\n"}
{"id": "1601.07451", "contents": "Title: Photonic currents in driven and dissipative resonator lattices Abstract: Arrays of coupled photonic cavities driven by external lasers represent a\nhighly controllable setup to explore photonic transport. In this paper we\naddress (quasi)-steady states of this system that exhibit photonic currents\nintroduced by engineering driving and dissipation. We investigate two\napproaches: in the first one, photonic currents arise as a consequence of a\nphase difference of applied lasers and in the second one, photons are injected\nlocally and currents develop as they redistribute over the lattice. Effects of\ninteractions are taken into account within a mean-field framework. In the first\napproach, we find that the current exhibits a resonant behavior with respect to\nthe driving frequency. Weak interactions shift the resonant frequency toward\nhigher values, while in the strongly interacting regime in our mean-field\ntreatment the effect stems from multiphotonic resonances of a single driven\ncavity. For the second approach, we show that the overall lattice current can\nbe controlled by incorporating few cavities with stronger dissipation rates\ninto the system. These cavities serve as sinks for photonic currents and their\neffect is maximal at the onset of quantum Zeno dynamics. \n\n"}
{"id": "1602.00133", "contents": "Title: SCOPE: Scalable Composite Optimization for Learning on Spark Abstract: Many machine learning models, such as logistic regression~(LR) and support\nvector machine~(SVM), can be formulated as composite optimization problems.\nRecently, many distributed stochastic optimization~(DSO) methods have been\nproposed to solve the large-scale composite optimization problems, which have\nshown better performance than traditional batch methods. However, most of these\nDSO methods are not scalable enough. In this paper, we propose a novel DSO\nmethod, called \\underline{s}calable \\underline{c}omposite\n\\underline{op}timization for l\\underline{e}arning~({SCOPE}), and implement it\non the fault-tolerant distributed platform \\mbox{Spark}. SCOPE is both\ncomputation-efficient and communication-efficient. Theoretical analysis shows\nthat SCOPE is convergent with linear convergence rate when the objective\nfunction is convex. Furthermore, empirical results on real datasets show that\nSCOPE can outperform other state-of-the-art distributed learning methods on\nSpark, including both batch learning methods and DSO methods. \n\n"}
{"id": "1602.01132", "contents": "Title: Interactive algorithms: from pool to stream Abstract: We consider interactive algorithms in the pool-based setting, and in the\nstream-based setting. Interactive algorithms observe suggested elements\n(representing actions or queries), and interactively select some of them and\nreceive responses. Pool-based algorithms can select elements at any order,\nwhile stream-based algorithms observe elements in sequence, and can only select\nelements immediately after observing them. We assume that the suggested\nelements are generated independently from some source distribution, and ask\nwhat is the stream size required for emulating a pool algorithm with a given\npool size. We provide algorithms and matching lower bounds for general pool\nalgorithms, and for utility-based pool algorithms. We further show that a\nmaximal gap between the two settings exists also in the special case of active\nlearning for binary classification. \n\n"}
{"id": "1602.02136", "contents": "Title: Reducing Runtime by Recycling Samples Abstract: Contrary to the situation with stochastic gradient descent, we argue that\nwhen using stochastic methods with variance reduction, such as SDCA, SAG or\nSVRG, as well as their variants, it could be beneficial to reuse previously\nused samples instead of fresh samples, even when fresh samples are available.\nWe demonstrate this empirically for SDCA, SAG and SVRG, studying the optimal\nsample size one should use, and also uncover be-havior that suggests running\nSDCA for an integer number of epochs could be wasteful. \n\n"}
{"id": "1602.02191", "contents": "Title: Convex Relaxation Regression: Black-Box Optimization of Smooth Functions\n  by Learning Their Convex Envelopes Abstract: Finding efficient and provable methods to solve non-convex optimization\nproblems is an outstanding challenge in machine learning and optimization\ntheory. A popular approach used to tackle non-convex problems is to use convex\nrelaxation techniques to find a convex surrogate for the problem.\nUnfortunately, convex relaxations typically must be found on a\nproblem-by-problem basis. Thus, providing a general-purpose strategy to\nestimate a convex relaxation would have a wide reaching impact. Here, we\nintroduce Convex Relaxation Regression (CoRR), an approach for learning convex\nrelaxations for a class of smooth functions. The main idea behind our approach\nis to estimate the convex envelope of a function $f$ by evaluating $f$ at a set\nof $T$ random points and then fitting a convex function to these function\nevaluations. We prove that with probability greater than $1-\\delta$, the\nsolution of our algorithm converges to the global optimizer of $f$ with error\n$\\mathcal{O} \\Big( \\big(\\frac{\\log(1/\\delta) }{T} \\big)^{\\alpha} \\Big)$ for\nsome $\\alpha> 0$. Our approach enables the use of convex optimization tools to\nsolve a class of non-convex optimization problems. \n\n"}
{"id": "1602.02350", "contents": "Title: Solving Ridge Regression using Sketched Preconditioned SVRG Abstract: We develop a novel preconditioning method for ridge regression, based on\nrecent linear sketching methods. By equipping Stochastic Variance Reduced\nGradient (SVRG) with this preconditioning process, we obtain a significant\nspeed-up relative to fast stochastic methods such as SVRG, SDCA and SAG. \n\n"}
{"id": "1602.02386", "contents": "Title: Network Inference by Learned Node-Specific Degree Prior Abstract: We propose a novel method for network inference from partially observed edges\nusing a node-specific degree prior. The degree prior is derived from observed\nedges in the network to be inferred, and its hyper-parameters are determined by\ncross validation. Then we formulate network inference as a matrix completion\nproblem regularized by our degree prior. Our theoretical analysis indicates\nthat this prior favors a network following the learned degree distribution, and\nmay lead to improved network recovery error bound than previous work.\nExperimental results on both simulated and real biological networks demonstrate\nthe superior performance of our method in various settings. \n\n"}
{"id": "1602.02899", "contents": "Title: Secure Multi-Party Computation Based Privacy Preserving Extreme Learning\n  Machine Algorithm Over Vertically Distributed Data Abstract: Especially in the Big Data era, the usage of different classification methods\nis increasing day by day. The success of these classification methods depends\non the effectiveness of learning methods. Extreme learning machine (ELM)\nclassification algorithm is a relatively new learning method built on\nfeed-forward neural-network. ELM classification algorithm is a simple and fast\nmethod that can create a model from high-dimensional data sets. Traditional ELM\nlearning algorithm implicitly assumes complete access to whole data set. This\nis a major privacy concern in most of cases. Sharing of private data (i.e.\nmedical records) is prevented because of security concerns. In this research,\nwe propose an efficient and secure privacy-preserving learning algorithm for\nELM classification over data that is vertically partitioned among several\nparties. The new learning method preserves the privacy on numerical attributes,\nbuilds a classification model without sharing private data without disclosing\nthe data of each party to others. \n\n"}
{"id": "1602.03220", "contents": "Title: Discriminative Regularization for Generative Models Abstract: We explore the question of whether the representations learned by classifiers\ncan be used to enhance the quality of generative models. Our conjecture is that\nlabels correspond to characteristics of natural data which are most salient to\nhumans: identity in faces, objects in images, and utterances in speech. We\npropose to take advantage of this by using the representations from\ndiscriminative classifiers to augment the objective function corresponding to a\ngenerative model. In particular we enhance the objective function of the\nvariational autoencoder, a popular generative model, with a discriminative\nregularization term. We show that enhancing the objective function in this way\nleads to samples that are clearer and have higher visual quality than the\nsamples from the standard variational autoencoders. \n\n"}
{"id": "1602.03534", "contents": "Title: Unsupervised Transductive Domain Adaptation Abstract: Supervised learning with large scale labeled datasets and deep layered models\nhas made a paradigm shift in diverse areas in learning and recognition.\nHowever, this approach still suffers generalization issues under the presence\nof a domain shift between the training and the test data distribution. In this\nregard, unsupervised domain adaptation algorithms have been proposed to\ndirectly address the domain shift problem. In this paper, we approach the\nproblem from a transductive perspective. We incorporate the domain shift and\nthe transductive target inference into our framework by jointly solving for an\nasymmetric similarity metric and the optimal transductive target label\nassignment. We also show that our model can easily be extended for deep feature\nlearning in order to learn features which are discriminative in the target\ndomain. Our experiments show that the proposed method significantly outperforms\nstate-of-the-art algorithms in both object recognition and digit classification\nexperiments by a large margin. \n\n"}
{"id": "1602.04133", "contents": "Title: Deep Gaussian Processes for Regression using Approximate Expectation\n  Propagation Abstract: Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations\nof Gaussian processes (GPs) and are formally equivalent to neural networks with\nmultiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic\nmodels and as such are arguably more flexible, have a greater capacity to\ngeneralise, and provide better calibrated uncertainty estimates than\nalternative deep models. This paper develops a new approximate Bayesian\nlearning scheme that enables DGPs to be applied to a range of medium to large\nscale regression problems for the first time. The new method uses an\napproximate Expectation Propagation procedure and a novel and efficient\nextension of the probabilistic backpropagation algorithm for learning. We\nevaluate the new method for non-linear regression on eleven real-world\ndatasets, showing that it always outperforms GP regression and is almost always\nbetter than state-of-the-art deterministic and sampling-based approximate\ninference methods for Bayesian neural networks. As a by-product, this work\nprovides a comprehensive analysis of six approximate Bayesian methods for\ntraining neural networks. \n\n"}
{"id": "1602.04398", "contents": "Title: Joint Dimensionality Reduction for Two Feature Vectors Abstract: Many machine learning problems, especially multi-modal learning problems,\nhave two sets of distinct features (e.g., image and text features in news story\nclassification, or neuroimaging data and neurocognitive data in cognitive\nscience research). This paper addresses the joint dimensionality reduction of\ntwo feature vectors in supervised learning problems. In particular, we assume a\ndiscriminative model where low-dimensional linear embeddings of the two feature\nvectors are sufficient statistics for predicting a dependent variable. We show\nthat a simple algorithm involving singular value decomposition can accurately\nestimate the embeddings provided that certain sample complexities are\nsatisfied, without specifying the nonlinear link function (regressor or\nclassifier). The main results establish sample complexities under multiple\nsettings. Sample complexities for different link functions only differ by\nconstant factors. \n\n"}
{"id": "1602.05205", "contents": "Title: Primal-Dual Rates and Certificates Abstract: We propose an algorithm-independent framework to equip existing optimization\nmethods with primal-dual certificates. Such certificates and corresponding rate\nof convergence guarantees are important for practitioners to diagnose progress,\nin particular in machine learning applications. We obtain new primal-dual\nconvergence rates, e.g., for the Lasso as well as many L1, Elastic Net, group\nLasso and TV-regularized problems. The theory applies to any norm-regularized\ngeneralized linear model. Our approach provides efficiently computable duality\ngaps which are globally defined, without modifying the original problems in the\nregion of interest. \n\n"}
{"id": "1602.06346", "contents": "Title: Policy Error Bounds for Model-Based Reinforcement Learning with Factored\n  Linear Models Abstract: In this paper we study a model-based approach to calculating approximately\noptimal policies in Markovian Decision Processes. In particular, we derive\nnovel bounds on the loss of using a policy derived from a factored linear\nmodel, a class of models which generalize numerous previous models out of those\nthat come with strong computational guarantees. For the first time in the\nliterature, we derive performance bounds for model-based techniques where the\nmodel inaccuracy is measured in weighted norms. Moreover, our bounds show a\ndecreased sensitivity to the discount factor and, unlike similar bounds derived\nfor other approaches, they are insensitive to measure mismatch. Similarly to\nprevious works, our proofs are also based on contraction arguments, but with\nthe main differences that we use carefully constructed norms building on Banach\nlattices, and the contraction property is only assumed for operators acting on\n\"compressed\" spaces, thus weakening previous assumptions, while strengthening\nprevious results. \n\n"}
{"id": "1602.07754", "contents": "Title: A Compressed Sensing Based Decomposition of Electrodermal Activity\n  Signals Abstract: The measurement and analysis of Electrodermal Activity (EDA) offers\napplications in diverse areas ranging from market research, to seizure\ndetection, to human stress analysis. Unfortunately, the analysis of EDA signals\nis made difficult by the superposition of numerous components which can obscure\nthe signal information related to a user's response to a stimulus. We show how\nsimple pre-processing followed by a novel compressed sensing based\ndecomposition can mitigate the effects of the undesired noise components and\nhelp reveal the underlying physiological signal. The proposed framework allows\nfor decomposition of EDA signals with provable bounds on the recovery of user\nresponses. We test our procedure on both synthetic and real-world EDA signals\nfrom wearable sensors and demonstrate that our approach allows for more\naccurate recovery of user responses as compared to the existing techniques. \n\n"}
{"id": "1603.05642", "contents": "Title: Optimal Black-Box Reductions Between Optimization Objectives Abstract: The diverse world of machine learning applications has given rise to a\nplethora of algorithms and optimization methods, finely tuned to the specific\nregression or classification task at hand. We reduce the complexity of\nalgorithm design for machine learning by reductions: we develop reductions that\ntake a method developed for one setting and apply it to the entire spectrum of\nsmoothness and strong-convexity in applications.\n  Furthermore, unlike existing results, our new reductions are OPTIMAL and more\nPRACTICAL. We show how these new reductions give rise to new and faster running\ntimes on training linear classifiers for various families of loss functions,\nand conclude with experiments showing their successes also in practice. \n\n"}
{"id": "1603.07285", "contents": "Title: A guide to convolution arithmetic for deep learning Abstract: We introduce a guide to help deep learning practitioners understand and\nmanipulate convolutional neural network architectures. The guide clarifies the\nrelationship between various properties (input shape, kernel shape, zero\npadding, strides and output shape) of convolutional, pooling and transposed\nconvolutional layers, as well as the relationship between convolutional and\ntransposed convolutional layers. Relationships are derived for various cases,\nand are illustrated in order to make them intuitive. \n\n"}
{"id": "1603.08212", "contents": "Title: Human Pose Estimation using Deep Consensus Voting Abstract: In this paper we consider the problem of human pose estimation from a single\nstill image. We propose a novel approach where each location in the image votes\nfor the position of each keypoint using a convolutional neural net. The voting\nscheme allows us to utilize information from the whole image, rather than rely\non a sparse set of keypoint locations. Using dense, multi-target votes, not\nonly produces good keypoint predictions, but also enables us to compute\nimage-dependent joint keypoint probabilities by looking at consensus voting.\nThis differs from most previous methods where joint probabilities are learned\nfrom relative keypoint locations and are independent of the image. We finally\ncombine the keypoints votes and joint probabilities in order to identify the\noptimal pose configuration. We show our competitive performance on the MPII\nHuman Pose and Leeds Sports Pose datasets. \n\n"}
{"id": "1603.08704", "contents": "Title: Interpretability of Multivariate Brain Maps in Brain Decoding:\n  Definition and Quantification Abstract: Brain decoding is a popular multivariate approach for hypothesis testing in\nneuroimaging. It is well known that the brain maps derived from weights of\nlinear classifiers are hard to interpret because of high correlations between\npredictors, low signal to noise ratios, and the high dimensionality of\nneuroimaging data. Therefore, improving the interpretability of brain decoding\napproaches is of primary interest in many neuroimaging studies. Despite\nextensive studies of this type, at present, there is no formal definition for\ninterpretability of multivariate brain maps. As a consequence, there is no\nquantitative measure for evaluating the interpretability of different brain\ndecoding methods. In this paper, first, we present a theoretical definition of\ninterpretability in brain decoding; we show that the interpretability of\nmultivariate brain maps can be decomposed into their reproducibility and\nrepresentativeness. Second, as an application of the proposed theoretical\ndefinition, we formalize a heuristic method for approximating the\ninterpretability of multivariate brain maps in a binary magnetoencephalography\n(MEG) decoding scenario. Third, we propose to combine the approximated\ninterpretability and the performance of the brain decoding model into a new\nmulti-objective criterion for model selection. Our results for the MEG data\nshow that optimizing the hyper-parameters of the regularized linear classifier\nbased on the proposed criterion results in more informative multivariate brain\nmaps. More importantly, the presented definition provides the theoretical\nbackground for quantitative evaluation of interpretability, and hence,\nfacilitates the development of more effective brain decoding algorithms in the\nfuture. \n\n"}
{"id": "1603.09630", "contents": "Title: Differentiable Pooling for Unsupervised Acoustic Model Adaptation Abstract: We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches. \n\n"}
{"id": "1604.04812", "contents": "Title: Structured Sparse Convolutional Autoencoder Abstract: This paper aims to improve the feature learning in Convolutional Networks\n(Convnet) by capturing the structure of objects. A new sparsity function is\nimposed on the extracted featuremap to capture the structure and shape of the\nlearned object, extracting interpretable features to improve the prediction\nperformance. The proposed algorithm is based on organizing the activation\nwithin and across featuremap by constraining the node activities through\n$\\ell_{2}$ and $\\ell_{1}$ normalization in a structured form. \n\n"}
{"id": "1604.04970", "contents": "Title: Deep Aesthetic Quality Assessment with Semantic Information Abstract: Human beings often assess the aesthetic quality of an image coupled with the\nidentification of the image's semantic content. This paper addresses the\ncorrelation issue between automatic aesthetic quality assessment and semantic\nrecognition. We cast the assessment problem as the main task among a multi-task\ndeep model, and argue that semantic recognition task offers the key to address\nthis problem. Based on convolutional neural networks, we employ a single and\nsimple multi-task framework to efficiently utilize the supervision of aesthetic\nand semantic labels. A correlation item between these two tasks is further\nintroduced to the framework by incorporating the inter-task relationship\nlearning. This item not only provides some useful insight about the correlation\nbut also improves assessment accuracy of the aesthetic task. Particularly, an\neffective strategy is developed to keep a balance between the two tasks, which\nfacilitates to optimize the parameters of the framework. Extensive experiments\non the challenging AVA dataset and Photo.net dataset validate the importance of\nsemantic recognition in aesthetic quality assessment, and demonstrate that\nmulti-task deep models can discover an effective aesthetic representation to\nachieve state-of-the-art results. \n\n"}
{"id": "1604.05091", "contents": "Title: End-to-End Tracking and Semantic Segmentation Using Recurrent Neural\n  Networks Abstract: In this work we present a novel end-to-end framework for tracking and\nclassifying a robot's surroundings in complex, dynamic and only partially\nobservable real-world environments. The approach deploys a recurrent neural\nnetwork to filter an input stream of raw laser measurements in order to\ndirectly infer object locations, along with their identity in both visible and\noccluded areas. To achieve this we first train the network using unsupervised\nDeep Tracking, a recently proposed theoretical framework for end-to-end space\noccupancy prediction. We show that by learning to track on a large amount of\nunsupervised data, the network creates a rich internal representation of its\nenvironment which we in turn exploit through the principle of inductive\ntransfer of knowledge to perform the task of it's semantic classification. As a\nresult, we show that only a small amount of labelled data suffices to steer the\nnetwork towards mastering this additional task. Furthermore we propose a novel\nrecurrent neural network architecture specifically tailored to tracking and\nsemantic classification in real-world robotics applications. We demonstrate the\ntracking and classification performance of the method on real-world data\ncollected at a busy road junction. Our evaluation shows that the proposed\nend-to-end framework compares favourably to a state-of-the-art, model-free\ntracking solution and that it outperforms a conventional one-shot training\nscheme for semantic classification. \n\n"}
{"id": "1604.05449", "contents": "Title: Streaming Label Learning for Modeling Labels on the Fly Abstract: It is challenging to handle a large volume of labels in multi-label learning.\nHowever, existing approaches explicitly or implicitly assume that all the\nlabels in the learning process are given, which could be easily violated in\nchanging environments. In this paper, we define and study streaming label\nlearning (SLL), i.e., labels are arrived on the fly, to model newly arrived\nlabels with the help of the knowledge learned from past labels. The core of SLL\nis to explore and exploit the relationships between new labels and past labels\nand then inherit the relationship into hypotheses of labels to boost the\nperformance of new classifiers. In specific, we use the label\nself-representation to model the label relationship, and SLL will be divided\ninto two steps: a regression problem and a empirical risk minimization (ERM)\nproblem. Both problems are simple and can be efficiently solved. We further\nshow that SLL can generate a tighter generalization error bound for new labels\nthan the general ERM framework with trace norm or Frobenius norm\nregularization. Finally, we implement extensive experiments on various\nbenchmark datasets to validate the new setting. And results show that SLL can\neffectively handle the constantly emerging new labels and provides excellent\nclassification performance. \n\n"}
{"id": "1604.05819", "contents": "Title: Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive\n  Models Abstract: Predictive models are finding an increasing number of applications in many\nindustries. As a result, a practical means for trading-off the cost of\ndeploying a model versus its effectiveness is needed. Our work is motivated by\nrisk prediction problems in healthcare. Cost-structures in domains such as\nhealthcare are quite complex, posing a significant challenge to existing\napproaches. We propose a novel framework for designing cost-sensitive\nstructured regularizers that is suitable for problems with complex cost\ndependencies. We draw upon a surprising connection to boolean circuits. In\nparticular, we represent the problem costs as a multi-layer boolean circuit,\nand then use properties of boolean circuits to define an extended feature\nvector and a group regularizer that exactly captures the underlying cost\nstructure. The resulting regularizer may then be combined with a fidelity\nfunction to perform model prediction, for example. For the challenging\nreal-world application of risk prediction for sepsis in intensive care units,\nthe use of our regularizer leads to models that are in harmony with the\nunderlying cost structure and thus provide an excellent prediction accuracy\nversus cost tradeoff. \n\n"}
{"id": "1604.07356", "contents": "Title: Fast nonlinear embeddings via structured matrices Abstract: We present a new paradigm for speeding up randomized computations of several\nfrequently used functions in machine learning. In particular, our paradigm can\nbe applied for improving computations of kernels based on random embeddings.\nAbove that, the presented framework covers multivariate randomized functions.\nAs a byproduct, we propose an algorithmic approach that also leads to a\nsignificant reduction of space complexity. Our method is based on careful\nrecycling of Gaussian vectors into structured matrices that share properties of\nfully random matrices. The quality of the proposed structured approach follows\nfrom combinatorial properties of the graphs encoding correlations between rows\nof these structured matrices. Our framework covers as special cases already\nknown structured approaches such as the Fast Johnson-Lindenstrauss Transform,\nbut is much more general since it can be applied also to highly nonlinear\nembeddings. We provide strong concentration results showing the quality of the\npresented paradigm. \n\n"}
{"id": "1605.00057", "contents": "Title: Distributed Cell Association for Energy Harvesting IoT Devices in Dense\n  Small Cell Networks: A Mean-Field Multi-Armed Bandit Approach Abstract: The emerging Internet of Things (IoT)-driven ultra-dense small cell networks\n(UD-SCNs) will need to combat a variety of challenges. On one hand, massive\nnumber of devices sharing the limited wireless resources will render\ncentralized control mechanisms infeasible due to the excessive cost of\ninformation acquisition and computations. On the other hand, to reduce energy\nconsumption from fixed power grid and/or battery, many IoT devices may need to\ndepend on the energy harvested from the ambient environment (e.g., from RF\ntransmissions, environmental sources). However, due to the opportunistic nature\nof energy harvesting, this will introduce uncertainty in the network operation.\nIn this article, we study the distributed cell association problem for energy\nharvesting IoT devices in UD-SCNs. After reviewing the state-of-the-art\nresearch on the cell association problem in small cell networks, we outline the\nmajor challenges for distributed cell association in IoT-driven UD-SCNs where\nthe IoT devices will need to perform cell association in a distributed manner\nin presence of uncertainty (e.g., limited knowledge on channel/network) and\nlimited computational capabilities. To this end, we propose an approach based\non mean-field multi-armed bandit games to solve the uplink cell association\nproblem for energy harvesting IoT devices in a UD-SCN. This approach is\nparticularly suitable to analyze large multi-agent systems under uncertainty\nand lack of information. We provide some theoretical results as well as\npreliminary performance evaluation results for the proposed approach. \n\n"}
{"id": "1605.01107", "contents": "Title: Decentralized Dynamic Discriminative Dictionary Learning Abstract: We consider discriminative dictionary learning in a distributed online\nsetting, where a network of agents aims to learn a common set of dictionary\nelements of a feature space and model parameters while sequentially receiving\nobservations. We formulate this problem as a distributed stochastic program\nwith a non-convex objective and present a block variant of the Arrow-Hurwicz\nsaddle point algorithm to solve it. Using Lagrange multipliers to penalize the\ndiscrepancy between them, only neighboring nodes exchange model information. We\nshow that decisions made with this saddle point algorithm asymptotically\nachieve a first-order stationarity condition on average. \n\n"}
{"id": "1605.01116", "contents": "Title: An evaluation of randomized machine learning methods for redundant data:\n  Predicting short and medium-term suicide risk from administrative records and\n  risk assessments Abstract: Accurate prediction of suicide risk in mental health patients remains an open\nproblem. Existing methods including clinician judgments have acceptable\nsensitivity, but yield many false positives. Exploiting administrative data has\na great potential, but the data has high dimensionality and redundancies in the\nrecording processes. We investigate the efficacy of three most effective\nrandomized machine learning techniques random forests, gradient boosting\nmachines, and deep neural nets with dropout in predicting suicide risk. Using a\ncohort of mental health patients from a regional Australian hospital, we\ncompare the predictive performance with popular traditional approaches\nclinician judgments based on a checklist, sparse logistic regression and\ndecision trees. The randomized methods demonstrated robustness against data\nredundancies and superior predictive performance on AUC and F-measure. \n\n"}
{"id": "1605.03795", "contents": "Title: Exponential Machines Abstract: Modeling interactions between features improves the performance of machine\nlearning solutions in many domains (e.g. recommender systems or sentiment\nanalysis). In this paper, we introduce Exponential Machines (ExM), a predictor\nthat models all interactions of every order. The key idea is to represent an\nexponentially large tensor of parameters in a factorized format called Tensor\nTrain (TT). The Tensor Train format regularizes the model and lets you control\nthe number of underlying parameters. To train the model, we develop a\nstochastic Riemannian optimization procedure, which allows us to fit tensors\nwith 2^160 entries. We show that the model achieves state-of-the-art\nperformance on synthetic data with high-order interactions and that it works on\npar with high-order factorization machines on a recommender system dataset\nMovieLens 100K. \n\n"}
{"id": "1605.06561", "contents": "Title: DynaNewton - Accelerating Newton's Method for Machine Learning Abstract: Newton's method is a fundamental technique in optimization with quadratic\nconvergence within a neighborhood around the optimum. However reaching this\nneighborhood is often slow and dominates the computational costs. We exploit\ntwo properties specific to empirical risk minimization problems to accelerate\nNewton's method, namely, subsampling training data and increasing strong\nconvexity through regularization. We propose a novel continuation method, where\nwe define a family of objectives over increasing sample sizes and with\ndecreasing regularization strength. Solutions on this path are tracked such\nthat the minimizer of the previous objective is guaranteed to be within the\nquadratic convergence region of the next objective to be optimized. Thereby\nevery Newton iteration is guaranteed to achieve super-linear contractions with\nregard to the chosen objective, which becomes a moving target. We provide a\ntheoretical analysis that motivates our algorithm, called DynaNewton, and\ncharacterizes its speed of convergence. Experiments on a wide range of data\nsets and problems consistently confirm the predicted computational savings. \n\n"}
{"id": "1605.07057", "contents": "Title: Bayesian Model Selection of Stochastic Block Models Abstract: A central problem in analyzing networks is partitioning them into modules or\ncommunities. One of the best tools for this is the stochastic block model,\nwhich clusters vertices into blocks with statistically homogeneous pattern of\nlinks. Despite its flexibility and popularity, there has been a lack of\nprincipled statistical model selection criteria for the stochastic block model.\nHere we propose a Bayesian framework for choosing the number of blocks as well\nas comparing it to the more elaborate degree- corrected block models,\nultimately leading to a universal model selection framework capable of\ncomparing multiple modeling combinations. We will also investigate its\nconnection to the minimum description length principle. \n\n"}
{"id": "1605.07127", "contents": "Title: Learning and Policy Search in Stochastic Dynamical Systems with Bayesian\n  Neural Networks Abstract: We present an algorithm for model-based reinforcement learning that combines\nBayesian neural networks (BNNs) with random roll-outs and stochastic\noptimization for policy learning. The BNNs are trained by minimizing\n$\\alpha$-divergences, allowing us to capture complicated statistical patterns\nin the transition dynamics, e.g. multi-modality and heteroskedasticity, which\nare usually missed by other common modeling approaches. We illustrate the\nperformance of our method by solving a challenging benchmark where model-based\napproaches usually fail and by obtaining promising results in a real-world\nscenario for controlling a gas turbine. \n\n"}
{"id": "1605.07571", "contents": "Title: Sequential Neural Models with Stochastic Layers Abstract: How can we efficiently propagate uncertainty in a latent state representation\nwith recurrent neural networks? This paper introduces stochastic recurrent\nneural networks which glue a deterministic recurrent neural network and a state\nspace model together to form a stochastic and sequential neural generative\nmodel. The clear separation of deterministic and stochastic layers allows a\nstructured variational inference network to track the factorization of the\nmodel's posterior distribution. By retaining both the nonlinear recursive\nstructure of a recurrent neural network and averaging over the uncertainty in a\nlatent path, like a state space model, we improve the state of the art results\non the Blizzard and TIMIT speech modeling data sets by a large margin, while\nachieving comparable performances to competing methods on polyphonic music\nmodeling. \n\n"}
{"id": "1605.07583", "contents": "Title: Recursive Sampling for the Nystr\\\"om Method Abstract: We give the first algorithm for kernel Nystr\\\"om approximation that runs in\n*linear time in the number of training points* and is provably accurate for all\nkernel matrices, without dependence on regularity or incoherence conditions.\nThe algorithm projects the kernel onto a set of $s$ landmark points sampled by\ntheir *ridge leverage scores*, requiring just $O(ns)$ kernel evaluations and\n$O(ns^2)$ additional runtime. While leverage score sampling has long been known\nto give strong theoretical guarantees for Nystr\\\"om approximation, by employing\na fast recursive sampling scheme, our algorithm is the first to make the\napproach scalable. Empirically we show that it finds more accurate, lower rank\nkernel approximations in less time than popular techniques such as uniformly\nsampled Nystr\\\"om approximation and the random Fourier features method. \n\n"}
{"id": "1605.07719", "contents": "Title: Reshaped Wirtinger Flow and Incremental Algorithm for Solving Quadratic\n  System of Equations Abstract: We study the phase retrieval problem, which solves quadratic system of\nequations, i.e., recovers a vector $\\boldsymbol{x}\\in \\mathbb{R}^n$ from its\nmagnitude measurements $y_i=|\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\\rangle|,\ni=1,..., m$. We develop a gradient-like algorithm (referred to as RWF\nrepresenting reshaped Wirtinger flow) by minimizing a nonconvex nonsmooth loss\nfunction. In comparison with existing nonconvex Wirtinger flow (WF) algorithm\n\\cite{candes2015phase}, although the loss function becomes nonsmooth, it\ninvolves only the second power of variable and hence reduces the complexity. We\nshow that for random Gaussian measurements, RWF enjoys geometric convergence to\na global optimal point as long as the number $m$ of measurements is on the\norder of $n$, the dimension of the unknown $\\boldsymbol{x}$. This improves the\nsample complexity of WF, and achieves the same sample complexity as truncated\nWirtinger flow (TWF) \\cite{chen2015solving}, but without truncation in gradient\nloop. Furthermore, RWF costs less computationally than WF, and runs faster\nnumerically than both WF and TWF. We further develop the incremental\n(stochastic) reshaped Wirtinger flow (IRWF) and show that IRWF converges\nlinearly to the true signal. We further establish performance guarantee of an\nexisting Kaczmarz method for the phase retrieval problem based on its\nconnection to IRWF. We also empirically demonstrate that IRWF outperforms\nexisting ITWF algorithm (stochastic version of TWF) as well as other batch\nalgorithms. \n\n"}
{"id": "1605.08298", "contents": "Title: Entanglement in Algebraic Quantum Mechanics: Majorana fermion systems Abstract: Many-body entanglement is studied within the algebraic approach to quantum\nphysics in systems made of Majorana fermions. In this framework, the notion of\nseparability stems from partitions of the algebra of observables and properties\nof the associated correlation functions, rather than on particle tensor\nproducts. This allows obtaining a complete characterization of non-separable\nMajorana fermion states. These results may find direct applications in quantum\nmetrology: using Majorana systems, sub-shot noise accuracy in parameter\nestimations can be achieved without preliminary, resource consuming, state\nentanglement operations. \n\n"}
{"id": "1605.08501", "contents": "Title: Local Region Sparse Learning for Image-on-Scalar Regression Abstract: Identification of regions of interest (ROI) associated with certain disease\nhas a great impact on public health. Imposing sparsity of pixel values and\nextracting active regions simultaneously greatly complicate the image analysis.\nWe address these challenges by introducing a novel region-selection penalty in\nthe framework of image-on-scalar regression. Our penalty combines the Smoothly\nClipped Absolute Deviation (SCAD) regularization, enforcing sparsity, and the\nSCAD of total variation (TV) regularization, enforcing spatial contiguity, into\none group, which segments contiguous spatial regions against zero-valued\nbackground. Efficient algorithm is based on the alternative direction method of\nmultipliers (ADMM) which decomposes the non-convex problem into two iterative\noptimization problems with explicit solutions. Another virtue of the proposed\nmethod is that a divide and conquer learning algorithm is developed, thereby\nallowing scaling to large images. Several examples are presented and the\nexperimental results are compared with other state-of-the-art approaches. \n\n"}
{"id": "1605.08872", "contents": "Title: Online Bayesian Collaborative Topic Regression Abstract: Collaborative Topic Regression (CTR) combines ideas of probabilistic matrix\nfactorization (PMF) and topic modeling (e.g., LDA) for recommender systems,\nwhich has gained increasing successes in many applications. Despite enjoying\nmany advantages, the existing CTR algorithms have some critical limitations.\nFirst of all, they are often designed to work in a batch learning manner,\nmaking them unsuitable to deal with streaming data or big data in real-world\nrecommender systems. Second, the document-specific topic proportions of LDA are\nfed to the downstream PMF, but not reverse, which is sub-optimal as the rating\ninformation is not exploited in discovering the low-dimensional representation\nof documents and thus can result in a sub-optimal representation for\nprediction. In this paper, we propose a novel scheme of Online Bayesian\nCollaborative Topic Regression (OBCTR) which is efficient and scalable for\nlearning from data streams. Particularly, we {\\it jointly} optimize the\ncombined objective function of both PMF and LDA in an online learning fashion,\nin which both PMF and LDA tasks can be reinforced each other during the online\nlearning process. Our encouraging experimental results on real-world data\nvalidate the effectiveness of the proposed method. \n\n"}
{"id": "1605.08882", "contents": "Title: Optimal Rates for Multi-pass Stochastic Gradient Methods Abstract: We analyze the learning properties of the stochastic gradient method when\nmultiple passes over the data and mini-batches are allowed. We study how\nregularization properties are controlled by the step-size, the number of passes\nand the mini-batch size. In particular, we consider the square loss and show\nthat for a universal step-size choice, the number of passes acts as a\nregularization parameter, and optimal finite sample bounds can be achieved by\nearly-stopping. Moreover, we show that larger step-sizes are allowed when\nconsidering mini-batches. Our analysis is based on a unifying approach,\nencompassing both batch and stochastic gradient methods as special cases. As a\nbyproduct, we derive optimal convergence results for batch gradient methods\n(even in the non-attainable cases). \n\n"}
{"id": "1606.00119", "contents": "Title: Contextual Bandits with Latent Confounders: An NMF Approach Abstract: Motivated by online recommendation and advertising systems, we consider a\ncausal model for stochastic contextual bandits with a latent low-dimensional\nconfounder. In our model, there are $L$ observed contexts and $K$ arms of the\nbandit. The observed context influences the reward obtained through a latent\nconfounder variable with cardinality $m$ ($m \\ll L,K$). The arm choice and the\nlatent confounder causally determines the reward while the observed context is\ncorrelated with the confounder. Under this model, the $L \\times K$ mean reward\nmatrix $\\mathbf{U}$ (for each context in $[L]$ and each arm in $[K]$)\nfactorizes into non-negative factors $\\mathbf{A}$ ($L \\times m$) and\n$\\mathbf{W}$ ($m \\times K$). This insight enables us to propose an\n$\\epsilon$-greedy NMF-Bandit algorithm that designs a sequence of interventions\n(selecting specific arms), that achieves a balance between learning this\nlow-dimensional structure and selecting the best arm to minimize regret. Our\nalgorithm achieves a regret of $\\mathcal{O}\\left(L\\mathrm{poly}(m, \\log K) \\log\nT \\right)$ at time $T$, as compared to $\\mathcal{O}(LK\\log T)$ for conventional\ncontextual bandits, assuming a constant gap between the best arm and the rest\nfor each context. These guarantees are obtained under mild sufficiency\nconditions on the factors that are weaker versions of the well-known\nStatistical RIP condition. We further propose a class of generative models that\nsatisfy our sufficient conditions, and derive a lower bound of\n$\\mathcal{O}\\left(Km\\log T\\right)$. These are the first regret guarantees for\nonline matrix completion with bandit feedback, when the rank is greater than\none. We further compare the performance of our algorithm with the state of the\nart, on synthetic and real world data-sets. \n\n"}
{"id": "1606.00136", "contents": "Title: Efficiently Bounding Optimal Solutions after Small Data Modification in\n  Large-Scale Empirical Risk Minimization Abstract: We study large-scale classification problems in changing environments where a\nsmall part of the dataset is modified, and the effect of the data modification\nmust be quickly incorporated into the classifier. When the entire dataset is\nlarge, even if the amount of the data modification is fairly small, the\ncomputational cost of re-training the classifier would be prohibitively large.\nIn this paper, we propose a novel method for efficiently incorporating such a\ndata modification effect into the classifier without actually re-training it.\nThe proposed method provides bounds on the unknown optimal classifier with the\ncost only proportional to the size of the data modification. We demonstrate\nthrough numerical experiments that the proposed method provides sufficiently\ntight bounds with negligible computational costs, especially when a small part\nof the dataset is modified in a large-scale classification problem. \n\n"}
{"id": "1606.00389", "contents": "Title: Stream Clipper: Scalable Submodular Maximization on Stream Abstract: We propose a streaming submodular maximization algorithm \"stream clipper\"\nthat performs as well as the offline greedy algorithm on document/video\nsummarization in practice. It adds elements from a stream either to a solution\nset $S$ or to an extra buffer $B$ based on two adaptive thresholds, and\nimproves $S$ by a final greedy step that starts from $S$ adding elements from\n$B$. During this process, swapping elements out of $S$ can occur if doing so\nyields improvements. The thresholds adapt based on if current memory\nutilization exceeds a budget, e.g., it increases the lower threshold, and\nremoves from the buffer $B$ elements below the new lower threshold. We show\nthat, while our approximation factor in the worst case is $1/2$ (like in\nprevious work, and corresponding to the tight bound), we show that there are\ndata-dependent conditions where our bound falls within the range $[1/2,\n1-1/e]$. In news and video summarization experiments, the algorithm\nconsistently outperforms other streaming methods, and, while using\nsignificantly less computation and memory, performs similarly to the offline\ngreedy algorithm. \n\n"}
{"id": "1606.01549", "contents": "Title: Gated-Attention Readers for Text Comprehension Abstract: In this paper we study the problem of answering cloze-style questions over\ndocuments. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop\narchitecture with a novel attention mechanism, which is based on multiplicative\ninteractions between the query embedding and the intermediate states of a\nrecurrent neural network document reader. This enables the reader to build\nquery-specific representations of tokens in the document for accurate answer\nselection. The GA Reader obtains state-of-the-art results on three benchmarks\nfor this task--the CNN \\& Daily Mail news stories and the Who Did What dataset.\nThe effectiveness of multiplicative interaction is demonstrated by an ablation\nstudy, and by comparing to alternative compositional operators for implementing\nthe gated-attention. The code is available at\nhttps://github.com/bdhingra/ga-reader. \n\n"}
{"id": "1606.02702", "contents": "Title: Efficient Smoothed Concomitant Lasso Estimation for High Dimensional\n  Regression Abstract: In high dimensional settings, sparse structures are crucial for efficiency,\nboth in term of memory, computation and performance. It is customary to\nconsider $\\ell_1$ penalty to enforce sparsity in such scenarios. Sparsity\nenforcing methods, the Lasso being a canonical example, are popular candidates\nto address high dimension. For efficiency, they rely on tuning a parameter\ntrading data fitting versus sparsity. For the Lasso theory to hold this tuning\nparameter should be proportional to the noise level, yet the latter is often\nunknown in practice. A possible remedy is to jointly optimize over the\nregression parameter as well as over the noise level. This has been considered\nunder several names in the literature: Scaled-Lasso, Square-root Lasso,\nConcomitant Lasso estimation for instance, and could be of interest for\nconfidence sets or uncertainty quantification. In this work, after illustrating\nnumerical difficulties for the Smoothed Concomitant Lasso formulation, we\npropose a modification we coined Smoothed Concomitant Lasso, aimed at\nincreasing numerical stability. We propose an efficient and accurate solver\nleading to a computational cost no more expansive than the one for the Lasso.\nWe leverage on standard ingredients behind the success of fast Lasso solvers: a\ncoordinate descent algorithm, combined with safe screening rules to achieve\nspeed efficiency, by eliminating early irrelevant features. \n\n"}
{"id": "1606.03401", "contents": "Title: Memory-Efficient Backpropagation Through Time Abstract: We propose a novel approach to reduce memory consumption of the\nbackpropagation through time (BPTT) algorithm when training recurrent neural\nnetworks (RNNs). Our approach uses dynamic programming to balance a trade-off\nbetween caching of intermediate results and recomputation. The algorithm is\ncapable of tightly fitting within almost any user-set memory budget while\nfinding an optimal execution policy minimizing the computational cost.\nComputational devices have limited memory capacity and maximizing a\ncomputational performance given a fixed memory budget is a practical use-case.\nWe provide asymptotic computational upper bounds for various regimes. The\nalgorithm is particularly effective for long sequences. For sequences of length\n1000, our algorithm saves 95\\% of memory usage while using only one third more\ntime per iteration than the standard BPTT. \n\n"}
{"id": "1606.03498", "contents": "Title: Improved Techniques for Training GANs Abstract: We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes. \n\n"}
{"id": "1606.04155", "contents": "Title: Rationalizing Neural Predictions Abstract: Prediction without justification has limited applicability. As a remedy, we\nlearn to extract pieces of input text as justifications -- rationales -- that\nare tailored to be short and coherent, yet sufficient for making the same\nprediction. Our approach combines two modular components, generator and\nencoder, which are trained to operate well together. The generator specifies a\ndistribution over text fragments as candidate rationales and these are passed\nthrough the encoder for prediction. Rationales are never given during training.\nInstead, the model is regularized by desiderata for rationales. We evaluate the\napproach on multi-aspect sentiment analysis against manually annotated test\ncases. Our approach outperforms attention-based baseline by a significant\nmargin. We also successfully illustrate the method on the question retrieval\ntask. \n\n"}
{"id": "1606.06352", "contents": "Title: Visualizing textual models with in-text and word-as-pixel highlighting Abstract: We explore two techniques which use color to make sense of statistical text\nmodels. One method uses in-text annotations to illustrate a model's view of\nparticular tokens in particular documents. Another uses a high-level,\n\"words-as-pixels\" graphic to display an entire corpus. Together, these methods\noffer both zoomed-in and zoomed-out perspectives into a model's understanding\nof text. We show how these interconnected methods help diagnose a classifier's\npoor performance on Twitter slang, and make sense of a topic model on\nhistorical political texts. \n\n"}
{"id": "1606.06565", "contents": "Title: Concrete Problems in AI Safety Abstract: Rapid progress in machine learning and artificial intelligence (AI) has\nbrought increasing attention to the potential impacts of AI technologies on\nsociety. In this paper we discuss one such potential impact: the problem of\naccidents in machine learning systems, defined as unintended and harmful\nbehavior that may emerge from poor design of real-world AI systems. We present\na list of five practical research problems related to accident risk,\ncategorized according to whether the problem originates from having the wrong\nobjective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an\nobjective function that is too expensive to evaluate frequently (\"scalable\nsupervision\"), or undesirable behavior during the learning process (\"safe\nexploration\" and \"distributional shift\"). We review previous work in these\nareas as well as suggesting research directions with a focus on relevance to\ncutting-edge AI systems. Finally, we consider the high-level question of how to\nthink most productively about the safety of forward-looking applications of AI. \n\n"}
{"id": "1606.08061", "contents": "Title: Exact gradient updates in time independent of output size for the\n  spherical loss family Abstract: An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g.\n200,000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the $D \\times d$ output weight matrix and computing the gradient\nneeded for backpropagation to previous layers. While efficient handling of\nlarge sparse network inputs is trivial, the case of large sparse targets is\nnot, and has thus so far been sidestepped with approximate alternatives such as\nhierarchical softmax or sampling-based approximations during training. In this\nwork we develop an original algorithmic approach which, for a family of loss\nfunctions that includes squared error and spherical softmax, can compute the\nexact loss, gradient update for the output weights, and gradient for\nbackpropagation, all in $O(d^{2})$ per example instead of $O(Dd)$, remarkably\nwithout ever computing the D-dimensional output. The proposed algorithm yields\na speedup of up to $D/4d$ i.e. two orders of magnitude for typical sizes, for\nthat critical part of the computations that often dominates the training time\nin this kind of network architecture. \n\n"}
{"id": "1607.00669", "contents": "Title: Understanding the Energy and Precision Requirements for Online Learning Abstract: It is well-known that the precision of data, hyperparameters, and internal\nrepresentations employed in learning systems directly impacts its energy,\nthroughput, and latency. The precision requirements for the training algorithm\nare also important for systems that learn on-the-fly. Prior work has shown that\nthe data and hyperparameters can be quantized heavily without incurring much\npenalty in classification accuracy when compared to floating point\nimplementations. These works suffer from two key limitations. First, they\nassume uniform precision for the classifier and for the training algorithm and\nthus miss out on the opportunity to further reduce precision. Second, prior\nworks are empirical studies. In this article, we overcome both these\nlimitations by deriving analytical lower bounds on the precision requirements\nof the commonly employed stochastic gradient descent (SGD) on-line learning\nalgorithm in the specific context of a support vector machine (SVM). Lower\nbounds on the data precision are derived in terms of the the desired\nclassification accuracy and precision of the hyperparameters used in the\nclassifier. Additionally, lower bounds on the hyperparameter precision in the\nSGD training algorithm are obtained. These bounds are validated using both\nsynthetic and the UCI breast cancer dataset. Additionally, the impact of these\nprecisions on the energy consumption of a fixed-point SVM with on-line training\nis studied. \n\n"}
{"id": "1607.03204", "contents": "Title: Information Projection and Approximate Inference for Structured Sparse\n  Variables Abstract: Approximate inference via information projection has been recently introduced\nas a general-purpose approach for efficient probabilistic inference given\nsparse variables. This manuscript goes beyond classical sparsity by proposing\nefficient algorithms for approximate inference via information projection that\nare applicable to any structure on the set of variables that admits enumeration\nusing a \\emph{matroid}. We show that the resulting information projection can\nbe reduced to combinatorial submodular optimization subject to matroid\nconstraints. Further, leveraging recent advances in submodular optimization, we\nprovide an efficient greedy algorithm with strong optimization-theoretic\nguarantees. The class of probabilistic models that can be expressed in this way\nis quite broad and, as we show, includes group sparse regression, group sparse\nprincipal components analysis and sparse canonical correlation analysis, among\nothers. Moreover, empirical results on simulated data and high dimensional\nneuroimaging data highlight the superior performance of the information\nprojection approach as compared to established baselines for a range of\nprobabilistic models. \n\n"}
{"id": "1607.04234", "contents": "Title: Fundamental limits for cooling of linear quantum refrigerators Abstract: We study the asymptotic dynamics of arbitrary linear quantum open systems\nwhich are periodically driven while coupled with generic bosonic reservoirs. We\nobtain exact results for the heat flowing into the network, which are valid\nbeyond the usual weak coupling or Markovian approximations. We prove the\nvalidity of the dynamical third law of thermodynamics (Nernst unattainability\nprinciple), showing that the ultimate limit for cooling is imposed by a\nfundamental heating mechanism which becomes dominant at low temperatures: the\nnon resonant creation of pairs of excitations in the reservoirs induced by the\ndriving field. This quantum effect, which is missed in the weak coupling\napproximation, restores the unattainability principle whose validity was\nrecently challenged. \n\n"}
{"id": "1607.05002", "contents": "Title: Geometric Mean Metric Learning Abstract: We revisit the task of learning a Euclidean metric from data. We approach\nthis problem from first principles and formulate it as a surprisingly simple\noptimization problem. Indeed, our formulation even admits a closed form\nsolution. This solution possesses several very attractive properties: (i) an\ninnate geometric appeal through the Riemannian geometry of positive definite\nmatrices; (ii) ease of interpretability; and (iii) computational speed several\norders of magnitude faster than the widely used LMNN and ITML methods.\nFurthermore, on standard benchmark datasets, our closed-form solution\nconsistently attains higher classification accuracy. \n\n"}
{"id": "1607.06739", "contents": "Title: Exact steady state of a Kerr resonator with one- and two-photon driving\n  and dissipation: Controllable Wigner-function multimodality and dissipative\n  phase transitions Abstract: We present exact results for the steady-state density matrix of a general\nclass of driven-dissipative systems consisting of a nonlinear Kerr resonator in\nthe presence of both coherent (one-photon) and parametric (two-photon) driving\nand dissipation. Thanks to the analytical solution, obtained via the complex\nP-representation formalism, we are able to explore any regime, including photon\nblockade, multiphoton resonant effects, and a mesoscopic regime with large\nphoton density and quantum correlations. We show how the interplay between one-\nand two-photon driving provides a way to control the multimodality of the\nWigner function in regimes where the semiclassical theory exhibits\nmultistability. We also study the emergence of dissipative phase transitions in\nthe thermodynamic limit of large photon numbers. \n\n"}
{"id": "1607.07902", "contents": "Title: Ultra-high Q Acoustic Resonance in Superfluid 4He Abstract: We report the measurement of the acoustic quality factor of a gram-scale,\nkilo-hertz frequency superfluid resonator, detected through the parametric\ncoupling to a superconducting niobium microwave cavity. For temperature between\n400mK and 50mK, we observe a $T^{-4}$ temperature dependence of the quality\nfactor, consistent with a 3-phonon dissipation mechanism. We observe Q factors\nup to $1.4\\cdot10^8$, consistent with the dissipation due to dilute $^3$He\nimpurities, and expect that significant further improvements are possible.\nThese experiments are relevant to exploring quantum behavior and decoherence of\nmassive macroscopic objects, the laboratory detection of continuous wave\ngravitational waves from pulsars, and the probing of possible limits to\nphysical length scales. \n\n"}
{"id": "1607.08194", "contents": "Title: Convolutional Neural Networks Analyzed via Convolutional Sparse Coding Abstract: Convolutional neural networks (CNN) have led to many state-of-the-art results\nspanning through various fields. However, a clear and profound theoretical\nunderstanding of the forward pass, the core algorithm of CNN, is still lacking.\nIn parallel, within the wide field of sparse approximation, Convolutional\nSparse Coding (CSC) has gained increasing attention in recent years. A\ntheoretical study of this model was recently conducted, establishing it as a\nreliable and stable alternative to the commonly practiced patch-based\nprocessing. Herein, we propose a novel multi-layer model, ML-CSC, in which\nsignals are assumed to emerge from a cascade of CSC layers. This is shown to be\ntightly connected to CNN, so much so that the forward pass of the CNN is in\nfact the thresholding pursuit serving the ML-CSC model. This connection brings\na fresh view to CNN, as we are able to attribute to this architecture\ntheoretical claims such as uniqueness of the representations throughout the\nnetwork, and their stable estimation, all guaranteed under simple local\nsparsity conditions. Lastly, identifying the weaknesses in the above pursuit\nscheme, we propose an alternative to the forward pass, which is connected to\ndeconvolutional, recurrent and residual networks, and has better theoretical\nguarantees. \n\n"}
{"id": "1608.00027", "contents": "Title: gLOP: the global and Local Penalty for Capturing Predictive\n  Heterogeneity Abstract: When faced with a supervised learning problem, we hope to have rich enough\ndata to build a model that predicts future instances well. However, in\npractice, problems can exhibit predictive heterogeneity: most instances might\nbe relatively easy to predict, while others might be predictive outliers for\nwhich a model trained on the entire dataset does not perform well. Identifying\nthese can help focus future data collection. We present gLOP, the global and\nLocal Penalty, a framework for capturing predictive heterogeneity and\nidentifying predictive outliers. gLOP is based on penalized regression for\nmultitask learning, which improves learning by leveraging training signal\ninformation from related tasks. We give two optimization algorithms for gLOP,\none space-efficient, and another giving the full regularization path. We also\ncharacterize uniqueness in terms of the data and tuning parameters, and present\nempirical results on synthetic data and on two health research problems. \n\n"}
{"id": "1608.00159", "contents": "Title: Learning Tree-Structured Detection Cascades for Heterogeneous Networks\n  of Embedded Devices Abstract: In this paper, we present a new approach to learning cascaded classifiers for\nuse in computing environments that involve networks of heterogeneous and\nresource-constrained, low-power embedded compute and sensing nodes. We present\na generalization of the classical linear detection cascade to the case of\ntree-structured cascades where different branches of the tree execute on\ndifferent physical compute nodes in the network. Different nodes have access to\ndifferent features, as well as access to potentially different computation and\nenergy resources. We concentrate on the problem of jointly learning the\nparameters for all of the classifiers in the cascade given a fixed cascade\narchitecture and a known set of costs required to carry out the computation at\neach node.To accomplish the objective of joint learning of all detectors, we\npropose a novel approach to combining classifier outputs during training that\nbetter matches the hard cascade setting in which the learned system will be\ndeployed. This work is motivated by research in the area of mobile health where\nenergy efficient real time detectors integrating information from multiple\nwireless on-body sensors and a smart phone are needed for real-time monitoring\nand delivering just- in-time adaptive interventions. We apply our framework to\ntwo activity recognition datasets as well as the problem of cigarette smoking\ndetection from a combination of wrist-worn actigraphy data and respiration\nchest band data. \n\n"}
{"id": "1608.00778", "contents": "Title: Exponential Family Embeddings Abstract: Word embeddings are a powerful approach for capturing semantic similarity\namong terms in a vocabulary. In this paper, we develop exponential family\nembeddings, a class of methods that extends the idea of word embeddings to\nother types of high-dimensional data. As examples, we studied neural data with\nreal-valued observations, count data from a market basket analysis, and ratings\ndata from a movie recommendation system. The main idea is to model each\nobservation conditioned on a set of other observations. This set is called the\ncontext, and the way the context is defined is a modeling choice that depends\non the problem. In language the context is the surrounding words; in\nneuroscience the context is close-by neurons; in market basket data the context\nis other items in the shopping cart. Each type of embedding model defines the\ncontext, the exponential family of conditional distributions, and how the\nlatent embedding vectors are shared across data. We infer the embeddings with a\nscalable algorithm based on stochastic gradient descent. On all three\napplications - neural activity of zebrafish, users' shopping behavior, and\nmovie ratings - we found exponential family embedding models to be more\neffective than other types of dimension reduction. They better reconstruct\nheld-out data and find interesting qualitative structure. \n\n"}
{"id": "1608.01976", "contents": "Title: Kernel Ridge Regression via Partitioning Abstract: In this paper, we investigate a divide and conquer approach to Kernel Ridge\nRegression (KRR). Given n samples, the division step involves separating the\npoints based on some underlying disjoint partition of the input space (possibly\nvia clustering), and then computing a KRR estimate for each partition. The\nconquering step is simple: for each partition, we only consider its own local\nestimate for prediction. We establish conditions under which we can give\ngeneralization bounds for this estimator, as well as achieve optimal minimax\nrates. We also show that the approximation error component of the\ngeneralization error is lesser than when a single KRR estimate is fit on the\ndata: thus providing both statistical and computational advantages over a\nsingle KRR estimate over the entire data (or an averaging over random\npartitions as in other recent work, [30]). Lastly, we provide experimental\nvalidation for our proposed estimator and our assumptions. \n\n"}
{"id": "1608.02257", "contents": "Title: Robust High-Dimensional Linear Regression Abstract: The effectiveness of supervised learning techniques has made them ubiquitous\nin research and practice. In high-dimensional settings, supervised learning\ncommonly relies on dimensionality reduction to improve performance and identify\nthe most important factors in predicting outcomes. However, the economic\nimportance of learning has made it a natural target for adversarial\nmanipulation of training data, which we term poisoning attacks. Prior\napproaches to dealing with robust supervised learning rely on strong\nassumptions about the nature of the feature matrix, such as feature\nindependence and sub-Gaussian noise with low variance. We propose an integrated\nmethod for robust regression that relaxes these assumptions, assuming only that\nthe feature matrix can be well approximated by a low-rank matrix. Our\ntechniques integrate improved robust low-rank matrix approximation and robust\nprinciple component regression, and yield strong performance guarantees.\nMoreover, we experimentally show that our methods significantly outperform\nstate of the art both in running time and prediction error. \n\n"}
{"id": "1608.02732", "contents": "Title: On Lower Bounds for Regret in Reinforcement Learning Abstract: This is a brief technical note to clarify the state of lower bounds on regret\nfor reinforcement learning. In particular, this paper:\n  - Reproduces a lower bound on regret for reinforcement learning, similar to\nthe result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010).\n  - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett\nand Tewari 2009) does not hold using the standard techniques without further\nwork. We suggest that this result should instead be considered a conjecture as\nit has no rigorous proof.\n  - Suggests that the conjectured lower bound given by (Bartlett and Tewari\n2009) is incorrect and, in fact, it is possible to improve the scaling of the\nupper bound to match the weaker lower bounds presented in this paper.\n  We hope that this note serves to clarify existing results in the field of\nreinforcement learning and provides interesting motivation for future work. \n\n"}
{"id": "1608.03585", "contents": "Title: Warm Starting Bayesian Optimization Abstract: We develop a framework for warm-starting Bayesian optimization, that reduces\nthe solution time required to solve an optimization problem that is one in a\nsequence of related problems. This is useful when optimizing the output of a\nstochastic simulator that fails to provide derivative information, for which\nBayesian optimization methods are well-suited. Solving sequences of related\noptimization problems arises when making several business decisions using one\noptimization model and input data collected over different time periods or\nmarkets. While many gradient-based methods can be warm started by initiating\noptimization at the solution to the previous problem, this warm start approach\ndoes not apply to Bayesian optimization methods, which carry a full metamodel\nof the objective function from iteration to iteration. Our approach builds a\njoint statistical model of the entire collection of related objective\nfunctions, and uses a value of information calculation to recommend points to\nevaluate. \n\n"}
{"id": "1608.03735", "contents": "Title: Causal Inference for Social Discrimination Reasoning Abstract: The discovery of discriminatory bias in human or automated decision making is\na task of increasing importance and difficulty, exacerbated by the pervasive\nuse of machine learning and data mining. Currently, discrimination discovery\nlargely relies upon correlation analysis of decisions records, disregarding the\nimpact of confounding biases. We present a method for causal discrimination\ndiscovery based on propensity score analysis, a statistical tool for filtering\nout the effect of confounding variables. We introduce causal measures of\ndiscrimination which quantify the effect of group membership on the decisions,\nand highlight causal discrimination/favoritism patterns by learning regression\ntrees over the novel measures. We validate our approach on two real world\ndatasets. Our proposed framework for causal discrimination has the potential to\nenhance the transparency of machine learning with tools for detecting\ndiscriminatory bias both in the training data and in the learning algorithms. \n\n"}
{"id": "1608.04471", "contents": "Title: Stein Variational Gradient Descent: A General Purpose Bayesian Inference\n  Algorithm Abstract: We propose a general purpose variational inference algorithm that forms a\nnatural counterpart of gradient descent for optimization. Our method\niteratively transports a set of particles to match the target distribution, by\napplying a form of functional gradient descent that minimizes the KL\ndivergence. Empirical studies are performed on various real world models and\ndatasets, on which our method is competitive with existing state-of-the-art\nmethods. The derivation of our method is based on a new theoretical result that\nconnects the derivative of KL divergence under smooth transforms with Stein's\nidentity and a recently proposed kernelized Stein discrepancy, which is of\nindependent interest. \n\n"}
{"id": "1608.05258", "contents": "Title: Parameter Learning for Log-supermodular Distributions Abstract: We consider log-supermodular models on binary variables, which are\nprobabilistic models with negative log-densities which are submodular. These\nmodels provide probabilistic interpretations of common combinatorial\noptimization tasks such as image segmentation. In this paper, we focus\nprimarily on parameter estimation in the models from known upper-bounds on the\nintractable log-partition function. We show that the bound based on separable\noptimization on the base polytope of the submodular function is always inferior\nto a bound based on \"perturb-and-MAP\" ideas. Then, to learn parameters, given\nthat our approximation of the log-partition function is an expectation (over\nour own randomization), we use a stochastic subgradient technique to maximize a\nlower-bound on the log-likelihood. This can also be extended to conditional\nmaximum likelihood. We illustrate our new results in a set of experiments in\nbinary image denoising, where we highlight the flexibility of a probabilistic\nmodel to learn with missing data. \n\n"}
{"id": "1608.06154", "contents": "Title: Multi-Sensor Prognostics using an Unsupervised Health Index based on\n  LSTM Encoder-Decoder Abstract: Many approaches for estimation of Remaining Useful Life (RUL) of a machine,\nusing its operational sensor data, make assumptions about how a system degrades\nor a fault evolves, e.g., exponential degradation. However, in many domains\ndegradation may not follow a pattern. We propose a Long Short Term Memory based\nEncoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI)\nfor a system using multi-sensor time-series data. LSTM-ED is trained to\nreconstruct the time-series corresponding to healthy state of a system. The\nreconstruction error is used to compute HI which is then used for RUL\nestimation. We evaluate our approach on publicly available Turbofan Engine and\nMilling Machine datasets. We also present results on a real-world industry\ndataset from a pulverizer mill where we find significant correlation between\nLSTM-ED based HI and maintenance costs. \n\n"}
{"id": "1608.06879", "contents": "Title: AIDE: Fast and Communication Efficient Distributed Optimization Abstract: In this paper, we present two new communication-efficient methods for\ndistributed minimization of an average of functions. The first algorithm is an\ninexact variant of the DANE algorithm that allows any local algorithm to return\nan approximate solution to a local subproblem. We show that such a strategy\ndoes not affect the theoretical guarantees of DANE significantly. In fact, our\napproach can be viewed as a robustification strategy since the method is\nsubstantially better behaved than DANE on data partition arising in practice.\nIt is well known that DANE algorithm does not match the communication\ncomplexity lower bounds. To bridge this gap, we propose an accelerated variant\nof the first method, called AIDE, that not only matches the communication lower\nbounds but can also be implemented using a purely first-order oracle. Our\nempirical results show that AIDE is superior to other communication efficient\nalgorithms in settings that naturally arise in machine learning applications. \n\n"}
{"id": "1608.08852", "contents": "Title: A Mathematical Framework for Feature Selection from Real-World Data with\n  Non-Linear Observations Abstract: In this paper, we study the challenge of feature selection based on a\nrelatively small collection of sample pairs $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$.\nThe observations $y_i \\in \\mathbb{R}$ are thereby supposed to follow a noisy\nsingle-index model, depending on a certain set of signal variables. A major\ndifficulty is that these variables usually cannot be observed directly, but\nrather arise as hidden factors in the actual data vectors $x_i \\in\n\\mathbb{R}^d$ (feature variables). We will prove that a successful variable\nselection is still possible in this setup, even when the applied estimator does\nnot have any knowledge of the underlying model parameters and only takes the\n'raw' samples $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$ as input. The model\nassumptions of our results will be fairly general, allowing for non-linear\nobservations, arbitrary convex signal structures as well as strictly convex\nloss functions. This is particularly appealing for practical purposes, since in\nmany applications, already standard methods, e.g., the Lasso or logistic\nregression, yield surprisingly good outcomes. Apart from a general discussion\nof the practical scope of our theoretical findings, we will also derive a\nrigorous guarantee for a specific real-world problem, namely sparse feature\nextraction from (proteomics-based) mass spectrometry data. \n\n"}
{"id": "1609.02116", "contents": "Title: Ask the GRU: Multi-Task Learning for Deep Text Recommendations Abstract: In a variety of application domains the content to be recommended to users is\nassociated with text. This includes research papers, movies with associated\nplot summaries, news articles, blog posts, etc. Recommendation approaches based\non latent factor models can be extended naturally to leverage text by employing\nan explicit mapping from text to factors. This enables recommendations for new,\nunseen content, and may generalize better, since the factors for all items are\nproduced by a compactly-parametrized model. Previous work has used topic models\nor averages of word embeddings for this mapping. In this paper we present a\nmethod leveraging deep recurrent neural networks to encode the text sequence\ninto a latent vector, specifically gated recurrent units (GRUs) trained\nend-to-end on the collaborative filtering task. For the task of scientific\npaper recommendation, this yields models with significantly higher accuracy. In\ncold-start scenarios, we beat the previous state-of-the-art, all of which\nignore word order. Performance is further improved by multi-task learning,\nwhere the text encoder network is trained for a combination of content\nrecommendation and item metadata prediction. This regularizes the collaborative\nfiltering model, ameliorating the problem of sparsity of the observed rating\nmatrix. \n\n"}
{"id": "1609.03344", "contents": "Title: Finite-sample and asymptotic analysis of generalization ability with an\n  application to penalized regression Abstract: In this paper, we study the performance of extremum estimators from the\nperspective of generalization ability (GA): the ability of a model to predict\noutcomes in new samples from the same population. By adapting the classical\nconcentration inequalities, we derive upper bounds on the empirical\nout-of-sample prediction errors as a function of the in-sample errors,\nin-sample data size, heaviness in the tails of the error distribution, and\nmodel complexity. We show that the error bounds may be used for tuning key\nestimation hyper-parameters, such as the number of folds $K$ in\ncross-validation. We also show how $K$ affects the bias-variance trade-off for\ncross-validation. We demonstrate that the $\\mathcal{L}_2$-norm difference\nbetween penalized and the corresponding un-penalized regression estimates is\ndirectly explained by the GA of the estimates and the GA of empirical moment\nconditions. Lastly, we prove that all penalized regression estimates are\n$L_2$-consistent for both the $n \\geqslant p$ and the $n < p$ cases.\nSimulations are used to demonstrate key results.\n  Keywords: generalization ability, upper bound of generalization error,\npenalized regression, cross-validation, bias-variance trade-off,\n$\\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,\nhigh-dimensional data. \n\n"}
{"id": "1609.03958", "contents": "Title: Noisy Inductive Matrix Completion Under Sparse Factor Models Abstract: Inductive Matrix Completion (IMC) is an important class of matrix completion\nproblems that allows direct inclusion of available features to enhance\nestimation capabilities. These models have found applications in personalized\nrecommendation systems, multilabel learning, dictionary learning, etc. This\npaper examines a general class of noisy matrix completion tasks where the\nunderlying matrix is following an IMC model i.e., it is formed by a mixing\nmatrix (a priori unknown) sandwiched between two known feature matrices. The\nmixing matrix here is assumed to be well approximated by the product of two\nsparse matrices---referred here to as \"sparse factor models.\" We leverage the\nmain theorem of Soni:2016:NMC and extend it to provide theoretical error bounds\nfor the sparsity-regularized maximum likelihood estimators for the class of\nproblems discussed in this paper. The main result is general in the sense that\nit can be used to derive error bounds for various noise models. In this paper,\nwe instantiate our main result for the case of Gaussian noise and provide\ncorresponding error bounds in terms of squared loss. \n\n"}
{"id": "1609.05539", "contents": "Title: On Randomized Distributed Coordinate Descent with Quantized Updates Abstract: In this paper, we study the randomized distributed coordinate descent\nalgorithm with quantized updates. In the literature, the iteration complexity\nof the randomized distributed coordinate descent algorithm has been\ncharacterized under the assumption that machines can exchange updates with an\ninfinite precision. We consider a practical scenario in which the messages\nexchange occurs over channels with finite capacity, and hence the updates have\nto be quantized. We derive sufficient conditions on the quantization error such\nthat the algorithm with quantized update still converge. We further verify our\ntheoretical results by running an experiment, where we apply the algorithm with\nquantized updates to solve a linear regression problem. \n\n"}
{"id": "1609.06804", "contents": "Title: Decoupled Asynchronous Proximal Stochastic Gradient Descent with\n  Variance Reduction Abstract: In the era of big data, optimizing large scale machine learning problems\nbecomes a challenging task and draws significant attention. Asynchronous\noptimization algorithms come out as a promising solution. Recently, decoupled\nasynchronous proximal stochastic gradient descent (DAP-SGD) is proposed to\nminimize a composite function. It is claimed to be able to off-loads the\ncomputation bottleneck from server to workers by allowing workers to evaluate\nthe proximal operators, therefore, server just need to do element-wise\noperations. However, it still suffers from slow convergence rate because of the\nvariance of stochastic gradient is nonzero. In this paper, we propose a faster\nmethod, decoupled asynchronous proximal stochastic variance reduced gradient\ndescent method (DAP-SVRG). We prove that our method has linear convergence for\nstrongly convex problem. Large-scale experiments are also conducted in this\npaper, and results demonstrate our theoretical analysis. \n\n"}
{"id": "1609.06831", "contents": "Title: Hawkes Processes with Stochastic Excitations Abstract: We propose an extension to Hawkes processes by treating the levels of\nself-excitation as a stochastic differential equation. Our new point process\nallows better approximation in application domains where events and intensities\naccelerate each other with correlated levels of contagion. We generalize a\nrecent algorithm for simulating draws from Hawkes processes whose levels of\nexcitation are stochastic processes, and propose a hybrid Markov chain Monte\nCarlo approach for model fitting. Our sampling procedure scales linearly with\nthe number of required events and does not require stationarity of the point\nprocess. A modular inference procedure consisting of a combination between\nGibbs and Metropolis Hastings steps is put forward. We recover expectation\nmaximization as a special case. Our general approach is illustrated for\ncontagion following geometric Brownian motion and exponential Langevin\ndynamics. \n\n"}
{"id": "1609.07574", "contents": "Title: Dynamic Pricing in High-dimensions Abstract: We study the pricing problem faced by a firm that sells a large number of\nproducts, described via a wide range of features, to customers that arrive over\ntime. Customers independently make purchasing decisions according to a general\nchoice model that includes products features and customers' characteristics,\nencoded as $d$-dimensional numerical vectors, as well as the price offered. The\nparameters of the choice model are a priori unknown to the firm, but can be\nlearned as the (binary-valued) sales data accrues over time. The firm's\nobjective is to minimize the regret, i.e., the expected revenue loss against a\nclairvoyant policy that knows the parameters of the choice model in advance,\nand always offers the revenue-maximizing price. This setting is motivated in\npart by the prevalence of online marketplaces that allow for real-time pricing.\nWe assume a structured choice model, parameters of which depend on $s_0$ out of\nthe $d$ product features. We propose a dynamic policy, called Regularized\nMaximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of\nthe high-dimensional model and obtains a logarithmic regret in $T$. More\nspecifically, the regret of our algorithm is of $O(s_0 \\log d \\cdot \\log T)$.\nFurthermore, we show that no policy can obtain regret better than $O(s_0 (\\log\nd + \\log T))$. \n\n"}
{"id": "1610.01076", "contents": "Title: Tutorial on Answering Questions about Images with Deep Learning Abstract: Together with the development of more accurate methods in Computer Vision and\nNatural Language Understanding, holistic architectures that answer on questions\nabout the content of real-world images have emerged. In this tutorial, we build\na neural-based approach to answer questions about images. We base our tutorial\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\nmodels that we present here can achieve a competitive performance on both\ndatasets, in fact, they are among the best methods that use a combination of\nLSTM with a global, full frame CNN representation of an image. We hope that\nafter reading this tutorial, the reader will be able to use Deep Learning\nframeworks, such as Keras and introduced Kraino, to build various architectures\nthat will lead to a further performance improvement on this challenging task. \n\n"}
{"id": "1610.02391", "contents": "Title: Grad-CAM: Visual Explanations from Deep Networks via Gradient-based\n  Localization Abstract: We propose a technique for producing \"visual explanations\" for decisions from\na large class of CNN-based models, making them more transparent. Our approach -\nGradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of\nany target concept, flowing into the final convolutional layer to produce a\ncoarse localization map highlighting important regions in the image for\npredicting the concept. Grad-CAM is applicable to a wide variety of CNN\nmodel-families: (1) CNNs with fully-connected layers, (2) CNNs used for\nstructured outputs, (3) CNNs used in tasks with multimodal inputs or\nreinforcement learning, without any architectural changes or re-training. We\ncombine Grad-CAM with fine-grained visualizations to create a high-resolution\nclass-discriminative visualization and apply it to off-the-shelf image\nclassification, captioning, and visual question answering (VQA) models,\nincluding ResNet-based architectures. In the context of image classification\nmodels, our visualizations (a) lend insights into their failure modes, (b) are\nrobust to adversarial images, (c) outperform previous methods on localization,\n(d) are more faithful to the underlying model and (e) help achieve\ngeneralization by identifying dataset bias. For captioning and VQA, we show\nthat even non-attention based models can localize inputs. We devise a way to\nidentify important neurons through Grad-CAM and combine it with neuron names to\nprovide textual explanations for model decisions. Finally, we design and\nconduct human studies to measure if Grad-CAM helps users establish appropriate\ntrust in predictions from models and show that Grad-CAM helps untrained users\nsuccessfully discern a 'stronger' nodel from a 'weaker' one even when both make\nidentical predictions. Our code is available at\nhttps://github.com/ramprs/grad-cam/, along with a demo at\nhttp://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E. \n\n"}
{"id": "1610.03414", "contents": "Title: Maximum entropy models capture melodic styles Abstract: We introduce a Maximum Entropy model able to capture the statistics of\nmelodies in music. The model can be used to generate new melodies that emulate\nthe style of the musical corpus which was used to train it. Instead of using\nthe $n-$body interactions of $(n-1)-$order Markov models, traditionally used in\nautomatic music generation, we use a $k-$nearest neighbour model with pairwise\ninteractions only. In that way, we keep the number of parameters low and avoid\nover-fitting problems typical of Markov models. We show that long-range musical\nphrases don't need to be explicitly enforced using high-order Markov\ninteractions, but can instead emerge from multiple, competing, pairwise\ninteractions. We validate our Maximum Entropy model by contrasting how much the\ngenerated sequences capture the style of the original corpus without\nplagiarizing it. To this end we use a data-compression approach to discriminate\nthe levels of borrowing and innovation featured by the artificial sequences.\nThe results show that our modelling scheme outperforms both fixed-order and\nvariable-order Markov models. This shows that, despite being based only on\npairwise interactions, this Maximum Entropy scheme opens the possibility to\ngenerate musically sensible alterations of the original phrases, providing a\nway to generate innovation. \n\n"}
{"id": "1610.03483", "contents": "Title: Learning in Implicit Generative Models Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for\nconstructing generative models with several appealing properties: they do not\nrequire a likelihood function to be specified, only a generating procedure;\nthey provide samples that are sharp and compelling; and they allow us to\nharness our knowledge of building highly accurate neural network classifiers.\nHere, we develop our understanding of GANs with the aim of forming a rich view\nof this growing area of machine learning---to build connections to the diverse\nset of statistical thinking on this topic, of which much can be gained by a\nmutual exchange of ideas. We frame GANs within the wider landscape of\nalgorithms for learning in implicit generative models--models that only specify\na stochastic procedure with which to generate data--and relate these ideas to\nmodelling problems in related fields, such as econometrics and approximate\nBayesian computation. We develop likelihood-free inference methods and\nhighlight hypothesis testing as a principle for learning in implicit generative\nmodels, using which we are able to derive the objective function used by GANs,\nand many other related objectives. The testing viewpoint directs our focus to\nthe general problem of density ratio estimation. There are four approaches for\ndensity ratio estimation, one of which is a solution using classifiers to\ndistinguish real from generated data. Other approaches such as divergence\nminimisation and moment matching have also been explored in the GAN literature,\nand we synthesise these views to form an understanding in terms of the\nrelationships between them and the wider literature, highlighting avenues for\nfuture exploration and cross-pollination. \n\n"}
{"id": "1610.06447", "contents": "Title: Regularized Optimal Transport and the Rot Mover's Distance Abstract: This paper presents a unified framework for smooth convex regularization of\ndiscrete optimal transport problems. In this context, the regularized optimal\ntransport turns out to be equivalent to a matrix nearness problem with respect\nto Bregman divergences. Our framework thus naturally generalizes a previously\nproposed regularization based on the Boltzmann-Shannon entropy related to the\nKullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We\ncall the regularized optimal transport distance the rot mover's distance in\nreference to the classical earth mover's distance. We develop two generic\nschemes that we respectively call the alternate scaling algorithm and the\nnon-negative alternate scaling algorithm, to compute efficiently the\nregularized optimal plans depending on whether the domain of the regularizer\nlies within the non-negative orthant or not. These schemes are based on\nDykstra's algorithm with alternate Bregman projections, and further exploit the\nNewton-Raphson method when applied to separable divergences. We enhance the\nseparable case with a sparse extension to deal with high data dimensions. We\nalso instantiate our proposed framework and discuss the inherent specificities\nfor well-known regularizers and statistical divergences in the machine learning\nand information geometry communities. Finally, we demonstrate the merits of our\nmethods with experiments using synthetic data to illustrate the effect of\ndifferent regularizers and penalties on the solutions, as well as real-world\ndata for a pattern recognition application to audio scene classification. \n\n"}
{"id": "1610.07046", "contents": "Title: Cat-state generation and stabilization for a nuclear spin through\n  electric quadrupole interaction Abstract: Spin cat states are superpositions of two or more coherent spin states (CSSs)\nthat are distinctly separated over the Bloch sphere. Additionally, the nuclei\nwith angular momenta greater than 1/2 possess a quadrupolar charge\ndistribution. At the intersection of these two phenomena, we devise a simple\nscheme for generating various types of nuclear spin cat states. The native\nbiaxial electric quadrupole interaction that is readily available in strained\nsolid-state systems plays a key role here. However, the fact that built-in\nstrain cannot be switched off poses a challenge for the stabilization of target\ncat states once they are prepared. We remedy this by abruptly diverting via a\nsingle rotation pulse the state evolution to the neighborhood of the fixed\npoints of the underlying classical Hamiltonian flow. Optimal process parameters\nare obtained as a function of electric field gradient biaxiality and nuclear\nspin angular momentum. The overall procedure is seen to be robust under 5%\ndeviations from optimal values. We show that higher level cat states with four\nsuperposed CSS can also be formed using three rotation pulses. Finally, for\nopen systems subject to decoherence we extract the scaling of cat state\nfidelity damping with respect to the spin quantum number. This reveals rates\ngreater than the dephasing of individual CSSs. Yet, our results affirm that\nthese cat states can preserve their fidelities for practically useful durations\nunder the currently attainable decoherence levels. \n\n"}
{"id": "1610.07116", "contents": "Title: Online Classification with Complex Metrics Abstract: We present a framework and analysis of consistent binary classification for\ncomplex and non-decomposable performance metrics such as the F-measure and the\nJaccard measure. The proposed framework is general, as it applies to both batch\nand online learning, and to both linear and non-linear models. Our work follows\nrecent results showing that the Bayes optimal classifier for many complex\nmetrics is given by a thresholding of the conditional probability of the\npositive class. This manuscript extends this thresholding characterization --\nshowing that the utility is strictly locally quasi-concave with respect to the\nthreshold for a wide range of models and performance metrics. This, in turn,\nmotivates simple normalized gradient ascent updates for threshold estimation.\nWe present a finite-sample regret analysis for the resulting procedure. In\nparticular, the risk for the batch case converges to the Bayes risk at the same\nrate as that of the underlying conditional probability estimation, and the risk\nof proposed online algorithm converges at a rate that depends on the\nconditional probability estimation risk. For instance, in the special case\nwhere the conditional probability model is logistic regression, our procedure\nachieves $O(\\frac{1}{\\sqrt{n}})$ sample complexity, both for batch and online\ntraining. Empirical evaluation shows that the proposed algorithms out-perform\nalternatives in practice, with comparable or better prediction performance and\nreduced run time for various metrics and datasets. \n\n"}
{"id": "1610.08077", "contents": "Title: A statistical framework for fair predictive algorithms Abstract: Predictive modeling is increasingly being employed to assist human\ndecision-makers. One purported advantage of replacing human judgment with\ncomputer models in high stakes settings-- such as sentencing, hiring, policing,\ncollege admissions, and parole decisions-- is the perceived \"neutrality\" of\ncomputers. It is argued that because computer models do not hold personal\nprejudice, the predictions they produce will be equally free from prejudice.\nThere is growing recognition that employing algorithms does not remove the\npotential for bias, and can even amplify it, since training data were\ninevitably generated by a process that is itself biased. In this paper, we\nprovide a probabilistic definition of algorithmic bias. We propose a method to\nremove bias from predictive models by removing all information regarding\nprotected variables from the permitted training data. Unlike previous work in\nthis area, our framework is general enough to accommodate arbitrary data types,\ne.g. binary, continuous, etc. Motivated by models currently in use in the\ncriminal justice system that inform decisions on pre-trial release and\nparoling, we apply our proposed method to a dataset on the criminal histories\nof individuals at the time of sentencing to produce \"race-neutral\" predictions\nof re-arrest. In the process, we demonstrate that the most common approach to\ncreating \"race-neutral\" models-- omitting race as a covariate-- still results\nin racially disparate predictions. We then demonstrate that the application of\nour proposed method to these data removes racial disparities from predictions\nwith minimal impact on predictive accuracy. \n\n"}
{"id": "1610.08452", "contents": "Title: Fairness Beyond Disparate Treatment & Disparate Impact: Learning\n  Classification without Disparate Mistreatment Abstract: Automated data-driven decision making systems are increasingly being used to\nassist, or even replace humans in many settings. These systems function by\nlearning from historical decisions, often taken by humans. In order to maximize\nthe utility of these systems (or, classifiers), their training involves\nminimizing the errors (or, misclassifications) over the given historical data.\nHowever, it is quite possible that the optimally trained classifier makes\ndecisions for people belonging to different social groups with different\nmisclassification rates (e.g., misclassification rates for females are higher\nthan for males), thereby placing these groups at an unfair disadvantage. To\naccount for and avoid such unfairness, in this paper, we introduce a new notion\nof unfairness, disparate mistreatment, which is defined in terms of\nmisclassification rates. We then propose intuitive measures of disparate\nmistreatment for decision boundary-based classifiers, which can be easily\nincorporated into their formulation as convex-concave constraints. Experiments\non synthetic as well as real world datasets show that our methodology is\neffective at avoiding disparate mistreatment, often at a small cost in terms of\naccuracy. \n\n"}
{"id": "1610.08613", "contents": "Title: Can Active Memory Replace Attention? Abstract: Several mechanisms to focus attention of a neural network on selected parts\nof its input or memory have been used successfully in deep learning models in\nrecent years. Attention has improved image classification, image captioning,\nspeech recognition, generative models, and learning algorithmic tasks, but it\nhad probably the largest impact on neural machine translation.\n  Recently, similar improvements have been obtained using alternative\nmechanisms that do not focus on a single part of a memory but operate on all of\nit in parallel, in a uniform way. Such mechanism, which we call active memory,\nimproved over attention in algorithmic tasks, image processing, and in\ngenerative modelling.\n  So far, however, active memory has not improved over attention for most\nnatural language processing tasks, in particular for machine translation. We\nanalyze this shortcoming in this paper and propose an extended model of active\nmemory that matches existing attention models on neural machine translation and\ngeneralizes better to longer sentences. We investigate this model and explain\nwhy previous active memory models did not succeed. Finally, we discuss when\nactive memory brings most benefits and where attention can be a better choice. \n\n"}
{"id": "1610.08763", "contents": "Title: CoType: Joint Extraction of Typed Entities and Relations with Knowledge\n  Bases Abstract: Extracting entities and relations for types of interest from text is\nimportant for understanding massive text corpora. Traditionally, systems of\nentity relation extraction have relied on human-annotated corpora for training\nand adopted an incremental pipeline. Such systems require additional human\nexpertise to be ported to a new domain, and are vulnerable to errors cascading\ndown the pipeline. In this paper, we investigate joint extraction of typed\nentities and relations with labeled data heuristically obtained from knowledge\nbases (i.e., distant supervision). As our algorithm for type labeling via\ndistant supervision is context-agnostic, noisy training data poses unique\nchallenges for the task. We propose a novel domain-independent framework,\ncalled CoType, that runs a data-driven text segmentation algorithm to extract\nentity mentions, and jointly embeds entity mentions, relation mentions, text\nfeatures and type labels into two low-dimensional spaces (for entity and\nrelation mentions respectively), where, in each space, objects whose types are\nclose will also have similar representations. CoType, then using these learned\nembeddings, estimates the types of test (unlinkable) mentions. We formulate a\njoint optimization problem to learn embeddings from text corpora and knowledge\nbases, adopting a novel partial-label loss function for noisy labeled data and\nintroducing an object \"translation\" function to capture the cross-constraints\nof entities and relations on each other. Experiments on three public datasets\ndemonstrate the effectiveness of CoType across different domains (e.g., news,\nbiomedical), with an average of 25% improvement in F1 score compared to the\nnext best method. \n\n"}
{"id": "1610.08904", "contents": "Title: Local Similarity-Aware Deep Feature Embedding Abstract: Existing deep embedding methods in vision tasks are capable of learning a\ncompact Euclidean space from images, where Euclidean distances correspond to a\nsimilarity metric. To make learning more effective and efficient, hard sample\nmining is usually employed, with samples identified through computing the\nEuclidean feature distance. However, the global Euclidean distance cannot\nfaithfully characterize the true feature similarity in a complex visual feature\nspace, where the intraclass distance in a high-density region may be larger\nthan the interclass distance in low-density regions. In this paper, we\nintroduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of\nlearning a similarity metric adaptive to local feature structure. The metric\ncan be used to select genuinely hard samples in a local neighborhood to guide\nthe deep embedding learning in an online and robust manner. The new layer is\nappealing in that it is pluggable to any convolutional networks and is trained\nend-to-end. Our local similarity-aware feature embedding not only demonstrates\nfaster convergence and boosted performance on two complex image retrieval\ndatasets, its large margin nature also leads to superior generalization results\nunder the large and open set scenarios of transfer learning and zero-shot\nlearning on ImageNet 2010 and ImageNet-10K datasets. \n\n"}
{"id": "1610.09127", "contents": "Title: Adaptive regularization for Lasso models in the context of\n  non-stationary data streams Abstract: Large scale, streaming datasets are ubiquitous in modern machine learning.\nStreaming algorithms must be scalable, amenable to incremental training and\nrobust to the presence of non-stationarity. In this work consider the problem\nof learning $\\ell_1$ regularized linear models in the context of streaming\ndata. In particular, the focus of this work revolves around how to select the\nregularization parameter when data arrives sequentially and the underlying\ndistribution is non-stationary (implying the choice of optimal regularization\nparameter is itself time-varying). We propose a framework through which to\ninfer an adaptive regularization parameter. Our approach employs an $\\ell_1$\npenalty constraint where the corresponding sparsity parameter is iteratively\nupdated via stochastic gradient descent. This serves to reformulate the choice\nof regularization parameter in a principled framework for online learning. The\nproposed method is derived for linear regression and subsequently extended to\ngeneralized linear models. We validate our approach using simulated and real\ndatasets and present an application to a neuroimaging dataset. \n\n"}
{"id": "1611.00035", "contents": "Title: Full-Capacity Unitary Recurrent Neural Networks Abstract: Recurrent neural networks are powerful models for processing sequential data,\nbut they are generally plagued by vanishing and exploding gradient problems.\nUnitary recurrent neural networks (uRNNs), which use unitary recurrence\nmatrices, have recently been proposed as a means to avoid these issues.\nHowever, in previous experiments, the recurrence matrices were restricted to be\na product of parameterized unitary matrices, and an open question remains: when\ndoes such a parameterization fail to represent all unitary matrices, and how\ndoes this restricted representational capacity limit what can be learned? To\naddress this question, we propose full-capacity uRNNs that optimize their\nrecurrence matrix over all unitary matrices, leading to significantly improved\nperformance over uRNNs that use a restricted-capacity recurrence matrix. Our\ncontribution consists of two main components. First, we provide a theoretical\nargument to determine if a unitary parameterization has restricted capacity.\nUsing this argument, we show that a recently proposed unitary parameterization\nhas restricted capacity for hidden state dimension greater than 7. Second, we\nshow how a complete, full-capacity unitary recurrence matrix can be optimized\nover the differentiable manifold of unitary matrices. The resulting\nmultiplicative gradient step is very simple and does not require gradient\nclipping or learning rate adaptation. We confirm the utility of our claims by\nempirically evaluating our new full-capacity uRNNs on both synthetic and\nnatural data, achieving superior performance compared to both LSTMs and the\noriginal restricted-capacity uRNNs. \n\n"}
{"id": "1611.01144", "contents": "Title: Categorical Reparameterization with Gumbel-Softmax Abstract: Categorical variables are a natural choice for representing discrete\nstructure in the world. However, stochastic neural networks rarely use\ncategorical latent variables due to the inability to backpropagate through\nsamples. In this work, we present an efficient gradient estimator that replaces\nthe non-differentiable sample from a categorical distribution with a\ndifferentiable sample from a novel Gumbel-Softmax distribution. This\ndistribution has the essential property that it can be smoothly annealed into a\ncategorical distribution. We show that our Gumbel-Softmax estimator outperforms\nstate-of-the-art gradient estimators on structured output prediction and\nunsupervised generative modeling tasks with categorical latent variables, and\nenables large speedups on semi-supervised classification. \n\n"}
{"id": "1611.01540", "contents": "Title: Topology and Geometry of Half-Rectified Network Optimization Abstract: The loss surface of deep neural networks has recently attracted interest in\nthe optimization and machine learning communities as a prime example of\nhigh-dimensional non-convex problem. Some insights were recently gained using\nspin glass models and mean-field approximations, but at the expense of strongly\nsimplifying the nonlinear nature of the model.\n  In this work, we do not make any such assumption and study conditions on the\ndata distribution and model architecture that prevent the existence of bad\nlocal minima. Our theoretical work quantifies and formalizes two important\n\\emph{folklore} facts: (i) the landscape of deep linear networks has a\nradically different topology from that of deep half-rectified ones, and (ii)\nthat the energy landscape in the non-linear case is fundamentally controlled by\nthe interplay between the smoothness of the data distribution and model\nover-parametrization. Our main theoretical contribution is to prove that\nhalf-rectified single layer networks are asymptotically connected, and we\nprovide explicit bounds that reveal the aforementioned interplay.\n  The conditioning of gradient descent is the next challenge we address. We\nstudy this question through the geometry of the level sets, and we introduce an\nalgorithm to efficiently estimate the regularity of such sets on large-scale\nnetworks. Our empirical results show that these level sets remain connected\nthroughout all the learning phase, suggesting a near convex behavior, but they\nbecome exponentially more curvy as the energy level decays, in accordance to\nwhat is observed in practice with very low curvature attractors. \n\n"}
{"id": "1611.01576", "contents": "Title: Quasi-Recurrent Neural Networks Abstract: Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep's computation on the previous timestep's\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks. \n\n"}
{"id": "1611.02594", "contents": "Title: A Coherent Quantum Annealer with Rydberg Atoms Abstract: There is a significant ongoing effort in realizing quantum annealing with\ndifferent physical platforms. The challenge is to achieve a fully programmable\nquantum device featuring coherent adiabatic quantum dynamics. Here we show that\ncombining the well-developed quantum simulation toolbox for Rydberg atoms with\nthe recently proposed Lechner-Hauke-Zoller~(LHZ) architecture allows one to\nbuild a prototype for a coherent adiabatic quantum computer with all-to-all\nIsing interactions and, therefore, a novel platform for quantum annealing. In\nLHZ a infinite-range spin-glass is mapped onto the low energy subspace of a\nspin-1/2 lattice gauge model with quasi-local 4-body parity constraints. This\nspin model can be emulated in a natural way with Rubidium and Cesium atoms in a\nbipartite optical lattice involving laser-dressed Rydberg-Rydberg interactions,\nwhich are several orders of magnitude larger than the relevant decoherence\nrates. This makes the exploration of coherent quantum enhanced optimization\nprotocols accessible with state-of-the-art atomic physics experiments. \n\n"}
{"id": "1611.04051", "contents": "Title: GANS for Sequences of Discrete Elements with the Gumbel-softmax\n  Distribution Abstract: Generative Adversarial Networks (GAN) have limitations when the goal is to\ngenerate sequences of discrete elements. The reason for this is that samples\nfrom a distribution on discrete objects such as the multinomial are not\ndifferentiable with respect to the distribution parameters. This problem can be\navoided by using the Gumbel-softmax distribution, which is a continuous\napproximation to a multinomial distribution parameterized in terms of the\nsoftmax function. In this work, we evaluate the performance of GANs based on\nrecurrent neural networks with Gumbel-softmax output distributions in the task\nof generating sequences of discrete elements. \n\n"}
{"id": "1611.04273", "contents": "Title: On the Quantitative Analysis of Decoder-Based Generative Models Abstract: The past several years have seen remarkable progress in generative models\nwhich produce convincing samples of images and other modalities. A shared\ncomponent of many powerful generative models is a decoder network, a parametric\ndeep neural net that defines a generative distribution. Examples include\nvariational autoencoders, generative adversarial networks, and generative\nmoment matching networks. Unfortunately, it can be difficult to quantify the\nperformance of these models because of the intractability of log-likelihood\nestimation, and inspecting samples can be misleading. We propose to use\nAnnealed Importance Sampling for evaluating log-likelihoods for decoder-based\nmodels and validate its accuracy using bidirectional Monte Carlo. The\nevaluation code is provided at https://github.com/tonywu95/eval_gen. Using this\ntechnique, we analyze the performance of decoder-based models, the\neffectiveness of existing log-likelihood estimators, the degree of overfitting,\nand the degree to which these models miss important modes of the data\ndistribution. \n\n"}
{"id": "1611.04561", "contents": "Title: Splitting matters: how monotone transformation of predictor variables\n  may improve the predictions of decision tree models Abstract: It is widely believed that the prediction accuracy of decision tree models is\ninvariant under any strictly monotone transformation of the individual\npredictor variables. However, this statement may be false when predicting new\nobservations with values that were not seen in the training-set and are close\nto the location of the split point of a tree rule. The sensitivity of the\nprediction error to the split point interpolation is high when the split point\nof the tree is estimated based on very few observations, reaching 9%\nmisclassification error when only 10 observations are used for constructing a\nsplit, and shrinking to 1% when relying on 100 observations. This study\ncompares the performance of alternative methods for split point interpolation\nand concludes that the best choice is taking the mid-point between the two\nclosest points to the split point of the tree. Furthermore, if the (continuous)\ndistribution of the predictor variable is known, then using its probability\nintegral for transforming the variable (\"quantile transformation\") will reduce\nthe model's interpolation error by up to about a half on average. Accordingly,\nthis study provides guidelines for both developers and users of decision tree\nmodels (including bagging and random forest). \n\n"}
{"id": "1611.05780", "contents": "Title: Gap Safe screening rules for sparsity enforcing penalties Abstract: In high dimensional regression settings, sparsity enforcing penalties have\nproved useful to regularize the data-fitting term. A recently introduced\ntechnique called screening rules propose to ignore some variables in the\noptimization leveraging the expected sparsity of the solutions and consequently\nleading to faster solvers. When the procedure is guaranteed not to discard\nvariables wrongly the rules are said to be safe. In this work, we propose a\nunifying framework for generalized linear models regularized with standard\nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1/\\ell_2$ norms. Our\ntechnique allows to discard safely more variables than previously considered\nsafe rules, particularly for low regularization parameters. Our proposed Gap\nSafe rules (so called because they rely on duality gap computation) can cope\nwith any iterative solver but are particularly well suited to (block)\ncoordinate descent methods. Applied to many standard learning tasks, Lasso,\nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic\nregression, etc., we report significant speed-ups compared to previously\nproposed safe rules on all tested data sets. \n\n"}
{"id": "1611.06265", "contents": "Title: Deep Clustering and Conventional Networks for Music Separation: Stronger\n  Together Abstract: Deep clustering is the first method to handle general audio separation\nscenarios with multiple sources of the same type and an arbitrary number of\nsources, performing impressively in speaker-independent speech separation\ntasks. However, little is known about its effectiveness in other challenging\nsituations such as music source separation. Contrary to conventional networks\nthat directly estimate the source signals, deep clustering generates an\nembedding for each time-frequency bin, and separates sources by clustering the\nbins in the embedding space. We show that deep clustering outperforms\nconventional networks on a singing voice separation task, in both matched and\nmismatched conditions, even though conventional networks have the advantage of\nend-to-end training for best signal approximation, presumably because its more\nflexible objective engenders better regularization. Since the strengths of deep\nclustering and conventional network architectures appear complementary, we\nexplore combining them in a single hybrid network trained via an approach akin\nto multi-task learning. Remarkably, the combination significantly outperforms\neither of its components. \n\n"}
{"id": "1611.06342", "contents": "Title: Quantized neural network design under weight capacity constraint Abstract: The complexity of deep neural network algorithms for hardware implementation\ncan be lowered either by scaling the number of units or reducing the\nword-length of weights. Both approaches, however, can accompany the performance\ndegradation although many types of research are conducted to relieve this\nproblem. Thus, it is an important question which one, between the network size\nscaling and the weight quantization, is more effective for hardware\noptimization. For this study, the performances of fully-connected deep neural\nnetworks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while\nchanging the network complexity and the word-length of weights. Based on these\nexperiments, we present the effective compression ratio (ECR) to guide the\ntrade-off between the network size and the precision of weights when the\nhardware resource is limited. \n\n"}
{"id": "1611.06652", "contents": "Title: Scalable Adaptive Stochastic Optimization Using Random Projections Abstract: Adaptive stochastic gradient methods such as AdaGrad have gained popularity\nin particular for training deep neural networks. The most commonly used and\nstudied variant maintains a diagonal matrix approximation to second order\ninformation by accumulating past gradients which are used to tune the step size\nadaptively. In certain situations the full-matrix variant of AdaGrad is\nexpected to attain better performance, however in high dimensions it is\ncomputationally impractical. We present Ada-LR and RadaGrad two computationally\nefficient approximations to full-matrix AdaGrad based on randomized\ndimensionality reduction. They are able to capture dependencies between\nfeatures and achieve similar performance to full-matrix AdaGrad but at a much\nsmaller computational cost. We show that the regret of Ada-LR is close to the\nregret of full-matrix AdaGrad which can have an up-to exponentially smaller\ndependence on the dimension than the diagonal variant. Empirically, we show\nthat Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task\nof training convolutional neural networks as well as recurrent neural networks,\nRadaGrad achieves faster convergence than diagonal AdaGrad. \n\n"}
{"id": "1611.06759", "contents": "Title: Emergence of Compositional Representations in Restricted Boltzmann\n  Machines Abstract: Extracting automatically the complex set of features composing real\nhigh-dimensional data is crucial for achieving high performance in\nmachine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically\nknown to be efficient for this purpose, and to be able to generate distributed\nand graded representations of the data. We characterize the structural\nconditions (sparsity of the weights, low effective temperature, nonlinearities\nin the activation functions of hidden units, and adaptation of fields\nmaintaining the activity in the visible layer) allowing RBM to operate in such\na compositional phase. Evidence is provided by the replica analysis of an\nadequate statistical ensemble of random RBMs and by RBM trained on the\nhandwritten digits dataset MNIST. \n\n"}
{"id": "1611.06863", "contents": "Title: Probabilistic structure discovery in time series data Abstract: Existing methods for structure discovery in time series data construct\ninterpretable, compositional kernels for Gaussian process regression models.\nWhile the learned Gaussian process model provides posterior mean and variance\nestimates, typically the structure is learned via a greedy optimization\nprocedure. This restricts the space of possible solutions and leads to\nover-confident uncertainty estimates. We introduce a fully Bayesian approach,\ninferring a full posterior over structures, which more reliably captures the\nuncertainty of the model. \n\n"}
{"id": "1611.07579", "contents": "Title: Programs as Black-Box Explanations Abstract: Recent work in model-agnostic explanations of black-box machine learning has\ndemonstrated that interpretability of complex models does not have to come at\nthe cost of accuracy or model flexibility. However, it is not clear what kind\nof explanations, such as linear models, decision trees, and rule lists, are the\nappropriate family to consider, and different tasks and models may benefit from\ndifferent kinds of explanations. Instead of picking a single family of\nrepresentations, in this work we propose to use \"programs\" as model-agnostic\nexplanations. We show that small programs can be expressive yet intuitive as\nexplanations, and generalize over a number of existing interpretable families.\nWe propose a prototype program induction method based on simulated annealing\nthat approximates the local behavior of black-box classifiers around a specific\nprediction using random perturbations. Finally, we present preliminary\napplication on small datasets and show that the generated explanations are\nintuitive and accurate for a number of classifiers. \n\n"}
{"id": "1611.08191", "contents": "Title: Interpreting the Predictions of Complex ML Models by Layer-wise\n  Relevance Propagation Abstract: Complex nonlinear models such as deep neural network (DNNs) have become an\nimportant tool for image classification, speech recognition, natural language\nprocessing, and many other fields of application. These models however lack\ntransparency due to their complex nonlinear structure and to the complex data\ndistributions to which they typically apply. As a result, it is difficult to\nfully characterize what makes these models reach a particular decision for a\ngiven input. This lack of transparency can be a drawback, especially in the\ncontext of sensitive applications such as medical analysis or security. In this\nshort paper, we summarize a recent technique introduced by Bach et al. [1] that\nexplains predictions by decomposing the classification decision of DNN models\nin terms of input variables. \n\n"}
{"id": "1611.08292", "contents": "Title: Identifying Significant Predictive Bias in Classifiers Abstract: We present a novel subset scan method to detect if a probabilistic binary\nclassifier has statistically significant bias -- over or under predicting the\nrisk -- for some subgroup, and identify the characteristics of this subgroup.\nThis form of model checking and goodness-of-fit test provides a way to\ninterpretably detect the presence of classifier bias or regions of poor\nclassifier fit. This allows consideration of not just subgroups of a priori\ninterest or small dimensions, but the space of all possible subgroups of\nfeatures. To address the difficulty of considering these exponentially many\npossible subgroups, we use subset scan and parametric bootstrap-based methods.\nExtending this method, we can penalize the complexity of the detected subgroup\nand also identify subgroups with high classification errors. We demonstrate\nthese methods and find interesting results on the COMPAS crime recidivism and\ncredit delinquency data. \n\n"}
{"id": "1611.08618", "contents": "Title: A Benchmark and Comparison of Active Learning for Logistic Regression Abstract: Logistic regression is by far the most widely used classifier in real-world\napplications. In this paper, we benchmark the state-of-the-art active learning\nmethods for logistic regression and discuss and illustrate their underlying\ncharacteristics. Experiments are carried out on three synthetic datasets and 44\nreal-world datasets, providing insight into the behaviors of these active\nlearning methods with respect to the area of the learning curve (which plots\nclassification accuracy as a function of the number of queried examples) and\ntheir computational costs. Surprisingly, one of the earliest and simplest\nsuggested active learning methods, i.e., uncertainty sampling, performs\nexceptionally well overall. Another remarkable finding is that random sampling,\nwhich is the rudimentary baseline to improve upon, is not overwhelmed by\nindividual active learning techniques in many cases. \n\n"}
{"id": "1611.09444", "contents": "Title: The empirical size of trained neural networks Abstract: ReLU neural networks define piecewise linear functions of their inputs.\nHowever, initializing and training a neural network is very different from\nfitting a linear spline. In this paper, we expand empirically upon previous\ntheoretical work to demonstrate features of trained neural networks. Standard\nnetwork initialization and training produce networks vastly simpler than a\nnaive parameter count would suggest and can impart odd features to the trained\nnetwork. However, we also show the forced simplicity is beneficial and, indeed,\ncritical for the wide success of these networks. \n\n"}
{"id": "1611.09630", "contents": "Title: Improving Variational Auto-Encoders using Householder Flow Abstract: Variational auto-encoders (VAE) are scalable and powerful generative models.\nHowever, the choice of the variational posterior determines tractability and\nflexibility of the VAE. Commonly, latent variables are modeled using the normal\ndistribution with a diagonal covariance matrix. This results in computational\nefficiency but typically it is not flexible enough to match the true posterior\ndistribution. One fashion of enriching the variational posterior distribution\nis application of normalizing flows, i.e., a series of invertible\ntransformations to latent variables with a simple posterior. In this paper, we\nfollow this line of thinking and propose a volume-preserving flow that uses a\nseries of Householder transformations. We show empirically on MNIST dataset and\nhistopathology data that the proposed flow allows to obtain more flexible\nvariational posterior and competitive results comparing to other normalizing\nflows. \n\n"}
{"id": "1612.00383", "contents": "Title: Tuning the Scheduling of Distributed Stochastic Gradient Descent with\n  Bayesian Optimization Abstract: We present an optimizer which uses Bayesian optimization to tune the system\nparameters of distributed stochastic gradient descent (SGD). Given a specific\ncontext, our goal is to quickly find efficient configurations which\nappropriately balance the load between the available machines to minimize the\naverage SGD iteration time. Our experiments consider setups with over thirty\nparameters. Traditional Bayesian optimization, which uses a Gaussian process as\nits model, is not well suited to such high dimensional domains. To reduce\nconvergence time, we exploit the available structure. We design a probabilistic\nmodel which simulates the behavior of distributed SGD and use it within\nBayesian optimization. Our model can exploit many runtime measurements for\ninference per evaluation of the objective function. Our experiments show that\nour resulting optimizer converges to efficient configurations within ten\niterations, the optimized configurations outperform those found by generic\noptimizer in thirty iterations by up to 2X. \n\n"}
{"id": "1612.00475", "contents": "Title: Transfer Learning Across Patient Variations with Hidden Parameter Markov\n  Decision Processes Abstract: Due to physiological variation, patients diagnosed with the same condition\nmay exhibit divergent, but related, responses to the same treatments. Hidden\nParameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning\nproblem by embedding these tasks into a low-dimensional space. However, the\noriginal formulation of HiP-MDP had a critical flaw: the embedding uncertainty\nwas modeled independently of the agent's state uncertainty, requiring an\nunnatural training procedure in which all tasks visited every part of the state\nspace---possible for robots that can be moved to a particular location,\nimpossible for human patients. We update the HiP-MDP framework and extend it to\nmore robustly develop personalized medicine strategies for HIV treatment. \n\n"}
{"id": "1612.01200", "contents": "Title: Intra-day Activity Better Predicts Chronic Conditions Abstract: In this work we investigate intra-day patterns of activity on a population of\n7,261 users of mobile health wearable devices and apps. We show that: (1) using\nintra-day step and sleep data recorded from passive trackers significantly\nimproves classification performance on self-reported chronic conditions related\nto mental health and nervous system disorders, (2) Convolutional Neural\nNetworks achieve top classification performance vs. baseline models when\ntrained directly on multivariate time series of activity data, and (3) jointly\npredicting all condition classes via multi-task learning can be leveraged to\nextract features that generalize across data sets and achieve the highest\nclassification performance. \n\n"}
{"id": "1612.01277", "contents": "Title: Cryptocurrency Portfolio Management with Deep Reinforcement Learning Abstract: Portfolio management is the decision-making process of allocating an amount\nof fund into different financial investment products. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. This paper presents a\nmodel-less convolutional neural network with historic prices of a set of\nfinancial assets as its input, outputting portfolio weights of the set. The\nnetwork is trained with 0.7 years' price data from a cryptocurrency exchange.\nThe training is done in a reinforcement manner, maximizing the accumulative\nreturn, which is regarded as the reward function of the network. Backtest\ntrading experiments with trading period of 30 minutes is conducted in the same\nmarket, achieving 10-fold returns in 1.8 months' periods. Some recently\npublished portfolio selection strategies are also used to perform the same\nback-tests, whose results are compared with the neural network. The network is\nnot limited to cryptocurrency, but can be applied to any other financial\nmarkets. \n\n"}
{"id": "1612.01717", "contents": "Title: Statistical mechanics of unsupervised feature learning in a restricted\n  Boltzmann machine with binary synapses Abstract: Revealing hidden features in unlabeled data is called unsupervised feature\nlearning, which plays an important role in pretraining a deep neural network.\nHere we provide a statistical mechanics analysis of the unsupervised learning\nin a restricted Boltzmann machine with binary synapses. A message passing\nequation to infer the hidden feature is derived, and furthermore, variants of\nthis equation are analyzed. A statistical analysis by replica theory describes\nthe thermodynamic properties of the model. Our analysis confirms an entropy\ncrisis preceding the non-convergence of the message passing equation,\nsuggesting a discontinuous phase transition as a key characteristic of the\nrestricted Boltzmann machine. Continuous phase transition is also confirmed\ndepending on the embedded feature strength in the data. The mean-field result\nunder the replica symmetric assumption agrees with that obtained by running\nmessage passing algorithms on single instances of finite sizes. Interestingly,\nin an approximate Hopfield model, the entropy crisis is absent, and a\ncontinuous phase transition is observed instead. We also develop an iterative\nequation to infer the hyper-parameter (temperature) hidden in the data, which\nin physics corresponds to iteratively imposing Nishimori condition. Our study\nprovides insights towards understanding the thermodynamic properties of the\nrestricted Boltzmann machine learning, and moreover important theoretical basis\nto build simplified deep networks. \n\n"}
{"id": "1612.02058", "contents": "Title: Error mitigation for short-depth quantum circuits Abstract: Two schemes are presented that mitigate the effect of errors and decoherence\nin short depth quantum circuits. The size of the circuits for which these\ntechniques can be applied is limited by the rate at which the errors in the\ncomputation are introduced. Near-term applications of early quantum devices,\nsuch as quantum simulations, rely on accurate estimates of expectation values\nto become relevant. Decoherence and gate errors lead to wrong estimates of the\nexpectation values of observables used to evaluate the noisy circuit. The two\nschemes we discuss are deliberately simple and don't require additional qubit\nresources, so to be as practically relevant in current experiments as possible.\nThe first method, extrapolation to the zero noise limit, subsequently cancels\npowers of the noise perturbations by an application of Richardson's deferred\napproach to the limit. The second method cancels errors by resampling\nrandomized circuits according to a quasi-probability distribution. \n\n"}
{"id": "1612.02437", "contents": "Title: Multi-partite entanglement Abstract: We give an introduction to the theory of multi-partite entanglement. We begin\nby describing the \"coordinate system\" of the field: Are we dealing with pure or\nmixed states, with single or multiple copies, what notion of \"locality\" is\nbeing used, do we aim to classify states according to their \"type of\nentanglement\" or to quantify it? Building on the general theory of\nmulti-partite entanglement - to the extent that it has been achieved - we turn\nto explaining important classes of multi-partite entangled states, including\nmatrix product states, stabilizer and graph states, bosonic and fermionic\nGaussian states, addressing applications in condensed matter theory. We end\nwith a brief discussion of various applications that rely on multi-partite\nentangled states: quantum networks, measurement-based quantum computing,\nnon-locality, and quantum metrology. \n\n"}
{"id": "1612.03096", "contents": "Title: Resilience of the quantum Rabi model in circuit QED Abstract: In circuit quantum electrodynamics (circuit QED), an artificial \"circuit\natom\" can couple to a quantized microwave radiation much stronger than its real\natomic counterpart. The celebrated quantum Rabi model describes the simplest\ninteraction of a two-level system with a single-mode boson field. When the\ncoupling is large enough, the bare multilevel structure of a realistic circuit\natom cannot be ignored even if the circuit is strongly anharmonic. We explored\nthis situation theoretically for flux (fluxonium) and charge (Cooper pair box)\ntype multi-level circuits tuned to their respective flux/charge degeneracy\npoints. We identified which spectral features of the quantum Rabi model survive\nand which are renormalized for large coupling. Despite significant\nrenormalization of the low-energy spectrum in the fluxonium case, the key\nquantum Rabi feature -- nearly-degenerate vacuum consisting of an atomic state\nentangled with a multi-photon field -- appears in both types of circuits when\nthe coupling is sufficiently large. Like in the quantum Rabi model, for very\nlarge couplings the entanglement spectrum is dominated by only two, nearly\nequal eigenvalues, in spite of the fact that a large number of bare atomic\nstates are actually involved in the atom-resonator ground state. We interpret\nthe emergence of the two-fold degeneracy of the vacuum of both circuits as an\nenvironmental suppression of flux/charge tunneling due to their dressing by\nvirtual low-/high-impedance photons in the resonator. For flux tunneling, the\ndressing is nothing else than the shunting of a Josephson atom with a large\ncapacitance of the resonator. Suppression of charge tunneling is a\nmanifestation of the dynamical Coulomb blockade of transport in tunnel\njunctions connected to resistive leads. \n\n"}
{"id": "1612.04899", "contents": "Title: Semi-Supervised Phone Classification using Deep Neural Networks and\n  Stochastic Graph-Based Entropic Regularization Abstract: We describe a graph-based semi-supervised learning framework in the context\nof deep neural networks that uses a graph-based entropic regularizer to favor\nsmooth solutions over a graph induced by the data. The main contribution of\nthis work is a computationally efficient, stochastic graph-regularization\ntechnique that uses mini-batches that are consistent with the graph structure,\nbut also provides enough stochasticity (in terms of mini-batch data diversity)\nfor convergence of stochastic gradient descent methods to good solutions. For\nthis work, we focus on results of frame-level phone classification accuracy on\nthe TIMIT speech corpus but our method is general and scalable to much larger\ndata sets. Results indicate that our method significantly improves\nclassification accuracy compared to the fully-supervised case when the fraction\nof labeled data is low, and it is competitive with other methods in the fully\nlabeled case. \n\n"}
{"id": "1612.06519", "contents": "Title: Exploring the Design Space of Deep Convolutional Neural Networks at\n  Large Scale Abstract: In recent years, the research community has discovered that deep neural\nnetworks (DNNs) and convolutional neural networks (CNNs) can yield higher\naccuracy than all previous solutions to a broad array of machine learning\nproblems. To our knowledge, there is no single CNN/DNN architecture that solves\nall problems optimally. Instead, the \"right\" CNN/DNN architecture varies\ndepending on the application at hand. CNN/DNNs comprise an enormous design\nspace. Quantitatively, we find that a small region of the CNN design space\ncontains 30 billion different CNN architectures.\n  In this dissertation, we develop a methodology that enables systematic\nexploration of the design space of CNNs. Our methodology is comprised of the\nfollowing four themes.\n  1. Judiciously choosing benchmarks and metrics.\n  2. Rapidly training CNN models.\n  3. Defining and describing the CNN design space.\n  4. Exploring the design space of CNN architectures.\n  Taken together, these four themes comprise an effective methodology for\ndiscovering the \"right\" CNN architectures to meet the needs of practical\napplications. \n\n"}
{"id": "1612.07019", "contents": "Title: Robust Learning with Kernel Mean p-Power Error Loss Abstract: Correntropy is a second order statistical measure in kernel space, which has\nbeen successfully applied in robust learning and signal processing. In this\npaper, we define a nonsecond order statistical measure in kernel space, called\nthe kernel mean-p power error (KMPE), including the correntropic loss (CLoss)\nas a special case. Some basic properties of KMPE are presented. In particular,\nwe apply the KMPE to extreme learning machine (ELM) and principal component\nanalysis (PCA), and develop two robust learning algorithms, namely ELM-KMPE and\nPCA-KMPE. Experimental results on synthetic and benchmark data show that the\ndeveloped algorithms can achieve consistently better performance when compared\nwith some existing methods. \n\n"}
{"id": "1612.07222", "contents": "Title: Bayesian Decision Process for Cost-Efficient Dynamic Ranking via\n  Crowdsourcing Abstract: Rank aggregation based on pairwise comparisons over a set of items has a wide\nrange of applications. Although considerable research has been devoted to the\ndevelopment of rank aggregation algorithms, one basic question is how to\nefficiently collect a large amount of high-quality pairwise comparisons for the\nranking purpose. Because of the advent of many crowdsourcing services, a crowd\nof workers are often hired to conduct pairwise comparisons with a small\nmonetary reward for each pair they compare. Since different workers have\ndifferent levels of reliability and different pairs have different levels of\nambiguity, it is desirable to wisely allocate the limited budget for\ncomparisons among the pairs of items and workers so that the global ranking can\nbe accurately inferred from the comparison results. To this end, we model the\nactive sampling problem in crowdsourced ranking as a Bayesian Markov decision\nprocess, which dynamically selects item pairs and workers to improve the\nranking accuracy under a budget constraint. We further develop a\ncomputationally efficient sampling policy based on knowledge gradient as well\nas a moment matching technique for posterior approximation. Experimental\nevaluations on both synthetic and real data show that the proposed policy\nachieves high ranking accuracy with a lower labeling cost. \n\n"}
{"id": "1612.07993", "contents": "Title: RSSL: Semi-supervised Learning in R Abstract: In this paper, we introduce a package for semi-supervised learning research\nin the R programming language called RSSL. We cover the purpose of the package,\nthe methods it includes and comment on their use and implementation. We then\nshow, using several code examples, how the package can be used to replicate\nwell-known results from the semi-supervised learning literature. \n\n"}
{"id": "1612.09151", "contents": "Title: Dark-Bright Soliton Dynamics Beyond the Mean-Field Approximation Abstract: The dynamics of dark-bright solitons beyond the mean-field approximation is\ninvestigated. We first examine the case of a single dark-bright soliton and its\noscillations within a parabolic trap. Subsequently, we move to the setting of\ncollisions, comparing the mean-field approximation to that involving multiple\norbitals in both the dark and the bright component. Fragmentation is present\nand significantly affects the dynamics, especially in the case of slower\nsolitons and in that of lower atom numbers. It is shown that the presence of\nfragmentation allows for bipartite entanglement between the distinguishable\nspecies. Most importantly the interplay between fragmentation and entanglement\nleads to the splitting of each of the parent mean-field dark-bright solitons,\nplaced off-center within the parabolic trap, into a fast and a slow daughter\nsolitary wave. The latter process is in direct contrast to the predictions of\nthe mean-field approximation. A variety of excitations including dark-bright\nsolitons in multiple (concurrently populated) orbitals is observed.\nDark-antidark states and domain-wall-bright soliton complexes can also be\nobserved to arise spontaneously in the beyond mean-field dynamics. \n\n"}
{"id": "1701.00299", "contents": "Title: Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs\n  by Selective Execution Abstract: We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward\ndeep neural network that allows selective execution. Given an input, only a\nsubset of D2NN neurons are executed, and the particular subset is determined by\nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs\nprovide a way to improve computational efficiency. To achieve dynamic selective\nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic\ngraph of differentiable modules) with controller modules. Each controller\nmodule is a sub-network whose output is a decision that controls whether other\nmodules can execute. A D2NN is trained end to end. Both regular and controller\nmodules in a D2NN are learnable and are jointly trained to optimize both\naccuracy and efficiency. Such training is achieved by integrating\nbackpropagation with reinforcement learning. With extensive experiments of\nvarious D2NN architectures on image classification tasks, we demonstrate that\nD2NNs are general and flexible, and can effectively optimize\naccuracy-efficiency trade-offs. \n\n"}
{"id": "1701.00677", "contents": "Title: New Methods of Enhancing Prediction Accuracy in Linear Models with\n  Missing Data Abstract: In this paper, prediction for linear systems with missing information is\ninvestigated. New methods are introduced to improve the Mean Squared Error\n(MSE) on the test set in comparison to state-of-the-art methods, through\nappropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft\nWeighted Prediction (SWP) algorithm and its efficacy are depicted and compared\nto previous works for non-missing scenarios. The algorithm is then modified and\noptimized for missing scenarios. It is shown that controlled over-fitting by\nsuggested algorithms will improve prediction accuracy in various cases.\nSimulation results approve our heuristics in enhancing the prediction accuracy. \n\n"}
{"id": "1701.05335", "contents": "Title: Validity of Clusters Produced By kernel-$k$-means With Kernel-Trick Abstract: This paper corrects the proof of the Theorem 2 from the Gower's paper\n\\cite[page 5]{Gower:1982} as well as corrects the Theorem 7 from Gower's paper\n\\cite{Gower:1986}. The first correction is needed in order to establish the\nexistence of the kernel function used commonly in the kernel trick e.g. for\n$k$-means clustering algorithm, on the grounds of distance matrix. The\ncorrection encompasses the missing if-part proof and dropping unnecessary\nconditions. The second correction deals with transformation of the kernel\nmatrix into a one embeddable in Euclidean space. \n\n"}
{"id": "1701.06511", "contents": "Title: Aggressive Sampling for Multi-class to Binary Reduction with\n  Applications to Text Classification Abstract: We address the problem of multi-class classification in the case where the\nnumber of classes is very large. We propose a double sampling strategy on top\nof a multi-class to binary reduction strategy, which transforms the original\nmulti-class problem into a binary classification problem over pairs of\nexamples. The aim of the sampling strategy is to overcome the curse of\nlong-tailed class distributions exhibited in majority of large-scale\nmulti-class classification problems and to reduce the number of pairs of\nexamples in the expanded data. We show that this strategy does not alter the\nconsistency of the empirical risk minimization principle defined over the\ndouble sample reduction. Experiments are carried out on DMOZ and Wikipedia\ncollections with 10,000 to 100,000 classes where we show the efficiency of the\nproposed approach in terms of training and prediction time, memory consumption,\nand predictive performance with respect to state-of-the-art approaches. \n\n"}
{"id": "1701.07194", "contents": "Title: Privileged Multi-label Learning Abstract: This paper presents privileged multi-label learning (PrML) to explore and\nexploit the relationship between labels in multi-label learning problems. We\nsuggest that for each individual label, it cannot only be implicitly connected\nwith other labels via the low-rank constraint over label predictors, but also\nits performance on examples can receive the explicit comments from other labels\ntogether acting as an \\emph{Oracle teacher}. We generate privileged label\nfeature for each example and its individual label, and then integrate it into\nthe framework of low-rank based multi-label learning. The proposed algorithm\ncan therefore comprehensively explore and exploit label relationships by\ninheriting all the merits of privileged information and low-rank constraints.\nWe show that PrML can be efficiently solved by dual coordinate descent\nalgorithm using iterative optimization strategy with cheap updates. Experiments\non benchmark datasets show that through privileged label features, the\nperformance can be significantly improved and PrML is superior to several\ncompeting methods in most cases. \n\n"}
{"id": "1701.07875", "contents": "Title: Wasserstein GAN Abstract: We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions. \n\n"}
{"id": "1701.08573", "contents": "Title: Is the essence of a quantum game captured completely in the original\n  classical game? Abstract: S. J. van Enk and R. Pike in PRA 66, 024306 (2002) argue that the equilibrium\nsolution to a quantum game isn't unique but is already present in the classical\ngame itself. In this work, we contest this assertion by showing that a random\nstrategy in a particular quantum (Hawk-Dove) game is unique to the quantum\ngame. In other words, one cannot obtain the equilibrium solution of the quantum\nHawk-Dove game in the classical Hawk-Dove game. Moreover, we provide an\nanalytical solution to the quantum $2\\times2$ strategic form Hawk-Dove game\nusing randomly mixed strategies. The random strategy which we describe is\nPareto optimal with their payoff classically unobtainable. We compare quantum\nstrategies to correlated strategies and find that correlated strategies in the\nquantum Hawk-Dove game or quantum Prisoner's dilemma yield the Nash equilibrium\nsolution. \n\n"}
{"id": "1701.08759", "contents": "Title: Coherence and entanglement of mechanical oscillators mediated by\n  coupling to different baths Abstract: We study the non-equilibrium dynamics of two coupled mechanical oscillators\nwith general linear couplings to two uncorrelated thermal baths at temperatures\n$T_1$ and $T_2$, respectively. We obtain the complete solution of the\nHeisenberg-Langevin equations, which reveal a coherent mixing among the normal\nmodes of the oscillators as a consequence of their off-diagonal couplings to\nthe baths. Unique renormalization aspects resulting from this mixing are\ndiscussed. Diagonal and off-diagonal (coherence) correlation functions are\nobtained analytically in the case of strictly Ohmic baths with different\ncouplings in the strong and weak coupling regimes. An asymptotic\nnon-equilibrium stationary state emerges for which we obtain the complete\nexpressions for the correlations and coherence. Remarkably the coherence\nsurvives in the high temperature, classical limit for $T_1 \\neq T_2$. In the\ncase of vanishing detuning between the oscillator normal modes both coupling to\none and the same bath the coherence retains memory of the initial conditions at\nlong time. A perturbative expansion of the early time evolution reveals that\nthe emergence of coherence is a consequence of the entanglement between the\nnormal modes of the oscillators \\emph{mediated} by their couplings to the\nbaths. This \\emph{suggests} the survival of entanglement in the high\ntemperature limit for different temperatures of the baths which is essentially\na consequence of the non-equilibrium nature of the asymptotic stationary state.\nAn out of equilibrium setup with small detuning and large $|T_1- T_2|$ produces\nnon-vanishing steady-state coherence and entanglement in the high temperature\nlimit of the baths. \n\n"}
{"id": "1701.08984", "contents": "Title: Superradiance phase transition in the presence of parameter fluctuations Abstract: We theoretically analyze the effect of parameter fluctuations on the\nsuperradiance phase transition in a setup where a large number of\nsuperconducting qubits are coupled to a single cavity. We include parameter\nfluctuations that are typical of superconducting architectures, such as\nfluctuations in qubit gaps, bias points and qubit-cavity coupling strengths. We\nfind that the phase transition should occur in this case, although it manifests\nitself somewhat differently from the case with no fluctuations. We also find\nthat fluctuations in the qubit gaps and qubit-cavity coupling strengths do not\nnecessarily make it more difficult to reach the transition point. Fluctuations\nin the bias points, however, increase the coupling strength required to reach\nthe quantum phase transition point and enter the superradiant phase. Similarly,\nthese fluctuations lower the critical temperature for the thermal phase\ntransition. \n\n"}
{"id": "1702.01824", "contents": "Title: Predicting Pairwise Relations with Neural Similarity Encoders Abstract: Matrix factorization is at the heart of many machine learning algorithms, for\nexample, dimensionality reduction (e.g. kernel PCA) or recommender systems\nrelying on collaborative filtering. Understanding a singular value\ndecomposition (SVD) of a matrix as a neural network optimization problem\nenables us to decompose large matrices efficiently while dealing naturally with\nmissing values in the given matrix. But most importantly, it allows us to learn\nthe connection between data points' feature vectors and the matrix containing\ninformation about their pairwise relations. In this paper we introduce a novel\nneural network architecture termed Similarity Encoder (SimEc), which is\ndesigned to simultaneously factorize a given target matrix while also learning\nthe mapping to project the data points' feature vectors into a similarity\npreserving embedding space. This makes it possible to, for example, easily\ncompute out-of-sample solutions for new data points. Additionally, we\ndemonstrate that SimEc can preserve non-metric similarities and even predict\nmultiple pairwise relations between data points at once. \n\n"}
{"id": "1702.04121", "contents": "Title: Practical Learning of Predictive State Representations Abstract: Over the past decade there has been considerable interest in spectral\nalgorithms for learning Predictive State Representations (PSRs). Spectral\nalgorithms have appealing theoretical guarantees; however, the resulting models\ndo not always perform well on inference tasks in practice. One reason for this\nbehavior is the mismatch between the intended task (accurate filtering or\nprediction) and the loss function being optimized by the algorithm (estimation\nerror in model parameters).\n  A natural idea is to improve performance by refining PSRs using an algorithm\nsuch as EM. Unfortunately it is not obvious how to apply apply an EM style\nalgorithm in the context of PSRs as the Log Likelihood is not well defined for\nall PSRs. We show that it is possible to overcome this problem using ideas from\nPredictive State Inference Machines.\n  We combine spectral algorithms for PSRs as a consistent and efficient\ninitialization with PSIM-style updates to refine the resulting model\nparameters. By combining these two ideas we develop Inference Gradients, a\nsimple, fast, and robust method for practical learning of PSRs. Inference\nGradients performs gradient descent in the PSR parameter space to optimize an\ninference-based loss function like PSIM. Because Inference Gradients uses a\nspectral initialization we get the same consistency benefits as PSRs. We show\nthat Inference Gradients outperforms both PSRs and PSIMs on real and synthetic\ndata sets. \n\n"}
{"id": "1702.05327", "contents": "Title: Solving Equations of Random Convex Functions via Anchored Regression Abstract: We consider the question of estimating a solution to a system of equations\nthat involve convex nonlinearities, a problem that is common in machine\nlearning and signal processing. Because of these nonlinearities, conventional\nestimators based on empirical risk minimization generally involve solving a\nnon-convex optimization program. We propose anchored regression, a new approach\nbased on convex programming that amounts to maximizing a linear functional\n(perhaps augmented by a regularizer) over a convex set. The proposed convex\nprogram is formulated in the natural space of the problem, and avoids the\nintroduction of auxiliary variables, making it computationally favorable.\nWorking in the native space also provides great flexibility as structural\npriors (e.g., sparsity) can be seamlessly incorporated.\n  For our analysis, we model the equations as being drawn from a fixed set\naccording to a probability law. Our main results provide guarantees on the\naccuracy of the estimator in terms of the number of equations we are solving,\nthe amount of noise present, a measure of statistical complexity of the random\nequations, and the geometry of the regularizer at the true solution. We also\nprovide recipes for constructing the anchor vector (that determines the linear\nfunctional to maximize) directly from the observed data. \n\n"}
{"id": "1702.05471", "contents": "Title: Maximally Correlated Principal Component Analysis Abstract: In the era of big data, reducing data dimensionality is critical in many\nareas of science. Widely used Principal Component Analysis (PCA) addresses this\nproblem by computing a low dimensional data embedding that maximally explain\nvariance of the data. However, PCA has two major weaknesses. Firstly, it only\nconsiders linear correlations among variables (features), and secondly it is\nnot suitable for categorical data. We resolve these issues by proposing\nMaximally Correlated Principal Component Analysis (MCPCA). MCPCA computes\ntransformations of variables whose covariance matrix has the largest Ky Fan\nnorm. Variable transformations are unknown, can be nonlinear and are computed\nin an optimization. MCPCA can also be viewed as a multivariate extension of\nMaximal Correlation. For jointly Gaussian variables we show that the covariance\nmatrix corresponding to the identity (or the negative of the identity)\ntransformations majorizes covariance matrices of non-identity functions. Using\nthis result we characterize global MCPCA optimizers for nonlinear functions of\njointly Gaussian variables for every rank constraint. For categorical variables\nwe characterize global MCPCA optimizers for the rank one constraint based on\nthe leading eigenvector of a matrix computed using pairwise joint\ndistributions. For a general rank constraint we propose a block coordinate\ndescend algorithm and show its convergence to stationary points of the MCPCA\noptimization. We compare MCPCA with PCA and other state-of-the-art\ndimensionality reduction methods including Isomap, LLE, multilayer autoencoders\n(neural networks), kernel PCA, probabilistic PCA and diffusion maps on several\nsynthetic and real datasets. We show that MCPCA consistently provides improved\nperformance compared to other methods. \n\n"}
{"id": "1702.07360", "contents": "Title: Neural Decision Trees Abstract: In this paper we propose a synergistic melting of neural networks and\ndecision trees (DT) we call neural decision trees (NDT). NDT is an architecture\na la decision tree where each splitting node is an independent multilayer\nperceptron allowing oblique decision functions or arbritrary nonlinear decision\nfunction if more than one layer is used. This way, each MLP can be seen as a\nnode of the tree. We then show that with the weight sharing asumption among\nthose units, we end up with a Hashing Neural Network (HNN) which is a\nmultilayer perceptron with sigmoid activation function for the last layer as\nopposed to the standard softmax. The output units then jointly represent the\nprobability to be in a particular region. The proposed framework allows for\nglobal optimization as opposed to greedy in DT and differentiability w.r.t. all\nparameters and the input, allowing easy integration in any learnable pipeline,\nfor example after CNNs for computer vision tasks. We also demonstrate the\nmodeling power of HNN allowing to learn union of disjoint regions for final\nclustering or classification making it more general and powerful than standard\nsoftmax MLP requiring linear separability thus reducing the need on the inner\nlayer to perform complex data transformations. We finally show experiments for\nsupervised, semi-suppervised and unsupervised tasks and compare results with\nstandard DTs and MLPs. \n\n"}
{"id": "1702.07790", "contents": "Title: Activation Ensembles for Deep Neural Networks Abstract: Many activation functions have been proposed in the past, but selecting an\nadequate one requires trial and error. We propose a new methodology of\ndesigning activation functions within a neural network at each layer. We call\nthis technique an \"activation ensemble\" because it allows the use of multiple\nactivation functions at each layer. This is done by introducing additional\nvariables, $\\alpha$, at each activation layer of a network to allow for\nmultiple activation functions to be active at each neuron. By design,\nactivations with larger $\\alpha$ values at a neuron is equivalent to having the\nlargest magnitude. Hence, those higher magnitude activations are \"chosen\" by\nthe network. We implement the activation ensembles on a variety of datasets\nusing an array of Feed Forward and Convolutional Neural Networks. By using the\nactivation ensemble, we achieve superior results compared to traditional\ntechniques. In addition, because of the flexibility of this methodology, we\nmore deeply explore activation functions and the features that they capture. \n\n"}
{"id": "1702.08235", "contents": "Title: Variational Inference using Implicit Distributions Abstract: Generative adversarial networks (GANs) have given us a great tool to fit\nimplicit generative models to data. Implicit distributions are ones we can\nsample from easily, and take derivatives of samples with respect to model\nparameters. These models are highly expressive and we argue they can prove just\nas useful for variational inference (VI) as they are for generative modelling.\nSeveral papers have proposed GAN-like algorithms for inference, however,\nconnections to the theory of VI are not always well understood. This paper\nprovides a unifying review of existing algorithms establishing connections\nbetween variational autoencoders, adversarially learned inference, operator VI,\nGAN-based image reconstruction, and more. Secondly, the paper provides a\nframework for building new algorithms: depending on the way the variational\nbound is expressed we introduce prior-contrastive and joint-contrastive\nmethods, and show practical inference algorithms based on either density ratio\nestimation or denoising. \n\n"}
{"id": "1702.08533", "contents": "Title: Competing Bandits: Learning under Competition Abstract: Most modern systems strive to learn from interactions with users, and many\nengage in exploration: making potentially suboptimal choices for the sake of\nacquiring new information. We initiate a study of the interplay between\nexploration and competition--how such systems balance the exploration for\nlearning and the competition for users. Here the users play three distinct\nroles: they are customers that generate revenue, they are sources of data for\nlearning, and they are self-interested agents which choose among the competing\nsystems. In our model, we consider competition between two multi-armed bandit\nalgorithms faced with the same bandit instance. Users arrive one by one and\nchoose among the two algorithms, so that each algorithm makes progress if and\nonly if it is chosen. We ask whether and to what extent competition\nincentivizes the adoption of better bandit algorithms. We investigate this\nissue for several models of user response, as we vary the degree of rationality\nand competitiveness in the model. Our findings are closely related to the\n\"competition vs. innovation\" relationship, a well-studied theme in economics. \n\n"}
{"id": "1702.08608", "contents": "Title: Towards A Rigorous Science of Interpretable Machine Learning Abstract: As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning. \n\n"}
{"id": "1702.08670", "contents": "Title: On architectural choices in deep learning: From network structure to\n  gradient convergence and parameter estimation Abstract: We study mechanisms to characterize how the asymptotic convergence of\nbackpropagation in deep architectures, in general, is related to the network\nstructure, and how it may be influenced by other design choices including\nactivation type, denoising and dropout rate. We seek to analyze whether network\narchitecture and input data statistics may guide the choices of learning\nparameters and vice versa. Given the broad applicability of deep architectures,\nthis issue is interesting both from theoretical and a practical standpoint.\nUsing properties of general nonconvex objectives (with first-order\ninformation), we first build the association between structural, distributional\nand learnability aspects of the network vis-\\`a-vis their interaction with\nparameter convergence rates. We identify a nice relationship between feature\ndenoising and dropout, and construct families of networks that achieve the same\nlevel of convergence. We then derive a workflow that provides systematic\nguidance regarding the choice of network sizes and learning parameters often\nmediated4 by input statistics. Our technical results are corroborated by an\nextensive set of evaluations, presented in this paper as well as independent\nempirical observations reported by other groups. We also perform experiments\nshowing the practical implications of our framework for choosing the best\nfully-connected design for a given problem. \n\n"}
{"id": "1702.08712", "contents": "Title: Algorithmic stability and hypothesis complexity Abstract: We introduce a notion of algorithmic stability of learning algorithms---that\nwe term \\emph{argument stability}---that captures stability of the hypothesis\noutput by the learning algorithm in the normed space of functions from which\nhypotheses are selected. The main result of the paper bounds the generalization\nerror of any learning algorithm in terms of its argument stability. The bounds\nare based on martingale inequalities in the Banach space to which the\nhypotheses belong. We apply the general bounds to bound the performance of some\nlearning algorithms based on empirical risk minimization and stochastic\ngradient descent. \n\n"}
{"id": "1702.08720", "contents": "Title: Learning Discrete Representations via Information Maximizing\n  Self-Augmented Training Abstract: Learning discrete representations of data is a central machine learning task\nbecause of the compactness of the representations and ease of interpretation.\nThe task includes clustering and hash learning as special cases. Deep neural\nnetworks are promising to be used because they can model the non-linearity of\ndata and scale to large datasets. However, their model complexity is huge, and\ntherefore, we need to carefully regularize the networks in order to learn\nuseful representations that exhibit intended invariance for applications of\ninterest. To this end, we propose a method called Information Maximizing\nSelf-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose\nthe invariance on discrete representations. More specifically, we encourage the\npredicted representations of augmented data points to be close to those of the\noriginal data points in an end-to-end fashion. At the same time, we maximize\nthe information-theoretic dependency between data and their predicted discrete\nrepresentations. Extensive experiments on benchmark datasets show that IMSAT\nproduces state-of-the-art results for both clustering and unsupervised hash\nlearning. \n\n"}
{"id": "1703.00535", "contents": "Title: Human Interaction with Recommendation Systems Abstract: Many recommendation algorithms rely on user data to generate recommendations.\nHowever, these recommendations also affect the data obtained from future users.\nThis work aims to understand the effects of this dynamic interaction. We\npropose a simple model where users with heterogeneous preferences arrive over\ntime. Based on this model, we prove that naive estimators, i.e. those which\nignore this feedback loop, are not consistent. We show that consistent\nestimators are efficient in the presence of myopic agents. Our results are\nvalidated using extensive simulations. \n\n"}
{"id": "1703.00579", "contents": "Title: Active Learning for Accurate Estimation of Linear Models Abstract: We explore the sequential decision making problem where the goal is to\nestimate uniformly well a number of linear models, given a shared budget of\nrandom contexts independently sampled from a known distribution. The decision\nmaker must query one of the linear models for each incoming context, and\nreceives an observation corrupted by noise levels that are unknown, and depend\non the model instance. We present Trace-UCB, an adaptive allocation algorithm\nthat learns the noise levels while balancing contexts accordingly across the\ndifferent linear functions, and derive guarantees for simple regret in both\nexpectation and high-probability. Finally, we extend the algorithm and its\nguarantees to high dimensional settings, where the number of linear models\ntimes the dimension of the contextual space is higher than the total budget of\nsamples. Simulations with real data suggest that Trace-UCB is remarkably\nrobust, outperforming a number of baselines even when its assumptions are\nviolated. \n\n"}
{"id": "1703.00676", "contents": "Title: A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels Abstract: Non-linear kernel methods can be approximated by fast linear ones using\nsuitable explicit feature maps allowing their application to large scale\nproblems. We investigate how convolution kernels for structured data are\ncomposed from base kernels and construct corresponding feature maps. On this\nbasis we propose exact and approximative feature maps for widely used graph\nkernels based on the kernel trick. We analyze for which kernels and graph\nproperties computation by explicit feature maps is feasible and actually more\nefficient. In particular, we derive approximative, explicit feature maps for\nstate-of-the-art kernels supporting real-valued attributes including the\nGraphHopper and graph invariant kernels. In extensive experiments we show that\nour approaches often achieve a classification accuracy close to the exact\nmethods based on the kernel trick, but require only a fraction of their running\ntime. Moreover, we propose and analyze algorithms for computing random walk,\nshortest-path and subgraph matching kernels by explicit and implicit feature\nmaps. Our theoretical results are confirmed experimentally by observing a phase\ntransition when comparing running time with respect to label diversity, walk\nlengths and subgraph size, respectively. \n\n"}
{"id": "1703.00839", "contents": "Title: Encrypted accelerated least squares regression Abstract: Information that is stored in an encrypted format is, by definition, usually\nnot amenable to statistical analysis or machine learning methods. In this paper\nwe present detailed analysis of coordinate and accelerated gradient descent\nalgorithms which are capable of fitting least squares and penalised ridge\nregression models, using data encrypted under a fully homomorphic encryption\nscheme. Gradient descent is shown to dominate in terms of encrypted\ncomputational speed, and theoretical results are proven to give parameter\nbounds which ensure correctness of decryption. The characteristics of encrypted\ncomputation are empirically shown to favour a non-standard acceleration\ntechnique. This demonstrates the possibility of approximating conventional\nstatistical regression methods using encrypted data without compromising\nprivacy. \n\n"}
{"id": "1703.01253", "contents": "Title: Machine Learning on Sequential Data Using a Recurrent Weighted Average Abstract: Recurrent Neural Networks (RNN) are a type of statistical model designed to\nhandle sequential data. The model reads a sequence one symbol at a time. Each\nsymbol is processed based on information collected from the previous symbols.\nWith existing RNN architectures, each symbol is processed using only\ninformation from the previous processing step. To overcome this limitation, we\npropose a new kind of RNN model that computes a recurrent weighted average\n(RWA) over every past processing step. Because the RWA can be computed as a\nrunning average, the computational overhead scales like that of any other RNN\narchitecture. The approach essentially reformulates the attention mechanism\ninto a stand-alone model. The performance of the RWA model is assessed on the\nvariable copy problem, the adding problem, classification of artificial\ngrammar, classification of sequences by length, and classification of the MNIST\nimages (where the pixels are read sequentially one at a time). On almost every\ntask, the RWA model is found to outperform a standard LSTM model. \n\n"}
{"id": "1703.02596", "contents": "Title: Customer Lifetime Value Prediction Using Embeddings Abstract: We describe the Customer LifeTime Value (CLTV) prediction system deployed at\nASOS.com, a global online fashion retailer. CLTV prediction is an important\nproblem in e-commerce where an accurate estimate of future value allows\nretailers to effectively allocate marketing spend, identify and nurture high\nvalue customers and mitigate exposure to losses. The system at ASOS provides\ndaily estimates of the future value of every customer and is one of the\ncornerstones of the personalised shopping experience. The state of the art in\nthis domain uses large numbers of handcrafted features and ensemble regressors\nto forecast value, predict churn and evaluate customer loyalty. Recently,\ndomains including language, vision and speech have shown dramatic advances by\nreplacing handcrafted features with features that are learned automatically\nfrom data. We detail the system deployed at ASOS and show that learning feature\nrepresentations is a promising extension to the state of the art in CLTV\nmodelling. We propose a novel way to generate embeddings of customers, which\naddresses the issue of the ever changing product catalogue and obtain a\nsignificant improvement over an exhaustive set of handcrafted features. \n\n"}
{"id": "1703.03020", "contents": "Title: Spectral Graph Convolutions for Population-based Disease Prediction Abstract: Exploiting the wealth of imaging and non-imaging information for disease\nprediction tasks requires models capable of representing, at the same time,\nindividual features as well as data associations between subjects from\npotentially large populations. Graphs provide a natural framework for such\ntasks, yet previous graph-based approaches focus on pairwise similarities\nwithout modelling the subjects' individual characteristics and features. On the\nother hand, relying solely on subject-specific imaging feature vectors fails to\nmodel the interaction and similarity between subjects, which can reduce\nperformance. In this paper, we introduce the novel concept of Graph\nConvolutional Networks (GCN) for brain analysis in populations, combining\nimaging and non-imaging data. We represent populations as a sparse graph where\nits vertices are associated with image-based feature vectors and the edges\nencode phenotypic information. This structure was used to train a GCN model on\npartially labelled graphs, aiming to infer the classes of unlabelled nodes from\nthe node features and pairwise associations between subjects. We demonstrate\nthe potential of the method on the challenging ADNI and ABIDE databases, as a\nproof of concept of the benefit from integrating contextual information in\nclassification tasks. This has a clear impact on the quality of the\npredictions, leading to 69.5% accuracy for ABIDE (outperforming the current\nstate of the art of 66.8%) and 77% for ADNI for prediction of MCI conversion,\nsignificantly outperforming standard linear classifiers where only individual\nfeatures are considered. \n\n"}
{"id": "1703.03152", "contents": "Title: Fidelity witnesses for fermionic quantum simulations Abstract: The experimental interest and developments in quantum spin-1/2-chains has\nincreased uninterruptedly over the last decade. In many instances, the target\nquantum simulation belongs to the broader class of non-interacting fermionic\nmodels, constituting an important benchmark. In spite of this class being\nanalytically efficiently tractable, no direct certification tool has yet been\nreported for it. In fact, in experiments, certification has almost exclusively\nrelied on notions of quantum state tomography scaling very unfavorably with the\nsystem size. Here, we develop experimentally-friendly fidelity witnesses for\nall pure fermionic Gaussian target states. Their expectation value yields a\ntight lower bound to the fidelity and can be measured efficiently. We derive\nwitnesses in full generality in the Majorana-fermion representation and apply\nthem to experimentally relevant spin-1/2 chains. Among others, we show how to\nefficiently certify strongly out-of-equilibrium dynamics in critical Ising\nchains. At the heart of the measurement scheme is a variant of importance\nsampling specially tailored to overlaps between covariance matrices. The method\nis shown to be robust against finite experimental-state infidelities. \n\n"}
{"id": "1703.04025", "contents": "Title: Learning Large-Scale Bayesian Networks with the sparsebn Package Abstract: Learning graphical models from data is an important problem with wide\napplications, ranging from genomics to the social sciences. Nowadays datasets\noften have upwards of thousands---sometimes tens or hundreds of thousands---of\nvariables and far fewer samples. To meet this challenge, we have developed a\nnew R package called sparsebn for learning the structure of large, sparse\ngraphical models with a focus on Bayesian networks. While there are many\nexisting software packages for this task, this package focuses on the unique\nsetting of learning large networks from high-dimensional data, possibly with\ninterventions. As such, the methods provided place a premium on scalability and\nconsistency in a high-dimensional setting. Furthermore, in the presence of\ninterventions, the methods implemented here achieve the goal of learning a\ncausal network from data. Additionally, the sparsebn package is fully\ncompatible with existing software packages for network analysis. \n\n"}
{"id": "1703.04318", "contents": "Title: Blocking Transferability of Adversarial Examples in Black-Box Learning\n  Systems Abstract: Advances in Machine Learning (ML) have led to its adoption as an integral\ncomponent in many applications, including banking, medical diagnosis, and\ndriverless cars. To further broaden the use of ML models, cloud-based services\noffered by Microsoft, Amazon, Google, and others have developed ML-as-a-service\ntools as black-box systems. However, ML classifiers are vulnerable to\nadversarial examples: inputs that are maliciously modified can cause the\nclassifier to provide adversary-desired outputs. Moreover, it is known that\nadversarial examples generated on one classifier are likely to cause another\nclassifier to make the same mistake, even if the classifiers have different\narchitectures or are trained on disjoint datasets. This property, which is\nknown as transferability, opens up the possibility of attacking black-box\nsystems by generating adversarial examples on a substitute classifier and\ntransferring the examples to the target classifier. Therefore, the key to\nprotect black-box learning systems against the adversarial examples is to block\ntheir transferability. To this end, we propose a training method that, as the\ninput is more perturbed, the classifier smoothly outputs lower confidence on\nthe original label and instead predicts that the input is \"invalid\". In\nessence, we augment the output class set with a NULL label and train the\nclassifier to reject the adversarial examples by classifying them as NULL. In\nexperiments, we apply a wide range of attacks based on adversarial examples on\nthe black-box systems. We show that a classifier trained with the proposed\nmethod effectively resists against the adversarial examples, while maintaining\nthe accuracy on clean data. \n\n"}
{"id": "1703.04908", "contents": "Title: Emergence of Grounded Compositional Language in Multi-Agent Populations Abstract: By capturing statistical patterns in large corpora, machine learning has\nenabled significant advances in natural language processing, including in\nmachine translation, question answering, and sentiment analysis. However, for\nagents to intelligently interact with humans, simply capturing the statistical\npatterns is insufficient. In this paper we investigate if, and how, grounded\ncompositional language can emerge as a means to achieve goals in multi-agent\npopulations. Towards this end, we propose a multi-agent learning environment\nand learning methods that bring about emergence of a basic compositional\nlanguage. This language is represented as streams of abstract discrete symbols\nuttered by agents over time, but nonetheless has a coherent structure that\npossesses a defined vocabulary and syntax. We also observe emergence of\nnon-verbal communication such as pointing and guiding when language\ncommunication is unavailable. \n\n"}
{"id": "1703.05921", "contents": "Title: Unsupervised Anomaly Detection with Generative Adversarial Networks to\n  Guide Marker Discovery Abstract: Obtaining models that capture imaging markers relevant for disease\nprogression and treatment monitoring is challenging. Models are typically based\non large amounts of data with annotated examples of known markers aiming at\nautomating detection. High annotation effort and the limitation to a vocabulary\nof known markers limit the power of such approaches. Here, we perform\nunsupervised learning to identify anomalies in imaging data as candidates for\nmarkers. We propose AnoGAN, a deep convolutional generative adversarial network\nto learn a manifold of normal anatomical variability, accompanying a novel\nanomaly scoring scheme based on the mapping from image space to a latent space.\nApplied to new data, the model labels anomalies, and scores image patches\nindicating their fit into the learned distribution. Results on optical\ncoherence tomography images of the retina demonstrate that the approach\ncorrectly identifies anomalous images, such as images containing retinal fluid\nor hyperreflective foci. \n\n"}
{"id": "1703.06413", "contents": "Title: Rotating Gaussian wave packets in weak external potentials Abstract: We address the time evolution of two- and three-dimensional nonrelativistic\nGaussian wave packets in the presence of a weak external potential of arbitrary\nfunctional form. The focus of our study is the phenomenon of rotation of a\nGaussian wave packet around its center of mass, as quantified by mean angular\nmomentum computed relative to the wave packet center. Using a semiclassical\napproximation of the eikonal type, we derive an explicit formula for a\ntime-dependent change of mean angular momentum of a wave packet induced by its\ninteraction with a weak external potential. As an example, we apply our\nanalytical approach to the scenario of a two-dimensional quantum particle\ncrossing a tilted ridge potential barrier. In particular, we demonstrate that\nthe initial orientation of the particle wave packet determines the sense of its\nrotation, and report a good agreement between analytical and numerical results. \n\n"}
{"id": "1703.07807", "contents": "Title: Learning to Partition using Score Based Compatibilities Abstract: We study the problem of learning to partition users into groups, where one\nmust learn the compatibilities between the users to achieve optimal groupings.\nWe define four natural objectives that optimize for average and worst case\ncompatibilities and propose new algorithms for adaptively learning optimal\ngroupings. When we do not impose any structure on the compatibilities, we show\nthat the group formation objectives considered are $NP$ hard to solve and we\neither give approximation guarantees or prove inapproximability results. We\nthen introduce an elegant structure, namely that of \\textit{intrinsic scores},\nthat makes many of these problems polynomial time solvable. We explicitly\ncharacterize the optimal groupings under this structure and show that the\noptimal solutions are related to \\emph{homophilous} and \\emph{heterophilous}\npartitions, well-studied in the psychology literature. For one of the four\nobjectives, we show $NP$ hardness under the score structure and give a\n$\\frac{1}{2}$ approximation algorithm for which no constant approximation was\nknown thus far. Finally, under the score structure, we propose an online low\nsample complexity PAC algorithm for learning the optimal partition. We\ndemonstrate the efficacy of the proposed algorithm on synthetic and real world\ndatasets. \n\n"}
{"id": "1703.08840", "contents": "Title: InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations Abstract: The goal of imitation learning is to mimic expert behavior without access to\nan explicit reward signal. Expert demonstrations provided by humans, however,\noften show significant variability due to latent factors that are typically not\nexplicitly modeled. In this paper, we propose a new algorithm that can infer\nthe latent structure of expert demonstrations in an unsupervised way. Our\nmethod, built on top of Generative Adversarial Imitation Learning, can not only\nimitate complex behaviors, but also learn interpretable and meaningful\nrepresentations of complex behavioral data, including visual demonstrations. In\nthe driving domain, we show that a model learned from human demonstrations is\nable to both accurately reproduce a variety of behaviors and accurately\nanticipate human actions using raw visual inputs. Compared with various\nbaselines, our method can better capture the latent structure underlying expert\ndemonstrations, often recovering semantically meaningful factors of variation\nin the data. \n\n"}
{"id": "1703.09310", "contents": "Title: Adaptive Simulation-based Training of AI Decision-makers using Bayesian\n  Optimization Abstract: This work studies how an AI-controlled dog-fighting agent with tunable\ndecision-making parameters can learn to optimize performance against an\nintelligent adversary, as measured by a stochastic objective function evaluated\non simulated combat engagements. Gaussian process Bayesian optimization (GPBO)\ntechniques are developed to automatically learn global Gaussian Process (GP)\nsurrogate models, which provide statistical performance predictions in both\nexplored and unexplored areas of the parameter space. This allows a learning\nengine to sample full-combat simulations at parameter values that are most\nlikely to optimize performance and also provide highly informative data points\nfor improving future predictions. However, standard GPBO methods do not provide\na reliable surrogate model for the highly volatile objective functions found in\naerial combat, and thus do not reliably identify global maxima. These issues\nare addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point\nSampling (HRMS) techniques. Simulation studies show that HRMS improves the\naccuracy of GP surrogate models, allowing AI decision-makers to more accurately\npredict performance and efficiently tune parameters. \n\n"}
{"id": "1703.09370", "contents": "Title: Ensembles of Deep LSTM Learners for Activity Recognition using Wearables Abstract: Recently, deep learning (DL) methods have been introduced very successfully\ninto human activity recognition (HAR) scenarios in ubiquitous and wearable\ncomputing. Especially the prospect of overcoming the need for manual feature\ndesign combined with superior classification capabilities render deep neural\nnetworks very attractive for real-life HAR application. Even though DL-based\napproaches now outperform the state-of-the-art in a number of recognitions\ntasks of the field, yet substantial challenges remain. Most prominently, issues\nwith real-life datasets, typically including imbalanced datasets and\nproblematic data quality, still limit the effectiveness of activity recognition\nusing wearables. In this paper we tackle such challenges through Ensembles of\ndeep Long Short Term Memory (LSTM) networks. We have developed modified\ntraining procedures for LSTM networks and combine sets of diverse LSTM learners\ninto classifier collectives. We demonstrate, both formally and empirically,\nthat Ensembles of deep LSTM learners outperform the individual LSTM networks.\nThrough an extensive experimental evaluation on three standard benchmarks\n(Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition\ncapabilities of our approach and its potential for real-life applications of\nhuman activity recognition. \n\n"}
{"id": "1703.09938", "contents": "Title: Grouped Convolutional Neural Networks for Multivariate Time Series Abstract: Analyzing multivariate time series data is important for many applications\nsuch as automated control, fault diagnosis and anomaly detection. One of the\nkey challenges is to learn latent features automatically from dynamically\nchanging multivariate input. In visual recognition tasks, convolutional neural\nnetworks (CNNs) have been successful to learn generalized feature extractors\nwith shared parameters over the spatial domain. However, when high-dimensional\nmultivariate time series is given, designing an appropriate CNN model structure\nbecomes challenging because the kernels may need to be extended through the\nfull dimension of the input volume. To address this issue, we present two\nstructure learning algorithms for deep CNN models. Our algorithms exploit the\ncovariance structure over multiple time series to partition input volume into\ngroups. The first algorithm learns the group CNN structures explicitly by\nclustering individual input sequences. The second algorithm learns the group\nCNN structures implicitly from the error backpropagation. In experiments with\ntwo real-world datasets, we demonstrate that our group CNNs outperform existing\nCNN based regression methods. \n\n"}
{"id": "1703.09947", "contents": "Title: Efficient Private ERM for Smooth Objectives Abstract: In this paper, we consider efficient differentially private empirical risk\nminimization from the viewpoint of optimization algorithms. For strongly convex\nand smooth objectives, we prove that gradient descent with output perturbation\nnot only achieves nearly optimal utility, but also significantly improves the\nrunning time of previous state-of-the-art private optimization algorithms, for\nboth $\\epsilon$-DP and $(\\epsilon, \\delta)$-DP. For non-convex but smooth\nobjectives, we propose an RRPSGD (Random Round Private Stochastic Gradient\nDescent) algorithm, which provably converges to a stationary point with privacy\nguarantee. Besides the expected utility bounds, we also provide guarantees in\nhigh probability form. Experiments demonstrate that our algorithm consistently\noutperforms existing method in both utility and running time. \n\n"}
{"id": "1704.00773", "contents": "Title: A comparative study of counterfactual estimators Abstract: We provide a comparative study of several widely used off-policy estimators\n(Empirical Average, Basic Importance Sampling and Normalized Importance\nSampling), detailing the different regimes where they are individually\nsuboptimal. We then exhibit properties optimal estimators should possess. In\nthe case where examples have been gathered using multiple policies, we show\nthat fused estimators dominate basic ones but can still be improved. \n\n"}
{"id": "1704.01423", "contents": "Title: Optimal control of superconducting gmon qubits using Pontryagin's\n  minimum principle: preparing a maximally entangled state with singular\n  bang-bang protocols Abstract: We apply the theory of optimal control to the dynamics of two \"gmon\" qubits,\nwith the goal of preparing a desired entangled ground state from an initial\nunentangled one. Given an initial state, a target state, and a Hamiltonian with\na set of permissible controls, can we reach the target state with coherent\nquantum evolution and, in that case, what is the minimum time required? The\nadiabatic theorem provides a far from optimal solution in the presence of a\nspectral gap. Optimal control yields the fastest possible way of reaching the\ntarget state and helps identify unreachable states. In the context of a simple\nquantum system, we provide examples of both reachable and unreachable target\nground states and show that the unreachability is due to a symmetry. We find\nthe optimal protocol in the reachable case using three different approaches:\n(i) a brute-force numerical minimization (ii) an efficient numerical\nminimization using the bang-bang ansatz expected from the Pontryagin minimum\nprinciple, and (iii) direct solution of the Pontryagin boundary value problem,\nwhich yields an analytical understanding of the numerically obtained optimal\nprotocols. Interestingly, our system provides an example of singular control,\nwhere the Pontryagin theorem does not guarantee bang-bang protocols.\nNevertheless, all three approaches give the same bang-bang protocol. \n\n"}
{"id": "1704.02718", "contents": "Title: Distributed Learning for Cooperative Inference Abstract: We study the problem of cooperative inference where a group of agents\ninteract over a network and seek to estimate a joint parameter that best\nexplains a set of observations. Agents do not know the network topology or the\nobservations of other agents. We explore a variational interpretation of the\nBayesian posterior density, and its relation to the stochastic mirror descent\nalgorithm, to propose a new distributed learning algorithm. We show that, under\nappropriate assumptions, the beliefs generated by the proposed algorithm\nconcentrate around the true parameter exponentially fast. We provide explicit\nnon-asymptotic bounds for the convergence rate. Moreover, we develop explicit\nand computationally efficient algorithms for observation models belonging to\nexponential families. \n\n"}
{"id": "1704.03144", "contents": "Title: Parametric Gaussian Process Regression for Big Data Abstract: This work introduces the concept of parametric Gaussian processes (PGPs),\nwhich is built upon the seemingly self-contradictory idea of making Gaussian\nprocesses parametric. Parametric Gaussian processes, by construction, are\ndesigned to operate in \"big data\" regimes where one is interested in\nquantifying the uncertainty associated with noisy data. The proposed\nmethodology circumvents the well-established need for stochastic variational\ninference, a scalable algorithm for approximating posterior distributions. The\neffectiveness of the proposed approach is demonstrated using an illustrative\nexample with simulated data and a benchmark dataset in the airline industry\nwith approximately 6 million records. \n\n"}
{"id": "1704.03296", "contents": "Title: Interpretable Explanations of Black Boxes by Meaningful Perturbation Abstract: As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks \"look\" in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations. \n\n"}
{"id": "1704.04997", "contents": "Title: Multimodal Prediction and Personalization of Photo Edits with Deep\n  Generative Models Abstract: Professional-grade software applications are powerful but\ncomplicated$-$expert users can achieve impressive results, but novices often\nstruggle to complete even basic tasks. Photo editing is a prime example: after\nloading a photo, the user is confronted with an array of cryptic sliders like\n\"clarity\", \"temp\", and \"highlights\". An automatically generated suggestion\ncould help, but there is no single \"correct\" edit for a given image$-$different\nexperts may make very different aesthetic decisions when faced with the same\nimage, and a single expert may make different choices depending on the intended\nuse of the image (or on a whim). We therefore want a system that can propose\nmultiple diverse, high-quality edits while also learning from and adapting to a\nuser's aesthetic preferences. In this work, we develop a statistical model that\nmeets these objectives. Our model builds on recent advances in neural network\ngenerative modeling and scalable inference, and uses hierarchical structure to\nlearn editing patterns across many diverse users. Empirically, we find that our\nmodel outperforms other approaches on this challenging multimodal prediction\ntask. \n\n"}
{"id": "1704.06001", "contents": "Title: Fast Generation for Convolutional Autoregressive Models Abstract: Convolutional autoregressive models have recently demonstrated\nstate-of-the-art performance on a number of generation tasks. While fast,\nparallel training methods have been crucial for their success, generation is\ntypically implemented in a na\\\"{i}ve fashion where redundant computations are\nunnecessarily repeated. This results in slow generation, making such models\ninfeasible for production environments. In this work, we describe a method to\nspeed up generation in convolutional autoregressive models. The key idea is to\ncache hidden states to avoid redundant computation. We apply our fast\ngeneration method to the Wavenet and PixelCNN++ models and achieve up to\n$21\\times$ and $183\\times$ speedups respectively. \n\n"}
{"id": "1704.06256", "contents": "Title: Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption Abstract: We consider the robust phase retrieval problem of recovering the unknown\nsignal from the magnitude-only measurements, where the measurements can be\ncontaminated by both sparse arbitrary corruption and bounded random noise. We\npropose a new nonconvex algorithm for robust phase retrieval, namely Robust\nWirtinger Flow to jointly estimate the unknown signal and the sparse\ncorruption. We show that our proposed algorithm is guaranteed to converge\nlinearly to the unknown true signal up to a minimax optimal statistical\nprecision in such a challenging setting. Compared with existing robust phase\nretrieval methods, we achieve an optimal sample complexity of $O(n)$ in both\nnoisy and noise-free settings. Thorough experiments on both synthetic and real\ndatasets corroborate our theory. \n\n"}
{"id": "1704.07066", "contents": "Title: Superradiance with local phase-breaking effects Abstract: We study the superradiant evolution of a set of $N$ two-level systems\nspontaneously radiating under the effect of phase-breaking mechanisms. We\ninvestigate the dynamics generated by non-radiative losses and pure dephasing,\nand their interplay with spontaneous emission. Our results show that in the\nparameter region relevant to many solid-state cavity quantum electrodynamics\nexperiments, even with a dephasing rate much faster than the radiative lifetime\nof a single two-level system, a sub-optimal collective superfluorescent burst\nis still observable. We also apply our theory to the dilute excitation regime,\noften used to describe optical excitations in solid-state systems. In this\nregime, excitations can be described in terms of bright and dark bosonic\nquasiparticles. We show how the effect of dephasing and losses in this regime\ntranslates into inter-mode scattering rates and quasiparticle lifetimes. \n\n"}
{"id": "1704.07352", "contents": "Title: Structured low-rank matrix learning: algorithms and applications Abstract: We consider the problem of learning a low-rank matrix, constrained to lie in\na linear subspace, and introduce a novel factorization for modeling such\nmatrices. A salient feature of the proposed factorization scheme is it\ndecouples the low-rank and the structural constraints onto separate factors. We\nformulate the optimization problem on the Riemannian spectrahedron manifold,\nwhere the Riemannian framework allows to develop computationally efficient\nconjugate gradient and trust-region algorithms. Experiments on problems such as\nstandard/robust/non-negative matrix completion, Hankel matrix learning and\nmulti-task learning demonstrate the efficacy of our approach. A shorter version\nof this work has been published in ICML'18. \n\n"}
{"id": "1704.08067", "contents": "Title: Exploiting random projections and sparsity with random forests and\n  gradient boosting methods -- Application to multi-label and multi-output\n  learning, random forest model compression and leveraging input sparsity Abstract: Within machine learning, the supervised learning field aims at modeling the\ninput-output relationship of a system, from past observations of its behavior.\nDecision trees characterize the input-output relationship through a series of\nnested $if-then-else$ questions, the testing nodes, leading to a set of\npredictions, the leaf nodes. Several of such trees are often combined together\nfor state-of-the-art performance: random forest ensembles average the\npredictions of randomized decision trees trained independently in parallel,\nwhile tree boosting ensembles train decision trees sequentially to refine the\npredictions made by the previous ones. The emergence of new applications\nrequires scalable supervised learning algorithms in terms of computational\npower and memory space with respect to the number of inputs, outputs, and\nobservations without sacrificing accuracy. In this thesis, we identify three\nmain areas where decision tree methods could be improved for which we provide\nand evaluate original algorithmic solutions: (i) learning over high dimensional\noutput spaces, (ii) learning with large sample datasets and stringent memory\nconstraints at prediction time and (iii) learning over high dimensional sparse\ninput spaces. \n\n"}
{"id": "1705.00557", "contents": "Title: Discourse-Based Objectives for Fast Unsupervised Sentence Representation\n  Learning Abstract: This work presents a novel objective function for the unsupervised training\nof neural network sentence encoders. It exploits signals from paragraph-level\ndiscourse coherence to train these models to understand text. Our objective is\npurely discriminative, allowing us to train models many times faster than was\npossible under prior methods, and it yields models which perform well in\nextrinsic evaluations. \n\n"}
{"id": "1705.00565", "contents": "Title: Reinforcement Learning in Different Phases of Quantum Control Abstract: The ability to prepare a physical system in a desired quantum state is\ncentral to many areas of physics such as nuclear magnetic resonance, cold\natoms, and quantum computing. Yet, preparing states quickly and with high\nfidelity remains a formidable challenge. In this work we implement cutting-edge\nReinforcement Learning (RL) techniques and show that their performance is\ncomparable to optimal control methods in the task of finding short,\nhigh-fidelity driving protocol from an initial to a target state in\nnon-integrable many-body quantum systems of interacting qubits. RL methods\nlearn about the underlying physical system solely through a single scalar\nreward (the fidelity of the resulting state) calculated from numerical\nsimulations of the physical system. We further show that quantum state\nmanipulation, viewed as an optimization problem, exhibits a spin-glass-like\nphase transition in the space of protocols as a function of the protocol\nduration. Our RL-aided approach helps identify variational protocols with\nnearly optimal fidelity, even in the glassy phase, where optimal state\nmanipulation is exponentially hard. This study highlights the potential\nusefulness of RL for applications in out-of-equilibrium quantum physics. \n\n"}
{"id": "1705.01936", "contents": "Title: Learning with Confident Examples: Rank Pruning for Robust Classification\n  with Noisy Labels Abstract: Noisy PN learning is the problem of binary classification when training\nexamples may be mislabeled (flipped) uniformly with noise rate rho1 for\npositive examples and rho0 for negative examples. We propose Rank Pruning (RP)\nto solve noisy PN learning and the open problem of estimating the noise rates,\ni.e. the fraction of wrong positive and negative labels. Unlike prior\nsolutions, RP is time-efficient and general, requiring O(T) for any\nunrestricted choice of probabilistic classifier with T fitting time. We prove\nRP has consistent noise estimation and equivalent expected risk as learning\nwith uncorrupted labels in ideal conditions, and derive closed-form solutions\nwhen conditions are non-ideal. RP achieves state-of-the-art noise estimation\nand F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the\namount of noise and performs similarly impressively when a large portion of\ntraining examples are noise drawn from a third distribution. To highlight, RP\nwith a CNN classifier can predict if an MNIST digit is a \"one\"or \"not\" with\nonly 0.25% error, and 0.46 error across all digits, even when 50% of positive\nexamples are mislabeled and 50% of observed positive labels are mislabeled\nnegative examples. \n\n"}
{"id": "1705.02047", "contents": "Title: Matrix Completion via Factorizing Polynomials Abstract: Predicting unobserved entries of a partially observed matrix has found wide\napplicability in several areas, such as recommender systems, computational\nbiology, and computer vision. Many scalable methods with rigorous theoretical\nguarantees have been developed for algorithms where the matrix is factored into\nlow-rank components, and embeddings are learned for the row and column\nentities. While there has been recent research on incorporating explicit side\ninformation in the low-rank matrix factorization setting, often implicit\ninformation can be gleaned from the data, via higher-order interactions among\nentities. Such implicit information is especially useful in cases where the\ndata is very sparse, as is often the case in real-world datasets. In this\npaper, we design a method to learn embeddings in the context of recommendation\nsystems, using the observation that higher powers of a graph transition\nprobability matrix encode the probability that a random walker will hit that\nnode in a given number of steps. We develop a coordinate descent algorithm to\nsolve the resulting optimization, that makes explicit computation of the higher\norder powers of the matrix redundant, preserving sparsity and making\ncomputations efficient. Experiments on several datasets show that our method,\nthat can use higher order information, outperforms methods that only use\nexplicitly available side information, those that use only second-order\nimplicit information and in some cases, methods based on deep neural networks\nas well. \n\n"}
{"id": "1705.02224", "contents": "Title: Detecting Adversarial Samples Using Density Ratio Estimates Abstract: Machine learning models, especially based on deep architectures are used in\neveryday applications ranging from self driving cars to medical diagnostics. It\nhas been shown that such models are dangerously susceptible to adversarial\nsamples, indistinguishable from real samples to human eye, adversarial samples\nlead to incorrect classifications with high confidence. Impact of adversarial\nsamples is far-reaching and their efficient detection remains an open problem.\nWe propose to use direct density ratio estimation as an efficient model\nagnostic measure to detect adversarial samples. Our proposed method works\nequally well with single and multi-channel samples, and with different\nadversarial sample generation methods. We also propose a method to use density\nratio estimates for generating adversarial samples with an added constraint of\npreserving density ratio. \n\n"}
{"id": "1705.04293", "contents": "Title: Bayesian Approaches to Distribution Regression Abstract: Distribution regression has recently attracted much interest as a generic\nsolution to the problem of supervised learning where labels are available at\nthe group level, rather than at the individual level. Current approaches,\nhowever, do not propagate the uncertainty in observations due to sampling\nvariability in the groups. This effectively assumes that small and large groups\nare estimated equally well, and should have equal weight in the final\nregression. We account for this uncertainty with a Bayesian distribution\nregression formalism, improving the robustness and performance of the model\nwhen group sizes vary. We frame our models in a neural network style, allowing\nfor simple MAP inference using backpropagation to learn the parameters, as well\nas MCMC-based inference which can fully propagate uncertainty. We demonstrate\nour approach on illustrative toy datasets, as well as on a challenging problem\nof predicting age from images. \n\n"}
{"id": "1705.05598", "contents": "Title: Learning how to explain neural networks: PatternNet and\n  PatternAttribution Abstract: DeConvNet, Guided BackProp, LRP, were invented to better understand deep\nneural networks. We show that these methods do not produce the theoretically\ncorrect explanation for a linear model. Yet they are used on multi-layer\nnetworks with millions of parameters. This is a cause for concern since linear\nmodels are simple neural networks. We argue that explanation methods for neural\nnets should work reliably in the limit of simplicity, the linear models. Based\non our analysis of linear models we propose a generalization that yields two\nexplanation techniques (PatternNet and PatternAttribution) that are\ntheoretically sound for linear models and produce improved explanations for\ndeep networks. \n\n"}
{"id": "1705.07038", "contents": "Title: The Landscape of Deep Learning Algorithms Abstract: This paper studies the landscape of empirical risk of deep neural networks by\ntheoretically analyzing its convergence behavior to the population risk as well\nas its stationary points and properties. For an $l$-layer linear neural\nnetwork, we prove its empirical risk uniformly converges to its population risk\nat the rate of $\\mathcal{O}(r^{2l}\\sqrt{d\\log(l)}/\\sqrt{n})$ with training\nsample size of $n$, the total weight dimension of $d$ and the magnitude bound\n$r$ of weight of each layer. We then derive the stability and generalization\nbounds for the empirical risk based on this result. Besides, we establish the\nuniform convergence of gradient of the empirical risk to its population\ncounterpart. We prove the one-to-one correspondence of the non-degenerate\nstationary points between the empirical and population risks with convergence\nguarantees, which describes the landscape of deep neural networks. In addition,\nwe analyze these properties for deep nonlinear neural networks with sigmoid\nactivation functions. We prove similar results for convergence behavior of\ntheir empirical risks as well as the gradients and analyze properties of their\nnon-degenerate stationary points.\n  To our best knowledge, this work is the first one theoretically\ncharacterizing landscapes of deep learning algorithms. Besides, our results\nprovide the sample complexity of training a good deep neural network. We also\nprovide theoretical understanding on how the neural network depth $l$, the\nlayer width, the network size $d$ and parameter magnitude determine the neural\nnetwork landscapes. \n\n"}
{"id": "1705.07199", "contents": "Title: The High-Dimensional Geometry of Binary Neural Networks Abstract: Recent research has shown that one can train a neural network with binary\nweights and activations at train time by augmenting the weights with a\nhigh-precision continuous latent variable that accumulates small changes from\nstochastic gradient descent. However, there is a dearth of theoretical analysis\nto explain why we can effectively capture the features in our data with binary\nweights and activations. Our main result is that the neural networks with\nbinary weights and activations trained using the method of Courbariaux, Hubara\net al. (2016) work because of the high-dimensional geometry of binary vectors.\nIn particular, the ideal continuous vectors that extract out features in the\nintermediate representations of these BNNs are well-approximated by binary\nvectors in the sense that dot products are approximately preserved. Compared to\nprevious research that demonstrated the viability of such BNNs, our work\nexplains why these BNNs work in terms of the HD geometry. Our theory serves as\na foundation for understanding not only BNNs but a variety of methods that seek\nto compress traditional neural networks. Furthermore, a better understanding of\nmultilayer binary neural networks serves as a starting point for generalizing\nBNNs to other neural network architectures such as recurrent neural networks. \n\n"}
{"id": "1705.07366", "contents": "Title: Forward Thinking: Building Deep Random Forests Abstract: The success of deep neural networks has inspired many to wonder whether other\nlearners could benefit from deep, layered architectures. We present a general\nframework called forward thinking for deep learning that generalizes the\narchitectural flexibility and sophistication of deep neural networks while also\nallowing for (i) different types of learning functions in the network, other\nthan neurons, and (ii) the ability to adaptively deepen the network as needed\nto improve results. This is done by training one layer at a time, and once a\nlayer is trained, the input data are mapped forward through the layer to create\na new learning problem. The process is then repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new dataset, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. In the case where the neurons of deep neural nets are\nreplaced with decision trees, we call the result a Forward Thinking Deep Random\nForest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the\nMNIST dataset. We also provide a general mathematical formulation that allows\nfor other types of deep learning problems to be considered. \n\n"}
{"id": "1705.07857", "contents": "Title: Real Time Image Saliency for Black Box Classifiers Abstract: In this work we develop a fast saliency detection method that can be applied\nto any differentiable image classifier. We train a masking model to manipulate\nthe scores of the classifier by masking salient parts of the input image. Our\nmodel generalises well to unseen images and requires a single forward pass to\nperform saliency detection, therefore suitable for use in real-time systems. We\ntest our approach on CIFAR-10 and ImageNet datasets and show that the produced\nsaliency maps are easily interpretable, sharp, and free of artifacts. We\nsuggest a new metric for saliency and test our method on the ImageNet object\nlocalisation task. We achieve results outperforming other weakly supervised\nmethods. \n\n"}
{"id": "1705.08584", "contents": "Title: MMD GAN: Towards Deeper Understanding of Moment Matching Network Abstract: Generative moment matching network (GMMN) is a deep generative model that\ndiffers from Generative Adversarial Network (GAN) by replacing the\ndiscriminator in GAN with a two-sample test based on kernel maximum mean\ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been\nstudied, the empirical performance of GMMN is still not as competitive as that\nof GAN on challenging and large benchmark datasets. The computational\nefficiency of GMMN is also less desirable in comparison with GAN, partially due\nto its requirement for a rather large batch size during the training. In this\npaper, we propose to improve both the model expressiveness of GMMN and its\ncomputational efficiency by introducing adversarial kernel learning techniques,\nas the replacement of a fixed Gaussian kernel in the original GMMN. The new\napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN.\nThe new distance measure in MMD GAN is a meaningful loss that enjoys the\nadvantage of weak topology and can be optimized via gradient descent with\nrelatively small batch sizes. In our evaluation on multiple benchmark datasets,\nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN\nsignificantly outperforms GMMN, and is competitive with other representative\nGAN works. \n\n"}
{"id": "1705.08665", "contents": "Title: Bayesian Compression for Deep Learning Abstract: Compression and computational efficiency in deep learning have become a\nproblem of great significance. In this work, we argue that the most principled\nand effective way to attack this problem is by adopting a Bayesian point of\nview, where through sparsity inducing priors we prune large parts of the\nnetwork. We introduce two novelties in this paper: 1) we use hierarchical\npriors to prune nodes instead of individual weights, and 2) we use the\nposterior uncertainties to determine the optimal fixed point precision to\nencode the weights. Both factors significantly contribute to achieving the\nstate of the art in terms of compression rates, while still staying competitive\nwith methods designed to optimize for speed or energy efficiency. \n\n"}
{"id": "1705.08736", "contents": "Title: Non-Stationary Spectral Kernels Abstract: We propose non-stationary spectral kernels for Gaussian process regression.\nWe propose to model the spectral density of a non-stationary kernel function as\na mixture of input-dependent Gaussian process frequency density surfaces. We\nsolve the generalised Fourier transform with such a model, and present a family\nof non-stationary and non-monotonic kernels that can learn input-dependent and\npotentially long-range, non-monotonic covariances between inputs. We derive\nefficient inference using model whitening and marginalized posterior, and show\nwith case studies that these kernels are necessary when modelling even rather\nsimple time series, image or geospatial data with non-stationary\ncharacteristics. \n\n"}
{"id": "1705.09011", "contents": "Title: Principled Hybrids of Generative and Discriminative Domain Adaptation Abstract: We propose a probabilistic framework for domain adaptation that blends both\ngenerative and discriminative modeling in a principled way. Under this\nframework, generative and discriminative models correspond to specific choices\nof the prior over parameters. This provides us a very general way to\ninterpolate between generative and discriminative extremes through different\nchoices of priors. By maximizing both the marginal and the conditional\nlog-likelihoods, models derived from this framework can use both labeled\ninstances from the source domain as well as unlabeled instances from both\nsource and target domains. Under this framework, we show that the popular\nreconstruction loss of autoencoder corresponds to an upper bound of the\nnegative marginal log-likelihoods of unlabeled instances, where marginal\ndistributions are given by proper kernel density estimations. This provides a\nway to interpret the empirical success of autoencoders in domain adaptation and\nsemi-supervised learning. We instantiate our framework using neural networks,\nand build a concrete model, DAuto. Empirically, we demonstrate the\neffectiveness of DAuto on text, image and speech datasets, showing that it\noutperforms related competitors when domain adaptation is possible. \n\n"}
{"id": "1705.09303", "contents": "Title: Latent Geometry and Memorization in Generative Models Abstract: It can be difficult to tell whether a trained generative model has learned to\ngenerate novel examples or has simply memorized a specific set of outputs. In\npublished work, it is common to attempt to address this visually, for example\nby displaying a generated example and its nearest neighbor(s) in the training\nset (in, for example, the L2 metric). As any generative model induces a\nprobability density on its output domain, we propose studying this density\ndirectly. We first study the geometry of the latent representation and\ngenerator, relate this to the output density, and then develop techniques to\ncompute and inspect the output density. As an application, we demonstrate that\n\"memorization\" tends to a density made of delta functions concentrated on the\nmemorized examples. We note that without first understanding the geometry, the\nmeasurement would be essentially impossible to make. \n\n"}
{"id": "1705.09847", "contents": "Title: Lifelong Generative Modeling Abstract: Lifelong learning is the problem of learning multiple consecutive tasks in a\nsequential manner, where knowledge gained from previous tasks is retained and\nused to aid future learning over the lifetime of the learner. It is essential\ntowards the development of intelligent machines that can adapt to their\nsurroundings. In this work we focus on a lifelong learning approach to\nunsupervised generative modeling, where we continuously incorporate newly\nobserved distributions into a learned model. We do so through a student-teacher\nVariational Autoencoder architecture which allows us to learn and preserve all\nthe distributions seen so far, without the need to retain the past data nor the\npast models. Through the introduction of a novel cross-model regularizer,\ninspired by a Bayesian update rule, the student model leverages the information\nlearned by the teacher, which acts as a probabilistic knowledge store. The\nregularizer reduces the effect of catastrophic interference that appears when\nwe learn over sequences of distributions. We validate our model's performance\non sequential variants of MNIST, FashionMNIST, PermutedMNIST, SVHN and Celeb-A\nand demonstrate that our model mitigates the effects of catastrophic\ninterference faced by neural networks in sequential learning scenarios. \n\n"}
{"id": "1705.10369", "contents": "Title: Emergent Communication in a Multi-Modal, Multi-Step Referential Game Abstract: Inspired by previous work on emergent communication in referential games, we\npropose a novel multi-modal, multi-step referential game, where the sender and\nreceiver have access to distinct modalities of an object, and their information\nexchange is bidirectional and of arbitrary duration. The multi-modal multi-step\nsetting allows agents to develop an internal communication significantly closer\nto natural language, in that they share a single set of messages, and that the\nlength of the conversation may vary according to the difficulty of the task. We\nexamine these properties empirically using a dataset consisting of images and\ntextual descriptions of mammals, where the agents are tasked with identifying\nthe correct object. Our experiments indicate that a robust and efficient\ncommunication protocol emerges, where gradual information exchange informs\nbetter predictions and higher communication bandwidth improves generalization. \n\n"}
{"id": "1705.10843", "contents": "Title: Objective-Reinforced Generative Adversarial Networks (ORGAN) for\n  Sequence Generation Models Abstract: In unsupervised data generation tasks, besides the generation of a sample\nbased on previous observations, one would often like to give hints to the model\nin order to bias the generation towards desirable metrics. We propose a method\nthat combines Generative Adversarial Networks (GANs) and reinforcement learning\n(RL) in order to accomplish exactly that. While RL biases the data generation\nprocess towards arbitrary metrics, the GAN component of the reward function\nensures that the model still remembers information learned from data. We build\nupon previous results that incorporated GANs and RL in order to generate\nsequence data and test this model in several settings for the generation of\nmolecules encoded as text sequences (SMILES) and in the context of music\ngeneration, showing for each case that we can effectively bias the generation\nprocess towards desired metrics. \n\n"}
{"id": "1705.10924", "contents": "Title: Sequential Dynamic Decision Making with Deep Neural Nets on a Test-Time\n  Budget Abstract: Deep neural network (DNN) based approaches hold significant potential for\nreinforcement learning (RL) and have already shown remarkable gains over\nstate-of-art methods in a number of applications. The effectiveness of DNN\nmethods can be attributed to leveraging the abundance of supervised data to\nlearn value functions, Q-functions, and policy function approximations without\nthe need for feature engineering. Nevertheless, the deployment of DNN-based\npredictors with very deep architectures can pose an issue due to computational\nand other resource constraints at test-time in a number of applications. We\npropose a novel approach for reducing the average latency by learning a\ncomputationally efficient gating function that is capable of recognizing states\nin a sequential decision process for which policy prescriptions of a shallow\nnetwork suffices and deeper layers of the DNN have little marginal utility. The\noverall system is adaptive in that it dynamically switches control actions\nbased on state-estimates in order to reduce average latency without sacrificing\nterminal performance. We experiment with a number of alternative loss-functions\nto train gating functions and shallow policies and show that in a number of\napplications a speed-up of up to almost 5X can be obtained with little loss in\nperformance. \n\n"}
{"id": "1705.10941", "contents": "Title: Spectral Norm Regularization for Improving the Generalizability of Deep\n  Learning Abstract: We investigate the generalizability of deep learning based on the sensitivity\nto input perturbation. We hypothesize that the high sensitivity to the\nperturbation of data degrades the performance on it. To reduce the sensitivity\nto perturbation, we propose a simple and effective regularization method,\nreferred to as spectral norm regularization, which penalizes the high spectral\nnorm of weight matrices in neural networks. We provide supportive evidence for\nthe abovementioned hypothesis by experimentally confirming that the models\ntrained using spectral norm regularization exhibit better generalizability than\nother baseline methods. \n\n"}
{"id": "1706.00473", "contents": "Title: Deep Learning: A Bayesian Perspective Abstract: Deep learning is a form of machine learning for nonlinear high dimensional\npattern matching and prediction. By taking a Bayesian probabilistic\nperspective, we provide a number of insights into more efficient algorithms for\noptimisation and hyper-parameter tuning. Traditional high-dimensional data\nreduction techniques, such as principal component analysis (PCA), partial least\nsquares (PLS), reduced rank regression (RRR), projection pursuit regression\n(PPR) are all shown to be shallow learners. Their deep learning counterparts\nexploit multiple deep layers of data reduction which provide predictive\nperformance gains. Stochastic gradient descent (SGD) training optimisation and\nDropout (DO) regularization provide estimation and variable selection. Bayesian\nregularization is central to finding weights and connections in networks to\noptimize the predictive bias-variance trade-off. To illustrate our methodology,\nwe provide an analysis of international bookings on Airbnb. Finally, we\nconclude with directions for future research. \n\n"}
{"id": "1706.00820", "contents": "Title: Information, Privacy and Stability in Adaptive Data Analysis Abstract: Traditional statistical theory assumes that the analysis to be performed on a\ngiven data set is selected independently of the data themselves. This\nassumption breaks downs when data are re-used across analyses and the analysis\nto be performed at a given stage depends on the results of earlier stages. Such\ndependency can arise when the same data are used by several scientific studies,\nor when a single analysis consists of multiple stages.\n  How can we draw statistically valid conclusions when data are re-used? This\nis the focus of a recent and active line of work. At a high level, these\nresults show that limiting the information revealed by earlier stages of\nanalysis controls the bias introduced in later stages by adaptivity.\n  Here we review some known results in this area and highlight the role of\ninformation-theoretic concepts, notably several one-shot notions of mutual\ninformation. \n\n"}
{"id": "1706.00885", "contents": "Title: IDK Cascades: Fast Deep Learning by Learning not to Overthink Abstract: Advances in deep learning have led to substantial increases in prediction\naccuracy but have been accompanied by increases in the cost of rendering\npredictions. We conjecture that fora majority of real-world inputs, the recent\nadvances in deep learning have created models that effectively \"overthink\" on\nsimple inputs. In this paper, we revisit the classic question of building model\ncascades that primarily leverage class asymmetry to reduce cost. We introduce\nthe \"I Don't Know\"(IDK) prediction cascades framework, a general framework to\nsystematically compose a set of pre-trained models to accelerate inference\nwithout a loss in prediction accuracy. We propose two search based methods for\nconstructing cascades as well as a new cost-aware objective within this\nframework. The proposed IDK cascade framework can be easily adopted in the\nexisting model serving systems without additional model re-training. We\nevaluate the proposed techniques on a range of benchmarks to demonstrate the\neffectiveness of the proposed framework. \n\n"}
{"id": "1706.01566", "contents": "Title: Open Loop Hyperparameter Optimization and Determinantal Point Processes Abstract: Driven by the need for parallelizable hyperparameter optimization methods,\nthis paper studies \\emph{open loop} search methods: sequences that are\npredetermined and can be generated before a single configuration is evaluated.\nExamples include grid search, uniform random search, low discrepancy sequences,\nand other sampling distributions. In particular, we propose the use of\n$k$-determinantal point processes in hyperparameter optimization via random\nsearch. Compared to conventional uniform random search where hyperparameter\nsettings are sampled independently, a $k$-DPP promotes diversity. We describe\nan approach that transforms hyperparameter search spaces for efficient use with\na $k$-DPP. In addition, we introduce a novel Metropolis-Hastings algorithm\nwhich can sample from $k$-DPPs defined over any space from which uniform\nsamples can be drawn, including spaces with a mixture of discrete and\ncontinuous dimensions or tree structure. Our experiments show significant\nbenefits in realistic scenarios with a limited budget for training supervised\nlearners, whether in serial or parallel. \n\n"}
{"id": "1706.02052", "contents": "Title: Are Saddles Good Enough for Deep Learning? Abstract: Recent years have seen a growing interest in understanding deep neural\nnetworks from an optimization perspective. It is understood now that converging\nto low-cost local minima is sufficient for such models to become effective in\npractice. However, in this work, we propose a new hypothesis based on recent\ntheoretical findings and empirical studies that deep neural network models\nactually converge to saddle points with high degeneracy. Our findings from this\nwork are new, and can have a significant impact on the development of gradient\ndescent based methods for training deep networks. We validated our hypotheses\nusing an extensive experimental evaluation on standard datasets such as MNIST\nand CIFAR-10, and also showed that recent efforts that attempt to escape\nsaddles finally converge to saddles with high degeneracy, which we define as\n`good saddles'. We also verified the famous Wigner's Semicircle Law in our\nexperimental results. \n\n"}
{"id": "1706.02275", "contents": "Title: Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments Abstract: We explore deep reinforcement learning methods for multi-agent domains. We\nbegin by analyzing the difficulty of traditional algorithms in the multi-agent\ncase: Q-learning is challenged by an inherent non-stationarity of the\nenvironment, while policy gradient suffers from a variance that increases as\nthe number of agents grows. We then present an adaptation of actor-critic\nmethods that considers action policies of other agents and is able to\nsuccessfully learn policies that require complex multi-agent coordination.\nAdditionally, we introduce a training regimen utilizing an ensemble of policies\nfor each agent that leads to more robust multi-agent policies. We show the\nstrength of our approach compared to existing methods in cooperative as well as\ncompetitive scenarios, where agent populations are able to discover various\nphysical and informational coordination strategies. \n\n"}
{"id": "1706.02901", "contents": "Title: Characterizing Types of Convolution in Deep Convolutional Recurrent\n  Neural Networks for Robust Speech Emotion Recognition Abstract: Deep convolutional neural networks are being actively investigated in a wide\nrange of speech and audio processing applications including speech recognition,\naudio event detection and computational paralinguistics, owing to their ability\nto reduce factors of variations, for learning from speech. However, studies\nhave suggested to favor a certain type of convolutional operations when\nbuilding a deep convolutional neural network for speech applications although\nthere has been promising results using different types of convolutional\noperations. In this work, we study four types of convolutional operations on\ndifferent input features for speech emotion recognition under noisy and clean\nconditions in order to derive a comprehensive understanding. Since affective\nbehavioral information has been shown to reflect temporally varying of mental\nstate and convolutional operation are applied locally in time, all deep neural\nnetworks share a deep recurrent sub-network architecture for further temporal\nmodeling. We present detailed quantitative module-wise performance analysis to\ngain insights into information flows within the proposed architectures. In\nparticular, we demonstrate the interplay of affective information and the other\nirrelevant information during the progression from one module to another.\nFinally we show that all of our deep neural networks provide state-of-the-art\nperformance on the eNTERFACE'05 corpus. \n\n"}
{"id": "1706.03149", "contents": "Title: An Expectation-Maximization Algorithm for the Fractal Inverse Problem Abstract: We present an Expectation-Maximization algorithm for the fractal inverse\nproblem: the problem of fitting a fractal model to data. In our setting the\nfractals are Iterated Function Systems (IFS), with similitudes as the family of\ntransformations. The data is a point cloud in ${\\mathbb R}^H$ with arbitrary\ndimension $H$. Each IFS defines a probability distribution on ${\\mathbb R}^H$,\nso that the fractal inverse problem can be cast as a problem of parameter\nestimation. We show that the algorithm reconstructs well-known fractals from\ndata, with the model converging to high precision parameters. We also show the\nutility of the model as an approximation for datasources outside the IFS model\nclass. \n\n"}
{"id": "1706.03825", "contents": "Title: SmoothGrad: removing noise by adding noise Abstract: Explaining the output of a deep network remains a challenge. In the case of\nan image classifier, one type of explanation is to identify pixels that\nstrongly influence the final decision. A starting point for this strategy is\nthe gradient of the class score function with respect to the input image. This\ngradient can be interpreted as a sensitivity map, and there are several\ntechniques that elaborate on this basic idea. This paper makes two\ncontributions: it introduces SmoothGrad, a simple method that can help visually\nsharpen gradient-based sensitivity maps, and it discusses lessons in the\nvisualization of these maps. We publish the code for our experiments and a\nwebsite with our results. \n\n"}
{"id": "1706.04161", "contents": "Title: Lost Relatives of the Gumbel Trick Abstract: The Gumbel trick is a method to sample from a discrete probability\ndistribution, or to estimate its normalizing partition function. The method\nrelies on repeatedly applying a random perturbation to the distribution in a\nparticular way, each time solving for the most likely configuration. We derive\nan entire family of related methods, of which the Gumbel trick is one member,\nand show that the new methods have superior properties in several settings with\nminimal additional computational cost. In particular, for the Gumbel trick to\nyield computational benefits for discrete graphical models, Gumbel\nperturbations on all configurations are typically replaced with so-called\nlow-rank perturbations. We show how a subfamily of our new methods adapts to\nthis setting, proving new upper and lower bounds on the log partition function\nand deriving a family of sequential samplers for the Gibbs distribution.\nFinally, we balance the discussion by showing how the simpler analytical form\nof the Gumbel trick enables additional theoretical results. \n\n"}
{"id": "1706.04241", "contents": "Title: On Optimistic versus Randomized Exploration in Reinforcement Learning Abstract: We discuss the relative merits of optimistic and randomized approaches to\nexploration in reinforcement learning. Optimistic approaches presented in the\nliterature apply an optimistic boost to the value estimate at each state-action\npair and select actions that are greedy with respect to the resulting\noptimistic value function. Randomized approaches sample from among\nstatistically plausible value functions and select actions that are greedy with\nrespect to the random sample. Prior computational experience suggests that\nrandomized approaches can lead to far more statistically efficient learning. We\npresent two simple analytic examples that elucidate why this is the case. In\nprinciple, there should be optimistic approaches that fare well relative to\nrandomized approaches, but that would require intractable computation.\nOptimistic approaches that have been proposed in the literature sacrifice\nstatistical efficiency for the sake of computational efficiency. Randomized\napproaches, on the other hand, may enable simultaneous statistical and\ncomputational efficiency. \n\n"}
{"id": "1706.05378", "contents": "Title: A framework for Multi-A(rmed)/B(andit) testing with online FDR control Abstract: We propose an alternative framework to existing setups for controlling false\nalarms when multiple A/B tests are run over time. This setup arises in many\npractical applications, e.g. when pharmaceutical companies test new treatment\noptions against control pills for different diseases, or when internet\ncompanies test their default webpages versus various alternatives over time.\nOur framework proposes to replace a sequence of A/B tests by a sequence of\nbest-arm MAB instances, which can be continuously monitored by the data\nscientist. When interleaving the MAB tests with an an online false discovery\nrate (FDR) algorithm, we can obtain the best of both worlds: low sample\ncomplexity and any time online FDR control. Our main contributions are: (i) to\npropose reasonable definitions of a null hypothesis for MAB instances; (ii) to\ndemonstrate how one can derive an always-valid sequential p-value that allows\ncontinuous monitoring of each MAB test; and (iii) to show that using rejection\nthresholds of online-FDR algorithms as the confidence levels for the MAB\nalgorithms results in both sample-optimality, high power and low FDR at any\npoint in time. We run extensive simulations to verify our claims, and also\nreport results on real data collected from the New Yorker Cartoon Caption\ncontest. \n\n"}
{"id": "1706.05598", "contents": "Title: On the Optimization Landscape of Tensor Decompositions Abstract: Non-convex optimization with local search heuristics has been widely used in\nmachine learning, achieving many state-of-art results. It becomes increasingly\nimportant to understand why they can work for these NP-hard problems on typical\ndata. The landscape of many objective functions in learning has been\nconjectured to have the geometric property that \"all local optima are\n(approximately) global optima\", and thus they can be solved efficiently by\nlocal search algorithms. However, establishing such property can be very\ndifficult.\n  In this paper, we analyze the optimization landscape of the random\nover-complete tensor decomposition problem, which has many applications in\nunsupervised learning, especially in learning latent variable models. In\npractice, it can be efficiently solved by gradient ascent on a non-convex\nobjective. We show that for any small constant $\\epsilon > 0$, among the set of\npoints with function values $(1+\\epsilon)$-factor larger than the expectation\nof the function, all the local maxima are approximate global maxima.\nPreviously, the best-known result only characterizes the geometry in small\nneighborhoods around the true components. Our result implies that even with an\ninitialization that is barely better than the random guess, the gradient ascent\nalgorithm is guaranteed to solve this problem.\n  Our main technique uses Kac-Rice formula and random matrix theory. To our\nbest knowledge, this is the first time when Kac-Rice formula is successfully\napplied to counting the number of local minima of a highly-structured random\npolynomial with dependent coefficients. \n\n"}
{"id": "1706.05806", "contents": "Title: SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning\n  Dynamics and Interpretability Abstract: We propose a new technique, Singular Vector Canonical Correlation Analysis\n(SVCCA), a tool for quickly comparing two representations in a way that is both\ninvariant to affine transform (allowing comparison between different layers and\nnetworks) and fast to compute (allowing more comparisons to be calculated than\nwith previous methods). We deploy this tool to measure the intrinsic\ndimensionality of layers, showing in some cases needless over-parameterization;\nto probe learning dynamics throughout training, finding that networks converge\nto final representations from the bottom up; to show where class-specific\ninformation in networks is formed; and to suggest new training regimes that\nsimultaneously save computation and overfit less. Code:\nhttps://github.com/google/svcca/ \n\n"}
{"id": "1706.06083", "contents": "Title: Towards Deep Learning Models Resistant to Adversarial Attacks Abstract: Recent work has demonstrated that deep neural networks are vulnerable to\nadversarial examples---inputs that are almost indistinguishable from natural\ndata and yet classified incorrectly by the network. In fact, some of the latest\nfindings suggest that the existence of adversarial attacks may be an inherent\nweakness of deep learning models. To address this problem, we study the\nadversarial robustness of neural networks through the lens of robust\noptimization. This approach provides us with a broad and unifying view on much\nof the prior work on this topic. Its principled nature also enables us to\nidentify methods for both training and attacking neural networks that are\nreliable and, in a certain sense, universal. In particular, they specify a\nconcrete security guarantee that would protect against any adversary. These\nmethods let us train networks with significantly improved resistance to a wide\nrange of adversarial attacks. They also suggest the notion of security against\na first-order adversary as a natural and broad security guarantee. We believe\nthat robustness against such well-defined classes of adversaries is an\nimportant stepping stone towards fully resistant deep learning models. Code and\npre-trained models are available at https://github.com/MadryLab/mnist_challenge\nand https://github.com/MadryLab/cifar10_challenge. \n\n"}
{"id": "1706.07094", "contents": "Title: Constrained Bayesian Optimization with Noisy Experiments Abstract: Randomized experiments are the gold standard for evaluating the effects of\nchanges to real-world systems. Data in these tests may be difficult to collect\nand outcomes may have high variance, resulting in potentially large measurement\nerror. Bayesian optimization is a promising technique for efficiently\noptimizing multiple continuous parameters, but existing approaches degrade in\nperformance when the noise level is high, limiting its applicability to many\nrandomized experiments. We derive an expression for expected improvement under\ngreedy batch optimization with noisy observations and noisy constraints, and\ndevelop a quasi-Monte Carlo approximation that allows it to be efficiently\noptimized. Simulations with synthetic functions show that optimization\nperformance on noisy, constrained problems outperforms existing methods. We\nfurther demonstrate the effectiveness of the method with two real-world\nexperiments conducted at Facebook: optimizing a ranking system, and optimizing\nserver compiler flags. \n\n"}
{"id": "1706.07853", "contents": "Title: Loom: Exploiting Weight and Activation Precisions to Accelerate\n  Convolutional Neural Networks Abstract: Loom (LM), a hardware inference accelerator for Convolutional Neural Networks\n(CNNs) is presented. In LM every bit of data precision that can be saved\ntranslates to proportional performance gains. Specifically, for convolutional\nlayers LM's execution time scales inversely proportionally with the precisions\nof both weights and activations. For fully-connected layers LM's performance\nscales inversely proportionally with the precision of the weights. LM targets\narea- and bandwidth-constrained System-on-a-Chip designs such as those found on\nmobile devices that cannot afford the multi-megabyte buffers that would be\nneeded to store each layer on-chip. Accordingly, given a data bandwidth budget,\nLM boosts energy efficiency and performance over an equivalent bit-parallel\naccelerator. For both weights and activations LM can exploit profile-derived\nperlayer precisions. However, at runtime LM further trims activation precisions\nat a much smaller than a layer granularity. Moreover, it can naturally exploit\nweight precision variability at a smaller granularity than a layer. On average,\nacross several image classification CNNs and for a configuration that can\nperform the equivalent of 128 16b x 16b multiply-accumulate operations per\ncycle LM outperforms a state-of-the-art bit-parallel accelerator [1] by 4.38x\nwithout any loss in accuracy while being 3.54x more energy efficient. LM can\ntrade-off accuracy for additional improvements in execution performance and\nenergy efficiency and compares favorably to an accelerator that targeted only\nactivation precisions. We also study 2- and 4-bit LM variants and find the the\n2-bit per cycle variant is the most energy efficient. \n\n"}
{"id": "1706.07880", "contents": "Title: Collaborative Deep Learning in Fixed Topology Networks Abstract: There is significant recent interest to parallelize deep learning algorithms\nin order to handle the enormous growth in data and model sizes. While most\nadvances focus on model parallelization and engaging multiple computing agents\nvia using a central parameter server, aspect of data parallelization along with\ndecentralized computation has not been explored sufficiently. In this context,\nthis paper presents a new consensus-based distributed SGD (CDSGD) (and its\nmomentum variant, CDMSGD) algorithm for collaborative deep learning over fixed\ntopology networks that enables data parallelization as well as decentralized\ncomputation. Such a framework can be extremely useful for learning agents with\naccess to only local/private data in a communication constrained environment.\nWe analyze the convergence properties of the proposed algorithm with strongly\nconvex and nonconvex objective functions with fixed and diminishing step sizes\nusing concepts of Lyapunov function construction. We demonstrate the efficacy\nof our algorithms in comparison with the baseline centralized SGD and the\nrecently proposed federated averaging algorithm (that also enables data\nparallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100. \n\n"}
{"id": "1706.08894", "contents": "Title: Unsupervised Feature Selection Based on Space Filling Concept Abstract: The paper deals with the adaptation of a new measure for the unsupervised\nfeature selection problems. The proposed measure is based on space filling\nconcept and is called the coverage measure. This measure was used for judging\nthe quality of an experimental space filling design. In the present work, the\ncoverage measure is adapted for selecting the smallest informative subset of\nvariables by reducing redundancy in data. This paper proposes a simple analogy\nto apply this measure. It is implemented in a filter algorithm for unsupervised\nfeature selection problems.\n  The proposed filter algorithm is robust with high dimensional data and can be\nimplemented without extra parameters. Further, it is tested with simulated data\nand real world case studies including environmental data and hyperspectral\nimage. Finally, the results are evaluated by using random forest algorithm. \n\n"}
{"id": "1706.09395", "contents": "Title: Recovery of Missing Samples Using Sparse Approximation via a Convex\n  Similarity Measure Abstract: In this paper, we study the missing sample recovery problem using methods\nbased on sparse approximation. In this regard, we investigate the algorithms\nused for solving the inverse problem associated with the restoration of missed\nsamples of image signal. This problem is also known as inpainting in the\ncontext of image processing and for this purpose, we suggest an iterative\nsparse recovery algorithm based on constrained $l_1$-norm minimization with a\nnew fidelity metric. The proposed metric called Convex SIMilarity (CSIM) index,\nis a simplified version of the Structural SIMilarity (SSIM) index, which is\nconvex and error-sensitive. The optimization problem incorporating this\ncriterion, is then solved via Alternating Direction Method of Multipliers\n(ADMM). Simulation results show the efficiency of the proposed method for\nmissing sample recovery of 1D patch vectors and inpainting of 2D image signals. \n\n"}
{"id": "1706.10208", "contents": "Title: On Fairness, Diversity and Randomness in Algorithmic Decision Making Abstract: Consider a binary decision making process where a single machine learning\nclassifier replaces a multitude of humans. We raise questions about the\nresulting loss of diversity in the decision making process. We study the\npotential benefits of using random classifier ensembles instead of a single\nclassifier in the context of fairness-aware learning and demonstrate various\nattractive properties: (i) an ensemble of fair classifiers is guaranteed to be\nfair, for several different measures of fairness, (ii) an ensemble of unfair\nclassifiers can still achieve fair outcomes, and (iii) an ensemble of\nclassifiers can achieve better accuracy-fairness trade-offs than a single\nclassifier. Finally, we introduce notions of distributional fairness to\ncharacterize further potential benefits of random classifier ensembles. \n\n"}
{"id": "1707.00577", "contents": "Title: Generalization Properties of Doubly Stochastic Learning Algorithms Abstract: Doubly stochastic learning algorithms are scalable kernel methods that\nperform very well in practice. However, their generalization properties are not\nwell understood and their analysis is challenging since the corresponding\nlearning sequence may not be in the hypothesis space induced by the kernel. In\nthis paper, we provide an in-depth theoretical analysis for different variants\nof doubly stochastic learning algorithms within the setting of nonparametric\nregression in a reproducing kernel Hilbert space and considering the square\nloss. Particularly, we derive convergence results on the generalization error\nfor the studied algorithms either with or without an explicit penalty term. To\nthe best of our knowledge, the derived results for the unregularized variants\nare the first of this kind, while the results for the regularized variants\nimprove those in the literature. The novelties in our proof are a sample error\nbound that requires controlling the trace norm of a cumulative operator, and a\nrefined analysis of bounding initial error. \n\n"}
{"id": "1707.00797", "contents": "Title: Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE Abstract: We propose a number of new algorithms for learning deep energy models and\ndemonstrate their properties. We show that our SteinCD performs well in term of\ntest likelihood, while SteinGAN performs well in terms of generating realistic\nlooking images. Our results suggest promising directions for learning better\nmodels by combining GAN-style methods with traditional energy-based learning. \n\n"}
{"id": "1707.01476", "contents": "Title: Convolutional 2D Knowledge Graph Embeddings Abstract: Link prediction for knowledge graphs is the task of predicting missing\nrelationships between entities. Previous work on link prediction has focused on\nshallow, fast models which can scale to large knowledge graphs. However, these\nmodels learn less expressive features than deep, multi-layer models -- which\npotentially limits performance. In this work, we introduce ConvE, a multi-layer\nconvolutional network model for link prediction, and report state-of-the-art\nresults for several established datasets. We also show that the model is highly\nparameter efficient, yielding the same performance as DistMult and R-GCN with\n8x and 17x fewer parameters. Analysis of our model suggests that it is\nparticularly effective at modelling nodes with high indegree -- which are\ncommon in highly-connected, complex knowledge graphs such as Freebase and\nYAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer\nfrom test set leakage, due to inverse relations from the training set being\npresent in the test set -- however, the extent of this issue has so far not\nbeen quantified. We find this problem to be severe: a simple rule-based model\ncan achieve state-of-the-art results on both WN18 and FB15k. To ensure that\nmodels are evaluated on datasets where simply exploiting inverse relations\ncannot yield competitive results, we investigate and validate several commonly\nused datasets -- deriving robust variants where necessary. We then perform\nexperiments on these robust datasets for our own and several previously\nproposed models and find that ConvE achieves state-of-the-art Mean Reciprocal\nRank across most datasets. \n\n"}
{"id": "1707.04135", "contents": "Title: Heisenberg-Langevin vs. quantum master equation Abstract: The quantum master equation is an important tool in the study of quantum open\nsystems. It is often derived under a set of approximations, chief among them\nthe Born (factorization) and Markov (neglect of memory effects) approximations.\nIn this article we study the paradigmatic model of quantum Brownian motion of\nan harmonic oscillator coupled to a bath of oscillators with a Drude-Ohmic\nspectral density. We obtain analytically the \\emph{exact} solution of the\nHeisenberg-Langevin equations, with which we study correlation functions in the\nasymptotic stationary state. We compare the \\emph{exact} correlation functions\nto those obtained in the asymptotic long time limit with the quantum master\nequation in the Born approximation \\emph{with and without} the Markov\napproximation. In the latter case we implement a systematic derivative\nexpansion that yields the \\emph{exact} asymptotic limit under the factorization\napproximation \\emph{only}. We find discrepancies that could be significant when\nthe bandwidth of the bath $\\Lambda$ is much larger than the typical scales of\nthe system. We study the \\emph{exact} interaction energy as a \\emph{proxy} for\nthe correlations missed by the Born approximation and find that its dependence\non $\\Lambda$ is similar to the \\emph{discrepancy} between the exact solution\nand that of the quantum master equation in the Born approximation. We quantify\nthe regime of validity of the quantum master equation in the Born approximation\nwith or without the Markov approximation in terms of the system's relaxation\nrate $\\gamma$, its \\emph{unrenormalized} natural frequency $\\Omega$ and\n$\\Lambda$: $\\gamma/\\Omega \\ll 1$ and \\emph{also} $\\gamma \\Lambda/\\Omega^2 \\ll\n1$. The reliability of the Born approximation is discussed within the context\nof recent experimental settings and more general environments. \n\n"}
{"id": "1707.05807", "contents": "Title: Improving Gibbs Sampler Scan Quality with DoGS Abstract: The pairwise influence matrix of Dobrushin has long been used as an\nanalytical tool to bound the rate of convergence of Gibbs sampling. In this\nwork, we use Dobrushin influence as the basis of a practical tool to certify\nand efficiently improve the quality of a discrete Gibbs sampler. Our\nDobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection\norders for a given sampling budget and variable subset of interest, explicit\nbounds on total variation distance to stationarity, and certifiable\nimprovements over the standard systematic and uniform random scan Gibbs\nsamplers. In our experiments with joint image segmentation and object\nrecognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising\nmodel inference, DoGS consistently deliver higher-quality inferences with\nsignificantly smaller sampling budgets than standard Gibbs samplers. \n\n"}
{"id": "1707.05841", "contents": "Title: Linear Time Complexity Deep Fourier Scattering Network and Extension to\n  Nonlinear Invariants Abstract: In this paper we propose a scalable version of a state-of-the-art\ndeterministic time-invariant feature extraction approach based on consecutive\nchanges of basis and nonlinearities, namely, the scattering network. The first\nfocus of the paper is to extend the scattering network to allow the use of\nhigher order nonlinearities as well as extracting nonlinear and Fourier based\nstatistics leading to the required invariants of any inherently structured\ninput. In order to reach fast convolutions and to leverage the intrinsic\nstructure of wavelets, we derive our complete model in the Fourier domain. In\naddition of providing fast computations, we are now able to exploit sparse\nmatrices due to extremely high sparsity well localized in the Fourier domain.\nAs a result, we are able to reach a true linear time complexity with inputs in\nthe Fourier domain allowing fast and energy efficient solutions to machine\nlearning tasks. Validation of the features and computational results will be\npresented through the use of these invariant coefficients to perform\nclassification on audio recordings of bird songs captured in multiple different\nsoundscapes. In the end, the applicability of the presented solutions to deep\nartificial neural networks is discussed. \n\n"}
{"id": "1707.06347", "contents": "Title: Proximal Policy Optimization Algorithms Abstract: We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time. \n\n"}
{"id": "1707.06487", "contents": "Title: A Nonlinear Kernel Support Matrix Machine for Matrix Learning Abstract: In many problems of supervised tensor learning (STL), real world data such as\nface images or MRI scans are naturally represented as matrices, which are also\ncalled as second order tensors. Most existing classifiers based on tensor\nrepresentation, such as support tensor machine (STM) need to solve iteratively\nwhich occupy much time and may suffer from local minima. In this paper, we\npresent a kernel support matrix machine (KSMM) to perform supervised learning\nwhen data are represented as matrices. KSMM is a general framework for the\nconstruction of matrix-based hyperplane to exploit structural information. We\nanalyze a unifying optimization problem for which we propose an asymptotically\nconvergent algorithm. Theoretical analysis for the generalization bounds is\nderived based on Rademacher complexity with respect to a probability\ndistribution. We demonstrate the merits of the proposed method by exhaustive\nexperiments on both simulation study and a number of real-word datasets from a\nvariety of application domains. \n\n"}
{"id": "1707.06618", "contents": "Title: Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex\n  Optimization Abstract: We present a unified framework to analyze the global convergence of Langevin\ndynamics based algorithms for nonconvex finite-sum optimization with $n$\ncomponent functions. At the core of our analysis is a direct analysis of the\nergodicity of the numerical approximations to Langevin dynamics, which leads to\nfaster convergence rates. Specifically, we show that gradient Langevin dynamics\n(GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the almost\nminimizer within $\\tilde O\\big(nd/(\\lambda\\epsilon) \\big)$ and $\\tilde\nO\\big(d^7/(\\lambda^5\\epsilon^5) \\big)$ stochastic gradient evaluations\nrespectively, where $d$ is the problem dimension, and $\\lambda$ is the spectral\ngap of the Markov chain generated by GLD. Both results improve upon the best\nknown gradient complexity results (Raginsky et al., 2017). Furthermore, for the\nfirst time we prove the global convergence guarantee for variance reduced\nstochastic gradient Langevin dynamics (SVRG-LD) to the almost minimizer within\n$\\tilde O\\big(\\sqrt{n}d^5/(\\lambda^4\\epsilon^{5/2})\\big)$ stochastic gradient\nevaluations, which outperforms the gradient complexities of GLD and SGLD in a\nwide regime. Our theoretical analyses shed some light on using Langevin\ndynamics based algorithms for nonconvex optimization with provable guarantees. \n\n"}
{"id": "1707.07113", "contents": "Title: Adversarial Variational Optimization of Non-Differentiable Simulators Abstract: Complex computer simulators are increasingly used across fields of science as\ngenerative models tying parameters of an underlying theory to experimental\nobservations. Inference in this setup is often difficult, as simulators rarely\nadmit a tractable density or likelihood function. We introduce Adversarial\nVariational Optimization (AVO), a likelihood-free inference algorithm for\nfitting a non-differentiable generative model incorporating ideas from\ngenerative adversarial networks, variational optimization and empirical Bayes.\nWe adapt the training procedure of generative adversarial networks by replacing\nthe differentiable generative network with a domain-specific simulator. We\nsolve the resulting non-differentiable minimax problem by minimizing\nvariational upper bounds of the two adversarial objectives. Effectively, the\nprocedure results in learning a proposal distribution over simulator\nparameters, such that the JS divergence between the marginal distribution of\nthe synthetic data and the empirical distribution of observed data is\nminimized. We evaluate and compare the method with simulators producing both\ndiscrete and continuous data. \n\n"}
{"id": "1707.07287", "contents": "Title: Pairing an arbitrary regressor with an artificial neural network\n  estimating aleatoric uncertainty Abstract: We suggest a general approach to quantification of different forms of\naleatoric uncertainty in regression tasks performed by artificial neural\nnetworks. It is based on the simultaneous training of two neural networks with\na joint loss function and a specific hyperparameter $\\lambda>0$ that allows for\nautomatically detecting noisy and clean regions in the input space and\ncontrolling their {\\em relative contribution} to the loss and its gradients.\nAfter the model has been trained, one of the networks performs predictions and\nthe other quantifies the uncertainty of these predictions by estimating the\nlocally averaged loss of the first one. Unlike in many classical uncertainty\nquantification methods, we do not assume any a priori knowledge of the ground\ntruth probability distribution, neither do we, in general, maximize the\nlikelihood of a chosen parametric family of distributions. We analyze the\nlearning process and the influence of clean and noisy regions of the input\nspace on the loss surface, depending on $\\lambda$. In particular, we show that\nsmall values of $\\lambda$ increase the relative contribution of clean regions\nto the loss and its gradients. This explains why choosing small $\\lambda$\nallows for better predictions compared with neural networks without uncertainty\ncounterparts and those based on classical likelihood maximization. Finally, we\ndemonstrate that one can naturally form ensembles of pairs of our networks and\nthus capture both aleatoric and epistemic uncertainty and avoid overfitting. \n\n"}
{"id": "1707.07328", "contents": "Title: Adversarial Examples for Evaluating Reading Comprehension Systems Abstract: Standard accuracy metrics indicate that reading comprehension systems are\nmaking rapid progress, but the extent to which these systems truly understand\nlanguage remains unclear. To reward systems with real language understanding\nabilities, we propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). Our method tests whether systems can answer\nquestions about paragraphs that contain adversarially inserted sentences, which\nare automatically generated to distract computer systems without changing the\ncorrect answer or misleading humans. In this adversarial setting, the accuracy\nof sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$;\nwhen the adversary is allowed to add ungrammatical sequences of words, average\naccuracy on four models decreases further to $7\\%$. We hope our insights will\nmotivate the development of new models that understand language more precisely. \n\n"}
{"id": "1707.07576", "contents": "Title: Interpreting Classifiers through Attribute Interactions in Datasets Abstract: In this work we present the novel ASTRID method for investigating which\nattribute interactions classifiers exploit when making predictions. Attribute\ninteractions in classification tasks mean that two or more attributes together\nprovide stronger evidence for a particular class label. Knowledge of such\ninteractions makes models more interpretable by revealing associations between\nattributes. This has applications, e.g., in pharmacovigilance to identify\ninteractions between drugs or in bioinformatics to investigate associations\nbetween single nucleotide polymorphisms. We also show how the found attribute\npartitioning is related to a factorisation of the data generating distribution\nand empirically demonstrate the utility of the proposed method. \n\n"}
{"id": "1707.08352", "contents": "Title: General Latent Feature Modeling for Data Exploration Tasks Abstract: This paper introduces a general Bayesian non- parametric latent feature model\nsuitable to per- form automatic exploratory analysis of heterogeneous datasets,\nwhere the attributes describing each object can be either discrete, continuous\nor mixed variables. The proposed model presents several important properties.\nFirst, it accounts for heterogeneous data while can be inferred in linear time\nwith respect to the number of objects and attributes. Second, its Bayesian\nnonparametric nature allows us to automatically infer the model complexity from\nthe data, i.e., the number of features necessary to capture the latent\nstructure in the data. Third, the latent features in the model are\nbinary-valued variables, easing the interpretability of the obtained latent\nfeatures in data exploration tasks. \n\n"}
{"id": "1707.08689", "contents": "Title: Multi-Robot Transfer Learning: A Dynamical System Perspective Abstract: Multi-robot transfer learning allows a robot to use data generated by a\nsecond, similar robot to improve its own behavior. The potential advantages are\nreducing the time of training and the unavoidable risks that exist during the\ntraining phase. Transfer learning algorithms aim to find an optimal transfer\nmap between different robots. In this paper, we investigate, through a\ntheoretical study of single-input single-output (SISO) systems, the properties\nof such optimal transfer maps. We first show that the optimal transfer learning\nmap is, in general, a dynamic system. The main contribution of the paper is to\nprovide an algorithm for determining the properties of this optimal dynamic map\nincluding its order and regressors (i.e., the variables it depends on). The\nproposed algorithm does not require detailed knowledge of the robots' dynamics,\nbut relies on basic system properties easily obtainable through simple\nexperimental tests. We validate the proposed algorithm experimentally through\nan example of transfer learning between two different quadrotor platforms.\nExperimental results show that an optimal dynamic map, with correct properties\nobtained from our proposed algorithm, achieves 60-70% reduction of transfer\nlearning error compared to the cases when the data is directly transferred or\ntransferred using an optimal static map. \n\n"}
{"id": "1707.09430", "contents": "Title: Human in the Loop: Interactive Passive Automata Learning via\n  Evidence-Driven State-Merging Algorithms Abstract: We present an interactive version of an evidence-driven state-merging (EDSM)\nalgorithm for learning variants of finite state automata. Learning these\nautomata often amounts to recovering or reverse engineering the model\ngenerating the data despite noisy, incomplete, or imperfectly sampled data\nsources rather than optimizing a purely numeric target function. Domain\nexpertise and human knowledge about the target domain can guide this process,\nand typically is captured in parameter settings. Often, domain expertise is\nsubconscious and not expressed explicitly. Directly interacting with the\nlearning algorithm makes it easier to utilize this knowledge effectively. \n\n"}
{"id": "1708.01422", "contents": "Title: Exploring the Function Space of Deep-Learning Machines Abstract: The function space of deep-learning machines is investigated by studying\ngrowth in the entropy of functions of a given error with respect to a reference\nfunction, realized by a deep-learning machine. Using physics-inspired methods\nwe study both sparsely and densely-connected architectures to discover a\nlayer-wise convergence of candidate functions, marked by a corresponding\nreduction in entropy when approaching the reference function, gain insight into\nthe importance of having a large number of layers, and observe phase\ntransitions as the error increases. \n\n"}
{"id": "1708.01715", "contents": "Title: Training Deep AutoEncoders for Collaborative Filtering Abstract: This paper proposes a novel model for the rating prediction task in\nrecommender systems which significantly outperforms previous state-of-the art\nmodels on a time-split Netflix data set. Our model is based on deep autoencoder\nwith 6 layers and is trained end-to-end without any layer-wise pre-training. We\nempirically demonstrate that: a) deep autoencoder models generalize much better\nthan the shallow ones, b) non-linear activation functions with negative parts\nare crucial for training deep models, and c) heavy use of regularization\ntechniques such as dropout is necessary to prevent over-fiting. We also propose\na new training algorithm based on iterative output re-feeding to overcome\nnatural sparseness of collaborate filtering. The new algorithm significantly\nspeeds up training and improves model performance. Our code is available at\nhttps://github.com/NVIDIA/DeepRecommender \n\n"}
{"id": "1708.01945", "contents": "Title: A Bootstrap Method for Error Estimation in Randomized Matrix\n  Multiplication Abstract: In recent years, randomized methods for numerical linear algebra have\nreceived growing interest as a general approach to large-scale problems.\nTypically, the essential ingredient of these methods is some form of randomized\ndimension reduction, which accelerates computations, but also creates random\napproximation error. In this way, the dimension reduction step encodes a\ntradeoff between cost and accuracy. However, the exact numerical relationship\nbetween cost and accuracy is typically unknown, and consequently, it may be\ndifficult for the user to precisely know (1) how accurate a given solution is,\nor (2) how much computation is needed to achieve a given level of accuracy. In\nthe current paper, we study randomized matrix multiplication (sketching) as a\nprototype setting for addressing these general problems. As a solution, we\ndevelop a bootstrap method for \\emph{directly estimating} the accuracy as a\nfunction of the reduced dimension (as opposed to deriving worst-case bounds on\nthe accuracy in terms of the reduced dimension). From a computational\nstandpoint, the proposed method does not substantially increase the cost of\nstandard sketching methods, and this is made possible by an \"extrapolation\"\ntechnique. In addition, we provide both theoretical and empirical results to\ndemonstrate the effectiveness of the proposed method. \n\n"}
{"id": "1708.04106", "contents": "Title: Rocket Launching: A Universal and Efficient Framework for Training\n  Well-performing Light Net Abstract: Models applied on real time response task, like click-through rate (CTR)\nprediction model, require high accuracy and rigorous response time. Therefore,\ntop-performing deep models of high depth and complexity are not well suited for\nthese applications with the limitations on the inference time. In order to\nfurther improve the neural networks' performance given the time and\ncomputational limitations, we propose an approach that exploits a cumbersome\nnet to help train the lightweight net for prediction. We dub the whole process\nrocket launching, where the cumbersome booster net is used to guide the\nlearning of the target light net throughout the whole training process. We\nanalyze different loss functions aiming at pushing the light net to behave\nsimilarly to the booster net, and adopt the loss with best performance in our\nexperiments. We use one technique called gradient block to improve the\nperformance of the light net and booster net further. Experiments on benchmark\ndatasets and real-life industrial advertisement data present that our light\nmodel can get performance only previously achievable with more complex models. \n\n"}
{"id": "1708.04375", "contents": "Title: Chimera patterns in conservative systems and ultracold atoms with\n  mediated nonlocal hopping Abstract: Experimental realizations of chimera patterns, characterized by coexisting\nregions of phase coherence and incoherence, have so far only been achieved for\nnon-conservative systems with dissipation. Moreover, theoretical studies of\nchimera patterns have also been limited either to the non-conservative case or\nto simplified models that describe the dynamics only in terms of a scalar phase\nfield. Here, we show for the first time explicitly that the formation of\nchimera patterns can also be observed in conservative Hamiltonian systems with\nnonlocal hopping in which both energy and particle number are conserved, and\nwhere the local phase and amplitude are non-separable even in the weak coupling\nregime. Effective nonlocality can be realized in a physical system with only\nlocal coupling if different time scales exist, which we illustrate by a minimal\nconservative model with an additional mediating channel. Finally, we show that\nchimera patterns should be observable in ultracold atomic systems: Nonlocal\nspatial hopping over up to tens of lattice sites with independently tunable\nhopping strength and on-site nonlinearity can be implemented in a two-component\nBose-Einstein condensate with a spin-dependent optical lattice, where the\nuntrapped component serves as the matter-wave mediating field. \n\n"}
{"id": "1709.00537", "contents": "Title: Communication-efficient Algorithm for Distributed Sparse Learning via\n  Two-way Truncation Abstract: We propose a communicationally and computationally efficient algorithm for\nhigh-dimensional distributed sparse learning. At each iteration, local machines\ncompute the gradient on local data and the master machine solves one shifted\n$l_1$ regularized minimization problem. The communication cost is reduced from\nconstant times of the dimension number for the state-of-the-art algorithm to\nconstant times of the sparsity number via Two-way Truncation procedure.\nTheoretically, we prove that the estimation error of the proposed algorithm\ndecreases exponentially and matches that of the centralized method under mild\nassumptions. Extensive experiments on both simulated data and real data verify\nthat the proposed algorithm is efficient and has performance comparable with\nthe centralized method on solving high-dimensional sparse learning problems. \n\n"}
{"id": "1709.01427", "contents": "Title: Stochastic Gradient Descent: Going As Fast As Possible But Not Faster Abstract: When applied to training deep neural networks, stochastic gradient descent\n(SGD) often incurs steady progression phases, interrupted by catastrophic\nepisodes in which loss and gradient norm explode. A possible mitigation of such\nevents is to slow down the learning process. This paper presents a novel\napproach to control the SGD learning rate, that uses two statistical tests. The\nfirst one, aimed at fast learning, compares the momentum of the normalized\ngradient vectors to that of random unit vectors and accordingly gracefully\nincreases or decreases the learning rate. The second one is a change point\ndetection test, aimed at the detection of catastrophic learning episodes; upon\nits triggering the learning rate is instantly halved. Both abilities of\nspeeding up and slowing down the learning rate allows the proposed approach,\ncalled SALeRA, to learn as fast as possible but not faster. Experiments on\nstandard benchmarks show that SALeRA performs well in practice, and compares\nfavorably to the state of the art. \n\n"}
{"id": "1709.03163", "contents": "Title: Variational inference for the multi-armed contextual bandit Abstract: In many biomedical, science, and engineering problems, one must sequentially\ndecide which action to take next so as to maximize rewards. One general class\nof algorithms for optimizing interactions with the world, while simultaneously\nlearning how the world operates, is the multi-armed bandit setting and, in\nparticular, the contextual bandit case. In this setting, for each executed\naction, one observes rewards that are dependent on a given 'context', available\nat each interaction with the world. The Thompson sampling algorithm has\nrecently been shown to enjoy provable optimality properties for this set of\nproblems, and to perform well in real-world settings. It facilitates generative\nand interpretable modeling of the problem at hand. Nevertheless, the design and\ncomplexity of the model limit its application, since one must both sample from\nthe distributions modeled and calculate their expected rewards. We here show\nhow these limitations can be overcome using variational inference to\napproximate complex models, applying to the reinforcement learning case\nadvances developed for the inference case in the machine learning community\nover the past two decades. We consider contextual multi-armed bandit\napplications where the true reward distribution is unknown and complex, which\nwe approximate with a mixture model whose parameters are inferred via\nvariational inference. We show how the proposed variational Thompson sampling\napproach is accurate in approximating the true distribution, and attains\nreduced regrets even with complex reward distributions. The proposed algorithm\nis valuable for practical scenarios where restrictive modeling assumptions are\nundesirable. \n\n"}
{"id": "1709.03183", "contents": "Title: Rates of Convergence of Spectral Methods for Graphon Estimation Abstract: This paper studies the problem of estimating the grahpon model - the\nunderlying generating mechanism of a network. Graphon estimation arises in many\napplications such as predicting missing links in networks and learning user\npreferences in recommender systems. The graphon model deals with a random graph\nof $n$ vertices such that each pair of two vertices $i$ and $j$ are connected\nindependently with probability $\\rho \\times f(x_i,x_j)$, where $x_i$ is the\nunknown $d$-dimensional label of vertex $i$, $f$ is an unknown symmetric\nfunction, and $\\rho$ is a scaling parameter characterizing the graph sparsity.\nRecent studies have identified the minimax error rate of estimating the graphon\nfrom a single realization of the random graph. However, there exists a wide gap\nbetween the known error rates of computationally efficient estimation\nprocedures and the minimax optimal error rate.\n  Here we analyze a spectral method, namely universal singular value\nthresholding (USVT) algorithm, in the relatively sparse regime with the average\nvertex degree $n\\rho=\\Omega(\\log n)$. When $f$ belongs to H\\\"{o}lder or Sobolev\nspace with smoothness index $\\alpha$, we show the error rate of USVT is at most\n$(n\\rho)^{ -2 \\alpha / (2\\alpha+d)}$, approaching the minimax optimal error\nrate $\\log (n\\rho)/(n\\rho)$ for $d=1$ as $\\alpha$ increases. Furthermore, when\n$f$ is analytic, we show the error rate of USVT is at most $\\log^d\n(n\\rho)/(n\\rho)$. In the special case of stochastic block model with $k$\nblocks, the error rate of USVT is at most $k/(n\\rho)$, which is larger than the\nminimax optimal error rate by at most a multiplicative factor $k/\\log k$. This\ncoincides with the computational gap observed for community detection. A key\nstep of our analysis is to derive the eigenvalue decaying rate of the edge\nprobability matrix using piecewise polynomial approximations of the graphon\nfunction $f$. \n\n"}
{"id": "1709.03670", "contents": "Title: Community Recovery in Hypergraphs Abstract: Community recovery is a central problem that arises in a wide variety of\napplications such as network clustering, motion segmentation, face clustering\nand protein complex detection. The objective of the problem is to cluster data\npoints into distinct communities based on a set of measurements, each of which\nis associated with the values of a certain number of data points. While most of\nthe prior works focus on a setting in which the number of data points involved\nin a measurement is two, this work explores a generalized setting in which the\nnumber can be more than two. Motivated by applications particularly in machine\nlearning and channel coding, we consider two types of measurements: (1)\nhomogeneity measurement which indicates whether or not the associated data\npoints belong to the same community; (2) parity measurement which denotes the\nmodulo-2 sum of the values of the data points. Such measurements are possibly\ncorrupted by Bernoulli noise. We characterize the fundamental limits on the\nnumber of measurements required to reconstruct the communities for the\nconsidered models. \n\n"}
{"id": "1709.04384", "contents": "Title: Generating Music Medleys via Playing Music Puzzle Games Abstract: Generating music medleys is about finding an optimal permutation of a given\nset of music clips. Toward this goal, we propose a self-supervised learning\ntask, called the music puzzle game, to train neural network models to learn the\nsequential patterns in music. In essence, such a game requires machines to\ncorrectly sort a few multisecond music fragments. In the training stage, we\nlearn the model by sampling multiple non-overlapping fragment pairs from the\nsame songs and seeking to predict whether a given pair is consecutive and is in\nthe correct chronological order. For testing, we design a number of puzzle\ngames with different difficulty levels, the most difficult one being music\nmedley, which requiring sorting fragments from different songs. On the basis of\nstate-of-the-art Siamese convolutional network, we propose an improved\narchitecture that learns to embed frame-level similarity scores computed from\nthe input fragment pairs to a common space, where fragment pairs in the correct\norder can be more easily identified. Our result shows that the resulting model,\ndubbed as the similarity embedding network (SEN), performs better than\ncompeting models across different games, including music jigsaw puzzle, music\nsequencing, and music medley. Example results can be found at our project\nwebsite, https://remyhuang.github.io/DJnet. \n\n"}
{"id": "1709.06129", "contents": "Title: When is a Convolutional Filter Easy To Learn? Abstract: We analyze the convergence of (stochastic) gradient descent algorithm for\nlearning a convolutional filter with Rectified Linear Unit (ReLU) activation\nfunction. Our analysis does not rely on any specific form of the input\ndistribution and our proofs only use the definition of ReLU, in contrast with\nprevious works that are restricted to standard Gaussian input. We show that\n(stochastic) gradient descent with random initialization can learn the\nconvolutional filter in polynomial time and the convergence rate depends on the\nsmoothness of the input distribution and the closeness of patches. To the best\nof our knowledge, this is the first recovery guarantee of gradient-based\nalgorithms for convolutional filter on non-Gaussian input distributions. Our\ntheory also justifies the two-stage learning rate strategy in deep neural\nnetworks. While our focus is theoretical, we also present experiments that\nillustrate our theoretical findings. \n\n"}
{"id": "1709.06404", "contents": "Title: Interactive Music Generation with Positional Constraints using\n  Anticipation-RNNs Abstract: Recurrent Neural Networks (RNNS) are now widely used on sequence generation\ntasks due to their ability to learn long-range dependencies and to generate\nsequences of arbitrary length. However, their left-to-right generation\nprocedure only allows a limited control from a potential user which makes them\nunsuitable for interactive and creative usages such as interactive music\ngeneration. This paper introduces a novel architecture called Anticipation-RNN\nwhich possesses the assets of the RNN-based generative models while allowing to\nenforce user-defined positional constraints. We demonstrate its efficiency on\nthe task of generating melodies satisfying positional constraints in the style\nof the soprano parts of the J.S. Bach chorale harmonizations. Sampling using\nthe Anticipation-RNN is of the same order of complexity than sampling from the\ntraditional RNN model. This fast and interactive generation of musical\nsequences opens ways to devise real-time systems that could be used for\ncreative purposes. \n\n"}
{"id": "1709.06680", "contents": "Title: Deep Lattice Networks and Partial Monotonic Functions Abstract: We propose learning deep models that are monotonic with respect to a\nuser-specified set of inputs by alternating layers of linear embeddings,\nensembles of lattices, and calibrators (piecewise linear functions), with\nappropriate constraints for monotonicity, and jointly training the resulting\nnetwork. We implement the layers and projections with new computational graph\nnodes in TensorFlow and use the ADAM optimizer and batched stochastic\ngradients. Experiments on benchmark and real-world datasets show that six-layer\nmonotonic deep lattice networks achieve state-of-the art performance for\nclassification and regression with monotonicity guarantees. \n\n"}
{"id": "1709.06994", "contents": "Title: Structured Probabilistic Pruning for Convolutional Neural Network\n  Acceleration Abstract: In this paper, we propose a novel progressive parameter pruning method for\nConvolutional Neural Network acceleration, named Structured Probabilistic\nPruning (SPP), which effectively prunes weights of convolutional layers in a\nprobabilistic manner. Unlike existing deterministic pruning approaches, where\nunimportant weights are permanently eliminated, SPP introduces a pruning\nprobability for each weight, and pruning is guided by sampling from the pruning\nprobabilities. A mechanism is designed to increase and decrease pruning\nprobabilities based on importance criteria in the training process. Experiments\nshow that, with 4x speedup, SPP can accelerate AlexNet with only 0.3% loss of\ntop-5 accuracy and VGG-16 with 0.8% loss of top-5 accuracy in ImageNet\nclassification. Moreover, SPP can be directly applied to accelerate\nmulti-branch CNN networks, such as ResNet, without specific adaptations. Our 2x\nspeedup ResNet-50 only suffers 0.8% loss of top-5 accuracy on ImageNet. We\nfurther show the effectiveness of SPP on transfer learning tasks. \n\n"}
{"id": "1709.08513", "contents": "Title: Coherent scattering from semi-infinite non-Hermitian potentials Abstract: When two identical (coherent) beams are injected at a semi-infinite\nnon-Hermitian medium from left and right, we show that both reflection\n$(r_L,r_R)$ and transmission $(t_L,t_R)$ amplitudes are non-reciprocal. In a\nparametric domain, there exists Spectral Singularity (SS) at a real energy\n$E=E_*$ and the determinant of the time-reversed two port S-matrix i.e.,\n$|\\det(S)|=|t_L t_R-r_L r_R|$ vanishes sharply at $E=E_*$ displaying the\nphenomenon of Coherent Perfect Absorption (CPA). In the complimentary\nparametric domain, the potential becomes either left or right reflectionless at\n$E=E_z$. But we rule out the existence of Invisibility despite $r_R(E_i)=0$ and\n$t_R(E_i)=1$ in these new models. We present two simple exactly solvable models\nwhere the expressions for $E_*$, $E_z$, $E_i$ and the parametric conditions on\nthe potential have been obtained in explicit and simple forms. Earlier, the\nnovel phenomena of SS and CPA have been found to occur only in the scattering\ncomplex potentials which are spatially localized (vanish asymptotically) and\nhaving $t_L=t_R$. \n\n"}
{"id": "1709.08853", "contents": "Title: Object-oriented Neural Programming (OONP) for Document Understanding Abstract: We propose Object-oriented Neural Programming (OONP), a framework for\nsemantically parsing documents in specific domains. Basically, OONP reads a\ndocument and parses it into a predesigned object-oriented data structure\n(referred to as ontology in this paper) that reflects the domain-specific\nsemantics of the document. An OONP parser models semantic parsing as a decision\nprocess: a neural net-based Reader sequentially goes through the document, and\nduring the process it builds and updates an intermediate ontology to summarize\nits partial understanding of the text it covers. OONP supports a rich family of\noperations (both symbolic and differentiable) for composing the ontology, and a\nbig variety of forms (both symbolic and differentiable) for representing the\nstate and the document. An OONP parser can be trained with supervision of\ndifferent forms and strength, including supervised learning (SL) ,\nreinforcement learning (RL) and hybrid of the two. Our experiments on both\nsynthetic and real-world document parsing tasks have shown that OONP can learn\nto handle fairly complicated ontology with training data of modest sizes. \n\n"}
{"id": "1710.02766", "contents": "Title: Bayesian Alignments of Warped Multi-Output Gaussian Processes Abstract: We propose a novel Bayesian approach to modelling nonlinear alignments of\ntime series based on latent shared information. We apply the method to the\nreal-world problem of finding common structure in the sensor data of wind\nturbines introduced by the underlying latent and turbulent wind field. The\nproposed model allows for both arbitrary alignments of the inputs and\nnon-parametric output warpings to transform the observations. This gives rise\nto multiple deep Gaussian process models connected via latent generating\nprocesses. We present an efficient variational approximation based on nested\nvariational compression and show how the model can be used to extract shared\ninformation between dependent time series, recovering an interpretable\nfunctional decomposition of the learning problem. We show results for an\nartificial data set and real-world data of two wind turbines. \n\n"}
{"id": "1710.03937", "contents": "Title: PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement\n  Learning and Sampling-based Planning Abstract: We present PRM-RL, a hierarchical method for long-range navigation task\ncompletion that combines sampling based path planning with reinforcement\nlearning (RL). The RL agents learn short-range, point-to-point navigation\npolicies that capture robot dynamics and task constraints without knowledge of\nthe large-scale topology. Next, the sampling-based planners provide roadmaps\nwhich connect robot configurations that can be successfully navigated by the RL\nagent. The same RL agents are used to control the robot under the direction of\nthe planning, enabling long-range navigation. We use the Probabilistic Roadmaps\n(PRMs) for the sampling-based planner. The RL agents are constructed using\nfeature-based and deep neural net policies in continuous state and action\nspaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation\ntasks with non-trivial robot dynamics: end-to-end differential drive indoor\nnavigation in office environments, and aerial cargo delivery in urban\nenvironments with load displacement constraints. Our results show improvement\nin task completion over both RL agents on their own and traditional\nsampling-based planners. In the indoor navigation task, PRM-RL successfully\ncompletes up to 215 m long trajectories under noisy sensor conditions, and the\naerial cargo delivery completes flights over 1000 m without violating the task\nconstraints in an environment 63 million times larger than used in training. \n\n"}
{"id": "1710.06530", "contents": "Title: An exciton-coupled electron transfer process controlled by non-Markovian\n  environments Abstract: We theoretically investigate an exciton-coupled electron transfer (XCET)\nprocess that is conversion of an exciton into a charge transfer state. This\nconversion happens in an exciton transfer (XT) process, and the electron moves\naway in an electron transfer(ET) process in multiple environments (baths). This\nXCET process plays an essential role in the harvesting of solar energy in\nbiological and photovoltaic materials. We develop a practical theoretical model\nto study the efficiency of XCET process that occurs either in consecutive or\nconcerted processes under the influence of non-Markovian baths. The role of\nquantum coherence in the XT-ET system and the baths is investigated using\nreduced hierarchal equations of motion (HEOM). This model includes independent\nbaths for each XT and ET state, in addition to a XCET bath for the conversion\nprocess. We found that, while quantum system-bath coherence is important in the\nXT and ET processes, coherence between the XT and ET processes must be\nsuppressed in order to realize efficient irreversible XCET process through the\nweak off-diagonal interaction between the XT and ET bridge sites arises from a\nXCET bath. \n\n"}
{"id": "1710.07400", "contents": "Title: Ligand Pose Optimization with Atomic Grid-Based Convolutional Neural\n  Networks Abstract: Docking is an important tool in computational drug discovery that aims to\npredict the binding pose of a ligand to a target protein through a combination\nof pose scoring and optimization. A scoring function that is differentiable\nwith respect to atom positions can be used for both scoring and gradient-based\noptimization of poses for docking. Using a differentiable grid-based atomic\nrepresentation as input, we demonstrate that a scoring function learned by\ntraining a convolutional neural network (CNN) to identify binding poses can\nalso be applied to pose optimization. We also show that an iteratively-trained\nCNN that includes poses optimized by the first CNN in its training set performs\neven better at optimizing randomly initialized poses than either the first CNN\nscoring function or AutoDock Vina. \n\n"}
{"id": "1710.07797", "contents": "Title: Optimal Rates for Learning with Nystr\\\"om Stochastic Gradient Methods Abstract: In the setting of nonparametric regression, we propose and study a\ncombination of stochastic gradient methods with Nystr\\\"om subsampling, allowing\nmultiple passes over the data and mini-batches. Generalization error bounds for\nthe studied algorithm are provided. Particularly, optimal learning rates are\nderived considering different possible choices of the step-size, the mini-batch\nsize, the number of iterations/passes, and the subsampling level. In comparison\nwith state-of-the-art algorithms such as the classic stochastic gradient\nmethods and kernel ridge regression with Nystr\\\"om, the studied algorithm has\nadvantages on the computational complexity, while achieving the same optimal\nlearning rates. Moreover, our results indicate that using mini-batches can\nreduce the total computational cost while achieving the same optimal\nstatistical results. \n\n"}
{"id": "1710.08585", "contents": "Title: Max-Margin Invariant Features from Transformed Unlabeled Data Abstract: The study of representations invariant to common transformations of the data\nis important to learning. Most techniques have focused on local approximate\ninvariance implemented within expensive optimization frameworks lacking\nexplicit theoretical guarantees. In this paper, we study kernels that are\ninvariant to a unitary group while having theoretical guarantees in addressing\nthe important practical issue of unavailability of transformed versions of\nlabelled data. A problem we call the Unlabeled Transformation Problem which is\na special form of semi-supervised learning and one-shot learning. We present a\ntheoretically motivated alternate approach to the invariant kernel SVM based on\nwhich we propose Max-Margin Invariant Features (MMIF) to solve this problem. As\nan illustration, we design an framework for face recognition and demonstrate\nthe efficacy of our approach on a large scale semi-synthetic dataset with\n153,000 images and a new challenging protocol on Labelled Faces in the Wild\n(LFW) while out-performing strong baselines. \n\n"}
{"id": "1710.09302", "contents": "Title: Deep Neural Networks Abstract: Deep Neural Networks (DNNs) are universal function approximators providing\nstate-of- the-art solutions on wide range of applications. Common perceptual\ntasks such as speech recognition, image classification, and object tracking are\nnow commonly tackled via DNNs. Some fundamental problems remain: (1) the lack\nof a mathematical framework providing an explicit and interpretable\ninput-output formula for any topology, (2) quantification of DNNs stability\nregarding adversarial examples (i.e. modified inputs fooling DNN predictions\nwhilst undetectable to humans), (3) absence of generalization guarantees and\ncontrollable behaviors for ambiguous patterns, (4) leverage unlabeled data to\napply DNNs to domains where expert labeling is scarce as in the medical field.\nAnswering those points would provide theoretical perspectives for further\ndevelopments based on a common ground. Furthermore, DNNs are now deployed in\ntremendous societal applications, pushing the need to fill this theoretical gap\nto ensure control, reliability, and interpretability. \n\n"}
{"id": "1710.09363", "contents": "Title: GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks Abstract: The Fisher information metric is an important foundation of information\ngeometry, wherein it allows us to approximate the local geometry of a\nprobability distribution. Recurrent neural networks such as the\nSequence-to-Sequence (Seq2Seq) networks that have lately been used to yield\nstate-of-the-art performance on speech translation or image captioning have so\nfar ignored the geometry of the latent embedding, that they iteratively learn.\nWe propose the information geometric Seq2Seq (GeoSeq2Seq) network which\nabridges the gap between deep recurrent neural networks and information\ngeometry. Specifically, the latent embedding offered by a recurrent network is\nencoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism\ncommon in computer vision. We utilise such a network to predict the shortest\nroutes between two nodes of a graph by learning the adjacency matrix using the\nGeoSeq2Seq formalism; our results show that for such a problem the\nprobabilistic representation of the latent embedding supersedes the\nnon-probabilistic embedding by 10-15\\%. \n\n"}
{"id": "1710.09567", "contents": "Title: Big Data Classification Using Augmented Decision Trees Abstract: We present an algorithm for classification tasks on big data. Experiments\nconducted as part of this study indicate that the algorithm can be as accurate\nas ensemble methods such as random forests or gradient boosted trees. Unlike\nensemble methods, the models produced by the algorithm can be easily\ninterpreted. The algorithm is based on a divide and conquer strategy and\nconsists of two steps. The first step consists of using a decision tree to\nsegment the large dataset. By construction, decision trees attempt to create\nhomogeneous class distributions in their leaf nodes. However, non-homogeneous\nleaf nodes are usually produced. The second step of the algorithm consists of\nusing a suitable classifier to determine the class labels for the\nnon-homogeneous leaf nodes. The decision tree segment provides a coarse segment\nprofile while the leaf level classifier can provide information about the\nattributes that affect the label within a segment. \n\n"}
{"id": "1710.10513", "contents": "Title: Crime incidents embedding using restricted Boltzmann machines Abstract: We present a new approach for detecting related crime series, by unsupervised\nlearning of the latent feature embeddings from narratives of crime record via\nthe Gaussian-Bernoulli Restricted Boltzmann Machines (RBM). This is a\ndrastically different approach from prior work on crime analysis, which\ntypically considers only time and location and at most category information.\nAfter the embedding, related cases are closer to each other in the Euclidean\nfeature space, and the unrelated cases are far apart, which is a good property\ncan enable subsequent analysis such as detection and clustering of related\ncases. Experiments over several series of related crime incidents hand labeled\nby the Atlanta Police Department reveal the promise of our embedding methods. \n\n"}
{"id": "1710.11070", "contents": "Title: Convergence Rates of Latent Topic Models Under Relaxed Identifiability\n  Conditions Abstract: In this paper we study the frequentist convergence rate for the Latent\nDirichlet Allocation (Blei et al., 2003) topic models. We show that the maximum\nlikelihood estimator converges to one of the finitely many equivalent\nparameters in Wasserstein's distance metric at a rate of $n^{-1/4}$ without\nassuming separability or non-degeneracy of the underlying topics and/or the\nexistence of more than three words per document, thus generalizing the previous\nworks of Anandkumar et al. (2012, 2014) from an information-theoretical\nperspective. We also show that the $n^{-1/4}$ convergence rate is optimal in\nthe worst case. \n\n"}
{"id": "1710.11198", "contents": "Title: Action-depedent Control Variates for Policy Optimization via Stein's\n  Identity Abstract: Policy gradient methods have achieved remarkable successes in solving\nchallenging reinforcement learning problems. However, it still often suffers\nfrom the large variance issue on policy gradient estimation, which leads to\npoor sample efficiency during training. In this work, we propose a control\nvariate method to effectively reduce variance for policy gradient methods.\nMotivated by the Stein's identity, our method extends the previous control\nvariate methods used in REINFORCE and advantage actor-critic by introducing\nmore general action-dependent baseline functions. Empirical studies show that\nour method significantly improves the sample efficiency of the state-of-the-art\npolicy gradient approaches. \n\n"}
{"id": "1710.11223", "contents": "Title: Fast and Scalable Learning of Sparse Changes in High-Dimensional\n  Gaussian Graphical Model Structure Abstract: We focus on the problem of estimating the change in the dependency structures\nof two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for\nsparse change estimation in GGMs involve expensive and difficult non-smooth\noptimization. We propose a novel method, DIFFEE for estimating DIFFerential\nnetworks via an Elementary Estimator under a high-dimensional situation. DIFFEE\nis solved through a faster and closed form solution that enables it to work in\nlarge-scale settings. We conduct a rigorous statistical analysis showing that\nsurprisingly DIFFEE achieves the same asymptotic convergence rates as the\nstate-of-the-art estimators that are much more difficult to compute. Our\nexperimental results on multiple synthetic datasets and one real-world data\nabout brain connectivity show strong performance improvements over baselines,\nas well as significant computational benefits. \n\n"}
{"id": "1710.11351", "contents": "Title: ChainerMN: Scalable Distributed Deep Learning Framework Abstract: One of the keys for deep learning to have made a breakthrough in various\nfields was to utilize high computing powers centering around GPUs. Enabling the\nuse of further computing abilities by distributed processing is essential not\nonly to make the deep learning bigger and faster but also to tackle unsolved\nchallenges. We present the design, implementation, and evaluation of ChainerMN,\nthe distributed deep learning framework we have developed. We demonstrate that\nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet\ndataset up to 128 GPUs with the parallel efficiency of 90%. \n\n"}
{"id": "1711.00350", "contents": "Title: Generalization without systematicity: On the compositional skills of\n  sequence-to-sequence recurrent networks Abstract: Humans can understand and produce new utterances effortlessly, thanks to\ntheir compositional skills. Once a person learns the meaning of a new verb\n\"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing\nand dax.\" In this paper, we introduce the SCAN domain, consisting of a set of\nsimple compositional navigation commands paired with the corresponding action\nsequences. We then test the zero-shot generalization capabilities of a variety\nof recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence\nmethods. We find that RNNs can make successful zero-shot generalizations when\nthe differences between training and test commands are small, so that they can\napply \"mix-and-match\" strategies to solve the task. However, when\ngeneralization requires systematic compositional skills (as in the \"dax\"\nexample above), RNNs fail spectacularly. We conclude with a proof-of-concept\nexperiment in neural machine translation, suggesting that lack of systematicity\nmight be partially responsible for neural networks' notorious training data\nthirst. \n\n"}
{"id": "1711.00867", "contents": "Title: The (Un)reliability of saliency methods Abstract: Saliency methods aim to explain the predictions of deep neural networks.\nThese methods lack reliability when the explanation is sensitive to factors\nthat do not contribute to the model prediction. We use a simple and common\npre-processing step ---adding a constant shift to the input data--- to show\nthat a transformation with no effect on the model can cause numerous methods to\nincorrectly attribute. In order to guarantee reliability, we posit that methods\nshould fulfill input invariance, the requirement that a saliency method mirror\nthe sensitivity of the model with respect to transformations of the input. We\nshow, through several examples, that saliency methods that do not satisfy input\ninvariance result in misleading attribution. \n\n"}
{"id": "1711.01012", "contents": "Title: Policy Optimization by Genetic Distillation Abstract: Genetic algorithms have been widely used in many practical optimization\nproblems. Inspired by natural selection, operators, including mutation,\ncrossover and selection, provide effective heuristics for search and black-box\noptimization. However, they have not been shown useful for deep reinforcement\nlearning, possibly due to the catastrophic consequence of parameter crossovers\nof neural networks. Here, we present Genetic Policy Optimization (GPO), a new\ngenetic algorithm for sample-efficient deep policy optimization. GPO uses\nimitation learning for policy crossover in the state space and applies policy\ngradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as\na genetic algorithm is able to provide superior performance over the\nstate-of-the-art policy gradient methods and achieves comparable or higher\nsample efficiency. \n\n"}
{"id": "1711.01558", "contents": "Title: Wasserstein Auto-Encoders Abstract: We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building\na generative model of the data distribution. WAE minimizes a penalized form of\nthe Wasserstein distance between the model distribution and the target\ndistribution, which leads to a different regularizer than the one used by the\nVariational Auto-Encoder (VAE). This regularizer encourages the encoded\ntraining distribution to match the prior. We compare our algorithm with several\nother techniques and show that it is a generalization of adversarial\nauto-encoders (AAE). Our experiments show that WAE shares many of the\nproperties of VAEs (stable training, encoder-decoder architecture, nice latent\nmanifold structure) while generating samples of better quality, as measured by\nthe FID score. \n\n"}
{"id": "1711.01567", "contents": "Title: Robust Speech Recognition Using Generative Adversarial Networks Abstract: This paper describes a general, scalable, end-to-end framework that uses the\ngenerative adversarial network (GAN) objective to enable robust speech\nrecognition. Encoders trained with the proposed approach enjoy improved\ninvariance by learning to map noisy audio to the same embedding space as that\nof clean audio. Unlike previous methods, the new framework does not rely on\ndomain expertise or simplifying assumptions as are often needed in signal\nprocessing, and directly encourages robustness in a data-driven way. We show\nthe new approach improves simulated far-field speech recognition of vanilla\nsequence-to-sequence models without specialized front-ends or preprocessing. \n\n"}
{"id": "1711.01744", "contents": "Title: KGAN: How to Break The Minimax Game in GAN Abstract: Generative Adversarial Networks (GANs) were intuitively and attractively\nexplained under the perspective of game theory, wherein two involving parties\nare a discriminator and a generator. In this game, the task of the\ndiscriminator is to discriminate the real and generated (i.e., fake) data,\nwhilst the task of the generator is to generate the fake data that maximally\nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs,\nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows\na connection between the general loss of a classification problem regarding a\nconvex loss function and a f-divergence between the true and fake data\ndistributions. Mathematically, we proposed a setting for the classification\nproblem of the true and fake data, wherein we can prove that the general loss\nof this classification problem is exactly the negative f-divergence for a\ncertain convex function f. This allows us to interpret the problem of learning\nthe generator for dismissing the f-divergence between the true and fake data\ndistributions as that of maximizing the general loss which is equivalent to the\nmin-max problem in GAN if the Logistic loss is used in the classification\nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows\nus to employ any convex loss function for the discriminator. Second, it\nsuggests that rather than limiting ourselves in NN-based discriminators, we can\nalternatively utilize other powerful families. Bearing this viewpoint, we then\npropose using the kernel-based family for discriminators. This family has two\nappealing features: i) a powerful capacity in classifying non-linear nature\ndata and ii) being convex in the feature space. Using the convexity of this\nfamily, we can further develop Fenchel duality to equivalently transform the\nmax-min problem to the max-max dual problem. \n\n"}
{"id": "1711.02317", "contents": "Title: Multi-Player Bandits Revisited Abstract: Multi-player Multi-Armed Bandits (MAB) have been extensively studied in the\nliterature, motivated by applications to Cognitive Radio systems. Driven by\nsuch applications as well, we motivate the introduction of several levels of\nfeedback for multi-player MAB algorithms. Most existing work assume that\nsensing information is available to the algorithm. Under this assumption, we\nimprove the state-of-the-art lower bound for the regret of any decentralized\nalgorithms and introduce two algorithms, RandTopM and MCTopM, that are shown to\nempirically outperform existing algorithms. Moreover, we provide strong\ntheoretical guarantees for these algorithms, including a notion of asymptotic\noptimality in terms of the number of selections of bad arms. We then introduce\na promising heuristic, called Selfish, that can operate without sensing\ninformation, which is crucial for emerging applications to Internet of Things\nnetworks. We investigate the empirical performance of this algorithm and\nprovide some first theoretical elements for the understanding of its behavior. \n\n"}
{"id": "1711.03321", "contents": "Title: A Separation Principle for Control in the Age of Deep Learning Abstract: We review the problem of defining and inferring a \"state\" for a control\nsystem based on complex, high-dimensional, highly uncertain measurement streams\nsuch as videos. Such a state, or representation, should contain all and only\nthe information needed for control, and discount nuisance variability in the\ndata. It should also have finite complexity, ideally modulated depending on\navailable resources. This representation is what we want to store in memory in\nlieu of the data, as it \"separates\" the control task from the measurement\nprocess. For the trivial case with no dynamics, a representation can be\ninferred by minimizing the Information Bottleneck Lagrangian in a function\nclass realized by deep neural networks. The resulting representation has much\nhigher dimension than the data, already in the millions, but it is smaller in\nthe sense of information content, retaining only what is needed for the task.\nThis process also yields representations that are invariant to nuisance factors\nand having maximally independent components. We extend these ideas to the\ndynamic case, where the representation is the posterior density of the task\nvariable given the measurements up to the current time, which is in general\nmuch simpler than the prediction density maintained by the classical Bayesian\nfilter. Again this can be finitely-parametrized using a deep neural network,\nand already some applications are beginning to emerge. No explicit assumption\nof Markovianity is needed; instead, complexity trades off approximation of an\noptimal representation, including the degree of Markovianity. \n\n"}
{"id": "1711.03560", "contents": "Title: SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and\n  Complements Abstract: We develop SHOPPER, a sequential probabilistic model of shopping data.\nSHOPPER uses interpretable components to model the forces that drive how a\ncustomer chooses products; in particular, we designed SHOPPER to capture how\nitems interact with other items. We develop an efficient posterior inference\nalgorithm to estimate these forces from large-scale data, and we analyze a\nlarge dataset from a major chain grocery store. We are interested in answering\ncounterfactual queries about changes in prices. We found that SHOPPER provides\naccurate predictions even under price interventions, and that it helps identify\ncomplementary and substitutable pairs of products. \n\n"}
{"id": "1711.03634", "contents": "Title: Alternating minimization for dictionary learning: Local Convergence\n  Guarantees Abstract: We present theoretical guarantees for an alternating minimization algorithm\nfor the dictionary learning/sparse coding problem. The dictionary learning\nproblem is to factorize vector samples $y^{1},y^{2},\\ldots, y^{n}$ into an\nappropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*},\\ldots,x^{n*}$.\nOur algorithm is a simple alternating minimization procedure that switches\nbetween $\\ell_1$ minimization and gradient descent in alternate steps.\nDictionary learning and specifically alternating minimization algorithms for\ndictionary learning are well studied both theoretically and empirically.\nHowever, in contrast to previous theoretical analyses for this problem, we\nreplace a condition on the operator norm (that is, the largest magnitude\nsingular value) of the true underlying dictionary $A^*$ with a condition on the\nmatrix infinity norm (that is, the largest magnitude term). Our guarantees are\nunder a reasonable generative model that allows for dictionaries with growing\noperator norms, and can handle an arbitrary level of overcompleteness, while\nhaving sparsity that is information theoretically optimal. We also establish\nupper bounds on the sample complexity of our algorithm. \n\n"}
{"id": "1711.04043", "contents": "Title: Few-Shot Learning with Graph Neural Networks Abstract: We propose to study the problem of few-shot learning with the prism of\ninference on a partially observed graphical model, constructed from a\ncollection of input images whose label can be either observed or not. By\nassimilating generic message-passing inference algorithms with their\nneural-network counterparts, we define a graph neural network architecture that\ngeneralizes several of the recently proposed few-shot learning models. Besides\nproviding improved numerical performance, our framework is easily extended to\nvariants of few-shot learning, such as semi-supervised or active learning,\ndemonstrating the ability of graph-based models to operate well on 'relational'\ntasks. \n\n"}
{"id": "1711.05411", "contents": "Title: Z-Forcing: Training Stochastic Recurrent Networks Abstract: Many efforts have been devoted to training generative latent variable models\nwith autoregressive decoders, such as recurrent neural networks (RNN).\nStochastic recurrent models have been successful in capturing the variability\nobserved in natural sequential data such as speech. We unify successful ideas\nfrom recently proposed architectures into a stochastic recurrent model: each\nstep in the sequence is associated with a latent variable that is used to\ncondition the recurrent dynamics for future steps. Training is performed with\namortized variational inference where the approximate posterior is augmented\nwith a RNN that runs backward through the sequence. In addition to maximizing\nthe variational lower bound, we ease training of the latent variables by adding\nan auxiliary cost which forces them to reconstruct the state of the backward\nrecurrent network. This provides the latent variables with a task-independent\nobjective that enhances the performance of the overall model. We found this\nstrategy to perform better than alternative approaches such as KL annealing.\nAlthough being conceptually simple, our model achieves state-of-the-art results\non standard speech benchmarks such as TIMIT and Blizzard and competitive\nperformance on sequential MNIST. Finally, we apply our model to language\nmodeling on the IMDB dataset where the auxiliary cost helps in learning\ninterpretable latent variables. Source Code:\n\\url{https://github.com/anirudh9119/zforcing_nips17} \n\n"}
{"id": "1711.05828", "contents": "Title: BoostJet: Towards Combining Statistical Aggregates with Neural\n  Embeddings for Recommendations Abstract: Recommenders have become widely popular in recent years because of their\nbroader applicability in many e-commerce applications. These applications rely\non recommenders for generating advertisements for various offers or providing\ncontent recommendations. However, the quality of the generated recommendations\ndepends on user features (like demography, temporality), offer features (like\npopularity, price), and user-offer features (like implicit or explicit\nfeedback). Current state-of-the-art recommenders do not explore such diverse\nfeatures concurrently while generating the recommendations.\n  In this paper, we first introduce the notion of Trackers which enables us to\ncapture the above-mentioned features and thus incorporate users' online\nbehaviour through statistical aggregates of different features (demography,\ntemporality, popularity, price). We also show how to capture offer-to-offer\nrelations, based on their consumption sequence, leveraging neural embeddings\nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel\nrecommender which integrates the Trackers along with the neural embeddings\nusing MatrixNet, an efficient distributed implementation of gradient boosted\ndecision tree, to improve the recommendation quality significantly. We provide\nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online\nbehaviour from tens of millions of online users, to demonstrate the\npracticality of BoostJet in terms of recommendation quality as well as\nscalability. \n\n"}
{"id": "1711.06424", "contents": "Title: A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit Abstract: Determining the appropriate batch size for mini-batch gradient descent is\nalways time consuming as it often relies on grid search. This paper considers a\nresizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed\nbandit for achieving best performance in grid search by selecting an\nappropriate batch size at each epoch with a probability defined as a function\nof its previous success/failure. This probability encourages exploration of\ndifferent batch size and then later exploitation of batch size with history of\nsuccess. At each epoch, the RMGD samples a batch size from its probability\ndistribution, then uses the selected batch size for mini-batch gradient\ndescent. After obtaining the validation loss at each epoch, the probability\ndistribution is updated to incorporate the effectiveness of the sampled batch\nsize. The RMGD essentially assists the learning process to explore the possible\ndomain of the batch size and exploit successful batch size. Experimental\nresults show that the RMGD achieves performance better than the best performing\nsingle batch size. Furthermore, it, obviously, attains this performance in a\nshorter amount of time than grid search. It is surprising that the RMGD\nachieves better performance than grid search. \n\n"}
{"id": "1711.07005", "contents": "Title: Convergence Analysis of the Dynamics of a Special Kind of Two-Layered\n  Neural Networks with $\\ell_1$ and $\\ell_2$ Regularization Abstract: In this paper, we made an extension to the convergence analysis of the\ndynamics of two-layered bias-free networks with one $ReLU$ output. We took into\nconsideration two popular regularization terms: the $\\ell_1$ and $\\ell_2$ norm\nof the parameter vector $w$, and added it to the square loss function with\ncoefficient $\\lambda/2$. We proved that when $\\lambda$ is small, the weight\nvector $w$ converges to the optimal solution $\\hat{w}$ (with respect to the new\nloss function) with probability $\\geq (1-\\varepsilon)(1-A_d)/2$ under random\ninitiations in a sphere centered at the origin, where $\\varepsilon$ is a small\nvalue and $A_d$ is a constant. Numerical experiments including phase diagrams\nand repeated simulations verified our theory. \n\n"}
{"id": "1711.07076", "contents": "Title: Does mitigating ML's impact disparity require treatment disparity? Abstract: Following related work in law and policy, two notions of disparity have come\nto shape the study of fairness in algorithmic decision-making. Algorithms\nexhibit treatment disparity if they formally treat members of protected\nsubgroups differently; algorithms exhibit impact disparity when outcomes differ\nacross subgroups, even if the correlation arises unintentionally. Naturally, we\ncan achieve impact parity through purposeful treatment disparity. In one thread\nof technical work, papers aim to reconcile the two forms of parity proposing\ndisparate learning processes (DLPs). Here, the learning algorithm can see group\nmembership during training but produce a classifier that is group-blind at test\ntime. In this paper, we show theoretically that: (i) When other features\ncorrelate to group membership, DLPs will (indirectly) implement treatment\ndisparity, undermining the policy desiderata they are designed to address; (ii)\nWhen group membership is partly revealed by other features, DLPs induce\nwithin-class discrimination; and (iii) In general, DLPs provide a suboptimal\ntrade-off between accuracy and impact parity. Based on our technical analysis,\nwe argue that transparent treatment disparity is preferable to occluded methods\nfor achieving impact parity. Experimental results on several real-world\ndatasets highlight the practical consequences of applying DLPs vs. per-group\nthresholds. \n\n"}
{"id": "1711.07099", "contents": "Title: Compression-Based Regularization with an Application to Multi-Task\n  Learning Abstract: This paper investigates, from information theoretic grounds, a learning\nproblem based on the principle that any regularity in a given dataset can be\nexploited to extract compact features from data, i.e., using fewer bits than\nneeded to fully describe the data itself, in order to build meaningful\nrepresentations of a relevant content (multiple labels). We begin by\nintroducing the noisy lossy source coding paradigm with the log-loss fidelity\ncriterion which provides the fundamental tradeoffs between the\n\\emph{cross-entropy loss} (average risk) and the information rate of the\nfeatures (model complexity). Our approach allows an information theoretic\nformulation of the \\emph{multi-task learning} (MTL) problem which is a\nsupervised learning framework in which the prediction models for several\nrelated tasks are learned jointly from common representations to achieve better\ngeneralization performance. Then, we present an iterative algorithm for\ncomputing the optimal tradeoffs and its global convergence is proven provided\nthat some conditions hold. An important property of this algorithm is that it\nprovides a natural safeguard against overfitting, because it minimizes the\naverage risk taking into account a penalization induced by the model\ncomplexity. Remarkably, empirical results illustrate that there exists an\noptimal information rate minimizing the \\emph{excess risk} which depends on the\nnature and the amount of available training data. An application to\nhierarchical text categorization is also investigated, extending previous\nworks. \n\n"}
{"id": "1711.09176", "contents": "Title: Selling to a No-Regret Buyer Abstract: We consider the problem of a single seller repeatedly selling a single item\nto a single buyer (specifically, the buyer has a value drawn fresh from known\ndistribution $D$ in every round). Prior work assumes that the buyer is fully\nrational and will perfectly reason about how their bids today affect the\nseller's decisions tomorrow. In this work we initiate a different direction:\nthe buyer simply runs a no-regret learning algorithm over possible bids. We\nprovide a fairly complete characterization of optimal auctions for the seller\nin this domain. Specifically:\n  - If the buyer bids according to EXP3 (or any \"mean-based\" learning\nalgorithm), then the seller can extract expected revenue arbitrarily close to\nthe expected welfare. This auction is independent of the buyer's valuation $D$,\nbut somewhat unnatural as it is sometimes in the buyer's interest to overbid. -\nThere exists a learning algorithm $\\mathcal{A}$ such that if the buyer bids\naccording to $\\mathcal{A}$ then the optimal strategy for the seller is simply\nto post the Myerson reserve for $D$ every round. - If the buyer bids according\nto EXP3 (or any \"mean-based\" learning algorithm), but the seller is restricted\nto \"natural\" auction formats where overbidding is dominated (e.g. Generalized\nFirst-Price or Generalized Second-Price), then the optimal strategy for the\nseller is a pay-your-bid format with decreasing reserves over time. Moreover,\nthe seller's optimal achievable revenue is characterized by a linear program,\nand can be unboundedly better than the best truthful auction yet simultaneously\nunboundedly worse than the expected welfare. \n\n"}
{"id": "1711.09576", "contents": "Title: Extracting Automata from Recurrent Neural Networks Using Queries and\n  Counterexamples Abstract: We present a novel algorithm that uses exact learning and abstraction to\nextract a deterministic finite automaton describing the state dynamics of a\ngiven trained RNN. We do this using Angluin's L* algorithm as a learner and the\ntrained RNN as an oracle. Our technique efficiently extracts accurate automata\nfrom trained RNNs, even when the state vectors are large and require fine\ndifferentiation. \n\n"}
{"id": "1711.10168", "contents": "Title: Semi-supervised learning of hierarchical representations of molecules\n  using neural message passing Abstract: With the rapid increase of compound databases available in medicinal and\nmaterial science, there is a growing need for learning representations of\nmolecules in a semi-supervised manner. In this paper, we propose an\nunsupervised hierarchical feature extraction algorithm for molecules (or more\ngenerally, graph-structured objects with fixed number of types of nodes and\nedges), which is applicable to both unsupervised and semi-supervised tasks. Our\nmethod extends recently proposed Paragraph Vector algorithm and incorporates\nneural message passing to obtain hierarchical representations of subgraphs. We\napplied our method to an unsupervised task and demonstrated that it outperforms\nexisting proposed methods in several benchmark datasets. We also experimentally\nshowed that semi-supervised tasks enhanced predictive performance compared with\nsupervised ones with labeled molecules only. \n\n"}
{"id": "1711.10589", "contents": "Title: Contextual Outlier Interpretation Abstract: Outlier detection plays an essential role in many data-driven applications to\nidentify isolated instances that are different from the majority. While many\nstatistical learning and data mining techniques have been used for developing\nmore effective outlier detection algorithms, the interpretation of detected\noutliers does not receive much attention. Interpretation is becoming\nincreasingly important to help people trust and evaluate the developed models\nthrough providing intrinsic reasons why the certain outliers are chosen. It is\ndifficult, if not impossible, to simply apply feature selection for explaining\noutliers due to the distinct characteristics of various detection models,\ncomplicated structures of data in certain applications, and imbalanced\ndistribution of outliers and normal instances. In addition, the role of\ncontrastive contexts where outliers locate, as well as the relation between\noutliers and contexts, are usually overlooked in interpretation. To tackle the\nissues above, in this paper, we propose a novel Contextual Outlier\nINterpretation (COIN) method to explain the abnormality of existing outliers\nspotted by detectors. The interpretability for an outlier is achieved from\nthree aspects: outlierness score, attributes that contribute to the\nabnormality, and contextual description of its neighborhoods. Experimental\nresults on various types of datasets demonstrate the flexibility and\neffectiveness of the proposed framework compared with existing interpretation\napproaches. \n\n"}
{"id": "1712.00287", "contents": "Title: Faithful Inversion of Generative Models for Effective Amortized\n  Inference Abstract: Inference amortization methods share information across multiple\nposterior-inference problems, allowing each to be carried out more efficiently.\nGenerally, they require the inversion of the dependency structure in the\ngenerative model, as the modeller must learn a mapping from observations to\ndistributions approximating the posterior. Previous approaches have involved\ninverting the dependency structure in a heuristic way that fails to capture\nthese dependencies correctly, thereby limiting the achievable accuracy of the\nresulting approximations. We introduce an algorithm for faithfully, and\nminimally, inverting the graphical model structure of any generative model.\nSuch inverses have two crucial properties: (a) they do not encode any\nindependence assertions that are absent from the model and; (b) they are local\nmaxima for the number of true independencies encoded. We prove the correctness\nof our approach and empirically show that the resulting minimally faithful\ninverses lead to better inference amortization than existing heuristic\napproaches. \n\n"}
{"id": "1712.00481", "contents": "Title: Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes Abstract: In order to submit a claim to insurance companies, a doctor needs to code a\npatient encounter with both the diagnosis (ICDs) and procedures performed\n(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant\nprocedures code is a cumbersome and time-consuming task as a doctor has to\nchoose from around 13,000 procedure codes with no predefined one-to-one\nmapping. In this paper, we propose a state-of-the-art deep learning method for\nautomatic and intelligent coding of procedures (CPTs) from the diagnosis codes\n(ICDs) entered by the doctor. Precisely, we cast the learning problem as a\nmulti-label classification problem and use distributed representation to learn\nthe input mapping of high-dimensional sparse ICDs codes. Our final model\ntrained on 2.3 million claims is able to outperform existing rule-based\nprobabilistic and association-rule mining based methods and has a recall of\n90@3. \n\n"}
{"id": "1712.00827", "contents": "Title: Entanglement and secret-key-agreement capacities of bipartite quantum\n  interactions and read-only memory devices Abstract: A bipartite quantum interaction corresponds to the most general quantum\ninteraction that can occur between two quantum systems in the presence of a\nbath. In this work, we determine bounds on the capacities of bipartite\ninteractions for entanglement generation and secret key agreement between two\nquantum systems. Our upper bound on the entanglement generation capacity of a\nbipartite quantum interaction is given by a quantity called the bidirectional\nmax-Rains information. Our upper bound on the secret-key-agreement capacity of\na bipartite quantum interaction is given by a related quantity called the\nbidirectional max-relative entropy of entanglement. We also derive tighter\nupper bounds on the capacities of bipartite interactions obeying certain\nsymmetries. Observing that reading of a memory device is a particular kind of\nbipartite quantum interaction, we leverage our bounds from the bidirectional\nsetting to deliver bounds on the capacity of a task that we introduce, called\nprivate reading of a wiretap memory cell. Given a set of point-to-point quantum\nwiretap channels, the goal of private reading is for an encoder to form\ncodewords from these channels, in order to establish secret key with a party\nwho controls one input and one output of the channels, while a passive\neavesdropper has access to one output of the channels. We derive both lower and\nupper bounds on the private reading capacities of a wiretap memory cell. We\nthen extend these results to determine achievable rates for the generation of\nentanglement between two distant parties who have coherent access to a\ncontrolled point-to-point channel, which is a particular kind of bipartite\ninteraction. \n\n"}
{"id": "1712.00912", "contents": "Title: Deep Learning Diffuse Optical Tomography Abstract: Diffuse optical tomography (DOT) has been investigated as an alternative\nimaging modality for breast cancer detection thanks to its excellent contrast\nto hemoglobin oxidization level. However, due to the complicated non-linear\nphoton scattering physics and ill-posedness, the conventional reconstruction\nalgorithms are sensitive to imaging parameters such as boundary conditions. To\naddress this, here we propose a novel deep learning approach that learns\nnon-linear photon scattering physics and obtains an accurate three dimensional\n(3D) distribution of optical anomalies. In contrast to the traditional\nblack-box deep learning approaches, our deep network is designed to invert the\nLippman-Schwinger integral equation using the recent mathematical theory of\ndeep convolutional framelets. As an example of clinical relevance, we applied\nthe method to our prototype DOT system. We show that our deep neural network,\ntrained with only simulation data, can accurately recover the location of\nanomalies within biomimetic phantoms and live animals without the use of an\nexogenous contrast agent. \n\n"}
{"id": "1712.01158", "contents": "Title: Statistical Inference for Incomplete Ranking Data: The Case of\n  Rank-Dependent Coarsening Abstract: We consider the problem of statistical inference for ranking data,\nspecifically rank aggregation, under the assumption that samples are incomplete\nin the sense of not comprising all choice alternatives. In contrast to most\nexisting methods, we explicitly model the process of turning a full ranking\ninto an incomplete one, which we call the coarsening process. To this end, we\npropose the concept of rank-dependent coarsening, which assumes that incomplete\nrankings are produced by projecting a full ranking to a random subset of ranks.\nFor a concrete instantiation of our model, in which full rankings are drawn\nfrom a Plackett-Luce distribution and observations take the form of pairwise\npreferences, we study the performance of various rank aggregation methods. In\naddition to predictive accuracy in the finite sample setting, we address the\ntheoretical question of consistency, by which we mean the ability to recover a\ntarget ranking when the sample size goes to infinity, despite a potential bias\nin the observations caused by the (unknown) coarsening. \n\n"}
{"id": "1712.01312", "contents": "Title: Learning Sparse Neural Networks through $L_0$ Regularization Abstract: We propose a practical method for $L_0$ norm regularization for neural\nnetworks: pruning the network during training by encouraging weights to become\nexactly zero. Such regularization is interesting since (1) it can greatly speed\nup training and inference, and (2) it can improve generalization. AIC and BIC,\nwell-known model selection criteria, are special cases of $L_0$ regularization.\nHowever, since the $L_0$ norm of weights is non-differentiable, we cannot\nincorporate it directly as a regularization term in the objective function. We\npropose a solution through the inclusion of a collection of non-negative\nstochastic gates, which collectively determine which weights to set to zero. We\nshow that, somewhat surprisingly, for certain distributions over the gates, the\nexpected $L_0$ norm of the resulting gated weights is differentiable with\nrespect to the distribution parameters. We further propose the \\emph{hard\nconcrete} distribution for the gates, which is obtained by \"stretching\" a\nbinary concrete distribution and then transforming its samples with a\nhard-sigmoid. The parameters of the distribution over the gates can then be\njointly optimized with the original network parameters. As a result our method\nallows for straightforward and efficient learning of model structures with\nstochastic gradient descent and allows for conditional computation in a\nprincipled way. We perform various experiments to demonstrate the effectiveness\nof the resulting approach and regularizer. \n\n"}
{"id": "1712.01447", "contents": "Title: Gaussian Process bandits with adaptive discretization Abstract: In this paper, the problem of maximizing a black-box function $f:\\mathcal{X}\n\\to \\mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process\n(GP) prior. In particular, a new algorithm for this problem is proposed, and\nhigh probability bounds on its simple and cumulative regret are established.\nThe query point selection rule in most existing methods involves an exhaustive\nsearch over an increasingly fine sequence of uniform discretizations of\n$\\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines\n$\\mathcal{X}$ which leads to a lower computational complexity, particularly\nwhen $\\mathcal{X}$ is a subset of a high dimensional Euclidean space. In\naddition to the computational gains, sufficient conditions are identified under\nwhich the regret bounds of the new algorithm improve upon the known results.\nFinally an extension of the algorithm to the case of contextual bandits is\nproposed, and high probability bounds on the contextual regret are presented. \n\n"}
{"id": "1712.01664", "contents": "Title: Learning a Generative Model for Validity in Complex Discrete Structures Abstract: Deep generative models have been successfully used to learn representations\nfor high-dimensional discrete spaces by representing discrete objects as\nsequences and employing powerful sequence-based deep models. Unfortunately,\nthese sequence-based models often produce invalid sequences: sequences which do\nnot represent any underlying discrete structure; invalid sequences hinder the\nutility of such models. As a step towards solving this problem, we propose to\nlearn a deep recurrent validator model, which can estimate whether a partial\nsequence can function as the beginning of a full, valid sequence. This\nvalidator provides insight as to how individual sequence elements influence the\nvalidity of the overall sequence, and can be used to constrain sequence based\nmodels to generate valid sequences -- and thus faithfully model discrete\nobjects. Our approach is inspired by reinforcement learning, where an oracle\nwhich can evaluate validity of complete sequences provides a sparse reward\nsignal. We demonstrate its effectiveness as a generative model of Python 3\nsource code for mathematical expressions, and in improving the ability of a\nvariational autoencoder trained on SMILES strings to decode valid molecular\nstructures. \n\n"}
{"id": "1712.01850", "contents": "Title: Determining a local Hamiltonian from a single eigenstate Abstract: We ask whether the knowledge of a single eigenstate of a local Hamiltonian is\nsufficient to uniquely determine the Hamiltonian. We present evidence that the\nanswer is \"yes\" for generic local Hamiltonians, given either the ground state\nor an excited eigenstate. In fact, knowing only the two-point equal-time\ncorrelation functions of local observables with respect to the eigenstate\nshould generically be sufficient to exactly recover the Hamiltonian for\nfinite-size systems, with numerical algorithms that run in a time that is\npolynomial in the system size. We also investigate the large-system limit, the\nsensitivity of the reconstruction to error, and the case when correlation\nfunctions are only known for observables on a fixed sub-region. Numerical\ndemonstrations support the results for finite one-dimensional spin chains\n(though caution must be taken when extrapolating to infinite-size systems in\nhigher dimensions). For the purpose of our analysis, we define the\n\"$k$-correlation spectrum\" of a state, which reveals properties of local\ncorrelations in the state and may be of independent interest. \n\n"}
{"id": "1712.02512", "contents": "Title: Gini-regularized Optimal Transport with an Application to\n  Spatio-Temporal Forecasting Abstract: Rapidly growing product lines and services require a finer-granularity\nforecast that considers geographic locales. However the open question remains,\nhow to assess the quality of a spatio-temporal forecast? In this manuscript we\nintroduce a metric to evaluate spatio-temporal forecasts. This metric is based\non an Opti- mal Transport (OT) problem. The metric we propose is a constrained\nOT objec- tive function using the Gini impurity function as a regularizer. We\ndemonstrate through computer experiments both the qualitative and the\nquantitative charac- teristics of the Gini regularized OT problem. Moreover, we\nshow that the Gini regularized OT problem converges to the classical OT\nproblem, when the Gini regularized problem is considered as a function of\n{\\lambda}, the regularization parame-ter. The convergence to the classical OT\nsolution is faster than the state-of-the-art Entropic-regularized OT[Cuturi,\n2013] and results in a numerically more stable algorithm. \n\n"}
{"id": "1712.02716", "contents": "Title: Dynamical properties of dissipative XYZ Heisenberg lattices Abstract: We study dynamical properties of dissipative XYZ Heisenberg lattices where\nanisotropic spin-spin coupling competes with local incoherent spin flip\nprocesses. In particular, we explore a region of the parameter space where\ndissipative magnetic phase transitions for the steady state have been recently\npredicted by mean-field theories and exact numerical methods. We investigate\nthe asymptotic decay rate towards the steady state both in 1D (up to the\nthermodynamical limit) and in finite-size 2D lattices, showing that critical\ndynamics does not occur in 1D, but it can emerge in 2D. We also analyze the\nbehavior of individual homodyne quantum trajectories, which well reveal the\nnature of the transition. \n\n"}
{"id": "1712.05016", "contents": "Title: Deep Prior Abstract: The recent literature on deep learning offers new tools to learn a rich\nprobability distribution over high dimensional data such as images or sounds.\nIn this work we investigate the possibility of learning the prior distribution\nover neural network parameters using such tools. Our resulting variational\nBayes algorithm generalizes well to new tasks, even when very few training\nexamples are provided. Furthermore, this learned prior allows the model to\nextrapolate correctly far from a given task's training data on a meta-dataset\nof periodic signals. \n\n"}
{"id": "1712.06214", "contents": "Title: Predicting Individual Physiologically Acceptable States for Discharge\n  from a Pediatric Intensive Care Unit Abstract: Objective: Predict patient-specific vitals deemed medically acceptable for\ndischarge from a pediatric intensive care unit (ICU). Design: The means of each\npatient's hr, sbp and dbp measurements between their medical and physical\ndischarge from the ICU were computed as a proxy for their physiologically\nacceptable state space (PASS) for successful ICU discharge. These individual\nPASS values were compared via root mean squared error (rMSE) to population\nage-normal vitals, a polynomial regression through the PASS values of a\nPediatric ICU (PICU) population and predictions from two recurrent neural\nnetwork models designed to predict personalized PASS within the first twelve\nhours following ICU admission. Setting: PICU at Children's Hospital Los Angeles\n(CHLA). Patients: 6,899 PICU episodes (5,464 patients) collected between 2009\nand 2016. Interventions: None. Measurements: Each episode data contained 375\nvariables representing vitals, labs, interventions, and drugs. They also\nincluded a time indicator for PICU medical discharge and physical discharge.\nMain Results: The rMSEs between individual PASS values and population\nage-normals (hr: 25.9 bpm, sbp: 13.4 mmHg, dbp: 13.0 mmHg) were larger than the\nrMSEs corresponding to the polynomial regression (hr: 19.1 bpm, sbp: 12.3 mmHg,\ndbp: 10.8 mmHg). The rMSEs from the best performing RNN model were the lowest\n(hr: 16.4 bpm; sbp: 9.9 mmHg, dbp: 9.0 mmHg). Conclusion: PICU patients are a\nunique subset of the general population, and general age-normal vitals may not\nbe suitable as target values indicating physiologic stability at discharge.\nAge-normal vitals that were specifically derived from the medical-to-physical\ndischarge window of ICU patients may be more appropriate targets for\n'acceptable' physiologic state for critical care patients. Going beyond simple\nage bins, an RNN model can provide more personalized target values. \n\n"}
{"id": "1712.06568", "contents": "Title: ES Is More Than Just a Traditional Finite-Difference Approximator Abstract: An evolution strategy (ES) variant based on a simplification of a natural\nevolution strategy recently attracted attention because it performs\nsurprisingly well in challenging deep reinforcement learning domains. It\nsearches for neural network parameters by generating perturbations to the\ncurrent set of parameters, checking their performance, and moving in the\naggregate direction of higher reward. Because it resembles a traditional\nfinite-difference approximation of the reward gradient, it can naturally be\nconfused with one. However, this ES optimizes for a different gradient than\njust reward: It optimizes for the average reward of the entire population,\nthereby seeking parameters that are robust to perturbation. This difference can\nchannel ES into distinct areas of the search space relative to gradient\ndescent, and also consequently to networks with distinct properties. This\nunique robustness-seeking property, and its consequences for optimization, are\ndemonstrated in several domains. They include humanoid locomotion, where\nnetworks from policy gradient-based reinforcement learning are significantly\nless robust to parameter perturbation than ES-based policies solving the same\ntask. While the implications of such robustness and robustness-seeking remain\nopen to further study, this work's main contribution is to highlight such\ndifferences and their potential importance. \n\n"}
{"id": "1712.06695", "contents": "Title: Accurate Inference for Adaptive Linear Models Abstract: Estimators computed from adaptively collected data do not behave like their\nnon-adaptive brethren. Rather, the sequential dependence of the collection\npolicy can lead to severe distributional biases that persist even in the\ninfinite data limit. We develop a general method -- $\\mathbf{W}$-decorrelation\n-- for transforming the bias of adaptive linear regression estimators into\nvariance. The method uses only coarse-grained information about the data\ncollection policy and does not need access to propensity scores or exact\nknowledge of the policy. We bound the finite-sample bias and variance of the\n$\\mathbf{W}$-estimator and develop asymptotically correct confidence intervals\nbased on a novel martingale central limit theorem. We then demonstrate the\nempirical benefits of the generic $\\mathbf{W}$-decorrelation procedure in two\ndifferent adaptive data settings: the multi-armed bandit and the autoregressive\ntime series. \n\n"}
{"id": "1712.08085", "contents": "Title: Multipartite Entanglement Swapping and Mechanical Cluster States Abstract: We present a protocol for generating multipartite quantum correlations across\na quantum network with a continuous-variable architecture. An arbitrary number\nof users possess two-mode entangled states, keeping one mode while sending the\nother to a central relay. Here a suitable multipartite Bell detection is\nperformed which conditionally generates a cluster state on the retained modes.\nThis cluster state can be suitably manipulated by the parties and used for\ntasks of quantum communication in a fully optical scenario. More interestingly,\nthe protocol can be used to create a purely-mechanical cluster state starting\nfrom a supply of optomechanical systems. We show that detecting the optical\nparts of optomechanical cavities may efficiently swap entanglement into their\nmechanical modes, creating cluster states up to 5 modes under suitable\ncryogenic conditions. \n\n"}
{"id": "1712.09482", "contents": "Title: Robust Loss Functions under Label Noise for Deep Neural Networks Abstract: In many applications of classifier learning, training data suffers from label\nnoise. Deep networks are learned using huge training data where the problem of\nnoisy labels is particularly relevant. The current techniques proposed for\nlearning deep networks under label noise focus on modifying the network\narchitecture and on algorithms for estimating true labels from noisy labels. An\nalternate approach would be to look for loss functions that are inherently\nnoise-tolerant. For binary classification there exist theoretical results on\nloss functions that are robust to label noise. In this paper, we provide some\nsufficient conditions on a loss function so that risk minimization under that\nloss function would be inherently tolerant to label noise for multiclass\nclassification problems. These results generalize the existing results on\nnoise-tolerant loss functions for binary classification. We study some of the\nwidely used loss functions in deep networks and show that the loss function\nbased on mean absolute value of error is inherently robust to label noise. Thus\nstandard back propagation is enough to learn the true classifier even under\nlabel noise. Through experiments, we illustrate the robustness of risk\nminimization with such loss functions for learning neural networks. \n\n"}
{"id": "1712.10082", "contents": "Title: Application of Convolutional Neural Network to Predict Airfoil Lift\n  Coefficient Abstract: The adaptability of the convolutional neural network (CNN) technique for\naerodynamic meta-modeling tasks is probed in this work. The primary objective\nis to develop suitable CNN architecture for variable flow conditions and object\ngeometry, in addition to identifying a sufficient data preparation process.\nMultiple CNN structures were trained to learn the lift coefficients of the\nairfoils with a variety of shapes in multiple flow Mach numbers, Reynolds\nnumbers, and diverse angles of attack. This is conducted to illustrate the\nconcept of the technique. A multi-layered perceptron (MLP) is also used for the\ntraining sets. The MLP results are compared with that of the CNN results. The\nnewly proposed meta-modeling concept has been found to be comparable with the\nMLP in learning capability; and more importantly, our CNN model exhibits a\ncompetitive prediction accuracy with minimal constraints in a geometric\nrepresentation. \n\n"}
{"id": "1801.01973", "contents": "Title: A Note on the Inception Score Abstract: Deep generative models are powerful tools that have produced impressive\nresults in recent years. These advances have been for the most part empirically\ndriven, making it essential that we use high quality evaluation metrics. In\nthis paper, we provide new insights into the Inception Score, a recently\nproposed and widely used evaluation metric for generative models, and\ndemonstrate that it fails to provide useful guidance when comparing models. We\ndiscuss both suboptimalities of the metric itself and issues with its\napplication. Finally, we call for researchers to be more systematic and careful\nwhen evaluating and comparing generative models, as the advancement of the\nfield depends upon it. \n\n"}
{"id": "1801.02124", "contents": "Title: Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal\n  Demonstrations Abstract: This paper considers the problem of inverse reinforcement learning in\nzero-sum stochastic games when expert demonstrations are known to be not\noptimal. Compared to previous works that decouple agents in the game by\nassuming optimality in expert strategies, we introduce a new objective function\nthat directly pits experts against Nash Equilibrium strategies, and we design\nan algorithm to solve for the reward function in the context of inverse\nreinforcement learning with deep neural networks as model approximations. In\nour setting the model and algorithm do not decouple by agent. In order to find\nNash Equilibrium in large-scale games, we also propose an adversarial training\nalgorithm for zero-sum stochastic games, and show the theoretical appeal of\nnon-existence of local optima in its objective function. In our numerical\nexperiments, we demonstrate that our Nash Equilibrium and inverse reinforcement\nlearning algorithms address games that are not amenable to previous approaches\nusing tabular representations. Moreover, with sub-optimal expert demonstrations\nour algorithms recover both reward functions and strategies with good quality. \n\n"}
{"id": "1801.03164", "contents": "Title: Paranom: A Parallel Anomaly Dataset Generator Abstract: In this paper, we present Paranom, a parallel anomaly dataset generator. We\ndiscuss its design and provide brief experimental results demonstrating its\nusefulness in improving the classification correctness of LSTM-AD, a\nstate-of-the-art anomaly detection model. \n\n"}
{"id": "1801.03329", "contents": "Title: Weakly Supervised One-Shot Detection with Attention Similarity Networks Abstract: Neural network models that are not conditioned on class identities were shown\nto facilitate knowledge transfer between classes and to be well-suited for\none-shot learning tasks. Following this motivation, we further explore and\nestablish such models and present a novel neural network architecture for the\ntask of weakly supervised one-shot detection. Our model is only conditioned on\na single exemplar of an unseen class and a larger target example that may or\nmay not contain an instance of the same class as the exemplar. By pairing a\nSiamese similarity network with an attention mechanism, we design a model that\nmanages to simultaneously identify and localise instances of classes unseen at\ntraining time. In experiments with datasets from the computer vision and audio\ndomains, the proposed method considerably outperforms the baseline methods for\nthe weakly supervised one-shot detection task. \n\n"}
{"id": "1801.03744", "contents": "Title: Which Neural Net Architectures Give Rise To Exploding and Vanishing\n  Gradients? Abstract: We give a rigorous analysis of the statistical behavior of gradients in a\nrandomly initialized fully connected network N with ReLU activations. Our\nresults show that the empirical variance of the squares of the entries in the\ninput-output Jacobian of N is exponential in a simple architecture-dependent\nconstant beta, given by the sum of the reciprocals of the hidden layer widths.\nWhen beta is large, the gradients computed by N at initialization vary wildly.\nOur approach complements the mean field theory analysis of random networks.\nFrom this point of view, we rigorously compute finite width corrections to the\nstatistics of gradients at the edge of chaos. \n\n"}
{"id": "1801.05512", "contents": "Title: Deep Neural Networks for Survival Analysis Based on a Multi-Task\n  Framework Abstract: Survival analysis/time-to-event models are extremely useful as they can help\ncompanies predict when a customer will buy a product, churn or default on a\nloan, and therefore help them improve their ROI. In this paper, we introduce a\nnew method to calculate survival functions using the Multi-Task Logistic\nRegression (MTLR) model as its base and a deep learning architecture as its\ncore. Based on the Concordance index (C-index) and Brier score, this method\noutperforms the MTLR in all the experiments disclosed in this paper as well as\nthe Cox Proportional Hazard (CoxPH) model when nonlinear dependencies are\nfound. \n\n"}
{"id": "1801.05852", "contents": "Title: Network Representation Learning: A Survey Abstract: With the widespread use of information technologies, information networks are\nbecoming increasingly popular to capture complex relationships across various\ndisciplines, such as social networks, citation networks, telecommunication\nnetworks, and biological networks. Analyzing these networks sheds light on\ndifferent aspects of social life such as the structure of societies,\ninformation diffusion, and communication patterns. In reality, however, the\nlarge scale of information networks often makes network analytic tasks\ncomputationally expensive or intractable. Network representation learning has\nbeen recently proposed as a new learning paradigm to embed network vertices\ninto a low-dimensional vector space, by preserving network topology structure,\nvertex content, and other side information. This facilitates the original\nnetwork to be easily handled in the new vector space for further analysis. In\nthis survey, we perform a comprehensive review of the current literature on\nnetwork representation learning in the data mining and machine learning field.\nWe propose new taxonomies to categorize and summarize the state-of-the-art\nnetwork representation learning techniques according to the underlying learning\nmechanisms, the network information intended to preserve, as well as the\nalgorithmic designs and methodologies. We summarize evaluation protocols used\nfor validating network representation learning including published benchmark\ndatasets, evaluation methods, and open source algorithms. We also perform\nempirical studies to compare the performance of representative algorithms on\ncommon datasets, and analyze their computational complexity. Finally, we\nsuggest promising research directions to facilitate future study. \n\n"}
{"id": "1801.06159", "contents": "Title: When Does Stochastic Gradient Algorithm Work Well? Abstract: In this paper, we consider a general stochastic optimization problem which is\noften at the core of supervised learning, such as deep learning and linear\nclassification. We consider a standard stochastic gradient descent (SGD) method\nwith a fixed, large step size and propose a novel assumption on the objective\nfunction, under which this method has the improved convergence rates (to a\nneighborhood of the optimal solutions). We then empirically demonstrate that\nthese assumptions hold for logistic regression and standard deep neural\nnetworks on classical data sets. Thus our analysis helps to explain when\nefficient behavior can be expected from the SGD method in training\nclassification models and deep neural networks. \n\n"}
{"id": "1801.06287", "contents": "Title: What Does a TextCNN Learn? Abstract: TextCNN, the convolutional neural network for text, is a useful deep learning\nalgorithm for sentence classification tasks such as sentiment analysis and\nquestion classification. However, neural networks have long been known as black\nboxes because interpreting them is a challenging task. Researchers have\ndeveloped several tools to understand a CNN for image classification by deep\nvisualization, but research about deep TextCNNs is still insufficient. In this\npaper, we are trying to understand what a TextCNN learns on two classical NLP\ndatasets. Our work focuses on functions of different convolutional kernels and\ncorrelations between convolutional kernels. \n\n"}
{"id": "1801.06637", "contents": "Title: Deep Hidden Physics Models: Deep Learning of Nonlinear Partial\n  Differential Equations Abstract: A long-standing problem at the interface of artificial intelligence and\napplied mathematics is to devise an algorithm capable of achieving human level\nor even superhuman proficiency in transforming observed data into predictive\nmathematical models of the physical world. In the current era of abundance of\ndata and advanced machine learning capabilities, the natural question arises:\nHow can we automatically uncover the underlying laws of physics from\nhigh-dimensional data generated from experiments? In this work, we put forth a\ndeep learning approach for discovering nonlinear partial differential equations\nfrom scattered and potentially noisy observations in space and time.\nSpecifically, we approximate the unknown solution as well as the nonlinear\ndynamics by two deep neural networks. The first network acts as a prior on the\nunknown solution and essentially enables us to avoid numerical differentiations\nwhich are inherently ill-conditioned and unstable. The second network\nrepresents the nonlinear dynamics and helps us distill the mechanisms that\ngovern the evolution of a given spatiotemporal data-set. We test the\neffectiveness of our approach for several benchmark problems spanning a number\nof scientific domains and demonstrate how the proposed framework can help us\naccurately learn the underlying dynamics and forecast future states of the\nsystem. In particular, we study the Burgers', Korteweg-de Vries (KdV),\nKuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations. \n\n"}
{"id": "1801.07172", "contents": "Title: Scale-invariant Feature Extraction of Neural Network and Renormalization\n  Group Flow Abstract: Theoretical understanding of how deep neural network (DNN) extracts features\nfrom input images is still unclear, but it is widely believed that the\nextraction is performed hierarchically through a process of coarse-graining. It\nreminds us of the basic concept of renormalization group (RG) in statistical\nphysics. In order to explore possible relations between DNN and RG, we use the\nRestricted Boltzmann machine (RBM) applied to Ising model and construct a flow\nof model parameters (in particular, temperature) generated by the RBM. We show\nthat the unsupervised RBM trained by spin configurations at various\ntemperatures from $T=0$ to $T=6$ generates a flow along which the temperature\napproaches the critical value $T_c=2.27$. This behavior is opposite to the\ntypical RG flow of the Ising model. By analyzing various properties of the\nweight matrices of the trained RBM, we discuss why it flows towards $T_c$ and\nhow the RBM learns to extract features of spin configurations. \n\n"}
{"id": "1801.08092", "contents": "Title: Generalizable Data-free Objective for Crafting Universal Adversarial\n  Perturbations Abstract: Machine learning models are susceptible to adversarial perturbations: small\nchanges to input that can cause large changes in output. It is also\ndemonstrated that there exist input-agnostic perturbations, called universal\nadversarial perturbations, which can change the inference of target model on\nmost of the data samples. However, existing methods to craft universal\nperturbations are (i) task specific, (ii) require samples from the training\ndata distribution, and (iii) perform complex optimizations. Additionally,\nbecause of the data dependence, fooling ability of the crafted perturbations is\nproportional to the available training data. In this paper, we present a novel,\ngeneralizable and data-free approaches for crafting universal adversarial\nperturbations. Independent of the underlying task, our objective achieves\nfooling via corrupting the extracted features at multiple layers. Therefore,\nthe proposed objective is generalizable to craft image-agnostic perturbations\nacross multiple vision tasks such as object recognition, semantic segmentation,\nand depth estimation. In the practical setting of black-box attack scenario\n(when the attacker does not have access to the target model and it's training\ndata), we show that our objective outperforms the data dependent objectives to\nfool the learned models. Further, via exploiting simple priors related to the\ndata distribution, our objective remarkably boosts the fooling ability of the\ncrafted perturbations. Significant fooling rates achieved by our objective\nemphasize that the current deep learning models are now at an increased risk,\nsince our objective generalizes across multiple tasks without the requirement\nof training data for crafting the perturbations. To encourage reproducible\nresearch, we have released the codes for our proposed algorithm. \n\n"}
{"id": "1801.08099", "contents": "Title: Logically-Constrained Reinforcement Learning Abstract: We present the first model-free Reinforcement Learning (RL) algorithm to\nsynthesise policies for an unknown Markov Decision Process (MDP), such that a\nlinear time property is satisfied. The given temporal property is converted\ninto a Limit Deterministic Buchi Automaton (LDBA) and a robust reward function\nis defined over the state-action pairs of the MDP according to the resulting\nLDBA. With this reward function, the policy synthesis procedure is\n\"constrained\" by the given specification. These constraints guide the MDP\nexploration so as to minimize the solution time by only considering the portion\nof the MDP that is relevant to satisfaction of the LTL property. This improves\nperformance and scalability of the proposed method by avoiding an exhaustive\nupdate over the whole state space while the efficiency of standard methods such\nas dynamic programming is hindered by excessive memory requirements, caused by\nthe need to store a full-model in memory. Additionally, we show that the RL\nprocedure sets up a local value iteration method to efficiently calculate the\nmaximum probability of satisfying the given property, at any given state of the\nMDP. We prove that our algorithm is guaranteed to find a policy whose traces\nprobabilistically satisfy the LTL property if such a policy exists, and\nadditionally we show that our method produces reasonable control policies even\nwhen the LTL property cannot be satisfied. The performance of the algorithm is\nevaluated via a set of numerical examples. We observe an improvement of one\norder of magnitude in the number of iterations required for the synthesis\ncompared to existing approaches. \n\n"}
{"id": "1801.08640", "contents": "Title: Considerations When Learning Additive Explanations for Black-Box Models Abstract: Many methods to explain black-box models, whether local or global, are\nadditive. In this paper, we study global additive explanations for non-additive\nmodels, focusing on four explanation methods: partial dependence, Shapley\nexplanations adapted to a global setting, distilled additive explanations, and\ngradient-based explanations. We show that different explanation methods\ncharacterize non-additive components in a black-box model's prediction function\nin different ways. We use the concepts of main and total effects to anchor\nadditive explanations, and quantitatively evaluate additive and non-additive\nexplanations. Even though distilled explanations are generally the most\naccurate additive explanations, non-additive explanations such as tree\nexplanations that explicitly model non-additive components tend to be even more\naccurate. Despite this, our user study showed that machine learning\npractitioners were better able to leverage additive explanations for various\ntasks. These considerations should be taken into account when considering which\nexplanation to trust and use to explain black-box models. \n\n"}
{"id": "1801.08702", "contents": "Title: Improving Bi-directional Generation between Different Modalities with\n  Variational Autoencoders Abstract: We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. A major approach to achieve this objective is to train a model that\nintegrates all the information of different modalities into a joint\nrepresentation and then to generate one modality from the corresponding other\nmodality via this joint representation. We simply applied this approach to\nvariational autoencoders (VAEs), which we call a joint multimodal variational\nautoencoder (JMVAE). However, we found that when this model attempts to\ngenerate a large dimensional modality missing at the input, the joint\nrepresentation collapses and this modality cannot be generated successfully.\nFurthermore, we confirmed that this difficulty cannot be resolved even using a\nknown solution. Therefore, in this study, we propose two models to prevent this\ndifficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that\nthese methods can prevent the difficulty above and that they generate\nmodalities bi-directionally with equal or higher likelihood than conventional\nVAE methods, which generate in only one direction. Moreover, we confirm that\nthese methods can obtain the joint representation appropriately, so that they\ncan generate various variations of modality by moving over the joint\nrepresentation or changing the value of another modality. \n\n"}
{"id": "1801.10123", "contents": "Title: Links: A High-Dimensional Online Clustering Method Abstract: We present a novel algorithm, called Links, designed to perform online\nclustering on unit vectors in a high-dimensional Euclidean space. The algorithm\nis appropriate when it is necessary to cluster data efficiently as it streams\nin, and is to be contrasted with traditional batch clustering algorithms that\nhave access to all data at once. For example, Links has been successfully\napplied to embedding vectors generated from face images or voice recordings for\nthe purpose of recognizing people, thereby providing real-time identification\nduring video or audio capture. \n\n"}
{"id": "1802.00130", "contents": "Title: Distributed Newton Methods for Deep Neural Networks Abstract: Deep learning involves a difficult non-convex optimization problem with a\nlarge number of weights between any two adjacent layers of a deep structure. To\nhandle large data sets or complicated networks, distributed training is needed,\nbut the calculation of function, gradient, and Hessian is expensive. In\nparticular, the communication and the synchronization cost may become a\nbottleneck. In this paper, we focus on situations where the model is\ndistributedly stored, and propose a novel distributed Newton method for\ntraining deep neural networks. By variable and feature-wise data partitions,\nand some careful designs, we are able to explicitly use the Jacobian matrix for\nmatrix-vector products in the Newton method. Some techniques are incorporated\nto reduce the running time as well as the memory consumption. First, to reduce\nthe communication cost, we propose a diagonalization method such that an\napproximate Newton direction can be obtained without communication between\nmachines. Second, we consider subsampled Gauss-Newton matrices for reducing the\nrunning time as well as the communication cost. Third, to reduce the\nsynchronization cost, we terminate the process of finding an approximate Newton\ndirection even though some nodes have not finished their tasks. Details of some\nimplementation issues in distributed environments are thoroughly investigated.\nExperiments demonstrate that the proposed method is effective for the\ndistributed training of deep neural networks. In compared with stochastic\ngradient methods, it is more robust and may give better test accuracy. \n\n"}
{"id": "1802.00844", "contents": "Title: Intriguing Properties of Randomly Weighted Networks: Generalizing While\n  Learning Next to Nothing Abstract: Training deep neural networks results in strong learned representations that\nshow good generalization capabilities. In most cases, training involves\niterative modification of all weights inside the network via back-propagation.\nIn Extreme Learning Machines, it has been suggested to set the first layer of a\nnetwork to fixed random values instead of learning it. In this paper, we\npropose to take this approach a step further and fix almost all layers of a\ndeep convolutional neural network, allowing only a small portion of the weights\nto be learned. As our experiments show, fixing even the majority of the\nparameters of the network often results in performance which is on par with the\nperformance of learning all of them. The implications of this intriguing\nproperty of deep neural networks are discussed and we suggest ways to harness\nit to create more robust representations. \n\n"}
{"id": "1802.01379", "contents": "Title: Online Compact Convexified Factorization Machine Abstract: Factorization Machine (FM) is a supervised learning approach with a powerful\ncapability of feature engineering. It yields state-of-the-art performance in\nvarious batch learning tasks where all the training data is made available\nprior to the training. However, in real-world applications where the data\narrives sequentially in a streaming manner, the high cost of re-training with\nbatch learning algorithms has posed formidable challenges in the online\nlearning scenario. The initial challenge is that no prior formulations of FM\ncould fulfill the requirements in Online Convex Optimization (OCO) -- the\nparamount framework for online learning algorithm design. To address the\naforementioned challenge, we invent a new convexification scheme leading to a\nCompact Convexified FM (CCFM) that seamlessly meets the requirements in OCO.\nHowever for learning Compact Convexified FM (CCFM) in the online learning\nsetting, most existing algorithms suffer from expensive projection operations.\nTo address this subsequent challenge, we follow the general projection-free\nalgorithmic framework of Online Conditional Gradient and propose an Online\nCompact Convex Factorization Machine (OCCFM) algorithm that eschews the\nprojection operation with efficient linear optimization steps. In support of\nthe proposed OCCFM in terms of its theoretical foundation, we prove that the\ndeveloped algorithm achieves a sub-linear regret bound. To evaluate the\nempirical performance of OCCFM, we conduct extensive experiments on 6\nreal-world datasets for online recommendation and binary classification tasks.\nThe experimental results show that OCCFM outperforms the state-of-art online\nlearning algorithms. \n\n"}
{"id": "1802.01421", "contents": "Title: First-order Adversarial Vulnerability of Neural Networks and Input\n  Dimension Abstract: Over the past few years, neural networks were proven vulnerable to\nadversarial images: targeted but imperceptible image perturbations lead to\ndrastically different predictions. We show that adversarial vulnerability\nincreases with the gradients of the training objective when viewed as a\nfunction of the inputs. Surprisingly, vulnerability does not depend on network\ntopology: for many standard network architectures, we prove that at\ninitialization, the $\\ell_1$-norm of these gradients grows as the square root\nof the input dimension, leaving the networks increasingly vulnerable with\ngrowing image size. We empirically show that this dimension dependence persists\nafter either usual or robust training, but gets attenuated with higher\nregularization. \n\n"}
{"id": "1802.01568", "contents": "Title: Selective Sampling and Mixture Models in Generative Adversarial Networks Abstract: In this paper, we propose a multi-generator extension to the adversarial\ntraining framework, in which the objective of each generator is to represent a\nunique component of a target mixture distribution. In the training phase, the\ngenerators cooperate to represent, as a mixture, the target distribution while\nmaintaining distinct manifolds. As opposed to traditional generative models,\ninference from a particular generator after training resembles selective\nsampling from a unique component in the target distribution. We demonstrate the\nfeasibility of the proposed architecture both analytically and with basic\nMulti-Layer Perceptron (MLP) models trained on the MNIST dataset. \n\n"}
{"id": "1802.01765", "contents": "Title: Training Generative Adversarial Networks via Primal-Dual Subgradient\n  Methods: A Lagrangian Perspective on GAN Abstract: We relate the minimax game of generative adversarial networks (GANs) to\nfinding the saddle points of the Lagrangian function for a convex optimization\nproblem, where the discriminator outputs and the distribution of generator\noutputs play the roles of primal variables and dual variables, respectively.\nThis formulation shows the connection between the standard GAN training process\nand the primal-dual subgradient methods for convex optimization. The inherent\nconnection does not only provide a theoretical convergence proof for training\nGANs in the function space, but also inspires a novel objective function for\ntraining. The modified objective function forces the distribution of generator\noutputs to be updated along the direction according to the primal-dual\nsubgradient methods. A toy example shows that the proposed method is able to\nresolve mode collapse, which in this case cannot be avoided by the standard GAN\nor Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and\nreal-world image datasets demonstrate the performance of the proposed method on\ngenerating diverse samples. \n\n"}
{"id": "1802.02500", "contents": "Title: Cadre Modeling: Simultaneously Discovering Subpopulations and Predictive\n  Models Abstract: We consider the problem in regression analysis of identifying subpopulations\nthat exhibit different patterns of response, where each subpopulation requires\na different underlying model. Unlike statistical cohorts, these subpopulations\nare not known a priori; thus, we refer to them as cadres. When the cadres and\ntheir associated models are interpretable, modeling leads to insights about the\nsubpopulations and their associations with the regression target. We introduce\na discriminative model that simultaneously learns cadre assignment and\ntarget-prediction rules. Sparsity-inducing priors are placed on the model\nparameters, under which independent feature selection is performed for both the\ncadre assignment and target-prediction processes. We learn models using\nadaptive step size stochastic gradient descent, and we assess cadre quality\nwith bootstrapped sample analysis. We present simulated results showing that,\nwhen the true clustering rule does not depend on the entire set of features,\nour method significantly outperforms methods that learn subpopulation-discovery\nand target-prediction rules separately. In a materials-by-design case study,\nour model provides state-of-the-art prediction of polymer glass transition\ntemperature. Importantly, the method identifies cadres of polymers that respond\ndifferently to structural perturbations, thus providing design insight for\ntargeting or avoiding specific transition temperature ranges. It identifies\nchemically meaningful cadres, each with interpretable models. Further\nexperimental results show that cadre methods have generalization that is\ncompetitive with linear and nonlinear regression models and can identify robust\nsubpopulations. \n\n"}
{"id": "1802.02548", "contents": "Title: Predicting Hurricane Trajectories using a Recurrent Neural Network Abstract: Hurricanes are cyclones circulating about a defined center whose closed wind\nspeeds exceed 75 mph originating over tropical and subtropical waters. At\nlandfall, hurricanes can result in severe disasters. The accuracy of predicting\ntheir trajectory paths is critical to reduce economic loss and save human\nlives. Given the complexity and nonlinearity of weather data, a recurrent\nneural network (RNN) could be beneficial in modeling hurricane behavior. We\npropose the application of a fully connected RNN to predict the trajectory of\nhurricanes. We employed the RNN over a fine grid to reduce typical truncation\nerrors. We utilized their latitude, longitude, wind speed, and pressure\npublicly provided by the National Hurricane Center (NHC) to predict the\ntrajectory of a hurricane at 6-hour intervals. Results show that this proposed\ntechnique is competitive to methods currently employed by the NHC and can\npredict up to approximately 120 hours of hurricane path. \n\n"}
{"id": "1802.03050", "contents": "Title: Thompson Sampling for Dynamic Pricing Abstract: In this paper we apply active learning algorithms for dynamic pricing in a\nprominent e-commerce website. Dynamic pricing involves changing the price of\nitems on a regular basis, and uses the feedback from the pricing decisions to\nupdate prices of the items. Most popular approaches to dynamic pricing use a\npassive learning approach, where the algorithm uses historical data to learn\nvarious parameters of the pricing problem, and uses the updated parameters to\ngenerate a new set of prices. We show that one can use active learning\nalgorithms such as Thompson sampling to more efficiently learn the underlying\nparameters in a pricing problem. We apply our algorithms to a real e-commerce\nsystem and show that the algorithms indeed improve revenue compared to pricing\nalgorithms that use passive learning. \n\n"}
{"id": "1802.03236", "contents": "Title: Learning Robust Options Abstract: Robust reinforcement learning aims to produce policies that have strong\nguarantees even in the face of environments/transition models whose parameters\nhave strong uncertainty. Existing work uses value-based methods and the usual\nprimitive action setting. In this paper, we propose robust methods for learning\ntemporally abstract actions, in the framework of options. We present a Robust\nOptions Policy Iteration (ROPI) algorithm with convergence guarantees, which\nlearns options that are robust to model uncertainty. We utilize ROPI to learn\nrobust options with the Robust Options Deep Q Network (RO-DQN) that solves\nmultiple tasks and mitigates model misspecification due to model uncertainty.\nWe present experimental results which suggest that policy iteration with linear\nfeatures may have an inherent form of robustness when using coarse feature\nrepresentations. In addition, we present experimental results which demonstrate\nthat robustness helps policy iteration implemented on top of deep neural\nnetworks to generalize over a much broader range of dynamics than non-robust\npolicy iteration. \n\n"}
{"id": "1802.03688", "contents": "Title: On the Rates of Convergence from Surrogate Risk Minimizers to the Bayes\n  Optimal Classifier Abstract: We study the rates of convergence from empirical surrogate risk minimizers to\nthe Bayes optimal classifier. Specifically, we introduce the notion of\n\\emph{consistency intensity} to characterize a surrogate loss function and\nexploit this notion to obtain the rate of convergence from an empirical\nsurrogate risk minimizer to the Bayes optimal classifier, enabling fair\ncomparisons of the excess risks of different surrogate risk minimizers. The\nmain result of the paper has practical implications including (1) showing that\nhinge loss is superior to logistic and exponential loss in the sense that its\nempirical minimizer converges faster to the Bayes optimal classifier and (2)\nguiding to modify surrogate loss functions to accelerate the convergence to the\nBayes optimal classifier. \n\n"}
{"id": "1802.03690", "contents": "Title: On the Generalization of Equivariance and Convolution in Neural Networks\n  to the Action of Compact Groups Abstract: Convolutional neural networks have been extremely successful in the image\nrecognition domain because they ensure equivariance to translations. There have\nbeen many recent attempts to generalize this framework to other domains,\nincluding graphs and data lying on manifolds. In this paper we give a rigorous,\ntheoretical treatment of convolution and equivariance in neural networks with\nrespect to not just translations, but the action of any compact group. Our main\nresult is to prove that (given some natural constraints) convolutional\nstructure is not just a sufficient, but also a necessary condition for\nequivariance to the action of a compact group. Our exposition makes use of\nconcepts from representation theory and noncommutative harmonic analysis and\nderives new generalized convolution formulae. \n\n"}
{"id": "1802.03830", "contents": "Title: Distributed Stochastic Multi-Task Learning with Graph Regularization Abstract: We propose methods for distributed graph-based multi-task learning that are\nbased on weighted averaging of messages from other machines. Uniform averaging\nor diminishing stepsize in these methods would yield consensus (single task)\nlearning. We show how simply skewing the averaging weights or controlling the\nstepsize allows learning different, but related, tasks on the different\nmachines. \n\n"}
{"id": "1802.04198", "contents": "Title: client2vec: Towards Systematic Baselines for Banking Applications Abstract: The workflow of data scientists normally involves potentially inefficient\nprocesses such as data mining, feature engineering and model selection. Recent\nresearch has focused on automating this workflow, partly or in its entirety, to\nimprove productivity. We choose the former approach and in this paper share our\nexperience in designing the client2vec: an internal library to rapidly build\nbaselines for banking applications. Client2vec uses marginalized stacked\ndenoising autoencoders on current account transactions data to create vector\nembeddings which represent the behaviors of our clients. These representations\ncan then be used in, and optimized against, a variety of tasks such as client\nsegmentation, profiling and targeting. Here we detail how we selected the\nalgorithmic machinery of client2vec and the data it works on and present\nexperimental results on several business cases. \n\n"}
{"id": "1802.04310", "contents": "Title: Stochastic quasi-Newton with adaptive step lengths for large-scale\n  problems Abstract: We provide a numerically robust and fast method capable of exploiting the\nlocal geometry when solving large-scale stochastic optimisation problems. Our\nkey innovation is an auxiliary variable construction coupled with an inverse\nHessian approximation computed using a receding history of iterates and\ngradients. It is the Markov chain nature of the classic stochastic gradient\nalgorithm that enables this development. The construction offers a mechanism\nfor stochastic line search adapting the step length. We numerically evaluate\nand compare against current state-of-the-art with encouraging performance on\nreal-world benchmark problems where the number of observations and unknowns is\nin the order of millions. \n\n"}
{"id": "1802.04504", "contents": "Title: Flipped-Adversarial AutoEncoders Abstract: We propose a flipped-Adversarial AutoEncoder (FAAE) that simultaneously\ntrains a generative model G that maps an arbitrary latent code distribution to\na data distribution and an encoder E that embodies an \"inverse mapping\" that\nencodes a data sample into a latent code vector. Unlike previous hybrid\napproaches that leverage adversarial training criterion in constructing\nautoencoders, FAAE minimizes re-encoding errors in the latent space and\nexploits adversarial criterion in the data space. Experimental evaluations\ndemonstrate that the proposed framework produces sharper reconstructed images\nwhile at the same time enabling inference that captures rich semantic\nrepresentation of data. \n\n"}
{"id": "1802.04537", "contents": "Title: Tighter Variational Bounds are Not Necessarily Better Abstract: We provide theoretical and empirical evidence that using tighter evidence\nlower bounds (ELBOs) can be detrimental to the process of learning an inference\nnetwork by reducing the signal-to-noise ratio of the gradient estimator. Our\nresults call into question common implicit assumptions that tighter ELBOs are\nbetter variational objectives for simultaneous model learning and inference\namortization schemes. Based on our insights, we introduce three new algorithms:\nthe partially importance weighted auto-encoder (PIWAE), the multiply importance\nweighted auto-encoder (MIWAE), and the combination importance weighted\nauto-encoder (CIWAE), each of which includes the standard importance weighted\nauto-encoder (IWAE) as a special case. We show that each can deliver\nimprovements over IWAE, even when performance is measured by the IWAE target\nitself. Furthermore, our results suggest that PIWAE may be able to deliver\nsimultaneous improvements in the training of both the inference and generative\nnetworks. \n\n"}
{"id": "1802.04907", "contents": "Title: Compressive Sensing Using Iterative Hard Thresholding with Low Precision\n  Data Representation: Theory and Applications Abstract: Modern scientific instruments produce vast amounts of data, which can\noverwhelm the processing ability of computer systems. Lossy compression of data\nis an intriguing solution, but comes with its own drawbacks, such as potential\nsignal loss, and the need for careful optimization of the compression ratio. In\nthis work, we focus on a setting where this problem is especially acute:\ncompressive sensing frameworks for interferometry and medical imaging. We ask\nthe following question: can the precision of the data representation be lowered\nfor all inputs, with recovery guarantees and practical performance? Our first\ncontribution is a theoretical analysis of the normalized Iterative Hard\nThresholding (IHT) algorithm when all input data, meaning both the measurement\nmatrix and the observation vector are quantized aggressively. We present a\nvariant of low precision normalized {IHT} that, under mild conditions, can\nstill provide recovery guarantees. The second contribution is the application\nof our quantization framework to radio astronomy and magnetic resonance\nimaging. We show that lowering the precision of the data can significantly\naccelerate image recovery. We evaluate our approach on telescope data and\nsamples of brain images using CPU and FPGA implementations achieving up to a 9x\nspeed-up with negligible loss of recovery quality. \n\n"}
{"id": "1802.04911", "contents": "Title: Large-Scale Sparse Inverse Covariance Estimation via Thresholding and\n  Max-Det Matrix Completion Abstract: The sparse inverse covariance estimation problem is commonly solved using an\n$\\ell_{1}$-regularized Gaussian maximum likelihood estimator known as\n\"graphical lasso\", but its computational cost becomes prohibitive for large\ndata sets. A recent line of results showed--under mild assumptions--that the\ngraphical lasso estimator can be retrieved by soft-thresholding the sample\ncovariance matrix and solving a maximum determinant matrix completion (MDMC)\nproblem. This paper proves an extension of this result, and describes a\nNewton-CG algorithm to efficiently solve the MDMC problem. Assuming that the\nthresholded sample covariance matrix is sparse with a sparse Cholesky\nfactorization, we prove that the algorithm converges to an $\\epsilon$-accurate\nsolution in $O(n\\log(1/\\epsilon))$ time and $O(n)$ memory. The algorithm is\nhighly efficient in practice: we solve the associated MDMC problems with as\nmany as 200,000 variables to 7-9 digits of accuracy in less than an hour on a\nstandard laptop computer running MATLAB. \n\n"}
{"id": "1802.05187", "contents": "Title: On the Blindspots of Convolutional Networks Abstract: Deep convolutional network has been the state-of-the-art approach for a wide\nvariety of tasks over the last few years. Its successes have, in many cases,\nturned it into the default model in quite a few domains. In this work, we will\ndemonstrate that convolutional networks have limitations that may, in some\ncases, hinder it from learning properties of the data, which are easily\nrecognizable by traditional, less demanding, models. To this end, we present a\nseries of competitive analysis studies on image recognition and text analysis\ntasks, for which convolutional networks are known to provide state-of-the-art\nresults. In our studies, we inject a truth-revealing signal, indiscernible for\nthe network, thus hitting time and again the network's blind spots. The signal\ndoes not impair the network's existing performances, but it does provide an\nopportunity for a significant performance boost by models that can capture it.\nThe various forms of the carefully designed signals shed a light on the\nstrengths and weaknesses of convolutional network, which may provide insights\nfor both theoreticians that study the power of deep architectures, and for\npractitioners that consider applying convolutional networks to the task at\nhand. \n\n"}
{"id": "1802.05355", "contents": "Title: The Role of Information Complexity and Randomization in Representation\n  Learning Abstract: A grand challenge in representation learning is to learn the different\nexplanatory factors of variation behind the high dimen- sional data. Encoder\nmodels are often determined to optimize performance on training data when the\nreal objective is to generalize well to unseen data. Although there is enough\nnumerical evidence suggesting that noise injection (during training) at the\nrepresentation level might improve the generalization ability of encoders, an\ninformation-theoretic understanding of this principle remains elusive. This\npaper presents a sample-dependent bound on the generalization gap of the\ncross-entropy loss that scales with the information complexity (IC) of the\nrepresentations, meaning the mutual information between inputs and their\nrepresentations. The IC is empirically investigated for standard multi-layer\nneural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the\ngap and the IC appear to be in direct correlation, suggesting that SGD selects\nencoders to implicitly minimize the IC. We specialize the IC to study the role\nof Dropout on the generalization capacity of deep encoders which is shown to be\ndirectly related to the encoder capacity, being a measure of the\ndistinguishability among samples from their representations. Our results\nsupport some recent regularization methods. \n\n"}
{"id": "1802.05821", "contents": "Title: Learning Latent Features with Pairwise Penalties in Low-Rank Matrix\n  Completion Abstract: Low-rank matrix completion has achieved great success in many real-world data\napplications. A matrix factorization model that learns latent features is\nusually employed and, to improve prediction performance, the similarities\nbetween latent variables can be exploited by pairwise learning using the graph\nregularized matrix factorization (GRMF) method. However, existing GRMF\napproaches often use the squared loss to measure the pairwise differences,\nwhich may be overly influenced by dissimilar pairs and lead to inferior\nprediction. To fully empower pairwise learning for matrix completion, we\npropose a general optimization framework that allows a rich class of\n(non-)convex pairwise penalty functions. A new and efficient algorithm is\ndeveloped to solve the proposed optimization problem, with a theoretical\nconvergence guarantee under mild assumptions. In an important situation where\nthe latent variables form a small number of subgroups, its statistical\nguarantee is also fully considered. In particular, we theoretically\ncharacterize the performance of the complexity-regularized maximum likelihood\nestimator, as a special case of our framework, which is shown to have smaller\nerrors when compared to the standard matrix completion framework without\npairwise penalties. We conduct extensive experiments on both synthetic and real\ndatasets to demonstrate the superior performance of this general framework. \n\n"}
{"id": "1802.05983", "contents": "Title: Disentangling by Factorising Abstract: We define and address the problem of unsupervised learning of disentangled\nrepresentations on data generated from independent factors of variation. We\npropose FactorVAE, a method that disentangles by encouraging the distribution\nof representations to be factorial and hence independent across the dimensions.\nWe show that it improves upon $\\beta$-VAE by providing a better trade-off\nbetween disentanglement and reconstruction quality. Moreover, we highlight the\nproblems of a commonly used disentanglement metric and introduce a new metric\nthat does not suffer from them. \n\n"}
{"id": "1802.06181", "contents": "Title: Semi-supervised multi-task learning for lung cancer diagnosis Abstract: Early detection of lung nodules is of great importance in lung cancer\nscreening. Existing research recognizes the critical role played by CAD systems\nin early detection and diagnosis of lung nodules. However, many CAD systems,\nwhich are used as cancer detection tools, produce a lot of false positives (FP)\nand require a further FP reduction step. Furthermore, guidelines for early\ndiagnosis and treatment of lung cancer are consist of different shape and\nvolume measurements of abnormalities. Segmentation is at the heart of our\nunderstanding of nodules morphology making it a major area of interest within\nthe field of computer aided diagnosis systems. This study set out to test the\nhypothesis that joint learning of false positive (FP) nodule reduction and\nnodule segmentation can improve the computer aided diagnosis (CAD) systems'\nperformance on both tasks. To support this hypothesis we propose a 3D deep\nmulti-task CNN to tackle these two problems jointly. We tested our system on\nLUNA16 dataset and achieved an average dice similarity coefficient (DSC) of 91%\nas segmentation accuracy and a score of nearly 92% for FP reduction. As a proof\nof our hypothesis, we showed improvements of segmentation and FP reduction\ntasks over two baselines. Our results support that joint training of these two\ntasks through a multi-task learning approach improves system performance on\nboth. We also showed that a semi-supervised approach can be used to overcome\nthe limitation of lack of labeled data for the 3D segmentation task. \n\n"}
{"id": "1802.06300", "contents": "Title: Exact and Robust Conformal Inference Methods for Predictive Machine\n  Learning With Dependent Data Abstract: We extend conformal inference to general settings that allow for time series\ndata. Our proposal is developed as a randomization method and accounts for\npotential serial dependence by including block structures in the permutation\nscheme. As a result, the proposed method retains the exact, model-free validity\nwhen the data are i.i.d. or more generally exchangeable, similar to usual\nconformal inference methods. When exchangeability fails, as is the case for\ncommon time series data, the proposed approach is approximately valid under\nweak assumptions on the conformity score. \n\n"}
{"id": "1802.06857", "contents": "Title: Global Pose Estimation with an Attention-based Recurrent Network Abstract: The ability for an agent to localize itself within an environment is crucial\nfor many real-world applications. For unknown environments, Simultaneous\nLocalization and Mapping (SLAM) enables incremental and concurrent building of\nand localizing within a map. We present a new, differentiable architecture,\nNeural Graph Optimizer, progressing towards a complete neural network solution\nfor SLAM by designing a system composed of a local pose estimation model, a\nnovel pose selection module, and a novel graph optimization process. The entire\narchitecture is trained in an end-to-end fashion, enabling the network to\nautomatically learn domain-specific features relevant to the visual odometry\nand avoid the involved process of feature engineering. We demonstrate the\neffectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom\nenvironment. \n\n"}
{"id": "1802.08534", "contents": "Title: Weighted Double Deep Multiagent Reinforcement Learning in Stochastic\n  Cooperative Environments Abstract: Recently, multiagent deep reinforcement learning (DRL) has received\nincreasingly wide attention. Existing multiagent DRL algorithms are inefficient\nwhen facing with the non-stationarity due to agents update their policies\nsimultaneously in stochastic cooperative environments. This paper extends the\nrecently proposed weighted double estimator to the multiagent domain and\npropose a multiagent DRL framework, named weighted double deep Q-network\n(WDDQN). By utilizing the weighted double estimator and the deep neural\nnetwork, WDDQN can not only reduce the bias effectively but also be extended to\nscenarios with raw visual inputs. To achieve efficient cooperation in the\nmultiagent domain, we introduce the lenient reward network and the scheduled\nreplay strategy. Experiments show that the WDDQN outperforms the existing DRL\nand multiaent DRL algorithms, i.e., double DQN and lenient Q-learning, in terms\nof the average reward and the convergence rate in stochastic cooperative\nenvironments. \n\n"}
{"id": "1802.08674", "contents": "Title: An Algorithmic Framework to Control Bias in Bandit-based Personalization Abstract: Personalization is pervasive in the online space as it leads to higher\nefficiency and revenue by allowing the most relevant content to be served to\neach user. However, recent studies suggest that personalization methods can\npropagate societal or systemic biases and polarize opinions; this has led to\ncalls for regulatory mechanisms and algorithms to combat bias and inequality.\nAlgorithmically, bandit optimization has enjoyed great success in learning user\npreferences and personalizing content or feeds accordingly. We propose an\nalgorithmic framework that allows for the possibility to control bias or\ndiscrimination in such bandit-based personalization. Our model allows for the\nspecification of general fairness constraints on the sensitive types of the\ncontent that can be displayed to a user. The challenge, however, is to come up\nwith a scalable and low regret algorithm for the constrained optimization\nproblem that arises. Our main technical contribution is a provably fast and\nlow-regret algorithm for the fairness-constrained bandit optimization problem.\nOur proofs crucially leverage the special structure of our problem. Experiments\non synthetic and real-world data sets show that our algorithmic framework can\ncontrol bias with only a minor loss to revenue. \n\n"}
{"id": "1802.08762", "contents": "Title: Diffusion Maps meet Nystr\\\"om Abstract: Diffusion maps are an emerging data-driven technique for non-linear\ndimensionality reduction, which are especially useful for the analysis of\ncoherent structures and nonlinear embeddings of dynamical systems. However, the\ncomputational complexity of the diffusion maps algorithm scales with the number\nof observations. Thus, long time-series data presents a significant challenge\nfor fast and efficient embedding. We propose integrating the Nystr\\\"om method\nwith diffusion maps in order to ease the computational demand. We achieve a\nspeedup of roughly two to four times when approximating the dominant diffusion\nmap components. \n\n"}
{"id": "1802.08768", "contents": "Title: Is Generator Conditioning Causally Related to GAN Performance? Abstract: Recent work (Pennington et al, 2017) suggests that controlling the entire\ndistribution of Jacobian singular values is an important design consideration\nin deep learning. Motivated by this, we study the distribution of singular\nvalues of the Jacobian of the generator in Generative Adversarial Networks\n(GANs). We find that this Jacobian generally becomes ill-conditioned at the\nbeginning of training. Moreover, we find that the average (with z from p(z))\nconditioning of the generator is highly predictive of two other ad-hoc metrics\nfor measuring the 'quality' of trained GANs: the Inception Score and the\nFrechet Inception Distance (FID). We test the hypothesis that this relationship\nis causal by proposing a 'regularization' technique (called Jacobian Clamping)\nthat softly penalizes the condition number of the generator Jacobian. Jacobian\nClamping improves the mean Inception Score and the mean FID for GANs trained on\nseveral datasets. It also greatly reduces inter-run variance of the\naforementioned scores, addressing (at least partially) one of the main\ncriticisms of GANs. \n\n"}
{"id": "1802.09484", "contents": "Title: Disentangling the independently controllable factors of variation by\n  interacting with the world Abstract: It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors, and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal. \n\n"}
{"id": "1802.09841", "contents": "Title: Adversarial Active Learning for Deep Networks: a Margin Based Approach Abstract: We propose a new active learning strategy designed for deep neural networks.\nThe goal is to minimize the number of data annotation queried from an oracle\nduring training. Previous active learning strategies scalable for deep networks\nwere mostly based on uncertain sample selection. In this work, we focus on\nexamples lying close to the decision boundary. Based on theoretical works on\nmargin theory for active learning, we know that such examples may help to\nconsiderably decrease the number of annotations. While measuring the exact\ndistance to the decision boundaries is intractable, we propose to rely on\nadversarial examples. We do not consider anymore them as a threat instead we\nexploit the information they provide on the distribution of the input space in\norder to approximate the distance to decision boundaries. We demonstrate\nempirically that adversarial active queries yield faster convergence of CNNs\ntrained on MNIST, the Shoe-Bag and the Quick-Draw datasets. \n\n"}
{"id": "1802.09933", "contents": "Title: Guaranteed Sufficient Decrease for Stochastic Variance Reduced Gradient\n  Optimization Abstract: In this paper, we propose a novel sufficient decrease technique for\nstochastic variance reduced gradient descent methods such as SVRG and SAGA. In\norder to make sufficient decrease for stochastic optimization, we design a new\nsufficient decrease criterion, which yields sufficient decrease versions of\nstochastic variance reduction algorithms such as SVRG-SD and SAGA-SD as a\nbyproduct. We introduce a coefficient to scale current iterate and to satisfy\nthe sufficient decrease property, which takes the decisions to shrink, expand\nor even move in the opposite direction, and then give two specific update rules\nof the coefficient for Lasso and ridge regression. Moreover, we analyze the\nconvergence properties of our algorithms for strongly convex problems, which\nshow that our algorithms attain linear convergence rates. We also provide the\nconvergence guarantees of our algorithms for non-strongly convex problems. Our\nexperimental results further verify that our algorithms achieve significantly\nbetter performance than their counterparts. \n\n"}
{"id": "1802.10592", "contents": "Title: Model-Ensemble Trust-Region Policy Optimization Abstract: Model-free reinforcement learning (RL) methods are succeeding in a growing\nnumber of tasks, aided by recent advances in deep learning. However, they tend\nto suffer from high sample complexity, which hinders their use in real-world\ndomains. Alternatively, model-based reinforcement learning promises to reduce\nsample complexity, but tends to require careful tuning and to date have\nsucceeded mainly in restrictive domains where simple models are sufficient for\nlearning. In this paper, we analyze the behavior of vanilla model-based\nreinforcement learning methods when deep neural networks are used to learn both\nthe model and the policy, and show that the learned policy tends to exploit\nregions where insufficient data is available for the model to be learned,\ncausing instability in training. To overcome this issue, we propose to use an\nensemble of models to maintain the model uncertainty and regularize the\nlearning process. We further show that the use of likelihood ratio derivatives\nyields much more stable learning than backpropagation through time. Altogether,\nour approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO)\nsignificantly reduces the sample complexity compared to model-free deep RL\nmethods on challenging continuous control benchmark tasks. \n\n"}
{"id": "1803.00116", "contents": "Title: Separators and Adjustment Sets in Causal Graphs: Complete Criteria and\n  an Algorithmic Framework Abstract: Principled reasoning about the identifiability of causal effects from\nnon-experimental data is an important application of graphical causal models.\nThis paper focuses on effects that are identifiable by covariate adjustment, a\ncommonly used estimation approach. We present an algorithmic framework for\nefficiently testing, constructing, and enumerating $m$-separators in ancestral\ngraphs (AGs), a class of graphical causal models that can represent uncertainty\nabout the presence of latent confounders. Furthermore, we prove a reduction\nfrom causal effect identification by covariate adjustment to $m$-separation in\na subgraph for directed acyclic graphs (DAGs) and maximal ancestral graphs\n(MAGs). Jointly, these results yield constructive criteria that characterize\nall adjustment sets as well as all minimal and minimum adjustment sets for\nidentification of a desired causal effect with multivariate exposures and\noutcomes in the presence of latent confounding. Our results extend several\nexisting solutions for special cases of these problems. Our efficient\nalgorithms allowed us to empirically quantify the identifiability gap between\ncovariate adjustment and the do-calculus in random DAGs and MAGs, covering a\nwide range of scenarios. Implementations of our algorithms are provided in the\nR package dagitty. \n\n"}
{"id": "1803.00491", "contents": "Title: The Power Mean Laplacian for Multilayer Graph Clustering Abstract: Multilayer graphs encode different kind of interactions between the same set\nof entities. When one wants to cluster such a multilayer graph, the natural\nquestion arises how one should merge the information different layers. We\nintroduce in this paper a one-parameter family of matrix power means for\nmerging the Laplacians from different layers and analyze it in expectation in\nthe stochastic block model. We show that this family allows to recover ground\ntruth clusters under different settings and verify this in real world data.\nWhile computing the matrix power mean can be very expensive for large graphs,\nwe introduce a numerical scheme to efficiently compute its eigenvectors for the\ncase of large sparse graphs. \n\n"}
{"id": "1803.00781", "contents": "Title: Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal\n  Exploration Abstract: Intrinsically motivated goal exploration algorithms enable machines to\ndiscover repertoires of policies that produce a diversity of effects in complex\nenvironments. These exploration algorithms have been shown to allow real world\nrobots to acquire skills such as tool use in high-dimensional continuous state\nand action spaces. However, they have so far assumed that self-generated goals\nare sampled in a specifically engineered feature space, limiting their\nautonomy. In this work, we propose to use deep representation learning\nalgorithms to learn an adequate goal space. This is a developmental 2-stage\napproach: first, in a perceptual learning stage, deep learning algorithms use\npassive raw sensor observations of world changes to learn a corresponding\nlatent space; then goal exploration happens in a second stage by sampling goals\nin this latent space. We present experiments where a simulated robot arm\ninteracts with an object, and we show that exploration algorithms using such\nlearned representations can match the performance obtained using engineered\nrepresentations. \n\n"}
{"id": "1803.00841", "contents": "Title: Gradient-based Sampling: An Adaptive Importance Sampling for\n  Least-squares Abstract: In modern data analysis, random sampling is an efficient and widely-used\nstrategy to overcome the computational difficulties brought by large sample\nsize. In previous studies, researchers conducted random sampling which is\naccording to the input data but independent on the response variable, however\nthe response variable may also be informative for sampling. In this paper we\npropose an adaptive sampling called the gradient-based sampling which is\ndependent on both the input data and the output for fast solving of\nleast-square (LS) problems. We draw the data points by random sampling from the\nfull data according to their gradient values. This sampling is computationally\nsaving, since the running time of computing the sampling probabilities is\nreduced to O(nd) where n is the full sample size and d is the dimension of the\ninput. Theoretically, we establish an error bound analysis of the general\nimportance sampling with respect to LS solution from full data. The result\nestablishes an improved performance of the use of our gradient- based sampling.\nSynthetic and real data sets are used to empirically argue that the\ngradient-based sampling has an obvious advantage over existing sampling methods\nfrom two aspects of statistical efficiency and computational saving. \n\n"}
{"id": "1803.01113", "contents": "Title: Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in\n  Distributed SGD Abstract: Distributed Stochastic Gradient Descent (SGD) when run in a synchronous\nmanner, suffers from delays in waiting for the slowest learners (stragglers).\nAsynchronous methods can alleviate stragglers, but cause gradient staleness\nthat can adversely affect convergence. In this work we present a novel\ntheoretical characterization of the speed-up offered by asynchronous methods by\nanalyzing the trade-off between the error in the trained model and the actual\ntraining runtime (wallclock time). The novelty in our work is that our runtime\nanalysis considers random straggler delays, which helps us design and compare\ndistributed SGD algorithms that strike a balance between stragglers and\nstaleness. We also present a new convergence analysis of asynchronous SGD\nvariants without bounded or exponential delay assumptions, and a novel learning\nrate schedule to compensate for gradient staleness. \n\n"}
{"id": "1803.01233", "contents": "Title: Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase\n  Procrustes Flow Abstract: We revisit the inductive matrix completion problem that aims to recover a\nrank-$r$ matrix with ambient dimension $d$ given $n$ features as the side prior\ninformation. The goal is to make use of the known $n$ features to reduce sample\nand computational complexities. We present and analyze a new gradient-based\nnon-convex optimization algorithm that converges to the true underlying matrix\nat a linear rate with sample complexity only linearly depending on $n$ and\nlogarithmically depending on $d$. To the best of our knowledge, all previous\nalgorithms either have a quadratic dependency on the number of features in\nsample complexity or a sub-linear computational convergence rate. In addition,\nwe provide experiments on both synthetic and real world data to demonstrate the\neffectiveness of our proposed algorithm. \n\n"}
{"id": "1803.01465", "contents": "Title: Query and Output: Generating Words by Querying Distributed Word\n  Representations for Paraphrase Generation Abstract: Most recent approaches use the sequence-to-sequence model for paraphrase\ngeneration. The existing sequence-to-sequence model tends to memorize the words\nand the patterns in the training dataset instead of learning the meaning of the\nwords. Therefore, the generated sentences are often grammatically correct but\nsemantically improper. In this work, we introduce a novel model based on the\nencoder-decoder framework, called Word Embedding Attention Network (WEAN). Our\nproposed model generates the words by querying distributed word representations\n(i.e. neural word embeddings), hoping to capturing the meaning of the according\nwords. Following previous work, we evaluate our model on two\nparaphrase-oriented tasks, namely text simplification and short text\nabstractive summarization. Experimental results show that our model outperforms\nthe sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two\nEnglish text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a\nChinese summarization dataset. Moreover, our model achieves state-of-the-art\nperformances on these three benchmark datasets. \n\n"}
{"id": "1803.01500", "contents": "Title: Memorization Precedes Generation: Learning Unsupervised GANs with Memory\n  Networks Abstract: We propose an approach to address two issues that commonly occur during\ntraining of unsupervised GANs. First, since GANs use only a continuous latent\ndistribution to embed multiple classes or clusters of data, they often do not\ncorrectly handle the structural discontinuity between disparate classes in a\nlatent space. Second, discriminators of GANs easily forget about past generated\nsamples by generators, incurring instability during adversarial training. We\nargue that these two infamous problems of unsupervised GAN training can be\nlargely alleviated by a learnable memory network to which both generators and\ndiscriminators can access. Generators can effectively learn representation of\ntraining samples to understand underlying cluster distributions of data, which\nease the structure discontinuity problem. At the same time, discriminators can\nbetter memorize clusters of previously generated samples, which mitigate the\nforgetting problem. We propose a novel end-to-end GAN model named memoryGAN,\nwhich involves a memory network that is unsupervisedly trainable and integrable\nto many existing GAN models. With evaluations on multiple datasets such as\nFashion-MNIST, CelebA, CIFAR10, and Chairs, we show that our model is\nprobabilistically interpretable, and generates realistic image samples of high\nvisual fidelity. The memoryGAN also achieves the state-of-the-art inception\nscores over unsupervised GAN models on the CIFAR10 dataset, without any\noptimization tricks and weaker divergences. \n\n"}
{"id": "1803.01682", "contents": "Title: Beyond Greedy Ranking: Slate Optimization via List-CVAE Abstract: The conventional solution to the recommendation problem greedily ranks\nindividual document candidates by prediction scores. However, this method fails\nto optimize the slate as a whole, and hence, often struggles to capture biases\ncaused by the page layout and document interdepedencies. The slate\nrecommendation problem aims to directly find the optimally ordered subset of\ndocuments (i.e. slates) that best serve users' interests. Solving this problem\nis hard due to the combinatorial explosion in all combinations of document\ncandidates and their display positions on the page. Therefore we propose a\nparadigm shift from the traditional viewpoint of solving a ranking problem to a\ndirect slate generation framework. In this paper, we introduce List Conditional\nVariational Auto-Encoders (List-CVAE), which learns the joint distribution of\ndocuments on the slate conditioned on user responses, and directly generates\nfull slates. Experiments on simulated and real-world data show that List-CVAE\noutperforms popular comparable ranking methods consistently on various scales\nof documents corpora. \n\n"}
{"id": "1803.01719", "contents": "Title: How to Start Training: The Effect of Initialization and Architecture Abstract: We identify and study two common failure modes for early training in deep\nReLU nets. For each we give a rigorous proof of when it occurs and how to avoid\nit, for fully connected and residual architectures. The first failure mode,\nexploding/vanishing mean activation length, can be avoided by initializing\nweights from a symmetric distribution with variance 2/fan-in and, for ResNets,\nby correctly weighting the residual modules. We prove that the second failure\nmode, exponentially large variance of activation length, never occurs in\nresidual nets once the first failure mode is avoided. In contrast, for fully\nconnected nets, we prove that this failure mode can happen and is avoided by\nkeeping constant the sum of the reciprocals of layer widths. We demonstrate\nempirically the effectiveness of our theoretical results in predicting when\nnetworks are able to start training. In particular, we note that many popular\ninitializations fail our criteria, whereas correct initialization and\narchitecture allows much deeper networks to be trained. \n\n"}
{"id": "1803.01814", "contents": "Title: Norm matters: efficient and accurate normalization schemes in deep\n  networks Abstract: Over the past few years, Batch-Normalization has been commonly used in deep\nnetworks, allowing faster training and high performance for a wide variety of\napplications. However, the reasons behind its merits remained unanswered, with\nseveral shortcomings that hindered its use for certain tasks. In this work, we\npresent a novel view on the purpose and function of normalization methods and\nweight-decay, as tools to decouple weights' norm from the underlying optimized\nobjective. This property highlights the connection between practices such as\nnormalization, weight decay and learning-rate adjustments. We suggest several\nalternatives to the widely used $L^2$ batch-norm, using normalization in $L^1$\nand $L^\\infty$ spaces that can substantially improve numerical stability in\nlow-precision implementations as well as provide computational and memory\nbenefits. We demonstrate that such methods enable the first batch-norm\nalternative to work for half-precision implementations. Finally, we suggest a\nmodification to weight-normalization, which improves its performance on\nlarge-scale tasks. \n\n"}
{"id": "1803.02782", "contents": "Title: A bag-to-class divergence approach to multiple-instance learning Abstract: In multi-instance (MI) learning, each object (bag) consists of multiple\nfeature vectors (instances), and is most commonly regarded as a set of points\nin a multidimensional space. A different viewpoint is that the instances are\nrealisations of random vectors with corresponding probability distribution, and\nthat a bag is the distribution, not the realisations. In MI classification,\neach bag in the training set has a class label, but the instances are\nunlabelled. By introducing the probability distribution space to bag-level\nclassification problems, dissimilarities between probability distributions\n(divergences) can be applied. The bag-to-bag Kullback-Leibler information is\nasymptotically the best classifier, but the typical sparseness of MI training\nsets is an obstacle. We introduce bag-to-class divergence to MI learning,\nemphasising the hierarchical nature of the random vectors that makes bags from\nthe same class different. We propose two properties for bag-to-class\ndivergences, and an additional property for sparse training sets. \n\n"}
{"id": "1803.02879", "contents": "Title: Deep Models of Interactions Across Sets Abstract: We use deep learning to model interactions across two or more sets of\nobjects, such as user-movie ratings, protein-drug bindings, or ternary\nuser-item-tag interactions. The canonical representation of such interactions\nis a matrix (or a higher-dimensional tensor) with an exchangeability property:\nthe encoding's meaning is not changed by permuting rows or columns. We argue\nthat models should hence be Permutation Equivariant (PE): constrained to make\nthe same predictions across such permutations. We present a parameter-sharing\nscheme and prove that it could not be made any more expressive without\nviolating PE. This scheme yields three benefits. First, we demonstrate\nstate-of-the-art performance on multiple matrix completion benchmarks. Second,\nour models require a number of parameters independent of the numbers of\nobjects, and thus scale well to large datasets. Third, models can be queried\nabout new objects that were not available at training time, but for which\ninteractions have since been observed. In experiments, our models achieved\nsurprisingly good generalization performance on this matrix extrapolation task,\nboth within domains (e.g., new users and new movies drawn from the same\ndistribution used for training) and even across domains (e.g., predicting music\nratings after training on movies). \n\n"}
{"id": "1803.03234", "contents": "Title: Improving Optimization for Models With Continuous Symmetry Breaking Abstract: Many loss functions in representation learning are invariant under a\ncontinuous symmetry transformation. For example, the loss function of word\nembeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate\nall word and context embedding vectors. We show that representation learning\nmodels for time series possess an approximate continuous symmetry that leads to\nslow convergence of gradient descent. We propose a new optimization algorithm\nthat speeds up convergence using ideas from gauge theory in physics. Our\nalgorithm leads to orders of magnitude faster convergence and to more\ninterpretable representations, as we show for dynamic extensions of matrix\nfactorization and word embedding models. We further present an example\napplication of our proposed algorithm that translates modern words into their\nhistoric equivalents. \n\n"}
{"id": "1803.03880", "contents": "Title: Combating Adversarial Attacks Using Sparse Representations Abstract: It is by now well-known that small adversarial perturbations can induce\nclassification errors in deep neural networks (DNNs). In this paper, we make\nthe case that sparse representations of the input data are a crucial tool for\ncombating such attacks. For linear classifiers, we show that a sparsifying\nfront end is provably effective against $\\ell_{\\infty}$-bounded attacks,\nreducing output distortion due to the attack by a factor of roughly $K / N$\nwhere $N$ is the data dimension and $K$ is the sparsity level. We then extend\nthis concept to DNNs, showing that a \"locally linear\" model can be used to\ndevelop a theoretical foundation for crafting attacks and defenses.\nExperimental results for the MNIST dataset show the efficacy of the proposed\nsparsifying front end. \n\n"}
{"id": "1803.04223", "contents": "Title: Leveraging Crowdsourcing Data For Deep Active Learning - An Application:\n  Learning Intents in Alexa Abstract: This paper presents a generic Bayesian framework that enables any deep\nlearning model to actively learn from targeted crowds. Our framework inherits\nfrom recent advances in Bayesian deep learning, and extends existing work by\nconsidering the targeted crowdsourcing approach, where multiple annotators with\nunknown expertise contribute an uncontrolled amount (often limited) of\nannotations. Our framework leverages the low-rank structure in annotations to\nlearn individual annotator expertise, which then helps to infer the true labels\nfrom noisy and sparse annotations. It provides a unified Bayesian model to\nsimultaneously infer the true labels and train the deep learning model in order\nto reach an optimal learning efficacy. Finally, our framework exploits the\nuncertainty of the deep learning model during prediction as well as the\nannotators' estimated expertise to minimize the number of required annotations\nand annotators for optimally training the deep learning model.\n  We evaluate the effectiveness of our framework for intent classification in\nAlexa (Amazon's personal assistant), using both synthetic and real-world\ndatasets. Experiments show that our framework can accurately learn annotator\nexpertise, infer true labels, and effectively reduce the amount of annotations\nin model training as compared to state-of-the-art approaches. We further\ndiscuss the potential of our proposed framework in bridging machine learning\nand crowdsourcing towards improved human-in-the-loop systems. \n\n"}
{"id": "1803.04307", "contents": "Title: The Everlasting Database: Statistical Validity at a Fair Price Abstract: The problem of handling adaptivity in data analysis, intentional or not,\npermeates a variety of fields, including test-set overfitting in ML challenges\nand the accumulation of invalid scientific discoveries. We propose a mechanism\nfor answering an arbitrarily long sequence of potentially adaptive statistical\nqueries, by charging a price for each query and using the proceeds to collect\nadditional samples. Crucially, we guarantee statistical validity without any\nassumptions on how the queries are generated. We also ensure with high\nprobability that the cost for $M$ non-adaptive queries is $O(\\log M)$, while\nthe cost to a potentially adaptive user who makes $M$ queries that do not\ndepend on any others is $O(\\sqrt{M})$. \n\n"}
{"id": "1803.04371", "contents": "Title: Optimal Rates of Sketched-regularized Algorithms for Least-Squares\n  Regression over Hilbert Spaces Abstract: We investigate regularized algorithms combining with projection for\nleast-squares regression problem over a Hilbert space, covering nonparametric\nregression over a reproducing kernel Hilbert space. We prove convergence\nresults with respect to variants of norms, under a capacity assumption on the\nhypothesis space and a regularity condition on the target function. As a\nresult, we obtain optimal rates for regularized algorithms with randomized\nsketches, provided that the sketch dimension is proportional to the effective\ndimension up to a logarithmic factor. As a byproduct, we obtain similar results\nfor Nystr\\\"{o}m regularized algorithms. Our results are the first ones with\noptimal, distribution-dependent rates that do not have any saturation effect\nfor sketched/Nystr\\\"{o}m regularized algorithms, considering both the\nattainable and non-attainable cases. \n\n"}
{"id": "1803.05419", "contents": "Title: Generalised Structural CNNs (SCNNs) for time series data with arbitrary\n  graph topology Abstract: Deep Learning methods, specifically convolutional neural networks (CNNs),\nhave seen a lot of success in the domain of image-based data, where the data\noffers a clearly structured topology in the regular lattice of pixels. This\n4-neighbourhood topological simplicity makes the application of convolutional\nmasks straightforward for time series data, such as video applications, but\nmany high-dimensional time series data are not organised in regular lattices,\nand instead values may have adjacency relationships with non-trivial\ntopologies, such as small-world networks or trees. In our application case,\nhuman kinematics, it is currently unclear how to generalise convolutional\nkernels in a principled manner. Therefore we define and implement here a\nframework for general graph-structured CNNs for time series analysis. Our\nalgorithm automatically builds convolutional layers using the specified\nadjacency matrix of the data dimensions and convolutional masks that scale with\nthe hop distance. In the limit of a lattice-topology our method produces the\nwell-known image convolutional masks. We test our method first on synthetic\ndata of arbitrarily-connected graphs and human hand motion capture data, where\nthe hand is represented by a tree capturing the mechanical dependencies of the\njoints. We are able to demonstrate, amongst other things, that inclusion of the\ngraph structure of the data dimensions improves model prediction significantly,\nwhen compared against a benchmark CNN model with only time convolution layers. \n\n"}
{"id": "1803.05591", "contents": "Title: On the insufficiency of existing momentum schemes for Stochastic\n  Optimization Abstract: Momentum based stochastic gradient methods such as heavy ball (HB) and\nNesterov's accelerated gradient descent (NAG) method are widely used in\npractice for training deep networks and other supervised learning models, as\nthey often provide significant improvements over stochastic gradient descent\n(SGD). Rigorously speaking, \"fast gradient\" methods have provable improvements\nover gradient descent only for the deterministic case, where the gradients are\nexact. In the stochastic case, the popular explanations for their wide\napplicability is that when these fast gradient methods are applied in the\nstochastic case, they partially mimic their exact gradient counterparts,\nresulting in some practical gain. This work provides a counterpoint to this\nbelief by proving that there exist simple problem instances where these methods\ncannot outperform SGD despite the best setting of its parameters. These\nnegative problem instances are, in an informal sense, generic; they do not look\nlike carefully constructed pathological instances. These results suggest (along\nwith empirical evidence) that HB or NAG's practical performance gains are a\nby-product of mini-batching.\n  Furthermore, this work provides a viable (and provable) alternative, which,\non the same set of problem instances, significantly improves over HB, NAG, and\nSGD's performance. This algorithm, referred to as Accelerated Stochastic\nGradient Descent (ASGD), is a simple to implement stochastic algorithm, based\non a relatively less popular variant of Nesterov's Acceleration. Extensive\nempirical results in this paper show that ASGD has performance gains over HB,\nNAG, and SGD. \n\n"}
{"id": "1803.05598", "contents": "Title: Large Margin Deep Networks for Classification Abstract: We present a formulation of deep learning that aims at producing a large\nmargin classifier. The notion of margin, minimum distance to a decision\nboundary, has served as the foundation of several theoretically profound and\nempirically successful results for both classification and regression tasks.\nHowever, most large margin algorithms are applicable only to shallow models\nwith a preset feature representation; and conventional margin methods for\nneural networks only enforce margin at the output layer. Such methods are\ntherefore not well suited for deep networks.\n  In this work, we propose a novel loss function to impose a margin on any\nchosen set of layers of a deep network (including input and hidden layers). Our\nformulation allows choosing any norm on the metric measuring the margin. We\ndemonstrate that the decision boundary obtained by our loss has nice properties\ncompared to standard classification loss functions. Specifically, we show\nimproved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on\nmultiple tasks: generalization from small training sets, corrupted labels, and\nrobustness against adversarial perturbations. The resulting loss is general and\ncomplementary to existing data augmentation (such as random/adversarial input\ntransform) and regularization techniques (such as weight decay, dropout, and\nbatch norm). \n\n"}
{"id": "1803.07276", "contents": "Title: Removing Confounding Factors Associated Weights in Deep Neural Networks\n  Improves the Prediction Accuracy for Healthcare Applications Abstract: The proliferation of healthcare data has brought the opportunities of\napplying data-driven approaches, such as machine learning methods, to assist\ndiagnosis. Recently, many deep learning methods have been shown with impressive\nsuccesses in predicting disease status with raw input data. However, the\n\"black-box\" nature of deep learning and the high-reliability requirement of\nbiomedical applications have created new challenges regarding the existence of\nconfounding factors. In this paper, with a brief argument that inappropriate\nhandling of confounding factors will lead to models' sub-optimal performance in\nreal-world applications, we present an efficient method that can remove the\ninfluences of confounding factors such as age or gender to improve the\nacross-cohort prediction accuracy of neural networks. One distinct advantage of\nour method is that it only requires minimal changes of the baseline model's\narchitecture so that it can be plugged into most of the existing neural\nnetworks. We conduct experiments across CT-scan, MRA, and EEG brain wave with\nconvolutional neural networks and LSTM to verify the efficiency of our method. \n\n"}
{"id": "1803.07658", "contents": "Title: Graph-based regularization for regression problems with alignment and\n  highly-correlated designs Abstract: Sparse models for high-dimensional linear regression and machine learning\nhave received substantial attention over the past two decades. Model selection,\nor determining which features or covariates are the best explanatory variables,\nis critical to the interpretability of a learned model. Much of the current\nliterature assumes that covariates are only mildly correlated. However, in many\nmodern applications covariates are highly correlated and do not exhibit key\nproperties (such as the restricted eigenvalue condition, restricted isometry\nproperty, or other related assumptions). This work considers a high-dimensional\nregression setting in which a graph governs both correlations among the\ncovariates and the similarity among regression coefficients -- meaning there is\n\\emph{alignment} between the covariates and regression coefficients. Using side\ninformation about the strength of correlations among features, we form a graph\nwith edge weights corresponding to pairwise covariances. This graph is used to\ndefine a graph total variation regularizer that promotes similar weights for\ncorrelated features.\n  This work shows how the proposed graph-based regularization yields\nmean-squared error guarantees for a broad range of covariance graph structures.\nThese guarantees are optimal for many specific covariance graphs, including\nblock and lattice graphs. Our proposed approach outperforms other methods for\nhighly-correlated design in a variety of experiments on synthetic data and real\nbiochemistry data. \n\n"}
{"id": "1803.07879", "contents": "Title: An Unsupervised Multivariate Time Series Kernel Approach for Identifying\n  Patients with Surgical Site Infection from Blood Samples Abstract: A large fraction of the electronic health records consists of clinical\nmeasurements collected over time, such as blood tests, which provide important\ninformation about the health status of a patient. These sequences of clinical\nmeasurements are naturally represented as time series, characterized by\nmultiple variables and the presence of missing data, which complicate analysis.\nIn this work, we propose a surgical site infection detection framework for\npatients undergoing colorectal cancer surgery that is completely unsupervised,\nhence alleviating the problem of getting access to labelled training data. The\nframework is based on powerful kernels for multivariate time series that\naccount for missing data when computing similarities. Our approach show\nsuperior performance compared to baselines that have to resort to imputation\ntechniques and performs comparable to a supervised classification baseline. \n\n"}
{"id": "1803.08118", "contents": "Title: Seglearn: A Python Package for Learning Sequences and Time Series Abstract: Seglearn is an open-source python package for machine learning time series or\nsequences using a sliding window segmentation approach. The implementation\nprovides a flexible pipeline for tackling classification, regression, and\nforecasting problems with multivariate sequence and contextual data. This\npackage is compatible with scikit-learn and is listed under scikit-learn\nRelated Projects. The package depends on numpy, scipy, and scikit-learn.\nSeglearn is distributed under the BSD 3-Clause License. Documentation includes\na detailed API description, user guide, and examples. Unit tests provide a high\ndegree of code coverage. \n\n"}
{"id": "1803.08416", "contents": "Title: Demystifying Deep Learning: A Geometric Approach to Iterative\n  Projections Abstract: Parametric approaches to Learning, such as deep learning (DL), are highly\npopular in nonlinear regression, in spite of their extremely difficult training\nwith their increasing complexity (e.g. number of layers in DL). In this paper,\nwe present an alternative semi-parametric framework which foregoes the\nordinarily required feedback, by introducing the novel idea of geometric\nregularization. We show that certain deep learning techniques such as residual\nnetwork (ResNet) architecture are closely related to our approach. Hence, our\ntechnique can be used to analyze these types of deep learning. Moreover, we\npresent preliminary results which confirm that our approach can be easily\ntrained to obtain complex structures. \n\n"}
{"id": "1803.08577", "contents": "Title: Unbiased scalable softmax optimization Abstract: Recent neural network and language models rely on softmax distributions with\nan extremely large number of categories. Since calculating the softmax\nnormalizing constant in this context is prohibitively expensive, there is a\ngrowing literature of efficiently computable but biased estimates of the\nsoftmax. In this paper we propose the first unbiased algorithms for maximizing\nthe softmax likelihood whose work per iteration is independent of the number of\nclasses and datapoints (and no extra work is required at the end of each\nepoch). We show that our proposed unbiased methods comprehensively outperform\nthe state-of-the-art on seven real world datasets. \n\n"}
{"id": "1803.08882", "contents": "Title: Trace your sources in large-scale data: one ring to find them all Abstract: An important preprocessing step in most data analysis pipelines aims to\nextract a small set of sources that explain most of the data. Currently used\nalgorithms for blind source separation (BSS), however, often fail to extract\nthe desired sources and need extensive cross-validation. In contrast, their\nrarely used probabilistic counterparts can get away with little\ncross-validation and are more accurate and reliable but no simple and scalable\nimplementations are available. Here we present a novel probabilistic BSS\nframework (DECOMPOSE) that can be flexibly adjusted to the data, is extensible\nand easy to use, adapts to individual sources and handles large-scale data\nthrough algorithmic efficiency. DECOMPOSE encompasses and generalises many\ntraditional BSS algorithms such as PCA, ICA and NMF and we demonstrate\nsubstantial improvements in accuracy and robustness on artificial and real\ndata. \n\n"}
{"id": "1803.09868", "contents": "Title: Bypassing Feature Squeezing by Increasing Adversary Strength Abstract: Feature Squeezing is a recently proposed defense method which reduces the\nsearch space available to an adversary by coalescing samples that correspond to\nmany different feature vectors in the original space into a single sample. It\nhas been shown that feature squeezing defenses can be combined in a joint\ndetection framework to achieve high detection rates against state-of-the-art\nattacks. However, we demonstrate on the MNIST and CIFAR-10 datasets that by\nincreasing the adversary strength of said state-of-the-art attacks, one can\nbypass the detection framework with adversarial examples of minimal visual\ndistortion. These results suggest for proposed defenses to validate against\nstronger attack configurations. \n\n"}
{"id": "1803.10397", "contents": "Title: Supervising Unsupervised Learning with Evolutionary Algorithm in Deep\n  Neural Network Abstract: A method to control results of gradient descent unsupervised learning in a\ndeep neural network by using evolutionary algorithm is proposed. To process\ncrossover of unsupervisedly trained models, the algorithm evaluates pointwise\nfitness of individual nodes in neural network. Labeled training data is\nrandomly sampled and breeding process selects nodes by calculating degree of\ntheir consistency on different sets of sampled data. This method supervises\nunsupervised training by evolutionary process. We also introduce modified\nRestricted Boltzmann Machine which contains repulsive force among nodes in a\nneural network and it contributes to isolate network nodes each other to avoid\naccidental degeneration of nodes by evolutionary process. These new methods are\napplied to document classification problem and it results better accuracy than\na traditional fully supervised classifier implemented with linear regression\nalgorithm. \n\n"}
{"id": "1803.10840", "contents": "Title: Defending against Adversarial Images using Basis Functions\n  Transformations Abstract: We study the effectiveness of various approaches that defend against\nadversarial attacks on deep networks via manipulations based on basis function\nrepresentations of images. Specifically, we experiment with low-pass filtering,\nPCA, JPEG compression, low resolution wavelet approximation, and\nsoft-thresholding. We evaluate these defense techniques using three types of\npopular attacks in black, gray and white-box settings. Our results show JPEG\ncompression tends to outperform the other tested defenses in most of the\nsettings considered, in addition to soft-thresholding, which performs well in\nspecific cases, and yields a more mild decrease in accuracy on benign examples.\nIn addition, we also mathematically derive a novel white-box attack in which\nthe adversarial perturbation is composed only of terms corresponding a to\npre-determined subset of the basis functions, of which a \"low frequency attack\"\nis a special case. \n\n"}
{"id": "1803.11008", "contents": "Title: On Hyperparameter Search in Cluster Ensembles Abstract: Quality assessments of models in unsupervised learning and clustering\nverification in particular have been a long-standing problem in the machine\nlearning research. The lack of robust and universally applicable cluster\nvalidity scores often makes the algorithm selection and hyperparameter\nevaluation a tough guess. In this paper, we show that cluster ensemble\naggregation techniques such as consensus clustering may be used to evaluate\nclusterings and their hyperparameter configurations. We use normalized mutual\ninformation to compare individual objects of a clustering ensemble to the\nconstructed consensus of the whole ensemble and show, that the resulting score\ncan serve as an overall quality measure for clustering problems. This method is\ncapable of highlighting the standout clustering and hyperparameter\nconfiguration in the ensemble even in the case of a distorted consensus. We\napply this very general framework to various data sets and give possible\ndirections for future research. \n\n"}
{"id": "1803.11521", "contents": "Title: A Novel Framework for Online Supervised Learning with Feature Selection Abstract: Current online learning methods suffer issues such as lower convergence rates\nand limited capability to select important features compared to their offline\ncounterparts. In this paper, a novel framework for online learning based on\nrunning averages is proposed. Many popular offline regularized methods such as\nLasso, Elastic Net, Minimax Concave Penalty (MCP), and Feature Selection with\nAnnealing (FSA) have their online versions introduced in this framework. The\nequivalence between the proposed online methods and their offline counterparts\nis proved, and then novel theoretical true support recovery and convergence\nguarantees are provided for some of the methods in this framework. Numerical\nexperiments indicate that the proposed methods enjoy high true support recovery\naccuracy and a faster convergence rate compared with conventional online and\noffline algorithms. Finally, applications to large datasets are presented,\nwhere again the proposed framework shows competitive results compared to\npopular online and offline algorithms. \n\n"}
{"id": "1804.00341", "contents": "Title: Sparse Principal Component Analysis via Variable Projection Abstract: Sparse principal component analysis (SPCA) has emerged as a powerful\ntechnique for modern data analysis, providing improved interpretation of\nlow-rank structures by identifying localized spatial structures in the data and\ndisambiguating between distinct time scales. We demonstrate a robust and\nscalable SPCA algorithm by formulating it as a value-function optimization\nproblem. This viewpoint leads to a flexible and computationally efficient\nalgorithm. Further, we can leverage randomized methods from linear algebra to\nextend the approach to the large-scale (big data) setting. Our proposed\ninnovation also allows for a robust SPCA formulation which obtains meaningful\nsparse principal components in spite of grossly corrupted input data. The\nproposed algorithms are demonstrated using both synthetic and real world data,\nand show exceptional computational efficiency and diagnostic performance. \n\n"}
{"id": "1804.00713", "contents": "Title: Controllable photonic time-bin qubits from a quantum dot Abstract: Photonic time bin qubits are well suited to transmission via optical fibres\nand waveguide circuits. The states take the form $\\frac{1}{\\sqrt{2}}(\\alpha\n\\ket{0} + e^{i\\phi}\\beta \\ket{1})$, with $\\ket{0}$ and $\\ket{1}$ referring to\nthe early and late time bin respectively. By controlling the phase of a laser\ndriving a spin-flip Raman transition in a single-hole-charged InAs quantum dot\nwe demonstrate complete control over the phase, $\\phi$. We show that this\nphoton generation process can be performed deterministically, with only a\nmoderate loss in coherence. Finally, we encode different qubits in different\nenergies of the Raman scattered light, demonstrating wavelength division\nmultiplexing at the single photon level. \n\n"}
{"id": "1804.01592", "contents": "Title: Robust and Resource Efficient Identification of Shallow Neural Networks\n  by Fewest Samples Abstract: We address the structure identification and the uniform approximation of sums\nof ridge functions $f(x)=\\sum_{i=1}^m g_i(a_i\\cdot x)$ on ${\\mathbb R}^d$,\nrepresenting a general form of a shallow feed-forward neural network, from a\nsmall number of query samples. Higher order differentiation, as used in our\nconstructive approximations, of sums of ridge functions or of their\ncompositions, as in deeper neural network, yields a natural connection between\nneural network weight identification and tensor product decomposition\nidentification. In the case of the shallowest feed-forward neural network,\nsecond order differentiation and tensors of order two (i.e., matrices) suffice\nas we prove in this paper. We use two sampling schemes to perform approximate\ndifferentiation - active sampling, where the sampling points are universal,\nactively, and randomly designed, and passive sampling, where sampling points\nwere preselected at random from a distribution with known density. Based on\nmultiple gathered approximated first and second order differentials, our\ngeneral approximation strategy is developed as a sequence of algorithms to\nperform individual sub-tasks. We first perform an active subspace search by\napproximating the span of the weight vectors $a_1,\\dots,a_m$. Then we use a\nstraightforward substitution, which reduces the dimensionality of the problem\nfrom $d$ to $m$. The core of the construction is then the stable and efficient\napproximation of weights expressed in terms of rank-$1$ matrices $a_i \\otimes\na_i$, realized by formulating their individual identification as a suitable\nnonlinear program. We prove the successful identification by this program of\nweight vectors being close to orthonormal and we also show how we can\ncostructively reduce to this case by a whitening procedure, without loss of any\ngenerality. \n\n"}
{"id": "1804.01600", "contents": "Title: Quantum Speed Limits under Continuous Quantum Measurements Abstract: The pace of evolution of physical systems is fundamentally constrained by\nquantum speed limits (QSL), which have found broad applications in quantum\nscience and technology. We consider the speed of evolution for quantum systems\nundergoing stochastic dynamics due to continuous measurements. It is shown that\nthat there are trajectories for which standard QSL are violated, and we provide\nestimates for the range of velocities in an ensemble of realizations of\ncontinuous measurement records. We determine the dispersion of the speed of\nevolution and characterize the full statistics of single trajectories. By\ncharacterizing the dispersion of the Bures angle, we further show that\ncontinuous quantum measurements induce Brownian dynamics in Hilbert space. \n\n"}
{"id": "1804.02485", "contents": "Title: Fortified Networks: Improving the Robustness of Deep Networks by\n  Modeling the Manifold of Hidden Representations Abstract: Deep networks have achieved impressive results across a variety of important\ntasks. However a known weakness is a failure to perform well when evaluated on\ndata which differ from the training distribution, even if these differences are\nvery small, as is the case with adversarial examples. We propose Fortified\nNetworks, a simple transformation of existing networks, which fortifies the\nhidden layers in a deep network by identifying when the hidden states are off\nof the data manifold, and maps these hidden states back to parts of the data\nmanifold where the network performs well. Our principal contribution is to show\nthat fortifying these hidden states improves the robustness of deep networks\nand our experiments (i) demonstrate improved robustness to standard adversarial\nattacks in both black-box and white-box threat models; (ii) suggest that our\nimprovements are not primarily due to the gradient masking problem and (iii)\nshow the advantage of doing this fortification in the hidden layers instead of\nthe input space. \n\n"}
{"id": "1804.02744", "contents": "Title: Unsupervised Learning of GMM with a Uniform Background Component Abstract: Gaussian Mixture Models are one of the most studied and mature models in\nunsupervised learning. However, outliers are often present in the data and\ncould influence the cluster estimation. In this paper, we study a new model\nthat assumes that data comes from a mixture of a number of Gaussians as well as\na uniform ``background'' component assumed to contain outliers and other\nnon-interesting observations. We develop a novel method based on robust loss\nminimization that performs well in clustering such GMM with a uniform\nbackground. We give theoretical guarantees for our clustering algorithm to\nobtain best clustering results with high probability. Besides, we show that the\nresult of our algorithm does not depend on initialization or local optima, and\nthe parameter tuning is an easy task. By numeric simulations, we demonstrate\nthat our algorithm enjoys high accuracy and achieves the best clustering\nresults given a large enough sample size. Finally, experimental comparisons\nwith typical clustering methods on real datasets witness the potential of our\nalgorithm in real applications. \n\n"}
{"id": "1804.03273", "contents": "Title: On the Supermodularity of Active Graph-based Semi-supervised Learning\n  with Stieltjes Matrix Regularization Abstract: Active graph-based semi-supervised learning (AG-SSL) aims to select a small\nset of labeled examples and utilize their graph-based relation to other\nunlabeled examples to aid in machine learning tasks. It is also closely related\nto the sampling theory in graph signal processing. In this paper, we revisit\nthe original formulation of graph-based SSL and prove the supermodularity of an\nAG-SSL objective function under a broad class of regularization functions\nparameterized by Stieltjes matrices. Under this setting, supermodularity yields\na novel greedy label sampling algorithm with guaranteed performance relative to\nthe optimal sampling set. Compared to three state-of-the-art graph signal\nsampling and recovery methods on two real-life community detection datasets,\nthe proposed AG-SSL method attains superior classification accuracy given\nlimited sample budgets. \n\n"}
{"id": "1804.03761", "contents": "Title: Derivative free optimization via repeated classification Abstract: We develop an algorithm for minimizing a function using $n$ batched function\nvalue measurements at each of $T$ rounds by using classifiers to identify a\nfunction's sublevel set. We show that sufficiently accurate classifiers can\nachieve linear convergence rates, and show that the convergence rate is tied to\nthe difficulty of active learning sublevel sets. Further, we show that the\nbootstrap is a computationally efficient approximation to the necessary\nclassification scheme.\n  The end result is a computationally efficient derivative-free algorithm\nrequiring no tuning that consistently outperforms other approaches on\nsimulations, standard benchmarks, real-world DNA binding optimization, and\nairfoil design problems whenever batched function queries are natural. \n\n"}
{"id": "1804.04205", "contents": "Title: Learning Topics using Semantic Locality Abstract: The topic modeling discovers the latent topic probability of the given text\ndocuments. To generate the more meaningful topic that better represents the\ngiven document, we proposed a new feature extraction technique which can be\nused in the data preprocessing stage. The method consists of three steps.\nFirst, it generates the word/word-pair from every single document. Second, it\napplies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.\nThird, it uses the K-means algorithm to merge the word pairs that have the\nsimilar semantic meaning.\n  Experiments are carried out on the Open Movie Database (OMDb), Reuters\nDataset and 20NewsGroup Dataset. The mean Average Precision score is used as\nthe evaluation metric. Comparing our results with other state-of-the-art topic\nmodels, such as Latent Dirichlet allocation and traditional Restricted\nBoltzmann Machines. Our proposed data preprocessing can improve the generated\ntopic accuracy by up to 12.99\\%. \n\n"}
{"id": "1804.04610", "contents": "Title: Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling Abstract: We study 3D shape modeling from a single image and make contributions to it\nin three aspects. First, we present Pix3D, a large-scale benchmark of diverse\nimage-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications\nin shape-related tasks including reconstruction, retrieval, viewpoint\nestimation, etc. Building such a large-scale dataset, however, is highly\nchallenging; existing datasets either contain only synthetic data, or lack\nprecise alignment between 2D images and 3D shapes, or only have a small number\nof images. Second, we calibrate the evaluation criteria for 3D shape\nreconstruction through behavioral studies, and use them to objectively and\nsystematically benchmark cutting-edge reconstruction algorithms on Pix3D.\nThird, we design a novel model that simultaneously performs 3D reconstruction\nand pose estimation; our multi-task learning approach achieves state-of-the-art\nperformance on both tasks. \n\n"}
{"id": "1804.05146", "contents": "Title: A comparison of methods for model selection when estimating individual\n  treatment effects Abstract: Practitioners in medicine, business, political science, and other fields are\nincreasingly aware that decisions should be personalized to each patient,\ncustomer, or voter. A given treatment (e.g. a drug or advertisement) should be\nadministered only to those who will respond most positively, and certainly not\nto those who will be harmed by it. Individual-level treatment effects can be\nestimated with tools adapted from machine learning, but different models can\nyield contradictory estimates. Unlike risk prediction models, however,\ntreatment effect models cannot be easily evaluated against each other using a\nheld-out test set because the true treatment effect itself is never directly\nobserved. Besides outcome prediction accuracy, several metrics that can\nleverage held-out data to evaluate treatment effects models have been proposed,\nbut they are not widely used. We provide a didactic framework that elucidates\nthe relationships between the different approaches and compare them all using a\nvariety of simulations of both randomized and observational data. Our results\nshow that researchers estimating heterogenous treatment effects need not limit\nthemselves to a single model-fitting algorithm. Instead of relying on a single\nmethod, multiple models fit by a diverse set of algorithms should be evaluated\nagainst each other using an objective function learned from the validation set.\nThe model minimizing that objective should be used for estimating the\nindividual treatment effect for future individuals. \n\n"}
{"id": "1804.05316", "contents": "Title: From CDF to PDF --- A Density Estimation Method for High Dimensional\n  Data Abstract: CDF2PDF is a method of PDF estimation by approximating CDF. The original idea\nof it was previously proposed in [1] called SIC. However, SIC requires\nadditional hyper-parameter tunning, and no algorithms for computing higher\norder derivative from a trained NN are provided in [1]. CDF2PDF improves SIC by\navoiding the time-consuming hyper-parameter tuning part and enabling higher\norder derivative computation to be done in polynomial time. Experiments of this\nmethod for one-dimensional data shows promising results. \n\n"}
{"id": "1804.05433", "contents": "Title: Adaptivity for Regularized Kernel Methods by Lepskii's Principle Abstract: We address the problem of {\\it adaptivity} in the framework of reproducing\nkernel Hilbert space (RKHS) regression. More precisely, we analyze estimators\narising from a linear regularization scheme $g_\\lam$. In practical\napplications, an important task is to choose the regularization parameter\n$\\lam$ appropriately, i.e. based only on the given data and independently on\nunknown structural assumptions on the regression function. An attractive\napproach avoiding data-splitting is the {\\it Lepskii Principle} (LP), also\nknown as the {\\it Balancing Principle} is this setting. We show that a modified\nparameter choice based on (LP) is minimax optimal adaptive, up to\n$\\log\\log(n)$. A convenient result is the fact that balancing in $L^2(\\nu)-$\nnorm, which is easiest, automatically gives optimal balancing in all stronger\nnorms, interpolating between $L^2(\\nu)$ and the RKHS. An analogous result is\nopen for other classical approaches to data dependent choices of the\nregularization parameter, e.g. for Hold-Out. \n\n"}
{"id": "1804.05806", "contents": "Title: Deep Embedding Kernel Abstract: In this paper, we propose a novel supervised learning method that is called\nDeep Embedding Kernel (DEK). DEK combines the advantages of deep learning and\nkernel methods in a unified framework. More specifically, DEK is a learnable\nkernel represented by a newly designed deep architecture. Compared with\npre-defined kernels, this kernel can be explicitly trained to map data to an\noptimized high-level feature space where data may have favorable features\ntoward the application. Compared with typical deep learning using SoftMax or\nlogistic regression as the top layer, DEK is expected to be more generalizable\nto new data. Experimental results show that DEK has superior performance than\ntypical machine learning methods in identity detection, classification,\nregression, dimension reduction, and transfer learning. \n\n"}
{"id": "1804.05837", "contents": "Title: Walk-Steered Convolution for Graph Classification Abstract: Graph classification is a fundamental but challenging issue for numerous\nreal-world applications. Despite recent great progress in image/video\nclassification, convolutional neural networks (CNNs) cannot yet cater to graphs\nwell because of graphical non-Euclidean topology. In this work, we propose a\nwalk-steered convolutional (WSC) network to assemble the essential success of\nstandard convolutional neural networks as well as the powerful representation\nability of random walk. Instead of deterministic neighbor searching used in\nprevious graphical CNNs, we construct multi-scale walk fields (a.k.a. local\nreceptive fields) with random walk paths to depict subgraph structures and\nadvocate graph scalability. To express the internal variations of a walk field,\nGaussian mixture models are introduced to encode principal components of walk\npaths therein. As an analogy to a standard convolution kernel on image,\nGaussian models implicitly coordinate those unordered vertices/nodes and edges\nin a local receptive field after projecting to the gradient space of Gaussian\nparameters. We further stack graph coarsening upon Gaussian encoding by using\ndynamic clustering, such that high-level semantics of graph can be well learned\nlike the conventional pooling on image. The experimental results on several\npublic datasets demonstrate the superiority of our proposed WSC method over\nmany state-of-the-arts for graph classification. \n\n"}
{"id": "1804.05862", "contents": "Title: Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian\n  Compression Approach Abstract: Modern neural networks are highly overparameterized, with capacity to\nsubstantially overfit to training data. Nevertheless, these networks often\ngeneralize well in practice. It has also been observed that trained networks\ncan often be \"compressed\" to much smaller representations. The purpose of this\npaper is to connect these two empirical observations. Our main technical result\nis a generalization bound for compressed networks based on the compressed size.\nCombined with off-the-shelf compression algorithms, the bound leads to state of\nthe art generalization guarantees; in particular, we provide the first\nnon-vacuous generalization guarantees for realistic architectures applied to\nthe ImageNet classification problem. As additional evidence connecting\ncompression and generalization, we show that compressibility of models that\ntend to overfit is limited: We establish an absolute limit on expected\ncompressibility as a function of expected generalization error, where the\nexpectations are over the random choice of training examples. The bounds are\ncomplemented by empirical results that show an increase in overfitting implies\nan increase in the number of bits required to describe a trained network. \n\n"}
{"id": "1804.06872", "contents": "Title: Co-teaching: Robust Training of Deep Neural Networks with Extremely\n  Noisy Labels Abstract: Deep learning with noisy labels is practically challenging, as the capacity\nof deep models is so high that they can totally memorize these noisy labels\nsooner or later during training. Nonetheless, recent studies on the\nmemorization effects of deep neural networks show that they would first\nmemorize training data of clean labels and then those of noisy labels.\nTherefore in this paper, we propose a new deep learning paradigm called\nCo-teaching for combating with noisy labels. Namely, we train two deep neural\nnetworks simultaneously, and let them teach each other given every mini-batch:\nfirstly, each network feeds forward all data and selects some data of possibly\nclean labels; secondly, two networks communicate with each other what data in\nthis mini-batch should be used for training; finally, each network back\npropagates the data selected by its peer network and updates itself. Empirical\nresults on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that\nCo-teaching is much superior to the state-of-the-art methods in the robustness\nof trained deep models. \n\n"}
{"id": "1804.07353", "contents": "Title: Unsupervised Representation Adversarial Learning Network: from\n  Reconstruction to Generation Abstract: A good representation for arbitrarily complicated data should have the\ncapability of semantic generation, clustering and reconstruction. Previous\nresearch has already achieved impressive performance on either one. This paper\naims at learning a disentangled representation effective for all of them in an\nunsupervised way. To achieve all the three tasks together, we learn the forward\nand inverse mapping between data and representation on the basis of a symmetric\nadversarial process. In theory, we minimize the upper bound of the two\nconditional entropy loss between the latent variables and the observations\ntogether to achieve the cycle consistency. The newly proposed RepGAN is tested\non MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised\nclassification, generation and reconstruction tasks. The result demonstrates\nthat RepGAN is able to learn a useful and competitive representation. To the\nauthor's knowledge, our work is the first one to achieve both a high\nunsupervised classification accuracy and low reconstruction error on MNIST.\nCodes are available at https://github.com/yzhouas/RepGAN-tensorflow. \n\n"}
{"id": "1804.08219", "contents": "Title: Adaptive Performance Assessment For Drivers Through Behavioral Advantage Abstract: The potential positive impact of autonomous driving and driver assistance\ntechnolo- gies have been a major impetus over the last decade. On the flip\nside, it has been a challenging problem to analyze the performance of human\ndrivers or autonomous driving agents quantitatively. In this work, we propose a\ngeneric method that compares the performance of drivers or autonomous driving\nagents even if the environmental conditions are different, by using the driver\nbehavioral advantage instead of absolute metrics, which efficiently removes the\nenvironmental factors. A concrete application of the method is also presented,\nwhere the performance of more than 100 truck drivers was evaluated and ranked\nin terms of fuel efficiency, covering more than 90,000 trips spanning an\naverage of 300 miles in a variety of driving conditions and environments. \n\n"}
{"id": "1804.08328", "contents": "Title: Taskonomy: Disentangling Task Transfer Learning Abstract: Do visual tasks have a relationship, or are they unrelated? For instance,\ncould having surface normals simplify estimating the depth of an image?\nIntuition answers these questions positively, implying existence of a structure\namong visual tasks. Knowing this structure has notable values; it is the\nconcept underlying transfer learning and provides a principled way for\nidentifying redundancies across tasks, e.g., to seamlessly reuse supervision\namong related tasks or solve many tasks in one system without piling up the\ncomplexity.\n  We proposes a fully computational approach for modeling the structure of\nspace of visual tasks. This is done via finding (first and higher-order)\ntransfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D,\nand semantic tasks in a latent space. The product is a computational taxonomic\nmap for task transfer learning. We study the consequences of this structure,\ne.g. nontrivial emerged relationships, and exploit them to reduce the demand\nfor labeled data. For example, we show that the total number of labeled\ndatapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3\n(compared to training independently) while keeping the performance nearly the\nsame. We provide a set of tools for computing and probing this taxonomical\nstructure including a solver that users can employ to devise efficient\nsupervision policies for their use cases. \n\n"}
{"id": "1804.09314", "contents": "Title: Deep Learning for Predicting Asset Returns Abstract: Deep learning searches for nonlinear factors for predicting asset returns.\nPredictability is achieved via multiple layers of composite factors as opposed\nto additive ones. Viewed in this way, asset pricing studies can be revisited\nusing multi-layer deep learners, such as rectified linear units (ReLU) or\nlong-short-term-memory (LSTM) for time-series effects. State-of-the-art\nalgorithms including stochastic gradient descent (SGD), TensorFlow and dropout\ndesign provide imple- mentation and efficient factor exploration. To illustrate\nour methodology, we revisit the equity market risk premium dataset of Welch and\nGoyal (2008). We find the existence of nonlinear factors which explain\npredictability of returns, in particular at the extremes of the characteristic\nspace. Finally, we conclude with directions for future research. \n\n"}
{"id": "1804.10272", "contents": "Title: Network Transplanting Abstract: This paper focuses on a new task, i.e., transplanting a\ncategory-and-task-specific neural network to a generic, modular network without\nstrong supervision. We design an functionally interpretable structure for the\ngeneric network. Like building LEGO blocks, we teach the generic network a new\ncategory by directly transplanting the module corresponding to the category\nfrom a pre-trained network with a few or even without sample annotations. Our\nmethod incrementally adds new categories to the generic network but does not\naffect representations of existing categories. In this way, our method breaks\nthe typical bottleneck of learning a net for massive tasks and categories, i.e.\nthe requirement of collecting samples for all tasks and categories at the same\ntime before the learning begins. Thus, we use a new distillation algorithm,\nnamely back-distillation, to overcome specific challenges of network\ntransplanting. Our method without training samples even outperformed the\nbaseline with 100 training samples. \n\n"}
{"id": "1805.00909", "contents": "Title: Reinforcement Learning and Control as Probabilistic Inference: Tutorial\n  and Review Abstract: The framework of reinforcement learning or optimal control provides a\nmathematical formalization of intelligent decision making that is powerful and\nbroadly applicable. While the general form of the reinforcement learning\nproblem enables effective reasoning about uncertainty, the connection between\nreinforcement learning and inference in probabilistic models is not immediately\nobvious. However, such a connection has considerable value when it comes to\nalgorithm design: formalizing a problem as probabilistic inference in principle\nallows us to bring to bear a wide array of approximate inference tools, extend\nthe model in flexible and powerful ways, and reason about compositionality and\npartial observability. In this article, we will discuss how a generalization of\nthe reinforcement learning or optimal control problem, which is sometimes\ntermed maximum entropy reinforcement learning, is equivalent to exact\nprobabilistic inference in the case of deterministic dynamics, and variational\ninference in the case of stochastic dynamics. We will present a detailed\nderivation of this framework, overview prior work that has drawn on this and\nrelated ideas to propose new reinforcement learning and control algorithms, and\ndescribe perspectives on future research. \n\n"}
{"id": "1805.01045", "contents": "Title: Alpha-Beta Divergence For Variational Inference Abstract: This paper introduces a variational approximation framework using direct\noptimization of what is known as the {\\it scale invariant Alpha-Beta\ndivergence} (sAB divergence). This new objective encompasses most variational\nobjectives that use the Kullback-Leibler, the R{\\'e}nyi or the gamma\ndivergences. It also gives access to objective functions never exploited before\nin the context of variational inference. This is achieved via two easy to\ninterpret control parameters, which allow for a smooth interpolation over the\ndivergence space while trading-off properties such as mass-covering of a target\ndistribution and robustness to outliers in the data. Furthermore, the sAB\nvariational objective can be optimized directly by repurposing existing methods\nfor Monte Carlo computation of complex variational objectives, leading to\nestimates of the divergence instead of variational lower bounds. We show the\nadvantages of this objective on Bayesian models for regression problems. \n\n"}
{"id": "1805.01078", "contents": "Title: Exploration of Numerical Precision in Deep Neural Networks Abstract: Reduced numerical precision is a common technique to reduce computational\ncost in many Deep Neural Networks (DNNs). While it has been observed that DNNs\nare resilient to small errors and noise, no general result exists that is\ncapable of predicting a given DNN system architecture's sensitivity to reduced\nprecision. In this project, we emulate arbitrary bit-width using a specified\nfloating-point representation with a truncation method, which is applied to the\nneural network after each batch. We explore the impact of several model\nparameters on the network's training accuracy and show results on the MNIST\ndataset. We then present a preliminary theoretical investigation of the error\nscaling in both forward and backward propagations. We end with a discussion of\nthe implications of these results as well as the potential for generalization\nto other network architectures. \n\n"}
{"id": "1805.01956", "contents": "Title: Motion Planning Among Dynamic, Decision-Making Agents with Deep\n  Reinforcement Learning Abstract: Robots that navigate among pedestrians use collision avoidance algorithms to\nenable safe and efficient operation. Recent works present deep reinforcement\nlearning as a framework to model the complex interactions and cooperation.\nHowever, they are implemented using key assumptions about other agents'\nbehavior that deviate from reality as the number of agents in the environment\nincreases. This work extends our previous approach to develop an algorithm that\nlearns collision avoidance among a variety of types of dynamic agents without\nassuming they follow any particular behavior rules. This work also introduces a\nstrategy using LSTM that enables the algorithm to use observations of an\narbitrary number of other agents, instead of previous methods that have a fixed\nobservation size. The proposed algorithm outperforms our previous approach in\nsimulation as the number of agents increases, and the algorithm is demonstrated\non a fully autonomous robotic vehicle traveling at human walking speed, without\nthe use of a 3D Lidar. \n\n"}
{"id": "1805.02087", "contents": "Title: A Constraint-Based Algorithm For Causal Discovery with Cycles, Latent\n  Variables and Selection Bias Abstract: Causal processes in nature may contain cycles, and real datasets may violate\ncausal sufficiency as well as contain selection bias. No constraint-based\ncausal discovery algorithm can currently handle cycles, latent variables and\nselection bias (CLS) simultaneously. I therefore introduce an algorithm called\nCyclic Causal Inference (CCI) that makes sound inferences with a conditional\nindependence oracle under CLS, provided that we can represent the cyclic causal\nprocess as a non-recursive linear structural equation model with independent\nerrors. Empirical results show that CCI outperforms CCD in the cyclic case as\nwell as rivals FCI and RFCI in the acyclic case. \n\n"}
{"id": "1805.02257", "contents": "Title: Bayesian Regularization for Graphical Models with Unequal Shrinkage Abstract: We consider a Bayesian framework for estimating a high-dimensional sparse\nprecision matrix, in which adaptive shrinkage and sparsity are induced by a\nmixture of Laplace priors. Besides discussing our formulation from the Bayesian\nstandpoint, we investigate the MAP (maximum a posteriori) estimator from a\npenalized likelihood perspective that gives rise to a new non-convex penalty\napproximating the $\\ell_0$ penalty. Optimal error rates for estimation\nconsistency in terms of various matrix norms along with selection consistency\nfor sparse structure recovery are shown for the unique MAP estimator under mild\nconditions. For fast and efficient computation, an EM algorithm is proposed to\ncompute the MAP estimator of the precision matrix and (approximate) posterior\nprobabilities on the edges of the underlying sparse structure. Through\nextensive simulation studies and a real application to a call center data, we\nhave demonstrated the fine performance of our method compared with existing\nalternatives. \n\n"}
{"id": "1805.02848", "contents": "Title: Identifiability of Generalized Hypergeometric Distribution (GHD)\n  Directed Acyclic Graphical Models Abstract: We introduce a new class of identifiable DAG models where the conditional\ndistribution of each node given its parents belongs to a family of generalized\nhypergeometric distributions (GHD). A family of generalized hypergeometric\ndistributions includes a lot of discrete distributions such as the binomial,\nBeta-binomial, negative binomial, Poisson, hyper-Poisson, and many more. We\nprove that if the data drawn from the new class of DAG models, one can fully\nidentify the graph structure. We further present a reliable and polynomial-time\nalgorithm that recovers the graph from finitely many data. We show through\ntheoretical results and numerical experiments that our algorithm is\nstatistically consistent in high-dimensional settings (p>n) if the indegree of\nthe graph is bounded, and out-performs state-of-the-art DAG learning\nalgorithms. \n\n"}
{"id": "1805.03911", "contents": "Title: Labelling as an unsupervised learning problem Abstract: Unravelling hidden patterns in datasets is a classical problem with many\npotential applications. In this paper, we present a challenge whose objective\nis to discover nonlinear relationships in noisy cloud of points. If a set of\npoint satisfies a nonlinear relationship that is unlikely to be due to\nrandomness, we will label the set with this relationship. Since points can\nsatisfy one, many or no such nonlinear relationships, cloud of points will\ntypically have one, multiple or no labels at all. This introduces the labelling\nproblem that will be studied in this paper.\n  The objective of this paper is to develop a framework for the labelling\nproblem. We introduce a precise notion of a label, and we propose an algorithm\nto discover such labels in a given dataset, which is then tested in synthetic\ndatasets. We also analyse, using tools from random matrix theory, the problem\nof discovering false labels in the dataset. \n\n"}
{"id": "1805.04577", "contents": "Title: Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound\n  Conditions Abstract: Error bound conditions (EBC) are properties that characterize the growth of\nan objective function when a point is moved away from the optimal set. They\nhave recently received increasing attention in the field of optimization for\ndeveloping optimization algorithms with fast convergence. However, the studies\nof EBC in statistical learning are hitherto still limited. The main\ncontributions of this paper are two-fold. First, we develop fast and\nintermediate rates of empirical risk minimization (ERM) under EBC for risk\nminimization with Lipschitz continuous, and smooth convex random functions.\nSecond, we establish fast and intermediate rates of an efficient stochastic\napproximation (SA) algorithm for risk minimization with Lipschitz continuous\nrandom functions, which requires only one pass of $n$ samples and adapts to\nEBC. For both approaches, the convergence rates span a full spectrum between\n$\\widetilde O(1/\\sqrt{n})$ and $\\widetilde O(1/n)$ depending on the power\nconstant in EBC, and could be even faster than $O(1/n)$ in special cases for\nERM. Moreover, these convergence rates are automatically adaptive without using\nany knowledge of EBC. Overall, this work not only strengthens the understanding\nof ERM for statistical learning but also brings new fast stochastic algorithms\nfor solving a broad range of statistical learning problems. \n\n"}
{"id": "1805.05185", "contents": "Title: Generative Adversarial Forests for Better Conditioned Adversarial\n  Learning Abstract: In recent times, many of the breakthroughs in various vision-related tasks\nhave revolved around improving learning of deep models; these methods have\nranged from network architectural improvements such as Residual Networks, to\nvarious forms of regularisation such as Batch Normalisation. In essence, many\nof these techniques revolve around better conditioning, allowing for deeper and\ndeeper models to be successfully learned. In this paper, we look towards better\nconditioning Generative Adversarial Networks (GANs) in an unsupervised learning\nsetting. Our method embeds the powerful discriminating capabilities of a\ndecision forest into the discriminator of a GAN. This results in a better\nconditioned model which learns in an extremely stable way. We demonstrate\nempirical results which show both clear qualitative and quantitative evidence\nof the effectiveness of our approach, gaining significant performance\nimprovements over several popular GAN-based approaches on the Oxford Flowers\nand Aligned Celebrity Faces datasets. \n\n"}
{"id": "1805.05189", "contents": "Title: Randomized Smoothing SVRG for Large-scale Nonsmooth Convex Optimization Abstract: In this paper, we consider the problem of minimizing the average of a large\nnumber of nonsmooth and convex functions. Such problems often arise in typical\nmachine learning problems as empirical risk minimization, but are\ncomputationally very challenging. We develop and analyze a new algorithm that\nachieves robust linear convergence rate, and both its time complexity and\ngradient complexity are superior than state-of-art nonsmooth algorithms and\nsubgradient-based schemes. Besides, our algorithm works without any extra error\nbound conditions on the objective function as well as the common\nstrongly-convex condition. We show that our algorithm has wide applications in\noptimization and machine learning problems, and demonstrate experimentally that\nit performs well on a large-scale ranking problem. \n\n"}
{"id": "1805.05324", "contents": "Title: Extended pipeline for content-based feature engineering in music genre\n  recognition Abstract: We present a feature engineering pipeline for the construction of musical\nsignal characteristics, to be used for the design of a supervised model for\nmusical genre identification. The key idea is to extend the traditional\ntwo-step process of extraction and classification with additive stand-alone\nphases which are no longer organized in a waterfall scheme. The whole system is\nrealized by traversing backtrack arrows and cycles between various stages. In\norder to give a compact and effective representation of the features, the\nstandard early temporal integration is combined with other selection and\nextraction phases: on the one hand, the selection of the most meaningful\ncharacteristics based on information gain, and on the other hand, the inclusion\nof the nonlinear correlation between this subset of features, determined by an\nautoencoder. The results of the experiments conducted on GTZAN dataset reveal a\nnoticeable contribution of this methodology towards the model's performance in\nclassification task. \n\n"}
{"id": "1805.06126", "contents": "Title: Market Self-Learning of Signals, Impact and Optimal Trading: Invisible\n  Hand Inference with Free Energy Abstract: We present a simple model of a non-equilibrium self-organizing market where\nasset prices are partially driven by investment decisions of a bounded-rational\nagent. The agent acts in a stochastic market environment driven by various\nexogenous \"alpha\" signals, agent's own actions (via market impact), and noise.\nUnlike traditional agent-based models, our agent aggregates all traders in the\nmarket, rather than being a representative agent. Therefore, it can be\nidentified with a bounded-rational component of the market itself, providing a\nparticular implementation of an Invisible Hand market mechanism. In such\nsetting, market dynamics are modeled as a fictitious self-play of such\nbounded-rational market-agent in its adversarial stochastic environment. As\nrewards obtained by such self-playing market agent are not observed from market\ndata, we formulate and solve a simple model of such market dynamics based on a\nneuroscience-inspired Bounded Rational Information Theoretic Inverse\nReinforcement Learning (BRIT-IRL). This results in effective asset price\ndynamics with a non-linear mean reversion - which in our model is generated\ndynamically, rather than being postulated. We argue that our model can be used\nin a similar way to the Black-Litterman model. In particular, it represents, in\na simple modeling framework, market views of common predictive signals, market\nimpacts and implied optimal dynamic portfolio allocations, and can be used to\nassess values of private signals. Moreover, it allows one to quantify a\n\"market-implied\" optimal investment strategy, along with a measure of market\nrationality. Our approach is numerically light, and can be implemented using\nstandard off-the-shelf software such as TensorFlow. \n\n"}
{"id": "1805.06299", "contents": "Title: Change Detection in Graph Streams by Learning Graph Embeddings on\n  Constant-Curvature Manifolds Abstract: The space of graphs is often characterised by a non-trivial geometry, which\ncomplicates learning and inference in practical applications. A common approach\nis to use embedding techniques to represent graphs as points in a conventional\nEuclidean space, but non-Euclidean spaces have often been shown to be better\nsuited for embedding graphs. Among these, constant-curvature Riemannian\nmanifolds (CCMs) offer embedding spaces suitable for studying the statistical\nproperties of a graph distribution, as they provide ways to easily compute\nmetric geodesic distances. In this paper, we focus on the problem of detecting\nchanges in stationarity in a stream of attributed graphs. To this end, we\nintroduce a novel change detection framework based on neural networks and CCMs,\nthat takes into account the non-Euclidean nature of graphs. Our contribution in\nthis work is twofold. First, via a novel approach based on adversarial\nlearning, we compute graph embeddings by training an autoencoder to represent\ngraphs on CCMs. Second, we introduce two novel change detection tests operating\non CCMs. We perform experiments on synthetic data, as well as two real-world\napplication scenarios: the detection of epileptic seizures using functional\nconnectivity brain networks, and the detection of hostility between two\nsubjects, using human skeletal graphs. Results show that the proposed methods\nare able to detect even small changes in a graph-generating process,\nconsistently outperforming approaches based on Euclidean embeddings. \n\n"}
{"id": "1805.06576", "contents": "Title: Mad Max: Affine Spline Insights into Deep Learning Abstract: We build a rigorous bridge between deep networks (DNs) and approximation\ntheory via spline functions and operators. Our key result is that a large class\nof DNs can be written as a composition of max-affine spline operators (MASOs),\nwhich provide a powerful portal through which to view and analyze their inner\nworkings. For instance, conditioned on the input signal, the output of a MASO\nDN can be written as a simple affine transformation of the input. This implies\nthat a DN constructs a set of signal-dependent, class-specific templates\nagainst which the signal is compared via a simple inner product; we explore the\nlinks to the classical theory of optimal classification via matched filters and\nthe effects of data memorization. Going further, we propose a simple penalty\nterm that can be added to the cost function of any DN learning algorithm to\nforce the templates to be orthogonal with each other; this leads to\nsignificantly improved classification performance and reduced overfitting with\nno change to the DN architecture. The spline partition of the input signal\nspace that is implicitly induced by a MASO directly links DNs to the theory of\nvector quantization (VQ) and $K$-means clustering, which opens up new geometric\navenue to study how DNs organize signals in a hierarchical fashion. To validate\nthe utility of the VQ interpretation, we develop and validate a new distance\nmetric for signals and images that quantifies the difference between their VQ\nencodings. (This paper is a significantly expanded version of A Spline Theory\nof Deep Learning from ICML 2018.) \n\n"}
{"id": "1805.07010", "contents": "Title: Learning Permutations with Sinkhorn Policy Gradient Abstract: Many problems at the intersection of combinatorics and computer science\nrequire solving for a permutation that optimally matches, ranks, or sorts some\ndata. These problems usually have a task-specific, often non-differentiable\nobjective function that data-driven algorithms can use as a learning signal. In\nthis paper, we propose the Sinkhorn Policy Gradient (SPG) algorithm for\nlearning policies on permutation matrices. The actor-critic neural network\narchitecture we introduce for SPG uniquely decouples representation learning of\nthe state space from the highly-structured action space of permutations with a\ntemperature-controlled Sinkhorn layer. The Sinkhorn layer produces continuous\nrelaxations of permutation matrices so that the actor-critic architecture can\nbe trained end-to-end. Our empirical results show that agents trained with SPG\ncan perform competitively on sorting, the Euclidean TSP, and matching tasks. We\nalso observe that SPG is significantly more data efficient at the matching task\nthan the baseline methods, which indicates that SPG is conducive to learning\nrepresentations that are useful for reasoning about permutations. \n\n"}
{"id": "1805.07075", "contents": "Title: Trusted Neural Networks for Safety-Constrained Autonomous Control Abstract: We propose Trusted Neural Network (TNN) models, which are deep neural network\nmodels that satisfy safety constraints critical to the application domain. We\ninvestigate different mechanisms for incorporating rule-based knowledge in the\nform of first-order logic constraints into a TNN model, where rules that encode\nsafety are accompanied by weights indicating their relative importance. This\nframework allows the TNN model to learn from knowledge available in form of\ndata as well as logical rules. We propose multiple approaches for solving this\nproblem: (a) a multi-headed model structure that allows trade-off between\nsatisfying logical constraints and fitting training data in a unified training\nframework, and (b) creating a constrained optimization problem and solving it\nin dual formulation by posing a new constrained loss function and using a\nproximal gradient descent algorithm. We demonstrate the efficacy of our TNN\nframework through experiments using the open-source TORCS~\\cite{BernhardCAA15}\n3D simulator for self-driving cars. Experiments using our first approach of a\nmulti-headed TNN model, on a dataset generated by a customized version of\nTORCS, show that (1) adding safety constraints to a neural network model\nresults in increased performance and safety, and (2) the improvement increases\nwith increasing importance of the safety constraints. Experiments were also\nperformed using the second approach of proximal algorithm for constrained\noptimization --- they demonstrate how the proposed method ensures that (1) the\noverall TNN model satisfies the constraints even when the training data\nviolates some of the constraints, and (2) the proximal gradient descent\nalgorithm on the constrained objective converges faster than the unconstrained\nversion. \n\n"}
{"id": "1805.07137", "contents": "Title: Knowledge Discovery from Layered Neural Networks based on Non-negative\n  Task Decomposition Abstract: Interpretability has become an important issue in the machine learning field,\nalong with the success of layered neural networks in various practical tasks.\nSince a trained layered neural network consists of a complex nonlinear\nrelationship between large number of parameters, we failed to understand how\nthey could achieve input-output mappings with a given data set. In this paper,\nwe propose the non-negative task decomposition method, which applies\nnon-negative matrix factorization to a trained layered neural network. This\nenables us to decompose the inference mechanism of a trained layered neural\nnetwork into multiple principal tasks of input-output mapping, and reveal the\nroles of hidden units in terms of their contribution to each principal task. \n\n"}
{"id": "1805.07226", "contents": "Title: Sequential Neural Likelihood: Fast Likelihood-free Inference with\n  Autoregressive Flows Abstract: We present Sequential Neural Likelihood (SNL), a new method for Bayesian\ninference in simulator models, where the likelihood is intractable but\nsimulating data from the model is possible. SNL trains an autoregressive flow\non simulated data in order to learn a model of the likelihood in the region of\nhigh posterior density. A sequential training procedure guides simulations and\nreduces simulation cost by orders of magnitude. We show that SNL is more\nrobust, more accurate and requires less tuning than related neural-based\nmethods, and we discuss diagnostics for assessing calibration, convergence and\ngoodness-of-fit. \n\n"}
{"id": "1805.07242", "contents": "Title: Siamese Capsule Networks Abstract: Capsule Networks have shown encouraging results on \\textit{defacto} benchmark\ncomputer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are\nyet to be tested on tasks where (1) the entities detected inherently have more\ncomplex internal representations and (2) there are very few instances per class\nto learn from and (3) where point-wise classification is not suitable. Hence,\nthis paper carries out experiments on face verification in both controlled and\nuncontrolled settings that together address these points. In doing so we\nintroduce \\textit{Siamese Capsule Networks}, a new variant that can be used for\npairwise learning tasks. The model is trained using contrastive loss with\n$\\ell_2$-normalized capsule encoded pose features. We find that \\textit{Siamese\nCapsule Networks} perform well against strong baselines on both pairwise\nlearning datasets, yielding best results in the few-shot learning setting where\nimage pairs in the test set contain unseen subjects. \n\n"}
{"id": "1805.07445", "contents": "Title: DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors Abstract: Boltzmann machines are powerful distributions that have been shown to be an\neffective prior over binary latent variables in variational autoencoders\n(VAEs). However, previous methods for training discrete VAEs have used the\nevidence lower bound and not the tighter importance-weighted bound. We propose\ntwo approaches for relaxing Boltzmann machines to continuous distributions that\npermit training with importance-weighted bounds. These relaxations are based on\ngeneralized overlapping transformations and the Gaussian integral trick.\nExperiments on the MNIST and OMNIGLOT datasets show that these relaxations\noutperform previous discrete VAEs with Boltzmann priors. An implementation\nwhich reproduces these results is available at\nhttps://github.com/QuadrantAI/dvae . \n\n"}
{"id": "1805.07601", "contents": "Title: Deep Generative Markov State Models Abstract: We propose a deep generative Markov State Model (DeepGenMSM) learning\nframework for inference of metastable dynamical systems and prediction of\ntrajectories. After unsupervised training on time series data, the model\ncontains (i) a probabilistic encoder that maps from high-dimensional\nconfiguration space to a small-sized vector indicating the membership to\nmetastable (long-lived) states, (ii) a Markov chain that governs the\ntransitions between metastable states and facilitates analysis of the long-time\ndynamics, and (iii) a generative part that samples the conditional distribution\nof configurations in the next time step. The model can be operated in a\nrecursive fashion to generate trajectories to predict the system evolution from\na defined starting state and propose new configurations. The DeepGenMSM is\ndemonstrated to provide accurate estimates of the long-time kinetics and\ngenerate valid distributions for molecular dynamics (MD) benchmark systems.\nRemarkably, we show that DeepGenMSMs are able to make long time-steps in\nmolecular configuration space and generate physically realistic structures in\nregions that were not seen in training data. \n\n"}
{"id": "1805.07654", "contents": "Title: Sampling-Free Variational Inference of Bayesian Neural Networks by\n  Variance Backpropagation Abstract: We propose a new Bayesian Neural Net formulation that affords variational\ninference for which the evidence lower bound is analytically tractable subject\nto a tight approximation. We achieve this tractability by (i) decomposing ReLU\nnonlinearities into the product of an identity and a Heaviside step function,\n(ii) introducing a separate path that decomposes the neural net expectation\nfrom its variance. We demonstrate formally that introducing separate latent\nbinary variables to the activations allows representing the neural network\nlikelihood as a chain of linear operations. Performing variational inference on\nthis construction enables a sampling-free computation of the evidence lower\nbound which is a more effective approximation than the widely applied Monte\nCarlo sampling and CLT related techniques. We evaluate the model on a range of\nregression and classification tasks against BNN inference alternatives, showing\ncompetitive or improved performance over the current state-of-the-art. \n\n"}
{"id": "1805.08000", "contents": "Title: Adversarial Noise Layer: Regularize Neural Network By Adding Noise Abstract: In this paper, we introduce a novel regularization method called Adversarial\nNoise Layer (ANL) and its efficient version called Class Adversarial Noise\nLayer (CANL), which are able to significantly improve CNN's generalization\nability by adding carefully crafted noise into the intermediate layer\nactivations. ANL and CANL can be easily implemented and integrated with most of\nthe mainstream CNN-based models. We compared the effects of the different types\nof noise and visually demonstrate that our proposed adversarial noise instruct\nCNN models to learn to extract cleaner feature maps, which further reduce the\nrisk of over-fitting. We also conclude that models trained with ANL or CANL are\nmore robust to the adversarial examples generated by FGSM than the traditional\nadversarial training approaches. \n\n"}
{"id": "1805.08102", "contents": "Title: PiPs: a Kernel-based Optimization Scheme for Analyzing Non-Stationary 1D\n  Signals Abstract: This paper proposes a novel kernel-based optimization scheme to handle tasks\nin the analysis, e.g., signal spectral estimation and single-channel source\nseparation of 1D non-stationary oscillatory data. The key insight of our\noptimization scheme for reconstructing the time-frequency information is that\nwhen a nonparametric regression is applied on some input values, the output\nregressed points would lie near the oscillatory pattern of the oscillatory 1D\nsignal only if these input values are a good approximation of the ground-truth\nphase function. In this work, Gaussian Process (GP) is chosen to conduct this\nnonparametric regression: the oscillatory pattern is encoded as the\nPattern-inducing Points (PiPs) which act as the training data points in the GP\nregression; while the targeted phase function is fed in to compute the\ncorrelation kernels, acting as the testing input. Better approximated phase\nfunction generates more precise kernels, thus resulting in smaller optimization\nloss error when comparing the kernel-based regression output with the original\nsignals. To the best of our knowledge, this is the first algorithm that can\nsatisfactorily handle fully non-stationary oscillatory data, close and\ncrossover frequencies, and general oscillatory patterns. Even in the example of\na signal {produced by slow variation in the parameters of a trigonometric\nexpansion}, we show that PiPs admits competitive or better performance in terms\nof accuracy and robustness than existing state-of-the-art algorithms. \n\n"}
{"id": "1805.08114", "contents": "Title: On the Convergence of Stochastic Gradient Descent with Adaptive\n  Stepsizes Abstract: Stochastic gradient descent is the method of choice for large scale\noptimization of machine learning objective functions. Yet, its performance is\ngreatly variable and heavily depends on the choice of the stepsizes. This has\nmotivated a large body of research on adaptive stepsizes. However, there is\ncurrently a gap in our theoretical understanding of these methods, especially\nin the non-convex setting. In this paper, we start closing this gap: we\ntheoretically analyze in the convex and non-convex settings a generalized\nversion of the AdaGrad stepsizes. We show sufficient conditions for these\nstepsizes to achieve almost sure asymptotic convergence of the gradients to\nzero, proving the first guarantee for generalized AdaGrad stepsizes in the\nnon-convex setting. Moreover, we show that these stepsizes allow to\nautomatically adapt to the level of noise of the stochastic gradients in both\nthe convex and non-convex settings, interpolating between $O(1/T)$ and\n$O(1/\\sqrt{T})$, up to logarithmic terms. \n\n"}
{"id": "1805.08578", "contents": "Title: \"Why Should I Trust Interactive Learners?\" Explaining Interactive\n  Queries of Classifiers to Users Abstract: Although interactive learning puts the user into the loop, the learner\nremains mostly a black box for the user. Understanding the reasons behind\nqueries and predictions is important when assessing how the learner works and,\nin turn, trust. Consequently, we propose the novel framework of explanatory\ninteractive learning: in each step, the learner explains its interactive query\nto the user, and she queries of any active classifier for visualizing\nexplanations of the corresponding predictions. We demonstrate that this can\nboost the predictive and explanatory powers of and the trust into the learned\nmodel, using text (e.g. SVMs) and image classification (e.g. neural networks)\nexperiments as well as a user study. \n\n"}
{"id": "1805.08665", "contents": "Title: Structured Bayesian Gaussian process latent variable model Abstract: We introduce a Bayesian Gaussian process latent variable model that\nexplicitly captures spatial correlations in data using a parameterized spatial\nkernel and leveraging structure-exploiting algebra on the model covariance\nmatrices for computational tractability. Inference is made tractable through a\ncollapsed variational bound with similar computational complexity to that of\nthe traditional Bayesian GP-LVM. Inference over partially-observed test cases\nis achieved by optimizing a \"partially-collapsed\" bound. Modeling\nhigh-dimensional time series systems is enabled through use of a dynamical GP\nlatent variable prior. Examples imputing missing data on images and\nsuper-resolution imputation of missing video frames demonstrate the model. \n\n"}
{"id": "1805.09386", "contents": "Title: Predictive Local Smoothness for Stochastic Gradient Methods Abstract: Stochastic gradient methods are dominant in nonconvex optimization especially\nfor deep models but have low asymptotical convergence due to the fixed\nsmoothness. To address this problem, we propose a simple yet effective method\nfor improving stochastic gradient methods named predictive local smoothness\n(PLS). First, we create a convergence condition to build a learning rate which\nvaries adaptively with local smoothness. Second, the local smoothness can be\npredicted by the latest gradients. Third, we use the adaptive learning rate to\nupdate the stochastic gradients for exploring linear convergence rates. By\napplying the PLS method, we implement new variants of three popular algorithms:\nPLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD),\nand PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their\nlinear convergence. Empirical results show that the variants have better\nperformance gains than the popular algorithms, such as, faster convergence and\nalleviating explosion and vanish of gradients. \n\n"}
{"id": "1805.09450", "contents": "Title: Large Data and Zero Noise Limits of Graph-Based Semi-Supervised Learning\n  Algorithms Abstract: Scalings in which the graph Laplacian approaches a differential operator in\nthe large graph limit are used to develop understanding of a number of\nalgorithms for semi-supervised learning; in particular the extension, to this\ngraph setting, of the probit algorithm, level set and kriging methods, are\nstudied. Both optimization and Bayesian approaches are considered, based around\na regularizing quadratic form found from an affine transformation of the\nLaplacian, raised to a, possibly fractional, exponent. Conditions on the\nparameters defining this quadratic form are identified under which well-defined\nlimiting continuum analogues of the optimization and Bayesian semi-supervised\nlearning problems may be found, thereby shedding light on the design of\nalgorithms in the large graph setting. The large graph limits of the\noptimization formulations are tackled through $\\Gamma-$convergence, using the\nrecently introduced $TL^p$ metric. The small labelling noise limits of the\nBayesian formulations are also identified, and contrasted with pre-existing\nharmonic function approaches to the problem. \n\n"}
{"id": "1805.09978", "contents": "Title: Distributed Cartesian Power Graph Segmentation for Graphon Estimation Abstract: We study an extention of total variation denoising over images to over\nCartesian power graphs and its applications to estimating non-parametric\nnetwork models. The power graph fused lasso (PGFL) segments a matrix by\nexploiting a known graphical structure, $G$, over the rows and columns. Our\nmain results shows that for any connected graph, under subGaussian noise, the\nPGFL achieves the same mean-square error rate as 2D total variation denoising\nfor signals of bounded variation. We study the use of the PGFL for denoising an\nobserved network $H$, where we learn the graph $G$ as the $K$-nearest\nneighborhood graph of an estimated metric over the vertices. We provide\ntheoretical and empirical results for estimating graphons, a non-parametric\nexchangeable network model, and compare to the state of the art graphon\nestimation methods. \n\n"}
{"id": "1805.10054", "contents": "Title: Stochastic algorithms with descent guarantees for ICA Abstract: Independent component analysis (ICA) is a widespread data exploration\ntechnique, where observed signals are modeled as linear mixtures of independent\ncomponents. From a machine learning point of view, it amounts to a matrix\nfactorization problem with a statistical independence criterion. Infomax is one\nof the most used ICA algorithms. It is based on a loss function which is a\nnon-convex log-likelihood. We develop a new majorization-minimization framework\nadapted to this loss function. We derive an online algorithm for the streaming\nsetting, and an incremental algorithm for the finite sum setting, with the\nfollowing benefits. First, unlike most algorithms found in the literature, the\nproposed methods do not rely on any critical hyper-parameter like a step size,\nnor do they require a line-search technique. Second, the algorithm for the\nfinite sum setting, although stochastic, guarantees a decrease of the loss\nfunction at each iteration. Experiments demonstrate progress on the\nstate-of-the-art for large scale datasets, without the necessity for any manual\nparameter tuning. \n\n"}
{"id": "1805.10118", "contents": "Title: Analyzing high-dimensional time-series data using kernel transfer\n  operator eigenfunctions Abstract: Kernel transfer operators, which can be regarded as approximations of\ntransfer operators such as the Perron-Frobenius or Koopman operator in\nreproducing kernel Hilbert spaces, are defined in terms of covariance and\ncross-covariance operators and have been shown to be closely related to the\nconditional mean embedding framework developed by the machine learning\ncommunity. The goal of this paper is to show how the dominant eigenfunctions of\nthese operators in combination with gradient-based optimization techniques can\nbe used to detect long-lived coherent patterns in high-dimensional time-series\ndata. The results will be illustrated using video data and a fluid flow\nexample. \n\n"}
{"id": "1805.10140", "contents": "Title: Symmetric and asymmetric discrimination of bosonic loss: Toy\n  applications to biological samples and photo-degradable materials Abstract: We consider quantum discrimination of bosonic loss based on both symmetric\nand asymmetric hypothesis testing. In both approaches, an entangled resource is\nable to outperform any classical strategy based on coherent-state transmitters\nin the regime of low photon numbers. In the symmetric case, we then consider\nthe low energy detection of bacterial growth in culture media. Assuming an\nexponential growth law for the bacterial concentration and the Beer-Lambert law\nfor the optical transmissivity of the sample, we find that the use of\nentanglement allows one to achieve a much faster detection of growth with\nrespect to the use of coherent states. This performance is also studied by\nassuming an exponential photo-degradable model, where the concentration is\nreduced by increasing the number of photons irradiated over the sample. This\ninvestigation is then extended to the readout of classical information from\nsuitably-designed photo-degradable optical memories. \n\n"}
{"id": "1805.10829", "contents": "Title: Sigsoftmax: Reanalysis of the Softmax Bottleneck Abstract: Softmax is an output activation function for modeling categorical probability\ndistributions in many applications of deep learning. However, a recent study\nrevealed that softmax can be a bottleneck of representational capacity of\nneural networks in language modeling (the softmax bottleneck). In this paper,\nwe propose an output activation function for breaking the softmax bottleneck\nwithout additional parameters. We re-analyze the softmax bottleneck from the\nperspective of the output set of log-softmax and identify the cause of the\nsoftmax bottleneck. On the basis of this analysis, we propose sigsoftmax, which\nis composed of a multiplication of an exponential function and sigmoid\nfunction. Sigsoftmax can break the softmax bottleneck. The experiments on\nlanguage modeling demonstrate that sigsoftmax and mixture of sigsoftmax\noutperform softmax and mixture of softmax, respectively. \n\n"}
{"id": "1805.10833", "contents": "Title: Bayesian Learning with Wasserstein Barycenters Abstract: We introduce and study a novel model-selection strategy for Bayesian\nlearning, based on optimal transport, along with its associated predictive\nposterior law: the Wasserstein population barycenter of the posterior law over\nmodels. We first show how this estimator, termed Bayesian Wasserstein\nbarycenter (BWB), arises naturally in a general, parameter-free Bayesian\nmodel-selection framework, when the considered Bayesian risk is the Wasserstein\ndistance. Examples are given, illustrating how the BWB extends some classic\nparametric and non-parametric selection strategies. Furthermore, we also\nprovide explicit conditions granting the existence and statistical consistency\nof the BWB, and discuss some of its general and specific properties, providing\ninsights into its advantages compared to usual choices, such as the model\naverage estimator. Finally, we illustrate how this estimator can be computed\nusing the stochastic gradient descent (SGD) algorithm in Wasserstein space\nintroduced in a companion paper arXiv:2201.04232v2 [math.OC], and provide a\nnumerical example for experimental validation of the proposed method. \n\n"}
{"id": "1805.11074", "contents": "Title: Reward Constrained Policy Optimization Abstract: Solving tasks in Reinforcement Learning is no easy feat. As the goal of the\nagent is to maximize the accumulated reward, it often learns to exploit\nloopholes and misspecifications in the reward signal resulting in unwanted\nbehavior. While constraints may solve this issue, there is no closed form\nsolution for general constraints. In this work we present a novel\nmulti-timescale approach for constrained policy optimization, called `Reward\nConstrained Policy Optimization' (RCPO), which uses an alternative penalty\nsignal to guide the policy towards a constraint satisfying one. We prove the\nconvergence of our approach and provide empirical evidence of its ability to\ntrain constraint satisfying policies. \n\n"}
{"id": "1805.11494", "contents": "Title: Efficient Bayesian Inference for a Gaussian Process Density Model Abstract: We reconsider a nonparametric density model based on Gaussian processes. By\naugmenting the model with latent P\\'olya--Gamma random variables and a latent\nmarked Poisson process we obtain a new likelihood which is conjugate to the\nmodel's Gaussian process prior. The augmented posterior allows for efficient\ninference by Gibbs sampling and an approximate variational mean field approach.\nFor the latter we utilise sparse GP approximations to tackle the infinite\ndimensionality of the problem. The performance of both algorithms and\ncomparisons with other density estimators are demonstrated on artificial and\nreal datasets with up to several thousand data points. \n\n"}
{"id": "1805.11571", "contents": "Title: Human-in-the-Loop Interpretability Prior Abstract: We often desire our models to be interpretable as well as accurate. Prior\nwork on optimizing models for interpretability has relied on easy-to-quantify\nproxies for interpretability, such as sparsity or the number of operations\nrequired. In this work, we optimize for interpretability by directly including\nhumans in the optimization loop. We develop an algorithm that minimizes the\nnumber of user studies to find models that are both predictive and\ninterpretable and demonstrate our approach on several data sets. Our human\nsubjects results show trends towards different proxy notions of\ninterpretability on different datasets, which suggests that different proxies\nare preferred on different tasks. \n\n"}
{"id": "1805.11703", "contents": "Title: Biologically Motivated Algorithms for Propagating Local Target\n  Representations Abstract: Finding biologically plausible alternatives to back-propagation of errors is\na fundamentally important challenge in artificial neural network research. In\nthis paper, we propose a learning algorithm called error-driven Local\nRepresentation Alignment (LRA-E), which has strong connections to predictive\ncoding, a theory that offers a mechanistic way of describing neurocomputational\nmachinery. In addition, we propose an improved variant of Difference Target\nPropagation, another procedure that comes from the same family of algorithms as\nLRA-E. We compare our procedures to several other biologically-motivated\nalgorithms, including two feedback alignment algorithms and Equilibrium\nPropagation. In two benchmarks, we find that both of our proposed algorithms\nyield stable performance and strong generalization compared to other competing\nback-propagation alternatives when training deeper, highly nonlinear networks,\nwith LRA-E performing the best overall. \n\n"}
{"id": "1805.11754", "contents": "Title: Optimal Testing in the Experiment-rich Regime Abstract: Motivated by the widespread adoption of large-scale A/B testing in industry,\nwe propose a new experimentation framework for the setting where potential\nexperiments are abundant (i.e., many hypotheses are available to test), and\nobservations are costly; we refer to this as the experiment-rich regime. Such\nscenarios require the experimenter to internalize the opportunity cost of\nassigning a sample to a particular experiment. We fully characterize the\noptimal policy and give an algorithm to compute it. Furthermore, we develop a\nsimple heuristic that also provides intuition for the optimal policy. We use\nsimulations based on real data to compare both the optimal algorithm and the\nheuristic to other natural alternative experimental design frameworks. In\nparticular, we discuss the paradox of power: high-powered classical tests can\nlead to highly inefficient sampling in the experiment-rich regime. \n\n"}
{"id": "1805.11811", "contents": "Title: Stochastic Zeroth-order Optimization via Variance Reduction method Abstract: Derivative-free optimization has become an important technique used in\nmachine learning for optimizing black-box models. To conduct updates without\nexplicitly computing gradient, most current approaches iteratively sample a\nrandom search direction from Gaussian distribution and compute the estimated\ngradient along that direction. However, due to the variance in the search\ndirection, the convergence rates and query complexities of existing methods\nsuffer from a factor of $d$, where $d$ is the problem dimension. In this paper,\nwe introduce a novel Stochastic Zeroth-order method with Variance Reduction\nunder Gaussian smoothing (SZVR-G) and establish the complexity for optimizing\nnon-convex problems. With variance reduction on both sample space and search\nspace, the complexity of our algorithm is sublinear to $d$ and is strictly\nbetter than current approaches, in both smooth and non-smooth cases. Moreover,\nwe extend the proposed method to the mini-batch version. Our experimental\nresults demonstrate the superior performance of the proposed method over\nexisting derivative-free optimization techniques. Furthermore, we successfully\napply our method to conduct a universal black-box attack to deep neural\nnetworks and present some interesting results. \n\n"}
{"id": "1805.11917", "contents": "Title: The Dynamics of Learning: A Random Matrix Approach Abstract: Understanding the learning dynamics of neural networks is one of the key\nissues for the improvement of optimization algorithms as well as for the\ntheoretical comprehension of why deep neural nets work so well today. In this\npaper, we introduce a random matrix-based framework to analyze the learning\ndynamics of a single-layer linear network on a binary classification problem,\nfor data of simultaneously large dimension and size, trained by gradient\ndescent. Our results provide rich insights into common questions in neural\nnets, such as overfitting, early stopping and the initialization of training,\nthereby opening the door for future studies of more elaborate structures and\nmodels appearing in today's neural networks. \n\n"}
{"id": "1805.12002", "contents": "Title: Why Is My Classifier Discriminatory? Abstract: Recent attempts to achieve fairness in predictive models focus on the balance\nbetween fairness and accuracy. In sensitive applications such as healthcare or\ncriminal justice, this trade-off is often undesirable as any increase in\nprediction error could have devastating consequences. In this work, we argue\nthat the fairness of predictions should be evaluated in context of the data,\nand that unfairness induced by inadequate samples sizes or unmeasured\npredictive variables should be addressed through data collection, rather than\nby constraining the model. We decompose cost-based metrics of discrimination\ninto bias, variance, and noise, and propose actions aimed at estimating and\nreducing each term. Finally, we perform case-studies on prediction of income,\nmortality, and review ratings, confirming the value of this analysis. We find\nthat data collection is often a means to reduce discrimination without\nsacrificing accuracy. \n\n"}
{"id": "1805.12227", "contents": "Title: Coherent control and wave mixing in an ensemble of silicon vacancy\n  centers in diamond Abstract: Strong light-matter interactions are critical for quantum technologies based\non light, such as memories or nonlinear interactions. Solid state materials\nwill be particularly important for such applications, because of the relative\nease of fabrication of components. Silicon vacancy centers (SiV) in diamond\nfeature especially narrow inhomogeneous spectral lines, which are rare in solid\nmaterials. Here, we demonstrate resonant coherent manipulation, stimulated\nRaman adiabatic passage, and strong light-matter interaction via four-wave\nmixing of a weak signal field in an ensemble of SiV centers. \n\n"}
{"id": "1806.00047", "contents": "Title: Following High-level Navigation Instructions on a Simulated Quadcopter\n  with Imitation Learning Abstract: We introduce a method for following high-level navigation instructions by\nmapping directly from images, instructions and pose estimates to continuous\nlow-level velocity commands for real-time control. The Grounded Semantic\nMapping Network (GSMN) is a fully-differentiable neural network architecture\nthat builds an explicit semantic map in the world reference frame by\nincorporating a pinhole camera projection model within the network. The\ninformation stored in the map is learned from experience, while the\nlocal-to-world transformation is computed explicitly. We train the model using\nDAggerFM, a modified variant of DAgger that trades tabular convergence\nguarantees for improved training speed and memory use. We test GSMN in virtual\nenvironments on a realistic quadcopter simulator and show that incorporating an\nexplicit mapping and grounding modules allows GSMN to outperform strong neural\nbaselines and almost reach an expert policy performance. Finally, we analyze\nthe learned map representations and show that using an explicit map leads to an\ninterpretable instruction-following model. \n\n"}
{"id": "1806.00069", "contents": "Title: Explaining Explanations: An Overview of Interpretability of Machine\n  Learning Abstract: There has recently been a surge of work in explanatory artificial\nintelligence (XAI). This research area tackles the important problem that\ncomplex machines and algorithms often cannot provide insights into their\nbehavior and thought processes. XAI allows users and parts of the internal\nsystem to be more transparent, providing explanations of their decisions in\nsome level of detail. These explanations are important to ensure algorithmic\nfairness, identify potential bias/problems in the training data, and to ensure\nthat the algorithms perform as expected. However, explanations produced by\nthese systems is neither standardized nor systematically assessed. In an effort\nto create best practices and identify open challenges, we provide our\ndefinition of explainability and show how it can be used to classify existing\nliterature. We discuss why current approaches to explanatory methods especially\nfor deep neural networks are insufficient. Finally, based on our survey, we\nconclude with suggested future research directions for explanatory artificial\nintelligence. \n\n"}
{"id": "1806.00271", "contents": "Title: Generative Modeling by Inclusive Neural Random Fields with Applications\n  in Image Generation and Anomaly Detection Abstract: Neural random fields (NRFs), referring to a class of generative models that\nuse neural networks to implement potential functions in random fields (a.k.a.\nenergy-based models), are not new but receive less attention with slow\nprogress. Different from various directed graphical models such as generative\nadversarial networks (GANs), NRFs provide an interesting family of undirected\ngraphical models for generative modeling. In this paper we propose a new\napproach, the inclusive-NRF approach, to learning NRFs for continuous data\n(e.g. images), by introducing inclusive-divergence minimized auxiliary\ngenerators and developing stochastic gradient sampling in an augmented space.\nBased on the new approach, specific inclusive-NRF models are developed and\nthoroughly evaluated in two important generative modeling applications - image\ngeneration and anomaly detection. The proposed models consistently improve over\nstate-of-the-art results in both applications. Remarkably, in addition to\nsuperior sample generation, one additional benefit of our inclusive-NRF\napproach is that, unlike GANs, it can directly provide (unnormalized) density\nestimate for sample evaluation. With these contributions and results, this\npaper significantly advances the learning and applications of NRFs to a new\nlevel, both theoretically and empirically, which have never been obtained\nbefore. \n\n"}
{"id": "1806.00572", "contents": "Title: Autoencoders Learn Generative Linear Models Abstract: We provide a series of results for unsupervised learning with autoencoders.\nSpecifically, we study shallow two-layer autoencoder architectures with shared\nweights. We focus on three generative models for data that are common in\nstatistical machine learning: (i) the mixture-of-gaussians model, (ii) the\nsparse coding model, and (iii) the sparsity model with non-negative\ncoefficients. For each of these models, we prove that under suitable choices of\nhyperparameters, architectures, and initialization, autoencoders learned by\ngradient descent can successfully recover the parameters of the corresponding\nmodel. To our knowledge, this is the first result that rigorously studies the\ndynamics of gradient descent for weight-sharing autoencoders. Our analysis can\nbe viewed as theoretical evidence that shallow autoencoder modules indeed can\nbe used as feature learning mechanisms for a variety of data models, and may\nshed insight on how to train larger stacked architectures with autoencoders as\nbasic building blocks. \n\n"}
{"id": "1806.00640", "contents": "Title: Binary Classification with Karmic, Threshold-Quasi-Concave Metrics Abstract: Complex performance measures, beyond the popular measure of accuracy, are\nincreasingly being used in the context of binary classification. These complex\nperformance measures are typically not even decomposable, that is, the loss\nevaluated on a batch of samples cannot typically be expressed as a sum or\naverage of losses evaluated at individual samples, which in turn requires new\ntheoretical and methodological developments beyond standard treatments of\nsupervised learning. In this paper, we advance this understanding of binary\nclassification for complex performance measures by identifying two key\nproperties: a so-called Karmic property, and a more technical\nthreshold-quasi-concavity property, which we show is milder than existing\nstructural assumptions imposed on performance measures. Under these properties,\nwe show that the Bayes optimal classifier is a threshold function of the\nconditional probability of positive class. We then leverage this result to come\nup with a computationally practical plug-in classifier, via a novel threshold\nestimator, and further, provide a novel statistical analysis of classification\nerror with respect to complex performance measures. \n\n"}
{"id": "1806.01468", "contents": "Title: Understanding Regularized Spectral Clustering via Graph Conductance Abstract: This paper uses the relationship between graph conductance and spectral\nclustering to study (i) the failures of spectral clustering and (ii) the\nbenefits of regularization. The explanation is simple. Sparse and stochastic\ngraphs create a lot of small trees that are connected to the core of the graph\nby only one edge. Graph conductance is sensitive to these noisy `dangling\nsets'. Spectral clustering inherits this sensitivity. The second part of the\npaper starts from a previously proposed form of regularized spectral clustering\nand shows that it is related to the graph conductance on a `regularized graph'.\nWe call the conductance on the regularized graph CoreCut. Based upon previous\narguments that relate graph conductance to spectral clustering (e.g. Cheeger\ninequality), minimizing CoreCut relaxes to regularized spectral clustering.\nSimple inspection of CoreCut reveals why it is less sensitive to small cuts in\nthe graph. Together, these results show that unbalanced partitions from\nspectral clustering can be understood as overfitting to noise in the periphery\nof a sparse and stochastic graph. Regularization fixes this overfitting. In\naddition to this statistical benefit, these results also demonstrate how\nregularization can improve the computational speed of spectral clustering. We\nprovide simulations and data examples to illustrate these results. \n\n"}
{"id": "1806.01933", "contents": "Title: Explainable Neural Networks based on Additive Index Models Abstract: Machine Learning algorithms are increasingly being used in recent years due\nto their flexibility in model fitting and increased predictive performance.\nHowever, the complexity of the models makes them hard for the data analyst to\ninterpret the results and explain them without additional tools. This has led\nto much research in developing various approaches to understand the model\nbehavior. In this paper, we present the Explainable Neural Network (xNN), a\nstructured neural network designed especially to learn interpretable features.\nUnlike fully connected neural networks, the features engineered by the xNN can\nbe extracted from the network in a relatively straightforward manner and the\nresults displayed. With appropriate regularization, the xNN provides a\nparsimonious explanation of the relationship between the features and the\noutput. We illustrate this interpretable feature--engineering property on\nsimulated examples. \n\n"}
{"id": "1806.02078", "contents": "Title: Convolutional Sequence to Sequence Non-intrusive Load Monitoring Abstract: A convolutional sequence to sequence non-intrusive load monitoring model is\nproposed in this paper. Gated linear unit convolutional layers are used to\nextract information from the sequences of aggregate electricity consumption.\nResidual blocks are also introduced to refine the output of the neural network.\nThe partially overlapped output sequences of the network are averaged to\nproduce the final output of the model. We apply the proposed model to the REDD\ndataset and compare it with the convolutional sequence to point model in the\nliterature. Results show that the proposed model is able to give satisfactory\ndisaggregation performance for appliances with varied characteristics. \n\n"}
{"id": "1806.02136", "contents": "Title: Efficient Differentiable Programming in a Functional Array-Processing\n  Language Abstract: We present a system for the automatic differentiation of a higher-order\nfunctional array-processing language. The core functional language underlying\nthis system simultaneously supports both source-to-source automatic\ndifferentiation and global optimizations such as loop transformations. Thanks\nto this feature, we demonstrate how for some real-world machine learning and\ncomputer vision benchmarks, the system outperforms the state-of-the-art\nautomatic differentiation tools. \n\n"}
{"id": "1806.02501", "contents": "Title: Simplifying Reward Design through Divide-and-Conquer Abstract: Designing a good reward function is essential to robot planning and\nreinforcement learning, but it can also be challenging and frustrating. The\nreward needs to work across multiple different environments, and that often\nrequires many iterations of tuning. We introduce a novel divide-and-conquer\napproach that enables the designer to specify a reward separately for each\nenvironment. By treating these separate reward functions as observations about\nthe underlying true reward, we derive an approach to infer a common reward\nacross all environments. We conduct user studies in an abstract grid world\ndomain and in a motion planning domain for a 7-DOF manipulator that measure\nuser effort and solution quality. We show that our method is faster, easier to\nuse, and produces a higher quality solution than the typical method of\ndesigning a reward jointly across all environments. We additionally conduct a\nseries of experiments that measure the sensitivity of these results to\ndifferent properties of the reward design task, such as the number of\nenvironments, the number of feasible solutions per environment, and the\nfraction of the total features that vary within each environment. We find that\nindependent reward design outperforms the standard, joint, reward design\nprocess but works best when the design problem can be divided into simpler\nsubproblems. \n\n"}
{"id": "1806.02922", "contents": "Title: Feature selection in functional data classification with recursive\n  maxima hunting Abstract: Dimensionality reduction is one of the key issues in the design of effective\nmachine learning methods for automatic induction. In this work, we introduce\nrecursive maxima hunting (RMH) for variable selection in classification\nproblems with functional data. In this context, variable selection techniques\nare especially attractive because they reduce the dimensionality, facilitate\nthe interpretation and can improve the accuracy of the predictive models. The\nmethod, which is a recursive extension of maxima hunting (MH), performs\nvariable selection by identifying the maxima of a relevance function, which\nmeasures the strength of the correlation of the predictor functional variable\nwith the class label. At each stage, the information associated with the\nselected variable is removed by subtracting the conditional expectation of the\nprocess. The results of an extensive empirical evaluation are used to\nillustrate that, in the problems investigated, RMH has comparable or higher\npredictive accuracy than the standard dimensionality reduction techniques, such\nas PCA and PLS, and state-of-the-art feature selection methods for functional\ndata, such as maxima hunting. \n\n"}
{"id": "1806.03085", "contents": "Title: A Stein variational Newton method Abstract: Stein variational gradient descent (SVGD) was recently proposed as a general\npurpose nonparametric variational inference algorithm [Liu & Wang, NIPS 2016]:\nit minimizes the Kullback-Leibler divergence between the target distribution\nand its approximation by implementing a form of functional gradient descent on\na reproducing kernel Hilbert space. In this paper, we accelerate and generalize\nthe SVGD algorithm by including second-order information, thereby approximating\na Newton-like iteration in function space. We also show how second-order\ninformation can lead to more effective choices of kernel. We observe\nsignificant computational gains over the original SVGD algorithm in multiple\ntest cases. \n\n"}
{"id": "1806.03146", "contents": "Title: Neural Message Passing with Edge Updates for Predicting Properties of\n  Molecules and Materials Abstract: Neural message passing on molecular graphs is one of the most promising\nmethods for predicting formation energy and other properties of molecules and\nmaterials. In this work we extend the neural message passing model with an edge\nupdate network which allows the information exchanged between atoms to depend\non the hidden state of the receiving atom. We benchmark the proposed model on\nthree publicly available datasets (QM9, The Materials Project and OQMD) and\nshow that the proposed model yields superior prediction of formation energies\nand other properties on all three datasets in comparison with the best\npublished results. Furthermore we investigate different methods for\nconstructing the graph used to represent crystalline structures and we find\nthat using a graph based on K-nearest neighbors achieves better prediction\naccuracy than using maximum distance cutoff or the Voronoi tessellation graph. \n\n"}
{"id": "1806.03207", "contents": "Title: Learning in Integer Latent Variable Models with Nested Automatic\n  Differentiation Abstract: We develop nested automatic differentiation (AD) algorithms for exact\ninference and learning in integer latent variable models. Recently, Winner,\nSujono, and Sheldon showed how to reduce marginalization in a class of integer\nlatent variable models to evaluating a probability generating function which\ncontains many levels of nested high-order derivatives. We contribute faster and\nmore stable AD algorithms for this challenging problem and a novel algorithm to\ncompute exact gradients for learning. These contributions lead to significantly\nfaster and more accurate learning algorithms, and are the first AD algorithms\nwhose running time is polynomial in the number of levels of nesting. \n\n"}
{"id": "1806.03432", "contents": "Title: Hierarchical Clustering with Prior Knowledge Abstract: Hierarchical clustering is a class of algorithms that seeks to build a\nhierarchy of clusters. It has been the dominant approach to constructing\nembedded classification schemes since it outputs dendrograms, which capture the\nhierarchical relationship among members at all levels of granularity,\nsimultaneously. Being greedy in the algorithmic sense, a hierarchical\nclustering partitions data at every step solely based on a similarity /\ndissimilarity measure. The clustering results oftentimes depend on not only the\ndistribution of the underlying data, but also the choice of dissimilarity\nmeasure and the clustering algorithm. In this paper, we propose a method to\nincorporate prior domain knowledge about entity relationship into the\nhierarchical clustering. Specifically, we use a distance function in\nultrametric space to encode the external ontological information. We show that\npopular linkage-based algorithms can faithfully recover the encoded structure.\nSimilar to some regularized machine learning techniques, we add this distance\nas a penalty term to the original pairwise distance to regulate the final\nstructure of the dendrogram. As a case study, we applied this method on real\ndata in the building of a customer behavior based product taxonomy for an\nAmazon service, leveraging the information from a larger Amazon-wide browse\nstructure. The method is useful when one wants to leverage the relational\ninformation from external sources, or the data used to generate the distance\nmatrix is noisy and sparse. Our work falls in the category of semi-supervised\nor constrained clustering. \n\n"}
{"id": "1806.03664", "contents": "Title: Conditional Noise-Contrastive Estimation of Unnormalised Models Abstract: Many parametric statistical models are not properly normalised and only\nspecified up to an intractable partition function, which renders parameter\nestimation difficult. Examples of unnormalised models are Gibbs distributions,\nMarkov random fields, and neural network models in unsupervised deep learning.\nIn previous work, the estimation principle called noise-contrastive estimation\n(NCE) was introduced where unnormalised models are estimated by learning to\ndistinguish between data and auxiliary noise. An open question is how to best\nchoose the auxiliary noise distribution. We here propose a new method that\naddresses this issue. The proposed method shares with NCE the idea of\nformulating density estimation as a supervised learning problem but in contrast\nto NCE, the proposed method leverages the observed data when generating noise\nsamples. The noise can thus be generated in a semi-automated manner. We first\npresent the underlying theory of the new method, show that score matching\nemerges as a limiting case, validate the method on continuous and discrete\nvalued synthetic data, and show that we can expect an improved performance\ncompared to NCE when the data lie in a lower-dimensional manifold. Then we\ndemonstrate its applicability in unsupervised deep learning by estimating a\nfour-layer neural image model. \n\n"}
{"id": "1806.03763", "contents": "Title: Smoothed analysis of the low-rank approach for smooth semidefinite\n  programs Abstract: We consider semidefinite programs (SDPs) of size n with equality constraints.\nIn order to overcome scalability issues, Burer and Monteiro proposed a\nfactorized approach based on optimizing over a matrix Y of size $n$ by $k$ such\nthat $X = YY^*$ is the SDP variable. The advantages of such formulation are\ntwofold: the dimension of the optimization variable is reduced and positive\nsemidefiniteness is naturally enforced. However, the problem in Y is\nnon-convex. In prior work, it has been shown that, when the constraints on the\nfactorized variable regularly define a smooth manifold, provided k is large\nenough, for almost all cost matrices, all second-order stationary points\n(SOSPs) are optimal. Importantly, in practice, one can only compute points\nwhich approximately satisfy necessary optimality conditions, leading to the\nquestion: are such points also approximately optimal? To this end, and under\nsimilar assumptions, we use smoothed analysis to show that approximate SOSPs\nfor a randomly perturbed objective function are approximate global optima, with\nk scaling like the square root of the number of constraints (up to log\nfactors). Moreover, we bound the optimality gap at the approximate solution of\nthe perturbed problem with respect to the original problem. We particularize\nour results to an SDP relaxation of phase retrieval. \n\n"}
{"id": "1806.03968", "contents": "Title: CapsGAN: Using Dynamic Routing for Generative Adversarial Networks Abstract: In this paper, we propose a novel technique for generating images in the 3D\ndomain from images with high degree of geometrical transformations. By\ncoalescing two popular concurrent methods that have seen rapid ascension to the\nmachine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and\nCapsule networks (Sabour, Hinton et. al.) - we present: \\textbf{CapsGAN}. We\nshow that CapsGAN performs better than or equal to traditional CNN based GANs\nin generating images with high geometric transformations using rotated MNIST.\nIn the process, we also show the efficacy of using capsules architecture in the\nGANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the\nperformance control and training stability by experimenting with using\nWasserstein distance (gradient clipping, penalty) and Spectral Normalization.\nThe experimental findings of this paper should propel the application of\ncapsules and GANs in the still exciting and nascent domain of 3D image\ngeneration, and plausibly video (frame) generation. \n\n"}
{"id": "1806.04398", "contents": "Title: Attentive cross-modal paratope prediction Abstract: Antibodies are a critical part of the immune system, having the function of\ndirectly neutralising or tagging undesirable objects (the antigens) for future\ndestruction. Being able to predict which amino acids belong to the paratope,\nthe region on the antibody which binds to the antigen, can facilitate antibody\ndesign and contribute to the development of personalised medicine. The\nsuitability of deep neural networks has recently been confirmed for this task,\nwith Parapred outperforming all prior physical models. Our contribution is\ntwofold: first, we significantly outperform the computational efficiency of\nParapred by leveraging \\`a trous convolutions and self-attention. Secondly, we\nimplement cross-modal attention by allowing the antibody residues to attend\nover antigen residues. This leads to new state-of-the-art results on this task,\nalong with insightful interpretations. \n\n"}
{"id": "1806.04418", "contents": "Title: Quaternion Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are powerful architectures to model\nsequential data, due to their capability to learn short and long-term\ndependencies between the basic elements of a sequence. Nonetheless, popular\ntasks such as speech or images recognition, involve multi-dimensional input\nfeatures that are characterized by strong internal dependencies between the\ndimensions of the input vector. We propose a novel quaternion recurrent neural\nnetwork (QRNN), alongside with a quaternion long-short term memory neural\nnetwork (QLSTM), that take into account both the external relations and these\ninternal structural dependencies with the quaternion algebra. Similarly to\ncapsules, quaternions allow the QRNN to code internal dependencies by composing\nand processing multidimensional features as single entities, while the\nrecurrent operation reveals correlations between the elements composing the\nsequence. We show that both QRNN and QLSTM achieve better performances than RNN\nand LSTM in a realistic application of automatic speech recognition. Finally,\nwe show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of\nfree parameters needed, compared to real-valued RNNs and LSTMs to reach better\nresults, leading to a more compact representation of the relevant information. \n\n"}
{"id": "1806.04465", "contents": "Title: Gaussian mixture models with Wasserstein distance Abstract: Generative models with both discrete and continuous latent variables are\nhighly motivated by the structure of many real-world data sets. They present,\nhowever, subtleties in training often manifesting in the discrete latent being\nunder leveraged. In this paper, we show that such models are more amenable to\ntraining when using the Optimal Transport framework of Wasserstein\nAutoencoders. We find our discrete latent variable to be fully leveraged by the\nmodel when trained, without any modifications to the objective function or\nsignificant fine tuning. Our model generates comparable samples to other\napproaches while using relatively simple neural networks, since the discrete\nlatent variable carries much of the descriptive burden. Furthermore, the\ndiscrete latent provides significant control over generation. \n\n"}
{"id": "1806.04743", "contents": "Title: INFERNO: Inference-Aware Neural Optimisation Abstract: Complex computer simulations are commonly required for accurate data\nmodelling in many scientific disciplines, making statistical inference\nchallenging due to the intractability of the likelihood evaluation for the\nobserved data. Furthermore, sometimes one is interested on inference drawn over\na subset of the generative model parameters while taking into account model\nuncertainty or misspecification on the remaining nuisance parameters. In this\nwork, we show how non-linear summary statistics can be constructed by\nminimising inference-motivated losses via stochastic gradient descent such they\nprovided the smallest uncertainty for the parameters of interest. As a use\ncase, the problem of confidence interval estimation for the mixture coefficient\nin a multi-dimensional two-component mixture model (i.e. signal vs background)\nis considered, where the proposed technique clearly outperforms summary\nstatistics based on probabilistic classification, which are a commonly used\nalternative but do not account for the presence of nuisance parameters. \n\n"}
{"id": "1806.04965", "contents": "Title: The streaming rollout of deep networks - towards fully model-parallel\n  execution Abstract: Deep neural networks, and in particular recurrent networks, are promising\ncandidates to control autonomous agents that interact in real-time with the\nphysical world. However, this requires a seamless integration of temporal\nfeatures into the network's architecture. For the training of and inference\nwith recurrent neural networks, they are usually rolled out over time, and\ndifferent rollouts exist. Conventionally during inference, the layers of a\nnetwork are computed in a sequential manner resulting in sparse temporal\nintegration of information and long response times. In this study, we present a\ntheoretical framework to describe rollouts, the level of model-parallelization\nthey induce, and demonstrate differences in solving specific tasks. We prove\nthat certain rollouts, also for networks with only skip and no recurrent\nconnections, enable earlier and more frequent responses, and show empirically\nthat these early responses have better performance. The streaming rollout\nmaximizes these properties and enables a fully parallel execution of the\nnetwork reducing runtime on massively parallel devices. Finally, we provide an\nopen-source toolbox to design, train, evaluate, and interact with streaming\nrollouts. \n\n"}
{"id": "1806.04994", "contents": "Title: Only Bayes should learn a manifold (on the estimation of differential\n  geometric structure from data) Abstract: We investigate learning of the differential geometric structure of a data\nmanifold embedded in a high-dimensional Euclidean space. We first analyze\nkernel-based algorithms and show that under the usual regularizations,\nnon-probabilistic methods cannot recover the differential geometric structure,\nbut instead find mostly linear manifolds or spaces equipped with teleports. To\nproperly learn the differential geometric structure, non-probabilistic methods\nmust apply regularizations that enforce large gradients, which go against\ncommon wisdom. We repeat the analysis for probabilistic methods and find that\nunder reasonable priors, the geometric structure can be recovered. Fully\nexploiting the recovered structure, however, requires the development of\nstochastic extensions to classic Riemannian geometry. We take early steps in\nthat regard. Finally, we partly extend the analysis to modern models based on\nneural networks, thereby highlighting geometric and probabilistic shortcomings\nof current deep generative models. \n\n"}
{"id": "1806.05403", "contents": "Title: On the Perceptron's Compression Abstract: We study and provide exposition to several phenomena that are related to the\nperceptron's compression. One theme concerns modifications of the perceptron\nalgorithm that yield better guarantees on the margin of the hyperplane it\noutputs. These modifications can be useful in training neural networks as well,\nand we demonstrate them with some experimental data. In a second theme, we\ndeduce conclusions from the perceptron's compression in various contexts. \n\n"}
{"id": "1806.05413", "contents": "Title: Learning Dynamics of Linear Denoising Autoencoders Abstract: Denoising autoencoders (DAEs) have proven useful for unsupervised\nrepresentation learning, but a thorough theoretical understanding is still\nlacking of how the input noise influences learning. Here we develop theory for\nhow noise influences learning in DAEs. By focusing on linear DAEs, we are able\nto derive analytic expressions that exactly describe their learning dynamics.\nWe verify our theoretical predictions with simulations as well as experiments\non MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise\nallows DAEs to ignore low variance directions in the inputs while learning to\nreconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs\nto standard regularised autoencoders, we show that noise has a similar\nregularisation effect to weight decay, but with faster training dynamics. We\nalso show that our theoretical predictions approximate learning dynamics on\nreal-world data and qualitatively match observed dynamics in nonlinear DAEs. \n\n"}
{"id": "1806.05490", "contents": "Title: Inference in Deep Gaussian Processes using Stochastic Gradient\n  Hamiltonian Monte Carlo Abstract: Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian\nProcesses that combine well calibrated uncertainty estimates with the high\nflexibility of multilayer models. One of the biggest challenges with these\nmodels is that exact inference is intractable. The current state-of-the-art\ninference method, Variational Inference (VI), employs a Gaussian approximation\nto the posterior distribution. This can be a potentially poor unimodal\napproximation of the generally multimodal posterior. In this work, we provide\nevidence for the non-Gaussian nature of the posterior and we apply the\nStochastic Gradient Hamiltonian Monte Carlo method to generate samples. To\nefficiently optimize the hyperparameters, we introduce the Moving Window MCEM\nalgorithm. This results in significantly better predictions at a lower\ncomputational cost than its VI counterpart. Thus our method establishes a new\nstate-of-the-art for inference in DGPs. \n\n"}
{"id": "1806.05514", "contents": "Title: The Exact Equivalence of Distance and Kernel Methods for Hypothesis\n  Testing Abstract: Distance-based tests, also called \"energy statistics\", are leading methods\nfor two-sample and independence tests from the statistics community.\nKernel-based tests, developed from \"kernel mean embeddings\", are leading\nmethods for two-sample and independence tests from the machine learning\ncommunity. A fixed-point transformation was previously proposed to connect the\ndistance methods and kernel methods for the population statistics. In this\npaper, we propose a new bijective transformation between metrics and kernels.\nIt simplifies the fixed-point transformation, inherits similar theoretical\nproperties, allows distance methods to be exactly the same as kernel methods\nfor sample statistics and p-value, and better preserves the data structure upon\ntransformation. Our results further advance the understanding in distance and\nkernel-based tests, streamline the code base for implementing these tests, and\nenable a rich literature of distance-based and kernel-based methodologies to\ndirectly communicate with each other. \n\n"}
{"id": "1806.05730", "contents": "Title: Learning Influence-Receptivity Network Structure with Guarantee Abstract: Traditional works on community detection from observations of information\ncascade assume that a single adjacency matrix parametrizes all the observed\ncascades. However, in reality the connection structure usually does not stay\nthe same across cascades. For example, different people have different topics\nof interest, therefore the connection structure depends on the\ninformation/topic content of the cascade. In this paper we consider the case\nwhere we observe a sequence of noisy adjacency matrices triggered by\ninformation/event with different topic distributions. We propose a novel latent\nmodel using the intuition that a connection is more likely to exist between two\nnodes if they are interested in similar topics, which are common with the\ninformation/event. Specifically, we endow each node with two node-topic\nvectors: an influence vector that measures how influential/authoritative they\nare on each topic; and a receptivity vector that measures how\nreceptive/susceptible they are to each topic. We show how these two node-topic\nstructures can be estimated from observed adjacency matrices with theoretical\nguarantee on estimation error, in cases where the topic distributions of the\ninformation/event are known, as well as when they are unknown. Experiments on\nsynthetic and real data demonstrate the effectiveness of our model and superior\nperformance compared to state-of-the-art methods. \n\n"}
{"id": "1806.06616", "contents": "Title: Comparison-Based Random Forests Abstract: Assume we are given a set of items from a general metric space, but we\nneither have access to the representation of the data nor to the distances\nbetween data points. Instead, suppose that we can actively choose a triplet of\nitems (A,B,C) and ask an oracle whether item A is closer to item B or to item\nC. In this paper, we propose a novel random forest algorithm for regression and\nclassification that relies only on such triplet comparisons. In the theory part\nof this paper, we establish sufficient conditions for the consistency of such a\nforest. In a set of comprehensive experiments, we then demonstrate that the\nproposed random forest is efficient both for classification and regression. In\nparticular, it is even competitive with other methods that have direct access\nto the metric representation of the data. \n\n"}
{"id": "1806.07562", "contents": "Title: Efficient inference in stochastic block models with vertex labels Abstract: We study the stochastic block model with two communities where vertices\ncontain side information in the form of a vertex label. These vertex labels may\nhave arbitrary label distributions, depending on the community memberships. We\nanalyze a linearized version of the popular belief propagation algorithm. We\nshow that this algorithm achieves the highest accuracy possible whenever a\ncertain function of the network parameters has a unique fixed point. Whenever\nthis function has multiple fixed points, the belief propagation algorithm may\nnot perform optimally. We show that increasing the information in the vertex\nlabels may reduce the number of fixed points and hence lead to optimality of\nbelief propagation. \n\n"}
{"id": "1806.08317", "contents": "Title: Fashion-Gen: The Generative Fashion Dataset and Challenge Abstract: We introduce a new dataset of 293,008 high definition (1360 x 1360 pixels)\nfashion images paired with item descriptions provided by professional stylists.\nEach item is photographed from a variety of angles. We provide baseline results\non 1) high-resolution image generation, and 2) image generation conditioned on\nthe given text descriptions. We invite the community to improve upon these\nbaselines. In this paper, we also outline the details of a challenge that we\nare launching based upon this dataset. \n\n"}
{"id": "1806.08840", "contents": "Title: Deep SNP: An End-to-end Deep Neural Network with Attention-based\n  Localization for Break-point Detection in SNP Array Genomic data Abstract: Diagnosis and risk stratification of cancer and many other diseases require\nthe detection of genomic breakpoints as a prerequisite of calling copy number\nalterations (CNA). This, however, is still challenging and requires\ntime-consuming manual curation. As deep-learning methods outperformed classical\nstate-of-the-art algorithms in various domains and have also been successfully\napplied to life science problems including medicine and biology, we here\npropose Deep SNP, a novel Deep Neural Network to learn from genomic data.\nSpecifically, we used a manually curated dataset from 12 genomic single\nnucleotide polymorphism array (SNPa) profiles as truth-set and aimed at\npredicting the presence or absence of genomic breakpoints, an indicator of\nstructural chromosomal variations, in windows of 40,000 probes. We compare our\nresults with well-known neural network models as well as Rawcopy though this\ntool is designed to predict breakpoints and in addition genomic segments with\nhigh sensitivity. We show, that Deep SNP is capable of successfully predicting\nthe presence or absence of a breakpoint in large genomic windows and\noutperforms state-of-the-art neural network models. Qualitative examples\nsuggest that integration of a localization unit may enable breakpoint detection\nand prediction of genomic segments, even if the breakpoint coordinates were not\nprovided for network training. These results warrant further evaluation of\nDeepSNP for breakpoint localization and subsequent calling of genomic segments. \n\n"}
{"id": "1806.08887", "contents": "Title: The Sparse Manifold Transform Abstract: We present a signal representation framework called the sparse manifold\ntransform that combines key ideas from sparse coding, manifold learning, and\nslow feature analysis. It turns non-linear transformations in the primary\nsensory signal space into linear interpolations in a representational embedding\nspace while maintaining approximate invertibility. The sparse manifold\ntransform is an unsupervised and generative framework that explicitly and\nsimultaneously models the sparse discreteness and low-dimensional manifold\nstructure found in natural scenes. When stacked, it also models hierarchical\ncomposition. We provide a theoretical description of the transform and\ndemonstrate properties of the learned representation on both synthetic data and\nnatural videos. \n\n"}
{"id": "1806.09235", "contents": "Title: Towards a Better Understanding and Regularization of GAN Training\n  Dynamics Abstract: Generative adversarial networks (GANs) are notoriously difficult to train and\nthe reasons underlying their (non-)convergence behaviors are still not\ncompletely understood. By first considering a simple yet representative GAN\nexample, we mathematically analyze its local convergence behavior in a\nnon-asymptotic way. Furthermore, the analysis is extended to general GANs under\ncertain assumptions. We find that in order to ensure a good convergence rate,\ntwo factors of the Jacobian in the GAN training dynamics should be\nsimultaneously avoided, which are (i) the Phase Factor, i.e., the Jacobian has\ncomplex eigenvalues with a large imaginary-to-real ratio, and (ii) the\nConditioning Factor, i.e., the Jacobian is ill-conditioned. Previous methods of\nregularizing the Jacobian can only alleviate one of these two factors, while\nmaking the other more severe. Thus we propose a new JAcobian REgularization\n(JARE) for GANs, which simultaneously addresses both factors by construction.\nFinally, we conduct experiments that confirm our theoretical analysis and\ndemonstrate the advantages of JARE over previous methods in stabilizing GANs. \n\n"}
{"id": "1806.09708", "contents": "Title: Mimic and Classify : A meta-algorithm for Conditional Independence\n  Testing Abstract: Given independent samples generated from the joint distribution\n$p(\\mathbf{x},\\mathbf{y},\\mathbf{z})$, we study the problem of Conditional\nIndependence (CI-Testing), i.e., whether the joint equals the CI distribution\n$p^{CI}(\\mathbf{x},\\mathbf{y},\\mathbf{z})= p(\\mathbf{z})\np(\\mathbf{y}|\\mathbf{z})p(\\mathbf{x}|\\mathbf{z})$ or not. We cast this problem\nunder the purview of the proposed, provable meta-algorithm, \"Mimic and\nClassify\", which is realized in two-steps: (a) Mimic the CI distribution close\nenough to recover the support, and (b) Classify to distinguish the joint and\nthe CI distribution. Thus, as long as we have a good generative model and a\ngood classifier, we potentially have a sound CI Tester. With this modular\nparadigm, CI Testing becomes amiable to be handled by state-of-the-art, both\ngenerative and classification methods from the modern advances in Deep\nLearning, which in general can handle issues related to curse of dimensionality\nand operation in small sample regime. We show intensive numerical experiments\non synthetic and real datasets where new mimic methods such conditional GANs,\nRegression with Neural Nets, outperform the current best CI Testing performance\nin the literature. Our theoretical results provide analysis on the estimation\nof null distribution as well as allow for general measures, i.e., when either\nsome of the random variables are discrete and some are continuous or when one\nor more of them are discrete-continuous mixtures. \n\n"}
{"id": "1806.10234", "contents": "Title: Scalable Gaussian Process Inference with Finite-data Mean and Variance\n  Guarantees Abstract: Gaussian processes (GPs) offer a flexible class of priors for nonparametric\nBayesian regression, but popular GP posterior inference methods are typically\nprohibitively slow or lack desirable finite-data guarantees on quality. We\ndevelop an approach to scalable approximate GP regression with finite-data\nguarantees on the accuracy of pointwise posterior mean and variance estimates.\nOur main contribution is a novel objective for approximate inference in the\nnonparametric setting: the preconditioned Fisher (pF) divergence. We show that\nunlike the Kullback--Leibler divergence (used in variational inference), the pF\ndivergence bounds the 2-Wasserstein distance, which in turn provides tight\nbounds the pointwise difference of the mean and variance functions. We\ndemonstrate that, for sparse GP likelihood approximations, we can minimize the\npF divergence efficiently. Our experiments show that optimizing the pF\ndivergence has the same computational requirements as variational sparse GPs\nwhile providing comparable empirical performance--in addition to our novel\nfinite-data quality guarantees. \n\n"}
{"id": "1806.10270", "contents": "Title: Optimal Piecewise Local-Linear Approximations Abstract: Existing works on \"black-box\" model interpretation use local-linear\napproximations to explain the predictions made for each data instance in terms\nof the importance assigned to the different features for arriving at the\nprediction. These works provide instancewise explanations and thus give a local\nview of the model. To be able to trust the model it is important to understand\nthe global model behavior and there are relatively fewer works which do the\nsame. Piecewise local-linear models provide a natural way to extend\nlocal-linear models to explain the global behavior of the model. In this work,\nwe provide a dynamic programming based framework to obtain piecewise\napproximations of the black-box model. We also provide provable fidelity, i.e.,\nhow well the explanations reflect the black-box model, guarantees. We carry out\nsimulations on synthetic and real datasets to show the utility of the proposed\napproach. At the end, we show that the ideas developed for our framework can\nalso be used to address the problem of clustering for one-dimensional data. We\ngive a polynomial time algorithm and prove that it achieves optimal clustering. \n\n"}
{"id": "1806.11326", "contents": "Title: Unsupervised Detection and Explanation of Latent-class Contextual\n  Anomalies Abstract: Detecting and explaining anomalies is a challenging effort. This holds\nespecially true when data exhibits strong dependencies and single measurements\nneed to be assessed and analyzed in their respective context. In this work, we\nconsider scenarios where measurements are non-i.i.d, i.e. where samples are\ndependent on corresponding discrete latent variables which are connected\nthrough some given dependency structure, the contextual information. Our\ncontribution is twofold: (i) Building atop of support vector data description\n(SVDD), we derive a method able to cope with latent-class dependency structure\nthat can still be optimized efficiently. We further show that our approach\nneatly generalizes vanilla SVDD as well as k-means and conditional random\nfields (CRF) and provide a corresponding probabilistic interpretation. (ii) In\nunsupervised scenarios where it is not possible to quantify the accuracy of an\nanomaly detector, having an human-interpretable solution is the key to success.\nBased on deep Taylor decomposition and a reformulation of our trained anomaly\ndetector as a neural network, we are able to backpropagate predictions to\npixel-domain and thus identify features and regions of high relevance. We\ndemonstrate the usefulness of our novel approach on toy data with known\nspatio-temporal structure and successfully validate on synthetic as well as\nreal world off-shore data from the oil industry. \n\n"}
{"id": "1807.00042", "contents": "Title: Neural Networks Trained to Solve Differential Equations Learn General\n  Representations Abstract: We introduce a technique based on the singular vector canonical correlation\nanalysis (SVCCA) for measuring the generality of neural network layers across a\ncontinuously-parametrized set of tasks. We illustrate this method by studying\ngenerality in neural networks trained to solve parametrized boundary value\nproblems based on the Poisson partial differential equation. We find that the\nfirst hidden layer is general, and that deeper layers are successively more\nspecific. Next, we validate our method against an existing technique that\nmeasures layer generality using transfer learning experiments. We find\nexcellent agreement between the two methods, and note that our method is much\nfaster, particularly for continuously-parametrized problems. Finally, we\nvisualize the general representations of the first layers, and interpret them\nas generalized coordinates over the input domain. \n\n"}
{"id": "1807.00311", "contents": "Title: Product-based Neural Networks for User Response Prediction over\n  Multi-field Categorical Data Abstract: User response prediction is a crucial component for personalized information\nretrieval and filtering scenarios, such as recommender system and web search.\nThe data in user response prediction is mostly in a multi-field categorical\nformat and transformed into sparse representations via one-hot encoding. Due to\nthe sparsity problems in representation and optimization, most research focuses\non feature engineering and shallow modeling. Recently, deep neural networks\nhave attracted research attention on such a problem for their high capacity and\nend-to-end training scheme. In this paper, we study user response prediction in\nthe scenario of click prediction. We first analyze a coupled gradient issue in\nlatent vector-based models and propose kernel product to learn field-aware\nfeature interactions. Then we discuss an insensitive gradient issue in\nDNN-based models and propose Product-based Neural Network (PNN) which adopts a\nfeature extractor to explore feature interactions. Generalizing the kernel\nproduct to a net-in-net architecture, we further propose Product-network In\nNetwork (PIN) which can generalize previous models. Extensive experiments on 4\nindustrial datasets and 1 contest dataset demonstrate that our models\nconsistently outperform 8 baselines on both AUC and log loss. Besides, PIN\nmakes great CTR improvement (relatively 34.67%) in online A/B test. \n\n"}
{"id": "1807.00468", "contents": "Title: Automated Directed Fairness Testing Abstract: Fairness is a critical trait in decision making. As machine-learning models\nare increasingly being used in sensitive application domains (e.g. education\nand employment) for decision making, it is crucial that the decisions computed\nby such models are free of unintended bias. But how can we automatically\nvalidate the fairness of arbitrary machine-learning models? For a given\nmachine-learning model and a set of sensitive input parameters, our AEQUITAS\napproach automatically discovers discriminatory inputs that highlight fairness\nviolation. At the core of AEQUITAS are three novel strategies to employ\nprobabilistic search over the input space with the objective of uncovering\nfairness violation. Our AEQUITAS approach leverages inherent robustness\nproperty in common machine-learning models to design and implement scalable\ntest generation methodologies. An appealing feature of our generated test\ninputs is that they can be systematically added to the training set of the\nunderlying model and improve its fairness. To this end, we design a fully\nautomated module that guarantees to improve the fairness of the underlying\nmodel.\n  We implemented AEQUITAS and we have evaluated it on six state-of-the-art\nclassifiers, including a classifier that was designed with fairness\nconstraints. We show that AEQUITAS effectively generates inputs to uncover\nfairness violation in all the subject classifiers and systematically improves\nthe fairness of the respective models using the generated test inputs. In our\nevaluation, AEQUITAS generates up to 70% discriminatory inputs (w.r.t. the\ntotal number of inputs generated) and leverages these inputs to improve the\nfairness up to 94%. \n\n"}
{"id": "1807.01202", "contents": "Title: Generating Multi-Categorical Samples with Generative Adversarial\n  Networks Abstract: We propose a method to train generative adversarial networks on mutivariate\nfeature vectors representing multiple categorical values. In contrast to the\ncontinuous domain, where GAN-based methods have delivered considerable results,\nGANs struggle to perform equally well on discrete data. We propose and compare\nseveral architectures based on multiple (Gumbel) softmax output layers taking\ninto account the structure of the data. We evaluate the performance of our\narchitecture on datasets with different sparsity, number of features, ranges of\ncategorical values, and dependencies among the features. Our proposed\narchitecture and method outperforms existing models. \n\n"}
{"id": "1807.01705", "contents": "Title: Transfer Learning for Clinical Time Series Analysis using Recurrent\n  Neural Networks Abstract: Deep neural networks have shown promising results for various clinical\nprediction tasks such as diagnosis, mortality prediction, predicting duration\nof stay in hospital, etc. However, training deep networks -- such as those\nbased on Recurrent Neural Networks (RNNs) -- requires large labeled data, high\ncomputational resources, and significant hyperparameter tuning effort. In this\nwork, we investigate as to what extent can transfer learning address these\nissues when using deep RNNs to model multivariate clinical time series. We\nconsider transferring the knowledge captured in an RNN trained on several\nsource tasks simultaneously using a large labeled dataset to build the model\nfor a target task with limited labeled data. An RNN pre-trained on several\ntasks provides generic features, which are then used to build simpler linear\nmodels for new target tasks without training task-specific RNNs. For\nevaluation, we train a deep RNN to identify several patient phenotypes on time\nseries from MIMIC-III database, and then use the features extracted using that\nRNN to build classifiers for identifying previously unseen phenotypes, and also\nfor a seemingly unrelated task of in-hospital mortality. We demonstrate that\n(i) models trained on features extracted using pre-trained RNN outperform or,\nin the worst case, perform as well as task-specific RNNs; (ii) the models using\nfeatures from pre-trained models are more robust to the size of labeled data\nthan task-specific RNNs; and (iii) features extracted using pre-trained RNN are\ngeneric enough and perform better than typical statistical hand-crafted\nfeatures. \n\n"}
{"id": "1807.01969", "contents": "Title: Variational Bayesian dropout: pitfalls and fixes Abstract: Dropout, a stochastic regularisation technique for training of neural\nnetworks, has recently been reinterpreted as a specific type of approximate\ninference algorithm for Bayesian neural networks. The main contribution of the\nreinterpretation is in providing a theoretical framework useful for analysing\nand extending the algorithm. We show that the proposed framework suffers from\nseveral issues; from undefined or pathological behaviour of the true posterior\nrelated to use of improper priors, to an ill-defined variational objective due\nto singularity of the approximating distribution relative to the true\nposterior. Our analysis of the improper log uniform prior used in variational\nGaussian dropout suggests the pathologies are generally irredeemable, and that\nthe algorithm still works only because the variational formulation annuls some\nof the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL)\ndivergence, a new approximate inference objective for approximation of\nhigh-dimensional distributions. We show that motivations for variational\nBernoulli dropout based on discretisation and noise have QKL as a limit.\nProperties of QKL are studied both theoretically and on a simple practical\nexample which shows that the QKL-optimal approximation of a full rank Gaussian\nwith a degenerate one naturally leads to the Principal Component Analysis\nsolution. \n\n"}
{"id": "1807.02582", "contents": "Title: Gaussian Processes and Kernel Methods: A Review on Connections and\n  Equivalences Abstract: This paper is an attempt to bridge the conceptual gaps between researchers\nworking on the two widely used approaches based on positive definite kernels:\nBayesian learning or inference using Gaussian processes on the one side, and\nfrequentist kernel methods based on reproducing kernel Hilbert spaces on the\nother. It is widely known in machine learning that these two formalisms are\nclosely related; for instance, the estimator of kernel ridge regression is\nidentical to the posterior mean of Gaussian process regression. However, they\nhave been studied and developed almost independently by two essentially\nseparate communities, and this makes it difficult to seamlessly transfer\nresults between them. Our aim is to overcome this potential difficulty. To this\nend, we review several old and new results and concepts from either side, and\njuxtapose algorithmic quantities from each framework to highlight close\nsimilarities. We also provide discussions on subtle philosophical and\ntheoretical differences between the two approaches. \n\n"}
{"id": "1807.02599", "contents": "Title: From Text to Topics in Healthcare Records: An Unsupervised Graph\n  Partitioning Methodology Abstract: Electronic Healthcare Records contain large volumes of unstructured data,\nincluding extensive free text. Yet this source of detailed information often\nremains under-used because of a lack of methodologies to extract interpretable\ncontent in a timely manner. Here we apply network-theoretical tools to analyse\nfree text in Hospital Patient Incident reports from the National Health\nService, to find clusters of documents with similar content in an unsupervised\nmanner at different levels of resolution. We combine deep neural network\nparagraph vector text-embedding with multiscale Markov Stability community\ndetection applied to a sparsified similarity graph of document vectors, and\nshowcase the approach on incident reports from Imperial College Healthcare NHS\nTrust, London. The multiscale community structure reveals different levels of\nmeaning in the topics of the dataset, as shown by descriptive terms extracted\nfrom the clusters of records. We also compare a posteriori against hand-coded\ncategories assigned by healthcare personnel, and show that our approach\noutperforms LDA-based models. Our content clusters exhibit good correspondence\nwith two levels of hand-coded categories, yet they also provide further medical\ndetail in certain areas and reveal complementary descriptors of incidents\nbeyond the external classification taxonomy. \n\n"}
{"id": "1807.02694", "contents": "Title: Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions Abstract: Consider the following class of learning schemes: $$\\hat{\\boldsymbol{\\beta}}\n:= \\arg\\min_{\\boldsymbol{\\beta}}\\;\\sum_{j=1}^n\n\\ell(\\boldsymbol{x}_j^\\top\\boldsymbol{\\beta}; y_j) + \\lambda\nR(\\boldsymbol{\\beta}),\\qquad\\qquad (1) $$ where $\\boldsymbol{x}_i \\in\n\\mathbb{R}^p$ and $y_i \\in \\mathbb{R}$ denote the $i^{\\text{th}}$ feature and\nresponse variable respectively. Let $\\ell$ and $R$ be the loss function and\nregularizer, $\\boldsymbol{\\beta}$ denote the unknown weights, and $\\lambda$ be\na regularization parameter. Finding the optimal choice of $\\lambda$ is a\nchallenging problem in high-dimensional regimes where both $n$ and $p$ are\nlarge. We propose two frameworks to obtain a computationally efficient\napproximation ALO of the leave-one-out cross validation (LOOCV) risk for\nnonsmooth losses and regularizers. Our two frameworks are based on the primal\nand dual formulations of (1). We prove the equivalence of the two approaches\nunder smoothness conditions. This equivalence enables us to justify the\naccuracy of both methods under such conditions. We use our approaches to obtain\na risk estimate for several standard problems, including generalized LASSO,\nnuclear norm regularization, and support vector machines. We empirically\ndemonstrate the effectiveness of our results for non-differentiable cases. \n\n"}
{"id": "1807.03133", "contents": "Title: Outfit Generation and Style Extraction via Bidirectional LSTM and\n  Autoencoder Abstract: When creating an outfit, style is a criterion in selecting each fashion item.\nThis means that style can be regarded as a feature of the overall outfit.\nHowever, in various previous studies on outfit generation, there have been few\nmethods focusing on global information obtained from an outfit. To address this\ndeficiency, we have incorporated an unsupervised style extraction module into a\nmodel to learn outfits. Using the style information of an outfit as a whole,\nthe proposed model succeeded in generating outfits more flexibly without\nrequiring additional information. Moreover, the style information extracted by\nthe proposed model is easy to interpret. The proposed model was evaluated on\ntwo human-generated outfit datasets. In a fashion item prediction task (missing\nprediction task), the proposed model outperformed a baseline method. In a style\nextraction task, the proposed model extracted some easily distinguishable\nstyles. In an outfit generation task, the proposed model generated an outfit\nwhile controlling its styles. This capability allows us to generate fashionable\noutfits according to various preferences. \n\n"}
{"id": "1807.03929", "contents": "Title: Quantification under prior probability shift: the ratio estimator and\n  its extensions Abstract: The quantification problem consists of determining the prevalence of a given\nlabel in a target population. However, one often has access to the labels in a\nsample from the training population but not in the target population. A common\nassumption in this situation is that of prior probability shift, that is, once\nthe labels are known, the distribution of the features is the same in the\ntraining and target populations. In this paper, we derive a new lower bound for\nthe risk of the quantification problem under the prior shift assumption.\nComplementing this lower bound, we present a new approximately minimax class of\nestimators, ratio estimators, which generalize several previous proposals in\nthe literature. Using a weaker version of the prior shift assumption, which can\nbe tested, we show that ratio estimators can be used to build confidence\nintervals for the quantification problem. We also extend the ratio estimator so\nthat it can: (i) incorporate labels from the target population, when they are\navailable and (ii) estimate how the prevalence of positive labels varies\naccording to a function of certain covariates. \n\n"}
{"id": "1807.04183", "contents": "Title: Optimization over Continuous and Multi-dimensional Decisions with\n  Observational Data Abstract: We consider the optimization of an uncertain objective over continuous and\nmulti-dimensional decision spaces in problems in which we are only provided\nwith observational data. We propose a novel algorithmic framework that is\ntractable, asymptotically consistent, and superior to comparable methods on\nexample problems. Our approach leverages predictive machine learning methods\nand incorporates information on the uncertainty of the predicted outcomes for\nthe purpose of prescribing decisions. We demonstrate the efficacy of our method\non examples involving both synthetic and real data sets. \n\n"}
{"id": "1807.04287", "contents": "Title: Hacking Alice's box in CV-QKD Abstract: Security analyses of quantum cryptographic protocols typically rely on\ncertain conditions; one such condition is that the sender (Alice) and receiver\n(Bob) have isolated devices inaccessible to third parties. If an eavesdropper\n(Eve) has a side-channel into one of the devices, then the key rate may be\nsensibly reduced. In this paper, we consider an attack on a coherent-state\nprotocol, where Eve not only taps the main communication channel but also hacks\nAlice's device. This is done by introducing a Trojan horse mode with low mean\nnumber of photons $\\bar{n}$ which is then modulated in a similar way to the\nsignal state. First we show that this strategy can be reduced to an attack\nwithout side channels but with higher loss and noise in the main channel. Then\nwe show how the key rate rapidly deteriorates for increasing photons $\\bar{n}$,\nbeing halved at long distances each time $\\bar{n}+1$ doubles. Our work suggests\nthat Alice's device should also be equipped with sensing systems that are able\nto detect and estimate the total number of incoming and outgoing photons. \n\n"}
{"id": "1807.04320", "contents": "Title: Automated Vulnerability Detection in Source Code Using Deep\n  Representation Learning Abstract: Increasing numbers of software vulnerabilities are discovered every year\nwhether they are reported publicly or discovered internally in proprietary\ncode. These vulnerabilities can pose serious risk of exploit and result in\nsystem compromise, information leaks, or denial of service. We leveraged the\nwealth of C and C++ open-source code available to develop a large-scale\nfunction-level vulnerability detection system using machine learning. To\nsupplement existing labeled vulnerability datasets, we compiled a vast dataset\nof millions of open-source functions and labeled it with carefully-selected\nfindings from three different static analyzers that indicate potential\nexploits. The labeled dataset is available at: https://osf.io/d45bw/. Using\nthese datasets, we developed a fast and scalable vulnerability detection tool\nbased on deep feature representation learning that directly interprets lexed\nsource code. We evaluated our tool on code from both real software packages and\nthe NIST SATE IV benchmark dataset. Our results demonstrate that deep feature\nrepresentation learning on source code is a promising approach for automated\nsoftware vulnerability detection. \n\n"}
{"id": "1807.05411", "contents": "Title: A Unified Framework for Sparse Relaxed Regularized Regression: SR3 Abstract: Regularized regression problems are ubiquitous in statistical modeling,\nsignal processing, and machine learning. Sparse regression in particular has\nbeen instrumental in scientific model discovery, including compressed sensing\napplications, variable selection, and high-dimensional analysis. We propose a\nbroad framework for sparse relaxed regularized regression, called SR3. The key\nidea is to solve a relaxation of the regularized problem, which has three\nadvantages over the state-of-the-art: (1) solutions of the relaxed problem are\nsuperior with respect to errors, false positives, and conditioning, (2)\nrelaxation allows extremely fast algorithms for both convex and nonconvex\nformulations, and (3) the methods apply to composite regularizers such as total\nvariation (TV) and its nonconvex variants. We demonstrate the advantages of SR3\n(computational efficiency, higher accuracy, faster convergence rates, greater\nflexibility) across a range of regularized regression problems with synthetic\nand real data, including applications in compressed sensing, LASSO, matrix\ncompletion, TV regularization, and group sparsity. To promote reproducible\nresearch, we also provide a companion MATLAB package that implements these\nexamples. \n\n"}
{"id": "1807.06343", "contents": "Title: Learning with SGD and Random Features Abstract: Sketching and stochastic gradient methods are arguably the most common\ntechniques to derive efficient large scale learning algorithms. In this paper,\nwe investigate their application in the context of nonparametric statistical\nlearning. More precisely, we study the estimator defined by stochastic gradient\nwith mini batches and random features. The latter can be seen as form of\nnonlinear sketching and used to define approximate kernel methods. The\nconsidered estimator is not explicitly penalized/constrained and regularization\nis implicit. Indeed, our study highlights how different parameters, such as\nnumber of features, iterations, step-size and mini-batch size control the\nlearning properties of the solutions. We do this by deriving optimal finite\nsample bounds, under standard assumptions. The obtained results are\ncorroborated and illustrated by numerical experiments. \n\n"}
{"id": "1807.06362", "contents": "Title: Confidence Intervals for Testing Disparate Impact in Fair Learning Abstract: We provide the asymptotic distribution of the major indexes used in the\nstatistical literature to quantify disparate treatment in machine learning. We\naim at promoting the use of confidence intervals when testing the so-called\ngroup disparate impact. We illustrate on some examples the importance of using\nconfidence intervals and not a single value. \n\n"}
{"id": "1807.06572", "contents": "Title: Explicating feature contribution using Random Forest proximity distances Abstract: In Random Forests, proximity distances are a metric representation of data\ninto decision space. By observing how changes in input map to the movement of\ninstances in this space we are able to determine the independent contribution\nof each feature to the decision-making process. For binary feature vectors,\nthis process is fully specified. As these changes in input move particular\ninstances nearer to the in-group or out-group, the independent contribution of\neach feature can be uncovered. Using this technique, we are able to calculate\nthe contribution of each feature in determining how black-box decisions were\nmade. This allows explication of the decision-making process, audit of the\nclassifier, and post-hoc analysis of errors in classification. \n\n"}
{"id": "1807.07187", "contents": "Title: Efficient Training on Very Large Corpora via Gramian Estimation Abstract: We study the problem of learning similarity functions over very large corpora\nusing neural network embedding models. These models are typically trained using\nSGD with sampling of random observed and unobserved pairs, with a number of\nsamples that grows quadratically with the corpus size, making it expensive to\nscale to very large corpora. We propose new efficient methods to train these\nmodels without having to sample unobserved pairs. Inspired by matrix\nfactorization, our approach relies on adding a global quadratic penalty to all\npairs of examples and expressing this term as the matrix-inner-product of two\ngeneralized Gramians. We show that the gradient of this term can be efficiently\ncomputed by maintaining estimates of the Gramians, and develop variance\nreduction schemes to improve the quality of the estimates. We conduct\nlarge-scale experiments that show a significant improvement in training time\nand generalization quality compared to traditional sampling methods. \n\n"}
{"id": "1807.08383", "contents": "Title: PaloBoost: An Overfitting-robust TreeBoost with Out-of-Bag Sample\n  Regularization Techniques Abstract: Stochastic Gradient TreeBoost is often found in many winning solutions in\npublic data science challenges. Unfortunately, the best performance requires\nextensive parameter tuning and can be prone to overfitting. We propose\nPaloBoost, a Stochastic Gradient TreeBoost model that uses novel regularization\ntechniques to guard against overfitting and is robust to parameter settings.\nPaloBoost uses the under-utilized out-of-bag samples to perform gradient-aware\npruning and estimate adaptive learning rates. Unlike other Stochastic Gradient\nTreeBoost models that use the out-of-bag samples to estimate test errors,\nPaloBoost treats the samples as a second batch of training samples to prune the\ntrees and adjust the learning rates. As a result, PaloBoost can dynamically\nadjust tree depths and learning rates to achieve faster learning at the start\nand slower learning as the algorithm converges. We illustrate how these\nregularization techniques can be efficiently implemented and propose a new\nformula for calculating feature importance to reflect the node coverages and\nlearning rates. Extensive experimental results on seven datasets demonstrate\nthat PaloBoost is robust to overfitting, is less sensitivity to the parameters,\nand can also effectively identify meaningful features. \n\n"}
{"id": "1807.09236", "contents": "Title: Improving pairwise comparison models using Empirical Bayes shrinkage Abstract: Comparison data arises in many important contexts, e.g. shopping, web clicks,\nor sports competitions. Typically we are given a dataset of comparisons and\nwish to train a model to make predictions about the outcome of unseen\ncomparisons. In many cases available datasets have relatively few comparisons\n(e.g. there are only so many NFL games per year) or efficiency is important\n(e.g. we want to quickly estimate the relative appeal of a product). In such\nsettings it is well known that shrinkage estimators outperform maximum\nlikelihood estimators. A complicating matter is that standard comparison models\nsuch as the conditional multinomial logit model are only models of conditional\noutcomes (who wins) and not of comparisons themselves (who competes). As such,\ndifferent models of the comparison process lead to different shrinkage\nestimators. In this work we derive a collection of methods for estimating the\npairwise uncertainty of pairwise predictions based on different assumptions\nabout the comparison process. These uncertainty estimates allow us both to\nexamine model uncertainty as well as perform Empirical Bayes shrinkage\nestimation of the model parameters. We demonstrate that our shrunk estimators\noutperform standard maximum likelihood methods on real comparison data from\nonline comparison surveys as well as from several sports contexts. \n\n"}
{"id": "1807.09946", "contents": "Title: Computationally Efficient Measures of Internal Neuron Importance Abstract: The challenge of assigning importance to individual neurons in a network is\nof interest when interpreting deep learning models. In recent work, Dhamdhere\net al. proposed Total Conductance, a \"natural refinement of Integrated\nGradients\" for attributing importance to internal neurons. Unfortunately, the\nauthors found that calculating conductance in tensorflow required the addition\nof several custom gradient operators and did not scale well. In this work, we\nshow that the formula for Total Conductance is mathematically equivalent to\nPath Integrated Gradients computed on a hidden layer in the network. We provide\na scalable implementation of Total Conductance using standard tensorflow\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\napproach that is applicable to calculating internal neuron importance. We find\nthat DeepLIFT produces strong empirical results and is faster to compute, but\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\nmay not always be preferred in practice. Colab notebook reproducing results:\nhttp://bit.ly/neuronintegratedgradients \n\n"}
{"id": "1807.09958", "contents": "Title: Rethinking the Form of Latent States in Image Captioning Abstract: RNNs and their variants have been widely adopted for image captioning. In\nRNNs, the production of a caption is driven by a sequence of latent states.\nExisting captioning models usually represent latent states as vectors, taking\nthis practice for granted. We rethink this choice and study an alternative\nformulation, namely using two-dimensional maps to encode latent states. This is\nmotivated by the curiosity about a question: how the spatial structures in the\nlatent states affect the resultant captions? Our study on MSCOCO and Flickr30k\nleads to two significant observations. First, the formulation with 2D states is\ngenerally more effective in captioning, consistently achieving higher\nperformance with comparable parameter sizes. Second, 2D states preserve spatial\nlocality. Taking advantage of this, we visually reveal the internal dynamics in\nthe process of caption generation, as well as the connections between input\nvisual domain and output linguistic domain. \n\n"}
{"id": "1807.10225", "contents": "Title: Medical Image Synthesis for Data Augmentation and Anonymization using\n  Generative Adversarial Networks Abstract: Data diversity is critical to success when training deep learning models.\nMedical imaging data sets are often imbalanced as pathologic findings are\ngenerally rare, which introduces significant challenges when training deep\nlearning models. In this work, we propose a method to generate synthetic\nabnormal MRI images with brain tumors by training a generative adversarial\nnetwork using two publicly available data sets of brain MRI. We demonstrate two\nunique benefits that the synthetic images provide. First, we illustrate\nimproved performance on tumor segmentation by leveraging the synthetic images\nas a form of data augmentation. Second, we demonstrate the value of generative\nmodels as an anonymization tool, achieving comparable tumor segmentation\nresults when trained on the synthetic data versus when trained on real subject\ndata. Together, these results offer a potential solution to two of the largest\nchallenges facing machine learning in medical imaging, namely the small\nincidence of pathological findings, and the restrictions around sharing of\npatient data. \n\n"}
{"id": "1807.10328", "contents": "Title: Selective Clustering Annotated using Modes of Projections Abstract: Selective clustering annotated using modes of projections (SCAMP) is a new\nclustering algorithm for data in $\\mathbb{R}^p$. SCAMP is motivated from the\npoint of view of non-parametric mixture modeling. Rather than maximizing a\nclassification likelihood to determine cluster assignments, SCAMP casts\nclustering as a search and selection problem. One consequence of this problem\nformulation is that the number of clusters is $\\textbf{not}$ a SCAMP tuning\nparameter. The search phase of SCAMP consists of finding sub-collections of the\ndata matrix, called candidate clusters, that obey shape constraints along each\ncoordinate projection. An extension of the dip test of Hartigan and Hartigan\n(1985) is developed to assist the search. Selection occurs by scoring each\ncandidate cluster with a preference function that quantifies prior belief about\nthe mixture composition. Clustering proceeds by selecting candidates to\nmaximize their total preference score. SCAMP concludes by annotating each\nselected cluster with labels that describe how cluster-level statistics compare\nto certain dataset-level quantities. SCAMP can be run multiple times on a\nsingle data matrix. Comparison of annotations obtained across iterations\nprovides a measure of clustering uncertainty. Simulation studies and\napplications to real data are considered. A C++ implementation with R interface\nis $\\href{https://github.com/RGLab/scamp}{available\\ online}$. \n\n"}
{"id": "1807.10956", "contents": "Title: Group-sparse SVD Models and Their Applications in Biological Data Abstract: Sparse Singular Value Decomposition (SVD) models have been proposed for\nbiclustering high dimensional gene expression data to identify block patterns\nwith similar expressions. However, these models do not take into account prior\ngroup effects upon variable selection. To this end, we first propose\ngroup-sparse SVD models with group Lasso (GL1-SVD) and group L0-norm penalty\n(GL0-SVD) for non-overlapping group structure of variables. However, such\ngroup-sparse SVD models limit their applicability in some problems with\noverlapping structure. Thus, we also propose two group-sparse SVD models with\noverlapping group Lasso (OGL1-SVD) and overlapping group L0-norm penalty\n(OGL0-SVD). We first adopt an alternating iterative strategy to solve GL1-SVD\nbased on a block coordinate descent method, and GL0-SVD based on a projection\nmethod. The key of solving OGL1-SVD is a proximal operator with overlapping\ngroup Lasso penalty. We employ an alternating direction method of multipliers\n(ADMM) to solve the proximal operator. Similarly, we develop an approximate\nmethod to solve OGL0-SVD. Applications of these methods and comparison with\ncompeting ones using simulated data demonstrate their effectiveness. Extensive\napplications of them onto several real gene expression data with gene prior\ngroup knowledge identify some biologically interpretable gene modules. \n\n"}
{"id": "1807.11876", "contents": "Title: Predicting Tactical Solutions to Operational Planning Problems under\n  Imperfect Information Abstract: This paper offers a methodological contribution at the intersection of\nmachine learning and operations research. Namely, we propose a methodology to\nquickly predict expected tactical descriptions of operational solutions\n(TDOSs). The problem we address occurs in the context of two-stage stochastic\nprogramming where the second stage is demanding computationally. We aim to\npredict at a high speed the expected TDOS associated with the second stage\nproblem, conditionally on the first stage variables. This may be used in\nsupport of the solution to the overall two-stage problem by avoiding the online\ngeneration of multiple second stage scenarios and solutions. We formulate the\ntactical prediction problem as a stochastic optimal prediction program, whose\nsolution we approximate with supervised machine learning. The training dataset\nconsists of a large number of deterministic operational problems generated by\ncontrolled probabilistic sampling. The labels are computed based on solutions\nto these problems (solved independently and offline), employing appropriate\naggregation and subselection methods to address uncertainty. Results on our\nmotivating application on load planning for rail transportation show that deep\nlearning models produce accurate predictions in very short computing time\n(milliseconds or less). The predictive accuracy is close to the lower bounds\ncalculated based on sample average approximation of the stochastic prediction\nprograms. \n\n"}
{"id": "1808.00380", "contents": "Title: A Differentially Private Kernel Two-Sample Test Abstract: Kernel two-sample testing is a useful statistical tool in determining whether\ndata samples arise from different distributions without imposing any parametric\nassumptions on those distributions. However, raw data samples can expose\nsensitive information about individuals who participate in scientific studies,\nwhich makes the current tests vulnerable to privacy breaches. Hence, we design\na new framework for kernel two-sample testing conforming to differential\nprivacy constraints, in order to guarantee the privacy of subjects in the data.\nUnlike existing differentially private parametric tests that simply add noise\nto data, kernel-based testing imposes a challenge due to a complex dependence\nof test statistics on the raw data, as these statistics correspond to\nestimators of distances between representations of probability measures in\nHilbert spaces. Our approach considers finite dimensional approximations to\nthose representations. As a result, a simple chi-squared test is obtained,\nwhere a test statistic depends on a mean and covariance of empirical\ndifferences between the samples, which we perturb for a privacy guarantee. We\ninvestigate the utility of our framework in two realistic settings and conclude\nthat our method requires only a relatively modest increase in sample size to\nachieve a similar level of power to the non-private tests in both settings. \n\n"}
{"id": "1808.00892", "contents": "Title: Semi-blind source separation with multichannel variational autoencoder Abstract: This paper proposes a multichannel source separation technique called the\nmultichannel variational autoencoder (MVAE) method, which uses a conditional\nVAE (CVAE) to model and estimate the power spectrograms of the sources in a\nmixture. By training the CVAE using the spectrograms of training examples with\nsource-class labels, we can use the trained decoder distribution as a universal\ngenerative model capable of generating spectrograms conditioned on a specified\nclass label. By treating the latent space variables and the class label as the\nunknown parameters of this generative model, we can develop a\nconvergence-guaranteed semi-blind source separation algorithm that consists of\niteratively estimating the power spectrograms of the underlying sources as well\nas the separation matrices. In experimental evaluations, our MVAE produced\nbetter separation performance than a baseline method. \n\n"}
{"id": "1808.00935", "contents": "Title: Inferring Parameters Through Inverse Multiobjective Optimization Abstract: Given a set of human's decisions that are observed, inverse optimization has\nbeen developed and utilized to infer the underlying decision making problem.\nThe majority of existing studies assumes that the decision making problem is\nwith a single objective function, and attributes data divergence to noises,\nerrors or bounded rationality, which, however, could lead to a corrupted\ninference when decisions are tradeoffs among multiple criteria. In this paper,\nwe take a data-driven approach and design a more sophisticated inverse\noptimization formulation to explicitly infer parameters of a multiobjective\ndecision making problem from noisy observations. This framework, together with\nour mathematical analyses and advanced algorithm developments, demonstrates a\nstrong capacity in estimating critical parameters, decoupling \"interpretable\"\ncomponents from noises or errors, deriving the denoised \\emph{optimal}\ndecisions, and ensuring statistical significance. In particular, for the whole\ndecision maker population, if suitable conditions hold, we will be able to\nunderstand the overall diversity and the distribution of their preferences over\nmultiple criteria, which is important when a precise inference on every single\ndecision maker is practically unnecessary or infeasible. Numerical results on a\nlarge number of experiments are reported to confirm the effectiveness of our\nunique inverse optimization model and the computational efficacy of the\ndeveloped algorithms. \n\n"}
{"id": "1808.01974", "contents": "Title: A Survey on Deep Transfer Learning Abstract: As a new classification platform, deep learning has recently received\nincreasing attention from researchers and has been successfully applied to many\ndomains. In some domains, like bioinformatics and robotics, it is very\ndifficult to construct a large-scale well-annotated dataset due to the expense\nof data acquisition and costly annotation, which limits its development.\nTransfer learning relaxes the hypothesis that the training data must be\nindependent and identically distributed (i.i.d.) with the test data, which\nmotivates us to use transfer learning to solve the problem of insufficient\ntraining data. This survey focuses on reviewing the current researches of\ntransfer learning by using deep neural network and its applications. We defined\ndeep transfer learning, category and review the recent research works based on\nthe techniques used in deep transfer learning. \n\n"}
{"id": "1808.02433", "contents": "Title: Robust Implicit Backpropagation Abstract: Arguably the biggest challenge in applying neural networks is tuning the\nhyperparameters, in particular the learning rate. The sensitivity to the\nlearning rate is due to the reliance on backpropagation to train the network.\nIn this paper we present the first application of Implicit Stochastic Gradient\nDescent (ISGD) to train neural networks, a method known in convex optimization\nto be unconditionally stable and robust to the learning rate. Our key\ncontribution is a novel layer-wise approximation of ISGD which makes its\nupdates tractable for neural networks. Experiments show that our method is more\nrobust to high learning rates and generally outperforms standard\nbackpropagation on a variety of tasks. \n\n"}
{"id": "1808.02510", "contents": "Title: Message Passing Graph Kernels Abstract: Graph kernels have recently emerged as a promising approach for tackling the\ngraph similarity and learning tasks at the same time. In this paper, we propose\na general framework for designing graph kernels. The proposed framework\ncapitalizes on the well-known message passing scheme on graphs. The kernels\nderived from the framework consist of two components. The first component is a\nkernel between vertices, while the second component is a kernel between graphs.\nThe main idea behind the proposed framework is that the representations of the\nvertices are implicitly updated using an iterative procedure. Then, these\nrepresentations serve as the building blocks of a kernel that compares pairs of\ngraphs. We derive four instances of the proposed framework, and show through\nextensive experiments that these instances are competitive with\nstate-of-the-art methods in various tasks. \n\n"}
{"id": "1808.03001", "contents": "Title: Compressed Sensing Using Binary Matrices of Nearly Optimal Dimensions Abstract: In this paper, we study the problem of compressed sensing using binary\nmeasurement matrices and $\\ell_1$-norm minimization (basis pursuit) as the\nrecovery algorithm. We derive new upper and lower bounds on the number of\nmeasurements to achieve robust sparse recovery with binary matrices. We\nestablish sufficient conditions for a column-regular binary matrix to satisfy\nthe robust null space property (RNSP) and show that the associated sufficient\nconditions % sparsity bounds for robust sparse recovery obtained using the RNSP\nare better by a factor of $(3 \\sqrt{3})/2 \\approx 2.6$ compared to the\nsufficient conditions obtained using the restricted isometry property (RIP).\nNext we derive universal \\textit{lower} bounds on the number of measurements\nthat any binary matrix needs to have in order to satisfy the weaker sufficient\ncondition based on the RNSP and show that bipartite graphs of girth six are\noptimal. Then we display two classes of binary matrices, namely parity check\nmatrices of array codes and Euler squares, which have girth six and are nearly\noptimal in the sense of almost satisfying the lower bound. In principle,\nrandomly generated Gaussian measurement matrices are \"order-optimal\". So we\ncompare the phase transition behavior of the basis pursuit formulation using\nbinary array codes and Gaussian matrices and show that (i) there is essentially\nno difference between the phase transition boundaries in the two cases and (ii)\nthe CPU time of basis pursuit with binary matrices is hundreds of times faster\nthan with Gaussian matrices and the storage requirements are less. Therefore it\nis suggested that binary matrices are a viable alternative to Gaussian matrices\nfor compressed sensing using basis pursuit. \\end{abstract} \n\n"}
{"id": "1808.03962", "contents": "Title: Generalized Dirac Oscillators with position-dependent mass Abstract: We study the $(1+1)$ dimensional generalized Dirac oscillator with a\nposition-dependent mass. In particular, bound states with zero energy as well\nas non zero energy have been obtained for suitable choices of the mass\nfunction/oscillator interaction. It has also been shown that in the presence of\nan electric field, bound states exist if the magnitude of the electric field\ndoes not exceed a critical value. \n\n"}
{"id": "1808.04228", "contents": "Title: DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human\n  Activity Recognition Abstract: Deep Convolutional Neural Networks (DCNNs) are currently popular in human\nactivity recognition applications. However, in the face of modern artificial\nintelligence sensor-based games, many research achievements cannot be\npractically applied on portable devices. DCNNs are typically resource-intensive\nand too large to be deployed on portable devices, thus this limits the\npractical application of complex activity detection. In addition, since\nportable devices do not possess high-performance Graphic Processing Units\n(GPUs), there is hardly any improvement in Action Game (ACT) experience.\nBesides, in order to deal with multi-sensor collaboration, all previous human\nactivity recognition models typically treated the representations from\ndifferent sensor signal sources equally. However, distinct types of activities\nshould adopt different fusion strategies. In this paper, a novel scheme is\nproposed. This scheme is used to train 2-bit Convolutional Neural Networks with\nweights and activations constrained to {-0.5,0,0.5}. It takes into account the\ncorrelation between different sensor signal sources and the activity types.\nThis model, which we refer to as DFTerNet, aims at producing a more reliable\ninference and better trade-offs for practical applications. Our basic idea is\nto exploit quantization of weights and activations directly in pre-trained\nfilter banks and adopt dynamic fusion strategies for different activity types.\nExperiments demonstrate that by using dynamic fusion strategy can exceed the\nbaseline model performance by up to ~5% on activity recognition like\nOPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we\nwere able to achieve performances closer to that of full-precision counterpart.\nThese results were also verified using the UniMiB-SHAR dataset. In addition,\nthe proposed method can achieve ~9x acceleration on CPUs and ~11x memory\nsaving. \n\n"}
{"id": "1808.04488", "contents": "Title: Electromagnetic lattice gauge invariance in two-dimensional\n  discrete-time quantum walks Abstract: Gauge invariance is one of the more important concepts in physics. We discuss\nthis concept in connection with the unitary evolution of discrete-time quantum\nwalks in one and two spatial dimensions, when they include the interaction with\nsynthetic, external electromagnetic fields. One introduces this interaction as\nadditional phases that play the role of gauge fields. Here, we present a way to\nincorporate those phases, which differs from previous works. Our proposal\nallows the discrete derivatives, that appear under a gauge transformation, to\ntreat time and space on the same footing, in a way which is similar to standard\nlattice gauge theories. By considering two steps of the evolution, we define a\ndensity current which is gauge invariant and conserved. In the continuum limit,\nthe dynamics of the particle, under a suitable choice of the parameters,\nbecomes the Dirac equation, and the conserved current satisfies the\ncorresponding conservation equation. \n\n"}
{"id": "1808.04880", "contents": "Title: A Precision Environment-Wide Association Study of Hypertension via\n  Supervised Cadre Models Abstract: We consider the problem in precision health of grouping people into\nsubpopulations based on their degree of vulnerability to a risk factor. These\nsubpopulations cannot be discovered with traditional clustering techniques\nbecause their quality is evaluated with a supervised metric: the ease of\nmodeling a response variable over observations within them. Instead, we apply\nthe supervised cadre model (SCM), which does use this metric. We extend the SCM\nformalism so that it may be applied to multivariate regression and binary\nclassification problems. We also develop a way to use conditional entropy to\nassess the confidence in the process by which a subject is assigned their\ncadre. Using the SCM, we generalize the environment-wide association study\n(EWAS) workflow to be able to model heterogeneity in population risk. In our\nEWAS, we consider more than two hundred environmental exposure factors and find\ntheir association with diastolic blood pressure, systolic blood pressure, and\nhypertension. This requires adapting the SCM to be applicable to data generated\nby a complex survey design. After correcting for false positives, we found 25\nexposure variables that had a significant association with at least one of our\nresponse variables. Eight of these were significant for a discovered\nsubpopulation but not for the overall population. Some of these associations\nhave been identified by previous researchers, while others appear to be novel.\nWe examine several discovered subpopulations in detail, and we find that they\nare interpretable and that they suggest further research questions. \n\n"}
{"id": "1808.05527", "contents": "Title: Deep Learning for Energy Markets Abstract: Deep Learning is applied to energy markets to predict extreme loads observed\nin energy grids. Forecasting energy loads and prices is challenging due to\nsharp peaks and troughs that arise due to supply and demand fluctuations from\nintraday system constraints. We propose deep spatio-temporal models and extreme\nvalue theory (EVT) to capture theses effects and in particular the tail\nbehavior of load spikes. Deep LSTM architectures with ReLU and $\\tanh$\nactivation functions can model trends and temporal dependencies while EVT\ncaptures highly volatile load spikes above a pre-specified threshold. To\nillustrate our methodology, we use hourly price and demand data from 4719 nodes\nof the PJM interconnection, and we construct a deep predictor. We show that\nDL-EVT outperforms traditional Fourier time series methods, both in-and\nout-of-sample, by capturing the observed nonlinearities in prices. Finally, we\nconclude with directions for future research. \n\n"}
{"id": "1808.05726", "contents": "Title: An N Time-Slice Dynamic Chain Event Graph Abstract: The Dynamic Chain Event Graph (DCEG) is able to depict many classes of\ndiscrete random processes exhibiting asymmetries in their developments and\ncontext-specific conditional probabilities structures. However, paradoxically,\nthis very generality has so far frustrated its wide application. So in this\npaper we develop an object-oriented method to fully analyse a particularly\nuseful and feasibly implementable new subclass of these graphical models called\nthe N Time-Slice DCEG (NT-DCEG). After demonstrating a close relationship\nbetween an NT-DCEG and a specific class of Markov processes, we discuss how\ngraphical modellers can exploit this connection to gain a deep understanding of\ntheir processes. We also show how to read from the topology of this graph\ncontext-specific independence statements that can then be checked by domain\nexperts. Our methods are illustrated throughout using examples of dynamic\nmultivariate processes describing inmate radicalisation in a prison. \n\n"}
{"id": "1808.05784", "contents": "Title: Multiview Boosting by Controlling the Diversity and the Accuracy of\n  View-specific Voters Abstract: In this paper we propose a boosting based multiview learning algorithm,\nreferred to as PB-MVBoost, which iteratively learns i) weights over\nview-specific voters capturing view-specific information; and ii) weights over\nviews by optimizing a PAC-Bayes multiview C-Bound that takes into account the\naccuracy of view-specific classifiers and the diversity between the views. We\nderive a generalization bound for this strategy following the PAC-Bayes theory\nwhich is a suitable tool to deal with models expressed as weighted combination\nover a set of voters. Different experiments on three publicly available\ndatasets show the efficiency of the proposed approach with respect to\nstate-of-art models. \n\n"}
{"id": "1808.06347", "contents": "Title: A Distribution Similarity Based Regularizer for Learning Bayesian\n  Networks Abstract: Probabilistic graphical models compactly represent joint distributions by\ndecomposing them into factors over subsets of random variables. In Bayesian\nnetworks, the factors are conditional probability distributions. For many\nproblems, common information exists among those factors. Adding similarity\nrestrictions can be viewed as imposing prior knowledge for model\nregularization. With proper restrictions, learned models usually generalize\nbetter. In this work, we study methods that exploit such high-level\nsimilarities to regularize the learning process and apply them to the task of\nmodeling the wave propagation in inhomogeneous media. We propose a novel\ndistribution-based penalization approach that encourages similar conditional\nprobability distribution rather than force the parameters to be similar\nexplicitly. We show in experiment that our proposed algorithm solves the\nmodeling wave propagation problem, which other baseline methods are not able to\nsolve. \n\n"}
{"id": "1808.06910", "contents": "Title: Scalable Population Synthesis with Deep Generative Modeling Abstract: Population synthesis is concerned with the generation of synthetic yet\nrealistic representations of populations. It is a fundamental problem in the\nmodeling of transport where the synthetic populations of micro-agents represent\na key input to most agent-based models. In this paper, a new methodological\nframework for how to 'grow' pools of micro-agents is presented. The model\nframework adopts a deep generative modeling approach from machine learning\nbased on a Variational Autoencoder (VAE). Compared to the previous population\nsynthesis approaches, including Iterative Proportional Fitting (IPF), Gibbs\nsampling and traditional generative models such as Bayesian Networks or Hidden\nMarkov Models, the proposed method allows fitting the full joint distribution\nfor high dimensions. The proposed methodology is compared with a conventional\nGibbs sampler and a Bayesian Network by using a large-scale Danish trip diary.\nIt is shown that, while these two methods outperform the VAE in the\nlow-dimensional case, they both suffer from scalability issues when the number\nof modeled attributes increases. It is also shown that the Gibbs sampler\nessentially replicates the agents from the original sample when the required\nconditional distributions are estimated as frequency tables. In contrast, the\nVAE allows addressing the problem of sampling zeros by generating agents that\nare virtually different from those in the original data but have similar\nstatistical properties. The presented approach can support agent-based modeling\nat all levels by enabling richer synthetic populations with smaller zones and\nmore detailed individual characteristics. \n\n"}
{"id": "1808.06918", "contents": "Title: On a New Improvement-Based Acquisition Function for Bayesian\n  Optimization Abstract: Bayesian optimization (BO) is a popular algorithm for solving challenging\noptimization tasks. It is designed for problems where the objective function is\nexpensive to evaluate, perhaps not available in exact form, without gradient\ninformation and possibly returning noisy values. Different versions of the\nalgorithm vary in the choice of the acquisition function, which recommends the\npoint to query the objective at next. Initially, researchers focused on\nimprovement-based acquisitions, while recently the attention has shifted to\nmore computationally expensive information-theoretical measures. In this paper\nwe present two major contributions to the literature. First, we propose a new\nimprovement-based acquisition function that recommends query points where the\nimprovement is expected to be high with high confidence. The proposed algorithm\nis evaluated on a large set of benchmark functions from the global optimization\nliterature, where it turns out to perform at least as well as current\nstate-of-the-art acquisition functions, and often better. This suggests that it\nis a powerful default choice for BO. The novel policy is then compared to\nwidely used global optimization solvers in order to confirm that BO methods\nreduce the computational costs of the optimization by keeping the number of\nfunction evaluations small. The second main contribution represents an\napplication to precision medicine, where the interest lies in the estimation of\nparameters of a partial differential equations model of the human pulmonary\nblood circulation system. Once inferred, these parameters can help clinicians\nin diagnosing a patient with pulmonary hypertension without going through the\nstandard invasive procedure of right heart catheterization, which can lead to\nside effects and complications (e.g. severe pain, internal bleeding,\nthrombosis). \n\n"}
{"id": "1808.07384", "contents": "Title: A Note on Inexact Condition for Cubic Regularized Newton's Method Abstract: This note considers the inexact cubic-regularized Newton's method (CR), which\nhas been shown in \\cite{Cartis2011a} to achieve the same order-level\nconvergence rate to a secondary stationary point as the exact CR\n\\citep{Nesterov2006}. However, the inexactness condition in \\cite{Cartis2011a}\nis not implementable due to its dependence on future iterates variable. This\nnote fixes such an issue by proving the same convergence rate for nonconvex\noptimization under an inexact adaptive condition that depends on only the\ncurrent iterate. Our proof controls the sufficient decrease of the function\nvalue over the total iterations rather than each iteration as used in the\nprevious studies, which can be of independent interest in other contexts. \n\n"}
{"id": "1808.07593", "contents": "Title: Caveats for information bottleneck in deterministic scenarios Abstract: Information bottleneck (IB) is a method for extracting information from one\nrandom variable $X$ that is relevant for predicting another random variable\n$Y$. To do so, IB identifies an intermediate \"bottleneck\" variable $T$ that has\nlow mutual information $I(X;T)$ and high mutual information $I(Y;T)$. The \"IB\ncurve\" characterizes the set of bottleneck variables that achieve maximal\n$I(Y;T)$ for a given $I(X;T)$, and is typically explored by maximizing the \"IB\nLagrangian\", $I(Y;T) - \\beta I(X;T)$. In some cases, $Y$ is a deterministic\nfunction of $X$, including many classification problems in supervised learning\nwhere the output class $Y$ is a deterministic function of the input $X$. We\ndemonstrate three caveats when using IB in any situation where $Y$ is a\ndeterministic function of $X$: (1) the IB curve cannot be recovered by\nmaximizing the IB Lagrangian for different values of $\\beta$; (2) there are\n\"uninteresting\" trivial solutions at all points of the IB curve; and (3) for\nmulti-layer classifiers that achieve low prediction error, different layers\ncannot exhibit a strict trade-off between compression and prediction, contrary\nto a recent proposal. We also show that when $Y$ is a small perturbation away\nfrom being a deterministic function of $X$, these three caveats arise in an\napproximate way. To address problem (1), we propose a functional that, unlike\nthe IB Lagrangian, can recover the IB curve in all cases. We demonstrate the\nthree caveats on the MNIST dataset. \n\n"}
{"id": "1808.08271", "contents": "Title: An elementary introduction to information geometry Abstract: In this survey, we describe the fundamental differential-geometric structures\nof information manifolds, state the fundamental theorem of information\ngeometry, and illustrate some use cases of these information manifolds in\ninformation sciences. The exposition is self-contained by concisely introducing\nthe necessary concepts of differential geometry, but proofs are omitted for\nbrevity. \n\n"}
{"id": "1808.08317", "contents": "Title: To Cluster, or Not to Cluster: An Analysis of Clusterability Methods Abstract: Clustering is an essential data mining tool that aims to discover inherent\ncluster structure in data. For most applications, applying clustering is only\nappropriate when cluster structure is present. As such, the study of\nclusterability, which evaluates whether data possesses such structure, is an\nintegral part of cluster analysis. However, methods for evaluating\nclusterability vary radically, making it challenging to select a suitable\nmeasure. In this paper, we perform an extensive comparison of measures of\nclusterability and provide guidelines that clustering users can reference to\nselect suitable measures for their applications. \n\n"}
{"id": "1808.08558", "contents": "Title: Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis\n  and its Generalization Error Abstract: Compression techniques for deep neural network models are becoming very\nimportant for the efficient execution of high-performance deep learning systems\non edge-computing devices. The concept of model compression is also important\nfor analyzing the generalization error of deep learning, known as the\ncompression-based error bound. However, there is still huge gap between a\npractically effective compression method and its rigorous background of\nstatistical learning theory. To resolve this issue, we develop a new\ntheoretical framework for model compression and propose a new pruning method\ncalled {\\it spectral pruning} based on this framework. We define the ``degrees\nof freedom'' to quantify the intrinsic dimensionality of a model by using the\neigenvalue distribution of the covariance matrix across the internal nodes and\nshow that the compression ability is essentially controlled by this quantity.\nMoreover, we present a sharp generalization error bound of the compressed model\nand characterize the bias--variance tradeoff induced by the compression\nprocedure. We apply our method to several datasets to justify our theoretical\nanalyses and show the superiority of the the proposed method. \n\n"}
{"id": "1808.09105", "contents": "Title: SOLAR: Deep Structured Representations for Model-Based Reinforcement\n  Learning Abstract: Model-based reinforcement learning (RL) has proven to be a data efficient\napproach for learning control tasks but is difficult to utilize in domains with\ncomplex observations such as images. In this paper, we present a method for\nlearning representations that are suitable for iterative model-based policy\nimprovement, even when the underlying dynamical system has complex dynamics and\nimage observations, in that these representations are optimized for inferring\nsimple dynamics and cost models given data from the current policy. This\nenables a model-based RL method based on the linear-quadratic regulator (LQR)\nto be used for systems with image observations. We evaluate our approach on a\nrange of robotics tasks, including manipulation with a real-world robotic arm\ndirectly from images. We find that our method produces substantially better\nfinal performance than other model-based RL methods while being significantly\nmore efficient than model-free RL. \n\n"}
{"id": "1808.10585", "contents": "Title: On the Minimal Supervision for Training Any Binary Classifier from Only\n  Unlabeled Data Abstract: Empirical risk minimization (ERM), with proper loss function and\nregularization, is the common practice of supervised classification. In this\npaper, we study training arbitrary (from linear to deep) binary classifier from\nonly unlabeled (U) data by ERM. We prove that it is impossible to estimate the\nrisk of an arbitrary binary classifier in an unbiased manner given a single set\nof U data, but it becomes possible given two sets of U data with different\nclass priors. These two facts answer a fundamental question---what the minimal\nsupervision is for training any binary classifier from only U data. Following\nthese findings, we propose an ERM-based learning method from two sets of U\ndata, and then prove it is consistent. Experiments demonstrate the proposed\nmethod could train deep models and outperform state-of-the-art methods for\nlearning from two sets of U data. \n\n"}
{"id": "1808.10850", "contents": "Title: Quantum walks in external gauge fields Abstract: Describing a particle in an external electromagnetic field is a basic task of\nquantum mechanics. The standard scheme for this is known as \"minimal coupling\",\nand consists of replacing the momentum operators in the Hamiltonian by modified\nones with an added vector potential. In lattice systems it is not so clear how\nto do this, because there is no continuous translation symmetry, and hence\nthere are no momenta. Moreover, when time is also discrete, as in quantum walk\nsystems, there is no Hamiltonian, only a unitary step operator. We present a\nunified framework of gauge theory for such discrete systems, keeping a close\nanalogy to the continuum case. In particular, we show how to implement minimal\ncoupling in a way that automatically guarantees unitary dynamics. The scheme\nworks in any lattice dimension, for any number of internal degree of freedom,\nfor walks that allow jumps to a finite neighborhood rather than to nearest\nneighbours, is naturally gauge invariant, and prepares possible extensions to\nnon-abelian gauge groups. \n\n"}
{"id": "1809.00082", "contents": "Title: NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation Abstract: Effective feature representation is key to the predictive performance of any\nalgorithm. This paper introduces a meta-procedure, called Non-Euclidean\nUpgrading (NEU), which learns feature maps that are expressive enough to embed\nthe universal approximation property (UAP) into most model classes while only\noutputting feature maps that preserve any model class's UAP. We show that NEU\ncan learn any feature map with these two properties if that feature map is\nasymptotically deformable into the identity. We also find that the\nfeature-representations learned by NEU are always submanifolds of the feature\nspace. NEU's properties are derived from a new deep neural model that is\nuniversal amongst all orientation-preserving homeomorphisms on the input space.\nWe derive qualitative and quantitative approximation guarantees for this\narchitecture. We quantify the number of parameters required for this new\narchitecture to memorize any set of input-output pairs while simultaneously\nfixing every point of the input space lying outside some compact set, and we\nquantify the size of this set as a function of our model's depth. Moreover, we\nshow that no deep feed-forward network with commonly used activation function\nhas all these properties. NEU's performance is evaluated against competing\nmachine learning methods on various regression and dimension reduction tasks\nboth with financial and simulated data. \n\n"}
{"id": "1809.00716", "contents": "Title: InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes\n  Dataset Abstract: Datasets have gained an enormous amount of popularity in the computer vision\ncommunity, from training and evaluation of Deep Learning-based methods to\nbenchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt,\nsynthetic imagery bears a vast potential due to scalability in terms of amounts\nof data obtainable without tedious manual ground truth annotations or\nmeasurements. Here, we present a dataset with the aim of providing a higher\ndegree of photo-realism, larger scale, more variability as well as serving a\nwider range of purposes compared to existing datasets. Our dataset leverages\nthe availability of millions of professional interior designs and millions of\nproduction-level furniture and object assets -- all coming with fine geometric\ndetails and high-resolution texture. We render high-resolution and high\nframe-rate video sequences following realistic trajectories while supporting\nvarious camera types as well as providing inertial measurements. Together with\nthe release of the dataset, we will make executable program of our interactive\nsimulator software as well as our renderer available at\nhttps://interiornetdataset.github.io. To showcase the usability and uniqueness\nof our dataset, we show benchmarking results of both sparse and dense SLAM\nalgorithms. \n\n"}
{"id": "1809.01129", "contents": "Title: Lipschitz Networks and Distributional Robustness Abstract: Robust risk minimisation has several advantages: it has been studied with\nregards to improving the generalisation properties of models and robustness to\nadversarial perturbation. We bound the distributionally robust risk for a model\nclass rich enough to include deep neural networks by a regularised empirical\nrisk involving the Lipschitz constant of the model. This allows us to\ninterpretand quantify the robustness properties of a deep neural network. As an\napplication we show the distributionally robust risk upperbounds the\nadversarial training risk. \n\n"}
{"id": "1809.01293", "contents": "Title: Stochastic Particle-Optimization Sampling and the Non-Asymptotic\n  Convergence Theory Abstract: Particle-optimization-based sampling (POS) is a recently developed effective\nsampling technique that interactively updates a set of particles. A\nrepresentative algorithm is the Stein variational gradient descent (SVGD). We\nprove, under certain conditions, SVGD experiences a theoretical pitfall, {\\it\ni.e.}, particles tend to collapse. As a remedy, we generalize POS to a\nstochastic setting by injecting random noise into particle updates, thus\nyielding particle-optimization sampling (SPOS). Notably, for the first time, we\ndevelop {\\em non-asymptotic convergence theory} for the SPOS framework (related\nto SVGD), characterizing algorithm convergence in terms of the 1-Wasserstein\ndistance w.r.t.\\! the numbers of particles and iterations. Somewhat\nsurprisingly, with the same number of updates (not too large) for each\nparticle, our theory suggests adopting more particles does not necessarily lead\nto a better approximation of a target distribution, due to limited\ncomputational budget and numerical errors. This phenomenon is also observed in\nSVGD and verified via an experiment on synthetic data. Extensive experimental\nresults verify our theory and demonstrate the effectiveness of our proposed\nframework. \n\n"}
{"id": "1809.01534", "contents": "Title: Utilizing Character and Word Embeddings for Text Normalization with\n  Sequence-to-Sequence Models Abstract: Text normalization is an important enabling technology for several NLP tasks.\nRecently, neural-network-based approaches have outperformed well-established\nmodels in this task. However, in languages other than English, there has been\nlittle exploration in this direction. Both the scarcity of annotated data and\nthe complexity of the language increase the difficulty of the problem. To\naddress these challenges, we use a sequence-to-sequence model with\ncharacter-based attention, which in addition to its self-learned character\nembeddings, uses word embeddings pre-trained with an approach that also models\nsubword information. This provides the neural model with access to more\nlinguistic information especially suitable for text normalization, without\nlarge parallel corpora. We show that providing the model with word-level\nfeatures bridges the gap for the neural network approach to achieve a\nstate-of-the-art F1 score on a standard Arabic language correction shared task\ndataset. \n\n"}
{"id": "1809.01571", "contents": "Title: Knowledge Integrated Classifier Design Based on Utility Optimization Abstract: This paper proposes a systematic framework to design a classification model\nthat yields a classifier which optimizes a utility function based on prior\nknowledge. Specifically, as the data size grows, we prove that the produced\nclassifier asymptotically converges to the optimal classifier, an extended\nversion of the Bayes rule, which maximizes the utility function. Therefore, we\nprovide a meaningful theoretical interpretation for modeling with the knowledge\nincorporated. Our knowledge incorporation method allows domain experts to guide\nthe classifier towards correctly classifying data that they think to be more\nsignificant. \n\n"}
{"id": "1809.02292", "contents": "Title: A Block Coordinate Ascent Algorithm for Mean-Variance Optimization Abstract: Risk management in dynamic decision problems is a primary concern in many\nfields, including financial investment, autonomous driving, and healthcare. The\nmean-variance function is one of the most widely used objective functions in\nrisk management due to its simplicity and interpretability. Existing algorithms\nfor mean-variance optimization are based on multi-time-scale stochastic\napproximation, whose learning rate schedules are often hard to tune, and have\nonly asymptotic convergence proof. In this paper, we develop a model-free\npolicy search framework for mean-variance optimization with finite-sample error\nbound analysis (to local optima). Our starting point is a reformulation of the\noriginal mean-variance function with its Fenchel dual, from which we propose a\nstochastic block coordinate ascent policy search algorithm. Both the asymptotic\nconvergence guarantee of the last iteration's solution and the convergence rate\nof the randomly picked solution are provided, and their applicability is\ndemonstrated on several benchmark domains. \n\n"}
{"id": "1809.02306", "contents": "Title: Unsupervised Cross-lingual Word Embedding by Multilingual Neural\n  Language Models Abstract: We propose an unsupervised method to obtain cross-lingual embeddings without\nany parallel data or pre-trained word embeddings. The proposed model, which we\ncall multilingual neural language models, takes sentences of multiple languages\nas an input. The proposed model contains bidirectional LSTMs that perform as\nforward and backward language models, and these networks are shared among all\nthe languages. The other parameters, i.e. word embeddings and linear\ntransformation between hidden states and outputs, are specific to each\nlanguage. The shared LSTMs can capture the common sentence structure among all\nlanguages. Accordingly, word embeddings of each language are mapped into a\ncommon latent space, making it possible to measure the similarity of words\nacross multiple languages. We evaluate the quality of the cross-lingual word\nembeddings on a word alignment task. Our experiments demonstrate that our model\ncan obtain cross-lingual embeddings of much higher quality than existing\nunsupervised models when only a small amount of monolingual data (i.e. 50k\nsentences) are available, or the domains of monolingual data are different\nacross languages. \n\n"}
{"id": "1809.02589", "contents": "Title: HyperGCN: A New Method of Training Graph Convolutional Networks on\n  Hypergraphs Abstract: In many real-world network datasets such as co-authorship, co-citation, email\ncommunication, etc., relationships are complex and go beyond pairwise.\nHypergraphs provide a flexible and natural modeling tool to model such complex\nrelationships. The obvious existence of such complex relationships in many\nreal-world networks naturaly motivates the problem of learning with\nhypergraphs. A popular learning paradigm is hypergraph-based semi-supervised\nlearning (SSL) where the goal is to assign labels to initially unlabeled\nvertices in a hypergraph. Motivated by the fact that a graph convolutional\nnetwork (GCN) has been effective for graph-based SSL, we propose HyperGCN, a\nnovel GCN for SSL on attributed hypergraphs. Additionally, we show how HyperGCN\ncan be used as a learning-based approach for combinatorial optimisation on\nNP-hard hypergraph problems. We demonstrate HyperGCN's effectiveness through\ndetailed experimentation on real-world hypergraphs. \n\n"}
{"id": "1809.02670", "contents": "Title: RetGK: Graph Kernels based on Return Probabilities of Random Walks Abstract: Graph-structured data arise in wide applications, such as computer vision,\nbioinformatics, and social networks. Quantifying similarities among graphs is a\nfundamental problem. In this paper, we develop a framework for computing graph\nkernels, based on return probabilities of random walks. The advantages of our\nproposed kernels are that they can effectively exploit various node attributes,\nwhile being scalable to large datasets. We conduct extensive graph\nclassification experiments to evaluate our graph kernels. The experimental\nresults show that our graph kernels significantly outperform existing\nstate-of-the-art approaches in both accuracy and computational efficiency. \n\n"}
{"id": "1809.04855", "contents": "Title: Stochastic Variational Optimization Abstract: Variational Optimization forms a differentiable upper bound on an objective.\nWe show that approaches such as Natural Evolution Strategies and Gaussian\nPerturbation, are special cases of Variational Optimization in which the\nexpectations are approximated by Gaussian sampling. These approaches are of\nparticular interest because they are parallelizable. We calculate the\napproximate bias and variance of the corresponding gradient estimators and\ndemonstrate that using antithetic sampling or a baseline is crucial to mitigate\ntheir problems. We contrast these methods with an alternative parallelizable\nmethod, namely Directional Derivatives. We conclude that, for differentiable\nobjectives, using Directional Derivatives is preferable to using Variational\nOptimization to perform parallel Stochastic Gradient Descent. \n\n"}
{"id": "1809.05014", "contents": "Title: Statistical Estimation of Ergodic Markov Chain Kernel over Discrete\n  State Space Abstract: We investigate the statistical complexity of estimating the parameters of a\ndiscrete-state Markov chain kernel from a single long sequence of state\nobservations. In the finite case, we characterize (modulo logarithmic factors)\nthe minimax sample complexity of estimation with respect to the operator\ninfinity norm, while in the countably infinite case, we analyze the problem\nwith respect to a natural entry-wise norm derived from total variation. We show\nthat in both cases, the sample complexity is governed by the mixing properties\nof the unknown chain, for which, in the finite-state case, there are known\nfinite-sample estimators with fully empirical confidence intervals. \n\n"}
{"id": "1809.05650", "contents": "Title: Detecting and Explaining Drifts in Yearly Grant Applications Abstract: During the lifetime of a Business Process changes can be made to the\nworkflow, the required resources, required documents, . . . . Different traces\nfrom the same Business Process within a single log file can thus differ\nsubstantially due to these changes. We propose a method that is able to detect\nconcept drift in multivariate log files with a dozen attributes. We test our\napproach on the BPI Challenge 2018 data con- sisting of applications for EU\ndirect payment from farmers in Germany where we use it to detect Concept Drift.\nIn contrast to other methods our algorithm does not require the manual\nselection of the features used to detect drift. Our method first creates a\nmodel that captures the re- lations between attributes and between events of\ndifferent time steps. This model is then used to score every event and trace.\nThese scores can be used to detect outlying cases and concept drift. Thanks to\nthe decomposability of the score we are able to perform detailed root-cause\nanalysis. \n\n"}
{"id": "1809.05815", "contents": "Title: Linear Independent Component Analysis over Finite Fields: Algorithms and\n  Bounds Abstract: Independent Component Analysis (ICA) is a statistical tool that decomposes an\nobserved random vector into components that are as statistically independent as\npossible. ICA over finite fields is a special case of ICA, in which both the\nobservations and the decomposed components take values over a finite alphabet.\nThis problem is also known as minimal redundancy representation or factorial\ncoding. In this work we focus on linear methods for ICA over finite fields. We\nintroduce a basic lower bound which provides a fundamental limit to the ability\nof any linear solution to solve this problem. Based on this bound, we present a\ngreedy algorithm that outperforms all currently known methods. Importantly, we\nshow that the overhead of our suggested algorithm (compared with the lower\nbound) typically decreases, as the scale of the problem grows. In addition, we\nprovide a sub-optimal variant of our suggested method that significantly\nreduces the computational complexity at a relatively small cost in performance.\nFinally, we discuss the universal abilities of linear transformations in\ndecomposing random vectors, compared with existing non-linear solutions. \n\n"}
{"id": "1809.06098", "contents": "Title: Policy Optimization via Importance Sampling Abstract: Policy optimization is an effective reinforcement learning approach to solve\ncontinuous control tasks. Recent achievements have shown that alternating\nonline and offline optimization is a successful choice for efficient trajectory\nreuse. However, deciding when to stop optimizing and collect new trajectories\nis non-trivial, as it requires to account for the variance of the objective\nfunction estimate. In this paper, we propose a novel, model-free, policy search\nalgorithm, POIS, applicable in both action-based and parameter-based settings.\nWe first derive a high-confidence bound for importance sampling estimation;\nthen we define a surrogate objective function, which is optimized offline\nwhenever a new batch of trajectories is collected. Finally, the algorithm is\ntested on a selection of continuous control tasks, with both linear and deep\npolicies, and compared with state-of-the-art policy optimization methods. \n\n"}
{"id": "1809.07802", "contents": "Title: Playing the Game of Universal Adversarial Perturbations Abstract: We study the problem of learning classifiers robust to universal adversarial\nperturbations. While prior work approaches this problem via robust\noptimization, adversarial training, or input transformation, we instead phrase\nit as a two-player zero-sum game. In this new formulation, both players\nsimultaneously play the same game, where one player chooses a classifier that\nminimizes a classification loss whilst the other player creates an adversarial\nperturbation that increases the same loss when applied to every sample in the\ntraining set. By observing that performing a classification (respectively\ncreating adversarial samples) is the best response to the other player, we\npropose a novel extension of a game-theoretic algorithm, namely fictitious\nplay, to the domain of training robust classifiers. Finally, we empirically\nshow the robustness and versatility of our approach in two defence scenarios\nwhere universal attacks are performed on several image classification datasets\n-- CIFAR10, CIFAR100 and ImageNet. \n\n"}
{"id": "1809.07823", "contents": "Title: Logically-Constrained Neural Fitted Q-Iteration Abstract: We propose a method for efficient training of Q-functions for\ncontinuous-state Markov Decision Processes (MDPs) such that the traces of the\nresulting policies satisfy a given Linear Temporal Logic (LTL) property. LTL, a\nmodal logic, can express a wide range of time-dependent logical properties\n(including \"safety\") that are quite similar to patterns in natural language. We\nconvert the LTL property into a limit deterministic Buchi automaton and\nconstruct an on-the-fly synchronised product MDP. The control policy is then\nsynthesised by defining an adaptive reward function and by applying a modified\nneural fitted Q-iteration algorithm to the synchronised structure, assuming\nthat no prior knowledge is available from the original MDP. The proposed method\nis evaluated in a numerical study to test the quality of the generated control\npolicy and is compared with conventional methods for policy synthesis such as\nMDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted\nvalue iteration). \n\n"}
{"id": "1809.08196", "contents": "Title: Analysis of Irregular Spatial Data with Machine Learning: Classification\n  of Building Patterns with a Graph Convolutional Neural Network Abstract: Machine learning methods such as convolutional neural networks (CNNs) are\nbecoming an integral part of scientific research in many disciplines, spatial\nvector data often fail to be analyzed using these powerful learning methods\nbecause of its irregularities. With the aid of graph Fourier transform and\nconvolution theorem, it is possible to convert the convolution as a point-wise\nproduct in Fourier domain and construct a learning architecture of CNN on graph\nfor the analysis task of irregular spatial data. In this study, we used the\nclassification task of building patterns as a case study to test this method,\nand experiments showed that this method has achieved outstanding results in\nidentifying regular and irregular patterns, and has significantly improved in\ncomparing with other methods. \n\n"}
{"id": "1809.08820", "contents": "Title: Orthogonally Decoupled Variational Gaussian Processes Abstract: Gaussian processes (GPs) provide a powerful non-parametric framework for\nreasoning over functions. Despite appealing theory, its superlinear\ncomputational and memory complexities have presented a long-standing challenge.\nState-of-the-art sparse variational inference methods trade modeling accuracy\nagainst complexity. However, the complexities of these methods still scale\nsuperlinearly in the number of basis functions, implying that that sparse GP\nmethods are able to learn from large datasets only when a small model is used.\nRecently, a decoupled approach was proposed that removes the unnecessary\ncoupling between the complexities of modeling the mean and the covariance\nfunctions of a GP. It achieves a linear complexity in the number of mean\nparameters, so an expressive posterior mean function can be modeled. While\npromising, this approach suffers from optimization difficulties due to\nill-conditioning and non-convexity. In this work, we propose an alternative\ndecoupled parametrization. It adopts an orthogonal basis in the mean function\nto model the residues that cannot be learned by the standard coupled approach.\nTherefore, our method extends, rather than replaces, the coupled approach to\nachieve strictly better performance. This construction admits a straightforward\nnatural gradient update rule, so the structure of the information manifold that\nis lost during decoupling can be leveraged to speed up learning. Empirically,\nour algorithm demonstrates significantly faster convergence in multiple\nexperiments. \n\n"}
{"id": "1809.10121", "contents": "Title: Safely Learning to Control the Constrained Linear Quadratic Regulator Abstract: We study the constrained linear quadratic regulator with unknown dynamics,\naddressing the tension between safety and exploration in data-driven control\ntechniques. We present a framework which allows for system identification\nthrough persistent excitation, while maintaining safety by guaranteeing the\nsatisfaction of state and input constraints. This framework involves a novel\nmethod for synthesizing robust constraint-satisfying feedback controllers,\nleveraging newly developed tools from system level synthesis. We connect\nstatistical results with cost sub-optimality bounds to give non-asymptotic\nguarantees on both estimation and controller performance. \n\n"}
{"id": "1810.00031", "contents": "Title: Active Fairness in Algorithmic Decision Making Abstract: Society increasingly relies on machine learning models for automated decision\nmaking. Yet, efficiency gains from automation have come paired with concern for\nalgorithmic discrimination that can systematize inequality. Recent work has\nproposed optimal post-processing methods that randomize classification\ndecisions for a fraction of individuals, in order to achieve fairness measures\nrelated to parity in errors and calibration. These methods, however, have\nraised concern due to the information inefficiency, intra-group unfairness, and\nPareto sub-optimality they entail. The present work proposes an alternative\nactive framework for fair classification, where, in deployment, a\ndecision-maker adaptively acquires information according to the needs of\ndifferent groups or individuals, towards balancing disparities in\nclassification performance. We propose two such methods, where information\ncollection is adapted to group- and individual-level needs respectively. We\nshow on real-world datasets that these can achieve: 1) calibration and single\nerror parity (e.g., equal opportunity); and 2) parity in both false positive\nand false negative rates (i.e., equal odds). Moreover, we show that by\nleveraging their additional degree of freedom, active approaches can\nsubstantially outperform randomization-based classifiers previously considered\noptimal, while avoiding limitations such as intra-group unfairness. \n\n"}
{"id": "1810.00440", "contents": "Title: Minimal Random Code Learning: Getting Bits Back from Compressed Model\n  Parameters Abstract: While deep neural networks are a highly successful model class, their large\nmemory footprint puts considerable strain on energy consumption, communication\nbandwidth, and storage requirements. Consequently, model size reduction has\nbecome an utmost goal in deep learning. A typical approach is to train a set of\ndeterministic weights, while applying certain techniques such as pruning and\nquantization, in order that the empirical weight distribution becomes amenable\nto Shannon-style coding schemes. However, as shown in this paper, relaxing\nweight determinism and using a full variational distribution over weights\nallows for more efficient coding schemes and consequently higher compression\nrates. In particular, following the classical bits-back argument, we encode the\nnetwork weights using a random sample, requiring only a number of bits\ncorresponding to the Kullback-Leibler divergence between the sampled\nvariational distribution and the encoding distribution. By imposing a\nconstraint on the Kullback-Leibler divergence, we are able to explicitly\ncontrol the compression rate, while optimizing the expected loss on the\ntraining set. The employed encoding scheme can be shown to be close to the\noptimal information-theoretical lower bound, with respect to the employed\nvariational family. Our method sets new state-of-the-art in neural network\ncompression, as it strictly dominates previous approaches in a Pareto sense: On\nthe benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best\ntest performance for a fixed memory budget, and vice versa, it achieves the\nhighest compression rates for a fixed test performance. \n\n"}
{"id": "1810.00553", "contents": "Title: Optimal Adaptive and Accelerated Stochastic Gradient Descent Abstract: Stochastic gradient descent (\\textsc{Sgd}) methods are the most powerful\noptimization tools in training machine learning and deep learning models.\nMoreover, acceleration (a.k.a. momentum) methods and diagonal scaling (a.k.a.\nadaptive gradient) methods are the two main techniques to improve the slow\nconvergence of \\textsc{Sgd}. While empirical studies have demonstrated\npotential advantages of combining these two techniques, it remains unknown\nwhether these methods can achieve the optimal rate of convergence for\nstochastic optimization. In this paper, we present a new class of adaptive and\naccelerated stochastic gradient descent methods and show that they exhibit the\noptimal sampling and iteration complexity for stochastic optimization. More\nspecifically, we show that diagonal scaling, initially designed to improve\nvanilla stochastic gradient, can be incorporated into accelerated stochastic\ngradient descent to achieve the optimal rate of convergence for smooth\nstochastic optimization. We also show that momentum, apart from being known to\nspeed up the convergence rate of deterministic optimization, also provides us\nnew ways of designing non-uniform and aggressive moving average schemes in\nstochastic optimization. Finally, we present some heuristics that help to\nimplement adaptive accelerated stochastic gradient descent methods and to\nfurther improve their practical performance for machine learning and deep\nlearning. \n\n"}
{"id": "1810.00956", "contents": "Title: Challenges of Using Text Classifiers for Causal Inference Abstract: Causal understanding is essential for many kinds of decision-making, but\ncausal inference from observational data has typically only been applied to\nstructured, low-dimensional datasets. While text classifiers produce\nlow-dimensional outputs, their use in causal inference has not previously been\nstudied. To facilitate causal analyses based on language data, we consider the\nrole that text classifiers can play in causal inference through established\nmodeling mechanisms from the causality literature on missing data and\nmeasurement error. We demonstrate how to conduct causal analyses using text\nclassifiers on simulated and Yelp data, and discuss the opportunities and\nchallenges of future work that uses text data in causal inference. \n\n"}
{"id": "1810.01256", "contents": "Title: Continual Learning of Context-dependent Processing in Neural Networks Abstract: Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it. \n\n"}
{"id": "1810.01588", "contents": "Title: Interpreting Layered Neural Networks via Hierarchical Modular\n  Representation Abstract: Interpreting the prediction mechanism of complex models is currently one of\nthe most important tasks in the machine learning field, especially with layered\nneural networks, which have achieved high predictive performance with various\npractical data sets. To reveal the global structure of a trained neural network\nin an interpretable way, a series of clustering methods have been proposed,\nwhich decompose the units into clusters according to the similarity of their\ninference roles. The main problems in these studies were that (1) we have no\nprior knowledge about the optimal resolution for the decomposition, or the\nappropriate number of clusters, and (2) there was no method with which to\nacquire knowledge about whether the outputs of each cluster have a positive or\nnegative correlation with the input and output dimension values. In this paper,\nto solve these problems, we propose a method for obtaining a hierarchical\nmodular representation of a layered neural network. The application of a\nhierarchical clustering method to a trained network reveals a tree-structured\nrelationship among hidden layer units, based on their feature vectors defined\nby their correlation with the input and output dimension values. \n\n"}
{"id": "1810.01683", "contents": "Title: Safe RuleFit: Learning Optimal Sparse Rule Model by Meta Safe Screening Abstract: We consider the problem of learning a sparse rule model, a prediction model\nin the form of a sparse linear combination of rules, where a rule is an\nindicator function defined over a hyper-rectangle in the input space. Since the\nnumber of all possible such rules is extremely large, it has been\ncomputationally intractable to select the optimal set of active rules. In this\npaper, to solve this difficulty for learning the optimal sparse rule model, we\npropose Safe RuleFit (SRF). Our basic idea is to develop meta safe screening\n(mSS), which is a non-trivial extension of well-known safe screening (SS)\ntechniques. While SS is used for screening out one feature, mSS can be used for\nscreening out multiple features by exploiting the inclusion-relations of\nhyper-rectangles in the input space. SRF provides a general framework for\nfitting sparse rule models for regression and classification, and it can be\nextended to handle more general sparse regularizations such as group\nregularization. We demonstrate the advantages of SRF through intensive\nnumerical experiments. \n\n"}
{"id": "1810.02225", "contents": "Title: Memristor-based Deep Convolution Neural Network: A Case Study Abstract: In this paper, we firstly introduce a method to efficiently implement\nlarge-scale high-dimensional convolution with realistic memristor-based circuit\ncomponents. An experiment verified simulator is adapted for accurate prediction\nof analog crossbar behavior. An improved conversion algorithm is developed to\nconvert convolution kernels to memristor-based circuits, which minimizes the\nerror with consideration of the data and kernel patterns in CNNs. With circuit\nsimulation for all convolution layers in ResNet-20, we found that 8-bit ADC/DAC\nis necessary to preserve software level classification accuracy. \n\n"}
{"id": "1810.02789", "contents": "Title: Doubly Semi-Implicit Variational Inference Abstract: We extend the existing framework of semi-implicit variational inference\n(SIVI) and introduce doubly semi-implicit variational inference (DSIVI), a way\nto perform variational inference and learning when both the approximate\nposterior and the prior distribution are semi-implicit. In other words, DSIVI\nperforms inference in models where the prior and the posterior can be expressed\nas an intractable infinite mixture of some analytic density with a highly\nflexible implicit mixing distribution. We provide a sandwich bound on the\nevidence lower bound (ELBO) objective that can be made arbitrarily tight.\nUnlike discriminator-based and kernel-based approaches to implicit variational\ninference, DSIVI optimizes a proper lower bound on ELBO that is asymptotically\nexact. We evaluate DSIVI on a set of problems that benefit from implicit\npriors. In particular, we show that DSIVI gives rise to a simple modification\nof VampPrior, the current state-of-the-art prior for variational autoencoders,\nwhich improves its performance. \n\n"}
{"id": "1810.03023", "contents": "Title: h-detach: Modifying the LSTM Gradient Towards Better Optimization Abstract: Recurrent neural networks are known for their notorious exploding and\nvanishing gradient problem (EVGP). This problem becomes more evident in tasks\nwhere the information needed to correctly solve them exist over long time\nscales, because EVGP prevents important gradient components from being\nback-propagated adequately over a large number of steps. We introduce a simple\nstochastic algorithm (\\textit{h}-detach) that is specific to LSTM optimization\nand targeted towards addressing this problem. Specifically, we show that when\nthe LSTM weights are large, the gradient components through the linear path\n(cell state) in the LSTM computational graph get suppressed. Based on the\nhypothesis that these components carry information about long term dependencies\n(which we show empirically), their suppression can prevent LSTMs from capturing\nthem. Our algorithm\\footnote{Our code is available at\nhttps://github.com/bhargav104/h-detach.} prevents gradients flowing through\nthis path from getting suppressed, thus allowing the LSTM to capture such\ndependencies better. We show significant improvements over vanilla LSTM\ngradient based training in terms of convergence speed, robustness to seed and\nlearning rate, and generalization using our modification of LSTM gradient on\nvarious benchmark datasets. \n\n"}
{"id": "1810.03222", "contents": "Title: Recovering Quantized Data with Missing Information Using Bilinear\n  Factorization and Augmented Lagrangian Method Abstract: In this paper, we propose a novel approach in order to recover a quantized\nmatrix with missing information. We propose a regularized convex cost function\ncomposed of a log-likelihood term and a Trace norm term. The Bi-factorization\napproach and the Augmented Lagrangian Method (ALM) are applied to find the\nglobal minimizer of the cost function in order to recover the genuine data. We\nprovide mathematical convergence analysis for our proposed algorithm. In the\nNumerical Experiments Section, we show the superiority of our method in\naccuracy and also its robustness in computational complexity compared to the\nstate-of-the-art literature methods. \n\n"}
{"id": "1810.03256", "contents": "Title: Deep Diffeomorphic Normalizing Flows Abstract: The Normalizing Flow (NF) models a general probability density by estimating\nan invertible transformation applied on samples drawn from a known\ndistribution. We introduce a new type of NF, called Deep Diffeomorphic\nNormalizing Flow (DDNF). A diffeomorphic flow is an invertible function where\nboth the function and its inverse are smooth. We construct the flow using an\nordinary differential equation (ODE) governed by a time-varying smooth vector\nfield. We use a neural network to parametrize the smooth vector field and a\nrecursive neural network (RNN) for approximating the solution of the ODE. Each\ncell in the RNN is a residual network implementing one Euler integration step.\nThe architecture of our flow enables efficient likelihood evaluation,\nstraightforward flow inversion, and results in highly flexible density\nestimation. An end-to-end trained DDNF achieves competitive results with\nstate-of-the-art methods on a suite of density estimation and variational\ninference tasks. Finally, our method brings concepts from Riemannian geometry\nthat, we believe, can open a new research direction for neural density\nestimation. \n\n"}
{"id": "1810.03463", "contents": "Title: Graph Embedding with Shifted Inner Product Similarity and Its Improved\n  Approximation Capability Abstract: We propose shifted inner-product similarity (SIPS), which is a novel yet very\nsimple extension of the ordinary inner-product similarity (IPS) for\nneural-network based graph embedding (GE). In contrast to IPS, that is limited\nto approximating positive-definite (PD) similarities, SIPS goes beyond the\nlimitation by introducing bias terms in IPS; we theoretically prove that SIPS\nis capable of approximating not only PD but also conditionally PD (CPD)\nsimilarities with many examples such as cosine similarity, negative Poincare\ndistance and negative Wasserstein distance. Since SIPS with sufficiently large\nneural networks learns a variety of similarities, SIPS alleviates the need for\nconfiguring the similarity function of GE. Approximation error rate is also\nevaluated, and experiments on two real-world datasets demonstrate that graph\nembedding using SIPS indeed outperforms existing methods. \n\n"}
{"id": "1810.03825", "contents": "Title: Adaptive Minimax Regret against Smooth Logarithmic Losses over\n  High-Dimensional $\\ell_1$-Balls via Envelope Complexity Abstract: We develop a new theoretical framework, the \\emph{envelope complexity}, to\nanalyze the minimax regret with logarithmic loss functions and derive a\nBayesian predictor that adaptively achieves the minimax regret over\nhigh-dimensional $\\ell_1$-balls within a factor of two. The prior is newly\nderived for achieving the minimax regret and called the\n\\emph{spike-and-tails~(ST) prior} as it looks like. The resulting regret bound\nis so simple that it is completely determined with the smoothness of the loss\nfunction and the radius of the balls except with logarithmic factors, and it\nhas a generalized form of existing regret/risk bounds. In the preliminary\nexperiment, we confirm that the ST prior outperforms the conventional\nminimax-regret prior under non-high-dimensional asymptotics. \n\n"}
{"id": "1810.04491", "contents": "Title: Multi-class Classification Model Inspired by Quantum Detection Theory Abstract: Machine Learning has become very famous currently which assist in identifying\nthe patterns from the raw data. Technological advancement has led to\nsubstantial improvement in Machine Learning which, thus helping to improve\nprediction. Current Machine Learning models are based on Classical Theory,\nwhich can be replaced by Quantum Theory to improve the effectiveness of the\nmodel. In the previous work, we developed binary classifier inspired by Quantum\nDetection Theory. In this extended abstract, our main goal is to develop\nmulti-class classifier. We generally use the terminology multinomial\nclassification or multi-class classification when we have a classification\nproblem for classifying observations or instances into one of three or more\nclasses. \n\n"}
{"id": "1810.04632", "contents": "Title: Non-linear process convolutions for multi-output Gaussian processes Abstract: The paper introduces a non-linear version of the process convolution\nformalism for building covariance functions for multi-output Gaussian\nprocesses. The non-linearity is introduced via Volterra series, one series per\neach output. We provide closed-form expressions for the mean function and the\ncovariance function of the approximated Gaussian process at the output of the\nVolterra series. The mean function and covariance function for the joint\nGaussian process are derived using formulae for the product moments of Gaussian\nvariables. We compare the performance of the non-linear model against the\nclassical process convolution approach in one synthetic dataset and two real\ndatasets. \n\n"}
{"id": "1810.04920", "contents": "Title: Pairwise Augmented GANs with Adversarial Reconstruction Loss Abstract: We propose a novel autoencoding model called Pairwise Augmented GANs. We\ntrain a generator and an encoder jointly and in an adversarial manner. The\ngenerator network learns to sample realistic objects. In turn, the encoder\nnetwork at the same time is trained to map the true data distribution to the\nprior in latent space. To ensure good reconstructions, we introduce an\naugmented adversarial reconstruction loss. Here we train a discriminator to\ndistinguish two types of pairs: an object with its augmentation and the one\nwith its reconstruction. We show that such adversarial loss compares objects\nbased on the content rather than on the exact match. We experimentally\ndemonstrate that our model generates samples and reconstructions of quality\ncompetitive with state-of-the-art on datasets MNIST, CIFAR10, CelebA and\nachieves good quantitative results on CIFAR10. \n\n"}
{"id": "1810.05193", "contents": "Title: Understanding Priors in Bayesian Neural Networks at the Unit Level Abstract: We investigate deep Bayesian neural networks with Gaussian weight priors and\na class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian\npriors are well known to induce an L2, \"weight decay\", regularization. Our\nresults characterize a more intricate regularization effect at the level of the\nunit activations. Our main result establishes that the induced prior\ndistribution on the units before and after activation becomes increasingly\nheavy-tailed with the depth of the layer. We show that first layer units are\nGaussian, second layer units are sub-exponential, and units in deeper layers\nare characterized by sub-Weibull distributions. Our results provide new\ntheoretical insight on deep Bayesian neural networks, which we corroborate with\nsimulation experiments. \n\n"}
{"id": "1810.05598", "contents": "Title: Tuning Fairness by Balancing Target Labels Abstract: The issue of fairness in machine learning models has recently attracted a lot\nof attention as ensuring it will ensure continued confidence of the general\npublic in the deployment of machine learning systems. We focus on mitigating\nthe harm incurred by a biased machine learning system that offers better\noutputs (e.g. loans, job interviews) for certain groups than for others. We\nshow that bias in the output can naturally be controlled in probabilistic\nmodels by introducing a latent target output. This formulation has several\nadvantages: first, it is a unified framework for several notions of group\nfairness such as Demographic Parity and Equality of Opportunity; second, it is\nexpressed as a marginalisation instead of a constrained problem; and third, it\nallows the encoding of our knowledge of what unbiased outputs should be.\nPractically, the second allows us to avoid unstable constrained optimisation\nprocedures and to reuse off-the-shelf toolboxes. The latter translates to the\nability to control the level of fairness by directly varying fairness target\nrates. In contrast, existing approaches rely on intermediate, arguably\nunintuitive, control parameters such as covariance thresholds. \n\n"}
{"id": "1810.06758", "contents": "Title: Discriminator Rejection Sampling Abstract: We propose a rejection sampling scheme using the discriminator of a GAN to\napproximately correct errors in the GAN generator distribution. We show that\nunder quite strict assumptions, this will allow us to recover the data\ndistribution exactly. We then examine where those strict assumptions break down\nand design a practical algorithm - called Discriminator Rejection Sampling\n(DRS) - that can be used on real data-sets. Finally, we demonstrate the\nefficacy of DRS on a mixture of Gaussians and on the SAGAN model,\nstate-of-the-art in the image generation task at the time of developing this\nwork. On ImageNet, we train an improved baseline that increases the Inception\nScore from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65\nto 14.79. We then use DRS to further improve on this baseline, improving the\nInception Score to 76.08 and the FID to 13.75. \n\n"}
{"id": "1810.06943", "contents": "Title: The Deep Weight Prior Abstract: Bayesian inference is known to provide a general framework for incorporating\nprior knowledge or specific properties into machine learning models via\ncarefully choosing a prior distribution. In this work, we propose a new type of\nprior distributions for convolutional neural networks, deep weight prior (DWP),\nthat exploit generative models to encourage a specific structure of trained\nconvolutional filters e.g., spatial correlations of weights. We define DWP in\nthe form of an implicit distribution and propose a method for variational\ninference with such type of implicit priors. In experiments, we show that DWP\nimproves the performance of Bayesian neural networks when training data are\nlimited, and initialization of weights with samples from DWP accelerates\ntraining of conventional convolutional neural networks. \n\n"}
{"id": "1810.06983", "contents": "Title: Decomposing feature-level variation with Covariate Gaussian Process\n  Latent Variable Models Abstract: The interpretation of complex high-dimensional data typically requires the\nuse of dimensionality reduction techniques to extract explanatory\nlow-dimensional representations. However, in many real-world problems these\nrepresentations may not be sufficient to aid interpretation on their own, and\nit would be desirable to interpret the model in terms of the original features\nthemselves. Our goal is to characterise how feature-level variation depends on\nlatent low-dimensional representations, external covariates, and non-linear\ninteractions between the two. In this paper, we propose to achieve this through\na structured kernel decomposition in a hybrid Gaussian Process model which we\ncall the Covariate Gaussian Process Latent Variable Model (c-GPLVM). We\ndemonstrate the utility of our model on simulated examples and applications in\ndisease progression modelling from high-dimensional gene expression data in the\npresence of additional phenotypes. In each setting we show how the c-GPLVM can\nextract low-dimensional structures from high-dimensional data sets whilst\nallowing a breakdown of feature-level variability that is not present in other\ncommonly used dimensionality reduction approaches. \n\n"}
{"id": "1810.07785", "contents": "Title: From Deep to Physics-Informed Learning of Turbulence: Diagnostics Abstract: We describe tests validating progress made toward acceleration and automation\nof hydrodynamic codes in the regime of developed turbulence by three Deep\nLearning (DL) Neural Network (NN) schemes trained on Direct Numerical\nSimulations of turbulence. Even the bare DL solutions, which do not take into\naccount any physics of turbulence explicitly, are impressively good overall\nwhen it comes to qualitative description of important features of turbulence.\nHowever, the early tests have also uncovered some caveats of the DL approaches.\nWe observe that the static DL scheme, implementing Convolutional GAN and\ntrained on spatial snapshots of turbulence, fails to reproduce intermittency of\nturbulent fluctuations at small scales and details of the turbulence geometry\nat large scales. We show that the dynamic NN schemes, namely LAT-NET and\nCompressed Convolutional LSTM, trained on a temporal sequence of turbulence\nsnapshots are capable to correct for the caveats of the static NN. We suggest a\npath forward towards improving reproducibility of the large-scale geometry of\nturbulence with NN. \n\n"}
{"id": "1810.07913", "contents": "Title: Robust Sparse Reduced Rank Regression in High Dimensions Abstract: We propose robust sparse reduced rank regression for analyzing large and\ncomplex high-dimensional data with heavy-tailed random noise. The proposed\nmethod is based on a convex relaxation of a rank- and sparsity-constrained\nnon-convex optimization problem, which is then solved using the alternating\ndirection method of multipliers algorithm. We establish non-asymptotic\nestimation error bounds under both Frobenius and nuclear norms in the\nhigh-dimensional setting. This is a major contribution over existing results in\nreduced rank regression, which mainly focus on rank selection and prediction\nconsistency. Our theoretical results quantify the tradeoff between\nheavy-tailedness of the random noise and statistical bias. For random noise\nwith bounded $(1+\\delta)$th moment with $\\delta \\in (0,1)$, the rate of\nconvergence is a function of $\\delta$, and is slower than the sub-Gaussian-type\ndeviation bounds; for random noise with bounded second moment, we obtain a rate\nof convergence as if sub-Gaussian noise were assumed. Furthermore, the\ntransition between the two regimes is smooth. We illustrate the performance of\nthe proposed method via extensive numerical studies and a data application. \n\n"}
{"id": "1810.07924", "contents": "Title: Explaining Machine Learning Models using Entropic Variable Projection Abstract: In this paper, we present a new explainability formalism designed to shed\nlight on how each input variable of a test set impacts the predictions of\nmachine learning models. Hence, we propose a group explainability formalism for\ntrained machine learning decision rules, based on their response to the\nvariability of the input variables distribution. In order to emphasize the\nimpact of each input variable, this formalism uses an information theory\nframework that quantifies the influence of all input-output observations based\non entropic projections. This is thus the first unified and model agnostic\nformalism enabling data scientists to interpret the dependence between the\ninput variables, their impact on the prediction errors, and their influence on\nthe output predictions. Convergence rates of the entropic projections are\nprovided in the large sample case. Most importantly, we prove that computing an\nexplanation in our framework has a low algorithmic complexity, making it\nscalable to real-life large datasets. We illustrate our strategy by explaining\ncomplex decision rules learned by using XGBoost, Random Forest or Deep Neural\nNetwork classifiers on various datasets such as Adult Income, MNIST, CelebA,\nBoston Housing, Iris, as well as synthetic ones. We finally make clear its\ndifferences with the explainability strategies LIME and SHAP, that are based on\nsingle observations. Results can be reproduced by using the freely distributed\nPython toolbox https://gems-ai.aniti.fr/. \n\n"}
{"id": "1810.08750", "contents": "Title: Learning Models with Uniform Performance via Distributionally Robust\n  Optimization Abstract: A common goal in statistics and machine learning is to learn models that can\nperform well against distributional shifts, such as latent heterogeneous\nsubpopulations, unknown covariate shifts, or unmodeled temporal effects. We\ndevelop and analyze a distributionally robust stochastic optimization (DRO)\nframework that learns a model providing good performance against perturbations\nto the data-generating distribution. We give a convex formulation for the\nproblem, providing several convergence guarantees. We prove finite-sample\nminimax upper and lower bounds, showing that distributional robustness\nsometimes comes at a cost in convergence rates. We give limit theorems for the\nlearned parameters, where we fully specify the limiting distribution so that\nconfidence intervals can be computed. On real tasks including generalizing to\nunknown subpopulations, fine-grained recognition, and providing good tail\nperformance, the distributionally robust approach often exhibits improved\nperformance. \n\n"}
{"id": "1810.09184", "contents": "Title: Learning sparse transformations through backpropagation Abstract: Many transformations in deep learning architectures are sparsely connected.\nWhen such transformations cannot be designed by hand, they can be learned, even\nthrough plain backpropagation, for instance in attention mechanisms. However,\nduring learning, such sparse structures are often represented in a dense form,\nas we do not know beforehand which elements will eventually become non-zero. We\nintroduce the adaptive, sparse hyperlayer, a method for learning a sparse\ntransformation, paramatrized sparsely: as index-tuples with associated values.\nTo overcome the lack of gradients from such a discrete structure, we introduce\na method of randomly sampling connections, and backpropagating over the\nrandomly wired computation graph. To show that this approach allows us to train\na model to competitive performance on real data, we use it to build two\narchitectures. First, an attention mechanism for visual classification. Second,\nwe implement a method for differentiable sorting: specifically, learning to\nsort unlabeled MNIST digits, given only the correct order. \n\n"}
{"id": "1810.09899", "contents": "Title: Dynamic Likelihood-free Inference via Ratio Estimation (DIRE) Abstract: Parametric statistical models that are implicitly defined in terms of a\nstochastic data generating process are used in a wide range of scientific\ndisciplines because they enable accurate modeling. However, learning the\nparameters from observed data is generally very difficult because their\nlikelihood function is typically intractable. Likelihood-free Bayesian\ninference methods have been proposed which include the frameworks of\napproximate Bayesian computation (ABC), synthetic likelihood, and its recent\ngeneralization that performs likelihood-free inference by ratio estimation\n(LFIRE). A major difficulty in all these methods is choosing summary statistics\nthat reduce the dimensionality of the data to facilitate inference. While\nseveral methods for choosing summary statistics have been proposed for ABC, the\nliterature for synthetic likelihood and LFIRE is very thin to date. We here\naddress this gap in the literature, focusing on the important special case of\ntime-series models. We show that convolutional neural networks trained to\npredict the input parameters from the data provide suitable summary statistics\nfor LFIRE. On a wide range of time-series models, a single neural network\narchitecture produced equally or more accurate posteriors than alternative\nmethods. \n\n"}
{"id": "1810.09945", "contents": "Title: Analyzing Neuroimaging Data Through Recurrent Deep Learning Models Abstract: The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples. \n\n"}
{"id": "1810.10775", "contents": "Title: Adversarially Robust Optimization with Gaussian Processes Abstract: In this paper, we consider the problem of Gaussian process (GP) optimization\nwith an added robustness requirement: The returned point may be perturbed by an\nadversary, and we require the function value to remain as high as possible even\nafter this perturbation. This problem is motivated by settings in which the\nunderlying functions during optimization and implementation stages are\ndifferent, or when one is interested in finding an entire region of good inputs\nrather than only a single point. We show that standard GP optimization\nalgorithms do not exhibit the desired robustness properties, and provide a\nnovel confidence-bound based algorithm StableOpt for this purpose. We\nrigorously establish the required number of samples for StableOpt to find a\nnear-optimal point, and we complement this guarantee with an\nalgorithm-independent lower bound. We experimentally demonstrate several\npotential applications of interest using real-world data sets, and we show that\nStableOpt consistently succeeds in finding a stable maximizer where several\nbaseline methods fail. \n\n"}
{"id": "1810.11677", "contents": "Title: The Variational Deficiency Bottleneck Abstract: We introduce a bottleneck method for learning data representations based on\ninformation deficiency, rather than the more traditional information\nsufficiency. A variational upper bound allows us to implement this method\nefficiently. The bound itself is bounded above by the variational information\nbottleneck objective, and the two methods coincide in the regime of single-shot\nMonte Carlo approximations. The notion of deficiency provides a principled way\nof approximating complicated channels by relatively simpler ones. We show that\nthe deficiency of one channel with respect to another has an operational\ninterpretation in terms of the optimal risk gap of decision problems, capturing\nclassification as a special case. Experiments demonstrate that the deficiency\nbottleneck can provide advantages in terms of minimal sufficiency as measured\nby information bottleneck curves, while retaining robust test performance in\nclassification tasks. \n\n"}
{"id": "1810.11750", "contents": "Title: Towards Understanding Learning Representations: To What Extent Do\n  Different Neural Networks Learn the Same Representation Abstract: It is widely believed that learning good representations is one of the main\nreasons for the success of deep neural networks. Although highly intuitive,\nthere is a lack of theory and systematic approach quantitatively characterizing\nwhat representations do deep neural networks learn. In this work, we move a\ntiny step towards a theory and better understanding of the representations.\nSpecifically, we study a simpler problem: How similar are the representations\nlearned by two networks with identical architecture but trained from different\ninitializations. We develop a rigorous theory based on the neuron activation\nsubspace match model. The theory gives a complete characterization of the\nstructure of neuron activation subspace matches, where the core concepts are\nmaximum match and simple match which describe the overall and the finest\nsimilarity between sets of neurons in two networks respectively. We also\npropose efficient algorithms to find the maximum match and simple matches.\nFinally, we conduct extensive experiments using our algorithms. Experimental\nresults suggest that, surprisingly, representations learned by the same\nconvolutional layers of networks trained from different initializations are not\nas similar as prevalently expected, at least in terms of subspace match. \n\n"}
{"id": "1810.11959", "contents": "Title: An Amalgamation of Classical and Quantum Machine Learning For the\n  Classification of Adenocarcinoma and Squamous Cell Carcinoma Patients Abstract: The ability to accurately classify disease subtypes is of vital importance,\nespecially in oncology where this capability could have a life saving impact.\nHere we report a classification between two subtypes of non-small cell lung\ncancer, namely Adeno- carcinoma vs Squamous cell carcinoma. The data consists\nof approximately 20,000 gene expression values for each of 104 patients. The\ndata was curated from [1] [2]. We used an amalgamation of classical and and\nquantum machine learning models to successfully classify these patients. We\nutilized feature selection methods based on univariate statistics in addition\nto XGBoost [3]. A novel and proprietary data representation method developed by\none of the authors called QCrush was also used as it was designed to\nincorporate a maximal amount of information under the size constraints of the\nD-Wave quantum annealing computer. The machine learning was performed by a\nQuantum Boltzmann Machine. This paper will report our results, the various\nclassical methods, and the quantum machine learning approach we utilized. \n\n"}
{"id": "1810.12161", "contents": "Title: Regularized Maximum Likelihood Estimation and Feature Selection in\n  Mixtures-of-Experts Models Abstract: Mixture of Experts (MoE) are successful models for modeling heterogeneous\ndata in many statistical learning problems including regression, clustering and\nclassification. Generally fitted by maximum likelihood estimation via the\nwell-known EM algorithm, their application to high-dimensional problems is\nstill therefore challenging. We consider the problem of fitting and feature\nselection in MoE models, and propose a regularized maximum likelihood\nestimation approach that encourages sparse solutions for heterogeneous\nregression data models with potentially high-dimensional predictors. Unlike\nstate-of-the art regularized MLE for MoE, the proposed modelings do not require\nan approximate of the penalty function. We develop two hybrid EM algorithms: an\nExpectation-Majorization-Maximization (EM/MM) algorithm, and an EM algorithm\nwith coordinate ascent algorithm. The proposed algorithms allow to\nautomatically obtaining sparse solutions without thresholding, and avoid matrix\ninversion by allowing univariate parameter updates. An experimental study shows\nthe good performance of the algorithms in terms of recovering the actual sparse\nsolutions, parameter estimation, and clustering of heterogeneous regression\ndata. \n\n"}
{"id": "1810.12361", "contents": "Title: Global Non-convex Optimization with Discretized Diffusions Abstract: An Euler discretization of the Langevin diffusion is known to converge to the\nglobal minimizers of certain convex and non-convex optimization problems. We\nshow that this property holds for any suitably smooth diffusion and that\ndifferent diffusions are suitable for optimizing different classes of convex\nand non-convex functions. This allows us to design diffusions suitable for\nglobally optimizing convex and non-convex functions not covered by the existing\nLangevin theory. Our non-asymptotic analysis delivers computable optimization\nand integration error bounds based on easily accessed properties of the\nobjective and chosen diffusion. Central to our approach are new explicit Stein\nfactor bounds on the solutions of Poisson equations. We complement these\nresults with improved optimization guarantees for targets other than the\nstandard Gibbs measure. \n\n"}
{"id": "1810.12460", "contents": "Title: A Novel Approach to Quantized Matrix Completion Using Huber Loss Measure Abstract: In this paper, we introduce a novel and robust approach to Quantized Matrix\nCompletion (QMC). First, we propose a rank minimization problem with\nconstraints induced by quantization bounds. Next, we form an unconstrained\noptimization problem by regularizing the rank function with Huber loss. Huber\nloss is leveraged to control the violation from quantization bounds due to two\nproperties: 1- It is differentiable, 2- It is less sensitive to outliers than\nthe quadratic loss. A Smooth Rank Approximation is utilized to endorse lower\nrank on the genuine data matrix. Thus, an unconstrained optimization problem\nwith differentiable objective function is obtained allowing us to advantage\nfrom Gradient Descent (GD) technique. Novel and firm theoretical analysis on\nproblem model and convergence of our algorithm to the global solution are\nprovided. Another contribution of our work is that our method does not require\nprojections or initial rank estimation unlike the state- of-the-art. In the\nNumerical Experiments Section, the noticeable outperformance of our proposed\nmethod in learning accuracy and computational complexity compared to those of\nthe state-of- the-art literature methods is illustrated as the main\ncontribution. \n\n"}
{"id": "1810.13317", "contents": "Title: Contrastive Multivariate Singular Spectrum Analysis Abstract: We introduce Contrastive Multivariate Singular Spectrum Analysis, a novel\nunsupervised method for dimensionality reduction and signal decomposition of\ntime series data. By utilizing an appropriate background dataset, the method\ntransforms a target time series dataset in a way that evinces the sub-signals\nthat are enhanced in the target dataset, as opposed to only those that account\nfor the greatest variance. This shifts the goal from finding signals that\nexplain the most variance to signals that matter the most to the analyst. We\ndemonstrate our method on an illustrative synthetic example, as well as show\nthe utility of our method in the downstream clustering of electrocardiogram\nsignals from the public MHEALTH dataset. \n\n"}
{"id": "1810.13425", "contents": "Title: Understanding Deep Neural Networks through Input Uncertainties Abstract: Techniques for understanding the functioning of complex machine learning\nmodels are becoming increasingly popular, not only to improve the validation\nprocess, but also to extract new insights about the data via exploratory\nanalysis. Though a large class of such tools currently exists, most assume that\npredictions are point estimates and use a sensitivity analysis of these\nestimates to interpret the model. Using lightweight probabilistic networks we\nshow how including prediction uncertainties in the sensitivity analysis leads\nto: (i) more robust and generalizable models; and (ii) a new approach for model\ninterpretation through uncertainty decomposition. In particular, we introduce a\nnew regularization that takes both the mean and variance of a prediction into\naccount and demonstrate that the resulting networks provide improved\ngeneralization to unseen data. Furthermore, we propose a new technique to\nexplain prediction uncertainties through uncertainties in the input domain,\nthus providing new ways to validate and interpret deep learning models. \n\n"}
{"id": "1811.00103", "contents": "Title: The Price of Fair PCA: One Extra Dimension Abstract: We investigate whether the standard dimensionality reduction technique of PCA\ninadvertently produces data representations with different fidelity for two\ndifferent populations. We show on several real-world data sets, PCA has higher\nreconstruction error on population A than on B (for example, women versus men\nor lower- versus higher-educated individuals). This can happen even when the\ndata set has a similar number of samples from A and B. This motivates our study\nof dimensionality reduction techniques which maintain similar fidelity for A\nand B. We define the notion of Fair PCA and give a polynomial-time algorithm\nfor finding a low dimensional representation of the data which is\nnearly-optimal with respect to this measure. Finally, we show on real-world\ndata sets that our algorithm can be used to efficiently generate a fair low\ndimensional representation of the data. \n\n"}
{"id": "1811.00152", "contents": "Title: Mixture Density Generative Adversarial Networks Abstract: Generative Adversarial Networks have surprising ability for generating sharp\nand realistic images, though they are known to suffer from the so-called mode\ncollapse problem. In this paper, we propose a new GAN variant called Mixture\nDensity GAN that while being capable of generating high-quality images,\novercomes this problem by encouraging the Discriminator to form clusters in its\nembedding space, which in turn leads the Generator to exploit these and\ndiscover different modes in the data. This is achieved by positioning Gaussian\ndensity functions in the corners of a simplex, using the resulting Gaussian\nmixture as a likelihood function over discriminator embeddings, and formulating\nan objective function for GAN training that is based on these likelihoods. We\ndemonstrate empirically (1) the quality of the generated images in Mixture\nDensity GAN and their strong similarity to real images, as measured by the\nFr\\'echet Inception Distance (FID), which compares very favourably with\nstate-of-the-art methods, and (2) the ability to avoid mode collapse and\ndiscover all data modes. \n\n"}
{"id": "1811.00293", "contents": "Title: Critical initialisation for deep signal propagation in noisy rectifier\n  neural networks Abstract: Stochastic regularisation is an important weapon in the arsenal of a deep\nlearning practitioner. However, despite recent theoretical advances, our\nunderstanding of how noise influences signal propagation in deep neural\nnetworks remains limited. By extending recent work based on mean field theory,\nwe develop a new framework for signal propagation in stochastic regularised\nneural networks. Our noisy signal propagation theory can incorporate several\ncommon noise distributions, including additive and multiplicative Gaussian\nnoise as well as dropout. We use this framework to investigate initialisation\nstrategies for noisy ReLU networks. We show that no critical initialisation\nstrategy exists using additive noise, with signal propagation exploding\nregardless of the selected noise distribution. For multiplicative noise (e.g.\ndropout), we identify alternative critical initialisation strategies that\ndepend on the second moment of the noise distribution. Simulations and\nexperiments on real-world data confirm that our proposed initialisation is able\nto stably propagate signals in deep networks, while using an initialisation\ndisregarding noise fails to do so. Furthermore, we analyse correlation dynamics\nbetween inputs. Stronger noise regularisation is shown to reduce the depth to\nwhich discriminatory information about the inputs to a noisy ReLU network is\nable to propagate, even when initialised at criticality. We support our\ntheoretical predictions for these trainable depths with simulations, as well as\nwith experiments on MNIST and CIFAR-10 \n\n"}
{"id": "1811.00542", "contents": "Title: Pymc-learn: Practical Probabilistic Machine Learning in Python Abstract: $\\textit{Pymc-learn}$ is a Python package providing a variety of\nstate-of-the-art probabilistic models for supervised and unsupervised machine\nlearning. It is inspired by $\\textit{scikit-learn}$ and focuses on bringing\nprobabilistic machine learning to non-specialists. It uses a general-purpose\nhigh-level language that mimics $\\textit{scikit-learn}$. Emphasis is put on\nease of use, productivity, flexibility, performance, documentation, and an API\nconsistent with $\\textit{scikit-learn}$. It depends on $\\textit{scikit-learn}$\nand $\\textit{pymc3}$ and is distributed under the new BSD-3 license,\nencouraging its use in both academia and industry. Source code, binaries, and\ndocumentation are available on http://github.com/pymc-learn/pymc-learn. \n\n"}
{"id": "1811.00577", "contents": "Title: Functional Nonlinear Sparse Models Abstract: Signal processing is rich in inherently continuous and often nonlinear\napplications, such as spectral estimation, optical imaging, and\nsuper-resolution microscopy, in which sparsity plays a key role in obtaining\nstate-of-the-art results. Coping with the infinite dimensionality and\nnon-convexity of these problems typically involves discretization and convex\nrelaxations, e.g., using atomic norms. Nevertheless, grid mismatch and other\ncoherence issues often lead to discretized versions of sparse signals that are\nnot sparse. Even if they are, recovering sparse solutions using convex\nrelaxations requires assumptions that may be hard to meet in practice. What is\nmore, problems involving nonlinear measurements remain non-convex even after\nrelaxing the sparsity objective. We address these issues by directly tackling\nthe continuous, nonlinear problem cast as a sparse functional optimization\nprogram. We prove that when these problems are non-atomic, they have no duality\ngap and can therefore be solved efficiently using duality and~(stochastic)\nconvex optimization methods. We illustrate the wide range of applications of\nthis approach by formulating and solving problems from nonlinear spectral\nestimation and robust classification. \n\n"}
{"id": "1811.00683", "contents": "Title: Quasi-random sampling for multivariate distributions via generative\n  neural networks Abstract: Generative moment matching networks (GMMNs) are introduced for generating\nquasi-random samples from multivariate models with any underlying copula in\norder to compute estimates under variance reduction. So far, quasi-random\nsampling for multivariate distributions required a careful design, exploiting\nspecific properties (such as conditional distributions) of the implied\nparametric copula or the underlying quasi-Monte Carlo (QMC) point set, and was\nonly tractable for a small number of models. Utilizing GMMNs allows one to\nconstruct quasi-random samples for a much larger variety of multivariate\ndistributions without such restrictions, including empirical ones from real\ndata with dependence structures not well captured by parametric copulas. Once\ntrained on pseudo-random samples from a parametric model or on real data, these\nneural networks only require a multivariate standard uniform randomized QMC\npoint set as input and are thus fast in estimating expectations of interest\nunder dependence with variance reduction. Numerical examples are considered to\ndemonstrate the approach, including applications inspired by risk management\npractice. All results are reproducible with the demos GMMN_QMC_paper,\nGMMN_QMC_data and GMMN_QMC_timings as part of the R package gnn. \n\n"}
{"id": "1811.00908", "contents": "Title: Single-Model Uncertainties for Deep Learning Abstract: We provide single-model estimates of aleatoric and epistemic uncertainty for\ndeep neural networks. To estimate aleatoric uncertainty, we propose\nSimultaneous Quantile Regression (SQR), a loss function to learn all the\nconditional quantiles of a given target variable. These quantiles can be used\nto compute well-calibrated prediction intervals. To estimate epistemic\nuncertainty, we propose Orthonormal Certificates (OCs), a collection of diverse\nnon-constant functions that map all training samples to zero. These\ncertificates map out-of-distribution examples to non-zero values, signaling\nepistemic uncertainty. Our uncertainty estimators are computationally\nattractive, as they do not require ensembling or retraining deep models, and\nachieve competitive performance. \n\n"}
{"id": "1811.01136", "contents": "Title: Margin-based Parallel Corpus Mining with Multilingual Sentence\n  Embeddings Abstract: Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version. \n\n"}
{"id": "1811.01159", "contents": "Title: Understanding and Comparing Scalable Gaussian Process Regression for Big\n  Data Abstract: As a non-parametric Bayesian model which produces informative predictive\ndistribution, Gaussian process (GP) has been widely used in various fields,\nlike regression, classification and optimization. The cubic complexity of\nstandard GP however leads to poor scalability, which poses challenges in the\nera of big data. Hence, various scalable GPs have been developed in the\nliterature in order to improve the scalability while retaining desirable\nprediction accuracy. This paper devotes to investigating the methodological\ncharacteristics and performance of representative global and local scalable GPs\nincluding sparse approximations and local aggregations from four main\nperspectives: scalability, capability, controllability and robustness. The\nnumerical experiments on two toy examples and five real-world datasets with up\nto 250K points offer the following findings. In terms of scalability, most of\nthe scalable GPs own a time complexity that is linear to the training size. In\nterms of capability, the sparse approximations capture the long-term spatial\ncorrelations, the local aggregations capture the local patterns but suffer from\nover-fitting in some scenarios. In terms of controllability, we could improve\nthe performance of sparse approximations by simply increasing the inducing\nsize. But this is not the case for local aggregations. In terms of robustness,\nlocal aggregations are robust to various initializations of hyperparameters due\nto the local attention mechanism. Finally, we highlight that the proper hybrid\nof global and local scalable GPs may be a promising way to improve both the\nmodel capability and scalability for big data. \n\n"}
{"id": "1811.01760", "contents": "Title: Kernel Conjugate Gradient Methods with Random Projections Abstract: We propose and study kernel conjugate gradient methods (KCGM) with random\nprojections for least-squares regression over a separable Hilbert space.\nConsidering two types of random projections generated by randomized sketches\nand Nystr\\\"{o}m subsampling, we prove optimal statistical results with respect\nto variants of norms for the algorithms under a suitable stopping rule.\nParticularly, our results show that if the projection dimension is proportional\nto the effective dimension of the problem, KCGM with randomized sketches can\ngeneralize optimally, while achieving a computational advantage. As a\ncorollary, we derive optimal rates for classic KCGM in the well-conditioned\nregimes for the case that the target function may not be in the hypothesis\nspace. \n\n"}
{"id": "1811.01969", "contents": "Title: Advances in Photonic Quantum Sensing Abstract: Quantum sensing has become a mature and broad field. It is generally related\nwith the idea of using quantum resources to boost the performance of a number\nof practical tasks, including the radar-like detection of faint objects, the\nreadout of information from optical memories or fragile physical systems, and\nthe optical resolution of extremely close point-like sources. Here we first\nfocus on the basic tools behind quantum sensing, discussing the most recent and\ngeneral formulations for the problems of quantum parameter estimation and\nhypothesis testing. With this basic background in our hands, we then review\nemerging applications of quantum sensing in the photonic regime both from a\ntheoretical and experimental point of view. Besides the state-of-the-art, we\nalso discuss open problems and potential next steps. \n\n"}
{"id": "1811.02033", "contents": "Title: Physics-Informed Generative Adversarial Networks for Stochastic\n  Differential Equations Abstract: We developed a new class of physics-informed generative adversarial networks\n(PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic\nproblems based on a limited number of scattered measurements. Unlike standard\nGANs relying only on data for training, here we encoded into the architecture\nof GANs the governing physical laws in the form of stochastic differential\nequations (SDEs) using automatic differentiation. In particular, we applied\nWasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability\ncompared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian\nprocesses of different correlation lengths based on data realizations collected\nfrom simultaneous reads at sparsely placed sensors. We obtained good\napproximation of the generated stochastic processes to the target ones even for\na mismatch between the input noise dimensionality and the effective\ndimensionality of the target stochastic processes. We also studied the\noverfitting issue for both the discriminator and generator, and we found that\noverfitting occurs also in the generator in addition to the discriminator as\npreviously reported. Subsequently, we considered the solution of elliptic SDEs\nrequiring approximations of three stochastic processes, namely the solution,\nthe forcing, and the diffusion coefficient. We used three generators for the\nPI-GANs, two of them were feed forward deep neural networks (DNNs) while the\nother one was the neural network induced by the SDE. Depending on the data, we\nemployed one or multiple feed forward DNNs as the discriminators in PI-GANs.\nHere, we have demonstrated the accuracy and effectiveness of PI-GANs in solving\nSDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high\ndimensional problems given more sensor data with low-polynomial growth in\ncomputational cost. \n\n"}
{"id": "1811.02091", "contents": "Title: Simple, Distributed, and Accelerated Probabilistic Programming Abstract: We describe a simple, low-level approach for embedding probabilistic\nprogramming in a deep learning ecosystem. In particular, we distill\nprobabilistic programming down to a single abstraction---the random variable.\nOur lightweight implementation in TensorFlow enables numerous applications: a\nmodel-parallel variational auto-encoder (VAE) with 2nd-generation tensor\nprocessing units (TPUv2s); a data-parallel autoregressive model (Image\nTransformer) with TPUv2s; and multi-GPU No-U-Turn Sampler (NUTS). For both a\nstate-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256\nCelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2\nchips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3. \n\n"}
{"id": "1811.02316", "contents": "Title: Stacked Penalized Logistic Regression for Selecting Views in Multi-View\n  Learning Abstract: In biomedical research, many different types of patient data can be\ncollected, such as various types of omics data and medical imaging modalities.\nApplying multi-view learning to these different sources of information can\nincrease the accuracy of medical classification models compared with\nsingle-view procedures. However, collecting biomedical data can be expensive\nand/or burdening for patients, so that it is important to reduce the amount of\nrequired data collection. It is therefore necessary to develop multi-view\nlearning methods which can accurately identify those views that are most\nimportant for prediction. In recent years, several biomedical studies have used\nan approach known as multi-view stacking (MVS), where a model is trained on\neach view separately and the resulting predictions are combined through\nstacking. In these studies, MVS has been shown to increase classification\naccuracy. However, the MVS framework can also be used for selecting a subset of\nimportant views. To study the view selection potential of MVS, we develop a\nspecial case called stacked penalized logistic regression (StaPLR). Compared\nwith existing view-selection methods, StaPLR can make use of faster\noptimization algorithms and is easily parallelized. We show that nonnegativity\nconstraints on the parameters of the function which combines the views play an\nimportant role in preventing unimportant views from entering the model. We\ninvestigate the performance of StaPLR through simulations, and consider two\nreal data examples. We compare the performance of StaPLR with an existing view\nselection method called the group lasso and observe that, in terms of view\nselection, StaPLR is often more conservative and has a consistently lower false\npositive rate. \n\n"}
{"id": "1811.02459", "contents": "Title: Nonlinear Evolution via Spatially-Dependent Linear Dynamics for\n  Electrophysiology and Calcium Data Abstract: Latent variable models have been widely applied for the analysis of time\nseries resulting from experimental neuroscience techniques. In these datasets,\nobservations are relatively smooth and possibly nonlinear. We present\nVariational Inference for Nonlinear Dynamics (VIND), a variational inference\nframework that is able to uncover nonlinear, smooth latent dynamics from\nsequential data. The framework is a direct extension of PfLDS; including a\nstructured approximate posterior describing spatially-dependent linear\ndynamics, as well as an algorithm that relies on the fixed-point iteration\nmethod to achieve convergence. We apply VIND to electrophysiology, single-cell\nvoltage and widefield imaging datasets with state-of-the-art results in\nreconstruction error. In single-cell voltage data, VIND finds a 5D latent\nspace, with variables akin to those of Hodgkin-Huxley-like models. VIND's\nlearned dynamics are further quantified by predicting future neural activity.\nVIND excels in this task, in some cases substantially outperforming current\nmethods. \n\n"}
{"id": "1811.02525", "contents": "Title: Double Adaptive Stochastic Gradient Optimization Abstract: Adaptive moment methods have been remarkably successful in deep learning\noptimization, particularly in the presence of noisy and/or sparse gradients. We\nfurther the advantages of adaptive moment techniques by proposing a family of\ndouble adaptive stochastic gradient methods~\\textsc{DASGrad}. They leverage the\ncomplementary ideas of the adaptive moment algorithms widely used by deep\nlearning community, and recent advances in adaptive probabilistic algorithms.We\nanalyze the theoretical convergence improvements of our approach in a\nstochastic convex optimization setting, and provide empirical validation of our\nfindings with convex and non convex objectives. We observe that the benefits\nof~\\textsc{DASGrad} increase with the model complexity and variability of the\ngradients, and we explore the resulting utility in extensions of\ndistribution-matching multitask learning. \n\n"}
{"id": "1811.02950", "contents": "Title: Quantum network transfer and storage with compact localized states\n  induced by local symmetries Abstract: We propose modulation protocols designed to generate, store and transfer\ncompact localized states in a quantum network. Induced by parameter tuning or\nlocal reflection symmetries, such states vanish outside selected domains of the\ncomplete system and are therefore ideal for information storage. Their creation\nand transfer is here achieved either via amplitude phase flips or via optimal\ntemporal control of inter-site couplings. We apply the concept to a decorated,\nlocally symmetric Lieb lattice where one sublattice is dimerized, and also\ndemonstrate it for more complex setups. The approach allows for a flexible\nstorage and transfer of states along independent paths in lattices supporting\nflat energetic bands. The generic network and protocols proposed can be\nutilized in various physical setups such as atomic or molecular spin lattices,\nphotonic waveguide arrays, and acoustic setups. \n\n"}
{"id": "1811.02979", "contents": "Title: Estimating Network Structure from Incomplete Event Data Abstract: Multivariate Bernoulli autoregressive (BAR) processes model time series of\nevents in which the likelihood of current events is determined by the times and\nlocations of past events. These processes can be used to model nonlinear\ndynamical systems corresponding to criminal activity, responses of patients to\ndifferent medical treatment plans, opinion dynamics across social networks,\nepidemic spread, and more. Past work examines this problem under the assumption\nthat the event data is complete, but in many cases only a fraction of events\nare observed. Incomplete observations pose a significant challenge in this\nsetting because the unobserved events still govern the underlying dynamical\nsystem. In this work, we develop a novel approach to estimating the parameters\nof a BAR process in the presence of unobserved events via an unbiased estimator\nof the complete data log-likelihood function. We propose a computationally\nefficient estimation algorithm which approximates this estimator via Taylor\nseries truncation and establish theoretical results for both the statistical\nerror and optimization error of our algorithm. We further justify our approach\nby testing our method on both simulated data and a real data set consisting of\ncrimes recorded by the city of Chicago. \n\n"}
{"id": "1811.03270", "contents": "Title: An Optimal Transport View on Generalization Abstract: We derive upper bounds on the generalization error of learning algorithms\nbased on their \\emph{algorithmic transport cost}: the expected Wasserstein\ndistance between the output hypothesis and the output hypothesis conditioned on\nan input example. The bounds provide a novel approach to study the\ngeneralization of learning algorithms from an optimal transport view and impose\nless constraints on the loss function, such as sub-gaussian or bounded. We\nfurther provide several upper bounds on the algorithmic transport cost in terms\nof total variation distance, relative entropy (or KL-divergence), and VC\ndimension, thus further bridging optimal transport theory and information\ntheory with statistical learning theory. Moreover, we also study different\nconditions for loss functions under which the generalization error of a\nlearning algorithm can be upper bounded by different probability metrics\nbetween distributions relating to the output hypothesis and/or the input data.\nFinally, under our established framework, we analyze the generalization in deep\nlearning and conclude that the generalization error in deep neural networks\n(DNNs) decreases exponentially to zero as the number of layers increases. Our\nanalyses of generalization error in deep learning mainly exploit the\nhierarchical structure in DNNs and the contraction property of $f$-divergence,\nwhich may be of independent interest in analyzing other learning models with\nhierarchical structure. \n\n"}
{"id": "1811.03679", "contents": "Title: Practical Bayesian Learning of Neural Networks via Adaptive Optimisation\n  Methods Abstract: We introduce a novel framework for the estimation of the posterior\ndistribution over the weights of a neural network, based on a new probabilistic\ninterpretation of adaptive optimisation algorithms such as AdaGrad and Adam. We\ndemonstrate the effectiveness of our Bayesian Adam method, Badam, by\nexperimentally showing that the learnt uncertainties correctly relate to the\nweights' predictive capabilities by weight pruning. We also demonstrate the\nquality of the derived uncertainty measures by comparing the performance of\nBadam to standard methods in a Thompson sampling setting for multi-armed\nbandits, where good uncertainty measures are required for an agent to balance\nexploration and exploitation. \n\n"}
{"id": "1811.04026", "contents": "Title: Adversarial Uncertainty Quantification in Physics-Informed Neural\n  Networks Abstract: We present a deep learning framework for quantifying and propagating\nuncertainty in systems governed by non-linear differential equations using\nphysics-informed neural networks. Specifically, we employ latent variable\nmodels to construct probabilistic representations for the system states, and\nput forth an adversarial inference procedure for training them on data, while\nconstraining their predictions to satisfy given physical laws expressed by\npartial differential equations. Such physics-informed constraints provide a\nregularization mechanism for effectively training deep generative models as\nsurrogates of physical systems in which the cost of data acquisition is high,\nand training data-sets are typically small. This provides a flexible framework\nfor characterizing uncertainty in the outputs of physical systems due to\nrandomness in their inputs or noise in their observations that entirely\nbypasses the need for repeatedly sampling expensive experiments or numerical\nsimulators. We demonstrate the effectiveness of our approach through a series\nof examples involving uncertainty propagation in non-linear conservation laws,\nand the discovery of constitutive laws for flow through porous media directly\nfrom noisy data. \n\n"}
{"id": "1811.04064", "contents": "Title: Block Belief Propagation for Parameter Learning in Markov Random Fields Abstract: Traditional learning methods for training Markov random fields require doing\ninference over all variables to compute the likelihood gradient. The iteration\ncomplexity for those methods therefore scales with the size of the graphical\nmodels. In this paper, we propose \\emph{block belief propagation learning}\n(BBPL), which uses block-coordinate updates of approximate marginals to compute\napproximate gradients, removing the need to compute inference on the entire\ngraphical model. Thus, the iteration complexity of BBPL does not scale with the\nsize of the graphs. We prove that the method converges to the same solution as\nthat obtained by using full inference per iteration, despite these\napproximations, and we empirically demonstrate its scalability improvements\nover standard training methods. \n\n"}
{"id": "1811.04288", "contents": "Title: IP Geolocation through Reverse DNS Abstract: IP Geolocation databases are widely used in online services to map end user\nIP addresses to their geographical locations. However, they use proprietary\ngeolocation methods and in some cases they have poor accuracy. We propose a\nsystematic approach to use publicly accessible reverse DNS hostnames for\ngeolocating IP addresses. Our method is designed to be combined with other\ngeolocation data sources. We cast the task as a machine learning problem where\nfor a given hostname, we generate and rank a list of potential location\ncandidates. We evaluate our approach against three state of the art academic\nbaselines and two state of the art commercial IP geolocation databases. We show\nthat our work significantly outperforms the academic baselines, and is\ncomplementary and competitive with commercial databases. To aid\nreproducibility, we open source our entire approach. \n\n"}
{"id": "1811.04319", "contents": "Title: Playing by the Book: An Interactive Game Approach for Action Graph\n  Extraction from Text Abstract: Understanding procedural text requires tracking entities, actions and effects\nas the narrative unfolds. We focus on the challenging real-world problem of\naction-graph extraction from material science papers, where language is highly\nspecialized and data annotation is expensive and scarce. We propose a novel\napproach, Text2Quest, where procedural text is interpreted as instructions for\nan interactive game. A learning agent completes the game by executing the\nprocedure correctly in a text-based simulated lab environment. The framework\ncan complement existing approaches and enables richer forms of learning\ncompared to static texts. We discuss potential limitations and advantages of\nthe approach, and release a prototype proof-of-concept, hoping to encourage\nresearch in this direction. \n\n"}
{"id": "1811.04451", "contents": "Title: Multi-Source Neural Variational Inference Abstract: Learning from multiple sources of information is an important problem in\nmachine-learning research. The key challenges are learning representations and\nformulating inference methods that take into account the complementarity and\nredundancy of various information sources. In this paper we formulate a\nvariational autoencoder based multi-source learning framework in which each\nencoder is conditioned on a different information source. This allows us to\nrelate the sources via the shared latent variables by computing divergence\nmeasures between individual source's posterior approximations. We explore a\nvariety of options to learn these encoders and to integrate the beliefs they\ncompute into a consistent posterior approximation. We visualise learned beliefs\non a toy dataset and evaluate our methods for learning shared representations\nand structured output prediction, showing trade-offs of learning separate\nencoders for each information source. Furthermore, we demonstrate how conflict\ndetection and redundancy can increase robustness of inference in a multi-source\nsetting. \n\n"}
{"id": "1811.04624", "contents": "Title: Importance Weighted Evolution Strategies Abstract: Evolution Strategies (ES) emerged as a scalable alternative to popular\nReinforcement Learning (RL) techniques, providing an almost perfect speedup\nwhen distributed across hundreds of CPU cores thanks to a reduced communication\noverhead. Despite providing large improvements in wall-clock time, ES is data\ninefficient when compared to competing RL methods. One of the main causes of\nsuch inefficiency is the collection of large batches of experience, which are\ndiscarded after each policy update. In this work, we study how to perform more\nthan one update per batch of experience by means of Importance Sampling while\npreserving the scalability of the original method. The proposed method,\nImportance Weighted Evolution Strategies (IW-ES), shows promising results and\nis a first step towards designing efficient ES algorithms. \n\n"}
{"id": "1811.04646", "contents": "Title: Global sensitivity analysis for optimization with variable selection Abstract: The optimization of high dimensional functions is a key issue in engineering\nproblems but it frequently comes at a cost that is not acceptable since it\nusually involves a complex and expensive computer code. Engineers often\novercome this limitation by first identifying which parameters drive the most\nthe function variations: non-influential variables are set to a fixed value and\nthe optimization procedure is carried out with the remaining influential\nvariables. Such variable selection is performed through influence measures that\nare meaningful for regression problems. However it does not account for the\nspecific structure of optimization problems where we would like to identify\nwhich variables most lead to constraints satisfaction and low values of the\nobjective function. In this paper, we propose a new sensitivity analysis that\naccounts for the specific aspects of optimization problems. In particular, we\nintroduce an influence measure based on the Hilbert-Schmidt Independence\nCriterion to characterize whether a design variable matters to reach low values\nof the objective function and to satisfy the constraints. This sensitivity\nmeasure makes it possible to sort the inputs and reduce the problem dimension.\nWe compare a random and a greedy strategies to set the values of the\nnon-influential variables before conducting a local optimization. Applications\nto several test-cases show that this variable selection and the greedy strategy\nsignificantly reduce the number of function evaluations at a limited cost in\nterms of solution performance. \n\n"}
{"id": "1811.05076", "contents": "Title: Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n  and its Statistical Optimality Abstract: We consider the problem of decomposing a higher-order tensor with binary\nentries. Such data problems arise frequently in applications such as\nneuroimaging, recommendation system, topic modeling, and sensor network\nlocalization. We propose a multilinear Bernoulli model, develop a\nrank-constrained likelihood-based estimation method, and obtain the theoretical\naccuracy guarantees. In contrast to continuous-valued problems, the binary\ntensor problem exhibits an interesting phase transition phenomenon according to\nthe signal-to-noise ratio. The error bound for the parameter tensor estimation\nis established, and we show that the obtained rate is minimax optimal under the\nconsidered model. Furthermore, we develop an alternating optimization algorithm\nwith convergence guarantees. The efficacy of our approach is demonstrated\nthrough both simulations and analyses of multiple data sets on the tasks of\ntensor completion and clustering. \n\n"}
{"id": "1811.07051", "contents": "Title: Symmetry constrained machine learning Abstract: Symmetry, a central concept in understanding the laws of nature, has been\nused for centuries in physics, mathematics, and chemistry, to help make\nmathematical models tractable. Yet, despite its power, symmetry has not been\nused extensively in machine learning, until rather recently. In this article we\nshow a general way to incorporate symmetries into machine learning models. We\ndemonstrate this with a detailed analysis on a rather simple real world machine\nlearning system - a neural network for classifying handwritten digits, lacking\nbias terms for every neuron. We demonstrate that ignoring symmetries can have\ndire over-fitting consequences, and that incorporating symmetry into the model\nreduces over-fitting, while at the same time reducing complexity, ultimately\nrequiring less training data, and taking less time and resources to train. \n\n"}
{"id": "1811.07073", "contents": "Title: Semi-Supervised Semantic Image Segmentation with Self-correcting\n  Networks Abstract: Building a large image dataset with high-quality object masks for semantic\nsegmentation is costly and time consuming. In this paper, we introduce a\nprincipled semi-supervised framework that only uses a small set of fully\nsupervised images (having semantic segmentation labels and box labels) and a\nset of images with only object bounding box labels (we call it the weak set).\nOur framework trains the primary segmentation model with the aid of an\nancillary model that generates initial segmentation labels for the weak set and\na self-correction module that improves the generated labels during training\nusing the increasingly accurate primary model. We introduce two variants of the\nself-correction module using either linear or convolutional functions.\nExperiments on the PASCAL VOC 2012 and Cityscape datasets show that our models\ntrained with a small fully supervised set perform similar to, or better than,\nmodels trained with a large fully supervised set while requiring ~7x less\nannotation effort. \n\n"}
{"id": "1811.07134", "contents": "Title: Deep Discriminative Learning for Unsupervised Domain Adaptation Abstract: The primary objective of domain adaptation methods is to transfer knowledge\nfrom a source domain to a target domain that has similar but different data\ndistributions. Thus, in order to correctly classify the unlabeled target domain\nsamples, the standard approach is to learn a common representation for both\nsource and target domain, thereby indirectly addressing the problem of learning\na classifier in the target domain. However, such an approach does not address\nthe task of classification in the target domain directly. In contrast, we\npropose an approach that directly addresses the problem of learning a\nclassifier in the unlabeled target domain. In particular, we train a classifier\nto correctly classify the training samples while simultaneously classifying the\nsamples in the target domain in an unsupervised manner. The corresponding model\nis referred to as Discriminative Encoding for Domain Adaptation (DEDA). We show\nthat this simple approach for performing unsupervised domain adaptation is\nindeed quite powerful. Our method achieves state of the art results in\nunsupervised adaptation tasks on various image classification benchmarks. We\nalso obtained state of the art performance on domain adaptation in Amazon\nreviews sentiment classification dataset. We perform additional experiments\nwhen the source data has less labeled examples and also on zero-shot domain\nadaptation task where no target domain samples are used for training. \n\n"}
{"id": "1811.08297", "contents": "Title: Finite Mixture Model of Nonparametric Density Estimation using Sampling\n  Importance Resampling for Persistence Landscape Abstract: Considering the creation of persistence landscape on a parametrized curve and\nstructure of sampling, there exists a random process for which a finite mixture\nmodel of persistence landscape (FMMPL) can provide a better description for a\ngiven dataset. In this paper, a nonparametric approach for computing integrated\nmean of square error (IMSE) in persistence landscape has been presented. As a\nresult, FMMPL is more accurate than the another way. Also, the sampling\nimportance resampling (SIR) has been presented a better description of\nimportant landmark from parametrized curve. The result, provides more accuracy\nand less space complexity than the landmarks selected with simple sampling. \n\n"}
{"id": "1811.08357", "contents": "Title: Learning deep kernels for exponential family densities Abstract: The kernel exponential family is a rich class of distributions, which can be\nfit efficiently and with statistical guarantees by score matching. Being\nrequired to choose a priori a simple kernel such as the Gaussian, however,\nlimits its practical applicability. We provide a scheme for learning a kernel\nparameterized by a deep network, which can find complex location-dependent\nlocal features of the data geometry. This gives a very rich class of density\nmodels, capable of fitting complex structures on moderate-dimensional problems.\nCompared to deep density models fit via maximum likelihood, our approach\nprovides a complementary set of strengths and tradeoffs: in empirical studies,\nthe former can yield higher likelihoods, whereas the latter gives better\nestimates of the gradient of the log density, the score, which describes the\ndistribution's shape. \n\n"}
{"id": "1811.08413", "contents": "Title: Sampling Can Be Faster Than Optimization Abstract: Optimization algorithms and Monte Carlo sampling algorithms have provided the\ncomputational foundations for the rapid growth in applications of statistical\nmachine learning in recent years. There is, however, limited theoretical\nunderstanding of the relationships between these two kinds of methodology, and\nlimited understanding of relative strengths and weaknesses. Moreover, existing\nresults have been obtained primarily in the setting of convex functions (for\noptimization) and log-concave functions (for sampling). In this setting, where\nlocal properties determine global properties, optimization algorithms are\nunsurprisingly more efficient computationally than sampling algorithms. We\ninstead examine a class of nonconvex objective functions that arise in mixture\nmodeling and multi-stable systems. In this nonconvex setting, we find that the\ncomputational complexity of sampling algorithms scales linearly with the model\ndimension while that of optimization algorithms scales exponentially. \n\n"}
{"id": "1811.08511", "contents": "Title: Joint association and classification analysis of multi-view data Abstract: Multi-view data, that is matched sets of measurements on the same subjects,\nhave become increasingly common with advances in multi-omics technology. Often,\nit is of interest to find associations between the views that are related to\nthe intrinsic class memberships. Existing association methods cannot directly\nincorporate class information, while existing classification methods do not\ntake into account between-views associations. In this work, we propose a\nframework for Joint Association and Classification Analysis of multi-view data\n(JACA). Our goal is not to merely improve the misclassification rates, but to\nprovide a latent representation of high-dimensional data that is both relevant\nfor the subtype discrimination and coherent across the views. We motivate the\nmethodology by establishing a connection between canonical correlation analysis\nand discriminant analysis. We also establish the estimation consistency of JACA\nin high-dimensional settings. A distinct advantage of JACA is that it can be\napplied to the multi-view data with block-missing structure, that is to cases\nwhere a subset of views or class labels is missing for some subjects. The\napplication of JACA to quantify the associations between RNAseq and miRNA views\nwith respect to consensus molecular subtypes in colorectal cancer data from The\nCancer Genome Atlas project leads to improved misclassification rates and\nstronger found associations compared to existing methods. \n\n"}
{"id": "1811.08723", "contents": "Title: Sequential Neural Methods for Likelihood-free Inference Abstract: Likelihood-free inference refers to inference when a likelihood function\ncannot be explicitly evaluated, which is often the case for models based on\nsimulators. Most of the literature is based on sample-based `Approximate\nBayesian Computation' methods, but recent work suggests that approaches based\non deep neural conditional density estimators can obtain state-of-the-art\nresults with fewer simulations. The neural approaches vary in how they choose\nwhich simulations to run and what they learn: an approximate posterior or a\nsurrogate likelihood. This work provides some direct controlled comparisons\nbetween these choices. \n\n"}
{"id": "1811.10735", "contents": "Title: Automatic Induction of Neural Network Decision Tree Algorithms Abstract: This work presents an approach to automatically induction for non-greedy\ndecision trees constructed from neural network architecture. This construction\ncan be used to transfer weights when growing or pruning a decision tree,\nallowing non-greedy decision tree algorithms to automatically learn and adapt\nto the ideal architecture. In this work, we examine the underpinning ideas\nwithin ensemble modelling and Bayesian model averaging which allow our neural\nnetwork to asymptotically approach the ideal architecture through weights\ntransfer. Experimental results demonstrate that this approach improves models\nover fixed set of hyperparameters for decision tree models and decision forest\nmodels. \n\n"}
{"id": "1811.10740", "contents": "Title: Mixture of Regression Experts in fMRI Encoding Abstract: fMRI semantic category understanding using linguistic encoding models attempt\nto learn a forward mapping that relates stimuli to the corresponding brain\nactivation. Classical encoding models use linear multi-variate methods to\npredict the brain activation (all voxels) given the stimulus. However, these\nmethods essentially assume multiple regions as one large uniform region or\nseveral independent regions, ignoring connections among them. In this paper, we\npresent a mixture of experts-based model where a group of experts captures\nbrain activity patterns related to particular regions of interest (ROI) and\nalso show the discrimination across different experts. The model is trained\nword stimuli encoded as 25-dimensional feature vectors as input and the\ncorresponding brain responses as output. Given a new word (25-dimensional\nfeature vector), it predicts the entire brain activation as the linear\ncombination of multiple experts brain activations. We argue that each expert\nlearns a certain region of brain activations corresponding to its category of\nwords, which solves the problem of identifying the regions with a simple\nencoding model. We showcase that proposed mixture of experts-based model indeed\nlearns region-based experts to predict the brain activations with high spatial\naccuracy. \n\n"}
{"id": "1811.10790", "contents": "Title: High-dimensional Index Volatility Models via Stein's Identity Abstract: We study the estimation of the parametric components of single and multiple\nindex volatility models. Using the first- and second-order Stein's identities,\nwe develop methods that are applicable for the estimation of the variance index\nin the high-dimensional setting requiring finite moment condition, which allows\nfor heavy-tailed data. Our approach complements the existing literature in the\nlow-dimensional setting, while relaxing the conditions on estimation, and\nprovides a novel approach in the high-dimensional setting. We prove that the\nstatistical rate of convergence of our variance index estimators consists of a\nparametric rate and a nonparametric rate, where the latter appears from the\nestimation of the mean link function. However, under standard assumptions, the\nparametric rate dominates the rate of convergence and our results match the\nminimax optimal rate for the mean index estimation. Simulation results\nillustrate finite sample properties of our methodology and back our theoretical\nconclusions. \n\n"}
{"id": "1811.10947", "contents": "Title: Reliable Semi-Supervised Learning when Labels are Missing at Random Abstract: Semi-supervised learning methods are motivated by the availability of large\ndatasets with unlabeled features in addition to labeled data. Unlabeled data\nis, however, not guaranteed to improve classification performance and has in\nfact been reported to impair the performance in certain cases. A fundamental\nsource of error arises from restrictive assumptions about the unlabeled\nfeatures, which result in unreliable classifiers that underestimate their\nprediction error probabilities. In this paper, we develop a semi-supervised\nlearning approach that relaxes such assumptions and is capable of providing\nclassifiers that reliably quantify the label uncertainty. The approach is\napplicable using any generative model with a supervised learning algorithm. We\nillustrate the approach using both handwritten digit and cloth classification\ndata where the labels are missing at random. \n\n"}
{"id": "1811.11339", "contents": "Title: Statistical Robust Chinese Remainder Theorem for Multiple Numbers:\n  Wrapped Gaussian Mixture Model Abstract: Generalized Chinese Remainder Theorem (CRT) has been shown to be a powerful\napproach to solve the ambiguity resolution problem. However, with its close\nrelationship to number theory, study in this area is mainly from a coding\ntheory perspective under deterministic conditions. Nevertheless, it can be\nproved that even with the best deterministic condition known, the probability\nof success in robust reconstruction degrades exponentially as the number of\nestimand increases. In this paper, we present the first rigorous analysis on\nthe underlying statistical model of CRT-based multiple parameter estimation,\nwhere a generalized Gaussian mixture with background knowledge on samplings is\nproposed. To address the problem, two novel approaches are introduced. One is\nto directly calculate the conditional maximal a posteriori probability (MAP)\nestimation of residue clustering, and the other is to iteratively search for\nMAP of both common residues and clustering. Moreover, remainder\nerror-correcting codes are introduced to improve the robustness further. It is\nshown that this statistically based scheme achieves much stronger robustness\ncompared to state-of-the-art deterministic schemes, especially in low and\nmedian Signal Noise Ratio (SNR) scenarios. \n\n"}
{"id": "1811.11368", "contents": "Title: First-order Newton-type Estimator for Distributed Estimation and\n  Inference Abstract: This paper studies distributed estimation and inference for a general\nstatistical problem with a convex loss that could be non-differentiable. For\nthe purpose of efficient computation, we restrict ourselves to stochastic\nfirst-order optimization, which enjoys low per-iteration complexity. To\nmotivate the proposed method, we first investigate the theoretical properties\nof a straightforward Divide-and-Conquer Stochastic Gradient Descent (DC-SGD)\napproach. Our theory shows that there is a restriction on the number of\nmachines and this restriction becomes more stringent when the dimension $p$ is\nlarge. To overcome this limitation, this paper proposes a new multi-round\ndistributed estimation procedure that approximates the Newton step only using\nstochastic subgradient. The key component in our method is the proposal of a\ncomputationally efficient estimator of $\\Sigma^{-1} w$, where $\\Sigma$ is the\npopulation Hessian matrix and $w$ is any given vector. Instead of estimating\n$\\Sigma$ (or $\\Sigma^{-1}$) that usually requires the second-order\ndifferentiability of the loss, the proposed First-Order Newton-type Estimator\n(FONE) directly estimates the vector of interest $\\Sigma^{-1} w$ as a whole and\nis applicable to non-differentiable losses. Our estimator also facilitates the\ninference for the empirical risk minimizer. It turns out that the key term in\nthe limiting covariance has the form of $\\Sigma^{-1} w$, which can be estimated\nby FONE. \n\n"}
{"id": "1811.11643", "contents": "Title: Bohmian mechanics for instrumentalists Abstract: We formulate Bohmian mechanics (BM) such that the main objects of concern are\nmacroscopic phenomena, while microscopic particle trajectories only play an\nauxiliary role. Such a formulation makes it easy to understand why BM always\nmakes the same measurable predictions as standard quantum mechanics (QM),\nirrespectively of the details of microscopic trajectories. Relativistic quantum\nfield theory (QFT) is interpreted as an effective long-distance theory that at\nsmaller distances must be replaced by some more fundamental theory. Analogy\nwith condensed-matter physics suggests that this more fundamental theory could\nhave a form of non-relativistic QM, offering a simple generic resolution of an\napparent conflict between BM and relativistic QFT. \n\n"}
{"id": "1811.12199", "contents": "Title: A Visual Interaction Framework for Dimensionality Reduction Based Data\n  Exploration Abstract: Dimensionality reduction is a common method for analyzing and visualizing\nhigh-dimensional data. However, reasoning dynamically about the results of a\ndimensionality reduction is difficult. Dimensionality-reduction algorithms use\ncomplex optimizations to reduce the number of dimensions of a dataset, but\nthese new dimensions often lack a clear relation to the initial data\ndimensions, thus making them difficult to interpret. Here we propose a visual\ninteraction framework to improve dimensionality-reduction based exploratory\ndata analysis. We introduce two interaction techniques, forward projection and\nbackward projection, for dynamically reasoning about dimensionally reduced\ndata. We also contribute two visualization techniques, prolines and feasibility\nmaps, to facilitate the effective use of the proposed interactions. We apply\nour framework to PCA and autoencoder-based dimensionality reductions. Through\ndata-exploration examples, we demonstrate how our visual interactions can\nimprove the use of dimensionality reduction in exploratory data analysis. \n\n"}
{"id": "1811.12258", "contents": "Title: BCCNet: Bayesian classifier combination neural network Abstract: Machine learning research for developing countries can demonstrate clear\nsustainable impact by delivering actionable and timely information to\nin-country government organisations (GOs) and NGOs in response to their\ncritical information requirements. We co-create products with UK and in-country\ncommercial, GO and NGO partners to ensure the machine learning algorithms\naddress appropriate user needs whether for tactical decision making or\nevidence-based policy decisions. In one particular case, we developed and\ndeployed a novel algorithm, BCCNet, to quickly process large quantities of\nunstructured data to prevent and respond to natural disasters. Crowdsourcing\nprovides an efficient mechanism to generate labels from unstructured data to\nprime machine learning algorithms for large scale data analysis. However, these\nlabels are often imperfect with qualities varying among different citizen\nscientists, which prohibits their direct use with many state-of-the-art machine\nlearning techniques. We describe BCCNet, a framework that simultaneously\naggregates biased and contradictory labels from the crowd and trains an\nautomatic classifier to process new data. Our case studies, mosquito sound\ndetection for malaria prevention and damage detection for disaster response,\nshow the efficacy of our method in the challenging context of developing world\napplications. \n\n"}
{"id": "1811.12465", "contents": "Title: Uncertainty propagation in neural networks for sparse coding Abstract: A novel method to propagate uncertainty through the soft-thresholding\nnonlinearity is proposed in this paper. At every layer the current distribution\nof the target vector is represented as a spike and slab distribution, which\nrepresents the probabilities of each variable being zero, or\nGaussian-distributed. Using the proposed method of uncertainty propagation, the\ngradients of the logarithms of normalisation constants are derived, that can be\nused to update a weight distribution. A novel Bayesian neural network for\nsparse coding is designed utilising both the proposed method of uncertainty\npropagation and Bayesian inference algorithm. \n\n"}
{"id": "1811.12843", "contents": "Title: Lipizzaner: A System That Scales Robust Generative Adversarial Network\n  Training Abstract: GANs are difficult to train due to convergence pathologies such as mode and\ndiscriminator collapse. We introduce Lipizzaner, an open source software system\nthat allows machine learning engineers to train GANs in a distributed and\nrobust way. Lipizzaner distributes a competitive coevolutionary algorithm\nwhich, by virtue of dual, adapting, generator and discriminator populations, is\nrobust to collapses. The algorithm is well suited to efficient distribution\nbecause it uses a spatial grid abstraction. Training is local to each cell and\nstrong intermediate training results are exchanged among overlapping\nneighborhoods allowing high performing solutions to propagate and improve with\nmore rounds of training. Experiments on common image datasets overcome critical\ncollapses. Communication overhead scales linearly when increasing the number of\ncompute instances and we observe that increasing scale leads to improved model\nperformance. \n\n"}
{"id": "1811.12929", "contents": "Title: Online Abstraction with MDP Homomorphisms for Deep Learning Abstract: Abstraction of Markov Decision Processes is a useful tool for solving complex\nproblems, as it can ignore unimportant aspects of an environment, simplifying\nthe process of learning an optimal policy. In this paper, we propose a new\nalgorithm for finding abstract MDPs in environments with continuous state\nspaces. It is based on MDP homomorphisms, a structure-preserving mapping\nbetween MDPs. We demonstrate our algorithm's ability to learn abstractions from\ncollected experience and show how to reuse the abstractions to guide\nexploration in new tasks the agent encounters. Our novel task transfer method\noutperforms baselines based on a deep Q-network in the majority of our\nexperiments. The source code is at https://github.com/ondrejba/aamas_19. \n\n"}
{"id": "1811.12932", "contents": "Title: Recurrent machines for likelihood-free inference Abstract: Likelihood-free inference is concerned with the estimation of the parameters\nof a non-differentiable stochastic simulator that best reproduce real\nobservations. In the absence of a likelihood function, most of the existing\ninference methods optimize the simulator parameters through a handcrafted\niterative procedure that tries to make the simulated data more similar to the\nobservations. In this work, we explore whether meta-learning can be used in the\nlikelihood-free context, for learning automatically from data an iterative\noptimization procedure that would solve likelihood-free inference problems. We\ndesign a recurrent inference machine that learns a sequence of parameter\nupdates leading to good parameter estimates, without ever specifying some\nexplicit notion of divergence between the simulated data and the real data\ndistributions. We demonstrate our approach on toy simulators, showing promising\nresults both in terms of performance and robustness. \n\n"}
{"id": "1812.00239", "contents": "Title: Building robust classifiers through generation of confident out of\n  distribution examples Abstract: Deep learning models are known to be overconfident in their predictions on\nout of distribution inputs. There have been several pieces of work to address\nthis issue, including a number of approaches for building Bayesian neural\nnetworks, as well as closely related work on detection of out of distribution\nsamples. Recently, there has been work on building classifiers that are robust\nto out of distribution samples by adding a regularization term that maximizes\nthe entropy of the classifier output on out of distribution data. To\napproximate out of distribution samples (which are not known apriori), a GAN\nwas used for generation of samples at the edges of the training distribution.\nIn this paper, we introduce an alternative GAN based approach for building a\nrobust classifier, where the idea is to use the GAN to explicitly generate out\nof distribution samples that the classifier is confident on (low entropy), and\nhave the classifier maximize the entropy for these samples. We showcase the\neffectiveness of our approach relative to state-of-the-art on hand-written\ncharacters as well as on a variety of natural image datasets. \n\n"}
{"id": "1812.00285", "contents": "Title: Learning Curriculum Policies for Reinforcement Learning Abstract: Curriculum learning in reinforcement learning is a training methodology that\nseeks to speed up learning of a difficult target task, by first training on a\nseries of simpler tasks and transferring the knowledge acquired to the target\ntask. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an\nopen problem that has been the subject of much recent work in this area. In\nthis paper, we build upon a recent method for curriculum design, which\nformulates the curriculum sequencing problem as a Markov Decision Process. We\nextend this model to handle multiple transfer learning algorithms, and show for\nthe first time that a curriculum policy over this MDP can be learned from\nexperience. We explore various representations that make this possible, and\nevaluate our approach by learning curriculum policies for multiple agents in\ntwo different domains. The results show that our method produces curricula that\ncan train agents to perform on a target task as fast or faster than existing\nmethods. \n\n"}
{"id": "1812.00877", "contents": "Title: Automatic lesion boundary detection in dermoscopy Abstract: This manuscript addresses the problem of the automatic lesion boundary\ndetection in dermoscopy, using deep neural networks. An approach is based on\nthe adaptation of the U-net convolutional neural network with skip connections\nfor lesion boundary segmentation task. I hope this paper could serve, to some\nextent, as an experiment of using deep convolutional networks in biomedical\nsegmentation task and as a guideline of the boundary detection benchmark,\ninspiring further attempts and researches. \n\n"}
{"id": "1812.00984", "contents": "Title: Protection Against Reconstruction and Its Applications in Private\n  Federated Learning Abstract: In large-scale statistical learning, data collection and model fitting are\nmoving increasingly toward peripheral devices---phones, watches, fitness\ntrackers---away from centralized data collection. Concomitant with this rise in\ndecentralized data are increasing challenges of maintaining privacy while\nallowing enough information to fit accurate, useful statistical models. This\nmotivates local notions of privacy---most significantly, local differential\nprivacy, which provides strong protections against sensitive data\ndisclosures---where data is obfuscated before a statistician or learner can\neven observe it, providing strong protections to individuals' data. Yet local\nprivacy as traditionally employed may prove too stringent for practical use,\nespecially in modern high-dimensional statistical and machine learning\nproblems. Consequently, we revisit the types of disclosures and adversaries\nagainst which we provide protections, considering adversaries with limited\nprior information and ensuring that with high probability, ensuring they cannot\nreconstruct an individual's data within useful tolerances. By reconceptualizing\nthese protections, we allow more useful data release---large privacy parameters\nin local differential privacy---and we design new (minimax) optimal locally\ndifferentially private mechanisms for statistical learning problems for\n\\emph{all} privacy levels. We thus present practicable approaches to\nlarge-scale locally private model training that were previously impossible,\nshowing theoretically and empirically that we can fit large-scale image\nclassification and language models with little degradation in utility. \n\n"}
{"id": "1812.01483", "contents": "Title: CompILE: Compositional Imitation Learning and Execution Abstract: We introduce Compositional Imitation Learning and Execution (CompILE): a\nframework for learning reusable, variable-length segments of\nhierarchically-structured behavior from demonstration data. CompILE uses a\nnovel unsupervised, fully-differentiable sequence segmentation module to learn\nlatent encodings of sequential data that can be re-composed and executed to\nperform new tasks. Once trained, our model generalizes to sequences of longer\nlength and from environment instances not seen during training. We evaluate\nCompILE in a challenging 2D multi-task environment and a continuous control\ntask, and show that it can find correct task boundaries and event encodings in\nan unsupervised manner. Latent codes and associated behavior policies\ndiscovered by CompILE can be used by a hierarchical agent, where the high-level\npolicy selects actions in the latent code space, and the low-level,\ntask-specific policies are simply the learned decoders. We found that our\nCompILE-based agent could learn given only sparse rewards, where agents without\ntask-specific policies struggle. \n\n"}
{"id": "1812.01495", "contents": "Title: Expanding search in the space of empirical ML Abstract: As researchers and practitioners of applied machine learning, we are given a\nset of requirements on the problem to be solved, the plausibly obtainable data,\nand the computational resources available. We aim to find (within those bounds)\nreliably useful combinations of problem, data, and algorithm. An emphasis on\nalgorithmic or technical novelty in ML conference publications leads to\nexploration of one dimension of this space. Data collection and ML deployment\nat scale in industry settings offers an environment for exploring the others.\nOur conferences and reviewing criteria can better support empirical ML by\nsoliciting and incentivizing experimentation and synthesis independent of\nalgorithmic innovation. \n\n"}
{"id": "1812.01640", "contents": "Title: Overcoming Catastrophic Forgetting by Soft Parameter Pruning Abstract: Catastrophic forgetting is a challenge issue in continual learning when a\ndeep neural network forgets the knowledge acquired from the former task after\nlearning on subsequent tasks. However, existing methods try to find the joint\ndistribution of parameters shared with all tasks. This idea can be questionable\nbecause this joint distribution may not present when the number of tasks\nincrease. On the other hand, It also leads to \"long-term\" memory issue when the\nnetwork capacity is limited since adding tasks will \"eat\" the network capacity.\nIn this paper, we proposed a Soft Parameters Pruning (SPP) strategy to reach\nthe trade-off between short-term and long-term profit of a learning model by\nfreeing those parameters less contributing to remember former task domain\nknowledge to learn future tasks, and preserving memories about previous tasks\nvia those parameters effectively encoding knowledge about tasks at the same\ntime. The SPP also measures the importance of parameters by information entropy\nin a label free manner. The experiments on several tasks shows SPP model\nachieved the best performance compared with others state-of-the-art methods.\nExperiment results also indicate that our method is less sensitive to\nhyper-parameter and better generalization. Our research suggests that a softer\nstrategy, i.e. approximate optimize or sub-optimal solution, will benefit\nalleviating the dilemma of memory. The source codes are available at\nhttps://github.com/lehaifeng/Learning_by_memory. \n\n"}
{"id": "1812.02224", "contents": "Title: Adapting Auxiliary Losses Using Gradient Similarity Abstract: One approach to deal with the statistical inefficiency of neural networks is\nto rely on auxiliary losses that help to build useful representations. However,\nit is not always trivial to know if an auxiliary task will be helpful for the\nmain task and when it could start hurting. We propose to use the cosine\nsimilarity between gradients of tasks as an adaptive weight to detect when an\nauxiliary loss is helpful to the main loss. We show that our approach is\nguaranteed to converge to critical points of the main task and demonstrate the\npractical usefulness of the proposed algorithm in a few domains: multi-task\nsupervised learning on subsets of ImageNet, reinforcement learning on\ngridworld, and reinforcement learning on Atari games. \n\n"}
{"id": "1812.02463", "contents": "Title: Anomaly detection with Wasserstein GAN Abstract: Generative adversarial networks are a class of generative algorithms that\nhave been widely used to produce state-of-the-art samples. In this paper, we\ninvestigate GAN to perform anomaly detection on time series dataset. In order\nto achieve this goal, a bibliography is made focusing on theoretical properties\nof GAN and GAN used for anomaly detection. A Wasserstein GAN has been chosen to\nlearn the representation of normal data distribution and a stacked encoder with\nthe generator performs the anomaly detection. W-GAN with encoder seems to\nproduce state of the art anomaly detection scores on MNIST dataset and we\ninvestigate its usage on multi-variate time series. \n\n"}
{"id": "1812.02633", "contents": "Title: MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Abstract: We consider the problem of handling missing data with deep latent variable\nmodels (DLVMs). First, we present a simple technique to train DLVMs when the\ntraining set contains missing-at-random data. Our approach, called MIWAE, is\nbased on the importance-weighted autoencoder (IWAE), and maximises a\npotentially tight lower bound of the log-likelihood of the observed data.\nCompared to the original IWAE, our algorithm does not induce any additional\ncomputational overhead due to the missing data. We also develop Monte Carlo\ntechniques for single and multiple imputation using a DLVM trained on an\nincomplete data set. We illustrate our approach by training a convolutional\nDLVM on a static binarisation of MNIST that contains 50% of missing pixels.\nLeveraging multiple imputation, a convolutional network trained on these\nincomplete digits has a test performance similar to one trained on complete\ndata. On various continuous and binary data sets, we also show that MIWAE\nprovides accurate single imputations, and is highly competitive with\nstate-of-the-art methods. \n\n"}
{"id": "1812.04801", "contents": "Title: Can I trust you more? Model-Agnostic Hierarchical Explanations Abstract: Interactions such as double negation in sentences and scene interactions in\nimages are common forms of complex dependencies captured by state-of-the-art\nmachine learning models. We propose Mah\\'e, a novel approach to provide\nModel-agnostic hierarchical \\'explanations of how powerful machine learning\nmodels, such as deep neural networks, capture these interactions as either\ndependent on or free of the context of data instances. Specifically, Mah\\'e\nprovides context-dependent explanations by a novel local interpretation\nalgorithm that effectively captures any-order interactions, and obtains\ncontext-free explanations through generalizing context-dependent interactions\nto explain global behaviors. Experimental results show that Mah\\'e obtains\nimproved local interaction interpretations over state-of-the-art methods and\nsuccessfully explains interactions that are context-free. \n\n"}
{"id": "1812.05477", "contents": "Title: Gaussian Process Deep Belief Networks: A Smooth Generative Model of\n  Shape with Uncertainty Propagation Abstract: The shape of an object is an important characteristic for many vision\nproblems such as segmentation, detection and tracking. Being independent of\nappearance, it is possible to generalize to a large range of objects from only\nsmall amounts of data. However, shapes represented as silhouette images are\nchallenging to model due to complicated likelihood functions leading to\nintractable posteriors. In this paper we present a generative model of shapes\nwhich provides a low dimensional latent encoding which importantly resides on a\nsmooth manifold with respect to the silhouette images. The proposed model\npropagates uncertainty in a principled manner allowing it to learn from small\namounts of data and providing predictions with associated uncertainty. We\nprovide experiments that show how our proposed model provides favorable\nquantitative results compared with the state-of-the-art while simultaneously\nproviding a representation that resides on a low-dimensional interpretable\nmanifold. \n\n"}
{"id": "1812.05793", "contents": "Title: Adversarial Sample Detection for Deep Neural Network through Model\n  Mutation Testing Abstract: Deep neural networks (DNN) have been shown to be useful in a wide range of\napplications. However, they are also known to be vulnerable to adversarial\nsamples. By transforming a normal sample with some carefully crafted human\nimperceptible perturbations, even highly accurate DNN make wrong decisions.\nMultiple defense mechanisms have been proposed which aim to hinder the\ngeneration of such adversarial samples. However, a recent work show that most\nof them are ineffective. In this work, we propose an alternative approach to\ndetect adversarial samples at runtime. Our main observation is that adversarial\nsamples are much more sensitive than normal samples if we impose random\nmutations on the DNN. We thus first propose a measure of `sensitivity' and show\nempirically that normal samples and adversarial samples have distinguishable\nsensitivity. We then integrate statistical hypothesis testing and model\nmutation testing to check whether an input sample is likely to be normal or\nadversarial at runtime by measuring its sensitivity. We evaluated our approach\non the MNIST and CIFAR10 datasets. The results show that our approach detects\nadversarial samples generated by state-of-the-art attacking methods efficiently\nand accurately. \n\n"}
{"id": "1812.06515", "contents": "Title: Higher-Order Spectral Clustering under Superimposed Stochastic Block\n  Model Abstract: Higher-order motif structures and multi-vertex interactions are becoming\nincreasingly important in studies that aim to improve our understanding of\nfunctionalities and evolution patterns of networks. To elucidate the role of\nhigher-order structures in community detection problems over complex networks,\nwe introduce the notion of a Superimposed Stochastic Block Model (SupSBM). The\nmodel is based on a random graph framework in which certain higher-order\nstructures or subgraphs are generated through an independent hyperedge\ngeneration process, and are then replaced with graphs that are superimposed\nwith directed or undirected edges generated by an inhomogeneous random graph\nmodel. Consequently, the model introduces controlled dependencies between edges\nwhich allow for capturing more realistic network phenomena, namely strong local\nclustering in a sparse network, short average path length, and community\nstructure. We proceed to rigorously analyze the performance of a number of\nrecently proposed higher-order spectral clustering methods on the SupSBM. In\nparticular, we prove non-asymptotic upper bounds on the misclustering error of\nspectral community detection for a SupSBM setting in which triangles or\n3-uniform hyperedges are superimposed with undirected edges. As part of our\nanalysis, we also derive new bounds on the misclustering error of higher-order\nspectral clustering methods for the standard SBM and the 3-uniform hypergraph\nSBM. Furthermore, for a non-uniform hypergraph SBM model in which one directly\nobserves both edges and 3-uniform hyperedges, we obtain a criterion that\ndescribes when to perform spectral clustering based on edges and when on\nhyperedges, based on a function of hyperedge density and observation quality. \n\n"}
{"id": "1812.06866", "contents": "Title: Bayesian Mean-parameterized Nonnegative Binary Matrix Factorization Abstract: Binary data matrices can represent many types of data such as social\nnetworks, votes, or gene expression. In some cases, the analysis of binary\nmatrices can be tackled with nonnegative matrix factorization (NMF), where the\nobserved data matrix is approximated by the product of two smaller nonnegative\nmatrices. In this context, probabilistic NMF assumes a generative model where\nthe data is usually Bernoulli-distributed. Often, a link function is used to\nmap the factorization to the $[0,1]$ range, ensuring a valid Bernoulli mean\nparameter. However, link functions have the potential disadvantage to lead to\nuninterpretable models. Mean-parameterized NMF, on the contrary, overcomes this\nproblem. We propose a unified framework for Bayesian mean-parameterized\nnonnegative binary matrix factorization models (NBMF). We analyze three models\nwhich correspond to three possible constraints that respect the\nmean-parametrization without the need for link functions. Furthermore, we\nderive a novel collapsed Gibbs sampler and a collapsed variational algorithm to\ninfer the posterior distribution of the factors. Next, we extend the proposed\nmodels to a nonparametric setting where the number of used latent dimensions is\nautomatically driven by the observed data. We analyze the performance of our\nNBMF methods in multiple datasets for different tasks such as dictionary\nlearning and prediction of missing data. Experiments show that our methods\nprovide similar or superior results than the state of the art, while\nautomatically detecting the number of relevant components. \n\n"}
{"id": "1812.07319", "contents": "Title: Evaluating the squared-exponential covariance function in Gaussian\n  processes with integral observations Abstract: This paper deals with the evaluation of double line integrals of the squared\nexponential covariance function. We propose a new approach in which the double\nintegral is reduced to a single integral using the error function. This single\nintegral is then computed with efficiently implemented numerical techniques.\nThe performance is compared against existing state of the art methods and the\nresults show superior properties in numerical robustness and accuracy per\ncomputation time. \n\n"}
{"id": "1812.07352", "contents": "Title: A Novel Variational Autoencoder with Applications to Generative\n  Modelling, Classification, and Ordinal Regression Abstract: We develop a novel probabilistic generative model based on the variational\nautoencoder approach. Notable aspects of our architecture are: a novel way of\nspecifying the latent variables prior, and the introduction of an ordinality\nenforcing unit. We describe how to do supervised, unsupervised and\nsemi-supervised learning, and nominal and ordinal classification, with the\nmodel. We analyze generative properties of the approach, and the classification\neffectiveness under nominal and ordinal classification, using two benchmark\ndatasets. Our results show that our model can achieve comparable results with\nrelevant baselines in both of the classification tasks. \n\n"}
{"id": "1812.07997", "contents": "Title: Explanatory Graphs for CNNs Abstract: This paper introduces a graphical model, namely an explanatory graph, which\nreveals the knowledge hierarchy hidden inside conv-layers of a pre-trained CNN.\nEach filter in a conv-layer of a CNN for object classification usually\nrepresents a mixture of object parts. We develop a simple yet effective method\nto disentangle object-part pattern components from each filter. We construct an\nexplanatory graph to organize the mined part patterns, where a node represents\na part pattern, and each edge encodes co-activation relationships and spatial\nrelationships between patterns. More crucially, given a pre-trained CNN, the\nexplanatory graph is learned without a need of annotating object parts.\nExperiments show that each graph node consistently represented the same object\npart through different images, which boosted the transferability of CNN\nfeatures. We transferred part patterns in the explanatory graph to the task of\npart localization, and our method significantly outperformed other approaches. \n\n"}
{"id": "1812.08398", "contents": "Title: Low-rank Interaction with Sparse Additive Effects Model for Large Data\n  Frames Abstract: Many applications of machine learning involve the analysis of large data\nframes-matrices collecting heterogeneous measurements (binary, numerical,\ncounts, etc.) across samples-with missing values. Low-rank models, as studied\nby Udell et al. [30], are popular in this framework for tasks such as\nvisualization, clustering and missing value imputation. Yet, available methods\nwith statistical guarantees and efficient optimization do not allow explicit\nmodeling of main additive effects such as row and column, or covariate effects.\nIn this paper, we introduce a low-rank interaction and sparse additive effects\n(LORIS) model which combines matrix regression on a dictionary and low-rank\ndesign, to estimate main effects and interactions simultaneously. We provide\nstatistical guarantees in the form of upper bounds on the estimation error of\nboth components. Then, we introduce a mixed coordinate gradient descent (MCGD)\nmethod which provably converges sub-linearly to an optimal solution and is\ncomputationally efficient for large scale data sets. We show on simulated and\nsurvey data that the method has a clear advantage over current practices, which\nconsist in dealing separately with additive effects in a preprocessing step. \n\n"}
{"id": "1812.09251", "contents": "Title: Separability gap and large deviation entanglement criterion Abstract: For a given Hamiltonian $H$ on a multipartite quantum system, one is\ninterested in finding the energy $E_0$ of its ground state. In the separability\napproximation, arising as a natural consequence of measurement in a separable\nbasis, one looks for the minimal expectation value $\\lambda_{\\rm\nmin}^{\\otimes}$ of $H$ among all product states. For several concrete model\nHamiltonians, we investigate the difference $\\lambda_{\\rm min}^{\\otimes}-E_0$,\ncalled separability gap, which vanishes if the ground state has a product\nstructure. In the generic case of a random Hermitian matrix of the Gaussian\northogonal ensemble, we find explicit bounds for the size of the gap which\ndepend on the number of subsystems and hold with probability one. This implies\nan effective entanglement criterion applicable for any multipartite quantum\nsystem: If an expectation value of a typical observable among a given state is\nsufficiently distant from the average value, the state is almost surely\nentangled. \n\n"}
{"id": "1812.10004", "contents": "Title: Overparameterized Nonlinear Learning: Gradient Descent Takes the\n  Shortest Path? Abstract: Many modern learning tasks involve fitting nonlinear models to data which are\ntrained in an overparameterized regime where the parameters of the model exceed\nthe size of the training dataset. Due to this overparameterization, the\ntraining loss may have infinitely many global minima and it is critical to\nunderstand the properties of the solutions found by first-order optimization\nschemes such as (stochastic) gradient descent starting from different\ninitializations. In this paper we demonstrate that when the loss has certain\nproperties over a minimally small neighborhood of the initial point, first\norder methods such as (stochastic) gradient descent have a few intriguing\nproperties: (1) the iterates converge at a geometric rate to a global optima\neven when the loss is nonconvex, (2) among all global optima of the loss the\niterates converge to one with a near minimal distance to the initial point, (3)\nthe iterates take a near direct route from the initial point to this global\noptima. As part of our proof technique, we introduce a new potential function\nwhich captures the precise tradeoff between the loss function and the distance\nto the initial point as the iterations progress. For Stochastic Gradient\nDescent (SGD), we develop novel martingale techniques that guarantee SGD never\nleaves a small neighborhood of the initialization, even with rather large\nlearning rates. We demonstrate the utility of our general theory for a variety\nof problem domains spanning low-rank matrix recovery to neural network\ntraining. Underlying our analysis are novel insights that may have implications\nfor training and generalization of more sophisticated learning problems\nincluding those involving deep neural network architectures. \n\n"}
{"id": "1812.10519", "contents": "Title: Maximum Likelihood Estimation and Graph Matching in Errorfully Observed\n  Networks Abstract: Given a pair of graphs with the same number of vertices, the inexact graph\nmatching problem consists in finding a correspondence between the vertices of\nthese graphs that minimizes the total number of induced edge disagreements. We\nstudy this problem from a statistical framework in which one of the graphs is\nan errorfully observed copy of the other. We introduce a corrupting channel\nmodel, and show that in this model framework, the solution to the graph\nmatching problem is a maximum likelihood estimator. Necessary and sufficient\nconditions for consistency of this MLE are presented, as well as a relaxed\nnotion of consistency in which a negligible fraction of the vertices need not\nbe matched correctly. The results are used to study matchability in several\nfamilies of random graphs, including edge independent models, random regular\ngraphs and small-world networks. We also use these results to introduce\nmeasures of matching feasibility, and experimentally validate the results on\nsimulated and real-world networks. \n\n"}
{"id": "1812.10761", "contents": "Title: Improving Generalization of Deep Neural Networks by Leveraging Margin\n  Distribution Abstract: Recent research has used margin theory to analyze the generalization\nperformance for deep neural networks (DNNs). The existed results are almost\nbased on the spectrally-normalized minimum margin. However, optimizing the\nminimum margin ignores a mass of information about the entire margin\ndistribution, which is crucial to generalization performance. In this paper, we\nprove a generalization upper bound dominated by the statistics of the entire\nmargin distribution. Compared with the minimum margin bounds, our bound\nhighlights an important measure for controlling the complexity, which is the\nratio of the margin standard deviation to the expected margin. We utilize a\nconvex margin distribution loss function on the deep neural networks to\nvalidate our theoretical results by optimizing the margin ratio. Experiments\nand visualizations confirm the effectiveness of our approach and the\ncorrelation between generalization gap and margin ratio. \n\n"}
{"id": "1812.11118", "contents": "Title: Reconciling modern machine learning practice and the bias-variance\n  trade-off Abstract: Breakthroughs in machine learning are rapidly changing science and society,\nyet our fundamental understanding of this technology has lagged far behind.\nIndeed, one of the central tenets of the field, the bias-variance trade-off,\nappears to be at odds with the observed behavior of methods used in the modern\nmachine learning practice. The bias-variance trade-off implies that a model\nshould balance under-fitting and over-fitting: rich enough to express\nunderlying structure in data, simple enough to avoid fitting spurious patterns.\nHowever, in the modern practice, very rich models such as neural networks are\ntrained to exactly fit (i.e., interpolate) the data. Classically, such models\nwould be considered over-fit, and yet they often obtain high accuracy on test\ndata. This apparent contradiction has raised questions about the mathematical\nfoundations of machine learning and their relevance to practitioners.\n  In this paper, we reconcile the classical understanding and the modern\npractice within a unified performance curve. This \"double descent\" curve\nsubsumes the textbook U-shaped bias-variance trade-off curve by showing how\nincreasing model capacity beyond the point of interpolation results in improved\nperformance. We provide evidence for the existence and ubiquity of double\ndescent for a wide spectrum of models and datasets, and we posit a mechanism\nfor its emergence. This connection between the performance and the structure of\nmachine learning models delineates the limits of classical analyses, and has\nimplications for both the theory and practice of machine learning. \n\n"}
{"id": "1812.11202", "contents": "Title: State representation learning with recurrent capsule networks Abstract: Unsupervised learning of compact and relevant state representations has been\nproved very useful at solving complex reinforcement learning tasks. In this\npaper, we propose a recurrent capsule network that learns such representations\nby trying to predict the future observations in an agent's trajectory. \n\n"}
{"id": "1812.11683", "contents": "Title: A dataset of 40K naturalistic 6-degree-of-freedom robotic grasp\n  demonstrations Abstract: Modern approaches to grasp planning often involve deep learning. However,\nthere are only a few large datasets of labelled grasping examples on physical\nrobots, and available datasets involve relatively simple planar grasps with\ntwo-fingered grippers. Here we present: 1) a new human grasp demonstration\nmethod that facilitates rapid collection of naturalistic grasp examples, with\nfull six-degree-of-freedom gripper positioning; and 2) a dataset of roughly\nforty thousand successful grasps on 109 different rigid objects with the\nRightHand Robotics three-fingered ReFlex gripper. \n\n"}
{"id": "1812.11794", "contents": "Title: Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n  Challenges, Solutions and Applications Abstract: Reinforcement learning (RL) algorithms have been around for decades and\nemployed to solve various sequential decision-making problems. These algorithms\nhowever have faced great challenges when dealing with high-dimensional\nenvironments. The recent development of deep learning has enabled RL methods to\ndrive optimal policies for sophisticated and capable agents, which can perform\nefficiently in these challenging environments. This paper addresses an\nimportant aspect of deep RL related to situations that require multiple agents\nto communicate and cooperate to solve complex tasks. A survey of different\napproaches to problems related to multi-agent deep RL (MADRL) is presented,\nincluding non-stationarity, partial observability, continuous state and action\nspaces, multi-agent training schemes, multi-agent transfer learning. The merits\nand demerits of the reviewed methods will be analyzed and discussed, with their\ncorresponding applications explored. It is envisaged that this review provides\ninsights about various MADRL methods and can lead to future development of more\nrobust and highly useful multi-agent learning methods for solving real-world\nproblems. \n\n"}
{"id": "1812.11917", "contents": "Title: Learning RUMs: Reducing Mixture to Single Component via PCA Abstract: We consider the problem of learning a mixture of Random Utility Models\n(RUMs). Despite the success of RUMs in various domains and the versatility of\nmixture RUMs to capture the heterogeneity in preferences, there has been only\nlimited progress in learning a mixture of RUMs from partial data such as\npairwise comparisons. In contrast, there have been significant advances in\nterms of learning a single component RUM using pairwise comparisons. In this\npaper, we aim to bridge this gap between mixture learning and single component\nlearning of RUM by developing a `reduction' procedure. We propose to utilize\nPCA-based spectral clustering that simultaneously `de-noises' pairwise\ncomparison data. We prove that our algorithm manages to cluster the partial\ndata correctly (i.e., comparisons from the same RUM component are grouped in\nthe same cluster) with high probability even when data is generated from a\npossibly {\\em heterogeneous} mixture of well-separated {\\em generic} RUMs. Both\nthe time and the sample complexities scale polynomially in model parameters\nincluding the number of items. Two key features in the analysis are in\nestablishing (1) a meaningful upper bound on the sub-Gaussian norm for RUM\ncomponents embedded into the vector space of pairwise marginals and (2) the\nrobustness of PCA with missing values in the $L_{2, \\infty}$ sense, which might\nbe of interest in their own right. \n\n"}
{"id": "1901.00570", "contents": "Title: Event detection in Twitter: A keyword volume approach Abstract: Event detection using social media streams needs a set of informative\nfeatures with strong signals that need minimal preprocessing and are highly\nassociated with events of interest. Identifying these informative features as\nkeywords from Twitter is challenging, as people use informal language to\nexpress their thoughts and feelings. This informality includes acronyms,\nmisspelled words, synonyms, transliteration and ambiguous terms. In this paper,\nwe propose an efficient method to select the keywords frequently used in\nTwitter that are mostly associated with events of interest such as protests.\nThe volume of these keywords is tracked in real time to identify the events of\ninterest in a binary classification scheme. We use keywords within word-pairs\nto capture the context. The proposed method is to binarize vectors of daily\ncounts for each word-pair by applying a spike detection temporal filter, then\nuse the Jaccard metric to measure the similarity of the binary vector for each\nword-pair with the binary vector describing event occurrence. The top n\nword-pairs are used as features to classify any day to be an event or non-event\nday. The selected features are tested using multiple classifiers such as Naive\nBayes, SVM, Logistic Regression, KNN and decision trees. They all produced AUC\nROC scores up to 0.91 and F1 scores up to 0.79. The experiment is performed\nusing the English language in multiple cities such as Melbourne, Sydney and\nBrisbane as well as the Indonesian language in Jakarta. The two experiments,\ncomprising different languages and locations, yielded similar results. \n\n"}
{"id": "1901.00811", "contents": "Title: From exploration to control: learning object manipulation skills through\n  novelty search and local adaptation Abstract: Programming a robot to deal with open-ended tasks remains a challenge, in\nparticular if the robot has to manipulate objects. Launching, grasping, pushing\nor any other object interaction can be simulated but the corresponding models\nare not reversible and the robot behavior thus cannot be directly deduced.\nThese behaviors are hard to learn without a demonstration as the search space\nis large and the reward sparse. We propose a method to autonomously generate a\ndiverse repertoire of simple object interaction behaviors in simulation. Our\ngoal is to bootstrap a robot learning and development process with limited\ninformation about what the robot has to achieve and how. This repertoire can be\nexploited to solve different tasks in reality thanks to a proposed adaptation\nmethod or could be used as a training set for data-hungry algorithms.\n  The proposed approach relies on the definition of a goal space and generates\na repertoire of trajectories to reach attainable goals, thus allowing the robot\nto control this goal space. The repertoire is built with an off-the-shelf\nsimulation thanks to a quality diversity algorithm. The result is a set of\nsolutions tested in simulation only. It may result in two different problems:\n(1) as the repertoire is discrete and finite, it may not contain the trajectory\nto deal with a given situation or (2) some trajectories may lead to a behavior\nin reality that differs from simulation because of a reality gap. We propose an\napproach to deal with both issues by using a local linearization of the mapping\nbetween the motion parameters and the observed effects. Furthermore, we present\nan approach to update the existing solutions repertoire with the tests done on\nthe real robot. The approach has been validated on two different experiments on\nthe Baxter robot: a ball launching and a joystick manipulation tasks. \n\n"}
{"id": "1901.02358", "contents": "Title: FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated\n  Recurrent Neural Network Abstract: This paper develops the FastRNN and FastGRNN algorithms to address the twin\nRNN limitations of inaccurate training and inefficient prediction. Previous\napproaches have improved accuracy at the expense of prediction costs making\nthem infeasible for resource-constrained and real-time applications. Unitary\nRNNs have increased accuracy somewhat by restricting the range of the state\ntransition matrix's singular values but have also increased the model size as\nthey require a larger number of hidden units to make up for the loss in\nexpressive power. Gated RNNs have obtained state-of-the-art accuracies by\nadding extra parameters thereby resulting in even larger models. FastRNN\naddresses these limitations by adding a residual connection that does not\nconstrain the range of the singular values explicitly and has only two extra\nscalar parameters. FastGRNN then extends the residual connection to a gate by\nreusing the RNN matrices to match state-of-the-art gated RNN accuracies but\nwith a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse\nand quantized resulted in accurate models that could be up to 35x smaller than\nleading gated and unitary RNNs. This allowed FastGRNN to accurately recognize\nthe \"Hey Cortana\" wakeword with a 1 KB model and to be deployed on severely\nresource-constrained IoT microcontrollers too tiny to store other RNN models.\nFastGRNN's code is available at https://github.com/Microsoft/EdgeML/. \n\n"}
{"id": "1901.02374", "contents": "Title: Graphical model inference: Sequential Monte Carlo meets deterministic\n  approximations Abstract: Approximate inference in probabilistic graphical models (PGMs) can be grouped\ninto deterministic methods and Monte-Carlo-based methods. The former can often\nprovide accurate and rapid inferences, but are typically associated with biases\nthat are hard to quantify. The latter enjoy asymptotic consistency, but can\nsuffer from high computational costs. In this paper we present a way of\nbridging the gap between deterministic and stochastic inference. Specifically,\nwe suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which\ncan leverage the output from deterministic inference methods. While generally\napplicable, we show explicitly how this can be done with loopy belief\npropagation, expectation propagation, and Laplace approximations. The resulting\nalgorithm can be viewed as a post-correction of the biases associated with\nthese methods and, indeed, numerical results show clear improvements over the\nbaseline deterministic methods as well as over \"plain\" SMC. \n\n"}
{"id": "1901.02928", "contents": "Title: Beyond the EM Algorithm: Constrained Optimization Methods for Latent\n  Class Model Abstract: Latent class model (LCM), which is a finite mixture of different categorical\ndistributions, is one of the most widely used models in statistics and machine\nlearning fields. Because of its non-continuous nature and the flexibility in\nshape, researchers in practice areas such as marketing and social sciences also\nfrequently use LCM to gain insights from their data. One likelihood-based\nmethod, the Expectation-Maximization (EM) algorithm, is often used to obtain\nthe model estimators. However, the EM algorithm is well-known for its\nnotoriously slow convergence. In this research, we explore alternative\nlikelihood-based methods that can potential remedy the slow convergence of the\nEM algorithm. More specifically, we regard likelihood-based approach as a\nconstrained nonlinear optimization problem, and apply quasi-Newton type methods\nto solve them. We examine two different constrained optimization methods to\nmaximize the log likelihood function. We present simulation study results to\nshow that the proposed methods not only converge in less iterations than the EM\nalgorithm but also produce more accurate model estimators. \n\n"}
{"id": "1901.03299", "contents": "Title: An Analysis of the Accuracy of the P300 BCI Abstract: The P300 Brain-Computer Interface (BCI) is a well-established communication\nchannel for severely disabled people. The P300 event-related potential is\nmostly characterized by its amplitude or its area, which correlate with the\nspelling accuracy of the P300 speller. Here, we introduce a novel approach for\nestimating the efficiency of this BCI by considering the P300 signal-to-noise\nratio (SNR), a parameter that estimates the spatial and temporal noise levels\nand has a significantly stronger correlation with spelling accuracy.\nFurthermore, we suggest a Gaussian noise model, which utilizes the P300\nevent-related potential SNR to predict spelling accuracy under various\nconditions for LDA-based classification. We demonstrate the utility of this\nanalysis using real data and discuss its potential applications, such as\nspeeding up the process of electrode selection. \n\n"}
{"id": "1901.03327", "contents": "Title: A New Tensioning Method using Deep Reinforcement Learning for Surgical\n  Pattern Cutting Abstract: Surgeons normally need surgical scissors and tissue grippers to cut through a\ndeformable surgical tissue. The cutting accuracy depends on the skills to\nmanipulate these two tools. Such skills are part of basic surgical skills\ntraining as in the Fundamentals of Laparoscopic Surgery. The gripper is used to\npinch a point on the surgical sheet and pull the tissue to a certain direction\nto maintain the tension while the scissors cut through a trajectory. As the\nsurgical materials are deformable, it requires a comprehensive tensioning\npolicy to yield appropriate tensioning direction at each step of the cutting\nprocess. Automating a tensioning policy for a given cutting trajectory will\nsupport not only the human surgeons but also the surgical robots to improve the\ncutting accuracy and reliability. This paper presents a multiple pinch point\napproach to modelling an autonomous tensioning planner based on a deep\nreinforcement learning algorithm. Experiments on a simulator show that the\nproposed method is superior to existing methods in terms of both performance\nand robustness. \n\n"}
{"id": "1901.03357", "contents": "Title: No-Regret Bayesian Optimization with Unknown Hyperparameters Abstract: Bayesian optimization (BO) based on Gaussian process models is a powerful\nparadigm to optimize black-box functions that are expensive to evaluate. While\nseveral BO algorithms provably converge to the global optimum of the unknown\nfunction, they assume that the hyperparameters of the kernel are known in\nadvance. This is not the case in practice and misspecification often causes\nthese algorithms to converge to poor local optima. In this paper, we present\nthe first BO algorithm that is provably no-regret and converges to the optimum\nwithout knowledge of the hyperparameters. During optimization we slowly adapt\nthe hyperparameters of stationary kernels and thereby expand the associated\nfunction class over time, so that the BO algorithm considers more complex\nfunction candidates. Based on the theoretical insights, we propose several\npractical algorithms that achieve the empirical sample efficiency of BO with\nonline hyperparameter estimation, but retain theoretical convergence\nguarantees. We evaluate our method on several benchmark problems. \n\n"}
{"id": "1901.03559", "contents": "Title: An investigation of model-free planning Abstract: The field of reinforcement learning (RL) is facing increasingly challenging\ndomains with combinatorial complexity. For an RL agent to address these\nchallenges, it is essential that it can plan effectively. Prior work has\ntypically utilized an explicit model of the environment, combined with a\nspecific planning algorithm (such as tree search). More recently, a new family\nof methods have been proposed that learn how to plan, by providing the\nstructure for planning via an inductive bias in the function approximator (such\nas a tree structured neural network), trained end-to-end by a model-free RL\nalgorithm. In this paper, we go even further, and demonstrate empirically that\nan entirely model-free approach, without special structure beyond standard\nneural network components such as convolutional networks and LSTMs, can learn\nto exhibit many of the characteristics typically associated with a model-based\nplanner. We measure our agent's effectiveness at planning in terms of its\nability to generalize across a combinatorial and irreversible state space, its\ndata efficiency, and its ability to utilize additional thinking time. We find\nthat our agent has many of the characteristics that one might expect to find in\na planning algorithm. Furthermore, it exceeds the state-of-the-art in\nchallenging combinatorial domains such as Sokoban and outperforms other\nmodel-free approaches that utilize strong inductive biases toward planning. \n\n"}
{"id": "1901.03678", "contents": "Title: Machine Learning Automation Toolbox (MLaut) Abstract: In this paper we present MLaut (Machine Learning AUtomation Toolbox) for the\npython data science ecosystem. MLaut automates large-scale evaluation and\nbenchmarking of machine learning algorithms on a large number of datasets.\nMLaut provides a high-level workflow interface to machine algorithm algorithms,\nimplements a local back-end to a database of dataset collections, trained\nalgorithms, and experimental results, and provides easy-to-use interfaces to\nthe scikit-learn and keras modelling libraries. Experiments are easy to set up\nwith default settings in a few lines of code, while remaining fully\ncustomizable to the level of hyper-parameter tuning, pipeline composition, or\ndeep learning architecture.\n  As a principal test case for MLaut, we conducted a large-scale supervised\nclassification study in order to benchmark the performance of a number of\nmachine learning algorithms - to our knowledge also the first larger-scale\nstudy on standard supervised learning data sets to include deep learning\nalgorithms. While corroborating a number of previous findings in literature, we\nfound (within the limitations of our study) that deep neural networks do not\nperform well on basic supervised learning, i.e., outside the more specialized,\nimage-, audio-, or text-based tasks. \n\n"}
{"id": "1901.03906", "contents": "Title: ChronoMID - Cross-Modal Neural Networks for 3-D Temporal Medical Imaging\n  Data Abstract: ChronoMID builds on the success of cross-modal convolutional neural networks\n(X-CNNs), making the novel application of the technique to medical imaging\ndata. Specifically, this paper presents and compares alternative approaches -\ntimestamps and difference images - to incorporate temporal information for the\nclassification of bone disease in mice, applied to micro-CT scans of mouse\ntibiae. Whilst much previous work on diseases and disease classification has\nbeen based on mathematical models incorporating domain expertise and the\nexplicit encoding of assumptions, the approaches given here utilise the growing\navailability of computing resources to analyse large datasets and uncover\nsubtle patterns in both space and time. After training on a balanced set of\nover 75000 images, all models incorporating temporal features outperformed a\nstate-of-the-art CNN baseline on an unseen, balanced validation set comprising\nover 20000 images. The top-performing model achieved 99.54% accuracy, compared\nto 73.02% for the CNN baseline. \n\n"}
{"id": "1901.03919", "contents": "Title: Semi-Supervised Regression using Cluster Ensemble and Low-Rank\n  Co-Association Matrix Decomposition under Uncertainties Abstract: In this paper, we solve a semi-supervised regression problem. Due to the lack\nof knowledge about the data structure and the presence of random noise, the\nconsidered data model is uncertain. We propose a method which combines graph\nLaplacian regularization and cluster ensemble methodologies. The co-association\nmatrix of the ensemble is calculated on both labeled and unlabeled data; this\nmatrix is used as a similarity matrix in the regularization framework to derive\nthe predicted outputs. We use the low-rank decomposition of the co-association\nmatrix to significantly speedup calculations and reduce memory. Numerical\nexperiments using the Monte Carlo approach demonstrate robustness, efficiency,\nand scalability of the proposed method. \n\n"}
{"id": "1901.04169", "contents": "Title: Towards Testing of Deep Learning Systems with Training Set Reduction Abstract: Testing the implementation of deep learning systems and their training\nroutines is crucial to maintain a reliable code base. Modern software\ndevelopment employs processes, such as Continuous Integration, in which changes\nto the software are frequently integrated and tested. However, testing the\ntraining routines requires running them and fully training a deep learning\nmodel can be resource-intensive, when using the full data set. Using only a\nsubset of the training data can improve test run time, but can also reduce its\neffectiveness. We evaluate different ways for training set reduction and their\nability to mimic the characteristics of model training with the original full\ndata set. Our results underline the usefulness of training set reduction,\nespecially in resource-constrained environments. \n\n"}
{"id": "1901.04420", "contents": "Title: Data Augmentation with Manifold Exploring Geometric Transformations for\n  Increased Performance and Robustness Abstract: In this paper we propose a novel augmentation technique that improves not\nonly the performance of deep neural networks on clean test data, but also\nsignificantly increases their robustness to random transformations, both affine\nand projective. Inspired by ManiFool, the augmentation is performed by a\nline-search manifold-exploration method that learns affine geometric\ntransformations that lead to the misclassification on an image, while ensuring\nthat it remains on the same manifold as the training data.\n  This augmentation method populates any training dataset with images that lie\non the border of the manifolds between two-classes and maximizes the variance\nthe network is exposed to during training. Our method was thoroughly evaluated\non the challenging tasks of fine-grained skin lesion classification from\nlimited data, and breast tumor classification of mammograms. Compared with\ntraditional augmentation methods, and with images synthesized by Generative\nAdversarial Networks our method not only achieves state-of-the-art performance\nbut also significantly improves the network's robustness. \n\n"}
{"id": "1901.04615", "contents": "Title: AutoPhase: Compiler Phase-Ordering for High Level Synthesis with Deep\n  Reinforcement Learning Abstract: The performance of the code generated by a compiler depends on the order in\nwhich the optimization passes are applied. In high-level synthesis, the quality\nof the generated circuit relates directly to the code generated by the\nfront-end compiler. Choosing a good order--often referred to as the\nphase-ordering problem--is an NP-hard problem. In this paper, we evaluate a new\ntechnique to address the phase-ordering problem: deep reinforcement learning.\nWe implement a framework in the context of the LLVM compiler to optimize the\nordering for HLS programs and compare the performance of deep reinforcement\nlearning to state-of-the-art algorithms that address the phase-ordering\nproblem. Overall, our framework runs one to two orders of magnitude faster than\nthese algorithms, and achieves a 16% improvement in circuit performance over\nthe -O3 compiler flag. \n\n"}
{"id": "1901.04878", "contents": "Title: Conditional deep surrogate models for stochastic, high-dimensional, and\n  multi-fidelity systems Abstract: We present a probabilistic deep learning methodology that enables the\nconstruction of predictive data-driven surrogates for stochastic systems.\nLeveraging recent advances in variational inference with implicit\ndistributions, we put forth a statistical inference framework that enables the\nend-to-end training of surrogate models on paired input-output observations\nthat may be stochastic in nature, originate from different information sources\nof variable fidelity, or be corrupted by complex noise processes. The resulting\nsurrogates can accommodate high-dimensional inputs and outputs and are able to\nreturn predictions with quantified uncertainty. The effectiveness our approach\nis demonstrated through a series of canonical studies, including the regression\nof noisy data, multi-fidelity modeling of stochastic processes, and uncertainty\npropagation in high-dimensional dynamical systems. \n\n"}
{"id": "1901.05147", "contents": "Title: The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition Abstract: Machine learning competitions such as those organized by Kaggle or KDD\nrepresent a useful benchmark for data science research. In this work, we\npresent our winning solution to the Game Data Mining competition hosted at the\n2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The\ncontest consisted of two tracks, and participants (more than 250, belonging to\nboth industry and academia) were to predict which players would stop playing\nthe game, as well as their remaining lifetime. The data were provided by a\nmajor worldwide video game company, NCSoft, and came from their successful\nmassively multiplayer online game Blade and Soul. Here, we describe the long\nshort-term memory approach and conditional inference survival ensemble model\nthat made us win both tracks of the contest, as well as the validation\nprocedure that we followed in order to prevent overfitting. In particular,\nchoosing a survival method able to deal with censored data was crucial to\naccurately predict the moment in which each player would leave the game, as\ncensoring is inherent in churn. The selected models proved to be robust against\nevolving conditions---since there was a change in the business model of the\ngame (from subscription-based to free-to-play) between the two sample datasets\nprovided---and efficient in terms of time cost. Thanks to these features and\nalso to their a ability to scale to large datasets, our models could be readily\nimplemented in real business settings. \n\n"}
{"id": "1901.05947", "contents": "Title: Stochastic Gradient Descent on a Tree: an Adaptive and Robust Approach\n  to Stochastic Convex Optimization Abstract: Online minimization of an unknown convex function over the interval $[0,1]$\nis considered under first-order stochastic bandit feedback, which returns a\nrandom realization of the gradient of the function at each query point. Without\nknowing the distribution of the random gradients, a learning algorithm\nsequentially chooses query points with the objective of minimizing regret\ndefined as the expected cumulative loss of the function values at the query\npoints in excess to the minimum value of the function. An approach based on\ndevising a biased random walk on an infinite-depth binary tree constructed\nthrough successive partitioning of the domain of the function is developed.\nEach move of the random walk is guided by a sequential test based on confidence\nbounds on the empirical mean constructed using the law of the iterated\nlogarithm. With no tuning parameters, this learning algorithm is robust to\nheavy-tailed noise with infinite variance and adaptive to unknown function\ncharacteristics (specifically, convex, strongly convex, and nonsmooth). It\nachieves the corresponding optimal regret orders (up to a $\\sqrt{\\log T}$ or a\n$\\log\\log T$ factor) in each class of functions and offers better or matching\nregret orders than the classical stochastic gradient descent approach which\nrequires the knowledge of the function characteristics for tuning the sequence\nof step-sizes. \n\n"}
{"id": "1901.06033", "contents": "Title: Continuous Hierarchical Representations with Poincar\\'e Variational\n  Auto-Encoders Abstract: The variational auto-encoder (VAE) is a popular method for learning a\ngenerative model and embeddings of the data. Many real datasets are\nhierarchically structured. However, traditional VAEs map data in a Euclidean\nlatent space which cannot efficiently embed tree-like structures. Hyperbolic\nspaces with negative curvature can. We therefore endow VAEs with a Poincar\\'e\nball model of hyperbolic geometry as a latent space and rigorously derive the\nnecessary methods to work with two main Gaussian generalisations on that space.\nWe empirically show better generalisation to unseen data than the Euclidean\ncounterpart, and can qualitatively and quantitatively better recover\nhierarchical structures. \n\n"}
{"id": "1901.06414", "contents": "Title: Foothill: A Quasiconvex Regularization for Edge Computing of Deep Neural\n  Networks Abstract: Deep neural networks (DNNs) have demonstrated success for many supervised\nlearning tasks, ranging from voice recognition, object detection, to image\nclassification. However, their increasing complexity might yield poor\ngeneralization error that make them hard to be deployed on edge devices.\nQuantization is an effective approach to compress DNNs in order to meet these\nconstraints. Using a quasiconvex base function in order to construct a binary\nquantizer helps training binary neural networks (BNNs) and adding noise to the\ninput data or using a concrete regularization function helps to improve\ngeneralization error. Here we introduce foothill function, an infinitely\ndifferentiable quasiconvex function. This regularizer is flexible enough to\ndeform towards $L_1$ and $L_2$ penalties. Foothill can be used as a binary\nquantizer, as a regularizer, or as a loss. In particular, we show this\nregularizer reduces the accuracy gap between BNNs and their full-precision\ncounterpart for image classification on ImageNet. \n\n"}
{"id": "1901.07777", "contents": "Title: Stochastic Gradient Trees Abstract: We present an algorithm for learning decision trees using stochastic gradient\ninformation as the source of supervision. In contrast to previous approaches to\ngradient-based tree learning, our method operates in the incremental learning\nsetting rather than the batch learning setting, and does not make use of soft\nsplits or require the construction of a new tree for every update. We\ndemonstrate how one can apply these decision trees to different problems by\nchanging only the loss function, using classification, regression, and\nmulti-instance learning as example applications. In the experimental\nevaluation, our method performs similarly to standard incremental\nclassification trees, outperforms state of the art incremental regression\ntrees, and achieves comparable performance with batch multi-instance learning\nmethods. \n\n"}
{"id": "1901.08096", "contents": "Title: Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps\n  for Time Series Prediction Abstract: Despite the recent popularity of deep generative state space models, few\ncomparisons have been made between network architectures and the inference\nsteps of the Bayesian filtering framework -- with most models simultaneously\napproximating both state transition and update steps with a single recurrent\nneural network (RNN). In this paper, we introduce the Recurrent Neural Filter\n(RNF), a novel recurrent autoencoder architecture that learns distinct\nrepresentations for each Bayesian filtering step, captured by a series of\nencoders and decoders. Testing this on three real-world time series datasets,\nwe demonstrate that the decoupled representations learnt not only improve the\naccuracy of one-step-ahead forecasts while providing realistic uncertainty\nestimates, but also facilitate multistep prediction through the separation of\nencoder stages. \n\n"}
{"id": "1901.08125", "contents": "Title: Interpretable Neural Networks for Predicting Mortality Risk using\n  Multi-modal Electronic Health Records Abstract: We present an interpretable neural network for predicting an important\nclinical outcome (1-year mortality) from multi-modal Electronic Health Record\n(EHR) data. Our approach builds on prior multi-modal machine learning models by\nnow enabling visualization of how individual factors contribute to the overall\noutcome risk, assuming other factors remain constant, which was previously\nimpossible.\n  We demonstrate the value of this approach using a large multi-modal clinical\ndataset including both EHR data and 31,278 echocardiographic videos of the\nheart from 26,793 patients. We generated separate models for (i) clinical data\nonly (CD) (e.g. age, sex, diagnoses and laboratory values), (ii) numeric\nvariables derived from the videos, which we call echocardiography-derived\nmeasures (EDM), and (iii) CD+EDM+raw videos (pixel data). The interpretable\nmulti-modal model maintained performance compared to non-interpretable models\n(Random Forest, XGBoost), and also performed significantly better than a model\nusing a single modality (average AUC=0.82). Clinically relevant insights and\nmulti-modal variable importance rankings were also facilitated by the new\nmodel, which have previously been impossible. \n\n"}
{"id": "1901.08152", "contents": "Title: Veridical Data Science Abstract: Building and expanding on principles of statistics, machine learning, and\nscientific inquiry, we propose the predictability, computability, and stability\n(PCS) framework for veridical data science. Our framework, comprised of both a\nworkflow and documentation, aims to provide responsible, reliable,\nreproducible, and transparent results across the entire data science life\ncycle. The PCS workflow uses predictability as a reality check and considers\nthe importance of computation in data collection/storage and algorithm design.\nIt augments predictability and computability with an overarching stability\nprinciple for the data science life cycle. Stability expands on statistical\nuncertainty considerations to assess how human judgment calls impact data\nresults through data and model/algorithm perturbations. Moreover, we develop\ninference procedures that build on PCS, namely PCS perturbation intervals and\nPCS hypothesis testing, to investigate the stability of data results relative\nto problem formulation, data cleaning, modeling decisions, and interpretations.\nWe illustrate PCS inference through neuroscience and genomics projects of our\nown and others and compare it to existing methods in high dimensional, sparse\nlinear model simulations. Over a wide range of misspecified simulation models,\nPCS inference demonstrates favorable performance in terms of ROC curves.\nFinally, we propose PCS documentation based on R Markdown or Jupyter Notebook,\nwith publicly available, reproducible codes and narratives to back up human\nchoices made throughout an analysis. The PCS workflow and documentation are\ndemonstrated in a genomics case study available on Zenodo. \n\n"}
{"id": "1901.08255", "contents": "Title: Confidence-based Graph Convolutional Networks for Semi-Supervised\n  Learning Abstract: Predicting properties of nodes in a graph is an important problem with\napplications in a variety of domains. Graph-based Semi-Supervised Learning\n(SSL) methods aim to address this problem by labeling a small subset of the\nnodes as seeds and then utilizing the graph structure to predict label scores\nfor the rest of the nodes in the graph. Recently, Graph Convolutional Networks\n(GCNs) have achieved impressive performance on the graph-based SSL task. In\naddition to label scores, it is also desirable to have confidence scores\nassociated with them. Unfortunately, confidence estimation in the context of\nGCN has not been previously explored. We fill this important gap in this paper\nand propose ConfGCN, which estimates labels scores along with their confidences\njointly in GCN-based setting. ConfGCN uses these estimated confidences to\ndetermine the influence of one node on another during neighborhood aggregation,\nthereby acquiring anisotropic capabilities. Through extensive analysis and\nexperiments on standard benchmarks, we find that ConfGCN is able to outperform\nstate-of-the-art baselines. We have made ConfGCN's source code available to\nencourage reproducible research. \n\n"}
{"id": "1901.08814", "contents": "Title: Empowering individual trait prediction using interactions Abstract: One component of precision medicine is to construct prediction models with\ntheir predictive ability as high as possible, e.g. to enable individual risk\nprediction. In genetic epidemiology, complex diseases have a polygenic basis\nand a common assumption is that biological and genetic features affect the\noutcome under consideration via interactions. In the case of omics data, the\nuse of standard approaches such as generalized linear models may be suboptimal\nand machine learning methods are appealing to make individual predictions.\nHowever, most of these algorithms focus mostly on main or marginal effects of\nthe single features in a dataset. On the other hand, the detection of\ninteracting features is an active area of research in the realm of genetic\nepidemiology. One big class of algorithms to detect interacting features is\nbased on the multifactor dimensionality reduction (MDR). Here, we extend the\nmodel-based MDR (MB-MDR), a powerful extension of the original MDR algorithm,\nto enable interaction empowered individual prediction. Using a comprehensive\nsimulation study we show that our new algorithm can use information hidden in\ninteractions more efficiently than two other state-of-the-art algorithms,\nnamely the Random Forest and Elastic Net, and clearly outperforms these if\ninteractions are present. The performance of these algorithms is comparable if\nno interactions are present. Further, we show that our new algorithm is\napplicable to real data by comparing the performance of the three algorithms on\na dataset of rheumatoid arthritis cases and healthy controls. As our new\nalgorithm is not only applicable to biological/genetic data but to all datasets\nwith discrete features, it may have practical implications in other\napplications as well, and we made our method available as an R package. \n\n"}
{"id": "1901.09018", "contents": "Title: Provably efficient RL with Rich Observations via Latent State Decoding Abstract: We study the exploration problem in episodic MDPs with rich observations\ngenerated from a small number of latent states. Under certain identifiability\nassumptions, we demonstrate how to estimate a mapping from the observations to\nlatent states inductively through a sequence of regression and clustering steps\n-- where previously decoded latent states provide labels for later regression\nproblems -- and use it to construct good exploration policies. We provide\nfinite-sample guarantees on the quality of the learned state decoding function\nand exploration policies, and complement our theory with an empirical\nevaluation on a class of hard exploration problems. Our method exponentially\nimproves over $Q$-learning with na\\\"ive exploration, even when $Q$-learning has\ncheating access to latent states. \n\n"}
{"id": "1901.09021", "contents": "Title: Complexity of Linear Regions in Deep Networks Abstract: It is well-known that the expressivity of a neural network depends on its\narchitecture, with deeper networks expressing more complex functions. In the\ncase of networks that compute piecewise linear functions, such as those with\nReLU activation, the number of distinct linear regions is a natural measure of\nexpressivity. It is possible to construct networks with merely a single region,\nor for which the number of linear regions grows exponentially with depth; it is\nnot clear where within this range most networks fall in practice, either before\nor after training. In this paper, we provide a mathematical framework to count\nthe number of linear regions of a piecewise linear network and measure the\nvolume of the boundaries between these regions. In particular, we prove that\nfor networks at initialization, the average number of regions along any\none-dimensional subspace grows linearly in the total number of neurons, far\nbelow the exponential upper bound. We also find that the average distance to\nthe nearest region boundary at initialization scales like the inverse of the\nnumber of neurons. Our theory suggests that, even after training, the number of\nlinear regions is far below exponential, an intuition that matches our\nempirical observations. We conclude that the practical expressivity of neural\nnetworks is likely far below that of the theoretical maximum, and that this gap\ncan be quantified. \n\n"}
{"id": "1901.09207", "contents": "Title: Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning Abstract: Humans are capable of attributing latent mental contents such as beliefs or\nintentions to others. The social skill is critical in daily life for reasoning\nabout the potential consequences of others' behaviors so as to plan ahead. It\nis known that humans use such reasoning ability recursively by considering what\nothers believe about their own beliefs. In this paper, we start from level-$1$\nrecursion and introduce a probabilistic recursive reasoning (PR2) framework for\nmulti-agent reinforcement learning. Our hypothesis is that it is beneficial for\neach agent to account for how the opponents would react to its future\nbehaviors. Under the PR2 framework, we adopt variational Bayes methods to\napproximate the opponents' conditional policies, to which each agent finds the\nbest response and then improve their own policies. We develop\ndecentralized-training-decentralized-execution algorithms, namely PR2-Q and\nPR2-Actor-Critic, that are proved to converge in the self-play scenarios when\nthere exists one Nash equilibrium. Our methods are tested on both the matrix\ngame and the differential game, which have a non-trivial equilibrium where\ncommon gradient-based methods fail to converge. Our experiments show that it is\ncritical to reason about how the opponents believe about what the agent\nbelieves. We expect our work to contribute a new idea of modeling the opponents\nto the multi-agent reinforcement learning community. \n\n"}
{"id": "1901.09557", "contents": "Title: Out-of-Sample Testing for GANs Abstract: We propose a new method to evaluate GANs, namely EvalGAN. EvalGAN relies on a\ntest set to directly measure the reconstruction quality in the original sample\nspace (no auxiliary networks are necessary), and it also computes the\n(log)likelihood for the reconstructed samples in the test set. Further, EvalGAN\nis agnostic to the GAN algorithm and the dataset. We decided to test it on\nthree state-of-the-art GANs over the well-known CIFAR-10 and CelebA datasets. \n\n"}
{"id": "1901.10231", "contents": "Title: Detection of Alzheimers Disease from MRI using Convolutional Neural\n  Networks, Exploring Transfer Learning And BellCNN Abstract: There is a need for automatic diagnosis of certain diseases from medical\nimages that could help medical practitioners for further assessment towards\ntreating the illness. Alzheimers disease is a good example of a disease that is\noften misdiagnosed. Alzheimers disease (Hear after referred to as AD), is\ncaused by atrophy of certain brain regions and by brain cell death and is the\nleading cause of dementia and memory loss [1]. MRI scans reveal this\ninformation but atrophied regions are different for different individuals which\nmakes the diagnosis a bit more trickier and often gets misdiagnosed [1, 13]. We\nbelieve that our approach to this particular problem would improve the\nassessment quality by pre-flagging the images which are more likely to have AD.\nWe propose two solutions to this; one with transfer learning [9] and other by\nBellCNN [14], a custom made Convolutional Neural Network (Hear after referred\nto as CNN). Advantages and disadvantages of each approach will also be\ndiscussed in their respective sections. The dataset used for this project is\nprovided by Open Access Series of Imaging Studies (Hear after referred to as\nOASIS) [2, 3, 4], which contains over 400 subjects, 100 of whom have mild to\nsevere dementia. The dataset has labeled these subjects by two standards of\ndiagnosis; MiniMental State Examination (Hear after referred to as MMSE) and\nClinical Dementia Rating (Hear after referred to as CDR). These are some of the\ngeneral tools and concepts which are prerequisites to our solution; CNN [5, 6],\nNeural Networks [10] (Hear after referred to as NN), Anaconda bundle for\npython, Regression, Tensorflow [7]. Keywords: Alzheimers Disease, Convolutional\nNeural Network, BellCNN, Image Recognition, Machine Learning, MRI, OASIS,\nTensorflow \n\n"}
{"id": "1901.10334", "contents": "Title: Rank-one Convexification for Sparse Regression Abstract: Sparse regression models are increasingly prevalent due to their ease of\ninterpretability and superior out-of-sample performance. However, the exact\nmodel of sparse regression with an $\\ell_0$ constraint restricting the support\nof the estimators is a challenging (\\NP-hard) non-convex optimization problem.\nIn this paper, we derive new strong convex relaxations for sparse regression.\nThese relaxations are based on the ideal (convex-hull) formulations for\nrank-one quadratic terms with indicator variables. The new relaxations can be\nformulated as semidefinite optimization problems in an extended space and are\nstronger and more general than the state-of-the-art formulations, including the\nperspective reformulation and formulations with the reverse Huber penalty and\nthe minimax concave penalty functions. Furthermore, the proposed rank-one\nstrengthening can be interpreted as a \\textit{non-separable, non-convex,\nunbiased} sparsity-inducing regularizer, which dynamically adjusts its penalty\naccording to the shape of the error function without inducing bias for the\nsparse solutions. In our computational experiments with benchmark datasets, the\nproposed conic formulations are solved within seconds and result in\nnear-optimal solutions (with 0.4\\% optimality gap) for non-convex\n$\\ell_0$-problems. Moreover, the resulting estimators also outperform\nalternative convex approaches from a statistical perspective, achieving high\nprediction accuracy and good interpretability. \n\n"}
{"id": "1901.10371", "contents": "Title: On the Effect of Low-Rank Weights on Adversarial Robustness of Neural\n  Networks Abstract: Recently, there has been an abundance of works on designing Deep Neural\nNetworks (DNNs) that are robust to adversarial examples. In particular, a\ncentral question is which features of DNNs influence adversarial robustness\nand, therefore, can be to used to design robust DNNs. In this work, this\nproblem is studied through the lens of compression which is captured by the\nlow-rank structure of weight matrices. It is first shown that adversarial\ntraining tends to promote simultaneously low-rank and sparse structure in the\nweight matrices of neural networks. This is measured through the notions of\neffective rank and effective sparsity. In the reverse direction, when the low\nrank structure is promoted by nuclear norm regularization and combined with\nsparsity inducing regularizations, neural networks show significantly improved\nadversarial robustness. The effect of nuclear norm regularization on\nadversarial robustness is paramount when it is applied to convolutional neural\nnetworks. Although still not competing with adversarial training, this result\ncontributes to understanding the key properties of robust classifiers. \n\n"}
{"id": "1901.10548", "contents": "Title: Latent Normalizing Flows for Discrete Sequences Abstract: Normalizing flows are a powerful class of generative models for continuous\nrandom variables, showing both strong model flexibility and the potential for\nnon-autoregressive generation. These benefits are also desired when modeling\ndiscrete random variables such as text, but directly applying normalizing flows\nto discrete sequences poses significant additional challenges. We propose a\nVAE-based generative model which jointly learns a normalizing flow-based\ndistribution in the latent space and a stochastic mapping to an observed\ndiscrete space. In this setting, we find that it is crucial for the flow-based\ndistribution to be highly multimodal. To capture this property, we propose\nseveral normalizing flow architectures to maximize model flexibility.\nExperiments consider common discrete sequence tasks of character-level language\nmodeling and polyphonic music generation. Our results indicate that an\nautoregressive flow-based model can match the performance of a comparable\nautoregressive baseline, and a non-autoregressive flow-based model can improve\ngeneration speed with a penalty to performance. \n\n"}
{"id": "1901.10568", "contents": "Title: Stochastic Gradient MCMC for Nonlinear State Space Models Abstract: State space models (SSMs) provide a flexible framework for modeling complex\ntime series via a latent stochastic process. Inference for nonlinear,\nnon-Gaussian SSMs is often tackled with particle methods that do not scale well\nto long time series. The challenge is two-fold: not only do computations scale\nlinearly with time, as in the linear case, but particle filters additionally\nsuffer from increasing particle degeneracy with longer series. Stochastic\ngradient MCMC methods have been developed to scale Bayesian inference for\nfinite-state hidden Markov models and linear SSMs using buffered stochastic\ngradient estimates to account for temporal dependencies. We extend these\nstochastic gradient estimators to nonlinear SSMs using particle methods. We\npresent error bounds that account for both buffering error and particle error\nin the case of nonlinear SSMs that are log-concave in the latent process. We\nevaluate our proposed particle buffered stochastic gradient using stochastic\ngradient MCMC for inference on both long sequential synthetic and\nminute-resolution financial returns data, demonstrating the importance of this\nclass of methods. \n\n"}
{"id": "1901.10621", "contents": "Title: Enhanced Variational Inference with Dyadic Transformation Abstract: Variational autoencoder is a powerful deep generative model with variational\ninference. The practice of modeling latent variables in the VAE's original\nformulation as normal distributions with a diagonal covariance matrix limits\nthe flexibility to match the true posterior distribution. We propose a new\ntransformation, dyadic transformation (DT), that can model a multivariate\nnormal distribution. DT is a single-stage transformation with low computational\nrequirements. We demonstrate empirically on MNIST dataset that DT enhances the\nposterior flexibility and attains competitive results compared to other VAE\nenhancements. \n\n"}
{"id": "1901.10655", "contents": "Title: On the Calibration of Multiclass Classification with Rejection Abstract: We investigate the problem of multiclass classification with rejection, where\na classifier can choose not to make a prediction to avoid critical\nmisclassification. First, we consider an approach based on simultaneous\ntraining of a classifier and a rejector, which achieves the state-of-the-art\nperformance in the binary case. We analyze this approach for the multiclass\ncase and derive a general condition for calibration to the Bayes-optimal\nsolution, which suggests that calibration is hard to achieve by general loss\nfunctions unlike the binary case. Next, we consider another traditional\napproach based on confidence scores, in which the existing work focuses on a\nspecific class of losses. We propose rejection criteria for more general losses\nfor this approach and guarantee calibration to the Bayes-optimal solution.\nFinally, we conduct experiments to validate the relevance of our theoretical\nfindings. \n\n"}
{"id": "1901.10902", "contents": "Title: InfoBot: Transfer and Exploration via the Information Bottleneck Abstract: A central challenge in reinforcement learning is discovering effective\npolicies for tasks where rewards are sparsely distributed. We postulate that in\nthe absence of useful reward signals, an effective exploration strategy should\nseek out {\\it decision states}. These states lie at critical junctions in the\nstate space from where the agent can transition to new, potentially unexplored\nregions. We propose to learn about decision states from prior experience. By\ntraining a goal-conditioned policy with an information bottleneck, we can\nidentify decision states by examining where the model actually leverages the\ngoal state. We find that this simple mechanism effectively identifies decision\nstates, even in partially observed settings. In effect, the model learns the\nsensory cues that correlate with potential subgoals. In new environments, this\nmodel can then identify novel subgoals for further exploration, guiding the\nagent through a sequence of potential decision states and through new regions\nof the state space. \n\n"}
{"id": "1901.11149", "contents": "Title: Which Factorization Machine Modeling is Better: A Theoretical Answer\n  with Optimal Guarantee Abstract: Factorization machine (FM) is a popular machine learning model to capture the\nsecond order feature interactions. The optimal learning guarantee of FM and its\ngeneralized version is not yet developed. For a rank $k$ generalized FM of $d$\ndimensional input, the previous best known sampling complexity is\n$\\mathcal{O}[k^{3}d\\cdot\\mathrm{polylog}(kd)]$ under Gaussian distribution.\nThis bound is sub-optimal comparing to the information theoretical lower bound\n$\\mathcal{O}(kd)$. In this work, we aim to tighten this bound towards optimal\nand generalize the analysis to sub-gaussian distribution. We prove that when\nthe input data satisfies the so-called $\\tau$-Moment Invertible Property, the\nsampling complexity of generalized FM can be improved to\n$\\mathcal{O}[k^{2}d\\cdot\\mathrm{polylog}(kd)/\\tau^{2}]$. When the second order\nself-interaction terms are excluded in the generalized FM, the bound can be\nimproved to the optimal $\\mathcal{O}[kd\\cdot\\mathrm{polylog}(kd)]$ up to the\nlogarithmic factors. Our analysis also suggests that the positive semi-definite\nconstraint in the conventional FM is redundant as it does not improve the\nsampling complexity while making the model difficult to optimize. We evaluate\nour improved FM model in real-time high precision GPS signal calibration task\nto validate its superiority. \n\n"}
{"id": "1901.11356", "contents": "Title: Functional Regularisation for Continual Learning with Gaussian Processes Abstract: We introduce a framework for Continual Learning (CL) based on Bayesian\ninference over the function space rather than the parameters of a deep neural\nnetwork. This method, referred to as functional regularisation for Continual\nLearning, avoids forgetting a previous task by constructing and memorising an\napproximate posterior belief over the underlying task-specific function. To\nachieve this we rely on a Gaussian process obtained by treating the weights of\nthe last layer of a neural network as random and Gaussian distributed. Then,\nthe training algorithm sequentially encounters tasks and constructs posterior\nbeliefs over the task-specific functions by using inducing point sparse\nGaussian process methods. At each step a new task is first learnt and then a\nsummary is constructed consisting of (i) inducing inputs -- a fixed-size subset\nof the task inputs selected such that it optimally represents the task -- and\n(ii) a posterior distribution over the function values at these inputs. This\nsummary then regularises learning of future tasks, through Kullback-Leibler\nregularisation terms. Our method thus unites approaches focused on\n(pseudo-)rehearsal with those derived from a sequential Bayesian inference\nperspective in a principled way, leading to strong results on accepted\nbenchmarks. \n\n"}
{"id": "cs/0004001", "contents": "Title: A Theory of Universal Artificial Intelligence based on Algorithmic\n  Complexity Abstract: Decision theory formally solves the problem of rational agents in uncertain\nworlds if the true environmental prior probability distribution is known.\nSolomonoff's theory of universal induction formally solves the problem of\nsequence prediction for unknown prior distribution. We combine both ideas and\nget a parameterless theory of universal Artificial Intelligence. We give strong\narguments that the resulting AIXI model is the most intelligent unbiased agent\npossible. We outline for a number of problem classes, including sequence\nprediction, strategic games, function minimization, reinforcement and\nsupervised learning, how the AIXI model can formally solve them. The major\ndrawback of the AIXI model is that it is uncomputable. To overcome this\nproblem, we construct a modified algorithm AIXI-tl, which is still effectively\nmore intelligent than any other time t and space l bounded agent. The\ncomputation time of AIXI-tl is of the order tx2^l. Other discussed topics are\nformal definitions of intelligence order relations, the horizon problem and\nrelations of the AIXI theory to other AI approaches. \n\n"}
{"id": "cs/0508043", "contents": "Title: Sequential Predictions based on Algorithmic Complexity Abstract: This paper studies sequence prediction based on the monotone Kolmogorov\ncomplexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is\nextremely close to Solomonoff's universal prior M, the latter being an\nexcellent predictor in deterministic as well as probabilistic environments,\nwhere performance is measured in terms of convergence of posteriors or losses.\nDespite this closeness to M, it is difficult to assess the prediction quality\nof m, since little is known about the closeness of their posteriors, which are\nthe important quantities for prediction. We show that for deterministic\ncomputable environments, the \"posterior\" and losses of m converge, but rapid\nconvergence could only be shown on-sequence; the off-sequence convergence can\nbe slow. In probabilistic environments, neither the posterior nor the losses\nconverge, in general. \n\n"}
{"id": "quant-ph/0308164", "contents": "Title: Estimation of the Local Density of States on a Quantum Computer Abstract: We report an efficient quantum algorithm for estimating the local density of\nstates (LDOS) on a quantum computer. The LDOS describes the redistribution of\nenergy levels of a quantum system under the influence of a perturbation.\nSometimes known as the ``strength function'' from nuclear spectroscopy\nexperiments, the shape of the LDOS is directly related to the survivial\nprobability of unperturbed eigenstates, and has recently been related to the\nfidelity decay (or ``Loschmidt echo'') under imperfect motion-reversal. For\nquantum systems that can be simulated efficiently on a quantum computer, the\nLDOS estimation algorithm enables an exponential speed-up over direct classical\ncomputation. \n\n"}
{"id": "quant-ph/0312070", "contents": "Title: Phase-dependent decoherence of optical transitions in Pr3+:LaF3 in the\n  presence of a driving field Abstract: The decoherence times of orthogonally phased components of the optical\ntransition dipole moment in a two-level system have been observed to differ by\nan order of magnitude. This phase anisotropy is observed in coherent transient\nexperiments where an optical driving field is present during extended periods\nof decoherence. The decoherence time of the component of the dipole moment in\nphase with the driving field is extended compared to T_2, obtained from\ntwo-pulse photon echoes, in analogy with the spin locking technique of NMR. \n\n"}
{"id": "quant-ph/0401019", "contents": "Title: Quantum computing Abstract: This article gives an elementary introduction to quantum computing. It is a\ndraft for a book chapter of the \"Handbook of Nature-Inspired and Innovative\nComputing\", Eds. A. Zomaya, G.J. Milburn, J. Dongarra, D. Bader, R. Brent, M.\nEshaghian-Wilner, F. Seredynski (Springer, Berlin Heidelberg New York, 2006). \n\n"}
{"id": "quant-ph/0403052", "contents": "Title: Scalable quantum computation in systems with Bose-Hubbard dynamics Abstract: Several proposals for quantum computation utilize a lattice type architecture\nwith qubits trapped by a periodic potential. For systems undergoing many body\ninteractions described by the Bose-Hubbard Hamiltonian, the ground state of the\nsystem carries number fluctuations that scale with the number of qubits. This\nprocess degrades the initialization of the quantum computer register and can\nintroduce errors during error correction. In an earlier manuscript we proposed\na solution to this problem tailored to the loading of cold atoms into an\noptical lattice via the Mott Insulator phase transition. It was shown that by\nadding an inhomogeneity to the lattice and performing a continuous measurement,\nthe unit filled state suitable for a quantum computer register can be\nmaintained. Here, we give a more rigorous derivation of the register fidelity\nin homogeneous and inhomogeneous lattices and provide evidence that the\nprotocol is effective in the finite temperature regime. \n\n"}
{"id": "quant-ph/0403111", "contents": "Title: A Quantum Many-Body Instability in the Thermodynamic Limit Abstract: Intrinsic decoherence in the thermodynamic limit is shown for a large class\nof many-body quantum systems in the unitary evolution in NMR and cavity QED.\nThe effect largely depends on the inability of the system to recover the\nphases. Gaussian decaying in time of the fidelity is proved for spin systems\nand radiation-matter interaction. \n\n"}
{"id": "quant-ph/0403113", "contents": "Title: Counterintuitive transitions in the multistate Landau-Zener problem with\n  linear level crossings Abstract: We generalize the Brundobler-Elser hypothesis in the multistate Landau-Zener\nproblem to the case when instead of a state with the highest slope of the\ndiabatic energy level there is a band of states with an arbitrary number of\nparallel levels having the same slope. We argue that the probabilities of\ncounterintuitive transitions among such states are exactly zero. \n\n"}
{"id": "quant-ph/0406057", "contents": "Title: Quantum Dynamics of a Particle with a Spin-dependent Velocity Abstract: We study the dynamics of a particle in continuous time and space, the\ndisplacement of which is governed by an internal degree of freedom (spin). In\none definite limit, the so-called quantum random walk is recovered but,\nalthough quite simple, the model possesses a rich variety of dynamics and goes\nfar beyond this problem. Generally speaking, our framework can describe the\nmotion of an electron in a magnetic sea near the Fermi level when linearisation\nof the dispersion law is possible, coupled to a transverse magnetic field.\nQuite unexpected behaviours are obtained. In particular, we find that when the\ninitial wave packet is fully localized in space, the $J_{z}$ angular momentum\ncomponent is frozen; this is an interesting example of an observable which,\nalthough it is not a constant of motion, has a constant expectation value. For\na non-completely localized wave packet, the effect still occurs although less\npronounced, and the spin keeps for ever memory of its initial state. Generally\nspeaking, as time goes on, the spatial density profile looks rather complex, as\na consequence of the competition between drift and precession, and displays\nvarious shapes according to the ratio between the Larmor period and the\ncharacteristic time of flight. The density profile gradually changes from a\nmultimodal quickly moving distribution when the scatttering rate is small, to a\nunimodal standing but flattening distribution in the opposite cas case. \n\n"}
{"id": "quant-ph/0501165", "contents": "Title: Tunnelling of condensate magnetization in a double-well potential Abstract: We study quantum dynamical properties of a spin-1 atomic Bose-Einstein\ncondensate in a double-well potential. Adopting a mean field theory and single\nspatial mode approximation, we characterize our model system as two coupled\nspins. For certain initial states, we find full magnetization oscillations\nbetween wells not accompanied by mass (or atom numbers) exchange. We identify\ndynamic regimes of collective spin variables arising from nonlinear\nself-interactions that are different from the usual Josephson oscillations. We\nalso discuss magnetization beats and incomplete oscillations of collective spin\nvariables other than the magnetization. Our study points to an alternative\napproach to observe coherent tunnelling of a condensate through a (spatial)\npotential barrier. \n\n"}
{"id": "quant-ph/0503174", "contents": "Title: Simulation of many-qubit quantum computation with matrix product states Abstract: Matrix product states provide a natural entanglement basis to represent a\nquantum register and operate quantum gates on it. This scheme can be\nmaterialized to simulate a quantum adiabatic algorithm solving hard instances\nof a NP-Complete problem. Errors inherent to truncations of the exact action of\ninteracting gates are controlled by the size of the matrices in the\nrepresentation. The property of finding the right solution for an instance and\nthe expected value of the energy are found to be remarkably robust against\nthese errors. As a symbolic example, we simulate the algorithm solving a\n100-qubit hard instance, that is, finding the correct product state out of ~\n10^30 possibilities. Accumulated statistics for up to 60 qubits point at a slow\ngrowth of the average minimum time to solve hard instances with\nhighly-truncated simulations of adiabatic quantum evolution. \n\n"}
{"id": "quant-ph/0508232", "contents": "Title: High fidelity measurement and quantum feedback control in circuit QED Abstract: Circuit QED is a promising solid-state quantum computing architecture. It\nalso has excellent potential as a platform for quantum control -- especially\nquantum feedback control -- experiments. However, the current scheme for\nmeasurement in circuit QED is low efficiency and has low signal-to-noise ratio\nfor single shot measurements. The low quality of this measurement makes the\nimplementation of feedback difficult, and here we propose two schemes for\nmeasurement in circuit QED architectures that can significantly improve\nsignal-to-noise, and potentially achieve quantum limited measurement. Such\nmeasurements would enable the implementation of quantum feedback protocols and\nwe illustrate this with a simple entanglement stabilization scheme. \n\n"}
{"id": "quant-ph/0509166", "contents": "Title: Quantum states on Harmonic lattices Abstract: We investigate bosonic Gaussian quantum states on an infinite cubic lattice\nin arbitrary spatial dimensions. We derive general properties of such states as\nground states of quadratic Hamiltonians for both critical and non-critical\ncases. Tight analytic relations between the decay of the interaction and the\ncorrelation functions are proven and the dependence of the correlation length\non band gap and effective mass is derived. We show that properties of critical\nground states depend on the gap of the point-symmetrized rather than on that of\nthe original Hamiltonian. For critical systems with polynomially decaying\ninteractions logarithmic deviations from polynomially decaying correlation\nfunctions are found. Moreover, we provide a generalization of the matrix\nproduct state representation for Gaussian states and show that properties hold\nanalogously to the case of finite dimensional spin systems. \n\n"}
{"id": "quant-ph/0511010", "contents": "Title: Dissipative decoherence in the Grover algorithm Abstract: Using the methods of quantum trajectories we study effects of dissipative\ndecoherence on the accuracy of the Grover quantum search algorithm. The\ndependence on the number of qubits and dissipation rate are determined and\ntested numerically with up to 16 qubits. As a result, our numerical and\nanalytical studies give the universal law for decay of fidelity and probability\nof searched state which are induced by dissipative decoherence effects. This\nlaw is in agreement with the results obtained previously for quantum chaos\nalgorithms. \n\n"}
{"id": "quant-ph/0512209", "contents": "Title: Quantum Computation, Complexity, and Many-Body Physics Abstract: Recently developed quantum algorithms suggest that quantum computers can\nsolve certain problems and perform certain tasks more efficiently than\nconventional computers. Among other reasons, this is due to the possibility of\ncreating non-classical correlations, or quantum entanglement, which is a\nphenomena hard or impossible to reproduce by classical-information methods.\n  In this thesis I first investigate the simulation of quantum systems on a\nquantum computer constructed of two-level quantum elements or qubits. For this\npurpose, I present algebra mappings that allow one to obtain physical\nproperties and compute correlation functions of fermionic, anyonic, and bosonic\nsystems with such a computer. The results obtained show that the complexity of\npreparing a quantum state which contains the desired information for the\ncomputation is crucial.\n  Second, I present a wide class of quantum computations, which could involve\nentangled states, that can be simulated with the same efficiency on both types\nof computers. The notion of generalized quantum entanglement then naturally\nemerges. This generalization of entanglement is based on the idea that\nentanglement is an observer-dependent concept, that is, relative to a set of\npreferred observables. \n\n"}
{"id": "quant-ph/0602091", "contents": "Title: Berry Phases and Quantum Phase Transitions Abstract: We study the connection between Berry phases and quantum phase transitions of\ngeneric quantum many-body systems. Consider sequences of Berry phases\nassociated to sequences of loops in the parameter space whose limit is a point.\nIf the sequence of Berry phases does not converge to zero, then the limit point\nis a quantum critical point. Quantum critical points are associated to failures\nof adiabaticity. We discuss the remarkable example of the anisotropic XY spin\nchain in a transverse magnetic field and detect the XX region of criticality. \n\n"}
{"id": "quant-ph/0605094", "contents": "Title: Homological Error Correction: Classical and Quantum Codes Abstract: We prove several theorems characterizing the existence of homological error\ncorrection codes both classically and quantumly. Not every classical code is\nhomological, but we find a family of classical homological codes saturating the\nHamming bound. In the quantum case, we show that for non-orientable surfaces it\nis impossible to construct homological codes based on qudits of dimension\n$D>2$, while for orientable surfaces with boundaries it is possible to\nconstruct them for arbitrary dimension $D$. We give a method to obtain planar\nhomological codes based on the construction of quantum codes on compact\nsurfaces without boundaries. We show how the original Shor's 9-qubit code can\nbe visualized as a homological quantum code. We study the problem of\nconstructing quantum codes with optimal encoding rate. In the particular case\nof toric codes we construct an optimal family and give an explicit proof of its\noptimality. For homological quantum codes on surfaces of arbitrary genus we\nalso construct a family of codes asymptotically attaining the maximum possible\nencoding rate. We provide the tools of homology group theory for graphs\nembedded on surfaces in a self-contained manner. \n\n"}
{"id": "quant-ph/0605212", "contents": "Title: Decoherence-based exploration of d-dimensional one-way quantum\n  computation Abstract: We study the effects of amplitude and phase damping decoherence in\nd-dimensional one-way quantum computation (QC). Our investigation shows how\ninformation transfer and entangling gate simulations are affected for d>=2. To\nunderstand motivations for extending the one-way model to higher dimensions, we\ndescribe how d-dimensional qudit cluster states deteriorate under environmental\nnoise. In order to protect quantum information from the environment we consider\nthe encoding of logical qubits into physical qudits and compare entangled pairs\nof linear qubit-cluster states with single qudit clusters of equal length and\ntotal dimension. Our study shows a significant reduction in the performance of\none-way QC for d>2 in the presence of Markovian type decoherence models. \n\n"}
{"id": "quant-ph/0607079", "contents": "Title: Exact propagators for atom-laser interactions Abstract: A class of exact propagators describing the interaction of an $N$-level atom\nwith a set of on-resonance $\\delta$-lasers is obtained by means of the Laplace\ntransform method. State-selective mirrors are described in the limit of strong\nlasers. The ladder, V and $\\Lambda$ configurations for a three-level atom are\ndiscussed. For the two level case, the transient effects arising as result of\nthe interaction between both a semi-infinite beam and a wavepacket with the\non-resonance laser are examined. \n\n"}
{"id": "quant-ph/0608212", "contents": "Title: Decoherence in a scalable adiabatic quantum computer Abstract: We consider the effects of decoherence on Landau-Zener crossings encountered\nin a large-scale adiabatic-quantum-computing setup. We analyze the dependence\nof the success probability, i.e. the probability for the system to end up in\nits new ground state, on the noise amplitude and correlation time. We determine\nthe optimal sweep rate that is required to maximize the success probability. We\nthen discuss the scaling of decoherence effects with increasing system size. We\nfind that those effects can be important for large systems, even if they are\nsmall for each of the small building blocks. \n\n"}
{"id": "quant-ph/0609119", "contents": "Title: Ultracold Bosons in a Tilted Multi-level Double-Well Potential Abstract: The N -body problem in a double well requires new features for quantum\ninformation processing, macroscopic quantum superposition, and other\nfundamental studies of quantum many body physics in ultracold atoms. One needs\n(a) tilt, and (b) to go beyond the single-particle ground state in each well,\ni.e., to two or more energy levels. For (a), we show that a small potential\ndifference between the wells, or tilt, causes the decoherence of Schrodinger\ncat states. However, these states reappear when the tilt can be compensated by\natom-atom interactions; these tilted cat states constitute partial cats that\nare protected from decoherence by the many body wavefunction. For (b), we\nprovide explicit criteria for when two energy levels are needed to describe the\nstate space. For typical experimental parameters, two levels are indeed\nrequired for creation of cat states. \n\n"}
{"id": "quant-ph/0609131", "contents": "Title: Observing quantum non-locality in the entanglement between modes of\n  massive particles Abstract: We consider the question of whether it is possible to use the entanglement\nbetween spatially separated modes of massive particles to observe nonlocal\nquantum correlations. Mode entanglement can be obtained using a single\nparticle, indicating that it requires careful consideration before concluding\nwhether experimental observation, e.g. violation of Bell inequalities, is\npossible or not. In the simplest setups analogous to optics experiments, that\nobservation is prohibited by fundamental conservation laws. However, we show\nthat using auxiliary particles, mode entanglement can be converted into forms\nthat allow the observation of quantum non-locality. The probability of\nsuccessful conversion depends on the nature and number of auxiliary particles\nused. In particular, we find that an auxiliary Bose-Einstein condensate allows\nthe conversion arbitrarily many times with a small error that depends only on\nthe initial state of the condensate. \n\n"}
{"id": "quant-ph/0609178", "contents": "Title: Coexistence of unlimited bipartite and genuine multipartite\n  entanglement: Promiscuous quantum correlations arising from discrete to\n  continuous variable systems Abstract: Quantum mechanics imposes 'monogamy' constraints on the sharing of\nentanglement. We show that, despite these limitations, entanglement can be\nfully 'promiscuous', i.e. simultaneously present in unlimited two-body and\nmany-body forms in states living in an infinite-dimensional Hilbert space.\nMonogamy just bounds the divergence rate of the various entanglement\ncontributions. This is demonstrated in simple families of N-mode (N >= 4)\nGaussian states of light fields or atomic ensembles, which therefore enable\ninfinitely more freedom in the distribution of information, as opposed to\nsystems of individual qubits. Such a finding is of importance for the\nquantification, understanding and potential exploitation of shared quantum\ncorrelations in continuous variable systems. We discuss how promiscuity\ngradually arises when considering simple families of discrete variable states,\nwith increasing Hilbert space dimension towards the continuous variable limit.\nSuch models are somehow analogous to Gaussian states with asymptotically\ndiverging, but finite squeezing. In this respect, we find that non-Gaussian\nstates (which in general are more entangled than Gaussian states), exhibit also\nthe interesting feature that their entanglement is more shareable: in the\nnon-Gaussian multipartite arena, unlimited promiscuity can be already achieved\namong three entangled parties, while this is impossible for Gaussian, even\ninfinitely squeezed states. \n\n"}
{"id": "quant-ph/0609216", "contents": "Title: A Quantum Approach to Classical Statistical Mechanics Abstract: We present a new approach to study the thermodynamic properties of\n$d$-dimensional classical systems by reducing the problem to the computation of\nground state properties of a $d$-dimensional quantum model. This\nclassical-to-quantum mapping allows us to deal with standard optimization\nmethods, such as simulated and quantum annealing, on an equal basis.\nConsequently, we extend the quantum annealing method to simulate classical\nsystems at finite temperatures. Using the adiabatic theorem of quantum\nmechanics, we derive the rates to assure convergence to the optimal\nthermodynamic state. For simulated and quantum annealing, we obtain the\nasymptotic rates of $T(t) \\approx (p N) /(k_B \\log t)$ and $\\gamma(t) \\approx\n(Nt)^{-\\bar{c}/N}$, for the temperature and magnetic field, respectively. Other\nannealing strategies, as well as their potential speed-up, are also discussed. \n\n"}
{"id": "quant-ph/0610094", "contents": "Title: Demonstration of optically modulated dispersion forces Abstract: We report the first experiment on the optical modulation of dispersion forces\nthrough a change of the carrier density in a Si membrane. For this purpose a\nhigh-vacuum based atomic force microscope and excitation light pulses from an\nAr laser are used. The experimental results are compared with two theoretical\nmodels. The modulation of the dispersion force will find applications in\noptomechanical micromachines. \n\n"}
{"id": "quant-ph/0611024", "contents": "Title: Thermodynamic Limit and Decoherence: Rigorous Results Abstract: Time evolution operator in quantum mechanics can be changed into a\nstatistical operator by a Wick rotation. This strict relation between\nstatistical mechanics and quantum evolution can reveal deep results when the\nthermodynamic limit is considered. These results translate in a set of theorems\nproving that these effects can be effectively at work producing an emerging\nclassical world without recurring to any external entity that in some cases\ncannot be properly defined. In a many-body system has been recently shown that\nGaussian decay of the coherence is the rule with a duration of recurrence more\nand more small as the number of particles increases. This effect has been\nobserved experimentally. More generally, a theorem about coherence of bulk\nmatter can be proved. All this takes us to the conclusion that a well definite\nboundary for the quantum to classical world does exist and that can be drawn by\nthe thermodynamic limit, extending in this way the deep link between\nstatistical mechanics and quantum evolution to a high degree. \n\n"}
{"id": "quant-ph/0611200", "contents": "Title: Gaussian Decoherence and Gaussian Echo from Spin Environments Abstract: We examine an exactly solvable model of decoherence -- a spin-system\ninteracting with a collection of environment spins. We show that in this simple\nmodel (introduced some time ago to illustrate environment--induced\nsuperselection) generic assumptions about the coupling strengths lead to a\nuniversal (Gaussian) suppression of coherence between pointer states. We\nexplore the regime of validity of this result and discuss its relation to\nspectral features of the environment. We also consider its relevance to the\nexperiments on the so-called Loschmidt echo (which measures, in effect, the\nfidelity between the initial and time-reversed or \"echo\" signal). In\nparticular, we show that for partial reversals (e.g., when of only a part of\nthe total Hamiltonian changes sign) fidelity will exhibit a Gaussian dependence\non the time of reversal. In such cases echo may become independent of the\ndetails of the reversal procedure or the specifics of the coupling to the\nenvironment. This puzzling behavior was observed in several NMR experiments.\nNatural candidates for such two environments (one of which is easily reversed,\nwhile the other is ``irreversible'') are suggested for the experiment involving\nferrocene. \n\n"}
{"id": "quant-ph/0611264", "contents": "Title: Statistics dependence of the entanglement entropy Abstract: The entanglement entropy of a distinguished region of a quantum many-body\nsystem reflects the entanglement present in its pure ground state. In this\nwork, we establish scaling laws for this entanglement for critical quasi-free\nfermionic and bosonic lattice systems, without resorting to numerical means. We\nconsider the geometrical setting of D-dimensional half-spaces which allows us\nto exploit a connection to the one-dimensional case. Intriguingly, we find a\ndifference in the scaling properties depending on whether the system is bosonic\n- where an area-law is first proven to hold - or fermionic, extending previous\nfindings for cubic regions. For bosonic systems with nearest neighbor\ninteraction we prove the conjectured area-law by computing the logarithmic\nnegativity analytically. We identify a length scale associated with\nentanglement, different from the correlation length. For fermions we determine\nthe logarithmic correction to the area-law, which depends on the topology of\nthe Fermi surface. We find that Lifshitz quantum phase transitions are\naccompanied with a non-analyticity in the prefactor of the leading order term. \n\n"}
{"id": "quant-ph/0612191", "contents": "Title: Multimode quantum limits to the linewidth of an atom laser Abstract: The linewidth of an atom laser can be limited by excitation of higher energy\nmodes in the source Bose-Einstein condensate, energy shifts in that condensate\ndue to the atomic interactions, or phase diffusion of the lasing mode due to\nthose interactions. The first two are effects that can be described with a\nsemiclassical model, and have been studied in detail for both pumped and\nunpumped atom lasers. The third is a purely quantum statistical effect, and has\nbeen studied only in zero dimensional models. We examine an unpumped atom laser\nin one dimension using a quantum field theory using stochastic methods based on\nthe truncated Wigner approach. This allows spatial and statistical effects to\nbe examined simultaneously, and the linewidth limit for unpumped atom lasers is\nquantified in various limits. \n\n"}
{"id": "quant-ph/0701102", "contents": "Title: Approximate quantum error correction, random codes, and quantum channel\n  capacity Abstract: We work out a theory of approximate quantum error correction that allows us\nto derive a general lower bound for the entanglement fidelity of a quantum\ncode. The lower bound is given in terms of Kraus operators of the quantum\nnoise. This result is then used to analyze the average error correcting\nperformance of codes that are randomly drawn from unitarily invariant code\nensembles. Our results confirm that random codes of sufficiently large block\nsize are highly suitable for quantum error correction. Moreover, employing a\nlemma of Bennett, Shor, Smolin, and Thapliyal, we prove that random coding\nattains information rates of the regularized coherent information. \n\n"}
{"id": "quant-ph/0701182", "contents": "Title: Generation of EPR-entangled radiation through an atomic reservoir Abstract: We propose a scheme for generating two-mode squeezing in high-Q resonators\nusing a beam of atoms with random arrival times, which acts as a reservoir for\nthe field. The scheme is based on four-wave mixing processes leading to\nemission into two cavity modes, which are resonant with the Rabi sidebands of\nthe atomic dipole transition, driven by a saturating classical field. At steady\nstate the cavity modes are in an Einstein-Podolski-Rosen (EPR) state, whose\ndegree of entanglement is controlled by the intensity and the frequency of the\ntransverse field. This scheme is robust against stochastic fluctuations in the\natomic beam, does not require atomic detection nor velocity selection, and can\nbe realized by presently available experimental setups with microwave\nresonators. \n\n"}
{"id": "quant-ph/0703017", "contents": "Title: A complete characterization of mixed state entanglement using\n  probability density functions Abstract: We propose that the entanglement of mixed states is characterised properly in\nterms of a probability density function $\\mathcal{P}(\\mathcal{E})$. There is a\nneed for such a measure since the prevalent measures (such as\n\\textit{concurrence} and \\textit{negativity}) are rough benchmarks, and not\nmonotones of each other. Considering the specific case of two qubit mixed\nstates, we provide an explicit construction of $\\mathcal{P}(\\mathcal{E})$ and\nshow that it is characterised by a set of parameters, of which concurrence is\nbut one particular combination. $\\mathcal{P}(\\mathcal{E})$ is manifestly\ninvariant under $SU(2) \\times SU(2)$ transformations. It can, in fact,\nreconstruct the state up to local operations\n  - with the specification of at most four additional parameters. Finally the\nnew measure resolves the controversy regarding the role of entanglement in\nquantum computation in NMR systems. \n\n"}
