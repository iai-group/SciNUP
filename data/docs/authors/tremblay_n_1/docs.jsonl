{"id": "0901.3202", "contents": "Title: Model-Consistent Sparse Estimation through the Bootstrap Abstract: We consider the least-square linear regression problem with regularization by\nthe $\\ell^1$-norm, a problem usually referred to as the Lasso. In this paper,\nwe first present a detailed asymptotic analysis of model consistency of the\nLasso in low-dimensional settings. For various decays of the regularization\nparameter, we compute asymptotic equivalents of the probability of correct\nmodel selection. For a specific rate decay, we show that the Lasso selects all\nthe variables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection procedure, referred to as the Bolasso, is\nextended to high-dimensional settings by a provably consistent two-step\nprocedure. \n\n"}
{"id": "0910.0610", "contents": "Title: Regularization Techniques for Learning with Matrices Abstract: There is growing body of learning problems for which it is natural to\norganize the parameters into matrix, so as to appropriately regularize the\nparameters under some matrix norm (in order to impose some more sophisticated\nprior knowledge). This work describes and analyzes a systematic method for\nconstructing such matrix-based, regularization methods. In particular, we focus\non how the underlying statistical properties of a given problem can help us\ndecide which regularization function is appropriate.\n  Our methodology is based on the known duality fact: that a function is\nstrongly convex with respect to some norm if and only if its conjugate function\nis strongly smooth with respect to the dual norm. This result has already been\nfound to be a key component in deriving and analyzing several learning\nalgorithms. We demonstrate the potential of this framework by deriving novel\ngeneralization and regret bounds for multi-task learning, multi-class learning,\nand kernel learning. \n\n"}
{"id": "1104.0729", "contents": "Title: Online and Batch Learning Algorithms for Data with Missing Features Abstract: We introduce new online and batch algorithms that are robust to data with\nmissing features, a situation that arises in many practical applications. In\nthe online setup, we allow for the comparison hypothesis to change as a\nfunction of the subset of features that is observed on any given round,\nextending the standard setting where the comparison hypothesis is fixed\nthroughout. In the batch setup, we present a convex relation of a non-convex\nproblem to jointly estimate an imputation function, used to fill in the values\nof missing features, along with the classification hypothesis. We prove regret\nbounds in the online setting and Rademacher complexity bounds for the batch\ni.i.d. setting. The algorithms are tested on several UCI datasets, showing\nsuperior performance over baselines. \n\n"}
{"id": "1104.3791", "contents": "Title: Fast matrix computations for pair-wise and column-wise commute times and\n  Katz scores Abstract: We first explore methods for approximating the commute time and Katz score\nbetween a pair of nodes. These methods are based on the approach of matrices,\nmoments, and quadrature developed in the numerical linear algebra community.\nThey rely on the Lanczos process and provide upper and lower bounds on an\nestimate of the pair-wise scores. We also explore methods to approximate the\ncommute times and Katz scores from a node to all other nodes in the graph.\nHere, our approach for the commute times is based on a variation of the\nconjugate gradient algorithm, and it provides an estimate of all the diagonals\nof the inverse of a matrix. Our technique for the Katz scores is based on\nexploiting an empirical localization property of the Katz matrix. We adopt\nalgorithms used for personalized PageRank computing to these Katz scores and\ntheoretically show that this approach is convergent. We evaluate these methods\non 17 real world graphs ranging in size from 1000 to 1,000,000 nodes. Our\nresults show that our pair-wise commute time method and column-wise Katz\nalgorithm both have attractive theoretical properties and empirical\nperformance. \n\n"}
{"id": "1106.1622", "contents": "Title: Large-Scale Convex Minimization with a Low-Rank Constraint Abstract: We address the problem of minimizing a convex function over the space of\nlarge matrices with low rank. While this optimization problem is hard in\ngeneral, we propose an efficient greedy algorithm and derive its formal\napproximation guarantees. Each iteration of the algorithm involves\n(approximately) finding the left and right singular vectors corresponding to\nthe largest singular value of a certain matrix, which can be calculated in\nlinear time. This leads to an algorithm which can scale to large matrices\narising in several applications such as matrix completion for collaborative\nfiltering and robust low rank matrix approximation. \n\n"}
{"id": "1107.2021", "contents": "Title: Multi-Instance Learning with Any Hypothesis Class Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size. \n\n"}
{"id": "1204.0062", "contents": "Title: Improved matrix algorithms via the Subsampled Randomized Hadamard\n  Transform Abstract: Several recent randomized linear algebra algorithms rely upon fast dimension\nreduction methods. A popular choice is the Subsampled Randomized Hadamard\nTransform (SRHT). In this article, we address the efficacy, in the Frobenius\nand spectral norms, of an SRHT-based low-rank matrix approximation technique\nintroduced by Woolfe, Liberty, Rohklin, and Tygert. We establish a slightly\nbetter Frobenius norm error bound than currently available, and a much sharper\nspectral norm error bound (in the presence of reasonable decay of the singular\nvalues). Along the way, we produce several results on matrix operations with\nSRHTs (such as approximate matrix multiplication) that may be of independent\ninterest. Our approach builds upon Tropp's in \"Improved analysis of the\nSubsampled Randomized Hadamard Transform\". \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1206.0333", "contents": "Title: Sparse Trace Norm Regularization Abstract: We study the problem of estimating multiple predictive functions from a\ndictionary of basis functions in the nonparametric regression setting. Our\nestimation scheme assumes that each predictive function can be estimated in the\nform of a linear combination of the basis functions. By assuming that the\ncoefficient matrix admits a sparse low-rank structure, we formulate the\nfunction estimation problem as a convex program regularized by the trace norm\nand the $\\ell_1$-norm simultaneously. We propose to solve the convex program\nusing the accelerated gradient (AG) method and the alternating direction method\nof multipliers (ADMM) respectively; we also develop efficient algorithms to\nsolve the key components in both AG and ADMM. In addition, we conduct\ntheoretical analysis on the proposed function estimation scheme: we derive a\nkey property of the optimal solution to the convex program; based on an\nassumption on the basis functions, we establish a performance bound of the\nproposed function estimation scheme (via the composite regularization).\nSimulation studies demonstrate the effectiveness and efficiency of the proposed\nalgorithms. \n\n"}
{"id": "1206.5162", "contents": "Title: Fast Variational Inference in the Conjugate Exponential Family Abstract: We present a general method for deriving collapsed variational inference\nalgo- rithms for probabilistic models in the conjugate exponential family. Our\nmethod unifies many existing approaches to collapsed variational inference. Our\ncollapsed variational inference leads to a new lower bound on the marginal\nlikelihood. We exploit the information geometry of the bound to derive much\nfaster optimization methods based on conjugate gradients for these models. Our\napproach is very general and is easily applied to any model where the mean\nfield update equations have been derived. Empirically we show significant\nspeed-ups for probabilistic models optimized using our bound. \n\n"}
{"id": "1206.6381", "contents": "Title: Shortest path distance in random k-nearest neighbor graphs Abstract: Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs. \n\n"}
{"id": "1207.3012", "contents": "Title: Optimal rates for first-order stochastic convex optimization under\n  Tsybakov noise condition Abstract: We focus on the problem of minimizing a convex function $f$ over a convex set\n$S$ given $T$ queries to a stochastic first order oracle. We argue that the\ncomplexity of convex minimization is only determined by the rate of growth of\nthe function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-like\nnoise condition. Specifically, we prove that if $f$ grows at least as fast as\n$\\|x-x^*_{f,S}\\|^\\kappa$ around its minimum, for some $\\kappa > 1$, then the\noptimal rate of learning $f(x^*_{f,S})$ is\n$\\Theta(T^{-\\frac{\\kappa}{2\\kappa-2}})$. The classic rate $\\Theta(1/\\sqrt T)$\nfor convex functions and $\\Theta(1/T)$ for strongly convex functions are\nspecial cases of our result for $\\kappa \\rightarrow \\infty$ and $\\kappa=2$, and\neven faster rates are attained for $\\kappa <2$. We also derive tight bounds for\nthe complexity of learning $x_{f,S}^*$, where the optimal rate is\n$\\Theta(T^{-\\frac{1}{2\\kappa-2}})$. Interestingly, these precise rates for\nconvex optimization also characterize the complexity of active learning and our\nresults further strengthen the connections between the two fields, both of\nwhich rely on feedback-driven queries. \n\n"}
{"id": "1207.6365", "contents": "Title: Low Rank Approximation and Regression in Input Sparsity Time Abstract: We design a new distribution over $\\poly(r \\eps^{-1}) \\times n$ matrices $S$\nso that for any fixed $n \\times d$ matrix $A$ of rank $r$, with probability at\nleast 9/10, $\\norm{SAx}_2 = (1 \\pm \\eps)\\norm{Ax}_2$ simultaneously for all $x\n\\in \\mathbb{R}^d$. Such a matrix $S$ is called a \\emph{subspace embedding}.\nFurthermore, $SA$ can be computed in $\\nnz(A) + \\poly(d \\eps^{-1})$ time, where\n$\\nnz(A)$ is the number of non-zero entries of $A$. This improves over all\nprevious subspace embeddings, which required at least $\\Omega(nd \\log d)$ time\nto achieve this property. We call our matrices $S$ \\emph{sparse embedding\nmatrices}.\n  Using our sparse embedding matrices, we obtain the fastest known algorithms\nfor $(1+\\eps)$-approximation for overconstrained least-squares regression,\nlow-rank approximation, approximating all leverage scores, and\n$\\ell_p$-regression. The leading order term in the time complexity of our\nalgorithms is $O(\\nnz(A))$ or $O(\\nnz(A)\\log n)$.\n  We optimize the low-order $\\poly(d/\\eps)$ terms in our running times (or for\nrank-$k$ approximation, the $n*\\poly(k/eps)$ term), and show various tradeoffs.\nFor instance, we also use our methods to design new preconditioners that\nimprove the dependence on $\\eps$ in least squares regression to $\\log 1/\\eps$.\nFinally, we provide preliminary experimental results which suggest that our\nalgorithms are competitive in practice. \n\n"}
{"id": "1208.0378", "contents": "Title: Fast Planar Correlation Clustering for Image Segmentation Abstract: We describe a new optimization scheme for finding high-quality correlation\nclusterings in planar graphs that uses weighted perfect matching as a\nsubroutine. Our method provides lower-bounds on the energy of the optimal\ncorrelation clustering that are typically fast to compute and tight in\npractice. We demonstrate our algorithm on the problem of image segmentation\nwhere this approach outperforms existing global optimization techniques in\nminimizing the objective and is competitive with the state of the art in\nproducing high-quality segmentations. \n\n"}
{"id": "1209.2139", "contents": "Title: Fused Multiple Graphical Lasso Abstract: In this paper, we consider the problem of estimating multiple graphical\nmodels simultaneously using the fused lasso penalty, which encourages adjacent\ngraphs to share similar structures. A motivating example is the analysis of\nbrain networks of Alzheimer's disease using neuroimaging data. Specifically, we\nmay wish to estimate a brain network for the normal controls (NC), a brain\nnetwork for the patients with mild cognitive impairment (MCI), and a brain\nnetwork for Alzheimer's patients (AD). We expect the two brain networks for NC\nand MCI to share common structures but not to be identical to each other;\nsimilarly for the two brain networks for MCI and AD. The proposed formulation\ncan be solved using a second-order method. Our key technical contribution is to\nestablish the necessary and sufficient condition for the graphs to be\ndecomposable. Based on this key property, a simple screening rule is presented,\nwhich decomposes the large graphs into small subgraphs and allows an efficient\nestimation of multiple independent (small) subgraphs, dramatically reducing the\ncomputational cost. We perform experiments on both synthetic and real data; our\nresults demonstrate the effectiveness and efficiency of the proposed approach. \n\n"}
{"id": "1211.0025", "contents": "Title: Venn-Abers predictors Abstract: This paper continues study, both theoretical and empirical, of the method of\nVenn prediction, concentrating on binary prediction problems. Venn predictors\nproduce probability-type predictions for the labels of test objects which are\nguaranteed to be well calibrated under the standard assumption that the\nobservations are generated independently from the same distribution. We give a\nsimple formalization and proof of this property. We also introduce Venn-Abers\npredictors, a new class of Venn predictors based on the idea of isotonic\nregression, and report promising empirical results both for Venn-Abers\npredictors and for their more computationally efficient simplified version. \n\n"}
{"id": "1211.2304", "contents": "Title: Probabilistic Combination of Classifier and Cluster Ensembles for\n  Non-transductive Learning Abstract: Unsupervised models can provide supplementary soft constraints to help\nclassify new target data under the assumption that similar objects in the\ntarget set are more likely to share the same class label. Such models can also\nhelp detect possible differences between training and target distributions,\nwhich is useful in applications where concept drift may take place. This paper\ndescribes a Bayesian framework that takes as input class labels from existing\nclassifiers (designed based on labeled data from the source domain), as well as\ncluster labels from a cluster ensemble operating solely on the target data to\nbe classified, and yields a consensus labeling of the target data. This\nframework is particularly useful when the statistics of the target data drift\nor change from those of the training data. We also show that the proposed\nframework is privacy-aware and allows performing distributed learning when\ndata/models have sharing restrictions. Experiments show that our framework can\nyield superior results to those provided by applying classifier ensembles only. \n\n"}
{"id": "1211.6085", "contents": "Title: Random Projections for Linear Support Vector Machines Abstract: Let X be a data matrix of rank \\rho, whose rows represent n points in\nd-dimensional space. The linear support vector machine constructs a hyperplane\nseparator that maximizes the 1-norm soft margin. We develop a new oblivious\ndimension reduction technique which is precomputed and can be applied to any\ninput matrix X. We prove that, with high probability, the margin and minimum\nenclosing ball in the feature space are preserved to within \\epsilon-relative\nerror, ensuring comparable generalization as in the original space in the case\nof classification. For regression, we show that the margin is preserved to\n\\epsilon-relative error with high probability. We present extensive experiments\nwith real and synthetic data to support our theory. \n\n"}
{"id": "1212.5637", "contents": "Title: Random Spanning Trees and the Prediction of Weighted Graphs Abstract: We investigate the problem of sequentially predicting the binary labels on\nthe nodes of an arbitrary weighted graph. We show that, under a suitable\nparametrization of the problem, the optimal number of prediction mistakes can\nbe characterized (up to logarithmic factors) by the cutsize of a random\nspanning tree of the graph. The cutsize is induced by the unknown adversarial\nlabeling of the graph nodes. In deriving our characterization, we obtain a\nsimple randomized algorithm achieving in expectation the optimal mistake bound\non any polynomially connected weighted graph. Our algorithm draws a random\nspanning tree of the original graph and then predicts the nodes of this tree in\nconstant expected amortized time and linear space. Experiments on real-world\ndatasets show that our method compares well to both global (Perceptron) and\nlocal (label propagation) methods, while being generally faster in practice. \n\n"}
{"id": "1302.0315", "contents": "Title: Sparse Multiple Kernel Learning with Geometric Convergence Rate Abstract: In this paper, we study the problem of sparse multiple kernel learning (MKL),\nwhere the goal is to efficiently learn a combination of a fixed small number of\nkernels from a large pool that could lead to a kernel classifier with a small\nprediction error. We develop an efficient algorithm based on the greedy\ncoordinate descent algorithm, that is able to achieve a geometric convergence\nrate under appropriate conditions. The convergence rate is achieved by\nmeasuring the size of functional gradients by an empirical $\\ell_2$ norm that\ndepends on the empirical data distribution. This is in contrast to previous\nalgorithms that use a functional norm to measure the size of gradients, which\nis independent from the data samples. We also establish a generalization error\nbound of the learned sparse kernel classifier using the technique of local\nRademacher complexity. \n\n"}
{"id": "1302.1611", "contents": "Title: Bounded regret in stochastic multi-armed bandits Abstract: We study the stochastic multi-armed bandit problem when one knows the value\n$\\mu^{(\\star)}$ of an optimal arm, as a well as a positive lower bound on the\nsmallest positive gap $\\Delta$. We propose a new randomized policy that attains\na regret {\\em uniformly bounded over time} in this setting. We also prove\nseveral lower bounds, which show in particular that bounded regret is not\npossible if one only knows $\\Delta$, and bounded regret of order $1/\\Delta$ is\nnot possible if one only knows $\\mu^{(\\star)}$ \n\n"}
{"id": "1302.2576", "contents": "Title: The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking Abstract: We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models. \n\n"}
{"id": "1302.2671", "contents": "Title: Latent Self-Exciting Point Process Model for Spatial-Temporal Networks Abstract: We propose a latent self-exciting point process model that describes\ngeographically distributed interactions between pairs of entities. In contrast\nto most existing approaches that assume fully observable interactions, here we\nconsider a scenario where certain interaction events lack information about\nparticipants. Instead, this information needs to be inferred from the available\nobservations. We develop an efficient approximate algorithm based on\nvariational expectation-maximization to infer unknown participants in an event\ngiven the location and the time of the event. We validate the model on\nsynthetic as well as real-world data, and obtain very promising results on the\nidentity-inference task. We also use our model to predict the timing and\nparticipants of future events, and demonstrate that it compares favorably with\nbaseline approaches. \n\n"}
{"id": "1302.2684", "contents": "Title: A Tensor Approach to Learning Mixed Membership Community Models Abstract: Community detection is the task of detecting hidden communities from observed\ninteractions. Guaranteed community detection has so far been mostly limited to\nmodels with non-overlapping communities such as the stochastic block model. In\nthis paper, we remove this restriction, and provide guaranteed community\ndetection for a family of probabilistic network models with overlapping\ncommunities, termed as the mixed membership Dirichlet model, first introduced\nby Airoldi et al. This model allows for nodes to have fractional memberships in\nmultiple communities and assumes that the community memberships are drawn from\na Dirichlet distribution. Moreover, it contains the stochastic block model as a\nspecial case. We propose a unified approach to learning these models via a\ntensor spectral decomposition method. Our estimator is based on low-order\nmoment tensor of the observed network, consisting of 3-star counts. Our\nlearning method is fast and is based on simple linear algebraic operations,\ne.g. singular value decomposition and tensor power iterations. We provide\nguaranteed recovery of community memberships and model parameters and present a\ncareful finite sample analysis of our learning method. As an important special\ncase, our results match the best known scaling requirements for the\n(homogeneous) stochastic block model. \n\n"}
{"id": "1302.4549", "contents": "Title: Breaking the Small Cluster Barrier of Graph Clustering Abstract: This paper investigates graph clustering in the planted cluster model in the\npresence of {\\em small clusters}. Traditional results dictate that for an\nalgorithm to provably correctly recover the clusters, {\\em all} clusters must\nbe sufficiently large (in particular, $\\tilde{\\Omega}(\\sqrt{n})$ where $n$ is\nthe number of nodes of the graph). We show that this is not really a\nrestriction: by a more refined analysis of the trace-norm based recovery\napproach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that\nsmall clusters, under certain mild assumptions, do not hinder recovery of large\nones.\n  Based on this result, we further devise an iterative algorithm to recover\n{\\em almost all clusters} via a \"peeling strategy\", i.e., recover large\nclusters first, leading to a reduced problem, and repeat this procedure. These\nresults are extended to the {\\em partial observation} setting, in which only a\n(chosen) part of the graph is observed.The peeling strategy gives rise to an\nactive learning algorithm, in which edges adjacent to smaller clusters are\nqueried more often as large clusters are learned (and removed).\n  From a high level, this paper sheds novel insights on high-dimensional\nstatistics and learning structured data, by presenting a structured matrix\nlearning problem for which a one shot convex relaxation approach necessarily\nfails, but a carefully constructed sequence of convex relaxationsdoes the job. \n\n"}
{"id": "1302.5729", "contents": "Title: Sparse Signal Estimation by Maximally Sparse Convex Optimization Abstract: This paper addresses the problem of sparsity penalized least squares for\napplications in sparse signal processing, e.g. sparse deconvolution. This paper\naims to induce sparsity more strongly than L1 norm regularization, while\navoiding non-convex optimization. For this purpose, this paper describes the\ndesign and use of non-convex penalty functions (regularizers) constrained so as\nto ensure the convexity of the total cost function, F, to be minimized. The\nmethod is based on parametric penalty functions, the parameters of which are\nconstrained to ensure convexity of F. It is shown that optimal parameters can\nbe obtained by semidefinite programming (SDP). This maximally sparse convex\n(MSC) approach yields maximally non-convex sparsity-inducing penalty functions\nconstrained such that the total cost function, F, is convex. It is demonstrated\nthat iterative MSC (IMSC) can yield solutions substantially more sparse than\nthe standard convex sparsity-inducing approach, i.e., L1 norm minimization. \n\n"}
{"id": "1304.5504", "contents": "Title: Optimal Stochastic Strongly Convex Optimization with a Logarithmic\n  Number of Projections Abstract: We consider stochastic strongly convex optimization with a complex inequality\nconstraint. This complex inequality constraint may lead to computationally\nexpensive projections in algorithmic iterations of the stochastic gradient\ndescent~(SGD) methods. To reduce the computation costs pertaining to the\nprojections, we propose an Epoch-Projection Stochastic Gradient\nDescent~(Epro-SGD) method. The proposed Epro-SGD method consists of a sequence\nof epochs; it applies SGD to an augmented objective function at each iteration\nwithin the epoch, and then performs a projection at the end of each epoch.\nGiven a strongly convex optimization and for a total number of $T$ iterations,\nEpro-SGD requires only $\\log(T)$ projections, and meanwhile attains an optimal\nconvergence rate of $O(1/T)$, both in expectation and with a high probability.\nTo exploit the structure of the optimization problem, we propose a proximal\nvariant of Epro-SGD, namely Epro-ORDA, based on the optimal regularized dual\naveraging method. We apply the proposed methods on real-world applications; the\nempirical results demonstrate the effectiveness of our methods. \n\n"}
{"id": "1304.6487", "contents": "Title: Locally linear representation for image clustering Abstract: It is a key to construct a similarity graph in graph-oriented subspace\nlearning and clustering. In a similarity graph, each vertex denotes a data\npoint and the edge weight represents the similarity between two points. There\nare two popular schemes to construct a similarity graph, i.e., pairwise\ndistance based scheme and linear representation based scheme. Most existing\nworks have only involved one of the above schemes and suffered from some\nlimitations. Specifically, pairwise distance based methods are sensitive to the\nnoises and outliers compared with linear representation based methods. On the\nother hand, there is the possibility that linear representation based\nalgorithms wrongly select inter-subspaces points to represent a point, which\nwill degrade the performance. In this paper, we propose an algorithm, called\nLocally Linear Representation (LLR), which integrates pairwise distance with\nlinear representation together to address the problems. The proposed algorithm\ncan automatically encode each data point over a set of points that not only\ncould denote the objective point with less residual error, but also are close\nto the point in Euclidean space. The experimental results show that our\napproach is promising in subspace learning and subspace clustering. \n\n"}
{"id": "1305.4324", "contents": "Title: Horizon-Independent Optimal Prediction with Log-Loss in Exponential\n  Families Abstract: We study online learning under logarithmic loss with regular parametric\nmodels. Hedayati and Bartlett (2012b) showed that a Bayesian prediction\nstrategy with Jeffreys prior and sequential normalized maximum likelihood\n(SNML) coincide and are optimal if and only if the latter is exchangeable, and\nif and only if the optimal strategy can be calculated without knowing the time\nhorizon in advance. They put forward the question what families have\nexchangeable SNML strategies. This paper fully answers this open problem for\none-dimensional exponential families. The exchangeability can happen only for\nthree classes of natural exponential family distributions, namely the Gaussian,\nGamma, and the Tweedie exponential family of order 3/2. Keywords: SNML\nExchangeability, Exponential Family, Online Learning, Logarithmic Loss,\nBayesian Strategy, Jeffreys Prior, Fisher Information1 \n\n"}
{"id": "1308.6273", "contents": "Title: New Algorithms for Learning Incoherent and Overcomplete Dictionaries Abstract: In sparse recovery we are given a matrix $A$ (the dictionary) and a vector of\nthe form $A X$ where $X$ is sparse, and the goal is to recover $X$. This is a\ncentral notion in signal processing, statistics and machine learning. But in\napplications such as sparse coding, edge detection, compression and super\nresolution, the dictionary $A$ is unknown and has to be learned from random\nexamples of the form $Y = AX$ where $X$ is drawn from an appropriate\ndistribution --- this is the dictionary learning problem. In most settings, $A$\nis overcomplete: it has more columns than rows. This paper presents a\npolynomial-time algorithm for learning overcomplete dictionaries; the only\npreviously known algorithm with provable guarantees is the recent work of\nSpielman, Wang and Wright who gave an algorithm for the full-rank case, which\nis rarely the case in applications. Our algorithm applies to incoherent\ndictionaries which have been a central object of study since they were\nintroduced in seminal work of Donoho and Huo. In particular, a dictionary is\n$\\mu$-incoherent if each pair of columns has inner product at most $\\mu /\n\\sqrt{n}$.\n  The algorithm makes natural stochastic assumptions about the unknown sparse\nvector $X$, which can contain $k \\leq c \\min(\\sqrt{n}/\\mu \\log n, m^{1/2\n-\\eta})$ non-zero entries (for any $\\eta > 0$). This is close to the best $k$\nallowable by the best sparse recovery algorithms even if one knows the\ndictionary $A$ exactly. Moreover, both the running time and sample complexity\ndepend on $\\log 1/\\epsilon$, where $\\epsilon$ is the target accuracy, and so\nour algorithms converge very quickly to the true dictionary. Our algorithm can\nalso tolerate substantial amounts of noise provided it is incoherent with\nrespect to the dictionary (e.g., Gaussian). In the noisy setting, our running\ntime and sample complexity depend polynomially on $1/\\epsilon$, and this is\nnecessary. \n\n"}
{"id": "1309.0242", "contents": "Title: Ensemble approaches for improving community detection methods Abstract: Statistical estimates can often be improved by fusion of data from several\ndifferent sources. One example is so-called ensemble methods which have been\nsuccessfully applied in areas such as machine learning for classification and\nclustering. In this paper, we present an ensemble method to improve community\ndetection by aggregating the information found in an ensemble of community\nstructures. This ensemble can found by re-sampling methods, multiple runs of a\nstochastic community detection method, or by several different community\ndetection algorithms applied to the same network. The proposed method is\nevaluated using random networks with community structures and compared with two\ncommonly used community detection methods. The proposed method when applied on\na stochastic community detection algorithm performs well with low computational\ncomplexity, thus offering both a new approach to community detection and an\nadditional community detection method. \n\n"}
{"id": "1309.3533", "contents": "Title: Mixed Membership Models for Time Series Abstract: In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework. \n\n"}
{"id": "1309.6707", "contents": "Title: Distributed Online Learning in Social Recommender Systems Abstract: In this paper, we consider decentralized sequential decision making in\ndistributed online recommender systems, where items are recommended to users\nbased on their search query as well as their specific background including\nhistory of bought items, gender and age, all of which comprise the context\ninformation of the user. In contrast to centralized recommender systems, in\nwhich there is a single centralized seller who has access to the complete\ninventory of items as well as the complete record of sales and user\ninformation, in decentralized recommender systems each seller/learner only has\naccess to the inventory of items and user information for its own products and\nnot the products and user information of other sellers, but can get commission\nif it sells an item of another seller. Therefore the sellers must distributedly\nfind out for an incoming user which items to recommend (from the set of own\nitems or items of another seller), in order to maximize the revenue from own\nsales and commissions. We formulate this problem as a cooperative contextual\nbandit problem, analytically bound the performance of the sellers compared to\nthe best recommendation strategy given the complete realization of user\narrivals and the inventory of items, as well as the context-dependent purchase\nprobabilities of each item, and verify our results via numerical examples on a\ndistributed data set adapted based on Amazon data. We evaluate the dependence\nof the performance of a seller on the inventory of items the seller has, the\nnumber of connections it has with the other sellers, and the commissions which\nthe seller gets by selling items of other sellers to its users. \n\n"}
{"id": "1310.0432", "contents": "Title: Online Learning of Dynamic Parameters in Social Networks Abstract: This paper addresses the problem of online learning in a dynamic setting. We\nconsider a social network in which each individual observes a private signal\nabout the underlying state of the world and communicates with her neighbors at\neach time period. Unlike many existing approaches, the underlying state is\ndynamic, and evolves according to a geometric random walk. We view the scenario\nas an optimization problem where agents aim to learn the true state while\nsuffering the smallest possible loss. Based on the decomposition of the global\nloss function, we introduce two update mechanisms, each of which generates an\nestimate of the true state. We establish a tight bound on the rate of change of\nthe underlying state, under which individuals can track the parameter with a\nbounded variance. Then, we characterize explicit expressions for the steady\nstate mean-square deviation(MSD) of the estimates from the truth, per\nindividual. We observe that only one of the estimators recovers the optimal\nMSD, which underscores the impact of the objective function decomposition on\nthe learning quality. Finally, we provide an upper bound on the regret of the\nproposed methods, measured as an average of errors in estimating the parameter\nin a finite time. \n\n"}
{"id": "1310.1502", "contents": "Title: Randomized Approximation of the Gram Matrix: Exact Computation and\n  Probabilistic Bounds Abstract: Given a real matrix A with n columns, the problem is to approximate the Gram\nproduct AA^T by c << n weighted outer products of columns of A. Necessary and\nsufficient conditions for the exact computation of AA^T (in exact arithmetic)\nfrom c >= rank(A) columns depend on the right singular vector matrix of A. For\na Monte-Carlo matrix multiplication algorithm by Drineas et al. that samples\nouter products, we present probabilistic bounds for the 2-norm relative error\ndue to randomization. The bounds depend on the stable rank or the rank of A,\nbut not on the matrix dimensions. Numerical experiments illustrate that the\nbounds are informative, even for stringent success probabilities and matrices\nof small dimension. We also derive bounds for the smallest singular value and\nthe condition number of matrices obtained by sampling rows from orthonormal\nmatrices. \n\n"}
{"id": "1310.1949", "contents": "Title: Least Squares Revisited: Scalable Approaches for Multi-class Prediction Abstract: This work provides simple algorithms for multi-class (and multi-label)\nprediction in settings where both the number of examples n and the data\ndimension d are relatively large. These robust and parameter free algorithms\nare essentially iterative least-squares updates and very versatile both in\ntheory and in practice. On the theoretical front, we present several variants\nwith convergence guarantees. Owing to their effective use of second-order\nstructure, these algorithms are substantially better than first-order methods\nin many practical scenarios. On the empirical side, we present a scalable\nstagewise variant of our approach, which achieves dramatic computational\nspeedups over popular optimization packages such as Liblinear and Vowpal Wabbit\non standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art\naccuracies. \n\n"}
{"id": "1310.8243", "contents": "Title: Para-active learning Abstract: Training examples are not all equally informative. Active learning strategies\nleverage this observation in order to massively reduce the number of examples\nthat need to be labeled. We leverage the same observation to build a generic\nstrategy for parallelizing learning algorithms. This strategy is effective\nbecause the search for informative examples is highly parallelizable and\nbecause we show that its performance does not deteriorate when the sifting\nprocess relies on a slightly outdated model. Parallel active learning is\nparticularly attractive to train nonlinear models with non-linear\nrepresentations because there are few practical parallel learning algorithms\nfor such models. We report preliminary experiments using both kernel SVMs and\nSGD-trained neural networks. \n\n"}
{"id": "1311.2547", "contents": "Title: Learning Mixtures of Linear Classifiers Abstract: We consider a discriminative learning (regression) problem, whereby the\nregression function is a convex combination of k linear classifiers. Existing\napproaches are based on the EM algorithm, or similar techniques, without\nprovable guarantees. We develop a simple method based on spectral techniques\nand a `mirroring' trick, that discovers the subspace spanned by the\nclassifiers' parameter vectors. Under a probabilistic assumption on the feature\nvector distribution, we prove that this approach has nearly optimal statistical\nefficiency. \n\n"}
{"id": "1311.2889", "contents": "Title: Reinforcement Learning for Matrix Computations: PageRank as an Example Abstract: Reinforcement learning has gained wide popularity as a technique for\nsimulation-driven approximate dynamic programming. A less known aspect is that\nthe very reasons that make it effective in dynamic programming can also be\nleveraged for using it for distributed schemes for certain matrix computations\ninvolving non-negative matrices. In this spirit, we propose a reinforcement\nlearning algorithm for PageRank computation that is fashioned after analogous\nschemes for approximate dynamic programming. The algorithm has the advantage of\nease of distributed implementation and more importantly, of being model-free,\ni.e., not dependent on any specific assumptions about the transition\nprobabilities in the random web-surfer model. We analyze its convergence and\nfinite time behavior and present some supporting numerical experiments. \n\n"}
{"id": "1311.3651", "contents": "Title: Smoothed Analysis of Tensor Decompositions Abstract: Low rank tensor decompositions are a powerful tool for learning generative\nmodels, and uniqueness results give them a significant advantage over matrix\ndecomposition methods. However, tensors pose significant algorithmic challenges\nand tensors analogs of much of the matrix algebra toolkit are unlikely to exist\nbecause of hardness results. Efficient decomposition in the overcomplete case\n(where rank exceeds dimension) is particularly challenging. We introduce a\nsmoothed analysis model for studying these questions and develop an efficient\nalgorithm for tensor decomposition in the highly overcomplete case (rank\npolynomial in the dimension). In this setting, we show that our algorithm is\nrobust to inverse polynomial error -- a crucial property for applications in\nlearning since we are only allowed a polynomial number of samples. While\nalgorithms are known for exact tensor decomposition in some overcomplete\nsettings, our main contribution is in analyzing their stability in the\nframework of smoothed analysis.\n  Our main technical contribution is to show that tensor products of perturbed\nvectors are linearly independent in a robust sense (i.e. the associated matrix\nhas singular values that are at least an inverse polynomial). This key result\npaves the way for applying tensor methods to learning problems in the smoothed\nsetting. In particular, we use it to obtain results for learning multi-view\nmodels and mixtures of axis-aligned Gaussians where there are many more\n\"components\" than dimensions. The assumption here is that the model is not\nadversarially chosen, formalized by a perturbation of model parameters. We\nbelieve this an appealing way to analyze realistic instances of learning\nproblems, since this framework allows us to overcome many of the usual\nlimitations of using tensor methods. \n\n"}
{"id": "1312.2164", "contents": "Title: Budgeted Influence Maximization for Multiple Products Abstract: The typical algorithmic problem in viral marketing aims to identify a set of\ninfluential users in a social network, who, when convinced to adopt a product,\nshall influence other users in the network and trigger a large cascade of\nadoptions. However, the host (the owner of an online social platform) often\nfaces more constraints than a single product, endless user attentions,\nunlimited budget and unbounded time; in reality, multiple products need to be\nadvertised, each user can tolerate only a small number of recommendations,\ninfluencing user has a cost and advertisers have only limited budgets, and the\nadoptions need to be maximized within a short time window.\n  Given theses myriads of user, monetary, and timing constraints, it is\nextremely challenging for the host to design principled and efficient viral\nmarket algorithms with provable guarantees. In this paper, we provide a novel\nsolution by formulating the problem as a submodular maximization in a\ncontinuous-time diffusion model under an intersection of a matroid and multiple\nknapsack constraints. We also propose an adaptive threshold greedy algorithm\nwhich can be faster than the traditional greedy algorithm with lazy evaluation,\nand scalable to networks with million of nodes. Furthermore, our mathematical\nformulation allows us to prove that the algorithm can achieve an approximation\nfactor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints are\nactive, which also improves over previous guarantees from combinatorial\noptimization literature. In the case when influencing each user has uniform\ncost, the approximation becomes even better to a factor of $1/3$. Extensive\nsynthetic and real world experiments demonstrate that our budgeted influence\nmaximization algorithm achieves the-state-of-the-art in terms of both\neffectiveness and scalability, often beating the next best by significant\nmargins. \n\n"}
{"id": "1312.5604", "contents": "Title: Learning Transformations for Classification Forests Abstract: This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework. \n\n"}
{"id": "1312.5847", "contents": "Title: Deep learning for neuroimaging: a validation study Abstract: Deep learning methods have recently made notable advances in the tasks of\nclassification and representation learning. These tasks are important for brain\nimaging and neuroscience discovery, making the methods attractive for porting\nto a neuroimager's toolbox. Success of these methods is, in part, explained by\nthe flexibility of deep learning models. However, this flexibility makes the\nprocess of porting to new areas a difficult parameter optimization problem. In\nthis work we demonstrate our results (and feasible parameter ranges) in\napplication of deep learning methods to structural and functional brain imaging\ndata. We also describe a novel constraint-based approach to visualizing high\ndimensional data. We use it to analyze the effect of parameter choices on data\ntransformations. Our results show that deep learning methods are able to learn\nphysiologically important representations and detect latent relations in\nneuroimaging data. \n\n"}
{"id": "1401.0304", "contents": "Title: Learning without Concentration Abstract: We obtain sharp bounds on the performance of Empirical Risk Minimization\nperformed in a convex class and with respect to the squared loss, without\nassuming that class members and the target are bounded functions or have\nrapidly decaying tails.\n  Rather than resorting to a concentration-based argument, the method used here\nrelies on a `small-ball' assumption and thus holds for classes consisting of\nheavy-tailed functions and for heavy-tailed targets.\n  The resulting estimates scale correctly with the `noise level' of the\nproblem, and when applied to the classical, bounded scenario, always improve\nthe known bounds. \n\n"}
{"id": "1401.0579", "contents": "Title: More Algorithms for Provable Dictionary Learning Abstract: In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration. \n\n"}
{"id": "1401.3409", "contents": "Title: Low-Rank Modeling and Its Applications in Image Analysis Abstract: Low-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions. \n\n"}
{"id": "1402.3722", "contents": "Title: word2vec Explained: deriving Mikolov et al.'s negative-sampling\n  word-embedding method Abstract: The word2vec software of Tomas Mikolov and colleagues\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\nprovides state-of-the-art word embeddings. The learning models behind the\nsoftware are described in two research papers. We found the description of the\nmodels in these papers to be somewhat cryptic and hard to follow. While the\nmotivations and presentation may be obvious to the neural-networks\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\nrationale behind the equations.\n  This note is an attempt to explain equation (4) (negative sampling) in\n\"Distributed Representations of Words and Phrases and their Compositionality\"\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean. \n\n"}
{"id": "1402.4512", "contents": "Title: Classification with Sparse Overlapping Groups Abstract: Classification with a sparsity constraint on the solution plays a central\nrole in many high dimensional machine learning applications. In some cases, the\nfeatures can be grouped together so that entire subsets of features can be\nselected or not selected. In many applications, however, this can be too\nrestrictive. In this paper, we are interested in a less restrictive form of\nstructured sparse feature selection: we assume that while features can be\ngrouped according to some notion of similarity, not all features in a group\nneed be selected for the task at hand. When the groups are comprised of\ndisjoint sets of features, this is sometimes referred to as the \"sparse group\"\nlasso, and it allows for working with a richer class of models than traditional\ngroup lasso methods. Our framework generalizes conventional sparse group lasso\nfurther by allowing for overlapping groups, an additional flexiblity needed in\nmany applications and one that presents further challenges. The main\ncontribution of this paper is a new procedure called Sparse Overlapping Group\n(SOG) lasso, a convex optimization program that automatically selects similar\nfeatures for classification in high dimensions. We establish model selection\nerror bounds for SOGlasso classification problems under a fairly general\nsetting. In particular, the error bounds are the first such results for\nclassification using the sparse group lasso. Furthermore, the general SOGlasso\nbound specializes to results for the lasso and the group lasso, some known and\nsome new. The SOGlasso is motivated by multi-subject fMRI studies in which\nfunctional activity is classified using brain voxels as features, source\nlocalization problems in Magnetoencephalography (MEG), and analyzing gene\nactivation patterns in microarray data analysis. Experiments with real and\nsynthetic data demonstrate the advantages of SOGlasso compared to the lasso and\ngroup lasso. \n\n"}
{"id": "1402.5728", "contents": "Title: Machine Learning Methods in the Computational Biology of Cancer Abstract: The objectives of this \"perspective\" paper are to review some recent advances\nin sparse feature selection for regression and classification, as well as\ncompressed sensing, and to discuss how these might be used to develop tools to\nadvance personalized cancer therapy. As an illustration of the possibilities, a\nnew algorithm for sparse regression is presented, and is applied to predict the\ntime to tumor recurrence in ovarian cancer. A new algorithm for sparse feature\nselection in classification problems is presented, and its validation in\nendometrial cancer is briefly discussed. Some open problems are also presented. \n\n"}
{"id": "1404.0400", "contents": "Title: A Deep Representation for Invariance And Music Classification Abstract: Representations in the auditory cortex might be based on mechanisms similar\nto the visual ventral stream; modules for building invariance to\ntransformations and multiple layers for compositionality and selectivity. In\nthis paper we propose the use of such computational modules for extracting\ninvariant and discriminative audio representations. Building on a theory of\ninvariance in hierarchical architectures, we propose a novel, mid-level\nrepresentation for acoustical signals, using the empirical distributions of\nprojections on a set of templates and their transformations. Under the\nassumption that, by construction, this dictionary of templates is composed from\nsimilar classes, and samples the orbit of variance-inducing signal\ntransformations (such as shift and scale), the resulting signature is\ntheoretically guaranteed to be unique, invariant to transformations and stable\nto deformations. Modules of projection and pooling can then constitute layers\nof deep networks, for learning composite representations. We present the main\ntheoretical and computational aspects of a framework for unsupervised learning\nof invariant audio representations, empirically evaluated on music genre\nclassification. \n\n"}
{"id": "1404.1504", "contents": "Title: A Compression Technique for Analyzing Disagreement-Based Active Learning Abstract: We introduce a new and improved characterization of the label complexity of\ndisagreement-based active learning, in which the leading quantity is the\nversion space compression set size. This quantity is defined as the size of the\nsmallest subset of the training data that induces the same version space. We\nshow various applications of the new characterization, including a tight\nanalysis of CAL and refined label complexity bounds for linear separators under\nmixtures of Gaussians and axis-aligned rectangles under product densities. The\nversion space compression set size, as well as the new characterization of the\nlabel complexity, can be naturally extended to agnostic learning problems, for\nwhich we show new speedup results for two well known active learning\nalgorithms. \n\n"}
{"id": "1405.3726", "contents": "Title: Topic words analysis based on LDA model Abstract: Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected. \n\n"}
{"id": "1405.6159", "contents": "Title: A Bi-clustering Framework for Consensus Problems Abstract: We consider grouping as a general characterization for problems such as\nclustering, community detection in networks, and multiple parametric model\nestimation. We are interested in merging solutions from different grouping\nalgorithms, distilling all their good qualities into a consensus solution. In\nthis paper, we propose a bi-clustering framework and perspective for reaching\nconsensus in such grouping problems. In particular, this is the first time that\nthe task of finding/fitting multiple parametric models to a dataset is formally\nposed as a consensus problem. We highlight the equivalence of these tasks and\nestablish the connection with the computational Gestalt program, that seeks to\nprovide a psychologically-inspired detection theory for visual events. We also\npresent a simple but powerful bi-clustering algorithm, specially tuned to the\nnature of the problem we address, though general enough to handle many\ndifferent instances inscribed within our characterization. The presentation is\naccompanied with diverse and extensive experimental results in clustering,\ncommunity detection, and multiple parametric model estimation in image\nprocessing applications. \n\n"}
{"id": "1406.4566", "contents": "Title: Guaranteed Scalable Learning of Latent Tree Models Abstract: We present an integrated approach for structure and parameter estimation in\nlatent tree graphical models. Our overall approach follows a\n\"divide-and-conquer\" strategy that learns models over small groups of variables\nand iteratively merges onto a global solution. The structure learning involves\ncombinatorial operations such as minimum spanning tree construction and local\nrecursive grouping; the parameter learning is based on the method of moments\nand on tensor decompositions. Our method is guaranteed to correctly recover the\nunknown tree structure and the model parameters with low sample complexity for\nthe class of linear multivariate latent tree models which includes discrete and\nGaussian distributions, and Gaussian mixtures. Our bulk asynchronous parallel\nalgorithm is implemented in parallel and the parallel computation complexity\nincreases only logarithmically with the number of variables and linearly with\ndimensionality of each variable. \n\n"}
{"id": "1406.5298", "contents": "Title: Semi-Supervised Learning with Deep Generative Models Abstract: The ever-increasing size of modern data sets combined with the difficulty of\nobtaining label information has made semi-supervised learning one of the\nproblems of significant practical importance in modern data analysis. We\nrevisit the approach to semi-supervised learning with generative models and\ndevelop new models that allow for effective generalisation from small labelled\ndata sets to large unlabelled ones. Generative approaches have thus far been\neither inflexible, inefficient or non-scalable. We show that deep generative\nmodels and approximate Bayesian inference exploiting recent advances in\nvariational methods can be used to provide significant improvements, making\ngenerative approaches highly competitive for semi-supervised learning. \n\n"}
{"id": "1406.5647", "contents": "Title: On semidefinite relaxations for the block model Abstract: The stochastic block model (SBM) is a popular tool for community detection in\nnetworks, but fitting it by maximum likelihood (MLE) involves a computationally\ninfeasible optimization problem. We propose a new semidefinite programming\n(SDP) solution to the problem of fitting the SBM, derived as a relaxation of\nthe MLE. We put ours and previously proposed SDPs in a unified framework, as\nrelaxations of the MLE over various sub-classes of the SBM, revealing a\nconnection to sparse PCA. Our main relaxation, which we call SDP-1, is tighter\nthan other recently proposed SDP relaxations, and thus previously established\ntheoretical guarantees carry over. However, we show that SDP-1 exactly recovers\ntrue communities over a wider class of SBMs than those covered by current\nresults. In particular, the assumption of strong assortativity of the SBM,\nimplicit in consistency conditions for previously proposed SDPs, can be relaxed\nto weak assortativity for our approach, thus significantly broadening the class\nof SBMs covered by the consistency results. We also show that strong\nassortativity is indeed a necessary condition for exact recovery for previously\nproposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs\nis based on primal-dual witness constructions, which provides some insight into\nthe nature of the solutions of various SDPs. We show how to combine features\nfrom SDP-1 and already available SDPs to achieve the most flexibility in terms\nof both assortativity and block-size constraints, as our relaxation has the\ntendency to produce communities of similar sizes. This tendency makes it the\nideal tool for fitting network histograms, a method gaining popularity in the\ngraphon estimation literature, as we illustrate on an example of a social\nnetworks of dolphins. We also provide empirical evidence that SDPs outperform\nspectral methods for fitting SBMs with a large number of blocks. \n\n"}
{"id": "1406.6603", "contents": "Title: A scaled gradient projection method for Bayesian learning in dynamical\n  systems Abstract: A crucial task in system identification problems is the selection of the most\nappropriate model class, and is classically addressed resorting to\ncross-validation or using asymptotic arguments. As recently suggested in the\nliterature, this can be addressed in a Bayesian framework, where model\ncomplexity is regulated by few hyperparameters, which can be estimated via\nmarginal likelihood maximization. It is thus of primary importance to design\neffective optimization methods to solve the corresponding optimization problem.\nIf the unknown impulse response is modeled as a Gaussian process with a\nsuitable kernel, the maximization of the marginal likelihood leads to a\nchallenging nonconvex optimization problem, which requires a stable and\neffective solution strategy. In this paper we address this problem by means of\na scaled gradient projection algorithm, in which the scaling matrix and the\nsteplength parameter play a crucial role to provide a meaning solution in a\ncomputational time comparable with second order methods. In particular, we\npropose both a generalization of the split gradient approach to design the\nscaling matrix in the presence of box constraints, and an effective\nimplementation of the gradient and objective function. The extensive numerical\nexperiments carried out on several test problems show that our method is very\neffective in providing in few tenths of a second solutions of the problems with\naccuracy comparable with state-of-the-art approaches. Moreover, the flexibility\nof the proposed strategy makes it easily adaptable to a wider range of problems\narising in different areas of machine learning, signal processing and system\nidentification. \n\n"}
{"id": "1407.1543", "contents": "Title: Dictionary Learning and Tensor Decomposition via the Sum-of-Squares\n  Method Abstract: We give a new approach to the dictionary learning (also known as \"sparse\ncoding\") problem of recovering an unknown $n\\times m$ matrix $A$ (for $m \\geq\nn$) from examples of the form \\[ y = Ax + e, \\] where $x$ is a random vector in\n$\\mathbb R^m$ with at most $\\tau m$ nonzero coordinates, and $e$ is a random\nnoise vector in $\\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,\nour algorithm recovers every column of $A$ within arbitrarily good constant\naccuracy in time $m^{O(\\log m/\\log(\\tau^{-1}))}$, in particular achieving\npolynomial time if $\\tau = m^{-\\delta}$ for any $\\delta>0$, and time $m^{O(\\log\nm)}$ if $\\tau$ is (a sufficiently small) constant. Prior algorithms with\ncomparable assumptions on the distribution required the vector $x$ to be much\nsparser---at most $\\sqrt{n}$ nonzero coordinates---and there were intrinsic\nbarriers preventing these algorithms from applying for denser $x$.\n  We achieve this by designing an algorithm for noisy tensor decomposition that\ncan recover, under quite general conditions, an approximate rank-one\ndecomposition of a tensor $T$, given access to a tensor $T'$ that is\n$\\tau$-close to $T$ in the spectral norm (when considered as a matrix). To our\nknowledge, this is the first algorithm for tensor decomposition that works in\nthe constant spectral-norm noise regime, where there is no guarantee that the\nlocal optima of $T$ and $T'$ have similar structures.\n  Our algorithm is based on a novel approach to using and analyzing the Sum of\nSquares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), and\nit can be viewed as an indication of the utility of this very general and\npowerful tool for unsupervised learning problems. \n\n"}
{"id": "1407.4416", "contents": "Title: In Defense of MinHash Over SimHash Abstract: MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n($\\mathcal{R}$), while the collision probability of SimHash is a function of\ncosine similarity ($\\mathcal{S}$). To provide a common basis for comparison, we\nevaluate retrieval results in terms of $\\mathcal{S}$ for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to $\\mathcal{S}$, by using a general inequality $\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}$. Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often $\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}$ holds where $z$\nis only slightly larger than 2 (e.g., $z\\leq 2.1$). Our restricted worst case\nanalysis by assuming $\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}$ shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse. \n\n"}
{"id": "1408.2156", "contents": "Title: Statistical guarantees for the EM algorithm: From population to\n  sample-based analysis Abstract: We develop a general framework for proving rigorous guarantees on the\nperformance of the EM algorithm and a variant known as gradient EM. Our\nanalysis is divided into two parts: a treatment of these algorithms at the\npopulation level (in the limit of infinite data), followed by results that\napply to updates based on a finite set of samples. First, we characterize the\ndomain of attraction of any global maximizer of the population likelihood. This\ncharacterization is based on a novel view of the EM updates as a perturbed form\nof likelihood ascent, or in parallel, of the gradient EM updates as a perturbed\nform of standard gradient ascent. Leveraging this characterization, we then\nprovide non-asymptotic guarantees on the EM and gradient EM algorithms when\napplied to a finite set of samples. We develop consequences of our general\ntheory for three canonical examples of incomplete-data problems: mixture of\nGaussians, mixture of regressions, and linear regression with covariates\nmissing completely at random. In each case, our theory guarantees that with a\nsuitable initialization, a relatively small number of EM (or gradient EM) steps\nwill yield (with high probability) an estimate that is within statistical error\nof the MLE. We provide simulations to confirm this theoretically predicted\nbehavior. \n\n"}
{"id": "1408.2552", "contents": "Title: Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction\n  of Tumors Abstract: Statistical machine learning methods, especially nonparametric Bayesian\nmethods, have become increasingly popular to infer clonal population structure\nof tumors. Here we describe the treeCRP, an extension of the Chinese restaurant\nprocess (CRP), a popular construction used in nonparametric mixture models, to\ninfer the phylogeny and genotype of major subclonal lineages represented in the\npopulation of cancer cells. We also propose new split-merge updates tailored to\nthe subclonal reconstruction problem that improve the mixing time of Markov\nchains. In comparisons with the tree-structured stick breaking prior used in\nPhyloSub, we demonstrate superior mixing and running time using the treeCRP\nwith our new split-merge procedures. We also show that given the same number of\nsamples, TSSB and treeCRP have similar ability to recover the subclonal\nstructure of a tumor. \n\n"}
{"id": "1408.5099", "contents": "Title: Uniform Sampling for Matrix Approximation Abstract: Random sampling has become a critical tool in solving massive matrix\nproblems. For linear regression, a small, manageable set of data rows can be\nrandomly selected to approximate a tall, skinny data matrix, improving\nprocessing time significantly. For theoretical performance guarantees, each row\nmust be sampled with probability proportional to its statistical leverage\nscore. Unfortunately, leverage scores are difficult to compute.\n  A simple alternative is to sample rows uniformly at random. While this often\nworks, uniform sampling will eliminate critical row information for many\nnatural instances. We take a fresh look at uniform sampling by examining what\ninformation it does preserve. Specifically, we show that uniform sampling\nyields a matrix that, in some sense, well approximates a large fraction of the\noriginal. While this weak form of approximation is not enough for solving\nlinear regression directly, it is enough to compute a better approximation.\n  This observation leads to simple iterative row sampling algorithms for matrix\napproximation that run in input-sparsity time and preserve row structure and\nsparsity at all intermediate steps. In addition to an improved understanding of\nuniform sampling, our main proof introduces a structural result of independent\ninterest: we show that every matrix can be made to have low coherence by\nreweighting a small subset of its rows. \n\n"}
{"id": "1409.3768", "contents": "Title: Optimization Methods for Sparse Pseudo-Likelihood Graphical Model\n  Selection Abstract: Sparse high dimensional graphical model selection is a popular topic in\ncontemporary machine learning. To this end, various useful approaches have been\nproposed in the context of $\\ell_1$-penalized estimation in the Gaussian\nframework. Though many of these inverse covariance estimation approaches are\ndemonstrably scalable and have leveraged recent advances in convex\noptimization, they still depend on the Gaussian functional form. To address\nthis gap, a convex pseudo-likelihood based partial correlation graph estimation\nmethod (CONCORD) has been recently proposed. This method uses coordinate-wise\nminimization of a regression based pseudo-likelihood, and has been shown to\nhave robust model selection properties in comparison with the Gaussian\napproach. In direct contrast to the parallel work in the Gaussian setting\nhowever, this new convex pseudo-likelihood framework has not leveraged the\nextensive array of methods that have been proposed in the machine learning\nliterature for convex optimization. In this paper, we address this crucial gap\nby proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for\nperforming $\\ell_1$-regularized inverse covariance matrix estimation in the\npseudo-likelihood framework. We present timing comparisons with coordinate-wise\nminimization and demonstrate that our approach yields tremendous payoffs for\n$\\ell_1$-penalized partial correlation graph estimation outside the Gaussian\nsetting, thus yielding the fastest and most scalable approach for such\nproblems. We undertake a theoretical analysis of our approach and rigorously\ndemonstrate convergence, and also derive rates thereof. \n\n"}
{"id": "1410.3886", "contents": "Title: Tighter Low-rank Approximation via Sampling the Leveraged Element Abstract: In this work, we propose a new randomized algorithm for computing a low-rank\napproximation to a given matrix. Taking an approach different from existing\nliterature, our method first involves a specific biased sampling, with an\nelement being chosen based on the leverage scores of its row and column, and\nthen involves weighted alternating minimization over the factored form of the\nintended low-rank matrix, to minimize error only on these samples. Our method\ncan leverage input sparsity, yet produce approximations in {\\em spectral} (as\nopposed to the weaker Frobenius) norm; this combines the best aspects of\notherwise disparate current results, but with a dependence on the condition\nnumber $\\kappa = \\sigma_1/\\sigma_r$. In particular we require $O(nnz(M) +\n\\frac{n\\kappa^2 r^5}{\\epsilon^2})$ computations to generate a rank-$r$\napproximation to $M$ in spectral norm. In contrast, the best existing method\nrequires $O(nnz(M)+ \\frac{nr^2}{\\epsilon^4})$ time to compute an approximation\nin Frobenius norm. Besides the tightness in spectral norm, we have a better\ndependence on the error $\\epsilon$. Our method is naturally and highly\nparallelizable.\n  Our new approach enables two extensions that are interesting on their own.\nThe first is a new method to directly compute a low-rank approximation (in\nefficient factored form) to the product of two given matrices; it computes a\nsmall random set of entries of the product, and then executes weighted\nalternating minimization (as before) on these. The sampling strategy is\ndifferent because now we cannot access leverage scores of the product matrix\n(but instead have to work with input matrices). The second extension is an\nimproved algorithm with smaller communication complexity for the distributed\nPCA setting (where each server has small set of rows of the matrix, and want to\ncompute low rank approximation with small amount of communication with other\nservers). \n\n"}
{"id": "1410.4355", "contents": "Title: Multi-Level Anomaly Detection on Time-Varying Graph Data Abstract: This work presents a novel modeling and analysis framework for graph\nsequences which addresses the challenge of detecting and contextualizing\nanomalies in labelled, streaming graph data. We introduce a generalization of\nthe BTER model of Seshadhri et al. by adding flexibility to community\nstructure, and use this model to perform multi-scale graph anomaly detection.\nSpecifically, probability models describing coarse subgraphs are built by\naggregating probabilities at finer levels, and these closely related\nhierarchical models simultaneously detect deviations from expectation. This\ntechnique provides insight into a graph's structure and internal context that\nmay shed light on a detected event. Additionally, this multi-scale analysis\nfacilitates intuitive visualizations by allowing users to narrow focus from an\nanomalous graph to particular subgraphs or nodes causing the anomaly.\n  For evaluation, two hierarchical anomaly detectors are tested against a\nbaseline Gaussian method on a series of sampled graphs. We demonstrate that our\ngraph statistics-based approach outperforms both a distribution-based detector\nand the baseline in a labeled setting with community structure, and it\naccurately detects anomalies in synthetic and real-world datasets at the node,\nsubgraph, and graph levels. To illustrate the accessibility of information made\npossible via this technique, the anomaly detector and an associated interactive\nvisualization tool are tested on NCAA football data, where teams and\nconferences that moved within the league are identified with perfect recall,\nand precision greater than 0.786. \n\n"}
{"id": "1410.6382", "contents": "Title: Attribute Efficient Linear Regression with Data-Dependent Sampling Abstract: In this paper we analyze a budgeted learning setting, in which the learner\ncan only choose and observe a small subset of the attributes of each training\nexample. We develop efficient algorithms for ridge and lasso linear regression,\nwhich utilize the geometry of the data by a novel data-dependent sampling\nscheme. When the learner has prior knowledge on the second moments of the\nattributes, the optimal sampling probabilities can be calculated precisely, and\nresult in data-dependent improvements factors for the excess risk over the\nstate-of-the-art that may be as large as $O(\\sqrt{d})$, where $d$ is the\nproblem's dimension. Moreover, under reasonable assumptions our algorithms can\nuse less attributes than full-information algorithms, which is the main concern\nin budgeted learning settings. To the best of our knowledge, these are the\nfirst algorithms able to do so in our setting. Where no such prior knowledge is\navailable, we develop a simple estimation technique that given a sufficient\namount of training examples, achieves similar improvements. We complement our\ntheoretical analysis with experiments on several data sets which support our\nclaims. \n\n"}
{"id": "1410.6801", "contents": "Title: Dimensionality Reduction for k-Means Clustering and Low Rank\n  Approximation Abstract: We show how to approximate a data matrix $\\mathbf{A}$ with a much smaller\nsketch $\\mathbf{\\tilde A}$ that can be used to solve a general class of\nconstrained k-rank approximation problems to within $(1+\\epsilon)$ error.\nImportantly, this class of problems includes $k$-means clustering and\nunconstrained low rank approximation (i.e. principal component analysis). By\nreducing data points to just $O(k)$ dimensions, our methods generically\naccelerate any exact, approximate, or heuristic algorithm for these ubiquitous\nproblems.\n  For $k$-means dimensionality reduction, we provide $(1+\\epsilon)$ relative\nerror results for many common sketching techniques, including random row\nprojection, column selection, and approximate SVD. For approximate principal\ncomponent analysis, we give a simple alternative to known algorithms that has\napplications in the streaming setting. Additionally, we extend recent work on\ncolumn-based matrix reconstruction, giving column subsets that not only `cover'\na good subspace for $\\bv{A}$, but can be used directly to compute this\nsubspace.\n  Finally, for $k$-means clustering, we show how to achieve a $(9+\\epsilon)$\napproximation by Johnson-Lindenstrauss projecting data points to just $O(\\log\nk/\\epsilon^2)$ dimensions. This gives the first result that leverages the\nspecific structure of $k$-means to achieve dimension independent of input size\nand sublinear in $k$. \n\n"}
{"id": "1410.7455", "contents": "Title: Parallel training of DNNs with Natural Gradient and Parameter Averaging Abstract: We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine. \n\n"}
{"id": "1411.0591", "contents": "Title: Bayesian feature selection with strongly-regularizing priors maps to the\n  Ising Model Abstract: Identifying small subsets of features that are relevant for prediction and/or\nclassification tasks is a central problem in machine learning and statistics.\nThe feature selection task is especially important, and computationally\ndifficult, for modern datasets where the number of features can be comparable\nto, or even exceed, the number of samples. Here, we show that feature selection\nwith Bayesian inference takes a universal form and reduces to calculating the\nmagnetizations of an Ising model, under some mild conditions. Our results\nexploit the observation that the evidence takes a universal form for\nstrongly-regularizing priors --- priors that have a large effect on the\nposterior probability even in the infinite data limit. We derive explicit\nexpressions for feature selection for generalized linear models, a large class\nof statistical techniques that include linear and logistic regression. We\nillustrate the power of our approach by analyzing feature selection in a\nlogistic regression-based classifier trained to distinguish between the letters\nB and D in the notMNIST dataset. \n\n"}
{"id": "1411.1158", "contents": "Title: On the Complexity of Learning with Kernels Abstract: A well-recognized limitation of kernel learning is the requirement to handle\na kernel matrix, whose size is quadratic in the number of training examples.\nMany methods have been proposed to reduce this computational cost, mostly by\nusing a subset of the kernel matrix entries, or some form of low-rank matrix\napproximation, or a random projection method. In this paper, we study lower\nbounds on the error attainable by such methods as a function of the number of\nentries observed in the kernel matrix or the rank of an approximate kernel\nmatrix. We show that there are kernel learning problems where no such method\nwill lead to non-trivial computational savings. Our results also quantify how\nthe problem difficulty depends on parameters such as the nature of the loss\nfunction, the regularization parameter, the norm of the desired predictor, and\nthe kernel matrix rank. Our results also suggest cases where more efficient\nkernel learning might be possible. \n\n"}
{"id": "1411.1990", "contents": "Title: A totally unimodular view of structured sparsity Abstract: This paper describes a simple framework for structured sparse recovery based\non convex optimization. We show that many structured sparsity models can be\nnaturally represented by linear matrix inequalities on the support of the\nunknown parameters, where the constraint matrix has a totally unimodular (TU)\nstructure. For such structured models, tight convex relaxations can be obtained\nin polynomial time via linear programming. Our modeling framework unifies the\nprevalent structured sparsity norms in the literature, introduces new\ninteresting ones, and renders their tightness and tractability arguments\ntransparent. \n\n"}
{"id": "1411.5988", "contents": "Title: Clustering evolving data using kernel-based methods Abstract: In this thesis, we propose several modelling strategies to tackle evolving\ndata in different contexts. In the framework of static clustering, we start by\nintroducing a soft kernel spectral clustering (SKSC) algorithm, which can\nbetter deal with overlapping clusters with respect to kernel spectral\nclustering (KSC) and provides more interpretable outcomes. Afterwards, a whole\nstrategy based upon KSC for community detection of static networks is proposed,\nwhere the extraction of a high quality training sub-graph, the choice of the\nkernel function, the model selection and the applicability to large-scale data\nare key aspects. This paves the way for the development of a novel clustering\nalgorithm for the analysis of evolving networks called kernel spectral\nclustering with memory effect (MKSC), where the temporal smoothness between\nclustering results in successive time steps is incorporated at the level of the\nprimal optimization problem, by properly modifying the KSC formulation. Later\non, an application of KSC to fault detection of an industrial machine is\npresented. Here, a smart pre-processing of the data by means of a proper\nwindowing operation is necessary to catch the ongoing degradation process\naffecting the machine. In this way, in a genuinely unsupervised manner, it is\npossible to raise an early warning when necessary, in an online fashion.\nFinally, we propose a new algorithm called incremental kernel spectral\nclustering (IKSC) for online learning of non-stationary data. This ambitious\nchallenge is faced by taking advantage of the out-of-sample property of kernel\nspectral clustering (KSC) to adapt the initial model, in order to tackle\nmerging, splitting or drifting of clusters across time. Real-world applications\nconsidered in this thesis include image segmentation, time-series clustering,\ncommunity detection of static and evolving networks. \n\n"}
{"id": "1412.2954", "contents": "Title: Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample\n  Complexity Abstract: We present a simple, general technique for reducing the sample complexity of\nmatrix and tensor decomposition algorithms applied to distributions. We use the\ntechnique to give a polynomial-time algorithm for standard ICA with sample\ncomplexity nearly linear in the dimension, thereby improving substantially on\nprevious bounds. The analysis is based on properties of random polynomials,\nnamely the spacings of an ensemble of polynomials. Our technique also applies\nto other applications of tensor decompositions, including spherical Gaussian\nmixture models. \n\n"}
{"id": "1412.3046", "contents": "Title: Provable Tensor Methods for Learning Mixtures of Generalized Linear\n  Models Abstract: We consider the problem of learning mixtures of generalized linear models\n(GLM) which arise in classification and regression problems. Typical learning\napproaches such as expectation maximization (EM) or variational Bayes can get\nstuck in spurious local optima. In contrast, we present a tensor decomposition\nmethod which is guaranteed to correctly recover the parameters. The key insight\nis to employ certain feature transformations of the input, which depend on the\ninput generative model. Specifically, we employ score function tensors of the\ninput and compute their cross-correlation with the response variable. We\nestablish that the decomposition of this tensor consistently recovers the\nparameters, under mild non-degeneracy conditions. We demonstrate that the\ncomputational and sample complexity of our method is a low order polynomial of\nthe input and the latent dimensions. \n\n"}
{"id": "1412.4005", "contents": "Title: Sparsity and adaptivity for the blind separation of partially correlated\n  sources Abstract: Blind source separation (BSS) is a very popular technique to analyze\nmultichannel data. In this context, the data are modeled as the linear\ncombination of sources to be retrieved. For that purpose, standard BSS methods\nall rely on some discrimination principle, whether it is statistical\nindependence or morphological diversity, to distinguish between the sources.\nHowever, dealing with real-world data reveals that such assumptions are rarely\nvalid in practice: the signals of interest are more likely partially\ncorrelated, which generally hampers the performances of standard BSS methods.\nIn this article, we introduce a novel sparsity-enforcing BSS method coined\nAdaptive Morphological Component Analysis (AMCA), which is designed to retrieve\nsparse and partially correlated sources. More precisely, it makes profit of an\nadaptive re-weighting scheme to favor/penalize samples based on their level of\ncorrelation. Extensive numerical experiments have been carried out which show\nthat the proposed method is robust to the partial correlation of sources while\nstandard BSS techniques fail. The AMCA algorithm is evaluated in the field of\nastrophysics for the separation of physical components from microwave data. \n\n"}
{"id": "1412.6558", "contents": "Title: Random Walk Initialization for Training Very Deep Feedforward Networks Abstract: Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases. \n\n"}
{"id": "1412.6830", "contents": "Title: Learning Activation Functions to Improve Deep Neural Networks Abstract: Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes. \n\n"}
{"id": "1501.00263", "contents": "Title: Communication-Efficient Distributed Optimization of Self-Concordant\n  Empirical Loss Abstract: We consider distributed convex optimization problems originated from sample\naverage approximation of stochastic optimization, or empirical risk\nminimization in machine learning. We assume that each machine in the\ndistributed computing system has access to a local empirical loss function,\nconstructed with i.i.d. data sampled from a common distribution. We propose a\ncommunication-efficient distributed algorithm to minimize the overall empirical\nloss, which is the average of the local empirical losses. The algorithm is\nbased on an inexact damped Newton method, where the inexact Newton steps are\ncomputed by a distributed preconditioned conjugate gradient method. We analyze\nits iteration complexity and communication efficiency for minimizing\nself-concordant empirical loss functions, and discuss the results for\ndistributed ridge regression, logistic regression and binary classification\nwith a smoothed hinge loss. In a standard setting for supervised learning, the\nrequired number of communication rounds of the algorithm does not increase with\nthe sample size, and only grows slowly with the number of machines. \n\n"}
{"id": "1501.00559", "contents": "Title: The Learnability of Unknown Quantum Measurements Abstract: Quantum machine learning has received significant attention in recent years,\nand promising progress has been made in the development of quantum algorithms\nto speed up traditional machine learning tasks. In this work, however, we focus\non investigating the information-theoretic upper bounds of sample complexity -\nhow many training samples are sufficient to predict the future behaviour of an\nunknown target function. This kind of problem is, arguably, one of the most\nfundamental problems in statistical learning theory and the bounds for\npractical settings can be completely characterised by a simple measure of\ncomplexity.\n  Our main result in the paper is that, for learning an unknown quantum\nmeasurement, the upper bound, given by the fat-shattering dimension, is\nlinearly proportional to the dimension of the underlying Hilbert space.\nLearning an unknown quantum state becomes a dual problem to ours, and as a\nbyproduct, we can recover Aaronson's famous result [Proc. R. Soc. A\n463:3089-3144 (2007)] solely using a classical machine learning technique. In\naddition, other famous complexity measures like covering numbers and Rademacher\ncomplexities are derived explicitly. We are able to connect measures of sample\ncomplexity with various areas in quantum information science, e.g. quantum\nstate/measurement tomography, quantum state discrimination and quantum random\naccess codes, which may be of independent interest. Lastly, with the assistance\nof general Bloch-sphere representation, we show that learning quantum\nmeasurements/states can be mathematically formulated as a neural network.\nConsequently, classical ML algorithms can be applied to efficiently accomplish\nthe two quantum learning tasks. \n\n"}
{"id": "1501.03854", "contents": "Title: Understanding Kernel Ridge Regression: Common behaviors from simple\n  functions to density functionals Abstract: Accurate approximations to density functionals have recently been obtained\nvia machine learning (ML). By applying ML to a simple function of one variable\nwithout any random sampling, we extract the qualitative dependence of errors on\nhyperparameters. We find universal features of the behavior in extreme limits,\nincluding both very small and very large length scales, and the noise-free\nlimit. We show how such features arise in ML models of density functionals. \n\n"}
{"id": "1502.02127", "contents": "Title: Hyperparameter Search in Machine Learning Abstract: We introduce the hyperparameter search problem in the field of machine\nlearning and discuss its main challenges from an optimization perspective.\nMachine learning methods attempt to build models that capture some element of\ninterest based on given data. Most common learning algorithms feature a set of\nhyperparameters that must be determined before training commences. The choice\nof hyperparameters can significantly affect the resulting model's performance,\nbut determining good values can be complex; hence a disciplined, theoretically\nsound search strategy is essential. \n\n"}
{"id": "1502.02367", "contents": "Title: Gated Feedback Recurrent Neural Networks Abstract: In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions. \n\n"}
{"id": "1502.03163", "contents": "Title: Gaussian Process Models for HRTF based Sound-Source Localization and\n  Active-Learning Abstract: From a machine learning perspective, the human ability localize sounds can be\nmodeled as a non-parametric and non-linear regression problem between binaural\nspectral features of sound received at the ears (input) and their sound-source\ndirections (output). The input features can be summarized in terms of the\nindividual's head-related transfer functions (HRTFs) which measure the spectral\nresponse between the listener's eardrum and an external point in $3$D. Based on\nthese viewpoints, two related problems are considered: how can one achieve an\noptimal sampling of measurements for training sound-source localization (SSL)\nmodels, and how can SSL models be used to infer the subject's HRTFs in\nlistening tests. First, we develop a class of binaural SSL models based on\nGaussian process regression and solve a \\emph{forward selection} problem that\nfinds a subset of input-output samples that best generalize to all SSL\ndirections. Second, we use an \\emph{active-learning} approach that updates an\nonline SSL model for inferring the subject's SSL errors via headphones and a\ngraphical user interface. Experiments show that only a small fraction of HRTFs\nare required for $5^{\\circ}$ localization accuracy and that the learned HRTFs\nare localized closer to their intended directions than non-individualized\nHRTFs. \n\n"}
{"id": "1502.03496", "contents": "Title: Spectral Sparsification of Random-Walk Matrix Polynomials Abstract: We consider a fundamental algorithmic question in spectral graph theory:\nCompute a spectral sparsifier of random-walk matrix-polynomial\n$$L_\\alpha(G)=D-\\sum_{r=1}^d\\alpha_rD(D^{-1}A)^r$$ where $A$ is the adjacency\nmatrix of a weighted, undirected graph, $D$ is the diagonal matrix of weighted\ndegrees, and $\\alpha=(\\alpha_1...\\alpha_d)$ are nonnegative coefficients with\n$\\sum_{r=1}^d\\alpha_r=1$. Recall that $D^{-1}A$ is the transition matrix of\nrandom walks on the graph. The sparsification of $L_\\alpha(G)$ appears to be\nalgorithmically challenging as the matrix power $(D^{-1}A)^r$ is defined by all\npaths of length $r$, whose precise calculation would be prohibitively\nexpensive.\n  In this paper, we develop the first nearly linear time algorithm for this\nsparsification problem: For any $G$ with $n$ vertices and $m$ edges, $d$\ncoefficients $\\alpha$, and $\\epsilon > 0$, our algorithm runs in time\n$O(d^2m\\log^2n/\\epsilon^{2})$ to construct a Laplacian matrix\n$\\tilde{L}=D-\\tilde{A}$ with $O(n\\log n/\\epsilon^{2})$ non-zeros such that\n$\\tilde{L}\\approx_{\\epsilon}L_\\alpha(G)$.\n  Matrix polynomials arise in mathematical analysis of matrix functions as well\nas numerical solutions of matrix equations. Our work is particularly motivated\nby the algorithmic problems for speeding up the classic Newton's method in\napplications such as computing the inverse square-root of the precision matrix\nof a Gaussian random field, as well as computing the $q$th-root transition (for\n$q\\geq1$) in a time-reversible Markov model. The key algorithmic step for both\napplications is the construction of a spectral sparsifier of a constant degree\nrandom-walk matrix-polynomials introduced by Newton's method. Our algorithm can\nalso be used to build efficient data structures for effective resistances for\nmulti-step time-reversible Markov models, and we anticipate that it could be\nuseful for other tasks in network analysis. \n\n"}
{"id": "1502.05767", "contents": "Title: Automatic differentiation in machine learning: a survey Abstract: Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\nmachine learning. Automatic differentiation (AD), also called algorithmic\ndifferentiation or simply \"autodiff\", is a family of techniques similar to but\nmore general than backpropagation for efficiently and accurately evaluating\nderivatives of numeric functions expressed as computer programs. AD is a small\nbut established field with applications in areas including computational fluid\ndynamics, atmospheric sciences, and engineering design optimization. Until very\nrecently, the fields of machine learning and AD have largely been unaware of\neach other and, in some cases, have independently discovered each other's\nresults. Despite its relevance, general-purpose AD has been missing from the\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\nunder the names \"dynamic computational graphs\" and \"differentiable\nprogramming\". We survey the intersection of AD and machine learning, cover\napplications where AD has direct relevance, and address the main implementation\ntechniques. By precisely defining the main differentiation techniques and their\ninterrelationships, we aim to bring clarity to the usage of the terms\n\"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as\nthese are encountered more and more in machine learning settings. \n\n"}
{"id": "1502.06531", "contents": "Title: Scalable Variational Inference in Log-supermodular Models Abstract: We consider the problem of approximate Bayesian inference in log-supermodular\nmodels. These models encompass regular pairwise MRFs with binary variables, but\nallow to capture high-order interactions, which are intractable for existing\napproximate inference techniques such as belief propagation, mean field, and\nvariants. We show that a recently proposed variational approach to inference in\nlog-supermodular models -L-FIELD- reduces to the widely-studied minimum norm\nproblem for submodular minimization. This insight allows to leverage powerful\nexisting tools, and hence to solve the variational problem orders of magnitude\nmore efficiently than previously possible. We then provide another natural\ninterpretation of L-FIELD, demonstrating that it exactly minimizes a specific\ntype of R\\'enyi divergence measure. This insight sheds light on the nature of\nthe variational approximations produced by L-FIELD. Furthermore, we show how to\nperform parallel inference as message passing in a suitable factor graph at a\nlinear convergence rate, without having to sum up over all the configurations\nof the factor. Finally, we apply our approach to a challenging image\nsegmentation task. Our experiments confirm scalability of our approach, high\nquality of the marginals, and the benefit of incorporating higher-order\npotentials. \n\n"}
{"id": "1503.00024", "contents": "Title: Influence Maximization with Bandits Abstract: We consider the problem of \\emph{influence maximization}, the problem of\nmaximizing the number of people that become aware of a product by finding the\n`best' set of `seed' users to expose the product to. Most prior work on this\ntopic assumes that we know the probability of each user influencing each other\nuser, or we have data that lets us estimate these influences. However, this\ninformation is typically not initially available or is difficult to obtain. To\navoid this assumption, we adopt a combinatorial multi-armed bandit paradigm\nthat estimates the influence probabilities as we sequentially try different\nseed sets. We establish bounds on the performance of this procedure under the\nexisting edge-level feedback as well as a novel and more realistic node-level\nfeedback. Beyond our theoretical results, we describe a practical\nimplementation and experimentally demonstrate its efficiency and effectiveness\non four real datasets. \n\n"}
{"id": "1503.00038", "contents": "Title: Sequential Feature Explanations for Anomaly Detection Abstract: In many applications, an anomaly detection system presents the most anomalous\ndata instance to a human analyst, who then must determine whether the instance\nis truly of interest (e.g. a threat in a security setting). Unfortunately, most\nanomaly detectors provide no explanation about why an instance was considered\nanomalous, leaving the analyst with no guidance about where to begin the\ninvestigation. To address this issue, we study the problems of computing and\nevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE\nof an anomaly is a sequence of features, which are presented to the analyst one\nat a time (in order) until the information contained in the highlighted\nfeatures is enough for the analyst to make a confident judgement about the\nanomaly. Since analyst effort is related to the amount of information that they\nconsider in an investigation, an explanation's quality is related to the number\nof features that must be revealed to attain confidence. One of our main\ncontributions is to present a novel framework for large scale quantitative\nevaluations of SFEs, where the quality measure is based on analyst effort. To\ndo this we construct anomaly detection benchmarks from real data sets along\nwith artificial experts that can be simulated for evaluation. Our second\ncontribution is to evaluate several novel explanation approaches within the\nframework and on traditional anomaly detection benchmarks, offering several\ninsights into the approaches. \n\n"}
{"id": "1503.00623", "contents": "Title: Unregularized Online Learning Algorithms with General Loss Functions Abstract: In this paper, we consider unregularized online learning algorithms in a\nReproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit\nconvergence rates of the unregularized online learning algorithms for\nclassification associated with a general gamma-activating loss (see Definition\n1 in the paper). Our results extend and refine the results in Ying and Pontil\n(2008) for the least-square loss and the recent result in Bach and Moulines\n(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we\nestablish a very general condition on the step sizes which guarantees the\nconvergence of the last iterate of such algorithms. Secondly, we establish, for\nthe first time, the convergence of the unregularized pairwise learning\nalgorithm with a general loss function and derive explicit rates under the\nassumption of polynomially decaying step sizes. Concrete examples are used to\nillustrate our main results. The main techniques are tools from convex\nanalysis, refined inequalities of Gaussian averages, and an induction approach. \n\n"}
{"id": "1503.04567", "contents": "Title: Learning Mixed Membership Community Models in Social Tagging Networks\n  through Tensor Methods Abstract: Community detection in graphs has been extensively studied both in theory and\nin applications. However, detecting communities in hypergraphs is more\nchallenging. In this paper, we propose a tensor decomposition approach for\nguaranteed learning of communities in a special class of hypergraphs modeling\nsocial tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform\nhypergraph consisting of (user, tag, resource) hyperedges. We posit a\nprobabilistic mixed membership community model, and prove that the tensor\nmethod consistently learns the communities under efficient sample complexity\nand separation requirements. \n\n"}
{"id": "1503.07240", "contents": "Title: Regularized Minimax Conditional Entropy for Crowdsourcing Abstract: There is a rapidly increasing interest in crowdsourcing for data labeling. By\ncrowdsourcing, a large number of labels can be often quickly gathered at low\ncost. However, the labels provided by the crowdsourcing workers are usually not\nof high quality. In this paper, we propose a minimax conditional entropy\nprinciple to infer ground truth from noisy crowdsourced labels. Under this\nprinciple, we derive a unique probabilistic labeling model jointly\nparameterized by worker ability and item difficulty. We also propose an\nobjective measurement principle, and show that our method is the only method\nwhich satisfies this objective measurement principle. We validate our method\nthrough a variety of real crowdsourcing datasets with binary, multiclass or\nordinal labels. \n\n"}
{"id": "1504.05287", "contents": "Title: Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares\n  Algorithms Abstract: Tensor rank and low-rank tensor decompositions have many applications in\nlearning and complexity theory. Most known algorithms use unfoldings of tensors\nand can only handle rank up to $n^{\\lfloor p/2 \\rfloor}$ for a $p$-th order\ntensor in $\\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose\n3rd order tensors when the rank is super-linear in the dimension. Using ideas\nfrom sum-of-squares hierarchy, we give the first quasi-polynomial time\nalgorithm that can decompose a random 3rd order tensor decomposition when the\nrank is as large as $n^{3/2}/\\textrm{polylog} n$.\n  We also give a polynomial time algorithm for certifying the injective norm of\nrandom low rank tensors. Our tensor decomposition algorithm exploits the\nrelationship between injective norm and the tensor components. The proof relies\non interesting tools for decoupling random variables to prove better matrix\nconcentration bounds, which can be useful in other settings. \n\n"}
{"id": "1505.02250", "contents": "Title: Newton Sketch: A Linear-time Optimization Algorithm with\n  Linear-Quadratic Convergence Abstract: We propose a randomized second-order method for optimization known as the\nNewton Sketch: it is based on performing an approximate Newton step using a\nrandomly projected or sub-sampled Hessian. For self-concordant functions, we\nprove that the algorithm has super-linear convergence with exponentially high\nprobability, with convergence and complexity guarantees that are independent of\ncondition numbers and related problem-dependent quantities. Given a suitable\ninitialization, similar guarantees also hold for strongly convex and smooth\nobjectives without self-concordance. When implemented using randomized\nprojections based on a sub-sampled Hadamard basis, the algorithm typically has\nsubstantially lower complexity than Newton's method. We also describe\nextensions of our methods to programs involving convex constraints that are\nequipped with self-concordant barriers. We discuss and illustrate applications\nto linear programs, quadratic programs with convex constraints, logistic\nregression and other generalized linear models, as well as semidefinite\nprograms. \n\n"}
{"id": "1505.03001", "contents": "Title: Detecting the large entries of a sparse covariance matrix in\n  sub-quadratic time Abstract: The covariance matrix of a $p$-dimensional random variable is a fundamental\nquantity in data analysis. Given $n$ i.i.d. observations, it is typically\nestimated by the sample covariance matrix, at a computational cost of\n$O(np^{2})$ operations. When $n,p$ are large, this computation may be\nprohibitively slow. Moreover, in several contemporary applications, the\npopulation matrix is approximately sparse, and only its few large entries are\nof interest. This raises the following question, at the focus of our work:\nAssuming approximate sparsity of the covariance matrix, can its large entries\nbe detected much faster, say in sub-quadratic time, without explicitly\ncomputing all its $p^{2}$ entries? In this paper, we present and theoretically\nanalyze two randomized algorithms that detect the large entries of an\napproximately sparse sample covariance matrix using only $O(np\\text{ poly log }\np)$ operations. Furthermore, assuming sparsity of the population matrix, we\nderive sufficient conditions on the underlying random variable and on the\nnumber of samples $n$, for the sample covariance matrix to satisfy our\napproximate sparsity requirements. Finally, we illustrate the performance of\nour algorithms via several simulations. \n\n"}
{"id": "1505.04732", "contents": "Title: Layered Adaptive Importance Sampling Abstract: Monte Carlo methods represent the \"de facto\" standard for approximating\ncomplicated integrals involving multidimensional target distributions. In order\nto generate random realizations from the target distribution, Monte Carlo\ntechniques use simpler proposal probability densities to draw candidate\nsamples. The performance of any such method is strictly related to the\nspecification of the proposal distribution, such that unfortunate choices\neasily wreak havoc on the resulting estimators. In this work, we introduce a\nlayered (i.e., hierarchical) procedure to generate samples employed within a\nMonte Carlo scheme. This approach ensures that an appropriate equivalent\nproposal density is always obtained automatically (thus eliminating the risk of\na catastrophic performance), although at the expense of a moderate increase in\nthe complexity. Furthermore, we provide a general unified importance sampling\n(IS) framework, where multiple proposal densities are employed and several IS\nschemes are introduced by applying the so-called deterministic mixture\napproach. Finally, given these schemes, we also propose a novel class of\nadaptive importance samplers using a population of proposals, where the\nadaptation is driven by independent parallel or interacting Markov Chain Monte\nCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefits\nof both IS and MCMC methods. \n\n"}
{"id": "1505.05663", "contents": "Title: Inferring Graphs from Cascades: A Sparse Recovery Framework Abstract: In the Network Inference problem, one seeks to recover the edges of an\nunknown graph from the observations of cascades propagating over this graph. In\nthis paper, we approach this problem from the sparse recovery perspective. We\nintroduce a general model of cascades, including the voter model and the\nindependent cascade model, for which we provide the first algorithm which\nrecovers the graph's edges with high probability and $O(s\\log m)$ measurements\nwhere $s$ is the maximum degree of the graph and $m$ is the number of nodes.\nFurthermore, we show that our algorithm also recovers the edge weights (the\nparameters of the diffusion process) and is robust in the context of\napproximate sparsity. Finally we prove an almost matching lower bound of\n$\\Omega(s\\log\\frac{m}{s})$ and validate our approach empirically on synthetic\ngraphs. \n\n"}
{"id": "1505.06770", "contents": "Title: Sketching for Sequential Change-Point Detection Abstract: We study sequential change-point detection procedures based on linear\nsketches of high-dimensional signal vectors using generalized likelihood ratio\n(GLR) statistics. The GLR statistics allow for an unknown post-change mean that\nrepresents an anomaly or novelty. We consider both fixed and time-varying\nprojections, derive theoretical approximations to two fundamental performance\nmetrics: the average run length (ARL) and the expected detection delay (EDD);\nthese approximations are shown to be highly accurate by numerical simulations.\nWe further characterize the relative performance measure of the sketching\nprocedure compared to that without sketching and show that there can be little\nperformance loss when the signal strength is sufficiently large, and enough\nnumber of sketches are used. Finally, we demonstrate the good performance of\nsketching procedures using simulation and real-data examples on solar flare\ndetection and failure detection in power networks. \n\n"}
{"id": "1505.06999", "contents": "Title: Some Open Problems in Optimal AdaBoost and Decision Stumps Abstract: The significance of the study of the theoretical and practical properties of\nAdaBoost is unquestionable, given its simplicity, wide practical use, and\neffectiveness on real-world datasets. Here we present a few open problems\nregarding the behavior of \"Optimal AdaBoost,\" a term coined by Rudin,\nDaubechies, and Schapire in 2004 to label the simple version of the standard\nAdaBoost algorithm in which the weak learner that AdaBoost uses always outputs\nthe weak classifier with lowest weighted error among the respective hypothesis\nclass of weak classifiers implicit in the weak learner. We concentrate on the\nstandard, \"vanilla\" version of Optimal AdaBoost for binary classification that\nresults from using an exponential-loss upper bound on the misclassification\ntraining error. We present two types of open problems. One deals with general\nweak hypotheses. The other deals with the particular case of decision stumps,\nas often and commonly used in practice. Answers to the open problems can have\nimmediate significant impact to (1) cementing previously established results on\nasymptotic convergence properties of Optimal AdaBoost, for finite datasets,\nwhich in turn can be the start to any convergence-rate analysis; (2)\nunderstanding the weak-hypotheses class of effective decision stumps generated\nfrom data, which we have empirically observed to be significantly smaller than\nthe typically obtained class, as well as the effect on the weak learner's\nrunning time and previously established improved bounds on the generalization\nperformance of Optimal AdaBoost classifiers; and (3) shedding some light on the\n\"self control\" that AdaBoost tends to exhibit in practice. \n\n"}
{"id": "1506.02108", "contents": "Title: Deeply Learning the Messages in Message Passing Inference Abstract: Deep structured output learning shows great promise in tasks like semantic\nimage segmentation. We proffer a new, efficient deep structured model learning\nscheme, in which we show how deep Convolutional Neural Networks (CNNs) can be\nused to estimate the messages in message passing inference for structured\nprediction with Conditional Random Fields (CRFs). With such CNN message\nestimators, we obviate the need to learn or evaluate potential functions for\nmessage calculation. This confers significant efficiency for learning, since\notherwise when performing structured learning for a CRF with CNN potentials it\nis necessary to undertake expensive inference for every stochastic gradient\niteration. The network output dimension for message estimation is the same as\nthe number of classes, in contrast to the network output for general CNN\npotential functions in CRFs, which is exponential in the order of the\npotentials. Hence CNN message learning has fewer network parameters and is more\nscalable for cases that a large number of classes are involved. We apply our\nmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. We\nachieve an intersection-over-union score of 73.4 on its test set, which is the\nbest reported result for methods using the VOC training images alone. This\nimpressive performance demonstrates the effectiveness and usefulness of our CNN\nmessage learning method. \n\n"}
{"id": "1506.02227", "contents": "Title: Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex\n  Losses Abstract: In this work we develop a new algorithm for regularized empirical risk\nminimization. Our method extends recent techniques of Shalev-Shwartz [02/2015],\nwhich enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes.\nMoreover, our method is able to better utilize the information in the data\ndefining the ERM problem. For convex loss functions, our complexity results\nmatch those of QUARTZ, which is a primal-dual method also allowing for\narbitrary mini-batching schemes. The advantage of a dual-free analysis comes\nfrom the fact that it guarantees convergence even for non-convex loss\nfunctions, as long as the average loss is convex. We illustrate through\nexperiments the utility of being able to design arbitrary mini-batching\nschemes. \n\n"}
{"id": "1506.02785", "contents": "Title: On the Error of Random Fourier Features Abstract: Kernel methods give powerful, flexible, and theoretically grounded approaches\nto solving many problems in machine learning. The standard approach, however,\nrequires pairwise evaluations of a kernel function, which can lead to\nscalability issues for very large datasets. Rahimi and Recht (2007) suggested a\npopular approach to handling this problem, known as random Fourier features.\nThe quality of this approximation, however, is not well understood. We improve\nthe uniform error bound of that paper, as well as giving novel understandings\nof the embedding's variance, approximation error, and use in some machine\nlearning methods. We also point out that surprisingly, of the two main variants\nof those features, the more widely used is strictly higher-variance for the\nGaussian kernel and has worse bounds. \n\n"}
{"id": "1506.03137", "contents": "Title: Symmetric Tensor Completion from Multilinear Entries and Learning\n  Product Mixtures over the Hypercube Abstract: We give an algorithm for completing an order-$m$ symmetric low-rank tensor\nfrom its multilinear entries in time roughly proportional to the number of\ntensor entries. We apply our tensor completion algorithm to the problem of\nlearning mixtures of product distributions over the hypercube, obtaining new\nalgorithmic results. If the centers of the product distribution are linearly\nindependent, then we recover distributions with as many as $\\Omega(n)$ centers\nin polynomial time and sample complexity. In the general case, we recover\ndistributions with as many as $\\tilde\\Omega(n)$ centers in quasi-polynomial\ntime, answering an open problem of Feldman et al. (SIAM J. Comp.) for the\nspecial case of distributions with incoherent bias vectors.\n  Our main algorithmic tool is the iterated application of a low-rank matrix\ncompletion algorithm for matrices with adversarially missing entries. \n\n"}
{"id": "1506.03705", "contents": "Title: Random Maxout Features Abstract: In this paper, we propose and study random maxout features, which are\nconstructed by first projecting the input data onto sets of randomly generated\nvectors with Gaussian elements, and then outputing the maximum projection value\nfor each set. We show that the resulting random feature map, when used in\nconjunction with linear models, allows for the locally linear estimation of the\nfunction of interest in classification tasks, and for the locally linear\nembedding of points when used for dimensionality reduction or data\nvisualization. We derive generalization bounds for learning that assess the\nerror in approximating locally linear functions by linear functions in the\nmaxout feature space, and empirically evaluate the efficacy of the approach on\nthe MNIST and TIMIT classification tasks. \n\n"}
{"id": "1506.07540", "contents": "Title: Global Optimality in Tensor Factorization, Deep Learning, and Beyond Abstract: Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization. \n\n"}
{"id": "1506.08105", "contents": "Title: Modelling of directional data using Kent distributions Abstract: The modelling of data on a spherical surface requires the consideration of\ndirectional probability distributions. To model asymmetrically distributed data\non a three-dimensional sphere, Kent distributions are often used. The moment\nestimates of the parameters are typically used in modelling tasks involving\nKent distributions. However, these lack a rigorous statistical treatment. The\nfocus of the paper is to introduce a Bayesian estimation of the parameters of\nthe Kent distribution which has not been carried out in the literature, partly\nbecause of its complex mathematical form. We employ the Bayesian\ninformation-theoretic paradigm of Minimum Message Length (MML) to bridge this\ngap and derive reliable estimators. The inferred parameters are subsequently\nused in mixture modelling of Kent distributions. The problem of inferring the\nsuitable number of mixture components is also addressed using the MML\ncriterion. We demonstrate the superior performance of the derived MML-based\nparameter estimates against the traditional estimators. We apply the MML\nprinciple to infer mixtures of Kent distributions to model empirical data\ncorresponding to protein conformations. We demonstrate the effectiveness of\nKent models to act as improved descriptors of protein structural data as\ncompared to commonly used von Mises-Fisher distributions. \n\n"}
{"id": "1506.08621", "contents": "Title: A spectral method for community detection in moderately-sparse\n  degree-corrected stochastic block models Abstract: We consider community detection in Degree-Corrected Stochastic Block Models\n(DC-SBM). We propose a spectral clustering algorithm based on a suitably\nnormalized adjacency matrix. We show that this algorithm consistently recovers\nthe block-membership of all but a vanishing fraction of nodes, in the regime\nwhere the lowest degree is of order log$(n)$ or higher. Recovery succeeds even\nfor very heterogeneous degree-distributions. The used algorithm does not rely\non parameters as input. In particular, it does not need to know the number of\ncommunities. \n\n"}
{"id": "1507.00473", "contents": "Title: The Optimal Sample Complexity of PAC Learning Abstract: This work establishes a new upper bound on the number of samples sufficient\nfor PAC learning in the realizable case. The bound matches known lower bounds\nup to numerical constant factors. This solves a long-standing open problem on\nthe sample complexity of PAC learning. The technique and analysis build on a\nrecent breakthrough by Hans Simon. \n\n"}
{"id": "1507.00825", "contents": "Title: Ridge Regression, Hubness, and Zero-Shot Learning Abstract: This paper discusses the effect of hubness in zero-shot learning, when ridge\nregression is used to find a mapping between the example space to the label\nspace. Contrary to the existing approach, which attempts to find a mapping from\nthe example space to the label space, we show that mapping labels into the\nexample space is desirable to suppress the emergence of hubs in the subsequent\nnearest neighbor search step. Assuming a simple data model, we prove that the\nproposed approach indeed reduces hubness. This was verified empirically on the\ntasks of bilingual lexicon extraction and image labeling: hubness was reduced\nwith both of these tasks and the accuracy was improved accordingly. \n\n"}
{"id": "1507.01160", "contents": "Title: Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret\n  Analysis Abstract: We consider the correlated multiarmed bandit (MAB) problem in which the\nrewards associated with each arm are modeled by a multivariate Gaussian random\nvariable, and we investigate the influence of the assumptions in the Bayesian\nprior on the performance of the upper credible limit (UCL) algorithm and a new\ncorrelated UCL algorithm. We rigorously characterize the influence of accuracy,\nconfidence, and correlation scale in the prior on the decision-making\nperformance of the algorithms. Our results show how priors and correlation\nstructure can be leveraged to improve performance. \n\n"}
{"id": "1507.02268", "contents": "Title: Optimal approximate matrix product in terms of stable rank Abstract: We prove, using the subspace embedding guarantee in a black box way, that one\ncan achieve the spectral norm guarantee for approximate matrix multiplication\nwith a dimensionality-reducing map having $m = O(\\tilde{r}/\\varepsilon^2)$\nrows. Here $\\tilde{r}$ is the maximum stable rank, i.e. squared ratio of\nFrobenius and operator norms, of the two matrices being multiplied. This is a\nquantitative improvement over previous work of [MZ11, KVZ14], and is also\noptimal for any oblivious dimensionality-reducing map. Furthermore, due to the\nblack box reliance on the subspace embedding property in our proofs, our\ntheorem can be applied to a much more general class of sketching matrices than\nwhat was known before, in addition to achieving better bounds. For example, one\ncan apply our theorem to efficient subspace embeddings such as the Subsampled\nRandomized Hadamard Transform or sparse subspace embeddings, or even with\nsubspace embedding constructions that may be developed in the future.\n  Our main theorem, via connections with spectral error matrix multiplication\nshown in prior work, implies quantitative improvements for approximate least\nsquares regression and low rank approximation. Our main result has also already\nbeen applied to improve dimensionality reduction guarantees for $k$-means\nclustering [CEMMP14], and implies new results for nonparametric regression\n[YPW15].\n  We also separately point out that the proof of the \"BSS\" deterministic\nrow-sampling result of [BSS12] can be modified to show that for any matrices\n$A, B$ of stable rank at most $\\tilde{r}$, one can achieve the spectral norm\nguarantee for approximate matrix multiplication of $A^T B$ by deterministically\nsampling $O(\\tilde{r}/\\varepsilon^2)$ rows that can be found in polynomial\ntime. The original result of [BSS12] was for rank instead of stable rank. Our\nobservation leads to a stronger version of a main theorem of [KMST10]. \n\n"}
{"id": "1507.02672", "contents": "Title: Semi-Supervised Learning with Ladder Networks Abstract: We combine supervised learning with unsupervised learning in deep neural\nnetworks. The proposed model is trained to simultaneously minimize the sum of\nsupervised and unsupervised cost functions by backpropagation, avoiding the\nneed for layer-wise pre-training. Our work builds on the Ladder network\nproposed by Valpola (2015), which we extend by combining the model with\nsupervision. We show that the resulting model reaches state-of-the-art\nperformance in semi-supervised MNIST and CIFAR-10 classification, in addition\nto permutation-invariant MNIST classification with all labels. \n\n"}
{"id": "1507.06535", "contents": "Title: Manitest: Are classifiers really invariant? Abstract: Invariance to geometric transformations is a highly desirable property of\nautomatic classifiers in many image recognition tasks. Nevertheless, it is\nunclear to which extent state-of-the-art classifiers are invariant to basic\ntransformations such as rotations and translations. This is mainly due to the\nlack of general methods that properly measure such an invariance. In this\npaper, we propose a rigorous and systematic approach for quantifying the\ninvariance to geometric transformations of any classifier. Our key idea is to\ncast the problem of assessing a classifier's invariance as the computation of\ngeodesics along the manifold of transformed images. We propose the Manitest\nmethod, built on the efficient Fast Marching algorithm to compute the\ninvariance of classifiers. Our new method quantifies in particular the\nimportance of data augmentation for learning invariance from data, and the\nincreased invariance of convolutional neural networks with depth. We foresee\nthat the proposed generic tool for measuring invariance to a large class of\ngeometric transformations and arbitrary classifiers will have many applications\nfor evaluating and comparing classifiers based on their invariance, and help\nimproving the invariance of existing classifiers. \n\n"}
{"id": "1508.00842", "contents": "Title: Perceptron like Algorithms for Online Learning to Rank Abstract: Perceptron is a classic online algorithm for learning a classification\nfunction. In this paper, we provide a novel extension of the perceptron\nalgorithm to the learning to rank problem in information retrieval. We consider\npopular listwise performance measures such as Normalized Discounted Cumulative\nGain (NDCG) and Average Precision (AP). A modern perspective on perceptron for\nclassification is that it is simply an instance of online gradient descent\n(OGD), during mistake rounds, using the hinge loss function. Motivated by this\ninterpretation, we propose a novel family of listwise, large margin ranking\nsurrogates. Members of this family can be thought of as analogs of the hinge\nloss. Exploiting a certain self-bounding property of the proposed family, we\nprovide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our\nperceptron-like algorithm. We show that, if there exists a perfect oracle\nranker which can correctly rank each instance in an online sequence of ranking\ndata, with some margin, the cumulative loss of perceptron algorithm on that\nsequence is bounded by a constant, irrespective of the length of the sequence.\nThis result is reminiscent of Novikoff's convergence theorem for the\nclassification perceptron. Moreover, we prove a lower bound on the cumulative\nloss achievable by any deterministic algorithm, under the assumption of\nexistence of perfect oracle ranker. The lower bound shows that our perceptron\nbound is not tight, and we propose another, \\emph{purely online}, algorithm\nwhich achieves the lower bound. We provide empirical results on simulated and\nlarge commercial datasets to corroborate our theoretical results. \n\n"}
{"id": "1508.03337", "contents": "Title: A Randomized Rounding Algorithm for Sparse PCA Abstract: We present and analyze a simple, two-step algorithm to approximate the\noptimal solution of the sparse PCA problem. Our approach first solves a L1\npenalized version of the NP-hard sparse PCA optimization problem and then uses\na randomized rounding strategy to sparsify the resulting dense solution. Our\nmain theoretical result guarantees an additive error approximation and provides\na tradeoff between sparsity and accuracy. Our experimental evaluation indicates\nthat our approach is competitive in practice, even compared to state-of-the-art\ntoolboxes such as Spasm. \n\n"}
{"id": "1508.04486", "contents": "Title: A Dictionary Learning Approach for Factorial Gaussian Models Abstract: In this paper, we develop a parameter estimation method for factorially\nparametrized models such as Factorial Gaussian Mixture Model and Factorial\nHidden Markov Model. Our contributions are two-fold. First, we show that the\nemission matrix of the standard Factorial Model is unidentifiable even if the\ntrue assignment matrix is known. Secondly, we address the issue of\nidentifiability by making a one component sharing assumption and derive a\nparameter learning algorithm for this case. Our approach is based on a\ndictionary learning problem of the form $X = O R$, where the goal is to learn\nthe dictionary $O$ given the data matrix $X$. We argue that due to the specific\nstructure of the activation matrix $R$ in the shared component factorial\nmixture model, and an incoherence assumption on the shared component, it is\npossible to extract the columns of the $O$ matrix without the need for\nalternating between the estimation of $O$ and $R$. \n\n"}
{"id": "1508.05170", "contents": "Title: Adaptive Online Learning Abstract: We propose a general framework for studying adaptive regret bounds in the\nonline learning framework, including model selection bounds and data-dependent\nbounds. Given a data- or model-dependent bound we ask, \"Does there exist some\nalgorithm achieving this bound?\" We show that modifications to recently\nintroduced sequential complexity measures can be used to answer this question\nby providing sufficient conditions under which adaptive rates can be achieved.\nIn particular each adaptive rate induces a set of so-called offset complexity\nmeasures, and obtaining small upper bounds on these quantities is sufficient to\ndemonstrate achievability. A cornerstone of our analysis technique is the use\nof one-sided tail inequalities to bound suprema of offset random processes.\n  Our framework recovers and improves a wide variety of adaptive bounds\nincluding quantile bounds, second-order data-dependent bounds, and small loss\nbounds. In addition we derive a new type of adaptive bound for online linear\noptimization based on the spectral norm, as well as a new online PAC-Bayes\ntheorem that holds for countably infinite sets. \n\n"}
{"id": "1509.00519", "contents": "Title: Importance Weighted Autoencoders Abstract: The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\nproposed generative model pairing a top-down generative network with a\nbottom-up recognition network which approximates posterior inference. It\ntypically makes strong assumptions about posterior inference, for instance that\nthe posterior distribution is approximately factorial, and that its parameters\ncan be approximated with nonlinear regression from the observations. As we show\nempirically, the VAE objective can lead to overly simplified representations\nwhich fail to use the network's entire modeling capacity. We present the\nimportance weighted autoencoder (IWAE), a generative model with the same\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\nbound derived from importance weighting. In the IWAE, the recognition network\nuses multiple samples to approximate the posterior, giving it increased\nflexibility to model complex posteriors which do not fit the VAE modeling\nassumptions. We show empirically that IWAEs learn richer latent space\nrepresentations than VAEs, leading to improved test log-likelihood on density\nestimation benchmarks. \n\n"}
{"id": "1509.04537", "contents": "Title: Accelerated filtering on graphs using Lanczos method Abstract: Signal-processing on graphs has developed into a very active field of\nresearch during the last decade. In particular, the number of applications\nusing frames constructed from graphs, like wavelets on graphs, has\nsubstantially increased. To attain scalability for large graphs, fast\ngraph-signal filtering techniques are needed. In this contribution, we propose\nan accelerated algorithm based on the Lanczos method that adapts to the\nLaplacian spectrum without explicitly computing it. The result is an accurate,\nrobust, scalable and efficient algorithm. Compared to existing methods based on\nChebyshev polynomials, our solution achieves higher accuracy without increasing\nthe overall complexity significantly. Furthermore, it is particularly well\nsuited for graphs with large spectral gaps. \n\n"}
{"id": "1509.05142", "contents": "Title: Fast Gaussian Process Regression for Big Data Abstract: Gaussian Processes are widely used for regression tasks. A known limitation\nin the application of Gaussian Processes to regression tasks is that the\ncomputation of the solution requires performing a matrix inversion. The\nsolution also requires the storage of a large matrix in memory. These factors\nrestrict the application of Gaussian Process regression to small and moderate\nsize data sets. We present an algorithm that combines estimates from models\ndeveloped using subsets of the data obtained in a manner similar to the\nbootstrap. The sample size is a critical parameter for this algorithm.\nGuidelines for reasonable choices of algorithm parameters, based on detailed\nexperimental study, are provided. Various techniques have been proposed to\nscale Gaussian Processes to large scale regression tasks. The most appropriate\nchoice depends on the problem context. The proposed method is most appropriate\nfor problems where an additive model works well and the response depends on a\nsmall number of features. The minimax rate of convergence for such problems is\nattractive and we can build effective models with a small subset of the data.\nThe Stochastic Variational Gaussian Process and the Sparse Gaussian Process are\nalso appropriate choices for such problems. These methods pick a subset of data\nbased on theoretical considerations. The proposed algorithm uses bagging and\nrandom sampling. Results from experiments conducted as part of this study\nindicate that the algorithm presented in this work can be as effective as these\nmethods. Model stacking can be used to combine the model developed with the\nproposed method with models from other methods for large scale regression such\nas Gradient Boosted Trees. This can yield performance gains. \n\n"}
{"id": "1509.05642", "contents": "Title: Subgraph-based filterbanks for graph signals Abstract: We design a critically-sampled compact-support biorthogonal transform for\ngraph signals, via graph filterbanks. Instead of partitioning the nodes in two\nsets so as to remove one every two nodes in the filterbank downsampling\noperations, the design is based on a partition of the graph in connected\nsubgraphs. Coarsening is achieved by defining one \"supernode\" for each subgraph\nand the edges for this coarsened graph derives from the connectivity between\nthe subgraphs. Unlike the \"one every two nodes\" downsampling on bipartite\ngraphs, this coarsening operation does not have an exact formulation in the\ngraph Fourier domain. Instead, we rely on the local Fourier bases of each\nsubgraph to define filtering operations. We apply successfully this method to\ndecompose graph signals, and show promising performance on compression and\ndenoising. \n\n"}
{"id": "1509.05789", "contents": "Title: BLC: Private Matrix Factorization Recommenders via Automatic Group\n  Learning Abstract: We propose a privacy-enhanced matrix factorization recommender that exploits\nthe fact that users can often be grouped together by interest. This allows a\nform of \"hiding in the crowd\" privacy. We introduce a novel matrix\nfactorization approach suited to making recommendations in a shared group (or\nnym) setting and the BLC algorithm for carrying out this matrix factorization\nin a privacy-enhanced manner. We demonstrate that the increased privacy does\nnot come at the cost of reduced recommendation accuracy. \n\n"}
{"id": "1509.07577", "contents": "Title: A Review of Feature Selection Methods Based on Mutual Information Abstract: In this work we present a review of the state of the art of information\ntheoretic feature selection methods. The concepts of feature relevance,\nredundance and complementarity (synergy) are clearly defined, as well as Markov\nblanket. The problem of optimal feature selection is defined. A unifying\ntheoretical framework is described, which can retrofit successful heuristic\ncriteria, indicating the approximations made by each method. A number of open\nproblems in the field are presented. \n\n"}
{"id": "1509.08333", "contents": "Title: High-dimensional Time Series Prediction with Missing Values Abstract: High-dimensional time series prediction is needed in applications as diverse\nas demand forecasting and climatology. Often, such applications require methods\nthat are both highly scalable, and deal with noisy data in terms of corruptions\nor missing values. Classical time series methods usually fall short of handling\nboth these issues. In this paper, we propose to adapt matrix matrix completion\napproaches that have previously been successfully applied to large scale noisy\ndata, but which fail to adequately model high-dimensional time series due to\ntemporal dependencies. We present a novel temporal regularized matrix\nfactorization (TRMF) framework which supports data-driven temporal dependency\nlearning and enables forecasting ability to our new matrix factorization\napproach. TRMF is highly general, and subsumes many existing matrix\nfactorization approaches for time series data. We make interesting connections\nto graph regularized matrix factorization methods in the context of learning\nthe dependencies. Experiments on both real and synthetic data show that TRMF\noutperforms several existing approaches for common time series tasks. \n\n"}
{"id": "1509.08863", "contents": "Title: Accelerated Spectral Clustering Using Graph Filtering Of Random Signals Abstract: We build upon recent advances in graph signal processing to propose a faster\nspectral clustering algorithm. Indeed, classical spectral clustering is based\non the computation of the first k eigenvectors of the similarity matrix'\nLaplacian, whose computation cost, even for sparse matrices, becomes\nprohibitive for large datasets. We show that we can estimate the spectral\nclustering distance matrix without computing these eigenvectors: by graph\nfiltering random signals. Also, we take advantage of the stochasticity of these\nrandom vectors to estimate the number of clusters k. We compare our method to\nclassical spectral clustering on synthetic data, and show that it reaches equal\nperformance while being faster by a factor at least two for large datasets. \n\n"}
{"id": "1509.09188", "contents": "Title: Approximate Spectral Clustering: Efficiency and Guarantees Abstract: Approximate Spectral Clustering (ASC) is a popular and successful heuristic\nfor partitioning the nodes of a graph $G$ into clusters for which the ratio of\noutside connections compared to the volume (sum of degrees) is small. ASC\nconsists of the following two subroutines: i) compute an approximate Spectral\nEmbedding via the Power method; and ii) partition the resulting vector set with\nan approximate $k$-means clustering algorithm. The resulting $k$-means\npartition naturally induces a $k$-way node partition of $G$.\n  We give a comprehensive analysis of ASC building on the work of Peng et\nal.~(SICOMP'17), Boutsidis et al.~(ICML'15) and Ostrovsky et al.~(JACM'13). We\nshow that ASC i) runs efficiently, and ii) yields a good approximation of an\noptimal $k$-way node partition of $G$. Moreover, we strengthen the quality\nguarantees of a structural result of Peng et al. by a factor of $k$, and\nsimultaneously weaken the eigenvalue gap assumption. Further, we show that ASC\nfinds a $k$-way node partition of $G$ with the strengthened quality guarantees. \n\n"}
{"id": "1510.05956", "contents": "Title: Optimal Cluster Recovery in the Labeled Stochastic Block Model Abstract: We consider the problem of community detection or clustering in the labeled\nStochastic Block Model (LSBM) with a finite number $K$ of clusters of sizes\nlinearly growing with the global population of items $n$. Every pair of items\nis labeled independently at random, and label $\\ell$ appears with probability\n$p(i,j,\\ell)$ between two items in clusters indexed by $i$ and $j$,\nrespectively. The objective is to reconstruct the clusters from the observation\nof these random labels.\n  Clustering under the SBM and their extensions has attracted much attention\nrecently. Most existing work aimed at characterizing the set of parameters such\nthat it is possible to infer clusters either positively correlated with the\ntrue clusters, or with a vanishing proportion of misclassified items, or\nexactly matching the true clusters. We find the set of parameters such that\nthere exists a clustering algorithm with at most $s$ misclassified items in\naverage under the general LSBM and for any $s=o(n)$, which solves one open\nproblem raised in \\cite{abbe2015community}. We further develop an algorithm,\nbased on simple spectral methods, that achieves this fundamental performance\nlimit within $O(n \\mbox{polylog}(n))$ computations and without the a-priori\nknowledge of the model parameters. \n\n"}
{"id": "1510.08974", "contents": "Title: CONQUER: Confusion Queried Online Bandit Learning Abstract: We present a new recommendation setting for picking out two items from a\ngiven set to be highlighted to a user, based on contextual input. These two\nitems are presented to a user who chooses one of them, possibly stochastically,\nwith a bias that favours the item with the higher value. We propose a\nsecond-order algorithm framework that members of it use uses relative\nupper-confidence bounds to trade off exploration and exploitation, and some\nexplore via sampling. We analyze one algorithm in this framework in an\nadversarial setting with only mild assumption on the data, and prove a regret\nbound of $O(Q_T + \\sqrt{TQ_T\\log T} + \\sqrt{T}\\log T)$, where $T$ is the number\nof rounds and $Q_T$ is the cumulative approximation error of item values using\na linear model. Experiments with product reviews from 33 domains show the\nadvantage of our methods over algorithms designed for related settings, and\nthat UCB based algorithms are inferior to greed or sampling based algorithms. \n\n"}
{"id": "1511.00546", "contents": "Title: An Impossibility Result for Reconstruction in a Degree-Corrected\n  Planted-Partition Model Abstract: We consider the Degree-Corrected Stochastic Block Model (DC-SBM): a random\ngraph on $n$ nodes, having i.i.d. weights $(\\phi_u)_{u=1}^n$ (possibly\nheavy-tailed), partitioned into $q \\geq 2$ asymptotically equal-sized clusters.\nThe model parameters are two constants $a,b > 0$ and the finite second moment\nof the weights $\\Phi^{(2)}$. Vertices $u$ and $v$ are connected by an edge with\nprobability $\\frac{\\phi_u \\phi_v}{n}a$ when they are in the same class and with\nprobability $\\frac{\\phi_u \\phi_v}{n}b$ otherwise.\n  We prove that it is information-theoretically impossible to estimate the\nclusters in a way positively correlated with the true community structure when\n$(a-b)^2 \\Phi^{(2)} \\leq q(a+b)$.\n  As by-products of our proof we obtain $(1)$ a precise coupling result for\nlocal neighbourhoods in DC-SBM's, that we use in a follow up paper [Gulikers et\nal., 2017] to establish a law of large numbers for local-functionals and $(2)$\nthat long-range interactions are weak in (power-law) DC-SBM's. \n\n"}
{"id": "1511.00546", "contents": "Title: An Impossibility Result for Reconstruction in a Degree-Corrected\n  Planted-Partition Model Abstract: We consider the Degree-Corrected Stochastic Block Model (DC-SBM): a random\ngraph on $n$ nodes, having i.i.d. weights $(\\phi_u)_{u=1}^n$ (possibly\nheavy-tailed), partitioned into $q \\geq 2$ asymptotically equal-sized clusters.\nThe model parameters are two constants $a,b > 0$ and the finite second moment\nof the weights $\\Phi^{(2)}$. Vertices $u$ and $v$ are connected by an edge with\nprobability $\\frac{\\phi_u \\phi_v}{n}a$ when they are in the same class and with\nprobability $\\frac{\\phi_u \\phi_v}{n}b$ otherwise.\n  We prove that it is information-theoretically impossible to estimate the\nclusters in a way positively correlated with the true community structure when\n$(a-b)^2 \\Phi^{(2)} \\leq q(a+b)$.\n  As by-products of our proof we obtain $(1)$ a precise coupling result for\nlocal neighbourhoods in DC-SBM's, that we use in a follow up paper [Gulikers et\nal., 2017] to establish a law of large numbers for local-functionals and $(2)$\nthat long-range interactions are weak in (power-law) DC-SBM's. \n\n"}
{"id": "1511.00871", "contents": "Title: Properties of the Sample Mean in Graph Spaces and the\n  Majorize-Minimize-Mean Algorithm Abstract: One of the most fundamental concepts in statistics is the concept of sample\nmean. Properties of the sample mean that are well-defined in Euclidean spaces\nbecome unwieldy or even unclear in graph spaces. Open problems related to the\nsample mean of graphs include: non-existence, non-uniqueness, statistical\ninconsistency, lack of convergence results of mean algorithms, non-existence of\nmidpoints, and disparity to midpoints. We present conditions to resolve all six\nproblems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on\ngraph datasets representing images and molecules show that the MMM-Algorithm\nbest approximates a sample mean of graphs compared to six other mean\nalgorithms. \n\n"}
{"id": "1511.01644", "contents": "Title: Interpretable classifiers using rules and Bayesian analysis: Building a\n  better stroke prediction model Abstract: We aim to produce predictive models that are not only accurate, but are also\ninterpretable to human experts. Our models are decision lists, which consist of\na series of if...then... statements (e.g., if high blood pressure, then stroke)\nthat discretize a high-dimensional, multivariate feature space into a series of\nsimple, readily interpretable decision statements. We introduce a generative\nmodel called Bayesian Rule Lists that yields a posterior distribution over\npossible decision lists. It employs a novel prior structure to encourage\nsparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy\non par with the current top algorithms for prediction in machine learning. Our\nmethod is motivated by recent developments in personalized medicine, and can be\nused to produce highly accurate and interpretable medical scoring systems. We\ndemonstrate this by producing an alternative to the CHADS$_2$ score, actively\nused in clinical practice for estimating the risk of stroke in patients that\nhave atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more\naccurate. \n\n"}
{"id": "1511.04210", "contents": "Title: On the Quality of the Initial Basin in Overspecified Neural Networks Abstract: Deep learning, in the form of artificial neural networks, has achieved\nremarkable practical success in recent years, for a variety of difficult\nmachine learning applications. However, a theoretical explanation for this\nremains a major open problem, since training neural networks involves\noptimizing a highly non-convex objective function, and is known to be\ncomputationally hard in the worst case. In this work, we study the\n\\emph{geometric} structure of the associated non-convex objective function, in\nthe context of ReLU networks and starting from a random initialization of the\nnetwork parameters. We identify some conditions under which it becomes more\nfavorable to optimization, in the sense of (i) High probability of initializing\nat a point from which there is a monotonically decreasing path to a global\nminimum; and (ii) High probability of initializing at a basin (suitably\ndefined) with a small minimal objective value. A common theme in our results is\nthat such properties are more likely to hold for larger (\"overspecified\")\nnetworks, which accords with some recent empirical and theoretical\nobservations. \n\n"}
{"id": "1511.06443", "contents": "Title: Neural Network Matrix Factorization Abstract: Data often comes in the form of an array or matrix. Matrix factorization\ntechniques attempt to recover missing or corrupted entries by assuming that the\nmatrix can be written as the product of two low-rank matrices. In other words,\nmatrix factorization approximates the entries of the matrix by a simple, fixed\nfunction---namely, the inner product---acting on the latent feature vectors for\nthe corresponding row and column. Here we consider replacing the inner product\nby an arbitrary function that we learn from the data at the same time as we\nlearn the latent feature vectors. In particular, we replace the inner product\nby a multi-layer feed-forward neural network, and learn by alternating between\noptimizing the network for fixed latent features, and optimizing the latent\nfeatures for a fixed network. The resulting approach---which we call neural\nnetwork matrix factorization or NNMF, for short---dominates standard low-rank\ntechniques on a suite of benchmark but is dominated by some recent proposals\nthat take advantage of the graph features. Given the vast range of\narchitectures, activation functions, regularizers, and optimization techniques\nthat could be used within the NNMF framework, it seems likely the true\npotential of the approach has yet to be reached. \n\n"}
{"id": "1511.07125", "contents": "Title: What Happened to My Dog in That Network: Unraveling Top-down Generators\n  in Convolutional Neural Networks Abstract: Top-down information plays a central role in human perception, but plays\nrelatively little role in many current state-of-the-art deep networks, such as\nConvolutional Neural Networks (CNNs). This work seeks to explore a path by\nwhich top-down information can have a direct impact within current deep\nnetworks. We explore this path by learning and using \"generators\" corresponding\nto the network internal effects of three types of transformation (each a\nrestriction of a general affine transformation): rotation, scaling, and\ntranslation. We demonstrate how these learned generators can be used to\ntransfer top-down information to novel settings, as mediated by the \"feature\nflows\" that the transformations (and the associated generators) correspond to\ninside the network. Specifically, we explore three aspects: 1) using generators\nas part of a method for synthesizing transformed images --- given a previously\nunseen image, produce versions of that image corresponding to one or more\nspecified transformations, 2) \"zero-shot learning\" --- when provided with a\nfeature flow corresponding to the effect of a transformation of unknown amount,\nleverage learned generators as part of a method by which to perform an accurate\ncategorization of the amount of transformation, even for amounts never observed\nduring training, and 3) (inside-CNN) \"data augmentation\" --- improve the\nclassification performance of an existing network by using the learned\ngenerators to directly provide additional training \"inside the CNN\". \n\n"}
{"id": "1511.07305", "contents": "Title: Block Matrix Formulations for Evolving Networks Abstract: Many types of pairwise interaction take the form of a fixed set of nodes with\nedges that appear and disappear over time. In the case of discrete-time\nevolution, the resulting evolving network may be represented by a time-ordered\nsequence of adjacency matrices. We consider here the issue of representing the\nsystem as a single, higher dimensional block matrix, built from the individual\ntime-slices. We focus on the task of computing network centrality measures.\nFrom a modeling perspective, we show that there is a suitable block formulation\nthat allows us to recover dynamic centrality measures respecting time's arrow.\nFrom a computational perspective, we show that the new block formulation leads\nto the design of more effective numerical algorithms. \n\n"}
{"id": "1511.08405", "contents": "Title: Gains and Losses are Fundamentally Different in Regret Minimization: The\n  Sparse Case Abstract: We demonstrate that, in the classical non-stochastic regret minimization\nproblem with $d$ decisions, gains and losses to be respectively maximized or\nminimized are fundamentally different. Indeed, by considering the additional\nsparsity assumption (at each stage, at most $s$ decisions incur a nonzero\noutcome), we derive optimal regret bounds of different orders. Specifically,\nwith gains, we obtain an optimal regret guarantee after $T$ stages of order\n$\\sqrt{T\\log s}$, so the classical dependency in the dimension is replaced by\nthe sparsity size. With losses, we provide matching upper and lower bounds of\norder $\\sqrt{Ts\\log(d)/d}$, which is decreasing in $d$. Eventually, we also\nstudy the bandit setting, and obtain an upper bound of order $\\sqrt{Ts\\log\n(d/s)}$ when outcomes are losses. This bound is proven to be optimal up to the\nlogarithmic factor $\\sqrt{\\log(d/s)}$. \n\n"}
{"id": "1512.02866", "contents": "Title: Multi-Player Bandits -- a Musical Chairs Approach Abstract: We consider a variant of the stochastic multi-armed bandit problem, where\nmultiple players simultaneously choose from the same set of arms and may\ncollide, receiving no reward. This setting has been motivated by problems\narising in cognitive radio networks, and is especially challenging under the\nrealistic assumption that communication between players is limited. We provide\na communication-free algorithm (Musical Chairs) which attains constant regret\nwith high probability, as well as a sublinear-regret, communication-free\nalgorithm (Dynamic Musical Chairs) for the more difficult setting of players\ndynamically entering and leaving throughout the game. Moreover, both algorithms\ndo not require prior knowledge of the number of players. To the best of our\nknowledge, these are the first communication-free algorithms with these types\nof formal guarantees. We also rigorously compare our algorithms to previous\nworks, and complement our theoretical findings with experiments. \n\n"}
{"id": "1512.03219", "contents": "Title: Norm-Free Radon-Nikodym Approach to Machine Learning Abstract: For Machine Learning (ML) classification problem, where a vector of\n$\\mathbf{x}$--observations (values of attributes) is mapped to a single $y$\nvalue (class label), a generalized Radon--Nikodym type of solution is proposed.\nQuantum--mechanics --like probability states $\\psi^2(\\mathbf{x})$ are\nconsidered and \"Cluster Centers\", corresponding to the extremums of\n$<y\\psi^2(\\mathbf{x})>/<\\psi^2(\\mathbf{x})>$, are found from generalized\neigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes and\ncorresponding to them eigenvectors $\\psi^{[i]}(\\mathbf{x})$ define \"Cluster\nCenters\". The projection of a $\\psi$ state, localized at given $\\mathbf{x}$ to\nclassify, on these eigenvectors define the probability of $y^{[i]}$ outcome,\nthus avoiding using a norm ($L^2$ or other types), required for \"quality\ncriteria\" in a typical Machine Learning technique. A coverage of each `Cluster\nCenter\" is calculated, what potentially allows to separate system properties\n(described by $y^{[i]}$ outcomes) and system testing conditions (described by\n$C^{[i]}$ coverage). As an example of such application $y$ distribution\nestimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can be\nconsidered as Gauss quadratures generalization. This estimator allows to\nperform $y$ probability distribution estimation in a strongly non--Gaussian\ncase. \n\n"}
{"id": "1512.03844", "contents": "Title: Efficient Deep Feature Learning and Extraction via StochasticNets Abstract: Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances. \n\n"}
{"id": "1512.05059", "contents": "Title: Streaming Kernel Principal Component Analysis Abstract: Kernel principal component analysis (KPCA) provides a concise set of basis\nvectors which capture non-linear structures within large data sets, and is a\ncentral tool in data analysis and learning. To allow for non-linear relations,\ntypically a full $n \\times n$ kernel matrix is constructed over $n$ data\npoints, but this requires too much space and time for large values of $n$.\nTechniques such as the Nystr\\\"om method and random feature maps can help\ntowards this goal, but they do not explicitly maintain the basis vectors in a\nstream and take more space than desired. We propose a new approach for\nstreaming KPCA which maintains a small set of basis elements in a stream,\nrequiring space only logarithmic in $n$, and also improves the dependence on\nthe error parameter. Our technique combines together random feature maps with\nrecent advances in matrix sketching, it has guaranteed spectral norm error\nbounds with respect to the original kernel matrix, and it compares favorably in\npractice to state-of-the-art approaches. \n\n"}
{"id": "1512.08425", "contents": "Title: Convexified Modularity Maximization for Degree-corrected Stochastic\n  Block Models Abstract: The stochastic block model (SBM) is a popular framework for studying\ncommunity detection in networks. This model is limited by the assumption that\nall nodes in the same community are statistically equivalent and have equal\nexpected degrees. The degree-corrected stochastic block model (DCSBM) is a\nnatural extension of SBM that allows for degree heterogeneity within\ncommunities. This paper proposes a convexified modularity maximization approach\nfor estimating the hidden communities under DCSBM. Our approach is based on a\nconvex programming relaxation of the classical (generalized) modularity\nmaximization formulation, followed by a novel doubly-weighted $ \\ell_1 $-norm $\nk $-median procedure. We establish non-asymptotic theoretical guarantees for\nboth approximate clustering and perfect clustering. Our approximate clustering\nresults are insensitive to the minimum degree, and hold even in sparse regime\nwith bounded average degrees. In the special case of SBM, these theoretical\nresults match the best-known performance guarantees of computationally feasible\nalgorithms. Numerically, we provide an efficient implementation of our\nalgorithm, which is applied to both synthetic and real-world networks.\nExperiment results show that our method enjoys competitive performance compared\nto the state of the art in the literature. \n\n"}
{"id": "1601.00617", "contents": "Title: Coresets and Sketches Abstract: Geometric data summarization has become an essential tool in both geometric\napproximation algorithms and where geometry intersects with big data problems.\nIn linear or near-linear time large data sets can be compressed into a summary,\nand then more intricate algorithms can be run on the summaries whose results\napproximate those of the full data set. Coresets and sketches are the two most\nimportant classes of these summaries. We survey five types of coresets and\nsketches: shape-fitting, density estimation, high-dimensional vectors,\nhigh-dimensional point sets / matrices, and clustering. \n\n"}
{"id": "1601.05775", "contents": "Title: Local Network Community Detection with Continuous Optimization of\n  Conductance and Weighted Kernel K-Means Abstract: Local network community detection is the task of finding a single community\nof nodes concentrated around few given seed nodes in a localized way.\nConductance is a popular objective function used in many algorithms for local\ncommunity detection. This paper studies a continuous relaxation of conductance.\nWe show that continuous optimization of this objective still leads to discrete\ncommunities. We investigate the relation of conductance with weighted kernel\nk-means for a single community, which leads to the introduction of a new\nobjective function, $\\sigma$-conductance. Conductance is obtained by setting\n$\\sigma$ to $0$. Two algorithms, EMc and PGDc, are proposed to locally optimize\n$\\sigma$-conductance and automatically tune the parameter $\\sigma$. They are\nbased on expectation maximization and projected gradient descent, respectively.\nWe prove locality and give performance guarantees for EMc and PGDc for a class\nof dense and well separated communities centered around the seeds. Experiments\nare conducted on networks with ground-truth communities, comparing to\nstate-of-the-art graph diffusion algorithms for conductance optimization. On\nlarge graphs, results indicate that EMc and PGDc stay localized and produce\ncommunities most similar to the ground, while graph diffusion algorithms\ngenerate large communities of lower quality. \n\n"}
{"id": "1601.05936", "contents": "Title: Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic\n  Modeling in Speech Recognition Abstract: We propose to model the acoustic space of deep neural network (DNN)\nclass-conditional posterior probabilities as a union of low-dimensional\nsubspaces. To that end, the training posteriors are used for dictionary\nlearning and sparse coding. Sparse representation of the test posteriors using\nthis dictionary enables projection to the space of training data. Relying on\nthe fact that the intrinsic dimensions of the posterior subspaces are indeed\nvery small and the matrix of all posteriors belonging to a class has a very low\nrank, we demonstrate how low-dimensional structures enable further enhancement\nof the posteriors and rectify the spurious errors due to mismatch conditions.\nThe enhanced acoustic modeling method leads to improvements in continuous\nspeech recognition task using hybrid DNN-HMM (hidden Markov model) framework in\nboth clean and noisy conditions, where upto 15.4% relative reduction in word\nerror rate (WER) is achieved. \n\n"}
{"id": "1602.01052", "contents": "Title: Better safe than sorry: Risky function exploitation through safe\n  optimization Abstract: Exploration-exploitation of functions, that is learning and optimizing a\nmapping between inputs and expected outputs, is ubiquitous to many real world\nsituations. These situations sometimes require us to avoid certain outcomes at\nall cost, for example because they are poisonous, harmful, or otherwise\ndangerous. We test participants' behavior in scenarios in which they have to\nfind the optimum of a function while at the same time avoid outputs below a\ncertain threshold. In two experiments, we find that Safe-Optimization, a\nGaussian Process-based exploration-exploitation algorithm, describes\nparticipants' behavior well and that participants seem to care firstly whether\na point is safe and then try to pick the optimal point from all such safe\npoints. This means that their trade-off between exploration and exploitation\ncan be seen as an intelligent, approximate, and homeostasis-driven strategy. \n\n"}
{"id": "1602.02018", "contents": "Title: Compressive Spectral Clustering Abstract: Spectral clustering has become a popular technique due to its high\nperformance in many contexts. It comprises three main steps: create a\nsimilarity graph between N objects to cluster, compute the first k eigenvectors\nof its Laplacian matrix to define a feature vector for each object, and run\nk-means on these features to separate objects into k classes. Each of these\nthree steps becomes computationally intensive for large N and/or k. We propose\nto speed up the last two steps based on recent results in the emerging field of\ngraph signal processing: graph filtering of random signals, and random sampling\nof bandlimited graph signals. We prove that our method, with a gain in\ncomputation time that can reach several orders of magnitude, is in fact an\napproximation of spectral clustering, for which we are able to control the\nerror. We test the performance of our method on artificial and real-world\nnetwork data. \n\n"}
{"id": "1602.03146", "contents": "Title: DCM Bandits: Learning to Rank with Multiple Clicks Abstract: A search engine recommends to the user a list of web pages. The user examines\nthis list, from the first page to the last, and clicks on all attractive pages\nuntil the user is satisfied. This behavior of the user can be described by the\ndependent click model (DCM). We propose DCM bandits, an online learning variant\nof the DCM where the goal is to maximize the probability of recommending\nsatisfactory items, such as web pages. The main challenge of our learning\nproblem is that we do not observe which attractive item is satisfactory. We\npropose a computationally-efficient learning algorithm for solving our problem,\ndcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable\nassumptions; and also prove a matching lower bound up to logarithmic factors.\nWe evaluate our algorithm on synthetic and real-world problems, and show that\nit performs well even when our model is misspecified. This work presents the\nfirst practical and regret-optimal online algorithm for learning to rank with\nmultiple clicks in a cascade-like click model. \n\n"}
{"id": "1602.03619", "contents": "Title: Optimal Inference in Crowdsourced Classification via Belief Propagation Abstract: Crowdsourcing systems are popular for solving large-scale labelling tasks\nwith low-paid workers. We study the problem of recovering the true labels from\nthe possibly erroneous crowdsourced labels under the popular Dawid-Skene model.\nTo address this inference problem, several algorithms have recently been\nproposed, but the best known guarantee is still significantly larger than the\nfundamental limit. We close this gap by introducing a tighter lower bound on\nthe fundamental limit and proving that Belief Propagation (BP) exactly matches\nthis lower bound. The guaranteed optimality of BP is the strongest in the sense\nthat it is information-theoretically impossible for any other algorithm to\ncorrectly label a larger fraction of the tasks. Experimental results suggest\nthat BP is close to optimal for all regimes considered and improves upon\ncompeting state-of-the-art algorithms. \n\n"}
{"id": "1602.06516", "contents": "Title: Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling\n  Techniques Abstract: In a series of recent works, we have generalised the consistency results in\nthe stochastic block model literature to the case of uniform and non-uniform\nhypergraphs. The present paper continues the same line of study, where we focus\non partitioning weighted uniform hypergraphs---a problem often encountered in\ncomputer vision. This work is motivated by two issues that arise when a\nhypergraph partitioning approach is used to tackle computer vision problems:\n(i) The uniform hypergraphs constructed for higher-order learning contain all\nedges, but most have negligible weights. Thus, the adjacency tensor is nearly\nsparse, and yet, not binary. (ii) A more serious concern is that standard\npartitioning algorithms need to compute all edge weights, which is\ncomputationally expensive for hypergraphs. This is usually resolved in practice\nby merging the clustering algorithm with a tensor sampling strategy---an\napproach that is yet to be analysed rigorously. We build on our earlier work on\npartitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati,\nICML, 2015), and address the aforementioned issues by proposing provable and\nefficient partitioning algorithms. Our analysis justifies the empirical success\nof practical sampling techniques. We also complement our theoretical findings\nby elaborate empirical comparison of various hypergraph partitioning schemes. \n\n"}
{"id": "1602.06566", "contents": "Title: Interactive Storytelling over Document Collections Abstract: Storytelling algorithms aim to 'connect the dots' between disparate documents\nby linking starting and ending documents through a series of intermediate\ndocuments. Existing storytelling algorithms are based on notions of coherence\nand connectivity, and thus the primary way by which users can steer the story\nconstruction is via design of suitable similarity functions. We present an\nalternative approach to storytelling wherein the user can interactively and\niteratively provide 'must use' constraints to preferentially support the\nconstruction of some stories over others. The three innovations in our approach\nare distance measures based on (inferred) topic distributions, the use of\nconstraints to define sets of linear inequalities over paths, and the\nintroduction of slack and surplus variables to condition the topic distribution\nto preferentially emphasize desired terms over others. We describe experimental\nresults to illustrate the effectiveness of our interactive storytelling\napproach over multiple text datasets. \n\n"}
{"id": "1602.06662", "contents": "Title: Recurrent Orthogonal Networks and Long-Memory Tasks Abstract: Although RNNs have been shown to be powerful tools for processing sequential\ndata, finding architectures or optimization strategies that allow them to model\nvery long term dependencies is still an active area of research. In this work,\nwe carefully analyze two synthetic datasets originally outlined in (Hochreiter\nand Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store\ninformation over many time steps. We explicitly construct RNN solutions to\nthese problems, and using these constructions, illuminate both the problems\nthemselves and the way in which RNNs store different types of information in\ntheir hidden states. These constructions furthermore explain the success of\nrecent methods that specify unitary initializations or constraints on the\ntransition matrices. \n\n"}
{"id": "1602.07576", "contents": "Title: Group Equivariant Convolutional Networks Abstract: We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a\nnatural generalization of convolutional neural networks that reduces sample\ncomplexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of\nlayer that enjoys a substantially higher degree of weight sharing than regular\nconvolution layers. G-convolutions increase the expressive capacity of the\nnetwork without increasing the number of parameters. Group convolution layers\nare easy to use and can be implemented with negligible computational overhead\nfor discrete groups generated by translations, reflections and rotations.\nG-CNNs achieve state of the art results on CIFAR10 and rotated MNIST. \n\n"}
{"id": "1602.08017", "contents": "Title: Meta-learning within Projective Simulation Abstract: Learning models of artificial intelligence can nowadays perform very well on\na large variety of tasks. However, in practice different task environments are\nbest handled by different learning models, rather than a single, universal,\napproach. Most non-trivial models thus require the adjustment of several to\nmany learning parameters, which is often done on a case-by-case basis by an\nexternal party. Meta-learning refers to the ability of an agent to autonomously\nand dynamically adjust its own learning parameters, or meta-parameters. In this\nwork we show how projective simulation, a recently developed model of\nartificial intelligence, can naturally be extended to account for meta-learning\nin reinforcement learning settings. The projective simulation approach is based\non a random walk process over a network of clips. The suggested meta-learning\nscheme builds upon the same design and employs clip networks to monitor the\nagent's performance and to adjust its meta-parameters \"on the fly\". We\ndistinguish between \"reflexive adaptation\" and \"adaptation through learning\",\nand show the utility of both approaches. In addition, a trade-off between\nflexibility and learning-time is addressed. The extended model is examined on\nthree different kinds of reinforcement learning tasks, in which the agent has\ndifferent optimal values of the meta-parameters, and is shown to perform well,\nreaching near-optimal to optimal success rates in all of them, without ever\nneeding to manually adjust any meta-parameter. \n\n"}
{"id": "1603.04917", "contents": "Title: Splines and Wavelets on Circulant Graphs Abstract: We present novel families of wavelets and associated filterbanks for the\nanalysis and representation of functions defined on circulant graphs. In this\nwork, we leverage the inherent vanishing moment property of the circulant graph\nLaplacian operator, and by extension, the e-graph Laplacian, which is\nestablished as a parameterization of the former with respect to the degree per\nnode, for the design of vertex-localized and critically-sampled higher-order\ngraph (e-)spline wavelet filterbanks, which can reproduce and annihilate\nclasses of (exponential) polynomial signals on circulant graphs. In addition,\nwe discuss similarities and analogies of the detected properties and resulting\nconstructions with splines and spline wavelets in the Euclidean domain.\nUltimately, we consider generalizations to arbitrary graphs in the form of\ngraph approximations, with focus on graph product decompositions. In\nparticular, we proceed to show how the use of graph products facilitates a\nmulti-dimensional extension of the proposed constructions and properties. \n\n"}
{"id": "1603.04981", "contents": "Title: An Approximate Dynamic Programming Approach to Adversarial Online\n  Learning Abstract: We describe an approximate dynamic programming (ADP) approach to compute\napproximations of the optimal strategies and of the minimal losses that can be\nguaranteed in discounted repeated games with vector-valued losses. Such games\nprominently arise in the analysis of regret in repeated decision-making in\nadversarial environments, also known as adversarial online learning. At the\ncore of our approach is a characterization of the lower Pareto frontier of the\nset of expected losses that a player can guarantee in these games as the unique\nfixed point of a set-valued dynamic programming operator. When applied to the\nproblem of regret minimization with discounted losses, our approach yields\nalgorithms that achieve markedly improved performance bounds compared to\noff-the-shelf online learning algorithms like Hedge. These results thus suggest\nthe significant potential of ADP-based approaches in adversarial online\nlearning. \n\n"}
{"id": "1603.05642", "contents": "Title: Optimal Black-Box Reductions Between Optimization Objectives Abstract: The diverse world of machine learning applications has given rise to a\nplethora of algorithms and optimization methods, finely tuned to the specific\nregression or classification task at hand. We reduce the complexity of\nalgorithm design for machine learning by reductions: we develop reductions that\ntake a method developed for one setting and apply it to the entire spectrum of\nsmoothness and strong-convexity in applications.\n  Furthermore, unlike existing results, our new reductions are OPTIMAL and more\nPRACTICAL. We show how these new reductions give rise to new and faster running\ntimes on training linear classifiers for various families of loss functions,\nand conclude with experiments showing their successes also in practice. \n\n"}
{"id": "1603.05953", "contents": "Title: Katyusha: The First Direct Acceleration of Stochastic Gradient Methods Abstract: Nesterov's momentum trick is famously known for accelerating gradient\ndescent, and has been proven useful in building fast iterative algorithms.\nHowever, in the stochastic setting, counterexamples exist and prevent\nNesterov's momentum from providing similar acceleration, even if the underlying\nproblem is convex and finite-sum.\n  We introduce $\\mathtt{Katyusha}$, a direct, primal-only stochastic gradient\nmethod to fix this issue. In convex finite-sum stochastic optimization,\n$\\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an\noptimal parallel linear speedup in the mini-batch setting.\n  The main ingredient is $\\textit{Katyusha momentum}$, a novel \"negative\nmomentum\" on top of Nesterov's momentum. It can be incorporated into a\nvariance-reduction based algorithm and speed it up, both in terms of\n$\\textit{sequential and parallel}$ performance. Since variance reduction has\nbeen successfully applied to a growing list of practical problems, our paper\nsuggests that in each of such cases, one could potentially try to give Katyusha\na hug. \n\n"}
{"id": "1603.07834", "contents": "Title: An end-to-end convolutional selective autoencoder approach to Soybean\n  Cyst Nematode eggs detection Abstract: This paper proposes a novel selective autoencoder approach within the\nframework of deep convolutional networks. The crux of the idea is to train a\ndeep convolutional autoencoder to suppress undesired parts of an image frame\nwhile allowing the desired parts resulting in efficient object detection. The\nefficacy of the framework is demonstrated on a critical plant science problem.\nIn the United States, approximately $1 billion is lost per annum due to a\nnematode infection on soybean plants. Currently, plant-pathologists rely on\nlabor-intensive and time-consuming identification of Soybean Cyst Nematode\n(SCN) eggs in soil samples via manual microscopy. The proposed framework\nattempts to significantly expedite the process by using a series of manually\nlabeled microscopic images for training followed by automated high-throughput\negg detection. The problem is particularly difficult due to the presence of a\nlarge population of non-egg particles (disturbances) in the image frames that\nare very similar to SCN eggs in shape, pose and illumination. Therefore, the\nselective autoencoder is trained to learn unique features related to the\ninvariant shapes and sizes of the SCN eggs without handcrafting. After that, a\ncomposite non-maximum suppression and differencing is applied at the\npost-processing stage. \n\n"}
{"id": "1603.09596", "contents": "Title: High-dimensional approximate nearest neighbor: k-d Generalized\n  Randomized Forests Abstract: We propose a new data-structure, the generalized randomized kd forest, or\nkgeraf, for approximate nearest neighbor searching in high dimensions. In\nparticular, we introduce new randomization techniques to specify a set of\nindependently constructed trees where search is performed simultaneously, hence\nincreasing accuracy. We omit backtracking, and we optimize distance\ncomputations, thus accelerating queries. We release public domain software\ngeraf and we compare it to existing implementations of state-of-the-art methods\nincluding BBD-trees, Locality Sensitive Hashing, randomized kd forests, and\nproduct quantization. Experimental results indicate that our method would be\nthe method of choice in dimensions around 1,000, and probably up to 10,000, and\npointsets of cardinality up to a few hundred thousands or even one million;\nthis range of inputs is encountered in many critical applications today. For\ninstance, we handle a real dataset of $10^6$ images represented in 960\ndimensions with a query time of less than $1$sec on average and 90\\% responses\nbeing true nearest neighbors. \n\n"}
{"id": "1604.02492", "contents": "Title: Challenges in Bayesian Adaptive Data Analysis Abstract: Traditional statistical analysis requires that the analysis process and data\nare independent. By contrast, the new field of adaptive data analysis hopes to\nunderstand and provide algorithms and accuracy guarantees for research as it is\ncommonly performed in practice, as an iterative process of interacting\nrepeatedly with the same data set, such as repeated tests against a holdout\nset. Previous work has defined a model with a rather strong lower bound on\nsample complexity in terms of the number of queries, $n\\sim\\sqrt q$, arguing\nthat adaptive data analysis is much harder than static data analysis, where\n$n\\sim\\log q$ is possible. Instead, we argue that those strong lower bounds\npoint to a limitation of the previous model in that it must consider wildly\nasymmetric scenarios which do not hold in typical applications.\n  To better understand other difficulties of adaptivity, we propose a new\nBayesian version of the problem that mandates symmetry. Since the other lower\nbound techniques are ruled out, we can more effectively see difficulties that\nmight otherwise be overshadowed. As a first contribution to this model, we\nproduce a new problem using error-correcting codes on which a large family of\nmethods, including all previously proposed algorithms, require roughly\n$n\\sim\\sqrt[4]q$. These early results illustrate new difficulties in adaptive\ndata analysis regarding slightly correlated queries on problems with\nconcentrated uncertainty. \n\n"}
{"id": "1604.02606", "contents": "Title: A General Retraining Framework for Scalable Adversarial Classification Abstract: Traditional classification algorithms assume that training and test data come\nfrom similar distributions. This assumption is violated in adversarial\nsettings, where malicious actors modify instances to evade detection. A number\nof custom methods have been developed for both adversarial evasion attacks and\nrobust learning. We propose the first systematic and general-purpose retraining\nframework which can: a) boost robustness of an \\emph{arbitrary} learning\nalgorithm, in the face of b) a broader class of adversarial models than any\nprior methods. We show that, under natural conditions, the retraining framework\nminimizes an upper bound on optimal adversarial risk, and show how to extend\nthis result to account for approximations of evasion attacks. Extensive\nexperimental evaluation demonstrates that our retraining methods are nearly\nindistinguishable from state-of-the-art algorithms for optimizing adversarial\nrisk, but are more general and far more scalable. The experiments also confirm\nthat without retraining, our adversarial framework dramatically reduces the\neffectiveness of learning. In contrast, retraining significantly boosts\nrobustness to evasion attacks without significantly compromising overall\naccuracy. \n\n"}
{"id": "1604.06968", "contents": "Title: Agnostic Estimation of Mean and Covariance Abstract: We consider the problem of estimating the mean and covariance of a\ndistribution from iid samples in $\\mathbb{R}^n$, in the presence of an $\\eta$\nfraction of malicious noise; this is in contrast to much recent work where the\nnoise itself is assumed to be from a distribution of known type. The agnostic\nproblem includes many interesting special cases, e.g., learning the parameters\nof a single Gaussian (or finding the best-fit Gaussian) when $\\eta$ fraction of\ndata is adversarially corrupted, agnostically learning a mixture of Gaussians,\nagnostic ICA, etc. We present polynomial-time algorithms to estimate the mean\nand covariance with error guarantees in terms of information-theoretic lower\nbounds. As a corollary, we also obtain an agnostic algorithm for Singular Value\nDecomposition. \n\n"}
{"id": "1604.07101", "contents": "Title: Double Thompson Sampling for Dueling Bandits Abstract: In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for\ndueling bandit problems. As indicated by its name, D-TS selects both the first\nand the second candidates according to Thompson Sampling. Specifically, D-TS\nmaintains a posterior distribution for the preference matrix, and chooses the\npair of arms for comparison by sampling twice from the posterior distribution.\nThis simple algorithm applies to general Copeland dueling bandits, including\nCondorcet dueling bandits as its special case. For general Copeland dueling\nbandits, we show that D-TS achieves $O(K^2 \\log T)$ regret. For Condorcet\ndueling bandits, we further simplify the D-TS algorithm and show that the\nsimplified D-TS algorithm achieves $O(K \\log T + K^2 \\log \\log T)$ regret.\nSimulation results based on both synthetic and real-world data demonstrate the\nefficiency of the proposed D-TS algorithm. \n\n"}
{"id": "1604.07484", "contents": "Title: Deep Multi-fidelity Gaussian Processes Abstract: We develop a novel multi-fidelity framework that goes far beyond the\nclassical AR(1) Co-kriging scheme of Kennedy and O'Hagan (2000). Our method can\nhandle general discontinuous cross-correlations among systems with different\nlevels of fidelity. A combination of multi-fidelity Gaussian Processes (AR(1)\nCo-kriging) and deep neural networks enables us to construct a method that is\nimmune to discontinuities. We demonstrate the effectiveness of the new\ntechnology using standard benchmark problems designed to resemble the outputs\nof complicated high- and low-fidelity codes. \n\n"}
{"id": "1604.08079", "contents": "Title: UBL: an R package for Utility-based Learning Abstract: This document describes the R package UBL that allows the use of several\nmethods for handling utility-based learning problems. Classification and\nregression problems that assume non-uniform costs and/or benefits pose serious\nchallenges to predictive analytic tasks. In the context of meteorology,\nfinance, medicine, ecology, among many other, specific domain information\nconcerning the preference bias of the users must be taken into account to\nenhance the models predictive performance. To deal with this problem, a large\nnumber of techniques was proposed by the research community for both\nclassification and regression tasks. The main goal of UBL package is to\nfacilitate the utility-based predictive analytic task by providing a set of\nmethods to deal with this type of problems in the R environment. It is a\nversatile tool that provides mechanisms to handle both regression and\nclassification (binary and multiclass) tasks. Moreover, UBL package allows the\nuser to specify his domain preferences, but it also provides some automatic\nmethods that try to infer those preference bias from the domain, considering\nsome common known settings. \n\n"}
{"id": "1605.00252", "contents": "Title: Fast Rates for General Unbounded Loss Functions: from ERM to Generalized\n  Bayes Abstract: We present new excess risk bounds for general unbounded loss functions\nincluding log loss and squared loss, where the distribution of the losses may\nbe heavy-tailed. The bounds hold for general estimators, but they are optimized\nwhen applied to $\\eta$-generalized Bayesian, MDL, and empirical risk\nminimization estimators. In the case of log loss, the bounds imply convergence\nrates for generalized Bayesian inference under misspecification in terms of a\ngeneralization of the Hellinger metric as long as the learning rate $\\eta$ is\nset correctly. For general loss functions, our bounds rely on two separate\nconditions: the $v$-GRIP (generalized reversed information projection)\nconditions, which control the lower tail of the excess loss; and the newly\nintroduced witness condition, which controls the upper tail. The parameter $v$\nin the $v$-GRIP conditions determines the achievable rate and is akin to the\nexponent in the Tsybakov margin condition and the Bernstein condition for\nbounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in\ncombination with small model complexity leads to $\\tilde{O}(1/n)$ rates. The\nwitness condition allows us to connect the excess risk to an \"annealed\" version\nthereof, by which we generalize several previous results connecting Hellinger\nand R\\'enyi divergence to KL divergence. \n\n"}
{"id": "1605.00361", "contents": "Title: Monte Carlo with Determinantal Point Processes Abstract: We show that repulsive random variables can yield Monte Carlo methods with\nfaster convergence rates than the typical $N^{-1/2}$, where $N$ is the number\nof integrand evaluations. More precisely, we propose stochastic numerical\nquadratures involving determinantal point processes associated with\nmultivariate orthogonal polynomials, and we obtain root mean square errors that\ndecrease as $N^{-(1+1/d)/2}$, where $d$ is the dimension of the ambient space.\nFirst, we prove a central limit theorem (CLT) for the linear statistics of a\nclass of determinantal point processes, when the reference measure is a product\nmeasure supported on a hypercube, which satisfies the Nevai-class regularity\ncondition, a result which may be of independent interest. Next, we introduce a\nMonte Carlo method based on these determinantal point processes, and prove a\nCLT with explicit limiting variance for the quadrature error, when the\nreference measure satisfies a stronger regularity condition. As a corollary, by\ntaking a specific reference measure and using a construction similar to\nimportance sampling, we obtain a general Monte Carlo method, which applies to\nany measure with continuously derivable density. Loosely speaking, our method\ncan be interpreted as a stochastic counterpart to Gaussian quadrature, which,\nat the price of some convergence rate, is easily generalizable to any dimension\nand has a more explicit error term. \n\n"}
{"id": "1605.06276", "contents": "Title: Piece-wise quadratic approximations of arbitrary error functions for\n  fast and robust machine learning Abstract: Most of machine learning approaches have stemmed from the application of\nminimizing the mean squared distance principle, based on the computationally\nefficient quadratic optimization methods. However, when faced with\nhigh-dimensional and noisy data, the quadratic error functionals demonstrated\nmany weaknesses including high sensitivity to contaminating factors and\ndimensionality curse. Therefore, a lot of recent applications in machine\nlearning exploited properties of non-quadratic error functionals based on $L_1$\nnorm or even sub-linear potentials corresponding to quasinorms $L_p$ ($0<p<1$).\nThe back side of these approaches is increase in computational cost for\noptimization. Till so far, no approaches have been suggested to deal with {\\it\narbitrary} error functionals, in a flexible and computationally efficient\nframework. In this paper, we develop a theory and basic universal data\napproximation algorithms ($k$-means, principal components, principal manifolds\nand graphs, regularized and sparse regression), based on piece-wise quadratic\nerror potentials of subquadratic growth (PQSQ potentials). We develop a new and\nuniversal framework to minimize {\\it arbitrary sub-quadratic error potentials}\nusing an algorithm with guaranteed fast convergence to the local or global\nerror minimum. The theory of PQSQ potentials is based on the notion of the cone\nof minorant functions, and represents a natural approximation formalism based\non the application of min-plus algebra. The approach can be applied in most of\nexisting machine learning methods, including methods of data approximation and\nregularized and sparse regression, leading to the improvement in the\ncomputational cost/accuracy trade-off. We demonstrate that on synthetic and\nreal-life datasets PQSQ-based machine learning methods achieve orders of\nmagnitude faster computational performance than the corresponding\nstate-of-the-art methods. \n\n"}
{"id": "1605.06855", "contents": "Title: Smart broadcasting: Do you want to be seen? Abstract: Many users in online social networks are constantly trying to gain attention\nfrom their followers by broadcasting posts to them. These broadcasters are\nlikely to gain greater attention if their posts can remain visible for a longer\nperiod of time among their followers' most recent feeds. Then when to post? In\nthis paper, we study the problem of smart broadcasting using the framework of\ntemporal point processes, where we model users feeds and posts as discrete\nevents occurring in continuous time. Based on such continuous-time model, then\nchoosing a broadcasting strategy for a user becomes a problem of designing the\nconditional intensity of her posting events. We derive a novel formula which\nlinks this conditional intensity with the visibility of the user in her\nfollowers' feeds. Furthermore, by exploiting this formula, we develop an\nefficient convex optimization framework for the when-to-post problem. Our\nmethod can find broadcasting strategies that reach a desired visibility level\nwith provable guarantees. We experimented with data gathered from Twitter, and\nshow that our framework can consistently make broadcasters' post more visible\nthan alternatives. \n\n"}
{"id": "1605.07018", "contents": "Title: Online Learning with Feedback Graphs Without the Graphs Abstract: We study an online learning framework introduced by Mannor and Shamir (2011)\nin which the feedback is specified by a graph, in a setting where the graph may\nvary from round to round and is \\emph{never fully revealed} to the learner. We\nshow a large gap between the adversarial and the stochastic cases. In the\nadversarial case, we prove that even for dense feedback graphs, the learner\ncannot improve upon a trivial regret bound obtained by ignoring any additional\nfeedback besides her own loss. In contrast, in the stochastic case we give an\nalgorithm that achieves $\\widetilde \\Theta(\\sqrt{\\alpha T})$ regret over $T$\nrounds, provided that the independence numbers of the hidden feedback graphs\nare at most $\\alpha$. We also extend our results to a more general feedback\nmodel, in which the learner does not necessarily observe her own loss, and show\nthat, even in simple cases, concealing the feedback graphs might render a\nlearnable problem unlearnable. \n\n"}
{"id": "1605.07078", "contents": "Title: Learning Sensor Multiplexing Design through Back-propagation Abstract: Recent progress on many imaging and vision tasks has been driven by the use\nof deep feed-forward neural networks, which are trained by propagating\ngradients of a loss defined on the final output, back through the network up to\nthe first layer that operates directly on the image. We propose\nback-propagating one step further---to learn camera sensor designs jointly with\nnetworks that carry out inference on the images they capture. In this paper, we\nspecifically consider the design and inference problems in a typical color\ncamera---where the sensor is able to measure only one color channel at each\npixel location, and computational inference is required to reconstruct a full\ncolor image. We learn the camera sensor's color multiplexing pattern by\nencoding it as layer whose learnable weights determine which color channel,\nfrom among a fixed set, will be measured at each location. These weights are\njointly trained with those of a reconstruction network that operates on the\ncorresponding sensor measurements to produce a full color image. Our network\nachieves significant improvements in accuracy over the traditional Bayer\npattern used in most color cameras. It automatically learns to employ a sparse\ncolor measurement approach similar to that of a recent design, and moreover,\nimproves upon that design by learning an optimal layout for these measurements. \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.08233", "contents": "Title: Stochastic Variance Reduced Riemannian Eigensolver Abstract: We study the stochastic Riemannian gradient algorithm for matrix\neigen-decomposition. The state-of-the-art stochastic Riemannian algorithm\nrequires the learning rate to decay to zero and thus suffers from slow\nconvergence and sub-optimal solutions. In this paper, we address this issue by\ndeploying the variance reduction (VR) technique of stochastic gradient descent\n(SGD). The technique was originally developed to solve convex problems in the\nEuclidean space. We generalize it to Riemannian manifolds and realize it to\nsolve the non-convex eigen-decomposition problem. We are the first to propose\nand analyze the generalization of SVRG to Riemannian manifolds. Specifically,\nwe propose the general variance reduction form, SVRRG, in the framework of the\nstochastic Riemannian gradient optimization. It's then specialized to the\nproblem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a\nnovel and elegant theoretical analysis on this algorithm. The theory shows that\na fixed learning rate can be used in the Riemannian setting with an exponential\nglobal convergence rate guaranteed. The theoretical results make a significant\nimprovement over existing studies, with the effectiveness empirically verified. \n\n"}
{"id": "1605.08491", "contents": "Title: Provable Algorithms for Inference in Topic Models Abstract: Recently, there has been considerable progress on designing algorithms with\nprovable guarantees -- typically using linear algebraic methods -- for\nparameter learning in latent variable models. But designing provable algorithms\nfor inference has proven to be more challenging. Here we take a first step\ntowards provable inference in topic models. We leverage a property of topic\nmodels that enables us to construct simple linear estimators for the unknown\ntopic proportions that have small variance, and consequently can work with\nshort documents. Our estimators also correspond to finding an estimate around\nwhich the posterior is well-concentrated. We show lower bounds that for shorter\ndocuments it can be information theoretically impossible to find the hidden\ntopics. Finally, we give empirical results that demonstrate that our algorithm\nworks on realistic topic models. It yields good solutions on synthetic data and\nruns in time comparable to a {\\em single} iteration of Gibbs sampling. \n\n"}
{"id": "1605.09080", "contents": "Title: Spectral Methods for Correlated Topic Models Abstract: In this paper, we propose guaranteed spectral methods for learning a broad\nrange of topic models, which generalize the popular Latent Dirichlet Allocation\n(LDA). We overcome the limitation of LDA to incorporate arbitrary topic\ncorrelations, by assuming that the hidden topic proportions are drawn from a\nflexible class of Normalized Infinitely Divisible (NID) distributions. NID\ndistributions are generated through the process of normalizing a family of\nindependent Infinitely Divisible (ID) random variables. The Dirichlet\ndistribution is a special case obtained by normalizing a set of Gamma random\nvariables. We prove that this flexible topic model class can be learned via\nspectral methods using only moments up to the third order, with (low order)\npolynomial sample and computational complexity. The proof is based on a key new\ntechnique derived here that allows us to diagonalize the moments of the NID\ndistribution through an efficient procedure that requires evaluating only\nunivariate integrals, despite the fact that we are handling high dimensional\nmultivariate moments. In order to assess the performance of our proposed Latent\nNID topic model, we use two real datasets of articles collected from New York\nTimes and Pubmed. Our experiments yield improved perplexity on both datasets\ncompared with the baseline. \n\n"}
{"id": "1606.01868", "contents": "Title: Unifying Count-Based Exploration and Intrinsic Motivation Abstract: We consider an agent's uncertainty about its environment and the problem of\ngeneralizing this uncertainty across observations. Specifically, we focus on\nthe problem of exploration in non-tabular reinforcement learning. Drawing\ninspiration from the intrinsic motivation literature, we use density models to\nmeasure uncertainty, and propose a novel algorithm for deriving a pseudo-count\nfrom an arbitrary density model. This technique enables us to generalize\ncount-based exploration algorithms to the non-tabular case. We apply our ideas\nto Atari 2600 games, providing sensible pseudo-counts from raw pixels. We\ntransform these pseudo-counts into intrinsic rewards and obtain significantly\nimproved exploration in a number of hard games, including the infamously\ndifficult Montezuma's Revenge. \n\n"}
{"id": "1606.02077", "contents": "Title: Regret Bounds for Non-decomposable Metrics with Missing Labels Abstract: We consider the problem of recommending relevant labels (items) for a given\ndata point (user). In particular, we are interested in the practically\nimportant setting where the evaluation is with respect to non-decomposable\n(over labels) performance metrics like the $F_1$ measure, and the training data\nhas missing labels. To this end, we propose a generic framework that given a\nperformance metric $\\Psi$, can devise a regularized objective function and a\nthreshold such that all the values in the predicted score vector above and only\nabove the threshold are selected to be positive. We show that the regret or\ngeneralization error in the given metric $\\Psi$ is bounded ultimately by\nestimation error of certain underlying parameters. In particular, we derive\nregret bounds under three popular settings: a) collaborative filtering, b)\nmultilabel classification, and c) PU (positive-unlabeled) learning. For each of\nthe above problems, we can obtain precise non-asymptotic regret bound which is\nsmall even when a large fraction of labels is missing. Our empirical results on\nsynthetic and benchmark datasets demonstrate that by explicitly modeling for\nmissing labels and optimizing the desired performance metric, our algorithm\nindeed achieves significantly better performance (like $F_1$ score) when\ncompared to methods that do not model missing label information carefully. \n\n"}
{"id": "1606.04809", "contents": "Title: ASAGA: Asynchronous Parallel SAGA Abstract: We describe ASAGA, an asynchronous parallel version of the incremental\ngradient algorithm SAGA that enjoys fast linear convergence rates. Through a\nnovel perspective, we revisit and clarify a subtle but important technical\nissue present in a large fraction of the recent convergence rate proofs for\nasynchronous parallel optimization algorithms, and propose a simplification of\nthe recently introduced \"perturbed iterate\" framework that resolves it. We\nthereby prove that ASAGA can obtain a theoretical linear speedup on multi-core\nsystems even without sparsity assumptions. We present results of an\nimplementation on a 40-core architecture illustrating the practical speedup as\nwell as the hardware overhead. \n\n"}
{"id": "1606.04934", "contents": "Title: Improving Variational Inference with Inverse Autoregressive Flow Abstract: The framework of normalizing flows provides a general strategy for flexible\nvariational inference of posteriors over latent variables. We propose a new\ntype of normalizing flow, inverse autoregressive flow (IAF), that, in contrast\nto earlier published flows, scales well to high-dimensional latent spaces. The\nproposed flow consists of a chain of invertible transformations, where each\ntransformation is based on an autoregressive neural network. In experiments, we\nshow that IAF significantly improves upon diagonal Gaussian approximate\nposteriors. In addition, we demonstrate that a novel type of variational\nautoencoder, coupled with IAF, is competitive with neural autoregressive models\nin terms of attained log-likelihood on natural images, while allowing\nsignificantly faster synthesis. \n\n"}
{"id": "1606.06126", "contents": "Title: Bootstrapping with Models: Confidence Intervals for Off-Policy\n  Evaluation Abstract: For an autonomous agent, executing a poor policy may be costly or even\ndangerous. For such agents, it is desirable to determine confidence interval\nlower bounds on the performance of any given policy without executing said\npolicy. Current methods for exact high confidence off-policy evaluation that\nuse importance sampling require a substantial amount of data to achieve a tight\nlower bound. Existing model-based methods only address the problem in discrete\nstate spaces. Since exact bounds are intractable for many domains we trade off\nstrict guarantees of safety for more data-efficient approximate bounds. In this\ncontext, we propose two bootstrapping off-policy evaluation methods which use\nlearned MDP transition models in order to estimate lower confidence bounds on\npolicy performance with limited data in both continuous and discrete state\nspaces. Since direct use of a model may introduce bias, we derive a theoretical\nupper bound on model bias for when the model transition function is estimated\nwith i.i.d. trajectories. This bound broadens our understanding of the\nconditions under which model-based methods have high bias. Finally, we\nempirically evaluate our proposed methods and analyze the settings in which\ndifferent bootstrapping off-policy confidence interval methods succeed and\nfail. \n\n"}
{"id": "1606.06361", "contents": "Title: A Probabilistic Generative Grammar for Semantic Parsing Abstract: Domain-general semantic parsing is a long-standing goal in natural language\nprocessing, where the semantic parser is capable of robustly parsing sentences\nfrom domains outside of which it was trained. Current approaches largely rely\non additional supervision from new domains in order to generalize to those\ndomains. We present a generative model of natural language utterances and\nlogical forms and demonstrate its application to semantic parsing. Our approach\nrelies on domain-independent supervision to generalize to new domains. We\nderive and implement efficient algorithms for training, parsing, and sentence\ngeneration. The work relies on a novel application of hierarchical Dirichlet\nprocesses (HDPs) for structured prediction, which we also present in this\nmanuscript.\n  This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov\n(2022), where the model plays a central role in a larger natural language\nunderstanding system.\n  This manuscript provides a new simplified and more complete presentation of\nthe work first introduced in Saparov, Saraswat, and Mitchell (2017). The\ndescription and proofs of correctness of the training algorithm, parsing\nalgorithm, and sentence generation algorithm are much simplified in this new\npresentation. We also describe the novel application of hierarchical Dirichlet\nprocesses for structured prediction. In addition, we extend the earlier work\nwith a new model of word morphology, which utilizes the comprehensive\nmorphological data from Wiktionary. \n\n"}
{"id": "1606.06962", "contents": "Title: Towards stationary time-vertex signal processing Abstract: Graph-based methods for signal processing have shown promise for the analysis\nof data exhibiting irregular structure, such as those found in social,\ntransportation, and sensor networks. Yet, though these systems are often\ndynamic, state-of-the-art methods for signal processing on graphs ignore the\ndimension of time, treating successive graph signals independently or taking a\nglobal average. To address this shortcoming, this paper considers the\nstatistical analysis of time-varying graph signals. We introduce a novel\ndefinition of joint (time-vertex) stationarity, which generalizes the classical\ndefinition of time stationarity and the more recent definition appropriate for\ngraphs. Joint stationarity gives rise to a scalable Wiener optimization\nframework for joint denoising, semi-supervised learning, or more generally\ninversing a linear operator, that is provably optimal. Experimental results on\nreal weather data demonstrate that taking into account graph and time\ndimensions jointly can yield significant accuracy improvements in the\nreconstruction effort. \n\n"}
{"id": "1606.07312", "contents": "Title: Unsupervised preprocessing for Tactile Data Abstract: Tactile information is important for gripping, stable grasp, and in-hand\nmanipulation, yet the complexity of tactile data prevents widespread use of\nsuch sensors. We make use of an unsupervised learning algorithm that transforms\nthe complex tactile data into a compact, latent representation without the need\nto record ground truth reference data. These compact representations can either\nbe used directly in a reinforcement learning based controller or can be used to\ncalibrate the tactile sensor to physical quantities with only a few datapoints.\nWe show the quality of our latent representation by predicting important\nfeatures and with a simple control task. \n\n"}
{"id": "1606.07326", "contents": "Title: DropNeuron: Simplifying the Structure of Deep Neural Networks Abstract: Deep learning using multi-layer neural networks (NNs) architecture manifests\nsuperb power in modern machine learning systems. The trained Deep Neural\nNetworks (DNNs) are typically large. The question we would like to address is\nwhether it is possible to simplify the NN during training process to achieve a\nreasonable performance within an acceptable computational time. We presented a\nnovel approach of optimising a deep neural network through regularisation of\nnet- work architecture. We proposed regularisers which support a simple\nmechanism of dropping neurons during a network training process. The method\nsupports the construction of a simpler deep neural networks with compatible\nperformance with its simplified version. As a proof of concept, we evaluate the\nproposed method with examples including sparse linear regression, deep\nautoencoder and convolutional neural network. The valuations demonstrate\nexcellent performance.\n  The code for this work can be found in\nhttp://www.github.com/panweihit/DropNeuron \n\n"}
{"id": "1606.09375", "contents": "Title: Convolutional Neural Networks on Graphs with Fast Localized Spectral\n  Filtering Abstract: In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs. \n\n"}
{"id": "1606.09458", "contents": "Title: Vote-boosting ensembles Abstract: Vote-boosting is a sequential ensemble learning method in which the\nindividual classifiers are built on different weighted versions of the training\ndata. To build a new classifier, the weight of each training instance is\ndetermined in terms of the degree of disagreement among the current ensemble\npredictions for that instance. For low class-label noise levels, especially\nwhen simple base learners are used, emphasis should be made on instances for\nwhich the disagreement rate is high. When more flexible classifiers are used\nand as the noise level increases, the emphasis on these uncertain instances\nshould be reduced. In fact, at sufficiently high levels of class-label noise,\nthe focus should be on instances on which the ensemble classifiers agree. The\noptimal type of emphasis can be automatically determined using\ncross-validation. An extensive empirical analysis using the beta distribution\nas emphasis function illustrates that vote-boosting is an effective method to\ngenerate ensembles that are both accurate and robust. \n\n"}
{"id": "1607.00148", "contents": "Title: LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection Abstract: Mechanical devices such as engines, vehicles, aircrafts, etc., are typically\ninstrumented with numerous sensors to capture the behavior and health of the\nmachine. However, there are often external factors or variables which are not\ncaptured by sensors leading to time-series which are inherently unpredictable.\nFor instance, manual controls and/or unmonitored environmental conditions or\nload may lead to inherently unpredictable time-series. Detecting anomalies in\nsuch scenarios becomes challenging using standard approaches based on\nmathematical models that rely on stationarity, or prediction models that\nutilize prediction errors to detect anomalies. We propose a Long Short Term\nMemory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD)\nthat learns to reconstruct 'normal' time-series behavior, and thereafter uses\nreconstruction error to detect anomalies. We experiment with three publicly\navailable quasi predictable time-series datasets: power demand, space shuttle,\nand ECG, and two real-world engine datasets with both predictive and\nunpredictable behavior. We show that EncDec-AD is robust and can detect\nanomalies from predictable, unpredictable, periodic, aperiodic, and\nquasi-periodic time-series. Further, we show that EncDec-AD is able to detect\nanomalies from short time-series (length as small as 30) as well as long\ntime-series (length as large as 500). \n\n"}
{"id": "1607.02763", "contents": "Title: How to Allocate Resources For Features Acquisition? Abstract: We study classification problems where features are corrupted by noise and\nwhere the magnitude of the noise in each feature is influenced by the resources\nallocated to its acquisition. This is the case, for example, when multiple\nsensors share a common resource (power, bandwidth, attention, etc.). We develop\na method for computing the optimal resource allocation for a variety of\nscenarios and derive theoretical bounds concerning the benefit that may arise\nby non-uniform allocation. We further demonstrate the effectiveness of the\ndeveloped method in simulations. \n\n"}
{"id": "1607.03559", "contents": "Title: Fast Sampling for Strongly Rayleigh Measures with Application to\n  Determinantal Point Processes Abstract: In this note we consider sampling from (non-homogeneous) strongly Rayleigh\nprobability measures. As an important corollary, we obtain a fast mixing Markov\nChain sampler for Determinantal Point Processes. \n\n"}
{"id": "1607.06017", "contents": "Title: Doubly Accelerated Methods for Faster CCA and Generalized\n  Eigendecomposition Abstract: We study $k$-GenEV, the problem of finding the top $k$ generalized\neigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in\ncanonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and\n$\\mathtt{LazyCCA}$ to solve the two problems with running times linearly\ndependent on the input size and on $k$.\n  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend\nonly on the square root of the matrix condition number, and on the square root\nof the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.\nWe also provide the first gap-free results, which provide running times that\ndepend on $1/\\sqrt{\\varepsilon}$ rather than the eigengap. \n\n"}
{"id": "1607.08254", "contents": "Title: Stochastic Frank-Wolfe Methods for Nonconvex Optimization Abstract: We study Frank-Wolfe methods for nonconvex stochastic and finite-sum\noptimization problems. Frank-Wolfe methods (in the convex case) have gained\ntremendous recent interest in machine learning and optimization communities due\nto their projection-free property and their ability to exploit structured\nconstraints. However, our understanding of these algorithms in the nonconvex\nsetting is fairly limited. In this paper, we propose nonconvex stochastic\nFrank-Wolfe methods and analyze their convergence properties. For objective\nfunctions that decompose into a finite-sum, we leverage ideas from variance\nreduction techniques for convex optimization to obtain new variance reduced\nnonconvex Frank-Wolfe methods that have provably faster convergence than the\nclassical Frank-Wolfe method. Finally, we show that the faster convergence\nrates of our variance reduced methods also translate into improved convergence\nrates for the stochastic setting. \n\n"}
{"id": "1608.04414", "contents": "Title: Generalization of ERM in Stochastic Convex Optimization: The Dimension\n  Strikes Back Abstract: In stochastic convex optimization the goal is to minimize a convex function\n$F(x) \\doteq {\\mathbf E}_{{\\mathbf f}\\sim D}[{\\mathbf f}(x)]$ over a convex set\n$\\cal K \\subset {\\mathbb R}^d$ where $D$ is some unknown distribution and each\n$f(\\cdot)$ in the support of $D$ is convex over $\\cal K$. The optimization is\ncommonly based on i.i.d.~samples $f^1,f^2,\\ldots,f^n$ from $D$. A standard\napproach to such problems is empirical risk minimization (ERM) that optimizes\n$F_S(x) \\doteq \\frac{1}{n}\\sum_{i\\leq n} f^i(x)$. Here we consider the question\nof how many samples are necessary for ERM to succeed and the closely related\nquestion of uniform convergence of $F_S$ to $F$ over $\\cal K$. We demonstrate\nthat in the standard $\\ell_p/\\ell_q$ setting of Lipschitz-bounded functions\nover a $\\cal K$ of bounded radius, ERM requires sample size that scales\nlinearly with the dimension $d$. This nearly matches standard upper bounds and\nimproves on $\\Omega(\\log d)$ dependence proved for $\\ell_2/\\ell_2$ setting by\nShalev-Shwartz et al. (2009). In stark contrast, these problems can be solved\nusing dimension-independent number of samples for $\\ell_2/\\ell_2$ setting and\n$\\log d$ dependence for $\\ell_1/\\ell_\\infty$ setting using other approaches. We\nfurther show that our lower bound applies even if the functions in the support\nof $D$ are smooth and efficiently computable and even if an $\\ell_1$\nregularization term is added. Finally, we demonstrate that for a more general\nclass of bounded-range (but not Lipschitz-bounded) stochastic convex programs\nan infinite gap appears already in dimension 2. \n\n"}
{"id": "1608.04478", "contents": "Title: A Geometrical Approach to Topic Model Estimation Abstract: In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data. \n\n"}
{"id": "1608.04783", "contents": "Title: Application of multiview techniques to NHANES dataset Abstract: Disease prediction or classification using health datasets involve using\nwell-known predictors associated with the disease as features for the models.\nThis study considers multiple data components of an individual's health, using\nthe relationship between variables to generate features that may improve the\nperformance of disease classification models. In order to capture information\nfrom different aspects of the data, this project uses a multiview learning\napproach, using Canonical Correlation Analysis (CCA), a technique that finds\nprojections with maximum correlations between two data views. Data categories\ncollected from the NHANES survey (1999-2014) are used as views to learn the\nmultiview representations. The usefulness of the representations is\ndemonstrated by applying them as features in a Diabetes classification task. \n\n"}
{"id": "1608.05182", "contents": "Title: A Bayesian Nonparametric Approach for Estimating Individualized\n  Treatment-Response Curves Abstract: We study the problem of estimating the continuous response over time to\ninterventions using observational time series---a retrospective dataset where\nthe policy by which the data are generated is unknown to the learner. We are\nmotivated by applications where response varies by individuals and therefore,\nestimating responses at the individual-level is valuable for personalizing\ndecision-making. We refer to this as the problem of estimating individualized\ntreatment response (ITR) curves. In statistics, G-computation formula (Robins,\n1986) has been commonly used for estimating treatment responses from\nobservational data containing sequential treatment assignments. However, past\nstudies have focused predominantly on obtaining point-in-time estimates at the\npopulation level. We leverage the G-computation formula and develop a novel\nBayesian nonparametric (BNP) method that can flexibly model functional data and\nprovide posterior inference over the treatment response curves at both the\nindividual and population level. On a challenging dataset containing time\nseries from patients admitted to a hospital, we estimate responses to\ntreatments used in managing kidney function and show that the resulting fits\nare more accurate than alternative approaches. Accurate methods for obtaining\nITRs from observational data can dramatically accelerate the pace at which\npersonalized treatment plans become possible. \n\n"}
{"id": "1608.05983", "contents": "Title: Inverting Variational Autoencoders for Improved Generative Accuracy Abstract: Recent advances in semi-supervised learning with deep generative models have\nshown promise in generalizing from small labeled datasets\n($\\mathbf{x},\\mathbf{y}$) to large unlabeled ones ($\\mathbf{x}$). In the case\nwhere the codomain has known structure, a large unfeatured dataset\n($\\mathbf{y}$) is potentially available. We develop a parameter-efficient, deep\nsemi-supervised generative model for the purpose of exploiting this untapped\ndata source. Empirical results show improved performance in disentangling\nlatent variable semantics as well as improved discriminative prediction on\nMartian spectroscopic and handwritten digit domains. \n\n"}
{"id": "1608.07019", "contents": "Title: Comparison among dimensionality reduction techniques based on Random\n  Projection for cancer classification Abstract: Random Projection (RP) technique has been widely applied in many scenarios\nbecause it can reduce high-dimensional features into low-dimensional space\nwithin short time and meet the need of real-time analysis of massive data.\nThere is an urgent need of dimensionality reduction with fast increase of big\ngenomics data. However, the performance of RP is usually lower. We attempt to\nimprove classification accuracy of RP through combining other reduction\ndimension methods such as Principle Component Analysis (PCA), Linear\nDiscriminant Analysis (LDA), and Feature Selection (FS). We compared\nclassification accuracy and running time of different combination methods on\nthree microarray datasets and a simulation dataset. Experimental results show a\nremarkable improvement of 14.77% in classification accuracy of FS followed by\nRP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield\na more discriminative subspace with an increase of 13.65% on classification\naccuracy on the same dataset. FS followed by RP outperforms other combination\nmethods in classification accuracy on most of the datasets. \n\n"}
{"id": "1609.00904", "contents": "Title: High Dimensional Human Guided Machine Learning Abstract: Have you ever looked at a machine learning classification model and thought,\nI could have made that? Well, that is what we test in this project, comparing\nXGBoost trained on human engineered features to training directly on data. The\nhuman engineered features do not outperform XGBoost trained di- rectly on the\ndata, but they are comparable. This project con- tributes a novel method for\nutilizing human created classifi- cation models on high dimensional datasets. \n\n"}
{"id": "1609.01226", "contents": "Title: The Robustness of Estimator Composition Abstract: We formalize notions of robustness for composite estimators via the notion of\na breakdown point. A composite estimator successively applies two (or more)\nestimators: on data decomposed into disjoint parts, it applies the first\nestimator on each part, then the second estimator on the outputs of the first\nestimator. And so on, if the composition is of more than two estimators.\nInformally, the breakdown point is the minimum fraction of data points which if\nsignificantly modified will also significantly modify the output of the\nestimator, so it is typically desirable to have a large breakdown point. Our\nmain result shows that, under mild conditions on the individual estimators, the\nbreakdown point of the composite estimator is the product of the breakdown\npoints of the individual estimators. We also demonstrate several scenarios,\nranging from regression to statistical testing, where this analysis is easy to\napply, useful in understanding worst case robustness, and sheds powerful\ninsights onto the associated data analysis. \n\n"}
{"id": "1609.02487", "contents": "Title: Non-Backtracking Spectrum of Degree-Corrected Stochastic Block Models Abstract: Motivated by community detection, we characterise the spectrum of the\nnon-backtracking matrix $B$ in the Degree-Corrected Stochastic Block Model.\n  Specifically, we consider a random graph on $n$ vertices partitioned into two\nequal-sized clusters. The vertices have i.i.d. weights $\\{ \\phi_u \\}_{u=1}^n$\nwith second moment $\\Phi^{(2)}$. The intra-cluster connection probability for\nvertices $u$ and $v$ is $\\frac{\\phi_u \\phi_v}{n}a$ and the inter-cluster\nconnection probability is $\\frac{\\phi_u \\phi_v}{n}b$.\n  We show that with high probability, the following holds: The leading\neigenvalue of the non-backtracking matrix $B$ is asymptotic to $\\rho =\n\\frac{a+b}{2} \\Phi^{(2)}$. The second eigenvalue is asymptotic to $\\mu_2 =\n\\frac{a-b}{2} \\Phi^{(2)}$ when $\\mu_2^2 > \\rho$, but asymptotically bounded by\n$\\sqrt{\\rho}$ when $\\mu_2^2 \\leq \\rho$. All the remaining eigenvalues are\nasymptotically bounded by $\\sqrt{\\rho}$. As a result, a clustering\npositively-correlated with the true communities can be obtained based on the\nsecond eigenvector of $B$ in the regime where $\\mu_2^2 > \\rho.$\n  In a previous work we obtained that detection is impossible when $\\mu_2^2 <\n\\rho,$ meaning that there occurs a phase-transition in the sparse regime of the\nDegree-Corrected Stochastic Block Model.\n  As a corollary, we obtain that Degree-Corrected Erd\\H{o}s-R\\'enyi graphs\nasymptotically satisfy the graph Riemann hypothesis, a quasi-Ramanujan\nproperty.\n  A by-product of our proof is a weak law of large numbers for\nlocal-functionals on Degree-Corrected Stochastic Block Models, which could be\nof independent interest. \n\n"}
{"id": "1609.02487", "contents": "Title: Non-Backtracking Spectrum of Degree-Corrected Stochastic Block Models Abstract: Motivated by community detection, we characterise the spectrum of the\nnon-backtracking matrix $B$ in the Degree-Corrected Stochastic Block Model.\n  Specifically, we consider a random graph on $n$ vertices partitioned into two\nequal-sized clusters. The vertices have i.i.d. weights $\\{ \\phi_u \\}_{u=1}^n$\nwith second moment $\\Phi^{(2)}$. The intra-cluster connection probability for\nvertices $u$ and $v$ is $\\frac{\\phi_u \\phi_v}{n}a$ and the inter-cluster\nconnection probability is $\\frac{\\phi_u \\phi_v}{n}b$.\n  We show that with high probability, the following holds: The leading\neigenvalue of the non-backtracking matrix $B$ is asymptotic to $\\rho =\n\\frac{a+b}{2} \\Phi^{(2)}$. The second eigenvalue is asymptotic to $\\mu_2 =\n\\frac{a-b}{2} \\Phi^{(2)}$ when $\\mu_2^2 > \\rho$, but asymptotically bounded by\n$\\sqrt{\\rho}$ when $\\mu_2^2 \\leq \\rho$. All the remaining eigenvalues are\nasymptotically bounded by $\\sqrt{\\rho}$. As a result, a clustering\npositively-correlated with the true communities can be obtained based on the\nsecond eigenvector of $B$ in the regime where $\\mu_2^2 > \\rho.$\n  In a previous work we obtained that detection is impossible when $\\mu_2^2 <\n\\rho,$ meaning that there occurs a phase-transition in the sparse regime of the\nDegree-Corrected Stochastic Block Model.\n  As a corollary, we obtain that Degree-Corrected Erd\\H{o}s-R\\'enyi graphs\nasymptotically satisfy the graph Riemann hypothesis, a quasi-Ramanujan\nproperty.\n  A by-product of our proof is a weak law of large numbers for\nlocal-functionals on Degree-Corrected Stochastic Block Models, which could be\nof independent interest. \n\n"}
{"id": "1609.02613", "contents": "Title: Why is Differential Evolution Better than Grid Search for Tuning Defect\n  Predictors? Abstract: Context: One of the black arts of data mining is learning the magic\nparameters which control the learners. In software analytics, at least for\ndefect prediction, several methods, like grid search and differential evolution\n(DE), have been proposed to learn these parameters, which has been proved to be\nable to improve the performance scores of learners.\n  Objective: We want to evaluate which method can find better parameters in\nterms of performance score and runtime cost.\n  Methods: This paper compares grid search to differential evolution, which is\nan evolutionary algorithm that makes extensive use of stochastic jumps around\nthe search space.\n  Results: We find that the seemingly complete approach of grid search does no\nbetter, and sometimes worse, than the stochastic search. When repeated 20 times\nto check for conclusion validity, DE was over 210 times faster than grid search\nto tune Random Forests on 17 testing data sets with F-Measure\n  Conclusions: These results are puzzling: why does a quick partial search be\njust as effective as a much slower, and much more, extensive search? To answer\nthat question, we turned to the theoretical optimization literature. Bergstra\nand Bengio conjecture that grid search is not more effective than more\nrandomized searchers if the underlying search space is inherently low\ndimensional. This is significant since recent results show that defect\nprediction exhibits very low intrinsic dimensionality-- an observation that\nexplains why a fast method like DE may work as well as a seemingly more\nthorough grid search. This suggests, as a future research direction, that it\nmight be possible to peek at data sets before doing any optimization in order\nto match the optimization algorithm to the problem at hand. \n\n"}
{"id": "1609.02845", "contents": "Title: Distributed Online Optimization in Dynamic Environments Using Mirror\n  Descent Abstract: This work addresses decentralized online optimization in non-stationary\nenvironments. A network of agents aim to track the minimizer of a global\ntime-varying convex function. The minimizer evolves according to a known\ndynamics corrupted by an unknown, unstructured noise. At each time, the global\nfunction can be cast as a sum of a finite number of local functions, each of\nwhich is assigned to one agent in the network. Moreover, the local functions\nbecome available to agents sequentially, and agents do not have a prior\nknowledge of the future cost functions. Therefore, agents must communicate with\neach other to build an online approximation of the global function. We propose\na decentralized variation of the celebrated Mirror Descent, developed by\nNemirovksi and Yudin. Using the notion of Bregman divergence in lieu of\nEuclidean distance for projection, Mirror Descent has been shown to be a\npowerful tool in large-scale optimization. Our algorithm builds on Mirror\nDescent, while ensuring that agents perform a consensus step to follow the\nglobal function and take into account the dynamics of the global minimizer. To\nmeasure the performance of the proposed online algorithm, we compare it to its\noffline counterpart, where the global functions are available a priori. The gap\nbetween the two is called dynamic regret. We establish a regret bound that\nscales inversely in the spectral gap of the network, and more notably it\nrepresents the deviation of minimizer sequence with respect to the given\ndynamics. We then show that our results subsume a number of results in\ndistributed optimization. We demonstrate the application of our method to\ndecentralized tracking of dynamic parameters and verify the results via\nnumerical experiments. \n\n"}
{"id": "1609.03261", "contents": "Title: Less than a Single Pass: Stochastically Controlled Stochastic Gradient\n  Method Abstract: We develop and analyze a procedure for gradient-based optimization that we\nrefer to as stochastically controlled stochastic gradient (SCSG). As a member\nof the SVRG family of algorithms, SCSG makes use of gradient estimates at two\nscales, with the number of updates at the faster scale being governed by a\ngeometric random variable. Unlike most existing algorithms in this family, both\nthe computation cost and the communication cost of SCSG do not necessarily\nscale linearly with the sample size $n$; indeed, these costs are independent of\n$n$ when the target accuracy is low. An experimental evaluation on real\ndatasets confirms the effectiveness of SCSG. \n\n"}
{"id": "1609.03677", "contents": "Title: Unsupervised Monocular Depth Estimation with Left-Right Consistency Abstract: Learning based methods have shown very promising results for the task of\ndepth estimation in single images. However, most existing approaches treat\ndepth prediction as a supervised regression problem and as a result, require\nvast quantities of corresponding ground truth depth data for training. Just\nrecording quality depth data in a range of environments is a challenging\nproblem. In this paper, we innovate beyond existing approaches, replacing the\nuse of explicit depth data during training with easier-to-obtain binocular\nstereo footage.\n  We propose a novel training objective that enables our convolutional neural\nnetwork to learn to perform single image depth estimation, despite the absence\nof ground truth depth data. Exploiting epipolar geometry constraints, we\ngenerate disparity images by training our network with an image reconstruction\nloss. We show that solving for image reconstruction alone results in poor\nquality depth images. To overcome this problem, we propose a novel training\nloss that enforces consistency between the disparities produced relative to\nboth the left and right images, leading to improved performance and robustness\ncompared to existing approaches. Our method produces state of the art results\nfor monocular depth estimation on the KITTI driving dataset, even outperforming\nsupervised methods that have been trained with ground truth depth. \n\n"}
{"id": "1609.06457", "contents": "Title: AMOS: An Automated Model Order Selection Algorithm for Spectral Graph\n  Clustering Abstract: One of the longstanding problems in spectral graph clustering (SGC) is the\nso-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. In this paper, we\npropose AMOS, an automated model order selection algorithm for SGC. Based on a\nrecent analysis of clustering reliability for SGC under the random\ninterconnection model, AMOS works by incrementally increasing the number of\nclusters, estimating the quality of identified clusters, and providing a series\nof clustering reliability tests. Consequently, AMOS outputs clusters of minimal\nmodel order with statistical clustering reliability guarantees. Comparing to\nthree other automated graph clustering methods on real-world datasets, AMOS\nshows superior performance in terms of multiple external and internal\nclustering metrics. \n\n"}
{"id": "1609.06826", "contents": "Title: Bibliographic Analysis with the Citation Network Topic Model Abstract: Bibliographic analysis considers author's research areas, the citation\nnetwork and paper content among other things. In this paper, we combine these\nthree in a topic model that produces a bibliographic model of authors, topics\nand documents using a non-parametric extension of a combination of the Poisson\nmixed-topic link model and the author-topic model. We propose a novel and\nefficient inference algorithm for the model to explore subsets of research\npublications from CiteSeerX. Our model demonstrates improved performance in\nboth model fitting and a clustering task compared to several baselines. \n\n"}
{"id": "1609.06957", "contents": "Title: Early Warning System for Seismic Events in Coal Mines Using Machine\n  Learning Abstract: This document describes an approach to the problem of predicting dangerous\nseismic events in active coal mines up to 8 hours in advance. It was developed\nas a part of the AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic\nEvents in Active Coal Mines. The solutions presented consist of ensembles of\nvarious predictive models trained on different sets of features. The best one\nachieved a winning score of 0.939 AUC. \n\n"}
{"id": "1609.07200", "contents": "Title: Multilayer Spectral Graph Clustering via Convex Layer Aggregation Abstract: Multilayer graphs are commonly used for representing different relations\nbetween entities and handling heterogeneous data processing tasks. New\nchallenges arise in multilayer graph clustering for assigning clusters to a\ncommon multilayer node set and for combining information from each layer. This\npaper presents a theoretical framework for multilayer spectral graph clustering\nof the nodes via convex layer aggregation. Under a novel multilayer signal plus\nnoise model, we provide a phase transition analysis that establishes the\nexistence of a critical value on the noise level that permits reliable cluster\nseparation. The analysis also specifies analytical upper and lower bounds on\nthe critical value, where the bounds become exact when the clusters have\nidentical sizes. Numerical experiments on synthetic multilayer graphs are\nconducted to validate the phase transition analysis and study the effect of\nlayer weights and noise levels on clustering reliability. \n\n"}
{"id": "1609.09430", "contents": "Title: CNN Architectures for Large-Scale Audio Classification Abstract: Convolutional Neural Networks (CNNs) have proven very effective in image\nclassification and show promise for audio. We use various CNN architectures to\nclassify the soundtracks of a dataset of 70M training videos (5.24 million\nhours) with 30,871 video-level labels. We examine fully connected Deep Neural\nNetworks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We\ninvestigate varying the size of both training set and label vocabulary, finding\nthat analogs of the CNNs used in image classification do well on our audio\nclassification task, and larger training and label sets help up to a point. A\nmodel using embeddings from these classifiers does much better than raw\nfeatures on the Audio Set [5] Acoustic Event Detection (AED) classification\ntask. \n\n"}
{"id": "1609.09471", "contents": "Title: Classifier comparison using precision Abstract: New proposed models are often compared to state-of-the-art using statistical\nsignificance testing. Literature is scarce for classifier comparison using\nmetrics other than accuracy. We present a survey of statistical methods that\ncan be used for classifier comparison using precision, accounting for\ninter-precision correlation arising from use of same dataset. Comparisons are\nmade using per-class precision and methods presented to test global null\nhypothesis of an overall model comparison. Comparisons are extended to multiple\nmulti-class classifiers and to models using cross validation or its variants.\nPartial Bayesian update to precision is introduced when population prevalence\nof a class is known. Applications to compare deep architectures are studied. \n\n"}
{"id": "1610.04490", "contents": "Title: Amortised MAP Inference for Image Super-resolution Abstract: Image super-resolution (SR) is an underdetermined inverse problem, where a\nlarge number of plausible high-resolution images can explain the same\ndownsampled image. Most current single image SR methods use empirical risk\nminimisation, often with a pixel-wise mean squared error (MSE) loss. However,\nthe outputs from such methods tend to be blurry, over-smoothed and generally\nappear implausible. A more desirable approach would employ Maximum a Posteriori\n(MAP) inference, preferring solutions that always have a high probability under\nthe image prior, and thus appear more plausible. Direct MAP estimation for SR\nis non-trivial, as it requires us to build a model for the image prior from\nsamples. Furthermore, MAP inference is often performed via optimisation-based\niterative algorithms which don't compare well with the efficiency of\nneural-network-based alternatives. Here we introduce new methods for amortised\nMAP inference whereby we calculate the MAP estimate directly using a\nconvolutional neural network. We first introduce a novel neural network\narchitecture that performs a projection to the affine subspace of valid SR\nsolutions ensuring that the high resolution output of the network is always\nconsistent with the low resolution input. We show that, using this\narchitecture, the amortised MAP inference problem reduces to minimising the\ncross-entropy between two distributions, similar to training generative models.\nWe propose three methods to solve this optimisation problem: (1) Generative\nAdversarial Networks (GAN) (2) denoiser-guided SR which backpropagates\ngradient-estimates from denoising to train the network, and (3) a baseline\nmethod using a maximum-likelihood-trained image prior. Our experiments show\nthat the GAN based approach performs best on real image data. Lastly, we\nestablish a connection between GANs and amortised variational inference as in\ne.g. variational autoencoders. \n\n"}
{"id": "1610.05129", "contents": "Title: Risk-Aware Algorithms for Adversarial Contextual Bandits Abstract: In this work we consider adversarial contextual bandits with risk\nconstraints. At each round, nature prepares a context, a cost for each arm, and\nadditionally a risk for each arm. The learner leverages the context to pull an\narm and then receives the corresponding cost and risk associated with the\npulled arm. In addition to minimizing the cumulative cost, the learner also\nneeds to satisfy long-term risk constraints -- the average of the cumulative\nrisk from all pulled arms should not be larger than a pre-defined threshold. To\naddress this problem, we first study the full information setting where in each\nround the learner receives an adversarial convex loss and a convex constraint.\nWe develop a meta algorithm leveraging online mirror descent for the full\ninformation setting and extend it to contextual bandit with risk constraints\nsetting using expert advice. Our algorithms can achieve near-optimal regret in\nterms of minimizing the total cost, while successfully maintaining a sublinear\ngrowth of cumulative risk constraint violation. \n\n"}
{"id": "1610.06940", "contents": "Title: Safety Verification of Deep Neural Networks Abstract: Deep neural networks have achieved impressive experimental results in image\nclassification, but can surprisingly be unstable with respect to adversarial\nperturbations, that is, minimal changes to the input image that cause the\nnetwork to misclassify it. With potential applications including perception\nmodules and end-to-end controllers for self-driving cars, this raises concerns\nabout their safety. We develop a novel automated verification framework for\nfeed-forward multi-layer neural networks based on Satisfiability Modulo Theory\n(SMT). We focus on safety of image classification decisions with respect to\nimage manipulations, such as scratches or changes to camera angle or lighting\nconditions that would result in the same class being assigned by a human, and\ndefine safety for an individual decision in terms of invariance of the\nclassification within a small neighbourhood of the original image. We enable\nexhaustive search of the region by employing discretisation, and propagate the\nanalysis layer by layer. Our method works directly with the network code and,\nin contrast to existing methods, can guarantee that adversarial examples, if\nthey exist, are found for the given region and family of manipulations. If\nfound, adversarial examples can be shown to human testers and/or used to\nfine-tune the network. We implement the techniques using Z3 and evaluate them\non state-of-the-art networks, including regularised and deep learning networks.\nWe also compare against existing techniques to search for adversarial examples\nand estimate network robustness. \n\n"}
{"id": "1610.07569", "contents": "Title: Geometry of Polysemy Abstract: Vector representations of words have heralded a transformational approach to\nclassical problems in NLP; the most popular example is word2vec. However, a\nsingle vector does not suffice to model the polysemous nature of many\n(frequent) words, i.e., words with multiple meanings. In this paper, we propose\na three-fold approach for unsupervised polysemy modeling: (a) context\nrepresentations, (b) sense induction and disambiguation and (c) lexeme (as a\nword and sense pair) representations. A key feature of our work is the finding\nthat a sentence containing a target word is well represented by a low rank\nsubspace, instead of a point in a vector space. We then show that the subspaces\nassociated with a particular sense of the target word tend to intersect over a\nline (one-dimensional subspace), which we use to disambiguate senses using a\nclustering algorithm that harnesses the Grassmannian geometry of the\nrepresentations. The disambiguation algorithm, which we call $K$-Grassmeans,\nleads to a procedure to label the different senses of the target word in the\ncorpus -- yielding lexeme vector representations, all in an unsupervised manner\nstarting from a large (Wikipedia) corpus in English. Apart from several\nprototypical target (word,sense) examples and a host of empirical studies to\nintuit and justify the various geometric representations, we validate our\nalgorithms on standard sense induction and disambiguation datasets and present\nnew state-of-the-art results. \n\n"}
{"id": "1610.08738", "contents": "Title: Compressive K-means Abstract: The Lloyd-Max algorithm is a classical approach to perform K-means\nclustering. Unfortunately, its cost becomes prohibitive as the training dataset\ngrows large. We propose a compressive version of K-means (CKM), that estimates\ncluster centers from a sketch, i.e. from a drastically compressed\nrepresentation of the training dataset. We demonstrate empirically that CKM\nperforms similarly to Lloyd-Max, for a sketch size proportional to the number\nof cen-troids times the ambient dimension, and independent of the size of the\noriginal dataset. Given the sketch, the computational complexity of CKM is also\nindependent of the size of the dataset. Unlike Lloyd-Max which requires several\nreplicates, we further demonstrate that CKM is almost insensitive to\ninitialization. For a large dataset of 10^7 data points, we show that CKM can\nrun two orders of magnitude faster than five replicates of Lloyd-Max, with\nsimilar clustering performance on artificial data. Finally, CKM achieves lower\nclassification errors on handwritten digits classification. \n\n"}
{"id": "1611.00350", "contents": "Title: Adversarial Influence Maximization Abstract: We consider the problem of influence maximization in fixed networks for\ncontagion models in an adversarial setting. The goal is to select an optimal\nset of nodes to seed the influence process, such that the number of influenced\nnodes at the conclusion of the campaign is as large as possible. We formulate\nthe problem as a repeated game between a player and adversary, where the\nadversary specifies the edges along which the contagion may spread, and the\nplayer chooses sets of nodes to influence in an online fashion. We establish\nupper and lower bounds on the minimax pseudo-regret in both undirected and\ndirected networks. \n\n"}
{"id": "1611.00938", "contents": "Title: Fast Eigenspace Approximation using Random Signals Abstract: We focus in this work on the estimation of the first $k$ eigenvectors of any\ngraph Laplacian using filtering of Gaussian random signals. We prove that we\nonly need $k$ such signals to be able to exactly recover as many of the\nsmallest eigenvectors, regardless of the number of nodes in the graph. In\naddition, we address key issues in implementing the theoretical concepts in\npractice using accurate approximated methods. We also propose fast algorithms\nboth for eigenspace approximation and for the determination of the $k$th\nsmallest eigenvalue $\\lambda_k$. The latter proves to be extremely efficient\nunder the assumption of locally uniform distribution of the eigenvalue over the\nspectrum. Finally, we present experiments which show the validity of our method\nin practice and compare it to state-of-the-art methods for clustering and\nvisualization both on synthetic small-scale datasets and larger real-world\nproblems of millions of nodes. We show that our method allows a better scaling\nwith the number of nodes than all previous methods while achieving an almost\nperfect reconstruction of the eigenspace formed by the first $k$ eigenvectors. \n\n"}
{"id": "1611.00938", "contents": "Title: Fast Eigenspace Approximation using Random Signals Abstract: We focus in this work on the estimation of the first $k$ eigenvectors of any\ngraph Laplacian using filtering of Gaussian random signals. We prove that we\nonly need $k$ such signals to be able to exactly recover as many of the\nsmallest eigenvectors, regardless of the number of nodes in the graph. In\naddition, we address key issues in implementing the theoretical concepts in\npractice using accurate approximated methods. We also propose fast algorithms\nboth for eigenspace approximation and for the determination of the $k$th\nsmallest eigenvalue $\\lambda_k$. The latter proves to be extremely efficient\nunder the assumption of locally uniform distribution of the eigenvalue over the\nspectrum. Finally, we present experiments which show the validity of our method\nin practice and compare it to state-of-the-art methods for clustering and\nvisualization both on synthetic small-scale datasets and larger real-world\nproblems of millions of nodes. We show that our method allows a better scaling\nwith the number of nodes than all previous methods while achieving an almost\nperfect reconstruction of the eigenspace formed by the first $k$ eigenvectors. \n\n"}
{"id": "1611.01456", "contents": "Title: Learning heat diffusion graphs Abstract: Effective information analysis generally boils down to properly identifying\nthe structure or geometry of the data, which is often represented by a graph.\nIn some applications, this structure may be partly determined by design\nconstraints or pre-determined sensing arrangements, like in road transportation\nnetworks for example. In general though, the data structure is not readily\navailable and becomes pretty difficult to define. In particular, the global\nsmoothness assumptions, that most of the existing works adopt, are often too\ngeneral and unable to properly capture localized properties of data. In this\npaper, we go beyond this classical data model and rather propose to represent\ninformation as a sparse combination of localized functions that live on a data\nstructure represented by a graph. Based on this model, we focus on the problem\nof inferring the connectivity that best explains the data samples at different\nvertices of a graph that is a priori unknown. We concentrate on the case where\nthe observed data is actually the sum of heat diffusion processes, which is a\nquite common model for data on networks or other irregular structures. We cast\na new graph learning problem and solve it with an efficient nonconvex\noptimization algorithm. Experiments on both synthetic and real world data\nfinally illustrate the benefits of the proposed graph learning framework and\nconfirm that the data structure can be efficiently learned from data\nobservations only. We believe that our algorithm will help solving key\nquestions in diverse application domains such as social and biological network\nanalysis where it is crucial to unveil proper geometry for data understanding\nand inference. \n\n"}
{"id": "1611.03777", "contents": "Title: Tricks from Deep Learning Abstract: The deep learning community has devised a diverse set of methods to make\ngradient optimization, using large datasets, of large and highly complex models\nwith deeply cascaded nonlinearities, practical. Taken as a whole, these methods\nconstitute a breakthrough, allowing computational structures which are quite\nwide, very deep, and with an enormous number and variety of free parameters to\nbe effectively optimized. The result now dominates much of practical machine\nlearning, with applications in machine translation, computer vision, and speech\nrecognition. Many of these methods, viewed through the lens of algorithmic\ndifferentiation (AD), can be seen as either addressing issues with the gradient\nitself, or finding ways of achieving increased efficiency using tricks that are\nAD-related, but not provided by current AD systems.\n  The goal of this paper is to explain not just those methods of most relevance\nto AD, but also the technical constraints and mindset which led to their\ndiscovery. After explaining this context, we present a \"laundry list\" of\nmethods developed by the deep learning community. Two of these are discussed in\nfurther mathematical detail: a way to dramatically reduce the size of the tape\nwhen performing reverse-mode AD on a (theoretically) time-reversible process\nlike an ODE integrator; and a new mathematical insight that allows for the\nimplementation of a stochastic Newton's method. \n\n"}
{"id": "1611.04835", "contents": "Title: Multilinear Low-Rank Tensors on Graphs & Applications Abstract: We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework. \n\n"}
{"id": "1611.05181", "contents": "Title: Graph Learning from Data under Structural and Laplacian Constraints Abstract: Graphs are fundamental mathematical structures used in various fields to\nrepresent data, signals and processes. In this paper, we propose a novel\nframework for learning/estimating graphs from data. The proposed framework\nincludes (i) formulation of various graph learning problems, (ii) their\nprobabilistic interpretations and (iii) associated algorithms. Specifically,\ngraph learning problems are posed as estimation of graph Laplacian matrices\nfrom some observed data under given structural constraints (e.g., graph\nconnectivity and sparsity level). From a probabilistic perspective, the\nproblems of interest correspond to maximum a posteriori (MAP) parameter\nestimation of Gaussian-Markov random field (GMRF) models, whose precision\n(inverse covariance) is a graph Laplacian matrix. For the proposed graph\nlearning problems, specialized algorithms are developed by incorporating the\ngraph Laplacian and structural constraints. The experimental results\ndemonstrate that the proposed algorithms outperform the current\nstate-of-the-art methods in terms of accuracy and computational efficiency. \n\n"}
{"id": "1611.06440", "contents": "Title: Pruning Convolutional Neural Networks for Resource Efficient Inference Abstract: We propose a new formulation for pruning convolutional kernels in neural\nnetworks to enable efficient inference. We interleave greedy criteria-based\npruning with fine-tuning by backpropagation - a computationally efficient\nprocedure that maintains good generalization in the pruned network. We propose\na new criterion based on Taylor expansion that approximates the change in the\ncost function induced by pruning network parameters. We focus on transfer\nlearning, where large pretrained networks are adapted to specialized tasks. The\nproposed criterion demonstrates superior performance compared to other\ncriteria, e.g. the norm of kernel weights or feature map activation, for\npruning large CNNs after adaptation to fine-grained classification tasks\n(Birds-200 and Flowers-102) relaying only on the first order gradient\ninformation. We also show that pruning can lead to more than 10x theoretical\n(5x practical) reduction in adapted 3D-convolutional filters with a small drop\nin accuracy in a recurrent gesture classifier. Finally, we show results for the\nlarge-scale ImageNet dataset to emphasize the flexibility of our approach. \n\n"}
{"id": "1611.07012", "contents": "Title: GRAM: Graph-based Attention Model for Healthcare Representation Learning Abstract: Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts. \n\n"}
{"id": "1611.07725", "contents": "Title: iCaRL: Incremental Classifier and Representation Learning Abstract: A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail. \n\n"}
{"id": "1611.09207", "contents": "Title: AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech Abstract: Developers of text-to-speech synthesizers (TTS) often make use of human\nraters to assess the quality of synthesized speech. We demonstrate that we can\nmodel human raters' mean opinion scores (MOS) of synthesized speech using a\ndeep recurrent neural network whose inputs consist solely of a raw waveform.\nOur best models provide utterance-level estimates of MOS only moderately\ninferior to sampled human ratings, as shown by Pearson and Spearman\ncorrelations. When multiple utterances are scored and averaged, a scenario\ncommon in synthesizer quality assessment, AutoMOS achieves correlations\napproaching those of human raters. The AutoMOS model has a number of\napplications, such as the ability to explore the parameter space of a speech\nsynthesizer without requiring a human-in-the-loop. \n\n"}
{"id": "1611.09328", "contents": "Title: Accelerated Gradient Temporal Difference Learning Abstract: The family of temporal difference (TD) methods span a spectrum from\ncomputationally frugal linear methods like TD({\\lambda}) to data efficient\nleast squares methods. Least square methods make the best use of available data\ndirectly computing the TD solution and thus do not require tuning a typically\nhighly sensitive learning rate parameter, but require quadratic computation and\nstorage. Recent algorithmic developments have yielded several sub-quadratic\nmethods that use an approximation to the least squares TD solution, but incur\nbias. In this paper, we propose a new family of accelerated gradient TD (ATD)\nmethods that (1) provide similar data efficiency benefits to least-squares\nmethods, at a fraction of the computation and storage (2) significantly reduce\nparameter sensitivity compared to linear TD methods, and (3) are asymptotically\nunbiased. We illustrate these claims with a proof of convergence in expectation\nand experiments on several benchmark domains and a large-scale industrial\nenergy allocation domain. \n\n"}
{"id": "1611.09630", "contents": "Title: Improving Variational Auto-Encoders using Householder Flow Abstract: Variational auto-encoders (VAE) are scalable and powerful generative models.\nHowever, the choice of the variational posterior determines tractability and\nflexibility of the VAE. Commonly, latent variables are modeled using the normal\ndistribution with a diagonal covariance matrix. This results in computational\nefficiency but typically it is not flexible enough to match the true posterior\ndistribution. One fashion of enriching the variational posterior distribution\nis application of normalizing flows, i.e., a series of invertible\ntransformations to latent variables with a simple posterior. In this paper, we\nfollow this line of thinking and propose a volume-preserving flow that uses a\nseries of Householder transformations. We show empirically on MNIST dataset and\nhistopathology data that the proposed flow allows to obtain more flexible\nvariational posterior and competitive results comparing to other normalizing\nflows. \n\n"}
{"id": "1612.00193", "contents": "Title: Learning molecular energies using localized graph kernels Abstract: Recent machine learning methods make it possible to model potential energy of\natomic configurations with chemical-level accuracy (as calculated from\nab-initio calculations) and at speeds suitable for molecular dynam- ics\nsimulation. Best performance is achieved when the known physical constraints\nare encoded in the machine learning models. For example, the atomic energy is\ninvariant under global translations and rotations, it is also invariant to\npermutations of same-species atoms. Although simple to state, these symmetries\nare complicated to encode into machine learning algorithms. In this paper, we\npresent a machine learning approach based on graph theory that naturally\nincorporates translation, rotation, and permutation symmetries. Specifically,\nwe use a random walk graph kernel to measure the similarity of two adjacency\nmatrices, each of which represents a local atomic environment. This Graph\nApproximated Energy (GRAPE) approach is flexible and admits many possible\nextensions. We benchmark a simple version of GRAPE by predicting atomization\nenergies on a standard dataset of organic molecules. \n\n"}
{"id": "1612.00599", "contents": "Title: Communication Lower Bounds for Distributed Convex Optimization:\n  Partition Data on Features Abstract: Recently, there has been an increasing interest in designing distributed\nconvex optimization algorithms under the setting where the data matrix is\npartitioned on features. Algorithms under this setting sometimes have many\nadvantages over those under the setting where data is partitioned on samples,\nespecially when the number of features is huge. Therefore, it is important to\nunderstand the inherent limitations of these optimization problems. In this\npaper, with certain restrictions on the communication allowed in the\nprocedures, we develop tight lower bounds on communication rounds for a broad\nclass of non-incremental algorithms under this setting. We also provide a lower\nbound on communication rounds for a class of (randomized) incremental\nalgorithms. \n\n"}
{"id": "1612.00889", "contents": "Title: New Frameworks for Offline and Streaming Coreset Constructions Abstract: A coreset for a set of points is a small subset of weighted points that\napproximately preserves important properties of the original set. Specifically,\nif $P$ is a set of points, $Q$ is a set of queries, and $f:P\\times\nQ\\to\\mathbb{R}$ is a cost function, then a set $S\\subseteq P$ with weights\n$w:P\\to[0,\\infty)$ is an $\\epsilon$-coreset for some parameter $\\epsilon>0$ if\n$\\sum_{s\\in S}w(s)f(s,q)$ is a $(1+\\epsilon)$ multiplicative approximation to\n$\\sum_{p\\in P}f(p,q)$ for all $q\\in Q$. Coresets are used to solve fundamental\nproblems in machine learning under various big data models of computation. Many\nof the suggested coresets in the recent decade used, or could have used a\ngeneral framework for constructing coresets whose size depends quadratically on\nwhat is known as total sensitivity $t$.\n  In this paper we improve this bound from $O(t^2)$ to $O(t\\log t)$. Thus our\nresults imply more space efficient solutions to a number of problems, including\nprojective clustering, $k$-line clustering, and subspace approximation.\nMoreover, we generalize the notion of sensitivity sampling for sup-sampling\nthat supports non-multiplicative approximations, negative cost functions and\nmore. The main technical result is a generic reduction to the sample complexity\nof learning a class of functions with bounded VC dimension. We show that\nobtaining an $(\\nu,\\alpha)$-sample for this class of functions with appropriate\nparameters $\\nu$ and $\\alpha$ suffices to achieve space efficient\n$\\epsilon$-coresets.\n  Our result implies more efficient coreset constructions for a number of\ninteresting problems in machine learning; we show applications to\n$k$-median/$k$-means, $k$-line clustering, $j$-subspace approximation, and the\ninteger $(j,k)$-projective clustering problem. \n\n"}
{"id": "1612.00984", "contents": "Title: Estimating latent feature-feature interactions in large feature-rich\n  graphs Abstract: Real-world complex networks describe connections between objects; in reality,\nthose objects are often endowed with some kind of features. How does the\npresence or absence of such features interplay with the network link structure?\nAlthough the situation here described is truly ubiquitous, there is a limited\nbody of research dealing with large graphs of this kind. Many previous works\nconsidered homophily as the only possible transmission mechanism translating\nnode features into links. Other authors, instead, developed more sophisticated\nmodels, that are able to handle complex feature interactions, but are unfit to\nscale to very large networks. We expand on the MGJ model, where interactions\nbetween pairs of features can foster or discourage link formation. In this\nwork, we will investigate how to estimate the latent feature-feature\ninteractions in this model. We shall propose two solutions: the first one\nassumes feature independence and it is essentially based on Naive Bayes; the\nsecond one, which relaxes the independence assumption assumption, is based on\nperceptrons. In fact, we show it is possible to cast the model equation in\norder to see it as the prediction rule of a perceptron. We analyze how\nclassical results for the perceptrons can be interpreted in this context; then,\nwe define a fast and simple perceptron-like algorithm for this task, which can\nprocess $10^8$ links in minutes. We then compare these two techniques, first\nwith synthetic datasets that follows our model, gaining evidence that the Naive\nindependence assumptions are detrimental in practice. Secondly, we consider a\nreal, large-scale citation network where each node (i.e., paper) can be\ndescribed by different types of characteristics; there, our algorithm can\nassess how well each set of features can explain the links, and thus finding\nmeaningful latent feature-feature interactions. \n\n"}
{"id": "1612.02487", "contents": "Title: Interactive Elicitation of Knowledge on Feature Relevance Improves\n  Predictions in Small Data Sets Abstract: Providing accurate predictions is challenging for machine learning algorithms\nwhen the number of features is larger than the number of samples in the data.\nPrior knowledge can improve machine learning models by indicating relevant\nvariables and parameter values. Yet, this prior knowledge is often tacit and\nonly available from domain experts. We present a novel approach that uses\ninteractive visualization to elicit the tacit prior knowledge and uses it to\nimprove the accuracy of prediction models. The main component of our approach\nis a user model that models the domain expert's knowledge of the relevance of\ndifferent features for a prediction task. In particular, based on the expert's\nearlier input, the user model guides the selection of the features on which to\nelicit user's knowledge next. The results of a controlled user study show that\nthe user model significantly improves prior knowledge elicitation and\nprediction accuracy, when predicting the relative citation counts of scientific\ndocuments in a specific domain. \n\n"}
{"id": "1612.02712", "contents": "Title: Scalable Influence Maximization for Multiple Products in Continuous-Time\n  Diffusion Networks Abstract: A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability. \n\n"}
{"id": "1612.03839", "contents": "Title: Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD) Abstract: Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases. \n\n"}
{"id": "1612.05708", "contents": "Title: Mutual information for fitting deep nonlinear models Abstract: Deep nonlinear models pose a challenge for fitting parameters due to lack of\nknowledge of the hidden layer and the potentially non-affine relation of the\ninitial and observed layers. In the present work we investigate the use of\ninformation theoretic measures such as mutual information and Kullback-Leibler\n(KL) divergence as objective functions for fitting such models without\nknowledge of the hidden layer. We investigate one model as a proof of concept\nand one application of cogntive performance. We further investigate the use of\noptimizers with these methods. Mutual information is largely successful as an\nobjective, depending on the parameters. KL divergence is found to be similarly\nsuccesful, given some knowledge of the statistics of the hidden layer. \n\n"}
{"id": "1612.07523", "contents": "Title: Robustness of Voice Conversion Techniques Under Mismatched Conditions Abstract: Most of the existing studies on voice conversion (VC) are conducted in\nacoustically matched conditions between source and target signal. However, the\nrobustness of VC methods in presence of mismatch remains unknown. In this\npaper, we report a comparative analysis of different VC techniques under\nmismatched conditions. The extensive experiments with five different VC\ntechniques on CMU ARCTIC corpus suggest that performance of VC methods\nsubstantially degrades in noisy conditions. We have found that bilinear\nfrequency warping with amplitude scaling (BLFWAS) outperforms other methods in\nmost of the noisy conditions. We further explore the suitability of different\nspeech enhancement techniques for robust conversion. The objective evaluation\nresults indicate that spectral subtraction and log minimum mean square error\n(logMMSE) based speech enhancement techniques can be used to improve the\nperformance in specific noisy conditions. \n\n"}
{"id": "1612.09596", "contents": "Title: Counterfactual Prediction with Deep Instrumental Variables Networks Abstract: We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine. \n\n"}
{"id": "1701.01325", "contents": "Title: Outlier Detection for Text Data : An Extended Version Abstract: The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods. \n\n"}
{"id": "1701.02720", "contents": "Title: Towards End-to-End Speech Recognition with Deep Convolutional Neural\n  Networks Abstract: Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic features for\nautomatic speech recognition (ASR). Hybrid speech recognition systems\nincorporating CNNs with Hidden Markov Models/Gaussian Mixture Models\n(HMMs/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural\nNetworks (RNNs), which is proposed for labeling unsegmented sequences, makes it\nfeasible to train an end-to-end speech recognition system instead of hybrid\nsettings. However, RNNs are computationally expensive and sometimes difficult\nto train. In this paper, inspired by the advantages of both CNNs and the CTC\napproach, we propose an end-to-end speech framework for sequence labeling, by\ncombining hierarchical CNNs with CTC directly without recurrent connections. By\nevaluating the approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive with\nthe existing baseline systems. Moreover, we argue that CNNs have the capability\nto model temporal correlations with appropriate context information. \n\n"}
{"id": "1701.03655", "contents": "Title: Dictionary Learning from Incomplete Data Abstract: This paper extends the recently proposed and theoretically justified\niterative thresholding and $K$ residual means algorithm ITKrM to learning\ndicionaries from incomplete/masked training data (ITKrMM). It further adapts\nthe algorithm to the presence of a low rank component in the data and provides\na strategy for recovering this low rank component again from incomplete data.\nSeveral synthetic experiments show the advantages of incorporating information\nabout the corruption into the algorithm. Finally, image inpainting is\nconsidered as application example, which demonstrates the superior performance\nof ITKrMM in terms of speed at similar or better reconstruction quality\ncompared to its closest dictionary learning counterpart. \n\n"}
{"id": "1701.06279", "contents": "Title: dna2vec: Consistent vector representations of variable-length k-mers Abstract: One of the ubiquitous representation of long DNA sequence is dividing it into\nshorter k-mer components. Unfortunately, the straightforward vector encoding of\nk-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse\nyet, the distance between any pair of one-hot vectors is equidistant. This is\nparticularly problematic when applying the latest machine learning algorithms\nto solve problems in biological sequence analysis. In this paper, we propose a\nnovel method to train distributed representations of variable-length k-mers.\nOur method is based on the popular word embedding model word2vec, which is\ntrained on a shallow two-layer neural network. Our experiments provide evidence\nthat the summing of dna2vec vectors is akin to nucleotides concatenation. We\nalso demonstrate that there is correlation between Needleman-Wunsch similarity\nscore and cosine similarity of dna2vec vectors. \n\n"}
{"id": "1701.08528", "contents": "Title: Self-Adaptation of Activity Recognition Systems to New Sensors Abstract: Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach. \n\n"}
{"id": "1702.00763", "contents": "Title: Natasha: Faster Non-Convex Stochastic Optimization Via Strongly\n  Non-Convex Parameter Abstract: Given a nonconvex function that is an average of $n$ smooth functions, we\ndesign stochastic first-order methods to find its approximate stationary\npoints. The convergence of our new methods depends on the smallest (negative)\neigenvalue $-\\sigma$ of the Hessian, a parameter that describes how nonconvex\nthe function is.\n  Our methods outperform known results for a range of parameter $\\sigma$, and\ncan be used to find approximate local minima. Our result implies an interesting\ndichotomy: there exists a threshold $\\sigma_0$ so that the currently fastest\nmethods for $\\sigma>\\sigma_0$ and for $\\sigma<\\sigma_0$ have different\nbehaviors: the former scales with $n^{2/3}$ and the latter scales with\n$n^{3/4}$. \n\n"}
{"id": "1702.04459", "contents": "Title: Robust Stochastic Configuration Networks with Kernel Density Estimation Abstract: Neural networks have been widely used as predictive models to fit data\ndistribution, and they could be implemented through learning a collection of\nsamples. In many applications, however, the given dataset may contain noisy\nsamples or outliers which may result in a poor learner model in terms of\ngeneralization. This paper contributes to a development of robust stochastic\nconfiguration networks (RSCNs) for resolving uncertain data regression\nproblems. RSCNs are built on original stochastic configuration networks with\nweighted least squares method for evaluating the output weights, and the input\nweights and biases are incrementally and randomly generated by satisfying with\na set of inequality constrains. The kernel density estimation (KDE) method is\nemployed to set the penalty weights for each training samples, so that some\nnegative impacts, caused by noisy data or outliers, on the resulting learner\nmodel can be reduced. The alternating optimization technique is applied for\nupdating a RSCN model with improved penalty weights computed from the kernel\ndensity estimation function. Performance evaluation is carried out by a\nfunction approximation, four benchmark datasets and a case study on engineering\napplication. Comparisons to other robust randomised neural modelling\ntechniques, including the probabilistic robust learning algorithm for neural\nnetworks with random weights and improved RVFL networks, indicate that the\nproposed RSCNs with KDE perform favourably and demonstrate good potential for\nreal-world applications. \n\n"}
{"id": "1702.06602", "contents": "Title: Exemplar-Centered Supervised Shallow Parametric Data Embedding Abstract: Metric learning methods for dimensionality reduction in combination with\nk-Nearest Neighbors (kNN) have been extensively deployed in many\nclassification, data embedding, and information retrieval applications.\nHowever, most of these approaches involve pairwise training data comparisons,\nand thus have quadratic computational complexity with respect to the size of\ntraining set, preventing them from scaling to fairly big datasets. Moreover,\nduring testing, comparing test data against all the training data points is\nalso expensive in terms of both computational cost and resources required.\nFurthermore, previous metrics are either too constrained or too expressive to\nbe well learned. To effectively solve these issues, we present an\nexemplar-centered supervised shallow parametric data embedding model, using a\nMaximally Collapsing Metric Learning (MCML) objective. Our strategy learns a\nshallow high-order parametric embedding function and compares training/test\ndata only with learned or precomputed exemplars, resulting in a cost function\nwith linear computational complexity for both training and testing. We also\nempirically demonstrate, using several benchmark datasets, that for\nclassification in two-dimensional embedding space, our approach not only gains\nspeedup of kNN by hundreds of times, but also outperforms state-of-the-art\nsupervised embedding approaches. \n\n"}
{"id": "1702.06676", "contents": "Title: Counterfactual Control for Free from Generative Models Abstract: We introduce a method by which a generative model learning the joint\ndistribution between actions and future states can be used to automatically\ninfer a control scheme for any desired reward function, which may be altered on\nthe fly without retraining the model. In this method, the problem of action\nselection is reduced to one of gradient descent on the latent space of the\ngenerative model, with the model itself providing the means of evaluating\noutcomes and finding the gradient, much like how the reward network in Deep\nQ-Networks (DQN) provides gradient information for the action generator. Unlike\nDQN or Actor-Critic, which are conditional models for a specific reward, using\na generative model of the full joint distribution permits the reward to be\nchanged on the fly. In addition, the generated futures can be inspected to gain\ninsight in to what the network 'thinks' will happen, and to what went wrong\nwhen the outcomes deviate from prediction. \n\n"}
{"id": "1702.06921", "contents": "Title: Distributed Representation of Subgraphs Abstract: Network embeddings have become very popular in learning effective feature\nrepresentations of networks. Motivated by the recent successes of embeddings in\nnatural language processing, researchers have tried to find network embeddings\nin order to exploit machine learning algorithms for mining tasks like node\nclassification and edge prediction. However, most of the work focuses on\nfinding distributed representations of nodes, which are inherently ill-suited\nto tasks such as community detection which are intuitively dependent on\nsubgraphs.\n  Here, we propose sub2vec, an unsupervised scalable algorithm to learn feature\nrepresentations of arbitrary subgraphs. We provide means to characterize\nsimilarties between subgraphs and provide theoretical analysis of sub2vec and\ndemonstrate that it preserves the so-called local proximity. We also highlight\nthe usability of sub2vec by leveraging it for network mining tasks, like\ncommunity detection. We show that sub2vec gets significant gains over\nstate-of-the-art methods and node-embedding methods. In particular, sub2vec\noffers an approach to generate a richer vocabulary of features of subgraphs to\nsupport representation and reasoning. \n\n"}
{"id": "1702.07652", "contents": "Title: Control of Gene Regulatory Networks with Noisy Measurements and\n  Uncertain Inputs Abstract: This paper is concerned with the problem of stochastic control of gene\nregulatory networks (GRNs) observed indirectly through noisy measurements and\nwith uncertainty in the intervention inputs. The partial observability of the\ngene states and uncertainty in the intervention process are accounted for by\nmodeling GRNs using the partially-observed Boolean dynamical system (POBDS)\nsignal model with noisy gene expression measurements. Obtaining the optimal\ninfinite-horizon control strategy for this problem is not attainable in\ngeneral, and we apply reinforcement learning and Gaussian process techniques to\nfind a near-optimal solution. The POBDS is first transformed to a\ndirectly-observed Markov Decision Process in a continuous belief space, and the\nGaussian process is used for modeling the cost function over the belief and\nintervention spaces. Reinforcement learning then is used to learn the cost\nfunction from the available gene expression data. In addition, we employ\nsparsification, which enables the control of large partially-observed GRNs. The\nperformance of the resulting algorithm is studied through a comprehensive set\nof numerical experiments using synthetic gene expression data generated from a\nmelanoma gene regulatory network. \n\n"}
{"id": "1702.07956", "contents": "Title: Generative Adversarial Active Learning Abstract: We propose a new active learning by query synthesis approach using Generative\nAdversarial Networks (GAN). Different from regular active learning, the\nresulting algorithm adaptively synthesizes training instances for querying to\nincrease learning speed. We generate queries according to the uncertainty\nprinciple, but our idea can work with other active learning principles. We\nreport results from various numerical experiments to demonstrate the\neffectiveness the proposed approach. In some settings, the proposed algorithm\noutperforms traditional pool-based approaches. To the best our knowledge, this\nis the first active learning work using GAN. \n\n"}
{"id": "1702.07958", "contents": "Title: Efficient Online Bandit Multiclass Learning with $\\tilde{O}(\\sqrt{T})$\n  Regret Abstract: We present an efficient second-order algorithm with\n$\\tilde{O}(\\frac{1}{\\eta}\\sqrt{T})$ regret for the bandit online multiclass\nproblem. The regret bound holds simultaneously with respect to a family of loss\nfunctions parameterized by $\\eta$, for a range of $\\eta$ restricted by the norm\nof the competitor. The family of loss functions ranges from hinge loss\n($\\eta=0$) to squared hinge loss ($\\eta=1$). This provides a solution to the\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\n$\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\nalgorithm experimentally, showing that it also performs favorably against\nearlier algorithms. \n\n"}
{"id": "1702.08396", "contents": "Title: Learning Hierarchical Features from Generative Models Abstract: Deep neural networks have been shown to be very successful at learning\nfeature hierarchies in supervised learning tasks. Generative models, on the\nother hand, have benefited less from hierarchical models with multiple layers\nof latent variables. In this paper, we prove that hierarchical latent variable\nmodels do not take advantage of the hierarchical structure when trained with\nexisting variational methods, and provide some limitations on the kind of\nfeatures existing models can learn. Finally we propose an alternative\narchitecture that do not suffer from these limitations. Our model is able to\nlearn highly interpretable and disentangled hierarchical features on several\nnatural image datasets with no task specific regularization or prior knowledge. \n\n"}
{"id": "1702.08892", "contents": "Title: Bridging the Gap Between Value and Policy Based Reinforcement Learning Abstract: We establish a new connection between value and policy based reinforcement\nlearning (RL) based on a relationship between softmax temporal value\nconsistency and policy optimality under entropy regularization. Specifically,\nwe show that softmax consistent action values correspond to optimal entropy\nregularized policy probabilities along any action sequence, regardless of\nprovenance. From this observation, we develop a new RL algorithm, Path\nConsistency Learning (PCL), that minimizes a notion of soft consistency error\nalong multi-step action sequences extracted from both on- and off-policy\ntraces. We examine the behavior of PCL in different scenarios and show that PCL\ncan be interpreted as generalizing both actor-critic and Q-learning algorithms.\nWe subsequently deepen the relationship by showing how a single model can be\nused to represent both a policy and the corresponding softmax state values,\neliminating the need for a separate critic. The experimental evaluation\ndemonstrates that PCL significantly outperforms strong actor-critic and\nQ-learning baselines across several benchmarks. \n\n"}
{"id": "1702.08898", "contents": "Title: Lipschitz Optimisation for Lipschitz Interpolation Abstract: Techniques known as Nonlinear Set Membership prediction, Kinky Inference or\nLipschitz Interpolation are fast and numerically robust approaches to\nnonparametric machine learning that have been proposed to be utilised in the\ncontext of system identification and learning-based control. They utilise\npresupposed Lipschitz properties in order to compute inferences over unobserved\nfunction values. Unfortunately, most of these approaches rely on exact\nknowledge about the input space metric as well as about the Lipschitz constant.\nFurthermore, existing techniques to estimate the Lipschitz constants from the\ndata are not robust to noise or seem to be ad-hoc and typically are decoupled\nfrom the ultimate learning and prediction task. To overcome these limitations,\nwe propose an approach for optimising parameters of the presupposed metrics by\nminimising validation set prediction errors. To avoid poor performance due to\nlocal minima, we propose to utilise Lipschitz properties of the optimisation\nobjective to ensure global optimisation success. The resulting approach is a\nnew flexible method for nonparametric black-box learning. We provide\nexperimental evidence of the competitiveness of our approach on artificial as\nwell as on real data. \n\n"}
{"id": "1703.01442", "contents": "Title: Recurrent Poisson Factorization for Temporal Recommendation Abstract: Poisson factorization is a probabilistic model of users and items for\nrecommendation systems, where the so-called implicit consumer data is modeled\nby a factorized Poisson distribution. There are many variants of Poisson\nfactorization methods who show state-of-the-art performance on real-world\nrecommendation tasks. However, most of them do not explicitly take into account\nthe temporal behavior and the recurrent activities of users which is essential\nto recommend the right item to the right user at the right time. In this paper,\nwe introduce Recurrent Poisson Factorization (RPF) framework that generalizes\nthe classical PF methods by utilizing a Poisson process for modeling the\nimplicit feedback. RPF treats time as a natural constituent of the model and\nbrings to the table a rich family of time-sensitive factorization models. To\nelaborate, we instantiate several variants of RPF who are capable of handling\ndynamic user preferences and item specification (DRPF), modeling the\nsocial-aspect of product adoption (SRPF), and capturing the consumption\nheterogeneity among users and items (HRPF). We also develop a variational\nalgorithm for approximate posterior inference that scales up to massive data\nsets. Furthermore, we demonstrate RPF's superior performance over many\nstate-of-the-art methods on synthetic dataset, and large scale real-world\ndatasets on music streaming logs, and user-item interactions in M-Commerce\nplatforms. \n\n"}
{"id": "1703.01594", "contents": "Title: Graph sampling with determinantal processes Abstract: We present a new random sampling strategy for k-bandlimited signals defined\non graphs, based on determinantal point processes (DPP). For small graphs, ie,\nin cases where the spectrum of the graph is accessible, we exhibit a DPP\nsampling scheme that enables perfect recovery of bandlimited signals. For large\ngraphs, ie, in cases where the graph's spectrum is not accessible, we\ninvestigate, both theoretically and empirically, a sub-optimal but much faster\nDPP based on loop-erased random walks on the graph. Preliminary experiments\nshow promising results especially in cases where the number of measurements\nshould stay as small as possible and for graphs that have a strong community\nstructure. Our sampling scheme is efficient and can be applied to graphs with\nup to $10^6$ nodes. \n\n"}
{"id": "1703.01594", "contents": "Title: Graph sampling with determinantal processes Abstract: We present a new random sampling strategy for k-bandlimited signals defined\non graphs, based on determinantal point processes (DPP). For small graphs, ie,\nin cases where the spectrum of the graph is accessible, we exhibit a DPP\nsampling scheme that enables perfect recovery of bandlimited signals. For large\ngraphs, ie, in cases where the graph's spectrum is not accessible, we\ninvestigate, both theoretically and empirically, a sub-optimal but much faster\nDPP based on loop-erased random walks on the graph. Preliminary experiments\nshow promising results especially in cases where the number of measurements\nshould stay as small as possible and for graphs that have a strong community\nstructure. Our sampling scheme is efficient and can be applied to graphs with\nup to $10^6$ nodes. \n\n"}
{"id": "1703.02317", "contents": "Title: Convolutional Recurrent Neural Networks for Bird Audio Detection Abstract: Bird sounds possess distinctive spectral structure which may exhibit small\nshifts in spectrum depending on the bird species and environmental conditions.\nIn this paper, we propose using convolutional recurrent neural networks on the\ntask of automated bird audio detection in real-life environments. In the\nproposed method, convolutional layers extract high dimensional, local frequency\nshift invariant features, while recurrent layers capture longer term\ndependencies between the features extracted from short time frames. This method\nachieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data\nand obtains the second place in the Bird Audio Detection challenge. \n\n"}
{"id": "1703.04140", "contents": "Title: Multiscale Hierarchical Convolutional Networks Abstract: Deep neural network algorithms are difficult to analyze because they lack\nstructure allowing to understand the properties of underlying transforms and\ninvariants. Multiscale hierarchical convolutional networks are structured deep\nconvolutional networks where layers are indexed by progressively higher\ndimensional attributes, which are learned from training data. Each new layer is\ncomputed with multidimensional convolutions along spatial and attribute\nvariables. We introduce an efficient implementation of such networks where the\ndimensionality is progressively reduced by averaging intermediate layers along\nattribute indices. Hierarchical networks are tested on CIFAR image data bases\nwhere they obtain comparable precisions to state of the art networks, with much\nfewer parameters. We study some properties of the attributes learned from these\ndatabases. \n\n"}
{"id": "1703.04943", "contents": "Title: Matched bipartite block model with covariates Abstract: Community detection or clustering is a fundamental task in the analysis of\nnetwork data. Many real networks have a bipartite structure which makes\ncommunity detection challenging. In this paper, we consider a model which\nallows for matched communities in the bipartite setting, in addition to node\ncovariates with information about the matching. We derive a simple fast\nalgorithm for fitting the model based on variational inference ideas and show\nits effectiveness on both simulated and real data. A variation of the model to\nallow for degree-correction is also considered, in addition to a novel approach\nto fitting such degree-corrected models. \n\n"}
{"id": "1703.06476", "contents": "Title: Practical Coreset Constructions for Machine Learning Abstract: We investigate coresets - succinct, small summaries of large data sets - so\nthat solutions found on the summary are provably competitive with solution\nfound on the full data set. We provide an overview over the state-of-the-art in\ncoreset construction for machine learning. In Section 2, we present both the\nintuition behind and a theoretically sound framework to construct coresets for\ngeneral problems and apply it to $k$-means clustering. In Section 3 we\nsummarize existing coreset construction algorithms for a variety of machine\nlearning problems such as maximum likelihood estimation of mixture models,\nBayesian non-parametric models, principal component analysis, regression and\ngeneral empirical risk minimization. \n\n"}
{"id": "1703.07473", "contents": "Title: Episode-Based Active Learning with Bayesian Neural Networks Abstract: We investigate different strategies for active learning with Bayesian deep\nneural networks. We focus our analysis on scenarios where new, unlabeled data\nis obtained episodically, such as commonly encountered in mobile robotics\napplications. An evaluation of different strategies for acquisition, updating,\nand final training on the CIFAR-10 dataset shows that incremental network\nupdates with final training on the accumulated acquisition set are essential\nfor best performance, while limiting the amount of required human labeling\nlabor. \n\n"}
{"id": "1703.08581", "contents": "Title: Sequence-to-Sequence Models Can Directly Translate Foreign Speech Abstract: We present a recurrent encoder-decoder deep neural network architecture that\ndirectly translates speech in one language into text in another. The model does\nnot explicitly transcribe the speech into text in the source language, nor does\nit require supervision from the ground truth source language transcription\nduring training. We apply a slightly modified sequence-to-sequence with\nattention architecture that has previously been used for speech recognition and\nshow that it can be repurposed for this more complex task, illustrating the\npower of attention-based models. A single model trained end-to-end obtains\nstate-of-the-art performance on the Fisher Callhome Spanish-English speech\ntranslation task, outperforming a cascade of independently trained\nsequence-to-sequence speech recognition and machine translation models by 1.8\nBLEU points on the Fisher test set. In addition, we find that making use of the\ntraining data in both languages by multi-task training sequence-to-sequence\nspeech translation and recognition models with a shared encoder network can\nimprove performance by a further 1.4 BLEU points. \n\n"}
{"id": "1703.08816", "contents": "Title: Uncertainty quantification in graph-based classification of high\n  dimensional data Abstract: Classification of high dimensional data finds wide-ranging applications. In\nmany of these applications equipping the resulting classification with a\nmeasure of uncertainty may be as important as the classification itself. In\nthis paper we introduce, develop algorithms for, and investigate the properties\nof, a variety of Bayesian models for the task of binary classification; via the\nposterior distribution on the classification labels, these methods\nautomatically give measures of uncertainty. The methods are all based around\nthe graph formulation of semi-supervised learning.\n  We provide a unified framework which brings together a variety of methods\nwhich have been introduced in different communities within the mathematical\nsciences. We study probit classification in the graph-based setting, generalize\nthe level-set method for Bayesian inverse problems to the classification\nsetting, and generalize the Ginzburg-Landau optimization-based classifier to a\nBayesian setting; we also show that the probit and level set approaches are\nnatural relaxations of the harmonic function approach introduced in [Zhu et al\n2003].\n  We introduce efficient numerical methods, suited to large data-sets, for both\nMCMC-based sampling as well as gradient-based MAP estimation. Through numerical\nexperiments we study classification accuracy and uncertainty quantification for\nour models; these experiments showcase a suite of datasets commonly used to\nevaluate graph-based semi-supervised learning algorithms. \n\n"}
{"id": "1703.10951", "contents": "Title: Comparison of multi-task convolutional neural network (MT-CNN) and a few\n  other methods for toxicity prediction Abstract: Toxicity analysis and prediction are of paramount importance to human health\nand environmental protection. Existing computational methods are built from a\nwide variety of descriptors and regressors, which makes their performance\nanalysis difficult. For example, deep neural network (DNN), a successful\napproach in many occasions, acts like a black box and offers little conceptual\nelegance or physical understanding. The present work constructs a common set of\nmicroscopic descriptors based on established physical models for charges,\nsurface areas and free energies to assess the performance of multi-task\nconvolutional neural network (MT-CNN) architectures and a few other approaches,\nincluding random forest (RF) and gradient boosting decision tree (GBDT), on an\nequal footing. Comparison is also given to convolutional neural network (CNN)\nand non-convolutional deep neural network (DNN) algorithms. Four benchmark\ntoxicity data sets (i.e., endpoints) are used to evaluate various approaches.\nExtensive numerical studies indicate that the present MT-CNN architecture is\nable to outperform the state-of-the-art methods. \n\n"}
{"id": "1704.00003", "contents": "Title: Spectral Methods for Nonparametric Models Abstract: Nonparametric models are versatile, albeit computationally expensive, tool\nfor modeling mixture models. In this paper, we introduce spectral methods for\nthe two most popular nonparametric models: the Indian Buffet Process (IBP) and\nthe Hierarchical Dirichlet Process (HDP). We show that using spectral methods\nfor the inference of nonparametric models are computationally and statistically\nefficient. In particular, we derive the lower-order moments of the IBP and the\nHDP, propose spectral algorithms for both models, and provide reconstruction\nguarantees for the algorithms. For the HDP, we further show that applying\nhierarchical models on dataset with hierarchical structure, which can be solved\nwith the generalized spectral HDP, produces better solutions to that of flat\nmodels regarding likelihood performance. \n\n"}
{"id": "1704.00217", "contents": "Title: Adversarial Connective-exploiting Networks for Implicit Discourse\n  Relation Classification Abstract: Implicit discourse relation classification is of great challenge due to the\nlack of connectives as strong linguistic cues, which motivates the use of\nannotated implicit connectives to improve the recognition. We propose a feature\nimitation framework in which an implicit relation network is driven to learn\nfrom another neural network with access to connectives, and thus encouraged to\nextract similarly salient features for accurate classification. We develop an\nadversarial model to enable an adaptive imitation scheme through competition\nbetween the implicit network and a rival feature discriminator. Our method\neffectively transfers discriminability of connectives to the implicit features,\nand achieves state-of-the-art performance on the PDTB benchmark. \n\n"}
{"id": "1704.02958", "contents": "Title: On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel\n  Methods and Neural Networks Abstract: Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks. \n\n"}
{"id": "1704.03058", "contents": "Title: CERN: Confidence-Energy Recurrent Network for Group Activity Recognition Abstract: This work is about recognizing human activities occurring in videos at\ndistinct semantic levels, including individual actions, interactions, and group\nactivities. The recognition is realized using a two-level hierarchy of Long\nShort-Term Memory (LSTM) networks, forming a feed-forward deep architecture,\nwhich can be trained end-to-end. In comparison with existing architectures of\nLSTMs, we make two key contributions giving the name to our approach as\nConfidence-Energy Recurrent Network -- CERN. First, instead of using the common\nsoftmax layer for prediction, we specify a novel energy layer (EL) for\nestimating the energy of our predictions. Second, rather than finding the\ncommon minimum-energy class assignment, which may be numerically unstable under\nuncertainty, we specify that the EL additionally computes the p-values of the\nsolutions, and in this way estimates the most confident energy minimum. The\nevaluation on the Collective Activity and Volleyball datasets demonstrates: (i)\nadvantages of our two contributions relative to the common softmax and\nenergy-minimization formulations and (ii) a superior performance relative to\nthe state-of-the-art approaches. \n\n"}
{"id": "1704.03477", "contents": "Title: A Neural Representation of Sketch Drawings Abstract: We present sketch-rnn, a recurrent neural network (RNN) able to construct\nstroke-based drawings of common objects. The model is trained on thousands of\ncrude human-drawn images representing hundreds of classes. We outline a\nframework for conditional and unconditional sketch generation, and describe new\nrobust training methods for generating coherent sketch drawings in a vector\nformat. \n\n"}
{"id": "1704.03817", "contents": "Title: MAGAN: Margin Adaptation for Generative Adversarial Networks Abstract: We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs)\nalgorithm, a novel training procedure for GANs to improve stability and\nperformance by using an adaptive hinge loss function. We estimate the\nappropriate hinge loss margin with the expected energy of the target\ndistribution, and derive principled criteria for when to update the margin. We\nprove that our method converges to its global optimum under certain\nassumptions. Evaluated on the task of unsupervised image generation, the\nproposed training procedure is simple yet robust on a diverse set of data, and\nachieves qualitative and quantitative improvements compared to the\nstate-of-the-art. \n\n"}
{"id": "1704.05982", "contents": "Title: Retrospective Higher-Order Markov Processes for User Trails Abstract: Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy. \n\n"}
{"id": "1704.06084", "contents": "Title: Knowledge Fusion via Embeddings from Text, Knowledge Graphs, and Images Abstract: We present a baseline approach for cross-modal knowledge fusion. Different\nbasic fusion methods are evaluated on existing embedding approaches to show the\npotential of joining knowledge about certain concepts across modalities in a\nfused concept representation. \n\n"}
{"id": "1704.07926", "contents": "Title: From Language to Programs: Bridging Reinforcement Learning and Maximum\n  Marginal Likelihood Abstract: Our goal is to learn a semantic parser that maps natural language utterances\ninto executable programs when only indirect supervision is available: examples\nare labeled with the correct execution result, but not the program itself.\nConsequently, we must search the space of programs for those that output the\ncorrect result, while not being misled by spurious programs: incorrect programs\nthat coincidentally output the correct result. We connect two common learning\nparadigms, reinforcement learning (RL) and maximum marginal likelihood (MML),\nand then present a new learning algorithm that combines the strengths of both.\nThe new algorithm guards against spurious programs by combining the systematic\nsearch traditionally employed in MML with the randomized exploration of RL, and\nby updating parameters such that probability is spread more evenly across\nconsistent programs. We apply our learning algorithm to a new neural semantic\nparser and show significant gains over existing state-of-the-art results on a\nrecent context-dependent semantic parsing task. \n\n"}
{"id": "1704.07953", "contents": "Title: Linear Convergence of Accelerated Stochastic Gradient Descent for\n  Nonconvex Nonsmooth Optimization Abstract: In this paper, we study the stochastic gradient descent (SGD) method for the\nnonconvex nonsmooth optimization, and propose an accelerated SGD method by\ncombining the variance reduction technique with Nesterov's extrapolation\ntechnique. Moreover, based on the local error bound condition, we establish the\nlinear convergence of our method to obtain a stationary point of the nonconvex\noptimization. In particular, we prove that not only the sequence generated\nlinearly converges to a stationary point of the problem, but also the\ncorresponding sequence of objective values is linearly convergent. Finally,\nsome numerical experiments demonstrate the effectiveness of our method. To the\nbest of our knowledge, it is first proved that the accelerated SGD method\nconverges linearly to the local minimum of the nonconvex optimization. \n\n"}
{"id": "1704.08683", "contents": "Title: Matrix Completion and Related Problems via Strong Duality Abstract: This work studies the strong duality of non-convex matrix factorization\nproblems: we show that under certain dual conditions, these problems and its\ndual have the same optimum. This has been well understood for convex\noptimization, but little was known for non-convex problems. We propose a novel\nanalytical framework and show that under certain dual conditions, the optimal\nsolution of the matrix factorization program is the same as its bi-dual and\nthus the global optimality of the non-convex program can be achieved by solving\nits bi-dual which is convex. These dual conditions are satisfied by a wide\nclass of matrix factorization problems, although matrix factorization problems\nare hard to solve in full generality. This analytical framework may be of\nindependent interest to non-convex optimization more broadly.\n  We apply our framework to two prototypical matrix factorization problems:\nmatrix completion and robust Principal Component Analysis (PCA). These are\nexamples of efficiently recovering a hidden matrix given limited reliable\nobservations of it. Our framework shows that exact recoverability and strong\nduality hold with nearly-optimal sample complexity guarantees for matrix\ncompletion and robust PCA. \n\n"}
{"id": "1705.05615", "contents": "Title: Learning Edge Representations via Low-Rank Asymmetric Projections Abstract: We propose a new method for embedding graphs while preserving directed edge\ninformation. Learning such continuous-space vector representations (or\nembeddings) of nodes in a graph is an important first step for using network\ninformation (from social networks, user-item graphs, knowledge bases, etc.) in\nmany machine learning tasks.\n  Unlike previous work, we (1) explicitly model an edge as a function of node\nembeddings, and we (2) propose a novel objective, the \"graph likelihood\", which\ncontrasts information from sampled random walks with non-existent edges.\nIndividually, both of these contributions improve the learned representations,\nespecially when there are memory constraints on the total size of the\nembeddings. When combined, our contributions enable us to significantly improve\nthe state-of-the-art by learning more concise representations that better\npreserve the graph structure.\n  We evaluate our method on a variety of link-prediction task including social\nnetworks, collaboration networks, and protein interactions, showing that our\nproposed method learn representations with error reductions of up to 76% and\n55%, on directed and undirected graphs. In addition, we show that the\nrepresentations learned by our method are quite space efficient, producing\nembeddings which have higher structure-preserving accuracy but are 10 times\nsmaller. \n\n"}
{"id": "1705.07461", "contents": "Title: Shallow Updates for Deep Reinforcement Learning Abstract: Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN)\nhave achieved state-of-the-art results in a variety of challenging,\nhigh-dimensional domains. This success is mainly attributed to the power of\ndeep neural networks to learn rich domain representations for approximating the\nvalue function or policy. Batch reinforcement learning methods with linear\nrepresentations, on the other hand, are more stable and require less hyper\nparameter tuning. Yet, substantial feature engineering is necessary to achieve\ngood results. In this work we propose a hybrid approach -- the Least Squares\nDeep Q-Network (LS-DQN), which combines rich feature representations learned by\na DRL algorithm with the stability of a linear least squares method. We do this\nby periodically re-training the last hidden layer of a DRL network with a batch\nleast squares update. Key to our approach is a Bayesian regularization term for\nthe least squares update, which prevents over-fitting to the more recent data.\nWe tested LS-DQN on five Atari games and demonstrate significant improvement\nover vanilla DQN and Double-DQN. We also investigated the reasons for the\nsuperior performance of our method. Interestingly, we found that the\nperformance improvement can be attributed to the large batch size used by the\nLS method when optimizing the last layer. \n\n"}
{"id": "1705.07474", "contents": "Title: Why are Big Data Matrices Approximately Low Rank? Abstract: Matrices of (approximate) low rank are pervasive in data science, appearing\nin recommender systems, movie preferences, topic models, medical records, and\ngenomics. While there is a vast literature on how to exploit low rank structure\nin these datasets, there is less attention on explaining why the low rank\nstructure appears in the first place. Here, we explain the effectiveness of low\nrank models in data science by considering a simple generative model for these\nmatrices: we suppose that each row or column is associated to a (possibly high\ndimensional) bounded latent variable, and entries of the matrix are generated\nby applying a piecewise analytic function to these latent variables. These\nmatrices are in general full rank. However, we show that we can approximate\nevery entry of an $m \\times n$ matrix drawn from this model to within a fixed\nabsolute error by a low rank matrix whose rank grows as $\\mathcal O(\\log(m +\nn))$. Hence any sufficiently large matrix from such a latent variable model can\nbe approximated, up to a small entrywise error, by a low rank matrix. \n\n"}
{"id": "1705.08417", "contents": "Title: Reinforcement Learning with a Corrupted Reward Channel Abstract: No real-world reward function is perfect. Sensory errors and software bugs\nmay result in RL agents observing higher (or lower) rewards than they should.\nFor example, a reinforcement learning agent may prefer states where a sensory\nerror gives it the maximum reward, but where the true reward is actually small.\nWe formalise this problem as a generalised Markov Decision Problem called\nCorrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under\nstrong simplifying assumptions and when trying to compensate for the possibly\ncorrupt rewards. Two ways around the problem are investigated. First, by giving\nthe agent richer data, such as in inverse reinforcement learning and\nsemi-supervised reinforcement learning, reward corruption stemming from\nsystematic sensory errors may sometimes be completely managed. Second, by using\nrandomisation to blunt the agent's optimisation, reward corruption can be\npartially managed under some assumptions. \n\n"}
{"id": "1705.08865", "contents": "Title: Anti-spoofing Methods for Automatic SpeakerVerification System Abstract: Growing interest in automatic speaker verification (ASV)systems has lead to\nsignificant quality improvement of spoofing attackson them. Many research works\nconfirm that despite the low equal er-ror rate (EER) ASV systems are still\nvulnerable to spoofing attacks. Inthis work we overview different acoustic\nfeature spaces and classifiersto determine reliable and robust countermeasures\nagainst spoofing at-tacks. We compared several spoofing detection systems,\npresented so far,on the development and evaluation datasets of the Automatic\nSpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge\n2015.Experimental results presented in this paper demonstrate that the useof\nmagnitude and phase information combination provides a substantialinput into\nthe efficiency of the spoofing detection systems. Also wavelet-based features\nshow impressive results in terms of equal error rate. Inour overview we compare\nspoofing performance for systems based on dif-ferent classifiers. Comparison\nresults demonstrate that the linear SVMclassifier outperforms the conventional\nGMM approach. However, manyresearchers inspired by the great success of deep\nneural networks (DNN)approaches in the automatic speech recognition, applied\nDNN in thespoofing detection task and obtained quite low EER for known and\nun-known type of spoofing attacks. \n\n"}
{"id": "1705.08991", "contents": "Title: Approximation and Convergence Properties of Generative Adversarial\n  Learning Abstract: Generative adversarial networks (GAN) approximate a target data distribution\nby jointly optimizing an objective function through a \"two-player game\" between\na generator and a discriminator. Despite their empirical success, however, two\nvery basic questions on how well they can approximate the target distribution\nremain unanswered. First, it is not known how restricting the discriminator\nfamily affects the approximation quality. Second, while a number of different\nobjective functions have been proposed, we do not understand when convergence\nto the global minima of the objective function leads to convergence to the\ntarget distribution under various notions of distributional convergence.\n  In this paper, we address these questions in a broad and unified setting by\ndefining a notion of adversarial divergences that includes a number of recently\nproposed objective functions. We show that if the objective function is an\nadversarial divergence with some additional conditions, then using a restricted\ndiscriminator family has a moment-matching effect. Additionally, we show that\nfor objective functions that are strict adversarial divergences, convergence in\nthe objective function implies weak convergence, thus generalizing previous\nresults. \n\n"}
{"id": "1705.09319", "contents": "Title: Diagonal Rescaling For Neural Networks Abstract: We define a second-order neural network stochastic gradient training\nalgorithm whose block-diagonal structure effectively amounts to normalizing the\nunit activations. Investigating why this algorithm lacks in robustness then\nreveals two interesting insights. The first insight suggests a new way to scale\nthe stepsizes, clarifying popular algorithms such as RMSProp as well as old\nneural network tricks such as fanin stepsize scaling. The second insight\nstresses the practical importance of dealing with fast changes of the curvature\nof the cost. \n\n"}
{"id": "1705.09407", "contents": "Title: An Efficient Algorithm for Bayesian Nearest Neighbours Abstract: K-Nearest Neighbours (k-NN) is a popular classification and regression\nalgorithm, yet one of its main limitations is the difficulty in choosing the\nnumber of neighbours. We present a Bayesian algorithm to compute the posterior\nprobability distribution for k given a target point within a data-set,\nefficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or\nsimulation - alongside an exact solution for distributions within the\nexponential family. The central idea is that data points around our target are\ngenerated by the same probability distribution, extending outwards over the\nappropriate, though unknown, number of neighbours. Once the data is projected\nonto a distance metric of choice, we can transform the choice of k into a\nchange-point detection problem, for which there is an efficient solution: we\nrecursively compute the probability of the last change-point as we move towards\nour target, and thus de facto compute the posterior probability distribution\nover k. Applying this approach to both a classification and a regression UCI\ndata-sets, we compare favourably and, most importantly, by removing the need\nfor simulation, we are able to compute the posterior probability of k exactly\nand rapidly. As an example, the computational time for the Ripley data-set is a\nfew milliseconds compared to a few hours when using a MCMC approach. \n\n"}
{"id": "1705.09554", "contents": "Title: Robustness of classifiers to universal perturbations: a geometric\n  perspective Abstract: Deep networks have recently been shown to be vulnerable to universal\nperturbations: there exist very small image-agnostic perturbations that cause\nmost natural images to be misclassified by such classifiers. In this paper, we\npropose the first quantitative analysis of the robustness of classifiers to\nuniversal perturbations, and draw a formal link between the robustness to\nuniversal perturbations, and the geometry of the decision boundary.\nSpecifically, we establish theoretical bounds on the robustness of classifiers\nunder two decision boundary models (flat and curved models). We show in\nparticular that the robustness of deep networks to universal perturbations is\ndriven by a key property of their curvature: there exists shared directions\nalong which the decision boundary of deep networks is systematically positively\ncurved. Under such conditions, we prove the existence of small universal\nperturbations. Our analysis further provides a novel geometric method for\ncomputing universal perturbations, in addition to explaining their properties. \n\n"}
{"id": "1705.09677", "contents": "Title: Elementary Symmetric Polynomials for Optimal Experimental Design Abstract: We revisit the classical problem of optimal experimental design (OED) under a\nnew mathematical model grounded in a geometric motivation. Specifically, we\nintroduce models based on elementary symmetric polynomials; these polynomials\ncapture \"partial volumes\" and offer a graded interpolation between the widely\nused A-optimal design and D-optimal design models, obtaining each of them as\nspecial cases. We analyze properties of our models, and derive both greedy and\nconvex-relaxation algorithms for computing the associated designs. Our analysis\nestablishes approximation guarantees on these algorithms, while our empirical\nresults substantiate our claims and demonstrate a curious phenomenon concerning\nour greedy method. Finally, as a byproduct, we obtain new results on the theory\nof elementary symmetric polynomials that may be of independent interest. \n\n"}
{"id": "1705.09700", "contents": "Title: Multi-scale Online Learning and its Applications to Online Auctions Abstract: We consider revenue maximization in online auction/pricing problems. A seller\nsells an identical item in each period to a new buyer, or a new set of buyers.\nFor the online posted pricing problem, we show regret bounds that scale with\nthe best fixed price, rather than the range of the values. We also show regret\nbounds that are almost scale free, and match the offline sample complexity,\nwhen comparing to a benchmark that requires a lower bound on the market share.\nThese results are obtained by generalizing the classical learning from experts\nand multi-armed bandit problems to their multi-scale versions. In this version,\nthe reward of each action is in a different range, and the regret w.r.t. a\ngiven action scales with its own range, rather than the maximum range. \n\n"}
{"id": "1705.10498", "contents": "Title: Zonotope hit-and-run for efficient sampling from projection DPPs Abstract: Determinantal point processes (DPPs) are distributions over sets of items\nthat model diversity using kernels. Their applications in machine learning\ninclude summary extraction and recommendation systems. Yet, the cost of\nsampling from a DPP is prohibitive in large-scale applications, which has\ntriggered an effort towards efficient approximate samplers. We build a novel\nMCMC sampler that combines ideas from combinatorial geometry, linear\nprogramming, and Monte Carlo methods to sample from DPPs with a fixed sample\ncardinality, also called projection DPPs. Our sampler leverages the ability of\nthe hit-and-run MCMC kernel to efficiently move across convex bodies. Previous\ntheoretical results yield a fast mixing time of our chain when targeting a\ndistribution that is close to a projection DPP, but not a DPP in general. Our\nempirical results demonstrate that this extends to sampling projection DPPs,\ni.e., our sampler is more sample-efficient than previous approaches which in\nturn translates to faster convergence when dealing with costly-to-evaluate\nfunctions, such as summary extraction in our experiments. \n\n"}
{"id": "1705.10743", "contents": "Title: The Cramer Distance as a Solution to Biased Wasserstein Gradients Abstract: The Wasserstein probability metric has received much attention from the\nmachine learning community. Unlike the Kullback-Leibler divergence, which\nstrictly measures change in probability, the Wasserstein metric reflects the\nunderlying geometry between outcomes. The value of being sensitive to this\ngeometry has been demonstrated, among others, in ordinal regression and\ngenerative modelling. In this paper we describe three natural properties of\nprobability divergences that reflect requirements from machine learning: sum\ninvariance, scale sensitivity, and unbiased sample gradients. The Wasserstein\nmetric possesses the first two properties but, unlike the Kullback-Leibler\ndivergence, does not possess the third. We provide empirical evidence\nsuggesting that this is a serious issue in practice. Leveraging insights from\nprobabilistic forecasting we propose an alternative to the Wasserstein metric,\nthe Cram\\'er distance. We show that the Cram\\'er distance possesses all three\ndesired properties, combining the best of the Wasserstein and Kullback-Leibler\ndivergences. To illustrate the relevance of the Cram\\'er distance in practice\nwe design a new algorithm, the Cram\\'er Generative Adversarial Network (GAN),\nand show that it performs significantly better than the related Wasserstein\nGAN. \n\n"}
{"id": "1706.00476", "contents": "Title: The Mixing method: low-rank coordinate descent for semidefinite\n  programming with diagonal constraints Abstract: In this paper, we propose a low-rank coordinate descent approach to\nstructured semidefinite programming with diagonal constraints. The approach,\nwhich we call the Mixing method, is extremely simple to implement, has no free\nparameters, and typically attains an order of magnitude or better improvement\nin optimization performance over the current state of the art. We show that the\nmethod is strictly decreasing, converges to a critical point, and further that\nfor sufficient rank all non-optimal critical points are unstable. Moreover, we\nprove that with a step size, the Mixing method converges to the global optimum\nof the semidefinite program almost surely in a locally linear rate under random\ninitialization. This is the first low-rank semidefinite programming method that\nhas been shown to achieve a global optimum on the spherical manifold without\nassumption. We apply our algorithm to two related domains: solving the maximum\ncut semidefinite relaxation, and solving a maximum satisfiability relaxation\n(we also briefly consider additional applications such as learning word\nembeddings). In all settings, we demonstrate substantial improvement over the\nexisting state of the art along various dimensions, and in total, this work\nexpands the scope and scale of problems that can be solved using semidefinite\nprogramming methods. \n\n"}
{"id": "1706.01604", "contents": "Title: Hyperplane Clustering Via Dual Principal Component Pursuit Abstract: We extend the theoretical analysis of a recently proposed single subspace\nlearning algorithm, called Dual Principal Component Pursuit (DPCP), to the case\nwhere the data are drawn from of a union of hyperplanes. To gain insight into\nthe properties of the $\\ell_1$ non-convex problem associated with DPCP, we\ndevelop a geometric analysis of a closely related continuous optimization\nproblem. Then transferring this analysis to the discrete problem, our results\nstate that as long as the hyperplanes are sufficiently separated, the dominant\nhyperplane is sufficiently dominant and the points are uniformly distributed\ninside the associated hyperplanes, then the non-convex DPCP problem has a\nunique global solution, equal to the normal vector of the dominant hyperplane.\nThis suggests the correctness of a sequential hyperplane learning algorithm\nbased on DPCP. A thorough experimental evaluation reveals that hyperplane\nlearning schemes based on DPCP dramatically improve over the state-of-the-art\nmethods for the case of synthetic data, while are competitive to the\nstate-of-the-art in the case of 3D plane clustering for Kinect data. \n\n"}
{"id": "1706.01686", "contents": "Title: Limitations on Variance-Reduction and Acceleration Schemes for Finite\n  Sum Optimization Abstract: We study the conditions under which one is able to efficiently apply\nvariance-reduction and acceleration schemes on finite sum optimization\nproblems. First, we show that, perhaps surprisingly, the finite sum structure\nby itself, is not sufficient for obtaining a complexity bound of\n$\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly\nconvex individual functions - one must also know which individual function is\nbeing referred to by the oracle at each iteration. Next, we show that for a\nbroad class of first-order and coordinate-descent finite sum algorithms\n(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated'\ncomplexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless\nthe strong convexity parameter is given explicitly. Lastly, we show that when\nthis class of algorithms is used for minimizing $L$-smooth and convex finite\nsums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming\nthat (on average) the same update rule is used in every iteration, and\n$\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise. \n\n"}
{"id": "1706.01824", "contents": "Title: Robust Online Multi-Task Learning with Correlative and Personalized\n  Structures Abstract: Multi-Task Learning (MTL) can enhance a classifier's generalization\nperformance by learning multiple related tasks simultaneously. Conventional MTL\nworks under the offline or batch setting, and suffers from expensive training\ncost and poor scalability. To address such inefficiency issues, online learning\ntechniques have been applied to solve MTL problems. However, most existing\nalgorithms of online MTL constrain task relatedness into a presumed structure\nvia a single weight matrix, which is a strict restriction that does not always\nhold in practice. In this paper, we propose a robust online MTL framework that\novercomes this restriction by decomposing the weight matrix into two\ncomponents: the first one captures the low-rank common structure among tasks\nvia a nuclear norm and the second one identifies the personalized patterns of\noutlier tasks via a group lasso. Theoretical analysis shows the proposed\nalgorithm can achieve a sub-linear regret with respect to the best linear model\nin hindsight. Even though the above framework achieves good performance, the\nnuclear norm that simply adds all nonzero singular values together may not be a\ngood low-rank approximation. To improve the results, we use a log-determinant\nfunction as a non-convex rank approximation. The gradient scheme is applied to\noptimize log-determinant function and can obtain a closed-form solution for\nthis refined problem. Experimental results on a number of real-world\napplications verify the efficacy of our method. \n\n"}
{"id": "1706.02216", "contents": "Title: Inductive Representation Learning on Large Graphs Abstract: Low-dimensional embeddings of nodes in large graphs have proved extremely\nuseful in a variety of prediction tasks, from content recommendation to\nidentifying protein functions. However, most existing approaches require that\nall nodes in the graph are present during training of the embeddings; these\nprevious approaches are inherently transductive and do not naturally generalize\nto unseen nodes. Here we present GraphSAGE, a general, inductive framework that\nleverages node feature information (e.g., text attributes) to efficiently\ngenerate node embeddings for previously unseen data. Instead of training\nindividual embeddings for each node, we learn a function that generates\nembeddings by sampling and aggregating features from a node's local\nneighborhood. Our algorithm outperforms strong baselines on three inductive\nnode-classification benchmarks: we classify the category of unseen nodes in\nevolving information graphs based on citation and Reddit post data, and we show\nthat our algorithm generalizes to completely unseen graphs using a multi-graph\ndataset of protein-protein interactions. \n\n"}
{"id": "1706.02332", "contents": "Title: Low-shot learning with large-scale diffusion Abstract: This paper considers the problem of inferring image labels from images when\nonly a few annotated examples are available at training time. This setup is\noften referred to as low-shot learning, where a standard approach is to\nre-train the last few layers of a convolutional neural network learned on\nseparate classes for which training examples are abundant. We consider a\nsemi-supervised setting based on a large collection of images to support label\npropagation. This is possible by leveraging the recent advances on large-scale\nsimilarity graph construction.\n  We show that despite its conceptual simplicity, scaling label propagation up\nto hundred millions of images leads to state of the art accuracy in the\nlow-shot learning regime. \n\n"}
{"id": "1706.02375", "contents": "Title: Fast Black-box Variational Inference through Stochastic Trust-Region\n  Optimization Abstract: We introduce TrustVI, a fast second-order algorithm for black-box variational\ninference based on trust-region optimization and the reparameterization trick.\nAt each iteration, TrustVI proposes and assesses a step based on minibatches of\ndraws from the variational distribution. The algorithm provably converges to a\nstationary point. We implemented TrustVI in the Stan framework and compared it\nto two alternatives: Automatic Differentiation Variational Inference (ADVI) and\nHessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is\nbased on stochastic first-order optimization. The latter uses second-order\ninformation, but lacks convergence guarantees. TrustVI typically converged at\nleast one order of magnitude faster than ADVI, demonstrating the value of\nstochastic second-order information. TrustVI often found substantially better\nvariational distributions than HFSGVI, demonstrating that our convergence\ntheory can matter in practice. \n\n"}
{"id": "1706.02409", "contents": "Title: A Convex Framework for Fair Regression Abstract: We introduce a flexible family of fairness regularizers for (linear and\nlogistic) regression problems. These regularizers all enjoy convexity,\npermitting fast optimization, and they span the rang from notions of group\nfairness to strong individual fairness. By varying the weight on the fairness\nregularizer, we can compute the efficient frontier of the accuracy-fairness\ntrade-off on any given dataset, and we measure the severity of this trade-off\nvia a numerical quantity we call the Price of Fairness (PoF). The centerpiece\nof our results is an extensive comparative study of the PoF across six\ndifferent datasets in which fairness is a primary consideration. \n\n"}
{"id": "1706.05966", "contents": "Title: Deep Counterfactual Networks with Propensity-Dropout Abstract: We propose a novel approach for inferring the individualized causal effects\nof a treatment (intervention) from observational data. Our approach\nconceptualizes causal inference as a multitask learning problem; we model a\nsubject's potential outcomes using a deep multitask network with a set of\nshared layers among the factual and counterfactual outcomes, and a set of\noutcome-specific layers. The impact of selection bias in the observational data\nis alleviated via a propensity-dropout regularization scheme, in which the\nnetwork is thinned for every training example via a dropout probability that\ndepends on the associated propensity score. The network is trained in\nalternating phases, where in each phase we use the training examples of one of\nthe two potential outcomes (treated and control populations) to update the\nweights of the shared layers and the respective outcome-specific layers.\nExperiments conducted on data based on a real-world observational study show\nthat our algorithm outperforms the state-of-the-art. \n\n"}
{"id": "1706.06341", "contents": "Title: SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced\n  Learning Abstract: It is known that Boosting can be interpreted as a gradient descent technique\nto minimize an underlying loss function. Specifically, the underlying loss\nbeing minimized by the traditional AdaBoost is the exponential loss, which is\nproved to be very sensitive to random noise/outliers. Therefore, several\nBoosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to\nimprove the robustness of AdaBoost by replacing the exponential loss with some\ndesigned robust loss functions. In this work, we present a new way to robustify\nAdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning\n(SPL) into Boosting framework. Specifically, we design a new robust Boosting\nalgorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented\nby slightly modifying off-the-shelf Boosting packages. Extensive experiments\nand a theoretical characterization are also carried out to illustrate the\nmerits of the proposed SPLBoost. \n\n"}
{"id": "1706.07180", "contents": "Title: Compressive Statistical Learning with Random Feature Moments Abstract: We describe a general framework -- compressive statistical learning -- for\nresource-efficient large-scale learning: the training collection is compressed\nin one pass into a low-dimensional sketch (a vector of random empirical\ngeneralized moments) that captures the information relevant to the considered\nlearning task. A near-minimizer of the risk is computed from the sketch through\nthe solution of a nonlinear least squares problem. We investigate sufficient\nsketch sizes to control the generalization error of this procedure. The\nframework is illustrated on compressive PCA, compressive clustering, and\ncompressive Gaussian mixture Modeling with fixed known variance. The latter two\nare further developed in a companion paper. \n\n"}
{"id": "1706.07535", "contents": "Title: Efficient Approximate Solutions to Mutual Information Based Global\n  Feature Selection Abstract: Mutual Information (MI) is often used for feature selection when developing\nclassifier models. Estimating the MI for a subset of features is often\nintractable. We demonstrate, that under the assumptions of conditional\nindependence, MI between a subset of features can be expressed as the\nConditional Mutual Information (CMI) between pairs of features. But selecting\nfeatures with the highest CMI turns out to be a hard combinatorial problem. In\nthis work, we have applied two unique global methods, Truncated Power Method\n(TPower) and Low Rank Bilinear Approximation (LowRank), to solve the feature\nselection problem. These algorithms provide very good approximations to the\nNP-hard CMI based feature selection problem. We experimentally demonstrate the\neffectiveness of these procedures across multiple datasets and compare them\nwith existing MI based global and iterative feature selection procedures. \n\n"}
{"id": "1706.08934", "contents": "Title: Reexamining Low Rank Matrix Factorization for Trace Norm Regularization Abstract: Trace norm regularization is a widely used approach for learning low rank\nmatrices. A standard optimization strategy is based on formulating the problem\nas one of low rank matrix factorization which, however, leads to a non-convex\nproblem. In practice this approach works well, and it is often computationally\nfaster than standard convex solvers such as proximal gradient methods.\nNevertheless, it is not guaranteed to converge to a global optimum, and the\noptimization can be trapped at poor stationary points. In this paper we show\nthat it is possible to characterize all critical points of the non-convex\nproblem. This allows us to provide an efficient criterion to determine whether\na critical point is also a global minimizer. Our analysis suggests an iterative\nmeta-algorithm that dynamically expands the parameter space and allows the\noptimization to escape any non-global critical point, thereby converging to a\nglobal minimizer. The algorithm can be applied to problems such as matrix\ncompletion or multitask learning, and our analysis holds for any random\ninitialization of the factor matrices. Finally, we confirm the good performance\nof the algorithm on synthetic and real datasets. \n\n"}
{"id": "1706.09916", "contents": "Title: Graph Convolution: A High-Order and Adaptive Approach Abstract: In this paper, we presented a novel convolutional neural network framework\nfor graph modeling, with the introduction of two new modules specially designed\nfor graph-structured data: the $k$-th order convolution operator and the\nadaptive filtering module. Importantly, our framework of High-order and\nAdaptive Graph Convolutional Network (HA-GCN) is a general-purposed\narchitecture that fits various applications on both node and graph centrics, as\nwell as graph generative models. We conducted extensive experiments on\ndemonstrating the advantages of our framework. Particularly, our HA-GCN\noutperforms the state-of-the-art models on node classification and molecule\nproperty prediction tasks. It also generates 32% more real molecules on the\nmolecule generation task, both of which will significantly benefit real-world\napplications such as material design and drug screening. \n\n"}
{"id": "1707.00117", "contents": "Title: SAM: Semantic Attribute Modulation for Language Modeling and Style\n  Variation Abstract: This paper presents a Semantic Attribute Modulation (SAM) for language\nmodeling and style variation. The semantic attribute modulation includes\nvarious document attributes, such as titles, authors, and document categories.\nWe consider two types of attributes, (title attributes and category\nattributes), and a flexible attribute selection scheme by automatically scoring\nthem via an attribute attention mechanism. The semantic attributes are embedded\ninto the hidden semantic space as the generation inputs. With the attributes\nproperly harnessed, our proposed SAM can generate interpretable texts with\nregard to the input attributes. Qualitative analysis, including word semantic\nanalysis and attention values, shows the interpretability of SAM. On several\ntypical text datasets, we empirically demonstrate the superiority of the\nSemantic Attribute Modulated language model with different combinations of\ndocument attributes. Moreover, we present a style variation for the lyric\ngeneration using SAM, which shows a strong connection between the style\nvariation and the semantic attributes. \n\n"}
{"id": "1707.00143", "contents": "Title: Fast Approximate Nearest Neighbor Search With The Navigating\n  Spreading-out Graph Abstract: Approximate nearest neighbor search (ANNS) is a fundamental problem in\ndatabases and data mining. A scalable ANNS algorithm should be both\nmemory-efficient and fast. Some early graph-based approaches have shown\nattractive theoretical guarantees on search time complexity, but they all\nsuffer from the problem of high indexing time complexity. Recently, some\ngraph-based methods have been proposed to reduce indexing complexity by\napproximating the traditional graphs; these methods have achieved revolutionary\nperformance on million-scale datasets. Yet, they still can not scale to\nbillion-node databases. In this paper, to further improve the search-efficiency\nand scalability of graph-based methods, we start by introducing four aspects:\n(1) ensuring the connectivity of the graph; (2) lowering the average out-degree\nof the graph for fast traversal; (3) shortening the search path; and (4)\nreducing the index size. Then, we propose a novel graph structure called\nMonotonic Relative Neighborhood Graph (MRNG) which guarantees very low search\ncomplexity (close to logarithmic time). To further lower the indexing\ncomplexity and make it practical for billion-node ANNS problems, we propose a\nnovel graph structure named Navigating Spreading-out Graph (NSG) by\napproximating the MRNG. The NSG takes the four aspects into account\nsimultaneously. Extensive experiments show that NSG outperforms all the\nexisting algorithms significantly. In addition, NSG shows superior performance\nin the E-commercial search scenario of Taobao (Alibaba Group) and has been\nintegrated into their search engine at billion-node scale. \n\n"}
{"id": "1707.00724", "contents": "Title: Efficient Probabilistic Performance Bounds for Inverse Reinforcement\n  Learning Abstract: In the field of reinforcement learning there has been recent progress towards\nsafety and high-confidence bounds on policy performance. However, to our\nknowledge, no practical methods exist for determining high-confidence policy\nperformance bounds in the inverse reinforcement learning setting---where the\ntrue reward function is unknown and only samples of expert behavior are given.\nWe propose a sampling method based on Bayesian inverse reinforcement learning\nthat uses demonstrations to determine practical high-confidence upper bounds on\nthe $\\alpha$-worst-case difference in expected return between any evaluation\npolicy and the optimal policy under the expert's unknown reward function. We\nevaluate our proposed bound on both a standard grid navigation task and a\nsimulated driving task and achieve tighter and more accurate bounds than a\nfeature count-based baseline. We also give examples of how our proposed bound\ncan be utilized to perform risk-aware policy selection and risk-aware policy\nimprovement. Because our proposed bound requires several orders of magnitude\nfewer demonstrations than existing high-confidence bounds, it is the first\npractical method that allows agents that learn from demonstration to express\nconfidence in the quality of their learned policy. \n\n"}
{"id": "1707.01932", "contents": "Title: End-to-End Learning of Semantic Grasping Abstract: We consider the task of semantic robotic grasping, in which a robot picks up\nan object of a user-specified class using only monocular images. Inspired by\nthe two-stream hypothesis of visual reasoning, we present a semantic grasping\nframework that learns object detection, classification, and grasp planning in\nan end-to-end fashion. A \"ventral stream\" recognizes object class while a\n\"dorsal stream\" simultaneously interprets the geometric relationships necessary\nto execute successful grasps. We leverage the autonomous data collection\ncapabilities of robots to obtain a large self-supervised dataset for training\nthe dorsal stream, and use semi-supervised label propagation to train the\nventral stream with only a modest amount of human supervision. We\nexperimentally show that our approach improves upon grasping systems whose\ncomponents are not learned end-to-end, including a baseline method that uses\nbounding box detection. Furthermore, we show that jointly training our model\nwith auxiliary data consisting of non-semantic grasping data, as well as\nsemantically labeled images without grasp actions, has the potential to\nsubstantially improve semantic grasping performance. \n\n"}
{"id": "1707.03190", "contents": "Title: Accelerated Variance Reduced Stochastic ADMM Abstract: Recently, many variance reduced stochastic alternating direction method of\nmultipliers (ADMM) methods (e.g.\\ SAG-ADMM, SDCA-ADMM and SVRG-ADMM) have made\nexciting progress such as linear convergence rates for strongly convex\nproblems. However, the best known convergence rate for general convex problems\nis O(1/T) as opposed to O(1/T^2) of accelerated batch algorithms, where $T$ is\nthe number of iterations. Thus, there still remains a gap in convergence rates\nbetween existing stochastic ADMM and batch algorithms. To bridge this gap, we\nintroduce the momentum acceleration trick for batch optimization into the\nstochastic variance reduced gradient based ADMM (SVRG-ADMM), which leads to an\naccelerated (ASVRG-ADMM) method. Then we design two different momentum term\nupdate rules for strongly convex and general convex cases. We prove that\nASVRG-ADMM converges linearly for strongly convex problems. Besides having a\nlow per-iteration complexity as existing stochastic ADMM methods, ASVRG-ADMM\nimproves the convergence rate on general convex problems from O(1/T) to\nO(1/T^2). Our experimental results show the effectiveness of ASVRG-ADMM. \n\n"}
{"id": "1707.04347", "contents": "Title: Weakly Submodular Maximization Beyond Cardinality Constraints: Does\n  Randomization Help Greedy? Abstract: Submodular functions are a broad class of set functions, which naturally\narise in diverse areas. Many algorithms have been suggested for the\nmaximization of these functions. Unfortunately, once the function deviates from\nsubmodularity, the known algorithms may perform arbitrarily poorly. Amending\nthis issue, by obtaining approximation results for set functions generalizing\nsubmodular functions, has been the focus of recent works.\n  One such class, known as weakly submodular functions, has received a lot of\nattention. A key result proved by Das and Kempe (2011) showed that the\napproximation ratio of the greedy algorithm for weakly submodular maximization\nsubject to a cardinality constraint degrades smoothly with the distance from\nsubmodularity. However, no results have been obtained for maximization subject\nto constraints beyond cardinality. In particular, it is not known whether the\ngreedy algorithm achieves any non-trivial approximation ratio for such\nconstraints.\n  In this paper, we prove that a randomized version of the greedy algorithm\n(previously used by Buchbinder et al. (2014) for a different problem) achieves\nan approximation ratio of $(1 + 1/\\gamma)^{-2}$ for the maximization of a\nweakly submodular function subject to a general matroid constraint, where\n$\\gamma$ is a parameter measuring the distance of the function from\nsubmodularity. Moreover, we also experimentally compare the performance of this\nversion of the greedy algorithm on real world problems against natural\nbenchmarks, and show that the algorithm we study performs well also in\npractice. To the best of our knowledge, this is the first algorithm with a\nnon-trivial approximation guarantee for maximizing a weakly submodular function\nsubject to a constraint other than the simple cardinality constraint. In\nparticular, it is the first algorithm with such a guarantee for the important\nand broad class of matroid constraints. \n\n"}
{"id": "1707.05587", "contents": "Title: Graph learning under sparsity priors Abstract: Graph signals offer a very generic and natural representation for data that\nlives on networks or irregular structures. The actual data structure is however\noften unknown a priori but can sometimes be estimated from the knowledge of the\napplication domain. If this is not possible, the data structure has to be\ninferred from the mere signal observations. This is exactly the problem that we\naddress in this paper, under the assumption that the graph signals can be\nrepresented as a sparse linear combination of a few atoms of a structured graph\ndictionary. The dictionary is constructed on polynomials of the graph\nLaplacian, which can sparsely represent a general class of graph signals\ncomposed of localized patterns on the graph. We formulate a graph learning\nproblem, whose solution provides an ideal fit between the signal observations\nand the sparse graph signal model. As the problem is non-convex, we propose to\nsolve it by alternating between a signal sparse coding and a graph update step.\nWe provide experimental results that outline the good graph recovery\nperformance of our method, which generally compares favourably to other recent\nnetwork inference algorithms. \n\n"}
{"id": "1707.08616", "contents": "Title: Guiding Reinforcement Learning Exploration Using Natural Language Abstract: In this work we present a technique to use natural language to help\nreinforcement learning generalize to unseen environments. This technique uses\nneural machine translation, specifically the use of encoder-decoder networks,\nto learn associations between natural language behavior descriptions and\nstate-action information. We then use this learned model to guide agent\nexploration using a modified version of policy shaping to make it more\neffective at learning in unseen environments. We evaluate this technique using\nthe popular arcade game, Frogger, under ideal and non-ideal conditions. This\nevaluation shows that our modified policy shaping algorithm improves over a\nQ-learning agent as well as a baseline version of policy shaping. \n\n"}
{"id": "1708.00308", "contents": "Title: SenGen: Sentence Generating Neural Variational Topic Model Abstract: We present a new topic model that generates documents by sampling a topic for\none whole sentence at a time, and generating the words in the sentence using an\nRNN decoder that is conditioned on the topic of the sentence. We argue that\nthis novel formalism will help us not only visualize and model the topical\ndiscourse structure in a document better, but also potentially lead to more\ninterpretable topics since we can now illustrate topics by sampling\nrepresentative sentences instead of bag of words or phrases. We present a\nvariational auto-encoder approach for learning in which we use a factorized\nvariational encoder that independently models the posterior over topical\nmixture vectors of documents using a feed-forward network, and the posterior\nover topic assignments to sentences using an RNN. Our preliminary experiments\non two different datasets indicate early promise, but also expose many\nchallenges that remain to be addressed. \n\n"}
{"id": "1708.01519", "contents": "Title: A Latent Variable Model for Two-Dimensional Canonical Correlation\n  Analysis and its Variational Inference Abstract: Describing the dimension reduction (DR) techniques by means of probabilistic\nmodels has recently been given special attention. Probabilistic models, in\naddition to a better interpretability of the DR methods, provide a framework\nfor further extensions of such algorithms. One of the new approaches to the\nprobabilistic DR methods is to preserving the internal structure of data. It is\nmeant that it is not necessary that the data first be converted from the matrix\nor tensor format to the vector format in the process of dimensionality\nreduction. In this paper, a latent variable model for matrix-variate data for\ncanonical correlation analysis (CCA) is proposed. Since in general there is not\nany analytical maximum likelihood solution for this model, we present two\napproaches for learning the parameters. The proposed methods are evaluated\nusing the synthetic data in terms of convergence and quality of mappings. Also,\nreal data set is employed for assessing the proposed methods with several\nprobabilistic and none-probabilistic CCA based approaches. The results confirm\nthe superiority of the proposed methods with respect to the competing\nalgorithms. Moreover, this model can be considered as a framework for further\nextensions. \n\n"}
{"id": "1708.01648", "contents": "Title: 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks Abstract: The success of various applications including robotics, digital content\ncreation, and visualization demand a structured and abstract representation of\nthe 3D world from limited sensor data. Inspired by the nature of human\nperception of 3D shapes as a collection of simple parts, we explore such an\nabstract shape representation based on primitives. Given a single depth image\nof an object, we present 3D-PRNN, a generative recurrent neural network that\nsynthesizes multiple plausible shapes composed of a set of primitives. Our\ngenerative model encodes symmetry characteristics of common man-made objects,\npreserves long-range structural coherence, and describes objects of varying\ncomplexity with a compact representation. We also propose a method based on\nGaussian Fields to generate a large scale dataset of primitive-based shape\nrepresentations to train our network. We evaluate our approach on a wide range\nof examples and show that it outperforms nearest-neighbor based shape retrieval\nmethods and is on-par with voxel-based generative models while using a\nsignificantly reduced parameter space. \n\n"}
{"id": "1708.05123", "contents": "Title: Deep & Cross Network for Ad Click Predictions Abstract: Feature engineering has been the key to the success of many prediction\nmodels. However, the process is non-trivial and often requires manual feature\nengineering or exhaustive searching. DNNs are able to automatically learn\nfeature interactions; however, they generate all the interactions implicitly,\nand are not necessarily efficient in learning all types of cross features. In\nthis paper, we propose the Deep & Cross Network (DCN) which keeps the benefits\nof a DNN model, and beyond that, it introduces a novel cross network that is\nmore efficient in learning certain bounded-degree feature interactions. In\nparticular, DCN explicitly applies feature crossing at each layer, requires no\nmanual feature engineering, and adds negligible extra complexity to the DNN\nmodel. Our experimental results have demonstrated its superiority over the\nstate-of-art algorithms on the CTR prediction dataset and dense classification\ndataset, in terms of both model accuracy and memory usage. \n\n"}
{"id": "1708.05207", "contents": "Title: Learning Universal Adversarial Perturbations with Generative Models Abstract: Neural networks are known to be vulnerable to adversarial examples, inputs\nthat have been intentionally perturbed to remain visually similar to the source\ninput, but cause a misclassification. It was recently shown that given a\ndataset and classifier, there exists so called universal adversarial\nperturbations, a single perturbation that causes a misclassification when\napplied to any input. In this work, we introduce universal adversarial\nnetworks, a generative network that is capable of fooling a target classifier\nwhen it's generated output is added to a clean sample from a dataset. We show\nthat this technique improves on known universal adversarial attacks. \n\n"}
{"id": "1708.05629", "contents": "Title: Learning to Transfer Abstract: Transfer learning borrows knowledge from a source domain to facilitate\nlearning in a target domain. Two primary issues to be addressed in transfer\nlearning are what and how to transfer. For a pair of domains, adopting\ndifferent transfer learning algorithms results in different knowledge\ntransferred between them. To discover the optimal transfer learning algorithm\nthat maximally improves the learning performance in the target domain,\nresearchers have to exhaustively explore all existing transfer learning\nalgorithms, which is computationally intractable. As a trade-off, a sub-optimal\nalgorithm is selected, which requires considerable expertise in an ad-hoc way.\nMeanwhile, it is widely accepted in educational psychology that human beings\nimprove transfer learning skills of deciding what to transfer through\nmeta-cognitive reflection on inductive transfer learning practices. Motivated\nby this, we propose a novel transfer learning framework known as Learning to\nTransfer (L2T) to automatically determine what and how to transfer are the best\nby leveraging previous transfer learning experiences. We establish the L2T\nframework in two stages: 1) we first learn a reflection function encrypting\ntransfer learning skills from experiences; and 2) we infer what and how to\ntransfer for a newly arrived pair of domains by optimizing the reflection\nfunction. Extensive experiments demonstrate the L2T's superiority over several\nstate-of-the-art transfer learning algorithms and its effectiveness on\ndiscovering more transferable knowledge. \n\n"}
{"id": "1708.06020", "contents": "Title: Improving Deep Learning using Generic Data Augmentation Abstract: Deep artificial neural networks require a large corpus of training data in\norder to effectively learn, where collection of such training data is often\nexpensive and laborious. Data augmentation overcomes this issue by artificially\ninflating the training set with label preserving transformations. Recently\nthere has been extensive use of generic data augmentation to improve\nConvolutional Neural Network (CNN) task performance. This study benchmarks\nvarious popular data augmentation schemes to allow researchers to make informed\ndecisions as to which training methods are most appropriate for their data\nsets. Various geometric and photometric schemes are evaluated on a\ncoarse-grained data set using a relatively simple CNN. Experimental results,\nrun using 4-fold cross-validation and reported in terms of Top-1 and Top-5\naccuracy, indicate that cropping in geometric augmentation significantly\nincreases CNN task performance. \n\n"}
{"id": "1708.07178", "contents": "Title: Massively-Parallel Feature Selection for Big Data Abstract: We present the Parallel, Forward-Backward with Pruning (PFBP) algorithm for\nfeature selection (FS) in Big Data settings (high dimensionality and/or sample\nsize). To tackle the challenges of Big Data FS PFBP partitions the data matrix\nboth in terms of rows (samples, training examples) as well as columns\n(features). By employing the concepts of $p$-values of conditional independence\ntests and meta-analysis techniques PFBP manages to rely only on computations\nlocal to a partition while minimizing communication costs. Then, it employs\npowerful and safe (asymptotically sound) heuristics to make early, approximate\ndecisions, such as Early Dropping of features from consideration in subsequent\niterations, Early Stopping of consideration of features within the same\niteration, or Early Return of the winner in each iteration. PFBP provides\nasymptotic guarantees of optimality for data distributions faithfully\nrepresentable by a causal network (Bayesian network or maximal ancestral\ngraph). Our empirical analysis confirms a super-linear speedup of the algorithm\nwith increasing sample size, linear scalability with respect to the number of\nfeatures and processing cores, while dominating other competitive algorithms in\nits class. \n\n"}
{"id": "1708.07946", "contents": "Title: Sales Forecast in E-commerce using Convolutional Neural Network Abstract: Sales forecast is an essential task in E-commerce and has a crucial impact on\nmaking informed business decisions. It can help us to manage the workforce,\ncash flow and resources such as optimizing the supply chain of manufacturers\netc. Sales forecast is a challenging problem in that sales is affected by many\nfactors including promotion activities, price changes, and user preferences\netc. Traditional sales forecast techniques mainly rely on historical sales data\nto predict future sales and their accuracies are limited. Some more recent\nlearning-based methods capture more information in the model to improve the\nforecast accuracy. However, these methods require case-by-case manual feature\nengineering for specific commercial scenarios, which is usually a difficult,\ntime-consuming task and requires expert knowledge. To overcome the limitations\nof existing methods, we propose a novel approach in this paper to learn\neffective features automatically from the structured data using the\nConvolutional Neural Network (CNN). When fed with raw log data, our approach\ncan automatically extract effective features from that and then forecast sales\nusing those extracted features. We test our method on a large real-world\ndataset from CaiNiao.com and the experimental results validate the\neffectiveness of our method. \n\n"}
{"id": "1708.08705", "contents": "Title: Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary\n  Learning Abstract: The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,\nconsisting of a cascade of convolutional sparse layers, provides a new\ninterpretation of Convolutional Neural Networks (CNNs). Under this framework,\nthe computation of the forward pass in a CNN is equivalent to a pursuit\nalgorithm aiming to estimate the nested sparse representation vectors -- or\nfeature maps -- from a given input signal. Despite having served as a pivotal\nconnection between CNNs and sparse modeling, a deeper understanding of the\nML-CSC is still lacking: there are no pursuit algorithms that can serve this\nmodel exactly, nor are there conditions to guarantee a non-empty model. While\none can easily obtain signals that approximately satisfy the ML-CSC\nconstraints, it remains unclear how to simply sample from the model and, more\nimportantly, how one can train the convolutional filters from real data.\n  In this work, we propose a sound pursuit algorithm for the ML-CSC model by\nadopting a projection approach. We provide new and improved bounds on the\nstability of the solution of such pursuit and we analyze different practical\nalternatives to implement this in practice. We show that the training of the\nfilters is essential to allow for non-trivial signals in the model, and we\nderive an online algorithm to learn the dictionaries from real data,\neffectively resulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model for several\napplications in an unsupervised setting, providing competitive results. Our\nwork represents a bridge between matrix factorization, sparse dictionary\nlearning and sparse auto-encoders, and we analyze these connections in detail. \n\n"}
{"id": "1709.00199", "contents": "Title: A Two-Step Disentanglement Method Abstract: We address the problem of disentanglement of factors that generate a given\ndata into those that are correlated with the labeling and those that are not.\nOur solution is simpler than previous solutions and employs adversarial\ntraining. First, the part of the data that is correlated with the labels is\nextracted by training a classifier. Then, the other part is extracted such that\nit enables the reconstruction of the original data but does not contain label\ninformation. The utility of the new method is demonstrated on visual datasets\nas well as on financial data. Our code is available at\nhttps://github.com/naamahadad/A-Two-Step-Disentanglement-Method \n\n"}
{"id": "1709.01674", "contents": "Title: Probabilistic Rule Realization and Selection Abstract: Abstraction and realization are bilateral processes that are key in deriving\nintelligence and creativity. In many domains, the two processes are approached\nthrough rules: high-level principles that reveal invariances within similar yet\ndiverse examples. Under a probabilistic setting for discrete input spaces, we\nfocus on the rule realization problem which generates input sample\ndistributions that follow the given rules. More ambitiously, we go beyond a\nmechanical realization that takes whatever is given, but instead ask for\nproactively selecting reasonable rules to realize. This goal is demanding in\npractice, since the initial rule set may not always be consistent and thus\nintelligent compromises are needed. We formulate both rule realization and\nselection as two strongly connected components within a single and symmetric\nbi-convex problem, and derive an efficient algorithm that works at large scale.\nTaking music compositional rules as the main example throughout the paper, we\ndemonstrate our model's efficiency in not only music realization (composition)\nbut also music interpretation and understanding (analysis). \n\n"}
{"id": "1709.01870", "contents": "Title: Clustering of Data with Missing Entries using Non-convex Fusion\n  Penalties Abstract: The presence of missing entries in data often creates challenges for pattern\nrecognition algorithms. Traditional algorithms for clustering data assume that\nall the feature values are known for every data point. We propose a method to\ncluster data in the presence of missing information. Unlike conventional\nclustering techniques where every feature is known for each point, our\nalgorithm can handle cases where a few feature values are unknown for every\npoint. For this more challenging problem, we provide theoretical guarantees for\nclustering using a $\\ell_0$ fusion penalty based optimization problem.\nFurthermore, we propose an algorithm to solve a relaxation of this problem\nusing saturating non-convex fusion penalties. It is observed that this\nalgorithm produces solutions that degrade gradually with an increase in the\nfraction of missing feature values. We demonstrate the utility of the proposed\nmethod using a simulated dataset, the Wine dataset and also an under-sampled\ncardiac MRI dataset. It is shown that the proposed method is a promising\nclustering technique for datasets with large fractions of missing entries. \n\n"}
{"id": "1709.02797", "contents": "Title: On the exact relationship between the denoising function and the data\n  distribution Abstract: We prove an exact relationship between the optimal denoising function and the\ndata distribution in the case of additive Gaussian noise, showing that\ndenoising implicitly models the structure of data allowing it to be exploited\nin the unsupervised learning of representations. This result generalizes a\nknown relationship [2], which is valid only in the limit of small corruption\nnoise. \n\n"}
{"id": "1709.06010", "contents": "Title: Learning Neural Networks with Two Nonlinear Layers in Polynomial Time Abstract: We give a polynomial-time algorithm for learning neural networks with one\nlayer of sigmoids feeding into any Lipschitz, monotone activation function\n(e.g., sigmoid or ReLU). We make no assumptions on the structure of the\nnetwork, and the algorithm succeeds with respect to {\\em any} distribution on\nthe unit ball in $n$ dimensions (hidden weight vectors also have unit norm).\nThis is the first assumption-free, provably efficient algorithm for learning\nneural networks with two nonlinear layers.\n  Our algorithm-- {\\em Alphatron}-- is a simple, iterative update rule that\ncombines isotonic regression with kernel methods. It outputs a hypothesis that\nyields efficient oracle access to interpretable features. It also suggests a\nnew approach to Boolean learning problems via real-valued conditional-mean\nfunctions, sidestepping traditional hardness results from computational\nlearning theory.\n  Along these lines, we subsume and improve many longstanding results for PAC\nlearning Boolean functions to the more general, real-valued setting of {\\em\nprobabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d.\nnoise-tolerance. \n\n"}
{"id": "1709.06390", "contents": "Title: Analogical-based Bayesian Optimization Abstract: Some real-world problems revolve to solve the optimization problem\n\\max_{x\\in\\mathcal{X}}f\\left(x\\right) where f\\left(.\\right) is a black-box\nfunction and X might be the set of non-vectorial objects (e.g., distributions)\nwhere we can only define a symmetric and non-negative similarity score on it.\nThis setting requires a novel view for the standard framework of Bayesian\nOptimization that generalizes the core insightful spirit of this framework.\nWith this spirit, in this paper, we propose Analogical-based Bayesian\nOptimization that can maximize black-box function over a domain where only a\nsimilarity score can be defined. Our pathway is as follows: we first base on\nthe geometric view of Gaussian Processes (GP) to define the concept of\ninfluence level that allows us to analytically represent predictive means and\nvariances of GP posteriors and base on that view to enable replacing kernel\nsimilarity by a more genetic similarity score. Furthermore, we also propose two\nstrategies to find a batch of query points that can efficiently handle high\ndimensional data. \n\n"}
{"id": "1709.06493", "contents": "Title: Learning to update Auto-associative Memory in Recurrent Neural Networks\n  for Improving Sequence Memorization Abstract: Learning to remember long sequences remains a challenging task for recurrent\nneural networks. Register memory and attention mechanisms were both proposed to\nresolve the issue with either high computational cost to retain memory\ndifferentiability, or by discounting the RNN representation learning towards\nencoding shorter local contexts than encouraging long sequence encoding.\nAssociative memory, which studies the compression of multiple patterns in a\nfixed size memory, were rarely considered in recent years. Although some recent\nwork tries to introduce associative memory in RNN and mimic the energy decay\nprocess in Hopfield nets, it inherits the shortcoming of rule-based memory\nupdates, and the memory capacity is limited. This paper proposes a method to\nlearn the memory update rule jointly with task objective to improve memory\ncapacity for remembering long sequences. Also, we propose an architecture that\nuses multiple such associative memory for more complex input encoding. We\nobserved some interesting facts when compared to other RNN architectures on\nsome well-studied sequence learning tasks. \n\n"}
{"id": "1709.06548", "contents": "Title: Triangle Generative Adversarial Networks Abstract: A Triangle Generative Adversarial Network ($\\Delta$-GAN) is developed for\nsemi-supervised cross-domain joint distribution matching, where the training\ndata consists of samples from each domain, and supervision of domain\ncorrespondence is provided by only a few paired samples. $\\Delta$-GAN consists\nof four neural networks, two generators and two discriminators. The generators\nare designed to learn the two-way conditional distributions between the two\ndomains, while the discriminators implicitly define a ternary discriminative\nfunction, which is trained to distinguish real data pairs and two kinds of fake\ndata pairs. The generators and discriminators are trained together using\nadversarial learning. Under mild assumptions, in theory the joint distributions\ncharacterized by the two generators concentrate to the data distribution. In\nexperiments, three different kinds of domain pairs are considered, image-label,\nimage-image and image-attribute pairs. Experiments on semi-supervised image\nclassification, image-to-image translation and attribute-based image generation\ndemonstrate the superiority of the proposed approach. \n\n"}
{"id": "1709.06636", "contents": "Title: An Attention-based Collaboration Framework for Multi-View Network\n  Representation Learning Abstract: Learning distributed node representations in networks has been attracting\nincreasing attention recently due to its effectiveness in a variety of\napplications. Existing approaches usually study networks with a single type of\nproximity between nodes, which defines a single view of a network. However, in\nreality there usually exists multiple types of proximities between nodes,\nyielding networks with multiple views. This paper studies learning node\nrepresentations for networks with multiple views, which aims to infer robust\nnode representations across different views. We propose a multi-view\nrepresentation learning approach, which promotes the collaboration of different\nviews and lets them vote for the robust representations. During the voting\nprocess, an attention mechanism is introduced, which enables each node to focus\non the most informative views. Experimental results on real-world networks show\nthat the proposed approach outperforms existing state-of-the-art approaches for\nnetwork representation learning with a single view and other competitive\napproaches with multiple views. \n\n"}
{"id": "1709.08201", "contents": "Title: An Optimal Online Method of Selecting Source Policies for Reinforcement\n  Learning Abstract: Transfer learning significantly accelerates the reinforcement learning\nprocess by exploiting relevant knowledge from previous experiences. The problem\nof optimally selecting source policies during the learning process is of great\nimportance yet challenging. There has been little theoretical analysis of this\nproblem. In this paper, we develop an optimal online method to select source\npolicies for reinforcement learning. This method formulates online source\npolicy selection as a multi-armed bandit problem and augments Q-learning with\npolicy reuse. We provide theoretical guarantees of the optimal selection\nprocess and convergence to the optimal policy. In addition, we conduct\nexperiments on a grid-based robot navigation domain to demonstrate its\nefficiency and robustness by comparing to the state-of-the-art transfer\nlearning method. \n\n"}
{"id": "1709.08519", "contents": "Title: Enhanced Quantum Synchronization via Quantum Machine Learning Abstract: We study the quantum synchronization between a pair of two-level systems\ninside two coupled cavities. By using a digital-analog decomposition of the\nmaster equation that rules the system dynamics, we show that this approach\nleads to quantum synchronization between both two-level systems. Moreover, we\ncan identify in this digital-analog block decomposition the fundamental\nelements of a quantum machine learning protocol, in which the agent and the\nenvironment (learning units) interact through a mediating system, namely, the\nregister. If we can additionally equip this algorithm with a classical feedback\nmechanism, which consists of projective measurements in the register,\nreinitialization of the register state and local conditional operations on the\nagent and environment subspace, a powerful and flexible quantum machine\nlearning protocol emerges. Indeed, numerical simulations show that this\nprotocol enhances the synchronization process, even when every subsystem\nexperience different loss/decoherence mechanisms, and give us the flexibility\nto choose the synchronization state. Finally, we propose an implementation\nbased on current technologies in superconducting circuits. \n\n"}
{"id": "1709.09018", "contents": "Title: AutoEncoder by Forest Abstract: Auto-encoding is an important task which is typically realized by deep neural\nnetworks (DNNs) such as convolutional neural networks (CNN). In this paper, we\npropose EncoderForest (abbrv. eForest), the first tree ensemble based\nauto-encoder. We present a procedure for enabling forests to do backward\nreconstruction by utilizing the equivalent classes defined by decision paths of\nthe trees, and demonstrate its usage in both supervised and unsupervised\nsetting. Experiments show that, compared with DNN autoencoders, eForest is able\nto obtain lower reconstruction error with fast training speed, while the model\nitself is reusable and damage-tolerable. \n\n"}
{"id": "1709.10030", "contents": "Title: Sparse Hierarchical Regression with Polynomials Abstract: We present a novel method for exact hierarchical sparse polynomial\nregression. Our regressor is that degree $r$ polynomial which depends on at\nmost $k$ inputs, counting at most $\\ell$ monomial terms, which minimizes the\nsum of the squares of its prediction errors. The previous hierarchical sparse\nspecification aligns well with modern big data settings where many inputs are\nnot relevant for prediction purposes and the functional complexity of the\nregressor needs to be controlled as to avoid overfitting. We present a two-step\napproach to this hierarchical sparse regression problem. First, we discard\nirrelevant inputs using an extremely fast input ranking heuristic. Secondly, we\ntake advantage of modern cutting plane methods for integer optimization to\nsolve our resulting reduced hierarchical $(k, \\ell)$-sparse problem exactly.\nThe ability of our method to identify all $k$ relevant inputs and all $\\ell$\nmonomial terms is shown empirically to experience a phase transition.\nCrucially, the same transition also presents itself in our ability to reject\nall irrelevant features and monomials as well. In the regime where our method\nis statistically powerful, its computational complexity is interestingly on par\nwith Lasso based heuristics. The presented work fills a void in terms of a lack\nof powerful disciplined nonlinear sparse regression methods in high-dimensional\nsettings. Our method is shown empirically to scale to regression problems with\n$n\\approx 10,000$ observations for input dimension $p\\approx 1,000$. \n\n"}
{"id": "1710.00211", "contents": "Title: The Deep Ritz method: A deep learning-based numerical algorithm for\n  solving variational problems Abstract: We propose a deep learning based method, the Deep Ritz Method, for\nnumerically solving variational problems, particularly the ones that arise from\npartial differential equations. The Deep Ritz method is naturally nonlinear,\nnaturally adaptive and has the potential to work in rather high dimensions. The\nframework is quite simple and fits well with the stochastic gradient descent\nmethod used in deep learning. We illustrate the method on several problems\nincluding some eigenvalue problems. \n\n"}
{"id": "1710.00482", "contents": "Title: Weighted-SVD: Matrix Factorization with Weights on the Latent Factors Abstract: The Matrix Factorization models, sometimes called the latent factor models,\nare a family of methods in the recommender system research area to (1) generate\nthe latent factors for the users and the items and (2) predict users' ratings\non items based on their latent factors. However, current Matrix Factorization\nmodels presume that all the latent factors are equally weighted, which may not\nalways be a reasonable assumption in practice. In this paper, we propose a new\nmodel, called Weighted-SVD, to integrate the linear regression model with the\nSVD model such that each latent factor accompanies with a corresponding weight\nparameter. This mechanism allows the latent factors have different weights to\ninfluence the final ratings. The complexity of the Weighted-SVD model is\nslightly larger than the SVD model but much smaller than the SVD++ model. We\ncompared the Weighted-SVD model with several latent factor models on five\npublic datasets based on the Root-Mean-Squared-Errors (RMSEs). The results show\nthat the Weighted-SVD model outperforms the baseline methods in all the\nexperimental datasets under almost all settings. \n\n"}
{"id": "1710.00904", "contents": "Title: Online and Distributed Robust Regressions under Adversarial Data\n  Corruption Abstract: In today's era of big data, robust least-squares regression becomes a more\nchallenging problem when considering the adversarial corruption along with\nexplosive growth of datasets. Traditional robust methods can handle the noise\nbut suffer from several challenges when applied in huge dataset including 1)\ncomputational infeasibility of handling an entire dataset at once, 2) existence\nof heterogeneously distributed corruption, and 3) difficulty in corruption\nestimation when data cannot be entirely loaded. This paper proposes online and\ndistributed robust regression approaches, both of which can concurrently\naddress all the above challenges. Specifically, the distributed algorithm\noptimizes the regression coefficients of each data block via heuristic hard\nthresholding and combines all the estimates in a distributed robust\nconsolidation. Furthermore, an online version of the distributed algorithm is\nproposed to incrementally update the existing estimates with new incoming data.\nWe also prove that our algorithms benefit from strong robustness guarantees in\nterms of regression coefficient recovery with a constant upper bound on the\nerror of state-of-the-art batch methods. Extensive experiments on synthetic and\nreal datasets demonstrate that our approaches are superior to those of existing\nmethods in effectiveness, with competitive efficiency. \n\n"}
{"id": "1710.01013", "contents": "Title: Training Feedforward Neural Networks with Standard Logistic Activations\n  is Feasible Abstract: Training feedforward neural networks with standard logistic activations is\nconsidered difficult because of the intrinsic properties of these sigmoidal\nfunctions. This work aims at showing that these networks can be trained to\nachieve generalization performance comparable to those based on hyperbolic\ntangent activations. The solution consists on applying a set of conditions in\nparameter initialization, which have been derived from the study of the\nproperties of a single neuron from an information-theoretic perspective. The\nproposed initialization is validated through an extensive experimental\nanalysis. \n\n"}
{"id": "1710.02971", "contents": "Title: Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec Abstract: Since the invention of word2vec, the skip-gram model has significantly\nadvanced the research of network embedding, such as the recent emergence of the\nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of\nthe aforementioned models with negative sampling can be unified into the matrix\nfactorization framework with closed forms. Our analysis and proofs reveal that:\n(1) DeepWalk empirically produces a low-rank transformation of a network's\nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk\nwhen the size of vertices' context is set to one; (3) As an extension of LINE,\nPTE can be viewed as the joint factorization of multiple networks' Laplacians;\n(4) node2vec is factorizing a matrix related to the stationary distribution and\ntransition probability tensor of a 2nd-order random walk. We further provide\nthe theoretical connections between skip-gram based network embedding\nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF\nmethod as well as its approximation algorithm for computing network embedding.\nOur method offers significant improvements over DeepWalk and LINE for\nconventional network mining tasks. This work lays the theoretical foundation\nfor skip-gram based network embedding methods, leading to a better\nunderstanding of latent network representation learning. \n\n"}
{"id": "1710.03297", "contents": "Title: Sum-Product Networks for Hybrid Domains Abstract: While all kinds of mixed data -from personal data, over panel and scientific\ndata, to public and commercial data- are collected and stored, building\nprobabilistic graphical models for these hybrid domains becomes more difficult.\nUsers spend significant amounts of time in identifying the parametric form of\nthe random variables (Gaussian, Poisson, Logit, etc.) involved and learning the\nmixed models. To make this difficult task easier, we propose the first\ntrainable probabilistic deep architecture for hybrid domains that features\ntractable queries. It is based on Sum-Product Networks (SPNs) with piecewise\npolynomial leave distributions together with novel nonparametric decomposition\nand conditioning steps using the Hirschfeld-Gebelein-R\\'enyi Maximum\nCorrelation Coefficient. This relieves the user from deciding a-priori the\nparametric form of the random variables but is still expressive enough to\neffectively approximate any continuous distribution and permits efficient\nlearning and inference. Our empirical evidence shows that the architecture,\ncalled Mixed SPNs, can indeed capture complex distributions across a wide range\nof hybrid domains. \n\n"}
{"id": "1710.03634", "contents": "Title: LinXGBoost: Extension of XGBoost to Generalized Local Linear Models Abstract: XGBoost is often presented as the algorithm that wins every ML competition.\nSurprisingly, this is true even though predictions are piecewise constant. This\nmight be justified in high dimensional input spaces, but when the number of\nfeatures is low, a piecewise linear model is likely to perform better. XGBoost\nwas extended into LinXGBoost that stores at each leaf a linear model. This\nextension, equivalent to piecewise regularized least-squares, is particularly\nattractive for regression of functions that exhibits jumps or discontinuities.\nThose functions are notoriously hard to regress. Our extension is compared to\nthe vanilla XGBoost and Random Forest in experiments on both synthetic and\nreal-world data sets. \n\n"}
{"id": "1710.04584", "contents": "Title: Towards Scalable Spectral Clustering via Spectrum-Preserving\n  Sparsification Abstract: The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is\nthe main computational bottleneck in spectral clustering. In this work, we\nintroduce a highly-scalable, spectrum-preserving graph sparsification algorithm\nthat enables to build ultra-sparse NN (u-NN) graphs with guaranteed\npreservation of the original graph spectrums, such as the first few\neigenvectors of the original graph Laplacian. Our approach can immediately lead\nto scalable spectral clustering of large data networks without sacrificing\nsolution quality. The proposed method starts from constructing low-stretch\nspanning trees (LSSTs) from the original graphs, which is followed by\niteratively recovering small portions of \"spectrally critical\" off-tree edges\nto the LSSTs by leveraging a spectral off-tree embedding scheme. To determine\nthe suitable amount of off-tree edges to be recovered to the LSSTs, an\neigenvalue stability checking scheme is proposed, which enables to robustly\npreserve the first few Laplacian eigenvectors within the sparsified graph.\nAdditionally, an incremental graph densification scheme is proposed for\nidentifying extra edges that have been missing in the original NN graphs but\ncan still play important roles in spectral clustering tasks. Our experimental\nresults for a variety of well-known data sets show that the proposed method can\ndramatically reduce the complexity of NN graphs, leading to significant\nspeedups in spectral clustering. \n\n"}
{"id": "1710.04806", "contents": "Title: Deep Learning for Case-Based Reasoning through Prototypes: A Neural\n  Network that Explains Its Predictions Abstract: Deep neural networks are widely used for classification. These deep models\noften suffer from a lack of interpretability -- they are particularly difficult\nto understand because of their non-linear nature. As a result, neural networks\nare often treated as \"black box\" models, and in the past, have been trained\npurely to optimize the accuracy of predictions. In this work, we create a novel\nnetwork architecture for deep learning that naturally explains its own\nreasoning for each prediction. This architecture contains an autoencoder and a\nspecial prototype layer, where each unit of that layer stores a weight vector\nthat resembles an encoded training input. The encoder of the autoencoder allows\nus to do comparisons within the latent space, while the decoder allows us to\nvisualize the learned prototypes. The training objective has four terms: an\naccuracy term, a term that encourages every prototype to be similar to at least\none encoded input, a term that encourages every encoded input to be close to at\nleast one prototype, and a term that encourages faithful reconstruction by the\nautoencoder. The distances computed in the prototype layer are used as part of\nthe classification process. Since the prototypes are learned during training,\nthe learned network naturally comes with explanations for each prediction, and\nthe explanations are loyal to what the network actually computes. \n\n"}
{"id": "1710.04881", "contents": "Title: User Modelling for Avoiding Overfitting in Interactive Knowledge\n  Elicitation for Prediction Abstract: In human-in-the-loop machine learning, the user provides information beyond\nthat in the training data. Many algorithms and user interfaces have been\ndesigned to optimize and facilitate this human--machine interaction; however,\nfewer studies have addressed the potential defects the designs can cause.\nEffective interaction often requires exposing the user to the training data or\nits statistics. The design of the system is then critical, as this can lead to\ndouble use of data and overfitting, if the user reinforces noisy patterns in\nthe data. We propose a user modelling methodology, by assuming simple rational\nbehaviour, to correct the problem. We show, in a user study with 48\nparticipants, that the method improves predictive performance in a sparse\nlinear regression sentiment analysis task, where graded user knowledge on\nfeature relevance is elicited. We believe that the key idea of inferring user\nknowledge with probabilistic user models has general applicability in guarding\nagainst overfitting and improving interactive machine learning. \n\n"}
{"id": "1710.05488", "contents": "Title: A Geometric View of Optimal Transportation and Generative Model Abstract: In this work, we show the intrinsic relations between optimal transportation\nand convex geometry, especially the variational approach to solve Alexandrov\nproblem: constructing a convex polytope with prescribed face normals and\nvolumes. This leads to a geometric interpretation to generative models, and\nleads to a novel framework for generative models. By using the optimal\ntransportation view of GAN model, we show that the discriminator computes the\nKantorovich potential, the generator calculates the transportation map. For a\nlarge class of transportation costs, the Kantorovich potential can give the\noptimal transportation map by a close-form formula. Therefore, it is sufficient\nto solely optimize the discriminator. This shows the adversarial competition\ncan be avoided, and the computational architecture can be simplified.\nPreliminary experimental results show the geometric method outperforms WGAN for\napproximating probability measures with multiple clusters in low dimensional\nspace. \n\n"}
{"id": "1710.06034", "contents": "Title: Stochastic Variance Reduction for Policy Gradient Estimation Abstract: Recent advances in policy gradient methods and deep learning have\ndemonstrated their applicability for complex reinforcement learning problems.\nHowever, the variance of the performance gradient estimates obtained from the\nsimulation is often excessive, leading to poor sample efficiency. In this\npaper, we apply the stochastic variance reduced gradient descent (SVRG) to\nmodel-free policy gradient to significantly improve the sample-efficiency. The\nSVRG estimation is incorporated into a trust-region Newton conjugate gradient\nframework for the policy optimization. On several Mujoco tasks, our method\nachieves significantly better performance compared to the state-of-the-art\nmodel-free policy gradient methods in robotic continuous control such as trust\nregion policy optimization (TRPO) \n\n"}
{"id": "1710.06763", "contents": "Title: A complete characterization of optimal dictionaries for least squares\n  representation Abstract: Dictionaries are collections of vectors used for representations of elements\nin Euclidean spaces. While recent research on optimal dictionaries is focussed\non providing sparse (i.e., $\\ell_0$-optimal,) representations, here we consider\nthe problem of finding optimal dictionaries such that representations of\nsamples of a random vector are optimal in an $\\ell_2$-sense. For us, optimality\nof representation is equivalent to minimization of the average $\\ell_2$-norm of\nthe coefficients used to represent the random vector, with the lengths of the\ndictionary vectors being specified a priori. With the help of recent results on\nrank-$1$ decompositions of symmetric positive semidefinite matrices and the\ntheory of majorization, we provide a complete characterization of\n$\\ell_2$-optimal dictionaries. Our results are accompanied by polynomial time\nalgorithms that construct $\\ell_2$-optimal dictionaries from given data. \n\n"}
{"id": "1710.06952", "contents": "Title: Asynchronous Decentralized Parallel Stochastic Gradient Descent Abstract: Most commonly used distributed machine learning systems are either\nsynchronous or centralized asynchronous. Synchronous algorithms like\nAllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous\nalgorithms using a parameter server suffer from 1) communication bottleneck at\nparameter servers when workers are many, and 2) significantly worse convergence\nwhen the traffic to parameter server is congested. Can we design an algorithm\nthat is robust in a heterogeneous environment, while being communication\nefficient and maintaining the best-possible convergence rate? In this paper, we\npropose an asynchronous decentralized stochastic gradient decent algorithm\n(AD-PSGD) satisfying all above expectations. Our theoretical analysis shows\nAD-PSGD converges at the optimal $O(1/\\sqrt{K})$ rate as SGD and has linear\nspeedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of\ndecentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and\nstandard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a\nheterogeneous environment. When training ResNet-50 on ImageNet with up to 128\nGPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each\nepoch can be up to 4-8X faster than its synchronous counterparts in a\nnetwork-sharing HPC environment. To the best of our knowledge, AD-PSGD is the\nfirst asynchronous algorithm that achieves a similar epoch-wise convergence\nrate as AllReduce-SGD, at an over 100-GPU scale. \n\n"}
{"id": "1710.07437", "contents": "Title: Distributed Deep Transfer Learning by Basic Probability Assignment Abstract: Transfer learning is a popular practice in deep neural networks, but\nfine-tuning of large number of parameters is a hard task due to the complex\nwiring of neurons between splitting layers and imbalance distributions of data\nin pretrained and transferred domains. The reconstruction of the original\nwiring for the target domain is a heavy burden due to the size of\ninterconnections across neurons. We propose a distributed scheme that tunes the\nconvolutional filters individually while backpropagates them jointly by means\nof basic probability assignment. Some of the most recent advances in evidence\ntheory show that in a vast variety of the imbalanced regimes, optimizing of\nsome proper objective functions derived from contingency matrices prevents\nbiases towards high-prior class distributions. Therefore, the original filters\nget gradually transferred based on individual contributions to overall\nperformance of the target domain. This largely reduces the expected complexity\nof transfer learning whilst highly improves precision. Our experiments on\nstandard benchmarks and scenarios confirm the consistent improvement of our\ndistributed deep transfer learning strategy. \n\n"}
{"id": "1710.07438", "contents": "Title: Unified Backpropagation for Multi-Objective Deep Learning Abstract: A common practice in most of deep convolutional neural architectures is to\nemploy fully-connected layers followed by Softmax activation to minimize\ncross-entropy loss for the sake of classification. Recent studies show that\nsubstitution or addition of the Softmax objective to the cost functions of\nsupport vector machines or linear discriminant analysis is highly beneficial to\nimprove the classification performance in hybrid neural networks. We propose a\nnovel paradigm to link the optimization of several hybrid objectives through\nunified backpropagation. This highly alleviates the burden of extensive\nboosting for independent objective functions or complex formulation of\nmultiobjective gradients. Hybrid loss functions are linked by basic probability\nassignment from evidence theory. We conduct our experiments for a variety of\nscenarios and standard datasets to evaluate the advantage of our proposed\nunification approach to deliver consistent improvements into the classification\nperformance of deep convolutional neural networks. \n\n"}
{"id": "1710.07732", "contents": "Title: A Tight Excess Risk Bound via a Unified\n  PAC-Bayesian-Rademacher-Shtarkov-MDL Complexity Abstract: We present a novel notion of complexity that interpolates between and\ngeneralizes some classic existing complexity notions in learning theory: for\nestimators like empirical risk minimization (ERM) with arbitrary bounded\nlosses, it is upper bounded in terms of data-independent Rademacher complexity;\nfor generalized Bayesian estimators, it is upper bounded by the data-dependent\ninformation complexity (also known as stochastic or PAC-Bayesian,\n$\\mathrm{KL}(\\text{posterior} \\operatorname{\\|} \\text{prior})$ complexity. For\n(penalized) ERM, the new complexity reduces to (generalized) normalized maximum\nlikelihood (NML) complexity, i.e. a minimax log-loss individual-sequence\nregret. Our first main result bounds excess risk in terms of the new\ncomplexity. Our second main result links the new complexity via Rademacher\ncomplexity to $L_2(P)$ entropy, thereby generalizing earlier results of Opper,\nHaussler, Lugosi, and Cesa-Bianchi who did the log-loss case with $L_\\infty$.\nTogether, these results recover optimal bounds for VC- and large (polynomial\nentropy) classes, replacing localized Rademacher complexity by a simpler\nanalysis which almost completely separates the two aspects that determine the\nachievable rates: 'easiness' (Bernstein) conditions and model complexity. \n\n"}
{"id": "1710.08177", "contents": "Title: Progressive Learning for Systematic Design of Large Neural Networks Abstract: We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases. \n\n"}
{"id": "1710.08637", "contents": "Title: Improving Accuracy of Nonparametric Transfer Learning via Vector\n  Segmentation Abstract: Transfer learning using deep neural networks as feature extractors has become\nincreasingly popular over the past few years. It allows to obtain\nstate-of-the-art accuracy on datasets too small to train a deep neural network\non its own, and it provides cutting edge descriptors that, combined with\nnonparametric learning methods, allow rapid and flexible deployment of\nperforming solutions in computationally restricted settings. In this paper, we\nare interested in showing that the features extracted using deep neural\nnetworks have specific properties which can be used to improve accuracy of\ndownstream nonparametric learning methods. Namely, we demonstrate that for some\ndistributions where information is embedded in a few coordinates, segmenting\nfeature vectors can lead to better accuracy. We show how this model can be\napplied to real datasets by performing experiments using three mainstream deep\nneural network feature extractors and four databases, in vision and audio. \n\n"}
{"id": "1710.09513", "contents": "Title: Maximum Principle Based Algorithms for Deep Learning Abstract: The continuous dynamical system approach to deep learning is explored in\norder to devise alternative frameworks for training algorithms. Training is\nrecast as a control problem and this allows us to formulate necessary\noptimality conditions in continuous time using the Pontryagin's maximum\nprinciple (PMP). A modification of the method of successive approximations is\nthen used to solve the PMP, giving rise to an alternative training algorithm\nfor deep learning. This approach has the advantage that rigorous error\nestimates and convergence results can be established. We also show that it may\navoid some pitfalls of gradient-based methods, such as slow convergence on flat\nlandscapes near saddle points. Furthermore, we demonstrate that it obtains\nfavorable initial convergence rate per-iteration, provided Hamiltonian\nmaximization can be efficiently carried out - a step which is still in need of\nimprovement. Overall, the approach opens up new avenues to attack problems\nassociated with deep learning, such as trapping in slow manifolds and\ninapplicability of gradient-based methods for discrete trainable variables. \n\n"}
{"id": "1710.09599", "contents": "Title: Watch Your Step: Learning Node Embeddings via Graph Attention Abstract: Graph embedding methods represent nodes in a continuous vector space,\npreserving information from the graph (e.g. by sampling random walks). There\nare many hyper-parameters to these methods (such as random walk length) which\nhave to be manually tuned for every graph. In this paper, we replace random\nwalk hyper-parameters with trainable parameters that we automatically learn via\nbackpropagation. In particular, we learn a novel attention model on the power\nseries of the transition matrix, which guides the random walk to optimize an\nupstream objective. Unlike previous approaches to attention models, the method\nthat we propose utilizes attention parameters exclusively on the data (e.g. on\nthe random walk), and not used by the model for inference. We experiment on\nlink prediction tasks, as we aim to produce embeddings that best-preserve the\ngraph structure, generalizing to unseen information. We improve\nstate-of-the-art on a comprehensive suite of real world datasets including\nsocial, collaboration, and biological networks. Adding attention to random\nwalks can reduce the error by 20% to 45% on datasets we attempted. Further, our\nlearned attention parameters are different for every graph, and our\nautomatically-found values agree with the optimal choice of hyper-parameter if\nwe manually tune existing methods. \n\n"}
{"id": "1710.10006", "contents": "Title: Deep Learning for Accelerated Ultrasound Imaging Abstract: In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an\nincreasing demand to reconstruct high quality images from limited number of\ndata. However, the existing solutions require either hardware changes or\ncomputationally expansive algorithms. To overcome these limitations, here we\npropose a novel deep learning approach that interpolates the missing RF data by\nutilizing the sparsity of the RF data in the Fourier domain. Extensive\nexperimental results from sub-sampled RF data from a real US system confirmed\nthat the proposed method can effectively reduce the data rate without\nsacrificing the image quality. \n\n"}
{"id": "1710.10016", "contents": "Title: Regularization via Mass Transportation Abstract: The goal of regression and classification methods in supervised learning is\nto minimize the empirical risk, that is, the expectation of some loss function\nquantifying the prediction error under the empirical distribution. When facing\nscarce training data, overfitting is typically mitigated by adding\nregularization terms to the objective that penalize hypothesis complexity. In\nthis paper we introduce new regularization techniques using ideas from\ndistributionally robust optimization, and we give new probabilistic\ninterpretations to existing techniques. Specifically, we propose to minimize\nthe worst-case expected loss, where the worst case is taken over the ball of\nall (continuous or discrete) distributions that have a bounded transportation\ndistance from the (discrete) empirical distribution. By choosing the radius of\nthis ball judiciously, we can guarantee that the worst-case expected loss\nprovides an upper confidence bound on the loss on test data, thus offering new\ngeneralization bounds. We prove that the resulting regularized learning\nproblems are tractable and can be tractably kernelized for many popular loss\nfunctions. We validate our theoretical out-of-sample guarantees through\nsimulated and empirical experiments. \n\n"}
{"id": "1710.10036", "contents": "Title: Generalization Tower Network: A Novel Deep Neural Network Architecture\n  for Multi-Task Learning Abstract: Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by\nincorporating deep neural networks in learning representations from the input\nto RL. However, the conventional deep neural network architecture is limited in\nlearning representations for multi-task RL (MT-RL), as multiple tasks can refer\nto different kinds of representations. In this paper, we thus propose a novel\ndeep neural network architecture, namely generalization tower network (GTN),\nwhich can achieve MT-RL within a single learned model. Specifically, the\narchitecture of GTN is composed of both horizontal and vertical streams. In our\nGTN architecture, horizontal streams are used to learn representation shared in\nsimilar tasks. In contrast, the vertical streams are introduced to be more\nsuitable for handling diverse tasks, which encodes hierarchical shared\nknowledge of these tasks. The effectiveness of the introduced vertical stream\nis validated by experimental results. Experimental results further verify that\nour GTN architecture is able to advance the state-of-the-art MT-RL, via being\ntested on 51 Atari games. \n\n"}
{"id": "1710.10313", "contents": "Title: A Self-Training Method for Semi-Supervised GANs Abstract: Since the creation of Generative Adversarial Networks (GANs), much work has\nbeen done to improve their training stability, their generated image quality,\ntheir range of application but nearly none of them explored their self-training\npotential. Self-training has been used before the advent of deep learning in\norder to allow training on limited labelled training data and has shown\nimpressive results in semi-supervised learning. In this work, we combine these\ntwo ideas and make GANs self-trainable for semi-supervised learning tasks by\nexploiting their infinite data generation potential. Results show that using\neven the simplest form of self-training yields an improvement. We also show\nresults for a more complex self-training scheme that performs at least as well\nas the basic self-training scheme but with significantly less data\naugmentation. \n\n"}
{"id": "1710.10370", "contents": "Title: Topology Adaptive Graph Convolutional Networks Abstract: Spectral graph convolutional neural networks (CNNs) require approximation to\nthe convolution to alleviate the computational complexity, resulting in\nperformance loss. This paper proposes the topology adaptive graph convolutional\nnetwork (TAGCN), a novel graph convolutional network defined in the vertex\ndomain. We provide a systematic way to design a set of fixed-size learnable\nfilters to perform convolutions on graphs. The topologies of these filters are\nadaptive to the topology of the graph when they scan the graph to perform\nconvolution. The TAGCN not only inherits the properties of convolutions in CNN\nfor grid-structured data, but it is also consistent with convolution as defined\nin graph signal processing. Since no approximation to the convolution is\nneeded, TAGCN exhibits better performance than existing spectral CNNs on a\nnumber of data sets and is also computationally simpler than other recent\nmethods. \n\n"}
{"id": "1710.10467", "contents": "Title: Generalized End-to-End Loss for Speaker Verification Abstract: In this paper, we propose a new loss function called generalized end-to-end\n(GE2E) loss, which makes the training of speaker verification models more\nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike\nTE2E, the GE2E loss function updates the network in a way that emphasizes\nexamples that are difficult to verify at each step of the training process.\nAdditionally, the GE2E loss does not require an initial stage of example\nselection. With these properties, our model with the new loss function\ndecreases speaker verification EER by more than 10%, while reducing the\ntraining time by 60% at the same time. We also introduce the MultiReader\ntechnique, which allows us to do domain adaptation - training a more accurate\nmodel that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as\nwell as multiple dialects. \n\n"}
{"id": "1710.10570", "contents": "Title: Weight Initialization of Deep Neural Networks(DNNs) using Data\n  Statistics Abstract: Deep neural networks (DNNs) form the backbone of almost every\nstate-of-the-art technique in the fields such as computer vision, speech\nprocessing, and text analysis. The recent advances in computational technology\nhave made the use of DNNs more practical. Despite the overwhelming performances\nby DNN and the advances in computational technology, it is seen that very few\nresearchers try to train their models from the scratch. Training of DNNs still\nremains a difficult and tedious job. The main challenges that researchers face\nduring training of DNNs are the vanishing/exploding gradient problem and the\nhighly non-convex nature of the objective function which has up to million\nvariables. The approaches suggested in He and Xavier solve the vanishing\ngradient problem by providing a sophisticated initialization technique. These\napproaches have been quite effective and have achieved good results on standard\ndatasets, but these same approaches do not work very well on more practical\ndatasets. We think the reason for this is not making use of data statistics for\ninitializing the network weights. Optimizing such a high dimensional loss\nfunction requires careful initialization of network weights. In this work, we\npropose a data dependent initialization and analyze its performance against the\nstandard initialization techniques such as He and Xavier. We performed our\nexperiments on some practical datasets and the results show our algorithm's\nsuperior classification accuracy. \n\n"}
{"id": "1710.11381", "contents": "Title: Semantic Interpolation in Implicit Models Abstract: In implicit models, one often interpolates between sampled points in latent\nspace. As we show in this paper, care needs to be taken to match-up the\ndistributional assumptions on code vectors with the geometry of the\ninterpolating paths. Otherwise, typical assumptions about the quality and\nsemantics of in-between points may not be justified. Based on our analysis we\npropose to modify the prior code distribution to put significantly more\nprobability mass closer to the origin. As a result, linear interpolation paths\nare not only shortest paths, but they are also guaranteed to pass through\nhigh-density regions, irrespective of the dimensionality of the latent space.\nExperiments on standard benchmark image datasets demonstrate clear visual\nimprovements in the quality of the generated samples and exhibit more\nmeaningful interpolation paths. \n\n"}
{"id": "1711.00137", "contents": "Title: Pomegranate: fast and flexible probabilistic modeling in python Abstract: We present pomegranate, an open source machine learning package for\nprobabilistic modeling in Python. Probabilistic modeling encompasses a wide\nrange of methods that explicitly describe uncertainty using probability\ndistributions. Three widely used probabilistic models implemented in\npomegranate are general mixture models, hidden Markov models, and Bayesian\nnetworks. A primary focus of pomegranate is to abstract away the complexities\nof training models from their definition. This allows users to focus on\nspecifying the correct model for their application instead of being limited by\ntheir understanding of the underlying algorithms. An aspect of this focus\ninvolves the collection of additive sufficient statistics from data sets as a\nstrategy for training models. This approach trivially enables many useful\nlearning strategies, such as out-of-core learning, minibatch learning, and\nsemi-supervised learning, without requiring the user to consider how to\npartition data or modify the algorithms to handle these tasks themselves.\npomegranate is written in Cython to speed up calculations and releases the\nglobal interpreter lock to allow for built-in multithreaded parallelism, making\nit competitive with---or outperform---other implementations of similar\nalgorithms. This paper presents an overview of the design choices in\npomegranate, and how they have enabled complex features to be supported by\nsimple code. \n\n"}
{"id": "1711.00982", "contents": "Title: From which world is your graph? Abstract: Discovering statistical structure from links is a fundamental problem in the\nanalysis of social networks. Choosing a misspecified model, or equivalently, an\nincorrect inference algorithm will result in an invalid analysis or even\nfalsely uncover patterns that are in fact artifacts of the model. This work\nfocuses on unifying two of the most widely used link-formation models: the\nstochastic blockmodel (SBM) and the small world (or latent space) model (SWM).\nIntegrating techniques from kernel learning, spectral graph theory, and\nnonlinear dimensionality reduction, we develop the first statistically sound\npolynomial-time algorithm to discover latent patterns in sparse graphs for both\nmodels. When the network comes from an SBM, the algorithm outputs a block\nstructure. When it is from an SWM, the algorithm outputs estimates of each\nnode's latent position. \n\n"}
{"id": "1711.02421", "contents": "Title: Gaussian Lower Bound for the Information Bottleneck Limit Abstract: The Information Bottleneck (IB) is a conceptual method for extracting the\nmost compact, yet informative, representation of a set of variables, with\nrespect to the target. It generalizes the notion of minimal sufficient\nstatistics from classical parametric statistics to a broader\ninformation-theoretic sense. The IB curve defines the optimal trade-off between\nrepresentation complexity and its predictive power. Specifically, it is\nachieved by minimizing the level of mutual information (MI) between the\nrepresentation and the original variables, subject to a minimal level of MI\nbetween the representation and the target. This problem is shown to be in\ngeneral NP hard. One important exception is the multivariate Gaussian case, for\nwhich the Gaussian IB (GIB) is known to obtain an analytical closed form\nsolution, similar to Canonical Correlation Analysis (CCA). In this work we\nintroduce a Gaussian lower bound to the IB curve; we find an embedding of the\ndata which maximizes its \"Gaussian part\", on which we apply the GIB. This\nembedding provides an efficient (and practical) representation of any arbitrary\ndata-set (in the IB sense), which in addition holds the favorable properties of\na Gaussian distribution. Importantly, we show that the optimal Gaussian\nembedding is bounded from above by non-linear CCA. This allows a fundamental\nlimit for our ability to Gaussianize arbitrary data-sets and solve complex\nproblems by linear methods. \n\n"}
{"id": "1711.03190", "contents": "Title: Learning Credible Models Abstract: In many settings, it is important that a model be capable of providing\nreasons for its predictions (i.e., the model must be interpretable). However,\nthe model's reasoning may not conform with well-established knowledge. In such\ncases, while interpretable, the model lacks \\textit{credibility}. In this work,\nwe formally define credibility in the linear setting and focus on techniques\nfor learning models that are both accurate and credible. In particular, we\npropose a regularization penalty, expert yielded estimates (EYE), that\nincorporates expert knowledge about well-known relationships among covariates\nand the outcome of interest. We give both theoretical and empirical results\ncomparing our proposed method to several other regularization techniques.\nAcross a range of settings, experiments on both synthetic and real data show\nthat models learned using the EYE penalty are significantly more credible than\nthose learned using other penalties. Applied to a large-scale patient risk\nstratification task, our proposed technique results in a model whose top\nfeatures overlap significantly with known clinical risk factors, while still\nachieving good predictive performance. \n\n"}
{"id": "1711.04094", "contents": "Title: Enhancing Network Embedding with Auxiliary Information: An Explicit\n  Matrix Factorization Perspective Abstract: Recent advances in the field of network embedding have shown the\nlow-dimensional network representation is playing a critical role in network\nanalysis. However, most of the existing principles of network embedding do not\nincorporate auxiliary information such as content and labels of nodes flexibly.\nIn this paper, we take a matrix factorization perspective of network embedding,\nand incorporate structure, content and label information of the network\nsimultaneously. For structure, we validate that the matrix we construct\npreserves high-order proximities of the network. Label information can be\nfurther integrated into the matrix via the process of random walk sampling to\nenhance the quality of embedding in an unsupervised manner, i.e., without\nleveraging downstream classifiers. In addition, we generalize the Skip-Gram\nNegative Sampling model to integrate the content of the network in a matrix\nfactorization framework. As a consequence, network embedding can be learned in\na unified framework integrating network structure and node content as well as\nlabel information simultaneously. We demonstrate the efficacy of the proposed\nmodel with the tasks of semi-supervised node classification and link prediction\non a variety of real-world benchmark network datasets. \n\n"}
{"id": "1711.04150", "contents": "Title: STWalk: Learning Trajectory Representations in Temporal Graphs Abstract: Analyzing the temporal behavior of nodes in time-varying graphs is useful for\nmany applications such as targeted advertising, community evolution and outlier\ndetection. In this paper, we present a novel approach, STWalk, for learning\ntrajectory representations of nodes in temporal graphs. The proposed framework\nmakes use of structural properties of graphs at current and previous time-steps\nto learn effective node trajectory representations. STWalk performs random\nwalks on a graph at a given time step (called space-walk) as well as on graphs\nfrom past time-steps (called time-walk) to capture the spatio-temporal behavior\nof nodes. We propose two variants of STWalk to learn trajectory\nrepresentations. In one algorithm, we perform space-walk and time-walk as part\nof a single step. In the other variant, we perform space-walk and time-walk\nseparately and combine the learned representations to get the final trajectory\nembedding. Extensive experiments on three real-world temporal graph datasets\nvalidate the effectiveness of the learned representations when compared to\nthree baseline methods. We also show the goodness of the learned trajectory\nembeddings for change point detection, as well as demonstrate that arithmetic\noperations on these trajectory representations yield interesting and\ninterpretable results. \n\n"}
{"id": "1711.04679", "contents": "Title: Attention-based Information Fusion using Multi-Encoder-Decoder Recurrent\n  Neural Networks Abstract: With the rising number of interconnected devices and sensors, modeling\ndistributed sensor networks is of increasing interest. Recurrent neural\nnetworks (RNN) are considered particularly well suited for modeling sensory and\nstreaming data. When predicting future behavior, incorporating information from\nneighboring sensor stations is often beneficial. We propose a new RNN based\narchitecture for context specific information fusion across multiple spatially\ndistributed sensor stations. Hereby, latent representations of multiple local\nmodels, each modeling one sensor station, are jointed and weighted, according\nto their importance for the prediction. The particular importance is assessed\ndepending on the current context using a separate attention function. We\ndemonstrate the effectiveness of our model on three different real-world sensor\nnetwork datasets. \n\n"}
{"id": "1711.04735", "contents": "Title: Resurrecting the sigmoid in deep learning through dynamical isometry:\n  theory and practice Abstract: It is well known that the initialization of weights in deep neural networks\ncan have a dramatic impact on learning speed. For example, ensuring the mean\nsquared singular value of a network's input-output Jacobian is $O(1)$ is\nessential for avoiding the exponential vanishing or explosion of gradients. The\nstronger condition that all singular values of the Jacobian concentrate near\n$1$ is a property known as dynamical isometry. For deep linear networks,\ndynamical isometry can be achieved through orthogonal weight initialization and\nhas been shown to dramatically speed up learning; however, it has remained\nunclear how to extend these results to the nonlinear setting. We address this\nquestion by employing powerful tools from free probability theory to compute\nanalytically the entire singular value distribution of a deep network's\ninput-output Jacobian. We explore the dependence of the singular value\ndistribution on the depth of the network, the weight initialization, and the\nchoice of nonlinearity. Intriguingly, we find that ReLU networks are incapable\nof dynamical isometry. On the other hand, sigmoidal networks can achieve\nisometry, but only with orthogonal weight initialization. Moreover, we\ndemonstrate empirically that deep nonlinear networks achieving dynamical\nisometry learn orders of magnitude faster than networks that do not. Indeed, we\nshow that properly-initialized deep sigmoidal networks consistently outperform\ndeep ReLU networks. Overall, our analysis reveals that controlling the entire\ndistribution of Jacobian singular values is an important design consideration\nin deep learning. \n\n"}
{"id": "1711.07364", "contents": "Title: Classification with Costly Features using Deep Reinforcement Learning Abstract: We study a classification problem where each feature can be acquired for a\ncost and the goal is to optimize a trade-off between the expected\nclassification error and the feature cost. We revisit a former approach that\nhas framed the problem as a sequential decision-making problem and solved it by\nQ-learning with a linear approximation, where individual actions are either\nrequests for feature values or terminate the episode by providing a\nclassification decision. On a set of eight problems, we demonstrate that by\nreplacing the linear approximation with neural networks the approach becomes\ncomparable to the state-of-the-art algorithms developed specifically for this\nproblem. The approach is flexible, as it can be improved with any new\nreinforcement learning enhancement, it allows inclusion of pre-trained\nhigh-performance classifier, and unlike prior art, its performance is robust\nacross all evaluated datasets. \n\n"}
{"id": "1711.07414", "contents": "Title: The Promise and Peril of Human Evaluation for Model Interpretability Abstract: Transparency, user trust, and human comprehension are popular ethical\nmotivations for interpretable machine learning. In support of these goals,\nresearchers evaluate model explanation performance using humans and real world\napplications. This alone presents a challenge in many areas of artificial\nintelligence. In this position paper, we propose a distinction between\ndescriptive and persuasive explanations. We discuss reasoning suggesting that\nfunctional interpretability may be correlated with cognitive function and user\npreferences. If this is indeed the case, evaluation and optimization using\nfunctional metrics could perpetuate implicit cognitive bias in explanations\nthat threaten transparency. Finally, we propose two potential research\ndirections to disambiguate cognitive function and explanation models, retaining\ncontrol over the tradeoff between accuracy and interpretability. \n\n"}
{"id": "1711.07553", "contents": "Title: Residual Gated Graph ConvNets Abstract: Graph-structured data such as social networks, functional brain networks,\ngene regulatory networks, communications networks have brought the interest in\ngeneralizing deep learning techniques to graph domains. In this paper, we are\ninterested to design neural networks for graphs with variable length in order\nto solve learning problems such as vertex classification, graph classification,\ngraph regression, and graph generative tasks. Most existing works have focused\non recurrent neural networks (RNNs) to learn meaningful representations of\ngraphs, and more recently new convolutional neural networks (ConvNets) have\nbeen introduced. In this work, we want to compare rigorously these two\nfundamental families of architectures to solve graph learning tasks. We review\nexisting graph RNN and ConvNet architectures, and propose natural extension of\nLSTM and ConvNet to graphs with arbitrary size. Then, we design a set of\nanalytically controlled experiments on two basic graph problems, i.e. subgraph\nmatching and graph clustering, to test the different architectures. Numerical\nresults show that the proposed graph ConvNets are 3-17% more accurate and\n1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than\nvariational (non-learning) techniques. Finally, the most effective graph\nConvNet architecture uses gated edges and residuality. Residuality plays an\nessential role to learn multi-layer architectures as they provide a 10% gain of\nperformance. \n\n"}
{"id": "1711.08421", "contents": "Title: Relief-Based Feature Selection: Introduction and Review Abstract: Feature selection plays a critical role in biomedical data mining, driven by\nincreasing feature dimensionality in target problems and growing interest in\nadvanced but computationally expensive methodologies able to model complex\nassociations. Specifically, there is a need for feature selection methods that\nare computationally efficient, yet sensitive to complex patterns of\nassociation, e.g. interactions, so that informative features are not mistakenly\neliminated prior to downstream modeling. This paper focuses on Relief-based\nalgorithms (RBAs), a unique family of filter-style feature selection algorithms\nthat have gained appeal by striking an effective balance between these\nobjectives while flexibly adapting to various data characteristics, e.g.\nclassification vs. regression. First, this work broadly examines types of\nfeature selection and defines RBAs within that context. Next, we introduce the\noriginal Relief algorithm and associated concepts, emphasizing the intuition\nbehind how it works, how feature weights generated by the algorithm can be\ninterpreted, and why it is sensitive to feature interactions without evaluating\ncombinations of features. Lastly, we include an expansive review of RBA\nmethodological research beyond Relief and its popular descendant, ReliefF. In\nparticular, we characterize branches of RBA research, and provide comparative\nsummaries of RBA algorithms including contributions, strategies, functionality,\ntime complexity, adaptation to key data characteristics, and software\navailability. \n\n"}
{"id": "1711.08448", "contents": "Title: Node and layer eigenvector centralities for multiplex networks Abstract: Eigenvector-based centrality measures are among the most popular centrality\nmeasures in network science. The underlying idea is intuitive and the\nmathematical description is extremely simple in the framework of standard,\nmono-layer networks. Moreover, several efficient computational tools are\navailable for their computation. Moving up in dimensionality, several efforts\nhave been made in the past to describe an eigenvector-based centrality measure\nthat generalizes Bonacich index to the case of multiplex networks. In this\nwork, we propose a new definition of eigenvector centrality that relies on the\nPerron eigenvector of a multi-homogeneous map defined in terms of the tensor\ndescribing the network. We prove that existence and uniqueness of such\ncentrality are guaranteed under very mild assumptions on the multiplex network.\nExtensive numerical studies are proposed to test the newly introduced\ncentrality measure and to compare it to other existing eigenvector-based\ncentralities. \n\n"}
{"id": "1711.08646", "contents": "Title: IVE-GAN: Invariant Encoding Generative Adversarial Networks Abstract: Generative adversarial networks (GANs) are a powerful framework for\ngenerative tasks. However, they are difficult to train and tend to miss modes\nof the true data generation process. Although GANs can learn a rich\nrepresentation of the covered modes of the data in their latent space, the\nframework misses an inverse mapping from data to this latent space. We propose\nInvariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN\nframework that introduces such a mapping for individual samples from the data\nby utilizing features in the data which are invariant to certain\ntransformations. Since the model maps individual samples to the latent space,\nit naturally encourages the generator to cover all modes. We demonstrate the\neffectiveness of our approach in terms of generative performance and learning\nrich representations on several datasets including common benchmark image\ngeneration tasks. \n\n"}
{"id": "1711.09176", "contents": "Title: Selling to a No-Regret Buyer Abstract: We consider the problem of a single seller repeatedly selling a single item\nto a single buyer (specifically, the buyer has a value drawn fresh from known\ndistribution $D$ in every round). Prior work assumes that the buyer is fully\nrational and will perfectly reason about how their bids today affect the\nseller's decisions tomorrow. In this work we initiate a different direction:\nthe buyer simply runs a no-regret learning algorithm over possible bids. We\nprovide a fairly complete characterization of optimal auctions for the seller\nin this domain. Specifically:\n  - If the buyer bids according to EXP3 (or any \"mean-based\" learning\nalgorithm), then the seller can extract expected revenue arbitrarily close to\nthe expected welfare. This auction is independent of the buyer's valuation $D$,\nbut somewhat unnatural as it is sometimes in the buyer's interest to overbid. -\nThere exists a learning algorithm $\\mathcal{A}$ such that if the buyer bids\naccording to $\\mathcal{A}$ then the optimal strategy for the seller is simply\nto post the Myerson reserve for $D$ every round. - If the buyer bids according\nto EXP3 (or any \"mean-based\" learning algorithm), but the seller is restricted\nto \"natural\" auction formats where overbidding is dominated (e.g. Generalized\nFirst-Price or Generalized Second-Price), then the optimal strategy for the\nseller is a pay-your-bid format with decreasing reserves over time. Moreover,\nthe seller's optimal achievable revenue is characterized by a linear program,\nand can be unboundedly better than the best truthful auction yet simultaneously\nunboundedly worse than the expected welfare. \n\n"}
{"id": "1711.09649", "contents": "Title: One-Shot Coresets: The Case of k-Clustering Abstract: Scaling clustering algorithms to massive data sets is a challenging task.\nRecently, several successful approaches based on data summarization methods,\nsuch as coresets and sketches, were proposed. While these techniques provide\nprovably good and small summaries, they are inherently problem dependent - the\npractitioner has to commit to a fixed clustering objective before even\nexploring the data. However, can one construct small data summaries for a wide\nrange of clustering problems simultaneously? In this work, we affirmatively\nanswer this question by proposing an efficient algorithm that constructs such\none-shot summaries for k-clustering problems while retaining strong theoretical\nguarantees. \n\n"}
{"id": "1711.10157", "contents": "Title: Deformation estimation of an elastic object by partial observation using\n  a neural network Abstract: Deformation estimation of elastic object assuming an internal organ is\nimportant for the computer navigation of surgery. The aim of this study is to\nestimate the deformation of an entire three-dimensional elastic object using\ndisplacement information of very few observation points. A learning approach\nwith a neural network was introduced to estimate the entire deformation of an\nobject. We applied our method to two elastic objects; a rectangular\nparallelepiped model, and a human liver model reconstructed from computed\ntomography data. The average estimation error for the human liver model was\n0.041 mm when the object was deformed up to 66.4 mm, from only around 3 %\nobservations. These results indicate that the deformation of an entire elastic\nobject can be estimated with an acceptable level of error from limited\nobservations by applying a trained neural network to a new deformation. \n\n"}
{"id": "1711.10589", "contents": "Title: Contextual Outlier Interpretation Abstract: Outlier detection plays an essential role in many data-driven applications to\nidentify isolated instances that are different from the majority. While many\nstatistical learning and data mining techniques have been used for developing\nmore effective outlier detection algorithms, the interpretation of detected\noutliers does not receive much attention. Interpretation is becoming\nincreasingly important to help people trust and evaluate the developed models\nthrough providing intrinsic reasons why the certain outliers are chosen. It is\ndifficult, if not impossible, to simply apply feature selection for explaining\noutliers due to the distinct characteristics of various detection models,\ncomplicated structures of data in certain applications, and imbalanced\ndistribution of outliers and normal instances. In addition, the role of\ncontrastive contexts where outliers locate, as well as the relation between\noutliers and contexts, are usually overlooked in interpretation. To tackle the\nissues above, in this paper, we propose a novel Contextual Outlier\nINterpretation (COIN) method to explain the abnormality of existing outliers\nspotted by detectors. The interpretability for an outlier is achieved from\nthree aspects: outlierness score, attributes that contribute to the\nabnormality, and contextual description of its neighborhoods. Experimental\nresults on various types of datasets demonstrate the flexibility and\neffectiveness of the proposed framework compared with existing interpretation\napproaches. \n\n"}
{"id": "1711.11139", "contents": "Title: Easy High-Dimensional Likelihood-Free Inference Abstract: We introduce a framework using Generative Adversarial Networks (GANs) for\nlikelihood--free inference (LFI) and Approximate Bayesian Computation (ABC)\nwhere we replace the black-box simulator model with an approximator network and\ngenerate a rich set of summary features in a data driven fashion. On benchmark\ndata sets, our approach improves on others with respect to scalability, ability\nto handle high dimensional data and complex probability distributions. \n\n"}
{"id": "1711.11581", "contents": "Title: Outlier-robust moment-estimation via sum-of-squares Abstract: We develop efficient algorithms for estimating low-degree moments of unknown\ndistributions in the presence of adversarial outliers. The guarantees of our\nalgorithms improve in many cases significantly over the best previous ones,\nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al.\nWe also show that the guarantees of our algorithms match information-theoretic\nlower-bounds for the class of distributions we consider. These improved\nguarantees allow us to give improved algorithms for independent component\nanalysis and learning mixtures of Gaussians in the presence of outliers.\n  Our algorithms are based on a standard sum-of-squares relaxation of the\nfollowing conceptually-simple optimization problem: Among all distributions\nwhose moments are bounded in the same way as for the unknown distribution, find\nthe one that is closest in statistical distance to the empirical distribution\nof the adversarially-corrupted sample. \n\n"}
{"id": "1712.00003", "contents": "Title: Modeling Information Flow Through Deep Neural Networks Abstract: This paper proposes a principled information theoretic analysis of\nclassification for deep neural network structures, e.g. convolutional neural\nnetworks (CNN). The output of convolutional filters is modeled as a random\nvariable Y conditioned on the object class C and network filter bank F. The\nconditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a\nhighly compact and class-informative code, that can be computed from the filter\noutputs throughout an existing CNN and used to obtain higher classification\nresults than the original CNN itself. Experiments demonstrate the effectiveness\nof CENT feature analysis in two separate CNN classification contexts. 1) In the\nclassification of neurodegeneration due to Alzheimer's disease (AD) and natural\naging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in\nan AUC=94.6% for whole-brain AD classification, the highest reported accuracy\non the public OASIS dataset used and 12% higher than the softmax output of the\noriginal CNN trained for the task. 2) In the context of visual object\nclassification from 2D photographs, transfer learning based on a small set of\nCENT features identified throughout an existing CNN leads to AUC values\ncomparable to the 1000-feature softmax output of the original network when\nclassifying previously unseen object categories. The general information\ntheoretical analysis explains various recent CNN design successes, e.g. densely\nconnected CNN architectures, and provides insights for future research\ndirections in deep learning. \n\n"}
{"id": "1712.00558", "contents": "Title: Where Classification Fails, Interpretation Rises Abstract: An intriguing property of deep neural networks is their inherent\nvulnerability to adversarial inputs, which significantly hinders their\napplication in security-critical domains. Most existing detection methods\nattempt to use carefully engineered patterns to distinguish adversarial inputs\nfrom their genuine counterparts, which however can often be circumvented by\nadaptive adversaries. In this work, we take a completely different route by\nleveraging the definition of adversarial inputs: while deceiving for deep\nneural networks, they are barely discernible for human visions. Building upon\nrecent advances in interpretable models, we construct a new detection framework\nthat contrasts an input's interpretation against its classification. We\nvalidate the efficacy of this framework through extensive experiments using\nbenchmark datasets and attacks. We believe that this work opens a new direction\nfor designing adversarial input detection methods. \n\n"}
{"id": "1712.00559", "contents": "Title: Progressive Neural Architecture Search Abstract: We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet. \n\n"}
{"id": "1712.01193", "contents": "Title: A dual framework for low-rank tensor completion Abstract: One of the popular approaches for low-rank tensor completion is to use the\nlatent trace norm regularization. However, most existing works in this\ndirection learn a sparse combination of tensors. In this work, we fill this gap\nby proposing a variant of the latent trace norm that helps in learning a\nnon-sparse combination of tensors. We develop a dual framework for solving the\nlow-rank tensor completion problem. We first show a novel characterization of\nthe dual solution space with an interesting factorization of the optimal\nsolution. Overall, the optimal solution is shown to lie on a Cartesian product\nof Riemannian manifolds. Furthermore, we exploit the versatile Riemannian\noptimization framework for proposing computationally efficient trust region\nalgorithm. The experiments illustrate the efficacy of the proposed algorithm on\nseveral real-world datasets across applications. \n\n"}
{"id": "1712.01473", "contents": "Title: Deep linear neural networks with arbitrary loss: All local minima are\n  global Abstract: We consider deep linear networks with arbitrary convex differentiable loss.\nWe provide a short and elementary proof of the fact that all local minima are\nglobal minima if the hidden layers are either 1) at least as wide as the input\nlayer, or 2) at least as wide as the output layer. This result is the strongest\npossible in the following sense: If the loss is convex and Lipschitz but not\ndifferentiable then deep linear networks can have sub-optimal local minima. \n\n"}
{"id": "1712.02679", "contents": "Title: AdaComp : Adaptive Residual Gradient Compression for Data-Parallel\n  Distributed Training Abstract: Highly distributed training of Deep Neural Networks (DNNs) on future compute\nplatforms (offering 100 of TeraOps/s of computational capacity) is expected to\nbe severely communication constrained. To overcome this limitation, new\ngradient compression techniques are needed that are computationally friendly,\napplicable to a wide variety of layers seen in Deep Neural Networks and\nadaptable to variations in network architectures as well as their\nhyper-parameters. In this paper we introduce a novel technique - the Adaptive\nResidual Gradient Compression (AdaComp) scheme. AdaComp is based on localized\nselection of gradient residues and automatically tunes the compression rate\ndepending on local activity. We show excellent results on a wide spectrum of\nstate of the art Deep Learning models in multiple domains (vision, speech,\nlanguage), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers\n(SGD with momentum, Adam) and network parameters (number of learners,\nminibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate\nend-to-end compression rates of ~200X for fully-connected and recurrent layers,\nand ~40X for convolutional layers, without any noticeable degradation in model\naccuracies. \n\n"}
{"id": "1712.03298", "contents": "Title: Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural\n  Networks Abstract: Progress in deep learning is slowed by the days or weeks it takes to train\nlarge models. The natural solution of using more hardware is limited by\ndiminishing returns, and leads to inefficient use of additional resources. In\nthis paper, we present a large batch, stochastic optimization algorithm that is\nboth faster than widely used algorithms for fixed amounts of computation, and\nalso scales up substantially better as more computational resources become\navailable. Our algorithm implicitly computes the inverse Hessian of each\nmini-batch to produce descent directions; we do so without either an explicit\napproximation to the Hessian or Hessian-vector products. We demonstrate the\neffectiveness of our algorithm by successfully training large ImageNet models\n(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch\nsizes of up to 32000 with no loss in validation error relative to current\nbaselines, and no increase in the total number of steps. At smaller mini-batch\nsizes, our optimizer improves the validation error in these models by 0.8-0.9%.\nAlternatively, we can trade off this accuracy to reduce the number of training\nsteps needed by roughly 10-30%. Our work is practical and easily usable by\nothers -- only one hyperparameter (learning rate) needs tuning, and\nfurthermore, the algorithm is as computationally cheap as the commonly used\nAdam optimizer. \n\n"}
{"id": "1712.03337", "contents": "Title: Bayesian Joint Matrix Decomposition for Data Integration with\n  Heterogeneous Noise Abstract: Matrix decomposition is a popular and fundamental approach in machine\nlearning and data mining. It has been successfully applied into various fields.\nMost matrix decomposition methods focus on decomposing a data matrix from one\nsingle source. However, it is common that data are from different sources with\nheterogeneous noise. A few of matrix decomposition methods have been extended\nfor such multi-view data integration and pattern discovery. While only few\nmethods were designed to consider the heterogeneity of noise in such multi-view\ndata for data integration explicitly. To this end, we propose a joint matrix\ndecomposition framework (BJMD), which models the heterogeneity of noise by\nGaussian distribution in a Bayesian framework. We develop two algorithms to\nsolve this model: one is a variational Bayesian inference algorithm, which\nmakes full use of the posterior distribution; and another is a maximum a\nposterior algorithm, which is more scalable and can be easily paralleled.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nBJMD considering the heterogeneity of noise is superior or competitive to the\nstate-of-the-art methods. \n\n"}
{"id": "1712.03428", "contents": "Title: Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent Abstract: In this paper, we propose a novel approach to automatically determine the\nbatch size in stochastic gradient descent methods. The choice of the batch size\ninduces a trade-off between the accuracy of the gradient estimate and the cost\nin terms of samples of each update. We propose to determine the batch size by\noptimizing the ratio between a lower bound to a linear or quadratic Taylor\napproximation of the expected improvement and the number of samples used to\nestimate the gradient. The performance of the proposed approach is empirically\ncompared with related methods on popular classification tasks.\n  The work was presented at the NIPS workshop on Optimizing the Optimizers.\nBarcelona, Spain, 2016. \n\n"}
{"id": "1712.03999", "contents": "Title: Eye In-Painting with Exemplar Generative Adversarial Networks Abstract: This paper introduces a novel approach to in-painting where the identity of\nthe object to remove or change is preserved and accounted for at inference\ntime: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize\nexemplar information to produce high-quality, personalized in painting results.\nWe propose using exemplar information in the form of a reference image of the\nregion to in-paint, or a perceptual code describing that object. Unlike\nprevious conditional GAN formulations, this extra information can be inserted\nat multiple points within the adversarial network, thus increasing its\ndescriptive power. We show that ExGANs can produce photo-realistic personalized\nin-painting results that are both perceptually and semantically plausible by\napplying them to the task of closed to-open eye in-painting in natural\npictures. A new benchmark dataset is also introduced for the task of eye\nin-painting for future comparisons. \n\n"}
{"id": "1712.04910", "contents": "Title: FFT-Based Deep Learning Deployment in Embedded Systems Abstract: Deep learning has delivered its powerfulness in many application domains,\nespecially in image and speech recognition. As the backbone of deep learning,\ndeep neural networks (DNNs) consist of multiple layers of various types with\nhundreds to thousands of neurons. Embedded platforms are now becoming essential\nfor deep learning deployment due to their portability, versatility, and energy\nefficiency. The large model size of DNNs, while providing excellent accuracy,\nalso burdens the embedded platforms with intensive computation and storage.\nResearchers have investigated on reducing DNN model size with negligible\naccuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN\ntraining and inference model suitable for embedded platforms with reduced\nasymptotic complexity of both computation and storage, making our approach\ndistinguished from existing approaches. We develop the training and inference\nalgorithms based on FFT as the computing kernel and deploy the FFT-based\ninference model on embedded platforms achieving extraordinary processing speed. \n\n"}
{"id": "1712.05134", "contents": "Title: Learning Compact Recurrent Neural Networks with Block-Term Tensor\n  Decomposition Abstract: Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.\nHowever, when dealing with high dimensional inputs, the training of RNNs\nbecomes computational expensive due to the large number of model parameters.\nThis hinders RNNs from solving many important computer vision tasks, such as\nAction Recognition in Videos and Image Captioning. To overcome this problem, we\npropose a compact and flexible structure, namely Block-Term tensor\ndecomposition, which greatly reduces the parameters of RNNs and improves their\ntraining efficiency. Compared with alternative low-rank approximations, such as\ntensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only\nmore concise (when using the same rank), but also able to attain a better\napproximation to the original RNNs with much fewer parameters. On three\nchallenging tasks, including Action Recognition in Videos, Image Captioning and\nImage Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of\nboth prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes\n17,388 times fewer parameters than the standard LSTM to achieve an accuracy\nimprovement over 15.6\\% in the Action Recognition task on the UCF11 dataset. \n\n"}
{"id": "1712.06302", "contents": "Title: Visual Explanation by Interpretation: Improving Visual Feedback\n  Capabilities of Deep Neural Networks Abstract: Interpretation and explanation of deep models is critical towards wide\nadoption of systems that rely on them. In this paper, we propose a novel scheme\nfor both interpretation as well as explanation in which, given a pretrained\nmodel, we automatically identify internal features relevant for the set of\nclasses considered by the model, without relying on additional annotations. We\ninterpret the model through average visualizations of this reduced set of\nfeatures. Then, at test time, we explain the network prediction by accompanying\nthe predicted class label with supporting visualizations derived from the\nidentified features. In addition, we propose a method to address the artifacts\nintroduced by stridded operations in deconvNet-based visualizations. Moreover,\nwe introduce an8Flower, a dataset specifically designed for objective\nquantitative evaluation of methods for visual explanation.Experiments on the\nMNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces\ndetailed explanations with good coverage of relevant features of the classes of\ninterest \n\n"}
{"id": "1712.08880", "contents": "Title: Lectures on Randomized Numerical Linear Algebra Abstract: This chapter is based on lectures on Randomized Numerical Linear Algebra from\nthe 2016 Park City Mathematics Institute summer school on The Mathematics of\nData. \n\n"}
{"id": "1712.09473", "contents": "Title: Sketching for Kronecker Product Regression and P-splines Abstract: TensorSketch is an oblivious linear sketch introduced in Pagh'13 and later\nused in Pham, Pagh'13 in the context of SVMs for polynomial kernels. It was\nshown in Avron, Nguyen, Woodruff'14 that TensorSketch provides a subspace\nembedding, and therefore can be used for canonical correlation analysis, low\nrank approximation, and principal component regression for the polynomial\nkernel. We take TensorSketch outside of the context of polynomials kernels, and\nshow its utility in applications in which the underlying design matrix is a\nKronecker product of smaller matrices. This allows us to solve Kronecker\nproduct regression and non-negative Kronecker product regression, as well as\nregularized spline regression. Our main technical result is then in extending\nTensorSketch to other norms. That is, TensorSketch only provides input sparsity\ntime for Kronecker product regression with respect to the $2$-norm. We show how\nto solve Kronecker product regression with respect to the $1$-norm in time\nsublinear in the time required for computing the Kronecker product, as well as\nfor more general $p$-norms. \n\n"}
{"id": "1801.00318", "contents": "Title: Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification Abstract: Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system. \n\n"}
{"id": "1801.00329", "contents": "Title: ZOOpt: Toolbox for Derivative-Free Optimization Abstract: Recent advances in derivative-free optimization allow efficient approximation\nof the global-optimal solutions of sophisticated functions, such as functions\nwith many local optima, non-differentiable and non-continuous functions. This\narticle describes the ZOOpt (Zeroth Order Optimization) toolbox that provides\nefficient derivative-free solvers and is designed easy to use. ZOOpt provides\nsingle-machine parallel optimization on the basis of python core and\nmulti-machine distributed optimization for time-consuming tasks by\nincorporating with the Ray framework -- a famous platform for building\ndistributed applications. ZOOpt particularly focuses on optimization problems\nin machine learning, addressing high-dimensional and noisy problems such as\nhyper-parameter tuning and direct policy search. The toolbox is maintained\ntoward a ready-to-use tool in real-world machine learning tasks. \n\n"}
{"id": "1801.02850", "contents": "Title: Less is More: Culling the Training Set to Improve Robustness of Deep\n  Neural Networks Abstract: Deep neural networks are vulnerable to adversarial examples. Prior defenses\nattempted to make deep networks more robust by either changing the network\narchitecture or augmenting the training set with adversarial examples, but both\nhave inherent limitations. Motivated by recent research that shows outliers in\nthe training set have a high negative influence on the trained model, we\nstudied the relationship between model robustness and the quality of the\ntraining set. We first show that outliers give the model better generalization\nability but weaker robustness. Next, we propose an adversarial example\ndetection framework, in which we design two methods for removing outliers from\ntraining set to obtain the sanitized model and then detect adversarial example\nby calculating the difference of outputs between the original and the sanitized\nmodel. We evaluated the framework on both MNIST and SVHN. Based on the\ndifference measured by Kullback-Leibler divergence, we could detect adversarial\nexamples with accuracy between 94.67% to 99.89%. \n\n"}
{"id": "1801.03533", "contents": "Title: Selection Problems in the Presence of Implicit Bias Abstract: Over the past two decades, the notion of implicit bias has come to serve as\nan important component in our understanding of discrimination in activities\nsuch as hiring, promotion, and school admissions. Research on implicit bias\nposits that when people evaluate others -- for example, in a hiring context --\ntheir unconscious biases about membership in particular groups can have an\neffect on their decision-making, even when they have no deliberate intention to\ndiscriminate against members of these groups. A growing body of experimental\nwork has pointed to the effect that implicit bias can have in producing adverse\noutcomes.\n  Here we propose a theoretical model for studying the effects of implicit bias\non selection decisions, and a way of analyzing possible procedural remedies for\nimplicit bias within this model. A canonical situation represented by our model\nis a hiring setting: a recruiting committee is trying to choose a set of\nfinalists to interview among the applicants for a job, evaluating these\napplicants based on their future potential, but their estimates of potential\nare skewed by implicit bias against members of one group. In this model, we\nshow that measures such as the Rooney Rule, a requirement that at least one of\nthe finalists be chosen from the affected group, can not only improve the\nrepresentation of this affected group, but also lead to higher payoffs in\nabsolute terms for the organization performing the recruiting. However,\nidentifying the conditions under which such measures can lead to improved\npayoffs involves subtle trade-offs between the extent of the bias and the\nunderlying distribution of applicant characteristics, leading to novel\ntheoretical questions about order statistics in the presence of probabilistic\nside information. \n\n"}
{"id": "1801.03558", "contents": "Title: Inference Suboptimality in Variational Autoencoders Abstract: Amortized inference allows latent-variable models trained via variational\nlearning to scale to large datasets. The quality of approximate inference is\ndetermined by two factors: a) the capacity of the variational distribution to\nmatch the true posterior and b) the ability of the recognition network to\nproduce good variational parameters for each datapoint. We examine approximate\ninference in variational autoencoders in terms of these factors. We find that\ndivergence from the true posterior is often due to imperfect recognition\nnetworks, rather than the limited complexity of the approximating distribution.\nWe show that this is due partly to the generator learning to accommodate the\nchoice of approximation. Furthermore, we show that the parameters used to\nincrease the expressiveness of the approximation play a role in generalizing\ninference rather than simply improving the complexity of the approximation. \n\n"}
{"id": "1801.04503", "contents": "Title: Multivariate LSTM-FCNs for Time Series Classification Abstract: Over the past decade, multivariate time series classification has received\ngreat attention. We propose transforming the existing univariate time series\nclassification models, the Long Short Term Memory Fully Convolutional Network\n(LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series\nclassification model by augmenting the fully convolutional block with a\nsqueeze-and-excitation block to further improve accuracy. Our proposed models\noutperform most state-of-the-art models while requiring minimum preprocessing.\nThe proposed models work efficiently on various complex multivariate time\nseries classification tasks such as activity recognition or action recognition.\nFurthermore, the proposed models are highly efficient at test time and small\nenough to deploy on memory constrained systems. \n\n"}
{"id": "1801.04813", "contents": "Title: Predicting Movie Genres Based on Plot Summaries Abstract: This project explores several Machine Learning methods to predict movie\ngenres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent\nNeural Networks are used for text classification, while K-binary\ntransformation, rank method and probabilistic classification with learned\nprobability threshold are employed for the multi-label problem involved in the\ngenre tagging task.Experiments with more than 250,000 movies show that\nemploying the Gated Recurrent Units (GRU) neural networks for the probabilistic\nclassification with learned probability threshold approach achieves the best\nresult on the test set. The model attains a Jaccard Index of 50.0%, a F-score\nof 0.56, and a hit rate of 80.5%. \n\n"}
{"id": "1801.05039", "contents": "Title: Global Convergence of Policy Gradient Methods for the Linear Quadratic\n  Regulator Abstract: Direct policy gradient methods for reinforcement learning and continuous\ncontrol problems are a popular approach for a variety of reasons: 1) they are\neasy to implement without explicit knowledge of the underlying model 2) they\nare an \"end-to-end\" approach, directly optimizing the performance metric of\ninterest 3) they inherently allow for richly parameterized policies. A notable\ndrawback is that even in the most basic continuous control problem (that of\nlinear quadratic regulators), these methods must solve a non-convex\noptimization problem, where little is understood about their efficiency from\nboth computational and statistical perspectives. In contrast, system\nidentification and model based planning in optimal control theory have a much\nmore solid theoretical footing, where much is known with regards to their\ncomputational and statistical properties. This work bridges this gap showing\nthat (model free) policy gradient methods globally converge to the optimal\nsolution and are efficient (polynomially so in relevant problem dependent\nquantities) with regards to their sample and computational complexities. \n\n"}
{"id": "1801.05134", "contents": "Title: Understanding the Disharmony between Dropout and Batch Normalization by\n  Variance Shift Abstract: This paper first answers the question \"why do the two most powerful\ntechniques Dropout and Batch Normalization (BN) often lead to a worse\nperformance when they are combined together?\" in both theoretical and\nstatistical aspects. Theoretically, we find that Dropout would shift the\nvariance of a specific neural unit when we transfer the state of that network\nfrom train to test. However, BN would maintain its statistical variance, which\nis accumulated from the entire learning procedure, in the test phase. The\ninconsistency of that variance (we name this scheme as \"variance shift\") causes\nthe unstable numerical behavior in inference that leads to more erroneous\npredictions finally, when applying Dropout before BN. Thorough experiments on\nDenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to\nthe uncovered mechanism, we next explore several strategies that modifies\nDropout and try to overcome the limitations of their combination by avoiding\nthe variance shift risks. \n\n"}
{"id": "1801.05852", "contents": "Title: Network Representation Learning: A Survey Abstract: With the widespread use of information technologies, information networks are\nbecoming increasingly popular to capture complex relationships across various\ndisciplines, such as social networks, citation networks, telecommunication\nnetworks, and biological networks. Analyzing these networks sheds light on\ndifferent aspects of social life such as the structure of societies,\ninformation diffusion, and communication patterns. In reality, however, the\nlarge scale of information networks often makes network analytic tasks\ncomputationally expensive or intractable. Network representation learning has\nbeen recently proposed as a new learning paradigm to embed network vertices\ninto a low-dimensional vector space, by preserving network topology structure,\nvertex content, and other side information. This facilitates the original\nnetwork to be easily handled in the new vector space for further analysis. In\nthis survey, we perform a comprehensive review of the current literature on\nnetwork representation learning in the data mining and machine learning field.\nWe propose new taxonomies to categorize and summarize the state-of-the-art\nnetwork representation learning techniques according to the underlying learning\nmechanisms, the network information intended to preserve, as well as the\nalgorithmic designs and methodologies. We summarize evaluation protocols used\nfor validating network representation learning including published benchmark\ndatasets, evaluation methods, and open source algorithms. We also perform\nempirical studies to compare the performance of representative algorithms on\ncommon datasets, and analyze their computational complexity. Finally, we\nsuggest promising research directions to facilitate future study. \n\n"}
{"id": "1801.05856", "contents": "Title: Active Community Detection with Maximal Expected Model Change Abstract: We present a novel active learning algorithm for community detection on\nnetworks. Our proposed algorithm uses a Maximal Expected Model Change (MEMC)\ncriterion for querying network nodes label assignments. MEMC detects nodes that\nmaximally change the community assignment likelihood model following a query.\nOur method is inspired by detection in the benchmark Stochastic Block Model\n(SBM), where we provide sample complexity analysis and empirical study with SBM\nand real network data for binary as well as for the multi-class settings. The\nanalysis also covers the most challenging case of sparse degree and\nbelow-detection-threshold SBMs, where we observe a super-linear error\nreduction. MEMC is shown to be superior to the random selection baseline and\nother state-of-the-art active learners. \n\n"}
{"id": "1801.06934", "contents": "Title: On the Iteration Complexity Analysis of Stochastic Primal-Dual Hybrid\n  Gradient Approach with High Probability Abstract: In this paper, we propose a stochastic Primal-Dual Hybrid Gradient (PDHG)\napproach for solving a wide spectrum of regularized stochastic minimization\nproblems, where the regularization term is composite with a linear function. It\nhas been recognized that solving this kind of problem is challenging since the\nclosed-form solution of the proximal mapping associated with the regularization\nterm is not available due to the imposed linear composition, and the\nper-iteration cost of computing the full gradient of the expected objective\nfunction is extremely high when the number of input data samples is\nconsiderably large.\n  Our new approach overcomes these issues by exploring the special structure of\nthe regularization term and sampling a few data points at each iteration.\nRather than analyzing the convergence in expectation, we provide the detailed\niteration complexity analysis for the cases of both uniformly and non-uniformly\naveraged iterates with high probability. This strongly supports the good\npractical performance of the proposed approach. Numerical experiments\ndemonstrate that the efficiency of stochastic PDHG, which outperforms other\ncompeting algorithms, as expected by the high-probability convergence analysis. \n\n"}
{"id": "1801.07194", "contents": "Title: Optimizing Prediction Intervals by Tuning Random Forest via\n  Meta-Validation Abstract: Recent studies have shown that tuning prediction models increases prediction\naccuracy and that Random Forest can be used to construct prediction intervals.\nHowever, to our best knowledge, no study has investigated the need to, and the\nmanner in which one can, tune Random Forest for optimizing prediction intervals\n{ this paper aims to fill this gap. We explore a tuning approach that combines\nan effectively exhaustive search with a validation technique on a single Random\nForest parameter. This paper investigates which, out of eight validation\ntechniques, are beneficial for tuning, i.e., which automatically choose a\nRandom Forest configuration constructing prediction intervals that are reliable\nand with a smaller width than the default configuration. Additionally, we\npresent and validate three meta-validation techniques to determine which are\nbeneficial, i.e., those which automatically chose a beneficial validation\ntechnique. This study uses data from our industrial partner (Keymind Inc.) and\nthe Tukutuku Research Project, related to post-release defect prediction and\nWeb application effort estimation, respectively. Results from our study\nindicate that: i) the default configuration is frequently unreliable, ii) most\nof the validation techniques, including previously successfully adopted ones\nsuch as 50/50 holdout and bootstrap, are counterproductive in most of the\ncases, and iii) the 75/25 holdout meta-validation technique is always\nbeneficial; i.e., it avoids the likely counterproductive effects of validation\ntechniques. \n\n"}
{"id": "1801.07353", "contents": "Title: Flexible Deep Neural Network Processing Abstract: The recent success of Deep Neural Networks (DNNs) has drastically improved\nthe state of the art for many application domains. While achieving high\naccuracy performance, deploying state-of-the-art DNNs is a challenge since they\ntypically require billions of expensive arithmetic computations. In addition,\nDNNs are typically deployed in ensemble to boost accuracy performance, which\nfurther exacerbates the system requirements. This computational overhead is an\nissue for many platforms, e.g. data centers and embedded systems, with tight\nlatency and energy budgets. In this article, we introduce flexible DNNs\nensemble processing technique, which achieves large reduction in average\ninference latency while incurring small to negligible accuracy drop. Our\ntechnique is flexible in that it allows for dynamic adaptation between quality\nof results (QoR) and execution runtime. We demonstrate the effectiveness of the\ntechnique on AlexNet and ResNet-50 using the ImageNet dataset. This technique\ncan also easily handle other types of networks. \n\n"}
{"id": "1801.08196", "contents": "Title: Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory\n  and Applications Abstract: The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)\nof a graph Laplacian matrix have been widely used in spectral clustering and\ncommunity detection. However, in real-life applications the number of clusters\nor communities (say, $K$) is generally unknown a-priori. Consequently, the\nmajority of the existing methods either choose $K$ heuristically or they repeat\nthe clustering method with different choices of $K$ and accept the best\nclustering result. The first option, more often, yields suboptimal result,\nwhile the second option is computationally expensive. In this work, we propose\nan incremental method for constructing the eigenspectrum of the graph Laplacian\nmatrix. This method leverages the eigenstructure of graph Laplacian matrix to\nobtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection\nof all previously computed $K-1$ smallest eigenpairs. Our proposed method\nadapts the Laplacian matrix such that the batch eigenvalue decomposition\nproblem transforms into an efficient sequential leading eigenpair computation\nproblem. As a practical application, we consider user-guided spectral\nclustering. Specifically, we demonstrate that users can utilize the proposed\nincremental method for effective eigenpair computation and for determining the\ndesired number of clusters based on multiple clustering metrics. \n\n"}
{"id": "1802.00459", "contents": "Title: Nearly Optimal Dynamic $k$-Means Clustering for High-Dimensional Data Abstract: We consider the $k$-means clustering problem in the dynamic streaming\nsetting, where points from a discrete Euclidean space $\\{1, 2, \\ldots,\n\\Delta\\}^d$ can be dynamically inserted to or deleted from the dataset. For\nthis problem, we provide a one-pass coreset construction algorithm using space\n$\\tilde{O}(k\\cdot \\mathrm{poly}(d, \\log\\Delta))$, where $k$ is the target\nnumber of centers. To our knowledge, this is the first dynamic geometric data\nstream algorithm for $k$-means using space polynomial in dimension and nearly\noptimal (linear) in $k$. \n\n"}
{"id": "1802.01284", "contents": "Title: Task-Aware Compressed Sensing with Generative Adversarial Networks Abstract: In recent years, neural network approaches have been widely adopted for\nmachine learning tasks, with applications in computer vision. More recently,\nunsupervised generative models based on neural networks have been successfully\napplied to model data distributions via low-dimensional latent spaces. In this\npaper, we use Generative Adversarial Networks (GANs) to impose structure in\ncompressed sensing problems, replacing the usual sparsity constraint. We\npropose to train the GANs in a task-aware fashion, specifically for\nreconstruction tasks. We also show that it is possible to train our model\nwithout using any (or much) non-compressed data. Finally, we show that the\nlatent space of the GAN carries discriminative information and can further be\nregularized to generate input features for general inference tasks. We\ndemonstrate the effectiveness of our method on a variety of reconstruction and\nclassification problems. \n\n"}
{"id": "1802.01697", "contents": "Title: Deep Learning with a Rethinking Structure for Multi-label Classification Abstract: Multi-label classification (MLC) is an important class of machine learning\nproblems that come with a wide spectrum of applications, each demanding a\npossibly different evaluation criterion. When solving the MLC problems, we\ngenerally expect the learning algorithm to take the hidden correlation of the\nlabels into account to improve the prediction performance. Extracting the\nhidden correlation is generally a challenging task. In this work, we propose a\nnovel deep learning framework to better extract the hidden correlation with the\nhelp of the memory structure within recurrent neural networks. The memory\nstores the temporary guesses on the labels and effectively allows the framework\nto rethink about the goodness and correlation of the guesses before making the\nfinal prediction. Furthermore, the rethinking process makes it easy to adapt to\ndifferent evaluation criteria to match real-world application needs. In\nparticular, the framework can be trained in an end-to-end style with respect to\nany given MLC evaluation criteria. The end-to-end design can be seamlessly\ncombined with other deep learning techniques to conquer challenging MLC\nproblems like image tagging. Experimental results across many real-world data\nsets justify that the rethinking framework indeed improves MLC performance\nacross different evaluation criteria and leads to superior performance over\nstate-of-the-art MLC algorithms. \n\n"}
{"id": "1802.01709", "contents": "Title: Weakly-supervised Dictionary Learning Abstract: We present a probabilistic modeling and inference framework for\ndiscriminative analysis dictionary learning under a weak supervision setting.\nDictionary learning approaches have been widely used for tasks such as\nlow-level signal denoising and restoration as well as high-level classification\ntasks, which can be applied to audio and image analysis. Synthesis dictionary\nlearning aims at jointly learning a dictionary and corresponding sparse\ncoefficients to provide accurate data representation. This approach is useful\nfor denoising and signal restoration, but may lead to sub-optimal\nclassification performance. By contrast, analysis dictionary learning provides\na transform that maps data to a sparse discriminative representation suitable\nfor classification. We consider the problem of analysis dictionary learning for\ntime-series data under a weak supervision setting in which signals are assigned\nwith a global label instead of an instantaneous label signal. We propose a\ndiscriminative probabilistic model that incorporates both label information and\nsparsity constraints on the underlying latent instantaneous label signal using\ncardinality control. We present the expectation maximization (EM) procedure for\nmaximum likelihood estimation (MLE) of the proposed model. To facilitate a\ncomputationally efficient E-step, we propose both a chain and a novel tree\ngraph reformulation of the graphical model. The performance of the proposed\nmodel is demonstrated on both synthetic and real-world data. \n\n"}
{"id": "1802.02565", "contents": "Title: Applying Cooperative Machine Learning to Speed Up the Annotation of\n  Social Signals in Large Multi-modal Corpora Abstract: Scientific disciplines, such as Behavioural Psychology, Anthropology and\nrecently Social Signal Processing are concerned with the systematic exploration\nof human behaviour. A typical work-flow includes the manual annotation (also\ncalled coding) of social signals in multi-modal corpora of considerable size.\nFor the involved annotators this defines an exhausting and time-consuming task.\nIn the article at hand we present a novel method and also provide the tools to\nspeed up the coding procedure. To this end, we suggest and evaluate the use of\nCooperative Machine Learning (CML) techniques to reduce manual labelling\nefforts by combining the power of computational capabilities and human\nintelligence. The proposed CML strategy starts with a small number of labelled\ninstances and concentrates on predicting local parts first. Afterwards, a\nsession-independent classification model is created to finish the remaining\nparts of the database. Confidence values are computed to guide the manual\ninspection and correction of the predictions. To bring the proposed approach\ninto application we introduce NOVA - an open-source tool for collaborative and\nmachine-aided annotations. In particular, it gives labellers immediate access\nto CML strategies and directly provides visual feedback on the results. Our\nexperiments show that the proposed method has the potential to significantly\nreduce human labelling efforts. \n\n"}
{"id": "1802.03725", "contents": "Title: A Generative Model for Dynamic Networks with Applications Abstract: Networks observed in real world like social networks, collaboration networks\netc., exhibit temporal dynamics, i.e. nodes and edges appear and/or disappear\nover time. In this paper, we propose a generative, latent space based,\nstatistical model for such networks (called dynamic networks). We consider the\ncase where the number of nodes is fixed, but the presence of edges can vary\nover time. Our model allows the number of communities in the network to be\ndifferent at different time steps. We use a neural network based methodology to\nperform approximate inference in the proposed model and its simplified version.\nExperiments done on synthetic and real world networks for the task of community\ndetection and link prediction demonstrate the utility and effectiveness of our\nmodel as compared to other similar existing approaches. \n\n"}
{"id": "1802.03800", "contents": "Title: Drug response prediction by ensemble learning and drug-induced gene\n  expression signatures Abstract: Chemotherapeutic response of cancer cells to a given compound is one of the\nmost fundamental information one requires to design anti-cancer drugs. Recent\nadvances in producing large drug screens against cancer cell lines provided an\nopportunity to apply machine learning methods for this purpose. In addition to\ncytotoxicity databases, considerable amount of drug-induced gene expression\ndata has also become publicly available. Following this, several methods that\nexploit omics data were proposed to predict drug activity on cancer cells.\nHowever, due to the complexity of cancer drug mechanisms, none of the existing\nmethods are perfect. One possible direction, therefore, is to combine the\nstrengths of both the methods and the databases for improved performance. We\ndemonstrate that integrating a large number of predictions by the proposed\nmethod improves the performance for this task. The predictors in the ensemble\ndiffer in several aspects such as the method itself, the number of tasks method\nconsiders (multi-task vs. single-task) and the subset of data considered\n(sub-sampling). We show that all these different aspects contribute to the\nsuccess of the final ensemble. In addition, we attempt to use the drug screen\ndata together with two novel signatures produced from the drug-induced gene\nexpression profiles of cancer cell lines. Finally, we evaluate the method\npredictions by in vitro experiments in addition to the tests on data sets.The\npredictions of the methods, the signatures and the software are available from\n\\url{http://mtan.etu.edu.tr/drug-response-prediction/}. \n\n"}
{"id": "1802.04240", "contents": "Title: Reinforcement Learning for Solving the Vehicle Routing Problem Abstract: We present an end-to-end framework for solving the Vehicle Routing Problem\n(VRP) using reinforcement learning. In this approach, we train a single model\nthat finds near-optimal solutions for problem instances sampled from a given\ndistribution, only by observing the reward signals and following feasibility\nrules. Our model represents a parameterized stochastic policy, and by applying\na policy gradient algorithm to optimize its parameters, the trained model\nproduces the solution as a sequence of consecutive actions in real time,\nwithout the need to re-train for every new problem instance. On capacitated\nVRP, our approach outperforms classical heuristics and Google's OR-Tools on\nmedium-sized instances in solution quality with comparable computation time\n(after training). We demonstrate how our approach can handle problems with\nsplit delivery and explore the effect of such deliveries on the solution\nquality. Our proposed framework can be applied to other variants of the VRP\nsuch as the stochastic VRP, and has the potential to be applied more generally\nto combinatorial optimization problems. \n\n"}
{"id": "1802.04412", "contents": "Title: Efficient Exploration through Bayesian Deep Q-Networks Abstract: We study reinforcement learning (RL) in high dimensional episodic Markov\ndecision processes (MDP). We consider value-based RL when the optimal Q-value\nis a linear function of d-dimensional state-action feature representation. For\ninstance, in deep-Q networks (DQN), the Q-value is a linear function of the\nfeature representation layer (output layer). We propose two algorithms, one\nbased on optimism, LINUCB, and another based on posterior sampling, LINPSRL. We\nguarantee frequentist and Bayesian regret upper bounds of O(d sqrt{T}) for\nthese two algorithms, where T is the number of episodes. We extend these\nmethods to deep RL and propose Bayesian deep Q-networks (BDQN), which uses an\nefficient Thompson sampling algorithm for high dimensional RL. We deploy the\ndouble DQN (DDQN) approach, and instead of learning the last layer of Q-network\nusing linear regression, we use Bayesian linear regression, resulting in an\napproximated posterior over Q-function. This allows us to directly incorporate\nthe uncertainty over the Q-function and deploy Thompson sampling on the learned\nposterior distribution resulting in efficient exploration/exploitation\ntrade-off. We empirically study the behavior of BDQN on a wide range of Atari\ngames. Since BDQN carries out more efficient exploration and exploitation, it\nis able to reach higher return substantially faster compared to DDQN. \n\n"}
{"id": "1802.04447", "contents": "Title: Graph Coarsening with Preserved Spectral Properties Abstract: Large-scale graphs are widely used to represent object relationships in many\nreal world applications. The occurrence of large-scale graphs presents\nsignificant computational challenges to process, analyze, and extract\ninformation. Graph coarsening techniques are commonly used to reduce the\ncomputational load while attempting to maintain the basic structural properties\nof the original graph. As there is no consensus on the specific graph\nproperties preserved by coarse graphs, how to measure the differences between\noriginal and coarse graphs remains a key challenge. In this work, we introduce\na new perspective regarding the graph coarsening based on concepts from\nspectral graph theory. We propose and justify new distance functions that\ncharacterize the differences between original and coarse graphs. We show that\nthe proposed spectral distance naturally captures the structural differences in\nthe graph coarsening process. In addition, we provide efficient graph\ncoarsening algorithms to generate graphs which provably preserve the spectral\nproperties from original graphs. Experiments show that our proposed algorithms\nconsistently achieve better results compared to previous graph coarsening\nmethods on graph classification and block recovery tasks. \n\n"}
{"id": "1802.04475", "contents": "Title: Graph-Based Ascent Algorithms for Function Maximization Abstract: We study the problem of finding the maximum of a function defined on the\nnodes of a connected graph. The goal is to identify a node where the function\nobtains its maximum. We focus on local iterative algorithms, which traverse the\nnodes of the graph along a path, and the next iterate is chosen from the\nneighbors of the current iterate with probability distribution determined by\nthe function values at the current iterate and its neighbors. We study two\nalgorithms corresponding to a Metropolis-Hastings random walk with different\ntransition kernels: (i) The first algorithm is an exponentially weighted random\nwalk governed by a parameter $\\gamma$. (ii) The second algorithm is defined\nwith respect to the graph Laplacian and a smoothness parameter $k$. We derive\nconvergence rates for the two algorithms in terms of total variation distance\nand hitting times. We also provide simulations showing the relative convergence\nrates of our algorithms in comparison to an unbiased random walk, as a function\nof the smoothness of the graph function. Our algorithms may be categorized as a\nnew class of \"descent-based\" methods for function maximization on the nodes of\na graph. \n\n"}
{"id": "1802.04477", "contents": "Title: A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex\n  Optimization Abstract: We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth\nfinite-sum problems. In particular, the objective function is given by the\nsummation of a differentiable (possibly nonconvex) component, together with a\npossibly non-differentiable but convex component. We propose a proximal\nstochastic gradient algorithm based on variance reduction, called ProxSVRG+.\nOur main contribution lies in the analysis of ProxSVRG+. It recovers several\nexisting convergence results and improves/generalizes them (in terms of the\nnumber of stochastic gradient oracle calls and proximal oracle calls). In\nparticular, ProxSVRG+ generalizes the best results given by the SCSG algorithm,\nrecently proposed by [Lei et al., 2017] for the smooth nonconvex case.\nProxSVRG+ is also more straightforward than SCSG and yields simpler analysis.\nMoreover, ProxSVRG+ outperforms the deterministic proximal gradient descent\n(ProxGD) for a wide range of minibatch sizes, which partially solves an open\nproblem proposed in [Reddi et al., 2016b]. Also, ProxSVRG+ uses much less\nproximal oracle calls than ProxSVRG [Reddi et al., 2016b]. Moreover, for\nnonconvex functions satisfied Polyak-\\L{}ojasiewicz condition, we prove that\nProxSVRG+ achieves a global linear convergence rate without restart unlike\nProxSVRG. Thus, it can \\emph{automatically} switch to the faster linear\nconvergence in some regions as long as the objective function satisfies the PL\ncondition locally in these regions. ProxSVRG+ also improves ProxGD and\nProxSVRG/SAGA, and generalizes the results of SCSG in this case. Finally, we\nconduct several experiments and the experimental results are consistent with\nthe theoretical results. \n\n"}
{"id": "1802.06309", "contents": "Title: Learning Adversarially Fair and Transferable Representations Abstract: In this paper, we advocate for representation learning as the key to\nmitigating unfair prediction outcomes downstream. Motivated by a scenario where\nlearned representations are used by third parties with unknown objectives, we\npropose and explore adversarial representation learning as a natural method of\nensuring those parties act fairly. We connect group fairness (demographic\nparity, equalized odds, and equal opportunity) to different adversarial\nobjectives. Through worst-case theoretical guarantees and experimental\nvalidation, we show that the choice of this objective is crucial to fair\nprediction. Furthermore, we present the first in-depth experimental\ndemonstration of fair transfer learning and demonstrate empirically that our\nlearned representations admit fair predictions on new tasks while maintaining\nutility, an essential goal of fair representation learning. \n\n"}
{"id": "1802.06403", "contents": "Title: RadialGAN: Leveraging multiple datasets to improve target-specific\n  predictive models using Generative Adversarial Networks Abstract: Training complex machine learning models for prediction often requires a\nlarge amount of data that is not always readily available. Leveraging these\nexternal datasets from related but different sources is therefore an important\ntask if good predictive models are to be built for deployment in settings where\ndata can be rare. In this paper we propose a novel approach to the problem in\nwhich we use multiple GAN architectures to learn to translate from one dataset\nto another, thereby allowing us to effectively enlarge the target dataset, and\ntherefore learn better predictive models than if we simply used the target\ndataset. We show the utility of such an approach, demonstrating that our method\nimproves the prediction performance on the target domain over using just the\ntarget dataset and also show that our framework outperforms several other\nbenchmarks on a collection of real-world medical datasets. \n\n"}
{"id": "1802.06480", "contents": "Title: Accelerated Primal-Dual Policy Optimization for Safe Reinforcement\n  Learning Abstract: Constrained Markov Decision Process (CMDP) is a natural framework for\nreinforcement learning tasks with safety constraints, where agents learn a\npolicy that maximizes the long-term reward while satisfying the constraints on\nthe long-term cost. A canonical approach for solving CMDPs is the primal-dual\nmethod which updates parameters in primal and dual spaces in turn. Existing\nmethods for CMDPs only use on-policy data for dual updates, which results in\nsample inefficiency and slow convergence. In this paper, we propose a policy\nsearch method for CMDPs called Accelerated Primal-Dual Optimization (APDO),\nwhich incorporates an off-policy trained dual variable in the dual update\nprocedure while updating the policy in primal space with on-policy likelihood\nratio gradient. Experimental results on a simulated robot locomotion task show\nthat APDO achieves better sample efficiency and faster convergence than\nstate-of-the-art approaches for CMDPs. \n\n"}
{"id": "1802.06501", "contents": "Title: Recommendations with Negative Feedback via Pairwise Deep Reinforcement\n  Learning Abstract: Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users' personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users' feedback. Users' feedback can be positive and negative and both\ntypes of feedback have great potentials to boost recommendations. However, the\nnumber of negative feedback is much larger than that of positive one; thus\nincorporating them simultaneously is challenging since positive feedback could\nbe buried by negative one. In this paper, we develop a novel approach to\nincorporate them into the proposed deep recommender system (DEERS) framework.\nThe experimental results based on real-world e-commerce data demonstrate the\neffectiveness of the proposed framework. Further experiments have been\nconducted to understand the importance of both positive and negative feedback\nin recommendations. \n\n"}
{"id": "1802.06552", "contents": "Title: Are Generative Classifiers More Robust to Adversarial Attacks? Abstract: There is a rising interest in studying the robustness of deep neural network\nclassifiers against adversaries, with both advanced attack and defence\ntechniques being actively developed. However, most recent work focuses on\ndiscriminative classifiers, which only model the conditional distribution of\nthe labels given the inputs. In this paper, we propose and investigate the deep\nBayes classifier, which improves classical naive Bayes with conditional deep\ngenerative models. We further develop detection methods for adversarial\nexamples, which reject inputs with low likelihood under the generative model.\nExperimental results suggest that deep Bayes classifiers are more robust than\ndeep discriminative classifiers, and that the proposed detection methods are\neffective against many recently proposed attacks. \n\n"}
{"id": "1802.07244", "contents": "Title: Steering Social Activity: A Stochastic Optimal Control Point Of View Abstract: User engagement in online social networking depends critically on the level\nof social activity in the corresponding platform--the number of online actions,\nsuch as posts, shares or replies, taken by their users. Can we design\ndata-driven algorithms to increase social activity? At a user level, such\nalgorithms may increase activity by helping users decide when to take an action\nto be more likely to be noticed by their peers. At a network level, they may\nincrease activity by incentivizing a few influential users to take more\nactions, which in turn will trigger additional actions by other users. In this\npaper, we model social activity using the framework of marked temporal point\nprocesses, derive an alternate representation of these processes using\nstochastic differential equations (SDEs) with jumps and, exploiting this\nalternate representation, develop two efficient online algorithms with provable\nguarantees to steer social activity both at a user and at a network level. In\ndoing so, we establish a previously unexplored connection between optimal\ncontrol of jump SDEs and doubly stochastic marked temporal point processes,\nwhich is of independent interest. Finally, we experiment both with synthetic\nand real data gathered from Twitter and show that our algorithms consistently\nsteer social activity more effectively than the state of the art. \n\n"}
{"id": "1802.07687", "contents": "Title: Stochastic Video Generation with a Learned Prior Abstract: Generating video frames that accurately predict future world states is\nchallenging. Existing approaches either fail to capture the full distribution\nof outcomes, or yield blurry generations, or both. In this paper we introduce\nan unsupervised video generation model that learns a prior model of uncertainty\nin a given environment. Video frames are generated by drawing samples from this\nprior and combining them with a deterministic estimate of the future frame. The\napproach is simple and easily trained end-to-end on a variety of datasets.\nSample generations are both varied and sharp, even many frames into the future,\nand compare favorably to those from existing approaches. \n\n"}
{"id": "1802.08471", "contents": "Title: Optimized Algorithms to Sample Determinantal Point Processes Abstract: In this technical report, we discuss several sampling algorithms for\nDeterminantal Point Processes (DPP). DPPs have recently gained a broad interest\nin the machine learning and statistics literature as random point processes\nwith negative correlation, i.e., ones that can generate a \"diverse\" sample from\na set of items. They are parametrized by a matrix $\\mathbf{L}$, called\n$L$-ensemble, that encodes the correlations between items. The standard\nsampling algorithm is separated in three phases: 1/~eigendecomposition of\n$\\mathbf{L}$, 2/~an eigenvector sampling phase where $\\mathbf{L}$'s\neigenvectors are sampled independently via a Bernoulli variable parametrized by\ntheir associated eigenvalue, 3/~a Gram-Schmidt-type orthogonalisation procedure\nof the sampled eigenvectors.\n  In a naive implementation, the computational cost of the third step is on\naverage $\\mathcal{O}(N\\mu^3)$ where $\\mu$ is the average number of samples of\nthe DPP. We give an algorithm which runs in $\\mathcal{O}(N\\mu^2)$ and is\nextremely simple to implement. If memory is a constraint, we also describe a\ndual variant with reduced memory costs. In addition, we discuss implementation\ndetails often missing in the literature. \n\n"}
{"id": "1802.08471", "contents": "Title: Optimized Algorithms to Sample Determinantal Point Processes Abstract: In this technical report, we discuss several sampling algorithms for\nDeterminantal Point Processes (DPP). DPPs have recently gained a broad interest\nin the machine learning and statistics literature as random point processes\nwith negative correlation, i.e., ones that can generate a \"diverse\" sample from\na set of items. They are parametrized by a matrix $\\mathbf{L}$, called\n$L$-ensemble, that encodes the correlations between items. The standard\nsampling algorithm is separated in three phases: 1/~eigendecomposition of\n$\\mathbf{L}$, 2/~an eigenvector sampling phase where $\\mathbf{L}$'s\neigenvectors are sampled independently via a Bernoulli variable parametrized by\ntheir associated eigenvalue, 3/~a Gram-Schmidt-type orthogonalisation procedure\nof the sampled eigenvectors.\n  In a naive implementation, the computational cost of the third step is on\naverage $\\mathcal{O}(N\\mu^3)$ where $\\mu$ is the average number of samples of\nthe DPP. We give an algorithm which runs in $\\mathcal{O}(N\\mu^2)$ and is\nextremely simple to implement. If memory is a constraint, we also describe a\ndual variant with reduced memory costs. In addition, we discuss implementation\ndetails often missing in the literature. \n\n"}
{"id": "1802.08714", "contents": "Title: Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction Abstract: Taxi demand prediction is an important building block to enabling intelligent\ntransportation systems in a smart city. An accurate prediction model can help\nthe city pre-allocate resources to meet travel demand and to reduce empty taxis\non streets which waste energy and worsen the traffic congestion. With the\nincreasing popularity of taxi requesting services such as Uber and Didi Chuxing\n(in China), we are able to collect large-scale taxi demand data continuously.\nHow to utilize such big data to improve the demand prediction is an interesting\nand critical real-world problem. Traditional demand prediction methods mostly\nrely on time series forecasting techniques, which fail to model the complex\nnon-linear spatial and temporal relations. Recent advances in deep learning\nhave shown superior performance on traditionally challenging tasks such as\nimage classification by learning the complex features and correlations from\nlarge-scale data. This breakthrough has inspired researchers to explore deep\nlearning techniques on traffic prediction problems. However, existing methods\non traffic prediction have only considered spatial relation (e.g., using CNN)\nor temporal relation (e.g., using LSTM) independently. We propose a Deep\nMulti-View Spatial-Temporal Network (DMVST-Net) framework to model both spatial\nand temporal relations. Specifically, our proposed model consists of three\nviews: temporal view (modeling correlations between future demand values with\nnear time points via LSTM), spatial view (modeling local spatial correlation\nvia local CNN), and semantic view (modeling correlations among regions sharing\nsimilar temporal patterns). Experiments on large-scale real taxi demand data\ndemonstrate effectiveness of our approach over state-of-the-art methods. \n\n"}
{"id": "1802.10031", "contents": "Title: The Mirage of Action-Dependent Baselines in Reinforcement Learning Abstract: Policy gradient methods are a widely used class of model-free reinforcement\nlearning algorithms where a state-dependent baseline is used to reduce gradient\nestimator variance. Several recent papers extend the baseline to depend on both\nthe state and action and suggest that this significantly reduces variance and\nimproves sample efficiency without introducing bias into the gradient\nestimates. To better understand this development, we decompose the variance of\nthe policy gradient estimator and numerically show that learned\nstate-action-dependent baselines do not in fact reduce variance over a\nstate-dependent baseline in commonly tested benchmark domains. We confirm this\nunexpected result by reviewing the open-source code accompanying these prior\npapers, and show that subtle implementation decisions cause deviations from the\nmethods presented in the papers and explain the source of the previously\nobserved empirical gains. Furthermore, the variance decomposition highlights\nareas for improvement, which we demonstrate by illustrating a simple change to\nthe typical value function parameterization that can significantly improve\nperformance. \n\n"}
{"id": "1802.10168", "contents": "Title: ADMM-based Networked Stochastic Variational Inference Abstract: Owing to the recent advances in \"Big Data\" modeling and prediction tasks,\nvariational Bayesian estimation has gained popularity due to their ability to\nprovide exact solutions to approximate posteriors. One key technique for\napproximate inference is stochastic variational inference (SVI). SVI poses\nvariational inference as a stochastic optimization problem and solves it\niteratively using noisy gradient estimates. It aims to handle massive data for\npredictive and classification tasks by applying complex Bayesian models that\nhave observed as well as latent variables. This paper aims to decentralize it\nallowing parallel computation, secure learning and robustness benefits. We use\nAlternating Direction Method of Multipliers in a top-down setting to develop a\ndistributed SVI algorithm such that independent learners running inference\nalgorithms only require sharing the estimated model parameters instead of their\nprivate datasets. Our work extends the distributed SVI-ADMM algorithm that we\nfirst propose, to an ADMM-based networked SVI algorithm in which not only are\nthe learners working distributively but they share information according to\nrules of a graph by which they form a network. This kind of work lies under the\numbrella of `deep learning over networks' and we verify our algorithm for a\ntopic-modeling problem for corpus of Wikipedia articles. We illustrate the\nresults on latent Dirichlet allocation (LDA) topic model in large document\nclassification, compare performance with the centralized algorithm, and use\nnumerical experiments to corroborate the analytical results. \n\n"}
{"id": "1803.00225", "contents": "Title: Global Convergence of Block Coordinate Descent in Deep Learning Abstract: Deep learning has aroused extensive attention due to its great empirical\nsuccess. The efficiency of the block coordinate descent (BCD) methods has been\nrecently demonstrated in deep neural network (DNN) training. However,\ntheoretical studies on their convergence properties are limited due to the\nhighly nonconvex nature of DNN training. In this paper, we aim at providing a\ngeneral methodology for provable convergence guarantees for this type of\nmethods. In particular, for most of the commonly used DNN training models\ninvolving both two- and three-splitting schemes, we establish the global\nconvergence to a critical point at a rate of ${\\cal O}(1/k)$, where $k$ is the\nnumber of iterations. The results extend to general loss functions which have\nLipschitz continuous gradients and deep residual networks (ResNets). Our key\ndevelopment adds several new elements to the Kurdyka-{\\L}ojasiewicz inequality\nframework that enables us to carry out the global convergence analysis of BCD\nin the general scenario of deep learning. \n\n"}
{"id": "1803.01088", "contents": "Title: Practical Contextual Bandits with Regression Oracles Abstract: A major challenge in contextual bandits is to design general-purpose\nalgorithms that are both practically useful and theoretically well-founded. We\npresent a new technique that has the empirical and computational advantages of\nrealizability-based approaches combined with the flexibility of agnostic\nmethods. Our algorithms leverage the availability of a regression oracle for\nthe value-function class, a more realistic and reasonable oracle than the\nclassification oracles over policies typically assumed by agnostic methods. Our\napproach generalizes both UCB and LinUCB to far more expressive possible model\nclasses and achieves low regret under certain distributional assumptions. In an\nextensive empirical evaluation, compared to both realizability-based and\nagnostic baselines, we find that our approach typically gives comparable or\nsuperior results. \n\n"}
{"id": "1803.01229", "contents": "Title: GAN-based Synthetic Medical Image Augmentation for increased CNN\n  Performance in Liver Lesion Classification Abstract: Deep learning methods, and in particular convolutional neural networks\n(CNNs), have led to an enormous breakthrough in a wide range of computer vision\ntasks, primarily by using large-scale annotated datasets. However, obtaining\nsuch datasets in the medical domain remains a challenge. In this paper, we\npresent methods for generating synthetic medical images using recently\npresented deep learning Generative Adversarial Networks (GANs). Furthermore, we\nshow that generated medical images can be used for synthetic data augmentation,\nand improve the performance of CNN for medical image classification. Our novel\nmethod is demonstrated on a limited dataset of computed tomography (CT) images\nof 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first\nexploit GAN architectures for synthesizing high quality liver lesion ROIs. Then\nwe present a novel scheme for liver lesion classification using CNN. Finally,\nwe train the CNN using classic data augmentation and our synthetic data\naugmentation and compare performance. In addition, we explore the quality of\nour synthesized examples using visualization and expert assessment. The\nclassification performance using only classic data augmentation yielded 78.6%\nsensitivity and 88.4% specificity. By adding the synthetic data augmentation\nthe results increased to 85.7% sensitivity and 92.4% specificity. We believe\nthat this approach to synthetic data augmentation can generalize to other\nmedical classification applications and thus support radiologists' efforts to\nimprove diagnosis. \n\n"}
{"id": "1803.01576", "contents": "Title: Asymptotic Equivalence of Fixed-size and Varying-size Determinantal\n  Point Processes Abstract: Determinantal Point Processes (DPPs) are popular models for point processes\nwith repulsion. They appear in numerous contexts, from physics to graph theory,\nand display appealing theoretical properties. On the more practical side of\nthings, since DPPs tend to select sets of points that are some distance apart\n(repulsion), they have been advocated as a way of producing random subsets with\nhigh diversity. DPPs come in two variants: fixed-size and varying-size. A\nsample from a varying-size DPP is a subset of random cardinality, while in\nfixed-size \"$k$-DPPs\" the cardinality is fixed. The latter makes more sense in\nmany applications, but unfortunately their computational properties are less\nattractive, since, among other things, inclusion probabilities are harder to\ncompute. In this work we show that as the size of the ground set grows,\n$k$-DPPs and DPPs become equivalent, meaning that their inclusion probabilities\nconverge. As a by-product, we obtain saddlepoint formulas for inclusion\nprobabilities in $k$-DPPs. These turn out to be extremely accurate, and suffer\nless from numerical difficulties than exact methods do. Our results also\nsuggest that $k$-DPPs and DPPs also have equivalent maximum likelihood\nestimators. Finally, we obtain results on asymptotic approximations of\nelementary symmetric polynomials which may be of independent interest. \n\n"}
{"id": "1803.01840", "contents": "Title: TACO: Learning Task Decomposition via Temporal Alignment for Control Abstract: Many advanced Learning from Demonstration (LfD) methods consider the\ndecomposition of complex, real-world tasks into simpler sub-tasks. By reusing\nthe corresponding sub-policies within and between tasks, they provide training\ndata for each policy from different high-level tasks and compose them to\nperform novel ones. Existing approaches to modular LfD focus either on learning\na single high-level task or depend on domain knowledge and temporal\nsegmentation. In contrast, we propose a weakly supervised, domain-agnostic\napproach based on task sketches, which include only the sequence of sub-tasks\nperformed in each demonstration. Our approach simultaneously aligns the\nsketches with the observed demonstrations and learns the required sub-policies.\nThis improves generalisation in comparison to separate optimisation procedures.\nWe evaluate the approach on multiple domains, including a simulated 3D robot\narm control task using purely image-based observations. The results show that\nour approach performs commensurately with fully supervised approaches, while\nrequiring significantly less annotation effort. \n\n"}
{"id": "1803.03467", "contents": "Title: RippleNet: Propagating User Preferences on the Knowledge Graph for\n  Recommender Systems Abstract: To address the sparsity and cold start problem of collaborative filtering,\nresearchers usually make use of side information, such as social networks or\nitem attributes, to improve recommendation performance. This paper considers\nthe knowledge graph as the source of side information. To address the\nlimitations of existing embedding-based and path-based methods for\nknowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end\nframework that naturally incorporates the knowledge graph into recommender\nsystems. Similar to actual ripples propagating on the surface of water, Ripple\nNetwork stimulates the propagation of user preferences over the set of\nknowledge entities by automatically and iteratively extending a user's\npotential interests along links in the knowledge graph. The multiple \"ripples\"\nactivated by a user's historically clicked items are thus superposed to form\nthe preference distribution of the user with respect to a candidate item, which\ncould be used for predicting the final clicking probability. Through extensive\nexperiments on real-world datasets, we demonstrate that Ripple Network achieves\nsubstantial gains in a variety of scenarios, including movie, book and news\nrecommendation, over several state-of-the-art baselines. \n\n"}
{"id": "1803.04084", "contents": "Title: Link prediction for egocentrically sampled networks Abstract: Link prediction in networks is typically accomplished by estimating or\nranking the probabilities of edges for all pairs of nodes. In practice,\nespecially for social networks, the data are often collected by egocentric\nsampling, which means selecting a subset of nodes and recording all of their\nedges. This sampling mechanism requires different prediction tools than the\ntypical assumption of links missing at random. We propose a new computationally\nefficient link prediction algorithm for egocentrically sampled networks, which\nestimates the underlying probability matrix by estimating its row space. For\nnetworks created by sampling rows, our method outperforms many popular link\nprediction and graphon estimation techniques. \n\n"}
{"id": "1803.04223", "contents": "Title: Leveraging Crowdsourcing Data For Deep Active Learning - An Application:\n  Learning Intents in Alexa Abstract: This paper presents a generic Bayesian framework that enables any deep\nlearning model to actively learn from targeted crowds. Our framework inherits\nfrom recent advances in Bayesian deep learning, and extends existing work by\nconsidering the targeted crowdsourcing approach, where multiple annotators with\nunknown expertise contribute an uncontrolled amount (often limited) of\nannotations. Our framework leverages the low-rank structure in annotations to\nlearn individual annotator expertise, which then helps to infer the true labels\nfrom noisy and sparse annotations. It provides a unified Bayesian model to\nsimultaneously infer the true labels and train the deep learning model in order\nto reach an optimal learning efficacy. Finally, our framework exploits the\nuncertainty of the deep learning model during prediction as well as the\nannotators' estimated expertise to minimize the number of required annotations\nand annotators for optimally training the deep learning model.\n  We evaluate the effectiveness of our framework for intent classification in\nAlexa (Amazon's personal assistant), using both synthetic and real-world\ndatasets. Experiments show that our framework can accurately learn annotator\nexpertise, infer true labels, and effectively reduce the amount of annotations\nin model training as compared to state-of-the-art approaches. We further\ndiscuss the potential of our proposed framework in bridging machine learning\nand crowdsourcing towards improved human-in-the-loop systems. \n\n"}
{"id": "1803.04386", "contents": "Title: Flipout: Efficient Pseudo-Independent Weight Perturbations on\n  Mini-Batches Abstract: Stochastic neural net weights are used in a variety of contexts, including\nregularization, Bayesian neural nets, exploration in reinforcement learning,\nand evolution strategies. Unfortunately, due to the large number of weights,\nall the examples in a mini-batch typically share the same weight perturbation,\nthereby limiting the variance reduction effect of large mini-batches. We\nintroduce flipout, an efficient method for decorrelating the gradients within a\nmini-batch by implicitly sampling pseudo-independent weight perturbations for\neach example. Empirically, flipout achieves the ideal linear variance reduction\nfor fully connected networks, convolutional networks, and RNNs. We find\nsignificant speedups in training neural networks with multiplicative Gaussian\nperturbations. We show that flipout is effective at regularizing LSTMs, and\noutperforms previous methods. Flipout also enables us to vectorize evolution\nstrategies: in our experiments, a single GPU with flipout can handle the same\nthroughput as at least 40 CPU cores using existing methods, equivalent to a\nfactor-of-4 cost reduction on Amazon Web Services. \n\n"}
{"id": "1803.06084", "contents": "Title: A Kernel Theory of Modern Data Augmentation Abstract: Data augmentation, a technique in which a training set is expanded with\nclass-preserving transformations, is ubiquitous in modern machine learning\npipelines. In this paper, we seek to establish a theoretical framework for\nunderstanding data augmentation. We approach this from two directions: First,\nwe provide a general model of augmentation as a Markov process, and show that\nkernels appear naturally with respect to this model, even when we do not employ\nkernel classification. Next, we analyze more directly the effect of\naugmentation on kernel classifiers, showing that data augmentation can be\napproximated by first-order feature averaging and second-order variance\nregularization components. These frameworks both serve to illustrate the ways\nin which data augmentation affects the downstream learning model, and the\nresulting analyses provide novel connections between prior work in invariant\nkernels, tangent propagation, and robust optimization. Finally, we provide\nseveral proof-of-concept applications showing that our theory can be useful for\naccelerating machine learning workflows, such as reducing the amount of\ncomputation needed to train using augmented data, and predicting the utility of\na transformation prior to training. \n\n"}
{"id": "1803.06396", "contents": "Title: Reviving and Improving Recurrent Back-Propagation Abstract: In this paper, we revisit the recurrent back-propagation (RBP) algorithm,\ndiscuss the conditions under which it applies as well as how to satisfy them in\ndeep neural networks. We show that RBP can be unstable and propose two variants\nbased on conjugate gradient on the normal equations (CG-RBP) and Neumann series\n(Neumann-RBP). We further investigate the relationship between Neumann-RBP and\nback propagation through time (BPTT) and its truncated version (TBPTT). Our\nNeumann-RBP has the same time complexity as TBPTT but only requires constant\nmemory, whereas TBPTT's memory cost scales linearly with the number of\ntruncation steps. We examine all RBP variants along with BPTT and TBPTT in\nthree different application domains: associative memory with continuous\nHopfield networks, document classification in citation networks using graph\nneural networks and hyperparameter optimization for fully connected networks.\nAll experiments demonstrate that RBPs, especially the Neumann-RBP variant, are\nefficient and effective for optimizing convergent recurrent neural networks.\nCode is released at: \\url{https://github.com/lrjconan/RBP}. \n\n"}
{"id": "1803.06567", "contents": "Title: A Dual Approach to Scalable Verification of Deep Networks Abstract: This paper addresses the problem of formally verifying desirable properties\nof neural networks, i.e., obtaining provable guarantees that neural networks\nsatisfy specifications relating their inputs and outputs (robustness to bounded\nnorm adversarial perturbations, for example). Most previous work on this topic\nwas limited in its applicability by the size of the network, network\narchitecture and the complexity of properties to be verified. In contrast, our\nframework applies to a general class of activation functions and specifications\non neural network inputs and outputs. We formulate verification as an\noptimization problem (seeking to find the largest violation of the\nspecification) and solve a Lagrangian relaxation of the optimization problem to\nobtain an upper bound on the worst case violation of the specification being\nverified. Our approach is anytime i.e. it can be stopped at any time and a\nvalid bound on the maximum violation can be obtained. We develop specialized\nverification algorithms with provable tightness guarantees under special\nassumptions and demonstrate the practical significance of our general\nverification approach on a variety of verification tasks. \n\n"}
{"id": "1803.06604", "contents": "Title: A Robust AUC Maximization Framework with Simultaneous Outlier Detection\n  and Feature Selection for Positive-Unlabeled Classification Abstract: The positive-unlabeled (PU) classification is a common scenario in real-world\napplications such as healthcare, text classification, and bioinformatics, in\nwhich we only observe a few samples labeled as \"positive\" together with a large\nvolume of \"unlabeled\" samples that may contain both positive and negative\nsamples. Building robust classifier for the PU problem is very challenging,\nespecially for complex data where the negative samples overwhelm and mislabeled\nsamples or corrupted features exist. To address these three issues, we propose\na robust learning framework that unifies AUC maximization (a robust metric for\nbiased labels), outlier detection (for excluding wrong labels), and feature\nselection (for excluding corrupted features). The generalization error bounds\nare provided for the proposed model that give valuable insight into the\ntheoretical performance of the method and lead to useful practical guidance,\ne.g., to train a model, we find that the included unlabeled samples are\nsufficient as long as the sample size is comparable to the number of positive\nsamples in the training process. Empirical comparisons and two real-world\napplications on surgical site infection (SSI) and EEG seizure detection are\nalso conducted to show the effectiveness of the proposed model. \n\n"}
{"id": "1803.06905", "contents": "Title: TBD: Benchmarking and Analyzing Deep Neural Network Training Abstract: The recent popularity of deep neural networks (DNNs) has generated a lot of\nresearch interest in performing DNN-related computation efficiently. However,\nthe primary focus is usually very narrow and limited to (i) inference -- i.e.\nhow to efficiently execute already trained models and (ii) image classification\nnetworks as the primary benchmark for evaluation.\n  Our primary goal in this work is to break this myopic view by (i) proposing a\nnew benchmark for DNN training, called TBD (TBD is short for Training Benchmark\nfor DNNs), that uses a representative set of DNN models that cover a wide range\nof machine learning applications: image classification, machine translation,\nspeech recognition, object detection, adversarial networks, reinforcement\nlearning, and (ii) by performing an extensive performance analysis of training\nthese different applications on three major deep learning frameworks\n(TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU,\nmulti-GPU, and multi-machine). TBD currently covers six major application\ndomains and eight different state-of-the-art models.\n  We present a new toolchain for performance analysis for these models that\ncombines the targeted usage of existing performance analysis tools, careful\nselection of new and existing metrics and methodologies to analyze the results,\nand utilization of domain specific characteristics of DNN training. We also\nbuild a new set of tools for memory profiling in all three major frameworks;\nmuch needed tools that can finally shed some light on precisely how much memory\nis consumed by different data structures (weights, activations, gradients,\nworkspace) in DNN training. By using our tools and methodologies, we make\nseveral important observations and recommendations on where the future research\nand optimization of DNN training should be focused. \n\n"}
{"id": "1803.07068", "contents": "Title: D$^2$: Decentralized Training over Decentralized Data Abstract: While training a machine learning model using multiple workers, each of which\ncollects data from their own data sources, it would be most useful when the\ndata collected from different workers can be {\\em unique} and {\\em different}.\nIronically, recent analysis of decentralized parallel stochastic gradient\ndescent (D-PSGD) relies on the assumption that the data hosted on different\nworkers are {\\em not too different}. In this paper, we ask the question: {\\em\nCan we design a decentralized parallel stochastic gradient descent algorithm\nthat is less sensitive to the data variance across workers?} In this paper, we\npresent D$^2$, a novel decentralized parallel stochastic gradient descent\nalgorithm designed for large data variance \\xr{among workers} (imprecisely,\n\"decentralized\" data). The core of D$^2$ is a variance blackuction extension of\nthe standard D-PSGD algorithm, which improves the convergence rate from\n$O\\left({\\sigma \\over \\sqrt{nT}} + {(n\\zeta^2)^{\\frac{1}{3}} \\over\nT^{2/3}}\\right)$ to $O\\left({\\sigma \\over \\sqrt{nT}}\\right)$ where $\\zeta^{2}$\ndenotes the variance among data on different workers. As a result, D$^2$ is\nrobust to data variance among workers. We empirically evaluated D$^2$ on image\nclassification tasks where each worker has access to only the data of a limited\nset of labels, and find that D$^2$ significantly outperforms D-PSGD. \n\n"}
{"id": "1803.07612", "contents": "Title: Generating Multi-Agent Trajectories using Programmatic Weak Supervision Abstract: We study the problem of training sequential generative models for capturing\ncoordinated multi-agent trajectory behavior, such as offensive basketball\ngameplay. When modeling such settings, it is often beneficial to design\nhierarchical models that can capture long-term coordination using intermediate\nvariables. Furthermore, these intermediate variables should capture interesting\nhigh-level behavioral semantics in an interpretable and manipulatable way. We\npresent a hierarchical framework that can effectively learn such sequential\ngenerative models. Our approach is inspired by recent work on leveraging\nprogrammatically produced weak labels, which we extend to the spatiotemporal\nregime. In addition to synthetic settings, we show how to instantiate our\nframework to effectively model complex interactions between basketball players\nand generate realistic multi-agent trajectories of basketball gameplay over\nlong time periods. We validate our approach using both quantitative and\nqualitative evaluations, including a user study comparison conducted with\nprofessional sports analysts. \n\n"}
{"id": "1803.08182", "contents": "Title: Enforcing constraints for interpolation and extrapolation in Generative\n  Adversarial Networks Abstract: We suggest ways to enforce given constraints in the output of a Generative\nAdversarial Network (GAN) generator both for interpolation and extrapolation\n(prediction). For the case of dynamical systems, given a time series, we wish\nto train GAN generators that can be used to predict trajectories starting from\na given initial condition. In this setting, the constraints can be in algebraic\nand/or differential form. Even though we are predominantly interested in the\ncase of extrapolation, we will see that the tasks of interpolation and\nextrapolation are related. However, they need to be treated differently.\n  For the case of interpolation, the incorporation of constraints is built into\nthe training of the GAN. The incorporation of the constraints respects the\nprimary game-theoretic setup of a GAN so it can be combined with existing\nalgorithms. However, it can exacerbate the problem of instability during\ntraining that is well-known for GANs. We suggest adding small noise to the\nconstraints as a simple remedy that has performed well in our numerical\nexperiments.\n  The case of extrapolation (prediction) is more involved. During training, the\nGAN generator learns to interpolate a noisy version of the data and we enforce\nthe constraints. This approach has connections with model reduction that we can\nutilize to improve the efficiency and accuracy of the training. Depending on\nthe form of the constraints, we may enforce them also during prediction through\na projection step. We provide examples of linear and nonlinear systems of\ndifferential equations to illustrate the various constructions. \n\n"}
{"id": "1803.08591", "contents": "Title: End-to-End Learning for the Deep Multivariate Probit Model Abstract: The multivariate probit model (MVP) is a popular classic model for studying\nbinary responses of multiple entities. Nevertheless, the computational\nchallenge of learning the MVP model, given that its likelihood involves\nintegrating over a multidimensional constrained space of latent variables,\nsignificantly limits its application in practice. We propose a flexible deep\ngeneralization of the classic MVP, the Deep Multivariate Probit Model (DMVP),\nwhich is an end-to-end learning scheme that uses an efficient parallel sampling\nprocess of the multivariate probit model to exploit GPU-boosted deep neural\nnetworks. We present both theoretical and empirical analysis of the convergence\nbehavior of DMVP's sampling process with respect to the resolution of the\ncorrelation structure. We provide convergence guarantees for DMVP and our\nempirical analysis demonstrates the advantages of DMVP's sampling compared with\nstandard MCMC-based methods. We also show that when applied to multi-entity\nmodelling problems, which are natural DMVP applications, DMVP trains faster\nthan classical MVP, by at least an order of magnitude, captures rich\ncorrelations among entities, and further improves the joint likelihood of\nentities compared with several competitive models. \n\n"}
{"id": "1803.08661", "contents": "Title: Bayesian Optimization with Expensive Integrands Abstract: We propose a Bayesian optimization algorithm for objective functions that are\nsums or integrals of expensive-to-evaluate functions, allowing noisy\nevaluations. These objective functions arise in multi-task Bayesian\noptimization for tuning machine learning hyperparameters, optimization via\nsimulation, and sequential design of experiments with random environmental\nconditions. Our method is average-case optimal by construction when a single\nevaluation of the integrand remains within our evaluation budget. Achieving\nthis one-step optimality requires solving a challenging value of information\noptimization problem, for which we provide a novel efficient\ndiscretization-free computational method. We also provide consistency proofs\nfor our method in both continuum and discrete finite domains for objective\nfunctions that are sums. In numerical experiments comparing against previous\nstate-of-the-art methods, including those that also leverage sum or integral\nstructure, our method performs as well or better across a wide range of\nproblems and offers significant improvements when evaluations are noisy or the\nintegrand varies smoothly in the integrated variables. \n\n"}
{"id": "1803.09791", "contents": "Title: A Common Framework for Natural Gradient and Taylor based Optimisation\n  using Manifold Theory Abstract: This technical report constructs a theoretical framework to relate standard\nTaylor approximation based optimisation methods with Natural Gradient (NG), a\nmethod which is Fisher efficient with probabilistic models. Such a framework\nwill be shown to also provide mathematical justification to combine higher\norder methods with the method of NG. \n\n"}
{"id": "1803.10520", "contents": "Title: Quantum algorithms for training Gaussian Processes Abstract: Gaussian processes (GPs) are important models in supervised machine learning.\nTraining in Gaussian processes refers to selecting the covariance functions and\nthe associated parameters in order to improve the outcome of predictions, the\ncore of which amounts to evaluating the logarithm of the marginal likelihood\n(LML) of a given model. LML gives a concrete measure of the quality of\nprediction that a GP model is expected to achieve. The classical computation of\nLML typically carries a polynomial time overhead with respect to the input\nsize. We propose a quantum algorithm that computes the logarithm of the\ndeterminant of a Hermitian matrix, which runs in logarithmic time for sparse\nmatrices. This is applied in conjunction with a variant of the quantum linear\nsystem algorithm that allows for logarithmic time computation of the form\n$\\mathbf{y}^TA^{-1}\\mathbf{y}$, where $\\mathbf{y}$ is a dense vector and $A$ is\nthe covariance matrix. We hence show that quantum computing can be used to\nestimate the LML of a GP with exponentially improved efficiency under certain\nconditions. \n\n"}
{"id": "1803.10586", "contents": "Title: Stochastic Variational Inference with Gradient Linearization Abstract: Variational inference has experienced a recent surge in popularity owing to\nstochastic approaches, which have yielded practical tools for a wide range of\nmodel classes. A key benefit is that stochastic variational inference obviates\nthe tedious process of deriving analytical expressions for closed-form variable\nupdates. Instead, one simply needs to derive the gradient of the log-posterior,\nwhich is often much easier. Yet for certain model classes, the log-posterior\nitself is difficult to optimize using standard gradient techniques. One such\nexample are random field models, where optimization based on gradient\nlinearization has proven popular, since it speeds up convergence significantly\nand can avoid poor local optima. In this paper we propose stochastic\nvariational inference with gradient linearization (SVIGL). It is similarly\nconvenient as standard stochastic variational inference - all that is required\nis a local linearization of the energy gradient. Its benefit over stochastic\nvariational inference with conventional gradient methods is a clear improvement\nin convergence speed, while yielding comparable or even better variational\napproximations in terms of KL divergence. We demonstrate the benefits of SVIGL\nin three applications: Optical flow estimation, Poisson-Gaussian denoising, and\n3D surface reconstruction. \n\n"}
{"id": "1804.00709", "contents": "Title: Generative Adversarial Learning for Spectrum Sensing Abstract: A novel approach of training data augmentation and domain adaptation is\npresented to support machine learning applications for cognitive radio. Machine\nlearning provides effective tools to automate cognitive radio functionalities\nby reliably extracting and learning intrinsic spectrum dynamics. However, there\nare two important challenges to overcome, in order to fully utilize the machine\nlearning benefits with cognitive radios. First, machine learning requires\nsignificant amount of truthed data to capture complex channel and emitter\ncharacteristics, and train the underlying algorithm (e.g., a classifier).\nSecond, the training data that has been identified for one spectrum environment\ncannot be used for another one (e.g., after channel and emitter conditions\nchange). To address these challenges, a generative adversarial network (GAN)\nwith deep learning structures is used to 1)~generate additional synthetic\ntraining data to improve classifier accuracy, and 2) adapt training data to\nspectrum dynamics. This approach is applied to spectrum sensing by assuming\nonly limited training data without knowledge of spectrum statistics. Machine\nlearning classifiers are trained with limited, augmented and adapted training\ndata to detect signals. Results show that training data augmentation increases\nthe classifier accuracy significantly and this increase is sustained with\ndomain adaptation as spectrum conditions change. \n\n"}
{"id": "1804.00714", "contents": "Title: Predicting Electric Vehicle Charging Station Usage: Using Machine\n  Learning to Estimate Individual Station Statistics from Physical\n  Configurations of Charging Station Networks Abstract: Electric vehicles (EVs) have been gaining popularity due to their\nenvironmental friendliness and efficiency. EV charging station networks are\nscalable solutions for supporting increasing numbers of EVs within modern\nelectric grid constraints, yet few tools exist to aid the physical\nconfiguration design of new networks. We use neural networks to predict\nindividual charging station usage statistics from the station's physical\nlocation within a network. We have shown this quickly gives accurate estimates\nof average usage statistics given a proposed configuration, without the need\nfor running many computationally expensive simulations. The trained neural\nnetwork can help EV charging network designers rapidly test various placements\nof charging stations under additional individual constraints in order to find\nan optimal configuration given their design objectives. \n\n"}
{"id": "1804.01491", "contents": "Title: Online Multi-Label Classification: A Label Compression Method Abstract: Many modern applications deal with multi-label data, such as functional\ncategorizations of genes, image labeling and text categorization.\nClassification of such data with a large number of labels and latent\ndependencies among them is a challenging task, and it becomes even more\nchallenging when the data is received online and in chunks. Many of the current\nmulti-label classification methods require a lot of time and memory, which make\nthem infeasible for practical real-world applications. In this paper, we\npropose a fast linear label space dimension reduction method that transforms\nthe labels into a reduced encoded space and trains models on the obtained\npseudo labels. Additionally, it provides an analytical method to update the\ndecoding matrix which maps the labels into the original space and is used\nduring the test phase. Experimental results show the effectiveness of this\napproach in terms of running times and the prediction performance over\ndifferent measures. \n\n"}
{"id": "1804.02484", "contents": "Title: Approximating Hamiltonian dynamics with the Nystr\\\"om method Abstract: Simulating the time-evolution of quantum mechanical systems is BQP-hard and\nexpected to be one of the foremost applications of quantum computers. We\nconsider classical algorithms for the approximation of Hamiltonian dynamics\nusing subsampling methods from randomized numerical linear algebra. We derive a\nsimulation technique whose runtime scales polynomially in the number of qubits\nand the Frobenius norm of the Hamiltonian. As an immediate application, we show\nthat sample based quantum simulation, a type of evolution where the Hamiltonian\nis a density matrix, can be efficiently classically simulated under specific\nstructural conditions. Our main technical contribution is a randomized\nalgorithm for approximating Hermitian matrix exponentials. The proof leverages\na low-rank, symmetric approximation via the Nystr\\\"om method. Our results\nsuggest that under strong sampling assumptions there exist classical\npoly-logarithmic time simulations of quantum computations. \n\n"}
{"id": "1804.02668", "contents": "Title: Accelerating Prototype-Based Drug Discovery using Conditional Diversity\n  Networks Abstract: Designing a new drug is a lengthy and expensive process. As the space of\npotential molecules is very large (10^23-10^60), a common technique during drug\ndiscovery is to start from a molecule which already has some of the desired\nproperties. An interdisciplinary team of scientists generates hypothesis about\nthe required changes to the prototype. In this work, we develop an algorithmic\nunsupervised-approach that automatically generates potential drug molecules\ngiven a prototype drug. We show that the molecules generated by the system are\nvalid molecules and significantly different from the prototype drug. Out of the\ncompounds generated by the system, we identified 35 FDA-approved drugs. As an\nexample, our system generated Isoniazid - one of the main drugs for\nTuberculosis. The system is currently being deployed for use in collaboration\nwith pharmaceutical companies to further analyze the additional generated\nmolecules. \n\n"}
{"id": "1804.03346", "contents": "Title: Learning Latent Events from Network Message Logs Abstract: We consider the problem of separating error messages generated in large\ndistributed data center networks into error events. In such networks, each\nerror event leads to a stream of messages generated by hardware and software\ncomponents affected by the event. These messages are stored in a giant message\nlog. We consider the unsupervised learning problem of identifying the\nsignatures of events that generated these messages; here, the signature of an\nerror event refers to the mixture of messages generated by the event. One of\nthe main contributions of the paper is a novel mapping of our problem which\ntransforms it into a problem of topic discovery in documents. Events in our\nproblem correspond to topics and messages in our problem correspond to words in\nthe topic discovery problem. However, there is no direct analog of documents.\nTherefore, we use a non-parametric change-point detection algorithm, which has\nlinear computational complexity in the number of messages, to divide the\nmessage log into smaller subsets called episodes, which serve as the\nequivalents of documents. After this mapping has been done, we use a well-known\nalgorithm for topic discovery, called LDA, to solve our problem. We\ntheoretically analyze the change-point detection algorithm, and show that it is\nconsistent and has low sample complexity. We also demonstrate the scalability\nof our algorithm on a real data set consisting of $97$ million messages\ncollected over a period of $15$ days, from a distributed data center network\nwhich supports the operations of a large wireless service provider. \n\n"}
{"id": "1804.03758", "contents": "Title: Universal Successor Representations for Transfer Reinforcement Learning Abstract: The objective of transfer reinforcement learning is to generalize from a set\nof previous tasks to unseen new tasks. In this work, we focus on the transfer\nscenario where the dynamics among tasks are the same, but their goals differ.\nAlthough general value function (Sutton et al., 2011) has been shown to be\nuseful for knowledge transfer, learning a universal value function can be\nchallenging in practice. To attack this, we propose (1) to use universal\nsuccessor representations (USR) to represent the transferable knowledge and (2)\na USR approximator (USRA) that can be trained by interacting with the\nenvironment. Our experiments show that USR can be effectively applied to new\ntasks, and the agent initialized by the trained USRA can achieve the goal\nconsiderably faster than random initialization. \n\n"}
{"id": "1804.04577", "contents": "Title: Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and\n  Some New Implementations Abstract: In this paper we discuss policy iteration methods for approximate solution of\na finite-state discounted Markov decision problem, with a focus on\nfeature-based aggregation methods and their connection with deep reinforcement\nlearning schemes. We introduce features of the states of the original problem,\nand we formulate a smaller \"aggregate\" Markov decision problem, whose states\nrelate to the features. We discuss properties and possible implementations of\nthis type of aggregation, including a new approach to approximate policy\niteration. In this approach the policy improvement operation combines\nfeature-based aggregation with feature construction using deep neural networks\nor other calculations. We argue that the cost function of a policy may be\napproximated much more accurately by the nonlinear function of the features\nprovided by aggregation, than by the linear function of the features provided\nby neural network-based reinforcement learning, thereby potentially leading to\nmore effective policy improvement. \n\n"}
{"id": "1804.04849", "contents": "Title: The unreasonable effectiveness of the forget gate Abstract: Given the success of the gated recurrent unit, a natural question is whether\nall the gates of the long short-term memory (LSTM) network are necessary.\nPrevious research has shown that the forget gate is one of the most important\ngates in the LSTM. Here we show that a forget-gate-only version of the LSTM\nwith chrono-initialized biases, not only provides computational savings but\noutperforms the standard LSTM on multiple benchmark datasets and competes with\nsome of the best contemporary models. Our proposed network, the JANET, achieves\naccuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the\nstandard LSTM which yields accuracies of 98.5% and 91%. \n\n"}
{"id": "1804.04976", "contents": "Title: Online Fall Detection using Recurrent Neural Networks Abstract: Unintentional falls can cause severe injuries and even death, especially if\nno immediate assistance is given. The aim of Fall Detection Systems (FDSs) is\nto detect an occurring fall. This information can be used to trigger the\nnecessary assistance in case of injury. This can be done by using either\nambient-based sensors, e.g. cameras, or wearable devices. The aim of this work\nis to study the technical aspects of FDSs based on wearable devices and\nartificial intelligence techniques, in particular Deep Learning (DL), to\nimplement an effective algorithm for on-line fall detection. The proposed\nclassifier is based on a Recurrent Neural Network (RNN) model with underlying\nLong Short-Term Memory (LSTM) blocks. The method is tested on the publicly\navailable SisFall dataset, with extended annotation, and compared with the\nresults obtained by the SisFall authors. \n\n"}
{"id": "1804.05816", "contents": "Title: Models for Capturing Temporal Smoothness in Evolving Networks for\n  Learning Latent Representation of Nodes Abstract: In a dynamic network, the neighborhood of the vertices evolve across\ndifferent temporal snapshots of the network. Accurate modeling of this temporal\nevolution can help solve complex tasks involving real-life social and\ninteraction networks. However, existing models for learning latent\nrepresentation are inadequate for obtaining the representation vectors of the\nvertices for different time-stamps of a dynamic network in a meaningful way. In\nthis paper, we propose latent representation learning models for dynamic\nnetworks which overcome the above limitation by considering two different kinds\nof temporal smoothness: (i) retrofitted, and (ii) linear transformation. The\nretrofitted model tracks the representation vector of a vertex over time,\nfacilitating vertex-based temporal analysis of a network. On the other hand,\nlinear transformation based model provides a smooth transition operator which\nmaps the representation vectors of all vertices from one temporal snapshot to\nthe next (unobserved) snapshot-this facilitates prediction of the state of a\nnetwork in a future time-stamp. We validate the performance of our proposed\nmodels by employing them for solving the temporal link prediction task.\nExperiments on 9 real-life networks from various domains validate that the\nproposed models are significantly better than the existing models for\npredicting the dynamics of an evolving network. \n\n"}
{"id": "1804.06218", "contents": "Title: Hierarchical correlation reconstruction with missing data, for example\n  for biology-inspired neuron Abstract: Machine learning often needs to model density from a multidimensional data\nsample, including correlations between coordinates. Additionally, we often have\nmissing data case: that data points can miss values for some of coordinates.\nThis article adapts rapid parametric density estimation approach for this\npurpose: modelling density as a linear combination of orthonormal functions,\nfor which $L^2$ optimization says that (independently) estimated coefficient\nfor a given function is just average over the sample of value of this function.\nHierarchical correlation reconstruction first models probability density for\neach separate coordinate using all its appearances in data sample, then adds\ncorrections from independently modelled pairwise correlations using all samples\nhaving both coordinates, and so on independently adding correlations for\ngrowing numbers of variables using often decreasing evidence in data sample. A\nbasic application of such modelled multidimensional density can be imputation\nof missing coordinates: by inserting known coordinates to the density, and\ntaking expected values for the missing coordinates, or even their entire joint\nprobability distribution. Presented method can be compared with cascade\ncorrelations approach, offering several advantages in flexibility and accuracy.\nIt can be also used as artificial neuron: maximizing prediction capabilities\nfor only local behavior - modelling and predicting local connections. \n\n"}
{"id": "1804.06378", "contents": "Title: Graph-based Selective Outlier Ensembles Abstract: An ensemble technique is characterized by the mechanism that generates the\ncomponents and by the mechanism that combines them. A common way to achieve the\nconsensus is to enable each component to equally participate in the aggregation\nprocess. A problem with this approach is that poor components are likely to\nnegatively affect the quality of the consensus result. To address this issue,\nalternatives have been explored in the literature to build selective classifier\nand cluster ensembles, where only a subset of the components contributes to the\ncomputation of the consensus. Of the family of ensemble methods, outlier\nensembles are the least studied. Only recently, the selection problem for\noutlier ensembles has been discussed. In this work we define a new graph-based\nclass of ranking selection methods. A method in this class is characterized by\ntwo main steps: (1) Mapping the rankings onto a graph structure; and (2) Mining\nthe resulting graph to identify a subset of rankings. We define a specific\ninstance of the graph-based ranking selection class. Specifically, we map the\nproblem of selecting ensemble components onto a mining problem in a graph. An\nextensive evaluation was conducted on a variety of heterogeneous data and\nmethods. Our empirical results show that our approach outperforms\nstate-of-the-art selective outlier ensemble techniques. \n\n"}
{"id": "1804.06546", "contents": "Title: Deep Generative Networks For Sequence Prediction Abstract: This thesis investigates unsupervised time series representation learning for\nsequence prediction problems, i.e. generating nice-looking input samples given\na previous history, for high dimensional input sequences by decoupling the\nstatic input representation from the recurrent sequence representation. We\nintroduce three models based on Generative Stochastic Networks (GSN) for\nunsupervised sequence learning and prediction. Experimental results for these\nthree models are presented on pixels of sequential handwritten digit (MNIST)\ndata, videos of low-resolution bouncing balls, and motion capture data. The\nmain contribution of this thesis is to provide evidence that GSNs are a viable\nframework to learn useful representations of complex sequential input data, and\nto suggest a new framework for deep generative models to learn complex\nsequences by decoupling static input representations from dynamic time\ndependency representations. \n\n"}
{"id": "1804.06776", "contents": "Title: Improving Long-Horizon Forecasts with Expectation-Biased LSTM Networks Abstract: State-of-the-art forecasting methods using Recurrent Neural Net- works (RNN)\nbased on Long-Short Term Memory (LSTM) cells have shown exceptional performance\ntargeting short-horizon forecasts, e.g given a set of predictor features,\nforecast a target value for the next few time steps in the future. However, in\nmany applica- tions, the performance of these methods decays as the forecasting\nhorizon extends beyond these few time steps. This paper aims to explore the\nchallenges of long-horizon forecasting using LSTM networks. Here, we illustrate\nthe long-horizon forecasting problem in datasets from neuroscience and energy\nsupply management. We then propose expectation-biasing, an approach motivated\nby the literature of Dynamic Belief Networks, as a solution to improve\nlong-horizon forecasting using LSTMs. We propose two LSTM ar- chitectures along\nwith two methods for expectation biasing that significantly outperforms\nstandard practice. \n\n"}
{"id": "1804.06909", "contents": "Title: Modeling and Simultaneously Removing Bias via Adversarial Neural\n  Networks Abstract: In real world systems, the predictions of deployed Machine Learned models\naffect the training data available to build subsequent models. This introduces\na bias in the training data that needs to be addressed. Existing solutions to\nthis problem attempt to resolve the problem by either casting this in the\nreinforcement learning framework or by quantifying the bias and re-weighting\nthe loss functions. In this work, we develop a novel Adversarial Neural Network\n(ANN) model, an alternative approach which creates a representation of the data\nthat is invariant to the bias. We take the Paid Search auction as our working\nexample and ad display position features as the confounding features for this\nsetting. We show the success of this approach empirically on both synthetic\ndata as well as real world paid search auction data from a major search engine. \n\n"}
{"id": "1804.07870", "contents": "Title: Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation\n  Size Abstract: A key problem in research on adversarial examples is that vulnerability to\nadversarial examples is usually measured by running attack algorithms. Because\nthe attack algorithms are not optimal, the attack algorithms are prone to\noverestimating the size of perturbation needed to fool the target model. In\nother words, the attack-based methodology provides an upper-bound on the size\nof a perturbation that will fool the model, but security guarantees require a\nlower bound. CLEVER is a proposed scoring method to estimate a lower bound.\nUnfortunately, an estimate of a bound is not a bound. In this report, we show\nthat gradient masking, a common problem that causes attack methodologies to\nprovide only a very loose upper bound, causes CLEVER to overestimate the size\nof perturbation needed to fool the model. In other words, CLEVER does not\nresolve the key problem with the attack-based methodology, because it fails to\nprovide a lower bound. \n\n"}
{"id": "1804.07944", "contents": "Title: Variational Inference In Pachinko Allocation Machines Abstract: The Pachinko Allocation Machine (PAM) is a deep topic model that allows\nrepresenting rich correlation structures among topics by a directed acyclic\ngraph over topics. Because of the flexibility of the model, however,\napproximate inference is very difficult. Perhaps for this reason, only a small\nnumber of potential PAM architectures have been explored in the literature. In\nthis paper we present an efficient and flexible amortized variational inference\nmethod for PAM, using a deep inference network to parameterize the approximate\nposterior distribution in a manner similar to the variational autoencoder. Our\ninference method produces more coherent topics than state-of-art inference\nmethods for PAM while being an order of magnitude faster, which allows\nexploration of a wider range of PAM architectures than have previously been\nstudied. \n\n"}
{"id": "1804.08450", "contents": "Title: Decorrelated Batch Normalization Abstract: Batch Normalization (BN) is capable of accelerating the training of deep\nmodels by centering and scaling activations within mini-batches. In this work,\nwe propose Decorrelated Batch Normalization (DBN), which not just centers and\nscales activations but whitens them. We explore multiple whitening techniques,\nand find that PCA whitening causes a problem we call stochastic axis swapping,\nwhich is detrimental to learning. We show that ZCA whitening does not suffer\nfrom this problem, permitting successful learning. DBN retains the desirable\nqualities of BN and further improves BN's optimization efficiency and\ngeneralization ability. We design comprehensive experiments to show that DBN\ncan improve the performance of BN on multilayer perceptrons and convolutional\nneural networks. Furthermore, we consistently improve the accuracy of residual\nnetworks on CIFAR-10, CIFAR-100, and ImageNet. \n\n"}
{"id": "1804.08613", "contents": "Title: Parameter Transfer Unit for Deep Neural Networks Abstract: Parameters in deep neural networks which are trained on large-scale databases\ncan generalize across multiple domains, which is referred as \"transferability\".\nUnfortunately, the transferability is usually defined as discrete states and it\ndiffers with domains and network architectures. Existing works usually\nheuristically apply parameter-sharing or fine-tuning, and there is no\nprincipled approach to learn a parameter transfer strategy. To address the gap,\na parameter transfer unit (PTU) is proposed in this paper. The PTU learns a\nfine-grained nonlinear combination of activations from both the source and the\ntarget domain networks, and subsumes hand-crafted discrete transfer states. In\nthe PTU, the transferability is controlled by two gates which are artificial\nneurons and can be learned from data. The PTU is a general and flexible module\nwhich can be used in both CNNs and RNNs. Experiments are conducted with various\nnetwork architectures and multiple transfer domain pairs. Results demonstrate\nthe effectiveness of the PTU as it outperforms heuristic parameter-sharing and\nfine-tuning in most settings. \n\n"}
{"id": "1804.08774", "contents": "Title: Neural-Brane: Neural Bayesian Personalized Ranking for Attributed\n  Network Embedding Abstract: Network embedding methodologies, which learn a distributed vector\nrepresentation for each vertex in a network, have attracted considerable\ninterest in recent years. Existing works have demonstrated that vertex\nrepresentation learned through an embedding method provides superior\nperformance in many real-world applications, such as node classification, link\nprediction, and community detection. However, most of the existing methods for\nnetwork embedding only utilize topological information of a vertex, ignoring a\nrich set of nodal attributes (such as, user profiles of an online social\nnetwork, or textual contents of a citation network), which is abundant in all\nreal-life networks. A joint network embedding that takes into account both\nattributional and relational information entails a complete network information\nand could further enrich the learned vector representations. In this work, we\npresent Neural-Brane, a novel Neural Bayesian Personalized Ranking based\nAttributed Network Embedding. For a given network, Neural-Brane extracts latent\nfeature representation of its vertices using a designed neural network model\nthat unifies network topological information and nodal attributes; Besides, it\nutilizes Bayesian personalized ranking objective, which exploits the proximity\nordering between a similar node-pair and a dissimilar node-pair. We evaluate\nthe quality of vertex embedding produced by Neural-Brane by solving the node\nclassification and clustering tasks on four real-world datasets. Experimental\nresults demonstrate the superiority of our proposed method over the\nstate-of-the-art existing methods. \n\n"}
{"id": "1804.09530", "contents": "Title: Strong Baselines for Neural Semi-supervised Learning under Domain Shift Abstract: Novel neural models have been proposed in recent years for learning under\ndomain shift. Most models, however, only evaluate on a single task, on\nproprietary datasets, or compare to weak baselines, which makes comparison of\nmodels difficult. In this paper, we re-evaluate classic general-purpose\nbootstrapping approaches in the context of neural networks under domain shifts\nvs. recent neural approaches and propose a novel multi-task tri-training method\nthat reduces the time and space complexity of classic tri-training. Extensive\nexperiments on two benchmarks are negative: while our novel method establishes\na new state-of-the-art for sentiment analysis, it does not fare consistently\nthe best. More importantly, we arrive at the somewhat surprising conclusion\nthat classic tri-training, with some additions, outperforms the state of the\nart. We conclude that classic approaches constitute an important and strong\nbaseline. \n\n"}
{"id": "1804.10745", "contents": "Title: Generalizing Across Domains via Cross-Gradient Training Abstract: We present CROSSGRAD, a method to use multi-domain training data to learn a\nclassifier that generalizes to new domains. CROSSGRAD does not need an\nadaptation phase via labeled or unlabeled data, or domain features in the new\ndomain. Most existing domain adaptation methods attempt to erase domain signals\nusing techniques like domain adversarial training. In contrast, CROSSGRAD is\nfree to use domain signals for predicting labels, if it can prevent overfitting\non training domains. We conceptualize the task in a Bayesian setting, in which\na sampling step is implemented as data augmentation, based on domain-guided\nperturbations of input instances. CROSSGRAD parallelly trains a label and a\ndomain classifier on examples perturbed by loss gradients of each other's\nobjectives. This enables us to directly perturb inputs, without separating and\nre-mixing domain signals while making various distributional assumptions.\nEmpirical evaluation on three different applications where this setting is\nnatural establishes that (1) domain-guided perturbation provides consistently\nbetter generalization to unseen domains, compared to generic instance\nperturbation methods, and that (2) data augmentation is a more stable and\naccurate method than domain adversarial training. \n\n"}
{"id": "1804.10834", "contents": "Title: A Unified Framework for Domain Adaptation using Metric Learning on\n  Manifolds Abstract: We present a novel framework for domain adaptation, whereby both geometric\nand statistical differences between a labeled source domain and unlabeled\ntarget domain can be integrated by exploiting the curved Riemannian geometry of\nstatistical manifolds. Our approach is based on formulating transfer from\nsource to target as a problem of geometric mean metric learning on manifolds.\nSpecifically, we exploit the curved Riemannian manifold geometry of symmetric\npositive definite (SPD) covariance matrices. We exploit a simple but important\nobservation that as the space of covariance matrices is both a Riemannian space\nas well as a homogeneous space, the shortest path geodesic between two\ncovariances on the manifold can be computed analytically. Statistics on the SPD\nmatrix manifold, such as the geometric mean of two matrices can be reduced to\nsolving the well-known Riccati equation. We show how the Ricatti-based solution\ncan be constrained to not only reduce the statistical differences between the\nsource and target domains, such as aligning second order covariances and\nminimizing the maximum mean discrepancy, but also the underlying geometry of\nthe source and target domains using diffusions on the underlying source and\ntarget manifolds. A key strength of our proposed approach is that it enables\nintegrating multiple sources of variation between source and target in a\nunified way, by reducing the combined objective function to a nested set of\nRicatti equations where the solution can be represented by a cascaded series of\ngeometric mean computations. In addition to showing the theoretical optimality\nof our solution, we present detailed experiments using standard transfer\nlearning testbeds from computer vision comparing our proposed algorithms to\npast work in domain adaptation, showing improved results over a large variety\nof previous methods. \n\n"}
{"id": "1805.00327", "contents": "Title: A Taxonomy for Neural Memory Networks Abstract: In this paper, a taxonomy for memory networks is proposed based on their\nmemory organization. The taxonomy includes all the popular memory networks:\nvanilla recurrent neural network (RNN), long short term memory (LSTM ), neural\nstack and neural Turing machine and their variants. The taxonomy puts all these\nnetworks under a single umbrella and shows their relative expressive power ,\ni.e. vanilla RNN <=LSTM<=neural stack<=neural RAM. The differences and\ncommonality between these networks are analyzed. These differences are also\nconnected to the requirements of different tasks which can give the user\ninstructions of how to choose or design an appropriate memory network for a\nspecific task. As a conceptual simplified class of problems, four tasks of\nsynthetic symbol sequences: counting, counting with interference, reversing and\nrepeat counting are developed and tested to verify our arguments. And we use\ntwo natural language processing problems to discuss how this taxonomy helps\nchoosing the appropriate neural memory networks for real world problem. \n\n"}
{"id": "1805.00705", "contents": "Title: Investigating Audio, Visual, and Text Fusion Methods for End-to-End\n  Automatic Personality Prediction Abstract: We propose a tri-modal architecture to predict Big Five personality trait\nscores from video clips with different channels for audio, text, and video\ndata. For each channel, stacked Convolutional Neural Networks are employed. The\nchannels are fused both on decision-level and by concatenating their respective\nfully connected layers. It is shown that a multimodal fusion approach\noutperforms each single modality channel, with an improvement of 9.4\\% over the\nbest individual modality (video). Full backpropagation is also shown to be\nbetter than a linear combination of modalities, meaning complex interactions\nbetween modalities can be leveraged to build better models. Furthermore, we can\nsee the prediction relevance of each modality for each trait. The described\nmodel can be used to increase the emotional intelligence of virtual agents. \n\n"}
{"id": "1805.01891", "contents": "Title: Power Law in Sparsified Deep Neural Networks Abstract: The power law has been observed in the degree distributions of many\nbiological neural networks. Sparse deep neural networks, which learn an\neconomical representation from the data, resemble biological neural networks in\nmany ways. In this paper, we study if these artificial networks also exhibit\nproperties of the power law. Experimental results on two popular deep learning\nmodels, namely, multilayer perceptrons and convolutional neural networks, are\naffirmative. The power law is also naturally related to preferential\nattachment. To study the dynamical properties of deep networks in continual\nlearning, we propose an internal preferential attachment model to explain how\nthe network topology evolves. Experimental results show that with the arrival\nof a new task, the new connections made follow this preferential attachment\nprocess. \n\n"}
{"id": "1805.02136", "contents": "Title: Private Sequential Learning Abstract: We formulate a private learning model to study an intrinsic tradeoff between\nprivacy and query complexity in sequential learning. Our model involves a\nlearner who aims to determine a scalar value, $v^*$, by sequentially querying\nan external database and receiving binary responses. In the meantime, an\nadversary observes the learner's queries, though not the responses, and tries\nto infer from them the value of $v^*$. The objective of the learner is to\nobtain an accurate estimate of $v^*$ using only a small number of queries,\nwhile simultaneously protecting her privacy by making $v^*$ provably difficult\nto learn for the adversary. Our main results provide tight upper and lower\nbounds on the learner's query complexity as a function of desired levels of\nprivacy and estimation accuracy. We also construct explicit query strategies\nwhose complexity is optimal up to an additive constant. \n\n"}
{"id": "1805.02296", "contents": "Title: DIRECT: Deep Discriminative Embedding for Clustering of LIGO Data Abstract: In this paper, benefiting from the strong ability of deep neural network in\nestimating non-linear functions, we propose a discriminative embedding function\nto be used as a feature extractor for clustering tasks. The trained embedding\nfunction transfers knowledge from the domain of a labeled set of\nmorphologically-distinct images, known as classes, to a new domain within which\nnew classes can potentially be isolated and identified. Our target application\nin this paper is the Gravity Spy Project, which is an effort to characterize\ntransient, non-Gaussian noise present in data from the Advanced Laser\nInterferometer Gravitational-wave Observatory, or LIGO. Accumulating large,\nlabeled sets of noise features and identifying of new classes of noise lead to\na better understanding of their origin, which makes their removal from the data\nand/or detectors possible. \n\n"}
{"id": "1805.02396", "contents": "Title: Billion-scale Network Embedding with Iterative Random Projection Abstract: Network embedding, which learns low-dimensional vector representation for\nnodes in the network, has attracted considerable research attention recently.\nHowever, the existing methods are incapable of handling billion-scale networks,\nbecause they are computationally expensive and, at the same time, difficult to\nbe accelerated by distributed computing schemes. To address these problems, we\npropose RandNE (Iterative Random Projection Network Embedding), a novel and\nsimple billion-scale network embedding method. Specifically, we propose a\nGaussian random projection approach to map the network into a low-dimensional\nembedding space while preserving the high-order proximities between nodes. To\nreduce the time complexity, we design an iterative projection procedure to\navoid the explicit calculation of the high-order proximities. Theoretical\nanalysis shows that our method is extremely efficient, and friendly to\ndistributed computing schemes without any communication cost in the\ncalculation. We also design a dynamic updating procedure which can efficiently\nincorporate the dynamic changes of the networks without error aggregation.\nExtensive experimental results demonstrate the efficiency and efficacy of\nRandNE over state-of-the-art methods in several tasks including network\nreconstruction, link prediction and node classification on multiple datasets\nwith different scales, ranging from thousands to billions of nodes and edges. \n\n"}
{"id": "1805.02677", "contents": "Title: Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial\n  Convergence and SQ Lower Bounds Abstract: We study the complexity of training neural network models with one hidden\nnonlinear activation layer and an output weighted sum layer. We analyze\nGradient Descent applied to learning a bounded target function on $n$\nreal-valued inputs. We give an agnostic learning guarantee for GD: starting\nfrom a randomly initialized network, it converges in mean squared loss to the\nminimum error (in $2$-norm) of the best approximation of the target function\nusing a polynomial of degree at most $k$. Moreover, for any $k$, the size of\nthe network and number of iterations needed are both bounded by\n$n^{O(k)}\\log(1/\\epsilon)$. In particular, this applies to training networks of\nunbiased sigmoids and ReLUs. We also rigorously explain the empirical finding\nthat gradient descent discovers lower frequency Fourier components before\nhigher frequency components.\n  We complement this result with nearly matching lower bounds in the\nStatistical Query model. GD fits well in the SQ framework since each training\nstep is determined by an expectation over the input distribution. We show that\nany SQ algorithm that achieves significant improvement over a constant function\nwith queries of tolerance some inverse polynomial in the input dimensionality\n$n$ must use $n^{\\Omega(k)}$ queries even when the target functions are\nrestricted to a set of $n^{O(k)}$ degree-$k$ polynomials, and the input\ndistribution is uniform over the unit sphere; for this class the\ninformation-theoretic lower bound is only $\\Theta(k \\log n)$.\n  Our approach for both parts is based on spherical harmonics. We view gradient\ndescent as an operator on the space of functions, and study its dynamics. An\nessential tool is the Funk-Hecke theorem, which explains the eigenfunctions of\nthis operator in the case of the mean squared loss. \n\n"}
{"id": "1805.02830", "contents": "Title: Several Tunable GMM Kernels Abstract: While tree methods have been popular in practice, researchers and\npractitioners are also looking for simple algorithms which can reach similar\naccuracy of trees. In 2010, (Ping Li UAI'10) developed the method of\n\"abc-robust-logitboost\" and compared it with other supervised learning methods\non datasets used by the deep learning literature. In this study, we propose a\nseries of \"tunable GMM kernels\" which are simple and perform largely comparably\nto tree methods on the same datasets. Note that \"abc-robust-logitboost\"\nsubstantially improved the original \"GDBT\" in that (a) it developed a\ntree-split formula based on second-order information of the derivatives of the\nloss function; (b) it developed a new set of derivatives for multi-class\nclassification formulation.\n  In the prior study in 2017, the \"generalized min-max\" (GMM) kernel was shown\nto have good performance compared to the \"radial-basis function\" (RBF) kernel.\nHowever, as demonstrated in this paper, the original GMM kernel is often not as\ncompetitive as tree methods on the datasets used in the deep learning\nliterature. Since the original GMM kernel has no parameters, we propose tunable\nGMM kernels by adding tuning parameters in various ways. Three basic (i.e.,\nwith only one parameter) GMM kernels are the \"$e$GMM kernel\", \"$p$GMM kernel\",\nand \"$\\gamma$GMM kernel\", respectively. Extensive experiments show that they\nare able to produce good results for a large number of classification tasks.\nFurthermore, the basic kernels can be combined to boost the performance. \n\n"}
{"id": "1805.04276", "contents": "Title: Leveraging Grammar and Reinforcement Learning for Neural Program\n  Synthesis Abstract: Program synthesis is the task of automatically generating a program\nconsistent with a specification. Recent years have seen proposal of a number of\nneural approaches for program synthesis, many of which adopt a sequence\ngeneration paradigm similar to neural machine translation, in which\nsequence-to-sequence models are trained to maximize the likelihood of known\nreference programs. While achieving impressive results, this strategy has two\nkey limitations. First, it ignores Program Aliasing: the fact that many\ndifferent programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many\nsemantically correct programs, which can adversely affect the synthesizer\nperformance. Second, this strategy overlooks the fact that programs have a\nstrict syntax that can be efficiently checked. To address the first limitation,\nwe perform reinforcement learning on top of a supervised model with an\nobjective that explicitly maximizes the likelihood of generating semantically\ncorrect programs. For addressing the second limitation, we introduce a training\nprocedure that directly maximizes the probability of generating syntactically\ncorrect programs that fulfill the specification. We show that our contributions\nlead to improved accuracy of the models, especially in cases where the training\ndata is limited. \n\n"}
{"id": "1805.04933", "contents": "Title: Dyna: A Method of Momentum for Stochastic Optimization Abstract: An algorithm is presented for momentum gradient descent optimization based on\nthe first-order differential equation of the Newtonian dynamics. The fictitious\nmass is introduced to the dynamics of momentum for regularizing the adaptive\nstepsize of each individual parameter. The dynamic relaxation is adapted for\nstochastic optimization of nonlinear objective functions through an explicit\ntime integration with varying damping ratio. The adaptive stepsize is optimized\nfor each individual neural network layer based on the number of inputs. The\nadaptive stepsize for every parameter over the entire neural network is\nuniformly optimized with one upper bound, independent of sparsity, for better\noverall convergence rate. The numerical implementation of the algorithm is\nsimilar to the Adam Optimizer, possessing computational efficiency, similar\nmemory requirements, etc. There are three hyper-parameters in the algorithm\nwith clear physical interpretation. Preliminary trials show promise in\nperformance and convergence. \n\n"}
{"id": "1805.04958", "contents": "Title: Accelerating Message Passing for MAP with Benders Decomposition Abstract: We introduce a novel mechanism to tighten the local polytope relaxation for\nMAP inference in Markov random fields with low state space variables. We\nconsider a surjection of the variables to a set of hyper-variables and apply\nthe local polytope relaxation over these hyper-variables. The state space of\neach individual hyper-variable is constructed to be enumerable while the vector\nproduct of pairs is not easily enumerable making message passing inference\nintractable.\n  To circumvent the difficulty of enumerating the vector product of state\nspaces of hyper-variables we introduce a novel Benders decomposition approach.\nThis produces an upper envelope describing the message constructed from affine\nfunctions of the individual variables that compose the hyper-variable receiving\nthe message. The envelope is tight at the minimizers which are shared by the\ntrue message. Benders rows are constructed to be Pareto optimal and are\ngenerated using an efficient procedure targeted for binary problems. \n\n"}
{"id": "1805.05010", "contents": "Title: Detecting Adversarial Samples for Deep Neural Networks through Mutation\n  Testing Abstract: Recently, it has been shown that deep neural networks (DNN) are subject to\nattacks through adversarial samples. Adversarial samples are often crafted\nthrough adversarial perturbation, i.e., manipulating the original sample with\nminor modifications so that the DNN model labels the sample incorrectly. Given\nthat it is almost impossible to train perfect DNN, adversarial samples are\nshown to be easy to generate. As DNN are increasingly used in safety-critical\nsystems like autonomous cars, it is crucial to develop techniques for defending\nsuch attacks. Existing defense mechanisms which aim to make adversarial\nperturbation challenging have been shown to be ineffective. In this work, we\npropose an alternative approach. We first observe that adversarial samples are\nmuch more sensitive to perturbations than normal samples. That is, if we impose\nrandom perturbations on a normal and an adversarial sample respectively, there\nis a significant difference between the ratio of label change due to the\nperturbations. Observing this, we design a statistical adversary detection\nalgorithm called nMutant (inspired by mutation testing from software\nengineering community). Our experiments show that nMutant effectively detects\nmost of the adversarial samples generated by recently proposed attacking\nmethods. Furthermore, we provide an error bound with certain statistical\nsignificance along with the detection. \n\n"}
{"id": "1805.06530", "contents": "Title: Improving the Gaussian Mechanism for Differential Privacy: Analytical\n  Calibration and Optimal Denoising Abstract: The Gaussian mechanism is an essential building block used in multitude of\ndifferentially private data analysis algorithms. In this paper we revisit the\nGaussian mechanism and show that the original analysis has several important\nlimitations. Our analysis reveals that the variance formula for the original\nmechanism is far from tight in the high privacy regime ($\\varepsilon \\to 0$)\nand it cannot be extended to the low privacy regime ($\\varepsilon \\to \\infty$).\nWe address these limitations by developing an optimal Gaussian mechanism whose\nvariance is calibrated directly using the Gaussian cumulative density function\ninstead of a tail bound approximation. We also propose to equip the Gaussian\nmechanism with a post-processing step based on adaptive estimation techniques\nby leveraging that the distribution of the perturbation is known. Our\nexperiments show that analytical calibration removes at least a third of the\nvariance of the noise compared to the classical Gaussian mechanism, and that\ndenoising dramatically improves the accuracy of the Gaussian mechanism in the\nhigh-dimensional regime. \n\n"}
{"id": "1805.06792", "contents": "Title: Faster Rates for Convex-Concave Games Abstract: We consider the use of no-regret algorithms to compute equilibria for\nparticular classes of convex-concave games. While standard regret bounds would\nlead to convergence rates on the order of $O(T^{-1/2})$, recent work\n\\citep{RS13,SALS15} has established $O(1/T)$ rates by taking advantage of a\nparticular class of optimistic prediction algorithms. In this work we go\nfurther, showing that for a particular class of games one achieves a $O(1/T^2)$\nrate, and we show how this applies to the Frank-Wolfe method and recovers a\nsimilar bound \\citep{D15}. We also show that such no-regret techniques can even\nachieve a linear rate, $O(\\exp(-T))$, for equilibrium computation under\nadditional curvature assumptions. \n\n"}
{"id": "1805.06862", "contents": "Title: Design Identification of Curve Patterns on Cultural Heritage Objects:\n  Combining Template Matching and CNN-based Re-Ranking Abstract: The surfaces of many cultural heritage objects were embellished with various\npatterns, especially curve patterns. In practice, most of the unearthed\ncultural heritage objects are highly fragmented, e.g., sherds of potteries or\nvessels, and each of them only shows a very small portion of the underlying\nfull design, with noise and deformations. The goal of this paper is to address\nthe challenging problem of automatically identifying the underlying full design\nof curve patterns from such a sherd. Specifically, we formulate this problem as\ntemplate matching: curve structure segmented from the sherd is matched to each\nlocation with each possible orientation of each known full design. In this\npaper, we propose a new two-stage matching algorithm, with a different matching\ncost in each stage. In Stage 1, we use a traditional template matching, which\nis highly computationally efficient, over the whole search space and identify a\nsmall set of candidate matchings. In Stage 2, we derive a new matching cost by\ntraining a dual-source Convolutional Neural Network (CNN) and apply it to\nre-rank the candidate matchings identified in Stage 1. We collect 600 pottery\nsherds with 98 full designs from the Woodland Period in Southeastern North\nAmerica for experiments and the performance of the proposed algorithm is very\ncompetitive. \n\n"}
{"id": "1805.07324", "contents": "Title: GANE: A Generative Adversarial Network Embedding Abstract: Network embedding has become a hot research topic recently which can provide\nlow-dimensional feature representations for many machine learning applications.\nCurrent work focuses on either (1) whether the embedding is designed as an\nunsupervised learning task by explicitly preserving the structural connectivity\nin the network, or (2) whether the embedding is a by-product during the\nsupervised learning of a specific discriminative task in a deep neural network.\nIn this paper, we focus on bridging the gap of the two lines of the research.\nWe propose to adapt the Generative Adversarial model to perform network\nembedding, in which the generator is trying to generate vertex pairs, while the\ndiscriminator tries to distinguish the generated vertex pairs from real\nconnections (edges) in the network. Wasserstein-1 distance is adopted to train\nthe generator to gain better stability. We develop three variations of models,\nincluding GANE which applies cosine similarity, GANE-O1 which preserves the\nfirst-order proximity, and GANE-O2 which tries to preserves the second-order\nproximity of the network in the low-dimensional embedded vector space. We later\nprove that GANE-O2 has the same objective function as GANE-O1 when negative\nsampling is applied to simplify the training process in GANE-O2. Experiments\nwith real-world network datasets demonstrate that our models constantly\noutperform state-of-the-art solutions with significant improvements on\nprecision in link prediction, as well as on visualizations and accuracy in\nclustering tasks. \n\n"}
{"id": "1805.07557", "contents": "Title: Nostalgic Adam: Weighting more of the past gradients when designing the\n  adaptive learning rate Abstract: First-order optimization algorithms have been proven prominent in deep\nlearning. In particular, algorithms such as RMSProp and Adam are extremely\npopular. However, recent works have pointed out the lack of ``long-term memory\"\nin Adam-like algorithms, which could hamper their performance and lead to\ndivergence. In our study, we observe that there are benefits of weighting more\nof the past gradients when designing the adaptive learning rate. We therefore\npropose an algorithm called the Nostalgic Adam (NosAdam) with theoretically\nguaranteed convergence at the best known convergence rate. NosAdam can be\nregarded as a fix to the non-convergence issue of Adam in alternative to the\nrecent work of [Reddi et al., 2018]. Our preliminary numerical experiments show\nthat NosAdam is a promising alternative algorithm to Adam. The proofs, code and\nother supplementary materials can be found in an anonymously shared link. \n\n"}
{"id": "1805.07722", "contents": "Title: Task-Agnostic Meta-Learning for Few-shot Learning Abstract: Meta-learning approaches have been proposed to tackle the few-shot learning\nproblem.Typically, a meta-learner is trained on a variety of tasks in the hopes\nof being generalizable to new tasks. However, the generalizability on new tasks\nof a meta-learner could be fragile when it is over-trained on existing tasks\nduring meta-training phase. In other words, the initial model of a meta-learner\ncould be too biased towards existing tasks to adapt to new tasks, especially\nwhen only very few examples are available to update the model. To avoid a\nbiased meta-learner and improve its generalizability, we propose a novel\nparadigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we\npresent an entropy-based approach that meta-learns an unbiased initial model\nwith the largest uncertainty over the output labels by preventing it from\nover-performing in classification tasks. Alternatively, a more general\ninequality-minimization TAML is presented for more ubiquitous scenarios by\ndirectly minimizing the inequality of initial losses beyond the classification\ntasks wherever a suitable loss can be defined.Experiments on benchmarked\ndatasets demonstrate that the proposed approaches outperform compared\nmeta-learning algorithms in both few-shot classification and reinforcement\nlearning tasks. \n\n"}
{"id": "1805.07914", "contents": "Title: Imitating Latent Policies from Observation Abstract: In this paper, we describe a novel approach to imitation learning that infers\nlatent policies directly from state observations. We introduce a method that\ncharacterizes the causal effects of latent actions on observations while\nsimultaneously predicting their likelihood. We then outline an action alignment\nprocedure that leverages a small amount of environment interactions to\ndetermine a mapping between the latent and real-world actions. We show that\nthis corrected labeling can be used for imitating the observed behavior, even\nthough no expert actions are given. We evaluate our approach within classic\ncontrol environments and a platform game and demonstrate that it performs\nbetter than standard approaches. Code for this work is available at\nhttps://github.com/ashedwards/ILPO. \n\n"}
{"id": "1805.08052", "contents": "Title: Online Learning in Kernelized Markov Decision Processes Abstract: We consider online learning for minimizing regret in unknown, episodic Markov\ndecision processes (MDPs) with continuous states and actions. We develop\nvariants of the UCRL and posterior sampling algorithms that employ\nnonparametric Gaussian process priors to generalize across the state and action\nspaces. When the transition and reward functions of the true MDP are members of\nthe associated Reproducing Kernel Hilbert Spaces of functions induced by\nsymmetric psd kernels (frequentist setting), we show that the algorithms enjoy\nsublinear regret bounds. The bounds are in terms of explicit structural\nparameters of the kernels, namely a novel generalization of the information\ngain metric from kernelized bandit, and highlight the influence of transition\nand reward function structure on the learning performance. Our results are\napplicable to multidimensional state and action spaces with composite kernel\nstructures, and generalize results from the literature on kernelized bandits,\nand the adaptive control of parametric linear dynamical systems with quadratic\ncosts. \n\n"}
{"id": "1805.08096", "contents": "Title: Understanding Self-Paced Learning under Concave Conjugacy Theory Abstract: By simulating the easy-to-hard learning manners of humans/animals, the\nlearning regimes called curriculum learning~(CL) and self-paced learning~(SPL)\nhave been recently investigated and invoked broad interests. However, the\nintrinsic mechanism for analyzing why such learning regimes can work has not\nbeen comprehensively investigated. To this issue, this paper proposes a concave\nconjugacy theory for looking into the insight of CL/SPL. Specifically, by using\nthis theory, we prove the equivalence of the SPL regime and a latent concave\nobjective, which is closely related to the known non-convex regularized penalty\nwidely used in statistics and machine learning. Beyond the previous theory for\nexplaining CL/SPL insights, this new theoretical framework on one hand\nfacilitates two direct approaches for designing new SPL models for certain\ntasks, and on the other hand can help conduct the latent objective of\nself-paced curriculum learning, which is the advanced version of both CL/SPL\nand possess advantages of both learning regimes to a certain extent. This\nfurther facilitates a theoretical understanding for SPCL, instead of only\nCL/SPL as conventional. Under this theory, we attempt to attain intrinsic\nlatent objectives of two curriculum forms, the partial order and group\ncurriculums, which easily follow the theoretical understanding of the\ncorresponding SPCL regimes. \n\n"}
{"id": "1805.08206", "contents": "Title: Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers Abstract: We consider the problem of uncertainty estimation in the context of\n(non-Bayesian) deep neural classification. In this context, all known methods\nare based on extracting uncertainty signals from a trained network optimized to\nsolve the classification problem at hand. We demonstrate that such techniques\ntend to introduce biased estimates for instances whose predictions are supposed\nto be highly confident. We argue that this deficiency is an artifact of the\ndynamics of training with SGD-like optimizers, and it has some properties\nsimilar to overfitting. Based on this observation, we develop an uncertainty\nestimation algorithm that selectively estimates the uncertainty of highly\nconfident points, using earlier snapshots of the trained model, before their\nestimates are jittered (and way before they are ready for actual\nclassification). We present extensive experiments indicating that the proposed\nalgorithm provides uncertainty estimates that are consistently better than all\nknown methods. \n\n"}
{"id": "1805.08395", "contents": "Title: Learning to Optimize via Wasserstein Deep Inverse Optimal Control Abstract: We study the inverse optimal control problem in social sciences: we aim at\nlearning a user's true cost function from the observed temporal behavior. In\ncontrast to traditional phenomenological works that aim to learn a generative\nmodel to fit the behavioral data, we propose a novel variational principle and\ntreat user as a reinforcement learning algorithm, which acts by optimizing his\ncost function. We first propose a unified KL framework that generalizes\nexisting maximum entropy inverse optimal control methods. We further propose a\ntwo-step Wasserstein inverse optimal control framework. In the first step, we\ncompute the optimal measure with a novel mass transport equation. In the second\nstep, we formulate the learning problem as a generative adversarial network. In\ntwo real world experiments - recommender systems and social networks, we show\nthat our framework obtains significant performance gains over both existing\ninverse optimal control methods and point process based generative models. \n\n"}
{"id": "1805.08571", "contents": "Title: On Coresets for Logistic Regression Abstract: Coresets are one of the central methods to facilitate the analysis of large\ndata sets. We continue a recent line of research applying the theory of\ncoresets to logistic regression. First, we show a negative result, namely, that\nno strongly sublinear sized coresets exist for logistic regression. To deal\nwith intractable worst-case instances we introduce a complexity measure\n$\\mu(X)$, which quantifies the hardness of compressing a data set for logistic\nregression. $\\mu(X)$ has an intuitive statistical interpretation that may be of\nindependent interest. For data sets with bounded $\\mu(X)$-complexity, we show\nthat a novel sensitivity sampling scheme produces the first provably sublinear\n$(1\\pm\\varepsilon)$-coreset. We illustrate the performance of our method by\ncomparing to uniform sampling as well as to state of the art methods in the\narea. The experiments are conducted on real world benchmark data for logistic\nregression. \n\n"}
{"id": "1805.08975", "contents": "Title: Particle Filter Networks with Application to Visual Localization Abstract: Particle filtering is a powerful approach to sequential state estimation and\nfinds application in many domains, including robot localization, object\ntracking, etc. To apply particle filtering in practice, a critical challenge is\nto construct probabilistic system models, especially for systems with complex\ndynamics or rich sensory inputs such as camera images. This paper introduces\nthe Particle Filter Network (PFnet), which encodes both a system model and a\nparticle filter algorithm in a single neural network. The PF-net is fully\ndifferentiable and trained end-to-end from data. Instead of learning a generic\nsystem model, it learns a model optimized for the particle filter algorithm. We\napply the PF-net to a visual localization task, in which a robot must localize\nin a rich 3-D world, using only a schematic 2-D floor map. In simulation\nexperiments, PF-net consistently outperforms alternative learning\narchitectures, as well as a traditional model-based method, under a variety of\nsensor inputs. Further, PF-net generalizes well to new, unseen environments. \n\n"}
{"id": "1805.09360", "contents": "Title: Deep Reinforcement Learning of Marked Temporal Point Processes Abstract: In a wide variety of applications, humans interact with a complex environment\nby means of asynchronous stochastic discrete events in continuous time. Can we\ndesign online interventions that will help humans achieve certain goals in such\nasynchronous setting? In this paper, we address the above problem from the\nperspective of deep reinforcement learning of marked temporal point processes,\nwhere both the actions taken by an agent and the feedback it receives from the\nenvironment are asynchronous stochastic discrete events characterized using\nmarked temporal point processes. In doing so, we define the agent's policy\nusing the intensity and mark distribution of the corresponding process and then\nderive a flexible policy gradient method, which embeds the agent's actions and\nthe feedback it receives into real-valued vectors using deep recurrent neural\nnetworks. Our method does not make any assumptions on the functional form of\nthe intensity and mark distribution of the feedback and it allows for\narbitrarily complex reward functions. We apply our methodology to two different\napplications in personalized teaching and viral marketing and, using data\ngathered from Duolingo and Twitter, we show that it may be able to find\ninterventions to help learners and marketers achieve their goals more\neffectively than alternatives. \n\n"}
{"id": "1805.09461", "contents": "Title: Deep Reinforcement Learning For Sequence to Sequence Models Abstract: In recent times, sequence-to-sequence (seq2seq) models have gained a lot of\npopularity and provide state-of-the-art performance in a wide variety of tasks\nsuch as machine translation, headline generation, text summarization, speech to\ntext conversion, and image caption generation. The underlying framework for all\nthese models is usually a deep neural network comprising an encoder and a\ndecoder. Although simple encoder-decoder models produce competitive results,\nmany researchers have proposed additional improvements over these\nsequence-to-sequence models, e.g., using an attention-based model over the\ninput, pointer-generation models, and self-attention models. However, such\nseq2seq models suffer from two common problems: 1) exposure bias and 2)\ninconsistency between train/test measurement. Recently, a completely novel\npoint of view has emerged in addressing these two problems in seq2seq models,\nleveraging methods from reinforcement learning (RL). In this survey, we\nconsider seq2seq problems from the RL point of view and provide a formulation\ncombining the power of RL methods in decision-making with sequence-to-sequence\nmodels that enable remembering long-term memories. We present some of the most\nrecent frameworks that combine concepts from RL and deep neural networks and\nexplain how these two areas could benefit from each other in solving complex\nseq2seq tasks. Our work aims to provide insights into some of the problems that\ninherently arise with current approaches and how we can address them with\nbetter RL models. We also provide the source code for implementing most of the\nRL models discussed in this paper to support the complex task of abstractive\ntext summarization. \n\n"}
{"id": "1805.09547", "contents": "Title: Interpretable and Compositional Relation Learning by Joint Training with\n  an Autoencoder Abstract: Embedding models for entities and relations are extremely useful for\nrecovering missing facts in a knowledge base. Intuitively, a relation can be\nmodeled by a matrix mapping entity vectors. However, relations reside on low\ndimension sub-manifolds in the parameter space of arbitrary matrices---for one\nreason, composition of two relations $\\boldsymbol{M}_1,\\boldsymbol{M}_2$ may\nmatch a third $\\boldsymbol{M}_3$ (e.g. composition of relations\ncurrency_of_country and country_of_film usually matches\ncurrency_of_film_budget), which imposes compositional constraints to be\nsatisfied by the parameters (i.e. $\\boldsymbol{M}_1\\cdot\n\\boldsymbol{M}_2\\approx \\boldsymbol{M}_3$). In this paper we investigate a\ndimension reduction technique by training relations jointly with an\nautoencoder, which is expected to better capture compositional constraints. We\nachieve state-of-the-art on Knowledge Base Completion tasks with strongly\nimproved Mean Rank, and show that joint training with an autoencoder leads to\ninterpretable sparse codings of relations, helps discovering compositional\nconstraints and benefits from compositional training. Our source code is\nreleased at github.com/tianran/glimvec. \n\n"}
{"id": "1805.09898", "contents": "Title: Performing Co-Membership Attacks Against Deep Generative Models Abstract: In this paper we propose a new membership attack method called co-membership\nattacks against deep generative models including Variational Autoencoders\n(VAEs) and Generative Adversarial Networks (GANs). Specifically, membership\nattack aims to check whether a given instance x was used in the training data\nor not. A co-membership attack checks whether the given bundle of n instances\nwere in the training, with the prior knowledge that the bundle was either\nentirely used in the training or none at all. Successful membership attacks can\ncompromise the privacy of training data when the generative model is published.\nOur main idea is to cast membership inference of target data x as the\noptimization of another neural network (called the attacker network) to search\nfor the latent encoding to reproduce x. The final reconstruction error is used\ndirectly to conclude whether x was in the training data or not. We conduct\nextensive experiments on a variety of datasets and generative models showing\nthat: our attacker network outperforms prior membership attacks; co-membership\nattacks can be substantially more powerful than single attacks; and VAEs are\nmore susceptible to membership attacks compared to GANs. \n\n"}
{"id": "1805.10413", "contents": "Title: Fast Policy Learning through Imitation and Reinforcement Abstract: Imitation learning (IL) consists of a set of tools that leverage expert\ndemonstrations to quickly learn policies. However, if the expert is suboptimal,\nIL can yield policies with inferior performance compared to reinforcement\nlearning (RL). In this paper, we aim to provide an algorithm that combines the\nbest aspects of RL and IL. We accomplish this by formulating several popular RL\nand IL algorithms in a common mirror descent framework, showing that these\nalgorithms can be viewed as a variation on a single approach. We then propose\nLOKI, a strategy for policy learning that first performs a small but random\nnumber of IL iterations before switching to a policy gradient RL method. We\nshow that if the switching time is properly randomized, LOKI can learn to\noutperform a suboptimal expert and converge faster than running policy gradient\nfrom scratch. Finally, we evaluate the performance of LOKI experimentally in\nseveral simulated environments. \n\n"}
{"id": "1805.10616", "contents": "Title: Dynamic Network Model from Partial Observations Abstract: Can evolving networks be inferred and modeled without directly observing\ntheir nodes and edges? In many applications, the edges of a dynamic network\nmight not be observed, but one can observe the dynamics of stochastic cascading\nprocesses (e.g., information diffusion, virus propagation) occurring over the\nunobserved network. While there have been efforts to infer networks based on\nsuch data, providing a generative probabilistic model that is able to identify\nthe underlying time-varying network remains an open question. Here we consider\nthe problem of inferring generative dynamic network models based on network\ncascade diffusion data. We propose a novel framework for providing a\nnon-parametric dynamic network model--based on a mixture of coupled\nhierarchical Dirichlet processes-- based on data capturing cascade node\ninfection times. Our approach allows us to infer the evolving community\nstructure in networks and to obtain an explicit predictive distribution over\nthe edges of the underlying network--including those that were not involved in\ntransmission of any cascade, or are likely to appear in the future. We show the\neffectiveness of our approach using extensive experiments on synthetic as well\nas real-world networks. \n\n"}
{"id": "1805.10886", "contents": "Title: Importance Weighted Transfer of Samples in Reinforcement Learning Abstract: We consider the transfer of experience samples (i.e., tuples < s, a, s', r >)\nin reinforcement learning (RL), collected from a set of source tasks to improve\nthe learning process in a given target task. Most of the related approaches\nfocus on selecting the most relevant source samples for solving the target\ntask, but then all the transferred samples are used without considering anymore\nthe discrepancies between the task models. In this paper, we propose a\nmodel-based technique that automatically estimates the relevance (importance\nweight) of each source sample for solving the target task. In the proposed\napproach, all the samples are transferred and used by a batch RL algorithm to\nsolve the target task, but their contribution to the learning process is\nproportional to their importance weight. By extending the results for\nimportance weighting provided in supervised learning literature, we develop a\nfinite-sample analysis of the proposed batch RL algorithm. Furthermore, we\nempirically compare the proposed algorithm to state-of-the-art approaches,\nshowing that it achieves better learning performance and is very robust to\nnegative transfer, even when some source tasks are significantly different from\nthe target task. \n\n"}
{"id": "1805.10887", "contents": "Title: Block-optimized Variable Bit Rate Neural Image Compression Abstract: In this work, we propose an end-to-end block-based auto-encoder system for\nimage compression. We introduce novel contributions to neural-network based\nimage compression, mainly in achieving binarization simulation, variable bit\nrates with multiple networks, entropy-friendly representations, inference-stage\ncode optimization and performance-improving normalization layers in the\nauto-encoder. We evaluate and show the incremental performance increase of each\nof our contributions. \n\n"}
{"id": "1805.10915", "contents": "Title: Dirichlet-based Gaussian Processes for Large-scale Calibrated\n  Classification Abstract: In this paper, we study the problem of deriving fast and accurate\nclassification algorithms with uncertainty quantification. Gaussian process\nclassification provides a principled approach, but the corresponding\ncomputational burden is hardly sustainable in large-scale problems and devising\nefficient alternatives is a challenge. In this work, we investigate if and how\nGaussian process regression directly applied to the classification labels can\nbe used to tackle this question. While in this case training time is remarkably\nfaster, predictions need be calibrated for classification and uncertainty\nestimation. To this aim, we propose a novel approach based on interpreting the\nlabels as the output of a Dirichlet distribution. Extensive experimental\nresults show that the proposed approach provides essentially the same accuracy\nand uncertainty quantification of Gaussian process classification while\nrequiring only a fraction of computational resources. \n\n"}
{"id": "1805.10927", "contents": "Title: Scalable and Robust Community Detection with Randomized Sketching Abstract: This article explores and analyzes the unsupervised clustering of large\npartially observed graphs. We propose a scalable and provable randomized\nframework for clustering graphs generated from the stochastic block model. The\nclustering is first applied to a sub-matrix of the graph's adjacency matrix\nassociated with a reduced graph sketch constructed using random sampling. Then,\nthe clusters of the full graph are inferred based on the clusters extracted\nfrom the sketch using a correlation-based retrieval step. Uniform random node\nsampling is shown to improve the computational complexity over clustering of\nthe full graph when the cluster sizes are balanced. A new random degree-based\nnode sampling algorithm is presented which significantly improves upon the\nperformance of the clustering algorithm even when clusters are unbalanced. This\nframework improves the phase transitions for matrix-decomposition-based\nclustering with regard to computational complexity and minimum cluster size,\nwhich are shown to be nearly dimension-free in the low inter-cluster\nconnectivity regime. A third sampling technique is shown to improve balance by\nrandomly sampling nodes based on spatial distribution. We provide analysis and\nnumerical results using a convex clustering algorithm based on matrix\ncompletion. \n\n"}
{"id": "1805.11048", "contents": "Title: Scalable Spectral Clustering Using Random Binning Features Abstract: Spectral clustering is one of the most effective clustering approaches that\ncapture hidden cluster structures in the data. However, it does not scale well\nto large-scale problems due to its quadratic complexity in constructing\nsimilarity graphs and computing subsequent eigendecomposition. Although a\nnumber of methods have been proposed to accelerate spectral clustering, most of\nthem compromise considerable information loss in the original data for reducing\ncomputational bottlenecks. In this paper, we present a novel scalable spectral\nclustering method using Random Binning features (RB) to simultaneously\naccelerate both similarity graph construction and the eigendecomposition.\nSpecifically, we implicitly approximate the graph similarity (kernel) matrix by\nthe inner product of a large sparse feature matrix generated by RB. Then we\nintroduce a state-of-the-art SVD solver to effectively compute eigenvectors of\nthis large matrix for spectral clustering. Using these two building blocks, we\nreduce the computational cost from quadratic to linear in the number of data\npoints while achieving similar accuracy. Our theoretical analysis shows that\nspectral clustering via RB converges faster to the exact spectral clustering\nthan the standard Random Feature approximation. Extensive experiments on 8\nbenchmarks show that the proposed method either outperforms or matches the\nstate-of-the-art methods in both accuracy and runtime. Moreover, our method\nexhibits linear scalability in both the number of data samples and the number\nof RB features. \n\n"}
{"id": "1805.11063", "contents": "Title: Theory and Experiments on Vector Quantized Autoencoders Abstract: Deep neural networks with discrete latent variables offer the promise of\nbetter symbolic reasoning, and learning abstractions that are more useful to\nnew tasks. There has been a surge in interest in discrete latent variable\nmodels, however, despite several recent improvements, the training of discrete\nlatent variable models has remained challenging and their performance has\nmostly failed to match their continuous counterparts. Recent work on vector\nquantized autoencoders (VQ-VAE) has made substantial progress in this\ndirection, with its perplexity almost matching that of a VAE on datasets such\nas CIFAR-10. In this work, we investigate an alternate training technique for\nVQ-VAE, inspired by its connection to the Expectation Maximization (EM)\nalgorithm. Training the discrete bottleneck with EM helps us achieve better\nimage generation results on CIFAR-10, and together with knowledge distillation,\nallows us to develop a non-autoregressive machine translation model whose\naccuracy almost matches a strong greedy autoregressive baseline Transformer,\nwhile being 3.3 times faster at inference. \n\n"}
{"id": "1805.11204", "contents": "Title: A Statistical Recurrent Model on the Manifold of Symmetric Positive\n  Definite Matrices Abstract: In a number of disciplines, the data (e.g., graphs, manifolds) to be analyzed\nare non-Euclidean in nature. Geometric deep learning corresponds to techniques\nthat generalize deep neural network models to such non-Euclidean spaces.\nSeveral recent papers have shown how convolutional neural networks (CNNs) can\nbe extended to learn with graph-based data. In this work, we study the setting\nwhere the data (or measurements) are ordered, longitudinal or temporal in\nnature and live on a Riemannian manifold -- this setting is common in a variety\nof problems in statistical machine learning, vision and medical imaging. We\nshow how recurrent statistical recurrent network models can be defined in such\nspaces. We give an efficient algorithm and conduct a rigorous analysis of its\nstatistical properties. We perform extensive numerical experiments\ndemonstrating competitive performance with state of the art methods but with\nsignificantly less number of parameters. We also show applications to a\nstatistical analysis task in brain imaging, a regime where deep neural network\nmodels have only been utilized in limited ways. \n\n"}
{"id": "1805.11233", "contents": "Title: Retraining-Based Iterative Weight Quantization for Deep Neural Networks Abstract: Model compression has gained a lot of attention due to its ability to reduce\nhardware resource requirements significantly while maintaining accuracy of\nDNNs. Model compression is especially useful for memory-intensive recurrent\nneural networks because smaller memory footprint is crucial not only for\nreducing storage requirement but also for fast inference operations.\nQuantization is known to be an effective model compression method and\nresearchers are interested in minimizing the number of bits to represent\nparameters. In this work, we introduce an iterative technique to apply\nquantization, presenting high compression ratio without any modifications to\nthe training algorithm. In the proposed technique, weight quantization is\nfollowed by retraining the model with full precision weights. We show that\niterative retraining generates new sets of weights which can be quantized with\ndecreasing quantization loss at each iteration. We also show that quantization\nis efficiently able to leverage pruning, another effective model compression\nmethod. Implementation issues on combining the two methods are also addressed.\nOur experimental results demonstrate that an LSTM model using 1-bit quantized\nweights is sufficient for PTB dataset without any accuracy degradation while\nprevious methods demand at least 2-4 bits for quantized weights. \n\n"}
{"id": "1805.11328", "contents": "Title: Hamiltonian Variational Auto-Encoder Abstract: Variational Auto-Encoders (VAEs) have become very popular techniques to\nperform inference and learning in latent variable models as they allow us to\nleverage the rich representational power of neural networks to obtain flexible\napproximations of the posterior of latent variables as well as tight evidence\nlower bounds (ELBOs). Combined with stochastic variational inference, this\nprovides a methodology scaling to large datasets. However, for this methodology\nto be practically efficient, it is necessary to obtain low-variance unbiased\nestimators of the ELBO and its gradients with respect to the parameters of\ninterest. While the use of Markov chain Monte Carlo (MCMC) techniques such as\nHamiltonian Monte Carlo (HMC) has been previously suggested to achieve this\n[23, 26], the proposed methods require specifying reverse kernels which have a\nlarge impact on performance. Additionally, the resulting unbiased estimator of\nthe ELBO for most MCMC kernels is typically not amenable to the\nreparameterization trick. We show here how to optimally select reverse kernels\nin this setting and, by building upon Hamiltonian Importance Sampling (HIS)\n[17], we obtain a scheme that provides low-variance unbiased estimators of the\nELBO and its gradients using the reparameterization trick. This allows us to\ndevelop a Hamiltonian Variational Auto-Encoder (HVAE). This method can be\nreinterpreted as a target-informed normalizing flow [20] which, within our\ncontext, only requires a few evaluations of the gradient of the sampled\nlikelihood and trivial Jacobian calculations at each iteration. \n\n"}
{"id": "1805.11365", "contents": "Title: Lovasz Convolutional Networks Abstract: Semi-supervised learning on graph structured data has received significant\nattention with the recent introduction of Graph Convolution Networks (GCN).\nWhile traditional methods have focused on optimizing a loss augmented with\nLaplacian regularization framework, GCNs perform an implicit Laplacian type\nregularization to capture local graph structure. In this work, we propose\nLovasz Convolutional Network (LCNs) which are capable of incorporating global\ngraph properties. LCNs achieve this by utilizing Lovasz's orthonormal\nembeddings of the nodes. We analyse local and global properties of graphs and\ndemonstrate settings where LCNs tend to work better than GCNs. We validate the\nproposed method on standard random graph models such as stochastic block models\n(SBM) and certain community structure based graphs where LCNs outperform GCNs\nand learn more intuitive embeddings. We also perform extensive binary and\nmulti-class classification experiments on real world datasets to demonstrate\nLCN's effectiveness. In addition to simple graphs, we also demonstrate the use\nof LCNs on hyper-graphs by identifying settings where they are expected to work\nbetter than GCNs. \n\n"}
{"id": "1805.12076", "contents": "Title: Towards Understanding the Role of Over-Parametrization in Generalization\n  of Neural Networks Abstract: Despite existing work on ensuring generalization of neural networks in terms\nof scale sensitive complexity measures, such as norms, margin and sharpness,\nthese complexity measures do not offer an explanation of why neural networks\ngeneralize better with over-parametrization. In this work we suggest a novel\ncomplexity measure based on unit-wise capacities resulting in a tighter\ngeneralization bound for two layer ReLU networks. Our capacity bound correlates\nwith the behavior of test error with increasing network sizes, and could\npotentially explain the improvement in generalization with\nover-parametrization. We further present a matching lower bound for the\nRademacher complexity that improves over previous capacity lower bounds for\nneural networks. \n\n"}
{"id": "1805.12355", "contents": "Title: Deep-Energy: Unsupervised Training of Deep Neural Networks Abstract: The success of deep learning has been due, in no small part, to the\navailability of large annotated datasets. Thus, a major bottleneck in current\nlearning pipelines is the time-consuming human annotation of data. In scenarios\nwhere such input-output pairs cannot be collected, simulation is often used\ninstead, leading to a domain-shift between synthesized and real-world data.\nThis work offers an unsupervised alternative that relies on the availability of\ntask-specific energy functions, replacing the generic supervised loss. Such\nenergy functions are assumed to lead to the desired label as their minimizer\ngiven the input. The proposed approach, termed \"Deep Energy\", trains a Deep\nNeural Network (DNN) to approximate this minimization for any chosen input.\nOnce trained, a simple and fast feed-forward computation provides the inferred\nlabel. This approach allows us to perform unsupervised training of DNNs with\nreal-world inputs only, and without the need for manually-annotated labels, nor\nsynthetically created data. \"Deep Energy\" is demonstrated in this paper on\nthree different tasks -- seeded segmentation, image matting and single image\ndehazing -- exposing its generality and wide applicability. Our experiments\nshow that the solution provided by the network is often much better in quality\nthan the one obtained by a direct minimization of the energy function,\nsuggesting an added regularization property in our scheme. \n\n"}
{"id": "1805.12421", "contents": "Title: HOPF: Higher Order Propagation Framework for Deep Collective\n  Classification Abstract: Given a graph where every node has certain attributes associated with it and\nsome nodes have labels associated with them, Collective Classification (CC) is\nthe task of assigning labels to every unlabeled node using information from the\nnode as well as its neighbors. It is often the case that a node is not only\ninfluenced by its immediate neighbors but also by higher order neighbors,\nmultiple hops away. Recent state-of-the-art models for CC learn end-to-end\ndifferentiable variations of Weisfeiler-Lehman (WL) kernels to aggregate\nmulti-hop neighborhood information. In this work, we propose a Higher Order\nPropagation Framework, HOPF, which provides an iterative inference mechanism\nfor these powerful differentiable kernels. Such a combination of classical\niterative inference mechanism with recent differentiable kernels allows the\nframework to learn graph convolutional filters that simultaneously exploit the\nattribute and label information available in the neighborhood. Further, these\niterative differentiable kernels can scale to larger hops beyond the memory\nlimitations of existing differentiable kernels. We also show that existing WL\nkernel-based models suffer from the problem of Node Information Morphing where\nthe information of the node is morphed or overwhelmed by the information of its\nneighbors when considering multiple hops. To address this, we propose a\nspecific instantiation of HOPF, called the NIP models, which preserves the node\ninformation at every propagation step. The iterative formulation of NIP models\nfurther helps in incorporating distant hop information concisely as summaries\nof the inferred labels. We do an extensive evaluation across 11 datasets from\ndifferent domains. We show that existing CC models do not provide consistent\nperformance across datasets, while the proposed NIP model with iterative\ninference is more robust. \n\n"}
{"id": "1805.12528", "contents": "Title: Fusion Graph Convolutional Networks Abstract: Semi-supervised node classification in attributed graphs, i.e., graphs with\nnode features, involves learning to classify unlabeled nodes given a partially\nlabeled graph. Label predictions are made by jointly modeling the node and its'\nneighborhood features. State-of-the-art models for node classification on such\nattributed graphs use differentiable recursive functions that enable\naggregation and filtering of neighborhood information from multiple hops. In\nthis work, we analyze the representation capacity of these models to regulate\ninformation from multiple hops independently. From our analysis, we conclude\nthat these models despite being powerful, have limited representation capacity\nto capture multi-hop neighborhood information effectively. Further, we also\npropose a mathematically motivated, yet simple extension to existing graph\nconvolutional networks (GCNs) which has improved representation capacity. We\nextensively evaluate the proposed model, F-GCN on eight popular datasets from\ndifferent domains. F-GCN outperforms the state-of-the-art models for\nsemi-supervised learning on six datasets while being extremely competitive on\nthe other two. \n\n"}
{"id": "1805.12573", "contents": "Title: Learning a Prior over Intent via Meta-Inverse Reinforcement Learning Abstract: A significant challenge for the practical application of reinforcement\nlearning in the real world is the need to specify an oracle reward function\nthat correctly defines a task. Inverse reinforcement learning (IRL) seeks to\navoid this challenge by instead inferring a reward function from expert\nbehavior. While appealing, it can be impractically expensive to collect\ndatasets of demonstrations that cover the variation common in the real world\n(e.g. opening any type of door). Thus in practice, IRL must commonly be\nperformed with only a limited set of demonstrations where it can be exceedingly\ndifficult to unambiguously recover a reward function. In this work, we exploit\nthe insight that demonstrations from other tasks can be used to constrain the\nset of possible reward functions by learning a \"prior\" that is specifically\noptimized for the ability to infer expressive reward functions from limited\nnumbers of demonstrations. We demonstrate that our method can efficiently\nrecover rewards from images for novel tasks and provide intuition as to how our\napproach is analogous to learning a prior. \n\n"}
{"id": "1806.00336", "contents": "Title: A Reinforcement Learning Approach to Age of Information in Multi-User\n  Networks Abstract: Scheduling the transmission of time-sensitive data to multiple users over\nerror-prone communication channels is studied with the goal of minimizing the\nlong-term average age of information (AoI) at the users under a constraint on\nthe average number of transmissions at the source node. After each\ntransmission, the source receives an instantaneous ACK/NACK feedback from the\nintended receiver and decides on what time and to which user to transmit the\nnext update. The optimal scheduling policy is first studied under different\nfeedback mechanisms when the channel statistics are known; in particular, the\nstandard automatic repeat request (ARQ) and hybrid ARQ (HARQ) protocols are\nconsidered. Then a reinforcement learning (RL) approach is introduced, which\ndoes not assume any a priori information on the random processes governing the\nchannel states. Different RL methods are verified and compared through\nnumerical simulations. \n\n"}
{"id": "1806.00509", "contents": "Title: Semi-Recurrent CNN-based VAE-GAN for Sequential Data Generation Abstract: A semi-recurrent hybrid VAE-GAN model for generating sequential data is\nintroduced. In order to consider the spatial correlation of the data in each\nframe of the generated sequence, CNNs are utilized in the encoder, generator,\nand discriminator. The subsequent frames are sampled from the latent\ndistributions obtained by encoding the previous frames. As a result, the\ndependencies between the frames are maintained. Two testing frameworks for\nsynthesizing a sequence with any number of frames are also proposed. The\npromising experimental results on piano music generation indicates the\npotential of the proposed framework in modeling other sequential data such as\nvideo. \n\n"}
{"id": "1806.00775", "contents": "Title: Exploration in Structured Reinforcement Learning Abstract: We address reinforcement learning problems with finite state and action\nspaces where the underlying MDP has some known structure that could be\npotentially exploited to minimize the exploration rates of suboptimal (state,\naction) pairs. For any arbitrary structure, we derive problem-specific regret\nlower bounds satisfied by any learning algorithm. These lower bounds are made\nexplicit for unstructured MDPs and for those whose transition probabilities and\naverage reward functions are Lipschitz continuous w.r.t. the state and action.\nFor Lipschitz MDPs, the bounds are shown not to scale with the sizes $S$ and\n$A$ of the state and action spaces, i.e., they are smaller than $c\\log T$ where\n$T$ is the time horizon and the constant $c$ only depends on the Lipschitz\nstructure, the span of the bias function, and the minimal action sub-optimality\ngap. This contrasts with unstructured MDPs where the regret lower bound\ntypically scales as $SA\\log T$. We devise DEL (Directed Exploration Learning),\nan algorithm that matches our regret lower bounds. We further simplify the\nalgorithm for Lipschitz MDPs, and show that the simplified version is still\nable to efficiently exploit the structure. \n\n"}
{"id": "1806.00848", "contents": "Title: Learning graphs from data: A signal representation perspective Abstract: The construction of a meaningful graph topology plays a crucial role in the\neffective representation, processing, analysis and visualization of structured\ndata. When a natural choice of the graph is not readily available from the data\nsets, it is thus desirable to infer or learn a graph topology from the data. In\nthis tutorial overview, we survey solutions to the problem of graph learning,\nincluding classical viewpoints from statistics and physics, and more recent\napproaches that adopt a graph signal processing (GSP) perspective. We further\nemphasize the conceptual similarities and differences between classical and\nGSP-based graph inference methods, and highlight the potential advantage of the\nlatter in a number of theoretical and practical scenarios. We conclude with\nseveral open issues and challenges that are keys to the design of future signal\nprocessing and machine learning algorithms for learning graphs from data. \n\n"}
{"id": "1806.01445", "contents": "Title: Embedding Logical Queries on Knowledge Graphs Abstract: Learning low-dimensional embeddings of knowledge graphs is a powerful\napproach used to predict unobserved or missing edges between entities. However,\nan open challenge in this area is developing techniques that can go beyond\nsimple edge prediction and handle more complex logical queries, which might\ninvolve multiple unobserved edges, entities, and variables. For instance, given\nan incomplete biological knowledge graph, we might want to predict \"em what\ndrugs are likely to target proteins involved with both diseases X and Y?\" -- a\nquery that requires reasoning about all possible proteins that {\\em might}\ninteract with diseases X and Y. Here we introduce a framework to efficiently\nmake predictions about conjunctive logical queries -- a flexible but tractable\nsubset of first-order logic -- on incomplete knowledge graphs. In our approach,\nwe embed graph nodes in a low-dimensional space and represent logical operators\nas learned geometric operations (e.g., translation, rotation) in this embedding\nspace. By performing logical operations within a low-dimensional embedding\nspace, our approach achieves a time complexity that is linear in the number of\nquery variables, compared to the exponential complexity required by a naive\nenumeration-based approach. We demonstrate the utility of this framework in two\napplication studies on real-world datasets with millions of relations:\npredicting logical relationships in a network of drug-gene-disease interactions\nand in a graph-based representation of social interactions derived from a\npopular web forum. \n\n"}
{"id": "1806.01477", "contents": "Title: An Explainable Adversarial Robustness Metric for Deep Learning Neural\n  Networks Abstract: Deep Neural Networks(DNN) have excessively advanced the field of computer\nvision by achieving state of the art performance in various vision tasks. These\nresults are not limited to the field of vision but can also be seen in speech\nrecognition and machine translation tasks. Recently, DNNs are found to poorly\nfail when tested with samples that are crafted by making imperceptible changes\nto the original input images. This causes a gap between the validation and\nadversarial performance of a DNN. An effective and generalizable robustness\nmetric for evaluating the performance of DNN on these adversarial inputs is\nstill missing from the literature. In this paper, we propose Noise Sensitivity\nScore (NSS), a metric that quantifies the performance of a DNN on a specific\ninput under different forms of fix-directional attacks. An insightful\nmathematical explanation is provided for deeply understanding the proposed\nmetric. By leveraging the NSS, we also proposed a skewness based dataset\nrobustness metric for evaluating a DNN's adversarial performance on a given\ndataset. Extensive experiments using widely used state of the art architectures\nalong with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100,\nand ImageNet, are used to validate the effectiveness and generalization of our\nproposed metrics. Instead of simply measuring a DNN's adversarial robustness in\nthe input domain, as previous works, the proposed NSS is built on top of\ninsightful mathematical understanding of the adversarial attack and gives a\nmore explicit explanation of the robustness. \n\n"}
{"id": "1806.01528", "contents": "Title: The universal approximation power of finite-width deep ReLU networks Abstract: We show that finite-width deep ReLU neural networks yield rate-distortion\noptimal approximation (B\\\"olcskei et al., 2018) of polynomials, windowed\nsinusoidal functions, one-dimensional oscillatory textures, and the Weierstrass\nfunction, a fractal function which is continuous but nowhere differentiable.\nTogether with their recently established universal approximation property of\naffine function systems (B\\\"olcskei et al., 2018), this shows that deep neural\nnetworks approximate vastly different signal structures generated by the affine\ngroup, the Weyl-Heisenberg group, or through warping, and even certain\nfractals, all with approximation error decaying exponentially in the number of\nneurons. We also prove that in the approximation of sufficiently smooth\nfunctions finite-width deep networks require strictly smaller connectivity than\nfinite-depth wide networks. \n\n"}
{"id": "1806.01540", "contents": "Title: Combining Multiple Algorithms in Classifier Ensembles using Generalized\n  Mixture Functions Abstract: Classifier ensembles are pattern recognition structures composed of a set of\nclassification algorithms (members), organized in a parallel way, and a\ncombination method with the aim of increasing the classification accuracy of a\nclassification system. In this study, we investigate the application of a\ngeneralized mixture (GM) functions as a new approach for providing an efficient\ncombination procedure for these systems through the use of dynamic weights in\nthe combination process. Therefore, we present three GM functions to be applied\nas a combination method. The main advantage of these functions is that they can\ndefine dynamic weights at the member outputs, making the combination process\nmore efficient. In order to evaluate the feasibility of the proposed approach,\nan empirical analysis is conducted, applying classifier ensembles to 25\ndifferent classification data sets. In this analysis, we compare the use of the\nproposed approaches to ensembles using traditional combination methods as well\nas the state-of-the-art ensemble methods. Our findings indicated gains in terms\nof performance when comparing the proposed approaches to the traditional ones\nas well as comparable results with the state-of-the-art methods. \n\n"}
{"id": "1806.01678", "contents": "Title: A Projection Method for Metric-Constrained Optimization Abstract: We outline a new approach for solving optimization problems which enforce\ntriangle inequalities on output variables. We refer to this as\nmetric-constrained optimization, and give several examples where problems of\nthis form arise in machine learning applications and theoretical approximation\nalgorithms for graph clustering. Although these problem are interesting from a\ntheoretical perspective, they are challenging to solve in practice due to the\nhigh memory requirement of black-box solvers. In order to address this\nchallenge we first prove that the metric-constrained linear program relaxation\nof correlation clustering is equivalent to a special case of the metric\nnearness problem. We then developed a general solver for metric-constrained\nlinear and quadratic programs by generalizing and improving a simple projection\nalgorithm originally developed for metric nearness. We give several novel\napproximation guarantees for using our framework to find lower bounds for\noptimal solutions to several challenging graph clustering problems. We also\ndemonstrate the power of our framework by solving optimizing problems involving\nup to 10^{8} variables and 10^{11} constraints. \n\n"}
{"id": "1806.02032", "contents": "Title: Killing four birds with one Gaussian process: the relation between\n  different test-time attacks Abstract: In machine learning (ML) security, attacks like evasion, model stealing or\nmembership inference are generally studied in individually. Previous work has\nalso shown a relationship between some attacks and decision function curvature\nof the targeted model. Consequently, we study an ML model allowing direct\ncontrol over the decision surface curvature: Gaussian Process classifiers\n(GPCs). For evasion, we find that changing GPC's curvature to be robust against\none attack algorithm boils down to enabling a different norm or attack\nalgorithm to succeed. This is backed up by our formal analysis showing that\nstatic security guarantees are opposed to learning. Concerning intellectual\nproperty, we show formally that lazy learning does not necessarily leak all\ninformation when applied. In practice, often a seemingly secure curvature can\nbe found. For example, we are able to secure GPC against empirical membership\ninference by proper configuration. In this configuration, however, the GPC's\nhyper-parameters are leaked, e.g. model reverse engineering succeeds. We\nconclude that attacks on classification should not be studied in isolation, but\nin relation to each other. \n\n"}
{"id": "1806.02450", "contents": "Title: A Finite Time Analysis of Temporal Difference Learning With Linear\n  Function Approximation Abstract: Temporal difference learning (TD) is a simple iterative algorithm used to\nestimate the value function corresponding to a given policy in a Markov\ndecision process. Although TD is one of the most widely used algorithms in\nreinforcement learning, its theoretical analysis has proved challenging and few\nguarantees on its statistical efficiency are available. In this work, we\nprovide a simple and explicit finite time analysis of temporal difference\nlearning with linear function approximation. Except for a few key insights, our\nanalysis mirrors standard techniques for analyzing stochastic gradient descent\nalgorithms, and therefore inherits the simplicity and elegance of that\nliterature. Final sections of the paper show how all of our main results extend\nto the study of TD learning with eligibility traces, known as TD($\\lambda$),\nand to Q-learning applied in high-dimensional optimal stopping problems. \n\n"}
{"id": "1806.02978", "contents": "Title: JointGAN: Multi-Domain Joint Distribution Learning with Generative\n  Adversarial Nets Abstract: A new generative adversarial network is developed for joint distribution\nmatching. Distinct from most existing approaches, that only learn conditional\ndistributions, the proposed model aims to learn a joint distribution of\nmultiple random variables (domains). This is achieved by learning to sample\nfrom conditional distributions between the domains, while simultaneously\nlearning to sample from the marginals of each individual domain. The proposed\nframework consists of multiple generators and a single softmax-based critic,\nall jointly trained via adversarial learning. From a simple noise source, the\nproposed framework allows synthesis of draws from the marginals, conditional\ndraws given observations from a subset of random variables, or complete draws\nfrom the full joint distribution. Most examples considered are for joint\nanalysis of two domains, with examples for three domains also presented. \n\n"}
{"id": "1806.03168", "contents": "Title: Data-driven Analytics for Business Architectures: Proposed Use of Graph\n  Theory Abstract: Business Architecture (BA) plays a significant role in helping organizations\nunderstand enterprise structures and processes, and align them with strategic\nobjectives. However, traditional BAs are represented in fixed structure with\nstatic model elements and fail to dynamically capture business insights based\non internal and external data. To solve this problem, this paper introduces the\ngraph theory into BAs with aim of building extensible data-driven analytics and\nautomatically generating business insights. We use IBM's Component Business\nModel (CBM) as an example to illustrate various ways in which graph theory can\nbe leveraged for data-driven analytics, including what and how business\ninsights can be obtained. Future directions for applying graph theory to\nbusiness architecture analytics are discussed. \n\n"}
{"id": "1806.03342", "contents": "Title: Discovering Signals from Web Sources to Predict Cyber Attacks Abstract: Cyber attacks are growing in frequency and severity. Over the past year alone\nwe have witnessed massive data breaches that stole personal information of\nmillions of people and wide-scale ransomware attacks that paralyzed critical\ninfrastructure of several countries. Combating the rising cyber threat calls\nfor a multi-pronged strategy, which includes predicting when these attacks will\noccur. The intuition driving our approach is this: during the planning and\npreparation stages, hackers leave digital traces of their activities on both\nthe surface web and dark web in the form of discussions on platforms like\nhacker forums, social media, blogs and the like. These data provide predictive\nsignals that allow anticipating cyber attacks. In this paper, we describe\nmachine learning techniques based on deep neural networks and autoregressive\ntime series models that leverage external signals from publicly available Web\nsources to forecast cyber attacks. Performance of our framework across ground\ntruth data over real-world forecasting tasks shows that our methods yield a\nsignificant lift or increase of F1 for the top signals on predicted cyber\nattacks. Our results suggest that, when deployed, our system will be able to\nprovide an effective line of defense against various types of targeted cyber\nattacks. \n\n"}
{"id": "1806.04016", "contents": "Title: Baselines and a datasheet for the Cerema AWP dataset Abstract: This paper presents the recently published Cerema AWP (Adverse Weather\nPedestrian) dataset for various machine learning tasks and its exports in\nmachine learning friendly format. We explain why this dataset can be\ninteresting (mainly because it is a greatly controlled and fully annotated\nimage dataset) and present baseline results for various tasks. Moreover, we\ndecided to follow the very recent suggestions of datasheets for dataset, trying\nto standardize all the available information of the dataset, with a\ntransparency objective. \n\n"}
{"id": "1806.04310", "contents": "Title: MISSION: Ultra Large-Scale Feature Selection using Count-Sketches Abstract: Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions. \n\n"}
{"id": "1806.04339", "contents": "Title: When Will Gradient Methods Converge to Max-margin Classifier under ReLU\n  Models? Abstract: We study the implicit bias of gradient descent methods in solving a binary\nclassification problem over a linearly separable dataset. The classifier is\ndescribed by a nonlinear ReLU model and the objective function adopts the\nexponential loss function. We first characterize the landscape of the loss\nfunction and show that there can exist spurious asymptotic local minima besides\nasymptotic global minima. We then show that gradient descent (GD) can converge\nto either a global or a local max-margin direction, or may diverge from the\ndesired max-margin direction in a general context. For stochastic gradient\ndescent (SGD), we show that it converges in expectation to either the global or\nthe local max-margin direction if SGD converges. We further explore the\nimplicit bias of these algorithms in learning a multi-neuron network under\ncertain stationary conditions, and show that the learned classifier maximizes\nthe margins of each sample pattern partition under the ReLU activation. \n\n"}
{"id": "1806.04642", "contents": "Title: Accelerating Imitation Learning with Predictive Models Abstract: Sample efficiency is critical in solving real-world reinforcement learning\nproblems, where agent-environment interactions can be costly. Imitation\nlearning from expert advice has proved to be an effective strategy for reducing\nthe number of interactions required to train a policy. Online imitation\nlearning, which interleaves policy evaluation and policy optimization, is a\nparticularly effective technique with provable performance guarantees. In this\nwork, we seek to further accelerate the convergence rate of online imitation\nlearning, thereby making it more sample efficient. We propose two model-based\nalgorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based\non solving variational inequalities and MoBIL-Prox based on stochastic\nfirst-order updates. These two methods leverage a model to predict future\ngradients to speed up policy learning. When the model oracle is learned online,\nthese algorithms can provably accelerate the best known convergence rate up to\nan order. Our algorithms can be viewed as a generalization of stochastic\nMirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style\nanalysis of performance. \n\n"}
{"id": "1806.05357", "contents": "Title: Deep Multi-Output Forecasting: Learning to Accurately Predict Blood\n  Glucose Trajectories Abstract: In many forecasting applications, it is valuable to predict not only the\nvalue of a signal at a certain time point in the future, but also the values\nleading up to that point. This is especially true in clinical applications,\nwhere the future state of the patient can be less important than the patient's\noverall trajectory. This requires multi-step forecasting, a forecasting variant\nwhere one aims to predict multiple values in the future simultaneously.\nStandard methods to accomplish this can propagate error from prediction to\nprediction, reducing quality over the long term. In light of these challenges,\nwe propose multi-output deep architectures for multi-step forecasting in which\nwe explicitly model the distribution of future values of the signal over a\nprediction horizon. We apply these techniques to the challenging and clinically\nrelevant task of blood glucose forecasting. Through a series of experiments on\na real-world dataset consisting of 550K blood glucose measurements, we\ndemonstrate the effectiveness of our proposed approaches in capturing the\nunderlying signal dynamics. Compared to existing shallow and deep methods, we\nfind that our proposed approaches improve performance individually and capture\ncomplementary information, leading to a large improvement over the baseline\nwhen combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the\nresults suggest the efficacy of our proposed approach in predicting blood\nglucose level and multi-step forecasting more generally. \n\n"}
{"id": "1806.05403", "contents": "Title: On the Perceptron's Compression Abstract: We study and provide exposition to several phenomena that are related to the\nperceptron's compression. One theme concerns modifications of the perceptron\nalgorithm that yield better guarantees on the margin of the hyperplane it\noutputs. These modifications can be useful in training neural networks as well,\nand we demonstrate them with some experimental data. In a second theme, we\ndeduce conclusions from the perceptron's compression in various contexts. \n\n"}
{"id": "1806.05559", "contents": "Title: Extracting Parallel Sentences with Bidirectional Recurrent Neural\n  Networks to Improve Machine Translation Abstract: Parallel sentence extraction is a task addressing the data sparsity problem\nfound in multilingual natural language processing applications. We propose a\nbidirectional recurrent neural network based approach to extract parallel\nsentences from collections of multilingual texts. Our experiments with noisy\nparallel corpora show that we can achieve promising results against a\ncompetitive baseline by removing the need of specific feature engineering or\nadditional external resources. To justify the utility of our approach, we\nextract sentence pairs from Wikipedia articles to train machine translation\nsystems and show significant improvements in translation performance. \n\n"}
{"id": "1806.06086", "contents": "Title: Minibatch Gibbs Sampling on Large Graphical Models Abstract: Gibbs sampling is the de facto Markov chain Monte Carlo method used for\ninference and learning on large scale graphical models. For complicated factor\ngraphs with lots of factors, the performance of Gibbs sampling can be limited\nby the computational cost of executing a single update step of the Markov\nchain. This cost is proportional to the degree of the graph, the number of\nfactors adjacent to each variable. In this paper, we show how this cost can be\nreduced by using minibatching: subsampling the factors to form an estimate of\ntheir sum. We introduce several minibatched variants of Gibbs, show that they\ncan be made unbiased, prove bounds on their convergence rates, and show that\nunder some conditions they can result in asymptotic single-update-run-time\nspeedups over plain Gibbs sampling. \n\n"}
{"id": "1806.06392", "contents": "Title: Task-Relevant Object Discovery and Categorization for Playing\n  First-person Shooter Games Abstract: We consider the problem of learning to play first-person shooter (FPS) video\ngames using raw screen images as observations and keyboard inputs as actions.\nThe high-dimensionality of the observations in this type of applications leads\nto prohibitive needs of training data for model-free methods, such as the deep\nQ-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on\nlearning low-dimensional representations that may reduce the need for data.\nThis paper presents a new and efficient method for learning such\nrepresentations. Salient segments of consecutive frames are detected from their\noptical flow, and clustered based on their feature descriptors. The clusters\ntypically correspond to different discovered categories of objects. Segments\ndetected in new frames are then classified based on their nearest clusters.\nBecause only a few categories are relevant to a given task, the importance of a\ncategory is defined as the correlation between its occurrence and the agent's\nperformance. The result is encoded as a vector indicating objects that are in\nthe frame and their locations, and used as a side input to DRQN. Experiments on\nthe game Doom provide a good evidence for the benefit of this approach. \n\n"}
{"id": "1806.06763", "contents": "Title: Closing the Generalization Gap of Adaptive Gradient Methods in Training\n  Deep Neural Networks Abstract: Adaptive gradient methods, which adopt historical gradient information to\nautomatically adjust the learning rate, despite the nice property of fast\nconvergence, have been observed to generalize worse than stochastic gradient\ndescent (SGD) with momentum in training deep neural networks. This leaves how\nto close the generalization gap of adaptive gradient methods an open problem.\nIn this work, we show that adaptive gradient methods such as Adam, Amsgrad, are\nsometimes \"over adapted\". We design a new algorithm, called Partially adaptive\nmomentum estimation method, which unifies the Adam/Amsgrad with SGD by\nintroducing a partial adaptive parameter $p$, to achieve the best from both\nworlds. We also prove the convergence rate of our proposed algorithm to a\nstationary point in the stochastic nonconvex optimization setting. Experiments\non standard benchmarks show that our proposed algorithm can maintain a fast\nconvergence rate as Adam/Amsgrad while generalizing as well as SGD in training\ndeep neural networks. These results would suggest practitioners pick up\nadaptive gradient methods once again for faster training of deep neural\nnetworks. \n\n"}
{"id": "1806.06988", "contents": "Title: Deep Neural Decision Trees Abstract: Deep neural networks have been proven powerful at processing perceptual data,\nsuch as images and audio. However for tabular data, tree-based models are more\npopular. A nice property of tree-based models is their natural\ninterpretability. In this work, we present Deep Neural Decision Trees (DNDT) --\ntree models realised by neural networks. A DNDT is intrinsically interpretable,\nas it is a tree. Yet as it is also a neural network (NN), it can be easily\nimplemented in NN toolkits, and trained with gradient descent rather than\ngreedy splitting. We evaluate DNDT on several tabular datasets, verify its\nefficacy, and investigate similarities and differences between DNDT and vanilla\ndecision trees. Interestingly, DNDT self-prunes at both split and\nfeature-level. \n\n"}
{"id": "1806.07104", "contents": "Title: Online Linear Quadratic Control Abstract: We study the problem of controlling linear time-invariant systems with known\nnoisy dynamics and adversarially chosen quadratic losses. We present the first\nefficient online learning algorithms in this setting that guarantee\n$O(\\sqrt{T})$ regret under mild assumptions, where $T$ is the time horizon. Our\nalgorithms rely on a novel SDP relaxation for the steady-state distribution of\nthe system. Crucially, and in contrast to previously proposed relaxations, the\nfeasible solutions of our SDP all correspond to \"strongly stable\" policies that\nmix exponentially fast to a steady state. \n\n"}
{"id": "1806.07174", "contents": "Title: FRnet-DTI: Deep Convolutional Neural Networks with Evolutionary and\n  Structural Features for Drug-Target Interaction Abstract: The task of drug-target interaction prediction holds significant importance\nin pharmacology and therapeutic drug design. In this paper, we present\nFRnet-DTI, an auto encoder and a convolutional classifier for feature\nmanipulation and drug target interaction prediction. Two convolutional neural\nneworks are proposed where one model is used for feature manipulation and the\nother one for classification. Using the first method FRnet-1, we generate 4096\nfeatures for each of the instances in each of the datasets and use the second\nmethod, FRnet-2, to identify interaction probability employing those features.\nWe have tested our method on four gold standard datasets exhaustively used by\nother researchers. Experimental results shows that our method significantly\nimproves over the state-of-the-art method on three of the four drug-target\ninteraction gold standard datasets on both area under curve for Receiver\nOperating Characteristic(auROC) and area under Precision Recall curve(auPR)\nmetric. We also introduce twenty new potential drug-target pairs for\ninteraction based on high prediction scores. Codes Available: https: // github.\ncom/ farshidrayhanuiu/ FRnet-DTI/ Web Implementation: http: // farshidrayhan.\npythonanywhere. com/ FRnet-DTI/ \n\n"}
{"id": "1806.07353", "contents": "Title: Faster SGD training by minibatch persistency Abstract: It is well known that, for most datasets, the use of large-size minibatches\nfor Stochastic Gradient Descent (SGD) typically leads to slow convergence and\npoor generalization. On the other hand, large minibatches are of great\npractical interest as they allow for a better exploitation of modern GPUs.\nPrevious literature on the subject concentrated on how to adjust the main SGD\nparameters (in particular, the learning rate) when using large minibatches. In\nthis work we introduce an additional feature, that we call minibatch\npersistency, that consists in reusing the same minibatch for K consecutive SGD\niterations. The computational conjecture here is that a large minibatch\ncontains a significant sample of the training set, so one can afford to\nslightly overfitting it without worsening generalization too much. The approach\nis intended to speedup SGD convergence, and also has the advantage of reducing\nthe overhead related to data loading on the internal GPU memory. We present\ncomputational results on CIFAR-10 with an AlexNet architecture, showing that\neven small persistency values (K=2 or 5) already lead to a significantly faster\nconvergence and to a comparable (or even better) generalization than the\nstandard \"disposable minibatch\" approach (K=1), in particular when large\nminibatches are used. The lesson learned is that minibatch persistency can be a\nsimple yet effective way to deal with large minibatches. \n\n"}
{"id": "1806.07373", "contents": "Title: Few-Shot Segmentation Propagation with Guided Networks Abstract: Learning-based methods for visual segmentation have made progress on\nparticular types of segmentation tasks, but are limited by the necessary\nsupervision, the narrow definitions of fixed tasks, and the lack of control\nduring inference for correcting errors. To remedy the rigidity and annotation\nburden of standard approaches, we address the problem of few-shot segmentation:\ngiven few image and few pixel supervision, segment any images accordingly. We\npropose guided networks, which extract a latent task representation from any\namount of supervision, and optimize our architecture end-to-end for fast,\naccurate few-shot segmentation. Our method can switch tasks without further\noptimization and quickly update when given more guidance. We report the first\nresults for segmentation from one pixel per concept and show real-time\ninteractive video segmentation. Our unified approach propagates pixel\nannotations across space for interactive segmentation, across time for video\nsegmentation, and across scenes for semantic segmentation. Our guided segmentor\nis state-of-the-art in accuracy for the amount of annotation and time. See\nhttp://github.com/shelhamer/revolver for code, models, and more details. \n\n"}
{"id": "1806.07863", "contents": "Title: Learning ReLU Networks via Alternating Minimization Abstract: We propose and analyze a new family of algorithms for training neural\nnetworks with ReLU activations. Our algorithms are based on the technique of\nalternating minimization: estimating the activation patterns of each ReLU for\nall given samples, interleaved with weight updates via a least-squares step.\nThe main focus of our paper are 1-hidden layer networks with $k$ hidden neurons\nand ReLU activation. We show that under standard distributional assumptions on\nthe $d-$dimensional input data, our algorithm provably recovers the true\n`ground truth' parameters in a linearly convergent fashion. This holds as long\nas the weights are sufficiently well initialized; furthermore, our method\nrequires only $n=\\widetilde{O}(dk^2)$ samples. We also analyze the special case\nof 1-hidden layer networks with skipped connections, commonly used in\nResNet-type architectures, and propose a novel initialization strategy for the\nsame. For ReLU based ResNet type networks, we provide the first linear\nconvergence guarantee with an end-to-end algorithm. We also extend this\nframework to deeper networks and empirically demonstrate its convergence to a\nglobal minimum. \n\n"}
{"id": "1806.07944", "contents": "Title: Searching for a Single Community in a Graph Abstract: In standard graph clustering/community detection, one is interested in\npartitioning the graph into more densely connected subsets of nodes. In\ncontrast, the \"search\" problem of this paper aims to only find the nodes in a\n\"single\" such community, the target, out of the many communities that may\nexist. To do so , we are given suitable side information about the target; for\nexample, a very small number of nodes from the target are labeled as such.\n  We consider a general yet simple notion of side information: all nodes are\nassumed to have random weights, with nodes in the target having higher weights\non average. Given these weights and the graph, we develop a variant of the\nmethod of moments that identifies nodes in the target more reliably, and with\nlower computation, than generic community detection methods that do not use\nside information and partition the entire graph. Our empirical results show\nsignificant gains in runtime, and also gains in accuracy over other graph\nclustering algorithms. \n\n"}
{"id": "1806.07963", "contents": "Title: Latent heterogeneous multilayer community detection Abstract: We propose a method for simultaneously detecting shared and unshared\ncommunities in heterogeneous multilayer weighted and undirected networks. The\nmultilayer network is assumed to follow a generative probabilistic model that\ntakes into account the similarities and dissimilarities between the\ncommunities. We make use of a variational Bayes approach for jointly inferring\nthe shared and unshared hidden communities from multilayer network\nobservations. We show that our approach outperforms state-of-the-art algorithms\nin detecting disparate (shared and private) communities on synthetic data as\nwell as on real genome-wide fibroblast proliferation dataset. \n\n"}
{"id": "1806.08049", "contents": "Title: On the Robustness of Interpretability Methods Abstract: We argue that robustness of explanations---i.e., that similar inputs should\ngive rise to similar explanations---is a key desideratum for interpretability.\nWe introduce metrics to quantify robustness and demonstrate that current\nmethods do not perform well according to these metrics. Finally, we propose\nways that robustness can be enforced on existing interpretability approaches. \n\n"}
{"id": "1806.08079", "contents": "Title: GrCAN: Gradient Boost Convolutional Autoencoder with Neural Decision\n  Forest Abstract: Random forest and deep neural network are two schools of effective\nclassification methods in machine learning. While the random forest is robust\nirrespective of the data domain, the deep neural network has advantages in\nhandling high dimensional data. In view that a differentiable neural decision\nforest can be added to the neural network to fully exploit the benefits of both\nmodels, in our work, we further combine convolutional autoencoder with neural\ndecision forest, where autoencoder has its advantages in finding the hidden\nrepresentations of the input data. We develop a gradient boost module and embed\nit into the proposed convolutional autoencoder with neural decision forest to\nimprove the performance. The idea of gradient boost is to learn and use the\nresidual in the prediction. In addition, we design a structure to learn the\nparameters of the neural decision forest and gradient boost module at\ncontiguous steps. The extensive experiments on several public datasets\ndemonstrate that our proposed model achieves good efficiency and prediction\nperformance compared with a series of baseline methods. \n\n"}
{"id": "1806.08240", "contents": "Title: InfoCatVAE: Representation Learning with Categorical Variational\n  Autoencoders Abstract: This paper describes InfoCatVAE, an extension of the variational autoencoder\nthat enables unsupervised disentangled representation learning. InfoCatVAE uses\nmultimodal distributions for the prior and the inference network and then\nmaximizes the evidence lower bound objective (ELBO). We connect the new ELBO\nderived for our model with a natural soft clustering objective which explains\nthe robustness of our approach. We then adapt the InfoGANs method to our\nsetting in order to maximize the mutual information between the categorical\ncode and the generated inputs and obtain an improved model. \n\n"}
{"id": "1806.08267", "contents": "Title: Complex Gated Recurrent Neural Networks Abstract: Complex numbers have long been favoured for digital signal processing, yet\ncomplex representations rarely appear in deep learning architectures. RNNs,\nwidely used to process time series and sequence information, could greatly\nbenefit from complex representations. We present a novel complex gated\nrecurrent cell, which is a hybrid cell combining complex-valued and\nnorm-preserving state transitions with a gating mechanism. The resulting RNN\nexhibits excellent stability and convergence properties and performs\ncompetitively on the synthetic memory and adding task, as well as on the\nreal-world tasks of human motion prediction. \n\n"}
{"id": "1806.08295", "contents": "Title: How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement\n  Learning Experiments Abstract: Consistently checking the statistical significance of experimental results is\none of the mandatory methodological steps to address the so-called\n\"reproducibility crisis\" in deep reinforcement learning. In this tutorial\npaper, we explain how the number of random seeds relates to the probabilities\nof statistical errors. For both the t-test and the bootstrap confidence\ninterval test, we recall theoretical guidelines to determine the number of\nrandom seeds one should use to provide a statistically significant comparison\nof the performance of two algorithms. Finally, we discuss the influence of\ndeviations from the assumptions usually made by statistical tests. We show that\nthey can lead to inaccurate evaluations of statistical errors and provide\nguidelines to counter these negative effects. We make our code available to\nperform the tests. \n\n"}
{"id": "1806.08727", "contents": "Title: Jack the Reader - A Machine Reading Framework Abstract: Many Machine Reading and Natural Language Understanding tasks require reading\nsupporting text in order to answer questions. For example, in Question\nAnswering, the supporting text can be newswire or Wikipedia articles; in\nNatural Language Inference, premises can be seen as the supporting text and\nhypotheses as questions. Providing a set of useful primitives operating in a\nsingle framework of related tasks would allow for expressive modelling, and\neasier model comparison and replication. To that end, we present Jack the\nReader (Jack), a framework for Machine Reading that allows for quick model\nprototyping by component reuse, evaluation of new models on existing datasets\nas well as integrating new datasets and applying them on a growing set of\nimplemented baseline models. Jack is currently supporting (but not limited to)\nthree tasks: Question Answering, Natural Language Inference, and Link\nPrediction. It is developed with the aim of increasing research efficiency and\ncode reuse. \n\n"}
{"id": "1806.08941", "contents": "Title: A Recursive PLS (Partial Least Squares) based Approach for Enterprise\n  Threat Management Abstract: Most of the existing solutions to enterprise threat management are preventive\napproaches prescribing means to prevent policy violations with varying degrees\nof success. In this paper we consider the complementary scenario where a number\nof security violations have already occurred, or security threats, or\nvulnerabilities have been reported and a security administrator needs to\ngenerate optimal response to these security events. We present a principled\napproach to study and model the human expertise in responding to the emergent\nthreats owing to these security events. A recursive Partial Least Squares based\nadaptive learning model is defined using a factorial analysis of the security\nevents together with a method for estimating the effect of global context\ndependent semantic information used by the security administrators. Presented\nmodel is theoretically optimal and operationally recursive in nature to deal\nwith the set of security events being generated continuously. We discuss the\nunderlying challenges and ways in which the model could be operationalized in\ncentralized versus decentralized, and real-time versus batch processing modes. \n\n"}
{"id": "1806.09070", "contents": "Title: Generative Models for Pose Transfer Abstract: We investigate nearest neighbor and generative models for transferring pose\nbetween persons. We take in a video of one person performing a sequence of\nactions and attempt to generate a video of another person performing the same\nactions. Our generative model (pix2pix) outperforms k-NN at both generating\ncorresponding frames and generalizing outside the demonstrated action set. Our\nmost salient contribution is determining a pipeline (pose detection, face\ndetection, k-NN based pairing) that is effective at perform-ing the desired\ntask. We also detail several iterative improvements and failure modes. \n\n"}
{"id": "1806.09211", "contents": "Title: Equalizing Financial Impact in Supervised Learning Abstract: Notions of \"fair classification\" that have arisen in computer science\ngenerally revolve around equalizing certain statistics across protected groups.\nThis approach has been criticized as ignoring societal issues, including how\nerrors can hurt certain groups disproportionately. We pose a modification of\none of the fairness criteria from Hardt, Price, and Srebro [NIPS, 2016] that\nmakes a small step towards addressing this issue in the case of financial\ndecisions like giving loans. We call this new notion \"equalized financial\nimpact.\" \n\n"}
{"id": "1806.09504", "contents": "Title: Interpreting Embedding Models of Knowledge Bases: A Pedagogical Approach Abstract: Knowledge bases are employed in a variety of applications from natural\nlanguage processing to semantic web search; alas, in practice their usefulness\nis hurt by their incompleteness. Embedding models attain state-of-the-art\naccuracy in knowledge base completion, but their predictions are notoriously\nhard to interpret. In this paper, we adapt \"pedagogical approaches\" (from the\nliterature on neural networks) so as to interpret embedding models by\nextracting weighted Horn rules from them. We show how pedagogical approaches\nhave to be adapted to take upon the large-scale relational aspects of knowledge\nbases and show experimentally their strengths and weaknesses. \n\n"}
{"id": "1806.09856", "contents": "Title: Dropout-based Active Learning for Regression Abstract: Active learning is relevant and challenging for high-dimensional regression\nmodels when the annotation of the samples is expensive. Yet most of the\nexisting sampling methods cannot be applied to large-scale problems, consuming\ntoo much time for data processing. In this paper, we propose a fast active\nlearning algorithm for regression, tailored for neural network models. It is\nbased on uncertainty estimation from stochastic dropout output of the network.\nExperiments on both synthetic and real-world datasets show comparable or better\nperformance (depending on the accuracy metric) as compared to the baselines.\nThis approach can be generalized to other deep learning architectures. It can\nbe used to systematically improve a machine-learning model as it offers a\ncomputationally efficient way of sampling additional data. \n\n"}
{"id": "1806.10230", "contents": "Title: Guided evolutionary strategies: Augmenting random search with surrogate\n  gradients Abstract: Many applications in machine learning require optimizing a function whose\ntrue gradient is unknown, but where surrogate gradient information (directions\nthat may be correlated with, but not necessarily identical to, the true\ngradient) is available instead. This arises when an approximate gradient is\neasier to compute than the full gradient (e.g. in meta-learning or unrolled\noptimization), or when a true gradient is intractable and is replaced with a\nsurrogate (e.g. in certain reinforcement learning applications, or when using\nsynthetic gradients). We propose Guided Evolutionary Strategies, a method for\noptimally using surrogate gradient directions along with random search. We\ndefine a search distribution for evolutionary strategies that is elongated\nalong a guiding subspace spanned by the surrogate gradients. This allows us to\nestimate a descent direction which can then be passed to a first-order\noptimizer. We analytically and numerically characterize the tradeoffs that\nresult from tuning how strongly the search distribution is stretched along the\nguiding subspace, and we use this to derive a setting of the hyperparameters\nthat works well across problems. Finally, we apply our method to example\nproblems, demonstrating an improvement over both standard evolutionary\nstrategies and first-order methods (that directly follow the surrogate\ngradient). We provide a demo of Guided ES at\nhttps://github.com/brain-research/guided-evolutionary-strategies \n\n"}
{"id": "1806.10308", "contents": "Title: Matrix Completion from Non-Uniformly Sampled Entries Abstract: In this paper, we consider matrix completion from non-uniformly sampled\nentries including fully observed and partially observed columns. Specifically,\nwe assume that a small number of columns are randomly selected and fully\nobserved, and each remaining column is partially observed with uniform\nsampling. To recover the unknown matrix, we first recover its column space from\nthe fully observed columns. Then, for each partially observed column, we\nrecover it by finding a vector which lies in the recovered column space and\nconsists of the observed entries. When the unknown $m\\times n$ matrix is\nlow-rank, we show that our algorithm can exactly recover it from merely\n$\\Omega(rn\\ln n)$ entries, where $r$ is the rank of the matrix. Furthermore,\nfor a noisy low-rank matrix, our algorithm computes a low-rank approximation of\nthe unknown matrix and enjoys an additive error bound measured by Frobenius\nnorm. Experimental results on synthetic datasets verify our theoretical claims\nand demonstrate the effectiveness of our proposed algorithm. \n\n"}
{"id": "1806.11006", "contents": "Title: Learning Implicit Generative Models with the Method of Learned Moments Abstract: We propose a method of moments (MoM) algorithm for training large-scale\nimplicit generative models. Moment estimation in this setting encounters two\nproblems: it is often difficult to define the millions of moments needed to\nlearn the model parameters, and it is hard to determine which properties are\nuseful when specifying moments. To address the first issue, we introduce a\nmoment network, and define the moments as the network's hidden units and the\ngradient of the network's output with the respect to its parameters. To tackle\nthe second problem, we use asymptotic theory to highlight desiderata for\nmoments -- namely they should minimize the asymptotic variance of estimated\nmodel parameters -- and introduce an objective to learn better moments. The\nsequence of objectives created by this Method of Learned Moments (MoLM) can\ntrain high-quality neural image samplers. On CIFAR-10, we demonstrate that\nMoLM-trained generators achieve significantly higher Inception Scores and lower\nFrechet Inception Distances than those trained with gradient\npenalty-regularized and spectrally-normalized adversarial objectives. These\ngenerators also achieve nearly perfect Multi-Scale Structural Similarity Scores\non CelebA, and can create high-quality samples of 128x128 images. \n\n"}
{"id": "1806.11027", "contents": "Title: A Simple Stochastic Variance Reduced Algorithm with Fast Convergence\n  Rates Abstract: Recent years have witnessed exciting progress in the study of stochastic\nvariance reduced gradient methods (e.g., SVRG, SAGA), their accelerated\nvariants (e.g, Katyusha) and their extensions in many different settings (e.g.,\nonline, sparse, asynchronous, distributed). Among them, accelerated methods\nenjoy improved convergence rates but have complex coupling structures, which\nmakes them hard to be extended to more settings (e.g., sparse and asynchronous)\ndue to the existence of perturbation. In this paper, we introduce a simple\nstochastic variance reduced algorithm (MiG), which enjoys the best-known\nconvergence rates for both strongly convex and non-strongly convex problems.\nMoreover, we also present its efficient sparse and asynchronous variants, and\ntheoretically analyze its convergence rates in these settings. Finally,\nextensive experiments for various machine learning problems such as logistic\nregression are given to illustrate the practical improvement in both serial and\nasynchronous settings. \n\n"}
{"id": "1806.11212", "contents": "Title: Proxy Fairness Abstract: We consider the problem of improving fairness when one lacks access to a\ndataset labeled with protected groups, making it difficult to take advantage of\nstrategies that can improve fairness but require protected group labels, either\nat training or runtime. To address this, we investigate improving fairness\nmetrics for proxy groups, and test whether doing so results in improved\nfairness for the true sensitive groups. Results on benchmark and real-world\ndatasets demonstrate that such a proxy fairness strategy can work well in\npractice. However, we caution that the effectiveness likely depends on the\nchoice of fairness metric, as well as how aligned the proxy groups are with the\ntrue protected groups in terms of the constrained model parameters. \n\n"}
{"id": "1806.11258", "contents": "Title: Collective decision for open set recognition Abstract: In open set recognition (OSR), almost all existing methods are designed\nspecially for recognizing individual instances, even these instances are\ncollectively coming in batch. Recognizers in decision either reject or\ncategorize them to some known class using empirically-set threshold. Thus the\ndecision threshold plays a key role. However, the selection for it usually\ndepends on the knowledge of known classes, inevitably incurring risks due to\nlacking available information from unknown classes. On the other hand, a more\nrealistic OSR system should NOT just rest on a reject decision but should go\nfurther, especially for discovering the hidden unknown classes among the reject\ninstances, whereas existing OSR methods do not pay special attention. In this\npaper, we introduce a novel collective/batch decision strategy with an aim to\nextend existing OSR for new class discovery while considering correlations\namong the testing instances. Specifically, a collective decision-based OSR\nframework (CD-OSR) is proposed by slightly modifying the Hierarchical Dirichlet\nprocess (HDP). Thanks to HDP, our CD-OSR does not need to define the decision\nthreshold and can implement the open set recognition and new class discovery\nsimultaneously. Finally, extensive experiments on benchmark datasets indicate\nthe validity of CD-OSR. \n\n"}
{"id": "1806.11302", "contents": "Title: Generate the corresponding Image from Text Description using Modified\n  GAN-CLS Algorithm Abstract: Synthesizing images or texts automatically is a useful research area in the\nartificial intelligence nowadays. Generative adversarial networks (GANs), which\nare proposed by Goodfellow in 2014, make this task to be done more efficiently\nby using deep neural networks. We consider generating corresponding images from\nan input text description using a GAN. In this paper, we analyze the GAN-CLS\nalgorithm, which is a kind of advanced method of GAN proposed by Scott Reed in\n2016. First, we find the problem with this algorithm through inference. Then we\ncorrect the GAN-CLS algorithm according to the inference by modifying the\nobjective function of the model. Finally, we do the experiments on the\nOxford-102 dataset and the CUB dataset. As a result, our modified algorithm can\ngenerate images which are more plausible than the GAN-CLS algorithm in some\ncases. Also, some of the generated images match the input texts better. \n\n"}
{"id": "1806.11518", "contents": "Title: Sparse Three-parameter Restricted Indian Buffet Process for\n  Understanding International Trade Abstract: This paper presents a Bayesian nonparametric latent feature model specially\nsuitable for exploratory analysis of high-dimensional count data. We perform a\nnon-negative doubly sparse matrix factorization that has two main advantages:\nnot only we are able to better approximate the row input distributions, but the\ninferred topics are also easier to interpret. By combining the three-parameter\nand restricted Indian buffet processes into a single prior, we increase the\nmodel flexibility, allowing for a full spectrum of sparse solutions in the\nlatent space. We demonstrate the usefulness of our approach in the analysis of\ncountries' economic structure. Compared to other approaches, empirical results\nshow our model's ability to give easy-to-interpret information and better\ncapture the underlying sparsity structure of data. \n\n"}
{"id": "1807.00263", "contents": "Title: Accurate Uncertainties for Deep Learning Using Calibrated Regression Abstract: Methods for reasoning under uncertainty are a key building block of accurate\nand reliable machine learning systems. Bayesian methods provide a general\nframework to quantify uncertainty. However, because of model misspecification\nand the use of approximate inference, Bayesian uncertainty estimates are often\ninaccurate -- for example, a 90% credible interval may not contain the true\noutcome 90% of the time. Here, we propose a simple procedure for calibrating\nany regression algorithm; when applied to Bayesian and probabilistic models, it\nis guaranteed to produce calibrated uncertainty estimates given enough data.\nOur procedure is inspired by Platt scaling and extends previous work on\nclassification. We evaluate this approach on Bayesian linear regression,\nfeedforward, and recurrent neural networks, and find that it consistently\noutputs well-calibrated credible intervals while improving performance on time\nseries forecasting and model-based reinforcement learning tasks. \n\n"}
{"id": "1807.00311", "contents": "Title: Product-based Neural Networks for User Response Prediction over\n  Multi-field Categorical Data Abstract: User response prediction is a crucial component for personalized information\nretrieval and filtering scenarios, such as recommender system and web search.\nThe data in user response prediction is mostly in a multi-field categorical\nformat and transformed into sparse representations via one-hot encoding. Due to\nthe sparsity problems in representation and optimization, most research focuses\non feature engineering and shallow modeling. Recently, deep neural networks\nhave attracted research attention on such a problem for their high capacity and\nend-to-end training scheme. In this paper, we study user response prediction in\nthe scenario of click prediction. We first analyze a coupled gradient issue in\nlatent vector-based models and propose kernel product to learn field-aware\nfeature interactions. Then we discuss an insensitive gradient issue in\nDNN-based models and propose Product-based Neural Network (PNN) which adopts a\nfeature extractor to explore feature interactions. Generalizing the kernel\nproduct to a net-in-net architecture, we further propose Product-network In\nNetwork (PIN) which can generalize previous models. Extensive experiments on 4\nindustrial datasets and 1 contest dataset demonstrate that our models\nconsistently outperform 8 baselines on both AUC and log loss. Besides, PIN\nmakes great CTR improvement (relatively 34.67%) in online A/B test. \n\n"}
{"id": "1807.00516", "contents": "Title: Balanced Distribution Adaptation for Transfer Learning Abstract: Transfer learning has achieved promising results by leveraging knowledge from\nthe source domain to annotate the target domain which has few or none labels.\nExisting methods often seek to minimize the distribution divergence between\ndomains, such as the marginal distribution, the conditional distribution or\nboth. However, these two distances are often treated equally in existing\nalgorithms, which will result in poor performance in real applications.\nMoreover, existing methods usually assume that the dataset is balanced, which\nalso limits their performances on imbalanced tasks that are quite common in\nreal problems. To tackle the distribution adaptation problem, in this paper, we\npropose a novel transfer learning approach, named as Balanced Distribution\n\\underline{A}daptation~(BDA), which can adaptively leverage the importance of\nthe marginal and conditional distribution discrepancies, and several existing\nmethods can be treated as special cases of BDA. Based on BDA, we also propose a\nnovel Weighted Balanced Distribution Adaptation~(W-BDA) algorithm to tackle the\nclass imbalance issue in transfer learning. W-BDA not only considers the\ndistribution adaptation between domains but also adaptively changes the weight\nof each class. To evaluate the proposed methods, we conduct extensive\nexperiments on several transfer learning tasks, which demonstrate the\neffectiveness of our proposed algorithms over several state-of-the-art methods. \n\n"}
{"id": "1807.00939", "contents": "Title: Mining Illegal Insider Trading of Stocks: A Proactive Approach Abstract: Illegal insider trading of stocks is based on releasing non-public\ninformation (e.g., new product launch, quarterly financial report, acquisition\nor merger plan) before the information is made public. Detecting illegal\ninsider trading is difficult due to the complex, nonlinear, and non-stationary\nnature of the stock market. In this work, we present an approach that detects\nand predicts illegal insider trading proactively from large heterogeneous\nsources of structured and unstructured data using a deep-learning based\napproach combined with discrete signal processing on the time series data. In\naddition, we use a tree-based approach that visualizes events and actions to\naid analysts in their understanding of large amounts of unstructured data.\nUsing existing data, we have discovered that our approach has a good success\nrate in detecting illegal insider trading patterns. \n\n"}
{"id": "1807.01066", "contents": "Title: Behaviour Policy Estimation in Off-Policy Policy Evaluation: Calibration\n  Matters Abstract: In this work, we consider the problem of estimating a behaviour policy for\nuse in Off-Policy Policy Evaluation (OPE) when the true behaviour policy is\nunknown. Via a series of empirical studies, we demonstrate how accurate OPE is\nstrongly dependent on the calibration of estimated behaviour policy models: how\nprecisely the behaviour policy is estimated from data. We show how powerful\nparametric models such as neural networks can result in highly uncalibrated\nbehaviour policy models on a real-world medical dataset, and illustrate how a\nsimple, non-parametric, k-nearest neighbours model produces better calibrated\nbehaviour policy estimates and can be used to obtain superior importance\nsampling-based OPE estimates. \n\n"}
{"id": "1807.01134", "contents": "Title: Welfare and Distributional Impacts of Fair Classification Abstract: Current methodologies in machine learning analyze the effects of various\nstatistical parity notions of fairness primarily in light of their impacts on\npredictive accuracy and vendor utility loss. In this paper, we propose a new\nframework for interpreting the effects of fairness criteria by converting the\nconstrained loss minimization problem into a social welfare maximization\nproblem. This translation moves a classifier and its output into utility space\nwhere individuals, groups, and society at-large experience different welfare\nchanges due to classification assignments. Under this characterization,\npredictions and fairness constraints are seen as shaping societal welfare and\ndistribution and revealing individuals' implied welfare weights in\nsociety--weights that may then be interpreted through a fairness lens. The\nsocial welfare formulation of the fairness problem brings to the fore concerns\nof distributive justice that have always had a central albeit more implicit\nrole in standard algorithmic fairness approaches. \n\n"}
{"id": "1807.02234", "contents": "Title: Distributed Self-Paced Learning in Alternating Direction Method of\n  Multipliers Abstract: Self-paced learning (SPL) mimics the cognitive process of humans, who\ngenerally learn from easy samples to hard ones. One key issue in SPL is the\ntraining process required for each instance weight depends on the other samples\nand thus cannot easily be run in a distributed manner in a large-scale dataset.\nIn this paper, we reformulate the self-paced learning problem into a\ndistributed setting and propose a novel Distributed Self-Paced Learning method\n(DSPL) to handle large-scale datasets. Specifically, both the model and\ninstance weights can be optimized in parallel for each batch based on a\nconsensus alternating direction method of multipliers. We also prove the\nconvergence of our algorithm under mild conditions. Extensive experiments on\nboth synthetic and real datasets demonstrate that our approach is superior to\nthose of existing methods. \n\n"}
{"id": "1807.02290", "contents": "Title: Differentially Private Online Submodular Optimization Abstract: In this paper we develop the first algorithms for online submodular\nminimization that preserve differential privacy under full information feedback\nand bandit feedback. A sequence of $T$ submodular functions over a collection\nof $n$ elements arrive online, and at each timestep the algorithm must choose a\nsubset of $[n]$ before seeing the function. The algorithm incurs a cost equal\nto the function evaluated on the chosen set, and seeks to choose a sequence of\nsets that achieves low expected regret.\n  Our first result is in the full information setting, where the algorithm can\nobserve the entire function after making its decision at each timestep. We give\nan algorithm in this setting that is $\\epsilon$-differentially private and\nachieves expected regret\n$\\tilde{O}\\left(\\frac{n^{3/2}\\sqrt{T}}{\\epsilon}\\right)$. This algorithm works\nby relaxing submodular function to a convex function using the Lovasz\nextension, and then simulating an algorithm for differentially private online\nconvex optimization.\n  Our second result is in the bandit setting, where the algorithm can only see\nthe cost incurred by its chosen set, and does not have access to the entire\nfunction. This setting is significantly more challenging because the algorithm\ndoes not receive enough information to compute the Lovasz extension or its\nsubgradients. Instead, we construct an unbiased estimate using a single-point\nestimation, and then simulate private online convex optimization using this\nestimate. Our algorithm using bandit feedback is $\\epsilon$-differentially\nprivate and achieves expected regret\n$\\tilde{O}\\left(\\frac{n^{3/2}T^{3/4}}{\\epsilon}\\right)$. \n\n"}
{"id": "1807.02515", "contents": "Title: Blockchain as a Service: A Decentralized and Secure Computing Paradigm Abstract: Thanks to the advances in machine learning, data-driven analysis tools have\nbecome valuable solutions for various applications. However, there still remain\nessential challenges to develop effective data-driven methods because of the\nneed to acquire a large amount of data and to have sufficient computing power\nto handle the data. In many instances these challenges are addressed by relying\non a dominant cloud computing vendor, but, although commercial cloud vendors\nprovide valuable platforms for data analytics, they can suffer from a lack of\ntransparency, security, and privacy-perservation. Furthermore, reliance on\ncloud servers prevents applying big data analytics in environments where the\ncomputing power is scattered. To address these challenges, a decentralize,\nsecure, and privacy-preserving computing paradigm is proposed to enable an\nasynchronized cooperative computing process amongst scattered and untrustworthy\ncomputing nodes that may have limited computing power and computing\nintelligence. This paradigm is designed by exploring blockchain, decentralized\nlearning, homomorphic encryption, and software defined networking(SDN)\ntechniques. The performance of the proposed paradigm is evaluated via different\nscenarios in the simulation section. \n\n"}
{"id": "1807.02609", "contents": "Title: Anytime Neural Prediction via Slicing Networks Vertically Abstract: The pioneer deep neural networks (DNNs) have emerged to be deeper or wider\nfor improving their accuracy in various applications of artificial\nintelligence. However, DNNs are often too heavy to deploy in practice, and it\nis often required to control their architectures dynamically given computing\nresource budget, i.e., anytime prediction. While most existing approaches have\nfocused on training multiple shallow sub-networks jointly, we study training\nthin sub-networks instead. To this end, we first build many inclusive thin\nsub-networks (of the same depth) under a minor modification of existing\nmulti-branch DNNs, and found that they can significantly outperform the\nstate-of-art dense architecture for anytime prediction. This is remarkable due\nto their simplicity and effectiveness, but training many thin sub-networks\njointly faces a new challenge on training complexity. To address the issue, we\nalso propose a novel DNN architecture by forcing a certain sparsity pattern on\nmulti-branch network parameters, making them train efficiently for the purpose\nof anytime prediction. In our experiments on the ImageNet dataset, its\nsub-networks have up to $43.3\\%$ smaller sizes (FLOPs) compared to those of the\nstate-of-art anytime model with respect to the same accuracy. Finally, we also\npropose an alternative task under the proposed architecture using a\nhierarchical taxonomy, which brings a new angle for anytime prediction. \n\n"}
{"id": "1807.02839", "contents": "Title: Hierarchical stochastic graphlet embedding for graph-based pattern\n  recognition Abstract: Despite being very successful within the pattern recognition and machine\nlearning community, graph-based methods are often unusable because of the lack\nof mathematical operations defined in graph domain. Graph embedding, which maps\ngraphs to a vectorial space, has been proposed as a way to tackle these\ndifficulties enabling the use of standard machine learning techniques. However,\nit is well known that graph embedding functions usually suffer from the loss of\nstructural information. In this paper, we consider the hierarchical structure\nof a graph as a way to mitigate this loss of information. The hierarchical\nstructure is constructed by topologically clustering the graph nodes, and\nconsidering each cluster as a node in the upper hierarchical level. Once this\nhierarchical structure is constructed, we consider several configurations to\ndefine the mapping into a vector space given a classical graph embedding, in\nparticular, we propose to make use of the Stochastic Graphlet Embedding (SGE).\nBroadly speaking, SGE produces a distribution of uniformly sampled low to high\norder graphlets as a way to embed graphs into the vector space. In what\nfollows, the coarse-to-fine structure of a graph hierarchy and the statistics\nfetched by the SGE complements each other and includes important structural\ninformation with varied contexts. Altogether, these two techniques\nsubstantially cope with the usual information loss involved in graph embedding\ntechniques, obtaining a more robust graph representation. This fact has been\ncorroborated through a detailed experimental evaluation on various benchmark\ngraph datasets, where we outperform the state-of-the-art methods. \n\n"}
{"id": "1807.02910", "contents": "Title: Model Agnostic Supervised Local Explanations Abstract: Model interpretability is an increasingly important component of practical\nmachine learning. Some of the most common forms of interpretability systems are\nexample-based, local, and global explanations. One of the main challenges in\ninterpretability is designing explanation systems that can capture aspects of\neach of these explanation types, in order to develop a more thorough\nunderstanding of the model. We address this challenge in a novel model called\nMAPLE that uses local linear modeling techniques along with a dual\ninterpretation of random forests (both as a supervised neighborhood approach\nand as a feature selection method). MAPLE has two fundamental advantages over\nexisting interpretability systems. First, while it is effective as a black-box\nexplanation system, MAPLE itself is a highly accurate predictive model that\nprovides faithful self explanations, and thus sidesteps the typical\naccuracy-interpretability trade-off. Specifically, we demonstrate, on several\nUCI datasets, that MAPLE is at least as accurate as random forests and that it\nproduces more faithful local explanations than LIME, a popular interpretability\nsystem. Second, MAPLE provides both example-based and local explanations and\ncan detect global patterns, which allows it to diagnose limitations in its\nlocal explanations. \n\n"}
{"id": "1807.03095", "contents": "Title: Mammography Assessment using Multi-Scale Deep Classifiers Abstract: Applying deep learning methods to mammography assessment has remained a\nchallenging topic. Dense noise with sparse expressions, mega-pixel raw data\nresolution, lack of diverse examples have all been factors affecting\nperformance. The lack of pixel-level ground truths have especially limited\nsegmentation methods in pushing beyond approximately bounding regions. We\npropose a classification approach grounded in high performance tissue\nassessment as an alternative to all-in-one localization and assessment models\nthat is also capable of pinpointing the causal pixels. First, the objective of\nthe mammography assessment task is formalized in the context of local tissue\nclassifiers. Then, the accuracy of a convolutional neural net is evaluated on\nclassifying patches of tissue with suspicious findings at varying scales, where\nhighest obtained AUC is above $0.9$. The local evaluations of one such expert\ntissue classifier is used to augment the results of a heatmap regression model\nand additionally recover the exact causal regions at high resolution as a\nsaliency image suitable for clinical settings. \n\n"}
{"id": "1807.03142", "contents": "Title: Faster Bounding Box Annotation for Object Detection in Indoor Scenes Abstract: This paper proposes an approach for rapid bounding box annotation for object\ndetection datasets. The procedure consists of two stages: The first step is to\nannotate a part of the dataset manually, and the second step proposes\nannotations for the remaining samples using a model trained with the first\nstage annotations. We experimentally study which first/second stage split\nminimizes to total workload. In addition, we introduce a new fully labeled\nobject detection dataset collected from indoor scenes. Compared to other indoor\ndatasets, our collection has more class categories, different backgrounds,\nlighting conditions, occlusion and high intra-class differences. We train deep\nlearning based object detectors with a number of state-of-the-art models and\ncompare them in terms of speed and accuracy. The fully annotated dataset is\nreleased freely available for the research community. \n\n"}
{"id": "1807.03610", "contents": "Title: Window Opening Model using Deep Learning Methods Abstract: Occupant behavior (OB) and in particular window openings need to be\nconsidered in building performance simulation (BPS), in order to realistically\nmodel the indoor climate and energy consumption for heating ventilation and air\nconditioning (HVAC). However, the proposed OB window opening models are often\nbiased towards the over-represented class where windows remained closed. In\naddition, they require tuning for each occupant which can not be efficiently\nscaled to the increased number of occupants. This paper presents a window\nopening model for commercial buildings using deep learning methods. The model\nis trained using data from occupants from an office building in Germany. In\ntotal the model is evaluated using almost 20 mio. data points from 3\nindependent buildings, located in Aachen, Frankfurt and Philadelphia.\nEventually, the results of 3100 core hours of model development are summarized,\nwhich makes this study the largest of its kind in window states modeling.\nAdditionally, the practical potential of the proposed model was tested by\nincorporating it in the Modelica-based thermal building simulation. The\nresulting evaluation accuracy and F1 scores on the office buildings ranged\nbetween 86-89 % and 0.53-0.65 respectively. The performance dropped around 15 %\npoints in case of sparse input data, while the F1 score remained high. \n\n"}
{"id": "1807.04015", "contents": "Title: On Catastrophic Forgetting and Mode Collapse in Generative Adversarial\n  Networks Abstract: In this paper, we show that Generative Adversarial Networks (GANs) suffer\nfrom catastrophic forgetting even when they are trained to approximate a single\ntarget distribution. We show that GAN training is a continual learning problem\nin which the sequence of changing model distributions is the sequence of tasks\nto the discriminator. The level of mismatch between tasks in the sequence\ndetermines the level of forgetting. Catastrophic forgetting is interrelated to\nmode collapse and can make the training of GANs non-convergent. We investigate\nthe landscape of the discriminator's output in different variants of GANs and\nfind that when a GAN converges to a good equilibrium, real training datapoints\nare wide local maxima of the discriminator. We empirically show the\nrelationship between the sharpness of local maxima and mode collapse and\ngeneralization in GANs. We show how catastrophic forgetting prevents the\ndiscriminator from making real datapoints local maxima, and thus causes\nnon-convergence. Finally, we study methods for preventing catastrophic\nforgetting in GANs. \n\n"}
{"id": "1807.04241", "contents": "Title: DeepMove: Learning Place Representations through Large Scale Movement\n  Data Abstract: Understanding and reasoning about places and their relationships are critical\nfor many applications. Places are traditionally curated by a small group of\npeople as place gazetteers and are represented by an ID with spatial extent,\ncategory, and other descriptions. However, a place context is described to a\nlarge extent by movements made from/to other places. Places are linked and\nrelated to each other by these movements. This important context is missing\nfrom the traditional representation.\n  We present DeepMove, a novel approach for learning latent representations of\nplaces. DeepMove advances the current deep learning based place representations\nby directly model movements between places. We demonstrate DeepMove's latent\nrepresentations on place categorization and clustering tasks on large place and\nmovement datasets with respect to important parameters. Our results show that\nDeepMove outperforms state-of-the-art baselines. DeepMove's representations can\nprovide up to 15% higher than competing methods in matching rate of place\ncategory and result in up to 39% higher silhouette coefficient value for place\nclusters.\n  DeepMove is spatial and temporal context aware. It is scalable. It\noutperforms competing models using much smaller training dataset (a month or\n1/12 of data). These qualities make it suitable for a broad class of real-world\napplications. \n\n"}
{"id": "1807.04428", "contents": "Title: Convergence Rate of Block-Coordinate Maximization Burer-Monteiro Method\n  for Solving Large SDPs Abstract: Semidefinite programming (SDP) with diagonal constraints arise in many\noptimization problems, such as Max-Cut, community detection and group\nsynchronization. Although SDPs can be solved to arbitrary precision in\npolynomial time, generic convex solvers do not scale well with the dimension of\nthe problem. In order to address this issue, Burer and Monteiro proposed to\nreduce the dimension of the problem by appealing to a low-rank factorization\nand solve the subsequent non-convex problem instead. In this paper, we present\ncoordinate ascent based methods to solve this non-convex problem with provable\nconvergence guarantees. More specifically, we prove that the block-coordinate\nmaximization algorithm applied to the non-convex Burer-Monteiro method globally\nconverges to a first-order stationary point with a sublinear rate without any\nassumptions on the problem. We further show that this algorithm converges\nlinearly around a local maximum provided that the objective function exhibits\nquadratic decay. We establish that this condition generically holds when the\nrank of the factorization is sufficiently large. Furthermore, incorporating\nLanczos method to the block-coordinate maximization, we propose an algorithm\nthat is guaranteed to return a solution that provides $1-O(1/r)$ approximation\nto the original SDP without any assumptions, where $r$ is the rank of the\nfactorization. This approximation ratio is known to be optimal (up to\nconstants) under the unique games conjecture, and we can explicitly quantify\nthe number of iterations to obtain such a solution. \n\n"}
{"id": "1807.04680", "contents": "Title: Unseeded low-rank graph matching by transform-based unsupervised point\n  registration Abstract: The problem of learning a correspondence relationship between nodes of two\nnetworks has drawn much attention of the computer science community and\nrecently that of statisticians. The unseeded version of this problem, in which\nwe do not know any part of the true correspondence, is a long-standing\nchallenge. For low-rank networks, the problem can be translated into an\nunsupervised point registration problem, in which two point sets generated from\nthe same distribution are matchable by an unknown orthonormal transformation.\nConventional methods generally lack consistency guarantee and are usually\ncomputationally costly.\n  In this paper, we propose a novel approach to this problem. Instead of\nsimultaneously estimating the unknown correspondence and orthonormal\ntransformation to match up the two point sets, we match their distributions via\nminimizing our designed loss function capturing the discrepancy between their\nLaplace transforms, thus avoiding the optimization over all possible\ncorrespondences. This dramatically reduces the dimension of the optimization\nproblem from $\\Omega(n^2)$ parameters to $O(d^2)$ parameters, where $d$ is the\nfixed rank, and enables convenient theoretical analysis. In this paper, we\nprovide arguably the first consistency guarantee and explicit error rate for\ngeneral low-rank models. Our method provides control over the computational\ncomplexity ranging from $\\omega(n)$ (any growth rate faster than $n$) to\n$O(n^2)$ while pertaining consistency. We demonstrate the effectiveness of our\nmethod through several numerical examples. \n\n"}
{"id": "1807.04734", "contents": "Title: Scalable Convolutional Dictionary Learning with Constrained Recurrent\n  Sparse Auto-encoders Abstract: Given a convolutional dictionary underlying a set of observed signals, can a\ncarefully designed auto-encoder recover the dictionary in the presence of\nnoise? We introduce an auto-encoder architecture, termed constrained recurrent\nsparse auto-encoder (CRsAE), that answers this question in the affirmative.\nGiven an input signal and an approximate dictionary, the encoder finds a sparse\napproximation using FISTA. The decoder reconstructs the signal by applying the\ndictionary to the output of the encoder. The encoder and decoder in CRsAE\nparallel the sparse-coding and dictionary update steps in optimization-based\nalternating-minimization schemes for dictionary learning. As such, the\nparameters of the encoder and decoder are not independent, a constraint which\nwe enforce for the first time. We derive the back-propagation algorithm for\nCRsAE. CRsAE is a framework for blind source separation that, only knowing the\nnumber of sources (dictionary elements), and assuming sparsely-many can\noverlap, is able to separate them. We demonstrate its utility in the context of\nspike sorting, a source separation problem in computational neuroscience. We\ndemonstrate the ability of CRsAE to recover the underlying dictionary and\ncharacterize its sensitivity as a function of SNR. \n\n"}
{"id": "1807.04740", "contents": "Title: Negative Momentum for Improved Game Dynamics Abstract: Games generalize the single-objective optimization paradigm by introducing\ndifferent objective functions for different players. Differentiable games often\nproceed by simultaneous or alternating gradient updates. In machine learning,\ngames are gaining new importance through formulations like generative\nadversarial networks (GANs) and actor-critic systems. However, compared to\nsingle-objective optimization, game dynamics are more complex and less\nunderstood. In this paper, we analyze gradient-based methods with momentum on\nsimple games. We prove that alternating updates are more stable than\nsimultaneous updates. Next, we show both theoretically and empirically that\nalternating gradient updates with a negative momentum term achieves convergence\nin a difficult toy adversarial problem, but also on the notoriously difficult\nto train saturating GANs. \n\n"}
{"id": "1807.05836", "contents": "Title: Forecasting market states Abstract: We propose a novel methodology to define, analyze and forecast market states.\nIn our approach market states are identified by a reference sparse precision\nmatrix and a vector of expectation values. In our procedure, each multivariate\nobservation is associated with a given market state accordingly to a\nminimization of a penalized Mahalanobis distance. The procedure is made\ncomputationally very efficient and can be used with a large number of assets.\nWe demonstrate that this procedure is successful at clustering different states\nof the markets in an unsupervised manner. In particular, we describe an\nexperiment with one hundred log-returns and two states in which the methodology\nautomatically associates states prevalently to pre- and post- crisis periods\nwith one state gathering periods with average positive returns and the other\nstate periods with average negative returns, therefore discovering\nspontaneously the common classification of `bull' and `bear' markets. In\nanother experiment, with again one hundred log-returns and two states, we\ndemonstrate that this procedure can be efficiently used to forecast off-sample\nfuture market states with significant prediction accuracy. This methodology\nopens the way to a range of applications in risk management and trading\nstrategies in the context where the correlation structure plays a central role. \n\n"}
{"id": "1807.06036", "contents": "Title: Pangloss: Fast Entity Linking in Noisy Text Environments Abstract: Entity linking is the task of mapping potentially ambiguous terms in text to\ntheir constituent entities in a knowledge base like Wikipedia. This is useful\nfor organizing content, extracting structured data from textual documents, and\nin machine learning relevance applications like semantic search, knowledge\ngraph construction, and question answering. Traditionally, this work has\nfocused on text that has been well-formed, like news articles, but in common\nreal world datasets such as messaging, resumes, or short-form social media,\nnon-grammatical, loosely-structured text adds a new dimension to this problem.\n  This paper presents Pangloss, a production system for entity disambiguation\non noisy text. Pangloss combines a probabilistic linear-time key phrase\nidentification algorithm with a semantic similarity engine based on\ncontext-dependent document embeddings to achieve better than state-of-the-art\nresults (>5% in F1) compared to other research or commercially available\nsystems. In addition, Pangloss leverages a local embedded database with a\ntiered architecture to house its statistics and metadata, which allows rapid\ndisambiguation in streaming contexts and on-device disambiguation in low-memory\nenvironments such as mobile phones. \n\n"}
{"id": "1807.06473", "contents": "Title: Contextual Memory Trees Abstract: We design and study a Contextual Memory Tree (CMT), a learning memory\ncontroller that inserts new memories into an experience store of unbounded\nsize. It is designed to efficiently query for memories from that store,\nsupporting logarithmic time insertion and retrieval operations. Hence CMT can\nbe integrated into existing statistical learning algorithms as an augmented\nmemory unit without substantially increasing training and inference\ncomputation. Furthermore CMT operates as a reduction to classification,\nallowing it to benefit from advances in representation or architecture. We\ndemonstrate the efficacy of CMT by augmenting existing multi-class and\nmulti-label classification algorithms with CMT and observe statistical\nimprovement. We also test CMT learning on several image-captioning tasks to\ndemonstrate that it performs computationally better than a simple nearest\nneighbors memory system while benefitting from reward learning. \n\n"}
{"id": "1807.06481", "contents": "Title: Dynamic Sampling from Graphical Models Abstract: In this paper, we study the problem of sampling from a graphical model when\nthe model itself is changing dynamically with time. This problem derives its\ninterest from a variety of inference, learning, and sampling settings in\nmachine learning, computer vision, statistical physics, and theoretical\ncomputer science. While the problem of sampling from a static graphical model\nhas received considerable attention, theoretical works for its dynamic variants\nhave been largely lacking. The main contribution of this paper is an algorithm\nthat can sample dynamically from a broad class of graphical models over\ndiscrete random variables. Our algorithm is parallel and Las Vegas: it knows\nwhen to stop and it outputs samples from the exact distribution. We also\nprovide sufficient conditions under which this algorithm runs in time\nproportional to the size of the update, on general graphical models as well as\nwell-studied specific spin systems. In particular we obtain, for the Ising\nmodel (ferromagnetic or anti-ferromagnetic) and for the hardcore model the\nfirst dynamic sampling algorithms that can handle both edge and vertex updates\n(addition, deletion, change of functions), both efficient within regimes that\nare close to the respective uniqueness regimes, beyond which, even for the\nstatic and approximate sampling, no local algorithms were known or the problem\nitself is intractable. Our dynamic sampling algorithm relies on a local\nresampling algorithm and a new \"equilibrium\" property that is shown to be\nsatisfied by our algorithm at each step, and enables us to prove its\ncorrectness. This equilibrium property is robust enough to guarantee the\ncorrectness of our algorithm, helps us improve bounds on fast convergence on\nspecific models, and should be of independent interest. \n\n"}
{"id": "1807.06555", "contents": "Title: Training Recurrent Neural Networks against Noisy Computations during\n  Inference Abstract: We explore the robustness of recurrent neural networks when the computations\nwithin the network are noisy. One of the motivations for looking into this\nproblem is to reduce the high power cost of conventional computing of neural\nnetwork operations through the use of analog neuromorphic circuits. Traditional\nGPU/CPU-centered deep learning architectures exhibit bottlenecks in\npower-restricted applications, such as speech recognition in embedded systems.\nThe use of specialized neuromorphic circuits, where analog signals passed\nthrough memory-cell arrays are sensed to accomplish matrix-vector\nmultiplications, promises large power savings and speed gains but brings with\nit the problems of limited precision of computations and unavoidable analog\nnoise.\n  In this paper we propose a method, called {\\em Deep Noise Injection\ntraining}, to train RNNs to obtain a set of weights/biases that is much more\nrobust against noisy computation during inference. We explore several RNN\narchitectures, such as vanilla RNN and long-short-term memories (LSTM), and\nshow that after convergence of Deep Noise Injection training the set of trained\nweights/biases has more consistent performance over a wide range of noise\npowers entering the network during inference. Surprisingly, we find that Deep\nNoise Injection training improves overall performance of some networks even for\nnumerically accurate inference. \n\n"}
{"id": "1807.06560", "contents": "Title: Using link and content over time for embedding generation in Dynamic\n  Attributed Networks Abstract: In this work, we consider the problem of combining link, content and temporal\nanalysis for community detection and prediction in evolving networks. Such\ntemporal and content-rich networks occur in many real-life settings, such as\nbibliographic networks and question answering forums. Most of the work in the\nliterature (that uses both content and structure) deals with static snapshots\nof networks, and they do not reflect the dynamic changes occurring over\nmultiple snapshots. Incorporating dynamic changes in the communities into the\nanalysis can also provide useful insights about the changes in the network such\nas the migration of authors across communities. In this work, we propose\nChimera, a shared factorization model that can simultaneously account for graph\nlinks, content, and temporal analysis. This approach works by extracting the\nlatent semantic structure of the network in multidimensional form, but in a way\nthat takes into account the temporal continuity of these embeddings. Such an\napproach simplifies temporal analysis of the underlying network by using the\nembedding as a surrogate. A consequence of this simplification is that it is\nalso possible to use this temporal sequence of embeddings to predict future\ncommunities. We present experimental results illustrating the effectiveness of\nthe approach. \n\n"}
{"id": "1807.06576", "contents": "Title: Comparison of RNN Encoder-Decoder Models for Anomaly Detection Abstract: In this paper, we compare different types of Recurrent Neural Network (RNN)\nEncoder-Decoders in anomaly detection viewpoint. We focused on finding the\nmodel that can learn the same data more effectively. We compared multiple\nmodels under the same conditions, such as the number of parameters, optimizer,\nand learning rate. However, the difference is whether to predict the future\nsequence or restore the current sequence. We constructed the dataset with\nsimple vectors and used them for the experiment. Finally, we experimentally\nconfirmed that the model performs better when the model restores the current\nsequence, rather than predict the future sequence. \n\n"}
{"id": "1807.06650", "contents": "Title: Generative adversarial interpolative autoencoding: adversarial training\n  on latent space interpolations encourage convex latent distributions Abstract: We present a neural network architecture based upon the Autoencoder (AE) and\nGenerative Adversarial Network (GAN) that promotes a convex latent distribution\nby training adversarially on latent space interpolations. By using an AE as\nboth the generator and discriminator of a GAN, we pass a pixel-wise error\nfunction across the discriminator, yielding an AE which produces non-blurry\nsamples that match both high- and low-level features of the original images.\nInterpolations between images in this space remain within the latent-space\ndistribution of real images as trained by the discriminator, and therfore\npreserve realistic resemblances to the network inputs. Code available at\nhttps://github.com/timsainb/GAIA \n\n"}
{"id": "1807.06657", "contents": "Title: Airline Passenger Name Record Generation using Generative Adversarial\n  Networks Abstract: Passenger Name Records (PNRs) are at the heart of the travel industry.\nCreated when an itinerary is booked, they contain travel and passenger\ninformation. It is usual for airlines and other actors in the industry to\ninter-exchange and access each other's PNR, creating the challenge of using\nthem without infringing data ownership laws. To address this difficulty, we\npropose a method to generate realistic synthetic PNRs using Generative\nAdversarial Networks (GANs). Unlike other GAN applications, PNRs consist of\ncategorical and numerical features with missing/NaN values, which makes the use\nof GANs challenging. We propose a solution based on Cram\\'{e}r GANs,\ncategorical feature embedding and a Cross-Net architecture. The method was\ntested on a real PNR dataset, and evaluated in terms of distribution matching,\nmemorization, and performance of predictive models for two real business\nproblems: client segmentation and passenger nationality prediction. Results\nshow that the generated data matches well with the real PNRs without memorizing\nthem, and that it can be used to train models for real business applications. \n\n"}
{"id": "1807.07217", "contents": "Title: Deconfounding age effects with fair representation learning when\n  assessing dementia Abstract: One of the most prevalent symptoms among the elderly population, dementia,\ncan be detected by classifiers trained on linguistic features extracted from\nnarrative transcripts. However, these linguistic features are impacted in a\nsimilar but different fashion by the normal aging process. Aging is therefore a\nconfounding factor, whose effects have been hard for machine learning\nclassifiers (especially deep neural network based models) to ignore. We show\nDNN models are capable of estimating ages based on linguistic features.\nPredicting dementia based on this aging bias could lead to potentially\nnon-generalizable accuracies on clinical datasets, if not properly\ndeconfounded.\n  In this paper, we propose to address this deconfounding problem with fair\nrepresentation learning. We build neural network classifiers that learn\nlow-dimensional representations reflecting the impacts of dementia yet\ndiscarding the effects of age. To evaluate these classifiers, we specify a\nmodel-agnostic score $\\Delta_{eo}^{(N)}$ measuring how classifier results are\ndeconfounded from age. Our best models compromise accuracy by only 2.56\\% and\n1.54\\% on two clinical datasets compared to DNNs, and their $\\Delta_{eo}^{(2)}$\nscores are better than statistical (residulization and inverse probability\nweight) adjustments. \n\n"}
{"id": "1807.07291", "contents": "Title: Online Label Aggregation: A Variational Bayesian Approach Abstract: Noisy labeled data is more a norm than a rarity for crowd sourced contents.\nIt is effective to distill noise and infer correct labels through aggregation\nresults from crowd workers. To ensure the time relevance and overcome slow\nresponses of workers, online label aggregation is increasingly requested,\ncalling for solutions that can incrementally infer true label distribution via\nsubsets of data items. In this paper, we propose a novel online label\naggregation framework, BiLA, which employs variational Bayesian inference\nmethod and designs a novel stochastic optimization scheme for incremental\ntraining. BiLA is flexible to accommodate any generating distribution of labels\nby the exact computation of its posterior distribution. We also derive the\nconvergence bound of the proposed optimizer. We compare BiLA with the state of\nthe art based on minimax entropy, neural networks and expectation maximization\nalgorithms, on synthetic and real-world data sets. Our evaluation results on\nvarious online scenarios show that BiLA can effectively infer the true labels,\nwith an error rate reduction of at least 10 to 1.5 percent points for synthetic\nand real-world datasets, respectively. \n\n"}
{"id": "1807.07506", "contents": "Title: Improving Simple Models with Confidence Profiles Abstract: In this paper, we propose a new method called ProfWeight for transferring\ninformation from a pre-trained deep neural network that has a high test\naccuracy to a simpler interpretable model or a very shallow network of low\ncomplexity and a priori low test accuracy. We are motivated by applications in\ninterpretability and model deployment in severely memory constrained\nenvironments (like sensors). Our method uses linear probes to generate\nconfidence scores through flattened intermediate representations. Our transfer\nmethod involves a theoretically justified weighting of samples during the\ntraining of the simple model using confidence scores of these intermediate\nlayers. The value of our method is first demonstrated on CIFAR-10, where our\nweighting method significantly improves (3-4%) networks with only a fraction of\nthe number of Resnet blocks of a complex Resnet model. We further demonstrate\noperationally significant results on a real manufacturing problem, where we\ndramatically increase the test accuracy of a CART model (the domain standard)\nby roughly 13%. \n\n"}
{"id": "1807.07543", "contents": "Title: Understanding and Improving Interpolation in Autoencoders via an\n  Adversarial Regularizer Abstract: Autoencoders provide a powerful framework for learning compressed\nrepresentations by encoding all of the information needed to reconstruct a data\npoint in a latent code. In some cases, autoencoders can \"interpolate\": By\ndecoding the convex combination of the latent codes for two datapoints, the\nautoencoder can produce an output which semantically mixes characteristics from\nthe datapoints. In this paper, we propose a regularization procedure which\nencourages interpolated outputs to appear more realistic by fooling a critic\nnetwork which has been trained to recover the mixing coefficient from\ninterpolated data. We then develop a simple benchmark task where we can\nquantitatively measure the extent to which various autoencoders can interpolate\nand show that our regularizer dramatically improves interpolation in this\nsetting. We also demonstrate empirically that our regularizer produces latent\ncodes which are more effective on downstream tasks, suggesting a possible link\nbetween interpolation abilities and learning useful representations. \n\n"}
{"id": "1807.07998", "contents": "Title: Convolutional Neural Networks Analyzed via Inverse Problem Theory and\n  Sparse Representations Abstract: Inverse problems in imaging such as denoising, deblurring, superresolution\n(SR) have been addressed for many decades. In recent years, convolutional\nneural networks (CNNs) have been widely used for many inverse problem areas.\nAlthough their indisputable success, CNNs are not mathematically validated as\nto how and what they learn. In this paper, we prove that during training, CNN\nelements solve for inverse problems which are optimum solutions stored as CNN\nneuron filters. We discuss the necessity of mutual coherence between CNN layer\nelements in order for a network to converge to the optimum solution. We prove\nthat required mutual coherence can be provided by the usage of residual\nlearning and skip connections. We have set rules over training sets and depth\nof networks for better convergence, i.e. performance. \n\n"}
{"id": "1807.08108", "contents": "Title: Simultaneous Adversarial Training - Learn from Others Mistakes Abstract: Adversarial examples are maliciously tweaked images that can easily fool\nmachine learning techniques, such as neural networks, but they are normally not\nvisually distinguishable for human beings. One of the main approaches to solve\nthis problem is to retrain the networks using those adversarial examples,\nnamely adversarial training. However, standard adversarial training might not\nactually change the decision boundaries but cause the problem of gradient\nmasking, resulting in a weaker ability to generate adversarial examples.\nTherefore, it cannot alleviate the problem of black-box attacks, where\nadversarial examples generated from other networks can transfer to the targeted\none. In order to reduce the problem of black-box attacks, we propose a novel\nmethod that allows two networks to learn from each others' adversarial examples\nand become resilient to black-box attacks. We also combine this method with a\nsimple domain adaptation to further improve the performance. \n\n"}
{"id": "1807.08518", "contents": "Title: Implementing Neural Turing Machines Abstract: Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural\nNetworks, a new class of recurrent neural networks which decouple computation\nfrom memory by introducing an external memory unit. NTMs have demonstrated\nsuperior performance over Long Short-Term Memory Cells in several sequence\nlearning tasks. A number of open source implementations of NTMs exist but are\nunstable during training and/or fail to replicate the reported performance of\nNTMs. This paper presents the details of our successful implementation of a\nNTM. Our implementation learns to solve three sequential learning tasks from\nthe original NTM paper. We find that the choice of memory contents\ninitialization scheme is crucial in successfully implementing a NTM. Networks\nwith memory contents initialized to small constant values converge on average 2\ntimes faster than the next best memory contents initialization scheme. \n\n"}
{"id": "1807.08596", "contents": "Title: Recent Advances in Convolutional Neural Network Acceleration Abstract: In recent years, convolutional neural networks (CNNs) have shown great\nperformance in various fields such as image classification, pattern\nrecognition, and multi-media compression. Two of the feature properties, local\nconnectivity and weight sharing, can reduce the number of parameters and\nincrease processing speed during training and inference. However, as the\ndimension of data becomes higher and the CNN architecture becomes more\ncomplicated, the end-to-end approach or the combined manner of CNN is\ncomputationally intensive, which becomes limitation to CNN's further\nimplementation. Therefore, it is necessary and urgent to implement CNN in a\nfaster way. In this paper, we first summarize the acceleration methods that\ncontribute to but not limited to CNN by reviewing a broad variety of research\npapers. We propose a taxonomy in terms of three levels, i.e.~structure level,\nalgorithm level, and implementation level, for acceleration methods. We also\nanalyze the acceleration methods in terms of CNN architecture compression,\nalgorithm optimization, and hardware-based improvement. At last, we give a\ndiscussion on different perspectives of these acceleration and optimization\nmethods within each level. The discussion shows that the methods in each level\nstill have large exploration space. By incorporating such a wide range of\ndisciplines, we expect to provide a comprehensive reference for researchers who\nare interested in CNN acceleration. \n\n"}
{"id": "1807.08844", "contents": "Title: Lesion segmentation using U-Net network Abstract: This paper explains the method used in the segmentation challenge (Task 1) in\nthe International Skin Imaging Collaboration's (ISIC) Skin Lesion Analysis\nTowards Melanoma Detection challenge held in 2018. We have trained a U-Net\nnetwork to perform the segmentation. The key elements for the training were\nfirst to adjust the loss function to incorporate unbalanced proportion of\nbackground and second to perform post-processing operation to adjust the\ncontour of the prediction. \n\n"}
{"id": "1807.08993", "contents": "Title: Deep-CLASS at ISIC Machine Learning Challenge 2018 Abstract: This paper reports the method and evaluation results of MedAusbild team for\nISIC challenge task. Since early 2017, our team has worked on melanoma\nclassification [1][6], and has employed deep learning since beginning of 2018\n[7]. Deep learning helps researchers absolutely to treat and detect diseases by\nanalyzing medical data (e.g., medical images). One of the representative models\namong the various deep-learning models is a convolutional neural network (CNN).\nAlthough our team has an experience with segmentation and classification of\nbenign and malignant skin-lesions, we have participated in the task 3 of ISIC\nChallenge 2018 for classification of seven skin diseases, explained in this\npaper. \n\n"}
{"id": "1807.09356", "contents": "Title: Iterative Amortized Inference Abstract: Inference models are a key component in scaling variational inference to deep\nlatent variable models, most notably as encoder networks in variational\nauto-encoders (VAEs). By replacing conventional optimization-based inference\nwith a learned model, inference is amortized over data examples and therefore\nmore computationally efficient. However, standard inference models are\nrestricted to direct mappings from data to approximate posterior estimates. The\nfailure of these models to reach fully optimized approximate posterior\nestimates results in an amortization gap. We aim toward closing this gap by\nproposing iterative inference models, which learn to perform inference\noptimization through repeatedly encoding gradients. Our approach generalizes\nstandard inference models in VAEs and provides insight into several empirical\nfindings, including top-down inference techniques. We demonstrate the inference\noptimization capabilities of iterative inference models and show that they\noutperform standard inference models on several benchmark data sets of images\nand text. \n\n"}
{"id": "1807.09586", "contents": "Title: Perturb and Combine to Identify Influential Spreaders in Real-World\n  Networks Abstract: Some of the most effective influential spreader detection algorithms are\nunstable to small perturbations of the network structure. Inspired by bagging\nin Machine Learning, we propose the first Perturb and Combine (P&C) procedure\nfor networks. It (1) creates many perturbed versions of a given graph, (2)\napplies a node scoring function separately to each graph, and (3) combines the\nresults. Experiments conducted on real-world networks of various sizes with the\nk-core, generalized k-core, and PageRank algorithms reveal that P&C brings\nsubstantial improvements. Moreover, this performance boost can be obtained at\nalmost no extra cost through parallelization. Finally, a bias-variance analysis\nsuggests that P&C works mainly by reducing bias, and that therefore, it should\nbe capable of improving the performance of all vertex scoring functions,\nincluding stable ones. \n\n"}
{"id": "1807.09596", "contents": "Title: Contextual Stochastic Block Models Abstract: We provide the first information theoretic tight analysis for inference of\nlatent community structure given a sparse graph along with high dimensional\nnode covariates, correlated with the same latent communities. Our work bridges\nrecent theoretical breakthroughs in the detection of latent community structure\nwithout nodes covariates and a large body of empirical work using diverse\nheuristics for combining node covariates with graphs for inference. The\ntightness of our analysis implies in particular, the information theoretical\nnecessity of combining the different sources of information. Our analysis holds\nfor networks of large degrees as well as for a Gaussian version of the model. \n\n"}
{"id": "1807.09751", "contents": "Title: Multi-Perspective Neural Architecture for Recommendation System Abstract: Currently, there starts a research trend to leverage neural architecture for\nrecommendation systems. Though several deep recommender models are proposed,\nmost methods are too simple to characterize users' complex preference. In this\npaper, for a fine-grain analysis, users' ratings are explained from multiple\nperspectives, based on which, we propose our neural architecture. Specifically,\nour model employs several sequential stages to encode the user and item into\nhidden representations. In one stage, the user and item are represented from\nmultiple perspectives and in each perspective, the representations of user and\nitem put attentions to each other. Last, we metric the output representations\nof final stage to approach the users' rating. Extensive experiments demonstrate\nthat our method achieves substantial improvements against baselines. \n\n"}
{"id": "1807.09834", "contents": "Title: Applying Domain Randomization to Synthetic Data for Object Category\n  Detection Abstract: Recent advances in deep learning-based object detection techniques have\nrevolutionized their applicability in several fields. However, since these\nmethods rely on unwieldy and large amounts of data, a common practice is to\ndownload models pre-trained on standard datasets and fine-tune them for\nspecific application domains with a small set of domain relevant images. In\nthis work, we show that using synthetic datasets that are not necessarily\nphoto-realistic can be a better alternative to simply fine-tune pre-trained\nnetworks. Specifically, our results show an impressive 25% improvement in the\nmAP metric over a fine-tuning baseline when only about 200 labelled images are\navailable to train. Finally, an ablation study of our results is presented to\ndelineate the individual contribution of different components in the\nrandomization pipeline. \n\n"}
{"id": "1807.09842", "contents": "Title: Understanding and representing the semantics of large structured\n  documents Abstract: Understanding large, structured documents like scholarly articles, requests\nfor proposals or business reports is a complex and difficult task. It involves\ndiscovering a document's overall purpose and subject(s), understanding the\nfunction and meaning of its sections and subsections, and extracting low level\nentities and facts about them. In this research, we present a deep learning\nbased document ontology to capture the general purpose semantic structure and\ndomain specific semantic concepts from a large number of academic articles and\nbusiness documents. The ontology is able to describe different functional parts\nof a document, which can be used to enhance semantic indexing for a better\nunderstanding by human beings and machines. We evaluate our models through\nextensive experiments on datasets of scholarly articles from arXiv and Request\nfor Proposal documents. \n\n"}
{"id": "1807.09912", "contents": "Title: Meta-learning autoencoders for few-shot prediction Abstract: Compared to humans, machine learning models generally require significantly\nmore training examples and fail to extrapolate from experience to solve\npreviously unseen challenges. To help close this performance gap, we augment\nsingle-task neural networks with a meta-recognition model which learns a\nsuccinct model code via its autoencoder structure, using just a few informative\nexamples. The model code is then employed by a meta-generative model to\nconstruct parameters for the task-specific model. We demonstrate that for\npreviously unseen tasks, without additional training, this Meta-Learning\nAutoencoder (MeLA) framework can build models that closely match the true\nunderlying models, with loss significantly lower than given by fine-tuned\nbaseline networks, and performance that compares favorably with\nstate-of-the-art meta-learning algorithms. MeLA also adds the ability to\nidentify influential training examples and predict which additional data will\nbe most valuable to acquire to improve model prediction. \n\n"}
{"id": "1807.10643", "contents": "Title: Experimental Implementation of a Quantum Autoencoder via Quantum Adders Abstract: Quantum autoencoders allow for reducing the amount of resources in a quantum\ncomputation by mapping the original Hilbert space onto a reduced space with the\nrelevant information. Recently, it was proposed to employ approximate quantum\nadders to implement quantum autoencoders in quantum technologies. Here, we\ncarry out the experimental implementation of this proposal in the Rigetti cloud\nquantum computer employing up to three qubits. The experimental fidelities are\nin good agreement with the theoretical prediction, thus proving the feasibility\nto realize quantum autoencoders via quantum adders in state-of-the-art\nsuperconducting quantum technologies. \n\n"}
{"id": "1807.10957", "contents": "Title: Improving Sequential Determinantal Point Processes for Supervised Video\n  Summarization Abstract: It is now much easier than ever before to produce videos. While the\nubiquitous video data is a great source for information discovery and\nextraction, the computational challenges are unparalleled. Automatically\nsummarizing the videos has become a substantial need for browsing, searching,\nand indexing visual content. This paper is in the vein of supervised video\nsummarization using sequential determinantal point process (SeqDPP), which\nmodels diversity by a probabilistic distribution. We improve this model in two\nfolds. In terms of learning, we propose a large-margin algorithm to address the\nexposure bias problem in SeqDPP. In terms of modeling, we design a new\nprobabilistic distribution such that, when it is integrated into SeqDPP, the\nresulting model accepts user input about the expected length of the summary.\nMoreover, we also significantly extend a popular video summarization dataset by\n1) more egocentric videos, 2) dense user annotations, and 3) a refined\nevaluation scheme. We conduct extensive experiments on this dataset (about 60\nhours of videos in total) and compare our approach to several competitive\nbaselines. \n\n"}
{"id": "1807.11648", "contents": "Title: Composable Core-sets for Determinant Maximization Problems via Spectral\n  Spanners Abstract: We study a spectral generalization of classical combinatorial graph spanners\nto the spectral setting. Given a set of vectors $V\\subseteq \\Re^d$, we say a\nset $U\\subseteq V$ is an $\\alpha$-spectral spanner if for all $v\\in V$ there is\na probability distribution $\\mu_v$ supported on $U$ such that $$vv^\\intercal\n\\preceq \\alpha\\cdot\\mathbb{E}_{u\\sim\\mu_v} uu^\\intercal.$$ We show that any set\n$V$ has an $\\tilde{O}(d)$-spectral spanner of size $\\tilde{O}(d)$ and this\nbound is almost optimal in the worst case.\n  We use spectral spanners to study composable core-sets for spectral problems.\nWe show that for many objective functions one can use a spectral spanner,\nindependent of the underlying functions, as a core-set and obtain almost\noptimal composable core-sets. For example, for the determinant maximization\nproblem we obtain an $\\tilde{O}(k)^k$-composable core-set and we show that this\nis almost optimal in the worst case.\n  Our algorithm is a spectral analogue of the classical greedy algorithm for\nfinding (combinatorial) spanners in graphs. We expect that our spanners find\nmany other applications in distributed or parallel models of computation. Our\nproof is spectral. As a side result of our techniques, we show that the rank of\ndiagonally dominant lower-triangular matrices are robust under `small\nperturbations' which could be of independent interests. \n\n"}
{"id": "1807.11876", "contents": "Title: Predicting Tactical Solutions to Operational Planning Problems under\n  Imperfect Information Abstract: This paper offers a methodological contribution at the intersection of\nmachine learning and operations research. Namely, we propose a methodology to\nquickly predict expected tactical descriptions of operational solutions\n(TDOSs). The problem we address occurs in the context of two-stage stochastic\nprogramming where the second stage is demanding computationally. We aim to\npredict at a high speed the expected TDOS associated with the second stage\nproblem, conditionally on the first stage variables. This may be used in\nsupport of the solution to the overall two-stage problem by avoiding the online\ngeneration of multiple second stage scenarios and solutions. We formulate the\ntactical prediction problem as a stochastic optimal prediction program, whose\nsolution we approximate with supervised machine learning. The training dataset\nconsists of a large number of deterministic operational problems generated by\ncontrolled probabilistic sampling. The labels are computed based on solutions\nto these problems (solved independently and offline), employing appropriate\naggregation and subselection methods to address uncertainty. Results on our\nmotivating application on load planning for rail transportation show that deep\nlearning models produce accurate predictions in very short computing time\n(milliseconds or less). The predictive accuracy is close to the lower bounds\ncalculated based on sample average approximation of the stochastic prediction\nprograms. \n\n"}
{"id": "1808.00387", "contents": "Title: Just Interpolate: Kernel \"Ridgeless\" Regression Can Generalize Abstract: In the absence of explicit regularization, Kernel \"Ridgeless\" Regression with\nnonlinear kernels has the potential to fit the training data perfectly. It has\nbeen observed empirically, however, that such interpolated solutions can still\ngeneralize well on test data. We isolate a phenomenon of implicit\nregularization for minimum-norm interpolated solutions which is due to a\ncombination of high dimensionality of the input data, curvature of the kernel\nfunction, and favorable geometric properties of the data such as an eigenvalue\ndecay of the empirical covariance and kernel matrices. In addition to deriving\na data-dependent upper bound on the out-of-sample error, we present\nexperimental evidence suggesting that the phenomenon occurs in the MNIST\ndataset. \n\n"}
{"id": "1808.00408", "contents": "Title: Geometry of energy landscapes and the optimizability of deep neural\n  networks Abstract: Deep neural networks are workhorse models in machine learning with multiple\nlayers of non-linear functions composed in series. Their loss function is\nhighly non-convex, yet empirically even gradient descent minimisation is\nsufficient to arrive at accurate and predictive models. It is hitherto unknown\nwhy are deep neural networks easily optimizable. We analyze the energy\nlandscape of a spin glass model of deep neural networks using random matrix\ntheory and algebraic geometry. We analytically show that the multilayered\nstructure holds the key to optimizability: Fixing the number of parameters and\nincreasing network depth, the number of stationary points in the loss function\ndecreases, minima become more clustered in parameter space, and the tradeoff\nbetween the depth and width of minima becomes less severe. Our analytical\nresults are numerically verified through comparison with neural networks\ntrained on a set of classical benchmark datasets. Our model uncovers generic\ndesign principles of machine learning models. \n\n"}
{"id": "1808.00523", "contents": "Title: Mod-DeepESN: Modular Deep Echo State Network Abstract: Neuro-inspired recurrent neural network algorithms, such as echo state\nnetworks, are computationally lightweight and thereby map well onto untethered\ndevices. The baseline echo state network algorithms are shown to be efficient\nin solving small-scale spatio-temporal problems. However, they underperform for\ncomplex tasks that are characterized by multi-scale structures. In this\nresearch, an intrinsic plasticity-infused modular deep echo state network\narchitecture is proposed to solve complex and multiple timescale temporal\ntasks. It outperforms state-of-the-art for time series prediction tasks. \n\n"}
{"id": "1808.00601", "contents": "Title: Classification of Building Information Model (BIM) Structures with Deep\n  Learning Abstract: In this work we study an application of machine learning to the construction\nindustry and we use classical and modern machine learning methods to categorize\nimages of building designs into three classes: Apartment building, Industrial\nbuilding or Other. No real images are used, but only images extracted from\nBuilding Information Model (BIM) software, as these are used by the\nconstruction industry to store building designs. For this task, we compared\nfour different methods: the first is based on classical machine learning, where\nHistogram of Oriented Gradients (HOG) was used for feature extraction and a\nSupport Vector Machine (SVM) for classification; the other three methods are\nbased on deep learning, covering common pre-trained networks as well as ones\ndesigned from scratch. To validate the accuracy of the models, a database of\n240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and\nabove 89% for the neural networks. \n\n"}
{"id": "1808.01006", "contents": "Title: A Hybrid Variational Autoencoder for Collaborative Filtering Abstract: In today's day and age when almost every industry has an online presence with\nusers interacting in online marketplaces, personalized recommendations have\nbecome quite important. Traditionally, the problem of collaborative filtering\nhas been tackled using Matrix Factorization which is linear in nature. We\nextend the work of [11] on using variational autoencoders (VAEs) for\ncollaborative filtering with implicit feedback by proposing a hybrid,\nmulti-modal approach. Our approach combines movie embeddings (learned from a\nsibling VAE network) with user ratings from the Movielens 20M dataset and\napplies it to the task of movie recommendation. We empirically show how the VAE\nnetwork is empowered by incorporating movie embeddings. We also visualize movie\nand user embeddings by clustering their latent representations obtained from a\nVAE. \n\n"}
{"id": "1808.01535", "contents": "Title: Triplet Network with Attention for Speaker Diarization Abstract: In automatic speech processing systems, speaker diarization is a crucial\nfront-end component to separate segments from different speakers. Inspired by\nthe recent success of deep neural networks (DNNs) in semantic inferencing,\ntriplet loss-based architectures have been successfully used for this problem.\nHowever, existing work utilizes conventional i-vectors as the input\nrepresentation and builds simple fully connected networks for metric learning,\nthus not fully leveraging the modeling power of DNN architectures. This paper\ninvestigates the importance of learning effective representations from the\nsequences directly in metric learning pipelines for speaker diarization. More\nspecifically, we propose to employ attention models to learn embeddings and the\nmetric jointly in an end-to-end fashion. Experiments are conducted on the\nCALLHOME conversational speech corpus. The diarization results demonstrate\nthat, besides providing a unified model, the proposed approach achieves\nimproved performance when compared against existing approaches. \n\n"}
{"id": "1808.01550", "contents": "Title: Designing Adaptive Neural Networks for Energy-Constrained Image\n  Classification Abstract: As convolutional neural networks (CNNs) enable state-of-the-art computer\nvision applications, their high energy consumption has emerged as a key\nimpediment to their deployment on embedded and mobile devices. Towards\nefficient image classification under hardware constraints, prior work has\nproposed adaptive CNNs, i.e., systems of networks with different accuracy and\ncomputation characteristics, where a selection scheme adaptively selects the\nnetwork to be evaluated for each input image. While previous efforts have\ninvestigated different network selection schemes, we find that they do not\nnecessarily result in energy savings when deployed on mobile systems. The key\nlimitation of existing methods is that they learn only how data should be\nprocessed among the CNNs and not the network architectures, with each network\nbeing treated as a blackbox.\n  To address this limitation, we pursue a more powerful design paradigm where\nthe architecture settings of the CNNs are treated as hyper-parameters to be\nglobally optimized. We cast the design of adaptive CNNs as a hyper-parameter\noptimization problem with respect to energy, accuracy, and communication\nconstraints imposed by the mobile device. To efficiently solve this problem, we\nadapt Bayesian optimization to the properties of the design space, reaching\nnear-optimal configurations in few tens of function evaluations. Our method\nreduces the energy consumed for image classification on a mobile device by up\nto 6x, compared to the best previously published work that uses CNNs as\nblackboxes. Finally, we evaluate two image classification practices, i.e.,\nclassifying all images locally versus over the cloud under energy and\ncommunication constraints. \n\n"}
{"id": "1808.01684", "contents": "Title: Missing Value Imputation Based on Deep Generative Models Abstract: Missing values widely exist in many real-world datasets, which hinders the\nperforming of advanced data analytics. Properly filling these missing values is\ncrucial but challenging, especially when the missing rate is high. Many\napproaches have been proposed for missing value imputation (MVI), but they are\nmostly heuristics-based, lacking a principled foundation and do not perform\nsatisfactorily in practice. In this paper, we propose a probabilistic framework\nbased on deep generative models for MVI. Under this framework, imputing the\nmissing entries amounts to seeking a fixed-point solution between two\nconditional distributions defined on the missing entries and latent variables\nrespectively. These distributions are parameterized by deep neural networks\n(DNNs) which possess high approximation power and can capture the nonlinear\nrelationships between missing entries and the observed values. The learning of\nweight parameters of DNNs is performed by maximizing an approximation of the\nlog-likelihood of observed values. We conducted extensive evaluation on 13\ndatasets and compared with 11 baselines methods, where our methods largely\noutperforms the baselines. \n\n"}
{"id": "1808.01974", "contents": "Title: A Survey on Deep Transfer Learning Abstract: As a new classification platform, deep learning has recently received\nincreasing attention from researchers and has been successfully applied to many\ndomains. In some domains, like bioinformatics and robotics, it is very\ndifficult to construct a large-scale well-annotated dataset due to the expense\nof data acquisition and costly annotation, which limits its development.\nTransfer learning relaxes the hypothesis that the training data must be\nindependent and identically distributed (i.i.d.) with the test data, which\nmotivates us to use transfer learning to solve the problem of insufficient\ntraining data. This survey focuses on reviewing the current researches of\ntransfer learning by using deep neural network and its applications. We defined\ndeep transfer learning, category and review the recent research works based on\nthe techniques used in deep transfer learning. \n\n"}
{"id": "1808.02169", "contents": "Title: Fast Variance Reduction Method with Stochastic Batch Size Abstract: In this paper we study a family of variance reduction methods with randomized\nbatch size---at each step, the algorithm first randomly chooses the batch size\nand then selects a batch of samples to conduct a variance-reduced stochastic\nupdate. We give the linear convergence rate for this framework for composite\nfunctions, and show that the optimal strategy to achieve the optimal\nconvergence rate per data access is to always choose batch size of 1, which is\nequivalent to the SAGA algorithm. However, due to the presence of cache/disk IO\neffect in computer architecture, the number of data access cannot reflect the\nrunning time because of 1) random memory access is much slower than sequential\naccess, 2) when data is too big to fit into memory, disk seeking takes even\nlonger time. After taking these into account, choosing batch size of $1$ is no\nlonger optimal, so we propose a new algorithm called SAGA++ and show how to\ncalculate the optimal average batch size theoretically. Our algorithm\noutperforms SAGA and other existing batched and stochastic solvers on real\ndatasets. In addition, we also conduct a precise analysis to compare different\nupdate rules for variance reduction methods, showing that SAGA++ converges\nfaster than SVRG in theory. \n\n"}
{"id": "1808.02610", "contents": "Title: L-Shapley and C-Shapley: Efficient Model Interpretation for Structured\n  Data Abstract: We study instancewise feature importance scoring as a method for model\ninterpretation. Any such method yields, for each predicted instance, a vector\nof importance scores associated with the feature vector. Methods based on the\nShapley score have been proposed as a fair way of computing feature\nattributions of this kind, but incur an exponential complexity in the number of\nfeatures. This combinatorial explosion arises from the definition of the\nShapley value and prevents these methods from being scalable to large data sets\nand complex models. We focus on settings in which the data have a graph\nstructure, and the contribution of features to the target variable is\nwell-approximated by a graph-structured factorization. In such settings, we\ndevelop two algorithms with linear complexity for instancewise feature\nimportance scoring. We establish the relationship of our methods to the Shapley\nvalue and another closely related concept known as the Myerson value from\ncooperative game theory. We demonstrate on both language and image data that\nour algorithms compare favorably with other methods for model interpretation. \n\n"}
{"id": "1808.02822", "contents": "Title: Backprop Evolution Abstract: The back-propagation algorithm is the cornerstone of deep learning. Despite\nits importance, few variations of the algorithm have been attempted. This work\npresents an approach to discover new variations of the back-propagation\nequation. We use a domain specific lan- guage to describe update equations as a\nlist of primitive functions. An evolution-based method is used to discover new\npropagation rules that maximize the generalization per- formance after a few\nepochs of training. We find several update equations that can train faster with\nshort training times than standard back-propagation, and perform similar as\nstandard back-propagation at convergence. \n\n"}
{"id": "1808.03333", "contents": "Title: Linked Causal Variational Autoencoder for Inferring Paired Spillover\n  Effects Abstract: Modeling spillover effects from observational data is an important problem in\neconomics, business, and other fields of research. % It helps us infer the\ncausality between two seemingly unrelated set of events. For example, if\nconsumer spending in the United States declines, it has spillover effects on\neconomies that depend on the U.S. as their largest export market. In this\npaper, we aim to infer the causation that results in spillover effects between\npairs of entities (or units), we call this effect as \\textit{paired spillover}.\nTo achieve this, we leverage the recent developments in variational inference\nand deep learning techniques to propose a generative model called Linked Causal\nVariational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA\nincorporates an encoder neural network to learn the latent attributes and a\ndecoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the\n\\textit{latent attributes as confounders that are assumed to affect both the\ntreatment and the outcome of units}. Specifically, given a pair of units $u$\nand $\\bar{u}$, their individual treatment and outcomes, the encoder network of\nLCVA samples the confounders by conditioning on the observed covariates of $u$,\nthe treatments of both $u$ and $\\bar{u}$ and the outcome of $u$. Once inferred,\nthe latent attributes (or confounders) of $u$ captures the spillover effect of\n$\\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde\n(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that\nLCVA is significantly more robust than existing methods in capturing spillover\neffects. \n\n"}
{"id": "1808.05731", "contents": "Title: Efficiently Learning Mixtures of Mallows Models Abstract: Mixtures of Mallows models are a popular generative model for ranking data\ncoming from a heterogeneous population. They have a variety of applications\nincluding social choice, recommendation systems and natural language\nprocessing. Here we give the first polynomial time algorithm for provably\nlearning the parameters of a mixture of Mallows models with any constant number\nof components. Prior to our work, only the two component case had been settled.\nOur analysis revolves around a determinantal identity of Zagier which was\nproven in the context of mathematical physics, which we use to show polynomial\nidentifiability and ultimately to construct test functions to peel off one\ncomponent at a time.\n  To complement our upper bounds, we show information-theoretic lower bounds on\nthe sample complexity as well as lower bounds against restricted families of\nalgorithms that make only local queries. Together, these results demonstrate\nvarious impediments to improving the dependence on the number of components.\nThey also motivate the study of learning mixtures of Mallows models from the\nperspective of beyond worst-case analysis. In this direction, we show that when\nthe scaling parameters of the Mallows models have separation, there are much\nfaster learning algorithms. \n\n"}
{"id": "1808.05832", "contents": "Title: Importance mixing: Improving sample reuse in evolutionary policy search\n  methods Abstract: Deep neuroevolution, that is evolutionary policy search methods based on deep\nneural networks, have recently emerged as a competitor to deep reinforcement\nlearning algorithms due to their better parallelization capabilities. However,\nthese methods still suffer from a far worse sample efficiency. In this paper we\ninvestigate whether a mechanism known as \"importance mixing\" can significantly\nimprove their sample efficiency. We provide a didactic presentation of\nimportance mixing and we explain how it can be extended to reuse more samples.\nThen, from an empirical comparison based on a simple benchmark, we show that,\nthough it actually provides better sample efficiency, it is still far from the\nsample efficiency of deep reinforcement learning, though it is more stable. \n\n"}
{"id": "1808.06170", "contents": "Title: Linked Recurrent Neural Networks Abstract: Recurrent Neural Networks (RNNs) have been proven to be effective in modeling\nsequential data and they have been applied to boost a variety of tasks such as\ndocument classification, speech recognition and machine translation. Most of\nexisting RNN models have been designed for sequences assumed to be identically\nand independently distributed (i.i.d). However, in many real-world\napplications, sequences are naturally linked. For example, web documents are\nconnected by hyperlinks; and genes interact with each other. On the one hand,\nlinked sequences are inherently not i.i.d., which poses tremendous challenges\nto existing RNN models. On the other hand, linked sequences offer link\ninformation in addition to the sequential information, which enables\nunprecedented opportunities to build advanced RNN models. In this paper, we\nstudy the problem of RNN for linked sequences. In particular, we introduce a\nprincipled approach to capture link information and propose a linked Recurrent\nNeural Network (LinkedRNN), which models sequential and link information\ncoherently. We conduct experiments on real-world datasets from multiple domains\nand the experimental results validate the effectiveness of the proposed\nframework. \n\n"}
{"id": "1808.06865", "contents": "Title: Machine Learning for Spatiotemporal Sequence Forecasting: A Survey Abstract: Spatiotemporal systems are common in the real-world. Forecasting the\nmulti-step future of these spatiotemporal systems based on the past\nobservations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant\nand challenging problem. Although lots of real-world problems can be viewed as\nSTSF and many research works have proposed machine learning based methods for\nthem, no existing work has summarized and compared these methods from a unified\nperspective. This survey aims to provide a systematic review of machine\nlearning for STSF. In this survey, we define the STSF problem and classify it\ninto three subcategories: Trajectory Forecasting of Moving Point Cloud\n(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).\nWe then introduce the two major challenges of STSF: 1) how to learn a model for\nmulti-step forecasting and 2) how to adequately model the spatial and temporal\nstructures. After that, we review the existing works for solving these\nchallenges, including the general learning strategies for multi-step\nforecasting, the classical machine learning based methods for STSF, and the\ndeep learning based methods for STSF. We also compare these methods and point\nout some potential research directions. \n\n"}
{"id": "1808.07018", "contents": "Title: Hypernetwork Knowledge Graph Embeddings Abstract: Knowledge graphs are graphical representations of large databases of facts,\nwhich typically suffer from incompleteness. Inferring missing relations (links)\nbetween entities (nodes) is the task of link prediction. A recent\nstate-of-the-art approach to link prediction, ConvE, implements a convolutional\nneural network to extract features from concatenated subject and relation\nvectors. Whilst results are impressive, the method is unintuitive and poorly\nunderstood. We propose a hypernetwork architecture that generates simplified\nrelation-specific convolutional filters that (i) outperforms ConvE and all\nprevious approaches across standard datasets; and (ii) can be framed as tensor\nfactorization and thus set within a well established family of factorization\nmodels for link prediction. We thus demonstrate that convolution simply offers\na convenient computational means of introducing sparsity and parameter tying to\nfind an effective trade-off between non-linear expressiveness and the number of\nparameters to learn. \n\n"}
{"id": "1808.07243", "contents": "Title: Controversy Rules - Discovering Regions Where Classifiers (Dis-)Agree\n  Exceptionally Abstract: Finding regions for which there is higher controversy among different\nclassifiers is insightful with regards to the domain and our models. Such\nevaluation can falsify assumptions, assert some, or also, bring to the\nattention unknown phenomena. The present work describes an algorithm, which is\nbased on the Exceptional Model Mining framework, and enables that kind of\ninvestigations. We explore several public datasets and show the usefulness of\nthis approach in classification tasks. We show in this paper a few interesting\nobservations about those well explored datasets, some of which are general\nknowledge, and other that as far as we know, were not reported before. \n\n"}
{"id": "1808.07982", "contents": "Title: Proximal Policy Optimization and its Dynamic Version for Sequence\n  Generation Abstract: In sequence generation task, many works use policy gradient for model\noptimization to tackle the intractable backpropagation issue when maximizing\nthe non-differentiable evaluation metrics or fooling the discriminator in\nadversarial learning. In this paper, we replace policy gradient with proximal\npolicy optimization (PPO), which is a proved more efficient reinforcement\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\nperformance. \n\n"}
{"id": "1808.08023", "contents": "Title: A Jointly Learned Context-Aware Place of Interest Embedding for Trip\n  Recommendations Abstract: Trip recommendation is an important location-based service that helps relieve\nusers from the time and efforts for trip planning. It aims to recommend a\nsequence of places of interest (POIs) for a user to visit that maximizes the\nuser's satisfaction. When adding a POI to a recommended trip, it is essential\nto understand the context of the recommendation, including the POI popularity,\nother POIs co-occurring in the trip, and the preferences of the user. These\ncontextual factors are learned separately in existing studies, while in\nreality, they impact jointly on a user's choice of a POI to visit. In this\nstudy, we propose a POI embedding model to jointly learn the impact of these\ncontextual factors. We call the learned POI embedding a context-aware POI\nembedding. To showcase the effectiveness of this embedding, we apply it to\ngenerate trip recommendations given a user and a time budget. We propose two\ntrip recommendation algorithms based on our context-aware POI embedding. The\nfirst algorithm finds the exact optimal trip by transforming and solving the\ntrip recommendation problem as an integer linear programming problem. To\nachieve a high computation efficiency, the second algorithm finds a\nheuristically optimal trip based on adaptive large neighborhood search. We\nperform extensive experiments on real datasets. The results show that our\nproposed algorithms consistently outperform state-of-the-art algorithms in trip\nrecommendation quality, with an advantage of up to 43% in F1-score. \n\n"}
{"id": "1808.08111", "contents": "Title: Multiclass Universum SVM Abstract: We introduce Universum learning for multiclass problems and propose a novel\nformulation for multiclass universum SVM (MU-SVM). We also propose an analytic\nspan bound for model selection with almost 2-4x faster computation times than\nstandard resampling techniques. We empirically demonstrate the efficacy of the\nproposed MUSVM formulation on several real world datasets achieving > 20%\nimprovement in test accuracies compared to multi-class SVM. \n\n"}
{"id": "1808.08195", "contents": "Title: GoT-WAVE: Temporal network alignment using graphlet-orbit transitions Abstract: Global pairwise network alignment (GPNA) aims to find a one-to-one node\nmapping between two networks that identifies conserved network regions. GPNA\nalgorithms optimize node conservation (NC) and edge conservation (EC). NC\nquantifies topological similarity between nodes. Graphlet-based degree vectors\n(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were\nused as a dynamic NC measure within the first-ever algorithms for GPNA of\ntemporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger\nnetworks. We recently developed a different graphlet-based measure of temporal\nnode similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead\nof DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new\napproach, GoT-WAVE.\n  On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed\nby 64%. On real networks, when optimizing only dynamic NC, each method is\nsuperior ~50% of the time. While DynaWAVE benefits more from also optimizing\ndynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a\npromising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.\nFuture work on better incorporating dynamic EC may yield further improvements. \n\n"}
{"id": "1808.08627", "contents": "Title: Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation Abstract: As opposed to manual feature engineering which is tedious and difficult to\nscale, network representation learning has attracted a surge of research\ninterests as it automates the process of feature learning on graphs. The\nlearned low-dimensional node vector representation is generalizable and eases\nthe knowledge discovery process on graphs by enabling various off-the-shelf\nmachine learning tools to be directly applied. Recent research has shown that\nthe past decade of network embedding approaches either explicitly factorize a\ncarefully designed matrix to obtain the low-dimensional node vector\nrepresentation or are closely related to implicit matrix factorization, with\nthe fundamental assumption that the factorized node connectivity matrix is\nlow-rank. Nonetheless, the global low-rank assumption does not necessarily hold\nespecially when the factorized matrix encodes complex node interactions, and\nthe resultant single low-rank embedding matrix is insufficient to capture all\nthe observed connectivity patterns. In this regard, we propose a novel\nmulti-level network embedding framework BoostNE, which can learn multiple\nnetwork embedding representations of different granularity from coarse to fine\nwithout imposing the prevalent global low-rank assumption. The proposed BoostNE\nmethod is also in line with the successful gradient boosting method in ensemble\nlearning as multiple weak embeddings lead to a stronger and more effective one.\nWe assess the effectiveness of the proposed BoostNE framework by comparing it\nwith existing state-of-the-art network embedding methods on various datasets,\nand the experimental results corroborate the superiority of the proposed\nBoostNE network embedding framework. \n\n"}
{"id": "1808.08755", "contents": "Title: Learning from Positive and Unlabeled Data under the Selected At Random\n  Assumption Abstract: For many interesting tasks, such as medical diagnosis and web page\nclassification, a learner only has access to some positively labeled examples\nand many unlabeled examples. Learning from this type of data requires making\nassumptions about the true distribution of the classes and/or the mechanism\nthat was used to select the positive examples to be labeled. The commonly made\nassumptions, separability of the classes and positive examples being selected\ncompletely at random, are very strong. This paper proposes a weaker assumption\nthat assumes the positive examples to be selected at random, conditioned on\nsome of the attributes. To learn under this assumption, an EM method is\nproposed. Experiments show that our method is not only very capable of learning\nunder this assumption, but it also outperforms the state of the art for\nlearning under the selected completely at random assumption. \n\n"}
{"id": "1808.09907", "contents": "Title: Dropout with Tabu Strategy for Regularizing Deep Neural Networks Abstract: Dropout has proven to be an effective technique for regularization and\npreventing the co-adaptation of neurons in deep neural networks (DNN). It\nrandomly drops units with a probability $p$ during the training stage of DNN.\nDropout also provides a way of approximately combining exponentially many\ndifferent neural network architectures efficiently. In this work, we add a\ndiversification strategy into dropout, which aims at generating more different\nneural network architectures in a proper times of iterations. The dropped units\nin last forward propagation will be marked. Then the selected units for\ndropping in the current FP will be kept if they have been marked in the last\nforward propagation. We only mark the units from the last forward propagation.\nWe call this new technique Tabu Dropout. Tabu Dropout has no extra parameters\ncompared with the standard Dropout and also it is computationally cheap. The\nexperiments conducted on MNIST, Fashion-MNIST datasets show that Tabu Dropout\nimproves the performance of the standard dropout. \n\n"}
{"id": "1808.09940", "contents": "Title: Adversarial Deep Reinforcement Learning in Portfolio Management Abstract: In this paper, we implement three state-of-art continuous reinforcement\nlearning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy\nOptimization (PPO) and Policy Gradient (PG)in portfolio management. All of them\nare widely-used in game playing and robot control. What's more, PPO has\nappealing theoretical propeties which is hopefully potential in portfolio\nmanagement. We present the performances of them under different settings,\nincluding different learning rates, objective functions, feature combinations,\nin order to provide insights for parameters tuning, features selection and data\npreparation. We also conduct intensive experiments in China Stock market and\nshow that PG is more desirable in financial market than DDPG and PPO, although\nboth of them are more advanced. What's more, we propose a so called Adversarial\nTraining method and show that it can greatly improve the training efficiency\nand significantly promote average daily return and sharpe ratio in back test.\nBased on this new modification, our experiments results show that our agent\nbased on Policy Gradient can outperform UCRP. \n\n"}
{"id": "1808.10650", "contents": "Title: Graph reduction with spectral and cut guarantees Abstract: Can one reduce the size of a graph without significantly altering its basic\nproperties? The graph reduction problem is hereby approached from the\nperspective of restricted spectral approximation, a modification of the\nspectral similarity measure used for graph sparsification. This choice is\nmotivated by the observation that restricted approximation carries strong\nspectral and cut guarantees, and that it implies approximation results for\nunsupervised learning problems relying on spectral embeddings.\n  The paper then focuses on coarsening---the most common type of graph\nreduction. Sufficient conditions are derived for a small graph to approximate a\nlarger one in the sense of restricted similarity. These findings give rise to\nnearly-linear algorithms that, compared to both standard and advanced graph\nreduction methods, find coarse graphs of improved quality, often by a large\nmargin, without sacrificing speed. \n\n"}
{"id": "1809.00338", "contents": "Title: Look Across Elapse: Disentangled Representation Learning and\n  Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition Abstract: Despite the remarkable progress in face recognition related technologies,\nreliably recognizing faces across ages still remains a big challenge. The\nappearance of a human face changes substantially over time, resulting in\nsignificant intra-class variations. As opposed to current techniques for\nage-invariant face recognition, which either directly extract age-invariant\nfeatures for recognition, or first synthesize a face that matches target age\nbefore feature extraction, we argue that it is more desirable to perform both\ntasks jointly so that they can leverage each other. To this end, we propose a\ndeep Age-Invariant Model (AIM) for face recognition in the wild with three\ndistinct novelties. First, AIM presents a novel unified deep architecture\njointly performing cross-age face synthesis and recognition in a mutual\nboosting way. Second, AIM achieves continuous face rejuvenation/aging with\nremarkable photorealistic and identity-preserving properties, avoiding the\nrequirement of paired data and the true age of testing samples. Third, we\ndevelop effective and novel training strategies for end-to-end learning the\nwhole deep architecture, which generates powerful age-invariant face\nrepresentations explicitly disentangled from the age variation. Moreover, we\npropose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset\nto facilitate existing efforts and push the frontiers of age-invariant face\nrecognition research. Extensive experiments on both our CAFR and several other\ncross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the\nproposed AIM model over the state-of-the-arts. Benchmarking our model on one of\nthe most popular unconstrained face recognition datasets IJB-C additionally\nverifies the promising generalizability of AIM in recognizing faces in the\nwild. \n\n"}
{"id": "1809.00946", "contents": "Title: Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing\n  GANs Abstract: We present a framework for translating unlabeled images from one domain into\nanalog images in another domain. We employ a progressively growing\nskip-connected encoder-generator structure and train it with a GAN loss for\nrealistic output, a cycle consistency loss for maintaining same-domain\ntranslation identity, and a semantic consistency loss that encourages the\nnetwork to keep the input semantic features in the output. We apply our\nframework on the task of translating face images, and show that it is capable\nof learning semantic mappings for face images with no supervised one-to-one\nimage mapping. \n\n"}
{"id": "1809.00999", "contents": "Title: Towards Large Scale Training Of Autoencoders For Collaborative Filtering Abstract: In this paper, we apply a mini-batch based negative sampling method to\nefficiently train a latent factor autoencoder model on large scale and sparse\ndata for implicit feedback collaborative filtering. We compare our work against\na state-of-the-art baseline model on different experimental datasets and show\nthat this method can lead to a good and fast approximation of the baseline\nmodel performance. The source code is available in\nhttps://github.com/amoussawi/recoder . \n\n"}
{"id": "1809.01015", "contents": "Title: Automated segmentation on the entire cardiac cycle using a deep learning\n  work-flow Abstract: The segmentation of the left ventricle (LV) from CINE MRI images is essential\nto infer important clinical parameters. Typically, machine learning algorithms\nfor automated LV segmentation use annotated contours from only two cardiac\nphases, diastole, and systole. In this work, we present an analysis work-flow\nfor fully-automated LV segmentation that learns from images acquired through\nthe cardiac cycle. The workflow consists of three components: first, for each\nimage in the sequence, we perform an automated localization and subsequent\ncropping of the bounding box containing the cardiac silhouette. Second, we\nidentify the LV contours using a Temporal Fully Convolutional Neural Network\n(T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a\nrecurrent mechanism enforcing temporal coherence across consecutive frames.\nFinally, we further defined the boundaries using either one of two components:\nfully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials\nand Semantic Flow. Our initial experiments suggest that significant improvement\nin performance can potentially be achieved by using a recurrent neural network\ncomponent that explicitly learns cardiac motion patterns whilst performing LV\nsegmentation. \n\n"}
{"id": "1809.01494", "contents": "Title: Interpretation of Natural Language Rules in Conversational Machine\n  Reading Abstract: Most work in machine reading focuses on question answering problems where the\nanswer is directly expressed in the text to read. However, many real-world\nquestion answering problems require the reading of text not because it contains\nthe literal answer, but because it contains a recipe to derive an answer\ntogether with the reader's background knowledge. One example is the task of\ninterpreting regulations to answer \"Can I...?\" or \"Do I have to...?\" questions\nsuch as \"I am working in Canada. Do I have to carry on paying UK National\nInsurance?\" after reading a UK government website about this topic. This task\nrequires both the interpretation of rules and the application of background\nknowledge. It is further complicated due to the fact that, in practice, most\nquestions are underspecified, and a human assistant will regularly have to ask\nclarification questions such as \"How long have you been working abroad?\" when\nthe answer cannot be directly derived from the question and text. In this\npaper, we formalise this task and develop a crowd-sourcing strategy to collect\n32k task instances based on real-world rules and crowd-generated questions and\nscenarios. We analyse the challenges of this task and assess its difficulty by\nevaluating the performance of rule-based and machine-learning baselines. We\nobserve promising results when no background knowledge is necessary, and\nsubstantial room for improvement whenever background knowledge is needed. \n\n"}
{"id": "1809.01715", "contents": "Title: Bridging machine learning and cryptography in defence against\n  adversarial attacks Abstract: In the last decade, deep learning algorithms have become very popular thanks\nto the achieved performance in many machine learning and computer vision tasks.\nHowever, most of the deep learning architectures are vulnerable to so called\nadversarial examples. This questions the security of deep neural networks (DNN)\nfor many security- and trust-sensitive domains. The majority of the proposed\nexisting adversarial attacks are based on the differentiability of the DNN cost\nfunction.Defence strategies are mostly based on machine learning and signal\nprocessing principles that either try to detect-reject or filter out the\nadversarial perturbations and completely neglect the classical cryptographic\ncomponent in the defence. In this work, we propose a new defence mechanism\nbased on the second Kerckhoffs's cryptographic principle which states that the\ndefence and classification algorithm are supposed to be known, but not the key.\nTo be compliant with the assumption that the attacker does not have access to\nthe secret key, we will primarily focus on a gray-box scenario and do not\naddress a white-box one. More particularly, we assume that the attacker does\nnot have direct access to the secret block, but (a) he completely knows the\nsystem architecture, (b) he has access to the data used for training and\ntesting and (c) he can observe the output of the classifier for each given\ninput. We show empirically that our system is efficient against most famous\nstate-of-the-art attacks in black-box and gray-box scenarios. \n\n"}
{"id": "1809.01999", "contents": "Title: Recurrent World Models Facilitate Policy Evolution Abstract: A generative recurrent neural network is quickly trained in an unsupervised\nmanner to model popular reinforcement learning environments through compressed\nspatio-temporal representations. The world model's extracted features are fed\ninto compact and simple policies trained by evolution, achieving state of the\nart results in various environments. We also train our agent entirely inside of\nan environment generated by its own internal world model, and transfer this\npolicy back into the actual environment. Interactive version of paper at\nhttps://worldmodels.github.io \n\n"}
{"id": "1809.02209", "contents": "Title: ProdSumNet: reducing model parameters in deep neural networks via\n  product-of-sums matrix decompositions Abstract: We consider a general framework for reducing the number of trainable model\nparameters in deep learning networks by decomposing linear operators as a\nproduct of sums of simpler linear operators. Recently proposed deep learning\narchitectures such as CNN, KFC, Dilated CNN, etc. are all subsumed in this\nframework and we illustrate other types of neural network architectures within\nthis framework. We show that good accuracy on MNIST and Fashion MNIST can be\nobtained using a relatively small number of trainable parameters. In addition,\nsince implementation of the convolutional layer is resource-heavy, we consider\nan approach in the transform domain that obviates the need for convolutional\nlayers. One of the advantages of this general framework over prior approaches\nis that the number of trainable parameters is not fixed and can be varied\narbitrarily. In particular, we illustrate the tradeoff of varying the number of\ntrainable variables and the corresponding error rate. As an example, by using\nthis decomposition on a reference CNN architecture for MNIST with over 3x10^6\ntrainable parameters, we are able to obtain an accuracy of 98.44% using only\n3554 trainable parameters. \n\n"}
{"id": "1809.02709", "contents": "Title: Exploiting Edge Features in Graph Neural Networks Abstract: Edge features contain important information about graphs. However, current\nstate-of-the-art neural network models designed for graph learning, e.g. graph\nconvolutional networks (GCN) and graph attention networks (GAT), adequately\nutilize edge features, especially multi-dimensional edge features. In this\npaper, we build a new framework for a family of new graph neural network models\nthat can more sufficiently exploit edge features, including those of undirected\nor multi-dimensional edges. The proposed framework can consolidate current\ngraph neural network models; e.g. graph convolutional networks (GCN) and graph\nattention networks (GAT). The proposed framework and new models have the\nfollowing novelties: First, we propose to use doubly stochastic normalization\nof graph edge features instead of the commonly used row or symmetric\nnormalization approches used in current graph neural networks. Second, we\nconstruct new formulas for the operations in each individual layer so that they\ncan handle multi-dimensional edge features. Third, for the proposed new\nframework, edge features are adaptive across network layers. As a result, our\nproposed new framework and new models can exploit a rich source of graph\ninformation. We apply our new models to graph node classification on several\ncitation networks, whole graph classification, and regression on several\nmolecular datasets. Compared with the current state-of-the-art methods, i.e.\nGCNs and GAT, our models obtain better performance, which testify to the\nimportance of exploiting edge features in graph neural networks. \n\n"}
{"id": "1809.02728", "contents": "Title: Coupled IGMM-GANs for deep multimodal anomaly detection in human\n  mobility data Abstract: Detecting anomalous activity in human mobility data has a number of\napplications including road hazard sensing, telematic based insurance, and\nfraud detection in taxi services and ride sharing. In this paper we address two\nchallenges that arise in the study of anomalous human trajectories: 1) a lack\nof ground truth data on what defines an anomaly and 2) the dependence of\nexisting methods on significant pre-processing and feature engineering. While\ngenerative adversarial networks seem like a natural fit for addressing these\nchallenges, we find that existing GAN based anomaly detection algorithms\nperform poorly due to their inability to handle multimodal patterns. For this\npurpose we introduce an infinite Gaussian mixture model coupled with\n(bi-directional) generative adversarial networks, IGMM-GAN, that is able to\ngenerate synthetic, yet realistic, human mobility data and simultaneously\nfacilitates multimodal anomaly detection. Through estimation of a generative\nprobability density on the space of human trajectories, we are able to generate\nrealistic synthetic datasets that can be used to benchmark existing anomaly\ndetection methods. The estimated multimodal density also allows for a natural\ndefinition of outlier that we use for detecting anomalous trajectories. We\nillustrate our methodology and its improvement over existing GAN anomaly\ndetection on several human mobility datasets, along with MNIST. \n\n"}
{"id": "1809.02740", "contents": "Title: Ensembles of Nested Dichotomies with Multiple Subset Evaluation Abstract: A system of nested dichotomies is a method of decomposing a multi-class\nproblem into a collection of binary problems. Such a system recursively applies\nbinary splits to divide the set of classes into two subsets, and trains a\nbinary classifier for each split. Many methods have been proposed to perform\nthis split, each with various advantages and disadvantages. In this paper, we\npresent a simple, general method for improving the predictive performance of\nnested dichotomies produced by any subset selection techniques that employ\nrandomness to construct the subsets. We provide a theoretical expectation for\nperformance improvements, as well as empirical results showing that our method\nimproves the root mean squared error of nested dichotomies, regardless of\nwhether they are employed as an individual model or in an ensemble setting. \n\n"}
{"id": "1809.03048", "contents": "Title: Distance preserving model order reduction of graph-Laplacians and\n  cluster analysis Abstract: Graph-Laplacians and their spectral embeddings play an important role in\nmultiple areas of machine learning. This paper is focused on graph-Laplacian\ndimension reduction for the spectral clustering of data as a primary\napplication. Spectral embedding provides a low-dimensional parametrization of\nthe data manifold which makes the subsequent task (e.g., clustering) much\neasier. However, despite reducing the dimensionality of data, the overall\ncomputational cost may still be prohibitive for large data sets due to two\nfactors. First, computing the partial eigendecomposition of the graph-Laplacian\ntypically requires a large Krylov subspace. Second, after the spectral\nembedding is complete, one still has to operate with the same number of data\npoints. For example, clustering of the embedded data is typically performed\nwith various relaxations of k-means which computational cost scales poorly with\nrespect to the size of data set. In this work, we switch the focus from the\nentire data set to a subset of graph vertices (target subset). We develop two\nnovel algorithms for such low-dimensional representation of the original graph\nthat preserves important global distances between the nodes of the target\nsubset. In particular, it allows to ensure that target subset clustering is\nconsistent with the spectral clustering of the full data set if one would\nperform such. That is achieved by a properly parametrized reduced-order model\n(ROM) of the graph-Laplacian that approximates accurately the diffusion\ntransfer function of the original graph for inputs and outputs restricted to\nthe target subset. Working with a small target subset reduces greatly the\nrequired dimension of Krylov subspace and allows to exploit the conventional\nalgorithms (like approximations of k-means) in the regimes when they are most\nrobust and efficient. \n\n"}
{"id": "1809.03137", "contents": "Title: Tracking by Animation: Unsupervised Learning of Multi-Object Attentive\n  Trackers Abstract: Online Multi-Object Tracking (MOT) from videos is a challenging computer\nvision task which has been extensively studied for decades. Most of the\nexisting MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm\ncombined with popular machine learning approaches which largely reduce the\nhuman effort to tune algorithm parameters. However, the commonly used\nsupervised learning approaches require the labeled data (e.g., bounding boxes),\nwhich is expensive for videos. Also, the TBD framework is usually suboptimal\nsince it is not end-to-end, i.e., it considers the task as detection and\ntracking, but not jointly. To achieve both label-free and end-to-end learning\nof MOT, we propose a Tracking-by-Animation framework, where a differentiable\nneural model first tracks objects from input frames and then animates these\nobjects into reconstructed frames. Learning is then driven by the\nreconstruction error through backpropagation. We further propose a\nReprioritized Attentive Tracking to improve the robustness of data association.\nExperiments conducted on both synthetic and real video datasets show the\npotential of the proposed model. Our project page is publicly available at:\nhttps://github.com/zhen-he/tracking-by-animation \n\n"}
{"id": "1809.03267", "contents": "Title: Feature Learning for Meta-Paths in Knowledge Graphs Abstract: In this thesis, we study the problem of feature learning on heterogeneous\nknowledge graphs. These features can be used to perform tasks such as link\nprediction, classification and clustering on graphs. Knowledge graphs provide\nrich semantics encoded in the edge and node types. Meta-paths consist of these\ntypes and abstract paths in the graph. Until now, meta-paths can only be used\nas categorical features with high redundancy and are therefore unsuitable for\nmachine learning models. We propose meta-path embeddings to solve this problem\nby learning semantical and compact vector representations of them. Current\ngraph embedding methods only embed nodes and edge types and therefore miss\nsemantics encoded in the combination of them. Our method embeds meta-paths\nusing the skipgram model with an extension to deal with the redundancy and high\namount of meta-paths in big knowledge graphs. We critically evaluate our\nembedding approach by predicting links on Wikidata. The experiments indicate\nthat we learn a sensible embedding of the meta-paths but can improve it\nfurther. \n\n"}
{"id": "1809.03291", "contents": "Title: Action-conditional Sequence Modeling for Recommendation Abstract: In many online applications interactions between a user and a web-service are\norganized in a sequential way, e.g., user browsing an e-commerce website. In\nthis setting, recommendation system acts throughout user navigation by showing\nitems. Previous works have addressed this recommendation setup through the task\nof predicting the next item user will interact with. In particular, Recurrent\nNeural Networks (RNNs) has been shown to achieve substantial improvements over\ncollaborative filtering baselines. In this paper, we consider interactions\ntriggered by the recommendations of deployed recommender system in addition to\nbrowsing behavior. Indeed, it is reported that in online services interactions\nwith recommendations represent up to 30\\% of total interactions. Moreover, in\npractice, recommender system can greatly influence user behavior by promoting\nspecific items. In this paper, we extend the RNN modeling framework by taking\ninto account user interaction with recommended items. We propose and evaluate\nRNN architectures that consist of the recommendation action module and the\nstate-action fusion module. Using real-world large-scale datasets we\ndemonstrate improved performance on the next item prediction task compared to\nthe baselines. \n\n"}
{"id": "1809.03316", "contents": "Title: Hierarchical Video Understanding Abstract: We introduce a hierarchical architecture for video understanding that\nexploits the structure of real world actions by capturing targets at different\nlevels of granularity. We design the model such that it first learns simpler\ncoarse-grained tasks, and then moves on to learn more fine-grained targets. The\nmodel is trained with a joint loss on different granularity levels. We\ndemonstrate empirical results on the recent release of Something-Something\ndataset, which provides a hierarchy of targets, namely coarse-grained action\ngroups, fine-grained action categories, and captions. Experiments suggest that\nmodels that exploit targets at different levels of granularity achieve better\nperformance on all levels. \n\n"}
{"id": "1809.03368", "contents": "Title: Probabilistic Binary Neural Networks Abstract: Low bit-width weights and activations are an effective way of combating the\nincreasing need for both memory and compute power of Deep Neural Networks. In\nthis work, we present a probabilistic training method for Neural Network with\nboth binary weights and activations, called BLRNet. By embracing stochasticity\nduring training, we circumvent the need to approximate the gradient of\nnon-differentiable functions such as sign(), while still obtaining a fully\nBinary Neural Network at test time. Moreover, it allows for anytime ensemble\npredictions for improved performance and uncertainty estimates by sampling from\nthe weight distribution. Since all operations in a layer of the BLRNet operate\non random variables, we introduce stochastic versions of Batch Normalization\nand max pooling, which transfer well to a deterministic network at test time.\nWe evaluate the BLRNet on multiple standardized benchmarks. \n\n"}
{"id": "1809.04110", "contents": "Title: Joint Embedding of Meta-Path and Meta-Graph for Heterogeneous\n  Information Networks Abstract: Meta-graph is currently the most powerful tool for similarity search on\nheterogeneous information networks,where a meta-graph is a composition of\nmeta-paths that captures the complex structural information. However, current\nrelevance computing based on meta-graph only considers the complex structural\ninformation, but ignores its embedded meta-paths information. To address this\nproblem, we proposeMEta-GrAph-based network embedding models, called MEGA and\nMEGA++, respectively. The MEGA model uses normalized relevance or similarity\nmeasures that are derived from a meta-graph and its embedded meta-paths between\nnodes simultaneously, and then leverages tensor decomposition method to perform\nnode embedding. The MEGA++ further facilitates the use of coupled tensor-matrix\ndecomposition method to obtain a joint embedding for nodes, which\nsimultaneously considers the hidden relations of all meta information of a\nmeta-graph.Extensive experiments on two real datasets demonstrate thatMEGA and\nMEGA++ are more effective than state-of-the-art approaches. \n\n"}
{"id": "1809.04379", "contents": "Title: Bayesian Semi-supervised Learning with Graph Gaussian Processes Abstract: We propose a data-efficient Gaussian process-based Bayesian approach to the\nsemi-supervised learning problem on graphs. The proposed model shows extremely\ncompetitive performance when compared to the state-of-the-art graph neural\nnetworks on semi-supervised learning benchmark experiments, and outperforms the\nneural networks in active learning experiments where labels are scarce.\nFurthermore, the model does not require a validation data set for early\nstopping to control over-fitting. Our model can be viewed as an instance of\nempirical distribution regression weighted locally by network connectivity. We\nfurther motivate the intuitive construction of the model with a Bayesian linear\nmodel interpretation where the node features are filtered by an operator\nrelated to the graph Laplacian. The method can be easily implemented by\nadapting off-the-shelf scalable variational inference algorithms for Gaussian\nprocesses. \n\n"}
{"id": "1809.04559", "contents": "Title: Benchmarking and Optimization of Gradient Boosting Decision Tree\n  Algorithms Abstract: Gradient boosting decision trees (GBDTs) have seen widespread adoption in\nacademia, industry and competitive data science due to their state-of-the-art\nperformance in many machine learning tasks. One relative downside to these\nmodels is the large number of hyper-parameters that they expose to the\nend-user. To maximize the predictive power of GBDT models, one must either\nmanually tune the hyper-parameters, or utilize automated techniques such as\nthose based on Bayesian optimization. Both of these approaches are\ntime-consuming since they involve repeatably training the model for different\nsets of hyper-parameters. A number of software GBDT packages have started to\noffer GPU acceleration which can help to alleviate this problem. In this paper,\nwe consider three such packages: XGBoost, LightGBM and Catboost. Firstly, we\nevaluate the performance of the GPU acceleration provided by these packages\nusing large-scale datasets with varying shapes, sparsities and learning tasks.\nThen, we compare the packages in the context of hyper-parameter optimization,\nboth in terms of how quickly each package converges to a good validation score,\nand in terms of generalization performance. \n\n"}
{"id": "1809.04720", "contents": "Title: Sim-to-Real Transfer Learning using Robustified Controllers in Robotic\n  Tasks involving Complex Dynamics Abstract: Learning robot tasks or controllers using deep reinforcement learning has\nbeen proven effective in simulations. Learning in simulation has several\nadvantages. For example, one can fully control the simulated environment,\nincluding halting motions while performing computations. Another advantage when\nrobots are involved, is that the amount of time a robot is occupied learning a\ntask---rather than being productive---can be reduced by transferring the\nlearned task to the real robot. Transfer learning requires some amount of\nfine-tuning on the real robot. For tasks which involve complex (non-linear)\ndynamics, the fine-tuning itself may take a substantial amount of time. In\norder to reduce the amount of fine-tuning we propose to learn robustified\ncontrollers in simulation. Robustified controllers are learned by exploiting\nthe ability to change simulation parameters (both appearance and dynamics) for\nsuccessive training episodes. An additional benefit for this approach is that\nit alleviates the precise determination of physics parameters for the\nsimulator, which is a non-trivial task. We demonstrate our proposed approach on\na real setup in which a robot aims to solve a maze game, which involves complex\ndynamics due to static friction and potentially large accelerations. We show\nthat the amount of fine-tuning in transfer learning for a robustified\ncontroller is substantially reduced compared to a non-robustified controller. \n\n"}
{"id": "1809.04790", "contents": "Title: Adversarial Examples: Opportunities and Challenges Abstract: Deep neural networks (DNNs) have shown huge superiority over humans in image\nrecognition, speech processing, autonomous vehicles and medical diagnosis.\nHowever, recent studies indicate that DNNs are vulnerable to adversarial\nexamples (AEs), which are designed by attackers to fool deep learning models.\nDifferent from real examples, AEs can mislead the model to predict incorrect\noutputs while hardly be distinguished by human eyes, therefore threaten\nsecurity-critical deep-learning applications. In recent years, the generation\nand defense of AEs have become a research hotspot in the field of artificial\nintelligence (AI) security. This article reviews the latest research progress\nof AEs. First, we introduce the concept, cause, characteristics and evaluation\nmetrics of AEs, then give a survey on the state-of-the-art AE generation\nmethods with the discussion of advantages and disadvantages. After that, we\nreview the existing defenses and discuss their limitations. Finally, future\nresearch opportunities and challenges on AEs are prospected. \n\n"}
{"id": "1809.05183", "contents": "Title: Explainable time series tweaking via irreversible and reversible\n  temporal transformations Abstract: Time series classification has received great attention over the past decade\nwith a wide range of methods focusing on predictive performance by exploiting\nvarious types of temporal features. Nonetheless, little emphasis has been\nplaced on interpretability and explainability. In this paper, we formulate the\nnovel problem of explainable time series tweaking, where, given a time series\nand an opaque classifier that provides a particular classification decision for\nthe time series, we want to find the minimum number of changes to be performed\nto the given time series so that the classifier changes its decision to another\nclass. We show that the problem is NP-hard, and focus on two instantiations of\nthe problem, which we refer to as reversible and irreversible time series\ntweaking. The classifier under investigation is the random shapelet forest\nclassifier. Moreover, we propose two algorithmic solutions for the two problems\nalong with simple optimizations, as well as a baseline solution using the\nnearest neighbor classifier. An extensive experimental evaluation on a variety\nof real datasets demonstrates the usefulness and effectiveness of our problem\nformulation and solutions. \n\n"}
{"id": "1809.05242", "contents": "Title: Neural Network Topologies for Sparse Training Abstract: The sizes of deep neural networks (DNNs) are rapidly outgrowing the capacity\nof hardware to store and train them. Research over the past few decades has\nexplored the prospect of sparsifying DNNs before, during, and after training by\npruning edges from the underlying topology. The resulting neural network is\nknown as a sparse neural network. More recent work has demonstrated the\nremarkable result that certain sparse DNNs can train to the same precision as\ndense DNNs at lower runtime and storage cost. An intriguing class of these\nsparse DNNs is the X-Nets, which are initialized and trained upon a sparse\ntopology with neither reference to a parent dense DNN nor subsequent pruning.\nWe present an algorithm that deterministically generates sparse DNN topologies\nthat, as a whole, are much more diverse than X-Net topologies, while preserving\nX-Nets' desired characteristics. \n\n"}
{"id": "1809.05259", "contents": "Title: Random Warping Series: A Random Features Method for Time-Series\n  Embedding Abstract: Time series data analytics has been a problem of substantial interests for\ndecades, and Dynamic Time Warping (DTW) has been the most widely adopted\ntechnique to measure dissimilarity between time series. A number of\nglobal-alignment kernels have since been proposed in the spirit of DTW to\nextend its use to kernel-based estimation method such as support vector\nmachine. However, those kernels suffer from diagonal dominance of the Gram\nmatrix and a quadratic complexity w.r.t. the sample size. In this work, we\nstudy a family of alignment-aware positive definite (p.d.) kernels, with its\nfeature embedding given by a distribution of \\emph{Random Warping Series\n(RWS)}. The proposed kernel does not suffer from the issue of diagonal\ndominance while naturally enjoys a \\emph{Random Features} (RF) approximation,\nwhich reduces the computational complexity of existing DTW-based techniques\nfrom quadratic to linear in terms of both the number and the length of\ntime-series. We also study the convergence of the RF approximation for the\ndomain of time series of unbounded length. Our extensive experiments on 16\nbenchmark datasets demonstrate that RWS outperforms or matches state-of-the-art\nclassification and clustering methods in both accuracy and computational time.\nOur code and data is available at {\n\\url{https://github.com/IBM/RandomWarpingSeries}}. \n\n"}
{"id": "1809.05476", "contents": "Title: Hardware-Aware Machine Learning: Modeling and Optimization Abstract: Recent breakthroughs in Deep Learning (DL) applications have made DL models a\nkey component in almost every modern computing system. The increased popularity\nof DL applications deployed on a wide-spectrum of platforms have resulted in a\nplethora of design challenges related to the constraints introduced by the\nhardware itself. What is the latency or energy cost for an inference made by a\nDeep Neural Network (DNN)? Is it possible to predict this latency or energy\nconsumption before a model is trained? If yes, how can machine learners take\nadvantage of these models to design the hardware-optimal DNN for deployment?\nFrom lengthening battery life of mobile devices to reducing the runtime\nrequirements of DL models executing in the cloud, the answers to these\nquestions have drawn significant attention.\n  One cannot optimize what isn't properly modeled. Therefore, it is important\nto understand the hardware efficiency of DL models during serving for making an\ninference, before even training the model. This key observation has motivated\nthe use of predictive models to capture the hardware performance or energy\nefficiency of DL applications. Furthermore, DL practitioners are challenged\nwith the task of designing the DNN model, i.e., of tuning the hyper-parameters\nof the DNN architecture, while optimizing for both accuracy of the DL model and\nits hardware efficiency. Therefore, state-of-the-art methodologies have\nproposed hardware-aware hyper-parameter optimization techniques. In this paper,\nwe provide a comprehensive assessment of state-of-the-art work and selected\nresults on the hardware-aware modeling and optimization for DL applications. We\nalso highlight several open questions that are poised to give rise to novel\nhardware-aware designs in the next few years, as DL applications continue to\nsignificantly impact associated hardware systems and platforms. \n\n"}
{"id": "1809.05578", "contents": "Title: Ensemble Clustering for Graphs Abstract: We propose an ensemble clustering algorithm for graphs (ECG), which is based\non the Louvain algorithm and the concept of consensus clustering. We validate\nour approach by replicating a recently published study comparing graph\nclustering algorithms over artificial networks, showing that ECG outperforms\nthe leading algorithms from that study. We also illustrate how the ensemble\nobtained with ECG can be used to quantify the presence of community structure\nin the graph. \n\n"}
{"id": "1809.05964", "contents": "Title: Latent Space Optimal Transport for Generative Models Abstract: Variational Auto-Encoders enforce their learned intermediate latent-space\ndata distribution to be a simple distribution, such as an isotropic Gaussian.\nHowever, this causes the posterior collapse problem and loses manifold\nstructure which can be important for datasets such as facial images. A GAN can\ntransform a simple distribution to a latent-space data distribution and thus\npreserve the manifold structure, but optimizing a GAN involves solving a\nMin-Max optimization problem, which is difficult and not well understood so\nfar. Therefore, we propose a GAN-like method to transform a simple distribution\nto a data distribution in the latent space by solving only a minimization\nproblem. This minimization problem comes from training a discriminator between\na simple distribution and a latent-space data distribution. Then, we can\nexplicitly formulate an Optimal Transport (OT) problem that computes the\ndesired mapping between the two distributions. This means that we can transform\na distribution without solving the difficult Min-Max optimization problem.\nExperimental results on an eight-Gaussian dataset show that the proposed OT can\nhandle multi-cluster distributions. Results on the MNIST and the CelebA\ndatasets validate the effectiveness of the proposed method. \n\n"}
{"id": "1809.06018", "contents": "Title: Integrative Analysis of Patient Health Records and Neuroimages via\n  Memory-based Graph Convolutional Network Abstract: With the arrival of the big data era, more and more data are becoming readily\navailable in various real-world applications and those data are usually highly\nheterogeneous. Taking computational medicine as an example, we have both\nElectronic Health Records (EHR) and medical images for each patient. For\ncomplicated diseases such as Parkinson's and Alzheimer's, both EHR and\nneuroimaging information are very important for disease understanding because\nthey contain complementary aspects of the disease. However, EHR and neuroimage\nare completely different. So far the existing research has been mainly focusing\non one of them. In this paper, we proposed a framework, Memory-Based Graph\nConvolution Network (MemGCN), to perform integrative analysis with such\nmulti-modal data. Specifically, GCN is used to extract useful information from\nthe patients' neuroimages. The information contained in the patient EHRs before\nthe acquisition of each brain image is captured by a memory network because of\nits sequential nature. The information contained in each brain image is\ncombined with the information read out from the memory network to infer the\ndisease state at the image acquisition timestamp. To further enhance the\nanalytical power of MemGCN, we also designed a multi-hop strategy that allows\nmultiple reading and updating on the memory can be performed at each iteration.\nWe conduct experiments using the patient data from the Parkinson's Progression\nMarkers Initiative (PPMI) with the task of classification of Parkinson's\nDisease (PD) cases versus controls. We demonstrate that superior classification\nperformance can be achieved with our proposed framework, comparing with\nexisting approaches involving a single type of data. \n\n"}
{"id": "1809.06061", "contents": "Title: Transparency and Explanation in Deep Reinforcement Learning Neural\n  Networks Abstract: Autonomous AI systems will be entering human society in the near future to\nprovide services and work alongside humans. For those systems to be accepted\nand trusted, the users should be able to understand the reasoning process of\nthe system, i.e. the system should be transparent. System transparency enables\nhumans to form coherent explanations of the system's decisions and actions.\nTransparency is important not only for user trust, but also for software\ndebugging and certification. In recent years, Deep Neural Networks have made\ngreat advances in multiple application areas. However, deep neural networks are\nopaque. In this paper, we report on work in transparency in Deep Reinforcement\nLearning Networks (DRLN). Such networks have been extremely successful in\naccurately learning action control in image input domains, such as Atari games.\nIn this paper, we propose a novel and general method that (a) incorporates\nexplicit object recognition processing into deep reinforcement learning models,\n(b) forms the basis for the development of \"object saliency maps\", to provide\nvisualization of internal states of DRLNs, thus enabling the formation of\nexplanations and (c) can be incorporated in any existing deep reinforcement\nlearning framework. We present computational results and human experiments to\nevaluate our approach. \n\n"}
{"id": "1809.06146", "contents": "Title: Curriculum goal masking for continuous deep reinforcement learning Abstract: Deep reinforcement learning has recently gained a focus on problems where\npolicy or value functions are independent of goals. Evidence exists that the\nsampling of goals has a strong effect on the learning performance, but there is\na lack of general mechanisms that focus on optimizing the goal sampling\nprocess. In this work, we present a simple and general goal masking method that\nalso allows us to estimate a goal's difficulty level and thus realize a\ncurriculum learning approach for deep RL. Our results indicate that focusing on\ngoals with a medium difficulty level is appropriate for deep deterministic\npolicy gradient (DDPG) methods, while an \"aim for the stars and reach the\nmoon-strategy\", where hard goals are sampled much more often than simple goals,\nleads to the best learning performance in cases where DDPG is combined with for\nhindsight experience replay (HER). We demonstrate that the approach\nsignificantly outperforms standard goal sampling for different robotic object\nmanipulation problems. \n\n"}
{"id": "1809.06253", "contents": "Title: Multi-hop assortativities for networks classification Abstract: Several social, medical, engineering and biological challenges rely on\ndiscovering the functionality of networks from their structure and node\nmetadata, when it is available. For example, in chemoinformatics one might want\nto detect whether a molecule is toxic based on structure and atomic types, or\ndiscover the research field of a scientific collaboration network. Existing\ntechniques rely on counting or measuring structural patterns that are known to\nshow large variations from network to network, such as the number of triangles,\nor the assortativity of node metadata. We introduce the concept of multi-hop\nassortativity, that captures the similarity of the nodes situated at the\nextremities of a randomly selected path of a given length. We show that\nmulti-hop assortativity unifies various existing concepts and offers a\nversatile family of 'fingerprints' to characterize networks. These fingerprints\nallow in turn to recover the functionalities of a network, with the help of the\nmachine learning toolbox. Our method is evaluated empirically on established\nsocial and chemoinformatic network benchmarks. Results reveal that our\nassortativity based features are competitive providing highly accurate results\noften outperforming state of the art methods for the network classification\ntask. \n\n"}
{"id": "1809.06452", "contents": "Title: Robustness Guarantees for Bayesian Inference with Gaussian Processes Abstract: Bayesian inference and Gaussian processes are widely used in applications\nranging from robotics and control to biological systems. Many of these\napplications are safety-critical and require a characterization of the\nuncertainty associated with the learning model and formal guarantees on its\npredictions. In this paper we define a robustness measure for Bayesian\ninference against input perturbations, given by the probability that, for a\ntest point and a compact set in the input space containing the test point, the\nprediction of the learning model will remain $\\delta-$close for all the points\nin the set, for $\\delta>0.$ Such measures can be used to provide formal\nguarantees for the absence of adversarial examples. By employing the theory of\nGaussian processes, we derive tight upper bounds on the resulting robustness by\nutilising the Borell-TIS inequality, and propose algorithms for their\ncomputation. We evaluate our techniques on two examples, a GP regression\nproblem and a fully-connected deep neural network, where we rely on weak\nconvergence to GPs to study adversarial examples on the MNIST dataset. \n\n"}
{"id": "1809.07023", "contents": "Title: Removing the Feature Correlation Effect of Multiplicative Noise Abstract: Multiplicative noise, including dropout, is widely used to regularize deep\nneural networks (DNNs), and is shown to be effective in a wide range of\narchitectures and tasks. From an information perspective, we consider injecting\nmultiplicative noise into a DNN as training the network to solve the task with\nnoisy information pathways, which leads to the observation that multiplicative\nnoise tends to increase the correlation between features, so as to increase the\nsignal-to-noise ratio of information pathways. However, high feature\ncorrelation is undesirable, as it increases redundancy in representations. In\nthis work, we propose non-correlating multiplicative noise (NCMN), which\nexploits batch normalization to remove the correlation effect in a simple yet\neffective way. We show that NCMN significantly improves the performance of\nstandard multiplicative noise on image classification tasks, providing a better\nalternative to dropout for batch-normalized networks. Additionally, we present\na unified view of NCMN and shake-shake regularization, which explains the\nperformance gain of the latter. \n\n"}
{"id": "1809.07688", "contents": "Title: Inferring Multiplex Diffusion Network via Multivariate Marked Hawkes\n  Process Abstract: Understanding the diffusion in social network is an important task. However,\nthis task is challenging since (1) the network structure is usually hidden with\nonly observations of events like \"post\" or \"repost\" associated with each node,\nand (2) the interactions between nodes encompass multiple distinct patterns\nwhich in turn affect the diffusion patterns. For instance, social interactions\nseldom develop on a single channel, and multiple relationships can bind pairs\nof people due to their various common interests. Most previous work considers\nonly one of these two challenges which is apparently unrealistic. In this\npaper, we study the problem of \\emph{inferring multiplex network} in social\nnetworks. We propose the Multiplex Diffusion Model (MDM) which incorporates the\nmultivariate marked Hawkes process and topic model to infer the multiplex\nstructure of social network. A MCMC based algorithm is developed to infer the\nlatent multiplex structure and to estimate the node-related parameters. We\nevaluate our model based on both synthetic and real-world datasets. The results\nshow that our model is more effective in terms of uncovering the multiplex\nnetwork structure. \n\n"}
{"id": "1809.07691", "contents": "Title: A Survey on Theoretical Advances of Community Detection in Networks Abstract: Real-world networks usually have community structure, that is, nodes are\ngrouped into densely connected communities. Community detection is one of the\nmost popular and best-studied research topics in network science and has\nattracted attention in many different fields, including computer science,\nstatistics, social sciences, among others. Numerous approaches for community\ndetection have been proposed in literature, from ad-hoc algorithms to\nsystematic model-based approaches. The large number of available methods leads\nto a fundamental question: whether a certain method can provide consistent\nestimates of community labels. The stochastic blockmodel (SBM) and its variants\nprovide a convenient framework for the study of such problems. This article is\na survey on the recent theoretical advances of community detection. The authors\nreview a number of community detection methods and their theoretical\nproperties, including graph cut methods, profile likelihoods, the\npseudo-likelihood method, the variational method, belief propagation, spectral\nclustering, and semidefinite relaxations of the SBM. The authors also briefly\ndiscuss other research topics in community detection such as robust community\ndetection, community detection with nodal covariates and model selection, as\nwell as suggest a few possible directions for future research. \n\n"}
{"id": "1809.07703", "contents": "Title: Fighting Redundancy and Model Decay with Embeddings Abstract: Every day, hundreds of millions of new Tweets containing over 40 languages of\never-shifting vernacular flow through Twitter. Models that attempt to extract\ninsight from this firehose of information must face the torrential covariate\nshift that is endemic to the Twitter platform. While regularly-retrained\nalgorithms can maintain performance in the face of this shift, fixed model\nfeatures that fail to represent new trends and tokens can quickly become stale,\nresulting in performance degradation. To mitigate this problem we employ\nlearned features, or embedding models, that can efficiently represent the most\nrelevant aspects of a data distribution. Sharing these embedding models across\nteams can also reduce redundancy and multiplicatively increase cross-team\nmodeling productivity. In this paper, we detail the commoditized tools,\nalgorithms and pipelines that we have developed and are developing at Twitter\nto regularly generate high quality, up-to-date embeddings and share them\nbroadly across the company. \n\n"}
{"id": "1809.08151", "contents": "Title: SIC-MMAB: Synchronisation Involves Communication in Multiplayer\n  Multi-Armed Bandits Abstract: Motivated by cognitive radio networks, we consider the stochastic multiplayer\nmulti-armed bandit problem, where several players pull arms simultaneously and\ncollisions occur if one of them is pulled by several players at the same stage.\nWe present a decentralized algorithm that achieves the same performance as a\ncentralized one, contradicting the existing lower bounds for that problem. This\nis possible by \"hacking\" the standard model by constructing a communication\nprotocol between players that deliberately enforces collisions, allowing them\nto share their information at a negligible cost. This motivates the\nintroduction of a more appropriate dynamic setting without sensing, where\nsimilar communication protocols are no longer possible. However, we show that\nthe logarithmic growth of the regret is still achievable for this model with a\nnew algorithm. \n\n"}
{"id": "1809.08830", "contents": "Title: Wasserstein Distributionally Robust Kalman Filtering Abstract: We study a distributionally robust mean square error estimation problem over\na nonconvex Wasserstein ambiguity set containing only normal distributions. We\nshow that the optimal estimator and the least favorable distribution form a\nNash equilibrium. Despite the non-convex nature of the ambiguity set, we prove\nthat the estimation problem is equivalent to a tractable convex program. We\nfurther devise a Frank-Wolfe algorithm for this convex program whose\ndirection-searching subproblem can be solved in a quasi-closed form. Using\nthese ingredients, we introduce a distributionally robust Kalman filter that\nhedges against model risk. \n\n"}
{"id": "1809.09369", "contents": "Title: S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State\n  Representation Learning Abstract: State representation learning aims at learning compact representations from\nraw observations in robotics and control applications. Approaches used for this\nobjective are auto-encoders, learning forward models, inverse dynamics or\nlearning using generic priors on the state characteristics. However, the\ndiversity in applications and methods makes the field lack standard evaluation\ndatasets, metrics and tasks. This paper provides a set of environments, data\ngenerators, robotic control tasks, metrics and tools to facilitate iterative\nstate representation learning and evaluation in reinforcement learning\nsettings. \n\n"}
{"id": "1809.09401", "contents": "Title: Hypergraph Neural Networks Abstract: In this paper, we present a hypergraph neural networks (HGNN) framework for\ndata representation learning, which can encode high-order data correlation in a\nhypergraph structure. Confronting the challenges of learning representation for\ncomplex data in real practice, we propose to incorporate such data structure in\na hypergraph, which is more flexible on data modeling, especially when dealing\nwith complex data. In this method, a hyperedge convolution operation is\ndesigned to handle the data correlation during representation learning. In this\nway, traditional hypergraph learning procedure can be conducted using hyperedge\nconvolution operations efficiently. HGNN is able to learn the hidden layer\nrepresentation considering the high-order data structure, which is a general\nframework considering the complex data correlations. We have conducted\nexperiments on citation network classification and visual object recognition\ntasks and compared HGNN with graph convolutional networks and other traditional\nmethods. Experimental results demonstrate that the proposed HGNN method\noutperforms recent state-of-the-art methods. We can also reveal from the\nresults that the proposed HGNN is superior when dealing with multi-modal data\ncompared with existing methods. \n\n"}
{"id": "1809.09446", "contents": "Title: Nested cross-validation when selecting classifiers is overzealous for\n  most practical applications Abstract: When selecting a classification algorithm to be applied to a particular\nproblem, one has to simultaneously select the best algorithm for that dataset\n\\emph{and} the best set of hyperparameters for the chosen model. The usual\napproach is to apply a nested cross-validation procedure; hyperparameter\nselection is performed in the inner cross-validation, while the outer\ncross-validation computes an unbiased estimate of the expected accuracy of the\nalgorithm \\emph{with cross-validation based hyperparameter tuning}. The\nalternative approach, which we shall call `flat cross-validation', uses a\nsingle cross-validation step both to select the optimal hyperparameter values\nand to provide an estimate of the expected accuracy of the algorithm, that\nwhile biased may nevertheless still be used to select the best learning\nalgorithm. We tested both procedures using 12 different algorithms on 115 real\nlife binary datasets and conclude that using the less computationally expensive\nflat cross-validation procedure will generally result in the selection of an\nalgorithm that is, for all practical purposes, of similar quality to that\nselected via nested cross-validation, provided the learning algorithms have\nrelatively few hyperparameters to be optimised. \n\n"}
{"id": "1809.10241", "contents": "Title: Classifying Mammographic Breast Density by Residual Learning Abstract: Mammographic breast density, a parameter used to describe the proportion of\nbreast tissue fibrosis, is widely adopted as an evaluation characteristic of\nthe likelihood of breast cancer incidence. In this study, we present a\nradiomics approach based on residual learning for the classification of\nmammographic breast densities. Our method possesses several encouraging\nproperties such as being almost fully automatic, possessing big model capacity\nand flexibility. It can obtain outstanding classification results without the\nnecessity of result compensation using mammographs taken from different views.\nThe proposed method was instantiated with the INbreast dataset and\nclassification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS\n(Breast Imaging and Reporting Data System) category task and the two BI-RADS\ncategory task,respectively. The superior performances achieved compared to the\nexisting state-of-the-art methods along with its encouraging properties\nindicate that our method has a great potential to be applied as a\ncomputer-aided diagnosis tool. \n\n"}
{"id": "1809.10271", "contents": "Title: Batch-normalized Recurrent Highway Networks Abstract: Gradient control plays an important role in feed-forward networks applied to\nvarious computer vision tasks. Previous work has shown that Recurrent Highway\nNetworks minimize the problem of vanishing or exploding gradients. They achieve\nthis by setting the eigenvalues of the temporal Jacobian to 1 across the time\nsteps. In this work, batch normalized recurrent highway networks are proposed\nto control the gradient flow in an improved way for network convergence.\nSpecifically, the introduced model can be formed by batch normalizing the\ninputs at each recurrence loop. The proposed model is tested on an image\ncaptioning task using MSCOCO dataset. Experimental results indicate that the\nbatch normalized recurrent highway networks converge faster and performs better\ncompared with the traditional LSTM and RHN based models. \n\n"}
{"id": "1809.10312", "contents": "Title: Vector Learning for Cross Domain Representations Abstract: Recently, generative adversarial networks have gained a lot of popularity for\nimage generation tasks. However, such models are associated with complex\nlearning mechanisms and demand very large relevant datasets. This work borrows\nconcepts from image and video captioning models to form an image generative\nframework. The model is trained in a similar fashion as recurrent captioning\nmodel and uses the learned weights for image generation. This is done in an\ninverse direction, where the input is a caption and the output is an image. The\nvector representation of the sentence and frames are extracted from an\nencoder-decoder model which is initially trained on similar sentence and image\npairs. Our model conditions image generation on a natural language caption. We\nleverage a sequence-to-sequence model to generate synthetic captions that have\nthe same meaning for having a robust image generation. One key advantage of our\nmethod is that the traditional image captioning datasets can be used for\nsynthetic sentence paraphrases. Results indicate that images generated through\nmultiple captions are better at capturing the semantic meaning of the family of\ncaptions. \n\n"}
{"id": "1809.10658", "contents": "Title: Learning to Coordinate Multiple Reinforcement Learning Agents for\n  Diverse Query Reformulation Abstract: We propose a method to efficiently learn diverse strategies in reinforcement\nlearning for query reformulation in the tasks of document retrieval and\nquestion answering. In the proposed framework an agent consists of multiple\nspecialized sub-agents and a meta-agent that learns to aggregate the answers\nfrom sub-agents to produce a final answer. Sub-agents are trained on disjoint\npartitions of the training data, while the meta-agent is trained on the full\ntraining set. Our method makes learning faster, because it is highly\nparallelizable, and has better generalization performance than strong\nbaselines, such as an ensemble of agents trained on the full data. We show that\nthe improved performance is due to the increased diversity of reformulation\nstrategies. \n\n"}
{"id": "1809.10680", "contents": "Title: Supervised Nonnegative Matrix Factorization to Predict ICU Mortality\n  Risk Abstract: ICU mortality risk prediction is a tough yet important task. On one hand, due\nto the complex temporal data collected, it is difficult to identify the\neffective features and interpret them easily; on the other hand, good\nprediction can help clinicians take timely actions to prevent the mortality.\nThese correspond to the interpretability and accuracy problems. Most existing\nmethods lack of the interpretability, but recently Subgraph Augmented\nNonnegative Matrix Factorization (SANMF) has been successfully applied to time\nseries data to provide a path to interpret the features well. Therefore, we\nadopted this approach as the backbone to analyze the patient data. One\nlimitation of the raw SANMF method is its poor prediction ability due to its\nunsupervised nature. To deal with this problem, we proposed a supervised SANMF\nalgorithm by integrating the logistic regression loss function into the NMF\nframework and solved it with an alternating optimization procedure. We used the\nsimulation data to verify the effectiveness of this method, and then we applied\nit to ICU mortality risk prediction and demonstrated its superiority over other\nconventional supervised NMF methods. \n\n"}
{"id": "1809.10789", "contents": "Title: An Empirical Comparison of Syllabuses for Curriculum Learning Abstract: Syllabuses for curriculum learning have been developed on an ad-hoc, per task\nbasis and little is known about the relative performance of different\nsyllabuses. We identify a number of syllabuses used in the literature. We\ncompare the identified syllabuses based on their effect on the speed of\nlearning and generalization ability of a LSTM network on three sequential\nlearning tasks. We find that the choice of syllabus has limited effect on the\ngeneralization ability of a trained network. In terms of speed of learning our\nresults demonstrate that the best syllabus is task dependent but that a\nrecently proposed automated curriculum learning approach - Predictive Gain,\nperforms very competitively against all identified hand-crafted syllabuses. The\nbest performing hand-crafted syllabus which we term Look Back and Forward\ncombines a syllabus which steps through tasks in the order of their difficulty\nwith a uniform distribution over all tasks. Our experimental results provide an\nempirical basis for the choice of syllabus on a new problem that could benefit\nfrom curriculum learning. Additionally, insights derived from our results shed\nlight on how to successfully design new syllabuses. \n\n"}
{"id": "1809.11096", "contents": "Title: Large Scale GAN Training for High Fidelity Natural Image Synthesis Abstract: Despite recent progress in generative image modeling, successfully generating\nhigh-resolution, diverse samples from complex datasets such as ImageNet remains\nan elusive goal. To this end, we train Generative Adversarial Networks at the\nlargest scale yet attempted, and study the instabilities specific to such\nscale. We find that applying orthogonal regularization to the generator renders\nit amenable to a simple \"truncation trick,\" allowing fine control over the\ntrade-off between sample fidelity and variety by reducing the variance of the\nGenerator's input. Our modifications lead to models which set the new state of\nthe art in class-conditional image synthesis. When trained on ImageNet at\n128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of\n166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous\nbest IS of 52.52 and FID of 18.6. \n\n"}
{"id": "1810.00500", "contents": "Title: One Network to Solve All ROIs: Deep Learning CT for Any ROI using\n  Differentiated Backprojection Abstract: Computed tomography for region-of-interest (ROI) reconstruction has\nadvantages of reducing X-ray radiation dose and using a small detector.\nHowever, standard analytic reconstruction methods suffer from severe cupping\nartifacts, and existing model-based iterative reconstruction methods require\nextensive computations. Recently, we proposed a deep neural network to learn\nthe cupping artifact, but the network is not well generalized for different\nROIs due to the singularities in the corrupted images. Therefore, there is an\nincreasing demand for a neural network that works well for any ROI sizes. In\nthis paper, two types of neural networks are designed. The first type learns\nROI size-specific cupping artifacts from the analytic reconstruction images,\nwhereas the second type network is to learn to invert the finite Hilbert\ntransform from the truncated differentiated backprojection (DBP) data. Their\ngeneralizability for any ROI sizes is then examined. Experimental results show\nthat the new type of neural network significantly outperforms the existing\niterative methods for any ROI size in spite of significantly reduced run-time\ncomplexity. Since the proposed method consistently surpasses existing methods\nfor any ROIs, it can be used as a general CT reconstruction engine for many\npractical applications without compromising possible detector truncation. \n\n"}
{"id": "1810.00760", "contents": "Title: Riemannian Adaptive Optimization Methods Abstract: Several first order stochastic optimization methods commonly used in the\nEuclidean domain such as stochastic gradient descent (SGD), accelerated\ngradient descent or variance reduced methods have already been adapted to\ncertain Riemannian settings. However, some of the most popular of these\noptimization tools - namely Adam , Adagrad and the more recent Amsgrad - remain\nto be generalized to Riemannian manifolds. We discuss the difficulty of\ngeneralizing such adaptive schemes to the most agnostic Riemannian setting, and\nthen provide algorithms and convergence proofs for geodesically convex\nobjectives in the particular case of a product of Riemannian manifolds, in\nwhich adaptivity is implemented across manifolds in the cartesian product. Our\ngeneralization is tight in the sense that choosing the Euclidean space as\nRiemannian manifold yields the same algorithms and regret bounds as those that\nwere already known for the standard algorithms. Experimentally, we show faster\nconvergence and to a lower train loss value for Riemannian adaptive methods\nover their corresponding baselines on the realistic task of embedding the\nWordNet taxonomy in the Poincare ball. \n\n"}
{"id": "1810.00825", "contents": "Title: Set Transformer: A Framework for Attention-based Permutation-Invariant\n  Neural Networks Abstract: Many machine learning tasks such as multiple instance learning, 3D shape\nrecognition, and few-shot image classification are defined on sets of\ninstances. Since solutions to such problems do not depend on the order of\nelements of the set, models used to address them should be permutation\ninvariant. We present an attention-based neural network module, the Set\nTransformer, specifically designed to model interactions among elements in the\ninput set. The model consists of an encoder and a decoder, both of which rely\non attention mechanisms. In an effort to reduce computational complexity, we\nintroduce an attention scheme inspired by inducing point methods from sparse\nGaussian process literature. It reduces the computation time of self-attention\nfrom quadratic to linear in the number of elements in the set. We show that our\nmodel is theoretically attractive and we evaluate it on a range of tasks,\ndemonstrating the state-of-the-art performance compared to recent methods for\nset-structured data. \n\n"}
{"id": "1810.00846", "contents": "Title: Classification from Positive, Unlabeled and Biased Negative Data Abstract: In binary classification, there are situations where negative (N) data are\ntoo diverse to be fully labeled and we often resort to positive-unlabeled (PU)\nlearning in these scenarios. However, collecting a non-representative N set\nthat contains only a small portion of all possible N data can often be much\neasier in practice. This paper studies a novel classification framework which\nincorporates such biased N (bN) data in PU learning. We provide a method based\non empirical risk minimization to address this PUbN classification problem. Our\napproach can be regarded as a novel example-weighting algorithm, with the\nweight of each example computed through a preliminary step that draws\ninspiration from PU learning. We also derive an estimation error bound for the\nproposed method. Experimental results demonstrate the effectiveness of our\nalgorithm in not only PUbN learning scenarios but also ordinary PU learning\nscenarios on several benchmark datasets. \n\n"}
{"id": "1810.00974", "contents": "Title: Neural Regression Trees Abstract: Regression-via-Classification (RvC) is the process of converting a regression\nproblem to a classification one. Current approaches for RvC use ad-hoc\ndiscretization strategies and are suboptimal. We propose a neural regression\ntree model for RvC. In this model, we employ a joint optimization framework\nwhere we learn optimal discretization thresholds while simultaneously\noptimizing the features for each node in the tree. We empirically show the\nvalidity of our model by testing it on two challenging regression tasks where\nwe establish the state of the art. \n\n"}
{"id": "1810.01097", "contents": "Title: Quantization-Aware Phase Retrieval Abstract: We address the problem of phase retrieval (PR) from quantized measurements.\nThe goal is to reconstruct a signal from quadratic measurements encoded with a\nfinite precision, which is indeed the case in many practical applications. We\ndevelop a rank-1 projection algorithm that recovers the signal subject to\nensuring consistency with the measurement, that is, the recovered signal when\nencoded must yield the same set of measurements that one started with. The\nrank-1 projection stems from the idea of lifting, originally proposed in the\ncontext of PhaseLift. The consistency criterion is enforced using a one-sided\nquadratic cost. We also determine the probability with which different vectors\nlead to the same set of quantized measurements, which makes it impossible to\nresolve them. Naturally, this probability depends on how correlated such\nvectors are, and how coarsely/finely the measurements get quantized. The\nproposed algorithm is also capable of incorporating a sparsity constraint on\nthe signal. An analysis of the cost function reveals that it is bounded, both\nabove and below, by functions that are dependent on how well correlated the\nestimate is with the ground truth. We also derive the Cram\\'er-Rao lower bound\n(CRB) on the achievable reconstruction accuracy. A comparison with the\nstate-of-the- art algorithms shows that the proposed algorithm has a higher\nreconstruction accuracy and is about 2 to 3 dB away from the CRB. The edge, in\nterms of the reconstruction signal-to-noise ratio, over the competing\nalgorithms is higher (about 5 to 6 dB) when the quantization is coarse. \n\n"}
{"id": "1810.01400", "contents": "Title: Sketching for Latent Dirichlet-Categorical Models Abstract: Recent work has explored transforming data sets into smaller, approximate\nsummaries in order to scale Bayesian inference. We examine a related problem in\nwhich the parameters of a Bayesian model are very large and expensive to store\nin memory, and propose more compact representations of parameter values that\ncan be used during inference. We focus on a class of graphical models that we\nrefer to as latent Dirichlet-Categorical models, and show how a combination of\ntwo sketching algorithms known as count-min sketch and approximate counters\nprovide an efficient representation for them. We show that this sketch\ncombination -- which, despite having been used before in NLP applications, has\nnot been previously analyzed -- enjoys desirable properties. We prove that for\nthis class of models, when the sketches are used during Markov Chain Monte\nCarlo inference, the equilibrium of sketched MCMC converges to that of the\nexact chain as sketch parameters are tuned to reduce the error rate. \n\n"}
{"id": "1810.01414", "contents": "Title: PromID: human promoter prediction by deep learning Abstract: Computational identification of promoters is notoriously difficult as human\ngenes often have unique promoter sequences that provide regulation of\ntranscription and interaction with transcription initiation complex. While\nthere are many attempts to develop computational promoter identification\nmethods, we have no reliable tool to analyze long genomic sequences. In this\nwork we further develop our deep learning approach that was relatively\nsuccessful to discriminate short promoter and non-promoter sequences. Instead\nof focusing on the classification accuracy, in this work we predict the exact\npositions of the TSS inside the genomic sequences testing every possible\nlocation. We studied human promoters to find effective regions for\ndiscrimination and built corresponding deep learning models. These models use\nadaptively constructed negative set which iteratively improves the models\ndiscriminative ability. The developed promoter identification models\nsignificantly outperform the previously developed promoter prediction programs\nby considerably reducing the number of false positive predictions. The best\nmodel we have built has recall 0.76, precision 0.77 and MCC 0.76, while the\nnext best tool FPROM achieved precision 0.48 and MCC 0.60 for the recall of\n0.75. Our method is available at http://www.cbrc.kaust.edu.sa/PromID/. \n\n"}
{"id": "1810.01765", "contents": "Title: Predicting Factuality of Reporting and Bias of News Media Sources Abstract: We present a study on predicting the factuality of reporting and bias of news\nmedia. While previous work has focused on studying the veracity of claims or\ndocuments, here we are interested in characterizing entire news media. These\nare under-studied but arguably important research problems, both in their own\nright and as a prior for fact-checking systems. We experiment with a large list\nof news websites and with a rich set of features derived from (i) a sample of\narticles from the target news medium, (ii) its Wikipedia page, (iii) its\nTwitter account, (iv) the structure of its URL, and (v) information about the\nWeb traffic it attracts. The experimental results show sizable performance\ngains over the baselines, and confirm the importance of each feature type. \n\n"}
{"id": "1810.01861", "contents": "Title: Inhibited Softmax for Uncertainty Estimation in Neural Networks Abstract: We present a new method for uncertainty estimation and out-of-distribution\ndetection in neural networks with softmax output. We extend softmax layer with\nan additional constant input. The corresponding additional output is able to\nrepresent the uncertainty of the network. The proposed method requires neither\nadditional parameters nor multiple forward passes nor input preprocessing nor\nout-of-distribution datasets. We show that our method performs comparably to\nmore computationally expensive methods and outperforms baselines on our\nexperiments from image recognition and sentiment analysis domains. \n\n"}
{"id": "1810.01876", "contents": "Title: Spurious samples in deep generative models: bug or feature? Abstract: Traditional wisdom in generative modeling literature is that spurious samples\nthat a model can generate are errors and they should be avoided. Recent\nresearch, however, has shown interest in studying or even exploiting such\nsamples instead of eliminating them. In this paper, we ask the question whether\nsuch samples can be eliminated all together without sacrificing coverage of the\ngenerating distribution. For the class of models we consider, we experimentally\ndemonstrate that this is not possible without losing the ability to model some\nof the test samples. While our results need to be confirmed on a broader set of\nmodel families, these initial findings provide partial evidence that spurious\nsamples share structural properties with the learned dataset, which, in turn,\nsuggests they are not simply errors but a feature of deep generative nets. \n\n"}
{"id": "1810.02176", "contents": "Title: Adaptive Policies for Perimeter Surveillance Problems Abstract: Maximising the detection of intrusions is a fundamental and often critical\naim of perimeter surveillance. Commonly, this requires a decision-maker to\noptimally allocate multiple searchers to segments of the perimeter. We consider\na scenario where the decision-maker may sequentially update the searchers'\nallocation, learning from the observed data to improve decisions over time. In\nthis work we propose a formal model and solution methods for this sequential\nperimeter surveillance problem. Our model is a combinatorial multi-armed bandit\n(CMAB) with Poisson rewards and a novel filtered feedback mechanism - arising\nfrom the failure to detect certain intrusions. Our solution method is an upper\nconfidence bound approach and we derive upper and lower bounds on its expected\nperformance. We prove that the gap between these bounds is of constant order,\nand demonstrate empirically that our approach is more reliable in simulated\nproblems than competing algorithms. \n\n"}
{"id": "1810.02180", "contents": "Title: Improved Generalization Bounds for Adversarially Robust Learning Abstract: We consider a model of robust learning in an adversarial environment. The\nlearner gets uncorrupted training data with access to possible corruptions that\nmay be affected by the adversary during testing. The learner's goal is to build\na robust classifier, which will be tested on future adversarial examples. The\nadversary is limited to $k$ possible corruptions for each input. We model the\nlearner-adversary interaction as a zero-sum game. This model is closely related\nto the adversarial examples model of Schmidt et al. (2018); Madry et al.\n(2017).\n  Our main results consist of generalization bounds for the binary and\nmulticlass classification, as well as the real-valued case (regression). For\nthe binary classification setting, we both tighten the generalization bound of\nFeige et al. (2015), and are also able to handle infinite hypothesis classes.\nThe sample complexity is improved from\n$O(\\frac{1}{\\epsilon^4}\\log(\\frac{|H|}{\\delta}))$ to\n$O\\big(\\frac{1}{\\epsilon^2}(kVC(H)\\log^{\\frac{3}{2}+\\alpha}(kVC(H))+\\log(\\frac{1}{\\delta})\\big)$\nfor any $\\alpha > 0$. Additionally, we extend the algorithm and generalization\nbound from the binary to the multiclass and real-valued cases. Along the way,\nwe obtain results on fat-shattering dimension and Rademacher complexity of\n$k$-fold maxima over function classes; these may be of independent interest.\n  For binary classification, the algorithm of Feige et al. (2015) uses a regret\nminimization algorithm and an ERM oracle as a black box; we adapt it for the\nmulticlass and regression settings. The algorithm provides us with near-optimal\npolicies for the players on a given training sample. \n\n"}
{"id": "1810.02266", "contents": "Title: Concept-drifting Data Streams are Time Series; The Case for Continuous\n  Adaptation Abstract: Learning from data streams is an increasingly important topic in data mining,\nmachine learning, and artificial intelligence in general. A major focus in the\ndata stream literature is on designing methods that can deal with concept\ndrift, a challenge where the generating distribution changes over time. A\ngeneral assumption in most of this literature is that instances are\nindependently distributed in the stream. In this work we show that, in the\ncontext of concept drift, this assumption is contradictory, and that the\npresence of concept drift necessarily implies temporal dependence; and thus\nsome form of time series. This has important implications on model design and\ndeployment. We explore and highlight the these implications, and show that\nHoeffding-tree based ensembles, which are very popular for learning in streams,\nare not naturally suited to learning \\emph{within} drift; and can perform in\nthis scenario only at significant computational cost of destructive adaptation.\nOn the other hand, we develop and parameterize gradient-descent methods and\ndemonstrate how they can perform \\emph{continuous} adaptation with no explicit\ndrift-detection mechanism, offering major advantages in terms of accuracy and\nefficiency. As a consequence of our theoretical discussion and empirical\nobservations, we outline a number of recommendations for deploying methods in\nconcept-drifting streams. \n\n"}
{"id": "1810.02424", "contents": "Title: Feature Prioritization and Regularization Improve Standard Accuracy and\n  Adversarial Robustness Abstract: Adversarial training has been successfully applied to build robust models at\na certain cost. While the robustness of a model increases, the standard\nclassification accuracy declines. This phenomenon is suggested to be an\ninherent trade-off. We propose a model that employs feature prioritization by a\nnonlinear attention module and $L_2$ feature regularization to improve the\nadversarial robustness and the standard accuracy relative to adversarial\ntraining. The attention module encourages the model to rely heavily on robust\nfeatures by assigning larger weights to them while suppressing non-robust\nfeatures. The regularizer encourages the model to extract similar features for\nthe natural and adversarial images, effectively ignoring the added\nperturbation. In addition to evaluating the robustness of our model, we provide\njustification for the attention module and propose a novel experimental\nstrategy that quantitatively demonstrates that our model is almost ideally\naligned with salient data characteristics. Additional experimental results\nillustrate the power of our model relative to the state of the art methods. \n\n"}
{"id": "1810.02528", "contents": "Title: Local Stability and Performance of Simple Gradient Penalty\n  mu-Wasserstein GAN Abstract: Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance\nbetween a data distribution and sample distribution. Recent studies have\nproposed stabilizing the training process for the WGAN and implementing the\nLipschitz constraint. In this study, we prove the local stability of optimizing\nthe simple gradient penalty $\\mu$-WGAN(SGP $\\mu$-WGAN) under suitable\nassumptions regarding the equilibrium and penalty measure $\\mu$. The measure\nvalued differentiation concept is employed to deal with the derivative of the\npenalty terms, which is helpful for handling abstract singular measures with\nlower dimensional support. Based on this analysis, we claim that penalizing the\ndata manifold or sample manifold is the key to regularizing the original WGAN\nwith a gradient penalty. Experimental results obtained with unintuitive penalty\nmeasures that satisfy our assumptions are also provided to support our\ntheoretical results. \n\n"}
{"id": "1810.02837", "contents": "Title: Scaling Submodular Optimization Approaches for Control Applications in\n  Networked Systems Abstract: Often times, in many design problems, there is a need to select a small set\nof informative or representative elements from a large ground set of entities\nin an optimal fashion. Submodular optimization that provides for a formal way\nto solve such problems, has recently received significant attention from the\ncontrols community where such subset selection problems are abound. However,\nscaling these approaches to large systems can be challenging because of the\nhigh computational complexity of the overall flow, in-part due to the\nhigh-complexity compute-oracles used to determine the objective function\nvalues. In this work, we explore a well-known paradigm, namely leader-selection\nin a multi-agent networked environment to illustrate strategies for scalable\nsubmodular optimization. We study the performance of the state-of-the-art\nstochastic and distributed greedy algorithms as well as explore techniques that\naccelerate the computation oracles within the optimization loop. We finally\npresent results combining accelerated greedy algorithms with accelerated\ncomputation oracles and demonstrate significant speedups with little loss of\noptimality when compared to the baseline ordinary greedy algorithm. \n\n"}
{"id": "1810.03307", "contents": "Title: Local Explanation Methods for Deep Neural Networks Lack Sensitivity to\n  Parameter Values Abstract: Explaining the output of a complicated machine learning model like a deep\nneural network (DNN) is a central challenge in machine learning. Several\nproposed local explanation methods address this issue by identifying what\ndimensions of a single input are most responsible for a DNN's output. The goal\nof this work is to assess the sensitivity of local explanations to DNN\nparameter values. Somewhat surprisingly, we find that DNNs with\nrandomly-initialized weights produce explanations that are both visually and\nquantitatively similar to those produced by DNNs with learned weights. Our\nconjecture is that this phenomenon occurs because these explanations are\ndominated by the lower level features of a DNN, and that a DNN's architecture\nprovides a strong prior which significantly affects the representations learned\nat these lower layers. NOTE: This work is now subsumed by our recent\nmanuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we\nexpand on findings and address concerns raised in Sundararajan et. al. (2018). \n\n"}
{"id": "1810.03389", "contents": "Title: Rethinking Breiman's Dilemma in Neural Networks: Phase Transitions of\n  Margin Dynamics Abstract: Margin enlargement over training data has been an important strategy since\nperceptrons in machine learning for the purpose of boosting the robustness of\nclassifiers toward a good generalization ability. Yet Breiman (1999) showed a\ndilemma that a uniform improvement on margin distribution does NOT necessarily\nreduces generalization errors. In this paper, we revisit Breiman's dilemma in\ndeep neural networks with recently proposed spectrally normalized margins, from\na novel perspective based on phase transitions of normalized margin\ndistributions in training dynamics. Normalized margin distribution of a\nclassifier over the data, can be divided into two parts: low/small margins such\nas some negative margins for misclassified samples vs. high/large margins for\nhigh confident correctly classified samples, that often behave differently\nduring the training process. Low margins for training and test datasets are\noften effectively reduced in training, along with reductions of training and\ntest errors; while high margins may exhibit different dynamics, reflecting the\ntrade-off between expressive power of models and complexity of data. When data\ncomplexity is comparable to the model expressiveness, high margin distributions\nfor both training and test data undergo similar decrease-increase phase\ntransitions during training. In such cases, one can predict the trend of\ngeneralization or test error by margin-based generalization bounds with\nrestricted Rademacher complexities, shown in two ways in this paper with early\nstopping time exploiting such phase transitions. On the other hand,\nover-expressive models may have both low and high training margins undergoing\nuniform improvements, with a distinct phase transition in test margin dynamics.\nThis reconfirms the Breiman's dilemma associated with overparameterized neural\nnetworks where margins fail to predict overfitting. \n\n"}
{"id": "1810.03527", "contents": "Title: CHOPT : Automated Hyperparameter Optimization Framework for Cloud-Based\n  Machine Learning Platforms Abstract: Many hyperparameter optimization (HyperOpt) methods assume restricted\ncomputing resources and mainly focus on enhancing performance. Here we propose\na novel cloud-based HyperOpt (CHOPT) framework which can efficiently utilize\nshared computing resources while supporting various HyperOpt algorithms. We\nincorporate convenient web-based user interfaces, visualization, and analysis\ntools, enabling users to easily control optimization procedures and build up\nvaluable insights with an iterative analysis procedure. Furthermore, our\nframework can be incorporated with any cloud platform, thus complementarily\nincreasing the efficiency of conventional deep learning frameworks. We\ndemonstrate applications of CHOPT with tasks such as image recognition and\nquestion-answering, showing that our framework can find hyperparameter\nconfigurations competitive with previous work. We also show CHOPT is capable of\nproviding interesting observations through its analysing tools \n\n"}
{"id": "1810.03764", "contents": "Title: Generalized Latent Variable Recovery for Generative Adversarial Networks Abstract: The Generator of a Generative Adversarial Network (GAN) is trained to\ntransform latent vectors drawn from a prior distribution into realistic looking\nphotos. These latent vectors have been shown to encode information about the\ncontent of their corresponding images. Projecting input images onto the latent\nspace of a GAN is non-trivial, but previous work has successfully performed\nthis task for latent spaces with a uniform prior. We extend these techniques to\nlatent spaces with a Gaussian prior, and demonstrate our technique's\neffectiveness. \n\n"}
{"id": "1810.03806", "contents": "Title: The Adversarial Attack and Detection under the Fisher Information Metric Abstract: Many deep learning models are vulnerable to the adversarial attack, i.e.,\nimperceptible but intentionally-designed perturbations to the input can cause\nincorrect output of the networks. In this paper, using information geometry, we\nprovide a reasonable explanation for the vulnerability of deep learning models.\nBy considering the data space as a non-linear space with the Fisher information\nmetric induced from a neural network, we first propose an adversarial attack\nalgorithm termed one-step spectral attack (OSSA). The method is described by a\nconstrained quadratic form of the Fisher information matrix, where the optimal\nadversarial perturbation is given by the first eigenvector, and the model\nvulnerability is reflected by the eigenvalues. The larger an eigenvalue is, the\nmore vulnerable the model is to be attacked by the corresponding eigenvector.\nTaking advantage of the property, we also propose an adversarial detection\nmethod with the eigenvalues serving as characteristics. Both our attack and\ndetection algorithms are numerically optimized to work efficiently on large\ndatasets. Our evaluations show superior performance compared with other\nmethods, implying that the Fisher information is a promising approach to\ninvestigate the adversarial attacks and defenses. \n\n"}
{"id": "1810.04021", "contents": "Title: Deep Geodesic Learning for Segmentation and Anatomical Landmarking Abstract: In this paper, we propose a novel deep learning framework for anatomy\nsegmentation and automatic landmark- ing. Specifically, we focus on the\nchallenging problem of mandible segmentation from cone-beam computed tomography\n(CBCT) scans and identification of 9 anatomical landmarks of the mandible on\nthe geodesic space. The overall approach employs three inter-related steps. In\nstep 1, we propose a deep neu- ral network architecture with carefully designed\nregularization, and network hyper-parameters to perform image segmentation\nwithout the need for data augmentation and complex post- processing refinement.\nIn step 2, we formulate the landmark localization problem directly on the\ngeodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose\nto use a long short-term memory (LSTM) network to identify closely- spaced\nlandmarks, which is rather difficult to obtain using other standard detection\nnetworks. The proposed fully automated method showed superior efficacy compared\nto the state-of-the- art mandible segmentation and landmarking approaches in\ncraniofacial anomalies and diseased states. We used a very challenging CBCT\ndataset of 50 patients with a high-degree of craniomaxillofacial (CMF)\nvariability that is realistic in clinical practice. Complementary to the\nquantitative analysis, the qualitative visual inspection was conducted for\ndistinct CBCT scans from 250 patients with high anatomical variability. We have\nalso shown feasibility of the proposed work in an independent dataset from\nMICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance.\nLastly, we present an in-depth analysis of the proposed deep networks with\nrespect to the choice of hyper-parameters such as pooling and activation\nfunctions. \n\n"}
{"id": "1810.04642", "contents": "Title: Virtual Battery Parameter Identification using Transfer Learning based\n  Stacked Autoencoder Abstract: Recent studies have shown that the aggregated dynamic flexibility of an\nensemble of thermostatic loads can be modeled in the form of a virtual battery.\nThe existing methods for computing the virtual battery parameters require the\nknowledge of the first-principle models and parameter values of the loads in\nthe ensemble. In real-world applications, however, it is likely that the only\navailable information are end-use measurements such as power consumption, room\ntemperature, device on/off status, etc., while very little about the individual\nload models and parameters are known. We propose a transfer learning based deep\nnetwork framework for calculating virtual battery state of a given ensemble of\nflexible thermostatic loads, from the available end-use measurements. This\nproposed framework extracts first order virtual battery model parameters for\nthe given ensemble. We illustrate the effectiveness of this novel framework on\ndifferent ensembles of ACs and WHs. \n\n"}
{"id": "1810.05713", "contents": "Title: Improving Generalization of Sequence Encoder-Decoder Networks for\n  Inverse Imaging of Cardiac Transmembrane Potential Abstract: Deep learning models have shown state-of-the-art performance in many inverse\nreconstruction problems. However, it is not well understood what properties of\nthe latent representation may improve the generalization ability of the\nnetwork. Furthermore, limited models have been presented for inverse\nreconstructions over time sequences. In this paper, we study the generalization\nability of a sequence encoder decoder model for solving inverse reconstructions\non time sequences. Our central hypothesis is that the generalization ability of\nthe network can be improved by 1) constrained stochasticity and 2) global\naggregation of temporal information in the latent space. First, drawing from\nanalytical learning theory, we theoretically show that a stochastic latent\nspace will lead to an improved generalization ability. Second, we consider an\nLSTM encoder-decoder architecture that compresses a global latent vector from\nall last-layer units in the LSTM encoder. This model is compared with\nalternative LSTM encoder-decoder architectures, each in deterministic and\nstochastic versions. The results demonstrate that the generalization ability of\nan inverse reconstruction network can be improved by constrained stochasticity\ncombined with global aggregation of temporal information in the latent space. \n\n"}
{"id": "1810.05724", "contents": "Title: Unpaired High-Resolution and Scalable Style Transfer Using Generative\n  Adversarial Networks Abstract: Neural networks have proven their capabilities by outperforming many other\napproaches on regression or classification tasks on various kinds of data.\nOther astonishing results have been achieved using neural nets as data\ngenerators, especially in settings of generative adversarial networks (GANs).\nOne special application is the field of image domain translations. Here, the\ngoal is to take an image with a certain style (e.g. a photography) and\ntransform it into another one (e.g. a painting). If such a task is performed\nfor unpaired training examples, the corresponding GAN setting is complex, the\nneural networks are large, and this leads to a high peak memory consumption\nduring, both, training and evaluation phase. This sets a limit to the highest\nprocessable image size. We address this issue by the idea of not processing the\nwhole image at once, but to train and evaluate the domain translation on the\nlevel of overlapping image subsamples. This new approach not only enables us to\ntranslate high-resolution images that otherwise cannot be processed by the\nneural network at once, but also allows us to work with comparably small neural\nnetworks and with limited hardware resources. Additionally, the number of\nimages required for the training process is significantly reduced. We present\nhigh-quality results on images with a total resolution of up to over 50\nmegapixels and emonstrate that our method helps to preserve local image details\nwhile it also keeps global consistency. \n\n"}
{"id": "1810.05726", "contents": "Title: DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning Abstract: Robotic weed control has seen increased research of late with its potential\nfor boosting productivity in agriculture. Majority of works focus on developing\nrobotics for croplands, ignoring the weed management problems facing rangeland\nstock farmers. Perhaps the greatest obstacle to widespread uptake of robotic\nweed control is the robust classification of weed species in their natural\nenvironment. The unparalleled successes of deep learning make it an ideal\ncandidate for recognising various weed species in the complex rangeland\nenvironment. This work contributes the first large, public, multiclass image\ndataset of weed species from the Australian rangelands; allowing for the\ndevelopment of robust classification methods to make robotic weed control\nviable. The DeepWeeds dataset consists of 17,509 labelled images of eight\nnationally significant weed species native to eight locations across northern\nAustralia. This paper presents a baseline for classification performance on the\ndataset using the benchmark deep learning models, Inception-v3 and ResNet-50.\nThese models achieved an average classification accuracy of 95.1% and 95.7%,\nrespectively. We also demonstrate real time performance of the ResNet-50\narchitecture, with an average inference time of 53.4 ms per image. These strong\nresults bode well for future field implementation of robotic weed control\nmethods in the Australian rangelands. \n\n"}
{"id": "1810.06509", "contents": "Title: Predictor-Corrector Policy Optimization Abstract: We present a predictor-corrector framework, called PicCoLO, that can\ntransform a first-order model-free reinforcement or imitation learning\nalgorithm into a new hybrid method that leverages predictive models to\naccelerate policy learning. The new \"PicCoLOed\" algorithm optimizes a policy by\nrecursively repeating two steps: In the Prediction Step, the learner uses a\nmodel to predict the unseen future gradient and then applies the predicted\nestimate to update the policy; in the Correction Step, the learner runs the\nupdated policy in the environment, receives the true gradient, and then\ncorrects the policy using the gradient error. Unlike previous algorithms,\nPicCoLO corrects for the mistakes of using imperfect predicted gradients and\nhence does not suffer from model bias. The development of PicCoLO is made\npossible by a novel reduction from predictable online learning to adversarial\nonline learning, which provides a systematic way to modify existing first-order\nalgorithms to achieve the optimal regret with respect to predictable\ninformation. We show, in both theory and simulation, that the convergence rate\nof several first-order model-free algorithms can be improved by PicCoLO. \n\n"}
{"id": "1810.06640", "contents": "Title: Adversarial Text Generation Without Reinforcement Learning Abstract: Generative Adversarial Networks (GANs) have experienced a recent surge in\npopularity, performing competitively in a variety of tasks, especially in\ncomputer vision. However, GAN training has shown limited success in natural\nlanguage processing. This is largely because sequences of text are discrete,\nand thus gradients cannot propagate from the discriminator to the generator.\nRecent solutions use reinforcement learning to propagate approximate gradients\nto the generator, but this is inefficient to train. We propose to utilize an\nautoencoder to learn a low-dimensional representation of sentences. A GAN is\nthen trained to generate its own vectors in this space, which decode to\nrealistic utterances. We report both random and interpolated samples from the\ngenerator. Visualization of sentence vectors indicate our model correctly\nlearns the latent space of the autoencoder. Both human ratings and BLEU scores\nshow that our model generates realistic text against competitive baselines. \n\n"}
{"id": "1810.06684", "contents": "Title: Column generation based math-heuristic for classification trees Abstract: This paper explores the use of Column Generation (CG) techniques in\nconstructing univariate binary decision trees for classification tasks. We\npropose a novel Integer Linear Programming (ILP) formulation, based on\nroot-to-leaf paths in decision trees. The model is solved via a Column\nGeneration based heuristic. To speed up the heuristic, we use a restricted\ninstance data by considering a subset of decision splits, sampled from the\nsolutions of the well-known CART algorithm. Extensive numerical experiments\nshow that our approach is competitive with the state-of-the-art ILP-based\nalgorithms. In particular, the proposed approach is capable of handling big\ndata sets with tens of thousands of data rows. Moreover, for large data sets,\nit finds solutions competitive to CART. \n\n"}
{"id": "1810.06755", "contents": "Title: Discovering Fair Representations in the Data Domain Abstract: Interpretability and fairness are critical in computer vision and machine\nlearning applications, in particular when dealing with human outcomes, e.g.\ninviting or not inviting for a job interview based on application materials\nthat may include photographs. One promising direction to achieve fairness is by\nlearning data representations that remove the semantics of protected\ncharacteristics, and are therefore able to mitigate unfair outcomes. All\navailable models however learn latent embeddings which comes at the cost of\nbeing uninterpretable. We propose to cast this problem as data-to-data\ntranslation, i.e. learning a mapping from an input domain to a fair target\ndomain, where a fairness definition is being enforced. Here the data domain can\nbe images, or any tabular data representation. This task would be\nstraightforward if we had fair target data available, but this is not the case.\nTo overcome this, we learn a highly unconstrained mapping by exploiting\nstatistics of residuals - the difference between input data and its translated\nversion - and the protected characteristics. When applied to the CelebA dataset\nof face images with gender attribute as the protected characteristic, our model\nenforces equality of opportunity by adjusting the eyes and lips regions.\nIntriguingly, on the same dataset we arrive at similar conclusions when using\nsemantic attribute representations of images for translation. On face images of\nthe recent DiF dataset, with the same gender attribute, our method adjusts nose\nregions. In the Adult income dataset, also with protected gender attribute, our\nmodel achieves equality of opportunity by, among others, obfuscating the wife\nand husband relationship. Analyzing those systematic changes will allow us to\nscrutinize the interplay of fairness criterion, chosen protected\ncharacteristics, and prediction performance. \n\n"}
{"id": "1810.06793", "contents": "Title: Learning Two-layer Neural Networks with Symmetric Inputs Abstract: We give a new algorithm for learning a two-layer neural network under a\ngeneral class of input distributions. Assuming there is a ground-truth\ntwo-layer network $$ y = A \\sigma(Wx) + \\xi, $$ where $A,W$ are weight\nmatrices, $\\xi$ represents noise, and the number of neurons in the hidden layer\nis no larger than the input or output, our algorithm is guaranteed to recover\nthe parameters $A,W$ of the ground-truth network. The only requirement on the\ninput $x$ is that it is symmetric, which still allows highly complicated and\nstructured input.\n  Our algorithm is based on the method-of-moments framework and extends several\nresults in tensor decompositions. We use spectral algorithms to avoid the\ncomplicated non-convex optimization in learning neural networks. Experiments\nshow that our algorithm can robustly learn the ground-truth neural network with\na small number of samples for many symmetric input distributions. \n\n"}
{"id": "1810.06801", "contents": "Title: Quasi-hyperbolic momentum and Adam for deep learning Abstract: Momentum-based acceleration of stochastic gradient descent (SGD) is widely\nused in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM)\nas an extremely simple alteration of momentum SGD, averaging a plain SGD step\nwith a momentum step. We describe numerous connections to and identities with\nother algorithms, and we characterize the set of two-state optimization\nalgorithms that QHM can recover. Finally, we propose a QH variant of Adam\ncalled QHAdam, and we empirically demonstrate that our algorithms lead to\nsignificantly improved training in a variety of settings, including a new\nstate-of-the-art result on WMT16 EN-DE. We hope that these empirical results,\ncombined with the conceptual and practical simplicity of QHM and QHAdam, will\nspur interest from both practitioners and researchers. Code is immediately\navailable. \n\n"}
{"id": "1810.07481", "contents": "Title: Provable Robustness of ReLU networks via Maximization of Linear Regions Abstract: It has been shown that neural network classifiers are not robust. This raises\nconcerns about their usage in safety-critical systems. We propose in this paper\na regularization scheme for ReLU networks which provably improves the\nrobustness of the classifier by maximizing the linear regions of the classifier\nas well as the distance to the decision boundary. Our techniques allow even to\nfind the minimal adversarial perturbation for a fraction of test points for\nlarge networks. In the experiments we show that our approach improves upon\nadversarial training both in terms of lower and upper bounds on the robustness\nand is comparable or better than the state-of-the-art in terms of test error\nand robustness. \n\n"}
{"id": "1810.07725", "contents": "Title: RIn-Close_CVC2: an even more efficient enumerative algorithm for\n  biclustering of numerical datasets Abstract: RIn-Close_CVC is an efficient (take polynomial time per bicluster), complete\n(find all maximal biclusters), correct (all biclusters attend the user-defined\nlevel of consistency) and non-redundant (all the obtained biclusters are\nmaximal and the same bicluster is not enumerated more than once) enumerative\nalgorithm for mining maximal biclusters with constant values on columns in\nnumerical datasets. Despite RIn-Close_CVC has all these outstanding properties,\nit has a high computational cost in terms of memory usage because it must keep\na symbol table in memory to prevent a maximal bicluster to be found more than\nonce. In this paper, we propose a new version of RIn-Close_CVC, named\nRIn-Close_CVC2, that does not use a symbol table to prevent redundant\nbiclusters, and keeps all these four properties. We also prove that these\nalgorithms actually possess these properties. Experiments are carried out with\nsynthetic and real-world datasets to compare RIn-Close_CVC and RIn-Close_CVC2\nin terms of memory usage and runtime. The experimental results show that\nRIn-Close_CVC2 brings a large reduction in memory usage and, in average,\nsignificant runtime gain when compared to its predecessor. \n\n"}
{"id": "1810.07770", "contents": "Title: Small ReLU networks are powerful memorizers: a tight analysis of\n  memorization capacity Abstract: We study finite sample expressivity, i.e., memorization power of ReLU\nnetworks. Recent results require $N$ hidden nodes to memorize/interpolate\narbitrary $N$ data points. In contrast, by exploiting depth, we show that\n3-layer ReLU networks with $\\Omega(\\sqrt{N})$ hidden nodes can perfectly\nmemorize most datasets with $N$ points. We also prove that width\n$\\Theta(\\sqrt{N})$ is necessary and sufficient for memorizing $N$ data points,\nproving tight bounds on memorization capacity. The sufficiency result can be\nextended to deeper networks; we show that an $L$-layer network with $W$\nparameters in the hidden layers can memorize $N$ data points if $W =\n\\Omega(N)$. Combined with a recent upper bound $O(WL\\log W)$ on VC dimension,\nour construction is nearly tight for any fixed $L$. Subsequently, we analyze\nmemorization capacity of residual networks under a general position assumption;\nwe prove results that substantially reduce the known requirement of $N$ hidden\nnodes. Finally, we study the dynamics of stochastic gradient descent (SGD), and\nshow that when initialized near a memorizing global minimum of the empirical\nrisk, SGD quickly finds a nearby point with much smaller empirical risk. \n\n"}
{"id": "1810.07845", "contents": "Title: On Statistical Learning of Simplices: Unmixing Problem Revisited Abstract: We study the sample complexity of learning a high-dimensional simplex from a\nset of points uniformly sampled from its interior. Learning of simplices is a\nlong studied problem in computer science and has applications in computational\nbiology and remote sensing, mostly under the name of `spectral unmixing'. We\ntheoretically show that a sufficient sample complexity for reliable learning of\na $K$-dimensional simplex up to a total-variation error of $\\epsilon$ is\n$O\\left(\\frac{K^2}{\\epsilon}\\log\\frac{K}{\\epsilon}\\right)$, which yields a\nsubstantial improvement over existing bounds. Based on our new theoretical\nframework, we also propose a heuristic approach for the inference of simplices.\nExperimental results on synthetic and real-world datasets demonstrate a\ncomparable performance for our method on noiseless samples, while we outperform\nthe state-of-the-art in noisy cases. \n\n"}
{"id": "1810.07874", "contents": "Title: A Self-Organizing Tensor Architecture for Multi-View Clustering Abstract: In many real-world applications, data are often unlabeled and comprised of\ndifferent representations/views which often provide information complementary\nto each other. Although several multi-view clustering methods have been\nproposed, most of them routinely assume one weight for one view of features,\nand thus inter-view correlations are only considered at the view-level. These\napproaches, however, fail to explore the explicit correlations between features\nacross multiple views. In this paper, we introduce a tensor-based approach to\nincorporate the higher-order interactions among multiple views as a tensor\nstructure. Specifically, we propose a multi-linear multi-view clustering (MMC)\nmethod that can efficiently explore the full-order structural information among\nall views and reveal the underlying subspace structure embedded within the\ntensor. Extensive experiments on real-world datasets demonstrate that our\nproposed MMC algorithm clearly outperforms other related state-of-the-art\nmethods. \n\n"}
{"id": "1810.08171", "contents": "Title: Testing Matrix Rank, Optimally Abstract: We show that for the problem of testing if a matrix $A \\in F^{n \\times n}$\nhas rank at most $d$, or requires changing an $\\epsilon$-fraction of entries to\nhave rank at most $d$, there is a non-adaptive query algorithm making\n$\\widetilde{O}(d^2/\\epsilon)$ queries. Our algorithm works for any field $F$.\nThis improves upon the previous $O(d^2/\\epsilon^2)$ bound (SODA'03), and\nbypasses an $\\Omega(d^2/\\epsilon^2)$ lower bound of (KDD'14) which holds if the\nalgorithm is required to read a submatrix. Our algorithm is the first such\nalgorithm which does not read a submatrix, and instead reads a carefully\nselected non-adaptive pattern of entries in rows and columns of $A$. We\ncomplement our algorithm with a matching query complexity lower bound for\nnon-adaptive testers over any field. We also give tight bounds of\n$\\widetilde{\\Theta}(d^2)$ queries in the sensing model for which query access\ncomes in the form of $\\langle X_i, A\\rangle:=tr(X_i^\\top A)$; perhaps\nsurprisingly these bounds do not depend on $\\epsilon$.\n  We next develop a novel property testing framework for testing numerical\nproperties of a real-valued matrix $A$ more generally, which includes the\nstable rank, Schatten-$p$ norms, and SVD entropy. Specifically, we propose a\nbounded entry model, where $A$ is required to have entries bounded by $1$ in\nabsolute value. We give upper and lower bounds for a wide range of problems in\nthis model, and discuss connections to the sensing model above. \n\n"}
{"id": "1810.09113", "contents": "Title: The Bregman chord divergence Abstract: Distances are fundamental primitives whose choice significantly impacts the\nperformances of algorithms in machine learning and signal processing. However\nselecting the most appropriate distance for a given task is an endeavor.\nInstead of testing one by one the entries of an ever-expanding dictionary of\n{\\em ad hoc} distances, one rather prefers to consider parametric classes of\ndistances that are exhaustively characterized by axioms derived from first\nprinciples. Bregman divergences are such a class. However fine-tuning a Bregman\ndivergence is delicate since it requires to smoothly adjust a functional\ngenerator. In this work, we propose an extension of Bregman divergences called\nthe Bregman chord divergences. This new class of distances does not require\ngradient calculations, uses two scalar parameters that can be easily tailored\nin applications, and generalizes asymptotically Bregman divergences. \n\n"}
{"id": "1810.09166", "contents": "Title: Ensemble Method for Censored Demand Prediction Abstract: Many economic applications including optimal pricing and inventory management\nrequires prediction of demand based on sales data and estimation of sales\nreaction to a price change. There is a wide range of econometric approaches\nwhich are used to correct a bias in estimates of demand parameters on censored\nsales data. These approaches can also be applied to various classes of machine\nlearning models to reduce the prediction error of sales volume. In this study\nwe construct two ensemble models for demand prediction with and without\naccounting for demand censorship. Accounting for sales censorship is based on\nthe idea of censored quantile regression method where the model estimation is\nsplitted on two separate parts: a) prediction of zero sales by classification\nmodel; and b) prediction of non-zero sales by regression model. Models with and\nwithout accounting for censorship are based on the predictions aggregations of\nLeast squares, Ridge and Lasso regressions and Random Forest model. Having\nestimated the predictive properties of both models, we empirically test the\nbest predictive power of the model that takes into account the censored nature\nof demand. We also show that machine learning method with censorship accounting\nprovide bias corrected estimates of demand sensitivity for price change similar\nto econometric models. \n\n"}
{"id": "1810.09176", "contents": "Title: Node Representation Learning for Directed Graphs Abstract: We propose a novel approach for learning node representations in directed\ngraphs, which maintains separate views or embedding spaces for the two distinct\nnode roles induced by the directionality of the edges. We argue that the\nprevious approaches either fail to encode the edge directionality or their\nencodings cannot be generalized across tasks. With our simple \\emph{alternating\nrandom walk} strategy, we generate role specific vertex neighborhoods and train\nnode embeddings in their corresponding source/target roles while fully\nexploiting the semantics of directed graphs. We also unearth the limitations of\nevaluations on directed graphs in previous works and propose a clear strategy\nfor evaluating link prediction and graph reconstruction in directed graphs. We\nconduct extensive experiments to showcase our effectiveness on several\nreal-world datasets on link prediction, node classification and graph\nreconstruction tasks. We show that the embeddings from our approach are indeed\nrobust, generalizable and well performing across multiple kinds of tasks and\ngraphs. We show that we consistently outperform all baselines for node\nclassification task. In addition to providing a theoretical interpretation of\nour method we also show that we are considerably more robust than the other\ndirected graph approaches. \n\n"}
{"id": "1810.09270", "contents": "Title: A Model Parallel Proximal Stochastic Gradient Algorithm for Partially\n  Asynchronous Systems Abstract: Large models are prevalent in modern machine learning scenarios, including\ndeep learning, recommender systems, etc., which can have millions or even\nbillions of parameters. Parallel algorithms have become an essential solution\ntechnique to many large-scale machine learning jobs. In this paper, we propose\na model parallel proximal stochastic gradient algorithm, AsyB-ProxSGD, to deal\nwith large models using model parallel blockwise updates while in the meantime\nhandling a large amount of training data using proximal stochastic gradient\ndescent (ProxSGD). In our algorithm, worker nodes communicate with the\nparameter servers asynchronously, and each worker performs proximal stochastic\ngradient for only one block of model parameters during each iteration. Our\nproposed algorithm generalizes ProxSGD to the asynchronous and model parallel\nsetting. We prove that AsyB-ProxSGD achieves a convergence rate of\n$O(1/\\sqrt{K})$ to stationary points for nonconvex problems under\n\\emph{constant} minibatch sizes, where $K$ is the total number of block\nupdates. This rate matches the best-known rates of convergence for a wide range\nof gradient-like algorithms. Furthermore, we show that when the number of\nworkers is bounded by $O(K^{1/4})$, we can expect AsyB-ProxSGD to achieve\nlinear speedup as the number of workers increases. We implement the proposed\nalgorithm on MXNet and demonstrate its convergence behavior and near-linear\nspeedup on a real-world dataset involving both a large model size and large\namounts of data. \n\n"}
{"id": "1810.09502", "contents": "Title: How to train your MAML Abstract: The field of few-shot learning has recently seen substantial advancements.\nMost of these advancements came from casting few-shot learning as a\nmeta-learning problem. Model Agnostic Meta Learning or MAML is currently one of\nthe best approaches for few-shot learning via meta-learning. MAML is simple,\nelegant and very powerful, however, it has a variety of issues, such as being\nvery sensitive to neural network architectures, often leading to instability\nduring training, requiring arduous hyperparameter searches to stabilize\ntraining and achieve high generalization and being very computationally\nexpensive at both training and inference times. In this paper, we propose\nvarious modifications to MAML that not only stabilize the system, but also\nsubstantially improve the generalization performance, convergence speed and\ncomputational overhead of MAML, which we call MAML++. \n\n"}
{"id": "1810.09746", "contents": "Title: On PAC-Bayesian Bounds for Random Forests Abstract: Existing guarantees in terms of rigorous upper bounds on the generalization\nerror for the original random forest algorithm, one of the most frequently used\nmachine learning methods, are unsatisfying. We discuss and evaluate various\nPAC-Bayesian approaches to derive such bounds. The bounds do not require\nadditional hold-out data, because the out-of-bag samples from the bagging in\nthe training process can be exploited. A random forest predicts by taking a\nmajority vote of an ensemble of decision trees. The first approach is to bound\nthe error of the vote by twice the error of the corresponding Gibbs classifier\n(classifying with a single member of the ensemble selected at random). However,\nthis approach does not take into account the effect of averaging out of errors\nof individual classifiers when taking the majority vote. This effect provides a\nsignificant boost in performance when the errors are independent or negatively\ncorrelated, but when the correlations are strong the advantage from taking the\nmajority vote is small. The second approach based on PAC-Bayesian C-bounds\ntakes dependencies between ensemble members into account, but it requires\nestimating correlations between the errors of the individual classifiers. When\nthe correlations are high or the estimation is poor, the bounds degrade. In our\nexperiments, we compute generalization bounds for random forests on various\nbenchmark data sets. Because the individual decision trees already perform\nwell, their predictions are highly correlated and the C-bounds do not lead to\nsatisfactory results. For the same reason, the bounds based on the analysis of\nGibbs classifiers are typically superior and often reasonably tight. Bounds\nbased on a validation set coming at the cost of a smaller training set gave\nbetter performance guarantees, but worse performance in most experiments. \n\n"}
{"id": "1810.09828", "contents": "Title: DCSVM: Fast Multi-class Classification using Support Vector Machines Abstract: We present DCSVM, an efficient algorithm for multi-class classification using\nSupport Vector Machines. DCSVM is a divide and conquer algorithm which relies\non data sparsity in high dimensional space and performs a smart partitioning of\nthe whole training data set into disjoint subsets that are easily separable. A\nsingle prediction performed between two partitions eliminates at once one or\nmore classes in one partition, leaving only a reduced number of candidate\nclasses for subsequent steps. The algorithm continues recursively, reducing the\nnumber of classes at each step, until a final binary decision is made between\nthe last two classes left in the competition. In the best case scenario, our\nalgorithm makes a final decision between $k$ classes in $O(\\log k)$ decision\nsteps and in the worst case scenario DCSVM makes a final decision in $k-1$\nsteps, which is not worse than the existent techniques. \n\n"}
{"id": "1810.09957", "contents": "Title: NSML: Meet the MLaaS platform with a real-world case study Abstract: The boom of deep learning induced many industries and academies to introduce\nmachine learning based approaches into their concern, competitively. However,\nexisting machine learning frameworks are limited to sufficiently fulfill the\ncollaboration and management for both data and models. We proposed NSML, a\nmachine learning as a service (MLaaS) platform, to meet these demands. NSML\nhelps machine learning work be easily launched on a NSML cluster and provides a\ncollaborative environment which can afford development at enterprise scale.\nFinally, NSML users can deploy their own commercial services with NSML cluster.\nIn addition, NSML furnishes convenient visualization tools which assist the\nusers in analyzing their work. To verify the usefulness and accessibility of\nNSML, we performed some experiments with common examples. Furthermore, we\nexamined the collaborative advantages of NSML through three competitions with\nreal-world use cases. \n\n"}
{"id": "1810.10004", "contents": "Title: Time-Aware and Corpus-Specific Entity Relatedness Abstract: Entity relatedness has emerged as an important feature in a plethora of\napplications such as information retrieval, entity recommendation and entity\nlinking. Given an entity, for instance a person or an organization, entity\nrelatedness measures can be exploited for generating a list of highly-related\nentities. However, the relation of an entity to some other entity depends on\nseveral factors, with time and context being two of the most important ones\n(where, in our case, context is determined by a particular corpus). For\nexample, the entities related to the International Monetary Fund are different\nnow compared to some years ago, while these entities also may highly differ in\nthe context of a USA news portal compared to a Greek news portal. In this\npaper, we propose a simple but flexible model for entity relatedness which\nconsiders time and entity aware word embeddings by exploiting the underlying\ncorpus. The proposed model does not require external knowledge and is language\nindependent, which makes it widely useful in a variety of applications. \n\n"}
{"id": "1810.10053", "contents": "Title: Graph Laplacian mixture model Abstract: Graph learning methods have recently been receiving increasing interest as\nmeans to infer structure in datasets. Most of the recent approaches focus on\ndifferent relationships between a graph and data sample distributions, mostly\nin settings where all available data relate to the same graph. This is,\nhowever, not always the case, as data is often available in mixed form,\nyielding the need for methods that are able to cope with mixture data and learn\nmultiple graphs. We propose a novel generative model that represents a\ncollection of distinct data which naturally live on different graphs. We assume\nthe mapping of data to graphs is not known and investigate the problem of\njointly clustering a set of data and learning a graph for each of the clusters.\nExperiments demonstrate promising performance in data clustering and multiple\ngraph inference, and show desirable properties in terms of interpretability and\ncoping with high dimensionality on weather and traffic data, as well as digit\nclassification. \n\n"}
{"id": "1810.10358", "contents": "Title: Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent\n  Motion Imaging Abstract: Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in\nvivo perfusion quantification with magnetic resonance imaging (MRI). However,\nits use is limited by typically low accuracy due to low signal-to-noise ratio\n(SNR) at large gradient encoding magnitudes as well as dephasing artefacts\ncaused by subject motion, which is particularly challenging in fetal MRI. To\nmitigate this problem, we propose an implicit IVIM signal acquisition model\nwith which we learn full posterior distribution of perfusion parameters using\nartificial neural networks. This posterior then encapsulates the uncertainty of\nthe inferred parameter estimates, which we validate herein via numerical\nexperiments with rejection-based Bayesian sampling. Compared to\nstate-of-the-art IVIM estimation method of segmented least-squares fitting, our\nproposed approach improves parameter estimation accuracy by 65% on synthetic\nanisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method\nincreases repeatability of parameter estimation in placenta by 46%. \n\n"}
{"id": "1810.10627", "contents": "Title: Streaming Graph Neural Networks Abstract: Graphs are essential representations of many real-world data such as social\nnetworks. Recent years have witnessed the increasing efforts made to extend the\nneural network models to graph-structured data. These methods, which are\nusually known as the graph neural networks, have been applied to advance many\ngraphs related tasks such as reasoning dynamics of the physical system, graph\nclassification, and node classification. Most of the existing graph neural\nnetwork models have been designed for static graphs, while many real-world\ngraphs are inherently dynamic. For example, social networks are naturally\nevolving as new users joining and new relations being created. Current graph\nneural network models cannot utilize the dynamic information in dynamic graphs.\nHowever, the dynamic information has been proven to enhance the performance of\nmany graph analytic tasks such as community detection and link prediction.\nHence, it is necessary to design dedicated graph neural networks for dynamic\ngraphs. In this paper, we propose DGNN, a new {\\bf D}ynamic {\\bf G}raph {\\bf\nN}eural {\\bf N}etwork model, which can model the dynamic information as the\ngraph evolving. In particular, the proposed framework can keep updating node\ninformation by capturing the sequential information of edges (interactions),\nthe time intervals between edges and information propagation coherently.\nExperimental results on various dynamic graphs demonstrate the effectiveness of\nthe proposed framework. \n\n"}
{"id": "1810.11317", "contents": "Title: Superensemble Classifier for Improving Predictions in Imbalanced\n  Datasets Abstract: Learning from an imbalanced dataset is a tricky proposition. Because these\ndatasets are biased towards one class, most existing classifiers tend not to\nperform well on minority class examples. Conventional classifiers usually aim\nto optimize the overall accuracy without considering the relative distribution\nof each class. This article presents a superensemble classifier, to tackle and\nimprove predictions in imbalanced classification problems, that maps Hellinger\ndistance decision trees (HDDT) into radial basis function network (RBFN)\nframework. Regularity conditions for universal consistency and the idea of\nparameter optimization of the proposed model are provided. The proposed\ndistribution-free model can be applied for feature selection cum imbalanced\nclassification problems. We have also provided enough numerical evidence using\nvarious real-life data sets to assess the performance of the proposed model.\nIts effectiveness and competitiveness with respect to different\nstate-of-the-art models are shown. \n\n"}
{"id": "1810.11344", "contents": "Title: Benefits of over-parameterization with EM Abstract: Expectation Maximization (EM) is among the most popular algorithms for\nmaximum likelihood estimation, but it is generally only guaranteed to find its\nstationary points of the log-likelihood objective. The goal of this article is\nto present theoretical and empirical evidence that over-parameterization can\nhelp EM avoid spurious local optima in the log-likelihood. We consider the\nproblem of estimating the mean vectors of a Gaussian mixture model in a\nscenario where the mixing weights are known. Our study shows that the global\nbehavior of EM, when one uses an over-parameterized model in which the mixing\nweights are treated as unknown, is better than that when one uses the (correct)\nmodel with the mixing weights fixed to the known values. For symmetric\nGaussians mixtures with two components, we prove that introducing the\n(statistically redundant) weight parameters enables EM to find the global\nmaximizer of the log-likelihood starting from almost any initial mean\nparameters, whereas EM without this over-parameterization may very often fail.\nFor other Gaussian mixtures, we provide empirical evidence that shows similar\nbehavior. Our results corroborate the value of over-parameterization in solving\nnon-convex optimization problems, previously observed in other domains. \n\n"}
{"id": "1810.11497", "contents": "Title: Parsing Coordination for Spoken Language Understanding Abstract: Typical spoken language understanding systems provide narrow semantic parses\nusing a domain-specific ontology. The parses contain intents and slots that are\ndirectly consumed by downstream domain applications. In this work we discuss\nexpanding such systems to handle compound entities and intents by introducing a\ndomain-agnostic shallow parser that handles linguistic coordination. We show\nthat our model for parsing coordination learns domain-independent and\nslot-independent features and is able to segment conjunct boundaries of many\ndifferent phrasal categories. We also show that using adversarial training can\nbe effective for improving generalization across different slot types for\ncoordination parsing. \n\n"}
{"id": "1810.11546", "contents": "Title: Mobile Sensor Data Anonymization Abstract: Motion sensors such as accelerometers and gyroscopes measure the instant\nacceleration and rotation of a device, in three dimensions. Raw data streams\nfrom motion sensors embedded in portable and wearable devices may reveal\nprivate information about users without their awareness. For example, motion\ndata might disclose the weight or gender of a user, or enable their\nre-identification. To address this problem, we propose an on-device\ntransformation of sensor data to be shared for specific applications, such as\nmonitoring selected daily activities, without revealing information that\nenables user identification. We formulate the anonymization problem using an\ninformation-theoretic approach and propose a new multi-objective loss function\nfor training deep autoencoders. This loss function helps minimizing\nuser-identity information as well as data distortion to preserve the\napplication-specific utility. The training process regulates the encoder to\ndisregard user-identifiable patterns and tunes the decoder to shape the output\nindependently of users in the training set. The trained autoencoder can be\ndeployed on a mobile or wearable device to anonymize sensor data even for users\nwho are not included in the training dataset. Data from 24 users transformed by\nthe proposed anonymizing autoencoder lead to a promising trade-off between\nutility and privacy, with an accuracy for activity recognition above 92% and an\naccuracy for user identification below 7%. \n\n"}
{"id": "1810.11730", "contents": "Title: Low-shot Learning via Covariance-Preserving Adversarial Augmentation\n  Networks Abstract: Deep neural networks suffer from over-fitting and catastrophic forgetting\nwhen trained with small data. One natural remedy for this problem is data\naugmentation, which has been recently shown to be effective. However, previous\nworks either assume that intra-class variances can always be generalized to new\nclasses, or employ naive generation methods to hallucinate finite examples\nwithout modeling their latent distributions. In this work, we propose\nCovariance-Preserving Adversarial Augmentation Networks to overcome existing\nlimits of low-shot learning. Specifically, a novel Generative Adversarial\nNetwork is designed to model the latent distribution of each novel class given\nits related base counterparts. Since direct estimation of novel classes can be\ninductively biased, we explicitly preserve covariance information as the\n`variability' of base examples during the generation process. Empirical results\nshow that our model can generate realistic yet diverse examples, leading to\nsubstantial improvements on the ImageNet benchmark over the state of the art. \n\n"}
{"id": "1810.11754", "contents": "Title: On Learning Markov Chains Abstract: The problem of estimating an unknown discrete distribution from its samples\nis a fundamental tenet of statistical learning. Over the past decade, it\nattracted significant research effort and has been solved for a variety of\ndivergence measures. Surprisingly, an equally important problem, estimating an\nunknown Markov chain from its samples, is still far from understood. We\nconsider two problems related to the min-max risk (expected loss) of estimating\nan unknown $k$-state Markov chain from its $n$ sequential samples: predicting\nthe conditional distribution of the next sample with respect to the\nKL-divergence, and estimating the transition matrix with respect to a natural\nloss induced by KL or a more general $f$-divergence measure.\n  For the first measure, we determine the min-max prediction risk to within a\nlinear factor in the alphabet size, showing it is $\\Omega(k\\log\\log n\\ / n)$\nand $\\mathcal{O}(k^2\\log\\log n\\ / n)$. For the second, if the transition\nprobabilities can be arbitrarily small, then only trivial uniform risk upper\nbounds can be derived. We therefore consider transition probabilities that are\nbounded away from zero, and resolve the problem for essentially all\nsufficiently smooth $f$-divergences, including KL-, $L_2$-, Chi-squared,\nHellinger, and Alpha-divergences. \n\n"}
{"id": "1810.12091", "contents": "Title: Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data Abstract: Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available. \n\n"}
{"id": "1810.12278", "contents": "Title: Towards Principled Uncertainty Estimation for Deep Neural Networks Abstract: When the cost of misclassifying a sample is high, it is useful to have an\naccurate estimate of uncertainty in the prediction for that sample. There are\nalso multiple types of uncertainty which are best estimated in different ways,\nfor example, uncertainty that is intrinsic to the training set may be\nwell-handled by a Bayesian approach, while uncertainty introduced by shifts\nbetween training and query distributions may be better-addressed by\ndensity/support estimation. In this paper, we examine three types of\nuncertainty: model capacity uncertainty, intrinsic data uncertainty, and open\nset uncertainty, and review techniques that have been derived to address each\none. We then introduce a unified hierarchical model, which combines methods\nfrom Bayesian inference, invertible latent density inference, and\ndiscriminative classification in a single end-to-end deep neural network\ntopology to yield efficient per-sample uncertainty estimation in a detection\ncontext. This approach addresses all three uncertainty types and can readily\naccommodate prior/base rates for binary detection. We then discuss how to\nextend this model to a more generic multiclass recognition context. \n\n"}
{"id": "1810.12770", "contents": "Title: Explicit Feedbacks Meet with Implicit Feedbacks : A Combined Approach\n  for Recommendation System Abstract: Recommender systems recommend items more accurately by analyzing users'\npotential interest on different brands' items. In conjunction with users'\nrating similarity, the presence of users' implicit feedbacks like clicking\nitems, viewing items specifications, watching videos etc. have been proved to\nbe helpful for learning users' embedding, that helps better rating prediction\nof users. Most existing recommender systems focus on modeling of ratings and\nimplicit feedbacks ignoring users' explicit feedbacks. Explicit feedbacks can\nbe used to validate the reliability of the particular users and can be used to\nlearn about the users' characteristic. Users' characteristic mean what type of\nreviewers they are. In this paper, we explore three different models for\nrecommendation with more accuracy focusing on users' explicit feedbacks and\nimplicit feedbacks. First one is RHC-PMF that predicts users' rating more\naccurately based on user's three explicit feedbacks (rating, helpfulness score\nand centrality) and second one is RV-PMF, where user's implicit feedback (view\nrelationship) is considered. Last one is RHCV-PMF, where both type of feedbacks\nare considered. In this model users' explicit feedbacks' similarity indicate\nthe similarity of their reliability and characteristic and implicit feedback's\nsimilarity indicates their preference similarity. Extensive experiments on real\nworld dataset, i.e. Amazon.com online review dataset shows that our models\nperform better compare to base-line models in term of users' rating prediction.\nRHCV-PMF model also performs better rating prediction compare to baseline\nmodels for cold start users and cold start items. \n\n"}
{"id": "1810.12997", "contents": "Title: An Online-Learning Approach to Inverse Optimization Abstract: In this paper, we demonstrate how to learn the objective function of a\ndecision-maker while only observing the problem input data and the\ndecision-maker's corresponding decisions over multiple rounds. We present exact\nalgorithms for this online version of inverse optimization which converge at a\nrate of $ \\mathcal{O}(1/\\sqrt{T}) $ in the number of observations~$T$ and\ncompare their further properties. Especially, they all allow taking decisions\nwhich are essentially as good as those of the observed decision-maker already\nafter relatively few iterations, but are suited best for different settings\neach. Our approach is based on online learning and works for linear objectives\nover arbitrary feasible sets for which we have a linear optimization oracle. As\nsuch, it generalizes previous approaches based on KKT-system decomposition and\ndualization. We also introduce several generalizations, such as the approximate\nlearning of non-linear objective functions, dynamically changing as well as\nparameterized objectives and the case of suboptimal observed decisions. When\napplied to the stochastic offline case, our algorithms are able to give\nguarantees on the quality of the learned objectives in expectation. Finally, we\nshow the effectiveness and possible applications of our methods in indicative\ncomputational experiments. \n\n"}
{"id": "1810.13118", "contents": "Title: SplineNets: Continuous Neural Decision Graphs Abstract: We present SplineNets, a practical and novel approach for using conditioning\nin convolutional neural networks (CNNs). SplineNets are continuous\ngeneralizations of neural decision graphs, and they can dramatically reduce\nruntime complexity and computation costs of CNNs, while maintaining or even\nincreasing accuracy. Functions of SplineNets are both dynamic (i.e.,\nconditioned on the input) and hierarchical (i.e., conditioned on the\ncomputational path). SplineNets employ a unified loss function with a desired\nlevel of smoothness over both the network and decision parameters, while\nallowing for sparse activation of a subset of nodes for individual samples. In\nparticular, we embed infinitely many function weights (e.g. filters) on smooth,\nlow dimensional manifolds parameterized by compact B-splines, which are indexed\nby a position parameter. Instead of sampling from a categorical distribution to\npick a branch, samples choose a continuous position to pick a function weight.\nWe further show that by maximizing the mutual information between spline\npositions and class labels, the network can be optimally utilized and\nspecialized for classification tasks. Experiments show that our approach can\nsignificantly increase the accuracy of ResNets with negligible cost in speed,\nmatching the precision of a 110 level ResNet with a 32 level SplineNet. \n\n"}
{"id": "1810.13155", "contents": "Title: Structure Learning of Deep Neural Networks with Q-Learning Abstract: Recently, with convolutional neural networks gaining significant achievements\nin many challenging machine learning fields, hand-crafted neural networks no\nlonger satisfy our requirements as designing a network will cost a lot, and\nautomatically generating architectures has attracted increasingly more\nattention and focus. Some research on auto-generated networks has achieved\npromising results. However, they mainly aim at picking a series of single\nlayers such as convolution or pooling layers one by one. There are many elegant\nand creative designs in the carefully hand-crafted neural networks, such as\nInception-block in GoogLeNet, residual block in residual network and dense\nblock in dense convolutional network. Based on reinforcement learning and\ntaking advantages of the superiority of these networks, we propose a novel\nautomatic process to design a multi-block neural network, whose architecture\ncontains multiple types of blocks mentioned above, with the purpose to do\nstructure learning of deep neural networks and explore the possibility whether\ndifferent blocks can be composed together to form a well-behaved neural\nnetwork. The optimal network is created by the Q-learning agent who is trained\nto sequentially pick different types of blocks. To verify the validity of our\nproposed method, we use the auto-generated multi-block neural network to\nconduct experiments on image benchmark datasets MNIST, SVHN and CIFAR-10 image\nclassification task with restricted computational resources. The results\ndemonstrate that our method is very effective, achieving comparable or better\nperformance than hand-crafted networks and advanced auto-generated neural\nnetworks. \n\n"}
{"id": "1810.13259", "contents": "Title: Non-linear Canonical Correlation Analysis: A Compressed Representation\n  Approach Abstract: Canonical Correlation Analysis (CCA) is a linear representation learning\nmethod that seeks maximally correlated variables in multi-view data. Non-linear\nCCA extends this notion to a broader family of transformations, which are more\npowerful in many real-world applications. Given the joint probability, the\nAlternating Conditional Expectation (ACE) algorithm provides an optimal\nsolution to the non-linear CCA problem. However, it suffers from limited\nperformance and an increasing computational burden when only a finite number of\nsamples is available. In this work we introduce an information-theoretic\ncompressed representation framework for the non-linear CCA problem (CRCCA),\nwhich extends the classical ACE approach. Our suggested framework seeks compact\nrepresentations of the data that allow a maximal level of correlation. This way\nwe control the trade-off between the flexibility and the complexity of the\nmodel. CRCCA provides theoretical bounds and optimality conditions, as we\nestablish fundamental connections to rate-distortion theory, the information\nbottleneck and remote source coding. In addition, it allows a soft\ndimensionality reduction, as the compression level is determined by the mutual\ninformation between the original noisy data and the extracted signals. Finally,\nwe introduce a simple implementation of the CRCCA framework, based on lattice\nquantization. \n\n"}
{"id": "1810.13348", "contents": "Title: Multimodal Machine Learning for Automated ICD Coding Abstract: This study presents a multimodal machine learning model to predict ICD-10\ndiagnostic codes. We developed separate machine learning models that can handle\ndata from different modalities, including unstructured text, semi-structured\ntext and structured tabular data. We further employed an ensemble method to\nintegrate all modality-specific models to generate ICD-10 codes. Key evidence\nwas also extracted to make our prediction more convincing and explainable. We\nused the Medical Information Mart for Intensive Care III (MIMIC -III) dataset\nto validate our approach. For ICD code prediction, our best-performing model\n(micro-F1 = 0.7633, micro-AUC = 0.9541) significantly outperforms other\nbaseline models including TF-IDF (micro-F1 = 0.6721, micro-AUC = 0.7879) and\nText-CNN model (micro-F1 = 0.6569, micro-AUC = 0.9235). For interpretability,\nour approach achieves a Jaccard Similarity Coefficient (JSC) of 0.1806 on text\ndata and 0.3105 on tabular data, where well-trained physicians achieve 0.2780\nand 0.5002 respectively. \n\n"}
{"id": "1811.00075", "contents": "Title: The UEA multivariate time series classification archive, 2018 Abstract: In 2002, the UCR time series classification archive was first released with\nsixteen datasets. It gradually expanded, until 2015 when it increased in size\nfrom 45 datasets to 85 datasets. In October 2018 more datasets were added,\nbringing the total to 128. The new archive contains a wide range of problems,\nincluding variable length series, but it still only contains univariate time\nseries classification problems. One of the motivations for introducing the\narchive was to encourage researchers to perform a more rigorous evaluation of\nnewly proposed time series classification (TSC) algorithms. It has worked: most\nrecent research into TSC uses all 85 datasets to evaluate algorithmic advances.\nResearch into multivariate time series classification, where more than one\nseries are associated with each class label, is in a position where univariate\nTSC research was a decade ago. Algorithms are evaluated using very few datasets\nand claims of improvement are not based on statistical comparisons. We aim to\naddress this problem by forming the first iteration of the MTSC archive, to be\nhosted at the website www.timeseriesclassification.com. Like the univariate\narchive, this formulation was a collaborative effort between researchers at the\nUniversity of East Anglia (UEA) and the University of California, Riverside\n(UCR). The 2018 vintage consists of 30 datasets with a wide range of cases,\ndimensions and series lengths. For this first iteration of the archive we\nformat all data to be of equal length, include no series with missing data and\nprovide train/test splits. \n\n"}
{"id": "1811.00112", "contents": "Title: Generating Photo-Realistic Training Data to Improve Face Recognition\n  Accuracy Abstract: In this paper we investigate the feasibility of using synthetic data to\naugment face datasets. In particular, we propose a novel generative adversarial\nnetwork (GAN) that can disentangle identity-related attributes from\nnon-identity-related attributes. This is done by training an embedding network\nthat maps discrete identity labels to an identity latent space that follows a\nsimple prior distribution, and training a GAN conditioned on samples from that\ndistribution. Our proposed GAN allows us to augment face datasets by generating\nboth synthetic images of subjects in the training set and synthetic images of\nnew subjects not in the training set. By using recent advances in GAN training,\nwe show that the synthetic images generated by our model are photo-realistic,\nand that training with augmented datasets can indeed increase the accuracy of\nface recognition models as compared with models trained with real images alone. \n\n"}
{"id": "1811.00121", "contents": "Title: A Mixture Model Based Defense for Data Poisoning Attacks Against Naive\n  Bayes Spam Filters Abstract: Naive Bayes spam filters are highly susceptible to data poisoning attacks.\nHere, known spam sources/blacklisted IPs exploit the fact that their received\nemails will be treated as (ground truth) labeled spam examples, and used for\nclassifier training (or re-training). The attacking source thus generates\nemails that will skew the spam model, potentially resulting in great\ndegradation in classifier accuracy. Such attacks are successful mainly because\nof the poor representation power of the naive Bayes (NB) model, with only a\nsingle (component) density to represent spam (plus a possible attack). We\npropose a defense based on the use of a mixture of NB models. We demonstrate\nthat the learned mixture almost completely isolates the attack in a second NB\ncomponent, with the original spam component essentially unchanged by the\nattack. Our approach addresses both the scenario where the classifier is being\nre-trained in light of new data and, significantly, the more challenging\nscenario where the attack is embedded in the original spam training set. Even\nfor weak attack strengths, BIC-based model order selection chooses a\ntwo-component solution, which invokes the mixture-based defense. Promising\nresults are presented on the TREC 2005 spam corpus. \n\n"}
{"id": "1811.00152", "contents": "Title: Mixture Density Generative Adversarial Networks Abstract: Generative Adversarial Networks have surprising ability for generating sharp\nand realistic images, though they are known to suffer from the so-called mode\ncollapse problem. In this paper, we propose a new GAN variant called Mixture\nDensity GAN that while being capable of generating high-quality images,\novercomes this problem by encouraging the Discriminator to form clusters in its\nembedding space, which in turn leads the Generator to exploit these and\ndiscover different modes in the data. This is achieved by positioning Gaussian\ndensity functions in the corners of a simplex, using the resulting Gaussian\nmixture as a likelihood function over discriminator embeddings, and formulating\nan objective function for GAN training that is based on these likelihoods. We\ndemonstrate empirically (1) the quality of the generated images in Mixture\nDensity GAN and their strong similarity to real images, as measured by the\nFr\\'echet Inception Distance (FID), which compares very favourably with\nstate-of-the-art methods, and (2) the ability to avoid mode collapse and\ndiscover all data modes. \n\n"}
{"id": "1811.00246", "contents": "Title: SARN: Relational Reasoning through Sequential Attention Abstract: This paper proposes an attention module augmented relational network called\nSARN(Sequential Attention Relational Network) that can carry out relational\nreasoning by extracting reference objects and making efficient pairing between\nobjects. SARN greatly reduces the computational and memory requirements of the\nrelational network, which computes all object pairs. It also shows high\naccuracy on the Sort-of-CLEVR dataset compared to other models, especially on\nrelational questions. \n\n"}
{"id": "1811.00755", "contents": "Title: A General Framework for Multi-fidelity Bayesian Optimization with\n  Gaussian Processes Abstract: How can we efficiently gather information to optimize an unknown function,\nwhen presented with multiple, mutually dependent information sources with\ndifferent costs? For example, when optimizing a robotic system, intelligently\ntrading off computer simulations and real robot testings can lead to\nsignificant savings. Existing methods, such as multi-fidelity GP-UCB or Entropy\nSearch-based approaches, either make simplistic assumptions on the interaction\namong different fidelities or use simple heuristics that lack theoretical\nguarantees. In this paper, we study multi-fidelity Bayesian optimization with\ncomplex structural dependencies among multiple outputs, and propose\nMF-MI-Greedy, a principled algorithmic framework for addressing this problem.\nIn particular, we model different fidelities using additive Gaussian processes\nbased on shared latent structures with the target function. Then we use\ncost-sensitive mutual information gain for efficient Bayesian global\noptimization. We propose a simple notion of regret which incorporates the cost\nof different fidelities, and prove that MF-MI-Greedy achieves low regret. We\ndemonstrate the strong empirical performance of our algorithm on both synthetic\nand real-world datasets. \n\n"}
{"id": "1811.00821", "contents": "Title: OrthoNet: Multilayer Network Data Clustering Abstract: Network data appears in very diverse applications, like biological, social,\nor sensor networks. Clustering of network nodes into categories or communities\nhas thus become a very common task in machine learning and data mining. Network\ndata comes with some information about the network edges. In some cases, this\nnetwork information can even be given with multiple views or multiple layers,\neach one representing a different type of relationship between the network\nnodes. Increasingly often, network nodes also carry a feature vector. We\npropose in this paper to extend the node clustering problem, that commonly\nconsiders only the network information, to a problem where both the network\ninformation and the node features are considered together for learning a\nclustering-friendly representation of the feature space. Specifically, we\ndesign a generic two-step algorithm for multilayer network data clustering. The\nfirst step aggregates the different layers of network information into a graph\nrepresentation given by the geometric mean of the network Laplacian matrices.\nThe second step uses a neural net to learn a feature embedding that is\nconsistent with the structure given by the network layers. We propose a novel\nalgorithm for efficiently training the neural net via stochastic gradient\ndescent, which encourages the neural net outputs to span the leading\neigenvectors of the aggregated Laplacian matrix, in order to capture the\npairwise interactions on the network, and provide a clustering-friendly\nrepresentation of the feature space. We demonstrate with an extensive set of\nexperiments on synthetic and real datasets that our method leads to a\nsignificant improvement w.r.t. state-of-the-art multilayer graph clustering\nalgorithms, as it judiciously combines nodes features and network information\nin the node embedding algorithms. \n\n"}
{"id": "1811.00836", "contents": "Title: Multi-Kernel Regression with Sparsity Constraint Abstract: In this paper, we provide a Banach-space formulation of supervised learning\nwith generalized total-variation (gTV) regularization. We identify the class of\nkernel functions that are admissible in this framework. Then, we propose a\nvariation of supervised learning in a continuous-domain hybrid search space\nwith gTV regularization. We show that the solution admits a multi-kernel\nexpansion with adaptive positions. In this representation, the number of active\nkernels is upper-bounded by the number of data points while the gTV\nregularization imposes an $\\ell_1$ penalty on the kernel coefficients. Finally,\nwe illustrate numerically the outcome of our theory. \n\n"}
{"id": "1811.00944", "contents": "Title: Spectral Methods from Tensor Networks Abstract: A tensor network is a diagram that specifies a way to \"multiply\" a collection\nof tensors together to produce another tensor (or matrix). Many existing\nalgorithms for tensor problems (such as tensor decomposition and tensor PCA),\nalthough they are not presented this way, can be viewed as spectral methods on\nmatrices built from simple tensor networks. In this work we leverage the full\npower of this abstraction to design new algorithms for certain continuous\ntensor decomposition problems.\n  An important and challenging family of tensor problems comes from orbit\nrecovery, a class of inference problems involving group actions (inspired by\napplications such as cryo-electron microscopy). Orbit recovery problems over\nfinite groups can often be solved via standard tensor methods. However, for\ninfinite groups, no general algorithms are known. We give a new spectral\nalgorithm based on tensor networks for one such problem: continuous\nmulti-reference alignment over the infinite group SO(2). Our algorithm extends\nto the more general heterogeneous case. \n\n"}
{"id": "1811.01225", "contents": "Title: CAAD 2018: Powerful None-Access Black-Box Attack Based on Adversarial\n  Transformation Network Abstract: In this paper, we propose an improvement of Adversarial Transformation\nNetworks(ATN) to generate adversarial examples, which can fool white-box models\nand black-box models with a state of the art performance and won the 2rd place\nin the non-target task in CAAD 2018. \n\n"}
{"id": "1811.01316", "contents": "Title: Nonlinear Collaborative Scheme for Deep Neural Networks Abstract: Conventional research attributes the improvements of generalization ability\nof deep neural networks either to powerful optimizers or the new network\ndesign. Different from them, in this paper, we aim to link the generalization\nability of a deep network to optimizing a new objective function. To this end,\nwe propose a \\textit{nonlinear collaborative scheme} for deep network training,\nwith the key technique as combining different loss functions in a nonlinear\nmanner. We find that after adaptively tuning the weights of different loss\nfunctions, the proposed objective function can efficiently guide the\noptimization process. What is more, we demonstrate that, from the mathematical\nperspective, the nonlinear collaborative scheme can lead to (i) smaller KL\ndivergence with respect to optimal solutions; (ii) data-driven stochastic\ngradient descent; (iii) tighter PAC-Bayes bound. We also prove that its\nadvantage can be strengthened by nonlinearity increasing. To some extent, we\nbridge the gap between learning (i.e., minimizing the new objective function)\nand generalization (i.e., minimizing a PAC-Bayes bound) in the new scheme. We\nalso interpret our findings through the experiments on Residual Networks and\nDenseNet, showing that our new scheme performs superior to single-loss and\nmulti-loss schemes no matter with randomization or not. \n\n"}
{"id": "1811.01466", "contents": "Title: Practical Batch Bayesian Optimization for Less Expensive Functions Abstract: Bayesian optimization (BO) and its batch extensions are successful for\noptimizing expensive black-box functions. However, these traditional BO\napproaches are not yet ideal for optimizing less expensive functions when the\ncomputational cost of BO can dominate the cost of evaluating the blackbox\nfunction. Examples of these less expensive functions are cheap machine learning\nmodels, inexpensive physical experiment through simulators, and acquisition\nfunction optimization in Bayesian optimization. In this paper, we consider a\nbatch BO setting for situations where function evaluations are less expensive.\nOur model is based on a new exploration strategy using geometric distance that\nprovides an alternative way for exploration, selecting a point far from the\nobserved locations. Using that intuition, we propose to use Sobol sequence to\nguide exploration that will get rid of running multiple global optimization\nsteps as used in previous works. Based on the proposed distance exploration, we\npresent an efficient batch BO approach. We demonstrate that our approach\noutperforms other baselines and global optimization methods when the function\nevaluations are less expensive. \n\n"}
{"id": "1811.01704", "contents": "Title: ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural\n  Networks Abstract: Deep Neural Networks (DNNs) typically require massive amount of computation\nresource in inference tasks for computer vision applications. Quantization can\nsignificantly reduce DNN computation and storage by decreasing the bitwidth of\nnetwork encodings. Recent research affirms that carefully selecting the\nquantization levels for each layer can preserve the accuracy while pushing the\nbitwidth below eight bits. However, without arduous manual effort, this deep\nquantization can lead to significant accuracy loss, leaving it in a position of\nquestionable utility. As such, deep quantization opens a large hyper-parameter\nspace (bitwidth of the layers), the exploration of which is a major challenge.\nWe propose a systematic approach to tackle this problem, by automating the\nprocess of discovering the quantization levels through an end-to-end deep\nreinforcement learning framework (ReLeQ). We adapt policy optimization methods\nto the problem of quantization, and focus on finding the best design decisions\nin choosing the state and action spaces, network architecture and training\nframework, as well as the tuning of various hyperparamters. We show how ReLeQ\ncan balance speed and quality, and provide an asymmetric general solution for\nquantization of a large variety of deep networks (AlexNet, CIFAR-10, LeNet,\nMobileNet-V1, ResNet-20, SVHN, and VGG-11) that virtually preserves the\naccuracy (=< 0.3% loss) while minimizing the computation and storage cost. With\nthese DNNs, ReLeQ enables conventional hardware to achieve 2.2x speedup over\n8-bit execution. Similarly, a custom DNN accelerator achieves 2.0x speedup and\nenergy reduction compared to 8-bit runs. These encouraging results mark ReLeQ\nas the initial step towards automating the deep quantization of neural\nnetworks. \n\n"}
{"id": "1811.01747", "contents": "Title: The Knowref Coreference Corpus: Removing Gender and Number Cues for\n  Difficult Pronominal Anaphora Resolution Abstract: We introduce a new benchmark for coreference resolution and NLI, Knowref,\nthat targets common-sense understanding and world knowledge. Previous\ncoreference resolution tasks can largely be solved by exploiting the number and\ngender of the antecedents, or have been handcrafted and do not reflect the\ndiversity of naturally occurring text. We present a corpus of over 8,000\nannotated text passages with ambiguous pronominal anaphora. These instances are\nboth challenging and realistic. We show that various coreference systems,\nwhether rule-based, feature-rich, or neural, perform significantly worse on the\ntask than humans, who display high inter-annotator agreement. To explain this\nperformance gap, we show empirically that state-of-the art models often fail to\ncapture context, instead relying on the gender or number of candidate\nantecedents to make a decision. We then use problem-specific insights to\npropose a data-augmentation trick called antecedent switching to alleviate this\ntendency in models. Finally, we show that antecedent switching yields promising\nresults on other tasks as well: we use it to achieve state-of-the-art results\non the GAP coreference task. \n\n"}
{"id": "1811.01903", "contents": "Title: Lower Bounds for Parallel and Randomized Convex Optimization Abstract: We study the question of whether parallelization in the exploration of the\nfeasible set can be used to speed up convex optimization, in the local oracle\nmodel of computation. We show that the answer is negative for both\ndeterministic and randomized algorithms applied to essentially any of the\ninteresting geometries and nonsmooth, weakly-smooth, or smooth objective\nfunctions. In particular, we show that it is not possible to obtain a\npolylogarithmic (in the sequential complexity of the problem) number of\nparallel rounds with a polynomial (in the dimension) number of queries per\nround. In the majority of these settings and when the dimension of the space is\npolynomial in the inverse target accuracy, our lower bounds match the oracle\ncomplexity of sequential convex optimization, up to at most a logarithmic\nfactor in the dimension, which makes them (nearly) tight. Prior to our work,\nlower bounds for parallel convex optimization algorithms were only known in a\nsmall fraction of the settings considered in this paper, mainly applying to\nEuclidean ($\\ell_2$) and $\\ell_\\infty$ spaces. Our work provides a more general\napproach for proving lower bounds in the setting of parallel convex\noptimization. \n\n"}
{"id": "1811.01908", "contents": "Title: Fast Non-Bayesian Poisson Factorization for Implicit-Feedback\n  Recommendations Abstract: This work explores non-negative low-rank matrix factorization based on\nregularized Poisson models (PF or \"Poisson factorization\" for short) for\nrecommender systems with implicit-feedback data. The properties of Poisson\nlikelihood allow a shortcut for very fast computations over zero-valued inputs,\nand oftentimes results in very sparse factors for both users and items.\nCompared to HPF (a popular Bayesian formulation of the problem with\nhierarchical priors), the frequentist optimization-based approach presented\nhere tends to produce better top-N recommendations with significantly shorter\nfitting times, on top of having sparse solutions. \n\n"}
{"id": "1811.02096", "contents": "Title: Scale calibration for high-dimensional robust regression Abstract: We present a new method for high-dimensional linear regression when a scale\nparameter of the additive errors is unknown. The proposed estimator is based on\na penalized Huber $M$-estimator, for which theoretical results on estimation\nerror have recently been proposed in high-dimensional statistics literature.\nHowever, the variance of the error term in the linear model is intricately\nconnected to the optimal parameter used to define the shape of the Huber loss.\nOur main idea is to use an adaptive technique, based on Lepski's method, to\novercome the difficulties in solving a joint nonconvex optimization problem\nwith respect to the location and scale parameters. \n\n"}
{"id": "1811.02132", "contents": "Title: Student's t-Generative Adversarial Networks Abstract: Generative Adversarial Networks (GANs) have a great performance in image\ngeneration, but they need a large scale of data to train the entire framework,\nand often result in nonsensical results. We propose a new method referring to\nconditional GAN, which equipments the latent noise with mixture of Student's\nt-distribution with attention mechanism in addition to class information.\nStudent's t-distribution has long tails that can provide more diversity to the\nlatent noise. Meanwhile, the discriminator in our model implements two tasks\nsimultaneously, judging whether the images come from the true data\ndistribution, and identifying the class of each generated images. The\nparameters of the mixture model can be learned along with those of GANs.\nMoreover, we mathematically prove that any multivariate Student's\nt-distribution can be obtained by a linear transformation of a normal\nmultivariate Student's t-distribution. Experiments comparing the proposed\nmethod with typical GAN, DeliGAN and DCGAN indicate that, our method has a\ngreat performance on generating diverse and legible objects with limited data. \n\n"}
{"id": "1811.02172", "contents": "Title: Neural Phrase-to-Phrase Machine Translation Abstract: In this paper, we propose Neural Phrase-to-Phrase Machine Translation\n(NP$^2$MT). Our model uses a phrase attention mechanism to discover relevant\ninput (source) segments that are used by a decoder to generate output (target)\nphrases. We also design an efficient dynamic programming algorithm to decode\nsegments that allows the model to be trained faster than the existing neural\nphrase-based machine translation method by Huang et al. (2018). Furthermore,\nour method can naturally integrate with external phrase dictionaries during\ndecoding. Empirical experiments show that our method achieves comparable\nperformance with the state-of-the art methods on benchmark datasets. However,\nwhen the training and testing data are from different distributions or domains,\nour method performs better. \n\n"}
{"id": "1811.02228", "contents": "Title: Kernel Exponential Family Estimation via Doubly Dual Embedding Abstract: We investigate penalized maximum log-likelihood estimation for exponential\nfamily distributions whose natural parameter resides in a reproducing kernel\nHilbert space. Key to our approach is a novel technique, doubly dual embedding,\nthat avoids computation of the partition function. This technique also allows\nthe development of a flexible sampling strategy that amortizes the cost of\nMonte-Carlo sampling in the inference stage. The resulting estimator can be\neasily generalized to kernel conditional exponential families. We establish a\nconnection between kernel exponential family estimation and MMD-GANs, revealing\na new perspective for understanding GANs. Compared to the score matching based\nestimators, the proposed method improves both memory and time efficiency while\nenjoying stronger statistical properties, such as fully capturing smoothness in\nits statistical convergence rate while the score matching estimator appears to\nsaturate. Finally, we show that the proposed estimator empirically outperforms\nstate-of-the-art \n\n"}
{"id": "1811.02384", "contents": "Title: Robust Bhattacharyya bound linear discriminant analysis through adaptive\n  algorithm Abstract: In this paper, we propose a novel linear discriminant analysis criterion via\nthe Bhattacharyya error bound estimation based on a novel L1-norm (L1BLDA) and\nL2-norm (L2BLDA). Both L1BLDA and L2BLDA maximize the between-class scatters\nwhich are measured by the weighted pairwise distances of class means and\nmeanwhile minimize the within-class scatters under the L1-norm and L2-norm,\nrespectively. The proposed models can avoid the small sample size (SSS) problem\nand have no rank limit that may encounter in LDA. It is worth mentioning that,\nthe employment of L1-norm gives a robust performance of L1BLDA, and L1BLDA is\nsolved through an effective non-greedy alternating direction method of\nmultipliers (ADMM), where all the projection vectors can be obtained once for\nall. In addition, the weighting constants of L1BLDA and L2BLDA between the\nbetween-class and within-class terms are determined by the involved data set,\nwhich makes our L1BLDA and L2BLDA adaptive. The experimental results on both\nbenchmark data sets as well as the handwritten digit databases demonstrate the\neffectiveness of the proposed methods. \n\n"}
{"id": "1811.02598", "contents": "Title: Training Generative Adversarial Networks with Weights Abstract: The impressive success of Generative Adversarial Networks (GANs) is often\novershadowed by the difficulties in their training. Despite the continuous\nefforts and improvements, there are still open issues regarding their\nconvergence properties. In this paper, we propose a simple training variation\nwhere suitable weights are defined and assist the training of the Generator. We\nprovide theoretical arguments why the proposed algorithm is better than the\nbaseline training in the sense of speeding up the training process and of\ncreating a stronger Generator. Performance results showed that the new\nalgorithm is more accurate in both synthetic and image datasets resulting in\nimprovements ranging between 5% and 50%. \n\n"}
{"id": "1811.02722", "contents": "Title: Scalable Bottom-up Subspace Clustering using FP-Trees for High\n  Dimensional Data Abstract: Subspace clustering aims to find groups of similar objects (clusters) that\nexist in lower dimensional subspaces from a high dimensional dataset. It has a\nwide range of applications, such as analysing high dimensional sensor data or\nDNA sequences. However, existing algorithms have limitations in finding\nclusters in non-disjoint subspaces and scaling to large data, which impinge\ntheir applicability in areas such as bioinformatics and the Internet of Things.\nWe aim to address such limitations by proposing a subspace clustering algorithm\nusing a bottom-up strategy. Our algorithm first searches for base clusters in\nlow dimensional subspaces. It then forms clusters in higher-dimensional\nsubspaces using these base clusters, which we formulate as a frequent pattern\nmining problem. This formulation enables efficient search for clusters in\nhigher-dimensional subspaces, which is done using FP-trees. The proposed\nalgorithm is evaluated against traditional bottom-up clustering algorithms and\nstate-of-the-art subspace clustering algorithms. The experimental results show\nthat the proposed algorithm produces clusters with high accuracy, and scales\nwell to large volumes of data. We also demonstrate the algorithm's performance\nusing real-life data, including ten genomic datasets and a car parking\noccupancy dataset. \n\n"}
{"id": "1811.02934", "contents": "Title: Model Inconsistent but Correlated Noise: Multi-view Subspace Learning\n  with Regularized Mixture of Gaussians Abstract: Multi-view subspace learning (MSL) aims to find a low-dimensional subspace of\nthe data obtained from multiple views. Different from single view case, MSL\nshould take both common and specific knowledge among different views into\nconsideration. To enhance the robustness of model, the complexity,\nnon-consistency and similarity of noise in multi-view data should be fully\ntaken into consideration. Most current MSL methods only assume a simple\nGaussian or Laplacian distribution for the noise while neglect the complex\nnoise configurations in each view and noise correlations among different views\nof practical data. To this issue, this work initiates a MSL method by encoding\nthe multi-view-shared and single-view-specific noise knowledge in data.\nSpecifically, we model data noise in each view as a separated Mixture of\nGaussians (MoG), which can fit a wider range of complex noise types than\nconventional Gaussian/Laplacian. Furthermore, we link all single-view-noise as\na whole by regularizing them by a common MoG component, encoding the shared\nnoise knowledge among them. Such regularization component can be formulated as\na concise KL-divergence regularization term under a MAP framework, leading to\ngood interpretation of our model and simple EM-based solving strategy to the\nproblem. Experimental results substantiate the superiority of our method. \n\n"}
{"id": "1811.03195", "contents": "Title: Performance of Johnson-Lindenstrauss Transform for k-Means and k-Medians\n  Clustering Abstract: Consider an instance of Euclidean $k$-means or $k$-medians clustering. We\nshow that the cost of the optimal solution is preserved up to a factor of\n$(1+\\varepsilon)$ under a projection onto a random $O(\\log(k / \\varepsilon) /\n\\varepsilon^2)$-dimensional subspace. Further, the cost of every clustering is\npreserved within $(1+\\varepsilon)$. More generally, our result applies to any\ndimension reduction map satisfying a mild sub-Gaussian-tail condition. Our\nbound on the dimension is nearly optimal. Additionally, our result applies to\nEuclidean $k$-clustering with the distances raised to the $p$-th power for any\nconstant $p$.\n  For $k$-means, our result resolves an open problem posed by Cohen, Elder,\nMusco, Musco, and Persu (STOC 2015); for $k$-medians, it answers a question\nraised by Kannan. \n\n"}
{"id": "1811.03259", "contents": "Title: Bias and Generalization in Deep Generative Models: An Empirical Study Abstract: In high dimensional settings, density estimation algorithms rely crucially on\ntheir inductive bias. Despite recent empirical success, the inductive bias of\ndeep generative models is not well understood. In this paper we propose a\nframework to systematically investigate bias and generalization in deep\ngenerative models of images. Inspired by experimental methods from cognitive\npsychology, we probe each learning algorithm with carefully designed training\ndatasets to characterize when and how existing models generate novel attributes\nand their combinations. We identify similarities to human psychology and verify\nthat these patterns are consistent across commonly used models and\narchitectures. \n\n"}
{"id": "1811.03305", "contents": "Title: BAR: Bayesian Activity Recognition using variational inference Abstract: Uncertainty estimation in deep neural networks is essential for designing\nreliable and robust AI systems. Applications such as video surveillance for\nidentifying suspicious activities are designed with deep neural networks\n(DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable\nuncertainty estimates in safety and security critical applications will help to\nestablish trust in the AI system. Our contribution is to apply Bayesian deep\nlearning framework to visual activity recognition application and quantify\nmodel uncertainty along with principled confidence. We utilize the stochastic\nvariational inference technique while training the Bayesian DNNs to infer the\napproximate posterior distribution around model parameters and perform Monte\nCarlo sampling on the posterior of model parameters to obtain the predictive\ndistribution. We show that the Bayesian inference applied to DNNs provide\nreliable confidence measures for visual activity recognition task as compared\nto conventional DNNs. We also show that our method improves the visual activity\nrecognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We\nevaluate our models on Moments-In-Time (MiT) activity recognition dataset by\nselecting a subset of in- and out-of-distribution video samples. \n\n"}
{"id": "1811.03392", "contents": "Title: Transformative Machine Learning Abstract: The key to success in machine learning (ML) is the use of effective data\nrepresentations. Traditionally, data representations were hand-crafted.\nRecently it has been demonstrated that, given sufficient data, deep neural\nnetworks can learn effective implicit representations from simple input\nrepresentations. However, for most scientific problems, the use of deep\nlearning is not appropriate as the amount of available data is limited, and/or\nthe output models must be explainable. Nevertheless, many scientific problems\ndo have significant amounts of data available on related tasks, which makes\nthem amenable to multi-task learning, i.e. learning many related problems\nsimultaneously. Here we propose a novel and general representation learning\napproach for multi-task learning that works successfully with small amounts of\ndata. The fundamental new idea is to transform an input intrinsic data\nrepresentation (i.e., handcrafted features), to an extrinsic representation\nbased on what a pre-trained set of models predict about the examples. This\ntransformation has the dual advantages of producing significantly more accurate\npredictions, and providing explainable models. To demonstrate the utility of\nthis transformative learning approach, we have applied it to three real-world\nscientific problems: drug-design (quantitative structure activity relationship\nlearning), predicting human gene expression (across different tissue types and\ndrug treatments), and meta-learning for machine learning (predicting which\nmachine learning methods work best for a given problem). In all three problems,\ntransformative machine learning significantly outperforms the best intrinsic\nrepresentation. \n\n"}
{"id": "1811.03508", "contents": "Title: A simple yet effective baseline for non-attributed graph classification Abstract: Graphs are complex objects that do not lend themselves easily to typical\nlearning tasks. Recently, a range of approaches based on graph kernels or graph\nneural networks have been developed for graph classification and for\nrepresentation learning on graphs in general. As the developed methodologies\nbecome more sophisticated, it is important to understand which components of\nthe increasingly complex methods are necessary or most effective.\n  As a first step, we develop a simple yet meaningful graph representation, and\nexplore its effectiveness in graph classification. We test our baseline\nrepresentation for the graph classification task on a range of graph datasets.\nInterestingly, this simple representation achieves similar performance as the\nstate-of-the-art graph kernels and graph neural networks for non-attributed\ngraph classification. Its performance on classifying attributed graphs is\nslightly weaker as it does not incorporate attributes. However, given its\nsimplicity and efficiency, we believe that it still serves as an effective\nbaseline for attributed graph classification. Our graph representation is\nefficient (linear-time) to compute. We also provide a simple connection with\nthe graph neural networks.\n  Note that these observations are only for the task of graph classification\nwhile existing methods are often designed for a broader scope including node\nembedding and link prediction. The results are also likely biased due to the\nlimited amount of benchmark datasets available. Nevertheless, the good\nperformance of our simple baseline calls for the development of new, more\ncomprehensive benchmark datasets so as to better evaluate and analyze different\ngraph learning methods. Furthermore, given the computational efficiency of our\ngraph summary, we believe that it is a good candidate as a baseline method for\nfuture graph classification (or even other graph learning) studies. \n\n"}
{"id": "1811.03600", "contents": "Title: Measuring the Effects of Data Parallelism on Neural Network Training Abstract: Recent hardware developments have dramatically increased the scale of data\nparallelism available for neural network training. Among the simplest ways to\nharness next-generation hardware is to increase the batch size in standard\nmini-batch neural network training algorithms. In this work, we aim to\nexperimentally characterize the effects of increasing the batch size on\ntraining time, as measured by the number of steps necessary to reach a goal\nout-of-sample error. We study how this relationship varies with the training\nalgorithm, model, and data set, and find extremely large variation between\nworkloads. Along the way, we show that disagreements in the literature on how\nbatch size affects model quality can largely be explained by differences in\nmetaparameter tuning and compute budgets at different batch sizes. We find no\nevidence that larger batch sizes degrade out-of-sample performance. Finally, we\ndiscuss the implications of our results on efforts to train neural networks\nmuch faster in the future. Our experimental data is publicly available as a\ndatabase of 71,638,836 loss measurements taken over the course of training for\n168,160 individual models across 35 workloads. \n\n"}
{"id": "1811.04022", "contents": "Title: Convolutional neural networks in phase space and inverse problems Abstract: We study inverse problems consisting on determining medium properties using\nthe responses to probing waves from the machine learning point of view. Based\non the understanding of propagation of waves and their nonlinear interactions,\nwe construct a deep convolutional neural network in which the parameters are\nused to classify and reconstruct the coefficients of nonlinear wave equations\nthat model the medium properties. Furthermore, for given approximation\naccuracy, we obtain the depth and number of units of the network and their\nquantitative dependence on the complexity of the medium. \n\n"}
{"id": "1811.04351", "contents": "Title: Generalization Bounds for Vicinal Risk Minimization Principle Abstract: The vicinal risk minimization (VRM) principle, first proposed by\n\\citet{vapnik1999nature}, is an empirical risk minimization (ERM) variant that\nreplaces Dirac masses with vicinal functions. Although there is strong\nnumerical evidence showing that VRM outperforms ERM if appropriate vicinal\nfunctions are chosen, a comprehensive theoretical understanding of VRM is still\nlacking. In this paper, we study the generalization bounds for VRM. Our results\nsupport Vapnik's original arguments and additionally provide deeper insights\ninto VRM. First, we prove that the complexity of function classes convolving\nwith vicinal functions can be controlled by that of the original function\nclasses under the assumption that the function class is composed of\nLipschitz-continuous functions. Then, the resulting generalization bounds for\nVRM suggest that the generalization performance of VRM is also effected by the\nchoice of vicinity function and the quality of function classes. These findings\ncan be used to examine whether the choice of vicinal function is appropriate\nfor the VRM-based learning setting. Finally, we provide a theoretical\nexplanation for existing VRM models, e.g., uniform distribution-based models,\nGaussian distribution-based models, and mixup models. \n\n"}
{"id": "1811.04727", "contents": "Title: Universal Marginalizer for Amortised Inference and Embedding of\n  Generative Models Abstract: Probabilistic graphical models are powerful tools which allow us to formalise\nour knowledge about the world and reason about its inherent uncertainty. There\nexist a considerable number of methods for performing inference in\nprobabilistic graphical models; however, they can be computationally costly due\nto significant time burden and/or storage requirements; or they lack\ntheoretical guarantees of convergence and accuracy when applied to large scale\ngraphical models. To this end, we propose the Universal Marginaliser Importance\nSampler (UM-IS) -- a hybrid inference scheme that combines the flexibility of a\ndeep neural network trained on samples from the model and inherits the\nasymptotic guarantees of importance sampling. We show how combining samples\ndrawn from the graphical model with an appropriate masking function allows us\nto train a single neural network to approximate any of the corresponding\nconditional marginal distributions, and thus amortise the cost of inference. We\nalso show that the graph embeddings can be applied for tasks such as:\nclustering, classification and interpretation of relationships between the\nnodes. Finally, we benchmark the method on a large graph (>1000 nodes), showing\nthat UM-IS outperforms sampling-based methods by a large margin while being\ncomputationally efficient. \n\n"}
{"id": "1811.04820", "contents": "Title: Learning from positive and unlabeled data: a survey Abstract: Learning from positive and unlabeled data or PU learning is the setting where\na learner only has access to positive examples and unlabeled data. The\nassumption is that the unlabeled data can contain both positive and negative\nexamples. This setting has attracted increasing interest within the machine\nlearning literature as this type of data naturally arises in applications such\nas medical diagnosis and knowledge base completion. This article provides a\nsurvey of the current state of the art in PU learning. It proposes seven key\nresearch questions that commonly arise in this field and provides a broad\noverview of how the field has tried to address them. \n\n"}
{"id": "1811.05355", "contents": "Title: Iteratively Training Look-Up Tables for Network Quantization Abstract: Operating deep neural networks on devices with limited resources requires the\nreduction of their memory footprints and computational requirements. In this\npaper we introduce a training method, called look-up table quantization, LUT-Q,\nwhich learns a dictionary and assigns each weight to one of the dictionary's\nvalues. We show that this method is very flexible and that many other\ntechniques can be seen as special cases of LUT-Q. For example, we can constrain\nthe dictionary trained with LUT-Q to generate networks with pruned weight\nmatrices or restrict the dictionary to powers-of-two to avoid the need for\nmultiplications. In order to obtain fully multiplier-less networks, we also\nintroduce a multiplier-less version of batch normalization. Extensive\nexperiments on image recognition and object detection tasks show that LUT-Q\nconsistently achieves better performance than other methods with the same\nquantization bitwidth. \n\n"}
{"id": "1811.05443", "contents": "Title: Co-regularized Alignment for Unsupervised Domain Adaptation Abstract: Deep neural networks, trained with large amount of labeled data, can fail to\ngeneralize well when tested with examples from a \\emph{target domain} whose\ndistribution differs from the training data distribution, referred as the\n\\emph{source domain}. It can be expensive or even infeasible to obtain required\namount of labeled data in all possible domains. Unsupervised domain adaptation\nsets out to address this problem, aiming to learn a good predictive model for\nthe target domain using labeled examples from the source domain but only\nunlabeled examples from the target domain. Domain alignment approaches this\nproblem by matching the source and target feature distributions, and has been\nused as a key component in many state-of-the-art domain adaptation methods.\nHowever, matching the marginal feature distributions does not guarantee that\nthe corresponding class conditional distributions will be aligned across the\ntwo domains. We propose co-regularized domain alignment for unsupervised domain\nadaptation, which constructs multiple diverse feature spaces and aligns source\nand target distributions in each of them individually, while encouraging that\nalignments agree with each other with regard to the class predictions on the\nunlabeled target examples. The proposed method is generic and can be used to\nimprove any domain adaptation method which uses domain alignment. We\ninstantiate it in the context of a recent state-of-the-art method and observe\nthat it provides significant performance improvements on several domain\nadaptation benchmarks. \n\n"}
{"id": "1811.05467", "contents": "Title: Towards Neural Machine Translation for African Languages Abstract: Given that South African education is in crisis, strategies for improvement\nand sustainability of high-quality, up-to-date education must be explored. In\nthe migration of education online, inclusion of machine translation for\nlow-resourced local languages becomes necessary. This paper aims to spur the\nuse of current neural machine translation (NMT) techniques for low-resourced\nlocal languages. The paper demonstrates state-of-the-art performance on\nEnglish-to-Setswana translation using the Autshumato dataset. The use of the\nTransformer architecture beat previous techniques by 5.33 BLEU points. This\ndemonstrates the promise of using current NMT techniques for African languages. \n\n"}
{"id": "1811.05512", "contents": "Title: A domain agnostic measure for monitoring and evaluating GANs Abstract: Generative Adversarial Networks (GANs) have shown remarkable results in\nmodeling complex distributions, but their evaluation remains an unsettled\nissue. Evaluations are essential for: (i) relative assessment of different\nmodels and (ii) monitoring the progress of a single model throughout training.\nThe latter cannot be determined by simply inspecting the generator and\ndiscriminator loss curves as they behave non-intuitively. We leverage the\nnotion of duality gap from game theory to propose a measure that addresses both\n(i) and (ii) at a low computational cost. Extensive experiments show the\neffectiveness of this measure to rank different GAN models and capture the\ntypical GAN failure scenarios, including mode collapse and non-convergent\nbehaviours. This evaluation metric also provides meaningful monitoring on the\nprogression of the loss during training. It highly correlates with FID on\nnatural image datasets, and with domain specific scores for text, sound and\ncosmology data where FID is not directly suitable. In particular, our proposed\nmetric requires no labels or a pretrained classifier, making it domain\nagnostic. \n\n"}
{"id": "1811.05542", "contents": "Title: Extractive Summary as Discrete Latent Variables Abstract: In this paper, we compare various methods to compress a text using a neural\nmodel. We find that extracting tokens as latent variables significantly\noutperforms the state-of-the-art discrete latent variable models such as\nVQ-VAE. Furthermore, we compare various extractive compression schemes. There\nare two best-performing methods that perform equally. One method is to simply\nchoose the tokens with the highest tf-idf scores. Another is to train a\nbidirectional language model similar to ELMo and choose the tokens with the\nhighest loss. If we consider any subsequence of a text to be a text in a\nbroader sense, we conclude that language is a strong compression code of\nitself. Our finding justifies the high quality of generation achieved with\nhierarchical method, as their latent variables are nothing but natural language\nsummary. We also conclude that there is a hierarchy in language such that an\nentire text can be predicted much more easily based on a sequence of a small\nnumber of keywords, which can be easily found by classical methods as tf-idf.\nWe speculate that this extraction process may be useful for unsupervised\nhierarchical text generation. \n\n"}
{"id": "1811.05544", "contents": "Title: An Introductory Survey on Attention Mechanisms in NLP Problems Abstract: First derived from human intuition, later adapted to machine translation for\nautomatic token alignment, attention mechanism, a simple method that can be\nused for encoding sequence data based on the importance score each element is\nassigned, has been widely applied to and attained significant improvement in\nvarious tasks in natural language processing, including sentiment\nclassification, text summarization, question answering, dependency parsing,\netc. In this paper, we survey through recent works and conduct an introductory\nsummary of the attention mechanism in different NLP problems, aiming to provide\nour readers with basic knowledge on this widely used method, discuss its\ndifferent variants for different tasks, explore its association with other\ntechniques in machine learning, and examine methods for evaluating its\nperformance. \n\n"}
{"id": "1811.05614", "contents": "Title: SepNE: Bringing Separability to Network Embedding Abstract: Many successful methods have been proposed for learning low dimensional\nrepresentations on large-scale networks, while almost all existing methods are\ndesigned in inseparable processes, learning embeddings for entire networks even\nwhen only a small proportion of nodes are of interest. This leads to great\ninconvenience, especially on super-large or dynamic networks, where these\nmethods become almost impossible to implement. In this paper, we formalize the\nproblem of separated matrix factorization, based on which we elaborate a novel\nobjective function that preserves both local and global information. We further\npropose SepNE, a simple and flexible network embedding algorithm which\nindependently learns representations for different subsets of nodes in\nseparated processes. By implementing separability, our algorithm reduces the\nredundant efforts to embed irrelevant nodes, yielding scalability to\nsuper-large networks, automatic implementation in distributed learning and\nfurther adaptations. We demonstrate the effectiveness of this approach on\nseveral real-world networks with different scales and subjects. With comparable\naccuracy, our approach significantly outperforms state-of-the-art baselines in\nrunning times on large networks. \n\n"}
{"id": "1811.05646", "contents": "Title: Fast Distribution Grid Line Outage Identification with $\\mu$PMU Abstract: The growing integration of distributed energy resources (DERs) in urban\ndistribution grids raises various reliability issues due to DER's uncertain and\ncomplex behaviors. With a large-scale DER penetration, traditional outage\ndetection methods, which rely on customers making phone calls and smart meters'\n\"last gasp\" signals, will have limited performance, because the renewable\ngenerators can supply powers after line outages and many urban grids are mesh\nso line outages do not affect power supply. To address these drawbacks, we\npropose a data-driven outage monitoring approach based on the stochastic time\nseries analysis from micro phasor measurement unit ($\\mu$PMU). Specifically, we\nprove via power flow analysis that the dependency of time-series voltage\nmeasurements exhibits significant statistical changes after line outages. This\nmakes the theory on optimal change-point detection suitable to identify line\noutages via $\\mu$PMUs with fast and accurate sampling. However, existing change\npoint detection methods require post-outage voltage distribution unknown in\ndistribution systems. Therefore, we design a maximum likelihood-based method to\ndirectly learn the distribution parameters from $\\mu$PMU data. We prove that\nthe estimated parameters-based detection still achieves the optimal\nperformance, making it extremely useful for distribution grid outage\nidentifications. Simulation results show highly accurate outage identification\nin eight distribution grids with 14 configurations with and without DERs using\n$\\mu$PMU data. \n\n"}
{"id": "1811.05852", "contents": "Title: Predicting the time-evolution of multi-physics systems with\n  sequence-to-sequence models Abstract: In this work, sequence-to-sequence (seq2seq) models, originally developed for\nlanguage translation, are used to predict the temporal evolution of complex,\nmulti-physics computer simulations. The predictive performance of seq2seq\nmodels is compared to state transition models for datasets generated with\nmulti-physics codes with varying levels of complexity - from simple 1D\ndiffusion calculations to simulations of inertial confinement fusion\nimplosions. Seq2seq models demonstrate the ability to accurately emulate\ncomplex systems, enabling the rapid estimation of the evolution of quantities\nof interest in computationally expensive simulations. \n\n"}
{"id": "1811.05868", "contents": "Title: Pitfalls of Graph Neural Network Evaluation Abstract: Semi-supervised node classification in graphs is a fundamental problem in\ngraph mining, and the recently proposed graph neural networks (GNNs) have\nachieved unparalleled results on this task. Due to their massive success, GNNs\nhave attracted a lot of attention, and many novel architectures have been put\nforward. In this paper we show that existing evaluation strategies for GNN\nmodels have serious shortcomings. We show that using the same\ntrain/validation/test splits of the same datasets, as well as making\nsignificant changes to the training procedure (e.g. early stopping criteria)\nprecludes a fair comparison of different architectures. We perform a thorough\nempirical evaluation of four prominent GNN models and show that considering\ndifferent splits of the data leads to dramatically different rankings of\nmodels. Even more importantly, our findings suggest that simpler GNN\narchitectures are able to outperform the more sophisticated ones if the\nhyperparameters and the training procedure are tuned fairly for all models. \n\n"}
{"id": "1811.05927", "contents": "Title: Improvements on SCORE, Especially for Weak Signals Abstract: A network may have weak signals and severe degree heterogeneity, and may be\nvery sparse in one occurrence but very dense in another. SCORE (Jin, 2015) is a\nrecent approach to network community detection. It accommodates severe degree\nheterogeneity and is adaptive to different levels of sparsity, but its\nperformance for networks with weak signals is unclear. In this paper, we show\nthat in a broad class of network settings where we allow for weak signals,\nsevere degree heterogeneity, and a wide range of network sparsity, SCORE\nachieves prefect clustering and has the so-called \"exponential rate\" in Hamming\nclustering errors. The proof uses the most recent advancement on entry-wise\nbounds for the leading eigenvectors of the network adjacency matrix.\n  The theoretical analysis assures us that SCORE continues to work well in the\nweak signal settings, but it does not rule out the possibility that SCORE may\nbe further improved to have better performance in real applications, especially\nfor networks with weak signals. As a second contribution of the paper, we\npropose SCORE+ as an improved version of SCORE. We investigate SCORE+ with 8\nnetwork data sets and found that it outperforms several representative\napproaches. In particular, for the 6 data sets with relatively strong signals,\nSCORE+ has similar performance as that of SCORE, but for the 2 data sets\n(Simmons, Caltech) with possibly weak signals, SCORE+ has much lower error\nrates. SCORE+ proposes several changes to SCORE. We carefully explain the\nrationale underlying each of these changes, using a mixture of theoretical and\nnumerical study. \n\n"}
{"id": "1811.05932", "contents": "Title: Streaming Network Embedding through Local Actions Abstract: Recently, considerable research attention has been paid to network embedding,\na popular approach to construct feature vectors of vertices. Due to the curse\nof dimensionality and sparsity in graphical datasets, this approach has become\nindispensable for machine learning tasks over large networks. The majority of\nexisting literature has considered this technique under the assumption that the\nnetwork is static. However, networks in many applications, nodes and edges\naccrue to a growing network as a streaming. A small number of very recent\nresults have addressed the problem of embedding for dynamic networks. However,\nthey either rely on knowledge of vertex attributes, suffer high-time complexity\nor need to be re-trained without closed-form expression. Thus the approach of\nadapting the existing methods to the streaming environment faces non-trivial\ntechnical challenges.\n  These challenges motivate developing new approaches to the problems of\nstreaming network embedding. In this paper, We propose a new framework that is\nable to generate latent features for new vertices with high efficiency and low\ncomplexity under specified iteration rounds. We formulate a constrained\noptimization problem for the modification of the representation resulting from\na stream arrival. We show this problem has no closed-form solution and instead\ndevelop an online approximation solution. Our solution follows three steps: (1)\nidentify vertices affected by new vertices, (2) generate latent features for\nnew vertices, and (3) update the latent features of the most affected vertices.\nThe generated representations are provably feasible and not far from the\noptimal ones in terms of expectation. Multi-class classification and clustering\non five real-world networks demonstrate that our model can efficiently update\nvertex representations and simultaneously achieve comparable or even better\nperformance. \n\n"}
{"id": "1811.06272", "contents": "Title: Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search Abstract: Learning policies on data synthesized by models can in principle quench the\nthirst of reinforcement learning algorithms for large amounts of real\nexperience, which is often costly to acquire. However, simulating plausible\nexperience de novo is a hard problem for many complex environments, often\nresulting in biases for model-based policy evaluation and search. Instead of de\nnovo synthesis of data, here we assume logged, real experience and model\nalternative outcomes of this experience under counterfactual actions, actions\nthat were not actually taken. Based on this, we propose the\nCounterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies\nin POMDPs from off-policy experience. It leverages structural causal models for\ncounterfactual evaluation of arbitrary policies on individual off-policy\nepisodes. CF-GPS can improve on vanilla model-based RL algorithms by making use\nof available logged data to de-bias model predictions. In contrast to\noff-policy algorithms based on Importance Sampling which re-weight data, CF-GPS\nleverages a model to explicitly consider alternative outcomes, allowing the\nalgorithm to make better use of experience data. We find empirically that these\nadvantages translate into improved policy evaluation and search results on a\nnon-trivial grid-world task. Finally, we show that CF-GPS generalizes the\npreviously proposed Guided Policy Search and that reparameterization-based\nalgorithms such Stochastic Value Gradient can be interpreted as counterfactual\nmethods. \n\n"}
{"id": "1811.06341", "contents": "Title: Spatio-temporal Stacked LSTM for Temperature Prediction in Weather\n  Forecasting Abstract: Long Short-Term Memory (LSTM) is a well-known method used widely on sequence\nlearning and time series prediction. In this paper we deployed stacked LSTM\nmodel in an application of weather forecasting. We propose a 2-layer\nspatio-temporal stacked LSTM model which consists of independent LSTM models\nper location in the first LSTM layer. Subsequently, the input of the second\nLSTM layer is formed based on the combination of the hidden states of the first\nlayer LSTM models. The experiments show that by utilizing the spatial\ninformation the prediction performance of the stacked LSTM model improves in\nmost of the cases. \n\n"}
{"id": "1811.06407", "contents": "Title: Neural Predictive Belief Representations Abstract: Unsupervised representation learning has succeeded with excellent results in\nmany applications. It is an especially powerful tool to learn a good\nrepresentation of environments with partial or noisy observations. In partially\nobservable domains it is important for the representation to encode a belief\nstate, a sufficient statistic of the observations seen so far. In this paper,\nwe investigate whether it is possible to learn such a belief representation\nusing modern neural architectures. Specifically, we focus on one-step frame\nprediction and two variants of contrastive predictive coding (CPC) as the\nobjective functions to learn the representations. To evaluate these learned\nrepresentations, we test how well they can predict various pieces of\ninformation about the underlying state of the environment, e.g., position of\nthe agent in a 3D maze. We show that all three methods are able to learn belief\nrepresentations of the environment, they encode not only the state information,\nbut also its uncertainty, a crucial aspect of belief states. We also find that\nfor CPC multi-step predictions and action-conditioning are critical for\naccurate belief representations in visually complex environments. The ability\nof neural representations to capture the belief information has the potential\nto spur new advances for learning and planning in partially observable domains,\nwhere leveraging uncertainty is essential for optimal decision making. \n\n"}
{"id": "1811.06588", "contents": "Title: Infinite-Horizon Gaussian Processes Abstract: Gaussian processes provide a flexible framework for forecasting, removing\nnoise, and interpreting long temporal datasets. State space modelling (Kalman\nfiltering) enables these non-parametric models to be deployed on long datasets\nby reducing the complexity to linear in the number of data points. The\ncomplexity is still cubic in the state dimension $m$ which is an impediment to\npractical application. In certain special cases (Gaussian likelihood, regular\nspacing) the GP posterior will reach a steady posterior state when the data are\nvery long. We leverage this and formulate an inference scheme for GPs with\ngeneral likelihoods, where inference is based on single-sweep EP (assumed\ndensity filtering). The infinite-horizon model tackles the cubic cost in the\nstate dimensionality and reduces the cost in the state dimension $m$ to\n$\\mathcal{O}(m^2)$ per data point. The model is extended to online-learning of\nhyperparameters. We show examples for large finite-length modelling problems,\nand present how the method runs in real-time on a smartphone on a continuous\ndata stream updated at 100~Hz. \n\n"}
{"id": "1811.06746", "contents": "Title: nn-dependability-kit: Engineering Neural Networks for Safety-Critical\n  Autonomous Driving Systems Abstract: Can engineering neural networks be approached in a disciplined way similar to\nhow engineers build software for civil aircraft? We present\nnn-dependability-kit, an open-source toolbox to support safety engineering of\nneural networks for autonomous driving systems. The rationale behind\nnn-dependability-kit is to consider a structured approach (via Goal Structuring\nNotation) to argue the quality of neural networks. In particular, the tool\nrealizes recent scientific results including (a) novel dependability metrics\nfor indicating sufficient elimination of uncertainties in the product life\ncycle, (b) formal reasoning engine for ensuring that the generalization does\nnot lead to undesired behaviors, and (c) runtime monitoring for reasoning\nwhether a decision of a neural network in operation is supported by prior\nsimilarities in the training data. A proprietary version of\nnn-dependability-kit has been used to improve the quality of a level-3\nautonomous driving component developed by Audi for highway maneuvers. \n\n"}
{"id": "1811.06847", "contents": "Title: Adversarial Unsupervised Representation Learning for Activity\n  Time-Series Abstract: Sufficient physical activity and restful sleep play a major role in the\nprevention and cure of many chronic conditions. Being able to proactively\nscreen and monitor such chronic conditions would be a big step forward for\noverall health. The rapid increase in the popularity of wearable devices\nprovides a significant new source, making it possible to track the user's\nlifestyle real-time. In this paper, we propose a novel unsupervised\nrepresentation learning technique called activity2vec that learns and\n\"summarizes\" the discrete-valued activity time-series. It learns the\nrepresentations with three components: (i) the co-occurrence and magnitude of\nthe activity levels in a time-segment, (ii) neighboring context of the\ntime-segment, and (iii) promoting subject-invariance with adversarial training.\nWe evaluate our method on four disorder prediction tasks using linear\nclassifiers. Empirical evaluation demonstrates that our proposed method scales\nand performs better than many strong baselines. The adversarial regime helps\nimprove the generalizability of our representations by promoting subject\ninvariant features. We also show that using the representations at the level of\na day works the best since human activity is structured in terms of daily\nroutines \n\n"}
{"id": "1811.07006", "contents": "Title: Projected BNNs: Avoiding weight-space pathologies by learning latent\n  representations of neural network weights Abstract: As machine learning systems get widely adopted for high-stake decisions,\nquantifying uncertainty over predictions becomes crucial. While modern neural\nnetworks are making remarkable gains in terms of predictive accuracy,\ncharacterizing uncertainty over the parameters of these models is challenging\nbecause of the high dimensionality and complex correlations of the network\nparameter space. This paper introduces a novel variational inference framework\nfor Bayesian neural networks that (1) encodes complex distributions in\nhigh-dimensional parameter space with representations in a low-dimensional\nlatent space, and (2) performs inference efficiently on the low-dimensional\nrepresentations. Across a large array of synthetic and real-world datasets, we\nshow that our method improves uncertainty characterization and model\ngeneralization when compared with methods that work directly in the parameter\nspace. \n\n"}
{"id": "1811.07062", "contents": "Title: The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD\n  Training and Sample Size Abstract: We apply state-of-the-art tools in modern high-dimensional numerical linear\nalgebra to approximate efficiently the spectrum of the Hessian of modern\ndeepnets, with tens of millions of parameters, trained on real data. Our\nresults corroborate previous findings, based on small-scale networks, that the\nHessian exhibits \"spiked\" behavior, with several outliers isolated from a\ncontinuous bulk. We decompose the Hessian into different components and study\nthe dynamics with training and sample size of each term individually. \n\n"}
{"id": "1811.07073", "contents": "Title: Semi-Supervised Semantic Image Segmentation with Self-correcting\n  Networks Abstract: Building a large image dataset with high-quality object masks for semantic\nsegmentation is costly and time consuming. In this paper, we introduce a\nprincipled semi-supervised framework that only uses a small set of fully\nsupervised images (having semantic segmentation labels and box labels) and a\nset of images with only object bounding box labels (we call it the weak set).\nOur framework trains the primary segmentation model with the aid of an\nancillary model that generates initial segmentation labels for the weak set and\na self-correction module that improves the generated labels during training\nusing the increasingly accurate primary model. We introduce two variants of the\nself-correction module using either linear or convolutional functions.\nExperiments on the PASCAL VOC 2012 and Cityscape datasets show that our models\ntrained with a small fully supervised set perform similar to, or better than,\nmodels trained with a large fully supervised set while requiring ~7x less\nannotation effort. \n\n"}
{"id": "1811.07174", "contents": "Title: Link Prediction in Dynamic Graphs for Recommendation Abstract: Recent advances in employing neural networks on graph domains helped push the\nstate of the art in link prediction tasks, particularly in recommendation\nservices. However, the use of temporal contextual information, often modeled as\ndynamic graphs that encode the evolution of user-item relationships over time,\nhas been overlooked in link prediction problems. In this paper, we consider the\nhypothesis that leveraging such information enables models to make better\npredictions, proposing a new neural network approach for this. Our experiments,\nperformed on the widely used ML-100k and ML-1M datasets, show that our approach\nproduces better predictions in scenarios where the pattern of user-item\nrelationships change over time. In addition, they suggest that existing\napproaches are significantly impacted by those changes. \n\n"}
{"id": "1811.07279", "contents": "Title: Understanding Learned Models by Identifying Important Features at the\n  Right Resolution Abstract: In many application domains, it is important to characterize how complex\nlearned models make their decisions across the distribution of instances. One\nway to do this is to identify the features and interactions among them that\ncontribute to a model's predictive accuracy. We present a model-agnostic\napproach to this task that makes the following specific contributions. Our\napproach (i) tests feature groups, in addition to base features, and tries to\ndetermine the level of resolution at which important features can be\ndetermined, (ii) uses hypothesis testing to rigorously assess the effect of\neach feature on the model's loss, (iii) employs a hierarchical approach to\ncontrol the false discovery rate when testing feature groups and individual\nbase features for importance, and (iv) uses hypothesis testing to identify\nimportant interactions among features and feature groups. We evaluate our\napproach by analyzing random forest and LSTM neural network models learned in\ntwo challenging biomedical applications. \n\n"}
{"id": "1811.07457", "contents": "Title: Generalizable Adversarial Training via Spectral Normalization Abstract: Deep neural networks (DNNs) have set benchmarks on a wide array of supervised\nlearning tasks. Trained DNNs, however, often lack robustness to minor\nadversarial perturbations to the input, which undermines their true\npracticality. Recent works have increased the robustness of DNNs by fitting\nnetworks using adversarially-perturbed training samples, but the improved\nperformance can still be far below the performance seen in non-adversarial\nsettings. A significant portion of this gap can be attributed to the decrease\nin generalization performance due to adversarial training. In this work, we\nextend the notion of margin loss to adversarial settings and bound the\ngeneralization error for DNNs trained under several well-known gradient-based\nattack schemes, motivating an effective regularization scheme based on spectral\nnormalization of the DNN's weight matrices. We also provide a\ncomputationally-efficient method for normalizing the spectral norm of\nconvolutional layers with arbitrary stride and padding schemes in deep\nconvolutional networks. We evaluate the power of spectral normalization\nextensively on combinations of datasets, network architectures, and adversarial\ntraining schemes. The code is available at\nhttps://github.com/jessemzhang/dl_spectral_normalization. \n\n"}
{"id": "1811.07624", "contents": "Title: Approximate Eigenvalue Decompositions of Linear Transformations with a\n  Few Householder Reflectors Abstract: The ability to decompose a signal in an orthonormal basis (a set of\northogonal components, each normalized to have unit length) using a fast\nnumerical procedure rests at the heart of many signal processing methods and\napplications. The classic examples are the Fourier and wavelet transforms that\nenjoy numerically efficient implementations (FFT and FWT, respectively).\nUnfortunately, orthonormal transformations are in general unstructured, and\ntherefore they do not enjoy low computational complexity properties. In this\npaper, based on Householder reflectors, we introduce a class of orthonormal\nmatrices that are numerically efficient to manipulate: we control the\ncomplexity of matrix-vector multiplications with these matrices using a given\nparameter. We provide numerical algorithms that approximate any orthonormal or\nsymmetric transform with a new orthonormal or symmetric structure made up of\nproducts of a given number of Householder reflectors. We show analyses and\nnumerical evidence to highlight the accuracy of the proposed approximations and\nprovide an application to the case of learning fast Mahanalobis distance metric\ntransformations. \n\n"}
{"id": "1811.07746", "contents": "Title: An Empirical Assessment of the Complexity and Realism of Synthetic\n  Social Contact Networks Abstract: We use multiple measures of graph complexity to evaluate the realism of\nsynthetically-generated networks of human activity, in comparison with several\nstylized network models as well as a collection of empirical networks from the\nliterature. The synthetic networks are generated by integrating data about\nhuman populations from several sources, including the Census, transportation\nsurveys, and geographical data. The resulting networks represent an\napproximation of daily or weekly human interaction. Our results indicate that\nthe synthetically generated graphs according to our methodology are closer to\nthe real world graphs, as measured across multiple structural measures, than a\nrange of stylized graphs generated using common network models from the\nliterature. \n\n"}
{"id": "1811.07755", "contents": "Title: Building Efficient Deep Neural Networks with Unitary Group Convolutions Abstract: We propose unitary group convolutions (UGConvs), a building block for CNNs\nwhich compose a group convolution with unitary transforms in feature space to\nlearn a richer set of representations than group convolution alone. UGConvs\ngeneralize two disparate ideas in CNN architecture, channel shuffling (i.e.\nShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying\ninsights that lead to a deeper understanding of each technique. We\nexperimentally demonstrate that dense unitary transforms can outperform channel\nshuffling in DNN accuracy. On the other hand, different dense transforms\nexhibit comparable accuracy performance. Based on these observations we propose\nHadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar\naccuracy to circulant networks with lower computation complexity, and better\naccuracy than ShuffleNets with the same number of parameters and floating-point\nmultiplies. \n\n"}
{"id": "1811.08010", "contents": "Title: Stackelberg GAN: Towards Provable Minimax Equilibrium via\n  Multi-Generator Architectures Abstract: We study the problem of alleviating the instability issue in the GAN training\nprocedure via new architecture design. The discrepancy between the minimax and\nmaximin objective values could serve as a proxy for the difficulties that the\nalternating gradient descent encounters in the optimization of GANs. In this\nwork, we give new results on the benefits of multi-generator architecture of\nGANs. We show that the minimax gap shrinks to $\\epsilon$ as the number of\ngenerators increases with rate $\\widetilde{O}(1/\\epsilon)$. This improves over\nthe best-known result of $\\widetilde{O}(1/\\epsilon^2)$. At the core of our\ntechniques is a novel application of Shapley-Folkman lemma to the generic\nminimax problem, where in the literature the technique was only known to work\nwhen the objective function is restricted to the Lagrangian function of a\nconstraint optimization problem. Our proposed Stackelberg GAN performs well\nexperimentally in both synthetic and real-world datasets, improving Fr\\'echet\nInception Distance by $14.61\\%$ over the previous multi-generator GANs on the\nbenchmark datasets. \n\n"}
{"id": "1811.08035", "contents": "Title: Simultaneous 12-Lead Electrocardiogram Synthesis using a Single-Lead ECG\n  Signal: Application to Handheld ECG Devices Abstract: Recent introduction of wearable single-lead ECG devices of diverse\nconfigurations has caught the intrigue of the medical community. While these\ndevices provide a highly affordable support tool for the caregivers for\ncontinuous monitoring and to detect acute conditions, such as arrhythmia, their\nutility for cardiac diagnostics remains limited. This is because clinical\ndiagnosis of many cardiac pathologies is rooted in gleaning patterns from\nsynchronous 12-lead ECG. If synchronous 12-lead signals of clinical quality can\nbe synthesized from these single-lead devices, it can transform cardiac care by\nsubstantially reducing the costs and enhancing access to cardiac diagnostics.\nHowever, prior attempts to synthesize synchronous 12-lead ECG have not been\nsuccessful. Vectorcardiography (VCG) analysis suggests that cardiac axis\nsynthesized from earlier attempts deviates significantly from that estimated\nfrom 12-lead and/or Frank lead measurements. This work is perhaps the first\nsuccessful attempt to synthesize clinically equivalent synchronous 12-lead ECG\nfrom single-lead ECG. Our method employs a random forest machine learning model\nthat uses a subject's historical 12-lead recordings to estimate the morphology\nincluding the actual timing of various ECG events (relative to the measured\nsingle-lead ECG) for all 11 missing leads of the subject. Our method was\nvalidated on two benchmark datasets as well as paper ECG and AliveCor-Kardia\ndata obtained from the Heart, Artery, and Vein Center of Fresno, California.\nResults suggest that this approach can synthesize synchronous ECG with\naccuracies (R2) exceeding 90%. Accurate synthesis of 12-lead ECG from a\nsingle-lead device can ultimately enable its wider application and improved\npoint-of-care (POC) diagnostics. \n\n"}
{"id": "1811.08117", "contents": "Title: Limited Gradient Descent: Learning With Noisy Labels Abstract: Label noise may affect the generalization of classifiers, and the effective\nlearning of main patterns from samples with noisy labels is an important\nchallenge. Recent studies have shown that deep neural networks tend to\nprioritize the learning of simple patterns over the memorization of noise\npatterns. This suggests a possible method to search for the best generalization\nthat learns the main pattern until the noise begins to be memorized.\nTraditional approaches often employ a clean validation set to find the best\nstop timing of learning, i.e., early stopping. However, the generalization\nperformance of such methods relies on the quality of validation sets. Further,\nin practice, a clean validation set is sometimes difficult to obtain. To solve\nthis problem, we propose a method that can estimate the optimal stopping timing\nwithout a clean validation set, called limited gradient descent. We modified\nthe labels of a few samples in a noisy dataset to obtain false labels and to\ncreate a reverse pattern. By monitoring the learning progress of the noisy and\nreverse samples, we can determine the stop timing of learning. In this paper,\nwe also theoretically provide some necessary conditions on learning with noisy\nlabels. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate\nthat our approach has a comparable generalization performance to methods\nrelying on a clean validation set. Thus, on the noisy Clothing-1M dataset, our\napproach surpasses methods that rely on a clean validation set. \n\n"}
{"id": "1811.08790", "contents": "Title: Learning Quadratic Games on Networks Abstract: Individuals, or organizations, cooperate with or compete against one another\nin a wide range of practical situations. Such strategic interactions are often\nmodeled as games played on networks, where an individual's payoff depends not\nonly on her action but also on that of her neighbors. The current literature\nhas largely focused on analyzing the characteristics of network games in the\nscenario where the structure of the network, which is represented by a graph,\nis known beforehand. It is often the case, however, that the actions of the\nplayers are readily observable while the underlying interaction network remains\nhidden. In this paper, we propose two novel frameworks for learning, from the\nobservations on individual actions, network games with linear-quadratic\npayoffs, and in particular, the structure of the interaction network. Our\nframeworks are based on the Nash equilibrium of such games and involve solving\na joint optimization problem for the graph structure and the individual\nmarginal benefits. Both synthetic and real-world experiments demonstrate the\neffectiveness of the proposed frameworks, which have theoretical as well as\npractical implications for understanding strategic interactions in a network\nenvironment. \n\n"}
{"id": "1811.08871", "contents": "Title: Efficient nonmyopic active search with applications in drug and\n  materials discovery Abstract: Active search is a learning paradigm for actively identifying as many members\nof a given class as possible. A critical target scenario is high-throughput\nscreening for scientific discovery, such as drug or materials discovery. In\nthis paper, we approach this problem in Bayesian decision framework. We first\nderive the Bayesian optimal policy under a natural utility, and establish a\ntheoretical hardness of active search, proving that the optimal policy can not\nbe approximated for any constant ratio. We also study the batch setting for the\nfirst time, where a batch of $b>1$ points can be queried at each iteration. We\ngive an asymptotic lower bound, linear in batch size, on the adaptivity gap:\nhow much we could lose if we query $b$ points at a time for $t$ iterations,\ninstead of one point at a time for $bt$ iterations. We then introduce a novel\napproach to nonmyopic approximations of the optimal policy that admits\nefficient computation. Our proposed policy can automatically trade off\nexploration and exploitation, without relying on any tuning parameters. We also\ngeneralize our policy to batch setting, and propose two approaches to tackle\nthe combinatorial search challenge. We evaluate our proposed policies on a\nlarge database of drug discovery and materials science. Results demonstrate the\nsuperior performance of our proposed policy in both sequential and batch\nsetting; the nonmyopic behavior is also illustrated in various aspects. \n\n"}
{"id": "1811.08996", "contents": "Title: HyperAdam: A Learnable Task-Adaptive Adam for Network Training Abstract: Deep neural networks are traditionally trained using human-designed\nstochastic optimization algorithms, such as SGD and Adam. Recently, the\napproach of learning to optimize network parameters has emerged as a promising\nresearch topic. However, these learned black-box optimizers sometimes do not\nfully utilize the experience in human-designed optimizers, therefore have\nlimitation in generalization ability. In this paper, a new optimizer, dubbed as\n\\textit{HyperAdam}, is proposed that combines the idea of \"learning to\noptimize\" and traditional Adam optimizer. Given a network for training, its\nparameter update in each iteration generated by HyperAdam is an adaptive\ncombination of multiple updates generated by Adam with varying decay rates. The\ncombination weights and decay rates in HyperAdam are adaptively learned\ndepending on the task. HyperAdam is modeled as a recurrent neural network with\nAdamCell, WeightCell and StateCell. It is justified to be state-of-the-art for\nvarious network training, such as multilayer perceptron, CNN and LSTM. \n\n"}
{"id": "1811.09013", "contents": "Title: An Off-policy Policy Gradient Theorem Using Emphatic Weightings Abstract: Policy gradient methods are widely used for control in reinforcement\nlearning, particularly for the continuous action setting. There have been a\nhost of theoretically sound algorithms proposed for the on-policy setting, due\nto the existence of the policy gradient theorem which provides a simplified\nform for the gradient. In off-policy learning, however, where the behaviour\npolicy is not necessarily attempting to learn and follow the optimal policy for\nthe given task, the existence of such a theorem has been elusive. In this work,\nwe solve this open problem by providing the first off-policy policy gradient\ntheorem. The key to the derivation is the use of $emphatic$ $weightings$. We\ndevelop a new actor-critic algorithm$\\unicode{x2014}$called Actor Critic with\nEmphatic weightings (ACE)$\\unicode{x2014}$that approximates the simplified\ngradients provided by the theorem. We demonstrate in a simple counterexample\nthat previous off-policy policy gradient methods$\\unicode{x2014}$particularly\nOffPAC and DPG$\\unicode{x2014}$converge to the wrong solution whereas ACE finds\nthe optimal solution. \n\n"}
{"id": "1811.09702", "contents": "Title: Homogeneity-Based Transmissive Process to Model True and False News in\n  Social Networks Abstract: An overwhelming number of true and false news stories are posted and shared\nin social networks, and users diffuse the stories based on multiple factors.\nDiffusion of news stories from one user to another depends not only on the\nstories' content and the genuineness but also on the alignment of the topical\ninterests between the users. In this paper, we propose a novel Bayesian\nnonparametric model that incorporates homogeneity of news stories as the key\ncomponent that regulates the topical similarity between the posting and sharing\nusers' topical interests. Our model extends hierarchical Dirichlet process to\nmodel the topics of the news stories and incorporates Bayesian Gaussian process\nlatent variable model to discover the homogeneity values. We train our model on\na real-world social network dataset and find homogeneity values of news stories\nthat strongly relate to their labels of genuineness and their contents.\nFinally, we show that the supervised version of our model predicts the labels\nof news stories better than the state-of-the-art neural network and Bayesian\nmodels. \n\n"}
{"id": "1811.09886", "contents": "Title: Deep Learning Inference in Facebook Data Centers: Characterization,\n  Performance Optimizations and Hardware Implications Abstract: The application of deep learning techniques resulted in remarkable\nimprovement of machine learning models. In this paper provides detailed\ncharacterizations of deep learning models used in many Facebook social network\nservices. We present computational characteristics of our models, describe high\nperformance optimizations targeting existing systems, point out their\nlimitations and make suggestions for the future general-purpose/accelerated\ninference hardware. Also, we highlight the need for better co-design of\nalgorithms, numerics and computing platforms to address the challenges of\nworkloads often run in data centers. \n\n"}
{"id": "1811.09955", "contents": "Title: Online Newton Step Algorithm with Estimated Gradient Abstract: Online learning with limited information feedback (bandit) tries to solve the\nproblem where an online learner receives partial feedback information from the\nenvironment in the course of learning. Under this setting, Flaxman et al.[8]\nextended Zinkevich's classical Online Gradient Descent (OGD) algorithm [29] by\nproposing the Online Gradient Descent with Expected Gradient (OGDEG) algorithm.\nSpecifically, it uses a simple trick to approximate the gradient of the loss\nfunction $f_t$ by evaluating it at a single point and bounds the expected\nregret as $\\mathcal{O}(T^{5/6})$ [8], where the number of rounds is $T$.\nMeanwhile, past research efforts have shown that compared with the first-order\nalgorithms, second-order online learning algorithms such as Online Newton Step\n(ONS) [11] can significantly accelerate the convergence rate of traditional\nonline learning algorithms. Motivated by this, this paper aims to exploit the\nsecond-order information to speed up the convergence of the OGDEG algorithm. In\nparticular, we extend the ONS algorithm with the trick of expected gradient and\ndevelop a novel second-order online learning algorithm, i.e., Online Newton\nStep with Expected Gradient (ONSEG). Theoretically, we show that the proposed\nONSEG algorithm significantly reduces the expected regret of OGDEG algorithm\nfrom $\\mathcal{O}(T^{5/6})$ to $\\mathcal{O}(T^{2/3})$ in the bandit feedback\nscenario. Empirically, we further demonstrate the advantages of the proposed\nalgorithm on multiple real-world datasets. \n\n"}
{"id": "1811.10112", "contents": "Title: A Model-Based Reinforcement Learning Approach for a Rare Disease\n  Diagnostic Task Abstract: In this work, we present our various contributions to the objective of\nbuilding a decision support tool for the diagnosis of rare diseases. Our goal\nis to achieve a state of knowledge where the uncertainty about the patient's\ndisease is below a predetermined threshold. We aim to reach such states while\nminimizing the average number of medical tests to perform. In doing so, we take\ninto account the need, in many medical applications, to avoid, as much as\npossible, any misdiagnosis. To solve this optimization task, we investigate\nseveral reinforcement learning algorithm and make them operable in our\nhigh-dimensional and sparse-reward setting. We also present a way to combine\nexpert knowledge, expressed as conditional probabilities, with real clinical\ndata. This is crucial because the scarcity of data in the field of rare\ndiseases prevents any approach based solely on clinical data. Finally we show\nthat it is possible to integrate the ontological information about symptoms\nwhile remaining in our probabilistic reasoning. It enables our decision support\ntool to process information given at different level of precision by the user. \n\n"}
{"id": "1811.10158", "contents": "Title: Reinforcement Learning for Uplift Modeling Abstract: Uplift modeling aims to directly model the incremental impact of a treatment\non an individual response. In this work, we address the problem from a new\nangle and reformulate it as a Markov Decision Process (MDP). We conducted\nextensive experiments on both a synthetic dataset and real-world scenarios, and\nshowed that our method can achieve significant improvement over previous\nmethods. \n\n"}
{"id": "1811.10501", "contents": "Title: Deep Ensemble Tensor Factorization for Longitudinal Patient Trajectories\n  Classification Abstract: We present a generative approach to classify scarcely observed longitudinal\npatient trajectories. The available time series are represented as tensors and\nfactorized using generative deep recurrent neural networks. The learned factors\nrepresent the patient data in a compact way and can then be used in a\ndownstream classification task. For more robustness and accuracy in the\npredictions, we used an ensemble of those deep generative models to mimic\nBayesian posterior sampling. We illustrate the performance of our architecture\non an intensive-care case study of in-hospital mortality prediction with 96\nlongitudinal measurement types measured across the first 48-hour from\nadmission. Our combination of generative and ensemble strategies achieves an\nAUC of over 0.85, and outperforms the SAPS-II mortality score and GRU\nbaselines. \n\n"}
{"id": "1811.10658", "contents": "Title: HELOC Applicant Risk Performance Evaluation by Topological Hierarchical\n  Decomposition Abstract: Strong regulations in the financial industry mean that any decisions based on\nmachine learning need to be explained. This precludes the use of powerful\nsupervised techniques such as neural networks. In this study we propose a new\nunsupervised and semi-supervised technique known as the topological\nhierarchical decomposition (THD). This process breaks a dataset down into ever\nsmaller groups, where groups are associated with a simplicial complex that\napproximate the underlying topology of a dataset. We apply THD to the FICO\nmachine learning challenge dataset, consisting of anonymized home equity loan\napplications using the MAPPER algorithm to build simplicial complexes. We\nidentify different groups of individuals unable to pay back loans, and\nillustrate how the distribution of feature values in a simplicial complex can\nbe used to explain the decision to grant or deny a loan by extracting\nillustrative explanations from two THDs on the dataset. \n\n"}
{"id": "1811.10746", "contents": "Title: MATCH-Net: Dynamic Prediction in Survival Analysis using Convolutional\n  Neural Networks Abstract: Accurate prediction of disease trajectories is critical for early\nidentification and timely treatment of patients at risk. Conventional methods\nin survival analysis are often constrained by strong parametric assumptions and\nlimited in their ability to learn from high-dimensional data, while existing\nneural network models are not readily-adapted to the longitudinal setting. This\npaper develops a novel convolutional approach that addresses these drawbacks.\nWe present MATCH-Net: a Missingness-Aware Temporal Convolutional Hitting-time\nNetwork, designed to capture temporal dependencies and heterogeneous\ninteractions in covariate trajectories and patterns of missingness. To the best\nof our knowledge, this is the first investigation of temporal convolutions in\nthe context of dynamic prediction for personalized risk prognosis. Using\nreal-world data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate state-of-the-art performance without making any assumptions\nregarding underlying longitudinal or time-to-event processes attesting to the\nmodel's potential utility in clinical decision support. \n\n"}
{"id": "1811.10790", "contents": "Title: High-dimensional Index Volatility Models via Stein's Identity Abstract: We study the estimation of the parametric components of single and multiple\nindex volatility models. Using the first- and second-order Stein's identities,\nwe develop methods that are applicable for the estimation of the variance index\nin the high-dimensional setting requiring finite moment condition, which allows\nfor heavy-tailed data. Our approach complements the existing literature in the\nlow-dimensional setting, while relaxing the conditions on estimation, and\nprovides a novel approach in the high-dimensional setting. We prove that the\nstatistical rate of convergence of our variance index estimators consists of a\nparametric rate and a nonparametric rate, where the latter appears from the\nestimation of the mean link function. However, under standard assumptions, the\nparametric rate dominates the rate of convergence and our results match the\nminimax optimal rate for the mean index estimation. Simulation results\nillustrate finite sample properties of our methodology and back our theoretical\nconclusions. \n\n"}
{"id": "1811.10990", "contents": "Title: Generating Responses Expressing Emotion in an Open-domain Dialogue\n  System Abstract: Neural network-based Open-ended conversational agents automatically generate\nresponses based on predictive models learned from a large number of pairs of\nutterances. The generated responses are typically acceptable as a sentence but\nare often dull, generic, and certainly devoid of any emotion. In this paper, we\npresent neural models that learn to express a given emotion in the generated\nresponse. We propose four models and evaluate them against 3 baselines. An\nencoder-decoder framework-based model with multiple attention layers provides\nthe best overall performance in terms of expressing the required emotion. While\nit does not outperform other models on all emotions, it presents promising\nresults in most cases. \n\n"}
{"id": "1811.10999", "contents": "Title: Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment\n  Classification Abstract: Aspect-level sentiment classification (ASC) aims at identifying sentiment\npolarities towards aspects in a sentence, where the aspect can behave as a\ngeneral Aspect Category (AC) or a specific Aspect Term (AT). However, due to\nthe especially expensive and labor-intensive labeling, existing public corpora\nin AT-level are all relatively small. Meanwhile, most of the previous methods\nrely on complicated structures with given scarce data, which largely limits the\nefficacy of the neural models. In this paper, we exploit a new direction named\ncoarse-to-fine task transfer, which aims to leverage knowledge learned from a\nrich-resource source domain of the coarse-grained AC task, which is more easily\naccessible, to improve the learning in a low-resource target domain of the\nfine-grained AT task. To resolve both the aspect granularity inconsistency and\nfeature mismatch between domains, we propose a Multi-Granularity Alignment\nNetwork (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary\ntask can help the AC task modeling at the same fine-grained level with the AT\ntask. To alleviate the feature false alignment, a contrastive feature alignment\nmethod is adopted to align aspect-specific feature representations\nsemantically. In addition, a large-scale multi-domain dataset for the AC task\nis provided. Empirically, extensive experiments demonstrate the effectiveness\nof the MGAN. \n\n"}
{"id": "1811.11214", "contents": "Title: Understanding the impact of entropy on policy optimization Abstract: Entropy regularization is commonly used to improve policy optimization in\nreinforcement learning. It is believed to help with \\emph{exploration} by\nencouraging the selection of more stochastic policies. In this work, we analyze\nthis claim using new visualizations of the optimization landscape based on\nrandomly perturbing the loss function. We first show that even with access to\nthe exact gradient, policy optimization is difficult due to the geometry of the\nobjective function. Then, we qualitatively show that in some environments, a\npolicy with higher entropy can make the optimization landscape smoother,\nthereby connecting local optima and enabling the use of larger learning rates.\nThis paper presents new tools for understanding the optimization landscape,\nshows that policy entropy serves as a regularizer, and highlights the challenge\nof designing general-purpose policy optimization algorithms. \n\n"}
{"id": "1811.11310", "contents": "Title: Using Attribution to Decode Dataset Bias in Neural Network Models for\n  Chemistry Abstract: Deep neural networks have achieved state of the art accuracy at classifying\nmolecules with respect to whether they bind to specific protein targets. A key\nbreakthrough would occur if these models could reveal the fragment\npharmacophores that are causally involved in binding. Extracting chemical\ndetails of binding from the networks could potentially lead to scientific\ndiscoveries about the mechanisms of drug actions. But doing so requires shining\nlight into the black box that is the trained neural network model, a task that\nhas proved difficult across many domains. Here we show how the binding\nmechanism learned by deep neural network models can be interrogated, using a\nrecently described attribution method. We first work with carefully constructed\nsynthetic datasets, in which the 'fragment logic' of binding is fully known. We\nfind that networks that achieve perfect accuracy on held out test datasets\nstill learn spurious correlations due to biases in the datasets, and we are\nable to exploit this non-robustness to construct adversarial examples that fool\nthe model. The dataset bias makes these models unreliable for accurately\nrevealing information about the mechanisms of protein-ligand binding. In light\nof our findings, we prescribe a test that checks for dataset bias given a\nhypothesis. If the test fails, it indicates that either the model must be\nsimplified or regularized and/or that the training dataset requires\naugmentation. \n\n"}
{"id": "1811.11347", "contents": "Title: Effective Ways to Build and Evaluate Individual Survival Distributions Abstract: An accurate model of a patient's individual survival distribution can help\ndetermine the appropriate treatment for terminal patients. Unfortunately, risk\nscores (e.g., from Cox Proportional Hazard models) do not provide survival\nprobabilities, single-time probability models (e.g., the Gail model, predicting\n5 year probability) only provide for a single time point, and standard\nKaplan-Meier survival curves provide only population averages for a large class\nof patients meaning they are not specific to individual patients. This\nmotivates an alternative class of tools that can learn a model which provides\nan individual survival distribution which gives survival probabilities across\nall times - such as extensions to the Cox model, Accelerated Failure Time, an\nextension to Random Survival Forests, and Multi-Task Logistic Regression. This\npaper first motivates such \"individual survival distribution\" (ISD) models, and\nexplains how they differ from standard models. It then discusses ways to\nevaluate such models - namely Concordance, 1-Calibration, Brier score, and\nvarious versions of L1-loss - and then motivates and defines a novel approach\n\"D-Calibration\", which determines whether a model's probability estimates are\nmeaningful. We also discuss how these measures differ, and use them to evaluate\nseveral ISD prediction tools, over a range of survival datasets. \n\n"}
{"id": "1811.11427", "contents": "Title: Deep Collective Matrix Factorization for Augmented Multi-View Learning Abstract: Learning by integrating multiple heterogeneous data sources is a common\nrequirement in many tasks. Collective Matrix Factorization (CMF) is a technique\nto learn shared latent representations from arbitrary collections of matrices.\nIt can be used to simultaneously complete one or more matrices, for predicting\nthe unknown entries. Classical CMF methods assume linearity in the interaction\nof latent factors which can be restrictive and fails to capture complex\nnon-linear interactions. In this paper, we develop the first deep-learning\nbased method, called dCMF, for unsupervised learning of multiple shared\nrepresentations, that can model such non-linear interactions, from an arbitrary\ncollection of matrices. We address optimization challenges that arise due to\ndependencies between shared representations through Multi-Task Bayesian\nOptimization and design an acquisition function adapted for collective learning\nof hyperparameters. Our experiments show that dCMF significantly outperforms\nprevious CMF algorithms in integrating heterogeneous data for predictive\nmodeling. Further, on two tasks - recommendation and prediction of gene-disease\nassociation - dCMF outperforms state-of-the-art matrix completion algorithms\nthat can utilize auxiliary sources of information. \n\n"}
{"id": "1811.11881", "contents": "Title: Adversarial Bandits with Knapsacks Abstract: We consider Bandits with Knapsacks (henceforth, BwK), a general model for\nmulti-armed bandits under supply/budget constraints. In particular, a bandit\nalgorithm needs to solve a well-known knapsack problem: find an optimal packing\nof items into a limited-size knapsack. The BwK problem is a common\ngeneralization of numerous motivating examples, which range from dynamic\npricing to repeated auctions to dynamic ad allocation to network routing and\nscheduling. While the prior work on BwK focused on the stochastic version, we\npioneer the other extreme in which the outcomes can be chosen adversarially.\nThis is a considerably harder problem, compared to both the stochastic version\nand the \"classic\" adversarial bandits, in that regret minimization is no longer\nfeasible. Instead, the objective is to minimize the competitive ratio: the\nratio of the benchmark reward to the algorithm's reward.\n  We design an algorithm with competitive ratio O(log T) relative to the best\nfixed distribution over actions, where T is the time horizon; we also prove a\nmatching lower bound. The key conceptual contribution is a new perspective on\nthe stochastic version of the problem. We suggest a new algorithm for the\nstochastic version, which builds on the framework of regret minimization in\nrepeated games and admits a substantially simpler analysis compared to prior\nwork. We then analyze this algorithm for the adversarial version and use it as\na subroutine to solve the latter. \n\n"}
{"id": "1811.12156", "contents": "Title: Improved Deep Embeddings for Inferencing with Multi-Layered Networks Abstract: Inferencing with network data necessitates the mapping of its nodes into a\nvector space, where the relationships are preserved. However, with\nmulti-layered networks, where multiple types of relationships exist for the\nsame set of nodes, it is crucial to exploit the information shared between\nlayers, in addition to the distinct aspects of each layer. In this paper, we\npropose a novel approach that first obtains node embeddings in all layers\njointly via DeepWalk on a \\textit{supra} graph, which allows interactions\nbetween layers, and then fine-tunes the embeddings to encourage cohesive\nstructure in the latent space. With empirical studies in node classification,\nlink prediction and multi-layered community detection, we show that the\nproposed approach outperforms existing single- and multi-layered network\nembedding algorithms on several benchmarks. In addition to effectively scaling\nto a large number of layers (tested up to $37$), our approach consistently\nproduces highly modular community structure, even when compared to methods that\ndirectly optimize for the modularity function. \n\n"}
{"id": "1811.12162", "contents": "Title: Effective Resistance-based Germination of Seed Sets for Community\n  Detection Abstract: Community detection is, at its core, an attempt to attach an interpretable\nfunction to an otherwise indecipherable form. The importance of labeling\ncommunities has obvious implications for identifying clusters in social\nnetworks, but it has a number of equally relevant applications in product\nrecommendations, biological systems, and many forms of classification. The\nlocal variety of community detection starts with a small set of labeled seed\nnodes, and aims to estimate the community containing these nodes. One of the\nmost ubiquitous methods - due to its simplicity and efficiency - is\npersonalized PageRank. The most obvious bottleneck for deploying this form of\nPageRank successfully is the quality of the seeds. We introduce a \"germination\"\nstage for these seeds, where an effective resistance-based approach is used to\nincrease the quality and number of seeds from which a community is detected. By\nbreaking seed set expansion into a two-step process, we aim to utilize two\ndistinct random walk-based approaches in the regimes in which they excel. In\nsynthetic and real network data, a simple, greedy algorithm which minimizes the\neffective resistance diameter combined with PageRank achieves clear\nimprovements in precision and recall over a standalone PageRank procedure. \n\n"}
{"id": "1811.12169", "contents": "Title: Predicting Opioid Relapse Using Social Media Data Abstract: Opioid addiction is a severe public health threat in the U.S, causing massive\ndeaths and many social problems. Accurate relapse prediction is of practical\nimportance for recovering patients since relapse prediction promotes timely\nrelapse preventions that help patients stay clean. In this paper, we introduce\na Generative Adversarial Networks (GAN) model to predict the addiction relapses\nbased on sentiment images and social influences. Experimental results on real\nsocial media data from Reddit.com demonstrate that the GAN model delivers a\nbetter performance than comparable alternative techniques. The sentiment images\ngenerated by the model show that relapse is closely connected with two emotions\n`joy' and `negative'. This work is one of the first attempts to predict\nrelapses using massive social media data and generative adversarial nets. The\nproposed method, combined with knowledge of social media mining, has the\npotential to revolutionize the practice of opioid addiction prevention and\ntreatment. \n\n"}
{"id": "1811.12361", "contents": "Title: Smoothed Analysis in Unsupervised Learning via Decoupling Abstract: Smoothed analysis is a powerful paradigm in overcoming worst-case\nintractability in unsupervised learning and high-dimensional data analysis.\nWhile polynomial time smoothed analysis guarantees have been obtained for\nworst-case intractable problems like tensor decompositions and learning\nmixtures of Gaussians, such guarantees have been hard to obtain for several\nother important problems in unsupervised learning. A core technical challenge\nin analyzing algorithms is obtaining lower bounds on the least singular value\nfor random matrix ensembles with dependent entries, that are given by\nlow-degree polynomials of a few base underlying random variables.\n  In this work, we address this challenge by obtaining high-confidence lower\nbounds on the least singular value of new classes of structured random matrix\nensembles of the above kind. We then use these bounds to design algorithms with\npolynomial time smoothed analysis guarantees for the following three important\nproblems in unsupervised learning:\n  1. Robust subspace recovery, when the fraction $\\alpha$ of inliers in the\nd-dimensional subspace $T \\subset \\mathbb{R}^n$ is at least $\\alpha >\n(d/n)^\\ell$ for any constant integer $\\ell>0$. This contrasts with the known\nworst-case intractability when $\\alpha< d/n$, and the previous smoothed\nanalysis result which needed $\\alpha > d/n$ (Hardt and Moitra, 2013).\n  2. Learning overcomplete hidden markov models, where the size of the state\nspace is any polynomial in the dimension of the observations. This gives the\nfirst polynomial time guarantees for learning overcomplete HMMs in a smoothed\nanalysis model.\n  3. Higher order tensor decompositions, where we generalize the so-called\nFOOBI algorithm of Cardoso to find order-$\\ell$ rank-one tensors in a subspace.\nThis allows us to obtain polynomially robust decomposition algorithms for\n$2\\ell$'th order tensors with rank $O(n^{\\ell})$. \n\n"}
{"id": "1811.12799", "contents": "Title: Customer Lifetime Value in Video Games Using Deep Learning and\n  Parametric Models Abstract: Nowadays, video game developers record every virtual action performed by\ntheir players. As each player can remain in the game for years, this results in\nan exceptionally rich dataset that can be used to understand and predict player\nbehavior. In particular, this information may serve to identify the most\nvaluable players and foresee the amount of money they will spend in in-app\npurchases during their lifetime. This is crucial in free-to-play games, where\nup to 50% of the revenue is generated by just around 2% of the players, the\nso-called whales.\n  To address this challenge, we explore how deep neural networks can be used to\npredict customer lifetime value in video games, and compare their performance\nto parametric models such as Pareto/NBD. Our results suggest that convolutional\nneural network structures are the most efficient in predicting the economic\nvalue of individual players. They not only perform better in terms of accuracy,\nbut also scale to big data and significantly reduce computational time, as they\ncan work directly with raw sequential data and thus do not require any feature\nengineering process. This becomes important when datasets are very large, as is\noften the case with video game logs.\n  Moreover, convolutional neural networks are particularly well suited to\nidentify potential whales. Such an early identification is of paramount\nimportance for business purposes, as it would allow developers to implement\nin-game actions aimed at retaining big spenders and maximizing their lifetime,\nwhich would ultimately translate into increased revenue. \n\n"}
{"id": "1812.00141", "contents": "Title: A Dynamic Network and Representation LearningApproach for Quantifying\n  Economic Growth fromSatellite Imagery Abstract: Quantifying the improvement in human living standard, as well as the city\ngrowth in developing countries, is a challenging problem due to the lack of\nreliable economic data. Therefore, there is a fundamental need for alternate,\nlargely unsupervised, computational methods that can estimate the economic\nconditions in the developing regions. To this end, we propose a new network\nscience- and representation learning-based approach that can quantify economic\nindicators and visualize the growth of various regions. More precisely, we\nfirst create a dynamic network drawn out of high-resolution nightlight\nsatellite images. We then demonstrate that using representation learning to\nmine the resulting network, our proposed approach can accurately predict\nspatial gross economic expenditures over large regions. Our method, which\nrequires only nightlight images and limited survey data, can capture\ncity-growth, as well as how people's living standard is changing; this can\nultimately facilitate the decision makers' understanding of growth without\nheavily relying on expensive and time-consuming surveys. \n\n"}
{"id": "1812.00249", "contents": "Title: On Compressing U-net Using Knowledge Distillation Abstract: We study the use of knowledge distillation to compress the U-net\narchitecture. We show that, while standard distillation is not sufficient to\nreliably train a compressed U-net, introducing other regularization methods,\nsuch as batch normalization and class re-weighting, in knowledge distillation\nsignificantly improves the training process. This allows us to compress a U-net\nby over 1000x, i.e., to 0.1% of its original number of parameters, at a\nnegligible decrease in performance. \n\n"}
{"id": "1812.00335", "contents": "Title: GAN-EM: GAN based EM learning framework Abstract: Expectation maximization (EM) algorithm is to find maximum likelihood\nsolution for models having latent variables. A typical example is Gaussian\nMixture Model (GMM) which requires Gaussian assumption, however, natural images\nare highly non-Gaussian so that GMM cannot be applied to perform clustering\ntask on pixel space. To overcome such limitation, we propose a GAN based EM\nlearning framework that can maximize the likelihood of images and estimate the\nlatent variables with only the constraint of L-Lipschitz continuity. We call\nthis model GAN-EM, which is a framework for image clustering, semi-supervised\nclassification and dimensionality reduction. In M-step, we design a novel loss\nfunction for discriminator of GAN to perform maximum likelihood estimation\n(MLE) on data with soft class label assignments. Specifically, a conditional\ngenerator captures data distribution for $K$ classes, and a discriminator tells\nwhether a sample is real or fake for each class. Since our model is\nunsupervised, the class label of real data is regarded as latent variable,\nwhich is estimated by an additional network (E-net) in E-step. The proposed\nGAN-EM achieves state-of-the-art clustering and semi-supervised classification\nresults on MNIST, SVHN and CelebA, as well as comparable quality of generated\nimages to other recently developed generative models. \n\n"}
{"id": "1812.00371", "contents": "Title: Predicting Inpatient Discharge Prioritization With Electronic Health\n  Records Abstract: Identifying patients who will be discharged within 24 hours can improve\nhospital resource management and quality of care. We studied this problem using\neight years of Electronic Health Records (EHR) data from Stanford Hospital. We\nfit models to predict 24 hour discharge across the entire inpatient population.\nThe best performing models achieved an area under the receiver-operator\ncharacteristic curve (AUROC) of 0.85 and an AUPRC of 0.53 on a held out test\nset. This model was also well calibrated. Finally, we analyzed the utility of\nthis model in a decision theoretic framework to identify regions of ROC space\nin which using the model increases expected utility compared to the trivial\nalways negative or always positive classifiers. \n\n"}
{"id": "1812.00417", "contents": "Title: Snorkel DryBell: A Case Study in Deploying Weak Supervision at\n  Industrial Scale Abstract: Labeling training data is one of the most costly bottlenecks in developing\nmachine learning-based applications. We present a first-of-its-kind study\nshowing how existing knowledge resources from across an organization can be\nused as weak supervision in order to bring development time and cost down by an\norder of magnitude, and introduce Snorkel DryBell, a new weak supervision\nmanagement system for this setting. Snorkel DryBell builds on the Snorkel\nframework, extending it in three critical aspects: flexible, template-based\ningestion of diverse organizational knowledge, cross-feature production\nserving, and scalable, sampling-free execution. On three classification tasks\nat Google, we find that Snorkel DryBell creates classifiers of comparable\nquality to ones trained with tens of thousands of hand-labeled examples,\nconverts non-servable organizational resources to servable models for an\naverage 52% performance improvement, and executes over millions of data points\nin tens of minutes. \n\n"}
{"id": "1812.00420", "contents": "Title: Efficient Lifelong Learning with A-GEM Abstract: In lifelong learning, the learner is presented with a sequence of tasks,\nincrementally building a data-driven prior which may be leveraged to speed up\nlearning of a new task. In this work, we investigate the efficiency of current\nlifelong approaches, in terms of sample complexity, computational and memory\ncost. Towards this end, we first introduce a new and a more realistic\nevaluation protocol, whereby learners observe each example only once and\nhyper-parameter selection is done on a small and disjoint set of tasks, which\nis not used for the actual learning experience and evaluation. Second, we\nintroduce a new metric measuring how quickly a learner acquires a new skill.\nThird, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017),\ndubbed Averaged GEM (A-GEM), which enjoys the same or even better performance\nas GEM, while being almost as computationally and memory efficient as EWC\n(Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we\nshow that all algorithms including A-GEM can learn even more quickly if they\nare provided with task descriptors specifying the classification tasks under\nconsideration. Our experiments on several standard lifelong learning benchmarks\ndemonstrate that A-GEM has the best trade-off between accuracy and efficiency. \n\n"}
{"id": "1812.00564", "contents": "Title: Split learning for health: Distributed deep learning without sharing raw\n  patient data Abstract: Can health entities collaboratively train deep learning models without\nsharing sensitive raw data? This paper proposes several configurations of a\ndistributed deep learning method called SplitNN to facilitate such\ncollaborations. SplitNN does not share raw data or model details with\ncollaborating institutions. The proposed configurations of splitNN cater to\npractical settings of i) entities holding different modalities of patient data,\nii) centralized and local health entities collaborating on multiple tasks and\niii) learning without sharing labels. We compare performance and resource\nefficiency trade-offs of splitNN and other distributed deep learning methods\nlike federated learning, large batch synchronous stochastic gradient descent\nand show highly encouraging results for splitNN. \n\n"}
{"id": "1812.00856", "contents": "Title: Thompson Sampling for Noncompliant Bandits Abstract: Thompson sampling, a Bayesian method for balancing exploration and\nexploitation in bandit problems, has theoretical guarantees and exhibits strong\nempirical performance in many domains. Traditional Thompson sampling, however,\nassumes perfect compliance, where an agent's chosen action is treated as the\nimplemented action. This article introduces a stochastic noncompliance model\nthat relaxes this assumption. We prove that any noncompliance in a 2-armed\nBernoulli bandit increases existing regret bounds. With our noncompliance\nmodel, we derive Thompson sampling variants that explicitly handle both\nobserved and latent noncompliance. With extensive empirical analysis, we\ndemonstrate that our algorithms either match or outperform traditional Thompson\nsampling in both compliant and noncompliant environments. \n\n"}
{"id": "1812.00877", "contents": "Title: Automatic lesion boundary detection in dermoscopy Abstract: This manuscript addresses the problem of the automatic lesion boundary\ndetection in dermoscopy, using deep neural networks. An approach is based on\nthe adaptation of the U-net convolutional neural network with skip connections\nfor lesion boundary segmentation task. I hope this paper could serve, to some\nextent, as an experiment of using deep convolutional networks in biomedical\nsegmentation task and as a guideline of the boundary detection benchmark,\ninspiring further attempts and researches. \n\n"}
{"id": "1812.00883", "contents": "Title: Relation Networks for Optic Disc and Fovea Localization in Retinal\n  Images Abstract: Diabetic Retinopathy is the leading cause of blindness in the world. At least\n90\\% of new cases can be reduced with proper treatment and monitoring of the\neyes. However, scanning the entire population of patients is a difficult\nendeavor. Computer-aided diagnosis tools in retinal image analysis can make the\nprocess scalable and efficient. In this work, we focus on the problem of\nlocalizing the centers of the Optic disc and Fovea, a task crucial to the\nanalysis of retinal scans. Current systems recognize the Optic disc and Fovea\nindividually, without exploiting their relations during learning. We propose a\nnovel approach to localizing the centers of the Optic disc and Fovea by\nsimultaneously processing them and modeling their relative geometry and\nappearance. We show that our approach improves localization and recognition by\nincorporating object-object relations efficiently, and achieves highly\ncompetitive results. \n\n"}
{"id": "1812.00979", "contents": "Title: Deep Reinforcement Learning for Intelligent Transportation Systems Abstract: Intelligent Transportation Systems (ITSs) are envisioned to play a critical\nrole in improving traffic flow and reducing congestion, which is a pervasive\nissue impacting urban areas around the globe. Rapidly advancing vehicular\ncommunication and edge cloud computation technologies provide key enablers for\nsmart traffic management. However, operating viable real-time actuation\nmechanisms on a practically relevant scale involves formidable challenges,\ne.g., policy iteration and conventional Reinforcement Learning (RL) techniques\nsuffer from poor scalability due to state space explosion. Motivated by these\nissues, we explore the potential for Deep Q-Networks (DQN) to optimize traffic\nlight control policies. As an initial benchmark, we establish that the DQN\nalgorithms yield the \"thresholding\" policy in a single-intersection. Next, we\nexamine the scalability properties of DQN algorithms and their performance in a\nlinear network topology with several intersections along a main artery. We\ndemonstrate that DQN algorithms produce intelligent behavior, such as the\nemergence of \"greenwave\" patterns, reflecting their ability to learn favorable\ntraffic light actuations. \n\n"}
{"id": "1812.01106", "contents": "Title: Improving Traffic Safety Through Video Analysis in Jakarta, Indonesia Abstract: This project presents the results of a partnership between the Data Science\nfor Social Good fellowship, Jakarta Smart City and Pulse Lab Jakarta to create\na video analysis pipeline for the purpose of improving traffic safety in\nJakarta. The pipeline transforms raw traffic video footage into databases that\nare ready to be used for traffic analysis. By analyzing these patterns, the\ncity of Jakarta will better understand how human behavior and built\ninfrastructure contribute to traffic challenges and safety risks. The results\nof this work should also be broadly applicable to smart city initiatives around\nthe globe as they improve urban planning and sustainability through data\nscience approaches. \n\n"}
{"id": "1812.01114", "contents": "Title: Exploring galaxy evolution with generative models Abstract: Context. Generative models open up the possibility to interrogate scientific\ndata in a more data-driven way. Aims: We propose a method that uses generative\nmodels to explore hypotheses in astrophysics and other areas. We use a neural\nnetwork to show how we can independently manipulate physical attributes by\nencoding objects in latent space. Methods: By learning a latent space\nrepresentation of the data, we can use this network to forward model and\nexplore hypotheses in a data-driven way. We train a neural network to generate\nartificial data to test hypotheses for the underlying physical processes.\nResults: We demonstrate this process using a well-studied process in\nastrophysics, the quenching of star formation in galaxies as they move from\nlow-to high-density environments. This approach can help explore astrophysical\nand other phenomena in a way that is different from current methods based on\nsimulations and observations. \n\n"}
{"id": "1812.01186", "contents": "Title: FRAME Revisited: An Interpretation View Based on Particle Evolution Abstract: FRAME (Filters, Random fields, And Maximum Entropy) is an energy-based\ndescriptive model that synthesizes visual realism by capturing mutual patterns\nfrom structural input signals. The maximum likelihood estimation (MLE) is\napplied by default, yet conventionally causes the unstable training energy that\nwrecks the generated structures, which remains unexplained. In this paper, we\nprovide a new theoretical insight to analyze FRAME, from a perspective of\nparticle physics ascribing the weird phenomenon to KL-vanishing issue. In order\nto stabilize the energy dissipation, we propose an alternative Wasserstein\ndistance in discrete time based on the conclusion that the\nJordan-Kinderlehrer-Otto (JKO) discrete flow approximates KL discrete flow when\nthe time step size tends to 0. Besides, this metric can still maintain the\nmodel's statistical consistency. Quantitative and qualitative experiments have\nbeen respectively conducted on several widely used datasets. The empirical\nstudies have evidenced the effectiveness and superiority of our method. \n\n"}
{"id": "1812.01478", "contents": "Title: Matrix Factorization via Deep Learning Abstract: Matrix completion is one of the key problems in signal processing and machine\nlearning. In recent years, deep-learning-based models have achieved\nstate-of-the-art results in matrix completion. Nevertheless, they suffer from\ntwo drawbacks: (i) they can not be extended easily to rows or columns unseen\nduring training; and (ii) their results are often degraded in case discrete\npredictions are required. This paper addresses these two drawbacks by\npresenting a deep matrix factorization model and a generic method to allow\njoint training of the factorization model and the discretization operator.\nExperiments on a real movie rating dataset show the efficacy of the proposed\nmodels. \n\n"}
{"id": "1812.01677", "contents": "Title: A Pixel-Based Framework for Data-Driven Clothing Abstract: With the aim of creating virtual cloth deformations more similar to real\nworld clothing, we propose a new computational framework that recasts three\ndimensional cloth deformation as an RGB image in a two dimensional pattern\nspace. Then a three dimensional animation of cloth is equivalent to a sequence\nof two dimensional RGB images, which in turn are driven/choreographed via\nanimation parameters such as joint angles. This allows us to leverage popular\nCNNs to learn cloth deformations in image space. The two dimensional cloth\npixels are extended into the real world via standard body skinning techniques,\nafter which the RGB values are interpreted as texture offsets and displacement\nmaps. Notably, we illustrate that our approach does not require accurate\nunclothed body shapes or robust skinning techniques. Additionally, we discuss\nhow standard image based techniques such as image partitioning for higher\nresolution, GANs for merging partitioned image regions back together, etc., can\nreadily be incorporated into our framework. \n\n"}
{"id": "1812.01690", "contents": "Title: General-to-Detailed GAN for Infrequent Class Medical Images Abstract: Deep learning has significant potential for medical imaging. However, since\nthe incident rate of each disease varies widely, the frequency of classes in a\nmedical image dataset is imbalanced, leading to poor accuracy for such\ninfrequent classes. One possible solution is data augmentation of infrequent\nclasses using synthesized images created by Generative Adversarial Networks\n(GANs), but conventional GANs also require certain amount of images to learn.\nTo overcome this limitation, here we propose General-to-detailed GAN (GDGAN),\nserially connected two GANs, one for general labels and the other for detailed\nlabels. GDGAN produced diverse medical images, and the network trained with an\naugmented dataset outperformed other networks using existing methods with\nrespect to Area-Under-Curve (AUC) of Receiver Operating Characteristic (ROC)\ncurve. \n\n"}
{"id": "1812.01699", "contents": "Title: Assigning a Grade: Accurate Measurement of Road Quality Using Satellite\n  Imagery Abstract: Roads are critically important infrastructure to societal and economic\ndevelopment, with huge investments made by governments every year. However,\nmethods for monitoring those investments tend to be time-consuming, laborious,\nand expensive, placing them out of reach for many developing regions. In this\nwork, we develop a model for monitoring the quality of road infrastructure\nusing satellite imagery. For this task, we harness two trends: the increasing\navailability of high-resolution, often-updated satellite imagery, and the\nenormous improvement in speed and accuracy of convolutional neural\nnetwork-based methods for performing computer vision tasks. We employ a unique\ndataset of road quality information on 7000km of roads in Kenya combined with\n50cm resolution satellite imagery. We create models for a binary classification\ntask as well as a comprehensive 5-category classification task, with accuracy\nscores of 88 and 73 percent respectively. We also provide evidence of the\nrobustness of our methods with challenging held-out scenarios, though we note\nsome improvement is still required for confident analysis of a never before\nseen road. We believe these results are well-positioned to have substantial\nimpact on a broad set of transport applications. \n\n"}
{"id": "1812.01719", "contents": "Title: Knowing what you know in brain segmentation using Bayesian deep neural\n  networks Abstract: In this paper, we describe a Bayesian deep neural network (DNN) for\npredicting FreeSurfer segmentations of structural MRI volumes, in minutes\nrather than hours. The network was trained and evaluated on a large dataset (n\n= 11,480), obtained by combining data from more than a hundred different sites,\nand also evaluated on another completely held-out dataset (n = 418). The\nnetwork was trained using a novel spike-and-slab dropout-based variational\ninference approach. We show that, on these datasets, the proposed Bayesian DNN\noutperforms previously proposed methods, in terms of the similarity between the\nsegmentation predictions and the FreeSurfer labels, and the usefulness of the\nestimate uncertainty of these predictions. In particular, we demonstrated that\nthe prediction uncertainty of this network at each voxel is a good indicator of\nwhether the network has made an error and that the uncertainty across the whole\nbrain can predict the manual quality control ratings of a scan. The proposed\nBayesian DNN method should be applicable to any new network architecture for\naddressing the segmentation problem. \n\n"}
{"id": "1812.02207", "contents": "Title: Better Trees: An empirical study on hyperparameter tuning of\n  classification decision tree induction algorithms Abstract: Machine learning algorithms often contain many hyperparameters (HPs) whose\nvalues affect the predictive performance of the induced models in intricate\nways. Due to the high number of possibilities for these HP configurations and\ntheir complex interactions, it is common to use optimization techniques to find\nsettings that lead to high predictive performance. However, insights into\nefficiently exploring this vast space of configurations and dealing with the\ntrade-off between predictive and runtime performance remain challenging.\nFurthermore, there are cases where the default HPs fit the suitable\nconfiguration. Additionally, for many reasons, including model validation and\nattendance to new legislation, there is an increasing interest in interpretable\nmodels, such as those created by the Decision Tree (DT) induction algorithms.\nThis paper provides a comprehensive approach for investigating the effects of\nhyperparameter tuning for the two DT induction algorithms most often used, CART\nand C4.5. DT induction algorithms present high predictive performance and\ninterpretable classification models, though many HPs need to be adjusted.\nExperiments were carried out with different tuning strategies to induce models\nand to evaluate HPs' relevance using 94 classification datasets from OpenML.\nThe experimental results point out that different HP profiles for the tuning of\neach algorithm provide statistically significant improvements in most of the\ndatasets for CART, but only in one-third for C4.5. Although different\nalgorithms may present different tuning scenarios, the tuning techniques\ngenerally required few evaluations to find accurate solutions. Furthermore, the\nbest technique for all the algorithms was the IRACE. Finally, we found out that\ntuning a specific small subset of HPs is a good alternative for achieving\noptimal predictive performance. \n\n"}
{"id": "1812.02256", "contents": "Title: Relative Entropy Regularized Policy Iteration Abstract: We present an off-policy actor-critic algorithm for Reinforcement Learning\n(RL) that combines ideas from gradient-free optimization via stochastic search\nwith learned action-value function. The result is a simple procedure consisting\nof three steps: i) policy evaluation by estimating a parametric action-value\nfunction; ii) policy improvement via the estimation of a local non-parametric\npolicy; and iii) generalization by fitting a parametric policy. Each step can\nbe implemented in different ways, giving rise to several algorithm variants.\nOur algorithm draws on connections to existing literature on black-box\noptimization and 'RL as an inference' and it can be seen either as an extension\nof the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et\nal., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation\nEvolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997]\nto a policy iteration scheme. Our comparison on 31 continuous control tasks\nfrom parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al.,\n2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited\namount of compute and a single set of hyperparameters, demonstrate the\neffectiveness of our method and the state of art results. Videos, summarizing\nresults, can be found at goo.gl/HtvJKR . \n\n"}
{"id": "1812.02289", "contents": "Title: Learning Dynamic Embeddings from Temporal Interactions Abstract: Modeling a sequence of interactions between users and items (e.g., products,\nposts, or courses) is crucial in domains such as e-commerce, social networking,\nand education to predict future interactions. Representation learning presents\nan attractive solution to model the dynamic evolution of user and item\nproperties, where each user/item can be embedded in a euclidean space and its\nevolution can be modeled by dynamic changes in embedding. However, existing\nembedding methods either generate static embeddings, treat users and items\nindependently, or are not scalable.\n  Here we present JODIE, a coupled recurrent model to jointly learn the dynamic\nembeddings of users and items from a sequence of user-item interactions. JODIE\nhas three components. First, the update component updates the user and item\nembedding from each interaction using their previous embeddings with the two\nmutually-recursive Recurrent Neural Networks. Second, a novel projection\ncomponent is trained to forecast the embedding of users at any future time.\nFinally, the prediction component directly predicts the embedding of the item\nin a future interaction. For models that learn from a sequence of interactions,\ntraditional training data batching cannot be done due to complex user-user\ndependencies. Therefore, we present a novel batching algorithm called t-Batch\nthat generates time-consistent batches of training data that can run in\nparallel, giving massive speed-up.\n  We conduct six experiments on two prediction tasks---future interaction\nprediction and state change prediction---using four real-world datasets. We\nshow that JODIE outperforms six state-of-the-art algorithms in these tasks by\nup to 22.4%. Moreover, we show that JODIE is highly scalable and up to 9.2x\nfaster than comparable models. As an additional experiment, we illustrate that\nJODIE can predict student drop-out from courses five interactions in advance. \n\n"}
{"id": "1812.02356", "contents": "Title: dynnode2vec: Scalable Dynamic Network Embedding Abstract: Network representation learning in low dimensional vector space has attracted\nconsiderable attention in both academic and industrial domains. Most real-world\nnetworks are dynamic with addition/deletion of nodes and edges. The existing\ngraph embedding methods are designed for static networks and they cannot\ncapture evolving patterns in a large dynamic network. In this paper, we propose\na dynamic embedding method, dynnode2vec, based on the well-known graph\nembedding method node2vec. Node2vec is a random walk based embedding method for\nstatic networks. Applying static network embedding in dynamic settings has two\ncrucial problems: 1) Generating random walks for every time step is time\nconsuming 2) Embedding vector spaces in each timestamp are different. In order\nto tackle these challenges, dynnode2vec uses evolving random walks and\ninitializes the current graph embedding with previous embedding vectors. We\ndemonstrate the advantages of the proposed dynamic network embedding by\nconducting empirical evaluations on several large dynamic network datasets. \n\n"}
{"id": "1812.02975", "contents": "Title: ShuffleNASNets: Efficient CNN models through modified Efficient Neural\n  Architecture Search Abstract: Neural network architectures found by sophistic search algorithms achieve\nstrikingly good test performance, surpassing most human-crafted network models\nby significant margins. Although computationally efficient, their design is\noften very complex, impairing execution speed. Additionally, finding models\noutside of the search space is not possible by design. While our space is still\nlimited, we implement undiscoverable expert knowledge into the economic search\nalgorithm Efficient Neural Architecture Search (ENAS), guided by the design\nprinciples and architecture of ShuffleNet V2. While maintaining baseline-like\n2.85% test error on CIFAR-10, our ShuffleNASNets are significantly less\ncomplex, require fewer parameters, and are two times faster than the ENAS\nbaseline in a classification task. These models also scale well to a low\nparameter space, achieving less than 5% test error with little regularization\nand only 236K parameters. \n\n"}
{"id": "1812.03170", "contents": "Title: Variational Saccading: Efficient Inference for Large Resolution Images Abstract: Image classification with deep neural networks is typically restricted to\nimages of small dimensionality such as 224 x 244 in Resnet models [24]. This\nlimitation excludes the 4000 x 3000 dimensional images that are taken by modern\nsmartphone cameras and smart devices. In this work, we aim to mitigate the\nprohibitive inferential and memory costs of operating in such large dimensional\nspaces. To sample from the high-resolution original input distribution, we\npropose using a smaller proxy distribution to learn the co-ordinates that\ncorrespond to regions of interest in the high-dimensional space. We introduce a\nnew principled variational lower bound that captures the relationship of the\nproxy distribution's posterior and the original image's co-ordinate space in a\nway that maximizes the conditional classification likelihood. We empirically\ndemonstrate on one synthetic benchmark and one real world large resolution DSLR\ncamera image dataset that our method produces comparable results with ~10x\nfaster inference and lower memory consumption than a model that utilizes the\nentire original input distribution. Finally, we experiment with a more complex\nsetting using mini-maps from Starcraft II [56] to infer the number of\ncharacters in a complex 3d-rendered scene. Even in such complicated scenes our\nmodel provides strong localization: a feature missing from traditional\nclassification models. \n\n"}
{"id": "1812.03235", "contents": "Title: Improved Knowledge Graph Embedding using Background Taxonomic\n  Information Abstract: Knowledge graphs are used to represent relational information in terms of\ntriples. To enable learning about domains, embedding models, such as tensor\nfactorization models, can be used to make predictions of new triples. Often\nthere is background taxonomic information (in terms of subclasses and\nsubproperties) that should also be taken into account. We show that existing\nfully expressive (a.k.a. universal) models cannot provably respect subclass and\nsubproperty information. We show that minimal modifications to an existing\nknowledge graph completion method enables injection of taxonomic information.\nMoreover, we prove that our model is fully expressive, assuming a lower-bound\non the size of the embeddings. Experimental results on public knowledge graphs\nshow that despite its simplicity our approach is surprisingly effective. \n\n"}
{"id": "1812.03337", "contents": "Title: Secure Federated Transfer Learning Abstract: Machine learning relies on the availability of a vast amount of data for\ntraining. However, in reality, most data are scattered across different\norganizations and cannot be easily integrated under many legal and practical\nconstraints. In this paper, we introduce a new technique and framework, known\nas federated transfer learning (FTL), to improve statistical models under a\ndata federation. The federation allows knowledge to be shared without\ncompromising user privacy, and enables complimentary knowledge to be\ntransferred in the network. As a result, a target-domain party can build more\nflexible and powerful models by leveraging rich labels from a source-domain\nparty. A secure transfer cross validation approach is also proposed to guard\nthe FTL performance under the federation. The framework requires minimal\nmodifications to the existing model structure and provides the same level of\naccuracy as the non-privacy-preserving approach. This framework is very\nflexible and can be effectively adapted to various secure multi-party machine\nlearning tasks. \n\n"}
{"id": "1812.03350", "contents": "Title: Adaptive and Calibrated Ensemble Learning with Dependent Tail-free\n  Process Abstract: Ensemble learning is a mainstay in modern data science practice. Conventional\nensemble algorithms assigns to base models a set of deterministic, constant\nmodel weights that (1) do not fully account for variations in base model\naccuracy across subgroups, nor (2) provide uncertainty estimates for the\nensemble prediction, which could result in mis-calibrated (i.e. precise but\nbiased) predictions that could in turn negatively impact the algorithm\nperformance in real-word applications. In this work, we present an adaptive,\nprobabilistic approach to ensemble learning using dependent tail-free process\nas ensemble weight prior. Given input feature $\\mathbf{x}$, our method\noptimally combines base models based on their predictive accuracy in the\nfeature space $\\mathbf{x} \\in \\mathcal{X}$, and provides interpretable\nuncertainty estimates both in model selection and in ensemble prediction. To\nencourage scalable and calibrated inference, we derive a structured variational\ninference algorithm that jointly minimize KL objective and the model's\ncalibration score (i.e. Continuous Ranked Probability Score (CRPS)). We\nillustrate the utility of our method on both a synthetic nonlinear function\nregression task, and on the real-world application of spatio-temporal\nintegration of particle pollution prediction models in New England. \n\n"}
{"id": "1812.03425", "contents": "Title: Zero Initialization of modified Gated Recurrent Encoder-Decoder Network\n  for Short Term Load Forecasting Abstract: Single layer Feedforward Neural Network(FNN) is used many a time as a last\nlayer in models such as seq2seq or could be a simple RNN network. The\nimportance of such layer is to transform the output to our required dimensions.\nWhen it comes to weights and biases initialization, there is no such specific\ntechnique that could speed up the learning process. We could depend on deep\nnetwork initialization techniques such as Xavier or He initialization. But such\ninitialization fails to show much improvement in learning speed or accuracy. In\nthis paper we propose Zero Initialization (ZI) for weights of a single layer\nnetwork. We first test this technique with on a simple RNN network and compare\nthe results against Xavier, He and Identity initialization. As a final test we\nimplement it on a seq2seq network. It was found that ZI considerably reduces\nthe number of epochs used and improve the accuracy. The developed model has\nbeen applied for short-term load forecasting using the load data of Australian\nEnergy Market. The model is able to forecast the day ahead load accurately with\nerror of 0.94%. \n\n"}
{"id": "1812.03596", "contents": "Title: Task-Free Continual Learning Abstract: Methods proposed in the literature towards continual deep learning typically\noperate in a task-based sequential learning setup. A sequence of tasks is\nlearned, one at a time, with all data of current task available but not of\nprevious or future tasks. Task boundaries and identities are known at all\ntimes. This setup, however, is rarely encountered in practical applications.\nTherefore we investigate how to transform continual learning to an online\nsetup. We develop a system that keeps on learning over time in a streaming\nfashion, with data distributions gradually changing and without the notion of\nseparate tasks. To this end, we build on the work on Memory Aware Synapses, and\nshow how this method can be made online by providing a protocol to decide i)\nwhen to update the importance weights, ii) which data to use to update them,\nand iii) how to accumulate the importance weights at each update step.\nExperimental results show the validity of the approach in the context of two\napplications: (self-)supervised learning of a face recognition model by\nwatching soap series and learning a robot to avoid collisions. \n\n"}
{"id": "1812.03889", "contents": "Title: Regularization by architecture: A deep prior approach for inverse\n  problems Abstract: The present paper studies so-called deep image prior (DIP) techniques in the\ncontext of ill-posed inverse problems. DIP networks have been recently\nintroduced for applications in image processing; also first experimental\nresults for applying DIP to inverse problems have been reported. This paper\naims at discussing different interpretations of DIP and to obtain analytic\nresults for specific network designs and linear operators. The main\ncontribution is to introduce the idea of viewing these approaches as the\noptimization of Tikhonov functionals rather than optimizing networks. Besides\ntheoretical results, we present numerical verifications. \n\n"}
{"id": "1812.04069", "contents": "Title: Individual Fairness in Hindsight Abstract: Since many critical decisions impacting human lives are increasingly being\nmade by algorithms, it is important to ensure that the treatment of individuals\nunder such algorithms is demonstrably fair under reasonable notions of\nfairness. One compelling notion proposed in the literature is that of\nindividual fairness (IF), which advocates that similar individuals should be\ntreated similarly (Dwork et al. 2012). Originally proposed for offline\ndecisions, this notion does not, however, account for temporal considerations\nrelevant for online decision-making. In this paper, we extend the notion of IF\nto account for the time at which a decision is made, in settings where there\nexists a notion of conduciveness of decisions as perceived by the affected\nindividuals. We introduce two definitions: (i) fairness-across-time (FT) and\n(ii) fairness-in-hindsight (FH). FT is the simplest temporal extension of IF\nwhere treatment of individuals is required to be individually fair relative to\nthe past as well as future, while in FH, we require a one-sided notion of\nindividual fairness that is defined relative to only the past decisions. We\nshow that these two definitions can have drastically different implications in\nthe setting where the principal needs to learn the utility model. Linear regret\nrelative to optimal individually fair decisions is inevitable under FT for\nnon-trivial examples. On the other hand, we design a new algorithm: Cautious\nFair Exploration (CaFE), which satisfies FH and achieves sub-linear regret\nguarantees for a broad range of settings. We characterize lower bounds showing\nthat these guarantees are order-optimal in the worst case. FH can thus be\nembedded as a primary safeguard against unfair discrimination in algorithmic\ndeployments, without hindering the ability to take good decisions in the\nlong-run. \n\n"}
{"id": "1812.04202", "contents": "Title: Deep Learning on Graphs: A Survey Abstract: Deep learning has been shown to be successful in a number of domains, ranging\nfrom acoustics, images, to natural language processing. However, applying deep\nlearning to the ubiquitous graph data is non-trivial because of the unique\ncharacteristics of graphs. Recently, substantial research efforts have been\ndevoted to applying deep learning methods to graphs, resulting in beneficial\nadvances in graph analysis techniques. In this survey, we comprehensively\nreview the different types of deep learning methods on graphs. We divide the\nexisting methods into five categories based on their model architectures and\ntraining strategies: graph recurrent neural networks, graph convolutional\nnetworks, graph autoencoders, graph reinforcement learning, and graph\nadversarial methods. We then provide a comprehensive overview of these methods\nin a systematic manner mainly by following their development history. We also\nanalyze the differences and compositions of different methods. Finally, we\nbriefly outline the applications in which they have been used and discuss\npotential future research directions. \n\n"}
{"id": "1812.04456", "contents": "Title: Semi-supervised dual graph regularized dictionary learning Abstract: In this paper, we propose a semi-supervised dictionary learning method that\nuses both the information in labelled and unlabelled data and jointly trains a\nlinear classifier embedded on the sparse codes. The manifold structure of the\ndata in the sparse code space is preserved using the same approach as the\nLocally Linear Embedding method (LLE). This enables one to enforce the\npredictive power of the unlabelled data sparse codes. We show that our approach\nprovides significant improvements over other methods. The results can be\nfurther improved by training a simple nonlinear classifier as SVM on the sparse\ncodes. \n\n"}
{"id": "1812.04616", "contents": "Title: Von Mises-Fisher Loss for Training Sequence to Sequence Models with\n  Continuous Outputs Abstract: The Softmax function is used in the final layer of nearly all existing\nsequence-to-sequence models for language generation. However, it is usually the\nslowest layer to compute which limits the vocabulary size to a subset of most\nfrequent types; and it has a large memory footprint. We propose a general\ntechnique for replacing the softmax layer with a continuous embedding layer.\nOur primary innovations are a novel probabilistic loss, and a training and\ninference procedure in which we generate a probability distribution over\npre-trained word embeddings, instead of a multinomial distribution over the\nvocabulary obtained via softmax. We evaluate this new class of\nsequence-to-sequence models with continuous outputs on the task of neural\nmachine translation. We show that our models obtain upto 2.5x speed-up in\ntraining time while performing on par with the state-of-the-art models in terms\nof translation quality. These models are capable of handling very large\nvocabularies without compromising on translation quality. They also produce\nmore meaningful errors than in the softmax-based models, as these errors\ntypically lie in a subspace of the vector space of the reference translations. \n\n"}
{"id": "1812.04690", "contents": "Title: Learning representations of molecules and materials with atomistic\n  neural networks Abstract: Deep Learning has been shown to learn efficient representations for\nstructured data such as image, text or audio. In this chapter, we present\nneural network architectures that are able to learn efficient representations\nof molecules and materials. In particular, the continuous-filter convolutional\nnetwork SchNet accurately predicts chemical properties across compositional and\nconfigurational space on a variety of datasets. Beyond that, we analyze the\nobtained representations to find evidence that their spatial and chemical\nproperties agree with chemical intuition. \n\n"}
{"id": "1812.04722", "contents": "Title: Context is Key: New Approaches to Neural Coherence Modeling Abstract: We formulate coherence modeling as a regression task and propose two novel\nmethods to combine techniques from our setup with pairwise approaches. The\nfirst of our methods is a model that we call \"first-next,\" which operates\nsimilarly to selection sorting but conditions decision-making on information\nabout already-sorted sentences. The second consists of a technique for adding\ncontext to regression-based models by concatenating sentence-level\nrepresentations with an encoding of its corresponding out-of-order paragraph.\nThis latter model achieves Kendall-tau distance and positional accuracy scores\nthat match or exceed the current state-of-the-art on these metrics. Our results\nsuggest that many of the gains that come from more complex, machine-translation\ninspired approaches can be achieved with simpler, more efficient models. \n\n"}
{"id": "1812.04778", "contents": "Title: Bridging the Generalization Gap: Training Robust Models on Confounded\n  Biological Data Abstract: Statistical learning on biological data can be challenging due to confounding\nvariables in sample collection and processing. Confounders can cause models to\ngeneralize poorly and result in inaccurate prediction performance metrics if\nmodels are not validated thoroughly. In this paper, we propose methods to\ncontrol for confounding factors and further improve prediction performance. We\nintroduce OrthoNormal basis construction In cOnfounding factor Normalization\n(ONION) to remove confounding covariates and use the Domain-Adversarial Neural\nNetwork (DANN) to penalize models for encoding confounder information. We apply\nthe proposed methods to simulated and empirical patient data and show\nsignificant improvements in generalization. \n\n"}
{"id": "1812.04912", "contents": "Title: EasiCSDeep: A deep learning model for Cervical Spondylosis\n  Identification using surface electromyography signal Abstract: Cervical spondylosis (CS) is a common chronic disease that affects up to\ntwo-thirds of the population and poses a serious burden on individuals and\nsociety. The early identification has significant value in improving cure rate\nand reducing costs. However, the pathology is complex, and the mild symptoms\nincrease the difficulty of the diagnosis, especially in the early stage.\nBesides, the time-consuming and costliness of hospital medical service reduces\nthe attention to the CS identification. Thus, a convenient, low-cost\nintelligent CS identification method is imperious demanded. In this paper, we\npresent an intelligent method based on the deep learning to identify CS, using\nthe surface electromyography (sEMG) signal. Faced with the complex, high\ndimensionality and weak usability of the sEMG signal, we proposed and developed\na multi-channel EasiCSDeep algorithm based on the convolutional neural network,\nwhich consists of the feature extraction, spatial relationship representation\nand classification algorithm. To the best of our knowledge, this EasiCSDeep is\nthe first effort to employ the deep learning and the sEMG data to identify CS.\nCompared with previous state-of-the-art algorithm, our algorithm achieves a\nsignificant improvement. \n\n"}
{"id": "1812.05451", "contents": "Title: A Probabilistic Model of the Bitcoin Blockchain Abstract: The Bitcoin transaction graph is a public data structure organized as\ntransactions between addresses, each associated with a logical entity. In this\nwork, we introduce a complete probabilistic model of the Bitcoin Blockchain. We\nfirst formulate a set of conditional dependencies induced by the Bitcoin\nprotocol at the block level and derive a corresponding fully observed graphical\nmodel of a Bitcoin block. We then extend the model to include hidden entity\nattributes such as the functional category of the associated logical agent and\nderive asymptotic bounds on the privacy properties implied by this model. At\nthe network level, we show evidence of complex transaction-to-transaction\nbehavior and present a relevant discriminative model of the agent categories.\nPerformance of both the block-based graphical model and the network-level\ndiscriminative model is evaluated on a subset of the public Bitcoin Blockchain. \n\n"}
{"id": "1812.05721", "contents": "Title: Stochastic Gradient Descent for Spectral Embedding with Implicit\n  Orthogonality Constraint Abstract: In this paper, we propose a scalable algorithm for spectral embedding. The\nlatter is a standard tool for graph clustering. However, its computational\nbottleneck is the eigendecomposition of the graph Laplacian matrix, which\nprevents its application to large-scale graphs. Our contribution consists of\nreformulating spectral embedding so that it can be solved via stochastic\noptimization. The idea is to replace the orthogonality constraint with an\northogonalization matrix injected directly into the criterion. As the gradient\ncan be computed through a Cholesky factorization, our reformulation allows us\nto develop an efficient algorithm based on mini-batch gradient descent.\nExperimental results, both on synthetic and real data, confirm the efficiency\nof the proposed method in term of execution speed with respect to similar\nexisting techniques. \n\n"}
{"id": "1812.06080", "contents": "Title: Reconciling meta-learning and continual learning with online mixtures of\n  tasks Abstract: Learning-to-learn or meta-learning leverages data-driven inductive bias to\nincrease the efficiency of learning on a novel task. This approach encounters\ndifficulty when transfer is not advantageous, for instance, when tasks are\nconsiderably dissimilar or change over time. We use the connection between\ngradient-based meta-learning and hierarchical Bayes to propose a Dirichlet\nprocess mixture of hierarchical Bayesian models over the parameters of an\narbitrary parametric model such as a neural network. In contrast to\nconsolidating inductive biases into a single set of hyperparameters, our\napproach of task-dependent hyperparameter selection better handles latent\ndistribution shift, as demonstrated on a set of evolving, image-based, few-shot\nlearning benchmarks. \n\n"}
{"id": "1812.06145", "contents": "Title: Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition\n  with Multimodal Training Abstract: We present an efficient approach for leveraging the knowledge from multiple\nmodalities in training unimodal 3D convolutional neural networks (3D-CNNs) for\nthe task of dynamic hand gesture recognition. Instead of explicitly combining\nmultimodal information, which is commonplace in many state-of-the-art methods,\nwe propose a different framework in which we embed the knowledge of multiple\nmodalities in individual networks so that each unimodal network can achieve an\nimproved performance. In particular, we dedicate separate networks per\navailable modality and enforce them to collaborate and learn to develop\nnetworks with common semantics and better representations. We introduce a\n\"spatiotemporal semantic alignment\" loss (SSA) to align the content of the\nfeatures from different networks. In addition, we regularize this loss with our\nproposed \"focal regularization parameter\" to avoid negative knowledge transfer.\nExperimental results show that our framework improves the test time recognition\naccuracy of unimodal networks, and provides the state-of-the-art performance on\nvarious dynamic hand gesture recognition datasets. \n\n"}
{"id": "1812.06181", "contents": "Title: Efficient Interpretation of Deep Learning Models Using Graph Structure\n  and Cooperative Game Theory: Application to ASD Biomarker Discovery Abstract: Discovering imaging biomarkers for autism spectrum disorder (ASD) is critical\nto help explain ASD and predict or monitor treatment outcomes. Toward this end,\ndeep learning classifiers have recently been used for identifying ASD from\nfunctional magnetic resonance imaging (fMRI) with higher accuracy than\ntraditional learning strategies. However, a key challenge with deep learning\nmodels is understanding just what image features the network is using, which\ncan in turn be used to define the biomarkers. Current methods extract\nbiomarkers, i.e., important features, by looking at how the prediction changes\nif \"ignoring\" one feature at a time. In this work, we go beyond looking at only\nindividual features by using Shapley value explanation (SVE) from cooperative\ngame theory. Cooperative game theory is advantageous here because it directly\nconsiders the interaction between features and can be applied to any machine\nlearning method, making it a novel, more accurate way of determining\ninstance-wise biomarker importance from deep learning models. A barrier to\nusing SVE is its computational complexity: $2^N$ given $N$ features. We\nexplicitly reduce the complexity of SVE computation by two approaches based on\nthe underlying graph structure of the input data: 1) only consider the\ncentralized coalition of each feature; 2) a hierarchical pipeline which first\nclusters features into small communities, then applies SVE in each community.\nMonte Carlo approximation can be used for large permutation sets. We first\nvalidate our methods on the MNIST dataset and compare to human perception.\nNext, to insure plausibility of our biomarker results, we train a Random Forest\n(RF) to classify ASD/control subjects from fMRI and compare SVE results to\nstandard RF-based feature importance. Finally, we show initial results on\nranked fMRI biomarkers using SVE on a deep learning classifier for the\nASD/control dataset. \n\n"}
{"id": "1812.06227", "contents": "Title: Balanced Linear Contextual Bandits Abstract: Contextual bandit algorithms are sensitive to the estimation method of the\noutcome model as well as the exploration method used, particularly in the\npresence of rich heterogeneity or complex outcome models, which can lead to\ndifficult estimation problems along the path of learning. We develop algorithms\nfor contextual bandits with linear payoffs that integrate balancing methods\nfrom the causal inference literature in their estimation to make it less prone\nto problems of estimation bias. We provide the first regret bound analyses for\nlinear contextual bandits with balancing and show that our algorithms match the\nstate of the art theoretical guarantees. We demonstrate the strong practical\nadvantage of balanced contextual bandits on a large number of supervised\nlearning datasets and on a synthetic example that simulates model\nmisspecification and prejudice in the initial training data. \n\n"}
{"id": "1812.06303", "contents": "Title: Multi-Tasking Genetic Algorithm (MTGA) for Fuzzy System Optimization Abstract: Multi-task learning uses auxiliary data or knowledge from relevant tasks to\nfacilitate the learning in a new task. Multi-task optimization applies\nmulti-task learning to optimization to study how to effectively and efficiently\ntackle multiple optimization problems simultaneously. Evolutionary\nmulti-tasking, or multi-factorial optimization, is an emerging subfield of\nmulti-task optimization, which integrates evolutionary computation and\nmulti-task learning. This paper proposes a novel and easy-to-implement\nmulti-tasking genetic algorithm (MTGA), which copes well with significantly\ndifferent optimization tasks by estimating and using the bias among them.\nComparative studies with eight state-of-the-art single- and multi-task\napproaches in the literature on nine benchmarks demonstrated that on average\nthe MTGA outperformed all of them, and had lower computational cost than six of\nthem. Based on the MTGA, a simultaneous optimization strategy for fuzzy system\ndesign is also proposed. Experiments on simultaneous optimization of type-1 and\ninterval type-2 fuzzy logic controllers for couple-tank water level control\ndemonstrated that the MTGA can find better fuzzy logic controllers than other\napproaches. \n\n"}
{"id": "1812.06502", "contents": "Title: A Logarithmic Barrier Method For Proximal Policy Optimization Abstract: Proximal policy optimization(PPO) has been proposed as a first-order\noptimization method for reinforcement learning. We should notice that an\nexterior penalty method is used in it. Often, the minimizers of the exterior\npenalty functions approach feasibility only in the limits as the penalty\nparameter grows increasingly large. Therefore, it may result in the low level\nof sampling efficiency. This method, which we call proximal policy optimization\nwith barrier method (PPO-B), keeps almost all advantageous spheres of PPO such\nas easy implementation and good generalization. Specifically, a new surrogate\nobjective with interior penalty method is proposed to avoid the defect arose\nfrom exterior penalty method. Conclusions can be draw that PPO-B is able to\noutperform PPO in terms of sampling efficiency since PPO-B achieved clearly\nbetter performance on Atari and Mujoco environment than PPO. \n\n"}
{"id": "1812.07035", "contents": "Title: On the Continuity of Rotation Representations in Neural Networks Abstract: In neural networks, it is often desirable to work with various\nrepresentations of the same space. For example, 3D rotations can be represented\nwith quaternions or Euler angles. In this paper, we advance a definition of a\ncontinuous representation, which can be helpful for training deep neural\nnetworks. We relate this to topological concepts such as homeomorphism and\nembedding. We then investigate what are continuous and discontinuous\nrepresentations for 2D, 3D, and n-dimensional rotations. We demonstrate that\nfor 3D rotations, all representations are discontinuous in the real Euclidean\nspaces of four or fewer dimensions. Thus, widely used representations such as\nquaternions and Euler angles are discontinuous and difficult for neural\nnetworks to learn. We show that the 3D rotations have continuous\nrepresentations in 5D and 6D, which are more suitable for learning. We also\npresent continuous representations for the general case of the n-dimensional\nrotation group SO(n). While our main focus is on rotations, we also show that\nour constructions apply to other groups such as the orthogonal group and\nsimilarity transforms. We finally present empirical results, which show that\nour continuous rotation representations outperform discontinuous ones for\nseveral practical problems in graphics and vision, including a simple\nautoencoder sanity test, a rotation estimator for 3D point clouds, and an\ninverse kinematics solver for 3D human poses. \n\n"}
{"id": "1812.07051", "contents": "Title: Unsupervised Single Image Dehazing Using Dark Channel Prior Loss Abstract: Single image dehazing is a critical stage in many modern-day autonomous\nvision applications. Early prior-based methods often involved a time-consuming\nminimization of a hand-crafted energy function. Recent learning-based\napproaches utilize the representational power of deep neural networks (DNNs) to\nlearn the underlying transformation between hazy and clear images. Due to\ninherent limitations in collecting matching clear and hazy images, these\nmethods resort to training on synthetic data; constructed from indoor images\nand corresponding depth information. This may result in a possible domain shift\nwhen treating outdoor scenes. We propose a completely unsupervised method of\ntraining via minimization of the well-known, Dark Channel Prior (DCP) energy\nfunction. Instead of feeding the network with synthetic data, we solely use\nreal-world outdoor images and tune the network's parameters by directly\nminimizing the DCP. Although our \"Deep DCP\" technique can be regarded as a fast\napproximator of DCP, it actually improves its results significantly. This\nsuggests an additional regularization obtained via the network and learning\nprocess. Experiments show that our method performs on par with large-scale\nsupervised methods. \n\n"}
{"id": "1812.07103", "contents": "Title: Style Transfer and Extraction for the Handwritten Letters Using Deep\n  Learning Abstract: How can we learn, transfer and extract handwriting styles using deep neural\nnetworks? This paper explores these questions using a deep conditioned\nautoencoder on the IRON-OFF handwriting data-set. We perform three experiments\nthat systematically explore the quality of our style extraction procedure.\nFirst, We compare our model to handwriting benchmarks using multidimensional\nperformance metrics. Second, we explore the quality of style transfer, i.e. how\nthe model performs on new, unseen writers. In both experiments, we improve the\nmetrics of state of the art methods by a large margin. Lastly, we analyze the\nlatent space of our model, and we see that it separates consistently writing\nstyles. \n\n"}
{"id": "1812.07135", "contents": "Title: Globalness Detection in Online Social Network Abstract: Classification problems have made significant progress due to the maturity of\nartificial intelligence (AI). However, differentiating items from categories\nwithout noticeable boundaries is still a huge challenge for machines -- which\nis also crucial for machines to be intelligent.\n  In order to study the fuzzy concept on classification, we define and propose\na globalness detection with the four-stage operational flow. We then\ndemonstrate our framework on Facebook public pages inter-like graph with their\ngeo-location. Our prediction algorithm achieves high precision (89%) and recall\n(88%) of local pages. We evaluate the results on both states and countries\nlevel, finding that the global node ratios are relatively high in those states\n(NY, CA) having large and international cities. Several global nodes examples\nhave also been shown and studied in this paper.\n  It is our hope that our results unveil the perfect value from every\nclassification problem and provide a better understanding of global and local\nnodes in Online Social Networks (OSNs). \n\n"}
{"id": "1812.07142", "contents": "Title: Two Birds with One Network: Unifying Failure Event Prediction and\n  Time-to-failure Modeling Abstract: One of the key challenges in predictive maintenance is to predict the\nimpending downtime of an equipment with a reasonable prediction horizon so that\ncountermeasures can be put in place. Classically, this problem has been posed\nin two different ways which are typically solved independently: (1) Remaining\nuseful life (RUL) estimation as a long-term prediction task to estimate how\nmuch time is left in the useful life of the equipment and (2) Failure\nprediction (FP) as a short-term prediction task to assess the probability of a\nfailure within a pre-specified time window. As these two tasks are related,\nperforming them separately is sub-optimal and might results in inconsistent\npredictions for the same equipment. In order to alleviate these issues, we\npropose two methods: Deep Weibull model (DW-RNN) and multi-task learning\n(MTL-RNN). DW-RNN is able to learn the underlying failure dynamics by fitting\nWeibull distribution parameters using a deep neural network, learned with a\nsurvival likelihood, without training directly on each task. While DW-RNN makes\nan explicit assumption on the data distribution, MTL-RNN exploits the implicit\nrelationship between the long-term RUL and short-term FP tasks to learn the\nunderlying distribution. Additionally, both our methods can leverage the\nnon-failed equipment data for RUL estimation. We demonstrate that our methods\nconsistently outperform baseline RUL methods that can be used for FP while\nproducing consistent results for RUL and FP. We also show that our methods\nperform at par with baselines trained on the objectives optimized for either of\nthe two tasks. \n\n"}
{"id": "1812.07238", "contents": "Title: Sparsity in Variational Autoencoders Abstract: Working in high-dimensional latent spaces, the internal encoding of data in\nVariational Autoencoders becomes naturally sparse. We discuss this known but\ncontroversial phenomenon sometimes refereed to as overpruning, to emphasize the\nunder-use of the model capacity. In fact, it is an important form of\nself-regularization, with all the typical benefits associated with sparsity: it\nforces the model to focus on the really important features, highly reducing the\nrisk of overfitting. Especially, it is a major methodological guide for the\ncorrect tuning of the model capacity, progressively augmenting it to attain\nsparsity, or conversely reducing the dimension of the network removing links to\nzeroed out neurons. The degree of sparsity crucially depends on the network\narchitecture: for instance, convolutional networks typically show less\nsparsity, likely due to the tighter relation of features to different spatial\nregions of the input. \n\n"}
{"id": "1812.07484", "contents": "Title: Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor\n  Search Abstract: Approximate nearest neighbor algorithms are used to speed up nearest neighbor\nsearch in a wide array of applications. However, current indexing methods\nfeature several hyperparameters that need to be tuned to reach an acceptable\naccuracy--speed trade-off. A grid search in the parameter space is often\nimpractically slow due to a time-consuming index-building procedure. Therefore,\nwe propose an algorithm for automatically tuning the hyperparameters of\nindexing methods based on randomized space-partitioning trees. In particular,\nwe present results using randomized k-d trees, random projection trees and\nrandomized PCA trees. The tuning algorithm adds minimal overhead to the\nindex-building process but is able to find the optimal hyperparameters\naccurately. We demonstrate that the algorithm is significantly faster than\nexisting approaches, and that the indexing methods used are competitive with\nthe state-of-the-art methods in query time while being faster to build. \n\n"}
{"id": "1812.07538", "contents": "Title: XOR_p A maximally intertwined p-classes problem used as a benchmark with\n  built-in truth for neural networks gradient descent optimization Abstract: A natural p-classes generalization of the eXclusive OR problem, the\nsubtraction modulo p, where p is prime, is presented and solved using a single\nfully connected hidden layer with p-neurons. Although the problem is very\nsimple, the landscape is intricate and challenging and represents an\ninteresting benchmark for gradient descent optimization algorithms. Testing 9\noptimizers and 9 activation functions up to p = 191, the method converging most\noften and the fastest to a perfect classification is the Adam optimizer\ncombined with the ELU activation function. \n\n"}
{"id": "1812.08839", "contents": "Title: A General Approach to Domain Adaptation with Applications in Astronomy Abstract: The ability to build a model on a source task and subsequently adapt such\nmodel on a new target task is a pervasive need in many astronomical\napplications. The problem is generally known as transfer learning in machine\nlearning, where domain adaptation is a popular scenario. An example is to build\na predictive model on spectroscopic data to identify Supernovae IA, while\nsubsequently trying to adapt such model on photometric data. In this paper we\npropose a new general approach to domain adaptation that does not rely on the\nproximity of source and target distributions. Instead we simply assume a strong\nsimilarity in model complexity across domains, and use active learning to\nmitigate the dependency on source examples. Our work leads to a new formulation\nfor the likelihood as a function of empirical error using a theoretical\nlearning bound; the result is a novel mapping from generalization error to a\nlikelihood estimation. Results using two real astronomical problems, Supernova\nIa classification and identification of Mars landforms, show two main\nadvantages with our approach: increased accuracy performance and substantial\nsavings in computational cost. \n\n"}
{"id": "1812.08997", "contents": "Title: Stochastic Doubly Robust Gradient Abstract: When training a machine learning model with observational data, it is often\nencountered that some values are systemically missing. Learning from the\nincomplete data in which the missingness depends on some covariates may lead to\nbiased estimation of parameters and even harm the fairness of decision outcome.\nThis paper proposes how to adjust the causal effect of covariates on the\nmissingness when training models using stochastic gradient descent (SGD).\nInspired by the design of doubly robust estimator and its theoretical property\nof double robustness, we introduce stochastic doubly robust gradient (SDRG)\nconsisting of two models: weight-corrected gradients for inverse propensity\nscore weighting and per-covariate control variates for regression adjustment.\nAlso, we identify the connection between double robustness and variance\nreduction in SGD by demonstrating the SDRG algorithm with a unifying framework\nfor variance reduced SGD. The performance of our approach is empirically tested\nby showing the convergence in training image classifiers with several examples\nof missing data. \n\n"}
{"id": "1812.09430", "contents": "Title: Dynamic Graph Representation Learning via Self-Attention Networks Abstract: Learning latent representations of nodes in graphs is an important and\nubiquitous task with widespread applications such as link prediction, node\nclassification, and graph visualization. Previous methods on graph\nrepresentation learning mainly focus on static graphs, however, many real-world\ngraphs are dynamic and evolve over time. In this paper, we present Dynamic\nSelf-Attention Network (DySAT), a novel neural architecture that operates on\ndynamic graphs and learns node representations that capture both structural\nproperties and temporal evolutionary patterns. Specifically, DySAT computes\nnode representations by jointly employing self-attention layers along two\ndimensions: structural neighborhood and temporal dynamics. We conduct link\nprediction experiments on two classes of graphs: communication networks and\nbipartite rating networks. Our experimental results show that DySAT has a\nsignificant performance gain over several different state-of-the-art graph\nembedding baselines. \n\n"}
{"id": "1812.09645", "contents": "Title: Mixed Membership Recurrent Neural Networks Abstract: Models for sequential data such as the recurrent neural network (RNN) often\nimplicitly model a sequence as having a fixed time interval between\nobservations and do not account for group-level effects when multiple sequences\nare observed. We propose a model for grouped sequential data based on the RNN\nthat accounts for varying time intervals between observations in a sequence by\nlearning a group-level base parameter to which each sequence can revert. Our\napproach is motivated by the mixed membership framework, and we show how it can\nbe used for dynamic topic modeling in which the distribution on topics (not the\ntopics themselves) are evolving in time. We demonstrate our approach on a\ndataset of 3.4 million online grocery shopping orders made by 206K customers. \n\n"}
{"id": "1812.10730", "contents": "Title: Neuromemrisitive Architecture of HTM with On-Device Learning and\n  Neurogenesis Abstract: Hierarchical temporal memory (HTM) is a biomimetic sequence memory algorithm\nthat holds promise for invariant representations of spatial and spatiotemporal\ninputs. This paper presents a comprehensive neuromemristive crossbar\narchitecture for the spatial pooler (SP) and the sparse distributed\nrepresentation classifier, which are fundamental to the algorithm. There are\nseveral unique features in the proposed architecture that tightly link with the\nHTM algorithm. A memristor that is suitable for emulating the HTM synapses is\nidentified and a new Z-window function is proposed. The architecture exploits\nthe concept of synthetic synapses to enable potential synapses in the HTM. The\ncrossbar for the SP avoids dark spots caused by unutilized crossbar regions and\nsupports rapid on-chip training within 2 clock cycles. This research also\nleverages plasticity mechanisms such as neurogenesis and homeostatic intrinsic\nplasticity to strengthen the robustness and performance of the SP. The proposed\ndesign is benchmarked for image recognition tasks using MNIST and Yale faces\ndatasets, and is evaluated using different metrics including entropy,\nsparseness, and noise robustness. Detailed power analysis at different stages\nof the SP operations is performed to demonstrate the suitability for mobile\nplatforms. \n\n"}
{"id": "1812.10869", "contents": "Title: Hypergraph Clustering: A Modularity Maximization Approach Abstract: Clustering on hypergraphs has been garnering increased attention with\npotential applications in network analysis, VLSI design and computer vision,\namong others. In this work, we generalize the framework of modularity\nmaximization for clustering on hypergraphs. To this end, we introduce a\nhypergraph null model, analogous to the configuration model on undirected\ngraphs, and a node-degree preserving reduction to work with this model. This is\nused to define a modularity function that can be maximized using the popular\nand fast Louvain algorithm. We additionally propose a refinement over this\nclustering, by reweighting cut hyperedges in an iterative fashion. The efficacy\nand efficiency of our methods are demonstrated on several real-world datasets. \n\n"}
{"id": "1812.10924", "contents": "Title: Improving the Interpretability of Deep Neural Networks with Knowledge\n  Distillation Abstract: Deep Neural Networks have achieved huge success at a wide spectrum of\napplications from language modeling, computer vision to speech recognition.\nHowever, nowadays, good performance alone is not sufficient to satisfy the\nneeds of practical deployment where interpretability is demanded for cases\ninvolving ethics and mission critical applications. The complex models of Deep\nNeural Networks make it hard to understand and reason the predictions, which\nhinders its further progress. To tackle this problem, we apply the Knowledge\nDistillation technique to distill Deep Neural Networks into decision trees in\norder to attain good performance and interpretability simultaneously. We\nformulate the problem at hand as a multi-output regression problem and the\nexperiments demonstrate that the student model achieves significantly better\naccuracy performance (about 1\\% to 5\\%) than vanilla decision trees at the same\nlevel of tree depth. The experiments are implemented on the TensorFlow platform\nto make it scalable to big datasets. To the best of our knowledge, we are the\nfirst to distill Deep Neural Networks into vanilla decision trees on\nmulti-class datasets. \n\n"}
{"id": "1812.10962", "contents": "Title: A Variational Topological Neural Model for Cascade-based Diffusion in\n  Networks Abstract: Many works have been proposed in the literature to capture the dynamics of\ndiffusion in networks. While some of them define graphical markovian models to\nextract temporal relationships between node infections in networks, others\nconsider diffusion episodes as sequences of infections via recurrent neural\nmodels. In this paper we propose a model at the crossroads of these two\nextremes, which embeds the history of diffusion in infected nodes as hidden\ncontinuous states. Depending on the trajectory followed by the content before\nreaching a given node, the distribution of influence probabilities may vary.\nHowever, content trajectories are usually hidden in the data, which induces\nchallenging learning problems. We propose a topological recurrent neural model\nwhich exhibits good experimental performances for diffusion modelling and\nprediction. \n\n"}
{"id": "1812.11027", "contents": "Title: Exploring Weight Symmetry in Deep Neural Networks Abstract: We propose to impose symmetry in neural network parameters to improve\nparameter usage and make use of dedicated convolution and matrix multiplication\nroutines. Due to significant reduction in the number of parameters as a result\nof the symmetry constraints, one would expect a dramatic drop in accuracy.\nSurprisingly, we show that this is not the case, and, depending on network\nsize, symmetry can have little or no negative effect on network accuracy,\nespecially in deep overparameterized networks. We propose several ways to\nimpose local symmetry in recurrent and convolutional neural networks, and show\nthat our symmetry parameterizations satisfy universal approximation property\nfor single hidden layer networks. We extensively evaluate these\nparameterizations on CIFAR, ImageNet and language modeling datasets, showing\nsignificant benefits from the use of symmetry. For instance, our ResNet-101\nwith channel-wise symmetry has almost 25% less parameters and only 0.2%\naccuracy loss on ImageNet. Code for our experiments is available at\nhttps://github.com/hushell/deep-symmetry \n\n"}
{"id": "1812.11092", "contents": "Title: Multi-resolution neural networks for tracking seismic horizons from few\n  training images Abstract: Detecting a specific horizon in seismic images is a valuable tool for\ngeological interpretation. Because hand-picking the locations of the horizon is\na time-consuming process, automated computational methods were developed\nstarting three decades ago. Older techniques for such picking include\ninterpolation of control points however, in recent years neural networks have\nbeen used for this task. Until now, most networks trained on small patches from\nlarger images. This limits the networks ability to learn from large-scale\ngeologic structures. Moreover, currently available networks and training\nstrategies require label patches that have full and continuous annotations,\nwhich are also time-consuming to generate.\n  We propose a projected loss-function for training convolutional networks with\na multi-resolution structure, including variants of the U-net. Our networks\nlearn from a small number of large seismic images without creating patches. The\nprojected loss-function enables training on labels with just a few annotated\npixels and has no issue with the other unknown label pixels. Training uses all\ndata without reserving some for validation. Only the labels are split into\ntraining/testing. Contrary to other work on horizon tracking, we train the\nnetwork to perform non-linear regression, and not classification. As such, we\npropose labels as the convolution of a Gaussian kernel and the known horizon\nlocations that indicate uncertainty in the labels. The network output is the\nprobability of the horizon location. We demonstrate the proposed computational\ningredients on two different datasets, for horizon extrapolation and\ninterpolation. We show that the predictions of our methodology are accurate\neven in areas far from known horizon locations because our learning strategy\nexploits all data in large seismic images. \n\n"}
{"id": "1812.11295", "contents": "Title: Monocular 3D Pose Recovery via Nonconvex Sparsity with Theoretical\n  Analysis Abstract: For recovering 3D object poses from 2D images, a prevalent method is to\npre-train an over-complete dictionary $\\mathcal D=\\{B_i\\}_i^D$ of 3D basis\nposes. During testing, the detected 2D pose $Y$ is matched to dictionary by $Y\n\\approx \\sum_i M_i B_i$ where $\\{M_i\\}_i^D=\\{c_i \\Pi R_i\\}$, by estimating the\nrotation $R_i$, projection $\\Pi$ and sparse combination coefficients $c \\in\n\\mathbb R_{+}^D$. In this paper, we propose non-convex regularization $H(c)$ to\nlearn coefficients $c$, including novel leaky capped $\\ell_1$-norm\nregularization (LCNR), \\begin{align*} H(c)=\\alpha \\sum_{i } \\min(|c_i|,\\tau)+\n\\beta \\sum_{i } \\max(| c_i|,\\tau), \\end{align*} where $0\\leq \\beta \\leq \\alpha$\nand $0<\\tau$ is a certain threshold, so the invalid components smaller than\n$\\tau$ are composed with larger regularization and other valid components with\nsmaller regularization. We propose a multi-stage optimizer with convex\nrelaxation and ADMM. We prove that the estimation error $\\mathcal L(l)$ decays\nw.r.t. the stages $l$, \\begin{align*} Pr\\left(\\mathcal L(l) < \\rho^{l-1}\n\\mathcal L(0) + \\delta \\right) \\geq 1- \\epsilon, \\end{align*} where $0< \\rho\n<1, 0<\\delta, 0<\\epsilon \\ll 1$. Experiments on large 3D human datasets like\nH36M are conducted to support our improvement upon previous approaches. To the\nbest of our knowledge, this is the first theoretical analysis in this line of\nresearch, to understand how the recovery error is affected by fundamental\nfactors, e.g. dictionary size, observation noises, optimization times. We\ncharacterize the trade-off between speed and accuracy towards real-time\ninference in applications. \n\n"}
{"id": "1812.11305", "contents": "Title: SPI-Optimizer: an integral-Separated PI Controller for Stochastic\n  Optimization Abstract: To overcome the oscillation problem in the classical momentum-based\noptimizer, recent work associates it with the proportional-integral (PI)\ncontroller, and artificially adds D term producing a PID controller. It\nsuppresses oscillation with the sacrifice of introducing extra hyper-parameter.\nIn this paper, we start by analyzing: why momentum-based method oscillates\nabout the optimal point? and answering that: the fluctuation problem relates to\nthe lag effect of integral (I) term. Inspired by the conditional integration\nidea in classical control society, we propose SPI-Optimizer, an\nintegral-Separated PI controller based optimizer WITHOUT introducing extra\nhyperparameter. It separates momentum term adaptively when the inconsistency of\ncurrent and historical gradient direction occurs. Extensive experiments\ndemonstrate that SPIOptimizer generalizes well on popular network architectures\nto eliminate the oscillation, and owns competitive performance with faster\nconvergence speed (up to 40% epochs reduction ratio ) and more accurate\nclassification result on MNIST, CIFAR10, and CIFAR100 (up to 27.5% error\nreduction ratio) than the state-of-the-art methods. \n\n"}
{"id": "1812.11440", "contents": "Title: Brain MRI super-resolution using 3D generative adversarial networks Abstract: In this work we propose an adversarial learning approach to generate high\nresolution MRI scans from low resolution images. The architecture, based on the\nSRGAN model, adopts 3D convolutions to exploit volumetric information. For the\ndiscriminator, the adversarial loss uses least squares in order to stabilize\nthe training. For the generator, the loss function is a combination of a least\nsquares adversarial loss and a content term based on mean square error and\nimage gradients in order to improve the quality of the generated images. We\nexplore different solutions for the upsampling phase. We present promising\nresults that improve classical interpolation, showing the potential of the\napproach for 3D medical imaging super-resolution. Source code available at\nhttps://github.com/imatge-upc/3D-GAN-superresolution \n\n"}
{"id": "1901.00035", "contents": "Title: Convex Relaxations of Convolutional Neural Nets Abstract: We propose convex relaxations for convolutional neural nets with one hidden\nlayer where the output weights are fixed. For convex activation functions such\nas rectified linear units, the relaxations are convex second order cone\nprograms which can be solved very efficiently. We prove that the relaxation\nrecovers the global minimum under a planted model assumption, given\nsufficiently many training samples from a Gaussian distribution. We also\nidentify a phase transition phenomenon in recovering the global minimum for the\nrelaxation. \n\n"}
{"id": "1901.00055", "contents": "Title: Multiple Sclerosis Lesion Inpainting Using Non-Local Partial\n  Convolutions Abstract: Multiple sclerosis (MS) is an inflammatory demyelinating disease of the\ncentral nervous system (CNS) that results in focal injury to the grey and white\nmatter. The presence of white matter lesions biases morphometric analyses such\nas registration, individual longitudinal measurements and tissue segmentation\nfor brain volume measurements. Lesion-inpainting with intensities derived from\nsurrounding healthy tissue represents one approach to alleviate such problems.\nHowever, existing methods inpaint lesions based on texture information derived\nfrom local surrounding tissue, often leading to inconsistent inpainting and the\ngeneration of artifacts such as intensity discrepancy and blurriness. Based on\nthese observations, we propose non-local partial convolutions (NLPC) that\nintegrates a Unet-like network with the non-local module. The non-local module\nis exploited to capture long range dependencies between the lesion area and\nremaining normal-appearing brain regions. Then, the lesion area is filled by\nreferring to normal-appearing regions with more similar features. This method\ngenerates inpainted regions that appear more realistic and natural. Our\nquantitative experimental results also demonstrate superiority of this\ntechnique of existing state-of-the-art inpainting methods. \n\n"}
{"id": "1901.00172", "contents": "Title: Supervised Multiscale Dimension Reduction for Spatial Interaction\n  Networks Abstract: We introduce a multiscale supervised dimension reduction method for SPatial\nInteraction Network (SPIN) data, which consist of a collection of spatially\ncoordinated interactions. This type of predictor arises when the sampling unit\nof data is composed of a collection of primitive variables, each of them being\nessentially unique, so that it becomes necessary to group the variables in\norder to simplify the representation and enhance interpretability. In this\npaper, we introduce an empirical Bayes approach called spinlets, which first\nconstructs a partitioning tree to guide the reduction over multiple spatial\ngranularities, and then refines the representation of predictors according to\nthe relevance to the response. We consider an inverse Poisson regression model\nand propose a new multiscale generalized double Pareto prior, which is induced\nvia a tree-structured parameter expansion scheme. Our approach is motivated by\nan application in soccer analytics, in which we obtain compact vectorial\nrepresentations and readily interpretable visualizations of the complex network\nobjects, supervised by the response of interest. \n\n"}
{"id": "1901.00301", "contents": "Title: Warm-starting Contextual Bandits: Robustly Combining Supervised and\n  Bandit Feedback Abstract: We investigate the feasibility of learning from a mix of both fully-labeled\nsupervised data and contextual bandit data. We specifically consider settings\nin which the underlying learning signal may be different between these two data\nsources. Theoretically, we state and prove no-regret algorithms for learning\nthat is robust to misaligned cost distributions between the two sources.\nEmpirically, we evaluate some of these algorithms on a large selection of\ndatasets, showing that our approach is both feasible and helpful in practice. \n\n"}
{"id": "1901.00751", "contents": "Title: Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep\n  Learning Methods Abstract: This paper introduces a novel low-cost device prototype for the automatic\ndiagnosis of diseases, utilizing inputted symptoms and personal background. The\nengineering goal is to solve the problem of limited healthcare access with a\nsingle device. Diagnosing diseases automatically is an immense challenge, owing\nto their variable properties and symptoms. On the other hand, Neural Networks\nhave developed into a powerful tool in the field of machine learning, one that\nis showing to be extremely promising at computing diagnosis even with\ninconsistent variables.\n  In this research, a cheap device was created to allow for straightforward\ndiagnosis and treatment of human diseases. By utilizing Deep Neural Networks\n(DNNs) and Convolutional Neural Networks (CNNs), outfitted on a Raspberry Pi\nZero processor ($5), the device is able to detect up to 1537 different diseases\nand conditions and utilize a CNN for on-device visual diagnostics. The user can\ninput the symptoms using the buttons on the device and can take pictures using\nthe same mechanism. The algorithm processes inputted symptoms, providing\ndiagnosis and possible treatment options for common conditions. The purpose of\nthis work was to be able to diagnose diseases through an affordable processor\nwith high accuracy, as it is currently achieving an accuracy of 90% for Top-5\nsymptom-based diagnoses, and 91% for visual skin diseases. The NNs achieve\nperformance far above any other tested system, and its efficiency and ease of\nuse will prove it to be a helpful tool for people around the world. This device\ncould potentially provide low-cost universal access to vital diagnostics and\ntreatment options. \n\n"}
{"id": "1901.00786", "contents": "Title: Towards Global Remote Discharge Estimation: Using the Few to Estimate\n  The Many Abstract: Learning hydrologic models for accurate riverine flood prediction at scale is\na challenge of great importance. One of the key difficulties is the need to\nrely on in-situ river discharge measurements, which can be quite scarce and\nunreliable, particularly in regions where floods cause the most damage every\nyear. Accordingly, in this work we tackle the problem of river discharge\nestimation at different river locations. A core characteristic of the data at\nhand (e.g. satellite measurements) is that we have few measurements for many\nlocations, all sharing the same physics that underlie the water discharge. We\ncapture this scenario in a simple but powerful common mechanism regression\n(CMR) model with a local component as well as a shared one which captures the\nglobal discharge mechanism. The resulting learning objective is non-convex, but\nwe show that we can find its global optimum by leveraging the power of joining\nlocal measurements across sites. In particular, using a spectral initialization\nwith provable near-optimal accuracy, we can find the optimum using standard\ndescent methods. We demonstrate the efficacy of our approach for the problem of\ndischarge estimation using simulations. \n\n"}
{"id": "1901.00997", "contents": "Title: Concentration bounds for CVaR estimation: The cases of light-tailed and\n  heavy-tailed distributions Abstract: Conditional Value-at-Risk (CVaR) is a widely used risk metric in applications\nsuch as finance. We derive concentration bounds for CVaR estimates, considering\nseparately the cases of light-tailed and heavy-tailed distributions. In the\nlight-tailed case, we use a classical CVaR estimator based on the empirical\ndistribution constructed from the samples. For heavy-tailed random variables,\nwe assume a mild `bounded moment' condition, and derive a concentration bound\nfor a truncation-based estimator. Notably, our concentration bounds enjoy an\nexponential decay in the sample size, for heavy-tailed as well as light-tailed\ndistributions. To demonstrate the applicability of our concentration results,\nwe consider a CVaR optimization problem in a multi-armed bandit setting.\nSpecifically, we address the best CVaR-arm identification problem under a fixed\nbudget. We modify the well-known successive rejects algorithm to incorporate a\nCVaR-based criterion. Using the CVaR concentration result, we derive an\nupper-bound on the probability of incorrect identification by the proposed\nalgorithm. \n\n"}
{"id": "1901.01346", "contents": "Title: Efficient Representation Learning Using Random Walks for Dynamic Graphs Abstract: An important part of many machine learning workflows on graphs is vertex\nrepresentation learning, i.e., learning a low-dimensional vector representation\nfor each vertex in the graph. Recently, several powerful techniques for\nunsupervised representation learning have been demonstrated to give the\nstate-of-the-art performance in downstream tasks such as vertex classification\nand edge prediction. These techniques rely on random walks performed on the\ngraph in order to capture its structural properties. These structural\nproperties are then encoded in the vector representation space.\n  However, most contemporary representation learning methods only apply to\nstatic graphs while real-world graphs are often dynamic and change over time.\nStatic representation learning methods are not able to update the vector\nrepresentations when the graph changes; therefore, they must re-generate the\nvector representations on an updated static snapshot of the graph regardless of\nthe extent of the change in the graph. In this work, we propose computationally\nefficient algorithms for vertex representation learning that extend random walk\nbased methods to dynamic graphs. The computation complexity of our algorithms\ndepends upon the extent and rate of changes (the number of edges changed per\nupdate) and on the density of the graph. We empirically evaluate our algorithms\non real world datasets for downstream machine learning tasks of multi-class and\nmulti-label vertex classification. The results show that our algorithms can\nachieve competitive results to the state-of-the-art methods while being\ncomputationally efficient. \n\n"}
{"id": "1901.01427", "contents": "Title: Poincar\\'e Wasserstein Autoencoder Abstract: This work presents a reformulation of the recently proposed Wasserstein\nautoencoder framework on a non-Euclidean manifold, the Poincar\\'e ball model of\nthe hyperbolic space. By assuming the latent space to be hyperbolic, we can use\nits intrinsic hierarchy to impose structure on the learned latent space\nrepresentations. We demonstrate the model in the visual domain to analyze some\nof its properties and show competitive results on a graph link prediction task. \n\n"}
{"id": "1901.01960", "contents": "Title: Learning-based Optimization of the Under-sampling Pattern in MRI Abstract: Acquisition of Magnetic Resonance Imaging (MRI) scans can be accelerated by\nunder-sampling in k-space (i.e., the Fourier domain). In this paper, we\nconsider the problem of optimizing the sub-sampling pattern in a data-driven\nfashion. Since the reconstruction model's performance depends on the\nsub-sampling pattern, we combine the two problems. For a given sparsity\nconstraint, our method optimizes the sub-sampling pattern and reconstruction\nmodel, using an end-to-end learning strategy. Our algorithm learns from\nfull-resolution data that are under-sampled retrospectively, yielding a\nsub-sampling pattern and reconstruction model that are customized to the type\nof images represented in the training data. The proposed method, which we call\nLOUPE (Learning-based Optimization of the Under-sampling PattErn), was\nimplemented by modifying a U-Net, a widely-used convolutional neural network\narchitecture, that we append with the forward model that encodes the\nunder-sampling process. Our experiments with T1-weighted structural brain MRI\nscans show that the optimized sub-sampling pattern can yield significantly more\naccurate reconstructions compared to standard random uniform, variable density\nor equispaced under-sampling schemes. The code is made available at:\nhttps://github.com/cagladbahadir/LOUPE . \n\n"}
{"id": "1901.02470", "contents": "Title: Bilinear Bandits with Low-rank Structure Abstract: We introduce the bilinear bandit problem with low-rank structure in which an\naction takes the form of a pair of arms from two different entity types, and\nthe reward is a bilinear function of the known feature vectors of the arms. The\nunknown in the problem is a $d_1$ by $d_2$ matrix $\\mathbf{\\Theta}^*$ that\ndefines the reward, and has low rank $r \\ll \\min\\{d_1,d_2\\}$. Determination of\n$\\mathbf{\\Theta}^*$ with this low-rank structure poses a significant challenge\nin finding the right exploration-exploitation tradeoff. In this work, we\npropose a new two-stage algorithm called \"Explore-Subspace-Then-Refine\" (ESTR).\nThe first stage is an explicit subspace exploration, while the second stage is\na linear bandit algorithm called \"almost-low-dimensional OFUL\" (LowOFUL) that\nexploits and further refines the estimated subspace via a regularization\ntechnique. We show that the regret of ESTR is\n$\\widetilde{\\mathcal{O}}((d_1+d_2)^{3/2} \\sqrt{r T})$ where\n$\\widetilde{\\mathcal{O}}$ hides logarithmic factors and $T$ is the time\nhorizon, which improves upon the regret of\n$\\widetilde{\\mathcal{O}}(d_1d_2\\sqrt{T})$ attained for a na\\\"ive linear bandit\nreduction. We conjecture that the regret bound of ESTR is unimprovable up to\npolylogarithmic factors, and our preliminary experiment shows that ESTR\noutperforms a na\\\"ive linear bandit reduction. \n\n"}
{"id": "1901.02511", "contents": "Title: Multi-stream CNN based Video Semantic Segmentation for Automated Driving Abstract: Majority of semantic segmentation algorithms operate on a single frame even\nin the case of videos. In this work, the goal is to exploit temporal\ninformation within the algorithm model for leveraging motion cues and temporal\nconsistency. We propose two simple high-level architectures based on Recurrent\nFCN (RFCN) and Multi-Stream FCN (MSFCN) networks. In case of RFCN, a recurrent\nnetwork namely LSTM is inserted between the encoder and decoder. MSFCN combines\nthe encoders of different frames into a fused encoder via 1x1 channel-wise\nconvolution. We use a ResNet50 network as the baseline encoder and construct\nthree networks namely MSFCN of order 2 & 3 and RFCN of order 2. MSFCN-3\nproduces the best results with an accuracy improvement of 9% and 15% for\nHighway and New York-like city scenarios in the SYNTHIA-CVPR'16 dataset using\nmean IoU metric. MSFCN-3 also produced 11% and 6% for SegTrack V2 and DAVIS\ndatasets over the baseline FCN network. We also designed an efficient version\nof MSFCN-2 and RFCN-2 using weight sharing among the two encoders. The\nefficient MSFCN-2 provided an improvement of 11% and 5% for KITTI and SYNTHIA\nwith negligible increase in computational complexity compared to the baseline\nversion. \n\n"}
{"id": "1901.02549", "contents": "Title: Deep Neural Networks Predicting Oil Movement in a Development Unit Abstract: We present a novel technique for assessing the dynamics of multiphase fluid\nflow in the oil reservoir. We demonstrate an efficient workflow for handling\nthe 3D reservoir simulation data in a way which is orders of magnitude faster\nthan the conventional routine. The workflow (we call it \"Metamodel\") is based\non a projection of the system dynamics into a latent variable space, using\nVariational Autoencoder model, where Recurrent Neural Network predicts the\ndynamics. We show that being trained on multiple results of the conventional\nreservoir modelling, the Metamodel does not compromise the accuracy of the\nreservoir dynamics reconstruction in a significant way. It allows forecasting\nnot only the flow rates from the wells, but also the dynamics of pressure and\nfluid saturations within the reservoir. The results open a new perspective in\nthe optimization of oilfield development as the scenario screening could be\naccelerated sufficiently. \n\n"}
{"id": "1901.02871", "contents": "Title: The Lingering of Gradients: Theory and Applications Abstract: Classically, the time complexity of a first-order method is estimated by its\nnumber of gradient computations. In this paper, we study a more refined\ncomplexity by taking into account the `lingering' of gradients: once a gradient\nis computed at $x_k$, the additional time to compute gradients at\n$x_{k+1},x_{k+2},\\dots$ may be reduced.\n  We show how this improves the running time of several first-order methods.\nFor instance, if the `additional time' scales linearly with respect to the\ntraveled distance, then the `convergence rate' of gradient descent can be\nimproved from $1/T$ to $\\exp(-T^{1/3})$. On the application side, we solve a\nhypothetical revenue management problem on the Yahoo! Front Page Today Module\nwith 4.6m users to $10^{-6}$ error using only 6 passes of the dataset; and\nsolve a real-life support vector machine problem to an accuracy that is two\norders of magnitude better comparing to the state-of-the-art algorithm. \n\n"}
{"id": "1901.02878", "contents": "Title: A Constructive Approach for One-Shot Training of Neural Networks Using\n  Hypercube-Based Topological Coverings Abstract: In this paper we presented a novel constructive approach for training deep\nneural networks using geometric approaches. We show that a topological covering\ncan be used to define a class of distributed linear matrix inequalities, which\nin turn directly specify the shape and depth of a neural network architecture.\nThe key insight is a fundamental relationship between linear matrix\ninequalities and their ability to bound the shape of data, and the rectified\nlinear unit (ReLU) activation function employed in modern neural networks. We\nshow that unit cover geometry and cover porosity are two design variables in\ncover-constructive learning that play a critical role in defining the\ncomplexity of the model and generalizability of the resulting neural network\nclassifier. In the context of cover-constructive learning, these findings\nunderscore the age old trade-off between model complexity and overfitting (as\nquantified by the number of elements in the data cover) and generalizability on\ntest data. Finally, we benchmark on algorithm on the Iris, MNIST, and Wine\ndataset and show that the constructive algorithm is able to train a deep neural\nnetwork classifier in one shot, achieving equal or superior levels of training\nand test classification accuracy with reduced training time. \n\n"}
{"id": "1901.03091", "contents": "Title: An MBO scheme for clustering and semi-supervised clustering of signed\n  networks Abstract: We introduce a principled method for the signed clustering problem, where the\ngoal is to partition a graph whose edge weights take both positive and negative\nvalues, such that edges within the same cluster are mostly positive, while\nedges spanning across clusters are mostly negative. Our method relies on a\ngraph-based diffuse interface model formulation utilizing the Ginzburg-Landau\nfunctional, based on an adaptation of the classic numerical\nMerriman-Bence-Osher (MBO) scheme for minimizing such graph-based functionals.\nThe proposed objective function aims to minimize the total weight of\ninter-cluster positively-weighted edges, while maximizing the total weight of\nthe inter-cluster negatively-weighted edges. Our method scales to large sparse\nnetworks, and can be easily adjusted to incorporate labelled data information,\nas is often the case in the context of semi-supervised learning. We tested our\nmethod on a number of both synthetic stochastic block models and real-world\ndata sets (including financial correlation matrices), and obtained promising\nresults that compare favourably against a number of state-of-the-art approaches\nfrom the recent literature. \n\n"}
{"id": "1901.03124", "contents": "Title: Active Learning for One-Class Classification Using Two One-Class\n  Classifiers Abstract: This paper introduces a novel, generic active learning method for one-class\nclassification. Active learning methods play an important role to reduce the\nefforts of manual labeling in the field of machine learning. Although many\nactive learning approaches have been proposed during the last years, most of\nthem are restricted on binary or multi-class problems. One-class classifiers\nuse samples from only one class, the so-called target class, during training\nand hence require special active learning strategies. The few strategies\nproposed for one-class classification either suffer from their limitation on\nspecific one-class classifiers or their performance depends on particular\nassumptions about datasets like imbalance. Our proposed method bases on using\ntwo one-class classifiers, one for the desired target class and one for the\nso-called outlier class. It allows to invent new query strategies, to use\nbinary query strategies and to define simple stopping criteria. Based on the\nnew method, two query strategies are proposed. The provided experiments compare\nthe proposed approach with known strategies on various datasets and show\nimproved results in almost all situations. \n\n"}
{"id": "1901.03415", "contents": "Title: Context Aware Machine Learning Abstract: We propose a principle for exploring context in machine learning models.\nStarting with a simple assumption that each observation may or may not depend\non its context, a conditional probability distribution is decomposed into two\nparts: context-free and context-sensitive. Then by employing the log-linear\nword production model for relating random variables to their embedding space\nrepresentation and making use of the convexity of natural exponential function,\nwe show that the embedding of an observation can also be decomposed into a\nweighted sum of two vectors, representing its context-free and\ncontext-sensitive parts, respectively. This simple treatment of context\nprovides a unified view of many existing deep learning models, leading to\nrevisions of these models able to achieve significant performance boost.\nSpecifically, our upgraded version of a recent sentence embedding model not\nonly outperforms the original one by a large margin, but also leads to a new,\nprincipled approach for compositing the embeddings of bag-of-words features, as\nwell as a new architecture for modeling attention in deep neural networks. More\nsurprisingly, our new principle provides a novel understanding of the gates and\nequations defined by the long short term memory model, which also leads to a\nnew model that is able to converge significantly faster and achieve much lower\nprediction errors. Furthermore, our principle also inspires a new type of\ngeneric neural network layer that better resembles real biological neurons than\nthe traditional linear mapping plus nonlinear activation based architecture.\nIts multi-layer extension provides a new principle for deep neural networks\nwhich subsumes residual network (ResNet) as its special case, and its extension\nto convolutional neutral network model accounts for irrelevant input (e.g.,\nbackground in an image) in addition to filtering. \n\n"}
{"id": "1901.03829", "contents": "Title: Predicting Diffusion Reach Probabilities via Representation Learning on\n  Social Networks Abstract: Diffusion reach probability between two nodes on a network is defined as the\nprobability of a cascade originating from one node reaching to another node. An\ninfinite number of cascades would enable calculation of true diffusion reach\nprobabilities between any two nodes. However, there exists only a finite number\nof cascades and one usually has access only to a small portion of all available\ncascades. In this work, we addressed the problem of estimating diffusion reach\nprobabilities given only a limited number of cascades and partial information\nabout underlying network structure. Our proposed strategy employs node\nrepresentation learning to generate and feed node embeddings into machine\nlearning algorithms to create models that predict diffusion reach\nprobabilities. We provide experimental analysis using synthetically generated\ncascades on two real-world social networks. Results show that proposed method\nis superior to using values calculated from available cascades when the portion\nof cascades is small. \n\n"}
{"id": "1901.04028", "contents": "Title: Sales Demand Forecast in E-commerce using a Long Short-Term Memory\n  Neural Network Methodology Abstract: Generating accurate and reliable sales forecasts is crucial in the E-commerce\nbusiness. The current state-of-the-art techniques are typically univariate\nmethods, which produce forecasts considering only the historical sales data of\na single product. However, in a situation where large quantities of related\ntime series are available, conditioning the forecast of an individual time\nseries on past behaviour of similar, related time series can be beneficial.\nSince the product assortment hierarchy in an E-commerce platform contains large\nnumbers of related products, in which the sales demand patterns can be\ncorrelated, our attempt is to incorporate this cross-series information in a\nunified model. We achieve this by globally training a Long Short-Term Memory\nnetwork (LSTM) that exploits the non-linear demand relationships available in\nan E-commerce product assortment hierarchy. Aside from the forecasting\nframework, we also propose a systematic pre-processing framework to overcome\nthe challenges in the E-commerce business. We also introduce several product\ngrouping strategies to supplement the LSTM learning schemes, in situations\nwhere sales patterns in a product portfolio are disparate. We empirically\nevaluate the proposed forecasting framework on a real-world online marketplace\ndataset from Walmart.com. Our method achieves competitive results on category\nlevel and super-departmental level datasets, outperforming state-of-the-art\ntechniques. \n\n"}
{"id": "1901.04364", "contents": "Title: A Self-Correcting Deep Learning Approach to Predict Acute Conditions in\n  Critical Care Abstract: In critical care, intensivists are required to continuously monitor high\ndimensional vital signs and lab measurements to detect and diagnose acute\npatient conditions. This has always been a challenging task. In this study, we\npropose a novel self-correcting deep learning prediction approach to address\nthis challenge. We focus on an example of the prediction of acute kidney injury\n(AKI). Compared with the existing models, our method has a number of distinct\nfeatures: we utilized the accumulative data of patients in ICU; we developed a\nself-correcting mechanism that feeds errors from the previous predictions back\ninto the network; we also proposed a regularization method that takes into\naccount not only the model's prediction error on the label but also its\nestimation errors on the input data. This mechanism is applied in both\nregression and classification tasks. We compared the performance of our\nproposed method with the conventional deep learning models on two real-world\nclinical datasets and demonstrated that our proposed model constantly\noutperforms these baseline models. In particular, the proposed model achieved\narea under ROC curve at 0.893 on the MIMIC III dataset, and 0.871 on the\nPhilips eICU dataset. \n\n"}
{"id": "1901.05835", "contents": "Title: Unobtrusive and Multimodal Approach for Behavioral Engagement Detection\n  of Students Abstract: We propose a multimodal approach for detection of students' behavioral\nengagement states (i.e., On-Task vs. Off-Task), based on three unobtrusive\nmodalities: Appearance, Context-Performance, and Mouse. Final behavioral\nengagement states are achieved by fusing modality-specific classifiers at the\ndecision level. Various experiments were conducted on a student dataset\ncollected in an authentic classroom. \n\n"}
{"id": "1901.05906", "contents": "Title: Applying SVGD to Bayesian Neural Networks for Cyclical Time-Series\n  Prediction and Inference Abstract: A regression-based BNN model is proposed to predict spatiotemporal quantities\nlike hourly rider demand with calibrated uncertainties. The main contributions\nof this paper are (i) A feed-forward deterministic neural network (DetNN)\narchitecture that predicts cyclical time series data with sensitivity to\nanomalous forecasting events; (ii) A Bayesian framework applying SVGD to train\nlarge neural networks for such tasks, capable of producing time series\npredictions as well as measures of uncertainty surrounding the predictions.\nExperiments show that the proposed BNN reduces average estimation error by 10%\nacross 8 U.S. cities compared to a fine-tuned multilayer perceptron (MLP), and\n4% better than the same network architecture trained without SVGD. \n\n"}
{"id": "1901.06003", "contents": "Title: Gromov-Wasserstein Learning for Graph Matching and Node Embedding Abstract: A novel Gromov-Wasserstein learning framework is proposed to jointly match\n(align) graphs and learn embedding vectors for the associated graph nodes.\nUsing Gromov-Wasserstein discrepancy, we measure the dissimilarity between two\ngraphs and find their correspondence, according to the learned optimal\ntransport. The node embeddings associated with the two graphs are learned under\nthe guidance of the optimal transport, the distance of which not only reflects\nthe topological structure of each graph but also yields the correspondence\nacross the graphs. These two learning steps are mutually-beneficial, and are\nunified here by minimizing the Gromov-Wasserstein discrepancy with structural\nregularizers. This framework leads to an optimization problem that is solved by\na proximal point method. We apply the proposed method to matching problems in\nreal-world networks, and demonstrate its superior performance compared to\nalternative approaches. \n\n"}
{"id": "1901.06086", "contents": "Title: WALL-E: An Efficient Reinforcement Learning Research Framework Abstract: There are two halves to RL systems: experience collection time and policy\nlearning time. For a large number of samples in rollouts, experience collection\ntime is the major bottleneck. Thus, it is necessary to speed up the rollout\ngeneration time with multi-process architecture support. Our work, dubbed\nWALL-E, utilizes multiple rollout samplers running in parallel to rapidly\ngenerate experience. Due to our parallel samplers, we experience not only\nfaster convergence times, but also higher average reward thresholds. For\nexample, on the MuJoCo HalfCheetah-v2 task, with $N = 10$ parallel sampler\nprocesses, we are able to achieve much higher average return than those from\nusing only a single process architecture. \n\n"}
{"id": "1901.06576", "contents": "Title: Towards Physically Safe Reinforcement Learning under Supervision Abstract: This paper addresses the question of how a previously available control\npolicy $\\pi_s$ can be used as a supervisor to more quickly and safely train a\nnew learned control policy $\\pi_L$ for a robot. A weighted average of the\nsupervisor and learned policies is used during trials, with a heavier weight\ninitially on the supervisor, in order to allow safe and useful physical trials\nwhile the learned policy is still ineffective. During the process, the weight\nis adjusted to favor the learned policy. As weights are adjusted, the learned\nnetwork must compensate so as to give safe and reasonable outputs under the\ndifferent weights. A pioneer network is introduced that pre-learns a policy\nthat performs similarly to the current learned policy under the planned next\nstep for new weights; this pioneer network then replaces the currently learned\nnetwork in the next set of trials. Experiments in OpenAI Gym demonstrate the\neffectiveness of the proposed method. \n\n"}
{"id": "1901.07295", "contents": "Title: Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization Abstract: Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'\nimage from a pathological one, could be helpful in tasks such as anomaly\ndetection, understanding changes induced by pathology and disease or even as\ndata augmentation. We treat this task as a factor decomposition problem: we aim\nto separate what appears to be healthy and where disease is (as a map). The two\nfactors are then recombined (by a network) to reconstruct the input disease\nimage. We train our models in an adversarial way using either paired or\nunpaired settings, where we pair disease images and maps (as segmentation\nmasks) when available. We quantitatively evaluate the quality of pseudo healthy\nimages. We show in a series of experiments, performed in ISLES and BraTS\ndatasets, that our method is better than conditional GAN and CycleGAN,\nhighlighting challenges in using adversarial methods in the image translation\ntask of pseudo healthy image generation. \n\n"}
{"id": "1901.07329", "contents": "Title: The autofeat Python Library for Automated Feature Engineering and\n  Selection Abstract: This paper describes the autofeat Python library, which provides scikit-learn\nstyle linear regression and classification models with automated feature\nengineering and selection capabilities. Complex non-linear machine learning\nmodels, such as neural networks, are in practice often difficult to train and\neven harder to explain to non-statisticians, who require transparent analysis\nresults as a basis for important business decisions. While linear models are\nefficient and intuitive, they generally provide lower prediction accuracies.\nOur library provides a multi-step feature engineering and selection process,\nwhere first a large pool of non-linear features is generated, from which then a\nsmall and robust set of meaningful features is selected, which improve the\nprediction accuracy of a linear model while retaining its interpretability. \n\n"}
{"id": "1901.07487", "contents": "Title: Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for\n  Non-Convex Optimization Abstract: Recent studies on diffusion-based sampling methods have shown that Langevin\nMonte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and\nrigorous theoretical guarantees have been proven for both asymptotic and\nfinite-time regimes. Algorithmically, LMC-based algorithms resemble the\nwell-known gradient descent (GD) algorithm, where the GD recursion is perturbed\nby an additive Gaussian noise whose variance has a particular form. Fractional\nLangevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the\nGaussian noise is replaced by a heavy-tailed {\\alpha}-stable noise. As opposed\nto its Gaussian counterpart, these heavy-tailed perturbations can incur large\njumps and it has been empirically demonstrated that the choice of\n{\\alpha}-stable noise can provide several advantages in modern machine learning\nproblems, both in optimization and sampling contexts. However, as opposed to\nLMC, only asymptotic convergence properties of FLMC have been yet established.\nIn this study, we analyze the non-asymptotic behavior of FLMC for non-convex\noptimization and prove finite-time bounds for its expected suboptimality. Our\nresults show that the weak-error of FLMC increases faster than LMC, which\nsuggests using smaller step-sizes in FLMC. We finally extend our results to the\ncase where the exact gradients are replaced by stochastic gradients and show\nthat similar results hold in this setting as well. \n\n"}
{"id": "1901.07648", "contents": "Title: Finite-Sum Smooth Optimization with SARAH Abstract: The total complexity (measured as the total number of gradient computations)\nof a stochastic first-order optimization algorithm that finds a first-order\nstationary point of a finite-sum smooth nonconvex objective function\n$F(w)=\\frac{1}{n} \\sum_{i=1}^n f_i(w)$ has been proven to be at least\n$\\Omega(\\sqrt{n}/\\epsilon)$ for $n \\leq \\mathcal{O}(\\epsilon^{-2})$ where\n$\\epsilon$ denotes the attained accuracy $\\mathbb{E}[ \\|\\nabla\nF(\\tilde{w})\\|^2] \\leq \\epsilon$ for the outputted approximation $\\tilde{w}$\n(Fang et al., 2018). In this paper, we provide a convergence analysis for a\nslightly modified version of the SARAH algorithm (Nguyen et al., 2017a;b) and\nachieve total complexity that matches the lower-bound worst case complexity in\n(Fang et al., 2018) up to a constant factor when $n \\leq\n\\mathcal{O}(\\epsilon^{-2})$ for nonconvex problems. For convex optimization, we\npropose SARAH++ with sublinear convergence for general convex and linear\nconvergence for strongly convex problems; and we provide a practical version\nfor which numerical experiments on various datasets show an improved\nperformance. \n\n"}
{"id": "1901.07868", "contents": "Title: Constant Time Graph Neural Networks Abstract: The recent advancements in graph neural networks (GNNs) have led to\nstate-of-the-art performances in various applications, including\nchemo-informatics, question-answering systems, and recommender systems.\nHowever, scaling up these methods to huge graphs, such as social networks and\nWeb graphs, remains a challenge. In particular, the existing methods for\naccelerating GNNs either are not theoretically guaranteed in terms of the\napproximation error or incur at least a linear time computation cost. In this\nstudy, we reveal the query complexity of the uniform node sampling scheme for\nMessage Passing Neural Networks, including GraphSAGE, graph attention networks\n(GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis\nreveals that the complexity of the node sampling method is completely\nindependent of the number of the nodes, edges, and neighbors of the input and\ndepends only on the error tolerance and confidence probability while providing\na theoretical guarantee for the approximation error. To the best of our\nknowledge, this is the first paper to provide a theoretical guarantee of\napproximation for GNNs within constant time. Through experiments with synthetic\nand real-world datasets, we investigated the speed and precision of the node\nsampling scheme and validated our theoretical results. \n\n"}
{"id": "1901.07884", "contents": "Title: Rank consistent ordinal regression for neural networks with application\n  to age estimation Abstract: In many real-world prediction tasks, class labels include information about\nthe relative ordering between labels, which is not captured by commonly-used\nloss functions such as multi-category cross-entropy. Recently, the deep\nlearning community adopted ordinal regression frameworks to take such ordering\ninformation into account. Neural networks were equipped with ordinal regression\ncapabilities by transforming ordinal targets into binary classification\nsubtasks. However, this method suffers from inconsistencies among the different\nbinary classifiers. To resolve these inconsistencies, we propose the COnsistent\nRAnk Logits (CORAL) framework with strong theoretical guarantees for\nrank-monotonicity and consistent confidence scores. Moreover, the proposed\nmethod is architecture-agnostic and can extend arbitrary state-of-the-art deep\nneural network classifiers for ordinal regression tasks. The empirical\nevaluation of the proposed rank-consistent method on a range of face-image\ndatasets for age prediction shows a substantial reduction of the prediction\nerror compared to the reference ordinal regression network. \n\n"}
{"id": "1901.07924", "contents": "Title: Online Learning with Diverse User Preferences Abstract: In this paper, we investigate the impact of diverse user preference on\nlearning under the stochastic multi-armed bandit (MAB) framework. We aim to\nshow that when the user preferences are sufficiently diverse and each arm can\nbe optimal for certain users, the O(log T) regret incurred by exploring the\nsub-optimal arms under the standard stochastic MAB setting can be reduced to a\nconstant. Our intuition is that to achieve sub-linear regret, the number of\ntimes an optimal arm being pulled should scale linearly in time; when all arms\nare optimal for certain users and pulled frequently, the estimated arm\nstatistics can quickly converge to their true values, thus reducing the need of\nexploration dramatically. We cast the problem into a stochastic linear bandits\nmodel, where both the users preferences and the state of arms are modeled as\n{independent and identical distributed (i.i.d)} d-dimensional random vectors.\nAfter receiving the user preference vector at the beginning of each time slot,\nthe learner pulls an arm and receives a reward as the linear product of the\npreference vector and the arm state vector. We also assume that the state of\nthe pulled arm is revealed to the learner once its pulled. We propose a\nWeighted Upper Confidence Bound (W-UCB) algorithm and show that it can achieve\na constant regret when the user preferences are sufficiently diverse. The\nperformance of W-UCB under general setups is also completely characterized and\nvalidated with synthetic data. \n\n"}
{"id": "1901.08022", "contents": "Title: A Universally Optimal Multistage Accelerated Stochastic Gradient Method Abstract: We study the problem of minimizing a strongly convex, smooth function when we\nhave noisy estimates of its gradient. We propose a novel multistage accelerated\nalgorithm that is universally optimal in the sense that it achieves the optimal\nrate both in the deterministic and stochastic case and operates without\nknowledge of noise characteristics. The algorithm consists of stages that use a\nstochastic version of Nesterov's method with a specific restart and parameters\nselected to achieve the fastest reduction in the bias-variance terms in the\nconvergence rate bounds. \n\n"}
{"id": "1901.08255", "contents": "Title: Confidence-based Graph Convolutional Networks for Semi-Supervised\n  Learning Abstract: Predicting properties of nodes in a graph is an important problem with\napplications in a variety of domains. Graph-based Semi-Supervised Learning\n(SSL) methods aim to address this problem by labeling a small subset of the\nnodes as seeds and then utilizing the graph structure to predict label scores\nfor the rest of the nodes in the graph. Recently, Graph Convolutional Networks\n(GCNs) have achieved impressive performance on the graph-based SSL task. In\naddition to label scores, it is also desirable to have confidence scores\nassociated with them. Unfortunately, confidence estimation in the context of\nGCN has not been previously explored. We fill this important gap in this paper\nand propose ConfGCN, which estimates labels scores along with their confidences\njointly in GCN-based setting. ConfGCN uses these estimated confidences to\ndetermine the influence of one node on another during neighborhood aggregation,\nthereby acquiring anisotropic capabilities. Through extensive analysis and\nexperiments on standard benchmarks, we find that ConfGCN is able to outperform\nstate-of-the-art baselines. We have made ConfGCN's source code available to\nencourage reproducible research. \n\n"}
{"id": "1901.08431", "contents": "Title: Provable Smoothness Guarantees for Black-Box Variational Inference Abstract: Black-box variational inference tries to approximate a complex target\ndistribution though a gradient-based optimization of the parameters of a\nsimpler distribution. Provable convergence guarantees require structural\nproperties of the objective. This paper shows that for location-scale family\napproximations, if the target is M-Lipschitz smooth, then so is the objective,\nif the entropy is excluded. The key proof idea is to describe gradients in a\ncertain inner-product space, thus permitting use of Bessel's inequality. This\nresult gives insight into how to parameterize distributions, gives bounds the\nlocation of the optimal parameters, and is a key ingredient for convergence\nguarantees. \n\n"}
{"id": "1901.08511", "contents": "Title: A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for\n  Saddle Point Problems: Proximal Point Approach Abstract: In this paper we consider solving saddle point problems using two variants of\nGradient Descent-Ascent algorithms, Extra-gradient (EG) and Optimistic Gradient\nDescent Ascent (OGDA) methods. We show that both of these algorithms admit a\nunified analysis as approximations of the classical proximal point method for\nsolving saddle point problems. This viewpoint enables us to develop a new\nframework for analyzing EG and OGDA for bilinear and strongly convex-strongly\nconcave settings. Moreover, we use the proximal point approximation\ninterpretation to generalize the results for OGDA for a wide range of\nparameters. \n\n"}
{"id": "1901.08553", "contents": "Title: Data Interpolations in Deep Generative Models under Non-Simply-Connected\n  Manifold Topology Abstract: Exploiting the deep generative model's remarkable ability of learning the\ndata-manifold structure, some recent researches proposed a geometric data\ninterpolation method based on the geodesic curves on the learned data-manifold.\nHowever, this interpolation method often gives poor results due to a\ntopological difference between the model and the dataset. The model defines a\nfamily of simply-connected manifolds, whereas the dataset generally contains\ndisconnected regions or holes that make them non-simply-connected. To\ncompensate this difference, we propose a novel density regularizer that make\nthe interpolation path circumvent the holes denoted by low probability density.\nWe confirm that our method gives consistently better interpolation results from\nthe experiments with real-world image datasets. \n\n"}
{"id": "1901.08571", "contents": "Title: Nonparametric Inference under B-bits Quantization Abstract: Statistical inference based on lossy or incomplete samples is often needed in\nresearch areas such as signal/image processing, medical image storage, remote\nsensing, signal transmission. In this paper, we propose a nonparametric testing\nprocedure based on samples quantized to $B$ bits through a computationally\nefficient algorithm. Under mild technical conditions, we establish the\nasymptotic properties of the proposed test statistic and investigate how the\ntesting power changes as $B$ increases. In particular, we show that if $B$\nexceeds a certain threshold, the proposed nonparametric testing procedure\nachieves the classical minimax rate of testing (Shang and Cheng, 2015) for\nspline models. We further extend our theoretical investigations to a\nnonparametric linearity test and an adaptive nonparametric test, expanding the\napplicability of the proposed methods. Extensive simulation studies {together\nwith a real-data analysis} are used to demonstrate the validity and\neffectiveness of the proposed tests. \n\n"}
{"id": "1901.08573", "contents": "Title: Theoretically Principled Trade-off between Robustness and Accuracy Abstract: We identify a trade-off between robustness and accuracy that serves as a\nguiding principle in the design of defenses against adversarial examples.\nAlthough this problem has been widely studied empirically, much remains unknown\nconcerning the theory underlying this trade-off. In this work, we decompose the\nprediction error for adversarial examples (robust error) as the sum of the\nnatural (classification) error and boundary error, and provide a differentiable\nupper bound using the theory of classification-calibrated loss, which is shown\nto be the tightest possible upper bound uniform over all probability\ndistributions and measurable predictors. Inspired by our theoretical analysis,\nwe also design a new defense method, TRADES, to trade adversarial robustness\noff against accuracy. Our proposed algorithm performs well experimentally in\nreal-world datasets. The methodology is the foundation of our entry to the\nNeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of\n~2,000 submissions, surpassing the runner-up approach by $11.41\\%$ in terms of\nmean $\\ell_2$ perturbation distance. \n\n"}
{"id": "1901.08585", "contents": "Title: Graph heat mixture model learning Abstract: Graph inference methods have recently attracted a great interest from the\nscientific community, due to the large value they bring in data interpretation\nand analysis. However, most of the available state-of-the-art methods focus on\nscenarios where all available data can be explained through the same graph, or\ngroups corresponding to each graph are known a priori. In this paper, we argue\nthat this is not always realistic and we introduce a generative model for mixed\nsignals following a heat diffusion process on multiple graphs. We propose an\nexpectation-maximisation algorithm that can successfully separate signals into\ncorresponding groups, and infer multiple graphs that govern their behaviour. We\ndemonstrate the benefits of our method on both synthetic and real data. \n\n"}
{"id": "1901.08652", "contents": "Title: Learning agile and dynamic motor skills for legged robots Abstract: Legged robots pose one of the greatest challenges in robotics. Dynamic and\nagile maneuvers of animals cannot be imitated by existing methods that are\ncrafted by humans. A compelling alternative is reinforcement learning, which\nrequires minimal craftsmanship and promotes the natural evolution of a control\npolicy. However, so far, reinforcement learning research for legged robots is\nmainly limited to simulation, and only few and comparably simple examples have\nbeen deployed on real systems. The primary reason is that training with real\nrobots, particularly with dynamically balancing systems, is complicated and\nexpensive. In the present work, we introduce a method for training a neural\nnetwork policy in simulation and transferring it to a state-of-the-art legged\nsystem, thereby leveraging fast, automated, and cost-effective data generation\nschemes. The approach is applied to the ANYmal robot, a sophisticated\nmedium-dog-sized quadrupedal system. Using policies trained in simulation, the\nquadrupedal machine achieves locomotion skills that go beyond what had been\nachieved with prior methods: ANYmal is capable of precisely and\nenergy-efficiently following high-level body velocity commands, running faster\nthan before, and recovering from falling even in complex configurations. \n\n"}
{"id": "1901.08665", "contents": "Title: Fairness risk measures Abstract: Ensuring that classifiers are non-discriminatory or fair with respect to a\nsensitive feature (e.g., race or gender) is a topical problem. Progress in this\ntask requires fixing a definition of fairness, and there have been several\nproposals in this regard over the past few years. Several of these, however,\nassume either binary sensitive features (thus precluding categorical or\nreal-valued sensitive groups), or result in non-convex objectives (thus\nadversely affecting the optimisation landscape). In this paper, we propose a\nnew definition of fairness that generalises some existing proposals, while\nallowing for generic sensitive features and resulting in a convex objective.\nThe key idea is to enforce that the expected losses (or risks) across each\nsubgroup induced by the sensitive feature are commensurate. We show how this\nrelates to the rich literature on risk measures from mathematical finance. As a\nspecial case, this leads to a new convex fairness-aware objective based on\nminimising the conditional value at risk (CVaR). \n\n"}
{"id": "1901.08680", "contents": "Title: Multi-objective training of Generative Adversarial Networks with\n  multiple discriminators Abstract: Recent literature has demonstrated promising results for training Generative\nAdversarial Networks by employing a set of discriminators, in contrast to the\ntraditional game involving one generator against a single adversary. Such\nmethods perform single-objective optimization on some simple consolidation of\nthe losses, e.g. an arithmetic average. In this work, we revisit the\nmultiple-discriminator setting by framing the simultaneous minimization of\nlosses provided by different models as a multi-objective optimization problem.\nSpecifically, we evaluate the performance of multiple gradient descent and the\nhypervolume maximization algorithm on a number of different datasets. Moreover,\nwe argue that the previously proposed methods and hypervolume maximization can\nall be seen as variations of multiple gradient descent in which the update\ndirection can be computed efficiently. Our results indicate that hypervolume\nmaximization presents a better compromise between sample quality and\ncomputational cost than previous methods. \n\n"}
{"id": "1901.08710", "contents": "Title: When Can Neural Networks Learn Connected Decision Regions? Abstract: Previous work has questioned the conditions under which the decision regions\nof a neural network are connected and further showed the implications of the\ncorresponding theory to the problem of adversarial manipulation of classifiers.\nIt has been proven that for a class of activation functions including leaky\nReLU, neural networks having a pyramidal structure, that is no layer has more\nhidden units than the input dimension, produce necessarily connected decision\nregions. In this paper, we advance this important result by further developing\nthe sufficient and necessary conditions under which the decision regions of a\nneural network are connected. We then apply our framework to overcome the\nlimits of existing work and further study the capacity to learn connected\nregions of neural networks for a much wider class of activation functions\nincluding those widely used, namely ReLU, sigmoid, tanh, softlus, and\nexponential linear function. \n\n"}
{"id": "1901.08755", "contents": "Title: SecureBoost: A Lossless Federated Learning Framework Abstract: The protection of user privacy is an important concern in machine learning,\nas evidenced by the rolling out of the General Data Protection Regulation\n(GDPR) in the European Union (EU) in May 2018. The GDPR is designed to give\nusers more control over their personal data, which motivates us to explore\nmachine learning frameworks for data sharing that do not violate user privacy.\nTo meet this goal, in this paper, we propose a novel lossless\nprivacy-preserving tree-boosting system known as SecureBoost in the setting of\nfederated learning. SecureBoost first conducts entity alignment under a\nprivacy-preserving protocol and then constructs boosting trees across multiple\nparties with a carefully designed encryption strategy. This federated learning\nsystem allows the learning process to be jointly conducted over multiple\nparties with common user samples but different feature sets, which corresponds\nto a vertically partitioned data set. An advantage of SecureBoost is that it\nprovides the same level of accuracy as the non-privacy-preserving approach\nwhile at the same time, reveals no information of each private data provider.\nWe show that the SecureBoost framework is as accurate as other non-federated\ngradient tree-boosting algorithms that require centralized data and thus it is\nhighly scalable and practical for industrial applications such as credit risk\nanalysis. To this end, we discuss information leakage during the protocol\nexecution and propose ways to provably reduce it. \n\n"}
{"id": "1901.08898", "contents": "Title: Bayesian surrogate learning in dynamic simulator-based regression\n  problems Abstract: The estimation of unknown values of parameters (or hidden variables, control\nvariables) that characterise a physical system often relies on the comparison\nof measured data with synthetic data produced by some numerical simulator of\nthe system as the parameter values are varied. This process often encounters\ntwo major difficulties: the generation of synthetic data for each considered\nset of parameter values can be computationally expensive if the system model is\ncomplicated; and the exploration of the parameter space can be inefficient\nand/or incomplete, a typical example being when the exploration becomes trapped\nin a local optimum of the objection function that characterises the mismatch\nbetween the measured and synthetic data. A method to address both these issues\nis presented, whereby: a surrogate model (or proxy), which emulates the\ncomputationally expensive system simulator, is constructed using deep recurrent\nnetworks (DRN); and a nested sampling (NS) algorithm is employed to perform\nefficient and robust exploration of the parameter space. The analysis is\nperformed in a Bayesian context, in which the samples characterise the full\njoint posterior distribution of the parameters, from which parameter estimates\nand uncertainties are easily derived. The proposed approach is compared with\nconventional methods in some numerical examples, for which the results\ndemonstrate that one can accelerate the parameter estimation process by at\nleast an order of magnitude. \n\n"}
{"id": "1901.08910", "contents": "Title: Scalable Realistic Recommendation Datasets through Fractal Expansions Abstract: Recommender System research suffers currently from a disconnect between the\nsize of academic data sets and the scale of industrial production systems. In\norder to bridge that gap we propose to generate more massive user/item\ninteraction data sets by expanding pre-existing public data sets. User/item\nincidence matrices record interactions between users and items on a given\nplatform as a large sparse matrix whose rows correspond to users and whose\ncolumns correspond to items. Our technique expands such matrices to larger\nnumbers of rows (users), columns (items) and non zero values (interactions)\nwhile preserving key higher order statistical properties. We adapt the\nKronecker Graph Theory to user/item incidence matrices and show that the\ncorresponding fractal expansions preserve the fat-tailed distributions of user\nengagements, item popularity and singular value spectra of user/item\ninteraction matrices. Preserving such properties is key to building large\nrealistic synthetic data sets which in turn can be employed reliably to\nbenchmark Recommender Systems and the systems employed to train them. We\nprovide algorithms to produce such expansions and apply them to the MovieLens\n20 million data set comprising 20 million ratings of 27K movies by 138K users.\nThe resulting expanded data set has 10 billion ratings, 864K items and 2\nmillion users in its smaller version and can be scaled up or down. A larger\nversion features 655 billion ratings, 7 million items and 17 million users. \n\n"}
{"id": "1901.09178", "contents": "Title: A general model for plane-based clustering with loss function Abstract: In this paper, we propose a general model for plane-based clustering. The\ngeneral model contains many existing plane-based clustering methods, e.g.,\nk-plane clustering (kPC), proximal plane clustering (PPC), twin support vector\nclustering (TWSVC) and its extensions. Under this general model, one may obtain\nan appropriate clustering method for specific purpose. The general model is a\nprocedure corresponding to an optimization problem, where the optimization\nproblem minimizes the total loss of the samples. Thereinto, the loss of a\nsample derives from both within-cluster and between-cluster. In theory, the\ntermination conditions are discussed, and we prove that the general model\nterminates in a finite number of steps at a local or weak local optimal point.\nFurthermore, based on this general model, we propose a plane-based clustering\nmethod by introducing a new loss function to capture the data distribution\nprecisely. Experimental results on artificial and public available datasets\nverify the effectiveness of the proposed method. \n\n"}
{"id": "1901.09229", "contents": "Title: DELTA: DEep Learning Transfer using Feature Map with Attention for\n  Convolutional Networks Abstract: Transfer learning through fine-tuning a pre-trained neural network with an\nextremely large dataset, such as ImageNet, can significantly accelerate\ntraining while the accuracy is frequently bottlenecked by the limited dataset\nsize of the new target task. To solve the problem, some regularization methods,\nconstraining the outer layer weights of the target network using the starting\npoint as references (SPAR), have been studied. In this paper, we propose a\nnovel regularized transfer learning framework DELTA, namely DEep Learning\nTransfer using Feature Map with Attention. Instead of constraining the weights\nof neural network, DELTA aims to preserve the outer layer outputs of the target\nnetwork. Specifically, in addition to minimizing the empirical loss, DELTA\nintends to align the outer layer outputs of two networks, through constraining\na subset of feature maps that are precisely selected by attention that has been\nlearned in an supervised learning manner. We evaluate DELTA with the\nstate-of-the-art algorithms, including L2 and L2-SP. The experiment results\nshow that our proposed method outperforms these baselines with higher accuracy\nfor new tasks. \n\n"}
{"id": "1901.09342", "contents": "Title: On the Universality of Invariant Networks Abstract: Constraining linear layers in neural networks to respect symmetry\ntransformations from a group $G$ is a common design principle for invariant\nnetworks that has found many applications in machine learning.\n  In this paper, we consider a fundamental question that has received little\nattention to date: Can these networks approximate any (continuous) invariant\nfunction?\n  We tackle the rather general case where $G\\leq S_n$ (an arbitrary subgroup of\nthe symmetric group) that acts on $\\mathbb{R}^n$ by permuting coordinates. This\nsetting includes several recent popular invariant networks. We present two main\nresults: First, $G$-invariant networks are universal if high-order tensors are\nallowed. Second, there are groups $G$ for which higher-order tensors are\nunavoidable for obtaining universality.\n  $G$-invariant networks consisting of only first-order tensors are of special\ninterest due to their practical value. We conclude the paper by proving a\nnecessary condition for the universality of $G$-invariant networks that\nincorporate only first-order tensors. \n\n"}
{"id": "1901.09451", "contents": "Title: Bias in Bios: A Case Study of Semantic Representation Bias in a\n  High-Stakes Setting Abstract: We present a large-scale study of gender bias in occupation classification, a\ntask where the use of machine learning may lead to negative outcomes on\npeoples' lives. We analyze the potential allocation harms that can result from\nsemantic representation bias. To do so, we study the impact on occupation\nclassification of including explicit gender indicators---such as first names\nand pronouns---in different semantic representations of online biographies.\nAdditionally, we quantify the bias that remains when these indicators are\n\"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit\ngender indicators. As we demonstrate, differences in true positive rates\nbetween genders are correlated with existing gender imbalances in occupations,\nwhich may compound these imbalances. \n\n"}
{"id": "1901.09490", "contents": "Title: Stochastic Linear Bandits with Hidden Low Rank Structure Abstract: High-dimensional representations often have a lower dimensional underlying\nstructure. This is particularly the case in many decision making settings. For\nexample, when the representation of actions is generated from a deep neural\nnetwork, it is reasonable to expect a low-rank structure whereas conventional\nstructures like sparsity are not valid anymore. Subspace recovery methods, such\nas Principle Component Analysis (PCA) can find the underlying low-rank\nstructures in the feature space and reduce the complexity of the learning\ntasks. In this work, we propose Projected Stochastic Linear Bandit (PSLB), an\nalgorithm for high dimensional stochastic linear bandits (SLB) when the\nrepresentation of actions has an underlying low-dimensional subspace structure.\nPSLB deploys PCA based projection to iteratively find the low rank structure in\nSLBs. We show that deploying projection methods assures dimensionality\nreduction and results in a tighter regret upper bound that is in terms of the\ndimensionality of the subspace and its properties, rather than the\ndimensionality of the ambient space. We modify the image classification task\ninto the SLB setting and empirically show that, when a pre-trained DNN provides\nthe high dimensional feature representations, deploying PSLB results in\nsignificant reduction of regret and faster convergence to an accurate model\ncompared to state-of-art algorithm. \n\n"}
{"id": "1901.09557", "contents": "Title: Out-of-Sample Testing for GANs Abstract: We propose a new method to evaluate GANs, namely EvalGAN. EvalGAN relies on a\ntest set to directly measure the reconstruction quality in the original sample\nspace (no auxiliary networks are necessary), and it also computes the\n(log)likelihood for the reconstructed samples in the test set. Further, EvalGAN\nis agnostic to the GAN algorithm and the dataset. We decided to test it on\nthree state-of-the-art GANs over the well-known CIFAR-10 and CelebA datasets. \n\n"}
{"id": "1901.09656", "contents": "Title: EXIT Analysis for Community Detection Abstract: This paper employs the extrinsic information transfer (EXIT) method, a\ntechnique imported from the analysis of the iterative decoding of error control\ncodes, to study the performance of belief propagation in community detection in\nthe presence of side information. We consider both the detection of a single\n(hidden) community, as well as the problem of identifying two symmetric\ncommunities. For single community detection, this paper demonstrates the\nsuitability of EXIT to predict the asymptotic phase transition for weak\nrecovery. More importantly, EXIT analysis is leveraged to produce useful\ninsights such as the performance of belief propagation near the threshold. For\ntwo symmetric communities, the asymptotic residual error for belief propagation\nis calculated under finite-alphabet side information, generalizing a previous\nresult with noisy labels. EXIT analysis is used to illuminate the effect of\nside information on community detection, its relative importance depending on\nthe correlation of the graphical information with node labels, as well as the\neffect of side information on residual errors. \n\n"}
{"id": "1901.09674", "contents": "Title: Deep Generative Graph Distribution Learning for Synthetic Power Grids Abstract: Power system studies require the topological structures of real-world power\nnetworks; however, such data is confidential due to important security\nconcerns. Thus, power grid synthesis (PGS), i.e., creating realistic power\ngrids that imitate actual power networks, has gained significant attention. In\nthis letter, we cast PGS into a graph distribution learning (GDL) problem where\nthe probability distribution functions (PDFs) of the nodes (buses) and edges\n(lines) are captured. A novel deep GDL (DeepGDL) model is proposed to learn the\ntopological patterns of buses/lines with their physical features (e.g., power\ninjection and line impedance). Having a deep nonlinear recurrent structure,\nDeepGDL understands complex nonlinear topological properties and captures the\ngraph PDF. Sampling from the obtained PDF, we are able to create a large set of\nrealistic networks that all resemble the original power grid. Simulation\nresults show the significant accuracy of our created synthetic power grids in\nterms of various topological metrics and power flow measurements. \n\n"}
{"id": "1901.09681", "contents": "Title: Network Lens: Node Classification in Topologically Heterogeneous\n  Networks Abstract: We study the problem of identifying different behaviors occurring in\ndifferent parts of a large heterogenous network. We zoom in to the network\nusing lenses of different sizes to capture the local structure of the network.\nThese network signatures are then weighted to provide a set of predicted labels\nfor every node. We achieve a peak accuracy of $\\sim42\\%$ (random=$11\\%$) on two\nnetworks with $\\sim100,000$ and $\\sim1,000,000$ nodes each. Further, we perform\nbetter than random even when the given node is connected to up to 5 different\ntypes of networks. Finally, we perform this analysis on homogeneous networks\nand show that highly structured networks have high homogeneity. \n\n"}
{"id": "1901.09712", "contents": "Title: Ising Models with Latent Conditional Gaussian Variables Abstract: Ising models describe the joint probability distribution of a vector of\nbinary feature variables. Typically, not all the variables interact with each\nother and one is interested in learning the presumably sparse network structure\nof the interacting variables. However, in the presence of latent variables, the\nconventional method of learning a sparse model might fail. This is because the\nlatent variables induce indirect interactions of the observed variables. In the\ncase of only a few latent conditional Gaussian variables these spurious\ninteractions contribute an additional low-rank component to the interaction\nparameters of the observed Ising model. Therefore, we propose to learn a sparse\n+ low-rank decomposition of the parameters of an Ising model using a convex\nregularized likelihood problem. We show that the same problem can be obtained\nas the dual of a maximum-entropy problem with a new type of relaxation, where\nthe sample means collectively need to match the expected values only up to a\ngiven tolerance. The solution to the convex optimization problem has\nconsistency properties in the high-dimensional setting, where the number of\nobserved binary variables and the number of latent conditional Gaussian\nvariables are allowed to grow with the number of training samples. \n\n"}
{"id": "1901.09715", "contents": "Title: Revisiting the Bethe-Hessian: Improved Community Detection in Sparse\n  Heterogeneous Graphs Abstract: Spectral clustering is one of the most popular, yet still incompletely\nunderstood, methods for community detection on graphs. This article studies\nspectral clustering based on the Bethe-Hessian matrix $H_r = (r^2-1)I_n + D-rA$\nfor sparse heterogeneous graphs (following the degree-corrected stochastic\nblock model) in a two-class setting. For a specific value $r = \\zeta$,\nclustering is shown to be insensitive to the degree heterogeneity. We then\nstudy the behavior of the informative eigenvector of $H_{\\zeta}$ and, as a\nresult, predict the clustering accuracy. The article concludes with an overview\nof the generalization to more than two classes along with extensive simulations\non synthetic and real networks corroborating our findings. \n\n"}
{"id": "1901.09749", "contents": "Title: Fairwashing: the risk of rationalization Abstract: Black-box explanation is the problem of explaining how a machine learning\nmodel -- whose internal logic is hidden to the auditor and generally complex --\nproduces its outcomes. Current approaches for solving this problem include\nmodel explanation, outcome explanation as well as model inspection. While these\ntechniques can be beneficial by providing interpretability, they can be used in\na negative manner to perform fairwashing, which we define as promoting the\nfalse perception that a machine learning model respects some ethical values. In\nparticular, we demonstrate that it is possible to systematically rationalize\ndecisions taken by an unfair black-box model using the model explanation as\nwell as the outcome explanation approaches with a given fairness metric. Our\nsolution, LaundryML, is based on a regularized rule list enumeration algorithm\nwhose objective is to search for fair rule lists approximating an unfair\nblack-box model. We empirically evaluate our rationalization technique on\nblack-box models trained on real-world datasets and show that one can obtain\nrule lists with high fidelity to the black-box model while being considerably\nless unfair at the same time. \n\n"}
{"id": "1901.09827", "contents": "Title: Depth creates no more spurious local minima Abstract: We show that for any convex differentiable loss, a deep linear network has no\nspurious local minima as long as it is true for the two layer case. This\nreduction greatly simplifies the study on the existence of spurious local\nminima in deep linear networks. When applied to the quadratic loss, our result\nimmediately implies the powerful result in [Kawaguchi 2016]. Further, with the\nwork in [Zhou and Liang 2018], we can remove all the assumptions in [Kawaguchi\n2016]. This property holds for more general \"multi-tower\" linear networks too.\nOur proof builds on [Laurent and von Brecht 2018] and develops a new\nperturbation argument to show that any spurious local minimum must have full\nrank, a structural property which can be useful more generally. \n\n"}
{"id": "1901.09892", "contents": "Title: A Black-box Attack on Neural Networks Based on Swarm Evolutionary\n  Algorithm Abstract: Neural networks play an increasingly important role in the field of machine\nlearning and are included in many applications in society. Unfortunately,\nneural networks suffer from adversarial samples generated to attack them.\nHowever, most of the generation approaches either assume that the attacker has\nfull knowledge of the neural network model or are limited by the type of\nattacked model. In this paper, we propose a new approach that generates a\nblack-box attack to neural networks based on the swarm evolutionary algorithm.\nBenefiting from the improvements in the technology and theoretical\ncharacteristics of evolutionary algorithms, our approach has the advantages of\neffectiveness, black-box attack, generality, and randomness. Our experimental\nresults show that both the MNIST images and the CIFAR-10 images can be\nperturbed to successful generate a black-box attack with 100\\% probability on\naverage. In addition, the proposed attack, which is successful on distilled\nneural networks with almost 100\\% probability, is resistant to defensive\ndistillation. The experimental results also indicate that the robustness of the\nartificial intelligence algorithm is related to the complexity of the model and\nthe data set. In addition, we find that the adversarial samples to some extent\nreproduce the characteristics of the sample data learned by the neural network\nmodel. \n\n"}
{"id": "1901.10061", "contents": "Title: A Framework for Deep Constrained Clustering -- Algorithms and Advances Abstract: The area of constrained clustering has been extensively explored by\nresearchers and used by practitioners. Constrained clustering formulations\nexist for popular algorithms such as k-means, mixture models, and spectral\nclustering but have several limitations. A fundamental strength of deep\nlearning is its flexibility, and here we explore a deep learning framework for\nconstrained clustering and in particular explore how it can extend the field of\nconstrained clustering. We show that our framework can not only handle standard\ntogether/apart constraints (without the well documented negative effects\nreported earlier) generated from labeled side information but more complex\nconstraints generated from new types of side information such as continuous\nvalues and high-level domain knowledge. \n\n"}
{"id": "1901.10234", "contents": "Title: Representation Learning for Heterogeneous Information Networks via\n  Embedding Events Abstract: Network representation learning (NRL) has been widely used to help analyze\nlarge-scale networks through mapping original networks into a low-dimensional\nvector space. However, existing NRL methods ignore the impact of properties of\nrelations on the object relevance in heterogeneous information networks (HINs).\nTo tackle this issue, this paper proposes a new NRL framework, called\nEvent2vec, for HINs to consider both quantities and properties of relations\nduring the representation learning process. Specifically, an event (i.e., a\ncomplete semantic unit) is used to represent the relation among multiple\nobjects, and both event-driven first-order and second-order proximities are\ndefined to measure the object relevance according to the quantities and\nproperties of relations. We theoretically prove how event-driven proximities\ncan be preserved in the embedding space by Event2vec, which utilizes event\nembeddings to facilitate learning the object embeddings. Experimental studies\ndemonstrate the advantages of Event2vec over state-of-the-art algorithms on\nfour real-world datasets and three network analysis tasks (including network\nreconstruction, link prediction, and node classification). \n\n"}
{"id": "1901.10251", "contents": "Title: Multi-Agent Reinforcement Learning with Multi-Step Generative Models Abstract: We consider model-based reinforcement learning (MBRL) in 2-agent,\nhigh-fidelity continuous control problems -- an important domain for robots\ninteracting with other agents in the same workspace. For non-trivial dynamical\nsystems, MBRL typically suffers from accumulating errors. Several recent\nstudies have addressed this problem by learning latent variable models for\ntrajectory segments and optimizing over behavior in the latent space. In this\nwork, we investigate whether this approach can be extended to 2-agent\ncompetitive and cooperative settings. The fundamental challenge is how to learn\nmodels that capture interactions between agents, yet are disentangled to allow\nfor optimization of each agent behavior separately. We propose such models\nbased on a disentangled variational auto-encoder, and demonstrate our approach\non a simulated 2-robot manipulation task, where one robot can either help or\ndistract the other. We show that our approach has better sample efficiency than\na strong model-free RL baseline, and can learn both cooperative and adversarial\nbehavior from the same data. \n\n"}
{"id": "1901.11173", "contents": "Title: Peer-to-peer Federated Learning on Graphs Abstract: We consider the problem of training a machine learning model over a network\nof nodes in a fully decentralized framework. The nodes take a Bayesian-like\napproach via the introduction of a belief over the model parameter space. We\npropose a distributed learning algorithm in which nodes update their belief by\naggregate information from their one-hop neighbors to learn a model that best\nfits the observations over the entire network. In addition, we also obtain\nsufficient conditions to ensure that the probability of error is small for\nevery node in the network. We discuss approximations required for applying this\nalgorithm to train Deep Neural Networks (DNNs). Experiments on training linear\nregression model and on training a DNN show that the proposed learning rule\nalgorithm provides a significant improvement in the accuracy compared to the\ncase where nodes learn without cooperation. \n\n"}
{"id": "1901.11200", "contents": "Title: A Bad Arm Existence Checking Problem Abstract: We study a bad arm existing checking problem in which a player's task is to\njudge whether a positive arm exists or not among given K arms by drawing as\nsmall number of arms as possible. Here, an arm is positive if its expected loss\nsuffered by drawing the arm is at least a given threshold. This problem is a\nformalization of diagnosis of disease or machine failure. An interesting\nstructure of this problem is the asymmetry of positive and negative\n(non-positive) arms' roles; finding one positive arm is enough to judge\nexistence while all the arms must be discriminated as negative to judge\nnon-existence. We propose an algorithms with arm selection policy (policy to\ndetermine the next arm to draw) and stopping condition (condition to stop\ndrawing arms) utilizing this asymmetric problem structure and prove its\neffectiveness theoretically and empirically. \n\n"}
{"id": "1901.11213", "contents": "Title: Multi-GCN: Graph Convolutional Networks for Multi-View Networks, with\n  Applications to Global Poverty Abstract: With the rapid expansion of mobile phone networks in developing countries,\nlarge-scale graph machine learning has gained sudden relevance in the study of\nglobal poverty. Recent applications range from humanitarian response and\npoverty estimation to urban planning and epidemic containment. Yet the vast\nmajority of computational tools and algorithms used in these applications do\nnot account for the multi-view nature of social networks: people are related in\nmyriad ways, but most graph learning models treat relations as binary. In this\npaper, we develop a graph-based convolutional network for learning on\nmulti-view networks. We show that this method outperforms state-of-the-art\nsemi-supervised learning algorithms on three different prediction tasks using\nmobile phone datasets from three different developing countries. We also show\nthat, while designed specifically for use in poverty research, the algorithm\nalso outperforms existing benchmarks on a broader set of learning tasks on\nmulti-view networks, including node labelling in citation networks. \n\n"}
{"id": "1901.11352", "contents": "Title: Deep Learning for Inverse Problems: Bounds and Regularizers Abstract: Inverse problems arise in a number of domains such as medical imaging, remote\nsensing, and many more, relying on the use of advanced signal and image\nprocessing approaches -- such as sparsity-driven techniques -- to determine\ntheir solution. This paper instead studies the use of deep learning approaches\nto approximate the solution of inverse problems. In particular, the paper\nprovides a new generalization bound, depending on key quantity associated with\na deep neural network -- its Jacobian matrix -- that also leads to a number of\ncomputationally efficient regularization strategies applicable to inverse\nproblems. The paper also tests the proposed regularization strategies in a\nnumber of inverse problems including image super-resolution ones. Our numerical\nresults conducted on various datasets show that both fully connected and\nconvolutional neural networks regularized using the regularization or proxy\nregularization strategies originating from our theory exhibit much better\nperformance than deep networks regularized with standard approaches such as\nweight-decay. \n\n"}
