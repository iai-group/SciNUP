{"id": "0704.1074", "contents": "Title: Markov basis and Groebner basis of Segre-Veronese configuration for\n  testing independence in group-wise selections Abstract: We consider testing independence in group-wise selections with some\nrestrictions on combinations of choices. We present models for frequency data\nof selections for which it is easy to perform conditional tests by Markov chain\nMonte Carlo (MCMC) methods. When the restrictions on the combinations can be\ndescribed in terms of a Segre-Veronese configuration, an explicit form of a\nGr\\\"obner basis consisting of moves of degree two is readily available for\nperforming a Markov chain. We illustrate our setting with the National Center\nTest for university entrance examinations in Japan. We also apply our method to\ntesting independence hypotheses involving genotypes at more than one locus or\nhaplotypes of alleles on the same chromosome. \n\n"}
{"id": "0706.1062", "contents": "Title: Power-law distributions in empirical data Abstract: Power-law distributions occur in many situations of scientific interest and\nhave significant consequences for our understanding of natural and man-made\nphenomena. Unfortunately, the detection and characterization of power laws is\ncomplicated by the large fluctuations that occur in the tail of the\ndistribution -- the part of the distribution representing large but rare events\n-- and by the difficulty of identifying the range over which power-law behavior\nholds. Commonly used methods for analyzing power-law data, such as\nleast-squares fitting, can produce substantially inaccurate estimates of\nparameters for power-law distributions, and even in cases where such methods\nreturn accurate answers they are still unsatisfactory because they give no\nindication of whether the data obey a power law at all. Here we present a\nprincipled statistical framework for discerning and quantifying power-law\nbehavior in empirical data. Our approach combines maximum-likelihood fitting\nmethods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic\nand likelihood ratios. We evaluate the effectiveness of the approach with tests\non synthetic data and give critical comparisons to previous approaches. We also\napply the proposed methods to twenty-four real-world data sets from a range of\ndifferent disciplines, each of which has been conjectured to follow a power-law\ndistribution. In some cases we find these conjectures to be consistent with the\ndata while in others the power law is ruled out. \n\n"}
{"id": "0706.3435", "contents": "Title: Undercomplete Blind Subspace Deconvolution via Linear Prediction Abstract: We present a novel solution technique for the blind subspace deconvolution\n(BSSD) problem, where temporal convolution of multidimensional hidden\nindependent components is observed and the task is to uncover the hidden\ncomponents using the observation only. We carry out this task for the\nundercomplete case (uBSSD): we reduce the original uBSSD task via linear\nprediction to independent subspace analysis (ISA), which we can solve. As it\nhas been shown recently, applying temporal concatenation can also reduce uBSSD\nto ISA, but the associated ISA problem can easily become `high dimensional'\n[1]. The new reduction method circumvents this dimensionality problem. We\nperform detailed studies on the efficiency of the proposed technique by means\nof numerical simulations. We have found several advantages: our method can\nachieve high quality estimations for smaller number of samples and it can cope\nwith deeper temporal convolutions. \n\n"}
{"id": "0711.1612", "contents": "Title: Enhancing Sparsity by Reweighted L1 Minimization Abstract: It is now well understood that (1) it is possible to reconstruct sparse\nsignals exactly from what appear to be highly incomplete sets of linear\nmeasurements and (2) that this can be done by constrained L1 minimization. In\nthis paper, we study a novel method for sparse signal recovery that in many\nsituations outperforms L1 minimization in the sense that substantially fewer\nmeasurements are needed for exact recovery. The algorithm consists of solving a\nsequence of weighted L1-minimization problems where the weights used for the\nnext iteration are computed from the value of the current solution. We present\na series of experiments demonstrating the remarkable performance and broad\napplicability of this algorithm in the areas of sparse signal recovery,\nstatistical estimation, error correction and image processing. Interestingly,\nsuperior gains are also achieved when our method is applied to recover signals\nwith assumed near-sparsity in overcomplete representations--not by reweighting\nthe L1 norm of the coefficient sequence as is common, but by reweighting the L1\nnorm of the transformed object. An immediate consequence is the possibility of\nhighly efficient data acquisition protocols by improving on a technique known\nas compressed sensing. \n\n"}
{"id": "0801.4172", "contents": "Title: Computational aspects and applications of a new transform for solving\n  the complex exponentials approximation problem Abstract: Many real life problems can be reduced to the solution of a complex\nexponentials approximation problem which is usually ill posed. Recently a new\ntransform for solving this problem, formulated as a specific moments problem in\nthe plane, has been proposed in a theoretical framework. In this work some\ncomputational issues are addressed to make this new tool useful in practice. An\nalgorithm is developed and used to solve a Nuclear Magnetic Resonance\nspectrometry problem, two time series interpolation and extrapolation problems\nand a shape from moments problem. \n\n"}
{"id": "0802.2426", "contents": "Title: Controlled stratification for quantile estimation Abstract: In this paper we propose and discuss variance reduction techniques for the\nestimation of quantiles of the output of a complex model with random input\nparameters. These techniques are based on the use of a reduced model, such as a\nmetamodel or a response surface. The reduced model can be used as a control\nvariate; or a rejection method can be implemented to sample the realizations of\nthe input parameters in prescribed relevant strata; or the reduced model can be\nused to determine a good biased distribution of the input parameters for the\nimplementation of an importance sampling strategy. The different strategies are\nanalyzed and the asymptotic variances are computed, which shows the benefit of\nan adaptive controlled stratification method. This method is finally applied to\na real example (computation of the peak cladding temperature during a\nlarge-break loss of coolant accident in a nuclear reactor). \n\n"}
{"id": "0804.1399", "contents": "Title: On Estimation and Optimization of Mean Values of Bounded Variables Abstract: In this paper, we develop a general approach for probabilistic estimation and\noptimization. An explicit formula and a computational approach are established\nfor controlling the reliability of probabilistic estimation based on a mixed\ncriterion of absolute and relative errors. By employing the Chernoff-Hoeffding\nbound and the concept of sampling, the minimization of a probabilistic function\nis transformed into an optimization problem amenable for gradient descendent\nalgorithms. \n\n"}
{"id": "0804.2413", "contents": "Title: Bayesian Inference on Mixtures of Distributions Abstract: This survey covers state-of-the-art Bayesian techniques for the estimation of\nmixtures. It complements the earlier Marin, Mengersen and Robert (2005) by\nstudying new types of distributions, the multinomial, latent class and t\ndistributions. It also exhibits closed form solutions for Bayesian inference in\nsome discrete setups. Lastly, it sheds a new light on the computation of Bayes\nfactors via the approximation of Chib (1995). \n\n"}
{"id": "0804.3779", "contents": "Title: On Estimation of Finite Population Proportion Abstract: In this paper, we study the classical problem of estimating the proportion of\na finite population. First, we consider a fixed sample size method and derive\nan explicit sample size formula which ensures a mixed criterion of absolute and\nrelative errors. Second, we consider an inverse sampling scheme such that the\nsampling is continue until the number of units having a certain attribute\nreaches a threshold value or the whole population is examined. We have\nestablished a simple method to determine the threshold so that a prescribed\nrelative precision is guaranteed. Finally, we develop a multistage sampling\nscheme for constructing fixed-width confidence interval for the proportion of a\nfinite population. Powerful computational techniques are introduced to make it\npossible that the fixed-width confidence interval ensures prescribed level of\ncoverage probability. \n\n"}
{"id": "0804.3853", "contents": "Title: Modelling coloured residual noise in gravitational-wave signal\n  processing Abstract: We introduce a signal processing model for signals in non-white noise, where\nthe exact noise spectrum is a priori unknown. The model is based on a Student's\nt distribution and constitutes a natural generalization of the widely used\nnormal (Gaussian) model. This way, it allows for uncertainty in the noise\nspectrum, or more generally is also able to accommodate outliers (heavy-tailed\nnoise) in the data. Examples are given pertaining to data from gravitational\nwave detectors. \n\n"}
{"id": "0807.1106", "contents": "Title: Principal components analysis for sparsely observed correlated\n  functional data using a kernel smoothing approach Abstract: In this paper, we consider the problem of estimating the covariance kernel\nand its eigenvalues and eigenfunctions from sparse, irregularly observed, noise\ncorrupted and (possibly) correlated functional data. We present a method based\non pre-smoothing of individual sample curves through an appropriate kernel. We\nshow that the naive empirical covariance of the pre-smoothed sample curves\ngives highly biased estimator of the covariance kernel along its diagonal. We\nattend to this problem by estimating the diagonal and off-diagonal parts of the\ncovariance kernel separately. We then present a practical and efficient method\nfor choosing the bandwidth for the kernel by using an approximation to the\nleave-one-curve-out cross validation score. We prove that under standard\nregularity conditions on the covariance kernel and assuming i.i.d. samples, the\nrisk of our estimator, under $L^2$ loss, achieves the optimal nonparametric\nrate when the number of measurements per curve is bounded. We also show that\neven when the sample curves are correlated in such a way that the noiseless\ndata has a separable covariance structure, the proposed method is still\nconsistent and we quantify the role of this correlation in the risk of the\nestimator. \n\n"}
{"id": "0808.0853", "contents": "Title: Divergences Test Statistics for Discretely Observed Diffusion Processes Abstract: In this paper we propose the use of $\\phi$-divergences as test statistics to\nverify simple hypotheses about a one-dimensional parametric diffusion process\n$\\de X_t = b(X_t, \\theta)\\de t + \\sigma(X_t, \\theta)\\de W_t$, from discrete\nobservations $\\{X_{t_i}, i=0, ..., n\\}$ with $t_i = i\\Delta_n$, $i=0, 1, >...,\nn$, under the asymptotic scheme $\\Delta_n\\to0$, $n\\Delta_n\\to\\infty$ and\n$n\\Delta_n^2\\to 0$. The class of $\\phi$-divergences is wide and includes\nseveral special members like Kullback-Leibler, R\\'enyi, power and\n$\\alpha$-divergences. We derive the asymptotic distribution of the test\nstatistics based on $\\phi$-divergences. The limiting law takes different forms\ndepending on the regularity of $\\phi$. These convergence differ from the\nclassical results for independent and identically distributed random variables.\nNumerical analysis is used to show the small sample properties of the test\nstatistics in terms of estimated level and power of the test. \n\n"}
{"id": "0808.3511", "contents": "Title: Conditional probability based significance tests for sequential patterns\n  in multi-neuronal spike trains Abstract: In this paper we consider the problem of detecting statistically significant\nsequential patterns in multi-neuronal spike trains. These patterns are\ncharacterized by ordered sequences of spikes from different neurons with\nspecific delays between spikes. We have previously proposed a data mining\nscheme to efficiently discover such patterns which are frequent in the sense\nthat the count of non-overlapping occurrences of the pattern in the data stream\nis above a threshold. Here we propose a method to determine the statistical\nsignificance of these repeating patterns and to set the thresholds\nautomatically. The novelty of our approach is that we use a compound null\nhypothesis that includes not only models of independent neurons but also models\nwhere neurons have weak dependencies. The strength of interaction among the\nneurons is represented in terms of certain pair-wise conditional probabilities.\nWe specify our null hypothesis by putting an upper bound on all such\nconditional probabilities. We construct a probabilistic model that captures the\ncounting process and use this to calculate the mean and variance of the count\nfor any pattern. Using this we derive a test of significance for rejecting such\na null hypothesis. This also allows us to rank-order different significant\npatterns. We illustrate the effectiveness of our approach using spike trains\ngenerated from a non-homogeneous Poisson model with embedded dependencies. \n\n"}
{"id": "0809.1777", "contents": "Title: A Regularized Method for Selecting Nested Groups of Relevant Genes from\n  Microarray Data Abstract: Gene expression analysis aims at identifying the genes able to accurately\npredict biological parameters like, for example, disease subtyping or\nprogression. While accurate prediction can be achieved by means of many\ndifferent techniques, gene identification, due to gene correlation and the\nlimited number of available samples, is a much more elusive problem. Small\nchanges in the expression values often produce different gene lists, and\nsolutions which are both sparse and stable are difficult to obtain. We propose\na two-stage regularization method able to learn linear models characterized by\na high prediction performance. By varying a suitable parameter these linear\nmodels allow to trade sparsity for the inclusion of correlated genes and to\nproduce gene lists which are almost perfectly nested. Experimental results on\nsynthetic and microarray data confirm the interesting properties of the\nproposed method and its potential as a starting point for further biological\ninvestigations \n\n"}
{"id": "0809.4679", "contents": "Title: Multistage Estimation of Bounded-Variable Means Abstract: In this paper, we develop a multistage approach for estimating the mean of a\nbounded variable. We first focus on the multistage estimation of a binomial\nparameter and then generalize the estimation methods to the case of general\nbounded random variables. A fundamental connection between a binomial parameter\nand the mean of a bounded variable is established. Our multistage estimation\nmethods rigorously guarantee prescribed levels of precision and confidence. \n\n"}
{"id": "0811.3355", "contents": "Title: Approximate Bayesian computation (ABC) gives exact results under the\n  assumption of model error Abstract: Approximate Bayesian computation (ABC) or likelihood-free inference\nalgorithms are used to find approximations to posterior distributions without\nmaking explicit use of the likelihood function, depending instead on simulation\nof sample data sets from the model. In this paper we show that under the\nassumption of the existence of a uniform additive model error term, ABC\nalgorithms give exact results when sufficient summaries are used. This\ninterpretation allows the approximation made in many previous application\npapers to be understood, and should guide the choice of metric and tolerance in\nfuture work. ABC algorithms can be generalized by replacing the 0-1 cut-off\nwith an acceptance probability that varies with the distance of the simulated\ndata from the observed data. The acceptance density gives the distribution of\nthe error term, enabling the uniform error usually used to be replaced by a\ngeneral distribution. This generalization can also be applied to approximate\nMarkov chain Monte Carlo algorithms. In light of this work, ABC algorithms can\nbe seen as calibration techniques for implicit stochastic models, inferring\nparameter values in light of the computer model, data, prior beliefs about the\nparameter values, and any measurement or model errors. \n\n"}
{"id": "0811.3639", "contents": "Title: Zero-state Markov switching count-data models: an empirical assessment Abstract: In this study, a two-state Markov switching count-data model is proposed as\nan alternative to zero-inflated models to account for the preponderance of\nzeros sometimes observed in transportation count data, such as the number of\naccidents occurring on a roadway segment over some period of time. For this\naccident-frequency case, zero-inflated models assume the existence of two\nstates: one of the states is a zero-accident count state, in which accident\nprobabilities are so low that they cannot be statistically distinguished from\nzero, and the other state is a normal count state, in which counts can be\nnon-negative integers that are generated by some counting process, for example,\na Poisson or negative binomial. In contrast to zero-inflated models, Markov\nswitching models allow specific roadway segments to switch between the two\nstates over time. An important advantage of this Markov switching approach is\nthat it allows for the direct statistical estimation of the specific\nroadway-segment state (i.e., zero or count state) whereas traditional\nzero-inflated models do not. To demonstrate the applicability of this approach,\na two-state Markov switching negative binomial model (estimated with Bayesian\ninference) and standard zero-inflated negative binomial models are estimated\nusing five-year accident frequencies on Indiana interstate highway segments. It\nis shown that the Markov switching model is a viable alternative and results in\na superior statistical fit relative to the zero-inflated models. \n\n"}
{"id": "0811.3644", "contents": "Title: Markov switching multinomial logit model: an application to accident\n  injury severities Abstract: In this study, two-state Markov switching multinomial logit models are\nproposed for statistical modeling of accident injury severities. These models\nassume Markov switching in time between two unobserved states of roadway\nsafety. The states are distinct, in the sense that in different states accident\nseverity outcomes are generated by separate multinomial logit processes. To\ndemonstrate the applicability of the approach presented herein, two-state\nMarkov switching multinomial logit models are estimated for severity outcomes\nof accidents occurring on Indiana roads over a four-year time interval.\nBayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are\nused for model estimation. The estimated Markov switching models result in a\nsuperior statistical fit relative to the standard (single-state) multinomial\nlogit models. It is found that the more frequent state of roadway safety is\ncorrelated with better weather conditions. The less frequent state is found to\nbe correlated with adverse weather conditions. \n\n"}
{"id": "0901.0401", "contents": "Title: From Physics to Economics: An Econometric Example Using Maximum Relative\n  Entropy Abstract: Econophysics, is based on the premise that some ideas and methods from\nphysics can be applied to economic situations. We intend to show in this paper\nhow a physics concept such as entropy can be applied to an economic problem. In\nso doing, we demonstrate how information in the form of observable data and\nmoment constraints are introduced into the method of Maximum relative Entropy\n(MrE). A general example of updating with data and moments is shown. Two\nspecific econometric examples are solved in detail which can then be used as\ntemplates for real world problems. A numerical example is compared to a large\ndeviation solution which illustrates some of the advantages of the MrE method. \n\n"}
{"id": "0901.1038", "contents": "Title: Economic Models with Chaotic Money Exchange Abstract: This paper presents a novel study on gas-like models for economic systems.\nThe interacting agents and the amount of exchanged money at each trade are\nselected with different levels of randomness, from a purely random way to a\nmore chaotic one. Depending on the interaction rules, these statistical models\ncan present different asymptotic distributions of money in a community of\nindividuals with a closed economy. \n\n"}
{"id": "0902.0751", "contents": "Title: Gene ranking and biomarker discovery under correlation Abstract: Biomarker discovery and gene ranking is a standard task in genomic high\nthroughput analysis. Typically, the ordering of markers is based on a\nstabilized variant of the t-score, such as the moderated t or the SAM\nstatistic. However, these procedures ignore gene-gene correlations, which may\nhave a profound impact on the gene orderings and on the power of the subsequent\ntests.\n  We propose a simple procedure that adjusts gene-wise t-statistics to take\naccount of correlations among genes. The resulting correlation-adjusted\nt-scores (\"cat\" scores) are derived from a predictive perspective, i.e. as a\nscore for variable selection to discriminate group membership in two-class\nlinear discriminant analysis. In the absence of correlation the cat score\nreduces to the standard t-score. Moreover, using the cat score it is\nstraightforward to evaluate groups of features (i.e. gene sets). For\ncomputation of the cat score from small sample data we propose a shrinkage\nprocedure. In a comparative study comprising six different synthetic and\nempirical correlation structures we show that the cat score improves estimation\nof gene orderings and leads to higher power for fixed true discovery rate, and\nvice versa. Finally, we also illustrate the cat score by analyzing metabolomic\ndata.\n  The shrinkage cat score is implemented in the R package \"st\" available from\nURL http://cran.r-project.org/web/packages/st/ \n\n"}
{"id": "0902.0990", "contents": "Title: Directional Clustering Tests Based on Nearest Neighbor Contingency\n  Tables Abstract: Spatial interaction between two or more classes or species has important\nimplications in various fields and causes multivariate patterns such as\nsegregation or association. Segregation occurs when members of a class or\nspecies are more likely to be found near members of the same class or\nconspecifics; while association occurs when members of a class or species are\nmore likely to be found near members of another class or species. The null\npatterns considered are random labeling (RL) and complete spatial randomness\n(CSR) of points from two or more classes, which is called \\emph{CSR\nindependence}, henceforth. The clustering tests based on nearest neighbor\ncontingency tables (NNCTs) that are in use in literature are two-sided tests.\nIn this article, we consider the directional (i.e., one-sided) versions of the\ncell-specific NNCT-tests and introduce new directional NNCT-tests for the\ntwo-class case. We analyze the distributional properties; compare the empirical\nsignificant levels and empirical power estimates of the tests using extensive\nMonte Carlo simulations. We demonstrate that the new directional tests have\ncomparable performance with the currently available NNCT-tests in terms of\nempirical size and power. We use four example data sets for illustrative\npurposes and provide guidelines for using these NNCT-tests. \n\n"}
{"id": "0902.2808", "contents": "Title: Ultrametric Wavelet Regression of Multivariate Time Series: Application\n  to Colombian Conflict Analysis Abstract: We first pursue the study of how hierarchy provides a well-adapted tool for\nthe analysis of change. Then, using a time sequence-constrained hierarchical\nclustering, we develop the practical aspects of a new approach to wavelet\nregression. This provides a new way to link hierarchical relationships in a\nmultivariate time series data set with external signals. Violence data from the\nColombian conflict in the years 1990 to 2004 is used throughout. We conclude\nwith some proposals for further study on the relationship between social\nviolence and market forces, viz. between the Colombian conflict and the US\nnarcotics market. \n\n"}
{"id": "0902.3714", "contents": "Title: Target Detection via Network Filtering Abstract: A method of `network filtering' has been proposed recently to detect the\neffects of certain external perturbations on the interacting members in a\nnetwork. However, with large networks, the goal of detection seems a priori\ndifficult to achieve, especially since the number of observations available\noften is much smaller than the number of variables describing the effects of\nthe underlying network. Under the assumption that the network possesses a\ncertain sparsity property, we provide a formal characterization of the accuracy\nwith which the external effects can be detected, using a network filtering\nsystem that combines Lasso regression in a sparse simultaneous equation model\nwith simple residual analysis. We explore the implications of the technical\nconditions underlying our characterization, in the context of various network\ntopologies, and we illustrate our method using simulated data. \n\n"}
{"id": "0903.3002", "contents": "Title: Learning with Structured Sparsity Abstract: This paper investigates a new learning formulation called structured\nsparsity, which is a natural extension of the standard sparsity concept in\nstatistical learning and compressive sensing. By allowing arbitrary structures\non the feature set, this concept generalizes the group sparsity idea that has\nbecome popular in recent years. A general theory is developed for learning with\nstructured sparsity, based on the notion of coding complexity associated with\nthe structure. It is shown that if the coding complexity of the target signal\nis small, then one can achieve improved performance by using coding complexity\nregularization methods, which generalize the standard sparse regularization.\nMoreover, a structured greedy algorithm is proposed to efficiently solve the\nstructured sparsity problem. It is shown that the greedy algorithm\napproximately solves the coding complexity optimization problem under\nappropriate conditions. Experiments are included to demonstrate the advantage\nof structured sparsity over standard sparsity on some real applications. \n\n"}
{"id": "0904.2207", "contents": "Title: Delayed rejection schemes for efficient Markov-Chain Monte-Carlo\n  sampling of multimodal distributions Abstract: A number of problems in a variety of fields are characterised by target\ndistributions with a multimodal structure in which the presence of several\nisolated local maxima dramatically reduces the efficiency of Markov Chain Monte\nCarlo sampling algorithms. Several solutions, such as simulated tempering or\nthe use of parallel chains, have been proposed to facilitate the exploration of\nthe relevant parameter space. They provide effective strategies in the cases in\nwhich the dimension of the parameter space is small and/or the computational\ncosts are not a limiting factor. These approaches fail however in the case of\nhigh-dimensional spaces where the multimodal structure is induced by\ndegeneracies between regions of the parameter space. In this paper we present a\nfully Markovian way to efficiently sample this kind of distribution based on\nthe general Delayed Rejection scheme with an arbitrary number of steps, and\nprovide details for an efficient numerical implementation of the algorithm. \n\n"}
{"id": "0904.3646", "contents": "Title: Signed Chord Length Distribution. II Abstract: This paper continues description of applications of signed chord length\ndistribution started in part I (arXiv:0711.4734). It is shown simple relation\nbetween equation for some transfer integrals with source and target bodies and\ndifferent geometrical distributions for union of this bodies. The union of\ndisjoint bodies is always nonconvex object and for such a case derivatives of\ncorrelation function (used for definition of signed radii and chord lengths\ndistributions) always produce (quasi)densities with negative values. Many\nequations used in this part are direct consequences of analogue formulas in\npart I. \n\n"}
{"id": "0906.3287", "contents": "Title: The developmental dynamics of terrorist organizations Abstract: We identify robust statistical patterns in the frequency and severity of\nviolent attacks by terrorist organizations as they grow and age. Using\ngroup-level static and dynamic analyses of terrorist events worldwide from\n1968-2008 and a simulation model of organizational dynamics, we show that the\nproduction of violent events tends to accelerate with increasing size and\nexperience. This coupling of frequency, experience and size arises from a\nfundamental positive feedback loop in which attacks lead to growth which leads\nto increased production of new attacks. In contrast, event severity is\nindependent of both size and experience. Thus larger, more experienced\norganizations are more deadly because they attack more frequently, not because\ntheir attacks are more deadly, and large events are equally likely to come from\nlarge and small organizations. These results hold across political ideologies\nand time, suggesting that the frequency and severity of terrorism may be\nconstrained by fundamental processes. \n\n"}
{"id": "0907.3166", "contents": "Title: Checking election outcome accuracy Post-election audit sampling Abstract: This article\n  * provides an overview of post-election audit sampling research and compares\nvarious approaches to calculating post-election audit sample sizes, focusing on\nrisklimiting audits,\n  * discusses fundamental concepts common to all risk-limiting post-election\naudits, presenting new margin error bounds, sampling weights and sampling\nprobabilities that improve upon existing approaches and work for any size audit\nunit and for single or multi-winner election contests,\n  * provides two new simple formulas for estimating post-election audit sample\nsizes in cases when detailed data, expertise, or tools are not available,\n  * summarizes four improved methods for calculating risk-limiting election\naudit sample sizes, showing how to apply precise margin error bounds to improve\nthe accuracy and efficacy of existing methods, and\n  * discusses sampling mistakes that reduce post-election audit effectiveness. \n\n"}
{"id": "0907.4915", "contents": "Title: Nonasymptotic bounds on the estimation error for regenerative MCMC\n  algorithms Abstract: MCMC methods are used in Bayesian statistics not only to sample from\nposterior distributions but also to estimate expectations. Underlying functions\nare most often defined on a continuous state space and can be unbounded. We\nconsider a regenerative setting and Monte Carlo estimators based on i.i.d.\nblocks of a Markov chain trajectory. The main result is an inequality for the\nmean square error. We also consider confidence bounds. We first derive the\nresults in terms of the asymptotic variance and then bound the asymptotic\nvariance for both uniformly ergodic and geometrically ergodic Markov chains. \n\n"}
{"id": "0908.3882", "contents": "Title: Learning networks from high dimensional binary data: An application to\n  genomic instability data Abstract: Genomic instability, the propensity of aberrations in chromosomes, plays a\ncritical role in the development of many diseases. High throughput genotyping\nexperiments have been performed to study genomic instability in diseases. The\noutput of such experiments can be summarized as high dimensional binary\nvectors, where each binary variable records aberration status at one marker\nlocus. It is of keen interest to understand how these aberrations interact with\neach other. In this paper, we propose a novel method, \\texttt{LogitNet}, to\ninfer the interactions among aberration events. The method is based on\npenalized logistic regression with an extension to account for spatial\ncorrelation in the genomic instability data. We conduct extensive simulation\nstudies and show that the proposed method performs well in the situations\nconsidered. Finally, we illustrate the method using genomic instability data\nfrom breast cancer samples. \n\n"}
{"id": "0908.4334", "contents": "Title: One and two side generalisations of the log-Normal distribution by means\n  of a new product definition Abstract: In this manuscript we introduce a generalisation of the log-Normal\ndistribution that is inspired by a modification of the Kaypten multiplicative\nprocess using the $q$-product of Borges [Physica A \\textbf{340}, 95 (2004)].\nDepending on the value of q the distribution increases the tail for small (when\n$q<1$) or large (when $q>1$) values of the variable upon analysis. The usual\nlog-Normal distribution is retrieved when $q=1$. The main statistical features\nof this distribution are presented as well as a related random number\ngenerators and tables of quantiles of the Kolmogorov-Smirnov. Lastly, we\nillustrate the application of this distribution studying the adjustment of a\nset of variables of biological and financial origin. \n\n"}
{"id": "0910.2497", "contents": "Title: Maximum entropy Edgeworth estimates of the number of integer points in\n  polytopes Abstract: Abstract: The number of points $x=(x_1 ,x_2 ,...x_n)$ that lie in an integer\ncube $C$ in $R^n$ and satisfy the constraints $\\sum_j h_{ij}(x_j )=s_i ,1\\le\ni\\le d$ is approximated by an Edgeworth-corrected Gaussian formula based on the\nmaximum entropy density $p$ on $x \\in C$, that satisfies $E\\sum_j h_{ij}(x_j\n)=s_i ,1\\le i\\le d$. Under $p$, the variables $X_1 ,X_2 ,...X_n $ are\nindependent with densities of exponential form. Letting $S_i$ denote the random\nvariable $\\sum_j h_{ij}(X_j )$, conditional on $S=s, X$ is uniformly\ndistributed over the integers in $C$ that satisfy $S=s$. The number of points\nin $C$ satisfying $S=s$ is $p \\{S=s\\}\\exp (I(p))$ where $I(p)$ is the entropy\nof the density $p$. We estimate $p \\{S=s\\}$ by $p_Z(s)$, the density at $s$ of\nthe multivariate Gaussian $Z$ with the same first two moments as $S$; and when\n$d$ is large we use in addition an Edgeworth factor that requires the first\nfour moments of $S$ under $p$. The asymptotic validity of the\nEdgeworth-corrected estimate is proved and demonstrated for counting\ncontingency tables with given row and column sums as the number of rows and\ncolumns approaches infinity, and demonstrated for counting the number of graphs\nwith a given degree sequence, as the number of vertices approaches infinity. \n\n"}
{"id": "0912.1628", "contents": "Title: KF-CS: Compressive Sensing on Kalman Filtered Residual Abstract: We consider the problem of recursively reconstructing time sequences of\nsparse signals (with unknown and time-varying sparsity patterns) from a limited\nnumber of linear incoherent measurements with additive noise. The idea of our\nproposed solution, KF CS-residual (KF-CS) is to replace compressed sensing (CS)\non the observation by CS on the Kalman filtered (KF) observation residual\ncomputed using the previous estimate of the support. KF-CS error stability over\ntime is studied. Simulation comparisons with CS and LS-CS are shown. \n\n"}
{"id": "1001.0279", "contents": "Title: Regularization for Matrix Completion Abstract: We consider the problem of reconstructing a low rank matrix from noisy\nobservations of a subset of its entries. This task has applications in\nstatistical learning, computer vision, and signal processing. In these\ncontexts, \"noise\" generically refers to any contribution to the data that is\nnot captured by the low-rank model. In most applications, the noise level is\nlarge compared to the underlying signal and it is important to avoid\noverfitting. In order to tackle this problem, we define a regularized cost\nfunction well suited for spectral reconstruction methods. Within a random noise\nmodel, and in the large system limit, we prove that the resulting accuracy\nundergoes a phase transition depending on the noise level and on the fraction\nof observed entries. The cost function can be minimized using OPTSPACE (a\nmanifold gradient descent algorithm). Numerical simulations show that this\napproach is competitive with state-of-the-art alternatives. \n\n"}
{"id": "1001.2187", "contents": "Title: Skewness of maximum likelihood estimators in dispersion models Abstract: We introduce the dispersion models with a regression structure to extend the\ngeneralized linear models, the exponential family nonlinear models (Cordeiro\nand Paula, 1989) and the proper dispersion models (J{\\o}rgensen, 1997a). We\nprovide a matrix expression for the skewness of the maximum likelihood\nestimators of the regression parameters in dispersion models. The formula is\nsuitable for computer implementation and can be applied for several important\nsubmodels discussed in the literature. Expressions for the skewness of the\nmaximum likelihood estimators of the precision and dispersion parameters are\nalso derived. In particular, our results extend previous formulas obtained by\nCordeiro and Cordeiro (2001) and Cavalcanti et al. (2009). A simulation study\nis perfomed to show the practice importance of our results. \n\n"}
{"id": "1001.4351", "contents": "Title: Analytical continuation of imaginary axis data using maximum entropy Abstract: We study the maximum entropy (MaxEnt) approach for analytical continuation of\nspectral data from imaginary times to real frequencies. The total error is\ndivided in a statistical error, due to the noise in the input data, and a\nsystematic error, due to deviations of the default function, used in the MaxEnt\napproach, from the exact spectrum. We find that the MaxEnt approach in its\nclassical formulation can lead to a nonoptimal balance between the two types of\nerrors, leading to an unnecessary large statistical error. The statistical\nerror can be reduced by splitting up the data in several batches, performing a\nMaxEnt calculation for each batch and averaging. This can outweigh an increase\nin the systematic error resulting from this approach. The output from the\nMaxEnt result can be used as a default function for a new MaxEnt calculation.\nSuch iterations often lead to worse results due to an increase in the\nstatistical error. By splitting up the data in batches, the statistical error\nis reduced and and the increase resulting from iterations can be outweighed by\na decrease in the systematic error. Finally we consider a linearized version to\nobtain a better understanding of the method. \n\n"}
{"id": "1002.3128", "contents": "Title: Structured, sparse regression with application to HIV drug resistance Abstract: We introduce a new version of forward stepwise regression. Our modification\nfinds solutions to regression problems where the selected predictors appear in\na structured pattern, with respect to a predefined distance measure over the\ncandidate predictors. Our method is motivated by the problem of predicting\nHIV-1 drug resistance from protein sequences. We find that our method improves\nthe interpretability of drug resistance while producing comparable predictive\naccuracy to standard methods. We also demonstrate our method in a simulation\nstudy and present some theoretical results and connections. \n\n"}
{"id": "1002.4658", "contents": "Title: Principal Component Analysis with Contaminated Data: The High\n  Dimensional Case Abstract: We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero. \n\n"}
{"id": "1002.4775", "contents": "Title: A copula based approach to adaptive sampling Abstract: Our article is concerned with adaptive sampling schemes for Bayesian\ninference that update the proposal densities using previous iterates. We\nintroduce a copula based proposal density which is made more efficient by\ncombining it with antithetic variable sampling. We compare the copula based\nproposal to an adaptive proposal density based on a multivariate mixture of\nnormals and an adaptive random walk Metropolis proposal. We also introduce a\nrefinement of the random walk proposal which performs better for multimodal\ntarget distributions. We compare the sampling schemes using challenging but\nrealistic models and priors applied to real data examples. The results show\nthat for the examples studied, the adaptive independent \\MH{} proposals are\nmuch more efficient than the adaptive random walk proposals and that in general\nthe copula based proposal has the best acceptance rates and lowest\ninefficiencies. \n\n"}
{"id": "1003.0275", "contents": "Title: Statistical inference for time-changed L\\'{e}vy processes via composite\n  characteristic function estimation Abstract: In this article, the problem of semi-parametric inference on the parameters\nof a multidimensional L\\'{e}vy process $L_t$ with independent components based\non the low-frequency observations of the corresponding time-changed L\\'{e}vy\nprocess $L_{\\mathcal{T}(t)}$, where $\\mathcal{T}$ is a nonnegative,\nnondecreasing real-valued process independent of $L_t$, is studied. We show\nthat this problem is closely related to the problem of composite function\nestimation that has recently gotten much attention in statistical literature.\nUnder suitable identifiability conditions, we propose a consistent estimate for\nthe L\\'{e}vy density of $L_t$ and derive the uniform as well as the pointwise\nconvergence rates of the estimate proposed. Moreover, we prove that the rates\nobtained are optimal in a minimax sense over suitable classes of time-changed\nL\\'{e}vy models. Finally, we present a simulation study showing the performance\nof our estimation algorithm in the case of time-changed Normal Inverse Gaussian\n(NIG) L\\'{e}vy processes. \n\n"}
{"id": "1004.0637", "contents": "Title: Sales Distribution of Consumer Electronics Abstract: Using the uniform most powerful unbiased test, we observed the sales\ndistribution of consumer electronics in Japan on a daily basis and report that\nit follows both a lognormal distribution and a power-law distribution and\ndepends on the state of the market. We show that these switches occur quite\noften. The underlying sales dynamics found between both periods nicely matched\na multiplicative process. However, even though the multiplicative term in the\nprocess displays a size-dependent relationship when a steady lognormal\ndistribution holds, it shows a size-independent relationship when the power-law\ndistribution holds. This difference in the underlying dynamics is responsible\nfor the difference in the two observed distributions. \n\n"}
{"id": "1004.2995", "contents": "Title: Optimal selection of reduced rank estimators of high-dimensional\n  matrices Abstract: We introduce a new criterion, the Rank Selection Criterion (RSC), for\nselecting the optimal reduced rank estimator of the coefficient matrix in\nmultivariate response regression models. The corresponding RSC estimator\nminimizes the Frobenius norm of the fit plus a regularization term proportional\nto the number of parameters in the reduced rank model. The rank of the RSC\nestimator provides a consistent estimator of the rank of the coefficient\nmatrix; in general, the rank of our estimator is a consistent estimate of the\neffective rank, which we define to be the number of singular values of the\ntarget matrix that are appropriately large. The consistency results are valid\nnot only in the classic asymptotic regime, when $n$, the number of responses,\nand $p$, the number of predictors, stay bounded, and $m$, the number of\nobservations, grows, but also when either, or both, $n$ and $p$ grow, possibly\nmuch faster than $m$. We establish minimax optimal bounds on the mean squared\nerrors of our estimators. Our finite sample performance bounds for the RSC\nestimator show that it achieves the optimal balance between the approximation\nerror and the penalty term. Furthermore, our procedure has very low\ncomputational complexity, linear in the number of candidate models, making it\nparticularly appealing for large scale problems. We contrast our estimator with\nthe nuclear norm penalized least squares (NNP) estimator, which has an\ninherently higher computational complexity than RSC, for multivariate\nregression models. We show that NNP has estimation properties similar to those\nof RSC, albeit under stronger conditions. However, it is not as parsimonious as\nRSC. We offer a simple correction of the NNP estimator which leads to\nconsistent rank estimation. \n\n"}
{"id": "1004.3782", "contents": "Title: On Practical Algorithms for Entropy Estimation and the Improved Sample\n  Complexity of Compressed Counting Abstract: Estimating the p-th frequency moment of data stream is a very heavily studied\nproblem. The problem is actually trivial when p = 1, assuming the strict\nTurnstile model. The sample complexity of our proposed algorithm is essentially\nO(1) near p=1. This is a very large improvement over the previously believed\nO(1/eps^2) bound. The proposed algorithm makes the long-standing problem of\nentropy estimation an easy task, as verified by the experiments included in the\nappendix. \n\n"}
{"id": "1004.4522", "contents": "Title: Toy Model for Large Non-Symmetric Random Matrices Abstract: Non-symmetric rectangular correlation matrices occur in many problems in\neconomics. We test the method of extracting statistically meaningful\ncorrelations between input and output variables of large dimensionality and\nbuild a toy model for artificially included correlations in large random time\nseries.The results are then applied to analysis of polish macroeconomic data\nand can be used as an alternative to classical cointegration approach. \n\n"}
{"id": "1005.0366", "contents": "Title: Pattern Alternating Maximization Algorithm for Missing Data in Large P,\n  Small N Problems Abstract: We propose a new and computationally efficient algorithm for maximizing the\nobserved log-likelihood for a multivariate normal data matrix with missing\nvalues. We show that our procedure based on iteratively regressing the missing\non the observed variables, generalizes the standard EM algorithm by alternating\nbetween different complete data spaces and performing the E-Step incrementally.\nIn this non-standard setup we prove numerical convergence to a stationary point\nof the observed log-likelihood.\n  For high-dimensional data, where the number of variables may greatly exceed\nsample size, we add a Lasso penalty in the regression part of our algorithm and\nperform coordinate descent approximations. This leads to a computationally very\nattractive technique with sparse regression coefficients for missing data\nimputation. Simulations and results on four microarray datasets show that the\nnew method often outperforms other imputation techniques as k-nearest neighbors\nimputation, nuclear norm minimization or a penalized likelihood approach with\nan l1-penalty on the inverse covariance matrix. \n\n"}
{"id": "1005.4274", "contents": "Title: This is SPIRAL-TAP: Sparse Poisson Intensity Reconstruction ALgorithms -\n  Theory and Practice Abstract: The observations in many applications consist of counts of discrete events,\nsuch as photons hitting a detector, which cannot be effectively modeled using\nan additive bounded or Gaussian noise model, and instead require a Poisson\nnoise model. As a result, accurate reconstruction of a spatially or temporally\ndistributed phenomenon (f*) from Poisson data (y) cannot be effectively\naccomplished by minimizing a conventional penalized least-squares objective\nfunction. The problem addressed in this paper is the estimation of f* from y in\nan inverse problem setting, where (a) the number of unknowns may potentially be\nlarger than the number of observations and (b) f* admits a sparse\napproximation. The optimization formulation considered in this paper uses a\npenalized negative Poisson log-likelihood objective function with nonnegativity\nconstraints (since Poisson intensities are naturally nonnegative). In\nparticular, the proposed approach incorporates key ideas of using separable\nquadratic approximations to the objective function at each iteration and\npenalization terms related to l1 norms of coefficient vectors, total variation\nseminorms, and partition-based multiscale estimation methods. \n\n"}
{"id": "1007.4603", "contents": "Title: Bayesian Symbol Detection in Wireless Relay Networks via Likelihood-Free\n  Inference Abstract: This paper presents a general stochastic model developed for a class of\ncooperative wireless relay networks, in which imperfect knowledge of the\nchannel state information at the destination node is assumed. The framework\nincorporates multiple relay nodes operating under general known non-linear\nprocessing functions. When a non-linear relay function is considered, the\nlikelihood function is generally intractable resulting in the maximum\nlikelihood and the maximum a posteriori detectors not admitting closed form\nsolutions. We illustrate our methodology to overcome this intractability under\nthe example of a popular optimal non-linear relay function choice and\ndemonstrate how our algorithms are capable of solving the previously\nintractable detection problem. Overcoming this intractability involves\ndevelopment of specialised Bayesian models. We develop three novel algorithms\nto perform detection for this Bayesian model, these include a Markov chain\nMonte Carlo Approximate Bayesian Computation (MCMC-ABC) approach; an Auxiliary\nVariable MCMC (MCMC-AV) approach; and a Suboptimal Exhaustive Search Zero\nForcing (SES-ZF) approach. Finally, numerical examples comparing the symbol\nerror rate (SER) performance versus signal to noise ratio (SNR) of the three\ndetection algorithms are studied in simulated examples. \n\n"}
{"id": "1008.0149", "contents": "Title: Bayesian Cointegrated Vector Autoregression models incorporating\n  Alpha-stable noise for inter-day price movements via Approximate Bayesian\n  Computation Abstract: We consider a statistical model for pairs of traded assets, based on a\nCointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR\nmodels to incorporate estimation of model parameters in the presence of price\nseries level shifts which are not accurately modeled in the standard Gaussian\nerror correction model (ECM) framework. This involves developing a novel matrix\nvariate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and\nAlpha-stable errors inter-day in the ECM framework. To achieve this we derive a\nnovel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)\nrepresentation of Alpha-stable inter-day innovations. These results are\ngeneralized to asymmetric models for the innovation noise at inter-day\nboundaries allowing for skewed Alpha-stable models.\n  Our proposed model and sampling methodology is general, incorporating the\ncurrent literature on Gaussian models as a special subclass and also allowing\nfor price series level shifts either at random estimated time points or known a\npriori time points. We focus analysis on regularly observed non-Gaussian level\nshifts that can have significant effect on estimation performance in\nstatistical models failing to account for such level shifts, such as at the\nclose and open of markets. We compare the estimation accuracy of our model and\nestimation approach to standard frequentist and Bayesian procedures for CVAR\nmodels when non-Gaussian price series level shifts are present in the\nindividual series, such as inter-day boundaries. We fit a bi-variate\nAlpha-stable model to the inter-day jumps and model the effect of such jumps on\nestimation of matrix-variate CVAR model parameters using the likelihood based\nJohansen procedure and a Bayesian estimation. We illustrate our model and the\ncorresponding estimation procedures we develop on both synthetic and actual\ndata. \n\n"}
{"id": "1008.0705", "contents": "Title: On Estimating the Ability of NBA Players Abstract: This paper introduces a new model and methodology for estimating the ability\nof NBA players. The main idea is to directly measure how good a player is by\ncomparing how their team performs when they are on the court as opposed to when\nthey are off it. This is achieved in a such a way as to control for the\nchanging abilities of the other players on court at different times during a\nmatch. The new method uses multiple seasons' data in a structured way to\nestimate player ability in an isolated season, measuring separately defensive\nand offensive merit as well as combining these to give an overall rating. The\nuse of game statistics in predicting player ability will be considered. Results\nusing data from the 2008/9 season suggest that LeBron James, who won the NBA\nMVP award, was the best overall player. The best defensive player was Lamar\nOdom and the best rookie was Russell Westbrook, neither of whom won an NBA\naward that season. The results further indicate that whilst the\nfrequently-reported game statistics provide some information on offensive\nability, they do not perform well in the prediction of defensive ability. \n\n"}
{"id": "1009.3203", "contents": "Title: Intrinsic Inference on the Mean Geodesic of Planar Shapes and Tree\n  Discrimination by Leaf Growth Abstract: For planar landmark based shapes, taking into account the non-Euclidean\ngeometry of the shape space, a statistical test for a common mean first\ngeodesic principal component (GPC) is devised. It rests on one of two\nasymptotic scenarios, both of which are identical in a Euclidean geometry. For\nboth scenarios, strong consistency and central limit theorems are established,\nalong with an algorithm for the computation of a Ziezold mean geodesic. In\napplication, this allows to verify the geodesic hypothesis for leaf growth of\nCanadian black poplars and to discriminate genetically different trees by\nobservations of leaf shape growth over brief time intervals. With a test based\non Procrustes tangent space coordinates, not involving the shape space's\ncurvature, neither can be achieved. \n\n"}
{"id": "1010.0124", "contents": "Title: A model selection approach to genome wide association studies Abstract: For the vast majority of genome wide association studies (GWAS) published so\nfar, statistical analysis was performed by testing markers individually. In\nthis article we present some elementary statistical considerations which\nclearly show that in case of complex traits the approach based on multiple\nregression or generalized linear models is preferable to multiple testing. We\nintroduce a model selection approach to GWAS based on modifications of Bayesian\nInformation Criterion (BIC) and develop some simple search strategies to deal\nwith the huge number of potential models. Comprehensive simulations based on\nreal SNP data confirm that model selection has larger power than multiple\ntesting to detect causal SNPs in complex models. On the other hand multiple\ntesting has substantial problems with proper ranking of causal SNPs and tends\nto detect a certain number of false positive SNPs, which are not linked to any\nof the causal mutations. We show that this behavior is typical in GWAS for\ncomplex traits and can be explained by an aggregated influence of many small\nrandom sample correlations between genotypes of a SNP under investigation and\nother causal SNPs. We believe that our findings at least partially explain\nproblems with low power and nonreplicability of results in many real data GWAS.\nFinally, we discuss the advantages of our model selection approach in the\ncontext of real data analysis, where we consider publicly available gene\nexpression data as traits for individuals from the HapMap project. \n\n"}
{"id": "1010.2737", "contents": "Title: Identification and well-posedness in a class of nonparametric problems Abstract: This is a companion note to Zinde-Walsh (2010), arXiv:1009.4217v1[MATH.ST],\nto clarify and extend results on identification in a number of problems that\nlead to a system of convolution equations. Examples include identification of\nthe distribution of mismeasured variables, of a nonparametric regression\nfunction under Berkson type measurement error, some nonparametric panel data\nmodels, etc. The reason that identification in different problems can be\nconsidered in one approach is that they lead to the same system of convolution\nequations; moreover the solution can be given under more general assumptions\nthan those usually considered, by examining these equations in spaces of\ngeneralized functions. An important issue that did not receive sufficient\nattention is that of well-posedness. This note gives conditions under which\nwell-posedness obtains, an example that demonstrates that when well-posedness\ndoes not hold functions that are far apart can give rise to observable\narbitrarily close functions and discusses misspecification and estimation from\nthe stand-point of well-posedness. \n\n"}
{"id": "1010.5223", "contents": "Title: Good, great, or lucky? Screening for firms with sustained superior\n  performance using heavy-tailed priors Abstract: This paper examines historical patterns of ROA (return on assets) for a\ncohort of 53,038 publicly traded firms across 93 countries, measured over the\npast 45 years. Our goal is to screen for firms whose ROA trajectories suggest\nthat they have systematically outperformed their peer groups over time. Such a\nproject faces at least three statistical difficulties: adjustment for relevant\ncovariates, massive multiplicity, and longitudinal dependence. We conclude\nthat, once these difficulties are taken into account, demonstrably superior\nperformance appears to be quite rare. We compare our findings with other recent\nmanagement studies on the same subject, and with the popular literature on\ncorporate success. Our methodological contribution is to propose a new class of\npriors for use in large-scale simultaneous testing. These priors are based on\nthe hypergeometric inverted-beta family, and have two main attractive features:\nheavy tails and computational tractability. The family is a four-parameter\ngeneralization of the normal/inverted-beta prior, and is the natural conjugate\nprior for shrinkage coefficients in a hierarchical normal model. Our results\nemphasize the usefulness of these heavy-tailed priors in large multiple-testing\nproblems, as they have a mild rate of tail decay in the marginal likelihood\n$m(y)$---a property long recognized to be important in testing. \n\n"}
{"id": "1011.0748", "contents": "Title: Response of double-auction markets to instantaneous Selling-Buying\n  signals with stochastic Bid-Ask spread Abstract: Statistical properties of order-driven double-auction markets with Bid-Ask\nspread are investigated through the dynamical quantities such as response\nfunction. We first attempt to utilize the so-called {\\it\nMadhavan-Richardson-Roomans model} (MRR for short) to simulate the stochastic\nprocess of the price-change in empirical data sets (say, EUR/JPY or USD/JPY\nexchange rates) in which the Bid-Ask spread fluctuates in time. We find that\nthe MRR theory apparently fails to simulate so much as the qualitative\nbehaviour ('non-monotonic' behaviour) of the response function $R(l)$ ($l$\ndenotes the difference of times at which the response function is evaluated)\ncalculated from the data. Especially, we confirm that the stochastic nature of\nthe Bid-Ask spread causes apparent deviations from a linear relationship\nbetween the $R(l)$ and the auto-correlation function $C(l)$, namely, $R(l)\n\\propto -C(l)$. To make the microscopic model of double-auction markets having\nstochastic Bid-Ask spread, we use the minority game with a finite market\nhistory length and find numerically that appropriate extension of the game\nshows quite similar behaviour of the response function to the empirical\nevidence. We also reveal that the minority game modeling with the adaptive\n('annealed') look-up table reproduces the non-linear relationship $R(l) \\propto\n-f(C(l))$ ($f(x)$ stands for a non-linear function leading to\n'$\\lambda$-shapes') more effectively than the fixed (`quenched') look-up table\ndoes. \n\n"}
{"id": "1011.4916", "contents": "Title: Fast Bivariate Penalized Splines: the Sandwich Smoother Abstract: We propose a fast penalized spline method for bivariate smoothing. Univariate\nP-spline smoothers (Eilers and Marx, 1996) are applied simultaneously along\nboth coordinates. The new smoother has a sandwich form which suggested the name\n\"sandwich smoother\" to a referee. The sandwich smoother has a tensor product\nstructure that simplifies an asymptotic analysis and it can be fast computed.\nWe derive a local central limit theorem for the sandwich smoother, with simple\nexpressions for the asymptotic bias and variance, by showing that the sandwich\nsmoother is asymptotically equivalent to a bivariate kernel regression\nestimator with a product kernel. As far as we are aware, this is the first\ncentral limit theorem for a bivariate spline estimator of any type. Our\nsimulation study shows that the sandwich smoother is orders of magnitude faster\nto compute than other bivariate spline smoothers, even when the latter are\ncomputed using a fast GLAM (Generalized Linear Array Model) algorithm, and\ncomparable to them in terms of mean squared integrated errors. We extend the\nsandwich smoother to array data of higher dimensions, where a GLAM algorithm\nimproves the computational speed of the sandwich smoother. One important\napplication of the sandwich smoother is to estimate covariance functions in\nfunctional data analysis. In this application, our numerical results show that\nthe sandwich smoother is orders of magnitude faster than local linear\nregression. The speed of the sandwich formula is important because functional\ndata sets are becoming quite large. \n\n"}
{"id": "1012.6033", "contents": "Title: Large-scale interval and point estimates from an empirical Bayes\n  extension of confidence posteriors Abstract: The proposed approach extends the confidence posterior distribution to the\nsemi-parametric empirical Bayes setting. Whereas the Bayesian posterior is\ndefined in terms of a prior distribution conditional on the observed data, the\nconfidence posterior is defined such that the probability that the parameter\nvalue lies in any fixed subset of parameter space, given the observed data, is\nequal to the coverage rate of the corresponding confidence interval. A\nconfidence posterior that has correct frequentist coverage at each fixed\nparameter value is combined with the estimated local false discovery rate to\nyield a parameter distribution from which interval and point estimates are\nderived within the framework of minimizing expected loss. The point estimates\nexhibit suitable shrinkage toward the null hypothesis value, making them\npractical for automatically ranking features in order of priority. The\ncorresponding confidence intervals are also shrunken and tend to be much\nshorter than their fixed-parameter counterparts, as illustrated with gene\nexpression data. Further, simulations confirm a theoretical argument that the\nshrunken confidence intervals cover the parameter at a higher-than-nominal\nfrequency. \n\n"}
{"id": "1101.3824", "contents": "Title: Series Expansion for Interference in Wireless Networks Abstract: The spatial correlations in transmitter node locations introduced by common\nmultiple access protocols makes the analysis of interference, outage, and other\nrelated metrics in a wireless network extremely difficult. Most works therefore\nassume that nodes are distributed either as a Poisson point process (PPP) or a\ngrid, and utilize the independence properties of the PPP (or the regular\nstructure of the grid) to analyze interference, outage and other metrics.\nBut,the independence of node locations makes the PPP a dubious model for\nnontrivial MACs which intentionally introduce correlations, e.g. spatial\nseparation, while the grid is too idealized to model real networks. In this\npaper, we introduce a new technique based on the factorial moment expansion of\nfunctionals of point processes to analyze functions of interference, in\nparticular outage probability. We provide a Taylor-series type expansion of\nfunctions of interference, wherein increasing the number of terms in the series\nprovides a better approximation at the cost of increased complexity of\ncomputation. Various examples illustrate how this new approach can be used to\nfind outage probability in both Poisson and non-Poisson wireless networks. \n\n"}
{"id": "1101.4368", "contents": "Title: Inferences in Bayesian variable selection problems with large model\n  spaces Abstract: An important aspect of Bayesian model selection is how to deal with huge\nmodel spaces, since exhaustive enumeration of all the models entertained is\nunfeasible and inferences have to be based on the very small proportion of\nmodels visited. This is the case for the variable selection problem, with a\nmoderate to large number of possible explanatory variables being considered in\nthis paper. We review some of the strategies proposed in the literature and\nargue that inferences based on empirical frequencies via Markov Chain Monte\nCarlo sampling of the posterior distribution outperforms recently proposed\nsearching methods. We give a plausible yet very simple explanation of this\neffect, showing that estimators based on frequencies are unbiased. The results\nobtained in two illustrative examples provide strong evidence in favor of our\narguments. \n\n"}
{"id": "1101.4373", "contents": "Title: Statistical Multiresolution Dantzig Estimation in Imaging: Fundamental\n  Concepts and Algorithmic Framework Abstract: In this paper we are concerned with fully automatic and locally adaptive\nestimation of functions in a \"signal + noise\"-model where the regression\nfunction may additionally be blurred by a linear operator, e.g. by a\nconvolution. To this end, we introduce a general class of statistical\nmultiresolution estimators and develop an algorithmic framework for computing\nthose. By this we mean estimators that are defined as solutions of convex\noptimization problems with supremum-type constraints. We employ a combination\nof the alternating direction method of multipliers with Dykstra's algorithm for\ncomputing orthogonal projections onto intersections of convex sets and prove\nnumerical convergence. The capability of the proposed method is illustrated by\nvarious examples from imaging and signal detection. \n\n"}
{"id": "1101.4577", "contents": "Title: Bayesian Variable Selection for Probit Mixed Models Applied to Gene\n  Selection Abstract: In computational biology, gene expression datasets are characterized by very\nfew individual samples compared to a large number of measurements per sample.\nThus, it is appealing to merge these datasets in order to increase the number\nof observations and diversify the data, allowing a more reliable selection of\ngenes relevant to the biological problem. Besides, the increased size of a\nmerged dataset facilitates its re-splitting into training and validation sets.\nThis necessitates the introduction of the dataset as a random effect. In this\ncontext, extending a work of Lee et al. (2003), a method is proposed to select\nrelevant variables among tens of thousands in a probit mixed regression model,\nconsidered as part of a larger hierarchical Bayesian model. Latent variables\nare used to identify subsets of selected variables and the grouping (or\nblocking) technique of Liu (1994) is combined with a Metropolis-within-Gibbs\nalgorithm (Robert and Casella 2004). The method is applied to a merged dataset\nmade of three individual gene expression datasets, in which tens of thousands\nof measurements are available for each of several hundred human breast cancer\nsamples. Even for this large dataset comprised of around 20000 predictors, the\nmethod is shown to be efficient and feasible. As an illustration, it is used to\nselect the most important genes that characterize the estrogen receptor status\nof patients with breast cancer. \n\n"}
{"id": "1101.5734", "contents": "Title: Recursive $\\ell_{1,\\infty}$ Group lasso Abstract: We introduce a recursive adaptive group lasso algorithm for real-time\npenalized least squares prediction that produces a time sequence of optimal\nsparse predictor coefficient vectors. At each time index the proposed algorithm\ncomputes an exact update of the optimal $\\ell_{1,\\infty}$-penalized recursive\nleast squares (RLS) predictor. Each update minimizes a convex but\nnondifferentiable function optimization problem. We develop an online homotopy\nmethod to reduce the computational complexity. Numerical simulations\ndemonstrate that the proposed algorithm outperforms the $\\ell_1$ regularized\nRLS algorithm for a group sparse system identification problem and has lower\nimplementation complexity than direct group lasso solvers. \n\n"}
{"id": "1102.4101", "contents": "Title: Scaling and Hierarchy in Urban Economies Abstract: In several recent publications, Bettencourt, West and collaborators claim\nthat properties of cities such as gross economic production, personal income,\nnumbers of patents filed, number of crimes committed, etc., show super-linear\npower-scaling with total population, while measures of resource use show\nsub-linear power-law scaling. Re-analysis of the gross economic production and\npersonal income for cities in the United States, however, shows that the data\ncannot distinguish between power laws and other functional forms, including\nlogarithmic growth, and that size predicts relatively little of the variation\nbetween cities. The striking appearance of scaling in previous work is largely\nartifact of using extensive quantities (city-wide totals) rather than intensive\nones (per-capita rates). The remaining dependence of productivity on city size\nis explained by concentration of specialist service industries, with high\nvalue-added per worker, in larger cities, in accordance with the long-standing\neconomic notion of the \"hierarchy of central places\". \n\n"}
{"id": "1102.4210", "contents": "Title: A dynamic nonstationary spatio-temporal model for short term prediction\n  of precipitation Abstract: Precipitation is a complex physical process that varies in space and time.\nPredictions and interpolations at unobserved times and/or locations help to\nsolve important problems in many areas. In this paper, we present a\nhierarchical Bayesian model for spatio-temporal data and apply it to obtain\nshort term predictions of rainfall. The model incorporates physical knowledge\nabout the underlying processes that determine rainfall, such as advection,\ndiffusion and convection. It is based on a temporal autoregressive convolution\nwith spatially colored and temporally white innovations. By linking the\nadvection parameter of the convolution kernel to an external wind vector, the\nmodel is temporally nonstationary. Further, it allows for nonseparable and\nanisotropic covariance structures. With the help of the Voronoi tessellation,\nwe construct a natural parametrization, that is, space as well as time\nresolution consistent, for data lying on irregular grid points. In the\napplication, the statistical model combines forecasts of three other\nmeteorological variables obtained from a numerical weather prediction model\nwith past precipitation observations. The model is then used to predict\nthree-hourly precipitation over 24 hours. It performs better than a separable,\nstationary and isotropic version, and it performs comparably to a deterministic\nnumerical weather prediction model for precipitation and has the advantage that\nit quantifies prediction uncertainty. \n\n"}
{"id": "1103.1110", "contents": "Title: Pairwise ranking: choice of method can produce arbitrarily different\n  rank order Abstract: We examine three methods for ranking by pairwise comparison: Principal\nEigenvector, HodgeRank and Tropical Eigenvector. It is shown that the choice of\nmethod can produce arbitrarily different rank order.To be precise, for any two\nof the three methods, and for any pair of rankings of at least four items,\nthere exists a comparison matrix for the items such that the rankings found by\nthe two methods are the prescribed ones. We discuss the implications of this\nresult in practice, study the geometry of the methods, and state some open\nproblems. \n\n"}
{"id": "1103.2987", "contents": "Title: Bayesian versus frequentist upper limits Abstract: While gravitational waves have not yet been measured directly, data analysis\nfrom detection experiments commonly includes an upper limit statement. Such\nupper limits may be derived via a frequentist or Bayesian approach; the\ntheoretical implications are very different, and on the technical side, one\nnotable difference is that one case requires maximization of the likelihood\nfunction over parameter space, while the other requires integration. Using a\nsimple example (detection of a sinusoidal signal in white Gaussian noise), we\ninvestigate the differences in performance and interpretation, and the effect\nof the \"trials factor\", or \"look-elsewhere effect\". \n\n"}
{"id": "1103.6034", "contents": "Title: Semi-supervised Learning for Photometric Supernova Classification Abstract: We present a semi-supervised method for photometric supernova typing. Our\napproach is to first use the nonlinear dimension reduction technique diffusion\nmap to detect structure in a database of supernova light curves and\nsubsequently employ random forest classification on a spectroscopically\nconfirmed training set to learn a model that can predict the type of each newly\nobserved supernova. We demonstrate that this is an effective method for\nsupernova typing. As supernova numbers increase, our semi-supervised method\nefficiently utilizes this information to improve classification, a property not\nenjoyed by template based methods. Applied to supernova data simulated by\nKessler et al. (2010b) to mimic those of the Dark Energy Survey, our methods\nachieve (cross-validated) 95% Type Ia purity and 87% Type Ia efficiency on the\nspectroscopic sample, but only 50% Type Ia purity and 50% efficiency on the\nphotometric sample due to their spectroscopic follow-up strategy. To improve\nthe performance on the photometric sample, we search for better spectroscopic\nfollow-up procedures by studying the sensitivity of our machine learned\nsupernova classification on the specific strategy used to obtain training sets.\nWith a fixed amount of spectroscopic follow-up time, we find that deeper\nmagnitude-limited spectroscopic surveys are better for producing training sets.\nFor supernova Ia (II-P) typing, we obtain a 44% (1%) increase in purity to 72%\n(87%) and 30% (162%) increase in efficiency to 65% (84%) of the sample using a\n25th (24.5th) magnitude-limited survey instead of the shallower spectroscopic\nsample used in the original simulations. When redshift information is\navailable, we incorporate it into our analysis using a novel method of altering\nthe diffusion map representation of the supernovae. Incorporating host\nredshifts leads to a 5% improvement in Type Ia purity and 13% improvement in\nType Ia efficiency. \n\n"}
{"id": "1104.0341", "contents": "Title: Small-scale inference: Empirical Bayes and confidence methods for as few\n  as a single comparison Abstract: By restricting the possible values of the proportion of null hypotheses that\nare true, the local false discovery rate (LFDR) can be estimated using as few\nas one comparison. The proportion of proteins with equivalent abundance was\nestimated to be about 20% for patient group I and about 90% for group II. The\nsimultaneously-estimated LFDRs give approximately the same inferences as\nindividual-protein confidence levels for group I but are much closer to\nindividual-protein LFDR estimates for group II. Simulations confirm that\nconfidence-based inference or LFDR-based inference performs markedly better for\nlow or high proportions of true null hypotheses, respectively. \n\n"}
{"id": "1104.0475", "contents": "Title: On a recent development in stochastic inversion with applications to\n  hydrogeology Abstract: We comment on a recent approach to spatial stochastic inversion, which\ncenters on a concept known as \"anchors\" and conducts nonparametric estimation\nof the likelihood of the anchors (along with other model parameters) with\nrespect to data obtained from field processes. The approach is called \"anchored\ninversion\" or, less desirably, \"method of anchored distribution\". Conceptual\nand technical observations are made regarding the development, interpretation,\nand use of this approach. We also point out that this approach has been\nsuperseded by new developments. \n\n"}
{"id": "1104.2790", "contents": "Title: On Intrinsic Geometric Stability of Controller Abstract: This work explores the role of the intrinsic fluctuations in finite parameter\ncontroller configurations characterizing an ensemble of arbitrary irregular\nfilter circuits. Our analysis illustrates that the parametric intrinsic\ngeometric description exhibits a set of exact pair correction functions and\nglobal correlation volume with and without the variation of the mismatch\nfactor. The present consideration shows that the canonical fluctuations can\nprecisely be depicted without any approximation. The intrinsic geometric notion\noffers a clear picture of the fluctuating controllers, which as the limit of\nthe ensemble averaging reduce to the specified controller. For the constant\nmismatch factor controllers, the Gaussian fluctuations over equilibrium basis\naccomplish a well-defined, non-degenerate, flat regular intrinsic Riemannian\nsurface. An explicit computation further demonstrates that the underlying power\ncorrelations involve ordinary summations, even if we consider the variable\nmismatch factor controllers. Our intrinsic geometric framework describes a\ndefinite character to the canonical power fluctuations of the controllers and\nconstitutes a stable design strategy for the parameters. \n\n"}
{"id": "1105.0902", "contents": "Title: Modeling Network Evolution Using Graph Motifs Abstract: Network structures are extremely important to the study of political science.\nMuch of the data in its subfields are naturally represented as networks. This\nincludes trade, diplomatic and conflict relationships. The social structure of\nseveral organization is also of interest to many researchers, such as the\naffiliations of legislators or the relationships among terrorist. A key aspect\nof studying social networks is understanding the evolutionary dynamics and the\nmechanism by which these structures grow and change over time. While current\nmethods are well suited to describe static features of networks, they are less\ncapable of specifying models of change and simulating network evolution. In the\nfollowing paper I present a new method for modeling network growth and\nevolution. This method relies on graph motifs to generate simulated network\ndata with particular structural characteristic. This technique departs notably\nfrom current methods both in form and function. Rather than a closed-form\nmodel, or stochastic implementation from a single class of graphs, the proposed\n\"graph motif model\" provides a framework for building flexible and complex\nmodels of network evolution. The paper proceeds as follows: first a brief\nreview of the current literature on network modeling is provided to place the\ngraph motif model in context. Next, the graph motif model is introduced, and a\nsimple example is provided. As a proof of concept, three classic random graph\nmodels are recovered using the graph motif modeling method: the Erdos-Renyi\nbinomial random graph, the Watts-Strogatz \"small world\" model, and the\nBarabasi-Albert preferential attachment model. In the final section I discuss\nthe results of these simulations and subsequent advantage and disadvantages\npresented by using this technique to model social networks. \n\n"}
{"id": "1105.2220", "contents": "Title: Large-sample tests of extreme-value dependence for multivariate copulas Abstract: Starting from the characterization of extreme-value copulas based on\nmax-stability, large-sample tests of extreme-value dependence for multivariate\ncopulas are studied. The two key ingredients of the proposed tests are the\nempirical copula of the data and a multiplier technique for obtaining\napproximate p-values for the derived statistics. The asymptotic validity of the\nmultiplier approach is established, and the finite-sample performance of a\nlarge number of candidate test statistics is studied through extensive Monte\nCarlo experiments for data sets of dimension two to five. In the bivariate\ncase, the rejection rates of the best versions of the tests are compared with\nthose of the test of Ghoudi, Khoudraji and Rivest (1998) recently revisited by\nBen Ghorbal, Genest and Neslehova (2009). The proposed procedures are\nillustrated on bivariate financial data and trivariate geological data. \n\n"}
{"id": "1105.3638", "contents": "Title: Corrected portmanteau tests for VAR models with time-varying variance Abstract: The problem of test of fit for Vector AutoRegressive (VAR) processes with\nunconditionally heteroscedastic errors is studied. The volatility structure is\ndeterministic but time-varying and allows for changes that are commonly\nobserved in economic or financial multivariate series. Our analysis is based on\nthe residual autocovariances and autocorrelations obtained from Ordinary Least\nSquares (OLS), Generalized Least Squares (GLS)and Adaptive Least Squares (ALS)\nestimation of the autoregressive parameters. The ALS approach is the GLS\napproach adapted to the unknown time-varying volatility that is then estimated\nby kernel smoothing. The properties of the three types of residual\nautocovariances and autocorrelations are derived. In particular it is shown\nthat the ALS and GLS residual autocorrelations are asymptotically equivalent.\nIt is also found that the asymptotic distribution of the OLS residual\nautocorrelations can be quite different from the standard chi-square asymptotic\ndistribution obtained in a correctly specified VAR model with iid innovations.\nAs a consequence the standard portmanteau tests are unreliable in our\nframework. The correct critical values of the standard portmanteau tests based\non the OLS residuals are derived. Moreover, modified portmanteau statistics\nbased on ALS residual autocorrelations are introduced. Portmanteau tests with\nmodified statistics based on OLS and ALS residuals and standard chi-square\nasymptotic distributions under the null hypothesis are also proposed. An\nextension of our portmanteau approaches to testing the lag length in a vector\nerror correction type model for co-integrating relations is briefly\ninvestigated. The finite sample properties of the goodness-of-fit tests we\nconsider are investigated by Monte Carlo experiments. The theoretical results\nare also illustrated using two U.S. economic data sets. \n\n"}
{"id": "1106.0114", "contents": "Title: Fractional counting of authorship to quantify scientific research output Abstract: We investigate the problem of counting co-authorhip in order to quantify the\nimpact and relevance of scientific research output through normalized\n\\textit{h-index} and \\textit{g-index}. We use the papers whose authors belong\nto a subset of full professors of the Italian Settore Scientifico Disciplinare\n(SSD) FIS01 - Experimental Physics. In this SSD two populations, characterized\nby the number of co-authors of each paper, are roughly present. The total\nnumber of citations for each individuals, as well as their h-index and g-index,\nstrongly depends on the average number of co-authors. We show that, in order to\nremove the dependence of the various indices on the two populations, the best\nway to define a fractional counting of autorship is to divide the number of\ncitations received by each paper by the square root of the number of\nco-authors. This allows us to obtain some information which can be used for a\nbetter understanding of the scientific knowledge made through the process of\nwriting and publishing papers. \n\n"}
{"id": "1106.2246", "contents": "Title: General bootstrap for dual phi-divergence estimates Abstract: A general notion of bootstrapped $\\phi$-divergence estimates constructed by\nexchangeably weighting sample is introduced. Asymptotic properties of these\ngeneralized bootstrapped $\\phi$-divergence estimates are obtained, by mean of\nthe empirical process theory, which are applied to construct the bootstrap\nconfidence set with asymptotically correct coverage probability. Some of\npractical problems are discussed, including in particular, the choice of escort\nparameter and several examples of divergences are investigated. Simulation\nresults are provided to illustrate the finite sample performance of the\nproposed estimators. \n\n"}
{"id": "1106.2525", "contents": "Title: Uniform Stability of a Particle Approximation of the Optimal Filter\n  Derivative Abstract: Sequential Monte Carlo methods, also known as particle methods, are a widely\nused set of computational tools for inference in non-linear non-Gaussian\nstate-space models. In many applications it may be necessary to compute the\nsensitivity, or derivative, of the optimal filter with respect to the static\nparameters of the state-space model; for instance, in order to obtain maximum\nlikelihood model parameters of interest, or to compute the optimal controller\nin an optimal control problem. In Poyiadjis et al. [2011] an original particle\nalgorithm to compute the filter derivative was proposed and it was shown using\nnumerical examples that the particle estimate was numerically stable in the\nsense that it did not deteriorate over time. In this paper we substantiate this\nclaim with a detailed theoretical study. Lp bounds and a central limit theorem\nfor this particle approximation of the filter derivative are presented. It is\nfurther shown that under mixing conditions these Lp bounds and the asymptotic\nvariance characterized by the central limit theorem are uniformly bounded with\nrespect to the time index. We demon- strate the performance predicted by theory\nwith several numerical examples. We also use the particle approximation of the\nfilter derivative to perform online maximum likelihood parameter estimation for\na stochastic volatility model. \n\n"}
{"id": "1106.2793", "contents": "Title: abc: an R package for Approximate Bayesian Computation (ABC) Abstract: Many recent statistical applications involve inference under complex models,\nwhere it is computationally prohibitive to calculate likelihoods but possible\nto simulate data. Approximate Bayesian Computation (ABC) is devoted to these\ncomplex models because it bypasses evaluations of the likelihood function using\ncomparisons between observed and simulated summary statistics. We introduce the\nR abc package that implements several ABC algorithms for performing parameter\nestimation and model selection. In particular, the recently developed\nnon-linear heteroscedastic regression methods for ABC are implemented. The abc\npackage also includes a cross-validation tool for measuring the accuracy of ABC\nestimates, and to calculate the misclassification probabilities when performing\nmodel selection. The main functions are accompanied by appropriate summary and\nplotting tools. Considering an example of demographic inference with population\ngenetics data, we show the potential of the R package.\n  R is already widely used in bioinformatics and several fields of biology. The\nR abc package will make the ABC algorithms available to the large number of R\nusers. abc is a freely available R package under the GPL license, and it can be\ndownloaded at http://cran.r-project.org/web/packages/abc/index.html. \n\n"}
{"id": "1106.2832", "contents": "Title: Active Learning to Overcome Sample Selection Bias: Application to\n  Photometric Variable Star Classification Abstract: Despite the great promise of machine-learning algorithms to classify and\npredict astrophysical parameters for the vast numbers of astrophysical sources\nand transients observed in large-scale surveys, the peculiarities of the\ntraining data often manifest as strongly biased predictions on the data of\ninterest. Typically, training sets are derived from historical surveys of\nbrighter, more nearby objects than those from more extensive, deeper surveys\n(testing data). This sample selection bias can cause catastrophic errors in\npredictions on the testing data because a) standard assumptions for\nmachine-learned model selection procedures break down and b) dense regions of\ntesting space might be completely devoid of training data. We explore possible\nremedies to sample selection bias, including importance weighting (IW),\nco-training (CT), and active learning (AL). We argue that AL---where the data\nwhose inclusion in the training set would most improve predictions on the\ntesting set are queried for manual follow-up---is an effective approach and is\nappropriate for many astronomical applications. For a variable star\nclassification problem on a well-studied set of stars from Hipparcos and OGLE,\nAL is the optimal method in terms of error rate on the testing data, beating\nthe off-the-shelf classifier by 3.4% and the other proposed methods by at least\n3.0%. To aid with manual labeling of variable stars, we developed a web\ninterface which allows for easy light curve visualization and querying of\nexternal databases. Finally, we apply active learning to classify variable\nstars in the ASAS survey, finding dramatic improvement in our agreement with\nthe ACVS catalog, from 65.5% to 79.5%, and a significant increase in the\nclassifier's average confidence for the testing set, from 14.6% to 42.9%, after\na few AL iterations. \n\n"}
{"id": "1106.5242", "contents": "Title: High Dimensional Sparse Econometric Models: An Introduction Abstract: In this chapter we discuss conceptually high dimensional sparse econometric\nmodels as well as estimation of these models using L1-penalization and\npost-L1-penalization methods. Focusing on linear and nonparametric regression\nframeworks, we discuss various econometric examples, present basic theoretical\nresults, and illustrate the concepts and methods with Monte Carlo simulations\nand an empirical application. In the application, we examine and confirm the\nempirical validity of the Solow-Swan model for international economic growth. \n\n"}
{"id": "1106.5714", "contents": "Title: Non-parametric change-point detection using string matching algorithms Abstract: Given the output of a data source taking values in a finite alphabet, we wish\nto detect change-points, that is times when the statistical properties of the\nsource change. Motivated by ideas of match lengths in information theory, we\nintroduce a novel non-parametric estimator which we call CRECHE (CRossings\nEnumeration CHange Estimator). We present simulation evidence that this\nestimator performs well, both for simulated sources and for real data formed by\nconcatenating text sources. For example, we show that we can accurately detect\nthe point at which a source changes from a Markov chain to an IID source with\nthe same stationary distribution. Our estimator requires no assumptions about\nthe form of the source distribution, and avoids the need to estimate its\nprobabilities. Further, we establish consistency of the CRECHE estimator under\na related toy model, by establishing a fluid limit and using martingale\narguments. \n\n"}
{"id": "1106.5837", "contents": "Title: Grouped Variable Selection via Nested Spike and Slab Priors Abstract: In this paper we study grouped variable selection problems by proposing a\nspecified prior, called the nested spike and slab prior, to model collective\nbehavior of regression coefficients. At the group level, the nested spike and\nslab prior puts positive mass on the event that the l2-norm of the grouped\ncoefficients is equal to zero. At the individual level, each coefficient is\nassumed to follow a spike and slab prior. We carry out maximum a posteriori\nestimation for the model by applying blockwise coordinate descent algorithms to\nsolve an optimization problem involving an approximate objective modified by\nmajorization-minimization techniques. Simulation studies show that the proposed\nestimator performs relatively well in the situations in which the true and\nredundant covariates are both covered by the same group. Asymptotic analysis\nunder a frequentist's framework further shows that the l2 estimation error of\nthe proposed estimator can have a better upper bound if the group that covers\nthe true covariates does not cover too many redundant covariates. In addition,\ngiven some regular conditions hold, the proposed estimator is asymptotically\ninvariant to group structures, and its model selection consistency can be\nestablished without imposing irrepresentable-type conditions. \n\n"}
{"id": "1106.5919", "contents": "Title: Monte Carlo algorithms for model assessment via conflicting summaries Abstract: The development of statistical methods and numerical algorithms for model\nchoice is vital to many real-world applications. In practice, the ABC approach\ncan be instrumental for sequential model design; however, the theoretical basis\nof its use has been questioned. We present a measure-theoretic framework for\nusing the ABC error towards model choice and describe how easily existing\nrejection, Metropolis-Hastings and sequential importance sampling ABC\nalgorithms are extended for the purpose of model checking. Considering a panel\nof applications from evolutionary biology to dynamic systems, we discuss the\nchoice of summaries which differs from standard ABC approaches. The methods and\nalgorithms presented here may provide the workhorse machinery for an\nexploratory approach to ABC model choice, particularly as the application of\nstandard Bayesian tools can prove impossible. \n\n"}
{"id": "1107.1900", "contents": "Title: Behavior patterns of online users and the effect on information\n  filtering Abstract: Understanding the structure and evolution of web-based user-object bipartite\nnetworks is an important task since they play a fundamental role in online\ninformation filtering. In this paper, we focus on investigating the patterns of\nonline users' behavior and the effect on recommendation process. Empirical\nanalysis on the e-commercial systems show that users have significant taste\ndiversity and their interests for niche items highly overlap. Additionally,\nrecommendation process are investigated on both the real networks and the\nreshuffled networks in which real users' behavior patterns can be gradually\ndestroyed. Our results shows that the performance of personalized\nrecommendation methods is strongly related to the real network structure.\nDetail study on each item shows that recommendation accuracy for hot items is\nalmost maximum and quite robust to the reshuffling process. However, niche\nitems cannot be accurately recommended after removing users' behavior patterns.\nOur work also is meaningful in practical sense since it reveals an effective\ndirection to improve the accuracy and the robustness of the existing\nrecommender systems. \n\n"}
{"id": "1108.0298", "contents": "Title: Network Model-Assisted Inference from Respondent-Driven Sampling Data Abstract: Respondent-Driven Sampling is a method to sample hard-to-reach human\npopulations by link-tracing over their social networks. Beginning with a\nconvenience sample, each person sampled is given a small number of uniquely\nidentified coupons to distribute to other members of the target population,\nmaking them eligible for enrollment in the study. This can be an effective\nmeans to collect large diverse samples from many populations.\n  Inference from such data requires specialized techniques for two reasons.\nUnlike in standard sampling designs, the sampling process is both partially\nbeyond the control of the researcher, and partially implicitly defined.\nTherefore, it is not generally possible to directly compute the sampling\nweights necessary for traditional design-based inference. Any likelihood-based\ninference requires the modeling of the complex sampling process often beginning\nwith a convenience sample. We introduce a model-assisted approach, resulting in\na design-based estimator leveraging a working model for the structure of the\npopulation over which sampling is conducted.\n  We demonstrate that the new estimator has improved performance compared to\nexisting estimators and is able to adjust for the bias induced by the selection\nof the initial sample. We present sensitivity analyses for unknown population\nsizes and the misspecification of the working network model. We develop a\nbootstrap procedure to compute measures of uncertainty. We apply the method to\nthe estimation of HIV prevalence in a population of injecting drug users (IDU)\nin the Ukraine, and show how it can be extended to include application-specific\ninformation. \n\n"}
{"id": "1108.2471", "contents": "Title: Latent protein trees Abstract: Unbiased, label-free proteomics is becoming a powerful technique for\nmeasuring protein expression in almost any biological sample. The output of\nthese measurements after preprocessing is a collection of features and their\nassociated intensities for each sample. Subsets of features within the data are\nfrom the same peptide, subsets of peptides are from the same protein, and\nsubsets of proteins are in the same biological pathways, therefore, there is\nthe potential for very complex and informative correlational structure inherent\nin these data. Recent attempts to utilize this data often focus on the\nidentification of single features that are associated with a particular\nphenotype that is relevant to the experiment. However, to date, there have been\nno published approaches that directly model what we know to be multiple\ndifferent levels of correlation structure. Here we present a hierarchical\nBayesian model which is specifically designed to model such correlation\nstructure in unbiased, label-free proteomics. This model utilizes partial\nidentification information from peptide sequencing and database lookup as well\nas the observed correlation in the data to appropriately compress features into\nlatent proteins and to estimate their correlation structure. We demonstrate the\neffectiveness of the model using artificial/benchmark data and in the context\nof a series of proteomics measurements of blood plasma from a collection of\nvolunteers who were infected with two different strains of viral influenza. \n\n"}
{"id": "1108.2986", "contents": "Title: Tests for multivariate normality based on canonical correlations Abstract: We propose new affine invariant tests for multivariate normality, based on\nindependence characterizations of the sample moments of the normal\ndistribution. The test statistics are obtained using canonical correlations\nbetween sets of sample moments, generalizing the Lin-Mudholkar test for\nnormality. The tests are compared to some popular tests based on Mardia's\nskewness and kurtosis measures in an extensive simulation power study and are\nfound to offer higher power against many of the alternatives. \n\n"}
{"id": "1108.2999", "contents": "Title: Dual $\\phi$-divergences estimation in normal models Abstract: A class of robust estimators which are obtained from dual representation of\n$\\phi$-divergences, are studied empirically for the normal location model.\nMembers of this class of estimators are compared, and it is found that they are\nefficient at the true model and offer an attractive alternative to the maximum\nlikelihood, in term of robustness . \n\n"}
{"id": "1109.0442", "contents": "Title: A Student-t based filter for robust signal detection Abstract: The search for gravitational-wave signals in detector data is often hampered\nby the fact that many data analysis methods are based on the theory of\nstationary Gaussian noise, while actual measurement data frequently exhibit\nclear departures from these assumptions. Deriving methods from models more\nclosely reflecting the data's properties promises to yield more sensitive\nprocedures. The commonly used matched filter is such a detection method that\nmay be derived via a Gaussian model. In this paper we propose a generalized\nmatched-filtering technique based on a Student-t distribution that is able to\naccount for heavier-tailed noise and is robust against outliers in the data. On\nthe technical side, it generalizes the matched filter's least-squares method to\nan iterative, or adaptive, variation. In a simplified Monte Carlo study we show\nthat when applied to simulated signals buried in actual interferometer noise it\nleads to a higher detection rate than the usual (\"Gaussian\") matched filter. \n\n"}
{"id": "1109.4518", "contents": "Title: On Estimation and Selection for Topic Models Abstract: This article describes posterior maximization for topic models, identifying\ncomputational and conceptual gains from inference under a non-standard\nparametrization. We then show that fitted parameters can be used as the basis\nfor a novel approach to marginal likelihood estimation, via block-diagonal\napproximation to the information matrix,that facilitates choosing the number of\nlatent topics. This likelihood-based model selection is complemented with a\ngoodness-of-fit analysis built around estimated residual dispersion. Examples\nare provided to illustrate model selection as well as to compare our estimation\nagainst standard alternative techniques. \n\n"}
{"id": "1109.5278", "contents": "Title: Controlling the degree of caution in statistical inference with the\n  Bayesian and frequentist approaches as opposite extremes Abstract: In statistical practice, whether a Bayesian or frequentist approach is used\nin inference depends not only on the availability of prior information but also\non the attitude taken toward partial prior information, with frequentists\ntending to be more cautious than Bayesians. The proposed framework defines that\nattitude in terms of a specified amount of caution, thereby enabling data\nanalysis at the level of caution desired and on the basis of any prior\ninformation. The caution parameter represents the attitude toward partial prior\ninformation in much the same way as a loss function represents the attitude\ntoward risk. When there is very little prior information and nonzero caution,\nthe resulting inferences correspond to those of the candidate confidence\nintervals and p-values that are most similar to the credible intervals and\nhypothesis probabilities of the specified Bayesian posterior. On the other\nhand, in the presence of a known physical distribution of the parameter,\ninferences are based only on the corresponding physical posterior. In those\nextremes of either negligible prior information or complete prior information,\ninferences do not depend on the degree of caution. Partial prior information\nbetween those two extremes leads to intermediate inferences that are more\nfrequentistic to the extent that the caution is high and more Bayesian to the\nextent that the caution is low. \n\n"}
{"id": "1109.5485", "contents": "Title: A new estimator for the tail-dependence coefficient Abstract: Recently, the concept of tail dependence has been discussed in financial\napplications related to market or credit risk. The multivariate extreme value\ntheory is a proper tool to measure and model dependence, for example, of large\nloss events. A common measure of tail dependence is given by the so-called\ntail-dependence coefficient. We present a simple estimator of this latter that\navoids the drawbacks of the estimation procedure that has been used so far. We\nprove strong consistency and asymptotic normality and analyze the finite sample\nbehavior through simulation. We illustrate with an application to financial\ndata. \n\n"}
{"id": "1110.4411", "contents": "Title: Gaussian Process Regression Networks Abstract: We introduce a new regression framework, Gaussian process regression networks\n(GPRN), which combines the structural properties of Bayesian neural networks\nwith the non-parametric flexibility of Gaussian processes. This model\naccommodates input dependent signal and noise correlations between multiple\nresponse variables, input dependent length-scales and amplitudes, and\nheavy-tailed predictive distributions. We derive both efficient Markov chain\nMonte Carlo and variational Bayes inference procedures for this model. We apply\nGPRN as a multiple output regression and multivariate volatility model,\ndemonstrating substantially improved performance over eight popular multiple\noutput (multi-task) Gaussian process models and three multivariate volatility\nmodels on benchmark datasets, including a 1000 dimensional gene expression\ndataset. \n\n"}
{"id": "1110.5002", "contents": "Title: Testing the approximations described in \"Asymptotic formulae for\n  likelihood-based tests of new physics\" Abstract: \"Asymptotic formulae for likelihood-based tests of new physics\" presents a\nmathematical formalism for a new approximation for hypothesis testing in high\nenergy physics. The approximations are designed to greatly reduce the\ncomputational burden for such problems. We seek to test the conditions under\nwhich the approximations described remain valid. To do so, we perform parallel\ncalculations for a range of scenarios and compare the full calculation to the\napproximations to determine the limits and robustness of the approximation. We\ncompare this approximation against values calculated with the Collie framework,\nwhich for our analysis we assume produces true values. \n\n"}
{"id": "1110.6178", "contents": "Title: Parameter Estimation with BEAMS in the presence of biases and\n  correlations Abstract: The original formulation of BEAMS - Bayesian Estimation Applied to Multiple\nSpecies - showed how to use a dataset contaminated by points of multiple\nunderlying types to perform unbiased parameter estimation. An example is\ncosmological parameter estimation from a photometric supernova sample\ncontaminated by unknown Type Ibc and II supernovae. Where other methods require\ndata cuts to increase purity, BEAMS uses all of the data points in conjunction\nwith their probabilities of being each type. Here we extend the BEAMS formalism\nto allow for correlations between the data and the type probabilities of the\nobjects as can occur in realistic cases. We show with simple simulations that\nthis extension can be crucial, providing a 50% reduction in parameter\nestimation variance when such correlations do exist. We then go on to perform\ntests to quantify the importance of the type probabilities, one of which\nillustrates the effect of biasing the probabilities in various ways. Finally, a\ngeneral presentation of the selection bias problem is given, and discussed in\nthe context of future photometric supernova surveys and BEAMS, which lead to\nspecific recommendations for future supernova surveys. \n\n"}
{"id": "1111.6857", "contents": "Title: Multivariate information measures: an experimentalist's perspective Abstract: Information theory is widely accepted as a powerful tool for analyzing\ncomplex systems and it has been applied in many disciplines. Recently, some\ncentral components of information theory - multivariate information measures -\nhave found expanded use in the study of several phenomena. These information\nmeasures differ in subtle yet significant ways. Here, we will review the\ninformation theory behind each measure, as well as examine the differences\nbetween these measures by applying them to several simple model systems. In\naddition to these systems, we will illustrate the usefulness of the information\nmeasures by analyzing neural spiking data from a dissociated culture through\nearly stages of its development. We hope that this work will aid other\nresearchers as they seek the best multivariate information measure for their\nspecific research goals and system. Finally, we have made software available\nonline which allows the user to calculate all of the information measures\ndiscussed within this paper. \n\n"}
{"id": "1111.7089", "contents": "Title: Semiparametric modeling of autonomous nonlinear dynamical systems with\n  application to plant growth Abstract: We propose a semiparametric model for autonomous nonlinear dynamical systems\nand devise an estimation procedure for model fitting. This model incorporates\nsubject-specific effects and can be viewed as a nonlinear semiparametric mixed\neffects model. We also propose a computationally efficient model selection\nprocedure. We show by simulation studies that the proposed estimation as well\nas model selection procedures can efficiently handle sparse and noisy\nmeasurements. Finally, we apply the proposed method to a plant growth data used\nto study growth displacement rates within meristems of maize roots under two\ndifferent experimental conditions. \n\n"}
{"id": "1112.1024", "contents": "Title: Asymptotically Exact Inference in Conditional Moment Inequality Models Abstract: This paper derives the rate of convergence and asymptotic distribution for a\nclass of Kolmogorov-Smirnov style test statistics for conditional moment\ninequality models for parameters on the boundary of the identified set under\ngeneral conditions. In contrast to other moment inequality settings, the rate\nof convergence is faster than root-$n$, and the asymptotic distribution depends\nentirely on nonbinding moments. The results require the development of new\ntechniques that draw a connection between moment selection, irregular\nidentification, bandwidth selection and nonstandard M-estimation. Using these\nresults, I propose tests that are more powerful than existing approaches for\nchoosing critical values for this test statistic. I quantify the power\nimprovement by showing that the new tests can detect alternatives that converge\nto points on the identified set at a faster rate than those detected by\nexisting approaches. A monte carlo study confirms that the tests and the\nasymptotic approximations they use perform well in finite samples. In an\napplication to a regression of prescription drug expenditures on income with\ninterval data from the Health and Retirement Study, confidence regions based on\nthe new tests are substantially tighter than those based on existing methods. \n\n"}
{"id": "1112.5966", "contents": "Title: Calculation of the mean duration and age of onset of a chronic disease\n  and application to dementia in Germany Abstract: This paper descibes a new method of calculating the mean duration and mean\nage of onset of a chronic disease from incidence and mortality rates. It is\nbased on an ordinary differential equation resulting from a simple compartment\nmodel. Applicability of the method is demonstrated in data about dementia in\nGermany. \n\n"}
{"id": "1201.0167", "contents": "Title: Adaptive Test of Conditional Moment Inequalities Abstract: In this paper, I construct a new test of conditional moment inequalities,\nwhich is based on studentized kernel estimates of moment functions with many\ndifferent values of the bandwidth parameter. The test automatically adapts to\nthe unknown smoothness of moment functions and has uniformly correct asymptotic\nsize. The test has high power in a large class of models with conditional\nmoment inequalities. Some existing tests have nontrivial power against\nn^{-1/2}-local alternatives in a certain class of these models whereas my\nmethod only allows for nontrivial testing against (n/\\log n)^{-1/2}-local\nalternatives in this class. There exist, however, other classes of models with\nconditional moment inequalities where the mentioned tests have much lower power\nin comparison with the test developed in this paper. \n\n"}
{"id": "1201.0400", "contents": "Title: A classical measure of evidence for general null hypotheses Abstract: In science, the most widespread statistical quantities are perhaps\n$p$-values. A typical advice is to reject the null hypothesis $H_0$ if the\ncorresponding p-value is sufficiently small (usually smaller than 0.05). Many\ncriticisms regarding p-values have arisen in the scientific literature. The\nmain issue is that in general optimal p-values (based on likelihood ratio\nstatistics) are not measures of evidence over the parameter space $\\Theta$.\nHere, we propose an \\emph{objective} measure of evidence for very general null\nhypotheses that satisfies logical requirements (i.e., operations on the subsets\nof $\\Theta$) that are not met by p-values (e.g., it is a possibility measure).\nWe study the proposed measure in the light of the abstract belief calculus\nformalism and we conclude that it can be used to establish objective states of\nbelief on the subsets of $\\Theta$. Based on its properties, we strongly\nrecommend this measure as an additional summary of significance tests. At the\nend of the paper we give a short listing of possible open problems. \n\n"}
{"id": "1201.1141", "contents": "Title: A Problem in Particle Physics and Its Bayesian Analysis Abstract: There is a class of statistical problems that arises in several contexts, the\nLattice QCD problem of particle physics being one that has attracted the most\nattention. In essence, the problem boils down to the estimation of an infinite\nnumber of parameters from a finite number of equations, each equation being an\ninfinite sum of exponential functions. By introducing a latent parameter into\nthe QCD system, we are able to identify a pattern which tantamounts to reducing\nthe system to a telescopic series. A statistical model is then endowed on the\nseries, and inference about the unknown parameters done via a Bayesian\napproach. A computationally intensive Markov Chain Monte Carlo (MCMC) algorithm\nis invoked to implement the approach. The algorithm shares some parallels with\nthat used in the particle Kalman filter. The approach is validated against\nsimulated as well as data generated by a physics code pertaining to the quark\nmasses of protons. The value of our approach is that we are now able to answer\nquestions that could not be readily answered using some standard approaches in\nparticle physics. The structure of the Lattice QCD equations is not unique to\nphysics. Such architectures also appear in mathematical biology, nuclear\nmagnetic imaging, network analysis, ultracentrifuge, and a host of other\nrelaxation and time decay phenomena. Thus, the methodology of this paper should\nhave an appeal that transcends the Lattice QCD scenario which motivated us. \n\n"}
{"id": "1201.4403", "contents": "Title: Locally Adaptive Bayes Nonparametric Regression via Nested Gaussian\n  Processes Abstract: We propose a nested Gaussian process (nGP) as a locally adaptive prior for\nBayesian nonparametric regression. Specified through a set of stochastic\ndifferential equations (SDEs), the nGP imposes a Gaussian process prior for the\nfunction's $m$th-order derivative. The nesting comes in through including a\nlocal instantaneous mean function, which is drawn from another Gaussian process\ninducing adaptivity to locally-varying smoothness. We discuss the support of\nthe nGP prior in terms of the closure of a reproducing kernel Hilbert space,\nand consider theoretical properties of the posterior. The posterior mean under\nthe nGP prior is shown to be equivalent to the minimizer of a nested penalized\nsum-of-squares involving penalties for both the global and local roughness of\nthe function. Using highly-efficient Markov chain Monte Carlo for posterior\ninference, the proposed method performs well in simulation studies compared to\nseveral alternatives, and is scalable to massive data, illustrated through a\nproteomics application. \n\n"}
{"id": "1201.5133", "contents": "Title: Nonparametric estimation of pair-copula constructions with the empirical\n  pair-copula Abstract: A pair-copula construction is a decomposition of a multivariate copula into a\nstructured system, called regular vine, of bivariate copulae or pair-copulae.\nThe standard practice is to model these pair-copulae parametrically, which\ncomes at the cost of a large model risk, with errors propagating throughout the\nvine structure. The empirical pair-copula proposed in the paper provides a\nnonparametric alternative still achieving the parametric convergence rate. It\ncan be used as a basis for inference on dependence measures, for selecting and\npruning the vine structure, and for hypothesis tests concerning the form of the\npair-copulae. \n\n"}
{"id": "1201.5871", "contents": "Title: Null models for network data Abstract: The analysis of datasets taking the form of simple, undirected graphs\ncontinues to gain in importance across a variety of disciplines. Two choices of\nnull model, the logistic-linear model and the implicit log-linear model, have\ncome into common use for analyzing such network data, in part because each\naccounts for the heterogeneity of network node degrees typically observed in\npractice. Here we show how these both may be viewed as instances of a broader\nclass of null models, with the property that all members of this class give\nrise to essentially the same likelihood-based estimates of link probabilities\nin sparse graph regimes. This facilitates likelihood-based computation and\ninference, and enables practitioners to choose the most appropriate null model\nfrom this family based on application context. Comparative model fits for a\nvariety of network datasets demonstrate the practical implications of our\nresults. \n\n"}
{"id": "1201.5893", "contents": "Title: A new stochastic differential equation modelling incidence and\n  prevalence with an application to systemic lupus erythematosus in England and\n  Wales, 1995 Abstract: This article reformulates a common illness-death model in terms of a new\nsystem of stochastical differential equations (SDEs). The SDEs are used to\nestimate epidemiological characteristics and burden of systemic lupus\nerythematosus in England and Wales in 1995. \n\n"}
{"id": "1201.5962", "contents": "Title: How many statistics are needed to characterize the univariate extremes Abstract: Let $X_{1},X_{2},...$ be a sequence of independent random variables ($rv$)\nwith common distribution function ($df$) $F$ such that $F(1)=0$. We consider\nthe simple statistical problem : find a statistics family of size $m\\geq 1$\nwhose convergence, in probability or almost surely, to a point of some domain\n$\\mathcal{S} \\in \\mathbb{R}^{m}$ is equivalent that $F$ lies in the extremal\ndomain of attraction $\\Gamma$. Such a family, whenever it exists, is called an\nEmpirical Characterizing Statistics Family for the EXTtremes (ECSFEXT). The\ndeparture point of this theory goes back to Mason, who proved that the Hill\nestimator converges a.s. to a positive real number for some particular\nsequences if and only $F$ lies in the attaction domain of a Fr\\'echet's law.\nConsidered for the whole attraction domain, the question becomes more complex.\nWe provide here an ECSFEXT of nine (9) elements and also characterize the\nsubdomains of $\\Gamma$. The question of lowering m=9 to a minimum number is\nlaunched. \n\n"}
{"id": "1202.0949", "contents": "Title: Bayesian filtering for multi-object systems with independently generated\n  observations Abstract: A general approach for Bayesian filtering of multi-object systems is studied,\nwith particular emphasis on the model where each object generates observations\nindependently of other objects. The approach is based on variational calculus\napplied to generating functionals, using the general version of Faa di Bruno's\nformula for Gateaux differentials. This result enables us to determine some\ngeneral formulae for the updated generating functional after the application of\na multi-object analogue of Bayes' rule. \n\n"}
{"id": "1202.1377", "contents": "Title: Statistical significance in high-dimensional linear models Abstract: We propose a method for constructing p-values for general hypotheses in a\nhigh-dimensional linear model. The hypotheses can be local for testing a single\nregression parameter or they may be more global involving several up to all\nparameters. Furthermore, when considering many hypotheses, we show how to\nadjust for multiple testing taking dependence among the p-values into account.\nOur technique is based on Ridge estimation with an additional correction term\ndue to a substantial projection bias in high dimensions. We prove strong error\ncontrol for our p-values and provide sufficient conditions for detection: for\nthe former, we do not make any assumption on the size of the true underlying\nregression coefficients while regarding the latter, our procedure might not be\noptimal in terms of power. We demonstrate the method in simulated examples and\na real data application. \n\n"}
{"id": "1203.0098", "contents": "Title: Bayesian matching of unlabeled marked point sets using random fields,\n  with an application to molecular alignment Abstract: Statistical methodology is proposed for comparing unlabeled marked point\nsets, with an application to aligning steroid molecules in chemoinformatics.\nMethods from statistical shape analysis are combined with techniques for\npredicting random fields in spatial statistics in order to define a suitable\nmeasure of similarity between two marked point sets. Bayesian modeling of the\npredicted field overlap between pairs of point sets is proposed, and posterior\ninference of the alignment is carried out using Markov chain Monte Carlo\nsimulation. By representing the fields in reproducing kernel Hilbert spaces,\nthe degree of overlap can be computed without expensive numerical integration.\nSuperimposing entire fields rather than the configuration matrices of point\ncoordinates thereby avoids the problem that there is usually no clear\none-to-one correspondence between the points. In addition, mask parameters are\nintroduced in the model, so that partial matching of the marked point sets can\nbe carried out. We also propose an adaptation of the generalized Procrustes\nanalysis algorithm for the simultaneous alignment of multiple point sets. The\nmethodology is illustrated with a simulation study and then applied to a data\nset of 31 steroid molecules, where the relationship between shape and binding\nactivity to the corticosteroid binding globulin receptor is explored. \n\n"}
{"id": "1203.0106", "contents": "Title: Sparsity-Promoting Bayesian Dynamic Linear Models Abstract: Sparsity-promoting priors have become increasingly popular over recent years\ndue to an increased number of regression and classification applications\ninvolving a large number of predictors. In time series applications where\nobservations are collected over time, it is often unrealistic to assume that\nthe underlying sparsity pattern is fixed. We propose here an original class of\nflexible Bayesian linear models for dynamic sparsity modelling. The proposed\nclass of models expands upon the existing Bayesian literature on sparse\nregression using generalized multivariate hyperbolic distributions. The\nproperties of the models are explored through both analytic results and\nsimulation studies. We demonstrate the model on a financial application where\nit is shown that it accurately represents the patterns seen in the analysis of\nstock and derivative data, and is able to detect major events by filtering an\nartificial portfolio of assets. \n\n"}
{"id": "1203.1504", "contents": "Title: Does Geometric Algebra provide a loophole to Bell's Theorem? (with\n  corrections) Abstract: In 2007, and in a series of later papers, Joy Christian claimed to refute\nBell's theorem, presenting an alleged local realistic model of the singlet\ncorrelations using techniques from Geometric Algebra (GA). Several authors\npublished papers refuting his claims, and Christian's ideas did not gain\nacceptance. However, he recently succeeded in publishing yet more ambitious and\ncomplex versions of his theory in fairly mainstream journals. How could this\nbe? The mathematics and logic of Bell's theorem is simple and transparent and\nhas been intensely studied and debated for over 50 years. Christian claims to\nhave a mathematical counterexample to a purely mathematical theorem. Each new\nversion of Christian's model used new devices to circumvent Bell's theorem or\ndepended on a new way to misunderstand Bell's work. These devices and\nmisinterpretations are in common use by other Bell critics, so it useful to\nidentify and name them. I hope that this paper can serve as a useful resource\nto those who need to evaluate new \"disproofs of Bell's theorem\". Christian's\nfundamental idea is simple and quite original: he gives a probabilistic\ninterpretation of the fundamental GA equation a.b = (ab + ba)/2. After that,\nambiguous notation and technical complexity allow sign errors to be hidden from\nsight, and new mathematical errors can be introduced. \n\n"}
{"id": "1203.2591", "contents": "Title: Evolutionary Rotation in Switching Incentive Zero-Sum Games Abstract: In a laboratory experiment, round by round, individual interactions should\nlead to the social evolutionary rotation in population strategy state space.\nSuccessive switching the incentive parameter should lead to successive change\nof the rotation ---- both of its direction and its strength. In data from a\nswitching payoff matrix experiment of extended 2x2 games (Binmore, Swierzbinski\nand Proulx, 2001 [1]), we find the changing of the social evolutionary rotation\ncan be distinguished quantitatively. The evolutionary rotation can be captured\nby evolutionary dynamics. With eigenvalue from the Jacobian of a constrained\nreplicator dynamics model, an interpretation for observed rotation strength is\ngiven. In addition, equality-of-populations rank test shows that relative\nresponse coefficient of a group could persist cross the switching parameter\ngames. The data has successively been used to support Von Neumann's minimax\ntheory. Using the old data, with observed evolutionary rotation, this report\nprovides a new insight into evolutionary game theory and experimental social\ndynamics. \n\n"}
{"id": "1204.2257", "contents": "Title: Partial freeness of random matrices Abstract: We investigate the implications of free probability for random matrices. From\nrules for calculating all possible joint moments of two free random matrices,\nwe develop a notion of partial freeness which is quantified by the breakdown of\nthese rules. We provide a combinatorial interpretation for partial freeness as\nthe presence of closed paths in Hilbert space defined by particular joint\nmoments. We also discuss how asymptotic moment expansions provide an error term\non the density of states. We present MATLAB code for the calculation of moments\nand free cumulants of arbitrary random matrices. \n\n"}
{"id": "1204.6516", "contents": "Title: Detection of additive outliers in Poisson INteger-valued AutoRegressive\n  time series Abstract: Outlying observations are commonly encountered in the analysis of time\nseries. In this paper the problem of detecting additive outliers in\ninteger-valued time series is considered. We show how Gibbs sampling can be\nused to detect outlying observations in INAR(1) processes. The methodology\nproposed is illustrated using examples as well as an observed data set. \n\n"}
{"id": "1205.0411", "contents": "Title: Hypothesis testing using pairwise distances and associated kernels (with\n  Appendix) Abstract: We provide a unifying framework linking two classes of statistics used in\ntwo-sample and independence testing: on the one hand, the energy distances and\ndistance covariances from the statistics literature; on the other, distances\nbetween embeddings of distributions to reproducing kernel Hilbert spaces\n(RKHS), as established in machine learning. The equivalence holds when energy\ndistances are computed with semimetrics of negative type, in which case a\nkernel may be defined such that the RKHS distance between distributions\ncorresponds exactly to the energy distance. We determine the class of\nprobability distributions for which kernels induced by semimetrics are\ncharacteristic (that is, for which embeddings of the distributions to an RKHS\nare injective). Finally, we investigate the performance of this family of\nkernels in two-sample and independence tests: we show in particular that the\nenergy distance most commonly employed in statistics is just one member of a\nparametric family of kernels, and that other choices from this family can yield\nmore powerful tests. \n\n"}
{"id": "1205.0540", "contents": "Title: A Fitness Model for Scholarly Impact Analysis Abstract: We propose a model to analyze citation growth and influences of fitness\n(competitiveness) factors in an evolving citation network. Applying the\nproposed method to modeling citations to papers and scholars in the InfoVis\n2004 data, a benchmark collection about a 31-year history of information\nvisualization, leads to findings consistent with citation distributions in\ngeneral and observations of the domain in particular. Fitness variables based\non prior impacts and the time factor have significant influences on citation\noutcomes. We find considerably large effect sizes from the fitness modeling,\nwhich suggest inevitable bias in citation analysis due to these factors. While\nraw citation scores offer little insight into the growth of InfoVis,\nnormalization of the scores by influences of time and prior fitness offers a\nreasonable depiction of the field's development. The analysis demonstrates the\nproposed model's ability to produce results consistent with observed data and\nto support meaningful comparison of citation scores over time. \n\n"}
{"id": "1205.1461", "contents": "Title: On statistical researches of parliament elections in Russian Federation,\n  04.12.2011 Abstract: There is a lot of statistical researches of Russian elections 04.12.2011. The\npurpose of this activity is to give a mathematical proof of large\nfalsifications and to estimate possible 'real results of elections'. My purpose\nis to show that\n  1. Statistical argumentation allows to prove existence of falsifications and\nto give a lower estimate of falsification, near 1-2 percents.\n  2. Statistical proofs of stronger statements are incorrect from both points\nof view of mathematics and of natural sciences.\n  3. This problem is not a problem of pure mathematics (since it includes\nstrong indeterminacy of sociological nature). \n\n"}
{"id": "1205.2911", "contents": "Title: Factorial graphical lasso for dynamic networks Abstract: Dynamic networks models describe a growing number of important scientific\nprocesses, from cell biology and epidemiology to sociology and finance. There\nare many aspects of dynamical networks that require statistical considerations.\nIn this paper we focus on determining network structure. Estimating dynamic\nnetworks is a difficult task since the number of components involved in the\nsystem is very large. As a result, the number of parameters to be estimated is\nbigger than the number of observations. However, a characteristic of many\nnetworks is that they are sparse. For example, the molecular structure of genes\nmake interactions with other components a highly-structured and therefore\nsparse process.\n  Penalized Gaussian graphical models have been used to estimate sparse\nnetworks. However, the literature has focussed on static networks, which lack\nspecific temporal constraints. We propose a structured Gaussian dynamical\ngraphical model, where structures can consist of specific time dynamics, known\npresence or absence of links and block equality constraints on the parameters.\nThus, the number of parameters to be estimated is reduced and accuracy of the\nestimates, including the identification of the network, can be tuned up. Here,\nwe show that the constrained optimization problem can be solved by taking\nadvantage of an efficient solver, logdetPPA, developed in convex optimization.\nMoreover, model selection methods for checking the sensitivity of the inferred\nnetworks are described. Finally, synthetic and real data illustrate the\nproposed methodologies. \n\n"}
{"id": "1205.4770", "contents": "Title: Variance function estimation in high-dimensions Abstract: We consider the high-dimensional heteroscedastic regression model, where the\nmean and the log variance are modeled as a linear combination of input\nvariables. Existing literature on high-dimensional linear regres- sion models\nhas largely ignored non-constant error variances, even though they commonly\noccur in a variety of applications ranging from biostatis- tics to finance. In\nthis paper we study a class of non-convex penalized pseudolikelihood estimators\nfor both the mean and variance parameters. We show that the Heteroscedastic\nIterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracle\nproperty, that is, we prove that the rates of convergence are the same as if\nthe true model was known. We demonstrate numerical properties of the procedure\non a simulation study and real world data. \n\n"}
{"id": "1205.6310", "contents": "Title: Dynamic filtering of static dipoles in magnetoencephalography Abstract: We consider the problem of estimating neural activity from measurements of\nthe magnetic fields recorded by magnetoencephalography. We exploit the temporal\nstructure of the problem and model the neural current as a collection of\nevolving current dipoles, which appear and disappear, but whose locations are\nconstant throughout their lifetime. This fully reflects the physiological\ninterpretation of the model. In order to conduct inference under this proposed\nmodel, it was necessary to develop an algorithm based around state-of-the-art\nsequential Monte Carlo methods employing carefully designed importance\ndistributions. Previous work employed a bootstrap filter and an artificial\ndynamic structure where dipoles performed a random walk in space, yielding\nnonphysical artefacts in the reconstructions; such artefacts are not observed\nwhen using the proposed model. The algorithm is validated with simulated data,\nin which it provided an average localisation error which is approximately half\nthat of the bootstrap filter. An application to complex real data derived from\na somatosensory experiment is presented. Assessment of model fit via marginal\nlikelihood showed a clear preference for the proposed model and the associated\nreconstructions show better localisation. \n\n"}
{"id": "1206.0867", "contents": "Title: Testing linear hypotheses in high-dimensional regressions Abstract: For a multivariate linear model, Wilk's likelihood ratio test (LRT)\nconstitutes one of the cornerstone tools. However, the computation of its\nquantiles under the null or the alternative requires complex analytic\napproximations and more importantly, these distributional approximations are\nfeasible only for moderate dimension of the dependent variable, say $p\\le 20$.\nOn the other hand, assuming that the data dimension $p$ as well as the number\n$q$ of regression variables are fixed while the sample size $n$ grows, several\nasymptotic approximations are proposed in the literature for Wilk's $\\bLa$\nincluding the widely used chi-square approximation. In this paper, we consider\nnecessary modifications to Wilk's test in a high-dimensional context,\nspecifically assuming a high data dimension $p$ and a large sample size $n$.\nBased on recent random matrix theory, the correction we propose to Wilk's test\nis asymptotically Gaussian under the null and simulations demonstrate that the\ncorrected LRT has very satisfactory size and power, surely in the large $p$ and\nlarge $n$ context, but also for moderately large data dimensions like $p=30$ or\n$p=50$. As a byproduct, we give a reason explaining why the standard chi-square\napproximation fails for high-dimensional data. We also introduce a new\nprocedure for the classical multiple sample significance test in MANOVA which\nis valid for high-dimensional data. \n\n"}
{"id": "1206.2054", "contents": "Title: Maximum A Posteriori Covariance Estimation Using a Power Inverse Wishart\n  Prior Abstract: The estimation of the covariance matrix is an initial step in many\nmultivariate statistical methods such as principal components analysis and\nfactor analysis, but in many practical applications the dimensionality of the\nsample space is large compared to the number of samples, and the usual maximum\nlikelihood estimate is poor. Typically, improvements are obtained by modelling\nor regularization. From a practical point of view, these methods are often\ncomputationally heavy and rely on approximations. As a fast substitute, we\npropose an easily calculable maximum a posteriori (MAP) estimator based on a\nnew class of prior distributions generalizing the inverse Wishart prior,\ndiscuss its properties, and demonstrate the estimator on simulated and real\ndata. \n\n"}
{"id": "1206.3776", "contents": "Title: Measuring political sentiment on Twitter: factor-optimal design for\n  multinomial inverse regression Abstract: This article presents a short case study in text analysis: the scoring of\nTwitter posts for positive, negative, or neutral sentiment directed towards\nparticular US politicians. The study requires selection of a sub-sample of\nrepresentative posts for sentiment scoring, a common and costly aspect of\nsentiment mining. As a general contribution, our application is preceded by a\nproposed algorithm for maximizing sampling efficiency. In particular, we\noutline and illustrate greedy selection of documents to build designs that are\nD-optimal in a topic-factor decomposition of the original text. The strategy is\napplied to our motivating dataset of political posts, and we outline a new\ntechnique for predicting both generic and subject-specific document sentiment\nthrough use of variable interactions in multinomial inverse regression. Results\nare presented for analysis of 2.1 million Twitter posts around February 2012. \n\n"}
{"id": "1206.4952", "contents": "Title: Space-Efficient Sampling from Social Activity Streams Abstract: In order to efficiently study the characteristics of network domains and\nsupport development of network systems (e.g. algorithms, protocols that operate\non networks), it is often necessary to sample a representative subgraph from a\nlarge complex network. Although recent subgraph sampling methods have been\nshown to work well, they focus on sampling from memory-resident graphs and\nassume that the sampling algorithm can access the entire graph in order to\ndecide which nodes/edges to select. Many large-scale network datasets, however,\nare too large and/or dynamic to be processed using main memory (e.g., email,\ntweets, wall posts). In this work, we formulate the problem of sampling from\nlarge graph streams. We propose a streaming graph sampling algorithm that\ndynamically maintains a representative sample in a reservoir based setting. We\nevaluate the efficacy of our proposed methods empirically using several\nreal-world data sets. Across all datasets, we found that our method produce\nsamples that preserve better the original graph distributions. \n\n"}
{"id": "1206.6980", "contents": "Title: More power via graph-structured tests for differential expression of\n  gene networks Abstract: We consider multivariate two-sample tests of means, where the location shift\nbetween the two populations is expected to be related to a known graph\nstructure. An important application of such tests is the detection of\ndifferentially expressed genes between two patient populations, as shifts in\nexpression levels are expected to be coherent with the structure of graphs\nreflecting gene properties such as biological process, molecular function,\nregulation or metabolism. For a fixed graph of interest, we demonstrate that\naccounting for graph structure can yield more powerful tests under the\nassumption of smooth distribution shift on the graph. We also investigate the\nidentification of nonhomogeneous subgraphs of a given large graph, which poses\nboth computational and multiple hypothesis testing problems. The relevance and\nbenefits of the proposed approach are illustrated on synthetic data and on\nbreast and bladder cancer gene expression data analyzed in the context of KEGG\nand NCI pathways. \n\n"}
{"id": "1207.1134", "contents": "Title: Reconstruction of Signals from Magnitudes of Redundant Representations Abstract: This paper is concerned with the question of reconstructing a vector in a\nfinite-dimensional real or complex Hilbert space when only the magnitudes of\nthe coefficients of the vector under a redundant linear map are known. We\npresent new invertibility results as well an iterative algorithm that finds the\nleast-square solution and is robust in the presence of noise. We analyze its\nnumerical performance by comparing it to two versions of the Cramer-Rao lower\nbound. \n\n"}
{"id": "1207.1221", "contents": "Title: Robust Bayesian inference of networks using Dirichlet t-distributions Abstract: Bayesian graphical modeling provides an appealing way to obtain uncertainty\nestimates when inferring network structures, and much recent progress has been\nmade for Gaussian models. These models have been used extensively in\napplications to gene expression data, even in cases where there appears to be\nsignificant deviations from the Gaussian model. For more robust inferences, it\nis natural to consider extensions to t-distribution models. We argue that the\nclassical multivariate t-distribution, defined using a single latent Gamma\nrandom variable to rescale a Gaussian random vector, is of little use in highly\nmultivariate settings, and propose other, more flexible t-distributions. Using\nan independent Gamma-divisor for each component of the random vector defines\nwhat we term the alternative t-distribution. The associated model allows one to\nextract information from highly multivariate data even when most experiments\ncontain outliers for some of their measurements. However, the use of this\nalternative model comes at increased computational cost and imposes constraints\non the achievable correlation structures, raising the need for a compromise\nbetween the classical and alternative models. To this end we propose the use of\nDirichlet processes for adaptive clustering of the latent Gamma-scalars, each\nof which may then divide a group of latent Gaussian variables. Dirichlet\nprocesses are commonly used to cluster independent observations; here they are\nused instead to cluster the dependent components of a single observation. The\nresulting Dirichlet t-distribution interpolates naturally between the two\nextreme cases of the classical and alternative t-distributions and combines\nmore appealing modeling of the multivariate dependence structure with favorable\ncomputational properties. \n\n"}
{"id": "1207.1497", "contents": "Title: Hidden Markov models for the activity profile of terrorist groups Abstract: The main focus of this work is on developing models for the activity profile\nof a terrorist group, detecting sudden spurts and downfalls in this profile,\nand, in general, tracking it over a period of time. Toward this goal, a\n$d$-state hidden Markov model (HMM) that captures the latent states underlying\nthe dynamics of the group and thus its activity profile is developed. The\nsimplest setting of $d=2$ corresponds to the case where the dynamics are\ncoarsely quantized as Active and Inactive, respectively. A state estimation\nstrategy that exploits the underlying HMM structure is then developed for spurt\ndetection and tracking. This strategy is shown to track even nonpersistent\nchanges that last only for a short duration at the cost of learning the\nunderlying model. Case studies with real terrorism data from open-source\ndatabases are provided to illustrate the performance of the proposed\nmethodology. \n\n"}
{"id": "1207.3100", "contents": "Title: Set-valued dynamic treatment regimes for competing outcomes Abstract: Dynamic treatment regimes operationalize the clinical decision process as a\nsequence of functions, one for each clinical decision, where each function\ntakes as input up-to-date patient information and gives as output a single\nrecommended treatment. Current methods for estimating optimal dynamic treatment\nregimes, for example Q-learning, require the specification of a single outcome\nby which the `goodness' of competing dynamic treatment regimes are measured.\nHowever, this is an over-simplification of the goal of clinical decision\nmaking, which aims to balance several potentially competing outcomes. For\nexample, often a balance must be struck between treatment effectiveness and\nside-effect burden. We propose a method for constructing dynamic treatment\nregimes that accommodates competing outcomes by recommending sets of treatments\nat each decision point. Formally, we construct a sequence of set-valued\nfunctions that take as input up-to-date patient information and give as output\na recommended subset of the possible treatments. For a given patient history,\nthe recommended set of treatments contains all treatments that are not inferior\naccording to any of the competing outcomes. When there is more than one\ndecision point, constructing these set-valued functions requires solving a\nnon-trivial enumeration problem. We offer an exact enumeration algorithm by\nrecasting the problem as a linear mixed integer program. The proposed methods\nare illustrated using data from a depression study and the CATIE schizophrenia\nstudy. \n\n"}
{"id": "1207.3246", "contents": "Title: Testing instantaneous causality in presence of non constant\n  unconditional variance Abstract: The problem of testing instantaneous causality between variables with\ntime-varying unconditional variance is investigated. It is shown that the\nclassical tests based on the assumption of stationary processes must be avoided\nin our non standard framework. More precisely we underline that the standard\ntest does not control the type I errors, while the tests with White (1980) and\nHeteroscedastic Autocorrelation Consistent (HAC) corrections can suffer from a\nsevere loss of power when the variance is not constant. Consequently a modified\ntest based on a bootstrap procedure is proposed. The relevance of the modified\ntest is underlined through a simulation study. The tests considered in this\npaper are also compared by investigating the instantaneous causality relations\nbetween US macroeconomic variables. \n\n"}
{"id": "1207.6430", "contents": "Title: Optimal Data Collection For Informative Rankings Expose Well-Connected\n  Graphs Abstract: Given a graph where vertices represent alternatives and arcs represent\npairwise comparison data, the statistical ranking problem is to find a\npotential function, defined on the vertices, such that the gradient of the\npotential function agrees with the pairwise comparisons. Our goal in this paper\nis to develop a method for collecting data for which the least squares\nestimator for the ranking problem has maximal Fisher information. Our approach,\nbased on experimental design, is to view data collection as a bi-level\noptimization problem where the inner problem is the ranking problem and the\nouter problem is to identify data which maximizes the informativeness of the\nranking. Under certain assumptions, the data collection problem decouples,\nreducing to a problem of finding multigraphs with large algebraic connectivity.\nThis reduction of the data collection problem to graph-theoretic questions is\none of the primary contributions of this work. As an application, we study the\nYahoo! Movie user rating dataset and demonstrate that the addition of a small\nnumber of well-chosen pairwise comparisons can significantly increase the\nFisher informativeness of the ranking. As another application, we study the\n2011-12 NCAA football schedule and propose schedules with the same number of\ngames which are significantly more informative. Using spectral clustering\nmethods to identify highly-connected communities within the division, we argue\nthat the NCAA could improve its notoriously poor rankings by simply scheduling\nmore out-of-conference games. \n\n"}
{"id": "1207.6606", "contents": "Title: Weighted sampling, Maximum Likelihood and minimum divergence estimators Abstract: This paper explores Maximum Likelihood in parametric models in the context of\nSanov type Large Deviation Probabilities. MLE in parametric models under\nweighted sampling is shown to be associated with the minimization of a specific\ndivergence criterion defined with respect to the distribution of the weights.\nSome properties of the resulting inferential procedure are presented; Bahadur\nefficiency of tests are also considered in this context. \n\n"}
{"id": "1208.2396", "contents": "Title: Do cycles dissipate when subjects must choose simultaneously? Abstract: This question is raised by Cason, Friedman and Hopkins (CFH, 2012) after they\nfirstly found and indexed quantitatively the cycles in a continuous time\nexperiment. To answer this question, we use the data from standard RPS\nexperiment. Our experiments are of the traditional setting - in each of\nrepeated rounds, the subjects are paired with random matching, using pure\nstrategy and must choose simultaneously, and after each round, each subject\nobtains only private information. This economics environment is a decartelized\nand low-information one.\n  Using the cycle rotation indexes (CRI, developed by CFH) method, we find, the\ncycles not only exist but also persist in our experiment. Meanwhile, the\ncycles' direction are consistent with 'standard' learning models. That is the\nanswer to the CHF question: Cycles do not dissipate in the simultaneously\nchoose game.\n  In addtion, we discuss three questions (1) why significant cycles are uneasy\nto be obtained in traditional setting experiments; (2) why CRI can be an iconic\nindexing-method for 'standard' evolution dynamics; and (3) where more cycles\ncould be expected. \n\n"}
{"id": "1208.3577", "contents": "Title: Rejoinder to \"Statistical Modeling of Spatial Extremes\" Abstract: Rejoinder to \"Statistical Modeling of Spatial Extremes\" by A. C. Davison, S.\nA. Padoan and M. Ribatet [arXiv:1208.3378]. \n\n"}
{"id": "1208.4118", "contents": "Title: Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians Abstract: We present a Hamiltonian Monte Carlo algorithm to sample from multivariate\nGaussian distributions in which the target space is constrained by linear and\nquadratic inequalities or products thereof. The Hamiltonian equations of motion\ncan be integrated exactly and there are no parameters to tune. The algorithm\nmixes faster and is more efficient than Gibbs sampling. The runtime depends on\nthe number and shape of the constraints but the algorithm is highly\nparallelizable. In many cases, we can exploit special structure in the\ncovariance matrices of the untruncated Gaussian to further speed up the\nruntime. A simple extension of the algorithm permits sampling from\ndistributions whose log-density is piecewise quadratic, as in the \"Bayesian\nLasso\" model. \n\n"}
{"id": "1208.5467", "contents": "Title: Censored quantile regression processes under dependence and penalization Abstract: We consider quantile regression processes from censored data under dependent\ndata structures and derive a uniform Bahadur representation for those\nprocesses. We also consider cases where the dimension of the parameter in the\nquantile regression model is large. It is demonstrated that traditional\npenalized estimators such as the adaptive lasso yield sub-optimal rates if the\ncoefficients of the quantile regression cross zero. New penalization techniques\nare introduced which are able to deal with specific problems of censored data\nand yield estimates with an optimal rate. In contrast to most of the\nliterature, the asymptotic analysis does not require the assumption of\nindependent observations, but is based on rather weak assumptions, which are\nsatisfied for many kinds of dependent data. \n\n"}
{"id": "1208.5476", "contents": "Title: On characteristics of an ordinary differential equation and a related\n  inverse problem in epidemiology Abstract: In this work we examine the properties of a recently described ordinary\ndifferential equation that relates the age-specific prevalence of a chronic\ndisease with the incidence and mortalities of the diseased and healthy persons.\nThe equation has been used to estimate the incidence from prevalence data,\nwhich is an inverse problem. The ill-posedness of this problem is proven, too. \n\n"}
{"id": "1209.1270", "contents": "Title: A practical recipe to fit discrete power-law distributions Abstract: Power laws pervade statistical physics and complex systems, but,\ntraditionally, researchers in these fields have paid little attention to\nproperly fit these distributions. Who has not seen (or even shown) a log-log\nplot of a completely curved line pretending to be a power law? Recently,\nClauset et al. have proposed a method to decide if a set of values of a\nvariable has a distribution whose tail is a power law. The key of their\nprocedure is the identification of the minimum value of the variable for which\nthe fit holds, which is selected as the value for which the Kolmogorov-Smirnov\ndistance between the empirical distribution and its maximum-likelihood fit is\nminimum. However, it has been shown that this method can reject the power-law\nhypothesis even in the case of power-law simulated data. Here we propose a\nsimpler selection criterion, which is illustrated with the more involving case\nof discrete power-law distributions. \n\n"}
{"id": "1209.2072", "contents": "Title: On the impossibility of constructing good population mean estimators in\n  a realistic Respondent Driven Sampling model Abstract: Current methods for population mean estimation from data collected by\nRespondent Driven Sampling (RDS) are based on the Horvitz-Thompson estimator\ntogether with a set of assumptions on the sampling model under which the\ninclusion probabilities can be determined from the information contained in the\ndata. In this paper, we argue that such set of assumptions are too simplistic\nto be realistic and that under realistic sampling models, the situation is far\nmore complicated. Specifically, we study a realistic RDS sampling model that is\nmotivated by a real world RDS dataset. We show that, for this model, the\ninclusion probabilities, which are necessary for the application of the\nHorvitz-Thompson estimator, can not be determined by the information in the\nsample alone. An implication is that, unless additional information about the\nunderlying population network is obtained, it is hopeless to conceive of a\ngeneral theory of population mean estimation from current RDS data. \n\n"}
{"id": "1209.5350", "contents": "Title: Learning Topic Models and Latent Bayesian Networks Under Expansion\n  Constraints Abstract: Unsupervised estimation of latent variable models is a fundamental problem\ncentral to numerous applications of machine learning and statistics. This work\npresents a principled approach for estimating broad classes of such models,\nincluding probabilistic topic models and latent linear Bayesian networks, using\nonly second-order observed moments. The sufficient conditions for\nidentifiability of these models are primarily based on weak expansion\nconstraints on the topic-word matrix, for topic models, and on the directed\nacyclic graph, for Bayesian networks. Because no assumptions are made on the\ndistribution among the latent variables, the approach can handle arbitrary\ncorrelations among the topics or latent factors. In addition, a tractable\nlearning method via $\\ell_1$ optimization is proposed and studied in numerical\nexperiments. \n\n"}
{"id": "1209.6254", "contents": "Title: Diagnostics for Respondent-driven Sampling Abstract: Respondent-driven sampling (RDS) is a widely used method for sampling from\nhard-to-reach human populations, especially groups most at-risk for HIV/AIDS.\nData are collected through a peer-referral process in which current sample\nmembers harness existing social networks to recruit additional sample members.\nRDS has proven to be a practical method of data collection in many difficult\nsettings and has been adopted by leading public health organizations around the\nworld. Unfortunately, inference from RDS data requires many strong assumptions\nbecause the sampling design is not fully known and is partially beyond the\ncontrol of the researcher. In this paper, we introduce diagnostic tools for\nmost of the assumptions underlying RDS inference. We also apply these\ndiagnostics in a case study of 12 populations at increased risk for HIV/AIDS.\nWe developed these diagnostics to enable RDS researchers to better understand\ntheir data and to encourage future statistical research on RDS. \n\n"}
{"id": "1210.0493", "contents": "Title: Exponential-Family Random Graph Models for Rank-Order Relational Data Abstract: Rank-order relational data, in which each actor ranks the others according to\nsome criterion, often arise from sociometric measurements of judgment (e.g.,\nself-reported interpersonal interaction) or preference (e.g., relative liking).\nWe propose a class of exponential-family models for rank-order relational data\nand derive a new class of sufficient statistics for such data, which assume no\nmore than within-subject ordinal properties. Application of MCMC MLE to this\nfamily allows us to estimate effects for a variety of plausible mechanisms\ngoverning rank structure in cross-sectional context, and to model the evolution\nof such structures over time. We apply this framework to model the evolution of\nrelative liking judgments in an acquaintance process, and to model recall of\nrelative volume of interpersonal interaction among members of a technology\neducation program. \n\n"}
{"id": "1210.1840", "contents": "Title: A Further (Itakura-Saito/beta=0) Bi-stochaticization and Associated\n  Clustering/Regionalization of the 3,107-County 1995-2000 U. S. Migration\n  Network Abstract: We extend to the beta-divergence (Itakura-Saito) case beta =0, the\ncomparative bi-stochaticization analyses-previously conducted (arXiv:1208.3428)\nfor the (Kullback-Leibler) beta=1 and (squared-Euclidean) beta = 2 cases -of\nthe 3,107 - county 1995-2000 U. S. migration network. A heuristic, \"greedy\"\nalgorithm is devised. While the largest 25,329 entries of the 735,531 non-zero\nentries of the bi-stochasticized table - in the beta=1 case - are required to\ncomplete the widely-applied two-stage (double-standardization and\nstrong-component hierarchical clustering) procedure, 105,363 of the 735,531 are\nneeded (reflective of greater uniformity of entries) in the beta=0 instance.\nThe North Carolina counties of Mecklenburg (Charlotte) and Wake (Raleigh) are\nconsiderably relatively more cosmopolitan in the beta=0 study. The Colorado\ncounty of El Paso (Colorado Springs) replaces the Florida Atlantic county of\nBrevard (the \"Space Coast\") as the most cosmopolitan, with Brevard becoming the\nsecond-most. Honolulu County splinters away from the other four (still-grouped)\nHawaiian counties, becoming the fifth most cosmopolitan county nation-wide. The\nfive counties of Rhode Island remain intact as a regional entity, but the eight\ncounties of Connecticut fragment, leaving only five counties clustered. \n\n"}
{"id": "1210.6799", "contents": "Title: Multiple imputation of covariates by fully conditional specification:\n  accommodating the substantive model Abstract: Missing covariate data commonly occur in epidemiological and clinical\nresearch, and are often dealt with using multiple imputation (MI). Imputation\nof partially observed covariates is complicated if the substantive model is\nnon-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g.\nsquared) or interaction terms, and standard software implementations of MI may\nimpute covariates from models that are incompatible with such substantive\nmodels. We show how imputation by fully conditional specification, a popular\napproach for performing MI, can be modified so that covariates are imputed from\nmodels which are compatible with the substantive model. We investigate through\nsimulation the performance of this proposal, and compare it to existing\napproaches. Simulation results suggest our proposal gives consistent estimates\nfor a range of common substantive models, including models which contain\nnon-linear covariate effects or interactions, provided data are missing at\nrandom and the assumed imputation models are correctly specified and mutually\ncompatible. \n\n"}
{"id": "1211.1456", "contents": "Title: Non-parametric shrinkage mean estimation for quadratic loss functions\n  with unknown covariance matrices Abstract: In this paper, a shrinkage estimator for the population mean is proposed\nunder known quadratic loss functions with unknown covariance matrices. The new\nestimator is non-parametric in the sense that it does not assume a specific\nparametric distribution for the data and it does not require the prior\ninformation on the population covariance matrix. Analytical results on the\nimprovement of the proposed shrinkage estimator are provided and some\ncorresponding asymptotic properties are also derived. Finally, we demonstrate\nthe practical improvement of the proposed method over existing methods through\nextensive simulation studies and real data analysis. Keywords: High-dimensional\ndata; Shrinkage estimator; Large $p$ small $n$; $U$-statistic. \n\n"}
{"id": "1211.1547", "contents": "Title: A note on p-values interpreted as plausibilities Abstract: P-values are a mainstay in statistics but are often misinterpreted. We\npropose a new interpretation of p-value as a meaningful plausibility, where\nthis is to be interpreted formally within the inferential model framework. We\nshow that, for most practical hypothesis testing problems, there exists an\ninferential model such that the corresponding plausibility function, evaluated\nat the null hypothesis, is exactly the p-value. The advantages of this\nrepresentation are that the notion of plausibility is consistent with the way\npractitioners use and interpret p-values, and the plausibility calculation\navoids the troublesome conditioning on the truthfulness of the null. This\nconnection with plausibilities also reveals a shortcoming of standard p-values\nin problems with non-trivial parameter constraints. \n\n"}
{"id": "1211.1642", "contents": "Title: Randomized Dimension Reduction on Massive Data Abstract: Scalability of statistical estimators is of increasing importance in modern\napplications and dimension reduction is often used to extract relevant\ninformation from data. A variety of popular dimension reduction approaches can\nbe framed as symmetric generalized eigendecomposition problems. In this paper\nwe outline how taking into account the low rank structure assumption implicit\nin these dimension reduction approaches provides both computational and\nstatistical advantages. We adapt recent randomized low-rank approximation\nalgorithms to provide efficient solutions to three dimension reduction methods:\nPrincipal Component Analysis (PCA), Sliced Inverse Regression (SIR), and\nLocalized Sliced Inverse Regression (LSIR). A key observation in this paper is\nthat randomization serves a dual role, improving both computational and\nstatistical performance. This point is highlighted in our experiments on real\nand simulated data. \n\n"}
{"id": "1211.3671", "contents": "Title: L$_1$ Regularization for Reconstruction of a non-equilibrium Ising Model Abstract: The couplings in a sparse asymmetric, asynchronous Ising network are\nreconstructed using an exact learning algorithm. L$_1$ regularization is used\nto remove the spurious weak connections that would otherwise be found by simply\nminimizing the minus likelihood of a finite data set. In order to see how L$_1$\nregularization works in detail, we perform the calculation in several ways\nincluding (1) by iterative minimization of a cost function equal to minus the\nlog likelihood of the data plus an L$_1$ penalty term, and (2) an approximate\nscheme based on a quadratic expansion of the cost function around its minimum.\nIn these schemes, we track how connections are pruned as the strength of the\nL$_1$ penalty is increased from zero to large values. The performance of the\nmethods for various coupling strengths is quantified using ROC curves. \n\n"}
{"id": "1211.5290", "contents": "Title: EMMIX-uskew: An R Package for Fitting Mixtures of Multivariate Skew\n  t-distributions via the EM Algorithm Abstract: This paper describes an algorithm for fitting finite mixtures of unrestricted\nMultivariate Skew t (FM-uMST) distributions. The package EMMIX-uskew implements\na closed-form expectation-maximization (EM) algorithm for computing the maximum\nlikelihood (ML) estimates of the parameters for the (unrestricted) FM-MST model\nin R. EMMIX-uskew also supports visualization of fitted contours in two and\nthree dimensions, and random sample generation from a specified FM-uMST\ndistribution.\n  Finite mixtures of skew t-distributions have proven to be useful in modelling\nheterogeneous data with asymmetric and heavy tail behaviour, for example,\ndatasets from flow cytometry. In recent years, various versions of mixtures\nwith multivariate skew t (MST) distributions have been proposed. However, these\nmodels adopted some restricted characterizations of the component MST\ndistributions so that the E-step of the EM algorithm can be evaluated in closed\nform. This paper focuses on mixtures with unrestricted MST components, and\ndescribes an iterative algorithm for the computation of the ML estimates of its\nmodel parameters.\n  The usefulness of the proposed algorithm is demonstrated in three\napplications to real data sets. The first example illustrates the use of the\nmain function fmmst in the package by fitting a MST distribution to a bivariate\nunimodal flow cytometric sample. The second example fits a mixture of MST\ndistributions to the Australian Institute of Sport (AIS) data, and demonstrate\nthat EMMIX-uskew can provide better clustering results than mixtures with\nrestricted MST components. In the third example, EMMIX-uskew is applied to\nclassify cells in a trivariate flow cytometric dataset. Comparisons with other\navailable methods suggests that the EMMIX-uskew result achieved a lower\nmisclassification rate with respect to the labels given by benchmark gating\nanalysis. \n\n"}
{"id": "1211.5472", "contents": "Title: A Bayesian approach to estimate changes in condom use from limited HIV\n  prevalence data Abstract: Evaluation of HIV large scale interventions programme is becoming\nincreasingly important, but impact estimates frequently hinge on knowledge of\nchanges in behaviour such as the frequency of condom use (CU) over time, or\nother self-reported behaviour changes, for which we generally have limited or\npotentially biased data. We employ a Bayesian inference methodology that\nincorporates a dynamic HIV transmission dynamics model to estimate CU time\ntrends from HIV prevalence data. Estimation is implemented via particle Markov\nChain Monte Carlo methods, applied for the first time in this context. The\npreliminary choice of the formulation for the time varying parameter reflecting\nthe proportion of CU is critical in the context studied, due to the very\nlimited amount of CU and HIV data available We consider various novel\nformulations to explore the trajectory of CU in time, based on diffusion-driven\ntrajectories and smooth sigmoid curves. Extensive series of numerical\nsimulations indicate that informative results can be obtained regarding the\namplitude of the increase in CU during an intervention, with good levels of\nsensitivity and specificity performance in effectively detecting changes. The\napplication of this method to a real life problem illustrates how it can help\nevaluate HIV intervention from few observational studies and suggests that\nthese methods can potentially be applied in many different contexts. \n\n"}
{"id": "1212.4786", "contents": "Title: A statistical framework for joint eQTL analysis in multiple tissues Abstract: Mapping expression Quantitative Trait Loci (eQTLs) represents a powerful and\nwidely-adopted approach to identifying putative regulatory variants and linking\nthem to specific genes. Up to now eQTL studies have been conducted in a\nrelatively narrow range of tissues or cell types. However, understanding the\nbiology of organismal phenotypes will involve understanding regulation in\nmultiple tissues, and ongoing studies are collecting eQTL data in dozens of\ncell types. Here we present a statistical framework for powerfully detecting\neQTLs in multiple tissues or cell types (or, more generally, multiple\nsubgroups). The framework explicitly models the potential for each eQTL to be\nactive in some tissues and inactive in others. By modeling the sharing of\nactive eQTLs among tissues this framework increases power to detect eQTLs that\nare present in more than one tissue compared with \"tissue-by-tissue\" analyses\nthat examine each tissue separately. Conversely, by modeling the inactivity of\neQTLs in some tissues, the framework allows the proportion of eQTLs shared\nacross different tissues to be formally estimated as parameters of a model,\naddressing the difficulties of accounting for incomplete power when comparing\noverlaps of eQTLs identified by tissue-by-tissue analyses. Applying our\nframework to re-analyze data from transformed B cells, T cells and fibroblasts\nwe find that it substantially increases power compared with tissue-by-tissue\nanalysis, identifying 63% more genes with eQTLs (at FDR=0.05). Further the\nresults suggest that, in contrast to previous analyses of the same data, the\nmajority of eQTLs detectable in these data are shared among all three tissues. \n\n"}
{"id": "1212.5750", "contents": "Title: Week 50 Influenza Forecast for the 2012-2013 U.S. Season Abstract: We present results of a forecast initiated following assimilation of\nobservations for week Week 50 (i.e. the forecast begins December 16, 2012) of\nthe 2012-2013 influenza season for municipalities in the United States. The\nforecast was made on December 21, 2012. Results from forecasts initiated the\nthree previous weeks (Weeks 47-49) are also presented. Also results from\nforecasts generated with an SIRS model without absolute humidity forcing (no\nAH) are shown. \n\n"}
{"id": "1212.6757", "contents": "Title: Testing Regression Monotonicity in Econometric Models Abstract: Monotonicity is a key qualitative prediction of a wide array of economic\nmodels derived via robust comparative statics. It is therefore important to\ndesign effective and practical econometric methods for testing this prediction\nin empirical analysis. This paper develops a general nonparametric framework\nfor testing monotonicity of a regression function. Using this framework, a\nbroad class of new tests is introduced, which gives an empirical researcher a\nlot of flexibility to incorporate ex ante information she might have. The paper\nalso develops new methods for simulating critical values, which are based on\nthe combination of a bootstrap procedure and new selection algorithms. These\nmethods yield tests that have correct asymptotic size and are asymptotically\nnonconservative. It is also shown how to obtain an adaptive rate optimal test\nthat has the best attainable rate of uniform consistency against models whose\nregression function has Lipschitz-continuous first-order derivatives and that\nautomatically adapts to the unknown smoothness of the regression function.\nSimulations show that the power of the new tests in many cases significantly\nexceeds that of some prior tests, e.g. that of Ghosal, Sen, and Van der Vaart\n(2000). An application of the developed procedures to the dataset of Ellison\nand Ellison (2011) shows that there is some evidence of strategic entry\ndeterrence in pharmaceutical industry where incumbents may use strategic\ninvestment to prevent generic entries when their patents expire. \n\n"}
{"id": "1301.1178", "contents": "Title: Modeling high energy cosmic rays mass composition data via mixtures of\n  multivariate skew-t distributions Abstract: We consider multivariate skew-t distributions for modeling composition data\nof high energy cosmic rays. The model has been validated with simulated data\nfor different primary nuclei and hadronic models focusing on the depth of\nmaximum Xmax and number of muons N{\\mu} observables. Further, we consider\nmixtures of multivariate skew-t distributions for cosmic ray mass composition\ndetermination and event-by-event classification. With respect to other\napproaches in the field, it is based on analytical calculations and allows to\nincorporate different sets of constraints provided by the present hadronic\nmodels. We present some applications to simulated data sets generated with\ndifferent nuclear abundances assumptions. As it does not fully rely on the\nhadronic model predictions, the method is particularly suited to the current\nexperimental scenario in which evidences of discrepancies of the measured data\nwith respect to the models have been reported for some shower observables, such\nas the number of muons at ground level. \n\n"}
{"id": "1301.2975", "contents": "Title: Fast Approximate Bayesian Computation for discretely observed Markov\n  models using a factorised posterior distribution Abstract: Many modern statistical applications involve inference for complicated\nstochastic models for which the likelihood function is difficult or even\nimpossible to calculate, and hence conventional likelihood-based inferential\nechniques cannot be used. In such settings, Bayesian inference can be performed\nusing Approximate Bayesian Computation (ABC). However, in spite of many recent\ndevelopments to ABC methodology, in many applications the computational cost of\nABC necessitates the choice of summary statistics and tolerances that can\npotentially severely bias the estimate of the posterior.\n  We propose a new \"piecewise\" ABC approach suitable for discretely observed\nMarkov models that involves writing the posterior density of the parameters as\na product of factors, each a function of only a subset of the data, and then\nusing ABC within each factor. The approach has the advantage of side-stepping\nthe need to choose a summary statistic and it enables a stringent tolerance to\nbe set, making the posterior \"less approximate\". We investigate two methods for\nestimating the posterior density based on ABC samples for each of the factors:\nthe first is to use a Gaussian approximation for each factor, and the second is\nto use a kernel density estimate. Both methods have their merits. The Gaussian\napproximation is simple, fast, and probably adequate for many applications. On\nthe other hand, using instead a kernel density estimate has the benefit of\nconsistently estimating the true ABC posterior as the number of ABC samples\ntends to infinity. We illustrate the piecewise ABC approach for three examples;\nin each case, the approach enables \"exact matching\" between simulations and\ndata and offers fast and accurate inference. \n\n"}
{"id": "1301.3166", "contents": "Title: Diagnostic tools of approximate Bayesian computation using the coverage\n  property Abstract: Approximate Bayesian computation (ABC) is an approach for sampling from an\napproximate posterior distribution in the presence of a computationally\nintractable likelihood function. A common implementation is based on simulating\nmodel, parameter and dataset triples, (m,\\theta,y), from the prior, and then\naccepting as samples from the approximate posterior, those pairs (m,\\theta) for\nwhich y, or a summary of y, is \"close\" to the observed data. Closeness is\ntypically determined though a distance measure and a kernel scale parameter,\n\\epsilon. Appropriate choice of \\epsilon is important to producing a good\nquality approximation. This paper proposes diagnostic tools for the choice of\n\\epsilon based on assessing the coverage property, which asserts that credible\nintervals have the correct coverage levels. We provide theoretical results on\ncoverage for both model and parameter inference, and adapt these into\ndiagnostics for the ABC context. We re-analyse a study on human demographic\nhistory to determine whether the adopted posterior approximation was\nappropriate. R code implementing the proposed methodology is freely available\nin the package \"abc.\" \n\n"}
{"id": "1301.7118", "contents": "Title: A note on selection stability: combining stability and prediction Abstract: Recently, many regularized procedures have been proposed for variable\nselection in linear regression, but their performance depends on the tuning\nparameter selection. Here a criterion for the tuning parameter selection is\nproposed, which combines the strength of both stability selection and\ncross-validation and therefore is referred as the prediction and stability\nselection (PASS). The selection consistency is established assuming the data\ngenerating model is a subset of the full model, and the small sample\nperformance is demonstrated through some simulation studies where the\nassumption is either held or violated. \n\n"}
{"id": "1302.0360", "contents": "Title: On Weighted Low-Rank Approximation Abstract: Our main interest is the low-rank approximation of a matrix in R^m.n under a\nweighted Frobenius norm. This norm associates a weight to each of the (m x n)\nmatrix entries. We conjecture that the number of approximations is at most\nmin(m, n).\n  We also investigate how the approximations depend on the weight-values. \n\n"}
{"id": "1302.2525", "contents": "Title: Foundations of Descriptive and Inferential Statistics Abstract: These lecture notes were written with the aim to provide an accessible though\ntechnically solid introduction to the logic of systematical analyses of\nstatistical data to both undergraduate and postgraduate students, in particular\nin the Social Sciences, Economics, and the Financial Services. They may also\nserve as a general reference for the application of quantitative--empirical\nresearch methods. In an attempt to encourage the adoption of an\ninterdisciplinary perspective on quantitative problems arising in practice, the\nnotes cover the four broad topics (i) descriptive statistical processing of raw\ndata, (ii) elementary probability theory, (iii) the operationalisation of\none-dimensional latent statistical variables according to Likert's widely used\nscaling approach, and (iv) null hypothesis significance testing within the\nfrequentist approach to probability theory concerning (a) distributional\ndifferences of variables between subgroups of a target population, and (b)\nstatistical associations between two variables. The relevance of effect sizes\nfor making inferences is emphasised. These lecture notes are fully hyperlinked,\nthus providing a direct route to original scientific papers as well as to\ninteresting biographical information. They also list many commands for running\nstatistical functions and data analysis routines in the software packages R,\nSPSS, EXCEL and OpenOffice. The immediate involvement in actual data analysis\npractices is strongly recommended. \n\n"}
{"id": "1302.4404", "contents": "Title: Analysis of Forensic DNA Mixtures with Artefacts Abstract: DNA is now routinely used in criminal investigations and court cases,\nalthough DNA samples taken at crime scenes are of varying quality and therefore\npresent challenging problems for their interpretation. We present a statistical\nmodel for the quantitative peak information obtained from an electropherogram\n(EPG) of a forensic DNA sample and illustrate its potential use for the\nanalysis of criminal cases. In contrast to most previously used methods, we\ndirectly model the peak height information and incorporates important artefacts\nassociated with the production of the EPG. Our model has a number of unknown\nparameters, and we show that these can be estimated by the method of maximum\nlikelihood in the presence of multiple unknown contributors, and their\napproximate standard errors calculated; the computations exploit a Bayesian\nnetwork representation of the model. A case example from a UK trial, as\nreported in the literature, is used to illustrate the efficacy and use of the\nmodel, both in finding likelihood ratios to quantify the strength of evidence,\nand in the deconvolution of mixtures for the purpose of finding likely profiles\nof one or more unknown contributors to a DNA sample. Our model is readily\nextended to simultaneous analysis of more than one mixture as illustrated in a\ncase example. We show that combination of evidence from several samples may\ngive an evidential strength close to that of a single source trace and thus\nmodelling of peak height information provides for a potentially very efficient\nmixture analysis. \n\n"}
{"id": "1302.4922", "contents": "Title: Structure Discovery in Nonparametric Regression through Compositional\n  Kernel Search Abstract: Despite its importance, choosing the structural form of the kernel in\nnonparametric regression remains a black art. We define a space of kernel\nstructures which are built compositionally by adding and multiplying a small\nnumber of base kernels. We present a method for searching over this space of\nstructures which mirrors the scientific discovery process. The learned\nstructures can often decompose functions into interpretable components and\nenable long-range extrapolation on time-series datasets. Our structure search\nmethod outperforms many widely used kernels and kernel combination methods on a\nvariety of prediction tasks. \n\n"}
{"id": "1302.7088", "contents": "Title: Continuous-time Infinite Dynamic Topic Models Abstract: Topic models are probabilistic models for discovering topical themes in\ncollections of documents. In real world applications, these models provide us\nwith the means of organizing what would otherwise be unstructured collections.\nThey can help us cluster a huge collection into different topics or find a\nsubset of the collection that resembles the topical theme found in an article\nat hand.\n  The first wave of topic models developed were able to discover the prevailing\ntopics in a big collection of documents spanning a period of time. It was later\nrealized that these time-invariant models were not capable of modeling 1) the\ntime varying number of topics they discover and 2) the time changing structure\nof these topics. Few models were developed to address this two deficiencies.\nThe online-hierarchical Dirichlet process models the documents with a time\nvarying number of topics. It varies the structure of the topics over time as\nwell. However, it relies on document order, not timestamps to evolve the model\nover time. The continuous-time dynamic topic model evolves topic structure in\ncontinuous-time. However, it uses a fixed number of topics over time.\n  In this dissertation, I present a model, the continuous-time infinite dynamic\ntopic model, that combines the advantages of these two models 1) the\nonline-hierarchical Dirichlet process, and 2) the continuous-time dynamic topic\nmodel. More specifically, the model I present is a probabilistic topic model\nthat does the following: 1) it changes the number of topics over continuous\ntime, and 2) it changes the topic structure over continuous-time.\n  I compared the model I developed with the two other models with different\nsetting values. The results obtained were favorable to my model and showed the\nneed for having a model that has a continuous-time varying number of topics and\ntopic structure. \n\n"}
{"id": "1303.5195", "contents": "Title: Adding a systematic uncertainty to the signal estimation in the\n  on/off-zone measurements Abstract: The measurements with the background estimation from an off-zone are widely\nused in astrophysics, accelerator physics and other areas. Usually, the\nexpected number of the background events in the off-zone and in the on-zone is\nknown with a limited precision. This fact should be included as a systematic\nuncertainty. In this note an overview of the statistical methods which estimate\nthe range and the significance of the measured signal is done. The method which\nincludes a systematic uncertainty is developed for the on/off-zone measurements\nand compared with other existing methods. \n\n"}
{"id": "1303.6223", "contents": "Title: Random Intersection Trees Abstract: Finding interactions between variables in large and high-dimensional datasets\nis often a serious computational challenge. Most approaches build up\ninteraction sets incrementally, adding variables in a greedy fashion. The\ndrawback is that potentially informative high-order interactions may be\noverlooked. Here, we propose at an alternative approach for classification\nproblems with binary predictor variables, called Random Intersection Trees. It\nworks by starting with a maximal interaction that includes all variables, and\nthen gradually removing variables if they fail to appear in randomly chosen\nobservations of a class of interest. We show that informative interactions are\nretained with high probability, and the computational complexity of our\nprocedure is of order $p^\\kappa$ for a value of $\\kappa$ that can reach values\nas low as 1 for very sparse data; in many more general settings, it will still\nbeat the exponent $s$ obtained when using a brute force search constrained to\norder $s$ interactions. In addition, by using some new ideas based on min-wise\nhash schemes, we are able to further reduce the computational cost.\nInteractions found by our algorithm can be used for predictive modelling in\nvarious forms, but they are also often of interest in their own right as useful\ncharacterisations of what distinguishes a certain class from others. \n\n"}
{"id": "1303.6447", "contents": "Title: Statistical inference for Sobol pick freeze Monte Carlo method Abstract: Many mathematical models involve input parameters, which are not precisely\nknown. Global sensitivity analysis aims to identify the parameters whose\nuncertainty has the largest impact on the variability of a quantity of interest\n(output of the model). One of the statistical tools used to quantify the\ninfluence of each input variable on the output is the Sobol sensitivity index.\nWe consider the statistical estimation of this index from a finite sample of\nmodel outputs. We study asymptotic and non-asymptotic properties of two\nestimators of Sobol indices. These properties are applied to significance tests\nand estimation by confidence intervals. \n\n"}
{"id": "1303.6686", "contents": "Title: Individual Performance and Leader's Laterality in Interactive Contests Abstract: Left-handedness is known to provide an intrinsic and tactical advantage at\ntop level in many sports involving interactive contests. Again, most of the\nrenowned leaders of the world are known to have been left-handed. Leadership\nplays an important role in politics, sports and mentorship. In this paper we\nshow that Cricket captains who bat left-handed have a strategic advantage over\nthe right-handed captains in One Day International (ODI) and Test matches. The\npresent study involving 46 left-handed captains and 148 right-handed captains\nin ODI matches, reveal a strong relation between leader's laterality and\nteam-member performance, demonstrating the critical importance of\nleft-handedness and successful leadership. The odds for superior batting\nperformance in an ODI match under left-handed captains are 89% higher than the\nodds under right-handed captains. Our study shows that left-handed captains are\nmore successful in extracting superior performance from the batsmen and bowlers\nin ODI and Test matches; perhaps indicating left-handed leaders are better\nmotivators as leaders when compared to right-handed captains. \n\n"}
{"id": "1304.1839", "contents": "Title: Reconstruction of Signals from Magnitudes of Redundant Representations:\n  The Complex Case Abstract: This paper is concerned with the question of reconstructing a vector in a\nfinite-dimensional complex Hilbert space when only the magnitudes of the\ncoefficients of the vector under a redundant linear map are known. We present\nnew invertibility results as well an iterative algorithm that finds the\nleast-square solution and is robust in the presence of noise. We analyze its\nnumerical performance by comparing it to the Cramer-Rao lower bound. \n\n"}
{"id": "1304.2129", "contents": "Title: A gentle introduction to the discrete Laplace method for estimating\n  Y-STR haplotype frequencies Abstract: Y-STR data simulated under a Fisher-Wright model of evolution with a\nsingle-step mutation model turns out to be well predicted by a method using\ndiscrete Laplace distributions. \n\n"}
{"id": "1304.3480", "contents": "Title: Friendship Paradox Redux: Your Friends Are More Interesting Than You Abstract: Feld's friendship paradox states that \"your friends have more friends than\nyou, on average.\" This paradox arises because extremely popular people, despite\nbeing rare, are overrepresented when averaging over friends. Using a sample of\nthe Twitter firehose, we confirm that the friendship paradox holds for >98% of\nTwitter users. Because of the directed nature of the follower graph on Twitter,\nwe are further able to confirm more detailed forms of the friendship paradox:\neveryone you follow or who follows you has more friends and followers than you.\nThis is likely caused by a correlation we demonstrate between Twitter activity,\nnumber of friends, and number of followers. In addition, we discover two new\nparadoxes: the virality paradox that states \"your friends receive more viral\ncontent than you, on average,\" and the activity paradox, which states \"your\nfriends are more active than you, on average.\" The latter paradox is important\nin regulating online communication. It may result in users having difficulty\nmaintaining optimal incoming information rates, because following additional\nusers causes the volume of incoming tweets to increase super-linearly. While\nusers may compensate for increased information flow by increasing their own\nactivity, users become information overloaded when they receive more\ninformation than they are able or willing to process. We compare the average\nsize of cascades that are sent and received by overloaded and underloaded\nusers. And we show that overloaded users post and receive larger cascades and\nthey are poor detector of small cascades. \n\n"}
{"id": "1304.4929", "contents": "Title: A new method to obtain risk neutral probability, without stochastic\n  calculus and price modeling, confirms the universal validity of\n  Black-Scholes-Merton formula and volatility's role Abstract: A new method is proposed to obtain the risk neutral probability of share\nprices without stochastic calculus and price modeling, via an embedding of the\nprice return modeling problem in Le Cam's statistical experiments framework.\nStrategies-probabilities $P_{t_0,n}$ and $P_{T,n}$ are thus determined and\nused, respectively,for the trader selling the share's European call option at\ntime $t_0$ and for the buyer who may exercise it in the future, at $T; \\ n$\nincreases with the number of share's transactions in $[t_0,T].$ When the\ntransaction times are dense in $[t_0,T]$ it is shown, with mild conditions,\nthat under each of these probabilities $\\log \\frac{S_T}{S_{t_0}}$ has\ninfinitely divisible distribution and in particular normal distribution for\n\"calm\" share; $S_t$ is the share's price at time $t.$ The price of the share's\ncall is the limit of the expected values of the call's payoff under the\ntranslated $P_{t_0,n}.$ It coincides for \"calm\" share prices with the\nBlack-Scholes-Merton formula with variance not necessarily proportional to\n$(T-t_0),$ thus confirming formula's universal validity without model\nassumptions. Additional results clarify volatility's role in the transaction\nand the behaviors of the trader and the buyer. Traders may use the pricing\nformulae after estimation of the unknown parameters. \n\n"}
{"id": "1304.5642", "contents": "Title: Spatio-Temporal Low Count Processes with Application to Violent Crime\n  Events Abstract: There is significant interest in being able to predict where crimes will\nhappen, for example to aid in the efficient tasking of police and other\nprotective measures. We aim to model both the temporal and spatial dependencies\noften exhibited by violent crimes in order to make such predictions. The\ntemporal variation of crimes typically follows patterns familiar in time series\nanalysis, but the spatial patterns are irregular and do not vary smoothly\nacross the area. Instead we find that spatially disjoint regions exhibit\ncorrelated crime patterns. It is this indeterminate inter-region correlation\nstructure along with the low-count, discrete nature of counts of serious crimes\nthat motivates our proposed forecasting tool. In particular, we propose to\nmodel the crime counts in each region using an integer-valued first order\nautoregressive process. We take a Bayesian nonparametric approach to flexibly\ndiscover a clustering of these region-specific time series. We then describe\nhow to account for covariates within this framework. Both approaches adjust for\nseasonality. We demonstrate our approach through an analysis of weekly reported\nviolent crimes in Washington, D.C. between 2001-2008. Our forecasts outperform\nstandard methods while additionally providing useful tools such as prediction\nintervals. \n\n"}
{"id": "1304.5974", "contents": "Title: Dynamic stochastic blockmodels: Statistical models for time-evolving\n  networks Abstract: Significant efforts have gone into the development of statistical models for\nanalyzing data in the form of networks, such as social networks. Most existing\nwork has focused on modeling static networks, which represent either a single\ntime snapshot or an aggregate view over time. There has been recent interest in\nstatistical modeling of dynamic networks, which are observed at multiple points\nin time and offer a richer representation of many complex phenomena. In this\npaper, we propose a state-space model for dynamic networks that extends the\nwell-known stochastic blockmodel for static networks to the dynamic setting. We\nthen propose a procedure to fit the model using a modification of the extended\nKalman filter augmented with a local search. We apply the procedure to analyze\na dynamic social network of email communication. \n\n"}
{"id": "1304.6715", "contents": "Title: Skewness and kurtosis unbiased by Gaussian uncertainties Abstract: Noise is an unavoidable part of most measurements which can hinder a correct\ninterpretation of the data. Uncertainties propagate in the data analysis and\ncan lead to biased results even in basic descriptive statistics such as the\ncentral moments and cumulants. Expressions of noise-unbiased estimates of\ncentral moments and cumulants up to the fourth order are presented under the\nassumption of independent Gaussian uncertainties, for weighted and unweighted\nstatistics. These results are expected to be relevant for applications of the\nskewness and kurtosis estimators such as outlier detections, normality tests\nand in automated classification procedures. The comparison of estimators\ncorrected and not corrected for noise biases is illustrated with simulations as\na function of signal-to-noise ratio, employing different sample sizes and\nweighting schemes. \n\n"}
{"id": "1305.0060", "contents": "Title: Complexity penalized hydraulic fracture localization and moment tensor\n  estimation under limited model information Abstract: In this paper we present a novel technique for micro-seismic localization\nusing a group sparse penalization that is robust to the focal mechanism of the\nsource and requires only a velocity model of the stratigraphy rather than a\nfull Green's function model of the earth's response. In this technique we\nconstruct a set of perfect delta detector responses, one for each detector in\nthe array, to a seismic event at a given location and impose a group sparsity\nacross the array. This scheme is independent of the moment tensor and exploits\nthe time compactness of the incident seismic signal. Furthermore we present a\nmethod for improving the inversion of the moment tensor and Green's function\nwhen the geometry of seismic array is limited. In particular we demonstrate\nthat both Tikhonov regularization and truncated SVD can improve the recovery of\nthe moment tensor and be robust to noise. We evaluate our algorithm on\nsynthetic data and present error bounds for both estimation of the moment\ntensor as well as localization. Furthermore we discuss the estimated moment\ntensor accuracy as a function of both array geometry and fault orientation. \n\n"}
{"id": "1305.3692", "contents": "Title: Point estimates in phylogenetic reconstructions Abstract: Motivation: The construction of statistics for summarizing posterior samples\nreturned by a Bayesian phylogenetic study has so far been hindered by the poor\ngeometric insights available into the space of phylogenetic trees, and ad hoc\nmethods such as the derivation of a consensus tree makeup for the\nill-definition of the usual concepts of posterior mean, while bootstrap methods\nmitigate the absence of a sound concept of variance. Yielding satisfactory\nresults with sufficiently concentrated posterior distributions, such methods\nfall short of providing a faithful summary of posterior distributions if the\ndata do not offer compelling evidence for a single topology.\n  Results: Building upon previous work of Billera et al., summary statistics\nsuch as sample mean, median and variance are defined as the geometric median,\nFr\\'echet mean and variance, respectively. Their computation is enabled by\nrecently published works, and embeds an algorithm for computing shortest paths\nin the space of trees. Studying the phylogeny of a set of plants, where several\ntree topologies occur in the posterior sample, the posterior mean balances\ncorrectly the contributions from the different topologies, where a consensus\ntree would be biased. Comparisons of the posterior mean, median and consensus\ntrees with the ground truth using simulated data also reveals the benefits of a\nsound averaging method when reconstructing phylogenetic trees. \n\n"}
{"id": "1305.4273", "contents": "Title: Likelihood-free Simulation-based Optimal Design Abstract: Simulation-based optimal design techniques are a convenient tool for solving\na particular class of optimal design problems. The goal is to find the optimal\nconfiguration of factor settings with respect to an expected utility criterion.\nThis criterion depends on the specified probability model for the data and on\nthe assumed prior distribution for the model parameters. We develop new\nsimulation-based optimal design methods which incorporate likelihood-free\napproaches and utilize them in novel applications.\n  Most simulation-based design strategies solve the intractable expected\nutility integral at a specific design point by using Monte Carlo simulations\nfrom the probability model. Optimizing the criterion over the design points is\ncarried out in a separate step. M\\\"uller (1999) introduces an MCMC algorithm\nwhich simultaneously addresses the simulation as well as the optimization\nproblem. In principle, the optimal design can be found by detecting the utility\nmode of the sampled design points. Several improvements have been suggested to\nfacilitate this task for multidimensional design problems (see e.g. Amzal et\nal. 2006).\n  We aim to extend this simulation-based design methodology to design problems\nwhere the likelihood of the probability model is of an unknown analytical form\nbut it is possible to simulate from the probability model. We further assume\nthat prior observations are available. In such a setting it is seems natural to\nemploy approximate Bayesian computation (ABC) techniques in order to be able to\nsimulate from the conditional probability model. We provide a thorough review\nof adjacent literature and we investigate the benefits and the limitations of\nour design methodology for a particular paradigmatic example. \n\n"}
{"id": "1305.4413", "contents": "Title: A Penalized Multi-trait Mixed Model for Association Mapping in\n  Pedigree-based GWAS Abstract: In genome-wide association studies (GWAS), penalization is an important\napproach for identifying genetic markers associated with trait while mixed\nmodel is successful in accounting for a complicated dependence structure among\nsamples. Therefore, penalized linear mixed model is a tool that combines the\nadvantages of penalization approach and linear mixed model. In this study, a\nGWAS with multiple highly correlated traits is analyzed. For GWAS with multiple\nquantitative traits that are highly correlated, the analysis using traits\nmarginally inevitably lose some essential information among multiple traits. We\npropose a penalized-MTMM, a penalized multivariate linear mixed model that\nallows both the within-trait and between-trait variance components\nsimultaneously for multiple traits. The proposed penalized-MTMM estimates\nvariance components using an AI-REML method and conducts variable selection and\npoint estimation simultaneously using group MCP and sparse group MCP. Best\nlinear unbiased predictor (BLUP) is used to find predictive values and the\nPearson's correlations between predictive values and their corresponding\nobservations are used to evaluate prediction performance. Both prediction and\nselection performance of the proposed approach and its comparison with the\nuni-trait penalized-LMM are evaluated through simulation studies. We apply the\nproposed approach to a GWAS data from Genetic Analysis Workshop (GAW) 18. \n\n"}
{"id": "1305.5601", "contents": "Title: Optimal Periodic Sensor Scheduling in Networks of Dynamical Systems Abstract: We consider the problem of finding optimal time-periodic sensor schedules for\nestimating the state of discrete-time dynamical systems. We assume that\n{multiple} sensors have been deployed and that the sensors are subject to\nresource constraints, which limits the number of times each can be activated\nover one period of the periodic schedule. We seek an algorithm that strikes a\nbalance between estimation accuracy and total sensor activations over one\nperiod. We make a correspondence between active sensors and the nonzero columns\nof estimator gain. We formulate an optimization problem in which we minimize\nthe trace of the error covariance with respect to the estimator gain while\nsimultaneously penalizing the number of nonzero columns of the estimator gain.\nThis optimization problem is combinatorial in nature, and we employ the\nalternating direction method of multipliers (ADMM) to find its locally optimal\nsolutions. Numerical results and comparisons with other sensor scheduling\nalgorithms in the literature are provided to illustrate the effectiveness of\nour proposed method. \n\n"}
{"id": "1305.5870", "contents": "Title: The Optimal Hard Threshold for Singular Values is 4/sqrt(3) Abstract: We consider recovery of low-rank matrices from noisy data by hard\nthresholding of singular values, where singular values below a prescribed\nthreshold $\\lambda$ are set to 0. We study the asymptotic MSE in a framework\nwhere the matrix size is large compared to the rank of the matrix to be\nrecovered, and the signal-to-noise ratio of the low-rank piece stays constant.\nThe AMSE-optimal choice of hard threshold, in the case of n-by-n matrix in\nnoise level \\sigma, is simply $(4/\\sqrt{3}) \\sqrt{n}\\sigma \\approx 2.309\n\\sqrt{n}\\sigma$ when $\\sigma$ is known, or simply $2.858\\cdot y_{med}$ when\n$\\sigma$ is unknown, where $y_{med}$ is the median empirical singular value.\nFor nonsquare $m$ by $n$ matrices with $m \\neq n$, these thresholding\ncoefficients are replaced with different provided constants. In our asymptotic\nframework, this thresholding rule adapts to unknown rank and to unknown noise\nlevel in an optimal manner: it is always better than hard thresholding at any\nother value, no matter what the matrix is that we are trying to recover, and is\nalways better than ideal Truncated SVD (TSVD), which truncates at the true rank\nof the low-rank matrix we are trying to recover. Hard thresholding at the\nrecommended value to recover an n-by-n matrix of rank r guarantees an AMSE at\nmost $3nr\\sigma^2$. In comparison, the guarantee provided by TSVD is\n$5nr\\sigma^2$, the guarantee provided by optimally tuned singular value soft\nthresholding is $6nr\\sigma^2$, and the best guarantee achievable by any\nshrinkage of the data singular values is $2nr\\sigma^2$. Empirical evidence\nshows that these AMSE properties of the $4/\\sqrt{3}$ thresholding rule remain\nvalid even for relatively small n, and that performance improvement over TSVD\nand other shrinkage rules is substantial, turning it into the practical hard\nthreshold of choice. \n\n"}
{"id": "1305.5879", "contents": "Title: Statistical Significance of Clustering using Soft Thresholding Abstract: Clustering methods have led to a number of important discoveries in\nbioinformatics and beyond. A major challenge in their use is determining which\nclusters represent important underlying structure, as opposed to spurious\nsampling artifacts. This challenge is especially serious, and very few methods\nare available, when the data are very high in dimension. Statistical\nSignificance of Clustering (SigClust) is a recently developed cluster\nevaluation tool for high dimensional low sample size data. An important\ncomponent of the SigClust approach is the very definition of a single cluster\nas a subset of data sampled from a multivariate Gaussian distribution. The\nimplementation of SigClust requires the estimation of the eigenvalues of the\ncovariance matrix for the null multivariate Gaussian distribution. We show that\nthe original eigenvalue estimation can lead to a test that suffers from severe\ninflation of type-I error, in the important case where there are a few very\nlarge eigenvalues. This paper addresses this critical challenge using a novel\nlikelihood based soft thresholding approach to estimate these eigenvalues,\nwhich leads to a much improved SigClust. Major improvements in SigClust\nperformance are shown by both mathematical analysis, based on the new notion of\nTheoretical Cluster Index, and extensive simulation studies. Applications to\nsome cancer genomic data further demonstrate the usefulness of these\nimprovements. \n\n"}
{"id": "1305.7010", "contents": "Title: Estimation of an Origin/Destination matrix: Application to a ferry\n  transport data Abstract: The estimation of the number of passengers with the identical journey is a\ncommon problem for public transport authorities. This problem is also known as\nthe Origin- Destination estimation (OD) problem and it has been widely studied\nfor the past thirty years. However, the theory is missing when the observations\nare not limited to the passenger counts but also includes station surveys. Our\naim is to provide a solid framework for the estimation of an OD matrix when\nonly a portion of the journey counts are observable. Our method consists of a\nstatistical estimation technique for OD matrix when we have the sum-of-row\ncounts and survey-based observations. Our technique differs from the previous\nstudies in that it does not need a prior OD matrix which can be hard to obtain.\nInstead, we model the passengers behavior through the survey data, and use the\ndiagonalization of the partial OD matrix to reduce the space parameter and\nderive a consistent global OD matrix estimator. We demonstrate the robustness\nof our estimator and apply it to several examples showcasing the proposed\nmodels and approach. We highlight how other sources of data can be incorporated\nin the model such as explanatory variables, e.g. rainfall, indicator variables\nfor major events, etc, and inference made in a principled, non-heuristic way. \n\n"}
{"id": "1306.0959", "contents": "Title: Testing goodness-of-fit for logistic regression Abstract: Explicitly accounting for all applicable independent variables, even when the\nmodel being tested does not, is critical in testing goodness-of-fit for\nlogistic regression. This can increase statistical power by orders of\nmagnitude. \n\n"}
{"id": "1306.2250", "contents": "Title: Cyclic motions in Dekel-Scotchmer Game Experiments Abstract: TASP (Time Average Shapley Polygon, Bena{\\=\\i}m, Hofbauer and Hopkins,\n\\emph{Journal of Economic Theory}, 2009), as a novel evolutionary dynamics\nmodel, predicts that a game could converge to cycles instead of fix points\n(Nash equilibria). To verify TASP theory, using the four strategy\nDekel-Scotchmer games (Dekel and Scotchmer, \\emph{Journal of Economic Theory},\n1992), four experiments were conducted (Cason, Friedman and Hopkins,\n\\emph{Journal of Economic Theory}, 2010), in which, however, reported no\nevidence of cycles (Cason, Friedman and Hopkins, \\emph{The Review of Economic\nStudies}, 2013). We reanalysis the four experiment data by testing the\nstochastic averaging of angular momentum in period-by-period transitions of the\nsocial state. We find, the existence of persistent cycles in Dekel-Scotchmer\ngame can be confirmed. On the cycles, the predictions from evolutionary models\nhad been supported by the four experiments. \n\n"}
{"id": "1306.3171", "contents": "Title: Confidence Intervals and Hypothesis Testing for High-Dimensional\n  Regression Abstract: Fitting high-dimensional statistical models often requires the use of\nnon-linear parameter estimation procedures. As a consequence, it is generally\nimpossible to obtain an exact characterization of the probability distribution\nof the parameter estimates. This in turn implies that it is extremely\nchallenging to quantify the \\emph{uncertainty} associated with a certain\nparameter estimate. Concretely, no commonly accepted procedure exists for\ncomputing classical measures of uncertainty and statistical significance as\nconfidence intervals or $p$-values for these models.\n  We consider here high-dimensional linear regression problem, and propose an\nefficient algorithm for constructing confidence intervals and $p$-values. The\nresulting confidence intervals have nearly optimal size. When testing for the\nnull hypothesis that a certain parameter is vanishing, our method has nearly\noptimal power.\n  Our approach is based on constructing a `de-biased' version of regularized\nM-estimators. The new construction improves over recent work in the field in\nthat it does not assume a special structure on the design matrix. We test our\nmethod on synthetic data and a high-throughput genomic data set about\nriboflavin production rate. \n\n"}
{"id": "1306.4933", "contents": "Title: A Nonparametric Approach for Multiple Change Point Analysis of\n  Multivariate Data Abstract: Change point analysis has applications in a wide variety of fields. The\ngeneral problem concerns the inference of a change in distribution for a set of\ntime-ordered observations. Sequential detection is an online version in which\nnew data is continually arriving and is analyzed adaptively. We are concerned\nwith the related, but distinct, offline version, in which retrospective\nanalysis of an entire sequence is performed. For a set of multivariate\nobservations of arbitrary dimension, we consider nonparametric estimation of\nboth the number of change points and the positions at which they occur. We do\nnot make any assumptions regarding the nature of the change in distribution or\nany distribution assumptions beyond the existence of the alpha-th absolute\nmoment, for some alpha in (0,2). Estimation is based on hierarchical clustering\nand we propose both divisive and agglomerative algorithms. The divisive method\nis shown to provide consistent estimates of both the number and location of\nchange points under standard regularity assumptions. We compare the proposed\napproach with competing methods in a simulation study. Methods from cluster\nanalysis are applied to assess performance and to allow simple comparisons of\nlocation estimates, even when the estimated number differs. We conclude with\napplications in genetics, finance and spatio-temporal analysis. \n\n"}
{"id": "1306.5362", "contents": "Title: A Statistical Perspective on Algorithmic Leveraging Abstract: One popular method for dealing with large-scale data sets is sampling. For\nexample, by using the empirical statistical leverage scores as an importance\nsampling distribution, the method of algorithmic leveraging samples and\nrescales rows/columns of data matrices to reduce the data size before\nperforming computations on the subproblem. This method has been successful in\nimproving computational efficiency of algorithms for matrix problems such as\nleast-squares approximation, least absolute deviations approximation, and\nlow-rank matrix approximation. Existing work has focused on algorithmic issues\nsuch as worst-case running times and numerical issues associated with providing\nhigh-quality implementations, but none of it addresses statistical aspects of\nthis method.\n  In this paper, we provide a simple yet effective framework to evaluate the\nstatistical properties of algorithmic leveraging in the context of estimating\nparameters in a linear regression model with a fixed number of predictors. We\nshow that from the statistical perspective of bias and variance, neither\nleverage-based sampling nor uniform sampling dominates the other. This result\nis particularly striking, given the well-known result that, from the\nalgorithmic perspective of worst-case analysis, leverage-based sampling\nprovides uniformly superior worst-case algorithmic results, when compared with\nuniform sampling. Based on these theoretical results, we propose and analyze\ntwo new leveraging algorithms. A detailed empirical evaluation of existing\nleverage-based methods as well as these two new methods is carried out on both\nsynthetic and real data sets. The empirical results indicate that our theory is\na good predictor of practical performance of existing and new leverage-based\nalgorithms and that the new algorithms achieve improved performance. \n\n"}
{"id": "1306.5718", "contents": "Title: Fast Covariance Estimation for High-dimensional Functional Data Abstract: For smoothing covariance functions, we propose two fast algorithms that scale\nlinearly with the number of observations per function. Most available methods\nand software cannot smooth covariance matrices of dimension $J \\times J$ with\n$J>500$; the recently introduced sandwich smoother is an exception, but it is\nnot adapted to smooth covariance matrices of large dimensions such as $J \\ge\n10,000$. Covariance matrices of order $J=10,000$, and even $J=100,000$, are\nbecoming increasingly common, e.g., in 2- and 3-dimensional medical imaging and\nhigh-density wearable sensor data. We introduce two new algorithms that can\nhandle very large covariance matrices: 1) FACE: a fast implementation of the\nsandwich smoother and 2) SVDS: a two-step procedure that first applies singular\nvalue decomposition to the data matrix and then smoothes the eigenvectors.\nCompared to existing techniques, these new algorithms are at least an order of\nmagnitude faster in high dimensions and drastically reduce memory requirements.\nThe new algorithms provide instantaneous (few seconds) smoothing for matrices\nof dimension $J=10,000$ and very fast ($<$ 10 minutes) smoothing for\n$J=100,000$. Although SVDS is simpler than FACE, we provide ready to use,\nscalable R software for FACE. When incorporated into R package {\\it refund},\nFACE improves the speed of penalized functional regression by an order of\nmagnitude, even for data of normal size ($J <500$). We recommend that FACE be\nused in practice for the analysis of noisy and high-dimensional functional\ndata. \n\n"}
{"id": "1307.0252", "contents": "Title: Semi-supervised clustering methods Abstract: Cluster analysis methods seek to partition a data set into homogeneous\nsubgroups. It is useful in a wide variety of applications, including document\nprocessing and modern genetics. Conventional clustering methods are\nunsupervised, meaning that there is no outcome variable nor is anything known\nabout the relationship between the observations in the data set. In many\nsituations, however, information about the clusters is available in addition to\nthe values of the features. For example, the cluster labels of some\nobservations may be known, or certain observations may be known to belong to\nthe same cluster. In other cases, one may wish to identify clusters that are\nassociated with a particular outcome variable. This review describes several\nclustering algorithms (known as \"semi-supervised clustering\" methods) that can\nbe applied in these situations. The majority of these methods are modifications\nof the popular k-means clustering method, and several of them will be described\nin detail. A brief description of some other semi-supervised clustering\nalgorithms is also provided. \n\n"}
{"id": "1307.0898", "contents": "Title: Entropy of a Zipfian Distributed Lexicon Abstract: This article presents the calculation of the entropy of a system with Zipfian\ndistribution and shows that a communication system tends to present an exponent\nvalue close to one, but still greater than one, so that it might maximize\nentropy and hold a feasible lexicon with an increasing size. This result is in\nagreement with what is observed in natural languages and with the balance\nbetween the speaker and listener communication efforts. On the other hand, the\nentropy of the communicating source is very sensitive to the exponent value as\nwell as the length of the observable data, making it a poor parameter to\ncharacterize the communication process. \n\n"}
{"id": "1307.1524", "contents": "Title: Fundamentals of Heterogeneous Cellular Networks with Energy Harvesting Abstract: We develop a new tractable model for K-tier heterogeneous cellular networks\n(HetNets), where each base station (BS) is powered solely by a self-contained\nenergy harvesting module. The BSs across tiers differ in terms of the energy\nharvesting rate, energy storage capacity, transmit power and deployment\ndensity. Since a BS may not always have enough energy, it may need to be kept\nOFF and allowed to recharge while nearby users are served by neighboring BSs\nthat are ON. We show that the fraction of time a k^{th} tier BS can be kept ON,\ntermed availability \\rho_k, is a fundamental metric of interest. Using tools\nfrom random walk theory, fixed point analysis and stochastic geometry, we\ncharacterize the set of K-tuples (\\rho_1, \\rho_2, ... \\rho_K), termed the\navailability region, that is achievable by general uncoordinated operational\nstrategies, where the decision to toggle the current ON/OFF state of a BS is\ntaken independently of the other BSs. If the availability vector corresponding\nto the optimal system performance, e.g., in terms of rate, lies in this\navailability region, there is no performance loss due to the presence of\nunreliable energy sources. As a part of our analysis, we model the temporal\ndynamics of the energy level at each BS as a birth-death process, derive the\nenergy utilization rate, and use hitting/stopping time analysis to prove that\nthere exists a fundamental limit on \\rho_k that cannot be surpassed by any\nuncoordinated strategy. \n\n"}
{"id": "1307.2822", "contents": "Title: Nonparametric Bayes modeling of count processes Abstract: Data on count processes arise in a variety of applications, including\nlongitudinal, spatial and imaging studies measuring count responses. The\nliterature on statistical models for dependent count data is dominated by\nmodels built from hierarchical Poisson components. The Poisson assumption is\nnot warranted in many applications, and hierarchical Poisson models make\nrestrictive assumptions about over-dispersion in marginal distributions. This\narticle proposes a class of nonparametric Bayes count process models, which are\nconstructed through rounding real-valued underlying processes. The proposed\nclass of models accommodates applications in which one observes separate\ncount-valued functional data for each subject under study. Theoretical results\non large support and posterior consistency are established, and computational\nalgorithms are developed using Markov chain Monte Carlo. The methods are\nevaluated via simulation studies and illustrated through application to\nlongitudinal tumor counts and asthma inhaler usage. \n\n"}
{"id": "1307.6021", "contents": "Title: On modelling asymmetric data using two-piece sinh-arcsinh distributions Abstract: We introduce the univariate two--piece sinh-arcsinh distribution, which\ncontains two shape parameters that separately control skewness and kurtosis. We\nshow that this new model can capture higher levels of asymmetry than the\noriginal sinh-arcsinh distribution (Jones and Pewsey, 2009), in terms of some\nasymmetry measures, while keeping flexibility of the tails and tractability. We\nillustrate the performance of the proposed model with real data, and compare it\nto appropriate alternatives. Although we focus on the study of the univariate\nversions of the proposed distributions, we point out some multivariate\nextensions. \n\n"}
{"id": "1307.6539", "contents": "Title: Quantifying playmaking ability in hockey Abstract: It is often said that a sign of a great player is that he makes the players\naround him better. The player may or may not score much himself, but his\nteammates perform better when he plays. One way a hockey player can improve his\nor her teammates' performance is to create goal scoring opportunities.\nUnfortunately, in hockey goal scoring is relatively infrequent, and statistics\nlike assists can be unreliable as a measure of a player's playmaking ability.\nAssists also depend on playing time, power play usage, the strength of a\nplayer's linemates, and other factors. In this paper we develop a metric for\nquantifying playmaking ability that addresses these issues. Our playmaking\nmetric has two benefits over assists for which we can provide statistical\nevidence: it is more consistent than assists, and it is better than assists at\npredicting future assists. Quantifying player contributions using this measure\ncan assist decision-makers in identifying, acquiring, and integrating\nsuccessful playmakers into their lineups. \n\n"}
{"id": "1307.8046", "contents": "Title: Joint estimation of causal effects from observational and intervention\n  gene expression data Abstract: Background: Inference of gene regulatory networks from transcriptomic data\nhas been a wide research area in recent years. Proposed methods are mainly\nbased on the use of graphical Gaussian models for observational wild-type data\nand provide undirected graphs that are not able to accurately highlight the\ncausal relationships among genes. In the present work, we seek to improve\nestimation of causal effects among genes by jointly modeling observational\ntranscriptomic data with intervention data obtained by performing knock-outs or\nknock-downs on a subset of genes. By examining the impact of such expression\nperturbations on other genes, a more accurate reflection of regulatory\nrelationships may be obtained than through the use of wild-type data alone.\nResults: Using the framework of Gaussian Bayesian networks, we propose a Markov\nchain Monte Carlo algorithm with a Mallows model and an analytical likelihood\nmaximization to sample from the posterior distribution of causal node\norderings, and in turn, to estimate causal effects. The main advantage of the\nproposed algorithm over previously proposed methods is that it has the\nflexibility to accommodate any kind of intervention design, including partial\nor multiple knock-out experiments. Methods were compared on simulated data as\nwell as data from the DREAM 2007 challenge. Conclusions: The simulation study\nconfirmed the impossibility of estimating causal orderings of genes with\nobservation data only. The proposed algorithm was found, in most cases, to\nperform better than the previously proposed methods in terms of accuracy for\nthe estimation of causal effects. In addition, multiple knock-outs proved to\nbring valuable additional information compared to single knock-outs. The choice\nof optimal intervention design therefore appears to be a crucial aspect for\ncausal inference and an interesting challenge for future research. \n\n"}
{"id": "1308.0777", "contents": "Title: A testing based extraction algorithm for identifying significant\n  communities in networks Abstract: A common and important problem arising in the study of networks is how to\ndivide the vertices of a given network into one or more groups, called\ncommunities, in such a way that vertices of the same community are more\ninterconnected than vertices belonging to different ones. We propose and\ninvestigate a testing based community detection procedure called Extraction of\nStatistically Significant Communities (ESSC). The ESSC procedure is based on\n$p$-values for the strength of connection between a single vertex and a set of\nvertices under a reference distribution derived from a conditional\nconfiguration network model. The procedure automatically selects both the\nnumber of communities in the network and their size. Moreover, ESSC can handle\noverlapping communities and, unlike the majority of existing methods,\nidentifies \"background\" vertices that do not belong to a well-defined\ncommunity. The method has only one parameter, which controls the stringency of\nthe hypothesis tests. We investigate the performance and potential use of ESSC\nand compare it with a number of existing methods, through a validation study\nusing four real network data sets. In addition, we carry out a simulation study\nto assess the effectiveness of ESSC in networks with various types of community\nstructure, including networks with overlapping communities and those with\nbackground vertices. These results suggest that ESSC is an effective\nexploratory tool for the discovery of relevant community structure in complex\nnetwork systems. Data and software are available at\n\\urlhttp://www.unc.edu/~jameswd/research.html. \n\n"}
{"id": "1308.3925", "contents": "Title: Distance Correlation Methods for Discovering Associations in Large\n  Astrophysical Databases Abstract: High-dimensional, large-sample astrophysical databases of galaxy clusters,\nsuch as the Chandra Deep Field South COMBO-17 database, provide measurements on\nmany variables for thousands of galaxies and a range of redshifts. Current\nunderstanding of galaxy formation and evolution rests sensitively on\nrelationships between different astrophysical variables; hence an ability to\ndetect and verify associations or correlations between variables is important\nin astrophysical research. In this paper, we apply a recently defined\nstatistical measure called the distance correlation coefficient which can be\nused to identify new associations and correlations between astrophysical\nvariables. The distance correlation coefficient applies to variables of any\ndimension; it can be used to determine smaller sets of variables that provide\nequivalent astrophysical information; it is zero only when variables are\nindependent; and it is capable of detecting nonlinear associations that are\nundetectable by the classical Pearson correlation coefficient. Hence, the\ndistance correlation coefficient provides more information than the Pearson\ncoefficient. We analyze numerous pairs of variables in the COMBO-17 database\nwith the distance correlation method and with the maximal information\ncoefficient. We show that the Pearson coefficient can be estimated with higher\naccuracy from the corresponding distance correlation coefficient than from the\nmaximal information coefficient. For given values of the Pearson coefficient,\nthe distance correlation method has a greater ability than the maximal\ninformation coefficient to resolve astrophysical data into highly concentrated\nV-shapes, which enhances classification and pattern identification. These\nresults are observed over a range of redshifts beyond the local universe and\nfor galaxies from elliptical to spiral. \n\n"}
{"id": "1308.3968", "contents": "Title: Smooth projected density estimation Abstract: We introduce and analyse a new nonparametric estimator of a multi-dimensional\ndensity. Our smooth projection estimator (SPE) is defined by a least squares\nprojection of the sample onto an infinite dimensional mixture class via an\nundersmoothed nonparametric pilot estimate, which acts as a structural filter\nto regularise the solution. The undersmoothing is required to optimise the\nconvergence rate of the SPE, which is jointly determined by that of the pilot\nestimator to the true density in squared $\\mathbb{L}_{2}$ norm, and by that of\nthe pilot distribution function to the empirical distribution function in\nuniform norm. Our procedure was conceived with a view to exploiting well known\nresults in convex analysis and their connection to mixture densities. In the\ncontext of our work, this translates to the observation that the infinite\ndimensional minimisation problem, implicit in the construction of the SPE,\npossesses a solution of dimension at most $n+1$, where $n$ is the sample size.\nThe SPE thus enjoys practical advantages such as computational efficiency, ease\nof storage and rapid evaluation at a new data point. \n\n"}
{"id": "1308.4022", "contents": "Title: Variations of singular spectrum analysis for separability improvement:\n  non-orthogonal decompositions of time series Abstract: Singular spectrum analysis (SSA) as a nonparametric tool for decomposition of\nan observed time series into sum of interpretable components such as trend,\noscillations and noise is considered. The separability of these series\ncomponents by SSA means the possibility of such decomposition. Two variations\nof SSA, which weaken the separability conditions, are proposed. Both proposed\napproaches consider inner products corresponding to oblique coordinate systems\ninstead of the conventional Euclidean inner product. One of the approaches\nperforms iterations to obtain separating inner products. The other method\nchanges contributions of the components by involving the series derivative to\navoid component mixing. Performance of the suggested methods is demonstrated on\nsimulated and real-life data. \n\n"}
{"id": "1308.4079", "contents": "Title: Network estimation in State Space Model with L1-regularization\n  constraint Abstract: Biological networks have arisen as an attractive paradigm of genomic science\never since the introduction of large scale genomic technologies which carried\nthe promise of elucidating the relationship in functional genomics. Microarray\ntechnologies coupled with appropriate mathematical or statistical models have\nmade it possible to identify dynamic regulatory networks or to measure time\ncourse of the expression level of many genes simultaneously. However one of the\nfew limitations fall on the high-dimensional nature of such data coupled with\nthe fact that these gene expression data are known to include some hidden\nprocess. In that regards, we are concerned with deriving a method for inferring\na sparse dynamic network in a high dimensional data setting. We assume that the\nobservations are noisy measurements of gene expression in the form of mRNAs,\nwhose dynamics can be described by some unknown or hidden process. We build an\ninput-dependent linear state space model from these hidden states and\ndemonstrate how an incorporated $L_{1}$ regularization constraint in an\nExpectation-Maximization (EM) algorithm can be used to reverse engineer\ntranscriptional networks from gene expression profiling data. This corresponds\nto estimating the model interaction parameters. The proposed method is\nillustrated on time-course microarray data obtained from a well established\nT-cell data. At the optimum tuning parameters we found genes TRAF5, JUND, CDK4,\nCASP4, CD69, and C3X1 to have higher number of inwards directed connections and\nFYB, CCNA2, AKT1 and CASP8 to be genes with higher number of outwards directed\nconnections. We recommend these genes to be object for further investigation.\nCaspase 4 is also found to activate the expression of JunD which in turn\nrepresses the cell cycle regulator CDC2. \n\n"}
{"id": "1309.5073", "contents": "Title: Non-linear dependences in finance Abstract: The thesis is composed of three parts. Part I introduces the mathematical and\nstatistical tools that are relevant for the study of dependences, as well as\nstatistical tests of Goodness-of-fit for empirical probability distributions. I\npropose two extensions of usual tests when dependence is present in the sample\ndata and when observations have a fat-tailed distribution. The financial\ncontent of the thesis starts in Part II. I present there my studies regarding\nthe \"cross-sectional\" dependences among the time series of daily stock returns,\ni.e. the instantaneous forces that link several stocks together and make them\nbehave somewhat collectively rather than purely independently. A calibration of\na new factor model is presented here, together with a comparison to\nmeasurements on real data. Finally, Part III investigates the temporal\ndependences of single time series, using the same tools and measures of\ncorrelation. I propose two contributions to the study of the origin and\ndescription of \"volatility clustering\": one is a generalization of the\nARCH-like feedback construction where the returns are self-exciting, and the\nother one is a more original description of self-dependences in terms of\ncopulas. The latter can be formulated model-free and is not specific to\nfinancial time series. In fact, I also show here how concepts like recurrences,\nrecords, aftershocks and waiting times, that characterize the dynamics in a\ntime series can be written in the unifying framework of the copula. \n\n"}
{"id": "1309.6473", "contents": "Title: On nonnegative unbiased estimators Abstract: We study the existence of algorithms generating almost surely nonnegative\nunbiased estimators. We show that given a nonconstant real-valued function $f$\nand a sequence of unbiased estimators of $\\lambda\\in\\mathbb{R}$, there is no\nalgorithm yielding almost surely nonnegative unbiased estimators of\n$f(\\lambda)\\in\\mathbb{R}^+$. The study is motivated by pseudo-marginal Monte\nCarlo algorithms that rely on such nonnegative unbiased estimators. These\nmethods allow \"exact inference\" in intractable models, in the sense that\nintegrals with respect to a target distribution can be estimated without any\nsystematic error, even though the associated probability density function\ncannot be evaluated pointwise. We discuss the consequences of our results on\nthe applicability of pseudo-marginal algorithms and thus on the possibility of\nexact inference in intractable models. We illustrate our study with particular\nchoices of functions $f$ corresponding to known challenges in statistics, such\nas exact simulation of diffusions, inference in large datasets and doubly\nintractable distributions. \n\n"}
{"id": "1310.0150", "contents": "Title: Stability Abstract: Reproducibility is imperative for any scientific discovery. More often than\nnot, modern scientific findings rely on statistical analysis of\nhigh-dimensional data. At a minimum, reproducibility manifests itself in\nstability of statistical results relative to \"reasonable\" perturbations to data\nand to the model used. Jacknife, bootstrap, and cross-validation are based on\nperturbations to data, while robust statistics methods deal with perturbations\nto models. In this article, a case is made for the importance of stability in\nstatistics. Firstly, we motivate the necessity of stability for interpretable\nand reliable encoding models from brain fMRI signals. Secondly, we find strong\nevidence in the literature to demonstrate the central role of stability in\nstatistical inference, such as sensitivity analysis and effect detection.\nThirdly, a smoothing parameter selector based on estimation stability (ES),\nES-CV, is proposed for Lasso, in order to bring stability to bear on\ncross-validation (CV). ES-CV is then utilized in the encoding models to reduce\nthe number of predictors by 60% with almost no loss (1.3%) of prediction\nperformance across over 2,000 voxels. Last, a novel \"stability\" argument is\nseen to drive new results that shed light on the intriguing interactions\nbetween sample to sample variability and heavier tail error distribution (e.g.,\ndouble-exponential) in high-dimensional regression models with $p$ predictors\nand $n$ independent samples. In particular, when\n$p/n\\rightarrow\\kappa\\in(0.3,1)$ and the error distribution is\ndouble-exponential, the Ordinary Least Squares (OLS) is a better estimator than\nthe Least Absolute Deviation (LAD) estimator. \n\n"}
{"id": "1310.0275", "contents": "Title: Optimal exact tests for composite alternative hypotheses on cross\n  tabulated data Abstract: We present methodology for constructing exact significance tests for cross\ntabulated data for \"difficult\" composite alternative hypotheses that have no\nnatural test statistic. We construct a test for discovering Simpson's Paradox\nand a general test for discovering positive dependence between two ordinal\nvariables. Our tests are Bayesian extensions of the likelihood ratio test, they\nare optimal with respect to the prior distribution, and are also closely\nrelated to Bayes factors and Bayesian FDR controlling testing procedures. \n\n"}
{"id": "1310.0364", "contents": "Title: Segregation Indices for Disease Clustering Abstract: Spatial clustering has important implications in various fields. In\nparticular, disease clustering is of major public concern in epidemiology. In\nthis article, we propose the use of two distance-based segregation indices to\ntest the significance of disease clustering among subjects whose locations are\nfrom a homogeneous or an inhomogeneous population. We derive their asymptotic\ndistributions and compare them with other distance-based disease clustering\ntests in terms of empirical size and power by extensive Monte Carlo\nsimulations. The null pattern we consider is the random labeling (RL) of cases\nand controls to the given locations. Along this line, we investigate the\nsensitivity of the size of these tests to the underlying background pattern\n(e.g., clustered or homogenous) on which the RL is applied, the level of\nclustering and number of clusters, or differences in relative abundances of the\nclasses. We demonstrate that differences in relative abundance has the highest\nimpact on the empirical sizes of the tests. We also propose various non-RL\npatterns as alternatives to the RL pattern and assess the empirical power\nperformance of the tests under these alternatives. We illustrate the methods on\ntwo real-life examples from epidemiology. \n\n"}
{"id": "1310.1068", "contents": "Title: A novel spectral method for inferring general diploid selection from\n  time series genetic data Abstract: The increased availability of time series genetic variation data from\nexperimental evolution studies and ancient DNA samples has created new\nopportunities to identify genomic regions under selective pressure and to\nestimate their associated fitness parameters. However, it is a challenging\nproblem to compute the likelihood of nonneutral models for the population\nallele frequency dynamics, given the observed temporal DNA data. Here, we\ndevelop a novel spectral algorithm to analytically and efficiently integrate\nover all possible frequency trajectories between consecutive time points. This\nadvance circumvents the limitations of existing methods which require\nfine-tuning the discretization of the population allele frequency space when\nnumerically approximating requisite integrals. Furthermore, our method is\nflexible enough to handle general diploid models of selection where the\nheterozygote and homozygote fitness parameters can take any values, while\nprevious methods focused on only a few restricted models of selection. We\ndemonstrate the utility of our method on simulated data and also apply it to\nanalyze ancient DNA data from genetic loci associated with coat coloration in\nhorses. In contrast to previous studies, our exploration of the full fitness\nparameter space reveals that a heterozygote advantage form of balancing\nselection may have been acting on these loci. \n\n"}
{"id": "1310.2236", "contents": "Title: Analysis of AneuRisk65 data: warped logistic discrimination Abstract: We analyze the AneuRisk65 curvature functions using a likelihood-based\nwarping method for sparsely sampled curves, and combine it with logistic\nregression in order to discriminate subjects with aneurysms at or after the\nterminal bifurcation of the internal carotid artery (the most life-threatening)\nfrom subjects with no aneurysms or aneurysms along the carotid artery (the less\nserious). Significantly lower misclassification rates are obtained when the\nwarping functions are included in the logistic discrimination model, rather\nthan being treated as mere nuisance parameters. \n\n"}
{"id": "1310.3073", "contents": "Title: A cyclic time-dependent Markov process to model daily patterns in wind\n  turbine power production Abstract: Wind energy is becoming a top contributor to the renewable energy mix, which\nraises potential reliability issues for the grid due to the fluctuating nature\nof its source. To achieve adequate reserve commitment and to promote market\nparticipation, it is necessary to provide models that can capture daily\npatterns in wind power production. This paper presents a cyclic inhomogeneous\nMarkov process, which is based on a three-dimensional state-space (wind power,\nspeed and direction). Each time-dependent transition probability is expressed\nas a Bernstein polynomial. The model parameters are estimated by solving a\nconstrained optimization problem: The objective function combines two maximum\nlikelihood estimators, one to ensure that the Markov process long-term behavior\nreproduces the data accurately and another to capture daily fluctuations. A\nconvex formulation for the overall optimization problem is presented and its\napplicability demonstrated through the analysis of a case-study. The proposed\nmodel is capable of reproducing the diurnal patterns of a three-year dataset\ncollected from a wind turbine located in a mountainous region in Portugal. In\naddition, it is shown how to compute persistence statistics directly from the\nMarkov process transition matrices. Based on the case-study, the power\nproduction persistence through the daily cycle is analysed and discussed. \n\n"}
{"id": "1310.3970", "contents": "Title: Green Communication via Power-optimized HARQ Protocols Abstract: Recently, efficient use of energy has become an essential research topic for\ngreen communication. This paper studies the effect of optimal power controllers\non the performance of delay-sensitive communication setups utilizing hybrid\nautomatic repeat request (HARQ). The results are obtained for repetition time\ndiversity (RTD) and incremental redundancy (INR) HARQ protocols. In all cases,\nthe optimal power allocation, minimizing the outage-limited average\ntransmission power, is obtained under both continuous and bursting\ncommunication models. Also, we investigate the system throughput in different\nconditions. The results indicate that the power efficiency is increased\nsubstantially, if adaptive power allocation is utilized. For instance, assume\nRayleigh-fading channel, a maximum of two (re)transmission rounds with rates\n$\\{1,\\frac{1}{2}\\}$ nats-per-channel-use and an outage probability constraint\n${10}^{-3}$. Then, compared to uniform power allocation, optimal power\nallocation in RTD reduces the average power by 9 and 11 dB in the bursting and\ncontinuous communication models, respectively. In INR, these values are\nobtained to be 8 and 9 dB, respectively. \n\n"}
{"id": "1310.4887", "contents": "Title: Variable selection for BART: An application to gene regulation Abstract: We consider the task of discovering gene regulatory networks, which are\ndefined as sets of genes and the corresponding transcription factors which\nregulate their expression levels. This can be viewed as a variable selection\nproblem, potentially with high dimensionality. Variable selection is especially\nchallenging in high-dimensional settings, where it is difficult to detect\nsubtle individual effects and interactions between predictors. Bayesian\nAdditive Regression Trees [BART, Ann. Appl. Stat. 4 (2010) 266-298] provides a\nnovel nonparametric alternative to parametric regression approaches, such as\nthe lasso or stepwise regression, especially when the number of relevant\npredictors is sparse relative to the total number of available predictors and\nthe fundamental relationships are nonlinear. We develop a principled\npermutation-based inferential approach for determining when the effect of a\nselected predictor is likely to be real. Going further, we adapt the BART\nprocedure to incorporate informed prior information about variable importance.\nWe present simulations demonstrating that our method compares favorably to\nexisting parametric and nonparametric procedures in a variety of data settings.\nTo demonstrate the potential of our approach in a biological context, we apply\nit to the task of inferring the gene regulatory network in yeast (Saccharomyces\ncerevisiae). We find that our BART-based procedure is best able to recover the\nsubset of covariates with the largest signal compared to other variable\nselection methods. The methods developed in this work are readily available in\nthe R package bartMachine. \n\n"}
{"id": "1310.5336", "contents": "Title: The skew-t factor analysis model Abstract: Factor analysis is a classical data reduction technique that seeks a\npotentially lower number of unobserved variables that can account for the\ncorrelations among the observed variables. This paper presents an extension of\nthe factor analysis model by assuming jointly a restricted version of\nmultivariate skew t distribution for the latent factors and unobservable\nerrors, called the skew-t factor analysis model. The proposed model shows\nrobustness to violations of normality assumptions of the underlying latent\nfactors and provides flexibility in capturing extra skewness as well as heavier\ntails of the observed data. A computationally feasible ECM algorithm is\ndeveloped for computing maximum likelihood estimates of the parameters. The\nusefulness of the proposed methodology is illustrated by a real-life example\nand results also demonstrates its better performance over various existing\nmethods. \n\n"}
{"id": "1310.7467", "contents": "Title: Hypothesis Testing for Topological Data Analysis Abstract: Persistent homology is a vital tool for topological data analysis. Previous\nwork has developed some statistical estimators for characteristics of\ncollections of persistence diagrams. However, tools that provide statistical\ninference for observations that are persistence diagrams are limited.\nSpecifically, there is a need for tests that can assess the strength of\nevidence against a claim that two samples arise from the same population or\nprocess. We propose the use of randomization-style null hypothesis significance\ntests (NHST) for these situations. The test is based on a loss function that\ncomprises pairwise distances between the elements of each sample and all the\nelements in the other sample. We use this method to analyze a range of\nsimulated and experimental data. Through these examples we experimentally\nexplore the power of the p-values. Our results show that the\nrandomization-style NHST based on pairwise distances can distinguish between\nsamples from different processes, which suggests that its use for hypothesis\ntests upon persistence diagrams is reasonable. We demonstrate its application\non a real dataset of fMRI data of patients with ADHD. \n\n"}
{"id": "1310.7714", "contents": "Title: An Improved Bayesian Semiparametric Model for Palaeoclimate\n  Reconstruction: Cross-validation Based Model Assessment Abstract: Fossil-based palaeoclimate reconstruction is an important area of ecological\nscience that has gained momentum in the backdrop of the global climate change\ndebate. The hierarchical Bayesian paradigm provides an interesting platform for\nstudying such important scientific issue. However, our cross-validation based\nassessment of the existing Bayesian hierarchical models with respect to two\nmodern proxy data sets based on chironomid and pollen, respectively, revealed\nthat the models are inadequate for the data sets.\n  In this paper, we model the species assemblages (compositional data) by the\nzero-inflated multinomial distribution, while modelling the species response\nfunctions using Dirichlet process based Gaussian mixtures. This modelling\nstrategy yielded significantly improved performances, and a formal Bayesian\ntest of model adequacy, developed recently, showed that our new model is\nadequate for both the modern data sets. Furthermore, combining together the\nzero-inflated assumption, Importance Resampling Markov Chain Monte Carlo\n(IRMCMC) and the recently developed Transformation-based Markov Chain Monte\nCarlo (TMCMC), we develop a powerful and efficient computational methodology. \n\n"}
{"id": "1310.7850", "contents": "Title: Fundamental Limits of Nonintrusive Load Monitoring Abstract: Provided an arbitrary nonintrusive load monitoring (NILM) algorithm, we seek\nbounds on the probability of distinguishing between scenarios, given an\naggregate power consumption signal. We introduce a framework for studying a\ngeneral NILM algorithm, and analyze the theory in the general case. Then, we\nspecialize to the case where the error is Gaussian. In both cases, we are able\nto derive upper bounds on the probability of distinguishing scenarios. Finally,\nwe apply the results to real data to derive bounds on the probability of\ndistinguishing between scenarios as a function of the measurement noise, the\nsampling rate, and the device usage. \n\n"}
{"id": "1310.7918", "contents": "Title: Applications of threshold models and the weighted bootstrap for\n  Hungarian precipitation data Abstract: This paper presents applications of the peaks-over threshold methodology for\nboth the univariate and the recently introduced bivariate case, combined with a\nnovel bootstrap approach. We compare the proposed bootstrap methods to the more\ntraditional profile likelihood.\n  We have investigated 63 years of the European Climate Assessment daily\nprecipitation data for five Hungarian grid points, first separately for the\nsummer and winter months, then aiming at the detection of possible changes by\ninvestigating 20 years moving windows. We show that significant changes can be\nobserved both in the univariate and the bivariate cases, the most recent period\nbeing the most dangerous, as the return levels here are the highest. We\nillustrate these effects by bivariate coverage regions. \n\n"}
{"id": "1310.8574", "contents": "Title: Spatial statistics, image analysis and percolation theory Abstract: We develop a novel method for detection of signals and reconstruction of\nimages in the presence of random noise. The method uses results from\npercolation theory. We specifically address the problem of detection of\nmultiple objects of unknown shapes in the case of nonparametric noise. The\nnoise density is unknown and can be heavy-tailed. The objects of interest have\nunknown varying intensities. No boundary shape constraints are imposed on the\nobjects, only a set of weak bulk conditions is required. We view the object\ndetection problem as a multiple hypothesis testing for discrete statistical\ninverse problems. We present an algorithm that allows to detect greyscale\nobjects of various shapes in noisy images. We prove results on consistency and\nalgorithmic complexity of our procedures. Applications to cryo-electron\nmicroscopy are presented. \n\n"}
{"id": "1311.0317", "contents": "Title: Parsimonious Shifted Asymmetric Laplace Mixtures Abstract: A family of parsimonious shifted asymmetric Laplace mixture models is\nintroduced. We extend the mixture of factor analyzers model to the shifted\nasymmetric Laplace distribution. Imposing constraints on the constitute parts\nof the resulting decomposed component scale matrices leads to a family of\nparsimonious models. An explicit two-stage parameter estimation procedure is\ndescribed, and the Bayesian information criterion and the integrated completed\nlikelihood are compared for model selection. This novel family of models is\napplied to real data, where it is compared to its Gaussian analogue within\nclustering and classification paradigms. \n\n"}
{"id": "1311.0376", "contents": "Title: On the Bootstrap for Persistence Diagrams and Landscapes Abstract: Persistent homology probes topological properties from point clouds and\nfunctions. By looking at multiple scales simultaneously, one can record the\nbirths and deaths of topological features as the scale varies. In this paper we\nuse a statistical technique, the empirical bootstrap, to separate topological\nsignal from topological noise. In particular, we derive confidence sets for\npersistence diagrams and confidence bands for persistence landscapes. \n\n"}
{"id": "1311.0562", "contents": "Title: LP Mixed Data Science : Outline of Theory Abstract: This article presents the theoretical foundation of a new frontier of\nresearch-`LP Mixed Data Science'-that simultaneously extends and integrates the\npractice of traditional and novel statistical methods for nonparametric\nexploratory data modeling, and is applicable to the teaching and training of\nstatistics.\n  Statistics journals have great difficulty accepting papers unlike those\npreviously published. For statisticians with new big ideas a practical strategy\nis to publish them in many small applied studies which enables one to provide\nreferences to work of others. This essay outlines the many concepts, new\ntheory, and important algorithms of our new culture of statistical science\ncalled LP MIXED DATA SCIENCE. It provides comprehensive solutions to problems\nof data analysis and nonparametric modeling of many variables that are\ncontinuous or discrete, which does not yet have a large literature. It develops\na new modeling approach to nonparametric estimation of the multivariate copula\ndensity. We discuss the theory which we believe is very elegant (and can\nprovide a framework for United Statistical Algorithms, for traditional Small\nData methods and Big Data methods). \n\n"}
{"id": "1311.1039", "contents": "Title: Maximum penalized likelihood estimation in semiparametric\n  capture-recapture models Abstract: We discuss the semiparametric modeling of mark-recapture-recovery data where\nthe temporal and/or individual variation of model parameters is explained via\ncovariates. Typically, in such analyses a fixed (or mixed) effects parametric\nmodel is specified for the relationship between the model parameters and the\ncovariates of interest. In this paper, we discuss the modeling of the\nrelationship via the use of penalized splines, to allow for considerably more\nflexible functional forms. Corresponding models can be fitted via numerical\nmaximum penalized likelihood estimation, employing cross-validation to choose\nthe smoothing parameters in a data-driven way. Our contribution builds on and\nextends the existing literature, providing a unified inferential framework for\nsemiparametric mark-recapture-recovery models for open populations, where the\ninterest typically lies in the estimation of survival probabilities. The\napproach is applied to two real datasets, corresponding to grey herons (Ardea\nCinerea), where we model the survival probability as a function of\nenvironmental condition (a time-varying global covariate), and Soay sheep (Ovis\nAries), where we model the survival probability as a function of individual\nweight (a time-varying individual-specific covariate). The proposed\nsemiparametric approach is compared to a standard parametric (logistic)\nregression and new interesting underlying dynamics are observed in both cases. \n\n"}
{"id": "1311.2422", "contents": "Title: Clustering Categorical Time Series into Unknown Number of Clusters: A\n  Perfect Simulation based Approach Abstract: Pamminger and Fruwirth-Schnatter (2010) considered a Bayesian approach to\nmodel-based clustering of categorical time series assuming a fixed number of\nclusters. But the popular methods for selecting the number of clusters, for\nexample, the Bayes Information Criterion (BIC), turned out to have severe\nproblems in the categorical time series context.\n  In this paper, we circumvent the difficulties of choosing the number of\nclusters by adopting the Bayesian semiparametric mixture model approach\nintroduced by Bhattacharya (2008), who assume that the number of clusters is a\nrandom quantity, but is bounded above by a (possibly large) number of clusters.\nWe adopt the perfect simulation approach of Mukhopadhyay and Bhattacharya\n(2012) for posterior simulation for completely solving the problems of\nconvergence of the underlying Markov chain Monte Carlo (MCMC) approach.\nImportantly, within our main perfect simulation algorithm, there arose the\nnecessity to simulate perfectly from the joint distribution of a set of\ncontinuous random variables with log-concave full conditional densities. We\npropose and develop a novel and efficient perfect simulation methodology for\njoint distributions with log-concave full conditionals. This perfect sampling\nmethodology is of independent interest as well since in a very large and\nimportant class of Bayesian applications the full conditionals turn out to be\nlog-concave.\n  We will consider application of our model and methodology to the Austrian\nwage mobility data, also analysed by Pamminger and Fruwirth-Schnatter (2010),\nand adopting the methods developed in Mukhopadhyay et al. (2011), Mukhopadhyay\net al. (2012), will obtain the posterior modes of clusterings and also the\ndesired highest posterior distribution credible regions of the posterior\ndistribution of clusterings. \n\n"}
{"id": "1312.1818", "contents": "Title: Sparse latent factor models with interactions: Analysis of gene\n  expression data Abstract: Sparse latent multi-factor models have been used in many exploratory and\npredictive problems with high-dimensional multivariate observations. Because of\nconcerns with identifiability, the latent factors are almost always assumed to\nbe linearly related to measured feature variables. Here we explore the analysis\nof multi-factor models with different structures of interactions between latent\nfactors, including multiplicative effects as well as a more general framework\nfor nonlinear interactions introduced via the Gaussian Process. We utilize\nsparsity priors to test whether the factors and interaction terms have\nsignificant effect. The performance of the models is evaluated through\nsimulated and real data applications in genomics. Variation in the number of\ncopies of regions of the genome is a well-known and important feature of most\ncancers. We examine interactions between factors directly associated with\ndifferent chromosomal regions detected with copy number alteration in breast\ncancer data. In this context, significant interaction effects for specific\ngenes suggest synergies between duplications and deletions in different regions\nof the chromosome. \n\n"}
{"id": "1312.2041", "contents": "Title: Probabilistic models of genetic variation in structured populations\n  applied to global human studies Abstract: Modern population genetics studies typically involve genome-wide genotyping\nof individuals from a diverse network of ancestries. An important, unsolved\nproblem is how to formulate and estimate probabilistic models of observed\ngenotypes that allow for complex population structure. We formulate two general\nprobabilistic models, and we propose computationally efficient algorithms to\nestimate them. First, we show how principal component analysis (PCA) can be\nutilized to estimate a general model that includes the well-known\nPritchard-Stephens-Donnelly mixed-membership model as a special case. Noting\nsome drawbacks of this approach, we introduce a new \"logistic factor analysis\"\n(LFA) framework that seeks to directly model the logit transformation of\nprobabilities underlying observed genotypes in terms of latent variables that\ncapture population structure. We demonstrate these advances on data from the\nhuman genome diversity panel and 1000 genomes project, where we are able to\nidentify SNPs that are highly differentiated with respect to structure while\nmaking minimal modeling assumptions. \n\n"}
{"id": "1312.2098", "contents": "Title: Uncertainty Measures and Limiting Distributions for Filament Estimation Abstract: A filament is a high density, connected region in a point cloud. There are\nseveral methods for estimating filaments but these methods do not provide any\nmeasure of uncertainty. We give a definition for the uncertainty of estimated\nfilaments and we study statistical properties of the estimated filaments. We\nshow how to estimate the uncertainty measures and we construct confidence sets\nbased on a bootstrapping technique. We apply our methods to astronomy data and\nearthquake data. \n\n"}
{"id": "1312.2923", "contents": "Title: Lagrangian Time Series Models for Ocean Surface Drifter Trajectories Abstract: This paper proposes stochastic models for the analysis of ocean surface\ntrajectories obtained from freely-drifting satellite-tracked instruments. The\nproposed time series models are used to summarise large multivariate datasets\nand infer important physical parameters of inertial oscillations and other\nocean processes. Nonstationary time series methods are employed to account for\nthe spatiotemporal variability of each trajectory. Because the datasets are\nlarge, we construct computationally efficient methods through the use of\nfrequency-domain modelling and estimation, with the data expressed as\ncomplex-valued time series. We detail how practical issues related to sampling\nand model misspecification may be addressed using semi-parametric techniques\nfor time series, and we demonstrate the effectiveness of our stochastic models\nthrough application to both real-world data and to numerical model output. \n\n"}
{"id": "1312.4675", "contents": "Title: Bias Correction of Persistence Measures in Fractionally Integrated\n  Models Abstract: This paper investigates the accuracy of bootstrap-based bias correction of\npersistence measures for long memory fractionally integrated processes. The\nbootstrap method is based on the semi-parametric sieve approach, with the\ndynamics in the long memory process captured by an autoregressive\napproximation. With a view to improving accuracy, the sieve method is also\napplied to data pre-filtered by a semi-parametric estimate of the long memory\nparameter. Both versions of the bootstrap technique are used to estimate the\nfinite sample distributions of the sample autocorrelation coefficients and the\nimpulse response coefficients and, in turn, to bias-adjust these statistics.\nThe accuracy of the resultant estimators in the case of the autocorrelation\ncoefficients is also compared with that yielded by analytical bias adjustment\nmethods when available. The basic sieve technique is seen to yield a reduction\nin the bias of both persistence measures. The pre-filtered sieve produces a\nsubstantial further reduction in the bias of the estimated impulse response\nfunction, whilst the extra improvement yielded by pre-filtering in the case of\nthe sample autocorrelation function is shown to depend heavily on the accuracy\nof the pre-filter. \n\n"}
{"id": "1312.6102", "contents": "Title: Asymptotically Efficient Estimation of Weighted Average Derivatives with\n  an Interval Censored Variable Abstract: This paper studies the identification and estimation of weighted average\nderivatives of conditional location functionals including conditional mean and\nconditional quantiles in settings where either the outcome variable or a\nregressor is interval-valued. Building on Manski and Tamer (2002) who study\nnonparametric bounds for mean regression with interval data, we characterize\nthe identified set of weighted average derivatives of regression functions.\nSince the weighted average derivatives do not rely on parametric specifications\nfor the regression functions, the identified set is well-defined without any\nparametric assumptions. Under general conditions, the identified set is compact\nand convex and hence admits characterization by its support function. Using\nthis characterization, we derive the semiparametric efficiency bound of the\nsupport function when the outcome variable is interval-valued. We illustrate\nefficient estimation by constructing an efficient estimator of the support\nfunction for the case of mean regression with an interval censored outcome. \n\n"}
{"id": "1312.7614", "contents": "Title: Inference on causal and structural parameters using many moment\n  inequalities Abstract: This paper considers the problem of testing many moment inequalities where\nthe number of moment inequalities, denoted by $p$, is possibly much larger than\nthe sample size $n$. There is a variety of economic applications where solving\nthis problem allows to carry out inference on causal and structural parameters,\na notable example is the market structure model of Ciliberto and Tamer (2009)\nwhere $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter\nthe market. We consider the test statistic given by the maximum of $p$\nStudentized (or $t$-type) inequality-specific statistics, and analyze various\nways to compute critical values for the test statistic. Specifically, we\nconsider critical values based upon (i) the union bound combined with a\nmoderate deviation inequality for self-normalized sums, (ii) the multiplier and\nempirical bootstraps, and (iii) two-step and three-step variants of (i) and\n(ii) by incorporating the selection of uninformative inequalities that are far\nfrom being binding and a novel selection of weakly informative inequalities\nthat are potentially binding but do not provide first order information. We\nprove validity of these methods, showing that under mild conditions, they lead\nto tests with the error in size decreasing polynomially in $n$ while allowing\nfor $p$ being much larger than $n$, indeed $p$ can be of order $\\exp (n^{c})$\nfor some $c > 0$. Importantly, all these results hold without any restriction\non the correlation structure between $p$ Studentized statistics, and also hold\nuniformly with respect to suitably large classes of underlying distributions.\nMoreover, in the online supplement, we show validity of a test based on the\nblock multiplier bootstrap in the case of dependent data under some general\nmixing conditions. \n\n"}
{"id": "1401.1640", "contents": "Title: Quantifying intrinsic and extrinsic noise in gene transcription using\n  the linear noise approximation: An application to single cell data Abstract: A central challenge in computational modeling of dynamic biological systems\nis parameter inference from experimental time course measurements. However, one\nwould not only like to infer kinetic parameters but also study their\nvariability from cell to cell. Here we focus on the case where single-cell\nfluorescent protein imaging time series data are available for a population of\ncells. Based on van Kampen's linear noise approximation, we derive a dynamic\nstate space model for molecular populations which is then extended to a\nhierarchical model. This model has potential to address the sources of\nvariability relevant to single-cell data, namely, intrinsic noise due to the\nstochastic nature of the birth and death processes involved in reactions and\nextrinsic noise arising from the cell-to-cell variation of kinetic parameters.\nIn order to infer such a model from experimental data, one must also quantify\nthe measurement process where one has to allow for nonmeasurable molecular\nspecies as well as measurement noise of unknown level and variance. The\navailability of multiple single-cell time series data here provides a unique\ntestbed to fit such a model and quantify these different sources of variation\nfrom experimental data. \n\n"}
{"id": "1401.1867", "contents": "Title: Nonparametric 3D map of the IGM using the Lyman-alpha forest Abstract: Visualizing the high-redshift Universe is difficult due to the dearth of\navailable data; however, the Lyman-alpha forest provides a means to map the\nintergalactic medium at redshifts not accessible to large galaxy surveys.\nLarge-scale structure surveys, such as the Baryon Oscillation Spectroscopic\nSurvey (BOSS), have collected quasar (QSO) spectra that enable the\nreconstruction of HI density fluctuations. The data fall on a collection of\nlines defined by the lines-of-sight (LOS) of the QSO, and a major issue with\nproducing a 3D reconstruction is determining how to model the regions between\nthe LOS. We present a method that produces a 3D map of this relatively\nuncharted portion of the Universe by employing local polynomial smoothing, a\nnonparametric methodology. The performance of the method is analyzed on\nsimulated data that mimics the varying number of LOS expected in real data, and\nthen is applied to a sample region selected from BOSS. Evaluation of the\nreconstruction is assessed by considering various features of the predicted 3D\nmaps including visual comparison of slices, PDFs, counts of local minima and\nmaxima, and standardized correlation functions. This 3D reconstruction allows\nfor an initial investigation of the topology of this portion of the Universe\nusing persistent homology. \n\n"}
{"id": "1401.2293", "contents": "Title: Discussion of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard Abstract: Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089]. \n\n"}
{"id": "1401.2728", "contents": "Title: A semiparametric approach to mixed outcome latent variable models:\n  Estimating the association between cognition and regional brain volumes Abstract: Multivariate data that combine binary, categorical, count and continuous\noutcomes are common in the social and health sciences. We propose a\nsemiparametric Bayesian latent variable model for multivariate data of\narbitrary type that does not require specification of conditional\ndistributions. Drawing on the extended rank likelihood method by Hoff [Ann.\nAppl. Stat. 1 (2007) 265-283], we develop a semiparametric approach for latent\nvariable modeling with mixed outcomes and propose associated Markov chain Monte\nCarlo estimation methods. Motivated by cognitive testing data, we focus on\nbifactor models, a special case of factor analysis. We employ our\nsemiparametric Bayesian latent variable model to investigate the association\nbetween cognitive outcomes and MRI-measured regional brain volumes. \n\n"}
{"id": "1401.2822", "contents": "Title: Approximations for two-dimensional discrete scan statistics in some\n  block-factor type dependent models Abstract: We consider the two-dimensional discrete scan statistic generated by a\nblock-factor type model obtained from i.i.d. sequence. We present an\napproximation for the distribution of the scan statistics and the corresponding\nerror bounds. A simulation study illustrates our methodology. \n\n"}
{"id": "1401.5015", "contents": "Title: Model selection of stochastic simulation algorithm based on generalized\n  divergence measures Abstract: MCMC methods (Monte Carlo Markov Chain) are a class of methods used to\nperform simulations per a probability distribution $P$. These methods are often\nused when we have difficulties to directly sample per a given probability\ndistribution $P$ . This distribution is then considered as a target and\ngenerates a Markov chain $(X_n)_{n\\in\\mathbb{N}}$ that, when $n$ is large we\nhave $X_n\\sim P$. These MCMC methods consist of several simulation strategies\nincluding the \\emph{Independent Sampler (IS)}, the \\emph{Random Walk of\nMetropolis Hastings \\small{(RWMH)}}, the \\emph{Gibbs sampler}, the\n\\emph{Adaptive Metropolis (AM)} and \\emph{Metropolis Within Gibbs (MWG)}\nstrategy. Each of these strategies can generate a Markov chain and is\nassociated with a convergence speed. It is interesting, with a given target\nlaw, to compare several simulation strategies for determining the best.\nChauveau and Vandekerkhove \\cite{Chauv2007} have compared IS and RWMH\nstrategies using the Kullback-Leibler divergence measure. In our article we\nwill compare our five simulation methods already mentioned using generalized\ndivergence measures. These divergence measures are taken in family of\n$\\alpha$-divergence measures \\cite{Cichocki2010}, with a parameter $\\alpha$.\nThis is the R\\'enyi divergence, Tsallis divergence and $D_\\alpha$ divergence . \n\n"}
{"id": "1402.0136", "contents": "Title: IsoDOT Detects Differential RNA-isoform Expression/Usage with respect to\n  a Categorical or Continuous Covariate with High Sensitivity and Specificity Abstract: We have developed a statistical method named IsoDOT to assess differential\nisoform expression (DIE) and differential isoform usage (DIU) using RNA-seq\ndata. Here isoform usage refers to relative isoform expression given the total\nexpression of the corresponding gene. IsoDOT performs two tasks that cannot be\naccomplished by existing methods: to test DIE/DIU with respect to a continuous\ncovariate, and to test DIE/DIU for one case versus one control. The latter task\nis not an uncommon situation in practice, e.g., comparing paternal and maternal\nallele of one individual or comparing tumor and normal sample of one cancer\npatient. Simulation studies demonstrate the high sensitivity and specificity of\nIsoDOT. We apply IsoDOT to study the effects of haloperidol treatment on mouse\ntranscriptome and identify a group of genes whose isoform usages respond to\nhaloperidol treatment. \n\n"}
{"id": "1402.4296", "contents": "Title: Modeling heterogeneity in random graphs through latent space models: a\n  selective review Abstract: We present a selective review on probabilistic modeling of heterogeneity in\nrandom graphs. We focus on latent space models and more particularly on\nstochastic block models and their extensions that have undergone major\ndevelopments in the last five years. \n\n"}
{"id": "1402.4459", "contents": "Title: Significance Analysis for Pairwise Variable Selection in Classification Abstract: The goal of this article is to select important variables that can\ndistinguish one class of data from another. A marginal variable selection\nmethod ranks the marginal effects for classification of individual variables,\nand is a useful and efficient approach for variable selection. Our focus here\nis to consider the bivariate effect, in addition to the marginal effect. In\nparticular, we are interested in those pairs of variables that can lead to\naccurate classification predictions when they are viewed jointly. To accomplish\nthis, we propose a permutation test called Significance test of Joint Effect\n(SigJEff). In the absence of joint effect in the data, SigJEff is similar or\nequivalent to many marginal methods. However, when joint effects exist, our\nmethod can significantly boost the performance of variable selection. Such\njoint effects can help to provide additional, and sometimes dominating,\nadvantage for classification. We illustrate and validate our approach using\nboth simulated example and a real glioblastoma multiforme data set, which\nprovide promising results. \n\n"}
{"id": "1402.6536", "contents": "Title: On split sample and randomized confidence intervals for binomial\n  proportions Abstract: Split sample methods have recently been put forward as a way to reduce the\ncoverage oscillations that haunt confidence intervals for parameters of lattice\ndistributions, such as the binomial and Poisson distributions. We study split\nsample intervals in the binomial setting, showing that these intervals can be\nviewed as being based on adding discrete random noise to the data. It is shown\nthat they can be improved upon by using noise with a continuous distribution\ninstead, regardless of whether the randomization is determined by the data or\nan external source of randomness. We compare split sample intervals to the\nrandomized Stevens interval, which removes the coverage oscillations\ncompletely, and find the latter interval to have several advantages. \n\n"}
{"id": "1402.6781", "contents": "Title: Bias Reduction of Long Memory Parameter Estimators via the Pre-filtered\n  Sieve Bootstrap Abstract: This paper investigates the use of bootstrap-based bias correction of\nsemi-parametric estimators of the long memory parameter in fractionally\nintegrated processes. The re-sampling method involves the application of the\nsieve bootstrap to data pre-filtered by a preliminary semi-parametric estimate\nof the long memory parameter. Theoretical justification for using the bootstrap\ntechniques to bias adjust log-periodogram and semi-parametric local Whittle\nestimators of the memory parameter is provided. Simulation evidence comparing\nthe performance of the bootstrap bias correction with analytical bias\ncorrection techniques is also presented. The bootstrap method is shown to\nproduce notable bias reductions, in particular when applied to an estimator for\nwhich analytical adjustments have already been used. The empirical coverage of\nconfidence intervals based on the bias-adjusted estimators is very close to the\nnominal, for a reasonably large sample size, more so than for the comparable\nanalytically adjusted estimators. The precision of inferences (as measured by\ninterval length) is also greater when the bootstrap is used to bias correct\nrather than analytical adjustments. \n\n"}
{"id": "1402.6951", "contents": "Title: Modeling the Complex Dynamics and Changing Correlations of Epileptic\n  Events Abstract: Patients with epilepsy can manifest short, sub-clinical epileptic \"bursts\" in\naddition to full-blown clinical seizures. We believe the relationship between\nthese two classes of events---something not previously studied\nquantitatively---could yield important insights into the nature and intrinsic\ndynamics of seizures. A goal of our work is to parse these complex epileptic\nevents into distinct dynamic regimes. A challenge posed by the intracranial EEG\n(iEEG) data we study is the fact that the number and placement of electrodes\ncan vary between patients. We develop a Bayesian nonparametric Markov switching\nprocess that allows for (i) shared dynamic regimes between a variable number of\nchannels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary\nof dynamic regimes. We encode a sparse and changing set of dependencies between\nthe channels using a Markov-switching Gaussian graphical model for the\ninnovations process driving the channel dynamics and demonstrate the importance\nof this model in parsing and out-of-sample predictions of iEEG data. We show\nthat our model produces intuitive state assignments that can help automate\nclinical analysis of seizures and enable the comparison of sub-clinical bursts\nand full clinical seizures. \n\n"}
{"id": "1402.7079", "contents": "Title: Type Ia Supernova Colors and Ejecta Velocities: Hierarchical Bayesian\n  Regression with Non-Gaussian Distributions Abstract: We investigate the statistical dependence of the peak intrinsic colors of\nType Ia supernovae (SN Ia) on their expansion velocities at maximum light,\nmeasured from the Si II 6355 spectral feature. We construct a new hierarchical\nBayesian regression model, accounting for the random effects of intrinsic\nscatter, measurement error, and reddening by host galaxy dust, and implement a\nGibbs sampler and deviance information criteria to estimate the correlation.\nThe method is applied to the apparent colors from BVRI light curves and Si II\nvelocity data for 79 nearby SNe Ia. The apparent color distributions of high\n(HV) and normal velocity (NV) supernovae exhibit significant discrepancies for\nB-V and B-R, but not other colors. Hence, they are likely due to intrinsic\ncolor differences originating in the B-band, rather than dust reddening. The\nmean intrinsic B-V and B-R color differences between HV and NV groups are 0.06\n+/- 0.02 and 0.09 +/- 0.02 mag, respectively. A linear model finds significant\nslopes of -0.021 +/- 0.006 and -0.030 +/- 0.009 mag/(1000 km/s) for intrinsic\nB-V and B-R colors versus velocity, respectively. Since the ejecta velocity\ndistribution is skewed towards high velocities, these effects imply\nnon-Gaussian intrinsic color distributions with skewness up to +0.3. Accounting\nfor the intrinsic color-velocity correlation results in corrections to A_V\nextinction estimates as large as -0.12 mag for HV SNe Ia and +0.06 mag for NV\nevents. Velocity measurements from SN Ia spectra have potential to diminish\nsystematic errors from the confounding of intrinsic colors and dust reddening\naffecting supernova distances. \n\n"}
{"id": "1403.0904", "contents": "Title: Ridge Estimation of Inverse Covariance Matrices from High-Dimensional\n  Data Abstract: We study ridge estimation of the precision matrix in the high-dimensional\nsetting where the number of variables is large relative to the sample size. We\nfirst review two archetypal ridge estimators and note that their utilized\npenalties do not coincide with common ridge penalties. Subsequently, starting\nfrom a common ridge penalty, analytic expressions are derived for two\nalternative ridge estimators of the precision matrix. The alternative\nestimators are compared to the archetypes with regard to eigenvalue shrinkage\nand risk. The alternatives are also compared to the graphical lasso within the\ncontext of graphical modeling. The comparisons may give reason to prefer the\nproposed alternative estimators. \n\n"}
{"id": "1403.2285", "contents": "Title: Unsupervised Learning via Mixtures of Skewed Distributions with\n  Hypercube Contours Abstract: Mixture models whose components have skewed hypercube contours are developed\nvia a generalization of the multivariate shifted asymmetric Laplace density.\nSpecifically, we develop mixtures of multiple scaled shifted asymmetric Laplace\ndistributions. The component densities have two unique features: they include a\nmultivariate weight function, and the marginal distributions are also\nasymmetric Laplace. We use these mixtures of multiple scaled shifted asymmetric\nLaplace distributions for clustering applications, but they could equally well\nbe used in the supervised or semi-supervised paradigms. The\nexpectation-maximization algorithm is used for parameter estimation and the\nBayesian information criterion is used for model selection. Simulated and real\ndata sets are used to illustrate the approach and, in some cases, to visualize\nthe skewed hypercube structure of the components. \n\n"}
{"id": "1403.3500", "contents": "Title: R-vine Models for Spatial Time Series with an Application to Daily Mean\n  Temperature Abstract: We introduce an extension of R-vine copula models for the purpose of spatial\ndependency modeling and model based prediction at unobserved locations. The\nnewly derived spatial R-vine model combines the flexibility of vine copulas\nwith the classical geostatistical idea of modeling spatial dependencies by\nmeans of the distances between the variable locations. In particular the model\nis able to capture non-Gaussian spatial dependencies. For the purpose of model\ndevelopment and as an illustration we consider daily mean temperature data\nobserved at 54 monitoring stations in Germany. We identify a relationship\nbetween the vine copula parameters and the station distances and exploit it in\norder to reduce the huge number of parameters needed to parametrize a\n54-dimensional R-vine model needed to fit the data. The new distance based\nmodel parametrization results in a distinct reduction in the number of\nparameters and makes parameter estimation and prediction at unobserved\nlocations feasible. The prediction capabilities are validated using adequate\nscoring techniques, showing a better performance of the spatial R-vine copula\nmodel compared to a Gaussian spatial model. \n\n"}
{"id": "1403.3909", "contents": "Title: Graph Sample and Hold: A Framework for Big-Graph Analytics Abstract: Sampling is a standard approach in big-graph analytics; the goal is to\nefficiently estimate the graph properties by consulting a sample of the whole\npopulation. A perfect sample is assumed to mirror every property of the whole\npopulation. Unfortunately, such a perfect sample is hard to collect in complex\npopulations such as graphs (e.g. web graphs, social networks etc), where an\nunderlying network connects the units of the population. Therefore, a good\nsample will be representative in the sense that graph properties of interest\ncan be estimated with a known degree of accuracy. While previous work focused\nparticularly on sampling schemes used to estimate certain graph properties\n(e.g. triangle count), much less is known for the case when we need to estimate\nvarious graph properties with the same sampling scheme. In this paper, we\npropose a generic stream sampling framework for big-graph analytics, called\nGraph Sample and Hold (gSH). To begin, the proposed framework samples from\nmassive graphs sequentially in a single pass, one edge at a time, while\nmaintaining a small state. We then show how to produce unbiased estimators for\nvarious graph properties from the sample. Given that the graph analysis\nalgorithms will run on a sample instead of the whole population, the runtime\ncomplexity of these algorithm is kept under control. Moreover, given that the\nestimators of graph properties are unbiased, the approximation error is kept\nunder control. Finally, we show the performance of the proposed framework (gSH)\non various types of graphs, such as social graphs, among others. \n\n"}
{"id": "1403.4513", "contents": "Title: A quantitative approach to evolution of music and philosophy Abstract: The development of new statistical and computational methods is increasingly\nmaking it possible to bridge the gap between hard sciences and humanities. In\nthis study, we propose an approach based on a quantitative evaluation of\nattributes of objects in fields of humanities, from which concepts such as\ndialectics and opposition are formally defined mathematically. As case studies,\nwe analyzed the temporal evolution of classical music and philosophy by\nobtaining data for 8 features characterizing the corresponding fields for 7\nwell-known composers and philosophers, which were treated with multivariate\nstatistics and pattern recognition methods. A bootstrap method was applied to\navoid statistical bias caused by the small sample data set, with which hundreds\nof artificial composers and philosophers were generated, influenced by the 7\nnames originally chosen. Upon defining indices for opposition, skewness and\ncounter-dialectics, we confirmed the intuitive analysis of historians in that\nclassical music evolved according to a master-apprentice tradition, while in\nphilosophy changes were driven by opposition. Though these case studies were\nmeant only to show the possibility of treating phenomena in humanities\nquantitatively, including a quantitative measure of concepts such as dialectics\nand opposition the results are encouraging for further application of the\napproach presented here to many other areas, since it is entirely generic. \n\n"}
{"id": "1403.4890", "contents": "Title: Modeling an Augmented Lagrangian for Blackbox Constrained Optimization Abstract: Constrained blackbox optimization is a difficult problem, with most\napproaches coming from the mathematical programming literature. The statistical\nliterature is sparse, especially in addressing problems with nontrivial\nconstraints. This situation is unfortunate because statistical methods have\nmany attractive properties: global scope, handling noisy objectives,\nsensitivity analysis, and so forth. To narrow that gap, we propose a\ncombination of response surface modeling, expected improvement, and the\naugmented Lagrangian numerical optimization framework. This hybrid approach\nallows the statistical model to think globally and the augmented Lagrangian to\nact locally. We focus on problems where the constraints are the primary\nbottleneck, requiring expensive simulation to evaluate and substantial modeling\neffort to map out. In that context, our hybridization presents a simple yet\neffective solution that allows existing objective-oriented statistical\napproaches, like those based on Gaussian process surrogates and expected\nimprovement heuristics, to be applied to the constrained setting with minor\nmodification. This work is motivated by a challenging, real-data benchmark\nproblem from hydrology where, even with a simple linear objective function,\nlearning a nontrivial valid region complicates the search for a global minimum. \n\n"}
{"id": "1403.7118", "contents": "Title: A Unified Framework of Constrained Regression Abstract: Generalized additive models (GAMs) play an important role in modeling and\nunderstanding complex relationships in modern applied statistics. They allow\nfor flexible, data-driven estimation of covariate effects. Yet researchers\noften have a priori knowledge of certain effects, which might be monotonic or\nperiodic (cyclic) or should fulfill boundary conditions. We propose a unified\nframework to incorporate these constraints for both univariate and bivariate\neffect estimates and for varying coefficients. As the framework is based on\ncomponent-wise boosting methods, variables can be selected intrinsically, and\neffects can be estimated for a wide range of different distributional\nassumptions. Bootstrap confidence intervals for the effect estimates are\nderived to assess the models. We present three case studies from environmental\nsciences to illustrate the proposed seamless modeling framework. All discussed\nconstrained effect estimates are implemented in the comprehensive R package\nmboost for model-based boosting. \n\n"}
{"id": "1403.7274", "contents": "Title: Bias Correction in Species Distribution Models: Pooling Survey and\n  Collection Data for Multiple Species Abstract: Presence-only records may provide data on the distributions of rare species,\nbut commonly suffer from large, unknown biases due to their typically haphazard\ncollection schemes. Presence-absence or count data collected in systematic,\nplanned surveys are more reliable but typically less abundant.\n  We proposed a probabilistic model to allow for joint analysis of\npresence-only and survey data to exploit their complementary strengths. Our\nmethod pools presence-only and presence-absence data for many species and\nmaximizes a joint likelihood, simultaneously estimating and adjusting for the\nsampling bias affecting the presence-only data. By assuming that the sampling\nbias is the same for all species, we can borrow strength across species to\nefficiently estimate the bias and improve our inference from presence-only\ndata.\n  We evaluate our model's performance on data for 36 eucalypt species in\nsoutheastern Australia. We find that presence-only records exhibit a strong\nsampling bias toward the coast and toward Sydney, the largest city. Our\ndata-pooling technique substantially improves the out-of-sample predictive\nperformance of our model when the amount of available presence-absence data for\na given species is scarce.\n  If we have only presence-only data and no presence-absence data for a given\nspecies, but both types of data for several other species that suffer from the\nsame spatial sampling bias, then our method can obtain an unbiased estimate of\nthe first species' geographic range. \n\n"}
{"id": "1404.3560", "contents": "Title: A hierarchical Bayesian model for inference of copy number variants and\n  their association to gene expression Abstract: A number of statistical models have been successfully developed for the\nanalysis of high-throughput data from a single source, but few methods are\navailable for integrating data from different sources. Here we focus on\nintegrating gene expression levels with comparative genomic hybridization (CGH)\narray measurements collected on the same subjects. We specify a measurement\nerror model that relates the gene expression levels to latent copy number\nstates which, in turn, are related to the observed surrogate CGH measurements\nvia a hidden Markov model. We employ selection priors that exploit the\ndependencies across adjacent copy number states and investigate MCMC stochastic\nsearch techniques for posterior inference. Our approach results in a unified\nmodeling framework for simultaneously inferring copy number variants (CNV) and\nidentifying their significant associations with mRNA transcripts abundance. We\nshow performance on simulated data and illustrate an application to data from a\ngenomic study on human cancer cell lines. \n\n"}
{"id": "1404.5671", "contents": "Title: Inference from Small and Big Data Sets with Error Rates Abstract: In this paper we introduce randomized $t$-type statistics that will be\nreferred to as randomized pivots. We show that these randomized pivots yield\ncentral limit theorems with a significantly smaller magnitude of error as\ncompared to that of their classical counterparts under the same conditions.\nThis constitutes a desirable result when a relatively small number of data is\navailable. When a data set is too big to be processed, we use our randomized\npivots to make inference about the mean based on significantly smaller\nsub-samples. The approach taken is shown to relate naturally to estimating\ndistributions of both small and big data sets. \n\n"}
{"id": "1404.6473", "contents": "Title: Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests Abstract: This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided. \n\n"}
{"id": "1404.7197", "contents": "Title: Bayesian Model Comparison in Genetic Association Analysis: Linear Mixed\n  Modeling and SNP Set Testing Abstract: We consider the problems of hypothesis testing and model comparison under a\nflexible Bayesian linear regression model whose formulation is closely\nconnected with the linear mixed effect model and the parametric models for SNP\nset analysis in genetic association studies. We derive a class of analytic\napproximate Bayes factors and illustrate their connections with a variety of\nfrequentist test statistics, including the Wald statistic and the variance\ncomponent score statistic. Taking advantage of Bayesian model averaging and\nhierarchical modeling, we demonstrate some distinct advantages and\nflexibilities in the approaches utilizing the derived Bayes factors in the\ncontext of genetic association studies. We demonstrate our proposed methods\nusing real or simulated numerical examples in applications of single SNP\nassociation testing, multi-locus fine-mapping and SNP set association testing. \n\n"}
{"id": "1405.1444", "contents": "Title: Understanding Protein Dynamics with L1-Regularized Reversible Hidden\n  Markov Models Abstract: We present a machine learning framework for modeling protein dynamics. Our\napproach uses L1-regularized, reversible hidden Markov models to understand\nlarge protein datasets generated via molecular dynamics simulations. Our model\nis motivated by three design principles: (1) the requirement of massive\nscalability; (2) the need to adhere to relevant physical law; and (3) the\nnecessity of providing accessible interpretations, critical for both cellular\nbiology and rational drug design. We present an EM algorithm for learning and\nintroduce a model selection criteria based on the physical notion of\nconvergence in relaxation timescales. We contrast our model with standard\nmethods in biophysics and demonstrate improved robustness. We implement our\nalgorithm on GPUs and apply the method to two large protein simulation datasets\ngenerated respectively on the NCSA Bluewaters supercomputer and the\nFolding@Home distributed computing network. Our analysis identifies the\nconformational dynamics of the ubiquitin protein critical to cellular\nsignaling, and elucidates the stepwise activation mechanism of the c-Src kinase\nprotein. \n\n"}
{"id": "1405.2709", "contents": "Title: Effective Genetic Risk Prediction Using Mixed Models Abstract: To date, efforts to produce high-quality polygenic risk scores from\ngenome-wide studies of common disease have focused on estimating and\naggregating the effects of multiple SNPs. Here we propose a novel statistical\napproach for genetic risk prediction, based on random and mixed effects models.\nOur approach (termed GeRSI) circumvents the need to estimate the effect sizes\nof numerous SNPs by treating these effects as random, producing predictions\nwhich are consistently superior to current state of the art, as we demonstrate\nin extensive simulation. When applying GeRSI to seven phenotypes from the WTCCC\nstudy, we confirm that the use of random effects is most beneficial for\ndiseases that are known to be highly polygenic: hypertension (HT) and bipolar\ndisorder (BD). For HT, there are no significant associations in the WTCCC data.\nThe best existing model yields an AUC of 54%, while GeRSI improves it to 59%.\nFor BD, using GeRSI improves the AUC from 55% to 62%. For individuals ranked at\nthe top 10% of BD risk predictions, using GeRSI substantially increases the BD\nrelative risk from 1.4 to 2.5. \n\n"}
{"id": "1405.3340", "contents": "Title: Post-selection point and interval estimation of signal sizes in Gaussian\n  samples Abstract: We tackle the problem of the estimation of a vector of means from a single\nvector-valued observation $y$. Whereas previous work reduces the size of the\nestimates for the largest (absolute) sample elements via shrinkage (like\nJames-Stein) or biases estimated via empirical Bayes methodology, we take a\nnovel approach. We adapt recent developments by Lee et al (2013) in post\nselection inference for the Lasso to the orthogonal setting, where sample\nelements have different underlying signal sizes. This is exactly the setup\nencountered when estimating many means. It is shown that other selection\nprocedures, like selecting the $K$ largest (absolute) sample elements and the\nBenjamini-Hochberg procedure, can be cast into their framework, allowing us to\nleverage their results. Point and interval estimates for signal sizes are\nproposed. These seem to perform quite well against competitors, both recent and\nmore tenured.\n  Furthermore, we prove an upper bound to the worst case risk of our estimator,\nwhen combined with the Benjamini-Hochberg procedure, and show that it is within\na constant multiple of the minimax risk over a rich set of parameter spaces\nmeant to evoke sparsity. \n\n"}
{"id": "1405.4955", "contents": "Title: Nonstationary, Nonparametric, Nonseparable Bayesian Spatio-Temporal\n  Modeling Using Kernel Convolution of Order Based Dependent Dirichlet Process Abstract: In this article, using kernel convolution of order based dependent Dirichlet\nprocess (Griffin and Steel (2006)) we construct a nonstationary, nonseparable,\nnonparametric space-time process, which, as we show, satisfies desirable\nproperties, and includes the stationary, separable, parametric processes as\nspecial cases. We also investigate the smoothness properties of our proposed\nmodel. Since our model entails an infinite random series, for Bayesian model\nfitting purpose we must either truncate the series or more appropriately\nconsider a random number of summands, which renders the model dimension a\nrandom variable. We attack the variable dimensionality problem using\nTransdimensional Transformation based Markov Chain Monte Carlo introduced by\nDas and Bhattacharya (2019b), which can update all the variables and also\nchange dimensions in a single block using essentially a single random variable\ndrawn from some arbitrary density defined on a relevant support. For the sake\nof completeness we also address the problem of truncating the infinite series\nby providing a uniform bound on the error incurred by truncating the infinite\nseries.\n  We illustrate the effectiveness of our model and methodologies on a simulated\ndata set and demonstrate that our approach significantly outperforms that of\nFuentes and Reich (2013) which is based on principles somewhat similar to ours.\nWe also fit two real, spatial and spatio-temporal datasets with our approach\nand obtain quite encouraging results in both the cases. \n\n"}
{"id": "1405.6947", "contents": "Title: Computationally efficient spatial modeling of annual maximum 24 hour\n  precipitation. An application to data from Iceland Abstract: We propose a computationally efficient statistical method to obtain\ndistributional properties of annual maximum 24 hour precipitation on a 1 km by\n1 km regular grid over Iceland. A latent Gaussian model is built which takes\ninto account observations, spatial variations and outputs from a local\nmeteorological model. A covariate based on the meteorological model is\nconstructed at each observational site and each grid point in order to\nassimilate available scientific knowledge about precipitation into the\nstatistical model. The model is applied to two data sets on extreme\nprecipitation, one uncorrected data set and one data set that is corrected for\nphase and wind. The observations are assumed to follow the generalized extreme\nvalue distribution. At the latent level, we implement SPDE spatial models for\nboth the location and scale parameters of the likelihood. An efficient MCMC\nsampler which exploits the model structure is constructed, which yields fast\ncontinuous spatial predictions for spatially varying model parameters and\nquantiles. \n\n"}
{"id": "1405.7107", "contents": "Title: Laplace deconvolution on the basis of time domain data and its\n  application to Dynamic Contrast Enhanced imaging Abstract: In the present paper we consider the problem of Laplace deconvolution with\nnoisy discrete non-equally spaced observations on a finite time interval. We\npropose a new method for Laplace deconvolution which is based on expansions of\nthe convolution kernel, the unknown function and the observed signal over\nLaguerre functions basis (which acts as a surrogate eigenfunction basis of the\nLaplace convolution operator) using regression setting. The expansion results\nin a small system of linear equations with the matrix of the system being\ntriangular and Toeplitz. Due to this triangular structure, there is a common\nnumber $m$ of terms in the function expansions to control, which is realized\nvia complexity penalty. The advantage of this methodology is that it leads to\nvery fast computations, produces no boundary effects due to extension at zero\nand cut-off at $T$ and provides an estimator with the risk within a logarithmic\nfactor of the oracle risk. We emphasize that, in the present paper, we consider\nthe true observational model with possibly nonequispaced observations which are\navailable on a finite interval of length $T$ which appears in many different\ncontexts, and account for the bias associated with this model (which is not\npresent when $T\\rightarrow\\infty$). The study is motivated by perfusion imaging\nusing a short injection of contrast agent, a procedure which is applied for\nmedical assessment of micro-circulation within tissues such as cancerous\ntumors. Presence of a tuning parameter $a$ allows to choose the most\nadvantageous time units, so that both the kernel and the unknown right hand\nside of the equation are well represented for the deconvolution. The\nmethodology is illustrated by an extensive simulation study and a real data\nexample which confirms that the proposed technique is fast, efficient,\naccurate, usable from a practical point of view and very competitive. \n\n"}
{"id": "1406.0189", "contents": "Title: Convex Total Least Squares Abstract: We study the total least squares (TLS) problem that generalizes least squares\nregression by allowing measurement errors in both dependent and independent\nvariables. TLS is widely used in applied fields including computer vision,\nsystem identification and econometrics. The special case when all dependent and\nindependent variables have the same level of uncorrelated Gaussian noise, known\nas ordinary TLS, can be solved by singular value decomposition (SVD). However,\nSVD cannot solve many important practical TLS problems with realistic noise\nstructure, such as having varying measurement noise, known structure on the\nerrors, or large outliers requiring robust error-norms. To solve such problems,\nwe develop convex relaxation approaches for a general class of structured TLS\n(STLS). We show both theoretically and experimentally, that while the plain\nnuclear norm relaxation incurs large approximation errors for STLS, the\nre-weighted nuclear norm approach is very effective, and achieves better\naccuracy on challenging STLS problems than popular non-convex solvers. We\ndescribe a fast solution based on augmented Lagrangian formulation, and apply\nour approach to an important class of biological problems that use population\naverage measurements to infer cell-type and physiological-state specific\nexpression levels that are very hard to measure directly. \n\n"}
{"id": "1406.1197", "contents": "Title: Unbiased sampling of network ensembles Abstract: Sampling random graphs with given properties is a key step in the analysis of\nnetworks, as random ensembles represent basic null models required to identify\npatterns such as communities and motifs. An important requirement is that the\nsampling process is unbiased and efficient. The main approaches are\nmicrocanonical, i.e. they sample graphs that match the enforced constraints\nexactly. Unfortunately, when applied to strongly heterogeneous networks (like\nmost real-world examples), the majority of these approaches become biased\nand/or time-consuming. Moreover, the algorithms defined in the simplest cases,\nsuch as binary graphs with given degrees, are not easily generalizable to more\ncomplicated ensembles. Here we propose a solution to the problem via the\nintroduction of a \"Maximize and Sample\" (\"Max & Sam\" for short) method to\ncorrectly sample ensembles of networks where the constraints are `soft', i.e.\nrealized as ensemble averages. Our method is based on exact maximum-entropy\ndistributions and is therefore unbiased by construction, even for strongly\nheterogeneous networks. It is also more computationally efficient than most\nmicrocanonical alternatives. Finally, it works for both binary and weighted\nnetworks with a variety of constraints, including combined degree-strength\nsequences and full reciprocity structure, for which no alternative method\nexists. Our canonical approach can in principle be turned into an unbiased\nmicrocanonical one, via a restriction to the relevant subset. Importantly, the\nanalysis of the fluctuations of the constraints suggests that the\nmicrocanonical and canonical versions of all the ensembles considered here are\nnot equivalent. We show various real-world applications and provide a code\nimplementing all our algorithms. \n\n"}
{"id": "1406.1651", "contents": "Title: The insignificant evolution of the richness-mass relation of galaxy\n  clusters Abstract: We analysed the richness--mass scaling of 23 very massive clusters at\n$0.15<z<0.55$ with homogenously measured weak-lensing masses and richnesses\nwithin a fixed aperture of $0.5$ Mpc radius. We found that the richness--mass\nscaling is very tight (the scatter is $<0.09$ dex with 90 \\% probability) and\nindependent of cluster evolutionary status and morphology. This implies a close\nassociation between infall and evolution of dark matter and galaxies in the\ncentral region of clusters. We also found that the evolution of the\nrichness-mass intercept is minor at most, and, given the minor mass evolution\nacross the studied redshift range, the richness evolution of individual massive\nclusters also turns out to be very small. Finally, it was paramount to account\nfor the cluster mass function and the selection function. Ignoring them would\nled to biases larger than the (otherwise quoted) errors. Our study benefits\nfrom: a) weak-lensing masses instead of proxy-based masses thereby removing the\nambiguity between a real trend and one induced by an accounted evolution of the\nused mass proxy; b) the use of projected masses that simplify the statistical\nanalysis thereby not requiring consideration of the unknown covariance induced\nby the cluster orientation/triaxiality; c) the use of aperture masses as they\nare free of the pseudo-evolution of mass definitions anchored to the evolving\ndensity of the Universe; d) a proper accounting of the sample selection\nfunction and of the Malmquist-like effect induced by the cluster mass function;\ne) cosmological simulations for the computation of the cluster mass function,\nits evolution, and the mass growth of each individual cluster. \n\n"}
{"id": "1406.1901", "contents": "Title: Subsampling Methods for Persistent Homology Abstract: Persistent homology is a multiscale method for analyzing the shape of sets\nand functions from point cloud data arising from an unknown distribution\nsupported on those sets. When the size of the sample is large, direct\ncomputation of the persistent homology is prohibitive due to the combinatorial\nnature of the existing algorithms. We propose to compute the persistent\nhomology of several subsamples of the data and then combine the resulting\nestimates. We study the risk of two estimators and we prove that the\nsubsampling approach carries stable topological information while achieving a\ngreat reduction in computational complexity. \n\n"}
{"id": "1406.2462", "contents": "Title: Empirical risk minimization for heavy-tailed losses Abstract: The purpose of this paper is to discuss empirical risk minimization when the\nlosses are not necessarily bounded and may have a distribution with heavy\ntails. In such situations, usual empirical averages may fail to provide\nreliable estimates and empirical risk minimization may provide large excess\nrisk. However, some robust mean estimators proposed in the literature may be\nused to replace empirical means. In this paper, we investigate empirical risk\nminimization based on a robust estimate proposed by Catoni. We develop\nperformance bounds based on chaining arguments tailored to Catoni's mean\nestimator. \n\n"}
{"id": "1406.3089", "contents": "Title: Protein synthesis driven by dynamical stochastic transcription Abstract: In this manuscript we propose a mathematical framework to couple\ntranscription and translation in which mRNA production is described by a set of\nmaster equations while the dynamics of protein density is governed by a random\ndifferential equation. The coupling between the two processes is given by a\nstochastic perturbation whose statistics satisfies the master equations. In\nthis approach, from the knowledge of the analytical time dependent distribution\nof mRNA number, we are able to calculate the dynamics of the probability\ndensity of the protein population. \n\n"}
{"id": "1406.4549", "contents": "Title: Extensible grids: uniform sampling on a space-filling curve Abstract: We study the properties of points in $[0,1]^d$ generated by applying\nHilbert's space-filling curve to uniformly distributed points in $[0,1]$. For\ndeterministic sampling we obtain a discrepancy of $O(n^{-1/d})$ for $d\\ge2$.\nFor random stratified sampling, and scrambled van der Corput points, we get a\nmean squared error of $O(n^{-1-2/d})$ for integration of Lipshitz continuous\nintegrands, when $d\\ge3$. These rates are the same as one gets by sampling on\n$d$ dimensional grids and they show a deterioration with increasing $d$. The\nrate for Lipshitz functions is however best possible at that level of\nsmoothness and is better than plain IID sampling. Unlike grids, space-filling\ncurve sampling provides points at any desired sample size, and the van der\nCorput version is extensible in $n$. Additionally we show that certain\ndiscontinuous functions with infinite variation in the sense of Hardy and\nKrause can be integrated with a mean squared error of $O(n^{-1-1/d})$. It was\npreviously known only that the rate was $o(n^{-1})$. Other space-filling\ncurves, such as those due to Sierpinski and Peano, also attain these rates,\nwhile upper bounds for the Lebesgue curve are somewhat worse, as if the\ndimension were $\\log_2(3)$ times as high. \n\n"}
{"id": "1406.7536", "contents": "Title: Estimating the distribution of Galaxy Morphologies on a continuous space Abstract: The incredible variety of galaxy shapes cannot be summarized by human defined\ndiscrete classes of shapes without causing a possibly large loss of\ninformation. Dictionary learning and sparse coding allow us to reduce the high\ndimensional space of shapes into a manageable low dimensional continuous vector\nspace. Statistical inference can be done in the reduced space via probability\ndistribution estimation and manifold estimation. \n\n"}
{"id": "1407.3322", "contents": "Title: Distribution System Load and Forecast Model Abstract: This short document provides experimental evidence for the set of assumptions\non the mean load and forecast errors made in \\cite{Sevlian2014A_Outage} and\n\\cite{Sevlian2014B_Outage}. We show that the mean load at any given node is\ndistributed normally, where we compute the mean and variance. We then present\nan aggregation-error curve for a single day ahead forecaster. Residual analysis\nshows that beyond 500 customers, gaussian residuals is a reasonable model. We\nthen show the forecaster has uncorrelated errors. \n\n"}
{"id": "1407.4430", "contents": "Title: Sequential Logistic Principal Component Analysis (SLPCA): Dimensional\n  Reduction in Streaming Multivariate Binary-State System Abstract: Sequential or online dimensional reduction is of interests due to the\nexplosion of streaming data based applications and the requirement of adaptive\nstatistical modeling, in many emerging fields, such as the modeling of energy\nend-use profile. Principal Component Analysis (PCA), is the classical way of\ndimensional reduction. However, traditional Singular Value Decomposition (SVD)\nbased PCA fails to model data which largely deviates from Gaussian\ndistribution. The Bregman Divergence was recently introduced to achieve a\ngeneralized PCA framework. If the random variable under dimensional reduction\nfollows Bernoulli distribution, which occurs in many emerging fields, the\ngeneralized PCA is called Logistic PCA (LPCA). In this paper, we extend the\nbatch LPCA to a sequential version (i.e. SLPCA), based on the sequential convex\noptimization theory. The convergence property of this algorithm is discussed\ncompared to the batch version of LPCA (i.e. BLPCA), as well as its performance\nin reducing the dimension for multivariate binary-state systems. Its\napplication in building energy end-use profile modeling is also investigated. \n\n"}
{"id": "1407.5525", "contents": "Title: Hypothesis Testing For Network Data in Functional Neuroimaging Abstract: In recent years, it has become common practice in neuroscience to use\nnetworks to summarize relational information in a set of measurements,\ntypically assumed to be reflective of either functional or structural\nrelationships between regions of interest in the brain. One of the most basic\ntasks of interest in the analysis of such data is the testing of hypotheses, in\nanswer to questions such as \"Is there a difference between the networks of\nthese two groups of subjects?\" In the classical setting, where the unit of\ninterest is a scalar or a vector, such questions are answered through the use\nof familiar two-sample testing strategies. Networks, however, are not Euclidean\nobjects, and hence classical methods do not directly apply. We address this\nchallenge by drawing on concepts and techniques from geometry, and\nhigh-dimensional statistical inference. Our work is based on a precise\ngeometric characterization of the space of graph Laplacian matrices and a\nnonparametric notion of averaging due to Fr\\'echet. We motivate and illustrate\nour resulting methodologies for testing in the context of networks derived from\nfunctional neuroimaging data on human subjects from the 1000 Functional\nConnectomes Project. In particular, we show that this global test is more\nstatistical powerful, than a mass-univariate approach. In addition, we have\nalso provided a method for visualizing the individual contribution of each edge\nto the overall test statistic. \n\n"}
{"id": "1407.5616", "contents": "Title: TW-TOA Based Positioning in the Presence of Clock Imperfections Abstract: This paper studies the positioning problem based on two-way time-of-arrival\n(TW-TOA) measurements in asynchronous wireless sensor networks. Since the\noptimal estimator for this problem involves difficult nonconvex optimization,\nwe propose two suboptimal estimators based on squared-range least squares and\nleast absolute mean of residual errors. The former approach is formulated as a\ngeneral trust region subproblem which can be solved exactly under mild\nconditions. The latter approach is formulated as a difference of convex\nfunctions programming (DCP), which can be solved using a concave-convex\nprocedure. Simulation results illustrate the high performance of the proposed\ntechniques, especially for the DCP approach. \n\n"}
{"id": "1407.7479", "contents": "Title: Mixed Effects Modeling for Areal Data that Exhibit\n  Multivariate-Spatio-Temporal Dependencies Abstract: There are many data sources available that report related variables of\ninterest that are also referenced over geographic regions and time; however,\nthere are relatively few general statistical methods that one can readily use\nthat incorporate these multivariate-spatio-temporal dependencies. As such, we\nintroduce the multivariate-spatio-temporal mixed effects model (MSTM) to\nanalyze areal data with multivariate-spatio-temporal dependencies. The proposed\nMSTM extends the notion of Moran's I basis functions to the\nmultivariate-spatio-temporal setting. This extension leads to several\nmethodological contributions including extremely effective dimension reduction,\na dynamic linear model for multivariate-spatio-temporal areal processes, and\nthe reduction of a high-dimensional parameter space using a novel parameter\nmodel. Several examples are used to demonstrate that the MSTM provides an\nextremely viable solution to many important problems found in different and\ndistinct corners of the spatio-temporal statistics literature including:\nmodeling nonseparable and nonstationary covariances, combing data from multiple\nrepeated surveys, and analyzing massive multivariate-spatio-temporal datasets. \n\n"}
{"id": "1407.8382", "contents": "Title: Detection boundary and Higher Criticism approach for rare and weak\n  genetic effects Abstract: Genome-wide association studies (GWAS) have identified many genetic factors\nunderlying complex human traits. However, these factors have explained only a\nsmall fraction of these traits' genetic heritability. It is argued that many\nmore genetic factors remain undiscovered. These genetic factors likely are\nweakly associated at the population level and sparsely distributed across the\ngenome. In this paper, we adapt the recent innovations on Tukey's Higher\nCriticism (Tukey [The Higher Criticism (1976) Princeton Univ.]; Donoho and Jin\n[Ann. Statist. 32 (2004) 962-994]) to SNP-set analysis of GWAS, and develop a\nnew theoretical framework in large-scale inference to assess the joint\nsignificance of such rare and weak effects for a quantitative trait. In the\ncore of our theory is the so-called detection boundary, a curve in the\ntwo-dimensional phase space that quantifies the rarity and strength of genetic\neffects. Above the detection boundary, the overall effects of genetic factors\nare strong enough for reliable detection. Below the detection boundary, the\ngenetic factors are simply too rare and too weak for reliable detection. We\nshow that the HC-type methods are optimal in that they reliably yield detection\nonce the parameters of the genetic effects fall above the detection boundary\nand that many commonly used SNP-set methods are suboptimal. The superior\nperformance of the HC-type approach is demonstrated through simulations and the\nanalysis of a GWAS data set of Crohn's disease. \n\n"}
{"id": "1408.2128", "contents": "Title: High-dimensional unsupervised classification via parsimonious\n  contaminated mixtures Abstract: The contaminated Gaussian distribution represents a simple heavy-tailed\nelliptical generalization of the Gaussian distribution; unlike the\noften-considered t-distribution, it also allows for automatic detection of mild\noutlying or \"bad\" points in the same way that observations are typically\nassigned to the groups in the finite mixture model context. Starting from this\ndistribution, we propose the contaminated factor analysis model as a method for\ndimensionality reduction and detection of bad points in higher dimensions. A\nmixture of contaminated Gaussian factor analyzers (MCGFA) model follows\ntherefrom, and extends the recently proposed mixture of contaminated Gaussian\ndistributions to high-dimensional data. We introduce a family of 32\nparsimonious models formed by introducing constraints on the covariance and\ncontamination structures of the general MCGFA model. We outline a variant of\nthe expectation-maximization algorithm for parameter estimation. Various\nimplementation issues are discussed, and the novel family of models is compared\nto well-established approaches on both simulated and real data. \n\n"}
{"id": "1408.6500", "contents": "Title: On the Expectation-Maximization Unfolding with Smoothing Abstract: Error propagation formulae are derived for the expectation-maximization\niterative unfolding algorithm regularized by a smoothing step. The effective\nnumber of parameters in the fit to the observed data is defined for unfolding\nprocedures. Based upon this definition, the Akaike information criterion is\nproposed as a principle for choosing the smoothing parameters in an automatic,\ndata-dependent manner. The performance and the frequentist coverage of the\nresulting method are investigated using simulated samples. A number of issues\nof general relevance to all unfolding techniques are discussed, including\nirreducible bias, uncertainty increase due to a data-dependent choice of\nregularization strength, and presentation of results. \n\n"}
{"id": "1408.6583", "contents": "Title: Determination of Nonlinear Genetic Architecture using Compressed Sensing Abstract: We introduce a statistical method that can reconstruct nonlinear genetic\nmodels (i.e., including epistasis, or gene-gene interactions) from\nphenotype-genotype (GWAS) data. The computational and data resource\nrequirements are similar to those necessary for reconstruction of linear\ngenetic models (or identification of gene-trait associations), assuming a\ncondition of generalized sparsity, which limits the total number of gene-gene\ninteractions. An example of a sparse nonlinear model is one in which a typical\nlocus interacts with several or even many others, but only a small subset of\nall possible interactions exist. It seems plausible that most genetic\narchitectures fall in this category. Our method uses a generalization of\ncompressed sensing (L1-penalized regression) applied to nonlinear functions of\nthe sensing matrix. We give theoretical arguments suggesting that the method is\nnearly optimal in performance, and demonstrate its effectiveness on broad\nclasses of nonlinear genetic models using both real and simulated human\ngenomes. \n\n"}
{"id": "1409.3246", "contents": "Title: Wideband Sensing and Optimization for Cognitive Radio Networks with\n  Noise Variance Uncertainty Abstract: This paper considers wide-band spectrum sensing and optimization for\ncognitive radio (CR) networks with noise variance uncertainty. It is assumed\nthat the considered wide-band contains one or more white sub-bands. Under this\nassumption, we consider throughput maximization of the CR network while\nappropriately protecting the primary network. We address this problem as\nfollows. First, we propose novel ratio based test statistics for detecting the\nedges of each sub-band. Second, we employ simple energy comparison approach to\nchoose one reference white sub-band. Third, we propose novel generalized energy\ndetector (GED) for examining each of the remaining sub-bands by exploiting the\nnoise information of the reference white sub-band. Finally, we optimize the\nsensing time ($T_o$) to maximize the CR network throughput using the detection\nand false alarm probabilities of the GED. The proposed GED does not suffer from\nsignal to noise ratio (SNR) wall and outperforms the existing signal detectors.\nMoreover, the relationship between the proposed GED and conventional energy\ndetector (CED) is quantified analytically. We show that the optimal $T_o$\ndepends on the noise variance information. In particular, with $10$TV bands,\nSNR=$-20$dB and $2$s frame duration, we found that the optimal $T_o$ is\n$28.5$ms ($50.6$ms) with perfect (imperfect) noise variance scenario. \n\n"}
{"id": "1409.3795", "contents": "Title: On the correspondence from Bayesian log-linear modelling to logistic\n  regression modelling with $g$-priors Abstract: Consider a set of categorical variables where at least one of them is binary.\nThe log-linear model that describes the counts in the resulting contingency\ntable implies a specific logistic regression model, with the binary variable as\nthe outcome. Within the Bayesian framework, the $g$-prior and mixtures of\n$g$-priors are commonly assigned to the parameters of a generalized linear\nmodel. We prove that assigning a $g$-prior (or a mixture of $g$-priors) to the\nparameters of a certain log-linear model designates a $g$-prior (or a mixture\nof $g$-priors) on the parameters of the corresponding logistic regression. By\nderiving an asymptotic result, and with numerical illustrations, we demonstrate\nthat when a $g$-prior is adopted, this correspondence extends to the posterior\ndistribution of the model parameters. Thus, it is valid to translate inferences\nfrom fitting a log-linear model to inferences within the logistic regression\nframework, with regard to the presence of main effects and interaction terms. \n\n"}
{"id": "1409.8198", "contents": "Title: A procedure to detect general association based on concentration of\n  ranks Abstract: In modern high-throughput applications, it is important to identify pairwise\nassociations between variables, and desirable to use methods that are powerful\nand sensitive to a variety of association relationships. We describe RankCover,\na new non-parametric association test for association between two variables\nthat measures the concentration of paired ranked points. Here `concentration'\nis quantified using a disk-covering statistic that is similar to those employed\nin spatial data analysis. Analysis of simulated datasets demonstrates that the\nmethod is robust and often powerful in comparison to competing general\nassociation tests. We illustrate RankCover in the analysis of several real\ndatasets. \n\n"}
{"id": "1409.8502", "contents": "Title: Combining Particle MCMC with Rao-Blackwellized Monte Carlo Data\n  Association for Parameter Estimation in Multiple Target Tracking Abstract: We consider state and parameter estimation in multiple target tracking\nproblems with data association uncertainties and unknown number of targets. We\nshow how the problem can be recast into a conditionally linear Gaussian\nstate-space model with unknown parameters and present an algorithm for\ncomputationally efficient inference on the resulting model. The proposed\nalgorithm is based on combining the Rao-Blackwellized Monte Carlo data\nassociation algorithm with particle Markov chain Monte Carlo algorithms to\njointly estimate both parameters and data associations. Both particle marginal\nMetropolis-Hastings and particle Gibbs variants of particle MCMC are\nconsidered. We demonstrate the performance of the method both using simulated\ndata and in a real-data case study of using multiple target tracking to\nestimate the brown bear population in Finland. \n\n"}
{"id": "1410.0936", "contents": "Title: On the error of incidence estimation from prevalence data Abstract: This paper describes types of errors arising in a recently proposed method of\nincidence estimation from prevalence data. The errors are illustrated by a\nsimulation study about a hypothetical irreversible disease. In addition, a way\nof obtaining error bounds in practical applications of the method is proposed. \n\n"}
{"id": "1410.1013", "contents": "Title: Assess Sleep Stage by Modern Signal Processing Techniques Abstract: In this paper, two modern adaptive signal processing techniques, Empirical\nIntrinsic Geometry and Synchrosqueezing transform, are applied to quantify\ndifferent dynamical features of the respiratory and electroencephalographic\nsignals. We show that the proposed features are theoretically rigorously\nsupported, as well as capture the sleep information hidden inside the signals.\nThe features are used as input to multiclass support vector machines with the\nradial basis function to automatically classify sleep stages. The effectiveness\nof the classification based on the proposed features is shown to be comparable\nto human expert classification -- the proposed classification of awake, REM,\nN1, N2 and N3 sleeping stages based on the respiratory signal (resp.\nrespiratory and EEG signals) has the overall accuracy $81.7\\%$ (resp. $89.3\\%$)\nin the relatively normal subject group. In addition, by examining the\ncombination of the respiratory signal with the electroencephalographic signal,\nwe conclude that the respiratory signal consists of ample sleep information,\nwhich supplements to the information stored in the electroencephalographic\nsignal. \n\n"}
{"id": "1410.2285", "contents": "Title: A Statistical Approach to Crime Linkage Abstract: The object of this paper is to develop a statistical approach to criminal\nlinkage analysis that discovers and groups crime events that share a common\noffender and prioritizes suspects for further investigation. Bayes factors are\nused to describe the strength of evidence that two crimes are linked. Using\nconcepts from agglomerative hierarchical clustering, the Bayes factors for\ncrime pairs are combined to provide similarity measures for comparing two crime\nseries. This facilitates crime series clustering, crime series identification,\nand suspect prioritization. The ability of our models to make correct linkages\nand predictions is demonstrated under a variety of real-world scenarios with a\nlarge number of solved and unsolved breaking and entering crimes. For example,\na na\\\"ive Bayes model for pairwise case linkage can identify 82\\% of actual\nlinkages with a 5\\% false positive rate. For crime series identification,\n77\\%-89\\% of the additional crimes in a crime series can be identified from a\nranked list of 50 incidents. \n\n"}
{"id": "1410.2392", "contents": "Title: Control functionals for Monte Carlo integration Abstract: A non-parametric extension of control variates is presented. These leverage\ngradient information on the sampling density to achieve substantial variance\nreduction. It is not required that the sampling density be normalised. The\nnovel contribution of this work is based on two important insights; (i) a\ntrade-off between random sampling and deterministic approximation and (ii) a\nnew gradient-based function space derived from Stein's identity. Unlike\nclassical control variates, our estimators achieve super-root-$n$ convergence,\noften requiring orders of magnitude fewer simulations to achieve a fixed level\nof precision. Theoretical and empirical results are presented, the latter\nfocusing on integration problems arising in hierarchical models and models\nbased on non-linear ordinary differential equations. \n\n"}
{"id": "1410.6059", "contents": "Title: Integer percentages as electoral falsification fingerprints Abstract: We hypothesize that if election results are manipulated or forged, then, due\nto the well-known human attraction to round numbers, the frequency of reported\nround percentages can be increased. To test this hypothesis, we analyzed raw\ndata from seven federal elections held in the Russian Federation during the\nperiod from 2000 to 2012 and found that in all elections since 2004 the number\nof polling stations reporting turnout and/or leader's result expressed by an\ninteger percentage (as opposed to a fractional value) was much higher than\nexpected by pure chance. Monte Carlo simulations confirmed high statistical\nsignificance of the observed phenomenon, thereby suggesting its man-made\nnature. Geographical analysis showed that these anomalies were concentrated in\na specific subset of Russian regions which strongly suggests its orchestrated\norigin. Unlike previously proposed statistical indicators of alleged electoral\nfalsifications, our observations can hardly be explained differently but by a\nwidespread election fraud. \n\n"}
{"id": "1410.6560", "contents": "Title: Robust estimation of isoform expression with RNA-Seq data Abstract: Qualifying gene and isoform expression is one of the primary tasks for\nRNA-Seq experiments. Given a sequence of counts representing numbers of reads\nmapped to different positions (exons and junctions) of isoforms, methods based\non Poisson generalized linear models (GLM) with the identity link function have\nbeen proposed to estimate isoform expression levels from these counts. These\nPoisson based models have very limited ability in handling the overdispersion\nin the counts brought by various sources, and some of them are not robust to\noutliers. We propose a negative binomial based GLM with identity link, and use\na set of robustified quasi-likelihood equations to make it resistant to\noutliers. An efficient and reliable numeric algorithm has been identified to\nsolve these equations. In simulations, we find that our approach seems to\noutperform existing approaches. We also find evidence supporting this\nconclusion in real RNA-Seq data. \n\n"}
{"id": "1410.7690", "contents": "Title: Trend Filtering on Graphs Abstract: We introduce a family of adaptive estimators on graphs, based on penalizing\nthe $\\ell_1$ norm of discrete graph differences. This generalizes the idea of\ntrend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate\nnonparametric regression, to graphs. Analogous to the univariate case, graph\ntrend filtering exhibits a level of local adaptivity unmatched by the usual\n$\\ell_2$-based graph smoothers. It is also defined by a convex minimization\nproblem that is readily solved (e.g., by fast ADMM or Newton algorithms). We\ndemonstrate the merits of graph trend filtering through examples and theory. \n\n"}
{"id": "1410.7716", "contents": "Title: Forecasting the 2013--2014 Influenza Season using Wikipedia Abstract: Infectious diseases are one of the leading causes of morbidity and mortality\naround the world; thus, forecasting their impact is crucial for planning an\neffective response strategy. According to the Centers for Disease Control and\nPrevention (CDC), seasonal influenza affects between 5% to 20% of the U.S.\npopulation and causes major economic impacts resulting from hospitalization and\nabsenteeism. Understanding influenza dynamics and forecasting its impact is\nfundamental for developing prevention and mitigation strategies.\n  We combine modern data assimilation methods with Wikipedia access logs and\nCDC influenza like illness (ILI) reports to create a weekly forecast for\nseasonal influenza. The methods are applied to the 2013--2014 influenza season\nbut are sufficiently general to forecast any disease outbreak, given incidence\nor case count data. We adjust the initialization and parametrization of a\ndisease model and show that this allows us to determine systematic model bias.\nIn addition, we provide a way to determine where the model diverges from\nobservation and evaluate forecast accuracy.\n  Wikipedia article access logs are shown to be highly correlated with\nhistorical ILI records and allow for accurate prediction of ILI data several\nweeks before it becomes available. The results show that prior to the peak of\nthe flu season, our forecasting method projected the actual outcome with a high\nprobability. However, since our model does not account for re-infection or\nmultiple strains of influenza, the tail of the epidemic is not predicted well\nafter the peak of flu season has past. \n\n"}
{"id": "1411.0599", "contents": "Title: Dynamic spatial regression models for space-varying forest stand tables Abstract: Many forest management planning decisions are based on information about the\nnumber of trees by species and diameter per unit area. This information is\ncommonly summarized in a stand table, where a stand is defined as a group of\nforest trees of sufficiently uniform species composition, age, condition, or\nproductivity to be considered a homogeneous unit for planning purposes.\nTypically information used to construct stand tables is gleaned from observed\nsubsets of the forest selected using a probability-based sampling design. Such\nsampling campaigns are expensive and hence only a small number of sample units\nare typically observed. This data paucity means that stand tables can only be\nestimated for relatively large areal units. Contemporary forest management\nplanning and spatially explicit ecosystem models require stand table input at\nhigher spatial resolution than can be affordably provided using traditional\napproaches. We propose a dynamic multivariate Poisson spatial regression model\nthat accommodates both spatial correlation between observed diameter\ndistributions and also correlation between tree counts across diameter classes\nwithin each location. To improve fit and prediction at unobserved locations,\ndiameter specific intensities can be estimated using auxiliary data such as\nmanagement history or remotely sensed information. The proposed model is used\nto analyze a diverse forest inventory dataset collected on the United States\nForest Service Penobscot Experimental Forest in Bradley, Maine. Results\ndemonstrate that explicitly modeling the residual spatial structure via a\nmultivariate Gaussian process and incorporating information about forest\nstructure from LiDAR covariates improve model fit and can provide high spatial\nresolution stand table maps with associated estimates of uncertainty. \n\n"}
{"id": "1411.1045", "contents": "Title: Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on\n  ImageNet Abstract: Recent results suggest that state-of-the-art saliency models perform far from\noptimal in predicting fixations. This lack in performance has been attributed\nto an inability to model the influence of high-level image features such as\nobjects. Recent seminal advances in applying deep neural networks to tasks like\nobject recognition suggests that they are able to capture this kind of\nstructure. However, the enormous amount of training data necessary to train\nthese networks makes them difficult to apply directly to saliency prediction.\nWe present a novel way of reusing existing neural networks that have been\npretrained on the task of object recognition in models of fixation prediction.\nUsing the well-known network of Krizhevsky et al. (2012), we come up with a new\nsaliency model that significantly outperforms all state-of-the-art models on\nthe MIT Saliency Benchmark. We show that the structure of this network allows\nnew insights in the psychophysics of fixation selection and potentially their\nneural implementation. To train our network, we build on recent work on the\nmodeling of saliency as point processes. \n\n"}
{"id": "1411.1594", "contents": "Title: Joint estimation of $K$ related regression models with simple $L_1$-norm\n  penalties Abstract: We propose a new approach, along with refinements, based on $L_1$ penalties\nand aimed at jointly estimating several related regression models. Its main\ninterest is that it can be rewritten as a weighted lasso on a simple\ntransformation of the original data set. In particular, it does not need new\ndedicated algorithms and is ready to implement under a variety of regression\nmodels, {\\em e.g.}, using standard R packages. Moreover, asymptotic oracle\nproperties are derived along with preliminary non-asymptotic results,\nsuggesting good theoretical properties. Our approach is further compared with\nstate-of-the-art competitors under various settings on synthetic data: these\nempirical results confirm that our approach performs at least similarly to its\ncompetitors. As a final illustration, an analysis of road safety data is\nprovided. \n\n"}
{"id": "1411.2636", "contents": "Title: Bounding the Probability of Causation in Mediation Analysis Abstract: Given empirical evidence for the dependence of an outcome variable on an\nexposure variable, we can typically only provide bounds for the \"probability of\ncausation\" in the case of an individual who has developed the outcome after\nbeing exposed. We show how these bounds can be adapted or improved if further\ninformation becomes available. In addition to reviewing existing work on this\ntopic, we provide a new analysis for the case where a mediating variable can be\nobserved. In particular we show how the probability of causation can be bounded\nwhen there is no direct effect and no confounding.\n  Keywords: Causal inference, Mediation Analysis, Probability of Causation \n\n"}
{"id": "1411.3062", "contents": "Title: Structural Change in Sparsity Abstract: In the high-dimensional sparse modeling literature, it has been crucially\nassumed that the sparsity structure of the model is homogeneous over the entire\npopulation. That is, the identities of important regressors are invariant\nacross the population and across the individuals in the collected sample. In\npractice, however, the sparsity structure may not always be invariant in the\npopulation, due to heterogeneity across different sub-populations. We consider\na general, possibly non-smooth M-estimation framework, allowing a possible\nstructural change regarding the identities of important regressors in the\npopulation. Our penalized M-estimator not only selects covariates but also\ndiscriminates between a model with homogeneous sparsity and a model with a\nstructural change in sparsity. As a result, it is not necessary to know or\npretest whether the structural change is present, or where it occurs. We derive\nasymptotic bounds on the estimation loss of the penalized M-estimators, and\nachieve the oracle properties. We also show that when there is a structural\nchange, the estimator of the threshold parameter is super-consistent. If the\nsignal is relatively strong, the rates of convergence can be further improved\nand asymptotic distributional properties of the estimators including the\nthreshold estimator can be established using an adaptive penalization. The\nproposed methods are then applied to quantile regression and logistic\nregression models and are illustrated via Monte Carlo experiments. \n\n"}
{"id": "1411.3816", "contents": "Title: Monte Carlo error analyses of Spearman's rank test Abstract: Spearman's rank correlation test is commonly used in astronomy to discern\nwhether a set of two variables are correlated or not. Unlike most other\nquantities quoted in astronomical literature, the Spearman's rank correlation\ncoefficient is generally quoted with no attempt to estimate the errors on its\nvalue. This is a practice that would not be accepted for those other\nquantities, as it is often regarded that an estimate of a quantity without an\nestimate of its associated uncertainties is meaningless. This manuscript\ndescribes a number of easily implemented, Monte Carlo based methods to estimate\nthe uncertainty on the Spearman's rank correlation coefficient, or more\nprecisely to estimate its probability distribution. \n\n"}
{"id": "1411.4289", "contents": "Title: The impact of stochastic lead times on the bullwhip effect Abstract: In this article we want to review the research state on the bullwhip effect\nin supply chains with stochastic lead times and give a contribution to\nquantifying the bullwhip effect. We analyze the models quantifying the bullwhip\neffect in supply chains with stochastic lead times and find advantages and\ndisadvantages of their approaches to the bullwhip problem. Using real data we\nconfirm that real lead times are stochastic and can be modeled by a sequence of\nindependent identically distributed random variables. Moreover we modify a\nmodel where stochastic lead times and lead time demand forecasting are\nconsidered and give an analytical expression for the bullwhip effect measure\nwhich indicates that the distribution of a lead time and the delay parameter of\nthe lead time demand prediction are the main factors of the bullwhip\nphenomenon. Moreover we analyze a recent paper of Michna and Nielsen adding\nsimulation results. \n\n"}
{"id": "1411.5497", "contents": "Title: Time-warped growth processes, with applications to the modeling of\n  boom-bust cycles in house prices Abstract: House price increases have been steady over much of the last 40 years, but\nthere have been occasional declines, most notably in the recent housing bust\nthat started around 2007, on the heels of the preceding housing bubble. We\nintroduce a novel growth model that is motivated by time-warping models in\nfunctional data analysis and includes a nonmonotone time-warping component that\nallows the inclusion and description of boom-bust cycles and facilitates\ninsights into the dynamics of asset bubbles. The underlying idea is to model\nlongitudinal growth trajectories for house prices and other phenomena, where\ntemporal setbacks and deflation may be encountered, by decomposing such\ntrajectories into two components. A first component corresponds to underlying\nsteady growth driven by inflation that anchors the observed trajectories on a\nsimple first order linear differential equation, while a second boom-bust\ncomponent is implemented as time warping. Time warping is a commonly\nencountered phenomenon and reflects random variation along the time axis. Our\napproach to time warping is more general than previous approaches by admitting\nthe inclusion of nonmonotone warping functions. The anchoring of the\ntrajectories on an underlying linear dynamic system also makes the time-warping\ncomponent identifiable and enables straightforward estimation procedures for\nall model components. The application to the dynamics of housing prices as\nobserved for 19 metropolitan areas in the U.S. from December 1998 to July 2013\nreveals that the time setbacks corresponding to nonmonotone time warping vary\nsubstantially across markets and we find indications that they are related to\nmarket-specific growth rates. \n\n"}
{"id": "1411.6506", "contents": "Title: Bayesian Inference and Testing of Group Differences in Brain Networks Abstract: Network data are increasingly collected along with other variables of\ninterest. Our motivation is drawn from neurophysiology studies measuring brain\nconnectivity networks for a sample of individuals along with their membership\nto a low or high creative reasoning group. It is of paramount importance to\ndevelop statistical methods for testing of global and local changes in the\nstructural interconnections among brain regions across groups. We develop a\ngeneral Bayesian procedure for inference and testing of group differences in\nthe network structure, which relies on a nonparametric representation for the\nconditional probability mass function associated with a network-valued random\nvariable. By leveraging a mixture of low-rank factorizations, we allow simple\nglobal and local hypothesis testing adjusting for multiplicity. An efficient\nGibbs sampler is defined for posterior computation. We provide theoretical\nresults on the flexibility of the model and assess testing performance in\nsimulations. The approach is applied to provide novel insights on the\nrelationships between human brain networks and creativity. \n\n"}
{"id": "1411.6719", "contents": "Title: Asymptotically Optimal Discrete Time Nonlinear Filters From\n  Stochastically Convergent State Process Approximations Abstract: We consider the problem of approximating optimal in the Minimum Mean Squared\nError (MMSE) sense nonlinear filters in a discrete time setting, exploiting\nproperties of stochastically convergent state process approximations. More\nspecifically, we consider a class of nonlinear, partially observable stochastic\nsystems, comprised by a (possibly nonstationary) hidden stochastic process (the\nstate), observed through another conditionally Gaussian stochastic process (the\nobservations). Under general assumptions, we show that, given an approximating\nprocess which, for each time step, is stochastically convergent to the state\nprocess, an approximate filtering operator can be defined, which converges to\nthe true optimal nonlinear filter of the state in a strong and well defined\nsense. In particular, the convergence is compact in time and uniform in a\ncompletely characterized measurable set of probability measure almost unity,\nalso providing a purely quantitative justification of Egoroff's Theorem for the\nproblem at hand. The results presented in this paper can form a common basis\nfor the analysis and characterization of a number of heuristic approaches for\napproximating optimal nonlinear filters, such as approximate grid based\ntechniques, known to perform well in a variety of applications. \n\n"}
{"id": "1411.7009", "contents": "Title: Additive Gaussian Process Regression Abstract: Additive-interactive regression has recently been shown to offer attractive\nminimax error rates over traditional nonparametric multivariate regression in a\nwide variety of settings, including cases where the predictor count is much\nlarger than the sample size and many of the predictors have important effects\non the response, potentially through complex interactions. We present a\nBayesian implementation of additive-interactive regression using an additive\nGaussian process (AGP) prior and develop an efficient Markov chain sampler that\nextends stochastic search variable selection in this setting. Careful prior and\nhyper-parameter specification are developed in light of performance and\ncomputational considerations, and key innovations address difficulties in\nexploring a joint posterior distribution over multiple subsets of high\ndimensional predictor inclusion vectors. The method offers state-of-the-art\nsupport and interaction recovery while improving dramatically over competitors\nin terms of prediction accuracy on a diverse set of simulated and real data.\nResults from real data studies provide strong evidence that the\nadditive-interactive framework is an attractive modeling platform for\nhigh-dimensional nonparametric regression. \n\n"}
{"id": "1411.7782", "contents": "Title: Combining regional estimation and historical floods: a multivariate\n  semi-parametric peaks-over-threshold model with censored data Abstract: The estimation of extreme flood quantiles is challenging due to the relative\nscarcity of extreme data compared to typical target return periods. Several\napproaches have been developed over the years to face this challenge, including\nregional estimation and the use of historical flood data. This paper\ninvestigates the combination of both approaches using a multivariate\npeaks-over-threshold model, that allows estimating altogether the intersite\ndependence structure and the marginal distributions at each site. The joint\ndistribution of extremes at several sites is constructed using a\nsemi-parametric Dirichlet Mixture model. The existence of partially missing and\ncensored observations (historical data) is accounted for within a data\naugmentation scheme. This model is applied to a case study involving four\ncatchments in Southern France, for which historical data are available since\n1604. The comparison of marginal estimates from four versions of the model\n(with or without regionalizing the shape parameter; using or ignoring\nhistorical floods) highlights significant differences in terms of return level\nestimates. Moreover, the availability of historical data on several nearby\ncatchments allows investigating the asymptotic dependence properties of extreme\nfloods. Catchments display a a significant amount of asymptotic dependence,\ncalling for adapted multivariate statistical models. \n\n"}
{"id": "1411.7934", "contents": "Title: Estimating parameters of a multipartite loglinear graph model via the EM\n  algorithm Abstract: We will amalgamate the Rash model (for rectangular binary tables) and the\nnewly introduced $\\alpha$-$\\beta$ models (for random undirected graphs) in the\nframework of a semiparametric probabilistic graph model. Our purpose is to give\na partition of the vertices of an observed graph so that the generated\nsubgraphs and bipartite graphs obey these models, where their strongly\nconnected parameters give multiscale evaluation of the vertices at the same\ntime. In this way, a heterogeneous version of the stochastic block model is\nbuilt via mixtures of loglinear models and the parameters are estimated with a\nspecial EM iteration. In the context of social networks, the clusters can be\nidentified with social groups and the parameters with attitudes of people of\none group towards people of the other, which attitudes depend on the cluster\nmemberships. The algorithm is applied to randomly generated and real-word data. \n\n"}
{"id": "1412.1977", "contents": "Title: Quantum metrology with non-equilibrium steady states of quantum spin\n  chains Abstract: We consider parameter estimations with probes being the boundary\ndriven/dissipated non- equilibrium steady states of XXZ spin 1/2 chains. The\nparameters to be estimated are the dissipation coupling and the anisotropy of\nthe spin-spin interaction. In the weak coupling regime we compute the scaling\nof the Fisher information, i.e. the inverse best sensitivity among all\nestimators, with the number of spins. We find superlinear scalings and\ntransitions between the distinct, isotropic and anisotropic, phases. We also\nlook at the best relative error which decreases with the number of particles\nfaster than the shot-noise only for the estimation of anisotropy. \n\n"}
{"id": "1412.4005", "contents": "Title: Sparsity and adaptivity for the blind separation of partially correlated\n  sources Abstract: Blind source separation (BSS) is a very popular technique to analyze\nmultichannel data. In this context, the data are modeled as the linear\ncombination of sources to be retrieved. For that purpose, standard BSS methods\nall rely on some discrimination principle, whether it is statistical\nindependence or morphological diversity, to distinguish between the sources.\nHowever, dealing with real-world data reveals that such assumptions are rarely\nvalid in practice: the signals of interest are more likely partially\ncorrelated, which generally hampers the performances of standard BSS methods.\nIn this article, we introduce a novel sparsity-enforcing BSS method coined\nAdaptive Morphological Component Analysis (AMCA), which is designed to retrieve\nsparse and partially correlated sources. More precisely, it makes profit of an\nadaptive re-weighting scheme to favor/penalize samples based on their level of\ncorrelation. Extensive numerical experiments have been carried out which show\nthat the proposed method is robust to the partial correlation of sources while\nstandard BSS techniques fail. The AMCA algorithm is evaluated in the field of\nastrophysics for the separation of physical components from microwave data. \n\n"}
{"id": "1412.4533", "contents": "Title: Weighted principal component analysis: a weighted covariance\n  eigendecomposition approach Abstract: We present a new straightforward principal component analysis (PCA) method\nbased on the diagonalization of the weighted variance-covariance matrix through\ntwo spectral decomposition methods: power iteration and Rayleigh quotient\niteration. This method allows one to retrieve a given number of orthogonal\nprincipal components amongst the most meaningful ones for the case of problems\nwith weighted and/or missing data. Principal coefficients are then retrieved by\nfitting principal components to the data while providing the final\ndecomposition. Tests performed on real and simulated cases show that our method\nis optimal in the identification of the most significant patterns within data\nsets. We illustrate the usefulness of this method by assessing its quality on\nthe extrapolation of Sloan Digital Sky Survey quasar spectra from measured\nwavelengths to shorter and longer wavelengths. Our new algorithm also benefits\nfrom a fast and flexible implementation. \n\n"}
{"id": "1412.4912", "contents": "Title: Statistical Inference for Oscillation Processes Abstract: A new model for time series with a specific oscillation pattern is proposed.\nThe model consists of a hidden phase process controlling the speed of polling\nand a nonparametric curve characterizing the pattern, leading together to a\ngeneralized state space model. Identifiability of the model is proved and a\nmethod for statistical inference based on a particle smoother and a\nnonparametric EM algorithm is developed. In particular, the oscillation pattern\nand the unobserved phase process are estimated. The proposed algorithms are\ncomputationally efficient and their performance is assessed through simulations\nand an application to human electrocardiogram recordings. \n\n"}
{"id": "1412.5000", "contents": "Title: Randomization Inference for Treatment Effect Variation Abstract: Applied researchers are increasingly interested in whether and how treatment\neffects vary in randomized evaluations, especially variation not explained by\nobserved covariates. We propose a model-free approach for testing for the\npresence of such unexplained variation. To use this randomization-based\napproach, we must address the fact that the average treatment effect, generally\nthe object of interest in randomized experiments, actually acts as a nuisance\nparameter in this setting. We explore potential solutions and advocate for a\nmethod that guarantees valid tests in finite samples despite this nuisance. We\nalso show how this method readily extends to testing for heterogeneity beyond a\ngiven model, which can be useful for assessing the sufficiency of a given\nscientific theory. We finally apply our method to the National Head Start\nImpact Study, a large-scale randomized evaluation of a Federal preschool\nprogram, finding that there is indeed significant unexplained treatment effect\nvariation. \n\n"}
{"id": "1412.5250", "contents": "Title: High Dimensional Forecasting via Interpretable Vector Autoregression Abstract: Vector autoregression (VAR) is a fundamental tool for modeling multivariate\ntime series. However, as the number of component series is increased, the VAR\nmodel becomes overparameterized. Several authors have addressed this issue by\nincorporating regularized approaches, such as the lasso in VAR estimation.\nTraditional approaches address overparameterization by selecting a low lag\norder, based on the assumption of short range dependence, assuming that a\nuniversal lag order applies to all components. Such an approach constrains the\nrelationship between the components and impedes forecast performance. The\nlasso-based approaches work much better in high-dimensional situations but do\nnot incorporate the notion of lag order selection.\n  We propose a new class of hierarchical lag structures (HLag) that embed the\nnotion of lag selection into a convex regularizer. The key modeling tool is a\ngroup lasso with nested groups which guarantees that the sparsity pattern of\nlag coefficients honors the VAR's ordered structure. The HLag framework offers\nthree structures, which allow for varying levels of flexibility. A simulation\nstudy demonstrates improved performance in forecasting and lag order selection\nover previous approaches, and a macroeconomic application further highlights\nforecasting improvements as well as HLag's convenient, interpretable output. \n\n"}
{"id": "1412.5376", "contents": "Title: Nonparametric tests for detecting breaks in the jump behaviour of a\n  time-continuous process Abstract: This paper is concerned with tests for changes in the jump behaviour of a\ntime-continuous process. Based on results on weak convergence of a sequential\nempirical tail integral process, asymptotics of certain tests statistics for\nbreaks in the jump measure of an Ito semimartingale are constructed. Whenever\nlimiting distributions depend in a complicated way on the unknown jump measure,\nempirical quantiles are obtained using a multiplier bootstrap scheme. An\nextensive simulation study shows a good performance of our tests in finite\nsamples. \n\n"}
{"id": "1412.5656", "contents": "Title: A Note on Minimax Testing and Confidence Intervals in Moment Inequality\n  Models Abstract: This note uses a simple example to show how moment inequality models used in\nthe empirical economics literature lead to general minimax relative efficiency\ncomparisons. The main point is that such models involve inference on a low\ndimensional parameter, which leads naturally to a definition of \"distance\"\nthat, in full generality, would be arbitrary in minimax testing problems. This\ndefinition of distance is justified by the fact that it leads to a duality\nbetween minimaxity of confidence intervals and tests, which does not hold for\nother definitions of distance. Thus, the use of moment inequalities for\ninference in a low dimensional parametric model places additional structure on\nthe testing problem, which leads to stronger conclusions regarding minimax\nrelative efficiency than would otherwise be possible. \n\n"}
{"id": "1412.6520", "contents": "Title: Estimating a Common Period for a Set of Irregularly Sampled Functions\n  with Applications to Periodic Variable Star Data Abstract: We consider the estimation of a common period for a set of functions sampled\nat irregular intervals. The problem arises in astronomy, where the functions\nrepresent a star's brightness observed over time through different photometric\nfilters. While current methods can estimate periods accurately provided that\nthe brightness is well--sampled in at least one filter, there are no existing\nmethods that can provide accurate estimates when no brightness function is\nwell--sampled. In this paper we introduce two new methods for period estimation\nwhen brightnesses are poorly--sampled in all filters. The first, multiband\ngeneralized Lomb-Scargle (MGLS), extends the frequently used Lomb-Scargle\nmethod in a way that na\\\"{i}vely combines information across filters. The\nsecond, penalized generalized Lomb-Scargle (PGLS), builds on the first by more\nintelligently borrowing strength across filters. Specifically, we incorporate\nconstraints on the phases and amplitudes across the different functions using a\nnon--convex penalized likelihood function. We develop a fast algorithm to\noptimize the penalized likelihood by combining block coordinate descent with\nthe majorization-minimization (MM) principle. We illustrate our methods on\nsynthetic and real astronomy data. Both advance the state-of-the-art in period\nestimation; however, PGLS significantly outperforms MGLS when all functions are\nextremely poorly--sampled. \n\n"}
{"id": "1412.7096", "contents": "Title: Estimation of slowly decreasing Hawkes kernels: Application to high\n  frequency order book modelling Abstract: We present a modified version of the non parametric Hawkes kernel estimation\nprocedure studied in arXiv:1401.0903 that is adapted to slowly decreasing\nkernels. We show on numerical simulations involving a reasonable number of\nevents that this method allows us to estimate faithfully a power-law decreasing\nkernel over at least 6 decades. We then propose a 8-dimensional Hawkes model\nfor all events associated with the first level of some asset order book.\nApplying our estimation procedure to this model, allows us to uncover the main\nproperties of the coupled dynamics of trade, limit and cancel orders in\nrelationship with the mid-price variations. \n\n"}
{"id": "1501.00264", "contents": "Title: Bayesian Design of Experiments using Approximate Coordinate Exchange Abstract: The construction of decision-theoretic Bayesian designs for\nrealistically-complex nonlinear models is computationally challenging, as it\nrequires the optimization of analytically intractable expected utility\nfunctions over high-dimensional design spaces. We provide the most general\nsolution to date for this problem through a novel approximate coordinate\nexchange algorithm. This methodology uses a Gaussian process emulator to\napproximate the expected utility as a function of a single design coordinate in\na series of conditional optimization steps. It has flexibility to address\nproblems for any choice of utility function and for a wide range of statistical\nmodels with different numbers of variables, numbers of runs and randomization\nrestrictions. In contrast to existing approaches to Bayesian design, the method\ncan find multi-variable designs in large numbers of runs without resorting to\nasymptotic approximations to the posterior distribution or expected utility.\nThe methodology is demonstrated on a variety of challenging examples of\npractical importance, including design for pharmacokinetic models and design\nfor mixed models with discrete data. For many of these models, Bayesian designs\nare not currently available. Comparisons are made to results from the\nliterature, and to designs obtained from asymptotic approximations. \n\n"}
{"id": "1501.00478", "contents": "Title: Uniform Inference in High-dimensional Dynamic Panel Data Models Abstract: We establish oracle inequalities for a version of the Lasso in\nhigh-dimensional fixed effects dynamic panel data models. The inequalities are\nvalid for the coefficients of the dynamic and exogenous regressors. Separate\noracle inequalities are derived for the fixed effects. Next, we show how one\ncan conduct uniformly valid simultaneous inference on the parameters of the\nmodel and construct a uniformly valid estimator of the asymptotic covariance\nmatrix which is robust to conditional heteroskedasticity in the error terms.\nAllowing for conditional heteroskedasticity is important in dynamic models as\nthe conditional error variance may be non-constant over time and depend on the\ncovariates. Furthermore, our procedure allows for inference on high-dimensional\nsubsets of the parameter vector of an increasing cardinality. We show that the\nconfidence bands resulting from our procedure are asymptotically honest and\ncontract at the optimal rate. This rate is different for the fixed effects than\nfor the remaining parts of the parameter vector. \n\n"}
{"id": "1501.00592", "contents": "Title: Robust Classification of High Dimension Low Sample Size Data Abstract: The robustification of pattern recognition techniques has been the subject of\nintense research in recent years. Despite the multiplicity of papers on the\nsubject, very few articles have deeply explored the topic of robust\nclassification in the high dimension low sample size context. In this work, we\nexplore and compare the predictive performances of robust classification\ntechniques with a special concentration on robust discriminant analysis and\nrobust PCA applied to a wide variety of large $p$ small $n$ data sets. We also\nexplore the performance of random forest by way of comparing and contrasting\nthe differences single model methods and ensemble methods in this context. Our\nwork reveals that Random Forest, although not inherently designed to be robust\nto outliers, substantially outperforms the existing techniques specifically\ndesigned to achieve robustness. Indeed, random forest emerges as the best\npredictively on both real life and simulated data. \n\n"}
{"id": "1501.02108", "contents": "Title: Signal from noise retrieval from one and two-point Green's function -\n  comparison Abstract: We compare two methods of eigen-inference from large sets of data, based on\nthe analysis of one-point and two-point Green's functions, respectively. Our\nanalysis points at the superiority of eigen-inference based on one-point\nGreen's function. First, the applied by us method based on Pad?e approximants\nis orders of magnitude faster comparing to the eigen-inference based on\nuctuations (two-point Green's functions). Second, we have identified the source\nof potential instability of the two-point Green's function method, as arising\nfrom the spurious zero and negative modes of the estimator for a variance\noperator of the certain multidimensional Gaussian distribution, inherent for\nthe two-point Green's function eigen-inference method. Third, we have presented\nthe cases of eigen-inference based on negative spectral moments, for strictly\npositive spectra. Finally, we have compared the cases of eigen-inference of\nreal-valued and complex-valued correlated Wishart distributions, reinforcing\nour conclusions on an advantage of the one-point Green's function method. \n\n"}
{"id": "1501.02467", "contents": "Title: Fast and optimal nonparametric sequential design for astronomical\n  observations Abstract: The spectral energy distribution (SED) is a relatively easy way for\nastronomers to distinguish between different astronomical objects such as\ngalaxies, black holes, and stellar objects. By comparing the observations from\na source at different frequencies with template models, astronomers are able to\ninfer the type of this observed object. In this paper, we take a Bayesian model\naveraging perspective to learn astronomical objects, employing a Bayesian\nnonparametric approach to accommodate the deviation from convex combinations of\nknown log-SEDs. To effectively use telescope time for observations, we then\nstudy Bayesian nonparametric sequential experimental design without conjugacy,\nin which we use sequential Monte Carlo as an efficient tool to maximize the\nvolume of information stored in the posterior distribution of the parameters of\ninterest. A new technique for performing inferences in log-Gaussian Cox\nprocesses called the Poisson log-normal approximation is also proposed.\nSimulations show the speed, accuracy, and usefulness of our method. While the\nstrategy we propose in this paper is brand new in the astronomy literature, the\ninferential techniques developed apply to more general nonparametric sequential\nexperimental design problems. \n\n"}
{"id": "1501.04849", "contents": "Title: Bayesian Gaussian Copula Graphical Modeling for Dupuytren Disease Abstract: Dupuytren disease is a fibroproliferative disorder with unknown etiology that\noften progresses and eventually can cause permanent contractures of the\naffected fingers. In this paper, we provide a computationally efficient\nBayesian framework to discover potential risk factors and investigate which\nfingers are jointly affected. Our Bayesian approach is based on Gaussian copula\ngraphical models, which are one potential way to discover the underlying\nconditional independence structure of variables in multivariate mixed data. In\nparticular, we combine the semiparametric Gaussian copula with extended rank\nlikelihood which is appropriate to analyse multivariate mixed data with\narbitrary marginal distributions. For the graph structure learning, we\nconstruct a computationally efficient search algorithm which is a\ntrans-dimensional MCMC algorithm based on a birth-death process. In addition,\nto make our statistical method easily accessible to other researchers, we have\nimplemented our method in C++ and interfaced with R software as an R package\nBDgraph which is available online. \n\n"}
{"id": "1501.05303", "contents": "Title: Cosmic Web Reconstruction through Density Ridges: Method and Algorithm Abstract: The detection and characterization of filamentary structures in the cosmic\nweb allows cosmologists to constrain parameters that dictates the evolution of\nthe Universe. While many filament estimators have been proposed, they generally\nlack estimates of uncertainty, reducing their inferential power. In this paper,\nwe demonstrate how one may apply the Subspace Constrained Mean Shift (SCMS)\nalgorithm (Ozertem and Erdogmus (2011); Genovese et al. (2012)) to uncover\nfilamentary structure in galaxy data. The SCMS algorithm is a gradient ascent\nmethod that models filaments as density ridges, one-dimensional smooth curves\nthat trace high-density regions within the point cloud. We also demonstrate how\naugmenting the SCMS algorithm with bootstrap-based methods of uncertainty\nestimation allows one to place uncertainty bands around putative filaments. We\napply the SCMS method to datasets sampled from the P3M N-body simulation, with\ngalaxy number densities consistent with SDSS and WFIRST-AFTA and to LOWZ and\nCMASS data from the Baryon Oscillation Spectroscopic Survey (BOSS). To further\nassess the efficacy of SCMS, we compare the relative locations of BOSS\nfilaments with galaxy clusters in the redMaPPer catalog, and find that\nredMaPPer clusters are significantly closer (with p-values $< 10^{-9}$) to\nSCMS-detected filaments than to randomly selected galaxies. \n\n"}
{"id": "1501.07329", "contents": "Title: A Big Data Architecture Design for Smart Grids Based on Random Matrix\n  Theory Abstract: Model-based analysis tools, built on assumptions and simplifications, are\ndifficult to handle smart grids with data characterized by 4Vs data. This\npaper, using random matrix theory (RMT), motivates data-driven tools to\nperceive the complex grids in highdimension; meanwhile, an architecture with\ndetailed procedures is proposed. In algorithm perspective, the architecture\nperforms a high-dimensional analysis, and compares the findings with RMT\npredictions to conduct anomaly detections. Mean Spectral Radius (MSR), as a\nstatistical indicator, is defined to reflect the correlations of system data in\ndifferent dimensions. In management mode perspective, a group-work mode is\ndiscussed for smart grids operation. This mode breaks through regional\nlimitations for energy flows and data flows, and makes advanced big data\nanalyses possible. For a specific large-scale zone-dividing system with\nmultiple connected utilities, each site, operating under the group-work mode,\nis able to work out the regional MSR only with its own measured/simulated data.\nThe large-scale interconnected system, in this way, is naturally decoupled from\nstatistical parameters perspective, rather than from engineering models\nperspective. Furthermore, a comparative analysis of these distributed MSRs,\neven with imperceptible different raw data, will produce a contour line to\ndetect the event and locate the source. It demonstrates that the architecture\nis compatible with the block calculation only using the regional small\ndatabase; beyond that, this architecture, as a data-driven solution, is\nsensitive to system situation awareness, and practical for real large-scale\ninterconnected systems. Five case studies and their visualizations validate the\ndesigned architecture in various fields of power systems. To our best\nknowledge, this study is the first attempt to apply big data technology into\nsmart grids. \n\n"}
{"id": "1502.01974", "contents": "Title: Regionalization of Multiscale Spatial Processes using a Criterion for\n  Spatial Aggregation Error Abstract: The modifiable areal unit problem and the ecological fallacy are known\nproblems that occur when modeling multiscale spatial processes. We investigate\nhow these forms of spatial aggregation error can guide a regionalization over a\nspatial domain of interest. By \"regionalization\" we mean a specification of\ngeographies that define the spatial support for areal data. This topic has been\nstudied vigorously by geographers, but has been given less attention by spatial\nstatisticians. Thus, we propose a criterion for spatial aggregation error\n(CAGE), which we minimize to obtain an optimal regionalization. To define CAGE\nwe draw a connection between spatial aggregation error and a new multiscale\nrepresentation of the Karhunen-Loeve (K-L) expansion. This relationship between\nCAGE and the multiscale K-L expansion leads to illuminating theoretical\ndevelopments including: connections between spatial aggregation error, squared\nprediction error, spatial variance, and a novel extension of Obled-Creutin\neigenfunctions. The effectiveness of our approach is demonstrated through an\nanalysis of two datasets, one using the American Community Survey and one\nrelated to environmental ocean winds. \n\n"}
{"id": "1502.01975", "contents": "Title: Optimal Haplotype Assembly from High-Throughput Mate-Pair Reads Abstract: Humans have $23$ pairs of homologous chromosomes. The homologous pairs are\nalmost identical pairs of chromosomes. For the most part, differences in\nhomologous chromosome occur at certain documented positions called single\nnucleotide polymorphisms (SNPs). A haplotype of an individual is the pair of\nsequences of SNPs on the two homologous chromosomes. In this paper, we study\nthe problem of inferring haplotypes of individuals from mate-pair reads of\ntheir genome. We give a simple formula for the coverage needed for haplotype\nassembly, under a generative model. The analysis here leverages connections of\nthis problem with decoding convolutional codes. \n\n"}
{"id": "1502.02347", "contents": "Title: Local and Global Inference for High Dimensional Nonparanormal Graphical\n  Models Abstract: This paper proposes a unified framework to quantify local and global\ninferential uncertainty for high dimensional nonparanormal graphical models. In\nparticular, we consider the problems of testing the presence of a single edge\nand constructing a uniform confidence subgraph. Due to the presence of unknown\nmarginal transformations, we propose a pseudo likelihood based inferential\napproach. In sharp contrast to the existing high dimensional score test method,\nour method is free of tuning parameters given an initial estimator, and extends\nthe scope of the existing likelihood based inferential framework. Furthermore,\nwe propose a U-statistic multiplier bootstrap method to construct the\nconfidence subgraph. We show that the constructed subgraph is contained in the\ntrue graph with probability greater than a given nominal level. Compared with\nexisting methods for constructing confidence subgraphs, our method does not\nrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of the\nproposed inferential methods are verified by thorough numerical experiments and\nreal data analysis. \n\n"}
{"id": "1502.03391", "contents": "Title: Fast Embedding for JOFC Using the Raw Stress Criterion Abstract: The Joint Optimization of Fidelity and Commensurability (JOFC) manifold\nmatching methodology embeds an omnibus dissimilarity matrix consisting of\nmultiple dissimilarities on the same set of objects. One approach to this\nembedding optimizes the preservation of fidelity to each individual\ndissimilarity matrix together with commensurability of each given observation\nacross modalities via iterative majorization of a raw stress error criterion by\nsuccessive Guttman transforms. In this paper, we exploit the special structure\ninherent to JOFC to exactly and efficiently compute the successive Guttman\ntransforms, and as a result we are able to greatly speed up the JOFC procedure\nfor both in-sample and out-of-sample embedding. We demonstrate the scalability\nof our implementation on both real and simulated data examples. \n\n"}
{"id": "1502.06197", "contents": "Title: On Online Control of False Discovery Rate Abstract: Multiple hypotheses testing is a core problem in statistical inference and\narises in almost every scientific field. Given a sequence of null hypotheses\n$\\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg\n\\cite{benjamini1995controlling} introduced the false discovery rate (FDR)\ncriterion, which is the expected proportion of false positives among rejected\nnull hypotheses, and proposed a testing procedure that controls FDR below a\npre-assigned significance level. They also proposed a different criterion,\ncalled mFDR, which does not control a property of the realized set of tests;\nrather it controls the ratio of expected number of false discoveries to the\nexpected number of discoveries.\n  In this paper, we propose two procedures for multiple hypotheses testing that\nwe will call \"LOND\" and \"LORD\". These procedures control FDR and mFDR in an\n\\emph{online manner}. Concretely, we consider an ordered --possibly infinite--\nsequence of null hypotheses $\\mathcal{H} = (H_1,H_2,H_3,...)$ where, at each\nstep $i$, the statistician must decide whether to reject hypothesis $H_i$\nhaving access only to the previous decisions. To the best of our knowledge, our\nwork is the first that controls FDR in this setting. This model was introduced\nby Foster and Stine \\cite{alpha-investing} whose alpha-investing rule only\ncontrols mFDR in online manner.\n  In order to compare different procedures, we develop lower bounds on the\ntotal discovery rate under the mixture model and prove that both LOND and LORD\nhave nearly linear number of discoveries. We further propose adjustment to LOND\nto address arbitrary correlation among the $p$-values. Finally, we evaluate the\nperformance of our procedures on both synthetic and real data comparing them\nwith alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure. \n\n"}
{"id": "1502.06930", "contents": "Title: Tensor decomposition with generalized lasso penalties Abstract: We present an approach for penalized tensor decomposition (PTD) that\nestimates smoothly varying latent factors in multi-way data. This generalizes\nexisting work on sparse tensor decomposition and penalized matrix\ndecompositions, in a manner parallel to the generalized lasso for regression\nand smoothing problems. Our approach presents many nontrivial challenges at the\nintersection of modeling and computation, which are studied in detail. An\nefficient coordinate-wise optimization algorithm for (PTD) is presented, and\nits convergence properties are characterized. The method is applied both to\nsimulated data and real data on flu hospitalizations in Texas. These results\nshow that our penalized tensor decomposition can offer major improvements on\nexisting methods for analyzing multi-way data that exhibit smooth spatial or\ntemporal features. \n\n"}
{"id": "1502.07963", "contents": "Title: Confidence Intervals for Maximin Effects in Inhomogeneous Large-Scale\n  Data Abstract: One challenge of large-scale data analysis is that the assumption of an\nidentical distribution for all samples is often not realistic. An optimal\nlinear regression might, for example, be markedly different for distinct groups\nof the data. Maximin effects have been proposed as a computationally attractive\nway to estimate effects that are common across all data without fitting a\nmixture distribution explicitly. So far just point estimators of the common\nmaximin effects have been proposed in Meinshausen and B\\\"uhlmann (2014). Here\nwe propose asymptotically valid confidence regions for these effects. \n\n"}
{"id": "1503.00339", "contents": "Title: Variation of word frequencies in Russian literary texts Abstract: We study the variation of word frequencies in Russian literary texts. Our\nfindings indicate that the standard deviation of a word's frequency across\ntexts depends on its average frequency according to a power law with exponent\n$0.62,$ showing that the rarer words have a relatively larger degree of\nfrequency volatility (i.e., \"burstiness\").\n  Several latent factors models have been estimated to investigate the\nstructure of the word frequency distribution. The dependence of a word's\nfrequency volatility on its average frequency can be explained by the asymmetry\nin the distribution of latent factors. \n\n"}
{"id": "1503.00445", "contents": "Title: Detecting communities using asymptotical Surprise Abstract: Nodes in real-world networks are repeatedly observed to form dense clusters,\noften referred to as communities. Methods to detect these groups of nodes\nusually maximize an objective function, which implicitly contains the\ndefinition of a community. We here analyze a recently proposed measure called\nsurprise, which assesses the quality of the partition of a network into\ncommunities. In its current form, the formulation of surprise is rather\ndifficult to analyze. We here therefore develop an accurate asymptotic\napproximation. This allows for the development of an efficient algorithm for\noptimizing surprise. Incidentally, this leads to a straightforward extension of\nsurprise to weighted graphs. Additionally, the approximation makes it possible\nto analyze surprise more closely and compare it to other methods, especially\nmodularity. We show that surprise is (nearly) unaffected by the well known\nresolution limit, a particular problem for modularity. However, surprise may\ntend to overestimate the number of communities, whereas they may be\nunderestimated by modularity. In short, surprise works well in the limit of\nmany small communities, whereas modularity works better in the limit of few\nlarge communities. In this sense, surprise is more discriminative than\nmodularity, and may find communities where modularity fails to discern any\nstructure. \n\n"}
{"id": "1503.00466", "contents": "Title: Spectral estimation for diffusions with random sampling times Abstract: The nonparametric estimation of the volatility and the drift coefficient of a\nscalar diffusion is studied when the process is observed at random time points.\nThe constructed estimator generalizes the spectral method by Gobet, Hoffmann\nand Rei{\\ss} [Ann. Statist. 32 (2006), 2223-2253]. The estimation procedure is\noptimal in the minimax sense and adaptive with respect to the sampling time\ndistribution and the regularity of the coefficients. The proofs are based on\nthe eigenvalue problem for the generalized transition operator. The finite\nsample performance is illustrated in a numerical example. \n\n"}
{"id": "1503.01271", "contents": "Title: Performance analysis of an improved MUSIC DoA estimator Abstract: This paper adresses the statistical performance of subspace DoA estimation\nusing a sensor array, in the asymptotic regime where the number of samples and\nsensors both converge to infinity at the same rate. Improved subspace DoA\nestimators were derived (termed as G-MUSIC) in previous works, and were shown\nto be consistent and asymptotically Gaussian distributed in the case where the\nnumber of sources and their DoA remain fixed. In this case, which models widely\nspaced DoA scenarios, it is proved in the present paper that the traditional\nMUSIC method also provides DoA consistent estimates having the same asymptotic\nvariances as the G-MUSIC estimates. The case of DoA that are spaced of the\norder of a beamwidth, which models closely spaced sources, is also considered.\nIt is shown that G-MUSIC estimates are still able to consistently separate the\nsources, while it is no longer the case for the MUSIC ones. The asymptotic\nvariances of G-MUSIC estimates are also evaluated. \n\n"}
{"id": "1503.02059", "contents": "Title: Clustering strategy and method selection Abstract: This paper is a chapter in the forthcoming Handbook of Cluster Analysis,\nHennig et al. (2015). For definitions of basic clustering methods and some\nfurther methodology, other chapters of the Handbook are referred to. To read\nthis version of the paper without the Handbook, some knowledge of cluster\nanalysis methodology is required.\n  The aim of this chapter is to provide a framework for all the decisions that\nare required when carrying out a cluster analysis in practice. A general\nattitude to clustering is outlined, which connects these decisions closely to\nthe clustering aims in a given application. From this point of view, the\nchapter then discusses aspects of data processing such as the choice of the\nrepresentation of the objects to be clustered, dissimilarity design,\ntransformation and standardization of variables. Regarding the choice of the\nclustering method, it is explored how different methods correspond to different\nclustering aims. Then an overview of benchmarking studies comparing different\nclustering methods is given, as well as an out- line of theoretical approaches\nto characterize desiderata for clustering by axioms. Finally, aspects of\ncluster validation, i.e., the assessment of the quality of a clustering in a\ngiven dataset, are discussed, including finding an appropriate number of\nclusters, testing homogeneity, internal and external cluster validation,\nassessing clustering stability and data visualization. \n\n"}
{"id": "1503.03355", "contents": "Title: Automatic Unsupervised Tensor Mining with Quality Assessment Abstract: A popular tool for unsupervised modelling and mining multi-aspect data is\ntensor decomposition. In an exploratory setting, where and no labels or ground\ntruth are available how can we automatically decide how many components to\nextract? How can we assess the quality of our results, so that a domain expert\ncan factor this quality measure in the interpretation of our results? In this\npaper, we introduce AutoTen, a novel automatic unsupervised tensor mining\nalgorithm with minimal user intervention, which leverages and improves upon\nheuristics that assess the result quality. We extensively evaluate AutoTen's\nperformance on synthetic data, outperforming existing baselines on this very\nhard problem. Finally, we apply AutoTen on a variety of real datasets,\nproviding insights and discoveries. We view this work as a step towards a fully\nautomated, unsupervised tensor mining tool that can be easily adopted by\npractitioners in academia and industry. \n\n"}
{"id": "1503.06239", "contents": "Title: Block-Wise MAP Inference for Determinantal Point Processes with\n  Application to Change-Point Detection Abstract: Existing MAP inference algorithms for determinantal point processes (DPPs)\nneed to calculate determinants or conduct eigenvalue decomposition generally at\nthe scale of the full kernel, which presents a great challenge for real-world\napplications. In this paper, we introduce a class of DPPs, called BwDPPs, that\nare characterized by an almost block diagonal kernel matrix and thus can allow\nefficient block-wise MAP inference. Furthermore, BwDPPs are successfully\napplied to address the difficulty of selecting change-points in the problem of\nchange-point detection (CPD), which results in a new BwDPP-based CPD method,\nnamed BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is\nfirst created based on existing well-studied metrics. Then, these change-point\ncandidates are treated as DPP items, and DPP-based subset selection is\nconducted to give the final estimate of the change-points that favours both\nquality and diversity. The effectiveness of BwDppCpd is demonstrated through\nextensive experiments on five real-world datasets. \n\n"}
{"id": "1503.06575", "contents": "Title: Unveiling Spatial Epidemiology of HIV with Mobile Phone Data Abstract: An increasing amount of geo-referenced mobile phone data enables the\nidentification of behavioral patterns, habits and movements of people. With\nthis data, we can extract the knowledge potentially useful for many\napplications including the one tackled in this study - understanding spatial\nvariation of epidemics. We explored the datasets collected by a cell phone\nservice provider and linked them to spatial HIV prevalence rates estimated from\npublicly available surveys. For that purpose, 224 features were extracted from\nmobility and connectivity traces and related to the level of HIV epidemic in 50\nIvory Coast departments. By means of regression models, we evaluated predictive\nability of extracted features. Several models predicted HIV prevalence that are\nhighly correlated (>0.7) with actual values. Through contribution analysis we\nidentified key elements that impact the rate of infections. Our findings\nindicate that night connectivity and activity, spatial area covered by users\nand overall migrations are strongly linked to HIV. By visualizing the\ncommunication and mobility flows, we strived to explain the spatial structure\nof epidemics. We discovered that strong ties and hubs in communication and\nmobility align with HIV hot spots. \n\n"}
{"id": "1503.07591", "contents": "Title: Convex Optimization approach to signals with fast varying instantaneous\n  frequency Abstract: Motivated by the limitation of analyzing oscillatory signals composed of\nmultiple components with fast-varying instantaneous frequency, we approach the\ntime-frequency analysis problem by optimization. Based on the proposed adaptive\nharmonic model, the time-frequency representation of a signal is obtained by\ndirectly minimizing a functional, which involves few properties an \"ideal\ntime-frequency representation\" should satisfy, for example, the signal\nreconstruction and concentrative time frequency representation. FISTA (Fast\nIterative Shrinkage-Thresholding Algorithm) is applied to achieve an efficient\nnumerical approximation of the functional. We coin the algorithm as {\\it\nTime-frequency bY COnvex OptimizatioN} (Tycoon). The numerical results confirm\nthe potential of the Tycoon algorithm. \n\n"}
{"id": "1503.08650", "contents": "Title: Comparison of Bayesian predictive methods for model selection Abstract: The goal of this paper is to compare several widely used Bayesian model\nselection methods in practical model selection problems, highlight their\ndifferences and give recommendations about the preferred approaches. We focus\non the variable subset selection for regression and classification and perform\nseveral numerical experiments using both simulated and real world data. The\nresults show that the optimization of a utility estimate such as the\ncross-validation (CV) score is liable to finding overfitted models due to\nrelatively high variance in the utility estimates when the data is scarce. This\ncan also lead to substantial selection induced bias and optimism in the\nperformance evaluation for the selected model. From a predictive viewpoint,\nbest results are obtained by accounting for model uncertainty by forming the\nfull encompassing model, such as the Bayesian model averaging solution over the\ncandidate models. If the encompassing model is too complex, it can be robustly\nsimplified by the projection method, in which the information of the full model\nis projected onto the submodels. This approach is substantially less prone to\noverfitting than selection based on CV-score. Overall, the projection method\nappears to outperform also the maximum a posteriori model and the selection of\nthe most probable variables. The study also demonstrates that the model\nselection can greatly benefit from using cross-validation outside the searching\nprocess both for guiding the model size selection and assessing the predictive\nperformance of the finally selected model. \n\n"}
{"id": "1504.02661", "contents": "Title: Stochastic determination of matrix determinants Abstract: Matrix determinants play an important role in data analysis, in particular\nwhen Gaussian processes are involved. Due to currently exploding data volumes,\nlinear operations - matrices - acting on the data are often not accessible\ndirectly but are only represented indirectly in form of a computer routine.\nSuch a routine implements the transformation a data vector undergoes under\nmatrix multiplication. While efficient probing routines to estimate a matrix's\ndiagonal or trace, based solely on such computationally affordable\nmatrix-vector multiplications, are well known and frequently used in signal\ninference, there is no stochastic estimate for its determinant. We introduce a\nprobing method for the logarithm of a determinant of a linear operator. Our\nmethod rests upon a reformulation of the log-determinant by an integral\nrepresentation and the transformation of the involved terms into stochastic\nexpressions. This stochastic determinant determination enables large-size\napplications in Bayesian inference, in particular evidence calculations, model\ncomparison, and posterior determination. \n\n"}
{"id": "1504.02913", "contents": "Title: A pairwise likelihood approach to simultaneous clustering and\n  dimensional reduction of ordinal data Abstract: The literature on clustering for continuous data is rich and wide;\ndifferently, that one developed for categorical data is still limited. In some\ncases, the problem is made more difficult by the presence of noise\nvariables/dimensions that do not contain information about the clustering\nstructure and could mask it. The aim of this paper is to propose a model for\nsimultaneous clustering and dimensionality reduction of ordered categorical\ndata able to detect the discriminative dimensions discarding the noise ones.\nFollowing the underlying response variable approach, the observed variables are\nconsidered as a discretization of underlying first-order latent continuous\nvariables distributed as a Gaussian mixture. To recognize discriminative and\nnoise dimensions, these variables are considered to be linear combinations of\ntwo independent sets of second-order latent variables where only one contains\nthe information about the cluster structure while the other contains noise\ndimensions. The model specification involves multidimensional integrals that\nmake the maximum likelihood estimation cumbersome and in some cases infeasible.\nTo overcome this issue the parameter estimation is carried out through an\nEM-like algorithm maximizing a pairwise log-likelihood. Examples of application\nof the model on real and simulated data are performed to show the effectiveness\nof the proposal. \n\n"}
{"id": "1504.04354", "contents": "Title: The Long Memory of Order Flow in the Foreign Exchange Spot Market Abstract: We study the long memory of order flow for each of three liquid currency\npairs on a large electronic trading platform in the foreign exchange (FX) spot\nmarket. Due to the extremely high levels of market activity on the platform,\nand in contrast to existing empirical studies of other markets, our data\nenables us to perform statistically stable estimation without needing to\naggregate data from different trading days. We find strong evidence of long\nmemory, with a Hurst exponent of approximately 0.7, for each of the three\ncurrency pairs and on each trading day in our sample. We repeat our\ncalculations using data that spans different trading days, and we find no\nsignificant differences in our results. We test and reject the hypothesis that\nthe apparent long memory of order flow is an artifact caused by structural\nbreaks, in favour of the alternative hypothesis of true long memory. We\ntherefore conclude that the long memory of order flow in the FX spot market is\na robust empirical property that persists across daily boundaries. \n\n"}
{"id": "1504.04531", "contents": "Title: Hyperspectral pansharpening: a review Abstract: Pansharpening aims at fusing a panchromatic image with a multispectral one,\nto generate an image with the high spatial resolution of the former and the\nhigh spectral resolution of the latter. In the last decade, many algorithms\nhave been presented in the literature for pansharpening using multispectral\ndata. With the increasing availability of hyperspectral systems, these methods\nare now being adapted to hyperspectral images. In this work, we compare new\npansharpening techniques designed for hyperspectral data with some of the state\nof the art methods for multispectral pansharpening, which have been adapted for\nhyperspectral data. Eleven methods from different classes (component\nsubstitution, multiresolution analysis, hybrid, Bayesian and matrix\nfactorization) are analyzed. These methods are applied to three datasets and\ntheir effectiveness and robustness are evaluated with widely used performance\nindicators. In addition, all the pansharpening techniques considered in this\npaper have been implemented in a MATLAB toolbox that is made available to the\ncommunity. \n\n"}
{"id": "1504.05436", "contents": "Title: Estimating the Expected Value of Partial Perfect Information in Health\n  Economic Evaluations using Integrated Nested Laplace Approximation Abstract: The Expected Value of Perfect Partial Information (EVPPI) is a\ndecision-theoretic measure of the \"cost\" of parametric uncertainty in decision\nmaking used principally in health economic decision making. Despite this\ndecision-theoretic grounding, the uptake of EVPPI calculations in practice has\nbeen slow. This is in part due to the prohibitive computational time required\nto estimate the EVPPI via Monte Carlo simulations. However, recent developments\nhave demonstrated that the EVPPI can be estimated by non-parametric regression\nmethods, which have significantly decreased the computation time required to\napproximate the EVPPI. Under certain circumstances, high-dimensional Gaussian\nProcess regression is suggested, but this can still be prohibitively expensive.\nApplying fast computation methods developed in spatial statistics using\nIntegrated Nested Laplace Approximations (INLA) and projecting from a\nhigh-dimensional into a low-dimensional input space allows us to decrease the\ncomputation time for fitting these high-dimensional Gaussian Processes, often\nsubstantially. We demonstrate that the EVPPI calculated using our method for\nGaussian Process regression is in line with the standard Gaussian Process\nregression method and that despite the apparent methodological complexity of\nthis new method, R functions are available in the package BCEA to implement it\nsimply and efficiently. \n\n"}
{"id": "1504.05714", "contents": "Title: Estimation of Zero Intelligence Models by L1 Data Abstract: A unit volume zero intelligence (ZI) model is defined and the distribution of\nits L1 process is recursively described. Further, a generalized ZI (GZI) model\nallowing non-unit market orders, shifts of quotes and general in-spread events\nis proposed and a formula for the conditional distribution of its quotes is\ngiven, together with a formula for price impact. For both the models, MLE\nestimators are formulated and shown to be consistent and asymptotically normal.\nConsequently, the estimators are applied to data of six US stocks from nine\nelectronic markets. It is found that more complex variants of the models,\ndespite being significant, do not give considerably better predictions than\ntheir simple versions with constant intensities. \n\n"}
{"id": "1504.05837", "contents": "Title: New Perspectives on Multiple Source Localization in Wireless Sensor\n  Networks Abstract: In this paper we address the challenging problem of multiple source\nlocalization in Wireless Sensor Networks (WSN). We develop an efficient\nstatistical algorithm, based on the novel application of Sequential Monte Carlo\n(SMC) sampler methodology, that is able to deal with an unknown number of\nsources given quantized data obtained at the fusion center from different\nsensors with imperfect wireless channels. We also derive the Posterior\nCram\\'er-Rao Bound (PCRB) of the source location estimate. The PCRB is used to\nanalyze the accuracy of the proposed SMC sampler algorithm and the impact that\nquantization has on the accuracy of location estimates of the sources.\nExtensive experiments show that the benefits of the proposed scheme in terms of\nthe accuracy of the estimation method that are required for model selection\n(i.e., the number of sources) and the estimation of the source characteristics\ncompared to the classical importance sampling method. \n\n"}
{"id": "1504.08218", "contents": "Title: Relax, Tensors Are Here: Dependencies in International Processes Abstract: Previous models of international conflict have suffered two shortfalls. They\ntended not to embody dynamic changes, focusing rather on static slices of\nbehavior over time. These models have also been empirically evaluated in ways\nthat assumed the independence of each country, when in reality they are\nsearching for the interdependence among all countries. We illustrate a solution\nto these two hurdles and evaluate this new, dynamic, network based approach to\nthe dependencies among the ebb and flow of daily international interactions\nusing a newly developed, and openly available, database of events among\nnations. \n\n"}
{"id": "1505.04008", "contents": "Title: Delete or merge regressors for linear model selection Abstract: We consider a problem of linear model selection in the presence of both\ncontinuous and categorical predictors. Feasible models consist of subsets of\nnumerical variables and partitions of levels of factors. A new algorithm called\ndelete or merge regressors (DMR) is presented which is a stepwise backward\nprocedure involving ranking the predictors according to squared t-statistics\nand choosing the final model minimizing BIC. In the article we prove\nconsistency of DMR when the number of predictors tends to infinity with the\nsample size and describe a simulation study using a pertaining R package. The\nresults indicate significant advantage in time complexity and selection\naccuracy of our algorithm over Lasso-based methods described in the literature.\nMoreover, a version of DMR for generalized linear models is proposed. \n\n"}
{"id": "1505.04305", "contents": "Title: Detecting structural breaks in seasonal time series by regularized\n  optimization Abstract: Real-world systems are often complex, dynamic, and nonlinear. Understanding\nthe dynamics of a system from its observed time series is key to the prediction\nand control of the system's behavior. While most existing techniques tacitly\nassume some form of stationarity or continuity, abrupt changes, which are often\ndue to external disturbances or sudden changes in the intrinsic dynamics, are\ncommon in time series. Structural breaks, which are time points at which the\nstatistical patterns of a time series change, pose considerable challenges to\ndata analysis. Without identification of such break points, the same dynamic\nrule would be applied to the whole period of observation, whereas false\nidentification of structural breaks may lead to overfitting. In this paper, we\ncast the problem of decomposing a time series into its trend and seasonal\ncomponents as an optimization problem. This problem is ill-posed due to the\narbitrariness in the number of parameters. To overcome this difficulty, we\npropose the addition of a penalty function (i.e., a regularization term) that\naccounts for the number of parameters. Our approach simultaneously identifies\nseasonality and trend without the need of iterations, and allows the reliable\ndetection of structural breaks. The method is applied to recorded data on fish\npopulations and sea surface temperature, where it detects structural breaks\nthat would have been neglected otherwise. This suggests that our method can\nlead to a general approach for the monitoring, prediction, and prevention of\nstructural changes in real systems. \n\n"}
{"id": "1505.06954", "contents": "Title: A Geometric Approach to Pairwise Bayesian Alignment of Functional Data\n  Using Importance Sampling Abstract: We present a Bayesian model for pairwise nonlinear registration of functional\ndata. We use the Riemannian geometry of the space of warping functions to\ndefine appropriate prior distributions and sample from the posterior using\nimportance sampling. A simple square-root transformation is used to simplify\nthe geometry of the space of warping functions, which allows for computation of\nsample statistics, such as the mean and median, and a fast implementation of a\n$k$-means clustering algorithm. These tools allow for efficient posterior\ninference, where multiple modes of the posterior distribution corresponding to\nmultiple plausible alignments of the given functions are found. We also show\npointwise $95\\%$ credible intervals to assess the uncertainty of the alignment\nin different clusters. We validate this model using simulations and present\nmultiple examples on real data from different application domains including\nbiometrics and medicine. \n\n"}
{"id": "1505.07752", "contents": "Title: The Impact of Estimation: A New Method for Clustering and Trajectory\n  Estimation in Patient Flow Modeling Abstract: The ability to accurately forecast and control inpatient census, and thereby\nworkloads, is a critical and longstanding problem in hospital management.\nMajority of current literature focuses on optimal scheduling of inpatients, but\nlargely ignores the process of accurate estimation of the trajectory of\npatients throughout the treatment and recovery process. The result is that\ncurrent scheduling models are optimizing based on inaccurate input data. We\ndeveloped a Clustering and Scheduling Integrated (CSI) approach to capture\npatient flows through a network of hospital services. CSI functions by\nclustering patients into groups based on similarity of trajectory using a novel\nSemi-Markov model (SMM)-based clustering scheme proposed in this paper, as\nopposed to clustering by admit type or condition as in previous literature. The\nmethodology is validated by simulation and then applied to real patient data\nfrom a partner hospital where we see it outperforms current methods. Further,\nwe demonstrate that extant optimization methods achieve significantly better\nresults on key hospital performance measures under CSI, compared with\ntraditional estimation approaches, increasing elective admissions by 97% and\nutilization by 22% compared to 30% and 8% using traditional estimation\ntechniques. From a theoretical standpoint, the SMM-clustering is a novel\napproach applicable to any temporal-spatial stochastic data that is prevalent\nin many industries and application areas. \n\n"}
{"id": "1506.00185", "contents": "Title: Bayesian semiparametric power spectral density estimation with\n  applications in gravitational wave data analysis Abstract: The standard noise model in gravitational wave (GW) data analysis assumes\ndetector noise is stationary and Gaussian distributed, with a known power\nspectral density (PSD) that is usually estimated using clean off-source data.\nReal GW data often depart from these assumptions, and misspecified parametric\nmodels of the PSD could result in misleading inferences. We propose a Bayesian\nsemiparametric approach to improve this. We use a nonparametric Bernstein\npolynomial prior on the PSD, with weights attained via a Dirichlet process\ndistribution, and update this using the Whittle likelihood. Posterior samples\nare obtained using a blocked Metropolis-within-Gibbs sampler. We simultaneously\nestimate the reconstruction parameters of a rotating core collapse supernova GW\nburst that has been embedded in simulated Advanced LIGO noise. We also discuss\nan approach to deal with non-stationary data by breaking longer data streams\ninto smaller and locally stationary components. \n\n"}
{"id": "1506.02824", "contents": "Title: The performance and efficiency of Threshold Blocking Abstract: A common method to reduce the uncertainty of causal inferences from\nexperiments is to assign treatments in fixed proportions within groups of\nsimilar units: blocking. Previous results indicate that one can expect\nsubstantial reductions in variance if these groups are formed so to contain\nexactly as many units as treatment conditions. This approach can be contrasted\nto threshold blocking which, instead of specifying a fixed size, requires that\nthe groups contain a minimum number of units. In this paper, I investigate the\nadvantages of respective method. In particular, I show that threshold blocking\nis superior to fixed-sized blocking in the sense that it always finds a weakly\nbetter grouping for any objective and sample. However, this does not\nnecessarily hold when the objective function of the blocking problem is\nunknown, and a fixed-sized design can perform better in that case. I\nspecifically examine the factors that govern how the methods perform in the\ncommon situation where the objective is to reduce the estimator's variance, but\nwhere groups are constructed based on covariates. This reveals that the\nrelative performance of threshold blocking improves when the covariates become\nmore predictive of the outcome. \n\n"}
{"id": "1506.03920", "contents": "Title: A vine copula mixed effect model for trivariate meta-analysis of\n  diagnostic test accuracy studies accounting for disease prevalence Abstract: A bivariate copula mixed model has been recently proposed to synthesize\ndiagnostic test accuracy studies and it has been shown that is superior to the\nstandard generalized linear mixed model (GLMM) in this context. Here we call\ntrivariate vine copulas to extend the bivariate meta-analysis of diagnostic\ntest accuracy studies by accounting for disease prevalence. Our vine copula\nmixed model includes the trivariate GLMM as a special case and can also operate\non the original scale of sensitivity, specificity, and disease prevalence. Our\ngeneral methodology is illustrated by re-analysing the data of two published\nmeta-analyses. Our study suggests that there can be an improvement on\ntrivariate GLMM in fit to data and makes the argument for moving to vine copula\nrandom effects models especially because of their richness including reflection\nasymmetric tail dependence, and, computational feasibility despite their\nthree-dimensionality. \n\n"}
{"id": "1506.04125", "contents": "Title: A risk management approach to capital allocation Abstract: The European insurance sector will soon be faced with the application of\nSolvency 2 regulation norms. It will create a real change in risk management\npractices. The ORSA approach of the second pillar makes the capital allocation\nan important exercise for all insurers and specially for groups. Considering\nmulti-branches firms, capital allocation has to be based on a multivariate risk\nmodeling. Several allocation methods are present in the literature and insurers\npractices. In this paper, we present a new risk allocation method, we study its\ncoherence using an axiomatic approach, and we try to define what the best\nallocation choice for an insurance group is. \n\n"}
{"id": "1506.04430", "contents": "Title: Exact simulation of max-stable processes Abstract: Max-stable processes play an important role as models for spatial extreme\nevents. Their complex structure as the pointwise maximum over an infinite\nnumber of random functions makes simulation highly nontrivial. Algorithms based\non finite approximations that are used in practice are often not exact and\ncomputationally inefficient. We will present two algorithms for exact\nsimulation of a max-stable process at a finite number of locations. The first\nalgorithm generalizes the approach by \\citet{DM-2014} for Brown--Resnick\nprocesses and it is based on simulation from the spectral measure. The second\nalgorithm relies on the idea to simulate only the extremal functions, that is,\nthose functions in the construction of a max-stable process that effectively\ncontribute to the pointwise maximum. We study the complexity of both algorithms\nand prove that the second procedure is always more efficient. Moreover, we\nprovide closed expressions for their implementation that cover the most popular\nmodels for max-stable processes and extreme value copulas. For simulation on\ndense grids, an adaptive design of the second algorithm is proposed. \n\n"}
{"id": "1506.05661", "contents": "Title: Hierarchical networks of scientific journals Abstract: Scientific journals are the repositories of the gradually accumulating\nknowledge of mankind about the world surrounding us. Just as our knowledge is\norganised into classes ranging from major disciplines, subjects and fields to\nincreasingly specific topics, journals can also be categorised into groups\nusing various metrics. In addition to the set of topics characteristic for a\njournal, they can also be ranked regarding their relevance from the point of\noverall influence. One widespread measure is impact factor, but in the present\npaper we intend to reconstruct a much more detailed description by studying the\nhierarchical relations between the journals based on citation data. We use a\nmeasure related to the notion of m-reaching centrality and find a network which\nshows the level of influence of a journal from the point of the direction and\nefficiency with which information spreads through the network. We can also\nobtain an alternative network using a suitably modified nested hierarchy\nextraction method applied to the same data. The results are weakly\nmethodology-dependent and reveal non-trivial relations among journals. The two\nalternative hierarchies show large similarity with some striking differences,\nproviding together a complex picture of the intricate relations between\nscientific journals. \n\n"}
{"id": "1506.06169", "contents": "Title: A Model-Based Approach for Analog Spatio-Temporal Dynamic Forecasting Abstract: Analog forecasting has been applied in a variety of fields for predicting\nfuture states of complex nonlinear systems that require flexible forecasting\nmethods. Past analog methods have almost exclu- sively been used in an\nempirical framework without the structure of a model-based approach. We propose\na Bayesian model framework for analog forecasting, building upon previous\nanalog methods but accounting for parameter uncertainty. Thus, unlike\ntraditional analog forecasting methods, the use of Bayesian modeling allows one\nto rigorously quantify uncertainty to obtain realistic posterior predictive\ndistributions. The model is applied to the long-lead time forecasting of\nmid-May averaged soil moisture anomalies in Iowa over a high-resolution grid of\nspatial locations. Sea Surface Tem- perature (SST) is used to find past time\nperiods with similar trajectories to the current pre-forecast period. The\nanalog model is developed on projection coefficients from a basis expansion of\nthe soil moisture and SST fields. Separate models are constructed for locations\nfalling in each Iowa Crop Reporting District (CRD) and the forecasting ability\nof the proposed model is compared against a variety of alternative methods and\nmetrics. \n\n"}
{"id": "1506.06669", "contents": "Title: Understanding the Impact of Microcredit Expansions: A Bayesian\n  Hierarchical Analysis of 7 Randomised Experiments Abstract: Bayesian hierarchical models are a methodology for aggregation and synthesis\nof data from heterogeneous settings, used widely in statistics and other\ndisciplines. I apply this framework to the evidence from 7 randomized\nexperiments of expanding access to microcredit to assess the general impact of\nthe intervention on household outcomes and the heterogeneity in this impact\nacross sites. The results suggest that the effect of microcredit is likely to\nbe positive but small relative to control group average levels, and the\npossibility of a negative impact cannot be ruled out. By contrast, common\nmeta-analytic methods that pool all the data without assessing the\nheterogeneity misleadingly produce \"statistically significant\" results in 2 of\nthe 6 household outcomes. Standard pooling metrics for the studies indicate on\naverage 60% pooling on the treatment effects, suggesting that the site-specific\neffects are reasonably externally valid, and thus informative for each other\nand for the general case. The cross-study heterogeneity is almost entirely\ngenerated by heterogeneous effects for the 27% households who previously\noperated businesses before microcredit expansion, although this group is likely\nto see much larger impacts overall. A Ridge regression procedure to assess the\ncorrelations between site-specific covariates and treatment effects indicates\nthat the remaining heterogeneity is strongly correlated with differences in\neconomic variables, but not with differences in study design protocols. The\naverage interest rate and the average loan size have the strongest correlation\nwith the treatment effects, and both are negative. \n\n"}
{"id": "1506.08253", "contents": "Title: Bayesian Inference for Latent Biologic Structure with Determinantal\n  Point Processes (DPP) Abstract: We discuss the use of the determinantal point process (DPP) as a prior for\nlatent structure in biomedical applications, where inference often centers on\nthe interpretation of latent features as biologically or clinically meaningful\nstructure. Typical examples include mixture models, when the terms of the\nmixture are meant to represent clinically meaningful subpopulations (of\npatients, genes, etc.). Another class of examples are feature allocation\nmodels. We propose the DPP prior as a repulsive prior on latent mixture\ncomponents in the first example, and as prior on feature-specific parameters in\nthe second case. We argue that the DPP is in general an attractive prior model\nfor latent structure when biologically relevant interpretation of such\nstructure is desired. We illustrate the advantages of DPP prior in three case\nstudies, including inference in mixture models for magnetic resonance images\n(MRI) and for protein expression, and a feature allocation model for gene\nexpression using data from The Cancer Genome Atlas. An important part of our\nargument are efficient and straightforward posterior simulation methods. We\nimplement a variation of reversible jump Markov chain Monte Carlo simulation\nfor inference under the DPP prior, using a density with respect to the unit\nrate Poisson process. \n\n"}
{"id": "1506.08640", "contents": "Title: Leave Pima Indians alone: binary regression as a benchmark for Bayesian\n  computation Abstract: Abstract. Whenever a new approach to perform Bayesian computation is\nintroduced, a common practice is to showcase this approach on a binary\nregression model and datasets of moderate size. This paper discusses to which\nextent this practice is sound. It also reviews the current state of the art of\nBayesian computation, using binary regression as a running example. Both\nsampling-based algorithms (importance sampling, MCMC and SMC) and fast\napproximations (Laplace and EP) are covered. Extensive numerical results are\nprovided, some of which might go against conventional wisdom regarding the\neffectiveness of certain algorithms. Implications for other problems (variable\nselection) and other models are also discussed. \n\n"}
{"id": "1507.00171", "contents": "Title: The Statistical Performance of Collaborative Inference Abstract: The statistical analysis of massive and complex data sets will require the\ndevelopment of algorithms that depend on distributed computing and\ncollaborative inference. Inspired by this, we propose a collaborative framework\nthat aims to estimate the unknown mean $\\theta$ of a random variable $X$. In\nthe model we present, a certain number of calculation units, distributed across\na communication network represented by a graph, participate in the estimation\nof $\\theta$ by sequentially receiving independent data from $X$ while\nexchanging messages via a stochastic matrix $A$ defined over the graph. We give\nprecise conditions on the matrix $A$ under which the statistical precision of\nthe individual units is comparable to that of a (gold standard) virtual\ncentralized estimate, even though each unit does not have access to all of the\ndata. We show in particular the fundamental role played by both the non-trivial\neigenvalues of $A$ and the Ramanujan class of expander graphs, which provide\nremarkable performance for moderate algorithmic cost. \n\n"}
{"id": "1507.00433", "contents": "Title: Estimation of High-Dimensional Graphical Models Using Regularized Score\n  Matching Abstract: Graphical models are widely used to model stochastic dependences among large\ncollections of variables. We introduce a new method of estimating undirected\nconditional independence graphs based on the score matching loss, introduced by\nHyvarinen (2005), and subsequently extended in Hyvarinen (2007). The\nregularized score matching method we propose applies to settings with\ncontinuous observations and allows for computationally efficient treatment of\npossibly non-Gaussian exponential family models. In the well-explored Gaussian\nsetting, regularized score matching avoids issues of asymmetry that arise when\napplying the technique of neighborhood selection, and compared to existing\nmethods that directly yield symmetric estimates, the score matching approach\nhas the advantage that the considered loss is quadratic and gives piecewise\nlinear solution paths under $\\ell_1$ regularization. Under suitable\nirrepresentability conditions, we show that $\\ell_1$-regularized score matching\nis consistent for graph estimation in sparse high-dimensional settings. Through\nnumerical experiments and an application to RNAseq data, we confirm that\nregularized score matching achieves state-of-the-art performance in the\nGaussian case and provides a valuable tool for computationally efficient\nestimation in non-Gaussian graphical models. \n\n"}
{"id": "1507.00720", "contents": "Title: Correlated Random Measures Abstract: We develop correlated random measures, random measures where the atom weights\ncan exhibit a flexible pattern of dependence, and use them to develop powerful\nhierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric\nmodels are usually built from completely random measures, a Poisson-process\nbased construction in which the atom weights are independent. Completely random\nmeasures imply strong independence assumptions in the corresponding\nhierarchical model, and these assumptions are often misplaced in real-world\nsettings. Correlated random measures address this limitation. They model\ncorrelation within the measure by using a Gaussian process in concert with the\nPoisson process. With correlated random measures, for example, we can develop a\nlatent feature model for which we can infer both the properties of the latent\nfeatures and their dependency pattern. We develop several other examples as\nwell. We study a correlated random measure model of pairwise count data. We\nderive an efficient variational inference algorithm and show improved\npredictive performance on large data sets of documents, web clicks, and\nelectronic health records. \n\n"}
{"id": "1507.01242", "contents": "Title: Modeling for Dynamic Ordinal Regression Relationships: An Application to\n  Estimating Maturity of Rockfish in California Abstract: We develop a Bayesian nonparametric framework for modeling ordinal regression\nrelationships which evolve in discrete time. The motivating application\ninvolves a key problem in fisheries research on estimating dynamically evolving\nrelationships between age, length and maturity, the latter recorded on an\nordinal scale. The methodology builds from nonparametric mixture modeling for\nthe joint stochastic mechanism of covariates and latent continuous responses.\nThis approach yields highly flexible inference for ordinal regression functions\nwhile at the same time avoiding the computational challenges of parametric\nmodels. A novel dependent Dirichlet process prior for time-dependent mixing\ndistributions extends the model to the dynamic setting. The methodology is used\nfor a detailed study of relationships between maturity, age, and length for\nChilipepper rockfish, using data collected over 15 years along the coast of\nCalifornia. \n\n"}
{"id": "1507.01816", "contents": "Title: A Continuous-time Stochastic Block Model for Basketball Networks Abstract: For professional basketball, finding valuable and suitable players is the key\nto building a winning team. To deal with such challenges, basketball managers,\nscouts and coaches are increasingly turning to analytics. Objective evaluation\nof players and teams has always been the top goal of basketball analytics.\nTypical statistical analytics mainly focuses on the box score and has developed\nvarious metrics. In spite of the more and more advanced methods, metrics built\nupon box score statistics provide limited information about how players\ninteract with each other. Two players with similar box scores may deliver\ndistinct team plays. Thus professional basketball scouts have to watch real\ngames to evaluate players. Live scouting is effective, but suffers from\ninefficiency and subjectivity. In this paper, we go beyond the static box score\nand model basketball games as dynamic networks. The proposed Continuous-time\nStochastic Block Model clusters the players according to their playing style\nand performance. The model provides cluster-specific estimates of the\neffectiveness of players at scoring, rebounding, stealing, etc, and also\ncaptures player interaction patterns within and between clusters. By clustering\nsimilar players together, the model can help basketball scouts to narrow down\nthe search space. Moreover, the model is able to reveal the subtle differences\nin the offensive strategies of different teams. An application to NBA\nbasketball games illustrates the performance of the model. \n\n"}
{"id": "1507.02954", "contents": "Title: An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel\n  Estimation Abstract: In this work we design a receiver that iteratively passes soft information\nbetween the channel estimation and data decoding stages. The receiver\nincorporates sparsity-based parametric channel estimation. State-of-the-art\nsparsity-based iterative receivers simplify the channel estimation problem by\nrestricting the multipath delays to a grid. Our receiver does not impose such a\nrestriction. As a result it does not suffer from the leakage effect, which\ndestroys sparsity. Communication at near capacity rates in high SNR requires a\nlarge modulation order. Due to the close proximity of modulation symbols in\nsuch systems, the grid-based approximation is of insufficient accuracy. We show\nnumerically that a state-of-the-art iterative receiver with grid-based sparse\nchannel estimation exhibits a bit-error-rate floor in the high SNR regime. On\nthe contrary, our receiver performs very close to the perfect channel state\ninformation bound for all SNR values. We also demonstrate both theoretically\nand numerically that parametric channel estimation works well in dense\nchannels, i.e., when the number of multipath components is large and each\nindividual component cannot be resolved. \n\n"}
{"id": "1507.03401", "contents": "Title: An Evolutionary Spectrum Approach to Incorporate Large-scale\n  Geographical Descriptors on Global Processes Abstract: We introduce a nonstationary spatio-temporal statistical model for gridded\ndata on the sphere. The model specifies a computationally convenient covariance\nstructure that depends on heterogeneous geography. Widely used statistical\nmodels on a spherical domain are nonstationary for different latitudes, but\nstationary at the same latitude (axial symmetry). This assumption has been\nacknowledged to be too restrictive for quantities such as surface temperature,\nwhose statistical behavior is influenced by large scale geographical\ndescriptors such as land and ocean. We propose an evolutionary spectrum\napproach that is able to account for different regimes across the Earth's\ngeography, and results in a more general and flexible class of models that\nvastly outperforms axially symmetric models and captures longitudinal patterns\nthat would otherwise be assumed constant. The model can be estimated with in a\nmulti-step conditional likelihood approximation that preserves the\nnonstationary features while allowing for easily distributed computations: we\nshow how the fit of a data sets larger than 20 million data can be performed in\nless than one day on a state-of-the-art workstation. Once the parameters are\nestimated, it is possible to instantaneously generate surrogate runs from a\ncommon laptop. Further, the resulting estimates from the statistical model can\nbe regarded as a synthetic description (i.e. a compression) of the space-time\ncharacteristics of an entire initial condition ensemble. Compared to\ntraditional algorithms aiming at compressing the bit-by-bit information on each\nclimate model run, the proposed approach achieves vastly superior compression\nrates. \n\n"}
{"id": "1507.03538", "contents": "Title: Classifying X-ray Binaries: A Probabilistic Approach Abstract: In X-ray binary star systems consisting of a compact object that accretes\nmaterial from an orbiting secondary star, there is no straightforward means to\ndecide if the compact object is a black hole or a neutron star. To assist this\nclassification, we develop a Bayesian statistical model that makes use of the\nfact that X-ray binary systems appear to cluster based on their compact object\ntype when viewed from a 3-dimensional coordinate system derived from X-ray\nspectral data. The first coordinate of this data is the ratio of counts in mid\nto low energy band (color 1), the second coordinate is the ratio of counts in\nhigh to low energy band (color 2), and the third coordinate is the sum of\ncounts in all three bands. We use this model to estimate the probabilities that\nan X-ray binary system contains a black hole, non-pulsing neutron star, or\npulsing neutron star. In particular, we utilize a latent variable model in\nwhich the latent variables follow a Gaussian process prior distribution, and\nhence we are able to induce the spatial correlation we believe exists between\nsystems of the same type. The utility of this approach is evidenced by the\naccurate prediction of system types using Rossi X-ray Timing Explorer All Sky\nMonitor data, but it is not flawless. In particular, non-pulsing neutron\nsystems containing \"bursters\" that are close to the boundary demarcating\nsystems containing black holes tend to be classified as black hole systems. As\na byproduct of our analyses, we provide the astronomer with public R code that\ncan be used to predict the compact object type of X-ray binaries given training\ndata. \n\n"}
{"id": "1507.03955", "contents": "Title: Robust Estimation of Self-Exciting Generalized Linear Models with\n  Application to Neuronal Modeling Abstract: We consider the problem of estimating self-exciting generalized linear models\nfrom limited binary observations, where the history of the process serves as\nthe covariate. We analyze the performance of two classes of estimators, namely\nthe $\\ell_1$-regularized maximum likelihood and greedy estimators, for a\ncanonical self-exciting process and characterize the sampling tradeoffs\nrequired for stable recovery in the non-asymptotic regime. Our results extend\nthose of compressed sensing for linear and generalized linear models with\ni.i.d. covariates to those with highly inter-dependent covariates. We further\nprovide simulation studies as well as application to real spiking data from the\nmouse's lateral geniculate nucleus and the ferret's retinal ganglion cells\nwhich agree with our theoretical predictions. \n\n"}
{"id": "1507.04398", "contents": "Title: On the use of reproducing kernel Hilbert spaces in functional\n  classification Abstract: The H\\'ajek-Feldman dichotomy establishes that two Gaussian measures are\neither mutually absolutely continuous with respect to each other (and hence\nthere is a Radon-Nikodym density for each measure with respect to the other\none) or mutually singular. Unlike the case of finite dimensional Gaussian\nmeasures, there are non-trivial examples of both situations when dealing with\nGaussian stochastic processes. This paper provides:\n  (a) Explicit expressions for the optimal (Bayes) rule and the minimal\nclassification error probability in several relevant problems of supervised\nbinary classification of mutually absolutely continuous Gaussian processes. The\napproach relies on some classical results in the theory of Reproducing Kernel\nHilbert Spaces (RKHS).\n  (b) An interpretation, in terms of mutual singularity, for the \"near perfect\nclassification\" phenomenon described by Delaigle and Hall (2012). We show that\nthe asymptotically optimal rule proposed by these authors can be identified\nwith the sequence of optimal rules for an approximating sequence of\nclassification problems in the absolutely continuous case.\n  (c) A new model-based method for variable selection in binary classification\nproblems, which arises in a very natural way from the explicit knowledge of the\nRN-derivatives and the underlying RKHS structure. Different classifiers might\nbe used from the selected variables. In particular, the classical, linear\nfinite-dimensional Fisher rule turns out to be consistent under some standard\nconditions on the underlying functional model. \n\n"}
{"id": "1507.04528", "contents": "Title: A priori truncation method for posterior sampling from homogeneous\n  normalized completely random measure mixture models Abstract: This paper adopts a Bayesian nonparametric mixture model where the mixing\ndistribution belongs to the wide class of normalized homogeneous completely\nrandom measures. We propose a truncation method for the mixing distribution by\ndiscarding the weights of the unnormalized measure smaller than a threshold. We\nprove convergence in law of our approximation, provide some theoretical\nproperties and characterize its posterior distribution so that a blocked Gibbs\nsampler is devised. The versatility of the approximation is illustrated by two\ndifferent applications. In the first the normalized Bessel random measure,\nencompassing the Dirichlet process, is introduced; goodness of fit indexes show\nits good performances as mixing measure for density estimation. The second\ndescribes how to incorporate covariates in the support of the normalized\nmeasure, leading to a linear dependent model for regression and clustering. \n\n"}
{"id": "1507.05617", "contents": "Title: Simulation-based marginal likelihood for cluster strong lensing\n  cosmology Abstract: Comparisons between observed and predicted strong lensing properties of\ngalaxy clusters have been routinely used to claim either tension or consistency\nwith $\\Lambda$CDM cosmology. However, standard approaches to such cosmological\ntests are unable to quantify the preference for one cosmology over another. We\nadvocate approximating the relevant Bayes factor using a marginal likelihood\nthat is based on the following summary statistic: the posterior probability\ndistribution function for the parameters of the scaling relation between\nEinstein radii and cluster mass, $\\alpha$ and $\\beta$. We demonstrate, for the\nfirst time, a method of estimating the marginal likelihood using the X-ray\nselected $z>0.5$ MACS clusters as a case in point and employing both N-body and\nhydrodynamic simulations of clusters. We investigate the uncertainty in this\nestimate and consequential ability to compare competing cosmologies, that\narises from incomplete descriptions of baryonic processes, discrepancies in\ncluster selection criteria, redshift distribution, and dynamical state. The\nrelation between triaxial cluster masses at various overdensities provide a\npromising alternative to the strong lensing test. \n\n"}
{"id": "1507.06807", "contents": "Title: Bayesian inference for diffusion driven mixed-effects models Abstract: Stochastic differential equations (SDEs) provide a natural framework for\nmodelling intrinsic stochasticity inherent in many continuous-time physical\nprocesses. When such processes are observed in multiple individuals or\nexperimental units, SDE driven mixed-effects models allow the quantification of\nbetween (as well as within) individual variation. Performing Bayesian inference\nfor such models, using discrete time data that may be incomplete and subject to\nmeasurement error is a challenging problem and is the focus of this paper. We\nextend a recently proposed MCMC scheme to include the SDE driven mixed-effects\nframework. Fundamental to our approach is the development of a novel construct\nthat allows for efficient sampling of conditioned SDEs that may exhibit\nnonlinear dynamics between observation times. We apply the resulting scheme to\nsynthetic data generated from a simple SDE model of orange tree growth, and\nreal data consisting of observations on aphid numbers recorded under a variety\nof different treatment regimes. In addition, we provide a systematic comparison\nof our approach with an inference scheme based on a tractable approximation of\nthe SDE, that is, the linear noise approximation. \n\n"}
{"id": "1507.08727", "contents": "Title: Large Scale Signal Detection: A Unified Perspective Abstract: There is an overwhelmingly large literature and algorithms already available\non `large scale inference problems' based on different modeling techniques and\ncultures. Our primary goal in this paper is \\emph{not to add one more new\nmethodology} to the existing toolbox but instead (a) to clarify the mystery how\nthese different simultaneous inference methods are \\emph{connected}, (b) to\nprovide an alternative more intuitive derivation of the formulas that leads to\n\\emph{simpler} expressions, and (c) to develop a \\emph{unified} algorithm for\npractitioners. A detailed discussion on representation, estimation, inference,\nand model selection is given. Applications to a variety of real and simulated\ndatasets show promise. We end with several future research directions. \n\n"}
{"id": "1508.00459", "contents": "Title: Unsupervised Learning in Genome Informatics Abstract: With different genomes available, unsupervised learning algorithms are\nessential in learning genome-wide biological insights. Especially, the\nfunctional characterization of different genomes is essential for us to\nunderstand lives. In this book chapter, we review the state-of-the-art\nunsupervised learning algorithms for genome informatics from DNA to MicroRNA.\n  DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significant\nfraction of DNA regions (transcription factor binding sites) are bound by\nproteins (transcription factors) to regulate gene expression at different\ndevelopment stages in different tissues. To fully understand genetics, it is\nnecessary of us to apply unsupervised learning algorithms to learn and infer\nthose DNA regions. Here we review several unsupervised learning methods for\ndeciphering the genome-wide patterns of those DNA regions.\n  MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleic\nacid) species, regulate gene expression post-transcriptionally by forming\nimperfect base-pair with the target sites primarily at the 3$'$ untranslated\nregions of the messenger RNAs. Since the 1993 discovery of the first miRNA\n\\emph{let-7} in worms, a vast amount of studies have been dedicated to\nfunctionally characterizing the functional impacts of miRNA in a network\ncontext to understand complex diseases such as cancer. Here we review several\nrepresentative unsupervised learning frameworks on inferring miRNA regulatory\nnetwork by exploiting the static sequence-based information pertinent to the\nprior knowledge of miRNA targeting and the dynamic information of miRNA\nactivities implicated by the recently available large data compendia, which\ninterrogate genome-wide expression profiles of miRNAs and/or mRNAs across\nvarious cell conditions. \n\n"}
{"id": "1508.00934", "contents": "Title: Admissibility in Partial Conjunction Testing Abstract: Meta-analysis combines results from multiple studies aiming to increase power\nin finding their common effect. It would typically reject the null hypothesis\nof no effect if any one of the studies shows strong significance. The partial\nconjunction null hypothesis is rejected only when at least $r$ of $n$ component\nhypotheses are non-null with $r = 1$ corresponding to a usual meta-analysis.\nCompared with meta-analysis, it can encourage replicable findings across\nstudies. A by-product of it when applied to different $r$ values is a\nconfidence interval of $r$ quantifying the proportion of non-null studies.\nBenjamini and Heller (2008) provided a valid test for the partial conjunction\nnull by ignoring the $r - 1$ smallest p-values and applying a valid\nmeta-analysis p-value to the remaining $n - r + 1$ p-values. We provide\nsufficient and necessary conditions of admissible combined p-value for the\npartial conjunction hypothesis among monotone tests. Non-monotone tests always\ndominate monotone tests but are usually too unreasonable to be used in\npractice. Based on these findings, we propose a generalized form of Benjamini\nand Heller's test which allows usage of various types of meta-analysis\np-values, and apply our method to an example in assessing replicable benefit of\nnew anticoagulants across subgroups of patients for stroke prevention. \n\n"}
{"id": "1508.01280", "contents": "Title: Empirical Bayesian analysis of simultaneous changepoints in multiple\n  data sequences Abstract: Copy number variations in cancer cells and volatility fluctuations in stock\nprices are commonly manifested as changepoints occurring at the same positions\nacross related data sequences. We introduce a Bayesian modeling framework,\nBASIC, that employs a changepoint prior to capture the co-occurrence tendency\nin data of this type. We design efficient algorithms to sample from and\nmaximize over the BASIC changepoint posterior and develop a Monte Carlo\nexpectation-maximization procedure to select prior hyperparameters in an\nempirical Bayes fashion. We use the resulting BASIC framework to analyze DNA\ncopy number variations in the NCI-60 cancer cell lines and to identify\nimportant events that affected the price volatility of S&P 500 stocks from 2000\nto 2009. \n\n"}
{"id": "1508.01551", "contents": "Title: A Knowledge Gradient Policy for Sequencing Experiments to Identify the\n  Structure of RNA Molecules Using a Sparse Additive Belief Model Abstract: We present a sparse knowledge gradient (SpKG) algorithm for adaptively\nselecting the targeted regions within a large RNA molecule to identify which\nregions are most amenable to interactions with other molecules. Experimentally,\nsuch regions can be inferred from fluorescence measurements obtained by binding\na complementary probe with fluorescence markers to the targeted regions. We use\na biophysical model which shows that the fluorescence ratio under the log scale\nhas a sparse linear relationship with the coefficients describing the\naccessibility of each nucleotide, since not all sites are accessible (due to\nthe folding of the molecule). The SpKG algorithm uniquely combines the Bayesian\nranking and selection problem with the frequentist $\\ell_1$ regularized\nregression approach Lasso. We use this algorithm to identify the sparsity\npattern of the linear model as well as sequentially decide the best regions to\ntest before experimental budget is exhausted. Besides, we also develop two\nother new algorithms: batch SpKG algorithm, which generates more suggestions\nsequentially to run parallel experiments; and batch SpKG with a procedure which\nwe call length mutagenesis. It dynamically adds in new alternatives, in the\nform of types of probes, are created by inserting, deleting or mutating\nnucleotides within existing probes. In simulation, we demonstrate these\nalgorithms on the Group I intron (a mid-size RNA molecule), showing that they\nefficiently learn the correct sparsity pattern, identify the most accessible\nregion, and outperform several other policies. \n\n"}
{"id": "1508.03747", "contents": "Title: Nonparametric Distributed Learning Architecture for Big Data: Algorithm\n  and Applications Abstract: Dramatic increases in the size and complexity of modern datasets have made\ntraditional \"centralized\" statistical inference prohibitive. In addition to\ncomputational challenges associated with big data learning, the presence of\nnumerous data types (e.g. discrete, continuous, categorical, etc.) makes\nautomation and scalability difficult. A question of immediate concern is how to\ndesign a data-intensive statistical inference architecture without changing the\nbasic statistical modeling principles developed for \"small\" data over the last\ncentury. To address this problem, we present MetaLP, a flexible, distributed\nstatistical modeling framework. \n\n"}
{"id": "1508.04149", "contents": "Title: Investigating Galaxy-Filament Alignments in Hydrodynamic Simulations\n  using Density Ridges Abstract: In this paper, we study the filamentary structures and the galaxy alignment\nalong filaments at redshift $z=0.06$ in the MassiveBlack-II simulation, a\nstate-of-the-art, high-resolution hydrodynamical cosmological simulation which\nincludes stellar and AGN feedback in a volume of (100 Mpc$/h$)$^3$. The\nfilaments are constructed using the subspace constrained mean shift (SCMS;\nOzertem & Erdogmus (2011) and Chen et al. (2015a)). First, we show that\nreconstructed filaments using galaxies and reconstructed filaments using dark\nmatter particles are similar to each other; over $50\\%$ of the points on the\ngalaxy filaments have a corresponding point on the dark matter filaments within\ndistance $0.13$ Mpc$/h$ (and vice versa) and this distance is even smaller at\nhigh-density regions. Second, we observe the alignment of the major principal\naxis of a galaxy with respect to the orientation of its nearest filament and\ndetect a $2.5$ Mpc$/h$ critical radius for filament's influence on the\nalignment when the subhalo mass of this galaxy is between $10^9M_\\odot/h$ and\n$10^{12}M_\\odot/h$. Moreover, we find the alignment signal to increase\nsignificantly with the subhalo mass. Third, when a galaxy is close to filaments\n(less than $0.25$ Mpc$/h$), the galaxy alignment toward the nearest galaxy\ngroup depends on the galaxy subhalo mass. Finally, we find that galaxies close\nto filaments or groups tend to be rounder than those away from filaments or\ngroups. \n\n"}
{"id": "1508.04152", "contents": "Title: Exploring the relationship between the magnitudes of seismic events Abstract: The distribution of the magnitudes of seismic events is generally assumed to\nbe independent on past seismicity. However, by considering events in causal\nrelation, for example mother-daughter, it seems natural to assume that the\nmagnitude of a daughter event is conditionally dependent on the one of the\ncorresponding mother event. In order to find experimental evidence supporting\nthis hypothesis, we analyze different catalogs, both real and simulated, in two\ndifferent ways. From each catalog, we obtain the law of triggered events'\nmagnitude by kernel density. The results obtained show that the distribution\ndensity of triggered events' magnitude varies with the magnitude of their\ncorresponding mother events. As the intuition suggests, an increase of mother\nevents' magnitude induces an increase of the probability of having \"high\"\nvalues of triggered events' magnitude. In addition, we see a statistically\nsignificant increasing linear dependence of the magnitude means. \n\n"}
{"id": "1508.04217", "contents": "Title: Macroeconomic Forecasting and Variable Selection with a Very Large\n  Number of Predictors: A Penalized Regression Approach Abstract: This paper studies macroeconomic forecasting and variable selection using a\nfolded-concave penalized regression with a very large number of predictors. The\npenalized regression approach leads to sparse estimates of the regression\ncoefficients, and is applicable even if the dimensionality of the model is much\nlarger than the sample size. The first half of the paper discusses the\ntheoretical aspects of a folded-concave penalized regression when the model\nexhibits time series dependence. Specifically, we show the oracle inequality\nand the oracle property for ultrahigh-dimensional time-dependent regressors.\nThe latter half of the paper shows the validity of the penalized regression\nusing two motivating empirical applications. The first forecasts U.S. GDP with\nthe FRED-MD data using the MIDAS regression framework, where there are more\nthan 1000 covariates, while the sample size is at most 200. The second examines\nhow well the penalized regression screens the hidden portfolio with around 40\nstocks from more than 1800 potential stocks using NYSE stock price data. Both\napplications reveal that the penalized regression provides remarkable results\nin terms of forecasting performance and variable selection. \n\n"}
{"id": "1509.00922", "contents": "Title: Calibrating general posterior credible regions Abstract: An advantage of methods that base inference on a posterior distribution is\nthat credible regions are readily obtained. Except in well-specified\nsituations, however, there is no guarantee that such regions will achieve the\nnominal frequentist coverage probability, even approximately. To overcome this\ndifficulty, we propose a general strategy that introduces an additional scalar\ntuning parameter to control the posterior spread, and we develop an algorithm\nthat chooses this parameter so that the corresponding credible region achieves\nthe nominal coverage probability. \n\n"}
{"id": "1509.03521", "contents": "Title: Similarity-based semi-local estimation of EMOS models Abstract: Weather forecasts are typically given in the form of forecast ensembles\nobtained from multiple runs of numerical weather prediction models with varying\ninitial conditions and physics parameterizations. Such ensemble predictions\ntend to be biased and underdispersive and thus require statistical\npostprocessing. In the ensemble model output statistics (EMOS) approach, a\nprobabilistic forecast is given by a single parametric distribution with\nparameters depending on the ensemble members. This article proposes two\nsemi-local methods for estimating the EMOS coefficients where the training data\nfor a specific observation station are augmented with corresponding forecast\ncases from stations with similar characteristics. Similarities between stations\nare determined using either distance functions or clustering based on various\nfeatures of the climatology, forecast errors, ensemble predictions and\nlocations of the observation stations. In a case study on wind speed over\nEurope with forecasts from the Grand Limited Area Model Ensemble Prediction\nSystem, the proposed similarity-based semi-local models show significant\nimprovement in predictive performance compared to standard regional and local\nestimation methods. They further allow for estimating complex models without\nnumerical stability issues and are computationally more efficient than local\nparameter estimation. \n\n"}
{"id": "1509.03767", "contents": "Title: Algorithms for Envelope Estimation II Abstract: We propose a new algorithm for envelope estimation, along with a new root n\nconsistent method for computing starting values. The new algorithm, which does\nnot require optimization over a Grassmannian, is shown by simulation to be much\nfaster and typically more accurate that the best existing algorithm proposed by\nCook and Zhang (2015c). \n\n"}
{"id": "1509.04752", "contents": "Title: Bayesian inference for spatio-temporal spike-and-slab priors Abstract: In this work, we address the problem of solving a series of underdetermined\nlinear inverse problems subject to a sparsity constraint. We generalize the\nspike-and-slab prior distribution to encode a priori correlation of the support\nof the solution in both space and time by imposing a transformed Gaussian\nprocess on the spike-and-slab probabilities. An expectation propagation (EP)\nalgorithm for posterior inference under the proposed model is derived. For\nlarge scale problems, the standard EP algorithm can be prohibitively slow. We\ntherefore introduce three different approximation schemes to reduce the\ncomputational complexity. Finally, we demonstrate the proposed model using\nnumerical experiments based on both synthetic and real data sets. \n\n"}
{"id": "1509.06376", "contents": "Title: Detecting Effects of Filaments on Galaxy Properties in the Sloan Digital\n  Sky Survey III Abstract: We study the effects of filaments on galaxy properties in the Sloan Digital\nSky Survey (SDSS) Data Release 12 using filaments from the `Cosmic Web\nReconstruction' catalogue (Chen et al. 2016), a publicly available filament\ncatalogue for SDSS. Since filaments are tracers of medium-to-high density\nregions, we expect that galaxy properties associated with the environment are\ndependent on the distance to the nearest filament. Our analysis demonstrates\nthat a red galaxy or a high-mass galaxy tend to reside closer to filaments than\na blue or low-mass galaxy. After adjusting the effect from stellar mass, on\naverage, early-forming galaxies or large galaxies have a shorter distance to\nfilaments than late-forming galaxies or small galaxies. For the Main galaxy\nsample (MGS), all signals are very significant ($>6\\sigma$). For the LOWZ and\nCMASS sample, the stellar mass and size are significant ($>2 \\sigma$). The\nfilament effects we observe persist until $z = 0.7$ (the edge of the CMASS\nsample). Comparing our results to those using the galaxy distances from\nredMaPPer galaxy clusters as a reference, we find a similar result between\nfilaments and clusters. Moreover, we find that the effect of clusters on the\nstellar mass of nearby galaxies depends on the galaxy's filamentary\nenvironment. Our findings illustrate the strong correlation of galaxy\nproperties with proximity to density ridges, strongly supporting the claim that\ndensity ridges are good tracers of filaments. \n\n"}
{"id": "1509.08056", "contents": "Title: Discovery and Visualization of Nonstationary Causal Models Abstract: It is commonplace to encounter nonstationary data, of which the underlying\ngenerating process may change over time or across domains. The nonstationarity\npresents both challenges and opportunities for causal discovery. In this paper\nwe propose a principled framework to handle nonstationarity, and develop some\nmethods to address three important questions. First, we propose an enhanced\nconstraint-based method to detect variables whose local mechanisms are\nnonstationary and recover the skeleton of the causal structure over observed\nvariables. Second, we present a way to determine some causal directions by\ntaking advantage of information carried by changing distributions. Third, we\ndevelop a method for visualizing the nonstationarity of causal modules.\nExperimental results on various synthetic and real-world data sets are\npresented to demonstrate the efficacy of our methods. \n\n"}
{"id": "1509.08124", "contents": "Title: A testing-based approach to the discovery of differentially correlated\n  variable sets Abstract: Given data obtained under two sampling conditions, it is often of interest to\nidentify variables that behave differently in one condition than in the other.\nWe introduce a method for differential analysis of second-order behavior called\nDifferential Correlation Mining (DCM). The DCM method identifies differentially\ncorrelated sets of variables, with the property that the average pairwise\ncorrelation between variables in a set is higher under one sample condition\nthan the other. DCM is based on an iterative search procedure that adaptively\nupdates the size and elements of a candidate variable set. Updates are\nperformed via hypothesis testing of individual variables, based on the\nasymptotic distribution of their average differential correlation. We\ninvestigate the performance of DCM by applying it to simulated data as well as\nrecent experimental datasets in genomics and brain imaging. \n\n"}
{"id": "1510.00918", "contents": "Title: Testing for Characteristics of Attribute Linked Infinite Networks based\n  on Small Samples Abstract: The objective of this paper is to study the characteristics (geometric and\notherwise) of very large attribute based undirected networks. Real-world\nnetworks are often very large and fast evolving. Their analysis and\nunderstanding present a great challenge. An Attribute based network is a graph\nin which the edges depend on certain properties of the vertices on which they\nare incident. In context of a social network, the existence of links between\ntwo individuals may depend on certain attributes of the two of them. We use the\nLovasz type sampling strategy of observing a certain random process on a graph\nlocally , i.e., in the neighborhood of a node, and deriving information about\nglobal properties of the graph. The corresponding adjacency matrix is our\nprimary object of interest. We study the efficiency of recently proposed\nsampling strategies, modified to our set up, to estimate the degree\ndistribution, centrality measures, planarity etc. The limiting distributions\nare derived using recently developed probabilistic techniques for random\nmatrices and hence we devise relevant test statistics and confidence intervals\nfor different parameters / hypotheses of interest. We hope that our work will\nbe useful for social and computer scientists for designing sampling strategies\nand computational algorithms appropriate to their respective domains of\ninquiry. Extensive simulations studies are done to empirically verify the\nprobabilistic statements made in the paper. \n\n"}
{"id": "1510.03229", "contents": "Title: Statistically efficient tomography of low rank states with incomplete\n  measurements Abstract: The construction of physically relevant low dimensional state models, and the\ndesign of appropriate measurements are key issues in tackling quantum state\ntomography for large dimensional systems. We consider the statistical problem\nof estimating low rank states in the set-up of multiple ions tomography, and\ninvestigate how the estimation error behaves with a reduction in the number of\nmeasurement settings, compared with the standard ion tomography setup. We\npresent extensive simulation results showing that the error is robust with\nrespect to the choice of states of a given rank, the random selection of\nsettings, and that the number of settings can be significantly reduced with\nonly a negligible increase in error. We present an argument to explain these\nfindings based on a concentration inequality for the Fisher information matrix.\nIn the more general setup of random basis measurements we use this argument to\nshow that for certain rank $r$ states it suffices to measure in $O(r\\log d)$\nbases to achieve the average Fisher information over all bases. We present\nnumerical evidence for states upto 8 atoms, supporting a conjecture on a lower\nbound for the Fisher information which, if true, would imply a similar\nbehaviour in the case of Pauli bases. The relation to similar problems in\ncompressed sensing is also discussed. \n\n"}
{"id": "1510.03349", "contents": "Title: Toward a Better Understanding of Leaderboard Abstract: The leaderboard in machine learning competitions is a tool to show the\nperformance of various participants and to compare them. However, the\nleaderboard quickly becomes no longer accurate, due to hack or overfitting.\nThis article gives two pieces of advice to prevent easy hack or overfitting. By\nfollowing these advice, we reach the conclusion that something like the Ladder\nleaderboard introduced in [blum2015ladder] is inevitable. With this\nunderstanding, we naturally simplify Ladder by eliminating its redundant\ncomputation and explain how to choose the parameter and interpret it. We also\nprove that the sample complexity is cubic to the desired precision of the\nleaderboard. \n\n"}
{"id": "1510.03542", "contents": "Title: Heteroscedasticity Testing for Regression Models: A Dimension\n  Reduction-based Model Adaptive Abstract: Heteroscedasticity testing is of importance in regression analysis. Existing\nlocal smoothing tests suffer severely from curse of dimensionality even when\nthe number of covariates is moderate because of use of nonparametric\nestimation. In this paper, a dimension reduction-based model adaptive test is\nproposed which behaves like a local smoothing test as if the number of\ncovariates were equal to the number of their linear combinations in the mean\nregression function, in particular, equal to 1 when the mean function contains\na single index. The test statistic is asymptotically normal under the null\nhypothesis such that critical values are easily determined. The finite sample\nperformances of the test are examined by simulations and a real data analysis. \n\n"}
{"id": "1510.04406", "contents": "Title: A New Method for Avoiding Data Disclosure While Automatically Preserving\n  Multivariate Relations Abstract: Statistical disclosure limitation (SDL) methods aim to provide analysts\ngeneral access to a data set while limiting the risk of disclosure of\nindividual records. Many methods in the existing literature are aimed only at\nthe case of univariate distributions, but the multivariate case is crucial,\nsince most statistical analyses are multivariate in nature. Yet preserving the\nmultivariate structure of the data can be challenging, especially when both\ncontinuous and categorical variables are present. Here we present a new SDL\nmethod that automatically attains the correct multivariate structure,\nregardless of whether the data are continuous, categorical or mixed. In\naddition, operational methods for assessing data quality and risk will be\nexplored. \n\n"}
{"id": "1510.05391", "contents": "Title: Unifying inference on brain network variations in neurological diseases:\n  The Alzheimer's case Abstract: There is growing interest in understanding how the structural\ninterconnections among brain regions change with the occurrence of neurological\ndiseases. Diffusion weighted MRI imaging has allowed researchers to\nnon-invasively estimate a network of structural cortical connections made by\nwhite matter tracts, but current statistical methods for relating such networks\nto the presence or absence of a disease cannot exploit this rich network\ninformation. Standard practice considers each edge independently or summarizes\nthe network with a few simple features. We enable dramatic gains in biological\ninsight via a novel unifying methodology for inference on brain network\nvariations associated to the occurrence of neurological diseases. The key of\nthis approach is to define a probabilistic generative mechanism directly on the\nspace of network configurations via dependent mixtures of low-rank\nfactorizations, which efficiently exploit network information and allow the\nprobability mass function for the brain network-valued random variable to vary\nflexibly across the group of patients characterized by a specific neurological\ndisease and the one comprising age-matched cognitively healthy individuals. \n\n"}
{"id": "1510.05417", "contents": "Title: Piecewise-Linear Approximation for Feature Subset Selection in a\n  Sequential Logit Model Abstract: This paper concerns a method of selecting a subset of features for a\nsequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integer\nquadratic optimization formulation for solving the problem based on a quadratic\napproximation of the logistic loss function. However, since there is a\nsignificant gap between the logistic loss function and its quadratic\napproximation, their formulation may fail to find a good subset of features. To\novercome this drawback, we apply a piecewise-linear approximation to the\nlogistic loss function. Accordingly, we frame the feature subset selection\nproblem of minimizing an information criterion as a mixed integer linear\noptimization problem. The computational results demonstrate that our\npiecewise-linear approximation approach found a better subset of features than\nthe quadratic approximation approach. \n\n"}
{"id": "1510.06618", "contents": "Title: Determinantal Sampling Designs Abstract: In this article, recent results about point processes are used in sampling\ntheory. Precisely, we define and study a new class of sampling designs:\ndeterminantal sampling designs. The law of such designs is known, and there\nexists a simple selection algorithm. We compute exactly the variance of linear\nestimators constructed upon these designs by using the first and second order\ninclusion probabilities. Moreover, we obtain asymptotic and finite sample\ntheorems. We construct explicitly fixed size determinantal sampling designs\nwith given first order inclusion probabilities. We also address the search of\noptimal determinantal sampling designs. \n\n"}
{"id": "1510.06731", "contents": "Title: On the shadow moments of apparently infinite-mean phenomena Abstract: We propose an approach to compute the conditional moments of fat-tailed\nphenomena that, only looking at data, could be mistakenly considered as having\ninfinite mean. This type of problems manifests itself when a random variable Y\nhas a heavy- tailed distribution with an extremely wide yet bounded support. We\nintroduce the concept of dual distribution, by means of a log-transformation\nthat removes the upper bound. The tail of the dual distribution can then be\nstudied using extreme value theory, without making excessive parametric\nassumptions, and the estimates one obtains can be used to study the original\ndistribution and compute its moments by reverting the log- transformation. The\ncentral difference between our approach and a simple truncation is in the\nsmoothness of the transformation between the original and the dual\ndistribution, allowing use of extreme value theory. War casualties, operational\nrisk, environment blight, complex networks and many other econophysics\nphenomena are possible fields of application. \n\n"}
{"id": "1510.06817", "contents": "Title: A conditional randomization test to account for covariate imbalance in\n  randomized experiments Abstract: We consider the conditional randomization test as a way to account for\ncovariate imbalance in randomized experiments. The test accounts for covariate\nimbalance by comparing the observed test statistic to the null distribution of\nthe test statistic conditional on the observed covariate imbalance. We prove\nthat the conditional randomization test has the correct significance level and\nintroduce original notation to describe covariate balance more formally.\nThrough simulation, we verify that conditional randomization tests behave like\nmore traditional forms of covariate adjustmet but have the added benefit of\nhaving the correct conditional significance level. Finally, we apply the\napproach to a randomized product marketing experiment where covariate\ninformation was collected after randomization. \n\n"}
{"id": "1510.08440", "contents": "Title: Priors on exchangeable directed graphs Abstract: Directed graphs occur throughout statistical modeling of networks, and\nexchangeability is a natural assumption when the ordering of vertices does not\nmatter. There is a deep structural theory for exchangeable undirected graphs,\nwhich extends to the directed case via measurable objects known as digraphons.\nUsing digraphons, we first show how to construct models for exchangeable\ndirected graphs, including special cases such as tournaments, linear orderings,\ndirected acyclic graphs, and partial orderings. We then show how to construct\npriors on digraphons via the infinite relational digraphon model (di-IRM), a\nnew Bayesian nonparametric block model for exchangeable directed graphs, and\ndemonstrate inference on synthetic data. \n\n"}
{"id": "1510.08986", "contents": "Title: A Unified Theory of Confidence Regions and Testing for High Dimensional\n  Estimating Equations Abstract: We propose a new inferential framework for constructing confidence regions\nand testing hypotheses in statistical models specified by a system of high\ndimensional estimating equations. We construct an influence function by\nprojecting the fitted estimating equations to a sparse direction obtained by\nsolving a large-scale linear program. Our main theoretical contribution is to\nestablish a unified Z-estimation theory of confidence regions for high\ndimensional problems.\n  Different from existing methods, all of which require the specification of\nthe likelihood or pseudo-likelihood, our framework is likelihood-free. As a\nresult, our approach provides valid inference for a broad class of high\ndimensional constrained estimating equation problems, which are not covered by\nexisting methods.\n  Such examples include, noisy compressed sensing, instrumental variable\nregression, undirected graphical models, discriminant analysis and vector\nautoregressive models. We present detailed theoretical results for all these\nexamples. Finally, we conduct thorough numerical simulations, and a real\ndataset analysis to back up the developed theoretical results. \n\n"}
{"id": "1511.00154", "contents": "Title: A Bayesian Nonparametric approach to Reconstruction and Prediction of\n  Random Dynamical Systems Abstract: We propose a Bayesian nonparametric mixture model for the reconstruction and\nprediction from observed time series data, of discretized stochastic dynamical\nsystems, based on Markov Chain Monte Carlo methods (MCMC). Our results can be\nused by researchers in physical modeling interested in a fast and accurate\nestimation of low dimensional stochastic models when the size of the observed\ntime series is small and the noise process (perhaps) is non-Gaussian. The\ninference procedure is demonstrated specifically in the case of polynomial maps\nof arbitrary degree and when a Geometric Stick Breaking mixture process prior\nover the space of densities, is applied to the additive errors. Our method is\nparsimonious compared to Bayesian nonparametric techniques based on Dirichlet\nprocess mixtures, flexible and general. Simulations based on synthetic time\nseries are presented. \n\n"}
{"id": "1511.03334", "contents": "Title: Goodness of fit tests for high-dimensional linear models Abstract: In this work we propose a framework for constructing goodness of fit tests in\nboth low and high-dimensional linear models. We advocate applying regression\nmethods to the scaled residuals following either an ordinary least squares or\nLasso fit to the data, and using some proxy for prediction error as the final\ntest statistic. We call this family Residual Prediction (RP) tests. We show\nthat simulation can be used to obtain the critical values for such tests in the\nlow-dimensional setting, and demonstrate using both theoretical results and\nextensive numerical studies that some form of the parametric bootstrap can do\nthe same when the high-dimensional linear model is under consideration. We show\nthat RP tests can be used to test for significance of groups or individual\nvariables as special cases, and here they compare favourably with state of the\nart methods, but we also argue that they can be designed to test for as diverse\nmodel misspecifications as heteroscedasticity and nonlinearity. \n\n"}
{"id": "1511.05877", "contents": "Title: Generation of scenarios from calibrated ensemble forecasts with a dual\n  ensemble copula coupling approach Abstract: Probabilistic forecasts in the form of ensemble of scenarios are required for\ncomplex decision making processes. Ensemble forecasting systems provide such\nproducts but the spatio-temporal structures of the forecast uncertainty is lost\nwhen statistical calibration of the ensemble forecasts is applied for each lead\ntime and location independently. Non-parametric approaches allow the\nreconstruction of spatio-temporal joint probability distributions at a low\ncomputational cost. For example, the ensemble copula coupling (ECC) method\nrebuilds the multivariate aspect of the forecast from the original ensemble\nforecasts. Based on the assumption of error stationarity, parametric methods\naim to fully describe the forecast dependence structures. In this study, the\nconcept of ECC is combined with past data statistics in order to account for\nthe autocorrelation of the forecast error. The new approach, called d-ECC, is\napplied to wind forecasts from the high resolution ensemble system COSMO-DE-EPS\nrun operationally at the German weather service. Scenarios generated by ECC and\nd-ECC are compared and assessed in the form of time series by means of\nmultivariate verification tools and in a product oriented framework.\nVerification results over a 3 month period show that the innovative method\nd-ECC outperforms or performs as well as ECC in all investigated aspects. \n\n"}
{"id": "1511.06028", "contents": "Title: Optimal inference in a class of regression models Abstract: We consider the problem of constructing confidence intervals (CIs) for a\nlinear functional of a regression function, such as its value at a point, the\nregression discontinuity parameter, or a regression coefficient in a linear or\npartly linear regression. Our main assumption is that the regression function\nis known to lie in a convex function class, which covers most smoothness and/or\nshape assumptions used in econometrics. We derive finite-sample optimal CIs and\nsharp efficiency bounds under normal errors with known variance. We show that\nthese results translate to uniform (over the function class) asymptotic results\nwhen the error distribution is not known. When the function class is\ncentrosymmetric, these efficiency bounds imply that minimax CIs are close to\nefficient at smooth regression functions. This implies, in particular, that it\nis impossible to form CIs that are tighter using data-dependent tuning\nparameters, and maintain coverage over the whole function class. We specialize\nour results to inference on the regression discontinuity parameter, and\nillustrate them in simulations and an empirical application. \n\n"}
{"id": "1511.07464", "contents": "Title: On the Poisson equation for Metropolis-Hastings chains Abstract: This paper defines an approximation scheme for a solution of the Poisson\nequation of a geometrically ergodic Metropolis-Hastings chain $\\Phi$. The\napproximations give rise to a natural sequence of control variates for the\nergodic average $S_k(F)=(1/k)\\sum_{i=1}^{k} F(\\Phi_i)$, where $F$ is the force\nfunction in the Poisson equation. The main result of the paper shows that the\nsequence of the asymptotic variances (in the CLTs for the control-variate\nestimators) converges to zero and gives a rate of this convergence. Numerical\nexamples in the case of a double-well potential are discussed. \n\n"}
{"id": "1512.00218", "contents": "Title: Minimax theory for a class of non-linear statistical inverse problems Abstract: We study a class of statistical inverse problems with non-linear pointwise\noperators motivated by concrete statistical applications. A two-step procedure\nis proposed, where the first step smoothes the data and inverts the\nnon-linearity. This reduces the initial non-linear problem to a linear inverse\nproblem with deterministic noise, which is then solved in a second step. The\nnoise reduction step is based on wavelet thresholding and is shown to be\nminimax optimal (up to logarithmic factors) in a pointwise function-dependent\nsense. Our analysis is based on a modified notion of H\\\"older smoothness scales\nthat are natural in this setting. \n\n"}
{"id": "1512.00899", "contents": "Title: Estimating Learning Effects: A Short-Time Fourier Transform Regression\n  Model for MEG Source Localization Abstract: Magnetoencephalography (MEG) has a high temporal resolution well-suited for\nstudying perceptual learning. However, to identify where learning happens in\nthe brain, one needs to ap- ply source localization techniques to project MEG\nsensor data into brain space. Previous source localization methods, such as the\nshort-time Fourier transform (STFT) method by Gramfort et al.([Gramfort et al.,\n2013]) produced intriguing results, but they were not designed to incor- porate\ntrial-by-trial learning effects. Here we modify the approach in [Gramfort et\nal., 2013] to produce an STFT-based source localization method (STFT-R) that\nincludes an additional regression of the STFT components on covariates such as\nthe behavioral learning curve. We also exploit a hierarchical L 21 penalty to\ninduce structured sparsity of STFT components and to emphasize signals from\nregions of interest (ROIs) that are selected according to prior knowl- edge. In\nreconstructing the ROI source signals from simulated data, STFT-R achieved\nsmaller errors than a two-step method using the popular minimum-norm estimate\n(MNE), and in a real-world human learning experiment, STFT-R yielded more\ninterpretable results about what time-frequency components of the ROI signals\nwere correlated with learning. \n\n"}
{"id": "1512.01255", "contents": "Title: MERLiN: Mixture Effect Recovery in Linear Networks Abstract: Causal inference concerns the identification of cause-effect relationships\nbetween variables, e.g. establishing whether a stimulus affects activity in a\ncertain brain region. The observed variables themselves often do not constitute\nmeaningful causal variables, however, and linear combinations need to be\nconsidered. In electroencephalographic studies, for example, one is not\ninterested in establishing cause-effect relationships between electrode signals\n(the observed variables), but rather between cortical signals (the causal\nvariables) which can be recovered as linear combinations of electrode signals.\n  We introduce MERLiN (Mixture Effect Recovery in Linear Networks), a family of\ncausal inference algorithms that implement a novel means of constructing causal\nvariables from non-causal variables. We demonstrate through application to EEG\ndata how the basic MERLiN algorithm can be extended for application to\ndifferent (neuroimaging) data modalities. Given an observed linear mixture, the\nalgorithms can recover a causal variable that is a linear effect of another\ngiven variable. That is, MERLiN allows us to recover a cortical signal that is\naffected by activity in a certain brain region, while not being a direct effect\nof the stimulus. The Python/Matlab implementation for all presented algorithms\nis available on https://github.com/sweichwald/MERLiN \n\n"}
{"id": "1512.01473", "contents": "Title: Robust estimators of accelerated failure time regression with\n  generalized log-gamma errors Abstract: The generalized log-gamma (GLG) model is a very flexible family of\ndistributions to analyze datasets in many different areas of science and\ntechnology. In this paper, we propose estimators which are simultaneously\nhighly robust and highly efficient for the parameters of a GLG distribution in\nthe presence of censoring. We also introduced estimators with the same\nproperties for accelerated failure time models with censored observations and\nerror distribution belonging to the GLG family. We prove that the proposed\nestimators are asymptotically fully efficient and examine the maximum mean\nsquare error using Monte Carlo simulations. The simulations confirm that the\nproposed estimators are highly robust and highly efficient for finite sample\nsize. Finally, we illustrate the good behavior of the proposed estimators with\ntwo real datasets. \n\n"}
{"id": "1512.01617", "contents": "Title: Guarding against Spurious Discoveries in High Dimensions Abstract: Many data mining and statistical machine learning algorithms have been\ndeveloped to select a subset of covariates to associate with a response\nvariable. Spurious discoveries can easily arise in high-dimensional data\nanalysis due to enormous possibilities of such selections. How can we know\nstatistically our discoveries better than those by chance? In this paper, we\ndefine a measure of goodness of spurious fit, which shows how good a response\nvariable can be fitted by an optimally selected subset of covariates under the\nnull model, and propose a simple and effective LAMM algorithm to compute it. It\ncoincides with the maximum spurious correlation for linear models and can be\nregarded as a generalized maximum spurious correlation. We derive the\nasymptotic distribution of such goodness of spurious fit for generalized linear\nmodels and $L_1$ regression. Such an asymptotic distribution depends on the\nsample size, ambient dimension, the number of variables used in the fit, and\nthe covariance information. It can be consistently estimated by multiplier\nbootstrapping and used as a benchmark to guard against spurious discoveries. It\ncan also be applied to model selection, which considers only candidate models\nwith goodness of fits better than those by spurious fits. The theory and method\nare convincingly illustrated by simulated examples and an application to the\nbinary outcomes from German Neuroblastoma Trials. \n\n"}
{"id": "1512.02306", "contents": "Title: Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait\n  Association Mapping Abstract: Genome-wide association studies have proven to be essential for understanding\nthe genetic basis of disease. However, many complex traits---personality\ntraits, facial features, disease subtyping---are inherently high-dimensional,\nimpeding simple approaches to association mapping. We developed a nonparametric\nBayesian reduced rank regression model for multi-SNP, multi-trait association\nmapping that does not require the rank of the linear subspace to be specified.\nWe show in simulations and real data that our model shares strength over SNPs\nand over correlated traits, improving statistical power to identify genetic\nassociations with an interpretable, SNP-supervised low-dimensional linear\nprojection of the high-dimensional phenotype. On the HapMap phase 3 gene\nexpression QTL study data, we identify pleiotropic expression QTLs that\nclassical univariate tests are underpowered to find and that two step\napproaches cannot recover. Our Python software, BERRRI, is publicly available\nat GitHub: https://github.com/ashlee1031/BERRRI. \n\n"}
{"id": "1512.02452", "contents": "Title: Sequential Markov Chain Monte Carlo for Bayesian Filtering with Massive\n  Data Abstract: Advances in digital sensors, digital data storage and communications have\nresulted in systems being capable of accumulating large collections of data. In\nthe light of dealing with the challenges that massive data present, this work\nproposes solutions to inference and filtering problems within the Bayesian\nframework. Two novel Bayesian inference algorithms are developed for non-linear\nand non-Gaussian state space models, able to deal with large volumes of data\n(or observations). These are sequential Markov chain Monte Carlo (MCMC)\napproaches relying on two key ideas: 1) subsample the massive data and utilise\na smaller subset for filtering and inference, and 2) a divide and conquer type\napproach computing local filtering distributions each using a subset of the\nmeasurements. Simulation results highlight the accuracy and the large\ncomputational savings, that can reach 90% by the proposed algorithms when\ncompared with standard techniques. \n\n"}
{"id": "1512.02487", "contents": "Title: High-Dimensional Gaussian Copula Regression: Adaptive Estimation and\n  Statistical Inference Abstract: We develop adaptive estimation and inference methods for high-dimensional\nGaussian copula regression that achieve the same performance without the\nknowledge of the marginal transformations as that for high-dimensional linear\nregression. Using a Kendall's tau based covariance matrix estimator, an\n$\\ell_1$ regularized estimator is proposed and a corresponding de-biased\nestimator is developed for the construction of the confidence intervals and\nhypothesis tests. Theoretical properties of the procedures are studied and the\nproposed estimation and inference methods are shown to be adaptive to the\nunknown monotone marginal transformations. Prediction of the response for a\ngiven value of the covariates is also considered. The procedures are easy to\nimplement and perform well numerically. The methods are also applied to analyze\nthe Communities and Crime Unnormalized Data from the UCI Machine Learning\nRepository. \n\n"}
{"id": "1512.02834", "contents": "Title: On the Ambiguity of Interaction and Nonlinear Main Effects in a Regime\n  of Dependent Covariates Abstract: The analysis of large experimental datasets frequently reveals significant\ninteractions that are difficult to interpret within the theoretical framework\nguiding the research. Some of these interactions actually arise from the\npresence of unspecified nonlinear main effects and statistically dependent\ncovariates in the statistical model. Importantly, such nonlinear main effects\nmay be compatible (or, at least, not incompatible) with the current theoretical\nframework. In the present literature this issue has only been studied in terms\nof correlated (linearly dependent) covariates. Here we generalize to nonlinear\nmain effects (i.e., main effects of arbitrary shape) and dependent covariates.\nWe propose a novel nonparametric method to test for ambiguous interactions\nwhere present parametric methods fail. We illustrate the method with a set of\nsimulations and with reanalyses (a) of effects of parental education on their\nchildren's educational expectations and (b) of effects of word properties on\nfixation locations during reading of natural sentences, specifically of effects\nof length and morphological complexity of the word to be fixated next. The\nresolution of such ambiguities facilitates theoretical progress. \n\n"}
{"id": "1512.08731", "contents": "Title: A Variational EM Method for Mixed Membership Models with Multivariate\n  Rank Data: an Analysis of Public Policy Preferences Abstract: In this article, we consider modeling ranked responses from a heterogeneous\npopulation. Specifically, we analyze data from the Eurobarometer 34.1 survey\nregarding public policy preferences towards drugs, alcohol and AIDS. Such\npolicy preferences are likely to exhibit substantial differences within as well\nas across European nations reflecting a wide variety of cultures, political\naffiliations, ideological perspectives and common practices. We use a mixed\nmembership model to account for multiple subgroups with differing preferences\nand to allow each individual to possess partial membership in more than one\nsubgroup. Previous methods for fitting mixed membership models to rank data in\na univariate setting have utilized an MCMC approach and do not estimate the\nrelative frequency of each subgroup. We propose a variational EM approach for\nfitting mixed membership models with multivariate rank data. Our method allows\nfor fast approximate inference and explicitly estimates the subgroup sizes.\nAnalyzing the Eurobarometer 34.1 data, we find interpretable subgroups which\ngenerally agree with the \"left vs right\" classification of political\nideologies. \n\n"}
{"id": "1601.00129", "contents": "Title: The Reduced-Order Hybrid Monte Carlo Sampling Smoother Abstract: Hybrid Monte-Carlo (HMC) sampling smoother is a fully non-Gaussian\nfour-dimensional data assimilation algorithm that works by directly sampling\nthe posterior distribution formulated in the Bayesian framework. The smoother\nin its original formulation is computationally expensive due to the intrinsic\nrequirement of running the forward and adjoint models repeatedly. Here we\npresent computationally efficient versions of the HMC sampling smoother based\non reduced-order approximations of the underlying model dynamics. The schemes\ndeveloped herein are tested numerically using the shallow-water equations model\non Cartesian coordinates. The results reveal that the reduced-order versions of\nthe smoother are capable of accurately capturing the posterior probability\ndensity, while being significantly faster than the original full order\nformulation. \n\n"}
{"id": "1601.01126", "contents": "Title: Statistical methods for linguistic research: Foundational Ideas - Part I Abstract: We present the fundamental ideas underlying statistical hypothesis testing\nusing the frequentist framework. We begin with a simple example that builds up\nthe one-sample t-test from the beginning, explaining important concepts such as\nthe sampling distribution of the sample mean, and the iid assumption. Then we\nexamine the p-value in detail, and discuss several important misconceptions\nabout what a p-value does and does not tell us. This leads to a discussion of\nType I, II error and power, and Type S and M error. An important conclusion\nfrom this discussion is that one should aim to carry out appropriately powered\nstudies. Next, we discuss two common issues we have encountered in\npsycholinguistics and linguistics: running experiments until significance is\nreached, and the \"garden-of-forking-paths\" problem discussed by Gelman and\nothers, whereby the researcher attempts to find statistical significance by\nanalyzing the data in different ways. The best way to use frequentist methods\nis to run appropriately powered studies, check model assumptions, clearly\nseparate exploratory data analysis from confirmatory hypothesis testing, and\nalways attempt to replicate results. \n\n"}
{"id": "1601.02043", "contents": "Title: Autocorrelated errors in experimental data in the language sciences:\n  Some solutions offered by Generalized Additive Mixed Models Abstract: A problem that tends to be ignored in the statistical analysis of\nexperimental data in the language sciences is that responses often constitute\ntime series, which raises the problem of autocorrelated errors. If the errors\nindeed show autocorrelational structure, evaluation of the significance of\npredictors in the model becomes problematic due to potential anti-conservatism\nof p-values. This paper illustrates two tools offered by Generalized Additive\nMixed Models (GAMMs) (Lin and Zhang, 1999; Wood, 2006, 2011, 2013) for dealing\nwith autocorrelated errors, as implemented in the current version of the fourth\nauthor's mgcv package (1.8.9): the possibility to specify an ar(1) error model\nfor Gaussian models, and the possibility of using factor smooths for\nrandom-effect factors such as subject and item. These factor smooths are set up\nto have the same smoothing parameters, and are penalized to yield the\nnon-linear equivalent of random intercepts and random slopes in the classical\nlinear framework. Three case studies illustrate these issues. \n\n"}
{"id": "1601.03448", "contents": "Title: Functional summary statistics for point processes on the sphere with an\n  application to determinantal point processes Abstract: We study point processes on $\\mathbb S^d$, the $d$-dimensional unit sphere\n$\\mathbb S^d$, considering both the isotropic and the anisotropic case, and\nfocusing mostly on the spherical case $d=2$. The first part studies reduced\nPalm distributions and functional summary statistics, including nearest\nneighbour functions, empty space functions, and Ripley's and inhomogeneous\n$K$-functions. The second part partly discusses the appealing properties of\ndeterminantal point process (DPP) models on the sphere and partly considers the\napplication of functional summary statistics to DPPs. In fact DPPs exhibit\nrepulsiveness, but we also use them together with certain dependent thinnings\nwhen constructing point process models on the sphere with aggregation on the\nlarge scale and regularity on the small scale. We conclude with a discussion on\nfuture work on statistics for spatial point processes on the sphere. \n\n"}
{"id": "1601.04083", "contents": "Title: Observational studies with unknown time of treatment Abstract: Time plays a fundamental role in causal analyses, where the goal is to\nquantify the effect of a specific treatment on future outcomes. In a randomized\nexperiment, times of treatment, and when outcomes are observed, are typically\nwell defined. In an observational study, treatment time marks the point from\nwhich pre-treatment variables must be regarded as outcomes, and it is often\nstraightforward to establish. Motivated by a natural experiment in online\nmarketing, we consider a situation where useful conceptualizations of the\nexperiment behind an observational study of interest lead to uncertainty in the\ndetermination of times at which individual treatments take place. Of interest\nis the causal effect of heavy snowfall in several parts of the country on daily\nmeasures of online searches for batteries, and then purchases. The data\navailable give information on actual snowfall, whereas the natural treatment is\nthe anticipation of heavy snowfall, which is not observed. In this article, we\nintroduce formal assumptions and inference methodology centered around a novel\nnotion of plausible time of treatment. These methods allow us to explicitly\nbound the last plausible time of treatment in observational studies with\nunknown times of treatment, and ultimately yield valid causal estimates in such\nsituations. \n\n"}
{"id": "1601.05156", "contents": "Title: Bayesian Nonparametric Ordination for the Analysis of Microbial\n  Communities Abstract: Human microbiome studies use sequencing technologies to measure the abundance\nof bacterial species or Operational Taxonomic Units (OTUs) in samples of\nbiological material. Typically the data are organized in contingency tables\nwith OTU counts across heterogeneous biological samples. In the microbial\necology community, ordination methods are frequently used to investigate latent\nfactors or clusters that capture and describe variations of OTU counts across\nbiological samples. It remains important to evaluate how uncertainty in\nestimates of each biological sample's microbial distribution propagates to\nordination analyses, including visualization of clusters and projections of\nbiological samples on low dimensional spaces. We propose a Bayesian analysis\nfor dependent distributions to endow frequently used ordinations with estimates\nof uncertainty. A Bayesian nonparametric prior for dependent normalized random\nmeasures is constructed, which is marginally equivalent to the normalized\ngeneralized Gamma process, a well-known prior for nonparametric analyses. In\nour prior the dependence and similarity between microbial distributions is\nrepresented by latent factors that concentrate in a low dimensional space. We\nuse a shrinkage prior to tune the dimensionality of the latent factors. The\nresulting posterior samples of model parameters can be used to evaluate\nuncertainty in analyses routinely applied in microbiome studies. Specifically,\nby combining them with multivariate data analysis techniques we can visualize\ncredible regions in ecological ordination plots. The characteristics of the\nproposed model are illustrated through a simulation study and applications in\ntwo microbiome datasets. \n\n"}
{"id": "1601.05199", "contents": "Title: Portfolio Optimisation Under Flexible Dynamic Dependence Modelling Abstract: Signals coming from multivariate higher order conditional moments as well as\nthe information contained in exogenous covariates, can be effectively exploited\nby rational investors to allocate their wealth among different risky investment\nopportunities. This paper proposes a new flexible dynamic copula model being\nable to explain and forecast the time-varying shape of large dimensional asset\nreturns distributions. Moreover, we let the univariate marginal distributions\nto be driven by an updating mechanism based on the scaled score of the\nconditional distribution. This framework allows us to introduce time-variation\nin up to the fourth moment of the conditional distribution. The time-varying\ndependence pattern is subsequently modelled as function of a latent Markov\nSwitching process, allowing also for the inclusion of exogenous covariates in\nthe dynamic updating equation. We empirically assess that the proposed model\nsubstantially improves the optimal portfolio allocation of rational investors\nmaximising their expected utility. \n\n"}
{"id": "1601.05630", "contents": "Title: Significance-based community detection in weighted networks Abstract: Community detection is the process of grouping strongly connected nodes in a\nnetwork. Many community detection methods for un-weighted networks have a\ntheoretical basis in a null model. Communities discovered by these methods\ntherefore have interpretations in terms of statistical signficance. In this\npaper, we introduce a null for weighted networks called the continuous\nconfiguration model. We use the model both as a tool for community detection\nand for simulating weighted networks with null nodes. First, we propose a\ncommunity extraction algorithm for weighted networks which incorporates\niterative hypothesis testing under the null. We prove a central limit theorem\nfor edge-weight sums and asymptotic consistency of the algorithm under a\nweighted stochastic block model. We then incorporate the algorithm in a\ncommunity detection method called CCME. To benchmark the method, we provide a\nsimulation framework incorporating the null to plant \"background\" nodes in\nweighted networks with communities. We show that the empirical performance of\nCCME on these simulations is competitive with existing methods, particularly\nwhen overlapping communities and background nodes are present. To further\nvalidate the method, we present two real-world networks with potential\nbackground nodes and analyze them with CCME, yielding results that reveal\nmacro-features of the corresponding systems. \n\n"}
{"id": "1601.06630", "contents": "Title: Bayesian Estimation of Bipartite Matchings for Record Linkage Abstract: The bipartite record linkage task consists of merging two disparate datafiles\ncontaining information on two overlapping sets of entities. This is non-trivial\nin the absence of unique identifiers and it is important for a wide variety of\napplications given that it needs to be solved whenever we have to combine\ninformation from different sources. Most statistical techniques currently used\nfor record linkage are derived from a seminal paper by Fellegi and Sunter\n(1969). These techniques usually assume independence in the matching statuses\nof record pairs to derive estimation procedures and optimal point estimators.\nWe argue that this independence assumption is unreasonable and instead target a\nbipartite matching between the two datafiles as our parameter of interest.\nBayesian implementations allow us to quantify uncertainty on the matching\ndecisions and derive a variety of point estimators using different loss\nfunctions. We propose partial Bayes estimates that allow uncertain parts of the\nbipartite matching to be left unresolved. We evaluate our approach to record\nlinkage using a variety of challenging scenarios and show that it outperforms\nthe traditional methodology. We illustrate the advantages of our methods\nmerging two datafiles on casualties from the civil war of El Salvador. \n\n"}
{"id": "1601.06858", "contents": "Title: On distributionally robust extreme value analysis Abstract: We study distributional robustness in the context of Extreme Value Theory\n(EVT). We provide a data-driven method for estimating extreme quantiles in a\nmanner that is robust against incorrect model assumptions underlying the\napplication of the standard Extremal Types Theorem. Typical studies in\ndistributional robustness involve computing worst case estimates over a model\nuncertainty region expressed in terms of the Kullback-Leibler discrepancy. We\ngo beyond standard distributional robustness in that we investigate different\nforms of discrepancies, and prove rigorous results which are helpful for\nunderstanding the role of a putative model uncertainty region in the context of\nextreme quantile estimation. Finally, we illustrate our data-driven method in\nvarious settings, including examples showing how standard EVT can significantly\nunderestimate quantiles of interest. \n\n"}
{"id": "1602.00199", "contents": "Title: Gaussian approximation for the sup-norm of high-dimensional\n  matrix-variate U-statistics and its applications Abstract: This paper studies the Gaussian approximation of high-dimensional and\nnon-degenerate U-statistics of order two under the supremum norm. We propose a\ntwo-step Gaussian approximation procedure that does not impose structural\nassumptions on the data distribution. Specifically, subject to mild moment\nconditions on the kernel, we establish the explicit rate of convergence that\ndecays polynomially in sample size for a high-dimensional scaling limit, where\nthe dimension can be much larger than the sample size. We also supplement a\npractical Gaussian wild bootstrap method to approximate the quantiles of the\nmaxima of centered U-statistics and prove its asymptotic validity. The wild\nbootstrap is demonstrated on statistical applications for high-dimensional\nnon-Gaussian data including: (i) principled and data-dependent tuning parameter\nselection for regularized estimation of the covariance matrix and its related\nfunctionals; (ii) simultaneous inference for the covariance and rank\ncorrelation matrices. In particular, for the thresholded covariance matrix\nestimator with the bootstrap selected tuning parameter, we show that the\nGaussian-like convergence rates can be achieved for heavy-tailed data, which\nare less conservative than those obtained by the Bonferroni technique that\nignores the dependency in the underlying data distribution. In addition, we\nalso show that even for subgaussian distributions, error bounds of the\nbootstrapped thresholded covariance matrix estimator can be much tighter than\nthose of the minimax estimator with a universal threshold. \n\n"}
{"id": "1602.00202", "contents": "Title: Bayesian stochastic volatility models for high-frequency data Abstract: We formulate a discrete-time Bayesian stochastic volatility model for\nhigh-frequency stock-market data that directly accounts for microstructure\nnoise, and outline a Markov chain Monte Carlo algorithm for parameter\nestimation. The methods described in this paper are designed to be coherent\nacross all sampling timescales, with the goal of estimating the latent\nlog-volatility signal from data collected at arbitrarily short sampling\nperiods. In keeping with this goal, we carefully develop a method for eliciting\npriors. The empirical results derived from both simulated and real data show\nthat directly accounting for microstructure in a state-space formulation allows\nfor well-calibrated estimates of the log-volatility process driving prices. \n\n"}
{"id": "1602.00924", "contents": "Title: (Quantum) Fractional Brownian Motion and Multifractal Processes under\n  the Loop of a Tensor Networks Abstract: We derive fractional Brownian motion and stochastic processes with\nmultifractal properties using a framework of network of Gaussian conditional\nprobabilities. This leads to the derivation of new representations of\nfractional Brownian motion. These constructions are inspired from\nrenormalization. The main result of this paper consists of constructing each\nincrement of the process from two-dimensional gaussian noise inside the\nlight-cone of each seperate increment. Not only does this allows us to derive\nfractional Brownian motion, we can introduce extensions with multifractal\nflavour. In another part of this paper, we discuss the use of the multi-scale\nentanglement renormalization ansatz (MERA), introduced in the study critical\nsystems in quantum spin lattices, as a method for sampling integrals with\nrespect to such multifractal processes. After proper calibration, a MERA\npromises the generation of a sample of size $N$ of a multifractal process in\nthe order of $O(N\\log(N))$, an improvement over the known methods, such as the\nCholesky decomposition and the circulant methods, which scale between $O(N^2)$\nand $O(N^3)$. \n\n"}
{"id": "1602.01672", "contents": "Title: The Generalised Isolation-With-Migration Model: a Maximum-Likelihood\n  Implementation for Multilocus Data Sets Abstract: Statistical inference about the speciation process has often been based on\nthe isolation-with-migration (IM) model, especially when the research aim is to\nlearn about the presence or absence of gene flow during divergence. The\ngeneralised IM model introduced in this paper extends both the standard\ntwo-population IM model and the isolation-with-initial-migration (IIM) model,\nand encompasses both these models as special cases. It can be described as a\ntwo-population IM model in which migration rates and population sizes are\nallowed to change at some point in the past. By developing a maximum-likelihood\nimplementation of this GIM model, we enable inference on both historical and\ncontemporary rates of gene flow between two closely related species. Our method\nrelies on the spectral decomposition of the coalescent generator matrix and is\napplicable to data sets consisting of the numbers of nucleotide differences\nbetween one pair of DNA sequences at each of a large number of independent\nloci. \n\n"}
{"id": "1602.02055", "contents": "Title: Bayesian aggregation of average data: An application in drug development Abstract: Throughout the different phases of a drug development program, randomized\ntrials are used to establish the tolerability, safety, and efficacy of a\ncandidate drug. At each stage one aims to optimize the design of future studies\nby extrapolation from the available evidence at the time. This includes\ncollected trial data and relevant external data. However, relevant external\ndata are typically available as averages only, for example from trials on\nalternative treatments reported in the literature. Here we report on such an\nexample from a drug development for wet age-related macular degeneration. This\ndisease is the leading cause of severe vision loss in the elderly. While\ncurrent treatment options are efficacious, they are also a substantial burden\nfor the patient. Hence, new treatments are under development which need to be\ncompared against existing treatments. The general statistical problem this\nleads to is meta-analysis, which addresses the question of how we can combine\ndatasets collected under different conditions. Bayesian methods have long been\nused to achieve partial pooling. Here we consider the challenge when the model\nof interest is complex (hierarchical and nonlinear) and one dataset is given as\nraw data while the second dataset is given as averages only. In such a\nsituation, common meta-analytic methods can only be applied when the model is\nsufficiently simple for analytic approaches. When the model is too complex, for\nexample nonlinear, an analytic approach is not possible. We provide a Bayesian\nsolution by using simulation to approximately reconstruct the likelihood of the\nexternal summary and allowing the parameters in the model to vary under the\ndifferent conditions. We first evaluate our approach using fake-data\nsimulations and then report results for the drug development program that\nmotivated this research. \n\n"}
{"id": "1602.02575", "contents": "Title: DECOrrelated feature space partitioning for distributed sparse\n  regression Abstract: Fitting statistical models is computationally challenging when the sample\nsize or the dimension of the dataset is huge. An attractive approach for\ndown-scaling the problem size is to first partition the dataset into subsets\nand then fit using distributed algorithms. The dataset can be partitioned\neither horizontally (in the sample space) or vertically (in the feature space).\nWhile the majority of the literature focuses on sample space partitioning,\nfeature space partitioning is more effective when $p\\gg n$. Existing methods\nfor partitioning features, however, are either vulnerable to high correlations\nor inefficient in reducing the model dimension. In this paper, we solve these\nproblems through a new embarrassingly parallel framework named DECO for\ndistributed variable selection and parameter estimation. In DECO, variables are\nfirst partitioned and allocated to $m$ distributed workers. The decorrelated\nsubset data within each worker are then fitted via any algorithm designed for\nhigh-dimensional problems. We show that by incorporating the decorrelation\nstep, DECO can achieve consistent variable selection and parameter estimation\non each subset with (almost) no assumptions. In addition, the convergence rate\nis nearly minimax optimal for both sparse and weakly sparse models and does NOT\ndepend on the partition number $m$. Extensive numerical experiments are\nprovided to illustrate the performance of the new framework. \n\n"}
{"id": "1602.03551", "contents": "Title: Knowledge Transfer with Medical Language Embeddings Abstract: Identifying relationships between concepts is a key aspect of scientific\nknowledge synthesis. Finding these links often requires a researcher to\nlaboriously search through scien- tific papers and databases, as the size of\nthese resources grows ever larger. In this paper we describe how distributional\nsemantics can be used to unify structured knowledge graphs with unstructured\ntext to predict new relationships between medical concepts, using a\nprobabilistic generative model. Our approach is also designed to ameliorate\ndata sparsity and scarcity issues in the medical domain, which make language\nmodelling more challenging. Specifically, we integrate the medical relational\ndatabase (SemMedDB) with text from electronic health records (EHRs) to perform\nknowledge graph completion. We further demonstrate the ability of our model to\npredict relationships between tokens not appearing in the relational database. \n\n"}
{"id": "1602.03940", "contents": "Title: A regional compound Poisson process for hurricane and tropical storm\n  damage Abstract: In light of intense hurricane activity along the U.S. Atlantic coast,\nattention has turned to understanding both the economic impact and behaviour of\nthese storms. The compound Poisson-lognormal process has been proposed as a\nmodel for aggregate storm damage, but does not shed light on regional analysis\nsince storm path data are not used. In this paper, we propose a fully Bayesian\nregional prediction model which uses conditional autoregressive (CAR) models to\naccount for both storm paths and spatial patterns for storm damage. When fitted\nto historical data, the analysis from our model both confirms previous findings\nand reveals new insights on regional storm tendencies. Posterior predictive\nsamples can also be used for pricing regional insurance premiums, which we\nillustrate using three different risk measures. \n\n"}
{"id": "1602.04003", "contents": "Title: Bayesian smoothing of dipoles in Magneto-/Electro-encephalography Abstract: We describe a novel method for dynamic estimation of multi-dipole states from\nMagneto/Electro-encephalography (M/EEG) time series. The new approach builds on\nthe recent development of particle filters for M/EEG; these algorithms\napproximate, with samples and weights, the posterior distribution of the neural\nsources at time t given the data up to time t. However, for off-line inference\npurposes it is preferable to work with the smoothing distribution, i.e. the\ndistribution for the neural sources at time t conditioned on the whole time\nseries. In this study, we use a Monte Carlo algorithm to approximate the\nsmoothing distribution for a time-varying set of current dipoles. We show,\nusing numerical simulations, that the estimates provided by the smoothing\ndistribution are more accurate than those provided by the filtering\ndistribution, particularly at the appearance of the source. We validate the\nproposed algorithm using an experimental dataset recorded from an epileptic\npatient. Improved localization of the source onset can be particularly relevant\nin source modeling of epileptic patients, where the source onset brings\ninformation on the epileptogenic zone. \n\n"}
{"id": "1602.04391", "contents": "Title: Constrained Multi-Slot Optimization for Ranking Recommendations Abstract: Ranking items to be recommended to users is one of the main problems in large\nscale social media applications. This problem can be set up as a\nmulti-objective optimization problem to allow for trading off multiple,\npotentially conflicting objectives (that are driven by those items) against\neach other. Most previous approaches to this problem optimize for a single slot\nwithout considering the interaction effect of these items on one another.\n  In this paper, we develop a constrained multi-slot optimization formulation,\nwhich allows for modeling interactions among the items on the different slots.\nWe characterize the solution in terms of problem parameters and identify\nconditions under which an efficient solution is possible. The problem\nformulation results in a quadratically constrained quadratic program (QCQP). We\nprovide an algorithm that gives us an efficient solution by relaxing the\nconstraints of the QCQP minimally. Through simulated experiments, we show the\nbenefits of modeling interactions in a multi-slot ranking context, and the\nspeed and accuracy of our QCQP approximate solver against other state of the\nart methods. \n\n"}
{"id": "1602.04805", "contents": "Title: DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression Abstract: Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems. \n\n"}
{"id": "1602.05642", "contents": "Title: When the Filter Bubble Bursts: Collective Evaluation Dynamics in Online\n  Communities Abstract: We analyze online collective evaluation processes through positive and\nnegative votes in various social media. We find two modes of collective\nevaluations that stem from the existence of filter bubbles. Above a threshold\nof collective attention, negativity grows faster with positivity, as a sign of\nthe burst of a filter bubble when information reaches beyond the local social\ncontext of a user. We analyze how collectively evaluated content can reach\nlarge social contexts and create polarization, showing that emotions expressed\nthrough text play a key role in collective evaluation processes. \n\n"}
{"id": "1602.07754", "contents": "Title: A Compressed Sensing Based Decomposition of Electrodermal Activity\n  Signals Abstract: The measurement and analysis of Electrodermal Activity (EDA) offers\napplications in diverse areas ranging from market research, to seizure\ndetection, to human stress analysis. Unfortunately, the analysis of EDA signals\nis made difficult by the superposition of numerous components which can obscure\nthe signal information related to a user's response to a stimulus. We show how\nsimple pre-processing followed by a novel compressed sensing based\ndecomposition can mitigate the effects of the undesired noise components and\nhelp reveal the underlying physiological signal. The proposed framework allows\nfor decomposition of EDA signals with provable bounds on the recovery of user\nresponses. We test our procedure on both synthetic and real-world EDA signals\nfrom wearable sensors and demonstrate that our approach allows for more\naccurate recovery of user responses as compared to the existing techniques. \n\n"}
{"id": "1603.04929", "contents": "Title: Statistical Inference Abstract: What is Statistics? Opinions vary. In fact, there is a continuous spectrum of\nattitudes toward statistics ranging from pure theoreticians, proving asymptotic\nefficiency and searching for most powerful tests, to wild practitioners,\nblindly reporting p-values and claiming statistical significance for\nscientifically insignificant results. In these notes statistics is viewed as a\nbranch of mathematical engineering, that studies ways of extracting reliable\ninformation from limited data for learning, prediction, and decision making in\nthe presence of uncertainty. These ACM lecture notes are based on the courses\nthe author taught at the University of Southern California in 2012 and 2013,\nand at the California Institute of Technology in 2016. \n\n"}
{"id": "1603.05038", "contents": "Title: CoinCalc -- A new R package for quantifying simultaneities of event\n  series Abstract: We present the new R package CoinCalc for performing event coincidence\nanalysis (ECA), a novel statistical method to quantify the simultaneity of\nevents contained in two series of observations, either as simultaneous or\nlagged coincidences within a user-specific temporal tolerance window. The\npackage also provides different analytical as well as surrogate-based\nsignificance tests (valid under different assumptions about the nature of the\nobserved event series) as well as an intuitive visualization of the identified\ncoincidences. We demonstrate the usage of CoinCalc based on two typical\ngeoscientific example problems addressing the relationship between\nmeteorological extremes and plant phenology as well as that between soil\nproperties and land cover. \n\n"}
{"id": "1603.05583", "contents": "Title: Analyzing In-Game Movements of Soccer Players at Scale Abstract: It is challenging to get access to datasets related to the physical\nperformance of soccer players. The teams consider such information highly\nconfidential, especially if it covers in-game performance.Hence, most of the\nanalysis and evaluation of the players' performance do not contain much\ninformation on the physical aspect of the game, creating a blindspot in\nperformance analysis. We propose a novel method to solve this issue by deriving\nmovement characteristics of soccer players. We use event-based datasets from\ndata provider companies covering 50+ soccer leagues allowing us to analyze the\nmovement profiles of potentially tens of thousands of players without any major\ninvestment. Our methodology does not require expensive, dedicated player\ntracking system deployed in the stadium. We also compute the similarity of the\nplayers based on their movement characteristics and as such identify potential\ncandidates who may be able to replace a given player. Finally, we quantify the\nuniqueness and consistency of players in terms of their in-game movements. Our\nstudy is the first of its kind that focuses on the movements of soccer players\nat scale, while it derives novel, actionable insights for the soccer industry\nfrom event-based datasets. \n\n"}
{"id": "1603.06358", "contents": "Title: Bayesian inference for multiple Gaussian graphical models with\n  application to metabolic association networks Abstract: We investigate the effect of cadmium (a toxic environmental pollutant) on the\ncorrelation structure of a number of urinary metabolites using Gaussian\ngraphical models (GGMs). The inferred metabolic associations can provide\nimportant information on the physiological state of a metabolic system and\ninsights on complex metabolic relationships. Using the fitted GGMs, we\nconstruct differential networks, which highlight significant changes in\nmetabolite interactions under different experimental conditions. The analysis\nof such metabolic association networks can reveal differences in the underlying\nbiological reactions caused by cadmium exposure. We consider Bayesian inference\nand propose using the multiplicative (or Chung-Lu random graph) model as a\nprior on the graphical space. In the multiplicative model, each edge is chosen\nindependently with probability equal to the product of the connectivities of\nthe end nodes. This class of prior is parsimonious yet highly flexible; it can\nbe used to encourage sparsity or graphs with a pre-specified degree\ndistribution when such prior knowledge is available. We extend the\nmultiplicative model to multiple GGMs linking the probability of edge inclusion\nthrough logistic regression and demonstrate how this leads to joint inference\nfor multiple GGMs. A sequential Monte Carlo (SMC) algorithm is developed for\nestimating the posterior distribution of the graphs. \n\n"}
{"id": "1603.06619", "contents": "Title: Multivariate peaks over thresholds models Abstract: Multivariate peaks over thresholds modeling based on generalized Pareto\ndistributions has up to now only been used in few and mostly 2-dimensional\nsituations. This paper contributes theoretical understanding, physically based\nmodels, inference tools, and simulation methods to support routine use, with an\naim at higher dimensions. We derive a general point process model for extreme\nepisodes in data, and show how conditioning the distribution of extreme\nepisodes on threshold exceedance gives four basic representations of the family\nof generalized Pareto distributions. The first representation is constructed on\nthe real scale of the observations. The second one starts with a model on a\nstandard exponential scale which then is transformed to the real scale. The\nthird and fourth are reformulations of a spectral representation proposed in A.\nFerreira and L. de Haan [Bernoulli 20 (2014) 1717--1737]. Numerically tractable\nforms of densities and censored densities are found and give tools for flexible\nparametric likelihood inference. New simulation algorithms, explicit formulas\nfor probabilities and conditional probabilities, and conditions which make the\nconditional distribution of weighted component sums generalized Pareto are\nderived. \n\n"}
{"id": "1603.07117", "contents": "Title: A Proximal Point Algorithm for Minimum Divergence Estimators with\n  Application to Mixture Models Abstract: Estimators derived from a divergence criterion such as $\\varphi-$divergences\nare generally more robust than the maximum likelihood ones. We are interested\nin particular in the so-called MD$\\varphi$DE, an estimator built using a dual\nrepresentation of $\\varphi$--divergences. We present in this paper an iterative\nproximal point algorithm which permits to calculate such estimator. This\nalgorithm contains by its construction the well-known EM algorithm. Our work is\nbased on the paper of \\citep{Tseng} on the likelihood function. We provide\nseveral convergence properties of the sequence generated by the algorithm, and\nimprove the existing results by relaxing the identifiability condition on the\nproximal term, a condition which is not verified for most mixture models and\nhard to be verified for non mixture ones. Since convergence analysis uses\nregularity conditions (continuity and differentiability) of the objective\nfunction, which has a supremal form, we find it useful to present some\nanalytical approaches for studying such functions. Convergence of the EM\nalgorithm is discussed here again in a Gaussian and Weibull mixtures in the\nspirit of our approach. Simulations are provided to confirm the validity of our\nwork and the robustness of the resulting estimators against outliers. \n\n"}
{"id": "1603.07511", "contents": "Title: Statistical modelling of individual animal movement: an overview of key\n  methods and a discussion of practical challenges Abstract: With the influx of complex and detailed tracking data gathered from\nelectronic tracking devices, the analysis of animal movement data has recently\nemerged as a cottage industry amongst biostatisticians. New approaches of ever\ngreater complexity are continue to be added to the literature. In this paper,\nwe review what we believe to be some of the most popular and most useful\nclasses of statistical models used to analyze individual animal movement data.\nSpecifically we consider discrete-time hidden Markov models, more general\nstate-space models and diffusion processes. We argue that these models should\nbe core components in the toolbox for quantitative researchers working on\nstochastic modelling of individual animal movement. The paper concludes by\noffering some general observations on the direction of statistical analysis of\nanimal movement. There is a trend in movement ecology toward what are arguably\noverly-complex modelling approaches which are inaccessible to ecologists,\nunwieldy with large data sets or not based in mainstream statistical practice.\nAdditionally, some analysis methods developed within the ecological community\nignore fundamental properties of movement data, potentially leading to\nmisleading conclusions about animal movement. Corresponding approaches, e.g.\nbased on L\\'evy walk-type models, continue to be popular despite having been\nlargely discredited. We contend that there is a need for an appropriate balance\nbetween the extremes of either being overly complex or being overly simplistic,\nwhereby the discipline relies on models of intermediate complexity that are\nusable by general ecologists, but grounded in well-developed statistical\npractice and efficient to fit to large data sets. \n\n"}
{"id": "1603.07630", "contents": "Title: A Spatially-Varying Stochastic Differential Equation Model for Animal\n  Movement Abstract: Animal movement exhibits complex behavior which can be influenced by\nunobserved environmental conditions. We propose a model which allows for a\nspatially-varying movement rate and spatially-varying drift through a\nsemiparametric potential surface and a separate motility surface. These\nsurfaces are embedded in a stochastic differential equation framework which\nallows for complex animal movement patterns in space. The resulting model is\nused to analyze the spatially-varying behavior of ants to provide insight into\nthe spatial structure of ant movement in the nest. \n\n"}
{"id": "1603.07749", "contents": "Title: Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High\n  Dimensional Mediators Abstract: In many scientific studies, it becomes increasingly important to delineate\nthe causal pathways through a large number of mediators, such as genetic and\nbrain mediators. Structural equation modeling (SEM) is a popular technique to\nestimate the pathway effects, commonly expressed as products of coefficients.\nHowever, it becomes unstable to fit such models with high dimensional\nmediators, especially for a general setting where all the mediators are\ncausally dependent but the exact causal relationships between them are unknown.\nThis paper proposes a sparse mediation model using a regularized SEM approach,\nwhere sparsity here means that a small number of mediators have nonzero\nmediation effects between a treatment and an outcome. To address the model\nselection challenge, we innovate by introducing a new penalty called Pathway\nLasso. This penalty function is a convex relaxation of the non-convex product\nfunction, and it enables a computationally tractable optimization criterion to\nestimate and select many pathway effects simultaneously. We develop a fast\nADMM-type algorithm to compute the model parameters, and we show that the\niterative updates can be expressed in closed form. On both simulated data and a\nreal fMRI dataset, the proposed approach yields higher pathway selection\naccuracy and lower estimation bias than other competing methods. \n\n"}
{"id": "1603.07822", "contents": "Title: On clustering financial time series: a need for distances between\n  dependent random variables Abstract: The following working document summarizes our work on the clustering of\nfinancial time series. It was written for a workshop on information geometry\nand its application for image and signal processing. This workshop brought\nseveral experts in pure and applied mathematics together with applied\nresearchers from medical imaging, radar signal processing and finance. The\nauthors belong to the latter group. This document was written as a long\nintroduction to further development of geometric tools in financial\napplications such as risk or portfolio analysis. Indeed, risk and portfolio\nanalysis essentially rely on covariance matrices. Besides that the Gaussian\nassumption is known to be inaccurate, covariance matrices are difficult to\nestimate from empirical data. To filter noise from the empirical estimate,\nMantegna proposed using hierarchical clustering. In this work, we first show\nthat this procedure is statistically consistent. Then, we propose to use\nclustering with a much broader application than the filtering of empirical\ncovariance matrices from the estimate correlation coefficients. To be able to\ndo that, we need to obtain distances between the financial time series that\nincorporate all the available information in these cross-dependent random\nprocesses. \n\n"}
{"id": "1603.09000", "contents": "Title: Online Rules for Control of False Discovery Rate and False Discovery\n  Exceedance Abstract: Multiple hypothesis testing is a core problem in statistical inference and\narises in almost every scientific field. Given a set of null hypotheses\n$\\mathcal{H}(n) = (H_1,\\dotsc, H_n)$, Benjamini and Hochberg introduced the\nfalse discovery rate (FDR), which is the expected proportion of false positives\namong rejected null hypotheses, and proposed a testing procedure that controls\nFDR below a pre-assigned significance level. Nowadays FDR is the criterion of\nchoice for large scale multiple hypothesis testing. In this paper we consider\nthe problem of controlling FDR in an \"online manner\". Concretely, we consider\nan ordered --possibly infinite-- sequence of null hypotheses $\\mathcal{H} =\n(H_1,H_2,H_3,\\dots )$ where, at each step $i$, the statistician must decide\nwhether to reject hypothesis $H_i$ having access only to the previous\ndecisions. This model was introduced by Foster and Stine. We study a class of\n\"generalized alpha-investing\" procedures and prove that any rule in this class\ncontrols online FDR, provided $p$-values corresponding to true nulls are\nindependent from the other $p$-values. (Earlier work only established mFDR\ncontrol.) Next, we obtain conditions under which generalized alpha-investing\ncontrols FDR in the presence of general $p$-values dependencies. Finally, we\ndevelop a modified set of procedures that also allow to control the false\ndiscovery exceedance (the tail of the proportion of false discoveries).\nNumerical simulations and analytical results indicate that online procedures do\nnot incur a large loss in statistical power with respect to offline approaches,\nsuch as Benjamini-Hochberg. \n\n"}
{"id": "1604.00954", "contents": "Title: Inference on the tail process with application to financial time series\n  modelling Abstract: To draw inference on serial extremal dependence within heavy-tailed Markov\nchains, Drees, Segers and Warcho{\\l} [Extremes (2015) 18, 369--402] proposed\nnonparametric estimators of the spectral tail process. The methodology can be\nextended to the more general setting of a stationary, regularly varying time\nseries. The large-sample distribution of the estimators is derived via\nempirical process theory for cluster functionals. The finite-sample performance\nof these estimators is evaluated via Monte Carlo simulations. Moreover, two\ndifferent bootstrap schemes are employed which yield confidence intervals for\nthe pre-asymptotic spectral tail process: the stationary bootstrap and the\nmultiplier block bootstrap. The estimators are applied to stock price data to\nstudy the persistence of positive and negative shocks. \n\n"}
{"id": "1604.01055", "contents": "Title: Towards personalized causal inference of medication response in mobile\n  health: an instrumental variable approach for randomized trials with\n  imperfect compliance Abstract: Mobile health studies can leverage longitudinal sensor data from smartphones\nto guide the application of personalized medical interventions. In this paper,\nwe propose that adoption of an instrumental variable approach for randomized\ntrials with imperfect compliance provides a natural framework for personalized\ncausal inference of medication response in mobile health studies. Randomized\ntreatment suggestions can be easily delivered to the study participants via\nelectronic messages popping up on the smart-phone screen. Under quite general\nassumptions we can identify the causal effect of the actual treatment on the\nresponse in the presence of unobserved confounders. We implement a personalized\nrandomization test of the null hypothesis of no causal effect of treatment on\nresponse, and evaluate its performance in a large scale simulation study\nencompassing data generated from linear and non-linear time series models under\nseveral simulation conditions. In particular, we evaluate the empirical power\nof the proposed test under varying degrees of compliance between the suggested\nand actual treatment adopted by the participant. Our investigations provide\nencouraging results in terms of power and control of type I error rates.\nFinally, we compare the proposed instrumental variable approach to a simple\nintent-to-treat strategy, and develop randomization confidence intervals for\nthe causal effects. \n\n"}
{"id": "1604.01443", "contents": "Title: Analysis of distributional variation through multi-scale Beta-Binomial\n  modeling Abstract: Many statistical analyses involve the comparison of multiple data sets\ncollected under different conditions in order to identify the difference in the\nunderlying distributions. A common challenge in multi-sample comparison is the\npresence of various confounders, or extraneous causes other than the conditions\nof interest that also contribute to the difference across the distributions.\nThey result in false findings, i.e., identified differences that are not\nreplicable in follow-up investigations. We consider an ANOVA approach to\naddressing this issue in multi-sample comparison---by collecting replicate data\nsets under each condition, thereby allowing the identification of the\ninteresting distributional variation from the extraneous ones. We introduce a\nmulti-scale Bayesian hierarchical model for the analysis of distributional\nvariation (ANDOVA) under this design, based on a collection of Beta-Binomial\ntests targeting variations of different scales at different locations across\nthe sample space. Instead treating the tests independently, the model employs a\ngraphical structure to introduce dependency among the individual tests thereby\nallowing borrowing of strength among them. We derive efficient inference recipe\nthrough a combination of numerical integration and message passing, and\nevaluate the ability of our method to effectively address ANDOVA through\nextensive simulation. We utilize our method to analyze a DNase-seq data set for\nidentifying differences in transcriptional factor binding. \n\n"}
{"id": "1604.01544", "contents": "Title: Day of the week effect in paper submission/acceptance/rejection to/in/by\n  peer review journals Abstract: This paper aims at providing an introduction to the behavior of authors\nsubmitting a paper to a scientific journal. Dates of electronic submission of\npapers to the Journal of the Serbian Chemical Society have been recorded from\nthe 1st January 2013 till the 31st December 2014, thus over 2 years.\n  There is no Monday or Friday effect like in financial markets, but rather a\nTuesday-Wednesday effect occurs: papers are more often submitted on Wednesday;\nhowever, the relative number of going to be accepted papers is larger if these\nare submitted on Tuesday. On the other hand, weekend days (Saturday and Sunday)\nare not the best days to finalize and submit manuscripts. An interpretation\nbased on the type of submitted work (\"experimental chemistry\") and on the\ninfluence of (senior) coauthors is presented. A thermodynamic connection is\nproposed within an entropy context. A (new) entropic distance is defined in\norder to measure the \"opaqueness\" = disorder) of the submission process. \n\n"}
{"id": "1604.02634", "contents": "Title: Online Nonnegative Matrix Factorization with Outliers Abstract: We propose a unified and systematic framework for performing online\nnonnegative matrix factorization in the presence of outliers. Our framework is\nparticularly suited to large-scale data. We propose two solvers based on\nprojected gradient descent and the alternating direction method of multipliers.\nWe prove that the sequence of objective values converges almost surely by\nappealing to the quasi-martingale convergence theorem. We also show the\nsequence of learned dictionaries converges to the set of stationary points of\nthe expected loss function almost surely. In addition, we extend our basic\nproblem formulation to various settings with different constraints and\nregularizers. We also adapt the solvers and analyses to each setting. We\nperform extensive experiments on both synthetic and real datasets. These\nexperiments demonstrate the computational efficiency and efficacy of our\nalgorithms on tasks such as (parts-based) basis learning, image denoising,\nshadow removal and foreground-background separation. \n\n"}
{"id": "1604.03611", "contents": "Title: Sequential change-point detection based on nearest neighbors Abstract: We propose a new framework for the detection of change-points in online,\nsequential data analysis. The approach utilizes nearest neighbor information\nand can be applied to sequences of multivariate observations or non-Euclidean\ndata objects, such as network data. Different stopping rules are explored, and\none specific rule is recommended due to its desirable properties. An accurate\nanalytic approximation of the average run length is derived for the recommended\nrule, making it an easy off-the-shelf approach for real multivariate/object\nsequential data monitoring applications. Simulations reveal that the new\napproach has better performance than likelihood-based approaches for high\ndimensional data. The new approach is illustrated through a real dataset in\ndetecting global structural changes in social networks. \n\n"}
{"id": "1604.05224", "contents": "Title: BFDA: A Matlab Toolbox for Bayesian Functional Data Analysis Abstract: We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical\nmodel to smooth multiple functional data with the assumptions of the same\nunderlying Gaussian process distribution, a Gaussian process prior for the mean\nfunction, and an Inverse-Wishart process prior for the covariance function.\nThis model-based approach can borrow strength from all functional data to\nincrease the smoothing accuracy, as well as estimate the mean-covariance\nfunctions simultaneously. An option of approximating the Bayesian inference\nprocess using cubic B-spline basis functions is integrated in BFDA, which\nallows for efficiently dealing with high-dimensional functional data. Examples\nof using BFDA in various scenarios and conducting follow-up functional\nregression are provided. The advantages of BFDA include: (1) Simultaneously\nsmooths multiple functional data and estimates the mean-covariance functions in\na nonparametric way; (2) flexibly deals with sparse and high-dimensional\nfunctional data with stationary and nonstationary covariance functions, and\nwithout the requirement of common observation grids; (3) provides accurately\nsmoothed functional data for follow-up analysis. \n\n"}
{"id": "1604.06003", "contents": "Title: Shape constrained kernel-weighted least squares: Application to\n  production function estimation for Chilean manufacturing industries Abstract: In this paper we examine a novel way of imposing shape constraints on a local\npolynomial kernel estimator. The proposed approach is referred to as Shape\nConstrained Kernel-weighted Least Squares (SCKLS). We prove uniform consistency\nof the SCKLS estimator with monotonicity and convexity/concavity constraints\nand establish its convergence rate. The competitiveness of SCKLS is shown in a\ncomprehensive simulation study. Finally, we analyze Chilean manufacturing data\nusing the SCKLS estimator and quantify production in the plastics and wood\nindustries. The results show that exporting firms have significantly higher\nproductivity. \n\n"}
{"id": "1604.07093", "contents": "Title: Semi-supervised Vocabulary-informed Learning Abstract: Despite significant progress in object categorization, in recent years, a\nnumber of important challenges remain, mainly, ability to learn from limited\nlabeled data and ability to recognize object classes within large, potentially\nopen, set of labels. Zero-shot learning is one way of addressing these\nchallenges, but it has only been shown to work with limited sized class\nvocabularies and typically requires separation between supervised and\nunsupervised classes, allowing former to inform the latter but not vice versa.\nWe propose the notion of semi-supervised vocabulary-informed learning to\nalleviate the above mentioned challenges and address problems of supervised,\nzero-shot and open set recognition using a unified framework. Specifically, we\npropose a maximum margin framework for semantic manifold-based recognition that\nincorporates distance constraints from (both supervised and unsupervised)\nvocabulary atoms, ensuring that labeled samples are projected closest to their\ncorrect prototypes, in the embedding space, than to others. We show that\nresulting model shows improvements in supervised, zero-shot, and large open set\nrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets. \n\n"}
{"id": "1604.07125", "contents": "Title: Approximate Residual Balancing: De-Biased Inference of Average Treatment\n  Effects in High Dimensions Abstract: There are many settings where researchers are interested in estimating\naverage treatment effects and are willing to rely on the unconfoundedness\nassumption, which requires that the treatment assignment be as good as random\nconditional on pre-treatment variables. The unconfoundedness assumption is\noften more plausible if a large number of pre-treatment variables are included\nin the analysis, but this can worsen the performance of standard approaches to\ntreatment effect estimation. In this paper, we develop a method for de-biasing\npenalized regression adjustments to allow sparse regression methods like the\nlasso to be used for sqrt{n}-consistent inference of average treatment effects\nin high-dimensional linear models. Given linearity, we do not need to assume\nthat the treatment propensities are estimable, or that the average treatment\neffect is a sparse contrast of the outcome model parameters. Rather, in\naddition standard assumptions used to make lasso regression on the outcome\nmodel consistent under 1-norm error, we only require overlap, i.e., that the\npropensity score be uniformly bounded away from 0 and 1. Procedurally, our\nmethod combines balancing weights with a regularized regression adjustment. \n\n"}
{"id": "1604.07451", "contents": "Title: Learning Local Dependence In Ordered Data Abstract: In many applications, data come with a natural ordering. This ordering can\noften induce local dependence among nearby variables. However, in complex data,\nthe width of this dependence may vary, making simple assumptions such as a\nconstant neighborhood size unrealistic. We propose a framework for learning\nthis local dependence based on estimating the inverse of the Cholesky factor of\nthe covariance matrix. Penalized maximum likelihood estimation of this matrix\nyields a simple regression interpretation for local dependence in which\nvariables are predicted by their neighbors. Our proposed method involves\nsolving a convex, penalized Gaussian likelihood problem with a hierarchical\ngroup lasso penalty. The problem decomposes into independent subproblems which\ncan be solved efficiently in parallel using first-order methods. Our method\nyields a sparse, symmetric, positive definite estimator of the precision\nmatrix, encoding a Gaussian graphical model. We derive theoretical results not\nfound in existing methods attaining this structure. In particular, our\nconditions for signed support recovery and estimation consistency rates in\nmultiple norms are as mild as those in a regression problem. Empirical results\nshow our method performing favorably compared to existing methods. We apply our\nmethod to genomic data to flexibly model linkage disequilibrium. Our method is\nalso applied to improve the performance of discriminant analysis in sound\nrecording classification. \n\n"}
{"id": "1604.08320", "contents": "Title: Sequential Bayesian optimal experimental design via approximate dynamic\n  programming Abstract: The design of multiple experiments is commonly undertaken via suboptimal\nstrategies, such as batch (open-loop) design that omits feedback or greedy\n(myopic) design that does not account for future effects. This paper introduces\nnew strategies for the optimal design of sequential experiments. First, we\nrigorously formulate the general sequential optimal experimental design (sOED)\nproblem as a dynamic program. Batch and greedy designs are shown to result from\nspecial cases of this formulation. We then focus on sOED for parameter\ninference, adopting a Bayesian formulation with an information theoretic design\nobjective. To make the problem tractable, we develop new numerical approaches\nfor nonlinear design with continuous parameter, design, and observation spaces.\nWe approximate the optimal policy by using backward induction with regression\nto construct and refine value function approximations in the dynamic program.\nThe proposed algorithm iteratively generates trajectories via exploration and\nexploitation to improve approximation accuracy in frequently visited regions of\nthe state space. Numerical results are verified against analytical solutions in\na linear-Gaussian setting. Advantages over batch and greedy design are then\ndemonstrated on a nonlinear source inversion problem where we seek an optimal\npolicy for sequential sensing. \n\n"}
{"id": "1605.01311", "contents": "Title: Visualizing Count Data Regressions Using Rootograms Abstract: The rootogram is a graphical tool associated with the work of J. W. Tukey\nthat was originally used for assessing goodness of fit of univariate\ndistributions. Here we extend the rootogram to regression models and show that\nthis is particularly useful for diagnosing and treating issues such as\noverdispersion and/or excess zeros in count data models. We also introduce a\nweighted version of the rootogram that can be applied out of sample or to\n(weighted) subsets of the data, e.g., in finite mixture models. An empirical\nillustration revisiting a well-known data set from ethology is included, for\nwhich a negative binomial hurdle model is employed. Supplementary materials\nproviding two further illustrations are available online: the first, using data\nfrom public health, employs a two-component finite mixture of negative binomial\nmodels, the second, using data from finance, involves underdispersion. An R\nimplementation of our tools is available in the R package countreg. It also\ncontains the data and replication code. \n\n"}
{"id": "1605.01333", "contents": "Title: Minimax Estimation of the Volume of a Set with Smooth Boundary Abstract: We consider the problem of estimating the volume of a compact domain in a\nEuclidean space based on a uniform sample from the domain. We assume the domain\nhas a boundary with positive reach. We propose a data splitting approach to\ncorrect the bias of the plug-in estimator based on the sample alpha-convex\nhull. We show that this simple estimator achieves a minimax lower bound that we\nderive. Some numerical experiments corroborate our theoretical findings. \n\n"}
{"id": "1605.03872", "contents": "Title: Discovering Effect Modification in an Observational Study of Surgical\n  Mortality at Hospitals with Superior Nursing Abstract: There is effect modification if the magnitude or stability of a treatment\neffect varies systematically with the level of an observed covariate. A larger\nor more stable treatment effect is typically less sensitive to bias from\nunmeasured covariates, so it is important to recognize effect modification when\nit is present. We illustrate a recent proposal for conducting a sensitivity\nanalysis that empirically discovers effect modification by exploratory methods,\nbut controls the family-wise error rate in discovered groups. The example\nconcerns a study of mortality and use of the intensive care unit in 23,715\nmatched pairs of two Medicare patients, one of whom underwent surgery at a\nhospital identified for superior nursing, the other at a conventional hospital.\nThe pairs were matched exactly for 130 four-digit ICD-9 surgical procedure\ncodes and balanced 172 observed covariates. The pairs were then split into five\ngroups of pairs by CART in its effort to locate effect modification. The\nevidence of a beneficial effect of magnet hospitals on mortality is least\nsensitive to unmeasured biases in a large group of patients undergoing rather\nserious surgical procedures, but in the absence of other life-threatening\nconditions, such as a comorbidity of congestive heart failure or an emergency\nadmission leading to surgery. \n\n"}
{"id": "1605.04049", "contents": "Title: Modeling and detecting change in temporal networks via a dynamic degree\n  corrected stochastic block model Abstract: In many applications it is of interest to identify anomalous behavior within\na dynamic interacting system. Such anomalous interactions are reflected by\nstructural changes in the network representation of the system. We propose and\ninvestigate the use of a dynamic version of the degree corrected stochastic\nblock model (DCSBM) to model and monitor dynamic networks that undergo a\nsignificant structural change. We apply statistical process monitoring\ntechniques to the estimated parameters of the DCSBM to identify significant\nstructural changes in the network. Application of our surveillance strategy to\nthe dynamic U.S. Senate co-voting network reveals that we are able to detect\nsignificant changes in the network that reflect both times of cohesion and\ntimes of polarization among Republican and Democratic party members. These\nfindings provide valuable insight about the evolution of the bipartisan\npolitical system in the United States. Our analysis demonstrates that the\ndynamic DCSBM monitoring procedure effectively detects local and global\nstructural changes in dynamic networks. The DCSBM approach is an example of a\nmore general framework that combines parametric random graph models and\nstatistical process monitoring techniques for network surveillance. \n\n"}
{"id": "1605.04772", "contents": "Title: A Note on Efficient Performance Evaluation of the Cumulative Sum Chart\n  and the Sequential Probability Ratio Test Abstract: We establish a simple connection between certain in-control characteristics\nof the CUSUM Run Length and their out-of-control counterparts. The connection\nis in the form of paired integral (renewal) equations. The derivation exploits\nWald's likelihood ratio identity and the well-known fact that the CUSUM chart\nis equivalent to repetitive application of Wald's SPRT. The characteristics\nconsidered include the entire Run Length distribution and all of the\ncorresponding moments, starting from the zero-state ARL. A particular practical\nbenefit of our result is that it enables the in- and out-of-control\ncharacteristics of the CUSUM Run Length to be computed concurrently. Moreover,\ndue to the equivalence of the CUSUM chart to a sequence of SPRTs, the ASN and\nOC functions of an SPRT under the null and under the alternative can all be\ncomputed simultaneously as well. This would double up the efficiency of any\nnumerical method one may choose to devise to carry out the actual computations. \n\n"}
{"id": "1605.05476", "contents": "Title: Localizing the Ensemble Kalman Particle Filter Abstract: Ensemble methods such as the Ensemble Kalman Filter (EnKF) are widely used\nfor data assimilation in large-scale geophysical applications, as for example\nin numerical weather prediction (NWP). There is a growing interest for physical\nmodels with higher and higher resolution, which brings new challenges for data\nassimilation techniques because of the presence of non-linear and non-Gaussian\nfeatures that are not adequately treated by the EnKF. We propose two new\nlocalized algorithms based on the Ensemble Kalman Particle Filter (EnKPF), a\nhybrid method combining the EnKF and the Particle Filter (PF) in a way that\nmaintains scalability and sample diversity. Localization is a key element of\nthe success of EnKFs in practice, but it is much more challenging to apply to\nPFs. The algorithms that we introduce in the present paper provide a compromise\nbetween the EnKF and the PF while avoiding some of the problems of localization\nfor pure PFs. Numerical experiments with a simplified model of cumulus\nconvection based on a modified shallow water equation show that the proposed\nalgorithms perform better than the local EnKF. In particular, the PF nature of\nthe method allows to capture non-Gaussian characteristics of the estimated\nfields such as the location of wet and dry areas. \n\n"}
{"id": "1605.05779", "contents": "Title: Conditional analysis for mixed covariates, with application to feed\n  intake of lactating sows Abstract: We propose a novel modeling framework to study the effect of covariates of\nvarious types on the conditional distribution of the response. The methodology\naccommodates flexible model structure, allows for joint estimation of the\nquantiles at all levels, and involves a computationally efficient estimation\nalgorithm. Extensive numerical investigation confirms good performance of the\nproposed method. The methodology is motivated by and applied to a lactating sow\nstudy, where the primary interest is to understand how the dynamic change of\nminute-by-minute temperature in the farrowing rooms within a day (functional\ncovariate) is associated with low quantiles of feed intake of lactating sows,\nwhile accounting for other sow-specific information (vector covariate). \n\n"}
{"id": "1605.05910", "contents": "Title: A Frequency Domain Test for Propriety of Complex-Valued Vector Time\n  Series Abstract: This paper proposes a frequency domain approach to test the hypothesis that a\ncomplex-valued vector time series is proper, i.e., for testing whether the\nvector time series is uncorrelated with its complex conjugate. If the\nhypothesis is rejected, frequency bands causing the rejection will be\nidentified and might usefully be related to known properties of the physical\nprocesses. The test needs the associated spectral matrix which can be estimated\nby multitaper methods using, say, $K$ tapers. Standard asymptotic distributions\nfor the test statistic are of no use since they would require $K \\rightarrow\n\\infty,$ but, as $K$ increases so does resolution bandwidth which causes\nspectral blurring. In many analyses $K$ is necessarily kept small, and hence\nour efforts are directed at practical and accurate methodology for hypothesis\ntesting for small $K.$ Our generalized likelihood ratio statistic combined with\nexact cumulant matching gives very accurate rejection percentages and\noutperforms other methods. We also prove that the statistic on which the test\nis based is comprised of canonical coherencies arising from our complex-valued\nvector time series.Our methodology is demonstrated on ocean current data\ncollected at different depths in the Labrador Sea. Overall this work extends\nresults on propriety testing for complex-valued vectors to the complex-valued\nvector time series setting. \n\n"}
{"id": "1605.06459", "contents": "Title: Two-Qubit Separability Probabilities as Joint Functions of the Bloch\n  Radii of the Qubit Subsystems Abstract: We detect a certain pattern of behavior of separability probabilities\n$p(r_A,r_B)$ for two-qubit systems endowed with Hilbert-Schmidt, and more\ngenerally, random induced measures, where $r_A$ and $r_B$ are the Bloch radii\n($0 \\leq r_A,r_B \\leq 1$) of the qubit reduced states ($A,B$). We observe a\nrelative repulsion of radii effect, that is $p(r_A,r_A) < p(r_A,1-r_A)$, except\nfor rather narrow \"crossover\" intervals $[\\tilde{r}_A,\\frac{1}{2}]$. Among the\nseven specific cases we study are, firstly, the \"toy\" seven-dimensional\n$X$-states model and, then, the fifteen-dimensional two-qubit states obtained\nby tracing over the pure states in $4 \\times K$-dimensions, for $K=3, 4, 5$,\nwith $K=4$ corresponding to Hilbert-Schmidt (flat/Euclidean) measure. We also\nexamine the real (two-rebit) $K=4$, the $X$-states $K=5$, and Bures (minimal\nmonotone)--for which no nontrivial crossover behavior is observed--instances.\nIn the two $X$-states cases, we derive analytical results, for $K=3, 4$, we\npropose formulas that well-fit our numerical results, and for the other\nscenarios, rely presently upon large numerical analyses. The separability\nprobability crossover regions found expand in length (lower $\\tilde{r}_A$) as\n$K$ increases. This report continues our efforts (arXiv:1506.08739) to extend\nthe recent work of Milz and Strunz (J. Phys. A: 48 [2015] 035306) from a\nunivariate ($r_A$) framework---in which they found separability probabilities\nto hold constant with $r_A$---to a bivariate ($r_A,r_B$) one. We also analyze\nthe two-qutrit and qubit-qutrit counterparts reported in arXiv:1512.07210 in\nthis context, and study two-qubit separability probabilities of the form\n$p(r_A,\\frac{1}{2})$. \n\n"}
{"id": "1605.09107", "contents": "Title: Analysis of nonstationary modulated time series with applications to\n  oceanographic flow measurements Abstract: We propose a new class of univariate nonstationary time series models, using\nthe framework of modulated time series, which is appropriate for the analysis\nof rapidly-evolving time series as well as time series observations with\nmissing data. We extend our techniques to a class of bivariate time series that\nare isotropic. Exact inference is often not computationally viable for time\nseries analysis, and so we propose an estimation method based on the\nWhittle-likelihood, a commonly adopted pseudo-likelihood. Our inference\nprocedure is shown to be consistent under standard assumptions, as well as\nhaving considerably lower computational cost than exact likelihood in general.\nWe show the utility of this framework for the analysis of drifting instruments,\nan analysis that is key to characterising global ocean circulation and\ntherefore also for decadal to century-scale climate understanding. \n\n"}
{"id": "1606.00229", "contents": "Title: Uncertainty and filtering of hidden Markov models in discrete time Abstract: We consider the problem of filtering an unseen Markov chain from noisy\nobservations, in the presence of uncertainty regarding the parameters of the\nprocesses involved. Using the theory of nonlinear expectations, we describe the\nuncertainty in terms of a penalty function, which can be propagated forward in\ntime in the place of the filter. \n\n"}
{"id": "1606.02690", "contents": "Title: Integrative analysis of transcriptomic and metabolomic data via sparse\n  canonical correlation analysis with incorporation of biological information Abstract: Integrative analyses of different high dimensional data types are becoming\nincreasingly popular. Similarly, incorporating prior functional relationships\namong variables in data analysis has been a topic of increasing interest as it\nhelps elucidate underlying mechanisms among complex diseases. In this paper,\nthe goal is to assess association between transcriptomic and metabolomic data\nfrom a Predictive Health Institute (PHI) study including healthy adults at high\nrisk of developing cardiovascular diseases. To this end, we develop statistical\nmethods for identifying sparse structure in canonical correlation analysis\n(CCA) with incorporation of biological/structural information. Our proposed\nmethods use prior network structural information among genes and among\nmetabolites to guide selection of relevant genes and metabolites in sparse CCA,\nproviding insight on the molecular underpinning of cardiovascular disease. Our\nsimulations demonstrate that the structured sparse CCA methods outperform\nseveral existing sparse CCA methods in selecting relevant genes and metabolites\nwhen structural information is informative and are robust to mis-specified\nstructural information. Our analysis of the PHI study reveals that a number of\ngenes and metabolic pathways including some known to be associated with\ncardiovascular diseases are enriched in the subset of genes and metabolites\nselected by our proposed approach. \n\n"}
{"id": "1606.03295", "contents": "Title: Simultaneous inference for misaligned multivariate functional data Abstract: We consider inference for misaligned multivariate functional data that\nrepresents the same underlying curve, but where the functional samples have\nsystematic differences in shape. In this paper we introduce a new class of\ngenerally applicable models where warping effects are modeled through nonlinear\ntransformation of latent Gaussian variables and systematic shape differences\nare modeled by Gaussian processes. To model cross-covariance between sample\ncoordinates we introduce a class of low-dimensional cross-covariance structures\nsuitable for modeling multivariate functional data. We present a method for\ndoing maximum-likelihood estimation in the models and apply the method to three\ndata sets. The first data set is from a motion tracking system where the\nspatial positions of a large number of body-markers are tracked in\nthree-dimensions over time. The second data set consists of height and weight\nmeasurements for Danish boys. The third data set consists of three-dimensional\nspatial hand paths from a controlled obstacle-avoidance experiment. We use the\ndeveloped method to estimate the cross-covariance structure, and use a\nclassification setup to demonstrate that the method outperforms\nstate-of-the-art methods for handling misaligned curve data. \n\n"}
{"id": "1606.06328", "contents": "Title: Inferring Mobility Measures from GPS Traces with Missing Data Abstract: With increasing availability of smartphones with GPS capabilities,\nlarge-scale studies relating individual-level mobility patterns to a wide\nvariety of patient-centered outcomes, from mood disorders to surgical recovery,\nare becoming a reality. Similar past studies have been small in scale and have\nprovided wearable GPS devices to subjects. These devices typically collect\nmobility traces continuously without significant gaps in the data, and\nconsequently the problem of data missingness has been safely ignored.\nLeveraging subjects' own smartphones makes it possible to scale up and extend\nthe duration of these types of studies, but at the same time introduces a\nsubstantial challenge: to preserve a smartphone's battery, GPS can be active\nonly for a small portion of the time, frequently less than $10\\%$, leading to a\ntremendous missing data problem. We introduce a principled statistical\napproach, based on weighted resampling of the observed data, to impute the\nmissing mobility traces, which we then summarize using different mobility\nmeasures. We compare the strengths of our approach to linear interpolation, a\npopular approach for dealing with missing data, both analytically and through\nsimulation of missingness for empirical data. We conclude that our imputation\napproach better mirrors human mobility both theoretically and over a sample of\nGPS mobility traces from 182 individuals in the Geolife data set, where,\nrelative to linear interpolation, imputation resulted in a 10-fold reduction in\nthe error averaged across all mobility features. \n\n"}
{"id": "1606.06633", "contents": "Title: Multi-scale detection of variance changes in renewal processes in the\n  presence of rate change points Abstract: Non-stationarity of the rate or variance of events is a well-known problem in\nthe description and analysis of time series of events, such as neuronal spike\ntrains. A multiple filter test (MFT) for rate homogeneity has been proposed\nearlier that detects change points on multiple time scales simultaneously. It\nis based on a filtered derivative approach, and the rejection threshold derives\nfrom a Gaussian limit process $L$ which is independent of the point process\nparameters.\n  Here we extend the MFT to variance homogeneity of life times. When the rate\nis constant, the MFT extends directly to the null hypothesis of constant\nvariance. In the presence of rate change points, we propose to incorporate\nestimates of these in the test for variance homogeneity, using an adaptation of\nthe test statistic. The resulting limit process shows slight deviations from\n$L$ that depend on unknown process parameters. However, these deviations are\nsmall and do not considerably change the properties of the statistical test.\nThis allows practical application, e.g.~to neuronal spike trains, which\nindicates various profiles of rate and variance change points. \n\n"}
{"id": "1606.07153", "contents": "Title: Fast robustness quantification with variational Bayes Abstract: Bayesian hierarchical models are increasing popular in economics. When using\nhierarchical models, it is useful not only to calculate posterior expectations,\nbut also to measure the robustness of these expectations to reasonable\nalternative prior choices. We use variational Bayes and linear response methods\nto provide fast, accurate posterior means and robustness measures with an\napplication to measuring the effectiveness of microcredit in the developing\nworld. \n\n"}
{"id": "1606.07369", "contents": "Title: Personalized Prognostic Models for Oncology: A Machine Learning Approach Abstract: We have applied a little-known data transformation to subsets of the\nSurveillance, Epidemiology, and End Results (SEER) publically available data of\nthe National Cancer Institute (NCI) to make it suitable input to standard\nmachine learning classifiers. This transformation properly treats the\nright-censored data in the SEER data and the resulting Random Forest and\nMulti-Layer Perceptron models predict full survival curves. Treating the 6, 12,\nand 60 months points of the resulting survival curves as 3 binary classifiers,\nthe 18 resulting classifiers have AUC values ranging from .765 to .885. Further\nevidence that the models have generalized well from the training data is\nprovided by the extremely high levels of agreement between the random forest\nand neural network models predictions on the 6, 12, and 60 month binary\nclassifiers. \n\n"}
{"id": "1606.07855", "contents": "Title: Probabilistic Forecasting and Simulation of Electricity Markets via\n  Online Dictionary Learning Abstract: The problem of probabilistic forecasting and online simulation of real-time\nelectricity market with stochastic generation and demand is considered. By\nexploiting the parametric structure of the direct current optimal power flow, a\nnew technique based on online dictionary learning (ODL) is proposed. The ODL\napproach incorporates real-time measurements and historical traces to produce\nforecasts of joint and marginal probability distributions of future locational\nmarginal prices, power flows, and dispatch levels, conditional on the system\nstate at the time of forecasting. Compared with standard Monte Carlo simulation\ntechniques, the ODL approach offers several orders of magnitude improvement in\ncomputation time, making it feasible for online forecasting of market\noperations. Numerical simulations on large and moderate size power systems\nillustrate its performance and complexity features and its potential as a tool\nfor system operators. \n\n"}
{"id": "1606.09585", "contents": "Title: Hierarchical animal movement models for population-level inference Abstract: New methods for modeling animal movement based on telemetry data are\ndeveloped regularly. With advances in telemetry capabilities, animal movement\nmodels are becoming increasingly sophisticated. Despite a need for\npopulation-level inference, animal movement models are still predominantly\ndeveloped for individual-level inference. Most efforts to upscale the inference\nto the population-level are either post hoc or complicated enough that only the\ndeveloper can implement the model. Hierarchical Bayesian models provide an\nideal platform for the development of population-level animal movement models\nbut can be challenging to fit due to computational limitations or extensive\ntuning required. We propose a two-stage procedure for fitting hierarchical\nanimal movement models to telemetry data. The two-stage approach is\nstatistically rigorous and allows one to fit individual-level movement models\nseparately, then resample them using a secondary MCMC algorithm. The primary\nadvantages of the two-stage approach are that the first stage is easily\nparallelizable and the second stage is completely unsupervised, allowing for a\ncompletely automated fitting procedure in many cases. We demonstrate the\ntwo-stage procedure with two applications of animal movement models. The first\napplication involves a spatial point process approach to modeling telemetry\ndata and the second involves a more complicated continuous-time discrete-space\nanimal movement model. We fit these models to simulated data and real telemetry\ndata arising from a population of monitored Canada lynx in Colorado, USA. \n\n"}
{"id": "1607.03197", "contents": "Title: Semiparametric Estimation with Data Missing Not at Random Using an\n  Instrumental Variable Abstract: Missing data occur frequently in empirical studies in health and social\nsciences, often compromising our ability to make accurate inferences. An\noutcome is said to be missing not at random (MNAR) if, conditional on the\nobserved variables, the missing data mechanism still depends on the unobserved\noutcome. In such settings, identification is generally not possible without\nimposing additional assumptions. Identification is sometimes possible, however,\nif an instrumental variable (IV) is observed for all subjects which satisfies\nthe exclusion restriction that the IV affects the missingness process without\ndirectly influencing the outcome. In this paper, we provide necessary and\nsufficient conditions for nonparametric identification of the full data\ndistribution under MNAR with the aid of an IV. In addition, we give sufficient\nidentification conditions that are more straightforward to verify in practice.\nFor inference, we focus on estimation of a population outcome mean, for which\nwe develop a suite of semiparametric estimators that extend methods previously\ndeveloped for data missing at random. Specifically, we propose inverse\nprobability weighted estimation, outcome regression-based estimation and doubly\nrobust estimation of the mean of an outcome subject to MNAR. For illustration,\nthe methods are used to account for selection bias induced by HIV testing\nrefusal in the evaluation of HIV seroprevalence in Mochudi, Botswana, using\ninterviewer characteristics such as gender, age and years of experience as IVs. \n\n"}
{"id": "1607.03592", "contents": "Title: Cluster Sampling Filters for Non-Gaussian Data Assimilation Abstract: This paper presents a fully non-Gaussian version of the Hamiltonian Monte\nCarlo (HMC) sampling filter. The Gaussian prior assumption in the original HMC\nfilter is relaxed. Specifically, a clustering step is introduced after the\nforecast phase of the filter, and the prior density function is estimated by\nfitting a Gaussian Mixture Model (GMM) to the prior ensemble. Using the data\nlikelihood function, the posterior density is then formulated as a mixture\ndensity, and is sampled using a HMC approach (or any other scheme capable of\nsampling multimodal densities in high-dimensional subspaces). The main filter\ndeveloped herein is named \"cluster HMC sampling filter\" (ClHMC). A multi-chain\nversion of the ClHMC filter, namely MC-ClHMC is also proposed to guarantee that\nsamples are taken from the vicinities of all probability modes of the\nformulated posterior. The new methodologies are tested using a\nquasi-geostrophic (QG) model with double-gyre wind forcing and bi-harmonic\nfriction. Numerical results demonstrate the usefulness of using GMMs to relax\nthe Gaussian prior assumption in the HMC filtering paradigm. \n\n"}
{"id": "1607.03775", "contents": "Title: Causal inference to detect selection bias in road safety epidemiology Abstract: In the field of road safety, it is common to use responsibility analyses to\nassess the effect of a given factor on the risk of being responsible for an\naccident, among drivers involved in an accident only. Even if this design is\nnow widely adopted in the field, the question of selection bias is often\nraised. The structural Causal Model framework now provides valuable tools to\nassess causal effects from observational data and identify selection bias. In\nthis article, we briefly review recent results regarding the recoverability of\ncausal effects from selection biased data, and apply them to the case of\nresponsibility analyses. Our objective is to formally determine whether causal\neffects can be unbiasedly estimated through this type of analyses, when\navailable data are restricted to severe accidents, as it is commonly the case\nin practice. However, because speed has a direct effect on the severity of the\naccident, we show that causal odds-ratios are not estimable from responsibility\nanalyses. We present numerical results to illustrate our argument, the\nmagnitude of the bias and to discuss recent results from real data. \n\n"}
{"id": "1607.04424", "contents": "Title: Investigations of the effects of random sampling patterns on the\n  stability of generalized sampling Abstract: We investigate how the choice of spatial point process for generating random\nsampling patterns affects the numerical stability of non-uniform generalized\nsampling between Fourier bases and Daubechies scaling functions. Specifically,\nwe consider binomial, Poisson and determinantal point processes and demonstrate\nthat the more regular point patterns from the determinantal point process are\nsuperior. \n\n"}
{"id": "1607.06307", "contents": "Title: Estimating the unobservable moose - converting index to population size\n  using a Bayesian Hierarchical state space model Abstract: Indirect information on population size, like pellet counts or volunteer\ncounts, is the main source of information in most ecological studies and\napplied population management situations. Often, such observations are treaded\nas if they were actual measurements of population size. This assumption results\nin incorrect conclusions about a population's size and its dynamics. We propose\na model with a temporal varying link, denoted countability, between indirect\nobservations and actual population size. We show that, when indirect\nmeasurement has high precision (for instance many observation hours) the\nassumption of temporal varying countability can have a crucial effect on the\nestimated population dynamic. We apply the model on two local moose populations\nin Sweden. The estimated population dynamics is found to explain 30-50 percent\nof the total variability in the observation data; thus, countability accounts\nfor most of the variation. This unreliability of the estimated dynamics has a\nsubstantial negative impact on the ability to manage populations; for example,\nreducing (increasing) the number of animals that needs to be harvested in order\nto sustain the population above (below) a fixed level. Finally, large\ndifference in countability between two study areas implies a substantial\nspatial variation in the countability; this variation in itself is highly\nworthy of study. \n\n"}
{"id": "1607.06635", "contents": "Title: Density Estimation Trees as fast non-parametric modelling tools Abstract: Density Estimation Trees (DETs) are decision trees trained on a multivariate\ndataset to estimate its probability density function. While not competitive\nwith kernel techniques in terms of accuracy, they are incredibly fast,\nembarrassingly parallel and relatively small when stored to disk. These\nproperties make DETs appealing in the resource-expensive horizon of the LHC\ndata analysis. Possible applications may include selection optimization, fast\nsimulation and fast detector calibration. In this contribution I describe the\nalgorithm, made available to the HEP community in a RooFit implementation. A\nset of applications under discussion within the LHCb Collaboration are also\nbriefly illustrated. \n\n"}
{"id": "1608.00264", "contents": "Title: Frequency of Frequencies Distributions and Size Dependent Exchangeable\n  Random Partitions Abstract: Motivated by the fundamental problem of modeling the frequency of frequencies\n(FoF) distribution, this paper introduces the concept of a cluster structure to\ndefine a probability function that governs the joint distribution of a random\ncount and its exchangeable random partitions. A cluster structure, naturally\narising from a completely random measure mixed Poisson process, allows the\nprobability distribution of the random partitions of a subset of a population\nto be dependent on the population size, a distinct and motivated feature that\nmakes it more flexible than a partition structure. This allows it to model an\nentire FoF distribution whose structural properties change as the population\nsize varies. A FoF vector can be simulated by drawing an infinite number of\nPoisson random variables, or by a stick-breaking construction with a finite\nrandom number of steps. A generalized negative binomial process model is\nproposed to generate a cluster structure, where in the prior the number of\nclusters is finite and Poisson distributed, and the cluster sizes follow a\ntruncated negative binomial distribution. We propose a simple Gibbs sampling\nalgorithm to extrapolate the FoF vector of a population given the FoF vector of\na sample taken without replacement from the population. We illustrate our\nresults and demonstrate the advantages of the proposed models through the\nanalysis of real text, genomic, and survey data. \n\n"}
{"id": "1608.00696", "contents": "Title: Can we trust the bootstrap in high-dimension? Abstract: We consider the performance of the bootstrap in high-dimensions for the\nsetting of linear regression, where $p<n$ but $p/n$ is not close to zero. We\nconsider ordinary least-squares as well as robust regression methods and adopt\na minimalist performance requirement: can the bootstrap give us good confidence\nintervals for a single coordinate of $\\beta$? (where $\\beta$ is the true\nregression vector).\n  We show through a mix of numerical and theoretical work that the bootstrap is\nfraught with problems. Both of the most commonly used methods of bootstrapping\nfor regression -- residual bootstrap and pairs bootstrap -- give very poor\ninference on $\\beta$ as the ratio $p/n$ grows. We find that the residuals\nbootstrap tend to give anti-conservative estimates (inflated Type I error),\nwhile the pairs bootstrap gives very conservative estimates (severe loss of\npower) as the ratio $p/n$ grows. We also show that the jackknife resampling\ntechnique for estimating the variance of $\\hat{\\beta}$ severely overestimates\nthe variance in high dimensions.\n  We contribute alternative bootstrap procedures based on our theoretical\nresults that mitigate these problems. However, the corrections depend on\nassumptions regarding the underlying data-generation model, suggesting that in\nhigh-dimensions it may be difficult to have universal, robust bootstrapping\ntechniques. \n\n"}
{"id": "1608.02477", "contents": "Title: Low-Complexity Massive MIMO Subspace Estimation and Tracking from\n  Low-Dimensional Projections Abstract: Massive MIMO is a variant of multiuser MIMO, where the number of antennas $M$\nat the base-station is large, and generally much larger than the number of\nspatially multiplexed data streams to/from the users. It has been observed that\nin many realistic propagation scenarios as well as in spatially correlated\nchannel models used in standardizations, although the user channel vectors have\na very high-dim $M$, they lie on low-dim subspaces due to their limited angular\nspread. This low-dim subspace structure remains stable across many coherence\nblocks and can be exploited in several ways to improve the system performance.\nA main challenge, however, is to estimate this signal subspace from samples of\nusers' channel vectors as fast and efficiently as possible. In a recent work,\nwe addressed this problem and proposed a very effective novel algorithm\nreferred to as Approximate Maximum-Likelihood (AML), which was formulated as a\nsemi-definite program (SDP). In this paper, we address two problems left open\nin our previous work: computational complexity and tracking. The algorithm\nproposed in this paper is reminiscent of Multiple Measurement Vectors (MMV)\nproblem in Compressed Sensing and is proved to be equivalent to the AML\nAlgorithm for sufficiently dense angular grids. It has also a very low\ncomputational complexity and is able to track sharp transitions in the channel\nstatistics very quickly. Although mainly motivated by massive MIMO\napplications, our proposed algorithm is of independent interest in other\nrelated subspace estimation applications. We assess the estimation/tracking\nperformance of our proposed algorithm empirically via numerical simulations,\nespecially in practically relevant situations where a direct implementation of\nthe SDP would be infeasible in real-time. We also compare the performance of\nour algorithm with other related subspace estimation algorithms in the\nliterature. \n\n"}
{"id": "1608.03022", "contents": "Title: Dynamic Principal Component Analysis: Identifying the Relationship\n  between Multiple Air Pollutants Abstract: The dynamic nature of air quality chemistry and transport makes it difficult\nto identify the mixture of air pollutants for a region. In this study of air\nquality in the Houston metropolitan area we apply dynamic principal component\nanalysis (DPCA) to a normalized multivariate time series of daily concentration\nmeasurements of five pollutants (O3, CO, NO2, SO2, PM2.5) from January 1, 2009\nthrough December 31, 2011 for each of the 24 hours in a day. The resulting\ndynamic components are examined by hour across days for the 3 year period.\nDiurnal and seasonal patterns are revealed underlining times when DPCA performs\nbest and two principal components (PCs) explain most variability in the\nmultivariate series. DPCA is shown to be superior to static principal component\nanalysis (PCA) in discovery of linear relations among transformed pollutant\nmeasurements. DPCA captures the time-dependent correlation structure of the\nunderlying pollutants recorded at up to 34 monitoring sites in the region. In\nwinter mornings the first principal component (PC1) (mainly CO and NO2)\nexplains up to 70% of variability. Augmenting with the second principal\ncomponent (PC2) (mainly driven by SO2) the explained variability rises to 90%.\nIn the afternoon, O3 gains prominence in the second principal component. The\nseasonal profile of PCs' contribution to variance loses its distinction in the\nafternoon, yet cumulatively PC1 and PC2 still explain up to 65% of variability\nin ambient air data. DPCA provides a strategy for identifying the changing air\nquality profile for the region studied. \n\n"}
{"id": "1608.03769", "contents": "Title: Spatial Modeling, with Application to Complex Survey Data: Discussion of\n  \"Model-based Geostatistics for Prevalence Mapping in Low-Resource Settings\",\n  by Diggle and Giorgi Abstract: Prevalence mapping in low resource settings is an increasingly important\nendeavor to guide policy making and to spatially and temporally characterize\nthe burden of disease. We will focus our discussion on consideration of the\ncomplex design when analyzing survey data, and on spatial modeling. With\nrespect to the former, we consider two approaches: direct use of the weights,\nand a model-based approach using a spatial model to acknowledge clustering. For\nthe latter we consider continuously indexed Markovian Gaussian random field\nmodels. \n\n"}
{"id": "1608.04478", "contents": "Title: A Geometrical Approach to Topic Model Estimation Abstract: In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data. \n\n"}
{"id": "1608.05465", "contents": "Title: Regularization for supervised learning via the \"hubNet\" procedure Abstract: We propose a new method for supervised learning. The hubNet procedure fits a\nhub-based graphical model to the predictors, to estimate the amount of\n\"connection\" that each predictor has with other predictors. This yields a set\nof predictor weights that are then used in a regularized regression such as the\nlasso or elastic net. The resulting procedure is easy to implement, can\nsometimes yields higher prediction accuracy that the lasso, and can give\ninsights into the underlying structure of the predictors. HubNet can also be\ngeneralized seamlessly to other supervised problems such as regularized\nlogistic regression (and other GLMs), Cox's proportional hazards model, and\nnonlinear procedures such as random forests and boosting. We prove some\nrecovery results under a specialized model and illustrate the method on real\nand simulated data. \n\n"}
{"id": "1608.05498", "contents": "Title: Elicitability and backtesting: Perspectives for banking regulation Abstract: Conditional forecasts of risk measures play an important role in internal\nrisk management of financial institutions as well as in regulatory capital\ncalculations. In order to assess forecasting performance of a risk measurement\nprocedure, risk measure forecasts are compared to the realized financial losses\nover a period of time and a statistical test of correctness of the procedure is\nconducted. This process is known as backtesting. Such traditional backtests are\nconcerned with assessing some optimality property of a set of risk measure\nestimates. However, they are not suited to compare different risk estimation\nprocedures. We investigate the proposal of comparative backtests, which are\nbetter suited for method comparisons on the basis of forecasting accuracy, but\nnecessitate an elicitable risk measure. We argue that supplementing traditional\nbacktests with comparative backtests will enhance the existing trading book\nregulatory framework for banks by providing the correct incentive for accuracy\nof risk measure forecasts. In addition, the comparative backtesting framework\ncould be used by banks internally as well as by researchers to guide selection\nof forecasting methods. The discussion focuses on three risk measures,\nValue-at-Risk, expected shortfall and expectiles, and is supported by a\nsimulation study and data analysis. \n\n"}
{"id": "1608.05913", "contents": "Title: The Adequate Bootstrap Abstract: There is a fundamental disconnect between what is tested in a model adequacy\ntest, and what we would like to test. The usual approach is to test the null\nhypothesis \"Model M is the true model.\" However, Model M is never the true\nmodel. A model might still be useful even if we have enough data to reject it.\nIn this paper, we present a technique to assess the adequacy of a model from\nthe philosophical standpoint that we know the model is not true, but we want to\nknow if it is useful.\n  Our solution to this problem is to measure the parameter uncertainty in our\nestimates caused by the model uncertainty. We use bootstrap inference on\nsamples of a smaller size, for which the model cannot be rejected. We use a\nmodel adequacy test to choose a bootstrap size with limited probability of\nrejecting the model and perform inference for samples of this size based on a\nnonparametric bootstrap. Our idea is that if we base our inference on a sample\nsize at which we do not reject the model, then we should be happy with this\ninference, because we would have been confident in it if our original dataset\nhad been this size. \n\n"}
{"id": "1608.06805", "contents": "Title: Analyzing two-stage experiments in the presence of interference Abstract: Two-stage randomization is a powerful design for estimating treatment effects\nin the presence of interference; that is, when one individual's treatment\nassignment affects another individual's outcomes. Our motivating example is a\ntwo-stage randomized trial evaluating an intervention to reduce student\nabsenteeism in the School District of Philadelphia. In that experiment,\nhouseholds with multiple students were first assigned to treatment or control;\nthen, in treated households, one student was randomly assigned to treatment.\nUsing this example, we highlight key considerations for analyzing two-stage\nexperiments in practice. Our first contribution is to address additional\ncomplexities that arise when household sizes vary; in this case, researchers\nmust decide between assigning equal weight to households or equal weight to\nindividuals. We propose unbiased estimators for a broad class of individual-\nand household-weighted estimands, with corresponding theoretical and estimated\nvariances. Our second contribution is to connect two common approaches for\nanalyzing two-stage designs: linear regression and randomization inference. We\nshow that, with suitably chosen standard errors, these two approaches yield\nidentical point and variance estimates, which is somewhat surprising given the\ncomplex randomization scheme. Finally, we explore options for incorporating\ncovariates to improve precision. We confirm our analytic results via simulation\nstudies and apply these methods to the attendance study, finding substantively\nmeaningful spillover effects. \n\n"}
{"id": "1608.07193", "contents": "Title: Quantile Dependence between Stock Markets and its Application in\n  Volatility Forecasting Abstract: This paper examines quantile dependence between international stock markets\nand evaluates its use for improving volatility forecasting. First, we analyze\nquantile dependence and directional predictability between the US stock market\nand stock markets in the UK, Germany, France and Japan. We use the\ncross-quantilogram, which is a correlation statistic of quantile hit processes.\nThe detailed dependence between stock markets depends on specific quantile\nranges and this dependence is generally asymmetric; the negative spillover\neffect is stronger than the positive spillover effect and there exists strong\ndirectional predictability from the US market to the UK, Germany, France and\nJapan markets. Second, we consider a simple quantile-augmented volatility model\nthat accommodates the quantile dependence and directional predictability\nbetween the US market and these other markets. The quantile-augmented\nvolatility model provides superior in-sample and out-of-sample volatility\nforecasts. \n\n"}
{"id": "1608.08076", "contents": "Title: Bayesian Sequentially Monitored Multi-arm Experiments with Multiple\n  Comparison Adjustments Abstract: Randomized experiments play a major role in data-driven decision making\nacross many different fields and disciplines. In medicine, for example,\nrandomized controlled trials (RCTs) are the backbone of clinical trial\nmethodology for testing the efficacy of new drugs and therapies versus existing\ntreatments or placebo. In business and marketing, randomized experiments are\ntypically referred to as A/B tests when there are only two arms, or variants,\nin the experiment, and as multivariate A/B tests when there are more than two\narms. Typical applications of A/B tests include comparing the effectiveness of\ndifferent ad campaigns, evaluating how people respond to different website\nlayouts, or comparing different customer subpopulations to each other.\n  This paper focuses on multivariate A/B testing from a digital marketing\nperspective, and presents a method for the sequential monitoring of such\nexperiments while accounting for the issue of multiple comparisons. In adapting\nand combining the methods of two previous works, the method presented herein is\nstraightforward to implement using standard statistical software and performs\nquite well in various simulation studies, exhibiting better power and smaller\naverage sample sizes than comparable methods. \n\n"}
{"id": "1608.08126", "contents": "Title: Simultaneous penalized M-estimation of covariance matrices using\n  geodesically convex optimization Abstract: A common assumption when sampling $p$-dimensional observations from $K$\ndistinct group is the equality of the covariance matrices. In this paper, we\npropose two penalized $M$-estimation approaches for the estimation of the\ncovariance or scatter matrices under the broader assumption that they may\nsimply be close to each other, and hence roughly deviate from some positive\ndefinite \"center\". The first approach begins by generating a pooled\n$M$-estimator of scatter based on all the data, followed by a penalised\n$M$-estimator of scatter for each group, with the penalty term chosen so that\nthe individual scatter matrices are shrunk towards the pooled scatter matrix.\nIn the second approach, we minimize the sum of the individual group\n$M$-estimation cost functions together with an additive joint penalty term\nwhich enforces some similarity between the individual scatter estimators, i.e.\nshrinkage towards a mutual center. In both approaches, we utilize the concept\nof geodesic convexity to prove the existence and uniqueness of the penalized\nsolution under general conditions. We consider three specific penalty functions\nbased on the Euclidean, the Riemannian, and the Kullback-Leibler distances. In\nthe second approach, the distance based penalties are shown to lead to\nestimators of the mutual center that are related to the arithmetic, the\nRiemannian and the harmonic means of positive definite matrices, respectively.\nA penalty based on an ellipticity measure is also considered which is\nparticularly useful for shape matrix estimators. Fixed point equations are\nderived for each penalty function and the benefits of the estimators are\nillustrated in regularized discriminant analysis problem. \n\n"}
{"id": "1608.08291", "contents": "Title: Gaussian Process Models for Mortality Rates and Improvement Factors Abstract: We develop a Gaussian process (\"GP\") framework for modeling mortality rates\nand mortality improvement factors. GP regression is a nonparametric,\ndata-driven approach for determining the spatial dependence in mortality rates\nand jointly smoothing raw rates across dimensions, such as calendar year and\nage. The GP model quantifies uncertainty associated with smoothed historical\nexperience and generates full stochastic trajectories for out-of-sample\nforecasts. Our framework is well suited for updating projections when newly\navailable data arrives, and for dealing with \"edge\" issues where credibility is\nlower. We present a detailed analysis of Gaussian process model performance for\nUS mortality experience based on the CDC datasets. We investigate the\ninteraction between mean and residual modeling, Bayesian and non-Bayesian GP\nmethodologies, accuracy of in-sample and out-of-sample forecasting, and\nstability of model parameters. We also document the general decline, along with\nstrong age-dependency, in mortality improvement factors over the past few\nyears, contrasting our findings with the Society of Actuaries (\"SOA\") MP-2014\nand -2015 models that do not fully reflect these recent trends. \n\n"}
{"id": "1609.00065", "contents": "Title: Partitioned Cross-Validation for Divide-and-Conquer Density Estimation Abstract: We present an efficient method to estimate cross-validation bandwidth\nparameters for kernel density estimation in very large datasets where ordinary\ncross-validation is rendered highly inefficient, both statistically and\ncomputationally. Our approach relies on calculating multiple cross-validation\nbandwidths on partitions of the data, followed by suitable scaling and\naveraging to return a partitioned cross-validation bandwidth for the entire\ndataset. The partitioned cross-validation approach produces substantial\ncomputational gains over ordinary cross-validation. We additionally show that\npartitioned cross-validation can be statistically efficient compared to\nordinary cross-validation. We derive analytic expressions for the\nasymptotically optimal number of partitions and study its finite sample\naccuracy through a detailed simulation study. We additionally propose a\npermuted version of partitioned cross-validation which attains even higher\nefficiency. Theoretical properties of the estimators are studied and the\nmethodology is applied to the Higgs Boson dataset with 11 million observations \n\n"}
{"id": "1609.00814", "contents": "Title: Backward Nested Descriptors Asymptotics with Inference on Stem Cell\n  Differentiation Abstract: For sequences of random backward nested subspaces as occur, say, in dimension\nreduction for manifold or stratified space valued data, asymptotic results are\nderived. In fact, we formulate our results more generally for backward nested\nfamilies of descriptors (BNFD). Under rather general conditions, asymptotic\nstrong consistency holds. Under additional, still rather general hypotheses,\namong them existence of a.s. local twice differentiable charts, asymptotic\njoint normality of a BNFD can be shown. If charts factor suitably, this leads\nto individual asymptotic normality for the last element, a principal nested\nmean or a principal nested geodesic, say. It turns out that these results\npertain to principal nested spheres (PNS) and principal nested great subsphere\n(PNGS) analysis by Jung et al. (2010) as well as to the intrinsic mean on a\nfirst geodesic principal component (IMo1GPC) for manifolds and Kendall's shape\nspaces. A nested bootstrap two-sample test is derived and illustrated with\nsimulations. In a study on real data, PNGS is applied to track early human\nmesenchymal stem cell differentiation over a coarse time grid and, among\nothers, to locate a change point with direct consequences for the design of\nfurther studies. \n\n"}
{"id": "1609.00908", "contents": "Title: Some recent developments in statistics for spatial point patterns Abstract: This paper reviews developments in statistics for spatial point processes\nobtained within roughly the last decade. These developments include new classes\nof spatial point process models such as determinantal point processes, models\nincorporating both regularity and aggregation, and models where points are\nrandomly distributed around latent geometric structures. Regarding parametric\ninference the main focus is on various types of estimating functions derived\nfrom so-called innovation measures. Optimality of such estimating functions is\ndiscussed as well as computational issues. Maximum likelihood inference for\ndeterminantal point processes and Bayesian inference are briefly considered\ntoo. Concerning non-parametric inference, we consider extensions of functional\nsummary statistics to the case of inhomogeneous point processes as well as new\napproaches to simulation based inference. \n\n"}
{"id": "1609.02700", "contents": "Title: Efficient batch-sequential Bayesian optimization with moments of\n  truncated Gaussian vectors Abstract: We deal with the efficient parallelization of Bayesian global optimization\nalgorithms, and more specifically of those based on the expected improvement\ncriterion and its variants. A closed form formula relying on multivariate\nGaussian cumulative distribution functions is established for a generalized\nversion of the multipoint expected improvement criterion. In turn, the latter\nrelies on intermediate results that could be of independent interest concerning\nmoments of truncated Gaussian vectors. The obtained expansion of the criterion\nenables studying its differentiability with respect to point batches and\ncalculating the corresponding gradient in closed form. Furthermore , we derive\nfast numerical approximations of this gradient and propose efficient batch\noptimization strategies. Numerical experiments illustrate that the proposed\napproaches enable computational savings of between one and two order of\nmagnitudes, hence enabling derivative-based batch-sequential acquisition\nfunction maximization to become a practically implementable and efficient\nstandard. \n\n"}
{"id": "1609.04156", "contents": "Title: The Gaussian Graphical Model in Cross-sectional and Time-series Data Abstract: We discuss the Gaussian graphical model (GGM; an undirected network of\npartial correlation coefficients) and detail its utility as an exploratory data\nanalysis tool. The GGM shows which variables predict one-another, allows for\nsparse modeling of covariance structures, and may highlight potential causal\nrelationships between observed variables. We describe the utility in 3 kinds of\npsychological datasets: datasets in which consecutive cases are assumed\nindependent (e.g., cross-sectional data), temporally ordered datasets (e.g., n\n= 1 time series), and a mixture of the 2 (e.g., n > 1 time series). In\ntime-series analysis, the GGM can be used to model the residual structure of a\nvector-autoregression analysis (VAR), also termed graphical VAR. Two network\nmodels can then be obtained: a temporal network and a contemporaneous network.\nWhen analyzing data from multiple subjects, a GGM can also be formed on the\ncovariance structure of stationary means---the between-subjects network. We\ndiscuss the interpretation of these models and propose estimation methods to\nobtain these networks, which we implement in the R packages graphicalVAR and\nmlVAR. The methods are showcased in two empirical examples, and simulation\nstudies on these methods are included in the supplementary materials. \n\n"}
{"id": "1609.04222", "contents": "Title: Grouped functional time series forecasting: An application to\n  age-specific mortality rates Abstract: Age-specific mortality rates are often disaggregated by different attributes,\nsuch as sex, state and ethnicity. Forecasting age-specific mortality rates at\nthe national and sub-national levels plays an important role in developing\nsocial policy. However, independent forecasts at the sub-national levels may\nnot add up to the forecasts at the national level. To address this issue, we\nconsider reconciling forecasts of age-specific mortality rates, extending the\nmethods of Hyndman et al. (2011) to functional time series, where age is\nconsidered as a continuum. The grouped functional time series methods are used\nto produce point forecasts of mortality rates that are aggregated appropriately\nacross different disaggregation factors. For evaluating forecast uncertainty,\nwe propose a bootstrap method for reconciling interval forecasts. Using the\nregional age-specific mortality rates in Japan, obtained from the Japanese\nMortality Database, we investigate the one- to ten-step-ahead point and\ninterval forecast accuracies between the independent and grouped functional\ntime series forecasting methods. The proposed methods are shown to be useful\nfor reconciling forecasts of age-specific mortality rates at the national and\nsub-national levels. They also enjoy improved forecast accuracy averaged over\ndifferent disaggregation factors. Supplemental materials for the article are\navailable online. \n\n"}
{"id": "1609.04466", "contents": "Title: Phenotyping using Structured Collective Matrix Factorization of\n  Multi--source EHR Data Abstract: The increased availability of electronic health records (EHRs) have\nspearheaded the initiative for precision medicine using data driven approaches.\nEssential to this effort is the ability to identify patients with certain\nmedical conditions of interest from simple queries on EHRs, or EHR-based\nphenotypes. Existing rule--based phenotyping approaches are extremely labor\nintensive. Instead, dimensionality reduction and latent factor estimation\ntechniques from machine learning can be adapted for phenotype extraction with\nno (or minimal) human supervision.\n  We propose to identify an easily interpretable latent space shared across\nvarious sources of EHR data as potential candidates for phenotypes. By\nincorporating multiple EHR data sources (e.g., diagnosis, medications, and lab\nreports) available in heterogeneous datatypes in a generalized\n\\textit{Collective Matrix Factorization (CMF)}, our methods can generate rich\nphenotypes. Further, easy interpretability in phenotyping application requires\nsparse representations of the candidate phenotypes, for example each phenotype\nderived from patients' medication and diagnosis data should preferably be\nrepresented by handful of diagnosis and medications, ($5$--$10$ active\ncomponents). We propose a constrained formulation of CMF for estimating sparse\nphenotypes. We demonstrate the efficacy of our model through an extensive\nempirical study on EHR data from Vanderbilt University Medical Center. \n\n"}
{"id": "1609.04470", "contents": "Title: The Type Ia Supernova Color-Magnitude Relation and Host Galaxy Dust: A\n  Simple Hierarchical Bayesian Model Abstract: Conventional Type Ia supernova (SN Ia) cosmology analyses currently use a\nsimplistic linear regression of magnitude versus color and light curve shape,\nwhich does not model intrinsic SN Ia variations and host galaxy dust as\nphysically distinct effects, resulting in low color-magnitude slopes. We\nconstruct a probabilistic generative model for the dusty distribution of\nextinguished absolute magnitudes and apparent colors as the convolution of a\nintrinsic SN Ia color-magnitude distribution and a host galaxy dust\nreddening-extinction distribution. If the intrinsic color-magnitude ($M_B$ vs.\n$B-V$) slope $\\beta_{int}$ differs from the host galaxy dust law $R_B$, this\nconvolution results in a specific curve of mean extinguished absolute magnitude\nvs. apparent color. The derivative of this curve smoothly transitions from\n$\\beta_{int}$ in the blue tail to $R_B$ in the red tail of the apparent color\ndistribution. The conventional linear fit approximates this effective curve\nnear the average apparent color, resulting in an apparent slope $\\beta_{app}$\nbetween $\\beta_{int}$ and $R_B$. We incorporate these effects into a\nhierarchical Bayesian statistical model for SN Ia light curve measurements, and\nanalyze a dataset of SALT2 optical light curve fits of 248 nearby SN Ia at z <\n0.10. The conventional linear fit obtains $\\beta_{app} \\approx 3$. Our model\nfinds a $\\beta_{int} = 2.3 \\pm 0.3$ and a distinct dust law of $R_B = 3.8 \\pm\n0.3$, consistent with the average for Milky Way dust, while correcting a\nsystematic distance bias of $\\sim 0.10$ mag in the tails of the apparent color\ndistribution. Finally, we extend our model to examine the SN Ia luminosity-host\nmass dependence in terms of intrinsic and dust components. \n\n"}
{"id": "1609.04558", "contents": "Title: Statistical Inference in a Directed Network Model with Covariates Abstract: Networks are often characterized by node heterogeneity for which nodes\nexhibit different degrees of interaction and link homophily for which nodes\nsharing common features tend to associate with each other. In this paper, we\npropose a new directed network model to capture the former via node-specific\nparametrization and the latter by incorporating covariates. In particular, this\nmodel quantifies the extent of heterogeneity in terms of outgoingness and\nincomingness of each node by different parameters, thus allowing the number of\nheterogeneity parameters to be twice the number of nodes. We study the maximum\nlikelihood estimation of the model and establish the uniform consistency and\nasymptotic normality of the resulting estimators. Numerical studies demonstrate\nour theoretical findings and a data analysis confirms the usefulness of our\nmodel. \n\n"}
{"id": "1609.05615", "contents": "Title: Discussion of \"Fast Approximate Inference for Arbitrarily Large\n  Semiparametric Regression Models via Message Passing\" Abstract: Discussion paper on \"Fast Approximate Inference for Arbitrarily Large\nSemiparametric Regression Models via Message Passing\" by Wand\n[arXiv:1602.07412]. \n\n"}
{"id": "1609.06757", "contents": "Title: Quickest Change Detection Approach to Optimal Control in Markov Decision\n  Processes with Model Changes Abstract: Optimal control in non-stationary Markov decision processes (MDP) is a\nchallenging problem. The aim in such a control problem is to maximize the\nlong-term discounted reward when the transition dynamics or the reward function\ncan change over time. When a prior knowledge of change statistics is available,\nthe standard Bayesian approach to this problem is to reformulate it as a\npartially observable MDP (POMDP) and solve it using approximate POMDP solvers,\nwhich are typically computationally demanding. In this paper, the problem is\nanalyzed through the viewpoint of quickest change detection (QCD), a set of\ntools for detecting a change in the distribution of a sequence of random\nvariables. Current methods applying QCD to such problems only passively detect\nchanges by following prescribed policies, without optimizing the choice of\nactions for long term performance. We demonstrate that ignoring the\nreward-detection trade-off can cause a significant loss in long term rewards,\nand propose a two threshold switching strategy to solve the issue. A\nnon-Bayesian problem formulation is also proposed for scenarios where a\nBayesian formulation cannot be defined. The performance of the proposed two\nthreshold strategy is examined through numerical analysis on a non-stationary\nMDP task, and the strategy outperforms the state-of-the-art QCD methods in both\nBayesian and non-Bayesian settings. \n\n"}
{"id": "1609.08028", "contents": "Title: A Unified Statistical Framework for Single Cell and Bulk RNA Sequencing\n  Data Abstract: Recent advances in technology have enabled the measurement of RNA levels for\nindividual cells. Compared to traditional tissue-level bulk RNA-seq data,\nsingle cell sequencing yields valuable insights about gene expression profiles\nfor different cell types, which is potentially critical for understanding many\ncomplex human diseases. However, developing quantitative tools for such data\nremains challenging because of high levels of technical noise, especially the\n\"dropout\" events. A \"dropout\" happens when the RNA for a gene fails to be\namplified prior to sequencing, producing a \"false\" zero in the observed data.\nIn this paper, we propose a Unified RNA-Sequencing Model (URSM) for both single\ncell and bulk RNA-seq data, formulated as a hierarchical model. URSM borrows\nthe strength from both data sources and carefully models the dropouts in single\ncell data, leading to a more accurate estimation of cell type specific gene\nexpression profile. In addition, URSM naturally provides inference on the\ndropout entries in single cell data that need to be imputed for downstream\nanalyses, as well as the mixing proportions of different cell types in bulk\nsamples. We adopt an empirical Bayes approach, where parameters are estimated\nusing the EM algorithm and approximate inference is obtained by Gibbs sampling.\nSimulation results illustrate that URSM outperforms existing approaches both in\ncorrecting for dropouts in single cell data, as well as in deconvolving bulk\nsamples. We also demonstrate an application to gene expression data on fetal\nbrains, where our model successfully imputes the dropout genes and reveals cell\ntype specific expression patterns. \n\n"}
{"id": "1609.09380", "contents": "Title: Testing mutual independence in high dimension via distance covariance Abstract: In this paper, we introduce a ${\\mathcal L}_2$ type test for testing mutual\nindependence and banded dependence structure for high dimensional data. The\ntest is constructed based on the pairwise distance covariance and it accounts\nfor the non-linear and non-monotone dependences among the data, which cannot be\nfully captured by the existing tests based on either Pearson correlation or\nrank correlation. Our test can be conveniently implemented in practice as the\nlimiting null distribution of the test statistic is shown to be standard\nnormal. It exhibits excellent finite sample performance in our simulation\nstudies even when the sample size is small albeit dimension is high, and is\nshown to successfully identify nonlinear dependence in empirical data analysis.\nOn the theory side, asymptotic normality of our test statistic is shown under\nquite mild moment assumptions and with little restriction on the growth rate of\nthe dimension as a function of sample size. As a demonstration of good power\nproperties for our distance covariance based test, we further show that an\ninfeasible version of our test statistic has the rate optimality in the class\nof Gaussian distribution with equal correlation. \n\n"}
{"id": "1609.09435", "contents": "Title: On the statistical properties of viral misinformation in online social\n  media Abstract: The massive diffusion of online social media allows for the rapid and\nuncontrolled spreading of conspiracy theories, hoaxes, unsubstantiated claims,\nand false news. Such an impressive amount of misinformation can influence\npolicy preferences and encourage behaviors strongly divergent from recommended\npractices. In this paper, we study the statistical properties of viral\nmisinformation in online social media. By means of methods belonging to Extreme\nValue Theory, we show that the number of extremely viral posts over time\nfollows a homogeneous Poisson process, and that the interarrival times between\nsuch posts are independent and identically distributed, following an\nexponential distribution. Moreover, we characterize the uncertainty around the\nrate parameter of the Poisson process through Bayesian methods. Finally, we are\nable to derive the predictive posterior probability distribution of the number\nof posts exceeding a certain threshold of shares over a finite interval of\ntime. \n\n"}
{"id": "1609.09532", "contents": "Title: Inferring Brain Signals Synchronicity from a Sample of EEG Readings Abstract: Inferring patterns of synchronous brain activity from a heterogeneous sample\nof electroencephalograms (EEG) is scientifically and methodologically\nchallenging. While it is intuitively and statistically appealing to rely on\nreadings from more than one individual in order to highlight recurrent patterns\nof brain activation, pooling information across subjects presents non-trivial\nmethodological problems. We discuss some of the scientific issues associated\nwith the understanding of synchronized neuronal activity and propose a\nmethodological framework for statistical inference from a sample of EEG\nreadings. Our work builds on classical contributions in time-series, clustering\nand functional data analysis, in an effort to reframe a challenging inferential\nproblem in the context of familiar analytical techniques. Some attention is\npaid to computational issues, with a proposal based on the combination of\nmachine learning and Bayesian techniques. \n\n"}
{"id": "1610.02122", "contents": "Title: Significance testing in non-sparse high-dimensional linear models Abstract: In high-dimensional linear models, the sparsity assumption is typically made,\nstating that most of the parameters are equal to zero. Under the sparsity\nassumption, estimation and, recently, inference have been well studied.\nHowever, in practice, sparsity assumption is not checkable and more importantly\nis often violated; a large number of covariates might be expected to be\nassociated with the response, indicating that possibly all, rather than just a\nfew, parameters are non-zero. A natural example is a genome-wide gene\nexpression profiling, where all genes are believed to affect a common disease\nmarker. We show that existing inferential methods are sensitive to the sparsity\nassumption, and may, in turn, result in the severe lack of control of Type-I\nerror. In this article, we propose a new inferential method, named CorrT, which\nis robust to model misspecification such as heteroscedasticity and lack of\nsparsity. CorrT is shown to have Type I error approaching the nominal level for\n\\textit{any} models and Type II error approaching zero for sparse and many\ndense models.\n  In fact, CorrT is also shown to be optimal in a variety of frameworks:\nsparse, non-sparse and hybrid models where sparse and dense signals are mixed.\nNumerical experiments show a favorable performance of the CorrT test compared\nto the state-of-the-art methods. \n\n"}
{"id": "1610.02351", "contents": "Title: Panning for Gold: Model-X Knockoffs for High-dimensional Controlled\n  Variable Selection Abstract: Many contemporary large-scale applications involve building interpretable\nmodels linking a large set of potential covariates to a response in a nonlinear\nfashion, such as when the response is binary. Although this modeling problem\nhas been extensively studied, it remains unclear how to effectively control the\nfraction of false discoveries even in high-dimensional logistic regression, not\nto mention general high-dimensional nonlinear models. To address such a\npractical problem, we propose a new framework of $model$-$X$ knockoffs, which\nreads from a different perspective the knockoff procedure (Barber and Cand\\`es,\n2015) originally designed for controlling the false discovery rate in linear\nmodels. Whereas the knockoffs procedure is constrained to homoscedastic linear\nmodels with $n\\ge p$, the key innovation here is that model-X knockoffs provide\nvalid inference from finite samples in settings in which the conditional\ndistribution of the response is arbitrary and completely unknown. Furthermore,\nthis holds no matter the number of covariates. Correct inference in such a\nbroad setting is achieved by constructing knockoff variables probabilistically\ninstead of geometrically. To do this, our approach requires the covariates be\nrandom (independent and identically distributed rows) with a distribution that\nis known, although we provide preliminary experimental evidence that our\nprocedure is robust to unknown/estimated distributions. To our knowledge, no\nother procedure solves the $controlled$ variable selection problem in such\ngenerality, but in the restricted settings where competitors exist, we\ndemonstrate the superior power of knockoffs through simulations. Finally, we\napply our procedure to data from a case-control study of Crohn's disease in the\nUnited Kingdom, making twice as many discoveries as the original analysis of\nthe same data. \n\n"}
{"id": "1610.02548", "contents": "Title: Exploring brain transcriptomic patterns: a topological analysis using\n  spatial expression networks Abstract: Characterizing the transcriptome architecture of the human brain is\nfundamental in gaining an understanding of brain function and disease. A number\nof recent studies have investigated patterns of brain gene expression obtained\nfrom an extensive anatomical coverage across the entire human brain using\nexperimental data generated by the Allen Human Brain Atlas (AHBA) project. In\nthis paper, we propose a new representation of a gene's transcription activity\nthat explicitly captures the pattern of spatial co-expression across different\nanatomical brain regions. For each gene, we define a Spatial Expression Network\n(SEN), a network quantifying co-expression patterns amongst several anatomical\nlocations. Network similarity measures are then employed to quantify the\ntopological resemblance between pairs of SENs and identify naturally occurring\nclusters. Using network-theoretical measures, three large clusters have been\ndetected featuring distinct topological properties. We then evaluate whether\ntopological diversity of the SENs reflects significant differences in\nbiological function through a gene ontology analysis. We report on evidence\nsuggesting that one of the three SEN clusters consists of genes specifically\ninvolved in the nervous system, including genes related to brain disorders,\nwhile the remaining two clusters are representative of immunity, transcription\nand translation. These findings are consistent with previous studies showing\nthat brain gene clusters are generally associated with one of these three major\nbiological processes. \n\n"}
{"id": "1610.07524", "contents": "Title: Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments Abstract: Recidivism prediction instruments provide decision makers with an assessment\nof the likelihood that a criminal defendant will reoffend at a future point in\ntime. While such instruments are gaining increasing popularity across the\ncountry, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses a fairness criterion originating in the\nfield of educational and psychological testing that has recently been applied\nto assess the fairness of recidivism prediction instruments. We demonstrate how\nadherence to the criterion may lead to considerable disparate impact when\nrecidivism prevalence differs across groups. \n\n"}
{"id": "1610.08139", "contents": "Title: Neutron stars in the light of SKA: Data, statistics, and science Abstract: The Square Kilometre Array (SKA), when it becomes functional, is expected to\nenrich neutron star (NS) catalogues by at least an order of magnitude over\ntheir current state. This includes the discovery of new NS objects leading to\nbetter sampling of under-represented NS categories, precision measurements of\nintrinsic properties such as spin period and magnetic field, as also data on\nrelated phenomena such as microstructure, nulling, glitching, etc. This will\npresent a unique opportunity to seek answers to interesting and fundamental\nquestions about the extreme physics underlying these exotic objects in the\nuniverse. In this paper, we first present a meta-analysis (from a\nmethodological viewpoint) of statistical analyses performed using existing NS\ndata, with a two-fold goal: First, this should bring out how statistical models\nand methods are shaped and dictated by the science problem being addressed.\nSecond, it is hoped that these analyses will provide useful starting points for\ndeeper analyses involving richer data from SKA whenever it becomes available.\nWe also describe a few other areas of NS science which we believe will benefit\nfrom SKA which are of interest to the Indian NS community. \n\n"}
{"id": "1610.09388", "contents": "Title: A studentized permutation test for three-arm trials in the 'gold\n  standard' design Abstract: The 'gold standard' design for three-arm trials refers to trials with an\nactive control and a placebo control in addition to the experimental treatment\ngroup. This trial design is recommended when being ethically justifiable and it\nallows the simultaneous comparison of experimental treatment, active control,\nand placebo. Parametric testing methods have been studied plentifully over the\npast years. However, these methods often tend to be liberal or conservative\nwhen distributional assumptions are not met particularly with small sample\nsizes. In this article, we introduce a studentized permutation test for testing\nnon-inferiority and superiority of the experimental treatment compared to the\nactive control in three-arm trials in the `gold standard' design. The\nperformance of the studentized permutation test for finite sample sizes is\nassessed in a Monte-Carlo simulation study under various parameter\nconstellations. Emphasis is put on whether the studentized permutation test\nmeets the target significance level. For comparison purposes, commonly used\nWald-type tests are included in the simulation study. The simulation study\nshows that the presented studentized permutation test for assessing\nnon-inferiority in three-arm trials in the 'gold standard' design outperforms\nits competitors for count data. The methods discussed in this paper are\nimplemented in the R package ThreeArmedTrials which is available on the\ncomprehensive R archive network (CRAN). \n\n"}
{"id": "1611.01238", "contents": "Title: Corrected Bayesian information criterion for stochastic block models Abstract: Estimating the number of communities is one of the fundamental problems in\ncommunity detection. We re-examine the Bayesian paradigm for stochastic block\nmodels and propose a \"corrected Bayesian information criterion\",to determine\nthe number of communities and show that the proposed estimator is consistent\nunder mild conditions. The proposed criterion improves those used in Wang and\nBickel (2016) and Saldana et al. (2017) which tend to underestimate and\noverestimate the number of communities, respectively. Along the way, we\nestablish the Wilks theorem for stochastic block models. Moreover, we show\nthat, to obtain the consistency of model selection for stochastic block models,\nwe need a so-called \"consistency condition\". We also provide sufficient\nconditions for both homogenous networks and non-homogenous networks. The\nresults are further extended to degree corrected stochastic block models.\nNumerical studies demonstrate our theoretical results. \n\n"}
{"id": "1611.01310", "contents": "Title: Achieving Shrinkage in a Time-Varying Parameter Model Framework Abstract: Shrinkage for time-varying parameter (TVP) models is investigated within a\nBayesian framework, with the aim to automatically reduce time-varying\nparameters to static ones, if the model is overfitting. This is achieved\nthrough placing the double gamma shrinkage prior on the process variances. An\nefficient Markov chain Monte Carlo scheme is developed, exploiting boosting\nbased on the ancillarity-sufficiency interweaving strategy. The method is\napplicable both to TVP models for univariate as well as multivariate time\nseries. Applications include a TVP generalized Phillips curve for EU area\ninflation modelling and a multivariate TVP Cholesky stochastic volatility model\nfor joint modelling of the returns from the DAX-30 index. \n\n"}
{"id": "1611.01485", "contents": "Title: Flexible Bayesian additive joint models with an application to type 1\n  diabetes research Abstract: The joint modeling of longitudinal and time-to-event data is an important\ntool of growing popularity to gain insights into the association between a\nbiomarker and an event process. We develop a general framework of flexible\nadditive joint models that allows the specification of a variety of effects,\nsuch as smooth nonlinear, time-varying and random effects, in the longitudinal\nand survival parts of the models. Our extensions are motivated by the\ninvestigation of the relationship between fluctuating disease-specific markers,\nin this case autoantibodies, and the progression to the autoimmune disease type\n1 diabetes. By making use of Bayesian P-splines we are in particular able to\ncapture highly nonlinear subject-specific marker trajectories as well as a\ntime-varying association between the marker and the event process allowing new\ninsights into disease progression. The model is estimated within a Bayesian\nframework and implemented in the R-package bamlss. \n\n"}
{"id": "1611.03404", "contents": "Title: Learning an Astronomical Catalog of the Visible Universe through\n  Scalable Bayesian Inference Abstract: Celeste is a procedure for inferring astronomical catalogs that attains\nstate-of-the-art scientific results. To date, Celeste has been scaled to at\nmost hundreds of megabytes of astronomical images: Bayesian posterior inference\nis notoriously demanding computationally. In this paper, we report on a\nscalable, parallel version of Celeste, suitable for learning catalogs from\nmodern large-scale astronomical datasets. Our algorithmic innovations include a\nfast numerical optimization routine for Bayesian posterior inference and a\nstatistically efficient scheme for decomposing astronomical optimization\nproblems into subproblems.\n  Our scalable implementation is written entirely in Julia, a new high-level\ndynamic programming language designed for scientific and numerical computing.\nWe use Julia's high-level constructs for shared and distributed memory\nparallelism, and demonstrate effective load balancing and efficient scaling on\nup to 8192 Xeon cores on the NERSC Cori supercomputer. \n\n"}
{"id": "1611.03665", "contents": "Title: Functional Inference on Rotational Curves and Identification of Human\n  Gait at the Knee Joint Abstract: We extend Gaussian perturbation models in classical functional data analysis\nto the three-dimensional rotational group where a zero-mean Gaussian process in\nthe Lie algebra under the Lie exponential spreads multiplicatively around a\ncentral curve. As an estimator, we introduce point-wise extrinsic mean curves\nwhich feature strong perturbation consistency, and which are asymptotically\na.s. unique and differentiable, if the model is so. Further, we consider the\ngroup action of time warping and that of spatial isometries that are connected\nto the identity. The latter can be asymptotically consistently estimated if\nlifted to the unit quaternions. Introducing a generic loss for Lie groups, the\nformer can be estimated, and based on curve length, due to asymptotic\ndifferentiability, we propose two-sample permutation tests involving various\ncombinations of the group actions. This methodology allows inference on gait\npatterns due to the rotational motion of the lower leg with respect to the\nupper leg. This was previously not possible because, among others, the usual\nanalysis of separate Euler angles is not independent of marker placement, even\nif performed by trained specialists. \n\n"}
{"id": "1611.04460", "contents": "Title: Predictive, finite-sample model choice for time series under\n  stationarity and non-stationarity Abstract: In statistical research there usually exists a choice between structurally\nsimpler or more complex models. We argue that, even if a more complex, locally\nstationary time series model were true, then a simple, stationary time series\nmodel may be advantageous to work with under parameter uncertainty. We present\na new model choice methodology, where one of two competing approaches is chosen\nbased on its empirical, finite-sample performance with respect to prediction,\nin a manner that ensures interpretability. A rigorous, theoretical analysis of\nthe procedure is provided. As an important side result we prove, for possibly\ndiverging model order, that the localised Yule-Walker estimator is strongly,\nuniformly consistent under local stationarity. An R package, forecastSNSTS, is\nprovided and used to apply the methodology to financial and meteorological data\nin empirical examples. We further provide an extensive simulation study and\ndiscuss when it is preferable to base forecasts on the more volatile\ntime-varying estimates and when it is advantageous to forecast as if the data\nwere from a stationary process, even though they might not be. \n\n"}
{"id": "1611.05368", "contents": "Title: Neural Style Representations and the Large-Scale Classification of\n  Artistic Style Abstract: The artistic style of a painting is a subtle aesthetic judgment used by art\nhistorians for grouping and classifying artwork. The recently introduced\n`neural-style' algorithm substantially succeeds in merging the perceived\nartistic style of one image or set of images with the perceived content of\nanother. In light of this and other recent developments in image analysis via\nconvolutional neural networks, we investigate the effectiveness of a\n`neural-style' representation for classifying the artistic style of paintings. \n\n"}
{"id": "1611.05550", "contents": "Title: $e$PCA: High Dimensional Exponential Family PCA Abstract: Many applications, such as photon-limited imaging and genomics, involve large\ndatasets with noisy entries from exponential family distributions. It is of\ninterest to estimate the covariance structure and principal components of the\nnoiseless distribution. Principal Component Analysis (PCA), the standard method\nfor this setting, can be inefficient when the noise is non-Gaussian.\n  We develop $e$PCA (exponential family PCA), a new methodology for PCA on\nexponential family distributions. $e$PCA can be used for dimensionality\nreduction and denoising of large data matrices. $e$PCA involves the\neigendecomposition of a new covariance matrix estimator, constructed in a\nsimple and deterministic way using moment calculations, shrinkage, and random\nmatrix theory.\n  We provide several theoretical justifications for our estimator, including\nthe finite-sample convergence rate, and the Marchenko-Pastur law in high\ndimensions. $e$PCA compares favorably to PCA and various PCA alternatives for\nexponential families, in simulations as well as in XFEL and SNP data analysis.\nAn open-source implementation is available. \n\n"}
{"id": "1611.05806", "contents": "Title: Filling the gaps: Gaussian mixture models from noisy, truncated or\n  incomplete samples Abstract: Astronomical data often suffer from noise and incompleteness. We extend the\ncommon mixtures-of-Gaussians density estimation approach to account for\nsituations with a known sample incompleteness by simultaneous imputation from\nthe current model. The method, called GMMis, generalizes existing\nExpectation-Maximization techniques for truncated data to arbitrary truncation\ngeometries and probabilistic rejection processes, as long as they can be\nspecified and do not depend on the density itself. The method accounts for\nindependent multivariate normal measurement errors for each of the observed\nsamples and recovers an estimate of the error-free distribution from which both\nobserved and unobserved samples are drawn. It can perform a separation of a\nmixtures-of-Gaussian signal from a specified background distribution whose\namplitude may be unknown. We compare GMMis to the standard Gaussian mixture\nmodel for simple test cases with different types of incompleteness, and apply\nit to observational data from the NASA Chandra X-ray telescope. The python code\nis released as an open-source package at https://github.com/pmelchior/pyGMMis \n\n"}
{"id": "1611.05902", "contents": "Title: Practical heteroskedastic Gaussian process modeling for large simulation\n  experiments Abstract: We present a unified view of likelihood based Gaussian progress regression\nfor simulation experiments exhibiting input-dependent noise. Replication plays\nan important role in that context, however previous methods leveraging\nreplicates have either ignored the computational savings that come from such\ndesign, or have short-cut full likelihood-based inference to remain tractable.\nStarting with homoskedastic processes, we show how multiple applications of a\nwell-known Woodbury identity facilitate inference for all parameters under the\nlikelihood (without approximation), bypassing the typical full-data sized\ncalculations. We then borrow a latent-variable idea from machine learning to\naddress heteroskedasticity, adapting it to work within the same thrifty\ninferential framework, thereby simultaneously leveraging the computational and\nstatistical efficiency of designs with replication. The result is an\ninferential scheme that can be characterized as single objective function,\ncomplete with closed form derivatives, for rapid library-based optimization.\nIllustrations are provided, including real-world simulation experiments from\nmanufacturing and the management of epidemics. \n\n"}
{"id": "1611.05977", "contents": "Title: Robust and Scalable Column/Row Sampling from Corrupted Big Data Abstract: Conventional sampling techniques fall short of drawing descriptive sketches\nof the data when the data is grossly corrupted as such corruptions break the\nlow rank structure required for them to perform satisfactorily. In this paper,\nwe present new sampling algorithms which can locate the informative columns in\npresence of severe data corruptions. In addition, we develop new scalable\nrandomized designs of the proposed algorithms. The proposed approach is\nsimultaneously robust to sparse corruption and outliers and substantially\noutperforms the state-of-the-art robust sampling algorithms as demonstrated by\nexperiments conducted using both real and synthetic data. \n\n"}
{"id": "1611.06715", "contents": "Title: Pitfalls in testing with linear regression model by OLS Abstract: This is a comment on Economic Letters DOI\nhttp://dx.doi.org/10.1016/j.econlet.2015.10.015. We show that due to some\nmethodological aspects the main conclusions of the above mentioned paper should\nbe a little bit altered. \n\n"}
{"id": "1611.06818", "contents": "Title: Predicting Clinical Outcomes in Glioblastoma: An Application of\n  Topological and Functional Data Analysis Abstract: Glioblastoma multiforme (GBM) is an aggressive form of human brain cancer\nthat is under active study in the field of cancer biology. Its rapid\nprogression and the relative time cost of obtaining molecular data make other\nreadily-available forms of data, such as images, an important resource for\nactionable measures in patients. Our goal is to utilize information given by\nmedical images taken from GBM patients in statistical settings. To do this, we\ndesign a novel statistic---the smooth Euler characteristic transform\n(SECT)---that quantifies magnetic resonance images (MRIs) of tumors. Due to its\nwell-defined inner product structure, the SECT can be used in a wider range of\nfunctional and nonparametric modeling approaches than other previously proposed\ntopological summary statistics. When applied to a cohort of GBM patients, we\nfind that the SECT is a better predictor of clinical outcomes than both\nexisting tumor shape quantifications and common molecular assays. Specifically,\nwe demonstrate that SECT features alone explain more of the variance in GBM\npatient survival than gene expression, volumetric features, and morphometric\nfeatures. The main takeaways from our findings are thus twofold. First, they\nsuggest that images contain valuable information that can play an important\nrole in clinical prognosis and other medical decisions. Second, they show that\nthe SECT is a viable tool for the broader study of medical imaging informatics. \n\n"}
{"id": "1611.07230", "contents": "Title: Nonparametric adaptive estimation of order 1 Sobol indices in stochastic\n  models, with an application to Epidemiology Abstract: Global sensitivity analysis is a set of methods aiming at quantifying the\ncontribution of an uncertain input parameter of the model (or combination of\nparameters) on the variability of the response. We consider here the estimation\nof the Sobol indices of order 1 which are commonly-used indicators based on a\ndecomposition of the output's variance. In a deterministic framework, when the\nsame inputs always give the same outputs, these indices are usually estimated\nby replicated simulations of the model. In a stochastic framework, when the\nresponse given a set of input parameters is not unique due to randomness in the\nmodel, metamodels are often used to approximate the mean and dispersion of the\nresponse by deterministic functions. We propose a new non-parametric estimator\nwithout the need of defining a metamodel to estimate the Sobol indices of order\n1. The estimator is based on warped wavelets and is adaptive in the regularity\nof the model. The convergence of the mean square error to zero, when the number\nof simulations of the model tend to infinity, is computed and an elbow effect\nis shown, depending on the regularity of the model. Applications in\nEpidemiology are carried to illustrate the use of non-parametric estimators. \n\n"}
{"id": "1611.07237", "contents": "Title: Multivariate Intensity Estimation via Hyperbolic Wavelet Selection Abstract: We propose a new statistical procedure able in some way to overcome the curse\nof dimensionality without structural assumptions on the function to estimate.\nIt relies on a least-squares type penalized criterion and a new collection of\nmodels built from hyperbolic biorthogonal wavelet bases. We study its\nproperties in a unifying intensity estimation framework, where an oracle-type\ninequality and adaptation to mixed smoothness are shown to hold. Besides, we\ndescribe an algorithm for implementing the estimator with a quite reasonable\ncomplexity. \n\n"}
{"id": "1611.07412", "contents": "Title: Statistical Methods for Thermal Index Estimation Based on Accelerated\n  Destructive Degradation Test Data Abstract: Accelerated destructive degradation test (ADDT) is a technique that is\ncommonly used by industries to access material's long-term properties. In many\napplications, the accelerating variable is usually the temperature. In such\ncases, a thermal index (TI) is used to indicate the strength of the material.\nFor example, a TI of 200C may be interpreted as the material can be expected to\nmaintain a specific property at a temperature of 200C for 100,000 hours. A\nmaterial with a higher TI possesses a stronger resistance to thermal damage. In\nliterature, there are three methods available to estimate the TI based on ADDT\ndata, which are the traditional method based on the least-squares approach, the\nparametric method, and the semiparametric method. In this chapter, we provide a\ncomprehensive review of the three methods and illustrate how the TI can be\nestimated based on different models. We also conduct comprehensive simulation\nstudies to show the properties of different methods. We provide thorough\ndiscussions on the pros and cons of each method. The comparisons and discussion\nin this chapter can be useful for practitioners and future industrial\nstandards. \n\n"}
{"id": "1611.07469", "contents": "Title: Fast Measurements of Robustness to Changing Priors in Variational Bayes Abstract: In Bayesian analysis, the posterior follows from the data and a choice of a\nprior and a likelihood. One hopes that the posterior is robust to reasonable\nvariation in the choice of prior, since this choice is made by the modeler and\nis often somewhat subjective. A different, equally subjectively plausible\nchoice of prior may result in a substantially different posterior, and so\ndifferent conclusions drawn from the data. Were this to be the case, our\nconclusions would not be robust to the choice of prior. To determine whether\nour model is robust, we must quantify how sensitive our posterior is to\nperturbations of our prior. Despite the importance of the problem and a\nconsiderable body of literature, generic, easy-to-use methods to quantify\nBayesian robustness are still lacking.\n  Abstract In this paper, we demonstrate that powerful measures of robustness\ncan be easily calculated from Variational Bayes (VB) approximate posteriors. We\nbegin with local robustness, which measures the effect of infinitesimal changes\nto the prior on a posterior mean of interest. In particular, we show that the\ninfluence function of Gustafson (2012) has a simple, easy-to-calculate closed\nform expression for VB approximations. We then demonstrate how local robustness\nmeasures can be inadequate for non-local prior changes, such as replacing one\nprior entirely with another. We propose a simple approximate non-local\nrobustness measure and demonstrate its effectiveness on a simulated data set. \n\n"}
{"id": "1611.07911", "contents": "Title: An efficient surrogate model for emulation and physics extraction of\n  large eddy simulations Abstract: In the quest for advanced propulsion and power-generation systems,\nhigh-fidelity simulations are too computationally expensive to survey the\ndesired design space, and a new design methodology is needed that combines\nengineering physics, computer simulations and statistical modeling. In this\npaper, we propose a new surrogate model that provides efficient prediction and\nuncertainty quantification of turbulent flows in swirl injectors with varying\ngeometries, devices commonly used in many engineering applications. The novelty\nof the proposed method lies in the incorporation of known physical properties\nof the fluid flow as {simplifying assumptions} for the statistical model. In\nview of the massive simulation data at hand, which is on the order of hundreds\nof gigabytes, these assumptions allow for accurate flow predictions in around\nan hour of computation time. To contrast, existing flow emulators which forgo\nsuch simplications may require more computation time for training and\nprediction than is needed for conducting the simulation itself. Moreover, by\naccounting for coupling mechanisms between flow variables, the proposed model\ncan jointly reduce prediction uncertainty and extract useful flow physics,\nwhich can then be used to guide further investigations. \n\n"}
{"id": "1612.00111", "contents": "Title: Bayesian Non-parametric Simultaneous Quantile Regression for Complete\n  and Grid Data Abstract: In this paper, we consider Bayesian methods for non-parametric quantile\nregressions with multiple continuous predictors ranging values in the unit\ninterval. In the first method, the quantile function is assumed to be smooth\nover the explanatory variable and is expanded in tensor product of B-spline\nbasis functions. While in the second method, the distribution function is\nassumed to be smooth over the explanatory variable and is expanded in tensor\nproduct of B-spline basis functions. Unlike other existing methods of\nnon-parametric quantile regressions, the proposed methods estimate the whole\nquantile function instead of estimating on a grid of quantiles. Priors on the\nB-spline coefficients are put in such a way that the monotonicity of the\nestimated quantile levels are maintained unlike local polynomial quantile\nregression methods. The proposed methods have also been modified for quantile\ngrid data where only the percentile range of each response observations are\nknown. Simulations studies have been provided for both complete and quantile\ngrid data. The proposed method has been used to estimate the quantiles of US\nhousehold income data and North Atlantic hurricane intensity data. \n\n"}
{"id": "1612.00877", "contents": "Title: Bayesian sparse multiple regression for simultaneous rank reduction and\n  variable selection Abstract: We develop a Bayesian methodology aimed at simultaneously estimating low-rank\nand row-sparse matrices in a high-dimensional multiple-response linear\nregression model. We consider a carefully devised shrinkage prior on the matrix\nof regression coefficients which obviates the need to specify a prior on the\nrank, and shrinks the regression matrix towards low-rank and row-sparse\nstructures. We provide theoretical support to the proposed methodology by\nproving minimax optimality of the posterior mean under the prediction risk in\nultra-high dimensional settings where the number of predictors can grow\nsub-exponentially relative to the sample size. A one-step post-processing\nscheme induced by group lasso penalties on the rows of the estimated\ncoefficient matrix is proposed for variable selection, with default choices of\ntuning parameters. We additionally provide an estimate of the rank using a\nnovel optimization function achieving dimension reduction in the covariate\nspace. We exhibit the performance of the proposed methodology in an extensive\nsimulation study and a real data example. \n\n"}
{"id": "1612.01773", "contents": "Title: Peaks over thresholds modelling with multivariate generalized Pareto\n  distributions Abstract: When assessing the impact of extreme events, it is often not just a single\ncomponent, but the combined behaviour of several components which is important.\nStatistical modelling using multivariate generalized Pareto (GP) distributions\nconstitutes the multivariate analogue of univariate peaks over thresholds\nmodelling, which is widely used in finance and engineering. We develop general\nmethods for construction of multivariate GP distributions and use them to\ncreate a variety of new statistical models. A censored likelihood procedure is\nproposed to make inference on these models, together with a threshold selection\nprocedure, goodness-of-fit diagnostics, and a computationally tractable\nstrategy for model selection. The models are fitted to returns of stock prices\nof four UK-based banks and to rainfall data in the context of landslide risk\nestimation. Supplementary materials and codes are available online. \n\n"}
{"id": "1612.02195", "contents": "Title: Generalized Exponential smoothing in prediction of hierarchical time\n  series Abstract: Shang and Hyndman (2017) proposed a grouped functional time series\nforecasting approach as a combination of individual forecasts obtained using\ngeneralized least squares method. We modify their methodology using generalized\nexponential smoothing technique for the most disaggregated functional time\nseries in order to obtain more robust predictor. We discuss some properties of\nour proposals basing on results obtained via simulation studies and analysis of\nreal data related to a prediction of a demand for electricity in Australia in\n2016. \n\n"}
{"id": "1612.03054", "contents": "Title: DERGMs: Degeneracy-restricted exponential random graph models Abstract: Exponential random graph models, or ERGMs, are a flexible and general class\nof models for modeling dependent data. While the early literature has shown\nthem to be powerful in capturing many network features of interest, recent work\nhighlights difficulties related to the models' ill behavior, such as most of\nthe probability mass being concentrated on a very small subset of the parameter\nspace. This behavior limits both the applicability of an ERGM as a model for\nreal data and inference and parameter estimation via the usual Markov chain\nMonte Carlo algorithms.\n  To address this problem, we propose a new exponential family of models for\nrandom graphs that build on the standard ERGM framework. Specifically, we solve\nthe problem of computational intractability and `degenerate' model behavior by\nan interpretable support restriction. We introduce a new parameter based on the\ngraph-theoretic notion of degeneracy, a measure of sparsity whose value is\ncommonly low in real-worlds networks. The new model family is supported on the\nsample space of graphs with bounded degeneracy and is called\ndegeneracy-restricted ERGMs, or DERGMs for short. Since DERGMs generalize ERGMs\n-- the latter is obtained from the former by setting the degeneracy parameter\nto be maximal -- they inherit good theoretical properties, while at the same\ntime place their mass more uniformly over realistic graphs.\n  The support restriction allows the use of new (and fast) Monte Carlo methods\nfor inference, thus making the models scalable and computationally tractable.\nWe study various theoretical properties of DERGMs and illustrate how the\nsupport restriction improves the model behavior. We also present a fast Monte\nCarlo algorithm for parameter estimation that avoids many issues faced by\nMarkov Chain Monte Carlo algorithms used for inference in ERGMs. \n\n"}
{"id": "1612.03278", "contents": "Title: Improved prediction accuracy for disease risk mapping using Gaussian\n  Process stacked generalisation Abstract: Maps of infectious disease---charting spatial variations in the force of\ninfection, degree of endemicity, and the burden on human health---provide an\nessential evidence base to support planning towards global health targets.\nContemporary disease mapping efforts have embraced statistical modelling\napproaches to properly acknowledge uncertainties in both the available\nmeasurements and their spatial interpolation. The most common such approach is\nthat of Gaussian process regression, a mathematical framework comprised of two\ncomponents: a mean function harnessing the predictive power of multiple\nindependent variables, and a covariance function yielding spatio-temporal\nshrinkage against residual variation from the mean. Though many techniques have\nbeen developed to improve the flexibility and fitting of the covariance\nfunction, models for the mean function have typically been restricted to simple\nlinear terms. For infectious diseases, known to be driven by complex\ninteractions between environmental and socio-economic factors, improved\nmodelling of the mean function can greatly boost predictive power. Here we\npresent an ensemble approach based on stacked generalisation that allows for\nmultiple, non-linear algorithmic mean functions to be jointly embedded within\nthe Gaussian process framework. We apply this method to mapping Plasmodium\nfalciparum prevalence data in Sub-Saharan Africa and show that the generalised\nensemble approach markedly out-performs any individual method. \n\n"}
{"id": "1612.04093", "contents": "Title: Modified Cholesky Riemann Manifold Hamiltonian Monte Carlo: Exploiting\n  Sparsity for Fast Sampling of High-dimensional Targets Abstract: Riemann manifold Hamiltonian Monte Carlo (RMHMC) has the potential to produce\nhigh-quality Markov chain Monte Carlo-output even for very challenging target\ndistributions. To this end, a symmetric positive definite scaling matrix for\nRMHMC, which derives, via a modified Cholesky factorization, from the\npotentially indefinite negative Hessian of the target log-density is proposed.\nThe methodology is able to exploit the sparsity of the Hessian, stemming from\nconditional independence modeling assumptions, and thus admit fast\nimplementation of RMHMC even for high-dimensional target distributions.\nMoreover, the methodology can exploit log-concave conditional target densities,\noften encountered in Bayesian hierarchical models, for faster sampling and more\nstraight forward tuning. The proposed methodology is compared to alternatives\nfor some challenging targets, and is illustrated by applying a state space\nmodel to real data. \n\n"}
{"id": "1612.04185", "contents": "Title: Sleep Apnea Detection Based on Thoracic and Abdominal Movement Signals\n  of Wearable Piezo-Electric Bands Abstract: Physiologically, the thoracic (THO) and abdominal (ABD) movement signals,\ncaptured using wearable piezo-electric bands, provide information about various\ntypes of apnea, including central sleep apnea (CSA) and obstructive sleep apnea\n(OSA). However, the use of piezo-electric wearables in detecting sleep apnea\nevents has been seldom explored in the literature. This study explored the\npossibility of identifying sleep apnea events, including OSA and CSA, by solely\nanalyzing {one or both the THO and ABD signals. An adaptive non-harmonic model\nwas introduced to model the THO and ABD signals, which allows us to design\nfeatures for sleep apnea events. To confirm the suitability of the extracted\nfeatures, a support vector machine was applied to classify three categories --\nnormal and hypopnea, OSA, and CSA. According to a database of} 34 subjects, the\noverall classification accuracies were on average $75.9\\%\\pm 11.7\\%$ and\n$73.8\\%\\pm 4.4\\%$, respectively, based on the cross validation. When the\nfeatures determined from the THO and ABD signals were combined, the overall\nclassification accuracy became $81.8\\%\\pm 9.4\\%$. These features were applied\nfor designing a state machine for online apnea event detection. Two\nevent-by-event accuracy indices, S and I, were proposed for evaluating the\nperformance {of the state machine. For the same database, the} S index was\n$84.01\\%\\pm 9.06\\%$, and the I index was $77.21\\%\\pm 19.01\\%$. The results\nindicate the considerable potential of applying the proposed algorithm to\nclinical examinations for both screening and homecare purposes. \n\n"}
{"id": "1612.07010", "contents": "Title: Permutation in genetic association studies with covariates: controlling\n  the familywise error rate with score tests in generalized linear models Abstract: In genome-wide association (GWA) studies the goal is to detect associations\nbetween genetic markers and a given phenotype. The number of genetic markers\ncan be large and effective methods for control of the overall error rate is a\ncentral topic when analyzing GWA data. The Bonferroni method is known to be\nconservative when the tests are dependent. Permutation methods give exact\ncontrol of the overall error rate when the assumption of exchangeability is\nsatisfied, but are computationally intensive for large datasets. For regression\nmodels the exchangeability assumption is in general not satisfied and there is\nno standard solution on how to do permutation testing, except some approximate\nmethods. In this paper we will discuss permutation methods for control of the\nfamilywise error rate in genetic association studies and present an approximate\nsolution. These methods will be compared using simulated data. \n\n"}
{"id": "1612.09123", "contents": "Title: Population and trends in the global mean temperature Abstract: The Fisher Ideal index, developed to measure price inflation, is applied to\ndefine a population-weighted temperature trend. This method has the advantages\nthat the trend is representative for the population distribution throughout the\nsample but without conflating the trend in the population distribution and the\ntrend in the temperature. I show that the trend in the global area-weighted\naverage surface air temperature is different in key details from the\npopulation-weighted trend. I extend the index to include urbanization and the\nurban heat island effect. This substantially changes the trend again. I further\nextend the index to include international migration, but this has a minor\nimpact on the trend. \n\n"}
{"id": "1701.00311", "contents": "Title: Bayesian model selection consistency and oracle inequality with\n  intractable marginal likelihood Abstract: In this article, we investigate large sample properties of model selection\nprocedures in a general Bayesian framework when a closed form expression of the\nmarginal likelihood function is not available or a local asymptotic quadratic\napproximation of the log-likelihood function does not exist. Under appropriate\nidentifiability assumptions on the true model, we provide sufficient conditions\nfor a Bayesian model selection procedure to be consistent and obey the Occam's\nrazor phenomenon, i.e., the probability of selecting the \"smallest\" model that\ncontains the truth tends to one as the sample size goes to infinity. In order\nto show that a Bayesian model selection procedure selects the smallest model\ncontaining the truth, we impose a prior anti-concentration condition, requiring\nthe prior mass assigned by large models to a neighborhood of the truth to be\nsufficiently small. In a more general setting where the strong model\nidentifiability assumption may not hold, we introduce the notion of local\nBayesian complexity and develop oracle inequalities for Bayesian model\nselection procedures. Our Bayesian oracle inequality characterizes a trade-off\nbetween the approximation error and a Bayesian characterization of the local\ncomplexity of the model, illustrating the adaptive nature of averaging-based\nBayesian procedures towards achieving an optimal rate of posterior convergence.\nSpecific applications of the model selection theory are discussed in the\ncontext of high-dimensional nonparametric regression and density regression\nwhere the regression function or the conditional density is assumed to depend\non a fixed subset of predictors. As a result of independent interest, we\npropose a general technique for obtaining upper bounds of certain small ball\nprobability of stationary Gaussian processes. \n\n"}
{"id": "1701.04006", "contents": "Title: Probabilistic Numerical Methods for PDE-constrained Bayesian Inverse\n  Problems Abstract: This paper develops meshless methods for probabilistically describing\ndiscretisation error in the numerical solution of partial differential\nequations. This construction enables the solution of Bayesian inverse problems\nwhile accounting for the impact of the discretisation of the forward problem.\nIn particular, this drives statistical inferences to be more conservative in\nthe presence of significant solver error. Theoretical results are presented\ndescribing rates of convergence for the posteriors in both the forward and\ninverse problems. This method is tested on a challenging inverse problem with a\nnonlinear forward model. \n\n"}
{"id": "1701.04735", "contents": "Title: Asymptotic Theory and Statistical Decomposability gap Estimation for\n  Takayama's Index Abstract: In the spirit of recent asymptotic works on the General Poverty Index (GPI)\nin the field of Welfare Analysis, the asymptotic representation of the\nnon-decomposable Takayama's index, which has failed to be incorporated in the\nunified GPI approach, is addressed and established here. This representation\nallows also to extend to it, recent results of statistical decomposability gaps\nestimations. The theoretical results are applied to real databases. The\nconclusions of the undertaken applications recommend to use Takayama's index as\na practically decomposable one, in virtue of the low decomposability gaps with\nrespect to the large values of the index. \n\n"}
{"id": "1701.05128", "contents": "Title: A Constructive Approach to High-dimensional Regression Abstract: We develop a constructive approach to estimating sparse, high-dimensional\nlinear regression models. The approach is a computational algorithm motivated\nfrom the KKT conditions for the $\\ell_0$-penalized least squares solutions. It\ngenerates a sequence of solutions iteratively, based on support detection using\nprimal and dual information and root finding. We refer to the algorithm as SDAR\nfor brevity. Under a sparse Rieze condition on the design matrix and certain\nother conditions, we show that with high probability, the $\\ell_2$ estimation\nerror of the solution sequence decays exponentially to the minimax error bound\nin $O(\\sqrt{J}\\log(R))$ steps; and under a mutual coherence condition and\ncertain other conditions, the $\\ell_{\\infty}$ estimation error decays to the\noptimal error bound in $O(\\log(R))$ steps, where $J$ is the number of important\npredictors, $R$ is the relative magnitude of the nonzero target coefficients.\nComputational complexity analysis shows that the cost of SDAR is $O(np)$ per\niteration. Moreover the oracle least squares estimator can be exactly recovered\nwith high probability at the same cost if we know the sparsity level. We also\nconsider an adaptive version of SDAR to make it more practical in applications.\nNumerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR\nis competitive with or outperforms them in accuracy and efficiency. \n\n"}
{"id": "1701.05146", "contents": "Title: On parameter estimation with the Wasserstein distance Abstract: Statistical inference can be performed by minimizing, over the parameter\nspace, the Wasserstein distance between model distributions and the empirical\ndistribution of the data. We study asymptotic properties of such minimum\nWasserstein distance estimators, complementing results derived by Bassetti,\nBodini and Regazzini in 2006. In particular, our results cover the misspecified\nsetting, in which the data-generating process is not assumed to be part of the\nfamily of distributions described by the model. Our results are motivated by\nrecent applications of minimum Wasserstein estimators to complex generative\nmodels. We discuss some difficulties arising in the approximation of these\nestimators and illustrate their behavior in several numerical experiments. Two\nof our examples are taken from the literature on approximate Bayesian\ncomputation and have likelihood functions that are not analytically tractable.\nTwo other examples involve misspecified models. \n\n"}
{"id": "1701.05179", "contents": "Title: Covariate powered cross-weighted multiple testing Abstract: A fundamental task in the analysis of datasets with many variables is\nscreening for associations. This can be cast as a multiple testing task, where\nthe objective is achieving high detection power while controlling type I error.\nWe consider $m$ hypothesis tests represented by pairs $((P_i, X_i))_{1\\leq i\n\\leq m}$ of p-values $P_i$ and covariates $X_i$, such that $P_i \\perp X_i$ if\n$H_i$ is null. Here, we show how to use information potentially available in\nthe covariates about heterogeneities among hypotheses to increase power\ncompared to conventional procedures that only use the $P_i$. To this end, we\nupgrade existing weighted multiple testing procedures through the Independent\nHypothesis Weighting (IHW) framework to use data-driven weights that are\ncalculated as a function of the covariates. Finite sample guarantees, e.g.,\nfalse discovery rate (FDR) control, are derived from cross-weighting, a\ndata-splitting approach that enables learning the weight-covariate function\nwithout overfitting as long as the hypotheses can be partitioned into\nindependent folds, with arbitrary within-fold dependence. IHW has increased\npower compared to methods that do not use covariate information. A key\nimplication of IHW is that hypothesis rejection in common multiple testing\nsetups should not proceed according to the ranking of the p-values, but by an\nalternative ranking implied by the covariate-weighted p-values. \n\n"}
{"id": "1701.05870", "contents": "Title: Permutation tests for the equality of covariance operators of functional\n  data with applications to evolutionary biology Abstract: In this paper, we generalize the metric-based permutation test for the\nequality of covariance operators proposed by Pigoli et al. (2014) to the case\nof multiple samples of functional data. To this end, the non-parametric\ncombination methodology of Pesarin and Salmaso (2010) is used to combine all\nthe pairwise comparisons between samples into a global test. Different\ncombining functions and permutation strategies are reviewed and analyzed in\ndetail. The resulting test allows to make inference on the equality of the\ncovariance operators of multiple groups and, if there is evidence to reject the\nnull hypothesis, to identify the pairs of groups having different covariances.\nIt is shown that, for some combining functions, step-down adjusting procedures\nare available to control for the multiple testing problem in this setting. The\nempirical power of this new test is then explored via simulations and compared\nwith those of existing alternative approaches in different scenarios. Finally,\nthe proposed methodology is applied to data from wheel running activity\nexperiments, that used selective breeding to study the evolution of locomotor\nbehavior in mice. \n\n"}
{"id": "1701.06619", "contents": "Title: Bayesian Inference in the Presence of Intractable Normalizing Functions Abstract: Models with intractable normalizing functions arise frequently in statistics.\nCommon examples of such models include exponential random graph models for\nsocial networks and Markov point processes for ecology and disease modeling.\nInference for these models is complicated because the normalizing functions of\ntheir probability distributions include the parameters of interest. In Bayesian\nanalysis they result in so-called doubly intractable posterior distributions\nwhich pose significant computational challenges. Several Monte Carlo methods\nhave emerged in recent years to address Bayesian inference for such models. We\nprovide a framework for understanding the algorithms and elucidate connections\namong them. Through multiple simulated and real data examples, we compare and\ncontrast the computational and statistical efficiency of these algorithms and\ndiscuss their theoretical bases. Our study provides practical recommendations\nfor practitioners along with directions for future research for MCMC\nmethodologists. \n\n"}
{"id": "1701.07011", "contents": "Title: Controlling false discoveries in Bayesian gene networks with lasso\n  regression p-values Abstract: Bayesian networks can represent directed gene regulations and therefore are\nfavored over co-expression networks. However, hardly any Bayesian network study\nconcerns the false discovery control (FDC) of network edges, leading to low\naccuracies due to systematic biases from inconsistent false discovery levels in\nthe same study. We design four empirical tests to examine the FDC of Bayesian\nnetworks from three p-value based lasso regression variable selections --- two\nexisting and one we originate. Our method, lassopv, computes p-values for the\ncritical regularization strength at which a predictor starts to contribute to\nlasso regression. Using null and Geuvadis datasets, we find that lassopv\nobtains optimal FDC in Bayesian gene networks, whilst existing methods have\ndefective p-values. The FDC concept and tests extend to most network inference\nscenarios and will guide the design and improvement of new and existing\nmethods. Our novel variable selection method with lasso regression also allows\nFDC on other datasets and questions, even beyond network inference and\ncomputational biology. Lassopv is implemented in R and freely available at\nhttps://github.com/lingfeiwang/lassopv and\nhttps://cran.r-project.org/package=lassopv \n\n"}
{"id": "1701.07638", "contents": "Title: The impact of stochastic lead times on the bullwhip effect under\n  correlated demand and moving average forecasts Abstract: We quantify the bullwhip effect (which measures how the variance in\nreplenishment orders is amplified as the orders move up the supply chain) when\nrandom demands and random lead times are estimated using the industrially\npopular moving average forecasting method. We assume that the lead times\nconstitute a sequence of independent identically distributed random variables\nand correlated demands are described by a first order autoregressive process.\nWe obtain an expression that reveals the impact of demand and lead time\nforecasting on the bullwhip effect. We draw a number of conclusions on the\nbehavior of the bullwhip effect with respect to the demand auto-correlation and\nthe number of past lead times and demands used in the forecasts. Furthermore we\nfind the maxima and minima of the bullwhip measure as a function of the demand\nauto-correlation. \n\n"}
{"id": "1701.07787", "contents": "Title: Multi-locus data distinguishes between population growth and multiple\n  merger coalescents Abstract: We introduce a low dimensional function of the site frequency spectrum that\nis tailor-made for distinguishing coalescent models with multiple mergers from\nKingman coalescent models with population growth, and use this function to\nconstruct a hypothesis test between these model classes. The null and\nalternative sampling distributions of the statistic are intractable, but its\nlow dimensionality renders them amenable to Monte Carlo estimation. We\nconstruct kernel density estimates of the sampling distributions based on\nsimulated data, and show that the resulting hypothesis test dramatically\nimproves on the statistical power of a current state-of-the-art method. A key\nreason for this improvement is the use of multi-locus data, in particular\naveraging observed site frequency spectra across unlinked loci to reduce\nsampling variance. We also demonstrate the robustness of our method to nuisance\nand tuning parameters. Finally we show that the same kernel density estimates\ncan be used to conduct parameter estimation, and argue that our method is\nreadily generalisable for applications in model selection, parameter inference\nand experimental design. \n\n"}
{"id": "1701.07910", "contents": "Title: Combining Envelope Methodology and Aster Models for Variance Reduction\n  in Life History Analyses Abstract: Precise estimation of expected Darwinian fitness, the expected lifetime\nnumber of offspring of organism, is a central component of life history\nanalysis. The aster model serves as a defensible statistical model for\ndistributions of Darwinian fitness. The aster model is equipped to incorporate\nthe major life stages an organism travels through which separately may effect\nDarwinian fitness. Envelope methodology reduces asymptotic variability by\nestablishing a link between unknown parameters of interest and the asymptotic\ncovariance matrices of their estimators. It is known both theoretically and in\napplications that incorporation of envelope methodology reduces asymptotic\nvariability. We develop an envelope framework, including a new envelope\nestimator, that is appropriate for aster analyses. The level of precision\nprovided from our methods allows researchers to draw stronger conclusions about\nthe driving forces of Darwinian fitness from their life history analyses than\nthey could with the aster model alone. Our methods are illustrated on a\nsimulated dataset and a life history analysis of \\emph{Mimulus guttatus}\nflowers is provided. Useful variance reduction is obtained in both analyses. \n\n"}
{"id": "1701.08055", "contents": "Title: Modelling Competitive Sports: Bradley-Terry-\\'{E}l\\H{o} Models for\n  Supervised and On-Line Learning of Paired Competition Outcomes Abstract: Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins/draws/losses, batch/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds. \n\n"}
{"id": "1702.00434", "contents": "Title: Efficient algorithms for Bayesian Nearest Neighbor Gaussian Processes Abstract: We consider alternate formulations of recently proposed hierarchical Nearest\nNeighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved\nconvergence, faster computing time, and more robust and reproducible Bayesian\ninference. Algorithms are defined that improve CPU memory management and\nexploit existing high-performance numerical linear algebra libraries.\nComputational and inferential benefits are assessed for alternate NNGP\nspecifications using simulated datasets and remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska. The resulting data product is the\nfirst statistically robust map of forest canopy for the TIU. \n\n"}
{"id": "1702.00501", "contents": "Title: Adaptive gPCA: A method for structured dimensionality reduction Abstract: When working with large biological data sets, exploratory analysis is an\nimportant first step for understanding the latent structure and for generating\nhypotheses to be tested in subsequent analyses. However, when the number of\nvariables is large compared to the number of samples, standard methods such as\nprincipal components analysis give results which are unstable and difficult to\ninterpret.\n  To mitigate these problems, we have developed a method which allows the\nanalyst to incorporate side information about the relationships between the\nvariables in a way that encourages similar variables to have similar loadings\non the principal axes. This leads to a low-dimensional representation of the\nsamples which both describes the latent structure and which has axes which are\ninterpretable in terms of groups of closely related variables.\n  The method is derived by putting a prior encoding the relationships between\nthe variables on the data and following through the analysis on the posterior\ndistributions of the samples. We show that our method does well at\nreconstructing true latent structure in simulated data and we also demonstrate\nthe method on a dataset investigating the effects of antibiotics on the\ncomposition of bacteria in the human gut. \n\n"}
{"id": "1702.00584", "contents": "Title: Ultra Reliable Short Message Relaying with Wireless Power Transfer Abstract: We consider a dual-hop wireless network where an energy constrained relay\nnode first harvests energy through the received radio-frequency signal from the\nsource, and then uses the harvested energy to forward the source's information\nto the destination node. The throughput and delay metrics are investigated for\na decode-and-forward relaying mechanism at finite blocklength regime and\ndelay-limited transmission mode. We consider ultra-reliable communication\nscenarios under discussion for the next fifth-generation of wireless systems,\nwith error and latency constraints. The impact on these metrics of the\nblocklength, information bits, and relay position is investigated. \n\n"}
{"id": "1702.01906", "contents": "Title: Affiliation networks with an increasing degree sequence Abstract: Affiliation network is one kind of two-mode social network with two different\nsets of nodes (namely, a set of actors and a set of social events) and edges\nrepresenting the affiliation of the actors with the social events. Although a\nnumber of statistical models are proposed to analyze affiliation networks, the\nasymptotic behaviors of the estimator are still unknown or have not been\nproperly explored. In this paper, we study an affiliation model with the degree\nsequence as the exclusively natural sufficient statistic in the exponential\nfamily distributions. We establish the uniform consistency and asymptotic\nnormality of the maximum likelihood estimator when the numbers of actors and\nevents both go to infinity. Simulation studies and a real data example\ndemonstrate our theoretical results. \n\n"}
{"id": "1702.02025", "contents": "Title: Efficient fetal-maternal ECG signal separation from two channel maternal\n  abdominal ECG via diffusion-based channel selection Abstract: There is a need for affordable, widely deployable maternal-fetal ECG monitors\nto improve maternal and fetal health during pregnancy and delivery. Based on\nthe diffusion-based channel selection, here we present the mathematical\nformalism and clinical validation of an algorithm capable of accurate\nseparation of maternal and fetal ECG from a two channel signal acquired over\nmaternal abdomen. \n\n"}
{"id": "1702.02686", "contents": "Title: Rate Optimal Estimation and Confidence Intervals for High-dimensional\n  Regression with Missing Covariates Abstract: Although a majority of the theoretical literature in high-dimensional\nstatistics has focused on settings which involve fully-observed data, settings\nwith missing values and corruptions are common in practice. We consider the\nproblems of estimation and of constructing component-wise confidence intervals\nin a sparse high-dimensional linear regression model when some covariates of\nthe design matrix are missing completely at random. We analyze a variant of the\nDantzig selector [9] for estimating the regression model and we use a\nde-biasing argument to construct component-wise confidence intervals. Our first\nmain result is to establish upper bounds on the estimation error as a function\nof the model parameters (the sparsity level s, the expected fraction of\nobserved covariates $\\rho_*$, and a measure of the signal strength\n$\\|\\beta^*\\|_2$). We find that even in an idealized setting where the\ncovariates are assumed to be missing completely at random, somewhat\nsurprisingly and in contrast to the fully-observed setting, there is a\ndichotomy in the dependence on model parameters and much faster rates are\nobtained if the covariance matrix of the random design is known. To study this\nissue further, our second main contribution is to provide lower bounds on the\nestimation error showing that this discrepancy in rates is unavoidable in a\nminimax sense. We then consider the problem of high-dimensional inference in\nthe presence of missing data. We construct and analyze confidence intervals\nusing a de-biased estimator. In the presence of missing data, inference is\ncomplicated by the fact that the de-biasing matrix is correlated with the pilot\nestimator and this necessitates the design of a new estimator and a novel\nanalysis. We also complement our mathematical study with extensive simulations\non synthetic and semi-synthetic data that show the accuracy of our asymptotic\npredictions for finite sample sizes. \n\n"}
{"id": "1702.03628", "contents": "Title: Multilevel Monte Carlo in Approximate Bayesian Computation Abstract: In the following article we consider approximate Bayesian computation (ABC)\ninference. We introduce a method for numerically approximating ABC posteriors\nusing the multilevel Monte Carlo (MLMC). A sequential Monte Carlo version of\nthe approach is developed and it is shown under some assumptions that for a\ngiven level of mean square error, this method for ABC has a lower cost than\ni.i.d. sampling from the most accurate ABC approximation. Several numerical\nexamples are given. \n\n"}
{"id": "1702.04690", "contents": "Title: Simple rules for complex decisions Abstract: From doctors diagnosing patients to judges setting bail, experts often base\ntheir decisions on experience and intuition rather than on statistical models.\nWhile understandable, relying on intuition over models has often been found to\nresult in inferior outcomes. Here we present a new method,\nselect-regress-and-round, for constructing simple rules that perform well for\ncomplex decisions. These rules take the form of a weighted checklist, can be\napplied mentally, and nonetheless rival the performance of modern machine\nlearning algorithms. Our method for creating these rules is itself simple, and\ncan be carried out by practitioners with basic statistics knowledge. We\ndemonstrate this technique with a detailed case study of judicial decisions to\nrelease or detain defendants while they await trial. In this application, as in\nmany policy settings, the effects of proposed decision rules cannot be directly\nobserved from historical data: if a rule recommends releasing a defendant that\nthe judge in reality detained, we do not observe what would have happened under\nthe proposed action. We address this key counterfactual estimation problem by\ndrawing on tools from causal inference. We find that simple rules significantly\noutperform judges and are on par with decisions derived from random forests\ntrained on all available features. Generalizing to 22 varied decision-making\ndomains, we find this basic result replicates. We conclude with an analytical\nframework that helps explain why these simple decision rules perform as well as\nthey do. \n\n"}
{"id": "1702.05056", "contents": "Title: An Empirical Bayes Approach for High Dimensional Classification Abstract: We propose an empirical Bayes estimator based on Dirichlet process mixture\nmodel for estimating the sparse normalized mean difference, which could be\ndirectly applied to the high dimensional linear classification. In theory, we\nbuild a bridge to connect the estimation error of the mean difference and the\nmisclassification error, also provide sufficient conditions of sub-optimal\nclassifiers and optimal classifiers. In implementation, a variational Bayes\nalgorithm is developed to compute the posterior efficiently and could be\nparallelized to deal with the ultra-high dimensional case. \n\n"}
{"id": "1702.06240", "contents": "Title: Debiased Machine Learning of Conditional Average Treatment Effects and\n  Other Causal Functions Abstract: This paper provides estimation and inference methods for the best linear\npredictor (approximation) of a structural function, such as conditional average\nstructural and treatment effects, and structural derivatives, based on modern\nmachine learning (ML) tools. We represent this structural function as a\nconditional expectation of an unbiased signal that depends on a nuisance\nparameter, which we estimate by modern machine learning techniques. We first\nadjust the signal to make it insensitive (Neyman-orthogonal) with respect to\nthe first-stage regularization bias. We then project the signal onto a set of\nbasis functions, growing with sample size, which gives us the best linear\npredictor of the structural function. We derive a complete set of results for\nestimation and simultaneous inference on all parameters of the best linear\npredictor, conducting inference by Gaussian bootstrap. When the structural\nfunction is smooth and the basis is sufficiently rich, our estimation and\ninference result automatically targets this function. When basis functions are\ngroup indicators, the best linear predictor reduces to group average\ntreatment/structural effect, and our inference automatically targets these\nparameters. We demonstrate our method by estimating uniform confidence bands\nfor the average price elasticity of gasoline demand conditional on income. \n\n"}
{"id": "1702.06986", "contents": "Title: Rank conditional coverage and confidence intervals in high dimensional\n  problems Abstract: Confidence interval procedures used in low dimensional settings are often\ninappropriate for high dimensional applications. When a large number of\nparameters are estimated, marginal confidence intervals associated with the\nmost significant estimates have very low coverage rates: They are too small and\ncentered at biased estimates. The problem of forming confidence intervals in\nhigh dimensional settings has previously been studied through the lens of\nselection adjustment. In this framework, the goal is to control the proportion\nof non-covering intervals formed for selected parameters.\n  In this paper we approach the problem by considering the relationship between\nrank and coverage probability. Marginal confidence intervals have very low\ncoverage rates for significant parameters and high rates for parameters with\nmore boring estimates. Many selection adjusted intervals display the same\npattern. This connection motivates us to propose a new coverage criterion for\nconfidence intervals in multiple testing/covering problems --- the rank\nconditional coverage (RCC). This is the expected coverage rate of an interval\ngiven the significance ranking for the associated estimator. We propose\ninterval construction via bootstrapping which produces small intervals and have\na rank conditional coverage close to the nominal level. These methods are\nimplemented in the R package rcc. \n\n"}
{"id": "1702.07981", "contents": "Title: BayCount: A Bayesian Decomposition Method for Inferring Tumor\n  Heterogeneity using RNA-Seq Counts Abstract: Tumor is heterogeneous - a tumor sample usually consists of a set of\nsubclones with distinct transcriptional profiles and potentially different\ndegrees of aggressiveness and responses to drugs. Understanding tumor\nheterogeneity is therefore critical to precise cancer prognosis and treatment.\nIn this paper, we introduce BayCount, a Bayesian decomposition method to infer\ntumor heterogeneity with highly over-dispersed RNA sequencing count data. Using\nnegative binomial factor analysis, BayCount takes into account both the\nbetween-sample and gene-specific random effects on raw counts of sequencing\nreads mapped to each gene. For posterior inference, we develop an efficient\ncompound Poisson based blocked Gibbs sampler. Through extensive simulation\nstudies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer\nRNA sequencing count data, we show that BayCount is able to accurately estimate\nthe number of subclones, the proportions of these subclones in each tumor\nsample, and the gene expression profiles in each subclone. Our method\nrepresents the first effort in characterizing tumor heterogeneity using RNA\nsequencing count data that simultaneously removes the need of normalizing the\ncounts, achieves statistical robustness, and obtains biologically and\nclinically meaningful insights. \n\n"}
{"id": "1702.08560", "contents": "Title: Estimating the reproductive number, total outbreak size, and reporting\n  rates for Zika epidemics in South and Central America Abstract: As South and Central American countries prepare for increased birth defects\nfrom Zika virus outbreaks and plan for mitigation strategies to minimize\nongoing and future outbreaks, understanding important characteristics of Zika\noutbreaks and how they vary across regions is a challenging and important\nproblem. We developed a mathematical model for the 2015 Zika virus outbreak\ndynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly\navailable data provided by the Pan American Health Organization, using\nApproximate Bayesian Computation to estimate parameter distributions and\nprovide uncertainty quantification. An important model input is the at-risk\nsusceptible population, which can vary with a number of factors including\nclimate, elevation, population density, and socio-economic status. We informed\nthis initial condition using the highest historically reported dengue incidence\nmodified by the probable dengue reporting rates in the chosen countries. The\nmodel indicated that a country-level analysis was not appropriate for Colombia.\nWe then estimated the basic reproduction number, or the expected number of new\nhuman infections arising from a single infected human, to range between 4 and 6\nfor El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We\nestimated the reporting rate to be around 16% in El Salvador and 18% in\nSuriname with estimated total outbreak sizes of 73,395 and 21,647 people,\nrespectively. The uncertainty in parameter estimates highlights a need for\nresearch and data collection that will better constrain parameter ranges. \n\n"}
{"id": "1703.00056", "contents": "Title: Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments Abstract: Recidivism prediction instruments (RPI's) provide decision makers with an\nassessment of the likelihood that a criminal defendant will reoffend at a\nfuture point in time. While such instruments are gaining increasing popularity\nacross the country, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses several fairness criteria that have recently\nbeen applied to assess the fairness of recidivism prediction instruments. We\ndemonstrate that the criteria cannot all be simultaneously satisfied when\nrecidivism prevalence differs across groups. We then show how disparate impact\ncan arise when a recidivism prediction instrument fails to satisfy the\ncriterion of error rate balance. \n\n"}
{"id": "1703.00154", "contents": "Title: Inertial Odometry on Handheld Smartphones Abstract: Building a complete inertial navigation system using the limited quality data\nprovided by current smartphones has been regarded challenging, if not\nimpossible. This paper shows that by careful crafting and accounting for the\nweak information in the sensor samples, smartphones are capable of pure\ninertial navigation. We present a probabilistic approach for orientation and\nuse-case free inertial odometry, which is based on double-integrating rotated\naccelerations. The strength of the model is in learning additive and\nmultiplicative IMU biases online. We are able to track the phone position,\nvelocity, and pose in real-time and in a computationally lightweight fashion by\nsolving the inference with an extended Kalman filter. The information fusion is\ncompleted with zero-velocity updates (if the phone remains stationary),\naltitude correction from barometric pressure readings (if available), and\npseudo-updates constraining the momentary speed. We demonstrate our approach\nusing an iPad and iPhone in several indoor dead-reckoning applications and in a\nmeasurement tool setup. \n\n"}
{"id": "1703.00654", "contents": "Title: Nonparametric estimation of galaxy cluster's emissivity and point source\n  detection in astrophysics with two lasso penalties Abstract: Astrophysicists are interested in recovering the 3D gas emissivity of a\ngalaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and\npresence of point sources make this inverse problem even harder to solve. The\ncurrent state-of-the-art technique is two step: first identify the location of\npotential point sources, then mask these locations and deproject the data.\n  We instead model the data as a Poisson generalized linear model (involving\nblurring, Abel and wavelets operators) regularized by two lasso penalties to\ninduce sparse wavelet representation and sparse point sources. The amount of\nsparsity is controlled by two quantile universal thresholds. As a result, our\nmethod outperforms the existing one. \n\n"}
{"id": "1703.01234", "contents": "Title: A Bayesian computer model analysis of Robust Bayesian analyses Abstract: We harness the power of Bayesian emulation techniques, designed to aid the\nanalysis of complex computer models, to examine the structure of complex\nBayesian analyses themselves. These techniques facilitate robust Bayesian\nanalyses and/or sensitivity analyses of complex problems, and hence allow\nglobal exploration of the impacts of choices made in both the likelihood and\nprior specification. We show how previously intractable problems in robustness\nstudies can be overcome using emulation techniques, and how these methods allow\nother scientists to quickly extract approximations to posterior results\ncorresponding to their own particular subjective specification. The utility and\nflexibility of our method is demonstrated on a reanalysis of a real application\nwhere Bayesian methods were employed to capture beliefs about river flow. We\ndiscuss the obvious extensions and directions of future research that such an\napproach opens up. \n\n"}
{"id": "1703.02441", "contents": "Title: Statistical Analysis of the Ricker Model Abstract: The Ricker model was introduced in the context of managing fishing stocks. It\nis a discrete non-linear iterative model given by $N(t+1)=rN(t)\\exp(-N(t))$\nwhere $N(t)$ is the population at time $t$. The model treated in this paper\nincludes a random component $N(t+1)=rN(t)\\exp(-N(t)+\\varepsilon(t+1))$ and what\nis observed at time $t$ is a Poisson random variable with parameter $\\varphi\nN(t)$. Such a model has been analysed using `synthetic likelihood' and ABC\n(Approximate Bayesian Computation). In contrast this paper takes a\nnon-likelihood approach and treats the model in a consistent manner as an\napproximation. The goal is to specify those parameter values if any which are\nconsistent with the data. \n\n"}
{"id": "1703.02834", "contents": "Title: Exact Dimensionality Selection for Bayesian PCA Abstract: We present a Bayesian model selection approach to estimate the intrinsic\ndimensionality of a high-dimensional dataset. To this end, we introduce a novel\nformulation of the probabilisitic principal component analysis model based on a\nnormal-gamma prior distribution. In this context, we exhibit a closed-form\nexpression of the marginal likelihood which allows to infer an optimal number\nof components. We also propose a heuristic based on the expected shape of the\nmarginal likelihood curve in order to choose the hyperparameters. In\nnon-asymptotic frameworks, we show on simulated data that this exact\ndimensionality selection approach is competitive with both Bayesian and\nfrequentist state-of-the-art methods. \n\n"}
{"id": "1703.02870", "contents": "Title: Statistical Inference in Political Networks Research Abstract: Researchers interested in statistically modeling network data have a\nwell-established and quickly growing set of approaches from which to choose.\nSeveral of these methods have been regularly applied in research on political\nnetworks, while others have yet to permeate the field. Here, we review the most\nprominent methods of inferential network analysis---both for cross-sectionally\nand longitudinally observed networks including (temporal) exponential random\ngraph models, latent space models, the quadratic assignment procedure, and\nstochastic actor oriented models. For each method, we summarize its analytic\nform, identify prominent published applications in political science and\ndiscuss computational considerations. We conclude with a set of guidelines for\nselecting a method for a given application. \n\n"}
{"id": "1703.03165", "contents": "Title: Perturbation Bootstrap in Adaptive Lasso Abstract: The Adaptive Lasso(Alasso) was proposed by Zou [\\textit{J. Amer. Statist.\nAssoc. \\textbf{101} (2006) 1418-1429}] as a modification of the Lasso for the\npurpose of simultaneous variable selection and estimation of the parameters in\na linear regression model. Zou (2006) established that the Alasso estimator is\nvariable-selection consistent as well as asymptotically Normal in the indices\ncorresponding to the nonzero regression coefficients in certain\nfixed-dimensional settings. In an influential paper, Minnier, Tian and Cai\n[\\textit{J. Amer. Statist. Assoc. \\textbf{106} (2011) 1371-1382}] proposed a\nperturbation bootstrap method and established its distributional consistency\nfor the Alasso estimator in the fixed-dimensional setting. In this paper,\nhowever, we show that this (naive) perturbation bootstrap fails to achieve\nsecond order correctness in approximating the distribution of the Alasso\nestimator. We propose a modification to the perturbation bootstrap objective\nfunction and show that a suitably studentized version of our modified\nperturbation bootstrap Alasso estimator achieves second-order correctness even\nwhen the dimension of the model is allowed to grow to infinity with the sample\nsize. As a consequence, inferences based on the modified perturbation bootstrap\nwill be more accurate than the inferences based on the oracle Normal\napproximation. We give simulation studies demonstrating good finite-sample\nproperties of our modified perturbation bootstrap method as well as an\nillustration of our method on a real data set. \n\n"}
{"id": "1703.03862", "contents": "Title: Joint Embedding of Graphs Abstract: Feature extraction and dimension reduction for networks is critical in a wide\nvariety of domains. Efficiently and accurately learning features for multiple\ngraphs has important applications in statistical inference on graphs. We\npropose a method to jointly embed multiple undirected graphs. Given a set of\ngraphs, the joint embedding method identifies a linear subspace spanned by rank\none symmetric matrices and projects adjacency matrices of graphs into this\nsubspace. The projection coefficients can be treated as features of the graphs,\nwhile the embedding components can represent vertex features. We also propose a\nrandom graph model for multiple graphs that generalizes other classical models\nfor graphs. We show through theory and numerical experiments that under the\nmodel, the joint embedding method produces estimates of parameters with small\nerrors. Via simulation experiments, we demonstrate that the joint embedding\nmethod produces features which lead to state of the art performance in\nclassifying graphs. Applying the joint embedding method to human brain graphs,\nwe find it extracts interpretable features with good prediction accuracy in\ndifferent tasks. \n\n"}
{"id": "1703.04956", "contents": "Title: A Short Note on Almost Sure Convergence of Bayes Factors in the General\n  Set-Up Abstract: Although there is a significant literature on the asymptotic theory of Bayes\nfactor, the set-ups considered are usually specialized and often involves\nindependent and identically distributed data. Even in such specialized cases,\nmostly weak consistency results are available. In this article, for the first\ntime ever, we derive the almost sure convergence theory of Bayes factor in the\ngeneral set-up that includes even dependent data and misspecified models.\nSomewhat surprisingly, the key to the proof of such a general theory is a\nsimple application of a result of Shalizi (2009) to a well-known identity\nsatisfied by the Bayes factor. \n\n"}
{"id": "1703.04957", "contents": "Title: An algorithm for removing sensitive information: application to\n  race-independent recidivism prediction Abstract: Predictive modeling is increasingly being employed to assist human\ndecision-makers. One purported advantage of replacing or augmenting human\njudgment with computer models in high stakes settings-- such as sentencing,\nhiring, policing, college admissions, and parole decisions-- is the perceived\n\"neutrality\" of computers. It is argued that because computer models do not\nhold personal prejudice, the predictions they produce will be equally free from\nprejudice. There is growing recognition that employing algorithms does not\nremove the potential for bias, and can even amplify it if the training data\nwere generated by a process that is itself biased. In this paper, we provide a\nprobabilistic notion of algorithmic bias. We propose a method to eliminate bias\nfrom predictive models by removing all information regarding protected\nvariables from the data to which the models will ultimately be trained. Unlike\nprevious work in this area, our framework is general enough to accommodate data\non any measurement scale. Motivated by models currently in use in the criminal\njustice system that inform decisions on pre-trial release and parole, we apply\nour proposed method to a dataset on the criminal histories of individuals at\nthe time of sentencing to produce \"race-neutral\" predictions of re-arrest. In\nthe process, we demonstrate that a common approach to creating \"race-neutral\"\nmodels-- omitting race as a covariate-- still results in racially disparate\npredictions. We then demonstrate that the application of our proposed method to\nthese data removes racial disparities from predictions with minimal impact on\npredictive accuracy. \n\n"}
{"id": "1703.05264", "contents": "Title: Total Variation Regularized Tensor-on-scalar Regression Abstract: In this paper, we propose Total Variation Regularized Tensor-on-scalar\nRegression(TVTR), a novel method for estimating the association between a\ntensor outcome (a one dimensional or multidimensional array) and scalar\npredictors. While the statistical developments proposed here were motivated by\nthe brain mapping and activity tracking, the methodology is designed and\npresented in generality and is applicable to many other areas of scientific\nresearch. The estimator is the solution of a penalized regression problem where\nthe objective is the sum of square error plus a total variation (TV)\nregularization on the predicted mean across all subjects. We propose an\nalgorithm for the parameter estimation, which is efficient and scalable in\ndistributed computing platform. Proof of the algorithm convergence is provided\nand the statistical consistency of the estimator is presented via an oracle\ninequality. We presented 1D and 2D simulation results, and demonstrate that\nTVTR outperforms existing methods in most cases. We also demonstrate the\ngeneral applicability of the method by two real data examples including the\nanalysis of the 1D accelerometry subsample of a large community-based study for\nmood disorders and the analysis of the 3D MRI data from the attention\ndeficient/hyperactive deficient (ADHD) 200 consortium. \n\n"}
{"id": "1703.05532", "contents": "Title: Clustering of Gamma-Ray bursts through kernel principal component\n  analysis Abstract: We consider the problem related to clustering of gamma-ray bursts (from\n\"BATSE\" catalogue) through kernel principal component analysis in which our\nproposed kernel outperforms results of other competent kernels in terms of\nclustering accuracy and we obtain three physically interpretable groups of\ngamma-ray bursts. The effectivity of the suggested kernel in combination with\nkernel principal component analysis in revealing natural clusters in noisy and\nnonlinear data while reducing the dimension of the data is also explored in two\nsimulated data sets. \n\n"}
{"id": "1703.05687", "contents": "Title: Gaussian process regression for forecasting battery state of health Abstract: Accurately predicting the future capacity and remaining useful life of\nbatteries is necessary to ensure reliable system operation and to minimise\nmaintenance costs. The complex nature of battery degradation has meant that\nmechanistic modelling of capacity fade has thus far remained intractable;\nhowever, with the advent of cloud-connected devices, data from cells in various\napplications is becoming increasingly available, and the feasibility of\ndata-driven methods for battery prognostics is increasing. Here we propose\nGaussian process (GP) regression for forecasting battery state of health, and\nhighlight various advantages of GPs over other data-driven and mechanistic\napproaches. GPs are a type of Bayesian non-parametric method, and hence can\nmodel complex systems whilst handling uncertainty in a principled manner. Prior\ninformation can be exploited by GPs in a variety of ways: explicit mean\nfunctions can be used if the functional form of the underlying degradation\nmodel is available, and multiple-output GPs can effectively exploit\ncorrelations between data from different cells. We demonstrate the predictive\ncapability of GPs for short-term and long-term (remaining useful life)\nforecasting on a selection of capacity vs. cycle datasets from lithium-ion\ncells. \n\n"}
{"id": "1703.06031", "contents": "Title: Modeling spatial processes with unknown extremal dependence class Abstract: Many environmental processes exhibit weakening spatial dependence as events\nbecome more extreme. Well-known limiting models, such as max-stable or\ngeneralized Pareto processes, cannot capture this, which can lead to a\npreference for models that exhibit a property known as asymptotic independence.\nHowever, weakening dependence does not automatically imply asymptotic\nindependence, and whether the process is truly asymptotically (in)dependent is\nusually far from clear. The distinction is key as it can have a large impact\nupon extrapolation, i.e., the estimated probabilities of events more extreme\nthan those observed. In this work, we present a single spatial model that is\nable to capture both dependence classes in a parsimonious manner, and with a\nsmooth transition between the two cases. The model covers a wide range of\npossibilities from asymptotic independence through to complete dependence, and\npermits weakening dependence of extremes even under asymptotic dependence.\nCensored likelihood-based inference for the implied copula is feasible in\nmoderate dimensions due to closed-form margins. The model is applied to\noceanographic datasets with ambiguous true limiting dependence structure. \n\n"}
{"id": "1703.06154", "contents": "Title: An MCMC-free approach to post-selective inference Abstract: We develop a Monte Carlo-free approach to inference post output from\nrandomized algorithms with a convex loss and a convex penalty. The pivotal\nstatistic based on a truncated law, called the selective pivot, usually lacks\nclosed form expressions. Inference in these settings relies upon standard Monte\nCarlo sampling techniques at a reference parameter followed by an exponential\ntilting at the reference. Tilting can however be unstable for parameters that\nare far off from the reference parameter. We offer in this paper an alternative\napproach to construction of intervals and point estimates by proposing an\napproximation to the intractable selective pivot. Such an approximation solves\na convex optimization problem in |E| dimensions, where |E| is the size of the\nactive set observed from selection. We empirically show that the confidence\nintervals obtained by inverting the approximate pivot have valid coverage. \n\n"}
{"id": "1703.06222", "contents": "Title: A unified treatment of multiple testing with prior knowledge using the\n  p-filter Abstract: There is a significant literature on methods for incorporating knowledge into\nmultiple testing procedures so as to improve their power and precision. Some\ncommon forms of prior knowledge include (a) beliefs about which hypotheses are\nnull, modeled by non-uniform prior weights; (b) differing importances of\nhypotheses, modeled by differing penalties for false discoveries; (c) multiple\narbitrary partitions of the hypotheses into (possibly overlapping) groups; and\n(d) knowledge of independence, positive or arbitrary dependence between\nhypotheses or groups, suggesting the use of more aggressive or conservative\nprocedures. We present a unified algorithmic framework called p-filter for\nglobal null testing and false discovery rate (FDR) control that allows the\nscientist to incorporate all four types of prior knowledge (a)-(d)\nsimultaneously, recovering a variety of known algorithms as special cases. \n\n"}
{"id": "1703.06719", "contents": "Title: Bayesian reconstruction of past land-cover from pollen data: model\n  robustness and sensitivity to auxiliary variables Abstract: Realistic depictions of past land cover are needed to investigate prehistoric\nenvironmental changes, effects of anthropogenic deforestation, and long term\nland cover-climate feedbacks. Observation based reconstructions of past land\ncover are rare and commonly used model based reconstructions exhibit\nconsiderable differences. Recently \\citet[Spatial Statistics,\n24:14--31,][]{PirzaLPG2018_24} developed a statistical interpolation method\nthat produces spatially complete reconstructions of past land cover from pollen\nassemblage. These reconstructions incorporate a number of auxiliary datasets\nraising questions regarding the method's sensitivity to different auxiliary\ndatasets.\n  Here the sensitivity of the method is examined by performing spatial\nreconstructions for northern Europe during three time periods (1900 CE, 1725 CE\nand 4000 BCE). The auxiliary datasets considered include the most commonly\nutilized sources of past land-cover data --- e.g.\\ estimates produced by a\ndynamic vegetation (DVM) and anthropogenic land-cover change (ALCC) models.\nFive different auxiliary datasets were considered, including different climate\ndata driving the DVM and different ALCC models. The resulting reconstructions\nwere also evaluated using cross-validation for all the time periods. For the\nrecent time period, 1900 CE, the different land-cover reconstructions were\ncompared against a present day forest map.\n  The validation confirms that the statistical model provides a robust spatial\ninterpolation tool with low sensitivity to differences in auxiliary data and\nhigh capacity to capture information in the pollen based proxy data. Further\nauxiliary data with high spatial detail improves model performance for areas\nwith complex topography or few observations. \n\n"}
{"id": "1703.06808", "contents": "Title: Worth Weighting? How to Think About and Use Weights in Survey\n  Experiments Abstract: The popularity of online surveys has increased the prominence of using\nweights that capture units' probabilities of inclusion for claims of\nrepresentativeness. Yet, much uncertainty remains regarding how these weights\nshould be employed in the analysis of survey experiments: Should they be used\nor ignored? If they are used, which estimators are preferred? We offer\npractical advice, rooted in the Neyman-Rubin model, for researchers producing\nand working with survey experimental data. We examine simple, efficient\nestimators for analyzing these data, and give formulae for their biases and\nvariances. We provide simulations that examine these estimators as well as real\nexamples from experiments administered online through YouGov. We find that for\nexamining the existence of population treatment effects using high-quality,\nbroadly representative samples recruited by top online survey firms, sample\nquantities, which do not rely on weights, are often sufficient. We found that\nSample Average Treatment Effect (SATE) estimates did not appear to differ\nsubstantially from their weighted counterparts, and they avoided the\nsubstantial loss of statistical power that accompanies weighting. When precise\nestimates of Population Average Treatment Effects (PATE) are essential, we\nanalytically show post-stratifying on survey weights and/or covariates highly\ncorrelated with the outcome to be a conservative choice. While we show these\nsubstantial gains in simulations, we find limited evidence of them in practice. \n\n"}
{"id": "1703.07309", "contents": "Title: Phytoplankton Hotspot Prediction With an Unsupervised Spatial Community\n  Model Abstract: Many interesting natural phenomena are sparsely distributed and discrete.\nLocating the hotspots of such sparsely distributed phenomena is often difficult\nbecause their density gradient is likely to be very noisy. We present a novel\napproach to this search problem, where we model the co-occurrence relations\nbetween a robot's observations with a Bayesian nonparametric topic model. This\napproach makes it possible to produce a robust estimate of the spatial\ndistribution of the target, even in the absence of direct target observations.\nWe apply the proposed approach to the problem of finding the spatial locations\nof the hotspots of a specific phytoplankton taxon in the ocean. We use\nclassified image data from Imaging FlowCytobot (IFCB), which automatically\nmeasures individual microscopic cells and colonies of cells. Given these\nindividual taxon-specific observations, we learn a phytoplankton community\nmodel that characterizes the co-occurrence relations between taxa. We present\nexperiments with simulated robot missions drawn from real observation data\ncollected during a research cruise traversing the US Atlantic coast. Our\nresults show that the proposed approach outperforms nearest neighbor and\nk-means based methods for predicting the spatial distribution of hotspots from\nin-situ observations. \n\n"}
{"id": "1703.07904", "contents": "Title: Cross-Validation with Confidence Abstract: Cross-validation is one of the most popular model selection methods in\nstatistics and machine learning. Despite its wide applicability, traditional\ncross validation methods tend to select overfitting models, due to the\nignorance of the uncertainty in the testing sample. We develop a new,\nstatistically principled inference tool based on cross-validation that takes\ninto account the uncertainty in the testing sample. This new method outputs a\nset of highly competitive candidate models containing the best one with\nguaranteed probability. As a consequence, our method can achieve consistent\nvariable selection in a classical linear regression setting, for which existing\ncross-validation methods require unconventional split ratios. When used for\nregularizing tuning parameter selection, the method can provide a further\ntrade-off between prediction accuracy and model interpretability. We\ndemonstrate the performance of the proposed method in several simulated and\nreal data examples. \n\n"}
{"id": "1703.08596", "contents": "Title: The Inner Structure of Time-Dependent Signals Abstract: This paper shows how a time series of measurements of an evolving system can\nbe processed to create an inner time series that is unaffected by any\ninstantaneous invertible, possibly nonlinear transformation of the\nmeasurements. An inner time series contains information that does not depend on\nthe nature of the sensors, which the observer chose to monitor the system.\nInstead, it encodes information that is intrinsic to the evolution of the\nobserved system. Because of its sensor-independence, an inner time series may\nproduce fewer false negatives when it is used to detect events in the presence\nof sensor drift. Furthermore, if the observed physical system is comprised of\nnon-interacting subsystems, its inner time series is separable; i.e., it\nconsists of a collection of time series, each one being the inner time series\nof an isolated subsystem. Because of this property, an inner time series can be\nused to detect a specific behavior of one of the independent subsystems without\nusing blind source separation to disentangle that subsystem from the others.\nThe method is illustrated by applying it to: 1) an analytic example; 2) the\naudio waveform of one speaker; 3) video images from a moving camera; 4)\nmixtures of audio waveforms of two speakers. \n\n"}
{"id": "1703.08741", "contents": "Title: Clustering and Variable Selection in the Presence of Mixed Variable\n  Types and Missing Data Abstract: We consider the problem of model-based clustering in the presence of many\ncorrelated, mixed continuous and discrete variables, some of which may have\nmissing values. Discrete variables are treated with a latent continuous\nvariable approach and the Dirichlet process is used to construct a mixture\nmodel with an unknown number of components. Variable selection is also\nperformed to identify the variables that are most influential for determining\ncluster membership. The work is motivated by the need to cluster patients\nthought to potentially have autism spectrum disorder (ASD) on the basis of many\ncognitive and/or behavioral test scores. There are a modest number of patients\n(~480) in the data set along with many (~100) test score variables (many of\nwhich are discrete valued and/or missing). The goal of the work is to (i)\ncluster these patients into similar groups to help identify those with similar\nclinical presentation, and (ii) identify a sparse subset of tests that inform\nthe clusters in order to eliminate unnecessary testing. The proposed approach\ncompares very favorably to other methods via simulation of problems of this\ntype. The results of the ASD analysis suggested three clusters to be most\nlikely, while only four test scores had high (>0.5) posterior probability of\nbeing informative. This will result in much more efficient and informative\ntesting. The need to cluster observations on the basis of many correlated,\ncontinuous/discrete variables with missing values, is a common problem in the\nhealth sciences as well as in many other disciplines. \n\n"}
{"id": "1703.09007", "contents": "Title: Detection of Spatiotemporally Coherent Rainfall Anomalies Using Markov\n  Random Fields Abstract: Precipitation is a large-scale, spatio-temporally heterogeneous phenomenon,\nwith frequent anomalies exhibiting unusually high or low values. We use Markov\nRandom Fields (MRFs) to detect spatio-temporally coherent anomalies in gridded\nannual rainfall data across India from 1901-2005. MRFs are undirected graphical\nmodels where each node is associated with a \\{location,year\\} pair, with edges\nconnecting nodes representing adjacent locations or years. Some nodes represent\nobservations of precipitation, while the rest represent unobserved\n(\\emph{latent}) states that can take one of three values: high/low/normal. The\nMRF represents a probability distribution over the variables, using \\emph{node\npotential} and \\emph{edge potential} functions defined on nodes and edges of\nthe graph. Optimal values of latent state variables are estimated by maximizing\nthe posterior probability of the observations, using Gibbs sampling. Edge\npotentials enforce spatial and temporal coherence, and node potentials\ninfluence threshold for anomalies by affecting the prior probabilities of the\nstates. The model can be tuned to recover anomalies detected by threshold-based\nmethods. The competing influences of spatial and temporal coherence can be\nadjusted through edge potentials.\n  We study spatio-temporal properties of rainfall anomalies discovered by this\nmethod, using suitable measures. We identify nonstationarities in occurrence of\npositive and negative anomalies between the first and second halves of the 20th\ncentury. We find that between these periods, there has been decrease in\nrainfall during June-September (JJAS) and an increase during other months.\nThese effects are highlighted prominently in the statistics of anomalies.\nProperties of anomalies learnt from this approach could present tests of\nregional-scale rainfall simulations by climate models and statistical\nsimulators. \n\n"}
{"id": "1703.09701", "contents": "Title: Sampling Errors in Nested Sampling Parameter Estimation Abstract: Sampling errors in nested sampling parameter estimation differ from those in\nBayesian evidence calculation, but have been little studied in the literature.\nThis paper provides the first explanation of the two main sources of sampling\nerrors in nested sampling parameter estimation, and presents a new diagrammatic\nrepresentation for the process. We find no current method can accurately\nmeasure the parameter estimation errors of a single nested sampling run, and\npropose a method for doing so using a new algorithm for dividing nested\nsampling runs. We empirically verify our conclusions and the accuracy of our\nnew method. \n\n"}
{"id": "1703.09710", "contents": "Title: Fast and scalable Gaussian process modeling with applications to\n  astronomical time series Abstract: The growing field of large-scale time domain astronomy requires methods for\nprobabilistic data analysis that are computationally tractable, even with large\ndatasets. Gaussian Processes are a popular class of models used for this\npurpose but, since the computational cost scales, in general, as the cube of\nthe number of data points, their application has been limited to small\ndatasets. In this paper, we present a novel method for Gaussian Process\nmodeling in one-dimension where the computational requirements scale linearly\nwith the size of the dataset. We demonstrate the method by applying it to\nsimulated and real astronomical time series datasets. These demonstrations are\nexamples of probabilistic inference of stellar rotation periods, asteroseismic\noscillation spectra, and transiting planet parameters. The method exploits\nstructure in the problem when the covariance function is expressed as a mixture\nof complex exponentials, without requiring evenly spaced observations or\nuniform noise. This form of covariance arises naturally when the process is a\nmixture of stochastically-driven damped harmonic oscillators -- providing a\nphysical motivation for and interpretation of this choice -- but we also\ndemonstrate that it can be a useful effective model in some other cases. We\npresent a mathematical description of the method and compare it to existing\nscalable Gaussian Process methods. The method is fast and interpretable, with a\nrange of potential applications within astronomical data analysis and beyond.\nWe provide well-tested and documented open-source implementations of this\nmethod in C++, Python, and Julia. \n\n"}
{"id": "1703.10210", "contents": "Title: A test of weak separability for multi-way functional data, with\n  application to brain connectivity studies Abstract: This paper concerns the modeling of multi-way functional data where double or\nmultiple indices are involved. We introduce a concept of weak separability. The\nweakly separable structure supports the use of factorization methods that\ndecompose the signal into its spatial and temporal components. The analysis\nreveals interesting connections to the usual strongly separable covariance\nstructure, and provides insights into tensor methods for multi-way functional\ndata. We propose a formal test for the weak separability hypothesis, where the\nasymptotic null distribution of the test statistic is a chi-square type\nmixture. The method is applied to study brain functional connectivity derived\nfrom source localized magnetoencephalography signals during motor tasks. \n\n"}
{"id": "1703.10256", "contents": "Title: Predictive mean matching imputation in survey sampling Abstract: Predictive mean matching imputation is popular for handling item nonresponse\nin survey sampling. In this article, we study the asymptotic properties of the\npredictive mean matching estimator of the population mean. For variance\nestimation, the conventional bootstrap inference for matching estimators with\nfixed matches has been shown to be invalid due to the nonsmoothness nature of\nthe matching estimator. We propose asymptotically valid replication variance\nestimation. The key strategy is to construct replicates of the estimator\ndirectly based on linear terms, instead of individual records of variables.\nExtension to nearest neighbor imputation is also discussed. A simulation study\nconfirms that the new procedure provides valid variance estimation. \n\n"}
{"id": "1704.00352", "contents": "Title: Simple Measures of Individual Cluster-Membership Certainty for Hard\n  Partitional Clustering Abstract: We propose two probability-like measures of individual cluster-membership\ncertainty which can be applied to a hard partition of the sample such as that\nobtained from the Partitioning Around Medoids (PAM) algorithm, hierarchical\nclustering or k-means clustering. One measure extends the individual silhouette\nwidths and the other is obtained directly from the pairwise dissimilarities in\nthe sample. Unlike the classic silhouette, however, the measures behave like\nprobabilities and can be used to investigate an individual's tendency to belong\nto a cluster. We also suggest two possible ways to evaluate the hard partition.\nWe evaluate the performance of both measures in individuals with ambiguous\ncluster membership, using simulated binary datasets that have been partitioned\nby the PAM algorithm or continuous datasets that have been partitioned by\nhierarchical clustering and k-means clustering. For comparison, we also present\nresults from soft clustering algorithms such as soft analysis clustering\n(FANNY) and two model-based clustering methods. Our proposed measures perform\ncomparably to the posterior-probability estimators from either FANNY or the\nmodel-based clustering methods. We also illustrate the proposed measures by\napplying them to Fisher's classic iris data set. \n\n"}
{"id": "1704.00436", "contents": "Title: Sparse Bayesian learning with uncertainty models and multiple\n  dictionaries Abstract: Sparse Bayesian learning (SBL) has emerged as a fast and competitive method\nto perform sparse processing. The SBL algorithm, which is developed using a\nBayesian framework, approximately solves a non-convex optimization problem\nusing fixed point updates. It provides comparable performance and is\nsignificantly faster than convex optimization techniques used in sparse\nprocessing. We propose a signal model which accounts for dictionary mismatch\nand the presence of errors in the weight vector at low signal-to-noise ratios.\nA fixed point update equation is derived which incorporates the statistics of\nmismatch and weight errors. We also process observations from multiple\ndictionaries. Noise variances are estimated using stochastic maximum\nlikelihood. The derived update equations are studied quantitatively using\nbeamforming simulations applied to direction-of-arrival (DoA). Performance of\nSBL using single- and multi-frequency observations, and in the presence of\naliasing, is evaluated. SwellEx-96 experimental data demonstrates qualitatively\nthe advantages of SBL. \n\n"}
{"id": "1704.00520", "contents": "Title: Efficient acquisition rules for model-based approximate Bayesian\n  computation Abstract: Approximate Bayesian computation (ABC) is a method for Bayesian inference\nwhen the likelihood is unavailable but simulating from the model is possible.\nHowever, many ABC algorithms require a large number of simulations, which can\nbe costly. To reduce the computational cost, Bayesian optimisation (BO) and\nsurrogate models such as Gaussian processes have been proposed. Bayesian\noptimisation enables one to intelligently decide where to evaluate the model\nnext but common BO strategies are not designed for the goal of estimating the\nposterior distribution. Our paper addresses this gap in the literature. We\npropose to compute the uncertainty in the ABC posterior density, which is due\nto a lack of simulations to estimate this quantity accurately, and define a\nloss function that measures this uncertainty. We then propose to select the\nnext evaluation location to minimise the expected loss. Experiments show that\nthe proposed method often produces the most accurate approximations as compared\nto common BO strategies. \n\n"}
{"id": "1704.00642", "contents": "Title: Local nearest neighbour classification with applications to\n  semi-supervised learning Abstract: We derive a new asymptotic expansion for the global excess risk of a\nlocal-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon\nthe test point. This expansion elucidates conditions under which the dominant\ncontribution to the excess risk comes from the decision boundary of the optimal\nBayes classifier, but we also show that if these conditions are not satisfied,\nthen the dominant contribution may arise from the tails of the marginal\ndistribution of the features. Moreover, we prove that, provided the\n$d$-dimensional marginal distribution of the features has a finite $\\rho$th\nmoment for some $\\rho > 4$ (as well as other regularity conditions), a local\nchoice of $k$ can yield a rate of convergence of the excess risk of\n$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard\n$k$-nearest neighbour classifier, our theory would require $d \\geq 5$ and $\\rho\n> 4d/(d-4)$ finite moments to achieve this rate. These results motivate a new\n$k$-nearest neighbour classifier for semi-supervised learning problems, where\nthe unlabelled data are used to obtain an estimate of the marginal feature\ndensity, and fewer neighbours are used for classification when this density\nestimate is small. Our worst-case rates are complemented by a minimax lower\nbound, which reveals that the local, semi-supervised $k$-nearest neighbour\nclassifier attains the minimax optimal rate over our classes for the excess\nrisk, up to a subpolynomial factor in $n$. These theoretical improvements over\nthe standard $k$-nearest neighbour classifier are also illustrated through a\nsimulation study. \n\n"}
{"id": "1704.01066", "contents": "Title: Tests for qualitative features in the random coefficients model Abstract: The random coefficients model is an extension of the linear regression model\nthat allows for unobserved heterogeneity in the population by modeling the\nregression coefficients as random variables. Given data from this model, the\nstatistical challenge is to recover information about the joint density of the\nrandom coefficients which is a multivariate and ill-posed problem. Because of\nthe curse of dimensionality and the ill-posedness, pointwise nonparametric\nestimation of the joint density is difficult and suffers from slow convergence\nrates. Larger features, such as an increase of the density along some direction\nor a well-accentuated mode can, however, be much easier detected from data by\nmeans of statistical tests. In this article, we follow this strategy and\nconstruct tests and confidence statements for qualitative features of the joint\ndensity, such as increases, decreases and modes. We propose a multiple testing\napproach based on aggregating single tests which are designed to extract shape\ninformation on fixed scales and directions. Using recent tools for Gaussian\napproximations of multivariate empirical processes, we derive expressions for\nthe critical value. We apply our method to simulated and real data. \n\n"}
{"id": "1704.01190", "contents": "Title: Testing for arbitrary interference on experimentation platforms Abstract: Experimentation platforms are essential to modern large technology companies,\nas they are used to carry out many randomized experiments daily. The classic\nassumption of no interference among users, under which the outcome of one user\ndoes not depend on the treatment assigned to other users, is rarely tenable on\nsuch platforms. Here, we introduce an experimental design strategy for testing\nwhether this assumption holds. Our approach is in the spirit of the\nDurbin-Wu-Hausman test for endogeneity in econometrics, where multiple\nestimators return the same estimate if and only if the null hypothesis holds.\nThe design that we introduce makes no assumptions on the interference model\nbetween units, nor on the network among the units, and has a sharp bound on the\nvariance and an implied analytical bound on the type I error rate. We discuss\nhow to apply the proposed design strategy to large experimentation platforms,\nand we illustrate it in the context of an experiment on the LinkedIn platform. \n\n"}
{"id": "1704.02030", "contents": "Title: Using stacking to average Bayesian predictive distributions Abstract: The widely recommended procedure of Bayesian model averaging is flawed in the\nM-open setting in which the true data-generating process is not one of the\ncandidate models being fit. We take the idea of stacking from the point\nestimation literature and generalize to the combination of predictive\ndistributions, extending the utility function to any proper scoring rule, using\nPareto smoothed importance sampling to efficiently compute the required\nleave-one-out posterior distributions and regularization to get more stability.\nWe compare stacking of predictive distributions to several alternatives:\nstacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type\nweighting, and a variant of pseudo-BMA that is stabilized using the Bayesian\nbootstrap. Based on simulations and real-data applications, we recommend\nstacking of predictive distributions, with BB-pseudo-BMA as an approximate\nalternative when computation cost is an issue. \n\n"}
{"id": "1704.02771", "contents": "Title: Group Importance Sampling for Particle Filtering and MCMC Abstract: Bayesian methods and their implementations by means of sophisticated Monte\nCarlo techniques have become very popular in signal processing over the last\nyears. Importance Sampling (IS) is a well-known Monte Carlo technique that\napproximates integrals involving a posterior distribution by means of weighted\nsamples. In this work, we study the assignation of a single weighted sample\nwhich compresses the information contained in a population of weighted samples.\nPart of the theory that we present as Group Importance Sampling (GIS) has been\nemployed implicitly in different works in the literature. The provided analysis\nyields several theoretical and practical consequences. For instance, we discuss\nthe application of GIS into the Sequential Importance Resampling framework and\nshow that Independent Multiple Try Metropolis schemes can be interpreted as a\nstandard Metropolis-Hastings algorithm, following the GIS approach. We also\nintroduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.\nThe first one, named Group Metropolis Sampling method, produces a Markov chain\nof sets of weighted samples. All these sets are then employed for obtaining a\nunique global estimator. The second one is the Distributed Particle\nMetropolis-Hastings technique, where different parallel particle filters are\njointly used to drive an MCMC algorithm. Different resampled trajectories are\ncompared and then tested with a proper acceptance probability. The novel\nschemes are tested in different numerical experiments such as learning the\nhyperparameters of Gaussian Processes, two localization problems in a wireless\nsensor network (with synthetic and real data) and the tracking of vegetation\nparameters given satellite observations, where they are compared with several\nbenchmark Monte Carlo techniques. Three illustrative Matlab demos are also\nprovided. \n\n"}
{"id": "1704.03177", "contents": "Title: Time, Frequency & Time-Varying Causality Measures in Neuroscience Abstract: This article proposes a systematic methodological review and objective\ncriticism of existing methods enabling the derivation of time-varying\nGranger-causality statistics in neuroscience. The increasing interest and the\nhuge number of publications related to this topic calls for this systematic\nreview which describes the very complex methodological aspects. The capacity to\ndescribe the causal links between signals recorded at different brain locations\nduring a neuroscience experiment is of primary interest for neuroscientists,\nwho often have very precise prior hypotheses about the relationships between\nrecorded brain signals that arise at a specific time and in a specific\nfrequency band. The ability to compute a time-varying frequency-specific\ncausality statistic is therefore essential. Two steps are necessary to achieve\nthis: the first consists of finding a statistic that can be interpreted and\nthat directly answers the question of interest. The second concerns the model\nthat underlies the causality statistic and that has this time-frequency\nspecific causality interpretation. In this article, we will review\nGranger-causality statistics with their spectral and time-varying extensions. \n\n"}
{"id": "1704.03239", "contents": "Title: Sparse Bayesian vector autoregressions in huge dimensions Abstract: We develop a Bayesian vector autoregressive (VAR) model with multivariate\nstochastic volatility that is capable of handling vast dimensional information\nsets. Three features are introduced to permit reliable estimation of the model.\nFirst, we assume that the reduced-form errors in the VAR feature a factor\nstochastic volatility structure, allowing for conditional equation-by-equation\nestimation. Second, we apply recently developed global-local shrinkage priors\nto the VAR coefficients to cure the curse of dimensionality. Third, we utilize\nrecent innovations to efficiently sample from high-dimensional multivariate\nGaussian distributions. This makes simulation-based fully Bayesian inference\nfeasible when the dimensionality is large but the time series length is\nmoderate. We demonstrate the merits of our approach in an extensive simulation\nstudy and apply the model to US macroeconomic data to evaluate its forecasting\ncapabilities. \n\n"}
{"id": "1704.03360", "contents": "Title: Redistricting: Drawing the Line Abstract: We develop methods to evaluate whether a political districting accurately\nrepresents the will of the people. To explore and showcase our ideas, we\nconcentrate on the congressional districts for the U.S. House of\nrepresentatives and use the state of North Carolina and its redistrictings\nsince the 2010 census. Using a Monte Carlo algorithm, we randomly generate over\n24,000 redistrictings that are non-partisan and adhere to criteria from\nproposed legislation. Applying historical voting data to these random\nredistrictings, we find that the number of democratic and republican\nrepresentatives elected varies drastically depending on how districts are\ndrawn. Some results are more common, and we gain a clear range of expected\nelection outcomes. Using the statistics of our generated redistrictings, we\ncritique the particular congressional districtings used in the 2012 and 2016 NC\nelections as well as a districting proposed by a bipartisan redistricting\ncommission. We find that the 2012 and 2016 districtings are highly atypical and\nnot representative of the will of the people. On the other hand, our results\nindicate that a plan produced by a bipartisan panel of retired judges is highly\ntypical and representative. Since our analyses are based on an ensemble of\nreasonable redistrictings of North Carolina, they provide a baseline for a\ngiven election which incorporates the geometry of the state's population\ndistribution. \n\n"}
{"id": "1704.03459", "contents": "Title: Dynamic nested sampling: an improved algorithm for parameter estimation\n  and evidence calculation Abstract: We introduce dynamic nested sampling: a generalisation of the nested sampling\nalgorithm in which the number of \"live points\" varies to allocate samples more\nefficiently. In empirical tests the new method significantly improves\ncalculation accuracy compared to standard nested sampling with the same number\nof samples; this increase in accuracy is equivalent to speeding up the\ncomputation by factors of up to ~72 for parameter estimation and ~7 for\nevidence calculations. We also show that the accuracy of both parameter\nestimation and evidence calculations can be improved simultaneously. In\naddition, unlike in standard nested sampling, more accurate results can be\nobtained by continuing the calculation for longer. Popular standard nested\nsampling implementations can be easily adapted to perform dynamic nested\nsampling, and several dynamic nested sampling software packages are now\npublicly available. \n\n"}
{"id": "1704.03995", "contents": "Title: Optimal experimental design that minimizes the width of simultaneous\n  confidence bands Abstract: We propose an optimal experimental design for a curvilinear regression model\nthat minimizes the band-width of simultaneous confidence bands. Simultaneous\nconfidence bands for curvilinear regression are constructed by evaluating the\nvolume of a tube about a curve that is defined as a trajectory of a regression\nbasis vector (Naiman, 1986). The proposed criterion is constructed based on the\nvolume of a tube, and the corresponding optimal design that minimizes the\nvolume of tube is referred to as the tube-volume optimal (TV-optimal) design.\nFor Fourier and weighted polynomial regressions, the problem is formalized as\none of minimization over the cone of Hankel positive definite matrices, and the\ncriterion to minimize is expressed as an elliptic integral. We show that the\nM\\\"obius group keeps our problem invariant, and hence, minimization can be\nconducted over cross-sections of orbits. We demonstrate that for the weighted\npolynomial regression and the Fourier regression with three bases, the\ntube-volume optimal design forms an orbit of the M\\\"obius group containing\nD-optimal designs as representative elements. \n\n"}
{"id": "1704.05074", "contents": "Title: Big Data Analysis Using Shrinkage Strategies Abstract: In this paper, we apply shrinkage strategies to estimate regression\ncoefficients efficiently for the high-dimensional multiple regression model,\nwhere the number of samples is smaller than the number of predictors. We assume\nin the sparse linear model some of the predictors have very weak influence on\nthe response of interest. We propose to shrink estimators more than usual.\nSpecifically, we use integrated estimation strategies in sub and full models\nand shrink the integrated estimators by incorporating a bounded measurable\nfunction of some weights. The exhibited double shrunken estimators improve the\nprediction performance of sub models significantly selected from existing\nLasso-type variable selection methods. Monte Carlo simulation studies as well\nas real examples of eye data and Riboavin data confirm the superior performance\nof the estimators in the high-dimensional regression model. \n\n"}
{"id": "1704.07349", "contents": "Title: A Non-Gaussian, Nonparametric Structure for Gene-Gene and\n  Gene-Environment Interactions in Case-Control Studies Based on Hierarchies of\n  Dirichlet Processes Abstract: It is becoming increasingly clear that complex interactions among genes and\nenvironmental factors play crucial roles in triggering complex diseases. Thus,\nunderstanding such interactions is vital, which is possible only through\nstatistical models that adequately account for such intricate, albeit unknown,\ndependence structures. Bhattacharya & Bhattacharya (2016b) attempt such\nmodeling, relating finite mixtures composed of Dirichlet processes that\nrepresent unknown number of genetic sub-populations through a hierarchical\nmatrix-normal structure that incorporates gene-gene interactions, and possible\nmutations, induced by environmental variables. However, the product dependence\nstructure implied by their matrix-normal model seems to be too simple to be\nappropriate for general complex, realistic situations. In this article, we\npropose and develop a novel nonparametric Bayesian model for case-control\ngenotype data using hierarchies of Dirichlet processes that offers a more\nrealistic and nonparametric dependence structure between the genes, induced by\nthe environmental variables. In this regard, we propose a novel and highly\nparallelisable MCMC algorithm that is rendered quite efficient by the\ncombination of modern parallel computing technology, effective Gibbs sampling\nsteps, retrospective sampling and Transformation based Markov Chain Monte Carlo\n(TMCMC). We use appropriate Bayesian hypothesis testing procedures to detect\nthe roles of genes and environment in case-control studies. We apply our ideas\nto 5 biologically realistic case-control genotype datasets simulated under\ndistinct set-ups, and obtain encouraging results in each case. We finally apply\nour ideas to a real, myocardial infarction dataset, and obtain interesting\nresults on gene-gene and gene-environment interaction, while broadly agreeing\nwith the results reported in the literature. \n\n"}
{"id": "1704.07584", "contents": "Title: Estimating Sparse Signals Using Integrated Wideband Dictionaries Abstract: In this paper, we introduce a wideband dictionary framework for estimating\nsparse signals. By formulating integrated dictionary elements spanning bands of\nthe considered parameter space, one may efficiently find and discard large\nparts of the parameter space not active in the signal. After each iteration,\nthe zero-valued parts of the dictionary may be discarded to allow a refined\ndictionary to be formed around the active elements, resulting in a zoomed\ndictionary to be used in the following iterations. Implementing this scheme\nallows for more accurate estimates, at a much lower computational cost, as\ncompared to directly forming a larger dictionary spanning the whole parameter\nspace or performing a zooming procedure using standard dictionary elements.\nDifferent from traditional dictionaries, the wideband dictionary allows for the\nuse of dictionaries with fewer elements than the number of available samples\nwithout loss of resolution. The technique may be used on both one- and\nmulti-dimensional signals, and may be exploited to refine several traditional\nsparse estimators, here illustrated with the LASSO and the SPICE estimators.\nNumerical examples illustrate the improved performance. \n\n"}
{"id": "1704.07787", "contents": "Title: Finding Exogenous Variation in Data Abstract: We reconsider the classic problem of recovering exogenous variation from an\nendogenous regressor. Two-stage least squares recovers exogenous variation\nthrough presuming the existence of an instrumental variable. We rely instead on\nthe assumption that the regressor is a mixture of exogenous and endogenous\nobservations--say as the result of temporary natural experiments. With this\nassumption, we propose an alternative two-stage method based on\nnonparametrically estimating a mixture model to recover a subset of the\nexogenous observations. We demonstrate that our method recovers exogenous\nobservations in simulation and can be used to find pricing experiments hidden\nin grocery store scanner data. \n\n"}
{"id": "1704.07904", "contents": "Title: Prediction and Inference with Missing Data in Patient Alert Systems Abstract: We describe the Bedside Patient Rescue (BPR) project, the goal of which is\nrisk prediction of adverse events for non-ICU patients using ~200 variables\n(vitals, lab results, assessments, ...). There are several missing predictor\nvalues for most patients, which in the health sciences is the norm, rather than\nthe exception. A Bayesian approach is presented that addresses many of the\nshortcomings to standard approaches to missing predictors: (i) treatment of the\nuncertainty due to imputation is straight-forward in the Bayesian paradigm,\n(ii) the predictor distribution is flexibly modeled as an infinite normal\nmixture with latent variables to explicitly account for discrete predictors\n(i.e., as in multivariate probit regression models), and (iii) certain missing\nnot at random situations can be handled effectively by allowing the indicator\nof missingness into the predictor distribution only to inform the distribution\nof the missing variables. The proposed approach also has the benefit of\nproviding a distribution for the prediction, including the uncertainty inherent\nin the imputation. Therefore, we can ask questions such as: is it possible this\nindividual is at high risk but we are missing too much information to know for\nsure? How much would we reduce the uncertainty in our risk prediction by\nobtaining a particular missing value? This approach is applied to the BPR\nproblem resulting in excellent predictive capability to identify deteriorating\npatients. \n\n"}
{"id": "1704.08066", "contents": "Title: Bootstrap-Based Inference for Cube Root Asymptotics Abstract: This paper proposes a valid bootstrap-based distributional approximation for\nM-estimators exhibiting a Chernoff (1964)-type limiting distribution. For\nestimators of this kind, the standard nonparametric bootstrap is inconsistent.\nThe method proposed herein is based on the nonparametric bootstrap, but\nrestores consistency by altering the shape of the criterion function defining\nthe estimator whose distribution we seek to approximate. This modification\nleads to a generic and easy-to-implement resampling method for inference that\nis conceptually distinct from other available distributional approximations. We\nillustrate the applicability of our results with four examples in econometrics\nand machine learning. \n\n"}
{"id": "1704.08248", "contents": "Title: Modeling and replicating statistical topology, and evidence for CMB\n  non-homogeneity Abstract: Under the banner of `Big Data', the detection and classification of structure\nin extremely large, high dimensional, data sets, is, one of the central\nstatistical challenges of our times. Among the most intriguing approaches to\nthis challenge is `TDA', or `Topological Data Analysis', one of the primary\naims of which is providing non-metric, but topologically informative,\npre-analyses of data sets which make later, more quantitative analyses\nfeasible. While TDA rests on strong mathematical foundations from Topology, in\napplications it has faced challenges due to an inability to handle issues of\nstatistical reliability and robustness and, most importantly, in an inability\nto make scientific claims with verifiable levels of statistical confidence. We\npropose a methodology for the parametric representation, estimation, and\nreplication of persistence diagrams, the main diagnostic tool of TDA. The power\nof the methodology lies in the fact that even if only one persistence diagram\nis available for analysis -- the typical case for big data applications --\nreplications can be generated to allow for conventional statistical hypothesis\ntesting. The methodology is conceptually simple and computationally practical,\nand provides a broadly effective statistical procedure for persistence diagram\nTDA analysis. We demonstrate the basic ideas on a toy example, and the power of\nthe approach in a novel and revealing analysis of CMB non-homogeneity. \n\n"}
{"id": "1705.01727", "contents": "Title: Comparison of hidden Markov chain models and hidden Markov random field\n  models in estimation of computed tomography images Abstract: There is an interest to replace computed tomography (CT) images with magnetic\nresonance (MR) images for a number of diagnostic and therapeutic workflows. In\nthis article, predicting CT images from a number of magnetic resonance imaging\n(MRI) sequences using regression approach is explored. Two principal areas of\napplication for estimated CT images are dose calculations in MRI-based\nradiotherapy treatment planning and attenuation correction for positron\nemission tomography (PET)/MRI. The main purpose of this work is to investigate\nthe performance of hidden Markov (chain) models (HMMs) in comparison to hidden\nMarkov random field (HMRF) models when predicting CT images of head. Our study\nshows that HMMs have clear advantages over HMRF models in this particular\napplication. Obtained results suggest that HMMs deserve a further study for\ninvestigating their potential in modelling applications where the most natural\ntheoretical choice would be the class of HMRF models. \n\n"}
{"id": "1705.01788", "contents": "Title: An optimal transportation approach for assessing almost stochastic order Abstract: When stochastic dominance $F\\leq_{st}G$ does not hold, we can improve\nagreement to stochastic order by suitably trimming both distributions. In this\nwork we consider the $L_2-$Wasserstein distance, $\\mathcal W_2$, to stochastic\norder of these trimmed versions. Our characterization for that distance\nnaturally leads to consider a $\\mathcal W_2$-based index of disagreement with\nstochastic order, $\\varepsilon_{\\mathcal W_2}(F,G)$. We provide asymptotic\nresults allowing to test $H_0: \\varepsilon_{\\mathcal W_2}(F,G)\\geq\n\\varepsilon_0$ vs $H_a: \\varepsilon_{\\mathcal W_2}(F,G)<\\varepsilon_0$, that,\nunder rejection, would give statistical guarantee of almost stochastic\ndominance. We include a simulation study showing a good performance of the\nindex under the normal model. \n\n"}
{"id": "1705.02935", "contents": "Title: The co-evolution of emotional well-being with weak and strong friendship\n  ties Abstract: Social ties are strongly related to well-being. But what characterizes this\nrelationship? This study investigates social mechanisms explaining how social\nties affect well-being through social integration and social influence, and how\nwell-being affects social ties through social selection. We hypothesize that\nhighly integrated individuals - those with more extensive and dense friendship\nnetworks - report higher emotional well-being than others. Moreover, emotional\nwell-being should be influenced by the well-being of close friends. Finally,\nwell-being should affect friendship selection when individuals prefer others\nwith higher levels of well-being, and others whose well-being is similar to\ntheirs. We test our hypotheses using longitudinal social network and well-being\ndata of 117 individuals living in a graduate housing community. The application\nof a novel extension of Stochastic Actor-Oriented Models for ordered networks\n(ordered SAOMs) allows us to detail and test our hypotheses for weak- and\nstrong-tied friendship networks simultaneously. Results do not support our\nsocial integration and social influence hypotheses but provide evidence for\nselection: individuals with higher emotional well-being tend to have more\nstrong-tied friends, and there are homophily processes regarding emotional\nwell-being in strong-tied networks. Our study highlights the two-directional\nrelationship between social ties and well-being, and demonstrates the\nimportance of considering different tie strengths for various social processes. \n\n"}
{"id": "1705.05618", "contents": "Title: Robust functional regression model for marginal mean and\n  subject-specific inferences Abstract: We introduce flexible robust functional regression models, using various\nheavy-tailed processes, including a Student $t$-process. We propose efficient\nalgorithms in estimating parameters for the marginal mean inferences and in\npredicting conditional means as well interpolation and extrapolation for the\nsubject-specific inferences. We develop bootstrap prediction intervals for\nconditional mean curves. Numerical studies show that the proposed model\nprovides robust analysis against data contamination or distribution\nmisspecification, and the proposed prediction intervals maintain the nominal\nconfidence levels. A real data application is presented as an illustrative\nexample. \n\n"}
{"id": "1705.08580", "contents": "Title: Provable Estimation of the Number of Blocks in Block Models Abstract: Community detection is a fundamental unsupervised learning problem for\nunlabeled networks which has a broad range of applications. Many community\ndetection algorithms assume that the number of clusters $r$ is known apriori.\nIn this paper, we propose an approach based on semi-definite relaxations, which\ndoes not require prior knowledge of model parameters like many existing convex\nrelaxation methods and recovers the number of clusters and the clustering\nmatrix exactly under a broad parameter regime, with probability tending to one.\nOn a variety of simulated and real data experiments, we show that the proposed\nmethod often outperforms state-of-the-art techniques for estimating the number\nof clusters. \n\n"}
{"id": "1705.09561", "contents": "Title: Differentially private significance tests for regression coefficients Abstract: Many data producers seek to provide users access to confidential data without\nunduly compromising data subjects' privacy and confidentiality. One general\nstrategy is to require users to do analyses without seeing the confidential\ndata; for example, analysts only get access to synthetic data or query systems\nthat provide disclosure-protected outputs of statistical models. With synthetic\ndata or redacted outputs, the analyst never really knows how much to trust the\nresulting findings. In particular, if the user did the same analysis on the\nconfidential data, would regression coefficients of interest be statistically\nsignificant or not? We present algorithms for assessing this question that\nsatisfy differential privacy. We describe conditions under which the algorithms\nshould give accurate answers about statistical significance. We illustrate the\nproperties of the proposed methods using artificial and genuine data. \n\n"}
{"id": "1705.09874", "contents": "Title: Targeted Learning with Daily EHR Data Abstract: Electronic health records (EHR) data provide a cost and time-effective\nopportunity to conduct cohort studies of the effects of multiple time-point\ninterventions in the diverse patient population found in real-world clinical\nsettings. Because the computational cost of analyzing EHR data at daily (or\nmore granular) scale can be quite high, a pragmatic approach has been to\npartition the follow-up into coarser intervals of pre-specified length. Current\nguidelines suggest employing a 'small' interval, but the feasibility and\npractical impact of this recommendation has not been evaluated and no formal\nmethodology to inform this choice has been developed. We start filling these\ngaps by leveraging large-scale EHR data from a diabetes study to develop and\nillustrate a fast and scalable targeted learning approach that allows to follow\nthe current recommendation and study its practical impact on inference. More\nspecifically, we map daily EHR data into four analytic datasets using 90, 30,\n15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation\napproach, the longitudinal TMLE, to estimate the causal effects of four dynamic\ntreatment rules with each dataset, and compare the resulting inferences. To\novercome the computational challenges presented by the size of these data, we\npropose a novel TMLE implementation, the 'long-format TMLE', and rely on the\nlatest advances in scalable data-adaptive machine-learning software, xgboost\nand h2o, for estimation of the TMLE nuisance parameters. \n\n"}
{"id": "1705.09976", "contents": "Title: Insert \"Price\" to Coxian Phase-Type Models: An Application to Hospital\n  Charge and Length of Stay Data Abstract: In this paper, we discuss the connection between the RGRST models (Gardiner\net al 2002, Polverejan et al 2003) and the Coxian Phase-Type (CPH) models\n(Marshall et al 2007, Tang 2012) through a construction that converts a special\nsub-class of RGRST models to CPH models. Both of the two models are widely used\nto characterize the distribution of hospital charge and length of stay (LOS),\nbut the lack of connections between them makes the two models rarely used\ntogether. We claim that our construction can make up this gap and make it\npossible to take advantage of the two different models simultaneously. As a\nconsequence, we derive a measure of the \"price\" of staying in each medical\nstage (identified with phases of a CPH model), which can't be approached\nwithout considering the RGRST and CPH models together.A two-stage algorithm is\nprovided to generate consistent estimation of model parameters. Applying the\nalgorithm to a sample drawn from the New York State's Statewide Planning and\nResearch Cooperative System 2013 (SPARCS 2013), we estimate the prices in a\nfour-phase CPH model and discuss the implications. \n\n"}
{"id": "1705.10922", "contents": "Title: Network-based identification of disease genes in expression data: the\n  GeneSurrounder method Abstract: The advent of high--throughput transcription profiling technologies has\nenabled identification of genes and pathways associated with disease, providing\nnew avenues for precision medicine. A key challenge is to analyze this data in\nthe context of the regulatory networks and pathways that control cellular\nprocesses, while still obtaining insights that can be used to design new\ndiagnostic and therapeutic interventions. While classical differential\nexpression analysis provides specific and hence targetable gene-level insights,\nit does not include any systems-level information. On the other hand, pathway\nanalyses integrate systems-level information with expression data, but are\noften limited in their ability to indicate specific molecular targets. We\nintroduce GeneSurrounder, an analysis method that takes into account the\ncomplex structure of interaction networks to identify specific genes that\ndisrupt pathway activity in a disease-specific manner. GeneSurrounder\nintegrates transcriptomic data and pathway network information in a novel\ntwo-step procedure to detect genes that (i) appear to influence the expression\nof other genes local to it in the network and (ii) are part of a subnetwork of\ndifferentially expressed genes. Combined, this evidence can be used to pinpoint\nspecific genes that have a mechanistic role in the phenotype of interest.\nApplying GeneSurrounder to three distinct ovarian cancer studies using a global\nKEGG network, we show that our method is able to identify biologically relevant\ngenes and genes missed by single-gene association tests, integrate pathway and\nexpression data, and yield more consistent results across multiple studies of\nthe same phenotype than competing methods. \n\n"}
{"id": "1705.11140", "contents": "Title: Variational Sequential Monte Carlo Abstract: Many recent advances in large scale probabilistic inference rely on\nvariational methods. The success of variational approaches depends on (i)\nformulating a flexible parametric family of distributions, and (ii) optimizing\nthe parameters to find the member of this family that most closely approximates\nthe exact posterior. In this paper we present a new approximating family of\ndistributions, the variational sequential Monte Carlo (VSMC) family, and show\nhow to optimize it in variational inference. VSMC melds variational inference\n(VI) and sequential Monte Carlo (SMC), providing practitioners with flexible,\naccurate, and powerful Bayesian inference. The VSMC family is a variational\nfamily that can approximate the posterior arbitrarily well, while still\nallowing for efficient optimization of its parameters. We demonstrate its\nutility on state space models, stochastic volatility models for financial data,\nand deep Markov models of brain neural circuits. \n\n"}
{"id": "1706.00062", "contents": "Title: A Latent Trait Model for Multivariate Longitudinal Data With Two Sources\n  of Measurement Error Abstract: Personality traits are latent variables, and as such, are impossible to\nmeasure without the use of an assessment. Responses on the assessments can be\ninfluenced by both transient (state-related) error and measurement error,\nobscuring the true trait levels. Typically, these assessments utilize Likert\nscales, which yield only discrete data. The loss of information due to the\ndiscrete nature of the data represents an additional challenge in assessing the\nability of these instruments to measure the latent trait of interest.\n  This paper is concerned with parameter estimation in a model relating a\nlatent variable, as well transient error and measurement error components when\ndata are longitudinal and measured using a Likert scale. Two methods for\nparameter estimation are detailed: correlation reconstruction, a method that\nuses polychoric correlations, and maximum likelihood implemented using a\nStochastic EM algorithm. These methods are applied to a motivating dataset of\n440 college students taking the Big Five inventory twice in a two month period. \n\n"}
{"id": "1706.00103", "contents": "Title: From patterned response dependency to structured covariate dependency:\n  categorical-pattern-matching Abstract: Data generated from a system of interest typically consists of measurements\nfrom an ensemble of subjects across multiple response and covariate features,\nand is naturally represented by one response-matrix against one\ncovariate-matrix. Likely each of these two matrices simultaneously embraces\nheterogeneous data types: continuous, discrete and categorical. Here a matrix\nis used as a practical platform to ideally keep hidden dependency among/between\nsubjects and features intact on its lattice. Response and covariate dependency\nis individually computed and expressed through mutliscale blocks via a newly\ndeveloped computing paradigm named Data Mechanics. We propose a categorical\npattern matching approach to establish causal linkages in a form of information\nflows from patterned response dependency to structured covariate dependency.\nThe strength of an information flow is evaluated by applying the combinatorial\ninformation theory. This unified platform for system knowledge discovery is\nillustrated through five data sets. In each illustrative case, an information\nflow is demonstrated as an organization of discovered knowledge loci via\nemergent visible and readable heterogeneity. This unified approach\nfundamentally resolves many long standing issues, including statistical\nmodeling, multiple response, renormalization and feature selections, in data\nanalysis, but without involving man-made structures and distribution\nassumptions. The results reported here enhance the idea that linking patterns\nof response dependency to structures of covariate dependency is the true\nphilosophical foundation underlying data-driven computing and learning in\nsciences. \n\n"}
{"id": "1706.00308", "contents": "Title: On the asymptotic approximation to the probability distribution of\n  extremal precipitation Abstract: Based on the negative binomial model for the duration of wet periods measured\nin days, an asymptotic approximation is proposed for the distribution of the\nmaximum daily precipitation volume within a wet period. This approximation has\nthe form of a scale mixture of the Frechet distribution with the gamma mixing\ndistribution and coincides with the distribution of a positive power of a\nrandom variable having the Snedecor-Fisher distribution. The proof of this\nresult is based on the representation of the negative binomial distribution as\na mixed geometric (and hence, mixed Poisson) distribution and limit theorems\nfor extreme order statistics in samples with random sizes having mixed Poisson\ndistributions. Some analytic properties of the obtained limit distribution are\ndescribed. In particular, it is demonstrated that under certain conditions the\nlimit distribution is mixed exponential and hence, is infinitely divisible. It\nis shown that under the same conditions the limit distribution can be\nrepresented as a scale mixture of stable or Weibull or Pareto or folded normal\nlaws. The corresponding product representations for the limit random variable\ncan be used for its computer simulation. Several methods are proposed for the\nestimation of the parameters of the distribution of the maximum daily\nprecipitation volume. The results of fitting this distribution to real data are\npresented illustrating high adequacy of the proposed model. The obtained\nmixture representations for the limit laws and the corresponding asymptotic\napproximations provide better insight into the nature of mixed probability\n(\"Bayesian\") models. \n\n"}
{"id": "1706.01752", "contents": "Title: Robust approximate Bayesian inference Abstract: We discuss an approach for deriving robust posterior distributions from\n$M$-estimating functions using Approximate Bayesian Computation (ABC) methods.\nIn particular, we use $M$-estimating functions to construct suitable summary\nstatistics in ABC algorithms. The theoretical properties of the robust\nposterior distributions are discussed. Special attention is given to the\napplication of the method to linear mixed models. Simulation results and an\napplication to a clinical study demonstrate the usefulness of the method. An R\nimplementation is also provided in the robustBLME package. \n\n"}
{"id": "1706.02338", "contents": "Title: Testing the simplifying assumption in high-dimensional vine copulas Abstract: Testing the simplifying assumption in high-dimensional vine copulas is a\ndifficult task. Tests must be based on estimated observations and check\nconstraints on high-dimensional distributions. So far, corresponding tests have\nbeen limited to single conditional copulas with a low-dimensional set of\nconditioning variables. We propose a novel testing procedure that is\ncomputationally feasible for high-dimensional data sets and that exhibits a\npower that decreases only slightly with the dimension. By discretizing the\nsupport of the conditioning variables and incorporating a penalty in the test\nstatistic, we mitigate the curse of dimensionality by looking for the possibly\nstrongest deviation from the simplifying assumption. The use of a decision tree\nrenders the test computationally feasible for large dimensions. We derive the\nasymptotic distribution of the test and analyze its finite sample performance\nin an extensive simulation study. An application of the test to four real data\nsets is provided. \n\n"}
{"id": "1706.02380", "contents": "Title: Multi-sample Estimation of Bacterial Composition Matrix in Metagenomics\n  Data Abstract: Metagenomics sequencing is routinely applied to quantify bacterial abundances\nin microbiome studies, where the bacterial composition is estimated based on\nthe sequencing read counts. Due to limited sequencing depth and DNA dropouts,\nmany rare bacterial taxa might not be captured in the final sequencing reads,\nwhich results in many zero counts. Naive composition estimation using count\nnormalization leads to many zero proportions, which tend to result in\ninaccurate estimates of bacterial abundance and diversity. This paper takes a\nmulti-sample approach to the estimation of bacterial abundances in order to\nborrow information across samples and across species. Empirical results from\nreal data sets suggest that the composition matrix over multiple samples is\napproximately low rank, which motivates a regularized maximum likelihood\nestimation with a nuclear norm penalty. An efficient optimization algorithm\nusing the generalized accelerated proximal gradient and Euclidean projection\nonto simplex space is developed. The theoretical upper bounds and the minimax\nlower bounds of the estimation errors, measured by the Kullback-Leibler\ndivergence and the Frobenius norm, are established. Simulation studies\ndemonstrate that the proposed estimator outperforms the naive estimators. The\nmethod is applied to an analysis of a human gut microbiome dataset. \n\n"}
{"id": "1706.02624", "contents": "Title: Evidence synthesis for stochastic epidemic models Abstract: In recent years the role of epidemic models in informing public health\npolicies has progressively grown. Models have become increasingly realistic and\nmore complex, requiring the use of multiple data sources to estimate all\nquantities of interest. This review summarises the different types of\nstochastic epidemic models that use evidence synthesis and highlights current\nchallenges. \n\n"}
{"id": "1706.02946", "contents": "Title: On the role of the overall effect in exponential families Abstract: Exponential families of discrete probability distributions when the\nnormalizing constant (or overall effect) is added or removed are compared in\nthis paper. The latter setup, in which the exponential family is curved, is\nparticularly relevant when the sample space is an incomplete Cartesian product\nor when it is very large, so that the computational burden is significant. The\nlack or presence of the overall effect has a fundamental impact on the\nproperties of the exponential family. When the overall effect is added, the\nfamily becomes the smallest regular exponential family containing the curved\none. The procedure is related to the homogenization of an inhomogeneous variety\ndiscussed in algebraic geometry, of which a statistical interpretation is given\nas an augmentation of the sample space. The changes in the kernel basis\nrepresentation when the overall effect is included or removed are derived. The\ngeometry of maximum likelihood estimates, also allowing zero observed\nfrequencies, is described with and without the overall effect, and various\nalgorithms are compared. The importance of the results is illustrated by an\nexample from cell biology, showing that routinely including the overall effect\nleads to estimates which are not in the model intended by the researchers. \n\n"}
{"id": "1706.02972", "contents": "Title: Probability, Statistics and Planet Earth, I: Geotemporal covariances Abstract: The study of covariances (or positive definite functions) on the sphere (the\nEarth, in our motivation) goes back to Bochner and Schoenberg (1940--42) and to\nthe first author (1969, 1973), among others. Extending to the geotemporal case\n(sphere cross line, for position and time) was for a long time an obstacle to\ngeostatistical modelling. The characterisation question here was raised by the\nauthors and Mijatovi\\'c in 2016, and answered by Berg and Porcu in 2017.\nExtensions to multiple products (of spheres and lines) follows similarly\n(Guella, Menegatto and Peron, 2016). We survey results of this type, and\nrelated applications e.g. in numerical weather prediction. \n\n"}
{"id": "1706.03012", "contents": "Title: Multi-rubric Models for Ordinal Spatial Data with Application to Online\n  Ratings from Yelp Abstract: Interest in online rating data has increased in recent years in which ordinal\nratings of products or local businesses are provided by users of a website,\nsuch as Yelp or Amazon. One source of heterogeneity in ratings is that users\napply different standards when supplying their ratings; even if two users\nbenefit from a product the same amount, they may translate their benefit into\nratings in different ways. In this article we propose an ordinal data model,\nwhich we refer to as a multi-rubric model, which treats the criteria used to\nconvert a latent utility into a rating as user-specific random effects, with\nthe distribution of these random effects being modeled nonparametrically. We\ndemonstrate that this approach is capable of accounting for this type of\nvariability in addition to usual sources of heterogeneity due to item quality,\nuser biases, interactions between items and users, and the spatial structure of\nthe users and items. We apply the model developed here to publicly available\ndata from the website Yelp and demonstrate that it produces interpretable\nclusterings of users according to their rating behavior, in addition to\nproviding better predictions of ratings and better summaries of overall item\nquality. \n\n"}
{"id": "1706.03423", "contents": "Title: Image-Based Prognostics Using Penalized Tensor Regression Abstract: This paper proposes a new methodology to predict and update the residual\nuseful lifetime of a system using a sequence of degradation images. The\nmethodology integrates tensor linear algebra with traditional location-scale\nregression widely used in reliability and prognosis. To address the high\ndimensionality challenge, the degradation image streams are first projected to\na low-dimensional tensor subspace that is able to preserve their information.\nNext, the projected image tensors are regressed against time-to-failure via\npenalized location-scale tensor regression. The coefficient tensor is then\ndecomposed using CANDECOMP/PARAFAC (CP) and Tucker decompositions, which\nenables parameter estimation in a high-dimensional setting. Two optimization\nalgorithms with a global convergence property are developed for model\nestimation. The effectiveness of our models is validated using a simulated\ndataset and infrared degradation image streams from a rotating machinery. \n\n"}
{"id": "1706.03883", "contents": "Title: Multilevel Clustering via Wasserstein Means Abstract: We propose a novel approach to the problem of multilevel clustering, which\naims to simultaneously partition data in each group and discover grouping\npatterns among groups in a potentially large hierarchically structured corpus\nof data. Our method involves a joint optimization formulation over several\nspaces of discrete probability measures, which are endowed with Wasserstein\ndistance metrics. We propose a number of variants of this problem, which admit\nfast optimization algorithms, by exploiting the connection to the problem of\nfinding Wasserstein barycenters. Consistency properties are established for the\nestimates of both local and global clusters. Finally, experiment results with\nboth synthetic and real data are presented to demonstrate the flexibility and\nscalability of the proposed approach. \n\n"}
{"id": "1706.04152", "contents": "Title: Learning to Detect Sepsis with a Multitask Gaussian Process RNN\n  Classifier Abstract: We present a scalable end-to-end classifier that uses streaming physiological\nand medication data to accurately predict the onset of sepsis, a\nlife-threatening complication from infections that has high mortality and\nmorbidity. Our proposed framework models the multivariate trajectories of\ncontinuous-valued physiological time series using multitask Gaussian processes,\nseamlessly accounting for the high uncertainty, frequent missingness, and\nirregular sampling rates typically associated with real clinical data. The\nGaussian process is directly connected to a black-box classifier that predicts\nwhether a patient will become septic, chosen in our case to be a recurrent\nneural network to account for the extreme variability in the length of patient\nencounters. We show how to scale the computations associated with the Gaussian\nprocess in a manner so that the entire system can be discriminatively trained\nend-to-end using backpropagation. In a large cohort of heterogeneous inpatient\nencounters at our university health system we find that it outperforms several\nbaselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under\nthe Receiver Operating Characteristic and Precision Recall curves as compared\nto the NEWS score currently used by our hospital. \n\n"}
{"id": "1706.05029", "contents": "Title: Distance weighted discrimination of face images for gender\n  classification Abstract: We illustrate the advantages of distance weighted discrimination for\nclassification and feature extraction in a High Dimension Low Sample Size\n(HDLSS) situation. The HDLSS context is a gender classification problem of face\nimages in which the dimension of the data is several orders of magnitude larger\nthan the sample size. We compare distance weighted discrimination with Fisher's\nlinear discriminant, support vector machines, and principal component analysis\nby exploring their classification interpretation through insightful\nvisuanimations and by examining the classifiers' discriminant errors. This\nanalysis enables us to make new contributions to the understanding of the\ndrivers of human discrimination between males and females. \n\n"}
{"id": "1706.05446", "contents": "Title: Adversarial Variational Bayes Methods for Tweedie Compound Poisson Mixed\n  Models Abstract: The Tweedie Compound Poisson-Gamma model is routinely used for modeling\nnon-negative continuous data with a discrete probability mass at zero. Mixed\nmodels with random effects account for the covariance structure related to the\ngrouping hierarchy in the data. An important application of Tweedie mixed\nmodels is pricing the insurance policies, e.g. car insurance. However, the\nintractable likelihood function, the unknown variance function, and the\nhierarchical structure of mixed effects have presented considerable challenges\nfor drawing inferences on Tweedie. In this study, we tackle the Bayesian\nTweedie mixed-effects models via variational inference approaches. In\nparticular, we empower the posterior approximation by implicit models trained\nin an adversarial setting. To reduce the variance of gradients, we\nreparameterize random effects, and integrate out one local latent variable of\nTweedie. We also employ a flexible hyper prior to ensure the richness of the\napproximation. Our method is evaluated on both simulated and real-world data.\nResults show that the proposed method has smaller estimation bias on the random\neffects compared to traditional inference methods including MCMC; it also\nachieves a state-of-the-art predictive performance, meanwhile offering a richer\nestimation of the variance function. \n\n"}
{"id": "1706.05552", "contents": "Title: Performance Bounds for Finite Moving Average Change Detection:\n  Application to Global Navigation Satellite Systems Abstract: Due to the widespread deployment of Global Navigation Satellite Systems\n(GNSSs) for critical road or urban applications, one of the major challenges to\nbe solved is the provision of integrity to terrestrial environments, so that\nGNSS may be safety used in these applications. To do so, the integrity of the\nreceived GNSS signal must be analyzed in order to detect some local effect\ndisturbing the received signal. This is desirable because the presence of some\nlocal effect may cause large position errors, and hence compromise the signal\nintegrity. Moreover, the detection of such disturbing effects must be done\nbefore some pre-established delay. This kind of detection lies within the field\nof transient change detection. In this work, a finite moving average stopping\ntime is proposed in order to approach the signal integrity problem with a\ntransient change detection framework. The statistical performance of this\nstopping time is investigated and compared, in the context of multipath\ndetection, to other different methods available in the literature. Numerical\nresults are presented in order to assess their performance. \n\n"}
{"id": "1706.06469", "contents": "Title: On mitigating the analytical limitations of finely stratified\n  experiments Abstract: While attractive from a theoretical perspective, finely stratified\nexperiments such as paired designs suffer from certain analytical limitations\nnot present in block-randomized experiments with multiple treated and control\nindividuals in each block. In short, when using an appropriately weighted\ndifference-in-means to estimated the sample average treatment effect, the\ntraditional variance estimator in a paired experiment is conservative unless\nthe pairwise average treatment effects are constant across pairs; however, in\nmore coarsely stratified experiments, the corresponding variance estimator is\nunbiased if treatment effects are constant within blocks, even if they vary\nacross blocks. Using insights from classical least squares theory, we present\nan improved variance estimator appropriate in finely stratified experiments.\nThe variance estimator is still conservative in expectation for the true\nvariance of the difference-in-means estimator, but is asymptotically no larger\nthan the classical variance estimator under mild conditions. The improvements\nstem from the exploitation of effect modification, and thus the magnitude of\nthe improvement depends upon on the extent to which effect heterogeneity can be\nexplained by observed covariates. Aided by these estimators, a new test for the\nnull hypothesis of a constant treatment effect is proposed. These findings\nextend to some, but not all, super-population models, depending on whether or\nnot the covariates are viewed as fixed across samples in the super-population\nformulation under consideration. \n\n"}
{"id": "1706.07094", "contents": "Title: Constrained Bayesian Optimization with Noisy Experiments Abstract: Randomized experiments are the gold standard for evaluating the effects of\nchanges to real-world systems. Data in these tests may be difficult to collect\nand outcomes may have high variance, resulting in potentially large measurement\nerror. Bayesian optimization is a promising technique for efficiently\noptimizing multiple continuous parameters, but existing approaches degrade in\nperformance when the noise level is high, limiting its applicability to many\nrandomized experiments. We derive an expression for expected improvement under\ngreedy batch optimization with noisy observations and noisy constraints, and\ndevelop a quasi-Monte Carlo approximation that allows it to be efficiently\noptimized. Simulations with synthetic functions show that optimization\nperformance on noisy, constrained problems outperforms existing methods. We\nfurther demonstrate the effectiveness of the method with two real-world\nexperiments conducted at Facebook: optimizing a ranking system, and optimizing\nserver compiler flags. \n\n"}
{"id": "1706.07136", "contents": "Title: Multiscale Information Decomposition: Exact Computation for Multivariate\n  Gaussian Processes Abstract: Exploiting the theory of state space models, we derive the exact expressions\nof the information transfer, as well as redundant and synergistic transfer, for\ncoupled Gaussian processes observed at multiple temporal scales. All of the\nterms, constituting the frameworks known as interaction information\ndecomposition and partial information decomposition, can thus be analytically\nobtained for different time scales from the parameters of the VAR model that\nfits the processes. We report the application of the proposed methodology\nfirstly to benchmark Gaussian systems, showing that this class of systems may\ngenerate patterns of information decomposition characterized by mainly\nredundant or synergistic information transfer persisting across multiple time\nscales or even by the alternating prevalence of redundant and synergistic\nsource interaction depending on the time scale. Then, we apply our method to an\nimportant topic in neuroscience, i.e., the detection of causal interactions in\nhuman epilepsy networks, for which we show the relevance of partial information\ndecomposition to the detection of multiscale information transfer spreading\nfrom the seizure onset zone. \n\n"}
{"id": "1706.08289", "contents": "Title: Intrinsic data depth for Hermitian positive definite matrices Abstract: Nondegenerate covariance, correlation and spectral density matrices are\nnecessarily symmetric or Hermitian and positive definite. The main contribution\nof this paper is the development of statistical data depths for collections of\nHermitian positive definite matrices by exploiting the geometric structure of\nthe space as a Riemannian manifold. The depth functions allow one to naturally\ncharacterize most central or outlying matrices, but also provide a practical\nframework for inference in the context of samples of positive definite\nmatrices. First, the desired properties of an intrinsic data depth function\nacting on the space of Hermitian positive definite matrices are presented.\nSecond, we propose two computationally fast pointwise and integrated data depth\nfunctions that satisfy each of these requirements and investigate several\nrobustness and efficiency aspects. As an application, we construct depth-based\nconfidence regions for the intrinsic mean of a sample of positive definite\nmatrices, which is applied to the exploratory analysis of a collection of\ncovariance matrices associated to a multicenter research trial. \n\n"}
{"id": "1706.08327", "contents": "Title: Informed Sub-Sampling MCMC: Approximate Bayesian Inference for Large\n  Datasets Abstract: This paper introduces a framework for speeding up Bayesian inference\nconducted in presence of large datasets. We design a Markov chain whose\ntransition kernel uses an (unknown) fraction of (fixed size) of the available\ndata that is randomly refreshed throughout the algorithm. Inspired by the\nApproximate Bayesian Computation (ABC) literature, the subsampling process is\nguided by the fidelity to the observed data, as measured by summary statistics.\nThe resulting algorithm, Informed Sub-Sampling MCMC (ISS-MCMC), is a generic\nand flexible approach which, contrary to existing scalable methodologies,\npreserves the simplicity of the Metropolis-Hastings algorithm. Even though\nexactness is lost, i.e. the chain distribution approximates the posterior, we\nstudy and quantify theoretically this bias and show on a diverse set of\nexamples that it yields excellent performances when the computational budget is\nlimited. If available and cheap to compute, we show that setting the summary\nstatistics as the maximum likelihood estimator is supported by theoretical\narguments. \n\n"}
{"id": "1706.10179", "contents": "Title: Lasso Meets Horseshoe : A Survey Abstract: The goal of this paper is to contrast and survey the major advances in two of\nthe most commonly used high-dimensional techniques, namely, the Lasso and\nhorseshoe regularization. Lasso is a gold standard for predictor selection\nwhile horseshoe is a state-of-the-art Bayesian estimator for sparse signals.\nLasso is fast and scalable and uses convex optimization whilst the horseshoe is\nnon-convex. Our novel perspective focuses on three aspects: (i) theoretical\noptimality in high dimensional inference for the Gaussian sparse model and\nbeyond, (ii) efficiency and scalability of computation and (iii) methodological\ndevelopment and performance. \n\n"}
{"id": "1706.10273", "contents": "Title: Community Detection by $L_0$-penalized Graph Laplacian Abstract: Community detection in network analysis aims at partitioning nodes in a\nnetwork into $K$ disjoint communities. Most currently available algorithms\nassume that $K$ is known, but choosing a correct $K$ is generally very\ndifficult for real networks. In addition, many real networks contain outlier\nnodes not belonging to any community, but currently very few algorithm can\nhandle networks with outliers. In this paper, we propose a novel model free\ntightness criterion and an efficient algorithm to maximize this criterion for\ncommunity detection. This tightness criterion is closely related with the graph\nLaplacian with $L_0$ penalty. Unlike most community detection methods, our\nmethod does not require a known $K$ and can properly detect communities in\nnetworks with outliers.\n  Both theoretical and numerical properties of the method are analyzed. The\ntheoretical result guarantees that, under the degree corrected stochastic block\nmodel, even for networks with outliers, the maximizer of the tightness\ncriterion can extract communities with small misclassification rates even when\nthe number of communities grows to infinity as the network size grows.\nSimulation study shows that the proposed method can recover true communities\nmore accurately than other methods. Applications to a college football data and\na yeast protein-protein interaction data also reveal that the proposed method\nperforms significantly better. \n\n"}
{"id": "1707.00046", "contents": "Title: Fairer and more accurate, but for whom? Abstract: Complex statistical machine learning models are increasingly being used or\nconsidered for use in high-stakes decision-making pipelines in domains such as\nfinancial services, health care, criminal justice and human services. These\nmodels are often investigated as possible improvements over more classical\ntools such as regression models or human judgement. While the modeling approach\nmay be new, the practice of using some form of risk assessment to inform\ndecisions is not. When determining whether a new model should be adopted, it is\ntherefore essential to be able to compare the proposed model to the existing\napproach across a range of task-relevant accuracy and fairness metrics. Looking\nat overall performance metrics, however, may be misleading. Even when two\nmodels have comparable overall performance, they may nevertheless disagree in\ntheir classifications on a considerable fraction of cases. In this paper we\nintroduce a model comparison framework for automatically identifying subgroups\nin which the differences between models are most pronounced. Our primary focus\nis on identifying subgroups where the models differ in terms of\nfairness-related quantities such as racial or gender disparities. We present\nexperimental results from a recidivism prediction task and a hypothetical\nlending example. \n\n"}
{"id": "1707.00306", "contents": "Title: Variable Selection Methods for Model-based Clustering Abstract: Model-based clustering is a popular approach for clustering multivariate data\nwhich has seen applications in numerous fields. Nowadays, high-dimensional data\nare more and more common and the model-based clustering approach has adapted to\ndeal with the increasing dimensionality. In particular, the development of\nvariable selection techniques has received a lot of attention and research\neffort in recent years. Even for small size problems, variable selection has\nbeen advocated to facilitate the interpretation of the clustering results. This\nreview provides a summary of the methods developed for variable selection in\nmodel-based clustering. Existing R packages implementing the different methods\nare indicated and illustrated in application to two data analysis examples. \n\n"}
{"id": "1707.00731", "contents": "Title: Data Fusion Reconstruction of Spatially Embedded Complex Networks Abstract: We introduce a kernel Lasso (kLasso) optimization that simultaneously\naccounts for spatial regularity and network sparsity to reconstruct spatial\ncomplex networks from data. Through a kernel function, the proposed approach\nexploits spatial embedding distances to penalize overabundance of spatially\nlong-distance connections. Examples of both synthetic and real-world spatial\nnetworks show that the proposed method improves significantly upon existing\nnetwork reconstruction techniques that mainly concerns sparsity but not spatial\nregularity. Our results highlight the promise of data fusion in the\nreconstruction of complex networks, by utilizing both microscopic node-level\ndynamics (e.g., time series data) and macroscopic network-level information\n(metadata). \n\n"}
{"id": "1707.01195", "contents": "Title: The impossibility of \"fairness\": a generalized impossibility result for\n  decisions Abstract: Various measures can be used to estimate bias or unfairness in a predictor.\nPrevious work has already established that some of these measures are\nincompatible with each other. Here we show that, when groups differ in\nprevalence of the predicted event, several intuitive, reasonable measures of\nfairness (probability of positive prediction given occurrence or\nnon-occurrence; probability of occurrence given prediction or non-prediction;\nand ratio of predictions over occurrences for each group) are all mutually\nexclusive: if one of them is equal among groups, the other two must differ. The\nonly exceptions are for perfect, or trivial (always-positive or\nalways-negative) predictors. As a consequence, any non-perfect, non-trivial\npredictor must necessarily be \"unfair\" under two out of three reasonable sets\nof criteria. This result readily generalizes to a wide range of well-known\nstatistical quantities (sensitivity, specificity, false positive rate,\nprecision, etc.), all of which can be divided into three mutually exclusive\ngroups. Importantly, The results applies to all predictors, whether algorithmic\nor human. We conclude with possible ways to handle this effect when assessing\nand designing prediction methods. \n\n"}
{"id": "1707.01287", "contents": "Title: A Matern based multivariate Gaussian random process for a consistent\n  model of the horizontal wind components and related variables Abstract: The integration of physical relationships into stochastic models is of major\ninterest e.g. in data assimilation. Here, a multivariate Gaussian random field\nformulation is introduced, which represents the differential relations of the\ntwo-dimensional wind field and related variables such as streamfunction,\nvelocity potential, vorticity and divergence. The covariance model is based on\na flexible bivariate Mat\\'ern covariance function for streamfunction and\nvelocity potential. It allows for different variances in the potentials,\nnon-zero correlations between them, anisotropy and a flexible smoothness\nparameter. The joint covariance function of the related variables is derived\nanalytically. Further, it is shown that a consistent model with non-zero\ncorrelations between the potentials and positive definite covariance function\nis possible. The statistical model is fitted to forecasts of the horizontal\nwind fields of a mesoscale numerical weather prediction system. Parameter\nuncertainty is assessed by a parametric bootstrap method. The estimates reveal\nonly physically negligible correlations between the potentials. In contrast to\nthe numerical estimator, the statistical estimator of the ratio between the\nvariances of the rotational and divergent wind components is unbiased. \n\n"}
{"id": "1707.01838", "contents": "Title: A statistical analysis of particle trajectories in living cells Abstract: Recent advances in molecular biology and fluorescence microscopy imaging have\nmade possible the inference of the dynamics of single molecules in living\ncells. Such inference allows to determine the organization and function of the\ncell. The trajectories of particles in the cells, computed with tracking\nalgorithms, can be modelled with diffusion processes. Three types of diffusion\nare considered : (i) free diffusion; (ii) subdiffusion or (iii) superdiffusion.\nThe Mean Square Displacement (MSD) is generally used to determine the different\ntypes of dynamics of the particles in living cells (Qian, Sheetz and Elson\n1991). We propose here a non-parametric three-decision test as an alternative\nto the MSD method. The rejection of the null hypothesis -- free diffusion -- is\naccompanied by claims of the direction of the alternative (subdiffusion or a\nsuperdiffusion). We study the asymptotic behaviour of the test statistic under\nthe null hypothesis, and under parametric alternatives which are currently\nconsidered in the biophysics literature, (Monnier et al,2012) for example. In\naddition, we adapt the procedure of Benjamini and Hochberg (2000) to fit with\nthe three-decision test setting, in order to apply the test procedure to a\ncollection of independent trajectories. The performance of our procedure is\nmuch better than the MSD method as confirmed by Monte Carlo experiments. The\nmethod is demonstrated on real data sets corresponding to protein dynamics\nobserved in fluorescence microscopy. \n\n"}
{"id": "1707.02032", "contents": "Title: Matrix-Based Characterization of the Motion and Wrench Uncertainties in\n  Robotic Manipulators Abstract: Characterization of the uncertainty in robotic manipulators is the focus of\nthis paper. Based on the random matrix theory (RMT), we propose uncertainty\ncharacterization schemes in which the uncertainty is modeled at the macro\n(system) level. This is different from the traditional approaches that model\nthe uncertainty in the parametric space of micro (state) level. We show that\nperturbing the system matrices rather than the state of the system provides\nunique advantages especially for robotic manipulators. First, it requires only\nlimited statistical information that becomes effective when dealing with\ncomplex systems where detailed information on their variability is not\navailable. Second, the RMT-based models are aware of the system state and\nconfiguration that are significant factors affecting the level of uncertainty\nin system behavior. In this study, in addition to the motion uncertainty\nanalysis that was first proposed in our earlier work, we also develop an\nRMT-based model for the quantification of the static wrench uncertainty in\nmulti-agent cooperative systems. This model is aimed to be an alternative to\nthe elaborate parametric formulation when only rough bounds are available on\nthe system parameters. We discuss that how RMT-based model becomes advantageous\nwhen the complexity of the system increases. We perform experimental studies on\na KUKA youBot arm to demonstrate the superiority of the RMT-based motion\nuncertainty models. We show that how these models outperform the traditional\nmodels built upon Gaussianity assumption in capturing real-system uncertainty\nand providing accurate bounds on the state estimation errors. In addition, to\nexperimentally support our wrench uncertainty quantification model, we study\nthe behavior of a cooperative system of mobile robots. It is shown that one can\nrely on less demanding RMT-based formulation and yet meets the acceptable\naccuracy. \n\n"}
{"id": "1707.03307", "contents": "Title: Fast calibrated additive quantile regression Abstract: We propose a novel framework for fitting additive quantile regression models,\nwhich provides well calibrated inference about the conditional quantiles and\nfast automatic estimation of the smoothing parameters, for model structures as\ndiverse as those usable with distributional GAMs, while maintaining equivalent\nnumerical efficiency and stability. The proposed methods are at once\nstatistically rigorous and computationally efficient, because they are based on\nthe general belief updating framework of Bissiri et al. (2016) to loss based\ninference, but compute by adapting the stable fitting methods of Wood et al.\n(2016). We show how the pinball loss is statistically suboptimal relative to a\nnovel smooth generalisation, which also gives access to fast estimation\nmethods. Further, we provide a novel calibration method for efficiently\nselecting the 'learning rate' balancing the loss with the smoothing priors\nduring inference, thereby obtaining reliable quantile uncertainty estimates.\nOur work was motivated by a probabilistic electricity load forecasting\napplication, used here to demonstrate the proposed approach. The methods\ndescribed here are implemented by the qgam R package, available on the\nComprehensive R Archive Network (CRAN). \n\n"}
{"id": "1707.03469", "contents": "Title: Machine Learning in Appearance-based Robot Self-localization Abstract: An appearance-based robot self-localization problem is considered in the\nmachine learning framework. The appearance space is composed of all possible\nimages, which can be captured by a robot's visual system under all robot\nlocalizations. Using recent manifold learning and deep learning techniques, we\npropose a new geometrically motivated solution based on training data\nconsisting of a finite set of images captured in known locations of the robot.\nThe solution includes estimation of the robot localization mapping from the\nappearance space to the robot localization space, as well as estimation of the\ninverse mapping for modeling visual image features. The latter allows solving\nthe robot localization problem as the Kalman filtering problem. \n\n"}
{"id": "1707.05296", "contents": "Title: Piecewise-Deterministic Markov Chain Monte Carlo Abstract: A novel class of non-reversible Markov chain Monte Carlo schemes relying on\ncontinuous-time piecewise-deterministic Markov Processes has recently emerged.\nIn these algorithms, the state of the Markov process evolves according to a\ndeterministic dynamics which is modified using a Markov transition kernel at\nrandom event times. These methods enjoy remarkable features including the\nability to update only a subset of the state components while other components\nimplicitly keep evolving and the ability to use an unbiased estimate of the\ngradient of the log-target while preserving the target as invariant\ndistribution. However, they also suffer from important limitations. The\ndeterministic dynamics used so far do not exploit the structure of the target.\nMoreover, exact simulation of the event times is feasible for an important yet\nrestricted class of problems and, even when it is, it is application specific.\nThis limits the applicability of these techniques and prevents the development\nof a generic software implementation of them. We introduce novel MCMC methods\naddressing these shortcomings. In particular, we introduce novel\ncontinuous-time algorithms relying on exact Hamiltonian flows and novel\nnon-reversible discrete-time algorithms which can exploit complex dynamics such\nas approximate Hamiltonian dynamics arising from symplectic integrators while\npreserving the attractive features of continuous-time algorithms. We\ndemonstrate the performance of these schemes on a variety of applications. \n\n"}
{"id": "1707.07971", "contents": "Title: Using deterministic approximations to accelerate SMC for posterior\n  sampling Abstract: Sequential Monte Carlo has become a standard tool for Bayesian Inference of\ncomplex models. This approach can be computationally demanding, especially when\ninitialized from the prior distribution. On the other hand, deter-ministic\napproximations of the posterior distribution are often available with no\ntheoretical guaranties. We propose a bridge sampling scheme starting from such\na deterministic approximation of the posterior distribution and targeting the\ntrue one. The resulting Shortened Bridge Sampler (SBS) relies on a sequence of\ndistributions that is determined in an adaptive way. We illustrate the\nrobustness and the efficiency of the methodology on a large simulation study.\nWhen applied to network datasets, SBS inference leads to different statistical\nconclusions from the one supplied by the standard variational Bayes\napproximation. \n\n"}
{"id": "1707.08658", "contents": "Title: Change Point Detection with Optimal Transport and Geometric Discrepancy Abstract: We present novel retrospective change point detection approach based on\noptimal transport and geometric discrepancy. The method does not require any\nparametric assumptions about distributions separated by change points. It can\nbe used both for single and multiple change point detection and estimation,\nwhile the number of change points is either known or unknown. This result is\nachieved by construction of a certain sliding window statistic from which\nchange points can be derived with elementary convex geometry in a specific\nHilbert space. The work is illustrated with computational examples, both\nartificially constructed and based on actual data. \n\n"}
{"id": "1707.08774", "contents": "Title: Topological Data Analysis of Clostridioides difficile Infection and\n  Fecal Microbiota Transplantation Abstract: Computational topologists recently developed a method, called persistent\nhomology to analyze data presented in terms of similarity or dissimilarity.\nIndeed, persistent homology studies the evolution of topological features in\nterms of a single index, and is able to capture higher order features beyond\nthe usual clustering techniques. There are three descriptive statistics of\npersistent homology, namely barcode, persistence diagram and more recently,\npersistence landscape. Persistence landscape is useful for statistical\ninference as it belongs to a space of $p-$integrable functions, a separable\nBanach space. We apply tools in both computational topology and statistics to\nDNA sequences taken from Clostridioides difficile infected patients treated\nwith an experimental fecal microbiota transplantation. Our statistical and\ntopological data analysis are able to detect interesting patterns among\npatients and donors. It also provides visualization of DNA sequences in the\nform of clusters and loops. \n\n"}
{"id": "1707.09208", "contents": "Title: Sparse Identification and Estimation of Large-Scale Vector\n  AutoRegressive Moving Averages Abstract: The Vector AutoRegressive Moving Average (VARMA) model is fundamental to the\ntheory of multivariate time series; however, identifiability issues have led\npractitioners to abandon it in favor of the simpler but more restrictive Vector\nAutoRegressive (VAR) model. We narrow this gap with a new optimization-based\napproach to VARMA identification built upon the principle of parsimony. Among\nall equivalent data-generating models, we use convex optimization to seek the\nparameterization that is \"simplest\" in a certain sense. A user-specified\nstrongly convex penalty is used to measure model simplicity, and that same\npenalty is then used to define an estimator that can be efficiently computed.\nWe establish consistency of our estimators in a double-asymptotic regime. Our\nnon-asymptotic error bound analysis accommodates both model specification and\nparameter estimation steps, a feature that is crucial for studying large-scale\nVARMA algorithms. Our analysis also provides new results on penalized\nestimation of infinite-order VAR, and elastic net regression under a singular\ncovariance structure of regressors, which may be of independent interest. We\nillustrate the advantage of our method over VAR alternatives on three real data\nexamples. \n\n"}
{"id": "1707.09561", "contents": "Title: Fine-Gray competing risks model with high-dimensional covariates:\n  estimation and Inference Abstract: The purpose of this paper is to construct confidence intervals for the\nregression coefficients in the Fine-Gray model for competing risks data with\nrandom censoring, where the number of covariates can be larger than the sample\nsize. Despite strong motivation from biomedical applications, a\nhigh-dimensional Fine-Gray model has attracted relatively little attention\namong the methodological or theoretical literature. We fill in this gap by\ndeveloping confidence intervals based on a one-step bias-correction for a\nregularized estimation. We develop a theoretical framework for the partial\nlikelihood, which does not have independent and identically distributed entries\nand therefore presents many technical challenges. We also study the\napproximation error from the weighting scheme under random censoring for\ncompeting risks and establish new concentration results for time-dependent\nprocesses. In addition to the theoretical results and algorithms, we present\nextensive numerical experiments and an application to a study of non-cancer\nmortality among prostate cancer patients using the linked Medicare-SEER data. \n\n"}
{"id": "1708.00907", "contents": "Title: Sequential Specification Tests to Choose a Model: A Change-Point\n  Approach Abstract: Researchers faced with a sequence of candidate model specifications must\noften choose the best specification that does not violate a testable\nidentification assumption. One option in this scenario is sequential\nspecification tests: hypothesis tests of the identification assumption over the\nsequence. Borrowing an idea from the change-point literature, this paper shows\nhow to use the distribution of p-values from sequential specification tests to\nestimate the point in the sequence where the identification assumption ceases\nto hold. Unlike current approaches, this method is robust to individual errant\np-values and does not require choosing a test level or tuning parameter. This\npaper demonstrates the method's properties with a simulation study, and\nillustrates it by application to the problems of choosing a bandwidth in a\nregression discontinuity design while maintaining covariate balance and of\nchoosing a lag order for a time series model. \n\n"}
{"id": "1708.02635", "contents": "Title: Anomaly Detection in Multivariate Non-stationary Time Series for\n  Automatic DBMS Diagnosis Abstract: Anomaly detection in database management systems (DBMSs) is difficult because\nof increasing number of statistics (stat) and event metrics in big data system.\nIn this paper, I propose an automatic DBMS diagnosis system that detects\nanomaly periods with abnormal DB stat metrics and finds causal events in the\nperiods. Reconstruction error from deep autoencoder and statistical process\ncontrol approach are applied to detect time period with anomalies. Related\nevents are found using time series similarity measures between events and\nabnormal stat metrics. After training deep autoencoder with DBMS metric data,\nefficacy of anomaly detection is investigated from other DBMSs containing\nanomalies. Experiment results show effectiveness of proposed model, especially,\nbatch temporal normalization layer. Proposed model is used for publishing\nautomatic DBMS diagnosis reports in order to determine DBMS configuration and\nSQL tuning. \n\n"}
{"id": "1708.02703", "contents": "Title: Ellipsoidal Prediction Regions for Multivariate Uncertainty\n  Characterization Abstract: While substantial advances are observed in probabilistic forecasting for\npower system operation and electricity market applications, most approaches are\nstill developed in a univariate framework. This prevents from informing about\nthe interdependence structure among locations, lead times and variables of\ninterest. Such dependencies are key in a large share of operational problems\ninvolving renewable power generation, load and electricity prices for instance.\nThe few methods that account for dependencies translate to sampling scenarios\nbased on given marginals and dependence structures. However, for classes of\ndecision-making problems based on robust, interval chance-constrained\noptimization, necessary inputs take the form of polyhedra or ellipsoids.\nConsequently, we propose a systematic framework to readily generate and\nevaluate ellipsoidal prediction regions, with predefined probability and\nminimum volume. A skill score is proposed for quantitative assessment of the\nquality of prediction ellipsoids. A set of experiments is used to illustrate\nthe discrimination ability of the proposed scoring rule for misspecification of\nellipsoidal prediction regions. Application results based on three datasets\nwith wind, PV power and electricity prices, allow us to assess the skill of the\nresulting ellipsoidal prediction regions, in terms of calibration, sharpness\nand overall skill. \n\n"}
{"id": "1708.02736", "contents": "Title: Structural Break Detection in High-Dimensional Non-Stationary VAR models Abstract: Assuming stationarity is unrealistic in many time series applications. A more\nrealistic alternative is to allow for piecewise stationarity, where the model\nis allowed to change at given time points. In this article, the problem of\ndetecting the change points in a high-dimensional piecewise vector\nautoregressive model (VAR) is considered. Reformulated the problem as a\nhigh-dimensional variable selection, a penalized least square estimation using\ntotal variation LASSO penalty is proposed for estimation of model parameters.\nIt is shown that the developed method over-estimates the number of change\npoints. A backward selection criterion is thus proposed in conjunction with the\npenalized least square estimator to tackle this issue. We prove that the\nproposed two-stage procedure consistently detects the number of change points\nand their locations. A block coordinate descent algorithm is developed for\nefficient computation of model parameters. The performance of the method is\nillustrated using several simulation scenarios. \n\n"}
{"id": "1708.03018", "contents": "Title: Dimensional and statistical foundations for accumulated damage models Abstract: This paper develops a framework for creating damage accumulation models for\nengineered wood products by invoking the classical theory of\nnon--dimensionalization. The result is a general class of such models. Both the\nUS and Canadian damage accumulation models are revisited. It is shown how the\nformer may be generalized within that framework while deficiencies are\ndiscovered in the latter and overcome. Use of modern Bayesian statistical\nmethods for estimating the parameters in these models is proposed along with an\nillustrative application of these methods to a ramp load dataset. \n\n"}
{"id": "1708.03992", "contents": "Title: Multi-scale analysis of lead-lag relationships in high-frequency\n  financial markets Abstract: We propose a novel estimation procedure for scale-by-scale lead-lag\nrelationships of financial assets observed at high-frequency in a\nnon-synchronous manner. The proposed estimation procedure does not require any\ninterpolation processing of original datasets and is applicable to those with\nhighest time resolution available. Consistency of the proposed estimators is\nshown under the continuous-time framework that has been developed in our\nprevious work Hayashi and Koike (2018). An empirical application to a quote\ndataset of the NASDAQ-100 assets identifies two types of lead-lag relationships\nat different time scales. \n\n"}
{"id": "1708.04221", "contents": "Title: Efficient sequential Monte Carlo algorithms for integrated population\n  models Abstract: State-space models are commonly used to describe different forms of\necological data. We consider the case of count data with observation errors.\nFor such data the system process is typically multi-dimensional consisting of\ncoupled Markov processes, where each component corresponds to a different\ncharacterisation of the population, such as age group, gender or breeding\nstatus. The associated system process equations describe the biological\nmechanisms under which the system evolves over time. However, there is often\nlimited information in the count data alone to sensibly estimate demographic\nparameters of interest, so these are often combined with additional ecological\nobservations leading to an integrated data analysis. Unfortunately, fitting\nthese models to the data can be challenging, especially if the state-space\nmodel for the count data is non-linear or non-Gaussian. We propose an efficient\nparticle Markov chain Monte Carlo algorithm to estimate the demographic\nparameters without the need for resorting to linear or Gaussian approximations.\nIn particular, we exploit the integrated model structure to enhance the\nefficiency of the algorithm. We then incorporate the algorithm into a\nsequential Monte Carlo sampler in order to perform model comparison with\nregards to the dependence structure of the demographic parameters. Finally, we\ndemonstrate the applicability and computational efficiency of our algorithms on\ntwo real datasets. \n\n"}
{"id": "1708.05248", "contents": "Title: A nonparametric test for stationarity in functional time series Abstract: We propose a new measure for stationarity of a functional time series, which\nis based on an explicit representation of the $L^2$-distance between the\nspectral density operator of a non-stationary process and its best\n($L^2$-)approximation by a spectral density operator corresponding to a\nstationary process. This distance can easily be estimated by sums of\nHilbert-Schmidt inner products of periodogram operators (evaluated at different\nfrequencies), and asymptotic normality of an appropriately standardized version\nof the estimator can be established for the corresponding estimate under the\nnull hypothesis and alternative. As a result we obtain a simple asymptotic\nfrequency domain level $\\alpha$ test (using the quantiles of the normal\ndistribution) for the hypothesis of stationarity of functional time series.\nOther applications such as asymptotic confidence intervals for a measure of\nstationarity or the construction of tests for \"relevant deviations from\nstationarity\", are also briefly mentioned. We demonstrate in a small simulation\nstudy that the new method has very good finite sample properties. Moreover, we\napply our test to annual temperature curves. \n\n"}
{"id": "1708.05879", "contents": "Title: Regularized Estimation and Testing for High-Dimensional Multi-Block\n  Vector-Autoregressive Models Abstract: Dynamical systems comprising of multiple components that can be partitioned\ninto distinct blocks originate in many scientific areas. A pertinent example is\nthe interactions between financial assets and selected macroeconomic\nindicators, which has been studied at aggregate level---e.g. a stock index and\nan employment index---extensively in the macroeconomics literature. A key\nshortcoming of this approach is that it ignores potential influences from other\nrelated components (e.g. Gross Domestic Product) that may exert influence on\nthe system's dynamics and structure and thus produces incorrect results. To\nmitigate this issue, we consider a multi-block linear dynamical system with\nGranger-causal ordering between blocks, wherein the blocks' temporal dynamics\nare described by vector autoregressive processes and are influenced by blocks\nhigher in the system hierarchy. We derive the maximum likelihood estimator for\nthe posited model for Gaussian data in the high-dimensional setting based on\nappropriate regularization schemes for the parameters of the block components.\nTo optimize the underlying non-convex likelihood function, we develop an\niterative algorithm with convergence guarantees. We establish theoretical\nproperties of the maximum likelihood estimates, leveraging the decomposability\nof the regularizers and a careful analysis of the iterates. Finally, we develop\ntesting procedures for the null hypothesis of whether a block \"Granger-causes\"\nanother block of variables. The performance of the model and the testing\nprocedures are evaluated on synthetic data, and illustrated on a data set\ninvolving log-returns of the US S&P100 component stocks and key macroeconomic\nvariables for the 2001--16 period. \n\n"}
{"id": "1708.06152", "contents": "Title: Physiological Gaussian Process Priors for the Hemodynamics in fMRI\n  Analysis Abstract: Background: Inference from fMRI data faces the challenge that the hemodynamic\nsystem that relates neural activity to the observed BOLD fMRI signal is\nunknown.\n  New Method: We propose a new Bayesian model for task fMRI data with the\nfollowing features: (i) joint estimation of brain activity and the underlying\nhemodynamics, (ii) the hemodynamics is modeled nonparametrically with a\nGaussian process (GP) prior guided by physiological information and (iii) the\npredicted BOLD is not necessarily generated by a linear time-invariant (LTI)\nsystem. We place a GP prior directly on the predicted BOLD response, rather\nthan on the hemodynamic response function as in previous literature. This\nallows us to incorporate physiological information via the GP prior mean in a\nflexible way, and simultaneously gives us the nonparametric flexibility of the\nGP.\n  Results: Results on simulated data show that the proposed model is able to\ndiscriminate between active and non-active voxels also when the GP prior\ndeviates from the true hemodynamics. Our model finds time varying dynamics when\napplied to real fMRI data.\n  Comparison with Existing Method(s): The proposed model is better at detecting\nactivity in simulated data than standard models, without inflating the false\npositive rate. When applied to real fMRI data, our GP model in several cases\nfinds brain activity where previously proposed LTI models does not.\n  Conclusions: We have proposed a new non-linear model for the hemodynamics in\ntask fMRI, that is able to detect active voxels, and gives the opportunity to\nask new kinds of questions related to hemodynamics. \n\n"}
{"id": "1708.06443", "contents": "Title: Bias Reduction in Instrumental Variable Estimation through First-Stage\n  Shrinkage Abstract: The two-stage least-squares (2SLS) estimator is known to be biased when its\nfirst-stage fit is poor. I show that better first-stage prediction can\nalleviate this bias. In a two-stage linear regression model with Normal noise,\nI consider shrinkage in the estimation of the first-stage instrumental variable\ncoefficients. For at least four instrumental variables and a single endogenous\nregressor, I establish that the standard 2SLS estimator is dominated with\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\nin a first-stage high-dimensional Normal-means problem followed by a\ncontrol-function approach in the second stage. It preserves invariances of the\nstructural instrumental variable equations. \n\n"}
{"id": "1708.07061", "contents": "Title: Forecasting day-ahead electricity prices in Europe: the importance of\n  considering market integration Abstract: Motivated by the increasing integration among electricity markets, in this\npaper we propose two different methods to incorporate market integration in\nelectricity price forecasting and to improve the predictive performance. First,\nwe propose a deep neural network that considers features from connected markets\nto improve the predictive accuracy in a local market. To measure the importance\nof these features, we propose a novel feature selection algorithm that, by\nusing Bayesian optimization and functional analysis of variance, evaluates the\neffect of the features on the algorithm performance. In addition, using market\nintegration, we propose a second model that, by simultaneously predicting\nprices from two markets, improves the forecasting accuracy even further. As a\ncase study, we consider the electricity market in Belgium and the improvements\nin forecasting accuracy when using various French electricity features. We show\nthat the two proposed models lead to improvements that are statistically\nsignificant. Particularly, due to market integration, the predictive accuracy\nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage\nerror). In addition, we show that the proposed feature selection algorithm is\nable to perform a correct assessment, i.e. to discard the irrelevant features. \n\n"}
{"id": "1708.07196", "contents": "Title: A Bayesian Mixture Model for Clustering on the Stiefel Manifold Abstract: Analysis of a Bayesian mixture model for the Matrix Langevin distribution on\nthe Stiefel manifold is presented. The model exploits a particular\nparametrization of the Matrix Langevin distribution, various aspects of which\nare elaborated on. A general, and novel, family of conjugate priors, and an\nefficient Markov chain Monte Carlo (MCMC) sampling scheme for the corresponding\nposteriors is then developed for the mixture model. Theoretical properties of\nthe prior and posterior distributions, including posterior consistency, are\nexplored in detail. Extensive simulation experiments are presented to validate\nthe efficacy of the framework. Real-world examples, including a large scale\nneuroimaging dataset, are analyzed to demonstrate the computational\ntractability of the approach. \n\n"}
{"id": "1708.07604", "contents": "Title: A Mixed-Membership Model for Social Network Clustering Abstract: We propose a simple mixed membership model for social network clustering in\nthis paper. A flexible function is adopted to measure affinities among a set of\nentities in a social network. The model not only allows each entity in the\nnetwork to possess more than one membership, but also provides accurate\nstatistical inference about network structure. We estimate the membership\nparameters using an MCMC algorithm. We evaluate the performance of the proposed\nalgorithm by applying our model to two empirical social network data, the\nZachary club data and the bottlenose dolphin network data. We also conduct some\nnumerical studies based on synthetic networks for further assessing the\neffectiveness of our algorithm. In the end, some concluding remarks and future\nwork are addressed briefly. \n\n"}
{"id": "1708.07691", "contents": "Title: Aggregation and Resource Scheduling in Machine-type Communication\n  Networks: A Stochastic Geometry Approach Abstract: Data aggregation is a promising approach to enable massive machine-type\ncommunication (mMTC). This paper focuses on the aggregation phase where a\nmassive number of machine-type devices (MTDs) transmit to aggregators. By using\nnon-orthogonal multiple access (NOMA) principles, we allow several MTDs to\nshare the same orthogonal channel in our proposed hybrid access scheme. We\ndevelop an analytical framework based on stochastic geometry to investigate the\nsystem performance in terms of average success probability and average number\nof simultaneously served MTDs, under imperfect successive interference\ncancellation (SIC) at the aggregators, for two scheduling schemes: random\nresource scheduling (RRS) and channel-aware resource scheduling (CRS). We\nidentify the power constraints on the MTDs sharing the same channel to attain a\nfair coexistence with purely orthogonal multiple access (OMA) setups, then\npower control coefficients are found so that these MTDs perform with similar\nreliability. We show that under high access demand, the hybrid scheme with CRS\noutperforms the OMA setup by simultaneously serving more MTDs with reduced\npower consumption. \n\n"}
{"id": "1708.07852", "contents": "Title: Mixed Membership Estimation for Social Networks Abstract: In economics and social science, network data are regularly observed, and a\nthorough understanding of the network community structure facilitates the\ncomprehension of economic patterns and activities. Consider an undirected\nnetwork with $n$ nodes and $K$ communities. We model the network using the\nDegree-Corrected Mixed-Membership (DCMM) model, where for each node $i$, there\nexists a membership vector $\\pi_i = (\\pi_i(1), \\pi_i(2), \\ldots, \\pi_i(K))'$,\nwhere $\\pi_i(k)$ is the weight that node $i$ puts in community $k$, $1 \\leq k\n\\leq K$. In comparison to the well-known stochastic block model (SBM), the DCMM\npermits both severe degree heterogeneity and mixed memberships, making it\nconsiderably more realistic and general. We present an efficient approach,\nMixed-SCORE, for estimating the mixed membership vectors of all nodes and the\nother DCMM parameters. This approach is inspired by the discovery of a delicate\nsimplex structure in the spectral domain. We derive explicit error rates for\nthe Mixed-SCORE algorithm and demonstrate that it is rate-optimal over a broad\nparameter space. Our findings provide a novel statistical tool for network\ncommunity analysis, which can be used to understand network formations, extract\nnodal features, identify unobserved covariates in dyadic regressions, and\nestimate peer effects. We applied Mixed-SCORE to a political blog network, two\ntrade networks, a co-authorship network, and a citee network, and obtained\ninterpretable results. \n\n"}
{"id": "1708.08522", "contents": "Title: Causal Inference Under Network Interference: A Framework for Experiments\n  on Social Networks Abstract: No man is an island, as individuals interact and influence one another daily\nin our society. When social influence takes place in experiments on a\npopulation of interconnected individuals, the treatment on a unit may affect\nthe outcomes of other units, a phenomenon known as interference. This thesis\ndevelops a causal framework and inference methodology for experiments where\ninterference takes place on a network of influence (i.e. network interference).\nIn this framework, the network potential outcomes serve as the key quantity and\nflexible building blocks for causal estimands that represent a variety of\nprimary, peer, and total treatment effects. These causal estimands are\nestimated via principled Bayesian imputation of missing outcomes. The theory on\nthe unconfoundedness assumptions leading to simplified imputation highlights\nthe importance of including relevant network covariates in the potential\noutcome model. Additionally, experimental designs that result in balanced\ncovariates and sizes across treatment exposure groups further improve the\ncausal estimate, especially by mitigating potential outcome model\nmis-specification. The true potential outcome model is not typically known in\nreal-world experiments, so the best practice is to account for interference and\nconfounding network covariates through both balanced designs and model-based\nimputation. A full factorial simulated experiment is formulated to demonstrate\nthis principle by comparing performance across different randomization schemes\nduring the design phase and estimators during the analysis phase, under varying\nnetwork topology and true potential outcome models. Overall, this thesis\nasserts that interference is not just a nuisance for analysis but rather an\nopportunity for quantifying and leveraging peer effects in real-world\nexperiments. \n\n"}
{"id": "1708.09824", "contents": "Title: On the Bayesian calibration of expensive computer models with input\n  dependent parameters Abstract: Computer models, aiming at simulating a complex real system, are often\ncalibrated in the light of data to improve performance. Standard calibration\nmethods assume that the optimal values of calibration parameters are invariant\nto the model inputs. In several real world applications where models involve\ncomplex parametrizations whose optimal values depend on the model inputs, such\nan assumption can be too restrictive and may lead to misleading results. We\npropose a fully Bayesian methodology that produces input dependent optimal\nvalues for the calibration parameters, as well as it characterizes the\nassociated uncertainties via posterior distributions. Central to methodology is\nthe idea of formulating the calibration parameter as a step function whose\nuncertain structure is modeled properly via a binary treed process. Our method\nis particularly suitable to address problems where the computer model requires\nthe selection of a sub-model from a set of competing ones, but the choice of\nthe `best' sub-model may change with the input values. The method produces a\nselection probability for each sub-model given the input. We propose suitable\nreversible jump operations to facilitate the challenging computations. We\nassess the performance of our method against benchmark examples, and use it to\nanalyze a real world application with a large-scale climate model. \n\n"}
{"id": "1709.00092", "contents": "Title: RANK: Large-Scale Inference with Graphical Nonlinear Knockoffs Abstract: Power and reproducibility are key to enabling refined scientific discoveries\nin contemporary big data applications with general high-dimensional nonlinear\nmodels. In this paper, we provide theoretical foundations on the power and\nrobustness for the model-free knockoffs procedure introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2016) in high-dimensional setting when the\ncovariate distribution is characterized by Gaussian graphical model. We\nestablish that under mild regularity conditions, the power of the oracle\nknockoffs procedure with known covariate distribution in high-dimensional\nlinear models is asymptotically one as sample size goes to infinity. When\nmoving away from the ideal case, we suggest the modified model-free knockoffs\nmethod called graphical nonlinear knockoffs (RANK) to accommodate the unknown\ncovariate distribution. We provide theoretical justifications on the robustness\nof our modified procedure by showing that the false discovery rate (FDR) is\nasymptotically controlled at the target level and the power is asymptotically\none with the estimated covariate distribution. To the best of our knowledge,\nthis is the first formal theoretical result on the power for the knockoffs\nprocedure. Simulation results demonstrate that compared to existing approaches,\nour method performs competitively in both FDR control and power. A real data\nset is analyzed to further assess the performance of the suggested knockoffs\nprocedure. \n\n"}
{"id": "1709.00232", "contents": "Title: Estimating functions for jump-diffusions Abstract: Asymptotic theory for approximate martingale estimating functions is\ngeneralised to diffusions with finite-activity jumps, when the sampling\nfrequency and terminal sampling time go to infinity. Rate optimality and\nefficiency are of particular concern. Under mild assumptions, it is shown that\nestimators of drift, diffusion, and jump parameters are consistent and\nasymptotically normal, as well as rate-optimal for the drift and jump\nparameters. Additional conditions are derived, which ensure rate-optimality for\nthe diffusion parameter as well as efficiency for all parameters. The findings\nindicate a potentially fruitful direction for the further development of\nestimation for jump-diffusions. \n\n"}
{"id": "1709.00640", "contents": "Title: When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests,\n  $\\ell_2$-consistency and Neuroscience Applications Abstract: Many studies in biomedical and health sciences involve small sample sizes due\nto logistic or financial constraints. Often, identifying weak (but\nscientifically interesting) associations between a set of predictors and a\nresponse necessitates pooling datasets from multiple diverse labs or groups.\nWhile there is a rich literature in statistical machine learning to address\ndistributional shifts and inference in multi-site datasets, it is less clear\n${\\it when}$ such pooling is guaranteed to help (and when it does not) --\nindependent of the inference algorithms we use. In this paper, we present a\nhypothesis test to answer this question, both for classical and high\ndimensional linear regression. We precisely identify regimes where pooling\ndatasets across multiple sites is sensible, and how such policy decisions can\nbe made via simple checks executable on each site before any data transfer ever\nhappens. With a focus on Alzheimer's disease studies, we present empirical\nresults showing that in regimes suggested by our analysis, pooling a local\ndataset with data from an international study improves power. \n\n"}
{"id": "1709.01596", "contents": "Title: Evaluating Partisan Gerrymandering in Wisconsin Abstract: We examine the extent of gerrymandering for the 2010 General Assembly\ndistrict map of Wisconsin. We find that there is substantial variability in the\nelection outcome depending on what maps are used. We also found robust evidence\nthat the district maps are highly gerrymandered and that this gerrymandering\nlikely altered the partisan make up of the Wisconsin General Assembly in some\nelections. Compared to the distribution of possible redistricting plans for the\nGeneral Assembly, Wisconsin's chosen plan is an outlier in that it yields\nresults that are highly skewed to the Republicans when the statewide proportion\nof Democratic votes comprises more than 50-52% of the overall vote (with the\nprecise threshold depending on the election considered). Wisconsin's plan acts\nto preserve the Republican majority by providing extra Republican seats even\nwhen the Democratic vote increases into the range when the balance of power\nwould shift for the vast majority of redistricting plans. \n\n"}
{"id": "1709.01781", "contents": "Title: Parameterizations for Ensemble Kalman Inversion Abstract: The use of ensemble methods to solve inverse problems is attractive because\nit is a derivative-free methodology which is also well-adapted to\nparallelization. In its basic iterative form the method produces an ensemble of\nsolutions which lie in the linear span of the initial ensemble. Choice of the\nparameterization of the unknown field is thus a key component of the success of\nthe method. We demonstrate how both geometric ideas and hierarchical ideas can\nbe used to design effective parameterizations for a number of applied inverse\nproblems arising in electrical impedance tomography, groundwater flow and\nsource inversion. In particular we show how geometric ideas, including the\nlevel set method, can be used to reconstruct piecewise continuous fields, and\nwe show how hierarchical methods can be used to learn key parameters in\ncontinuous fields, such as length-scales, resulting in improved\nreconstructions. Geometric and hierarchical ideas are combined in the level set\nmethod to find piecewise constant reconstructions with interfaces of unknown\ntopology. \n\n"}
{"id": "1709.03154", "contents": "Title: Recent progress in log-concave density estimation Abstract: In recent years, log-concave density estimation via maximum likelihood\nestimation has emerged as a fascinating alternative to traditional\nnonparametric smoothing techniques, such as kernel density estimation, which\nrequire the choice of one or more bandwidths. The purpose of this article is to\ndescribe some of the properties of the class of log-concave densities on\n$\\mathbb{R}^d$ which make it so attractive from a statistical perspective, and\nto outline the latest methodological, theoretical and computational advances in\nthe area. \n\n"}
{"id": "1709.04196", "contents": "Title: Particle Filters and Data Assimilation Abstract: State-space models can be used to incorporate subject knowledge on the\nunderlying dynamics of a time series by the introduction of a latent Markov\nstate-process. A user can specify the dynamics of this process together with\nhow the state relates to partial and noisy observations that have been made.\nInference and prediction then involves solving a challenging inverse problem:\ncalculating the conditional distribution of quantities of interest given the\nobservations. This article reviews Monte Carlo algorithms for solving this\ninverse problem, covering methods based on the particle filter and the ensemble\nKalman filter. We discuss the challenges posed by models with high-dimensional\nstates, joint estimation of parameters and the state, and inference for the\nhistory of the state process. We also point out some potential new developments\nwhich will be important for tackling cutting-edge filtering applications. \n\n"}
{"id": "1709.05548", "contents": "Title: Forecasting of commercial sales with large scale Gaussian Processes Abstract: This paper argues that there has not been enough discussion in the field of\napplications of Gaussian Process for the fast moving consumer goods industry.\nYet, this technique can be important as it e.g., can provide automatic feature\nrelevance determination and the posterior mean can unlock insights on the data.\nSignificant challenges are the large size and high dimensionality of commercial\ndata at a point of sale. The study reviews approaches in the Gaussian Processes\nmodeling for large data sets, evaluates their performance on commercial sales\nand shows value of this type of models as a decision-making tool for\nmanagement. \n\n"}
{"id": "1709.06320", "contents": "Title: Nonnegative matrix factorization with side information for time series\n  recovery and prediction Abstract: Motivated by the reconstruction and the prediction of electricity\nconsumption, we extend Nonnegative Matrix Factorization~(NMF) to take into\naccount side information (column or row features). We consider general linear\nmeasurement settings, and propose a framework which models non-linear\nrelationships between features and the response variables. We extend previous\ntheoretical results to obtain a sufficient condition on the identifiability of\nthe NMF in this setting. Based the classical Hierarchical Alternating Least\nSquares~(HALS) algorithm, we propose a new algorithm (HALSX, or Hierarchical\nAlternating Least Squares with eXogeneous variables) which estimates the\nfactorization model. The algorithm is validated on both simulated and real\nelectricity consumption datasets as well as a recommendation dataset, to show\nits performance in matrix recovery and prediction for new rows and columns. \n\n"}
{"id": "1709.06556", "contents": "Title: Robust clustering of languages across Wikipedia growth Abstract: Wikipedia is the largest existing knowledge repository that is growing on a\ngenuine crowdsourcing support. While the English Wikipedia is the most\nextensive and the most researched one with over five million articles,\ncomparatively little is known about the behavior and growth of the remaining\n283 smaller Wikipedias, the smallest of which, Afar, has only one article. Here\nwe use a subset of this data, consisting of 14962 different articles, each of\nwhich exists in 26 different languages, from Arabic to Ukrainian. We study the\ngrowth of Wikipedias in these languages over a time span of 15 years. We show\nthat, while an average article follows a random path from one language to\nanother, there exist six well-defined clusters of Wikipedias that share common\ngrowth patterns. The make-up of these clusters is remarkably robust against the\nmethod used for their determination, as we verify via four different clustering\nmethods. Interestingly, the identified Wikipedia clusters have little\ncorrelation with language families and groups. Rather, the growth of Wikipedia\nacross different languages is governed by different factors, ranging from\nsimilarities in culture to information literacy. \n\n"}
{"id": "1709.08036", "contents": "Title: Conditional randomization tests of causal effects with interference\n  between units Abstract: Many causal questions involve interactions between units, also known as\ninterference, for example between individuals in households, students in\nschools, or firms in markets. In this paper, we formalize the concept of a\nconditioning mechanism, which provides a framework for constructing valid and\npowerful randomization tests under general forms of interference. We describe\nour framework in the context of two-stage randomized designs and apply our\napproach to a randomized evaluation of an intervention targeting student\nabsenteeism in the School District of Philadelphia. We show improvements over\nexisting methods in terms of computational and statistical power. \n\n"}
{"id": "1709.08221", "contents": "Title: Model Averaging and its Use in Economics Abstract: The method of model averaging has become an important tool to deal with model\nuncertainty, for example in situations where a large amount of different\ntheories exist, as are common in economics. Model averaging is a natural and\nformal response to model uncertainty in a Bayesian framework, and most of the\npaper deals with Bayesian model averaging. The important role of the prior\nassumptions in these Bayesian procedures is highlighted. In addition,\nfrequentist model averaging methods are also discussed. Numerical methods to\nimplement these methods are explained, and I point the reader to some freely\navailable computational resources. The main focus is on uncertainty regarding\nthe choice of covariates in normal linear regression models, but the paper also\ncovers other, more challenging, settings, with particular emphasis on sampling\nmodels commonly used in economics. Applications of model averaging in economics\nare reviewed and discussed in a wide range of areas, among which growth\neconomics, production modelling, finance and forecasting macroeconomic\nquantities. \n\n"}
{"id": "1709.09268", "contents": "Title: FSL-BM: Fuzzy Supervised Learning with Binary Meta-Feature for\n  Classification Abstract: This paper introduces a novel real-time Fuzzy Supervised Learning with Binary\nMeta-Feature (FSL-BM) for big data classification task. The study of real-time\nalgorithms addresses several major concerns, which are namely: accuracy, memory\nconsumption, and ability to stretch assumptions and time complexity. Attaining\na fast computational model providing fuzzy logic and supervised learning is one\nof the main challenges in the machine learning. In this research paper, we\npresent FSL-BM algorithm as an efficient solution of supervised learning with\nfuzzy logic processing using binary meta-feature representation using Hamming\nDistance and Hash function to relax assumptions. While many studies focused on\nreducing time complexity and increasing accuracy during the last decade, the\nnovel contribution of this proposed solution comes through integration of\nHamming Distance, Hash function, binary meta-features, binary classification to\nprovide real time supervised method. Hash Tables (HT) component gives a fast\naccess to existing indices; and therefore, the generation of new indices in a\nconstant time complexity, which supersedes existing fuzzy supervised algorithms\nwith better or comparable results. To summarize, the main contribution of this\ntechnique for real-time Fuzzy Supervised Learning is to represent hypothesis\nthrough binary input as meta-feature space and creating the Fuzzy Supervised\nHash table to train and validate model. \n\n"}
{"id": "1709.10401", "contents": "Title: Application of Compressive Sensing Techniques in Distributed Sensor\n  Networks: A Survey Abstract: In this survey paper, our goal is to discuss recent advances of compressive\nsensing (CS) based solutions in wireless sensor networks (WSNs) including the\nmain ongoing/recent research efforts, challenges and research trends in this\narea. In WSNs, CS based techniques are well motivated by not only the sparsity\nprior observed in different forms but also by the requirement of efficient\nin-network processing in terms of transmit power and communication bandwidth\neven with nonsparse signals. In order to apply CS in a variety of WSN\napplications efficiently, there are several factors to be considered beyond the\nstandard CS framework. We start the discussion with a brief introduction to the\ntheory of CS and then describe the motivational factors behind the potential\nuse of CS in WSN applications. Then, we identify three main areas along which\nthe standard CS framework is extended so that CS can be efficiently applied to\nsolve a variety of problems specific to WSNs. In particular, we emphasize on\nthe significance of extending the CS framework to (i). take communication\nconstraints into account while designing projection matrices and reconstruction\nalgorithms for signal reconstruction in centralized as well in decentralized\nsettings, (ii) solve a variety of inference problems such as detection,\nclassification and parameter estimation, with compressed data without signal\nreconstruction and (iii) take practical communication aspects such as\nmeasurement quantization, physical layer secrecy constraints, and imperfect\nchannel conditions into account. Finally, open research issues and challenges\nare discussed in order to provide perspectives for future research directions. \n\n"}
{"id": "1710.00473", "contents": "Title: Importance Sampling and its Optimality for Stochastic Simulation Models Abstract: We consider the problem of estimating an expected outcome from a stochastic\nsimulation model. Our goal is to develop a theoretical framework on importance\nsampling for such estimation. By investigating the variance of an importance\nsampling estimator, we propose a two-stage procedure that involves a regression\nstage and a sampling stage to construct the final estimator. We introduce a\nparametric and a nonparametric regression estimator in the first stage and\nstudy how the allocation between the two stages affects the performance of the\nfinal estimator. We analyze the variance reduction rates and derive oracle\nproperties of both methods. We evaluate the empirical performances of the\nmethods using two numerical examples and a case study on wind turbine\nreliability evaluation. \n\n"}
{"id": "1710.00875", "contents": "Title: Local likelihood estimation of complex tail dependence structures,\n  applied to U.S. precipitation extremes Abstract: To disentangle the complex non-stationary dependence structure of\nprecipitation extremes over the entire contiguous U.S., we propose a flexible\nlocal approach based on factor copula models. Our sub-asymptotic spatial\nmodeling framework yields non-trivial tail dependence structures, with a\nweakening dependence strength as events become more extreme, a feature commonly\nobserved with precipitation data but not accounted for in classical asymptotic\nextreme-value models. To estimate the local extremal behavior, we fit the\nproposed model in small regional neighborhoods to high threshold exceedances,\nunder the assumption of local stationarity, which allows us to gain in\nflexibility. Adopting a local censored likelihood approach, inference is made\non a fine spatial grid, and local estimation is performed by taking advantage\nof distributed computing resources and the embarrassingly parallel nature of\nthis estimation procedure. The local model is efficiently fitted at all grid\npoints, and uncertainty is measured using a block bootstrap procedure. An\nextensive simulation study shows that our approach can adequately capture\ncomplex, non-stationary dependencies, while our study of U.S. winter\nprecipitation data reveals interesting differences in local tail structures\nover space, which has important implications on regional risk assessment of\nextreme precipitation events. \n\n"}
{"id": "1710.00894", "contents": "Title: Detecting Epistatic Selection with Partially Observed Genotype Data\n  Using Copula Graphical Models Abstract: Recombinant Inbred Lines derived from divergent parental lines can display\nextensive segregation distortion and long-range linkage disequilibrium (LD)\nbetween distant loci. These genomic signatures are consistent with epistatic\nselection during inbreeding. Epistatic interactions affect growth and fertility\ntraits or even cause complete lethality. Detecting epistasis is challenging as\nmultiple testing approaches are under-powered and true long-range LD is\ndifficult to distinguish from drift.\n  Here we develop a method for reconstructing an underlying network of genomic\nsignatures of high-dimensional epistatic selection from multi-locus genotype\ndata. The network captures the conditionally dependent short- and long-range LD\nstructure and thus reveals \"aberrant\" marker-marker associations that are due\nto epistatic selection rather than gametic linkage. The network estimation\nrelies on penalized Gaussian copula graphical models, which accounts for a\nlarge number of markers p and a small number of individuals n.\n  A multi-core implementation of our algorithm makes it feasible to estimate\nthe graph in high-dimensions also in the presence of significant portions of\nmissing data. We demonstrate the efficiency of the proposed method on simulated\ndatasets as well as on genotyping data in A.thaliana and maize. In addition, we\nimplemented the method in the R package netgwas which is freely available at\nhttps://CRAN.R-project.org/package=netgwas. \n\n"}
{"id": "1710.00894", "contents": "Title: Detecting Epistatic Selection with Partially Observed Genotype Data\n  Using Copula Graphical Models Abstract: Recombinant Inbred Lines derived from divergent parental lines can display\nextensive segregation distortion and long-range linkage disequilibrium (LD)\nbetween distant loci. These genomic signatures are consistent with epistatic\nselection during inbreeding. Epistatic interactions affect growth and fertility\ntraits or even cause complete lethality. Detecting epistasis is challenging as\nmultiple testing approaches are under-powered and true long-range LD is\ndifficult to distinguish from drift.\n  Here we develop a method for reconstructing an underlying network of genomic\nsignatures of high-dimensional epistatic selection from multi-locus genotype\ndata. The network captures the conditionally dependent short- and long-range LD\nstructure and thus reveals \"aberrant\" marker-marker associations that are due\nto epistatic selection rather than gametic linkage. The network estimation\nrelies on penalized Gaussian copula graphical models, which accounts for a\nlarge number of markers p and a small number of individuals n.\n  A multi-core implementation of our algorithm makes it feasible to estimate\nthe graph in high-dimensions also in the presence of significant portions of\nmissing data. We demonstrate the efficiency of the proposed method on simulated\ndatasets as well as on genotyping data in A.thaliana and maize. In addition, we\nimplemented the method in the R package netgwas which is freely available at\nhttps://CRAN.R-project.org/package=netgwas. \n\n"}
{"id": "1710.01063", "contents": "Title: De novo construction of polyploid linkage maps using discrete graphical\n  models Abstract: Linkage maps are used to identify the location of genes responsible for\ntraits and diseases. New sequencing techniques have created opportunities to\nsubstantially increase the density of genetic markers. Such revolutionary\nadvances in technology have given rise to new challenges, such as creating\nhigh-density linkage maps. Current multiple testing approaches based on\npairwise recombination fractions are underpowered in the high-dimensional\nsetting and do not extend easily to polyploid species. We propose to construct\nlinkage maps using graphical models either via a sparse Gaussian copula or a\nnonparanormal skeptic approach. Linkage groups (LGs), typically chromosomes,\nand the order of markers in each LG are determined by inferring the conditional\nindependence relationships among large numbers of markers in the genome.\nThrough simulations, we illustrate the utility of our map construction method\nand compare its performance with other available methods, both when the data\nare clean and contain no missing observations and when data contain genotyping\nerrors and are incomplete. We apply the proposed method to two genotype\ndatasets: barley and potato from diploid and polypoid populations,\nrespectively. Our comprehensive map construction method makes full use of the\ndosage SNP data to reconstruct linkage map for any bi-parental diploid and\npolyploid species. We have implemented the method in the R package netgwas. \n\n"}
{"id": "1710.01063", "contents": "Title: De novo construction of polyploid linkage maps using discrete graphical\n  models Abstract: Linkage maps are used to identify the location of genes responsible for\ntraits and diseases. New sequencing techniques have created opportunities to\nsubstantially increase the density of genetic markers. Such revolutionary\nadvances in technology have given rise to new challenges, such as creating\nhigh-density linkage maps. Current multiple testing approaches based on\npairwise recombination fractions are underpowered in the high-dimensional\nsetting and do not extend easily to polyploid species. We propose to construct\nlinkage maps using graphical models either via a sparse Gaussian copula or a\nnonparanormal skeptic approach. Linkage groups (LGs), typically chromosomes,\nand the order of markers in each LG are determined by inferring the conditional\nindependence relationships among large numbers of markers in the genome.\nThrough simulations, we illustrate the utility of our map construction method\nand compare its performance with other available methods, both when the data\nare clean and contain no missing observations and when data contain genotyping\nerrors and are incomplete. We apply the proposed method to two genotype\ndatasets: barley and potato from diploid and polypoid populations,\nrespectively. Our comprehensive map construction method makes full use of the\ndosage SNP data to reconstruct linkage map for any bi-parental diploid and\npolyploid species. We have implemented the method in the R package netgwas. \n\n"}
{"id": "1710.01369", "contents": "Title: Bayesian Fused Lasso regression for dynamic binary networks Abstract: We propose a multinomial logistic regression model for link prediction in a\ntime series of directed binary networks. To account for the dynamic nature of\nthe data we employ a dynamic model for the model parameters that is strongly\nconnected with the fused lasso penalty. In addition to promoting sparseness,\nthis prior allows us to explore the presence of change points in the structure\nof the network. We introduce fast computational algorithms for estimation and\nprediction using both optimization and Bayesian approaches. The performance of\nthe model is illustrated using simulated data and data from a financial trading\nnetwork in the NYMEX natural gas futures market. Supplementary material\ncontaining the trading network data set and code to implement the algorithms is\navailable online. \n\n"}
{"id": "1710.01470", "contents": "Title: Multi-scale Invariant Fields: Estimation and Prediction Abstract: Extending the concept of multi-selfsimilar random field we study multi-scale\ninvariant (MSI) fields which have component-wise discrete scale invariant\nproperty. Assuming scale parameters as $\\lambda_i>1$, $i=1,\\ldots,d$ and the\nparameter space as $(1, \\infty)^d$, the first scale rectangle is referred to\nthe rectangle $ (1, \\lambda_1)\\times \\ldots \\times (1, \\lambda_d)$. Applying\ncertain component-wise geometric sampling of MSI field, the harmonic-like\nrepresentation and spectral density of the sampled MSI field are characterized.\nFurthermore, the covariance function and spectral density of the sampled Markov\nMSI field are presented by the variances and covariances of samples inside\nfirst scale rectangle. As an example of MSI field, a two-dimensional simple\nfractional Brownian sheet (sfBs) is demonstrated. Also real data of the\nprecipitation in some area of Brisbane in Australia for two days (25 and 26\nJanuary 2013) are examined. We show that precipitation on this area has MSI\nproperty and estimate it as a simple MSI field with stationary increments\ninside scale intervals.\n  This structure enables us to predict the precipitation in surface and time.\nWe apply the mean absolute percentage error as a measure for the accuracy of\nthe predictions. \n\n"}
{"id": "1710.02669", "contents": "Title: Aggregated moving functional median in robust prediction of hierarchical\n  functional time series - an application to forecasting web portal users\n  behaviors Abstract: In this article, a new nonparametric and robust method of forecasting\nhierarchical functional time series is presented. The method is compared with\nHyndman and Shang's method with respect to their unbiasedness, effectiveness,\nrobustness, and computational complexity. Taking into account results of the\nanalytical, simulation and empirical studies, we come to the conclusion that\nour proposal is superior over the proposal of Hyndman and Shang with respect to\nsome statistical criteria and especially with respect to robustness and\ncomputational complexity. An empirical usefulness of our method is presented on\nexample of management of a certain web portal divided into four subservices. An\nextensive simulation study involving hierarchical systems consisted of FAR(1)\nprocesses and Wiener processes has been conducted as well. \n\n"}
{"id": "1710.02931", "contents": "Title: Linked Matrix Factorization Abstract: In recent years, a number of methods have been developed for the dimension\nreduction and decomposition of multiple linked high-content data matrices.\nTypically these methods assume that just one dimension, rows or columns, is\nshared among the data sources. This shared dimension may represent common\nfeatures that are measured for different sample sets (i.e., horizontal\nintegration) or a common set of samples with measurements for different feature\nsets (i.e., vertical integration). In this article we introduce an approach for\nsimultaneous horizontal and vertical integration, termed Linked Matrix\nFactorization (LMF), for the more general situation where some matrices share\nrows (e.g., features) and some share columns (e.g., samples). Our motivating\napplication is a cytotoxicity study with accompanying genomic and molecular\nchemical attribute data. In this data set, the toxicity matrix (cell lines\n$\\times$ chemicals) shares its sample set with a genotype matrix (cell lines\n$\\times$ SNPs), and shares its feature set with a chemical molecular attribute\nmatrix (chemicals $\\times$ attributes). LMF gives a unified low-rank\nfactorization of these three matrices, which allows for the decomposition of\nsystematic variation that is shared among the three matrices and systematic\nvariation that is specific to each matrix. This may be used for efficient\ndimension reduction, exploratory visualization, and the imputation of missing\ndata even when entire rows or columns are missing from a constituent data\nmatrix. We present theoretical results concerning the uniqueness,\nidentifiability, and minimal parametrization of LMF, and evaluate it with\nextensive simulation studies. \n\n"}
{"id": "1710.05008", "contents": "Title: Automatic Detection and Uncertainty Quantification of Landmarks on\n  Elastic Curves Abstract: A population quantity of interest in statistical shape analysis is the\nlocation of landmarks, which are points that aid in reconstructing and\nrepresenting shapes of objects. We provide an automated, model-based approach\nto inferring landmarks given a sample of shape data. The model is formulated\nbased on a linear reconstruction of the shape, passing through the specified\npoints, and a Bayesian inferential approach is described for estimating unknown\nlandmark locations. The question of how many landmarks to select is addressed\nin two different ways: (1) by defining a criterion-based approach, and (2)\njoint estimation of the number of landmarks along with their locations.\nEfficient methods for posterior sampling are also discussed. We motivate our\napproach using several simulated examples, as well as data obtained from\napplications in computer vision and biology; additionally, we explore\nplacements and associated uncertainty in landmarks for various substructures\nextracted from magnetic resonance image slices. \n\n"}
{"id": "1710.05248", "contents": "Title: A Nonparametric Method for Producing Isolines of Bivariate Exceedance\n  Probabilities Abstract: We present a method for drawing isolines indicating regions of equal joint\nexceedance probability for bivariate data. The method relies on bivariate\nregular variation, a dependence framework widely used for extremes. This\nframework enables drawing isolines corresponding to very low exceedance\nprobabilities and these lines may lie beyond the range of the data. The method\nwe utilize for characterizing dependence in the tail is largely nonparametric.\nFurthermore, we extend this method to the case of asymptotic independence and\npropose a procedure which smooths the transition from asymptotic independence\nin the interior to the first-order behavior on the axes. We propose a\ndiagnostic plot for assessing isoline estimate and choice of smoothing, and a\nbootstrap procedure to visually assess uncertainty. \n\n"}
{"id": "1710.05284", "contents": "Title: Multivariate Generalized Linear Mixed Models for Joint Estimation of\n  Sporting Outcomes Abstract: This paper explores improvements in prediction accuracy and inference\ncapability when allowing for potential correlation in team-level random effects\nacross multiple game-level responses from different assumed distributions.\nFirst-order and fully exponential Laplace approximations are used to fit\nnormal-binary and Poisson-binary multivariate generalized linear mixed models\nwith non-nested random effects structures. We have built these models into the\nR package mvglmmRank, which is used to explore several seasons of American\ncollege football and basketball data. \n\n"}
{"id": "1710.06351", "contents": "Title: Multivariate Spatial-temporal Prediction on Latent Low-dimensional\n  Functional Structure with Non-stationarity Abstract: Multivariate spatio-temporal data arise more and more frequently in a wide\nrange of applications; however, there are relatively few general statistical\nmethods that can readily use that incorporate spatial, temporal and variable\ndependencies simultaneously. In this paper, we propose a new approach to\nrepresent non-parametrically the linear dependence structure of a multivariate\nspatio-temporal process in terms of latent common factors. The matrix structure\nof observations from the multivariate spatio-temporal process is well reserved\nthrough the matrix factor model configuration. The spatial loading functions\nare estimated non-parametrically by sieve approximation and the variable\nloading matrix is estimated via an eigen-analysis of a symmetric non-negative\ndefinite matrix. Though factor decomposition along the space mode is similar to\nthe low-rank approximation methods in spatial statistics, the fundamental\ndifference is that the low-dimensional structure is completely unknown in our\nsetting. Additionally, our method accommodates non-stationarity over space. The\nestimated loading functions facilitate spatial prediction. For temporal\nforecasting, we preserve the matrix structure of observations at each time\npoint by utilizing the matrix autoregressive model of order one MAR(1).\nAsymptotic properties of the proposed methods are established. Performance of\nthe proposed method is investigated on both synthetic and real datasets \n\n"}
{"id": "1710.06660", "contents": "Title: Variable selection for the prediction of C[0,1]-valued AR processes\n  using RKHS Abstract: A model for the prediction of functional time series is introduced, where\nobservations are assumed to be continuous random functions. We model the\ndependence of the data with a nonstandard autoregressive structure, motivated\nin terms of the Reproducing Kernel Hilbert Space (RKHS) generated by the\nauto-covariance function of the data. The new approach helps to find relevant\npoints of the curves in terms of prediction accuracy. This dimension reduction\ntechnique is particularly useful for applications, since the results are\nusually directly interpretable in terms of the original curves. An empirical\nstudy involving real and simulated data is included, which generates\ncompetitive results. Supplementary material includes R-Code, tables and\nmathematical comments. \n\n"}
{"id": "1710.08074", "contents": "Title: Regularized calibrated estimation of propensity scores with model\n  misspecification and high-dimensional data Abstract: Propensity score methods are widely used for estimating treatment effects\nfrom observational studies. A popular approach is to estimate propensity scores\nby maximum likelihood based on logistic regression, and then apply inverse\nprobability weighted estimators or extensions to estimate treatment effects.\nHowever, a challenging issue is that such inverse probability weighting methods\nincluding doubly robust methods can perform poorly even when the logistic model\nappears adequate as examined by conventional techniques. In addition, there is\nincreasing difficulty to appropriately estimate propensity scores when dealing\nwith a large number of covariates. To address these issues, we study calibrated\nestimation as an alternative to maximum likelihood estimation for fitting\nlogistic propensity score models. We show that, with possible model\nmisspecification, minimizing the expected calibration loss underlying the\ncalibrated estimators involves reducing both the expected likelihood loss and a\nmeasure of relative errors which controls the mean squared errors of inverse\nprobability weighted estimators. Furthermore, we propose a regularized\ncalibrated estimator by minimizing the calibration loss with a Lasso penalty.\nWe develop a novel Fisher scoring descent algorithm for computing the proposed\nestimator, and provide a high-dimensional analysis of the resulting inverse\nprobability weighted estimators of population means, leveraging the control of\nrelative errors for calibrated estimation. We present a simulation study and an\nempirical application to demonstrate the advantages of the proposed methods\ncompared with maximum likelihood and regularization. \n\n"}
{"id": "1710.08269", "contents": "Title: A Potts-Mixture Spatiotemporal Joint Model for Combined MEG and EEG Data Abstract: We develop a new methodology for determining the location and dynamics of\nbrain activity from combined magnetoencephalography (MEG) and\nelectroencephalography (EEG) data. The resulting inverse problem is ill-posed\nand is one of the most difficult problems in neuroimaging data analysis. In our\ndevelopment we propose a solution that combines the data from three different\nmodalities, MRI, MEG, and EEG, together. We propose a new Bayesian spatial\nfinite mixture model that builds on the mesostate-space model developed by\nDaunizeau and Friston (2007). Our new model incorporates two major extensions:\n(i) We combine EEG and MEG data together and formulate a joint model for\ndealing with the two modalities simultaneously; (ii) we incorporate the Potts\nmodel to represent the spatial dependence in an allocation process that\npartitions the cortical surface into a small number of latent states termed\nmesostates. The cortical surface is obtained from MRI. We formulate the new\nspatiotemporal model and derive an efficient procedure for simultaneous point\nestimation and model selection based on the iterated conditional modes\nalgorithm combined with local polynomial smoothing. The proposed method results\nin a novel estimator for the number of mixture components and is able to select\nactive brain regions which correspond to active variables in a high-dimensional\ndynamic linear model. The methodology is investigated using synthetic data and\nsimulation studies and then demonstrated on an application examining the neural\nresponse to the perception of scrambled faces. R software implementing the\nmethodology along with several sample datasets are available at the following\nGitHub repository https://github.com/v2south/PottsMix. \n\n"}
{"id": "1710.09791", "contents": "Title: Structural Variability from Noisy Tomographic Projections Abstract: In cryo-electron microscopy, the 3D electric potentials of an ensemble of\nmolecules are projected along arbitrary viewing directions to yield noisy 2D\nimages. The volume maps representing these potentials typically exhibit a great\ndeal of structural variability, which is described by their 3D covariance\nmatrix. Typically, this covariance matrix is approximately low-rank and can be\nused to cluster the volumes or estimate the intrinsic geometry of the\nconformation space. We formulate the estimation of this covariance matrix as a\nlinear inverse problem, yielding a consistent least-squares estimator. For $n$\nimages of size $N$-by-$N$ pixels, we propose an algorithm for calculating this\ncovariance estimator with computational complexity\n$\\mathcal{O}(nN^4+\\sqrt{\\kappa}N^6 \\log N)$, where the condition number\n$\\kappa$ is empirically in the range $10$--$200$. Its efficiency relies on the\nobservation that the normal equations are equivalent to a deconvolution problem\nin 6D. This is then solved by the conjugate gradient method with an appropriate\ncirculant preconditioner. The result is the first computationally efficient\nalgorithm for consistent estimation of 3D covariance from noisy projections. It\nalso compares favorably in runtime with respect to previously proposed\nnon-consistent estimators. Motivated by the recent success of eigenvalue\nshrinkage procedures for high-dimensional covariance matrices, we introduce a\nshrinkage procedure that improves accuracy at lower signal-to-noise ratios. We\nevaluate our methods on simulated datasets and achieve classification results\ncomparable to state-of-the-art methods in shorter running time. We also present\nresults on clustering volumes in an experimental dataset, illustrating the\npower of the proposed algorithm for practical determination of structural\nvariability. \n\n"}
{"id": "1710.10709", "contents": "Title: Distributional Consistency of Lasso by Perturbation Bootstrap Abstract: Least Absolute Shrinkage and Selection Operator or the Lasso, introduced by\nTibshirani (1996), is a popular estimation procedure in multiple linear\nregression when underlying design has a sparse structure, because of its\nproperty that it sets some regression coefficients exactly equal to 0. In this\narticle, we develop a perturbation bootstrap method and establish its validity\nin approximating the distribution of the Lasso in heteroscedastic linear\nregression. We allow the underlying covariates to be either random or\nnon-random. We show that the proposed bootstrap method works irrespective of\nthe nature of the covariates, unlike the resample-based bootstrap of Freedman\n(1981) which must be tailored based on the nature (random vs non-random) of the\ncovariates. Simulation study also justifies our method in finite samples. \n\n"}
{"id": "1710.10980", "contents": "Title: Statistical validation of financial time series via visibility graph Abstract: Statistical physics of complex systems exploits network theory not only to\nmodel, but also to effectively extract information from many dynamical\nreal-world systems. A pivotal case of study is given by financial systems:\nmarket prediction represents an unsolved scientific challenge yet with crucial\nimplications for society, as financial crises have devastating effects on real\neconomies. Thus, nowadays the quest for a robust estimator of market efficiency\nis both a scientific and institutional priority. In this work we study the\nvisibility graphs built from the time series of several trade market indices.\nWe propose a validation procedure for each link of these graphs against a null\nhypothesis derived from ARCH-type modeling of such series. Building on this\nframework, we devise a market indicator that turns out to be highly correlated\nand even predictive of financial instability periods. \n\n"}
{"id": "1710.11529", "contents": "Title: A 4D-Var Method with Flow-Dependent Background Covariances for the\n  Shallow-Water Equations Abstract: The 4D-Var method for filtering partially observed nonlinear chaotic\ndynamical systems consists of finding the maximum a-posteriori (MAP) estimator\nof the initial condition of the system given observations over a time window,\nand propagating it forward to the current time via the model dynamics. This\nmethod forms the basis of most currently operational weather forecasting\nsystems. In practice the optimization becomes infeasible if the time window is\ntoo long due to the non-convexity of the cost function, the effect of model\nerrors, and the limited precision of the ODE solvers. Hence the window has to\nbe kept sufficiently short, and the observations in the previous windows can be\ntaken into account via a Gaussian background (prior) distribution. The choice\nof the background covariance matrix is an important question that has received\nmuch attention in the literature. In this paper, we define the background\ncovariances in a principled manner, based on observations in the previous $b$\nassimilation windows, for a parameter $b\\ge 1$. The method is at most $b$ times\nmore computationally expensive than using fixed background covariances,\nrequires little tuning, and greatly improves the accuracy of 4D-Var. As a\nconcrete example, we focus on the shallow-water equations. The proposed method\nis compared against state-of-the-art approaches in data assimilation and is\nshown to perform favourably on simulated data. We also illustrate our approach\non data from the recent tsunami of 2011 in Fukushima, Japan. \n\n"}
{"id": "1711.00031", "contents": "Title: Quantile Functional Regression using Quantlets Abstract: In this paper, we develop a quantile functional regression modeling framework\nthat models the distribution of a set of common repeated observations from a\nsubject through the quantile function, which is regressed on a set of\ncovariates to determine how these factors affect various aspects of the\nunderlying subject-specific distribution. To account for smoothness in the\nquantile functions, we introduce custom basis functions we call\n\\textit{quantlets} that are sparse, regularized, near-lossless, and empirically\ndefined, adapting to the features of a given data set and containing a Gaussian\nsubspace so {non-Gaussianness} can be assessed. While these quantlets could be\nused within various functional regression frameworks, we build a Bayesian\nframework that uses nonlinear shrinkage of quantlet coefficients to regularize\nthe functional regression coefficients and allows fully Bayesian inferences\nafter fitting a Markov chain Monte Carlo. Specifically, we apply global tests\nto assess which covariates have any effect on the distribution at all, followed\nby local tests to identify at which specific quantiles the differences lie\nwhile adjusting for multiple testing, and to assess whether the covariate\naffects certain major aspects of the distribution, including location, scale,\nskewness, Gaussianness, or tails. If the difference lies in these commonly-used\nsummaries, our approach can still detect them, but our systematic modeling\nstrategy can also detect effects on other aspects of the distribution that\nmight be missed if one restricted attention to pre-chosen summaries. We\ndemonstrate the benefit of the basis space modeling through simulation studies,\nand illustrate the method using a biomedical imaging data set in which we\nrelate the distribution of pixel intensities from a tumor image to various\ndemographic, clinical, and genetic characteristics. \n\n"}
{"id": "1711.00101", "contents": "Title: Nonparametric covariance estimation for mixed longitudinal studies, with\n  applications in midlife women's health Abstract: In mixed longitudinal studies, a group of subjects enter the study at\ndifferent ages (cross-sectional) and are followed for successive years\n(longitudinal). In the context of such studies, we consider nonparametric\ncovariance estimation with samples of noisy and partially observed functional\ntrajectories. The proposed algorithm is based on a noniterative\nsequential-aggregation scheme with only basic matrix operations and closed-form\nsolutions in each step. The good performance of the proposed method is\nsupported by both theory and numerical experiments. We also apply the proposed\nprocedure to a study on the working memory of midlife women, based on data from\nthe Study of Women's Health Across the Nation (SWAN). \n\n"}
{"id": "1711.00484", "contents": "Title: Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments Abstract: Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons. \n\n"}
{"id": "1711.00497", "contents": "Title: Post-selection estimation and testing following aggregated association\n  tests Abstract: The practice of pooling several individual test statistics to form aggregate\ntests is common in many statistical application where individual tests may be\nunderpowered. While selection by aggregate tests can serve to increase power,\nthe selection process invalidates the individual test-statistics, making it\ndifficult to identify the ones that drive the signal in follow-up inference.\nHere, we develop a general approach for valid inference following selection by\naggregate testing. We present novel powerful post-selection tests for the\nindividual null hypotheses which are exact for the normal model and\nasymptotically justified otherwise. Our approach relies on the ability to\ncharacterize the distribution of the individual test statistics after\nconditioning on the event of selection. We provide efficient algorithms for\nestimation of the post-selection maximum-likelihood estimates and suggest\nconfidence intervals which rely on a novel switching regime for good coverage\nguarantees. We validate our methods via comprehensive simulation studies and\napply them to data from the Dallas Heart Study, demonstrating that single\nvariant association discovery following selection by an aggregated test is\nindeed possible in practice. \n\n"}
{"id": "1711.00708", "contents": "Title: On Game-Theoretic Risk Management (Part Three) - Modeling and\n  Applications Abstract: The game-theoretic risk management framework put forth in the precursor\nreports \"Towards a Theory of Games with Payoffs that are\nProbability-Distributions\" (arXiv:1506.07368 [q-fin.EC]) and \"Algorithms to\nCompute Nash-Equilibria in Games with Distributions as Payoffs\"\n(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to\nintegrate the previously developed theory into risk management processes. To\nthis end, we discuss how loss models (primarily but not exclusively\nnon-parametric) can be constructed from data. Furthermore, hints are given on\nhow a meaningful game theoretic model can be set up, and how it can be used in\nvarious stages of the ISO 27000 risk management process. Examples related to\nadvanced persistent threats and social engineering are given. We conclude by a\ndiscussion on the meaning and practical use of (mixed) Nash equilibria\nequilibria for risk management. \n\n"}
{"id": "1711.01312", "contents": "Title: NeuralFDR: Learning Discovery Thresholds from Hypothesis Features Abstract: As datasets grow richer, an important challenge is to leverage the full\nfeatures in the data to maximize the number of useful discoveries while\ncontrolling for false positives. We address this problem in the context of\nmultiple hypotheses testing, where for each hypothesis, we observe a p-value\nalong with a set of features specific to that hypothesis. For example, in\ngenetic association studies, each hypothesis tests the correlation between a\nvariant and the trait. We have a rich set of features for each variant (e.g.\nits location, conservation, epigenetics etc.) which could inform how likely the\nvariant is to have a true association. However popular testing approaches, such\nas Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting\n(IHW), either ignore these features or assume that the features are categorical\nor uni-variate. We propose a new algorithm, NeuralFDR, which automatically\nlearns a discovery threshold as a function of all the hypothesis features. We\nparametrize the discovery threshold as a neural network, which enables flexible\nhandling of multi-dimensional discrete and continuous features as well as\nefficient end-to-end optimization. We prove that NeuralFDR has strong false\ndiscovery rate (FDR) guarantees, and show that it makes substantially more\ndiscoveries in synthetic and real datasets. Moreover, we demonstrate that the\nlearned discovery threshold is directly interpretable. \n\n"}
{"id": "1711.01318", "contents": "Title: Improving Exoplanet Detection Power: Multivariate Gaussian Process\n  Models for Stellar Activity Abstract: The radial velocity method is one of the most successful techniques for\ndetecting exoplanets. It works by detecting the velocity of a host star induced\nby the gravitational effect of an orbiting planet, specifically the velocity\nalong our line of sight, which is called the radial velocity of the star.\nLow-mass planets typically cause their host star to move with radial velocities\nof 1 m/s or less. By analyzing a time series of stellar spectra from a host\nstar, modern astronomical instruments can in theory detect such planets.\nHowever, in practice, intrinsic stellar variability (e.g., star spots,\nconvective motion, pulsations) affects the spectra and often mimics a radial\nvelocity signal. This signal contamination makes it difficult to reliably\ndetect low-mass planets. A principled approach to recovering planet radial\nvelocity signals in the presence of stellar activity was proposed by Rajpaul et\nal. (2015). It uses a multivariate Gaussian process model to jointly capture\ntime series of the apparent radial velocity and multiple indicators of stellar\nactivity. We build on this work in two ways: (i) we propose using dimension\nreduction techniques to construct new high-information stellar activity\nindicators; and (ii) we extend the Rajpaul et al. (2015) model to a larger\nclass of models and use a power-based model comparison procedure to select the\nbest model. Despite significant interest in exoplanets, previous efforts have\nnot performed large-scale stellar activity model selection or attempted to\nevaluate models based on planet detection power. In the case of main sequence\nG2V stars, we find that our method substantially improves planet detection\npower compared to previous state-of-the-art approaches. \n\n"}
{"id": "1711.02141", "contents": "Title: Optimal rates of entropy estimation over Lipschitz balls Abstract: We consider the problem of minimax estimation of the entropy of a density\nover Lipschitz balls. Dropping the usual assumption that the density is bounded\naway from zero, we obtain the minimax rates $(n\\ln n)^{-s/(s+d)} + n^{-1/2}$\nfor $0<s\\leq 2$ for densities supported on $[0,1]^d$, where $s$ is the\nsmoothness parameter and $n$ is the number of independent samples. We\ngeneralize the results to densities with unbounded support: given an Orlicz\nfunctions $\\Psi$ of rapid growth (such as the sub-exponential and sub-Gaussian\nclasses), the minimax rates for densities with bounded $\\Psi$-Orlicz norm\nincrease to $(n\\ln n)^{-s/(s+d)} (\\Psi^{-1}(n))^{d(1-d/p(s+d))} + n^{-1/2}$,\nwhere $p$ is the norm parameter in the Lipschitz ball. We also show that the\nintegral-form plug-in estimators with kernel density estimates fail to achieve\nthe minimax rates, and characterize their worst case performances over the\nLipschitz ball.\n  One of the key steps in analyzing the bias relies on a novel application of\nthe Hardy-Littlewood maximal inequality, which also leads to a new inequality\non the Fisher information that may be of independent interest. \n\n"}
{"id": "1711.02774", "contents": "Title: The extended power distribution: A new distribution on $(0, 1)$ Abstract: We propose a two-parameter bounded probability distribution called the\nextended power distribution. This distribution on $(0, 1)$ is similar to the\nbeta distribution, however there are some advantages which we explore. We\ndefine the moments and quantiles of this distribution and show that it is\npossible to give an $r$-parameter extension of this distribution ($r>2$). We\nalso consider its complementary distribution and show that it has some\nflexibility advantages over the Kumaraswamy and beta distributions. This\ndistribution can be used as an alternative to the Kumaraswamy distribution\nsince it has a closed form for its cumulative function. However, it can be\nfitted to data where there are some samples that are exactly equal to 1, unlike\nthe Kumaraswamy and beta distributions which cannot be fitted to such data or\nmay require some censoring. Applications considered show the extended power\ndistribution performs favourably against the Kumaraswamy distribution in most\ncases. \n\n"}
{"id": "1711.03170", "contents": "Title: Penalized Orthogonal Iteration for Sparse Estimation of Generalized\n  Eigenvalue Problem Abstract: We propose a new algorithm for sparse estimation of eigenvectors in\ngeneralized eigenvalue problems (GEP). The GEP arises in a number of modern\ndata-analytic situations and statistical methods, including principal component\nanalysis (PCA), multiclass linear discriminant analysis (LDA), canonical\ncorrelation analysis (CCA), sufficient dimension reduction (SDR) and invariant\nco-ordinate selection. We propose to modify the standard generalized orthogonal\niteration with a sparsity-inducing penalty for the eigenvectors. To achieve\nthis goal, we generalize the equation-solving step of orthogonal iteration to a\npenalized convex optimization problem. The resulting algorithm, called\npenalized orthogonal iteration, provides accurate estimation of the true\neigenspace, when it is sparse. Also proposed is a computationally more\nefficient alternative, which works well for PCA and LDA problems. Numerical\nstudies reveal that the proposed algorithms are competitive, and that our\ntuning procedure works well. We demonstrate applications of the proposed\nalgorithm to obtain sparse estimates for PCA, multiclass LDA, CCA and SDR.\nSupplementary materials are available online. \n\n"}
{"id": "1711.05204", "contents": "Title: A Tutorial on Estimating Time-Varying Vector Autoregressive Models Abstract: Time series of individual subjects have become a common data type in\npsychological research. These data allow one to estimate models of\nwithin-subject dynamics, and thereby avoid the notorious problem of making\nwithin-subjects inferences from between-subjects data, and naturally address\nheterogeneity between subjects. A popular model for these data is the Vector\nAutoregressive (VAR) model, in which each variable is predicted as a linear\nfunction of all variables at previous time points. A key assumption of this\nmodel is that its parameters are constant (or stationary) across time. However,\nin many areas of psychological research time-varying parameters are plausible\nor even the subject of study. In this tutorial paper, we introduce methods to\nestimate time-varying VAR models based on splines and kernel-smoothing\nwith/without regularization. We use simulations to evaluate the relative\nperformance of all methods in scenarios typical in applied research, and\ndiscuss their strengths and weaknesses. Finally, we provide a step-by-step\ntutorial showing how to apply the discussed methods to an openly available time\nseries of mood-related measurements. \n\n"}
{"id": "1711.05869", "contents": "Title: Predictive Independence Testing, Predictive Conditional Independence\n  Testing, and Predictive Graphical Modelling Abstract: Testing (conditional) independence of multivariate random variables is a task\ncentral to statistical inference and modelling in general - though\nunfortunately one for which to date there does not exist a practicable\nworkflow. State-of-art workflows suffer from the need for heuristic or\nsubjective manual choices, high computational complexity, or strong parametric\nassumptions.\n  We address these problems by establishing a theoretical link between\nmultivariate/conditional independence testing, and model comparison in the\nmultivariate predictive modelling aka supervised learning task. This link\nallows advances in the extensively studied supervised learning workflow to be\ndirectly transferred to independence testing workflows - including automated\ntuning of machine learning type which addresses the need for a heuristic\nchoice, the ability to quantitatively trade-off computational demand with\naccuracy, and the modern black-box philosophy for checking and interfacing.\n  As a practical implementation of this link between the two workflows, we\npresent a python package 'pcit', which implements our novel multivariate and\nconditional independence tests, interfacing the supervised learning API of the\nscikit-learn package. Theory and package also allow for straightforward\nindependence test based learning of graphical model structure.\n  We empirically show that our proposed predictive independence test outperform\nor are on par to current practice, and the derived graphical model structure\nlearning algorithms asymptotically recover the 'true' graph. This paper, and\nthe 'pcit' package accompanying it, thus provide powerful, scalable,\ngeneralizable, and easy-to-use methods for multivariate and conditional\nindependence testing, as well as for graphical model structure learning. \n\n"}
{"id": "1711.06940", "contents": "Title: Robust Synthetic Control Abstract: We present a robust generalization of the synthetic control method for\ncomparative case studies. Like the classical method, we present an algorithm to\nestimate the unobservable counterfactual of a treatment unit. A distinguishing\nfeature of our algorithm is that of de-noising the data matrix via singular\nvalue thresholding, which renders our approach robust in multiple facets: it\nautomatically identifies a good subset of donors, overcomes the challenges of\nmissing data, and continues to work well in settings where covariate\ninformation may not be provided. To begin, we establish the condition under\nwhich the fundamental assumption in synthetic control-like approaches holds,\ni.e. when the linear relationship between the treatment unit and the donor pool\nprevails in both the pre- and post-intervention periods. We provide the first\nfinite sample analysis for a broader class of models, the Latent Variable\nModel, in contrast to Factor Models previously considered in the literature.\nFurther, we show that our de-noising procedure accurately imputes missing\nentries, producing a consistent estimator of the underlying signal matrix\nprovided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the\nfraction of observed data and $T$ is the time interval of interest. Under the\nsame setting, we prove that the mean-squared-error (MSE) in our prediction\nestimation scales as $O(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the\nnoise variance. Using a data aggregation method, we show that the MSE can be\nmade as small as $O(T^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to\na consistent estimator. We also introduce a Bayesian framework to quantify the\nmodel uncertainty through posterior probabilities. Our experiments, using both\nreal-world and synthetic datasets, demonstrate that our robust generalization\nyields an improvement over the classical synthetic control method. \n\n"}
{"id": "1711.06999", "contents": "Title: Conditionally conjugate mean-field variational Bayes for logistic models Abstract: Variational Bayes (VB) is a common strategy for approximate Bayesian\ninference, but simple methods are only available for specific classes of models\nincluding, in particular, representations having conditionally conjugate\nconstructions within an exponential family. Models with logit components are an\napparently notable exception to this class, due to the absence of conjugacy\nbetween the logistic likelihood and the Gaussian priors for the coefficients in\nthe linear predictor. To facilitate approximate inference within this widely\nused class of models, Jaakkola and Jordan (2000) proposed a simple variational\napproach which relies on a family of tangent quadratic lower bounds of logistic\nlog-likelihoods, thus restoring conjugacy between these approximate bounds and\nthe Gaussian priors. This strategy is still implemented successfully, but less\nattempts have been made to formally understand the reasons underlying its\nexcellent performance. To cover this key gap, we provide a formal connection\nbetween the above bound and a recent P\\'olya-gamma data augmentation for\nlogistic regression. Such a result places the computational methods associated\nwith the aforementioned bounds within the framework of variational inference\nfor conditionally conjugate exponential family models, thereby allowing recent\nadvances for this class to be inherited also by the methods relying on Jaakkola\nand Jordan (2000). \n\n"}
{"id": "1711.07629", "contents": "Title: On statistical approaches to generate Level 3 products from satellite\n  remote sensing retrievals Abstract: Satellite remote sensing of trace gases such as carbon dioxide (CO$_2$) has\nincreased our ability to observe and understand Earth's climate. However, these\nremote sensing data, specifically~Level 2 retrievals, tend to be irregular in\nspace and time, and hence, spatio-temporal prediction is required to infer\nvalues at any location and time point. Such inferences are not only required to\nanswer important questions about our climate, but they are also needed for\nvalidating the satellite instrument, since Level 2 retrievals are generally not\nco-located with ground-based remote sensing instruments. Here, we discuss\nstatistical approaches to construct Level 3 products from Level 2 retrievals,\nplacing particular emphasis on the strengths and potential pitfalls when using\nstatistical prediction in this context. Following this discussion, we use a\nspatio-temporal statistical modelling framework known as fixed rank kriging\n(FRK) to obtain global predictions and prediction standard errors of\ncolumn-averaged carbon dioxide based on Version 7r and Version 8r retrievals\nfrom the Orbiting Carbon Observatory-2 (OCO-2) satellite. The FRK predictions\nallow us to validate statistically the Level 2 retrievals globally even though\nthe data are at locations and at time points that do not coincide with\nvalidation data. Importantly, the validation takes into account the prediction\nuncertainty, which is dependent both on the temporally-varying density of\nobservations around the ground-based measurement sites and on the\nspatio-temporal high-frequency components of the trace gas field that are not\nexplicitly modelled. Here, for validation of remotely-sensed CO$_2$ data, we\nuse observations from the Total Carbon Column Observing Network. We demonstrate\nthat the resulting FRK product based on Version 8r compares better with TCCON\ndata than that based on Version 7r. \n\n"}
{"id": "1711.07801", "contents": "Title: Why \"Redefining Statistical Significance\" Will Not Improve\n  Reproducibility and Could Make the Replication Crisis Worse Abstract: A recent proposal to \"redefine statistical significance\" (Benjamin, et al.\nNature Human Behaviour, 2017) claims that false positive rates \"would\nimmediately improve\" by factors greater than two and replication rates would\ndouble simply by changing the conventional cutoff for 'statistical\nsignificance' from P<0.05 to P<0.005. I analyze the veracity of these claims,\nfocusing especially on how Benjamin, et al neglect the effects of P-hacking in\nassessing the impact of their proposal. My analysis shows that once P-hacking\nis accounted for the perceived benefits of the lower threshold all but\ndisappear, prompting two main conclusions: (i) The claimed improvements to\nfalse positive rate and replication rate in Benjamin, et al (2017) are\nexaggerated and misleading. (ii) There are plausible scenarios under which the\nlower cutoff will make the replication crisis worse. \n\n"}
{"id": "1711.07812", "contents": "Title: Jaccard analysis and LASSO-based feature selection for location\n  fingerprinting with limited computational complexity Abstract: We propose an approach to reduce both computational complexity and data\nstorage requirements for the online positioning stage of a fingerprinting-based\nindoor positioning system (FIPS) by introducing segmentation of the region of\ninterest (RoI) into sub-regions, sub-region selection using a modified Jaccard\nindex, and feature selection based on randomized least absolute shrinkage and\nselection operator (LASSO). We implement these steps into a Bayesian framework\nof position estimation using the maximum a posteriori (MAP) principle. An\nadditional benefit of these steps is that the time for estimating the position,\nand the required data storage are virtually independent of the size of the RoI\nand of the total number of available features within the RoI. Thus the proposed\nsteps facilitate application of FIPS to large areas. Results of an experimental\nanalysis using real data collected in an office building using a Nexus 6P smart\nphone as user device and a total station for providing position ground truth\ncorroborate the expected performance of the proposed approach. The positioning\naccuracy obtained by only processing 10 automatically identified features\ninstead of all available ones and limiting position estimation to 10\nautomatically identified sub-regions instead of the entire RoI is equivalent to\nprocessing all available data. In the chosen example, 50% of the errors are\nless than 1.8 m and 90% are less than 5 m. However, the computation time using\nthe automatically identified subset of data is only about 1% of that required\nfor processing the entire data set. \n\n"}
{"id": "1711.08970", "contents": "Title: Sparse and Low-Rank Matrix Decomposition for Automatic Target Detection\n  in Hyperspectral Imagery Abstract: Given a target prior information, our goal is to propose a method for\nautomatically separating targets of interests from the background in\nhyperspectral imagery. More precisely, we regard the given hyperspectral image\n(HSI) as being made up of the sum of low-rank background HSI and a sparse\ntarget HSI that contains the targets based on a pre-learned target dictionary\nconstructed from some online spectral libraries. Based on the proposed method,\ntwo strategies are briefly outlined and evaluated to realize the target\ndetection on both synthetic and real experiments. \n\n"}
{"id": "1711.09365", "contents": "Title: Ensemble-marginalized Kalman filter for linear time-dependent PDEs with\n  noisy boundary conditions: Application to heat transfer in building walls Abstract: In this work, we present the ensemble-marginalized Kalman filter (EnMKF), a\nsequential algorithm analogous to our previously proposed approach [1,2], for\nestimating the state and parameters of linear parabolic partial differential\nequations in initial-boundary value problems when the boundary data are noisy.\nWe apply EnMKF to infer the thermal properties of building walls and to\nestimate the corresponding heat flux from real and synthetic data. Compared\nwith a modified Ensemble Kalman Filter (EnKF) that is not marginalized, EnMKF\nreduces the bias error, avoids the collapse of the ensemble without needing to\nadd inflation, and converges to the mean field posterior using $50\\%$ or less\nof the ensemble size required by EnKF. According to our results, the\nmarginalization technique in EnMKF is key to performance improvement with\nsmaller ensembles at any fixed time. \n\n"}
{"id": "1711.10199", "contents": "Title: A two-stage Fisher exact test for multi-arm studies with binary outcome\n  variables Abstract: In small sample studies with binary outcome data, use of a normal\napproximation for hypothesis testing can lead to substantial inflation of the\ntype-I error-rate. Consequently, exact statistical methods are necessitated,\nand accordingly, much research has been conducted to facilitate this. Recently,\nthis has included methodology for the design of two-stage multi-arm studies\nutilising exact binomial tests. These designs were demonstrated to carry\nsubstantial efficiency advantages over a fixed sample design, but generally\nsuffered from strong conservatism. An alternative classical means of small\nsample inference with dichotomous data is Fisher's exact test. However, this\nmethod is limited to single-stage designs when there are multiple arms.\nTherefore, here, we propose a two-stage version of Fisher's exact test, with\nthe potential to stop early to accept or reject null hypotheses, which is\napplicable to multi-arm studies. In particular, we provide precise formulae\ndescribing the requirements for achieving weak or strong control of the\nfamilywise error-rate with this design. Following this, we describe how the\ndesign parameters may be optimised to confer desirable operating\ncharacteristics. For a motivating example based on a phase II clinical trial,\nwe demonstrate that on average our approach is less conservative than\ncorresponding optimal designs based on exact binomial tests. \n\n"}
{"id": "1711.10421", "contents": "Title: A Review of Dynamic Network Models with Latent Variables Abstract: We present a selective review of statistical modeling of dynamic networks. We\nfocus on models with latent variables, specifically, the latent space models\nand the latent class models (or stochastic blockmodels), which investigate both\nthe observed features and the unobserved structure of networks. We begin with\nan overview of the static models, and then we introduce the dynamic extensions.\nFor each dynamic model, we also discuss its applications that have been studied\nin the literature, with the data source listed in Appendix. Based on the\nreview, we summarize a list of open problems and challenges in dynamic network\nmodeling with latent variables. \n\n"}
{"id": "1711.10427", "contents": "Title: Latent Association Mining in Binary Data Abstract: We consider the problem of identifying stable sets of mutually associated\nfeatures in moderate or high-dimensional binary data. In this context we\ndevelop and investigate a method called Latent Association Mining for Binary\nData (LAMB). The LAMB method is based on a simple threshold model in which the\nobserved binary values represent a random thresholding of a latent continuous\nvector that may have a complex association structure. We consider a measure of\nlatent association that quantifies association in the latent continuous vector\nwithout bias due to the random thresholding. The LAMB method uses an iterative\ntesting based search procedure to identify stable sets of mutually associated\nfeatures. We compare the LAMB method with several competing methods on\nartificial binary-valued datasets and two real count-valued datasets. The LAMB\nmethod detects meaningful associations in these datasets. In the case of the\ncount-valued datasets, associations detected by the LAMB method are based only\non information about whether the counts are zero or non-zero, and is\ncompetitive with methods that have access to the full count data. \n\n"}
{"id": "1711.11190", "contents": "Title: A Multivariate Poisson-Log Normal Mixture Model for Clustering\n  Transcriptome Sequencing Data Abstract: High-dimensional data of discrete and skewed nature is commonly encountered\nin high-throughput sequencing studies. Analyzing the network itself or the\ninterplay between genes in this type of data continues to present many\nchallenges. As data visualization techniques become cumbersome for higher\ndimensions and unconvincing when there is no clear separation between\nhomogeneous subgroups within the data, cluster analysis provides an intuitive\nalternative. The aim of applying mixture model-based clustering in this context\nis to discover groups of co-expressed genes, which can shed light on biological\nfunctions and pathways of gene products. A mixture of multivariate Poisson-Log\nNormal (MPLN) model is proposed for clustering of high-throughput transcriptome\nsequencing data. The MPLN model is able to fit a wide range of correlation and\noverdispersion situations, and is ideal for modeling multivariate count data\nfrom RNA sequencing studies. Parameter estimation is carried out via a Markov\nchain Monte Carlo expectation-maximization algorithm (MCMC-EM), and information\ncriteria are used for model selection. \n\n"}
{"id": "1711.11501", "contents": "Title: Fast Nonseparable Gaussian Stochastic Process with Application to\n  Methylation Level Interpolation Abstract: Gaussian stochastic process (GaSP) has been widely used as a prior over\nfunctions due to its flexibility and tractability in modeling. However, the\ncomputational cost in evaluating the likelihood is $O(n^3)$, where $n$ is the\nnumber of observed points in the process, as it requires to invert the\ncovariance matrix. This bottleneck prevents GaSP being widely used in\nlarge-scale data. We propose a general class of nonseparable GaSP models for\nmultiple functional observations with a fast and exact algorithm, in which the\ncomputation is linear ($O(n)$) and exact, requiring no approximation to compute\nthe likelihood. We show that the commonly used linear regression and separable\nmodels are special cases of the proposed nonseparable GaSP model. Through the\nstudy of an epigenetic application, the proposed nonseparable GaSP model can\naccurately predict the genome-wide DNA methylation levels and compares\nfavorably to alternative methods, such as linear regression, random forest and\nlocalized Kriging method. The algorithm for fast computation is implemented in\nthe ${\\tt FastGaSP}$ R package on CRAN. \n\n"}
{"id": "1712.00063", "contents": "Title: Probabilities of causation of climate changes Abstract: Multiple changes in Earth's climate system have been observed over the past\ndecades. Determining how likely each of these changes are to have been caused\nby human influence, is important for decision making on mitigation and\nadaptation policy. Here we describe an approach for deriving the probability\nthat anthropogenic forcings have caused a given observed change. The proposed\napproach is anchored into causal counterfactual theory (Pearl 2009) which has\nbeen introduced recently, and was in fact partly used already, in the context\nof extreme weather event attribution (EA). We argue that these concepts are\nalso relevant, and can be straightforwardly extended to, the context of\ndetection and attribution of long term trends associated to climate change\n(D&A). For this purpose, and in agreement with the principle of\n\"fingerprinting\" applied in the conventional D&A framework, a trajectory of\nchange is converted into an event occurrence defined by maximizing the causal\nevidence associated to the forcing under scrutiny. Other key assumptions used\nin the conventional D&A framework, in particular those related to numerical\nmodels error, can also be adapted conveniently to this approach. Our proposal\nthus allows to bridge the conventional framework with the standard causal\ntheory, in an attempt to improve the quantification of causal probabilities. An\nillustration suggests that our approach is prone to yield a significantly\nhigher estimate of the probability that anthropogenic forcings have caused the\nobserved temperature change, thus supporting more assertive causal claims. \n\n"}
{"id": "1712.00131", "contents": "Title: Benford's law first significant digit and distribution distances for\n  testing the reliability of financial reports in developing countries Abstract: We discuss a common suspicion about reported financial data, in 10 industrial\nsectors of the 6 so called \"main developing countries\" over the time interval\n[2000-2014]. These data are examined through Benford's law first significant\ndigit and through distribution distances tests. It is shown that several\nvisually anomalous data have to be a priori removed. Thereafter, the\ndistributions much better follow the first digit significant law, indicating\nthe usefulness of a Benford's law test from the research starting line. The\nsame holds true for distance tests. A few outliers are pointed out. \n\n"}
{"id": "1712.01263", "contents": "Title: Data-Driven Spatio-Temporal Analysis of Curbside Parking Demand: A\n  Case-Study in Seattle Abstract: Due to rapid expansion of urban areas in recent years, management of curbside\nparking has become increasingly important. To mitigate congestion, while\nmeeting a city's diverse needs, performance-based pricing schemes have received\na significant amount of attention. However, several recent studies suggest\nlocation, time-of-day, and awareness of policies are the primary factors that\ndrive parking decisions. In light of this, we provide an extensive data-driven\nstudy of the spatio-temporal characteristics of curbside parking. This work\nadvances the understanding of where and when to set pricing policies, as well\nas where to target information and incentives to drivers looking to park.\nHarnessing data provided by the Seattle Department of Transportation, we\ndevelop a Gaussian mixture model based technique to identify zones with similar\nspatial parking demand as quantified by spatial autocorrelation. In support of\nthis technique, we introduce a metric based on the repeatability of our\nGaussian mixture model to investigate temporal consistency. \n\n"}
{"id": "1712.02595", "contents": "Title: Gaussian Process Regression for In-situ Capacity Estimation of\n  Lithium-ion Batteries Abstract: Accurate on-board capacity estimation is of critical importance in\nlithium-ion battery applications. Battery charging/discharging often occurs\nunder a constant current load, and hence voltage vs. time measurements under\nthis condition may be accessible in practice. This paper presents a data-driven\ndiagnostic technique, Gaussian Process regression for In-situ Capacity\nEstimation (GP-ICE), which estimates battery capacity using voltage\nmeasurements over short periods of galvanostatic operation. Unlike previous\nworks, GP-ICE does not rely on interpreting the voltage-time data as\nIncremental Capacity (IC) or Differential Voltage (DV) curves. This overcomes\nthe need to differentiate the voltage-time data (a process which amplifies\nmeasurement noise), and the requirement that the range of voltage measurements\nencompasses the peaks in the IC/DV curves. GP-ICE is applied to two datasets,\nconsisting of 8 and 20 cells respectively. In each case, within certain voltage\nranges, as little as 10 seconds of galvanostatic operation enables capacity\nestimates with approximately 2-3% RMSE. \n\n"}
{"id": "1712.03310", "contents": "Title: Maximum entropy low-rank matrix recovery Abstract: We propose in this paper a novel, information-theoretic method, called\nMaxEnt, for efficient data acquisition for low-rank matrix recovery. This\nproposed method has important applications to a wide range of problems,\nincluding image processing and text document indexing. Fundamental to our\ndesign approach is the so-called maximum entropy principle, which states that\nthe measurement masks which maximize the entropy of observations, also maximize\nthe information gain on the unknown matrix $\\mathbf{X}$. Coupled with a\nlow-rank stochastic model for $\\mathbf{X}$, such a principle (i) reveals novel\nconnections between information-theoretic sampling and subspace packings, and\n(ii) yields efficient mask construction algorithms for matrix recovery, which\nsignificantly outperforms random measurements. We illustrate the effectiveness\nof MaxEnt in simulation experiments, and demonstrate its usefulness in two\nreal-world applications on image recovery and text document indexing. \n\n"}
{"id": "1712.03553", "contents": "Title: RNN-based counterfactual prediction, with an application to homestead\n  policy and public schooling Abstract: This paper proposes a method for estimating the effect of a policy\nintervention on an outcome over time. We train recurrent neural networks (RNNs)\non the history of control unit outcomes to learn a useful representation for\npredicting future outcomes. The learned representation of control units is then\napplied to the treated units for predicting counterfactual outcomes. RNNs are\nspecifically structured to exploit temporal dependencies in panel data, and are\nable to learn negative and nonlinear interactions between control unit\noutcomes. We apply the method to the problem of estimating the long-run impact\nof U.S. homestead policy on public school spending. \n\n"}
{"id": "1712.03646", "contents": "Title: Dynamic Mixed Frequency Synthesis for Economic Nowcasting Abstract: We develop a novel Bayesian framework for dynamic modeling of mixed frequency\ndata to nowcast quarterly U.S. GDP growth. The introduced framework utilizes\nfoundational Bayesian theory and treats data sampled at different frequencies\nas latent factors that are later synthesized, allowing flexible methodological\nspecifications based on interests and utility. Time-varying inter-dependencies\nbetween the mixed frequency data are learnt and effectively mapped onto easily\ninterpretable parameters. A macroeconomic study of nowcasting quarterly U.S.\nGDP growth using a number of monthly economic variables demonstrates\nimprovements in terms of nowcast performance and interpretability compared to\nthe standard in the literature. The study further shows that incorporating\ninformation during a quarter markedly improves the performance in terms of both\npoint and density nowcasts. \n\n"}
{"id": "1712.04243", "contents": "Title: Approximation of Supremum of Max-Stable Stationary Processes and\n  Pickands Constants Abstract: Let $X(t),t\\in \\mathbb{R}$ be a stochastically continuous stationary\nmax-stable process with Fr\\'{e}chet marginals $\\Phi_\\alpha, \\alpha>0$ and set\n$M_X(T)=\\sup_{t \\in [0,T]} X(t),T>0$. In the light of the seminal articles\n[1,2], it follows that $A_T=M_X(T)/T^{1/\\alpha}$ converges in distribution as\n$T\\to \\infty$ to $\\mathcal{H}_Z^{1/\\alpha} X(1)$, where $\\mathcal{H}_Z$ is the\nPickands constant corresponding to the spectral process $Z$ of $X$. In this\ncontribution we derive explicit formulas for $\\mathcal{H}_Z$ in terms of $Z$\nand show necessary and sufficient conditions for its positivity. From our\nanalysis it follows that $A_T^\\beta,T>0$ is uniformly integrable for any $\\beta\n\\in (0,\\alpha)$. Further, we discuss the dissipative Rosi\\'nski (or mixed\nmoving maxima) representation of $X$. Additionally, for Brown-Resnick $X$ we\nshow the validity of the celebrated Slepian inequality and obtain lower bounds\non the growth of supremum of Gaussian processes with stationary increments by\nexploiting the link between Pickands constants and Wills functional. Moreover,\nwe derive upper bounds for supremum of centered Gaussian processes given in\nterms of Wills functional, and discuss the relation between Pickands and\nPiterbarg constants. \n\n"}
{"id": "1712.06532", "contents": "Title: Dependence and dependence structures: estimation and visualization using\n  the unifying concept of distance multivariance Abstract: Distance multivariance is a multivariate dependence measure, which can detect\ndependencies between an arbitrary number of random vectors each of which can\nhave a distinct dimension. Here we discuss several new aspects, present a\nconcise overview and use it as the basis for several new results and concepts:\nIn particular, we show that distance multivariance unifies (and extends)\ndistance covariance and the Hilbert-Schmidt independence criterion HSIC,\nmoreover also the classical linear dependence measures: covariance, Pearson's\ncorrelation and the RV coefficient appear as limiting cases. Based on distance\nmultivariance several new measures are defined: a multicorrelation which\nsatisfies a natural set of multivariate dependence measure axioms and\n$m$-multivariance which is a dependence measure yielding tests for pairwise\nindependence and independence of higher order. These tests are computationally\nfeasible and under very mild moment conditions they are consistent against all\nalternatives. Moreover, a general visualization scheme for higher order\ndependencies is proposed, including consistent estimators (based on distance\nmultivariance) for the dependence structure.\n  Many illustrative examples are provided. All functions for the use of\ndistance multivariance in applications are published in the R-package\n'multivariance'. \n\n"}
{"id": "1712.06575", "contents": "Title: Combinatorics of chemical reaction systems Abstract: We propose a concise stochastic mechanics framework for chemical reaction\nsystems that allows to formulate evolution equations for three general types of\ndata: the probability generating functions, the exponential moment generating\nfunctions and the factorial moment generating functions. This formulation\nconstitutes an intimate synergy between techniques of statistical physics and\nof combinatorics. We demonstrate how to analytically solve the evolution\nequations for all six elementary types of single-species chemical reactions by\neither combinatorial normal-ordering techniques, or, for the binary reactions,\nby means of Sobolev-Jacobi orthogonal polynomials. The former set of results in\nparticular highlights the relationship between infinitesimal generators of\nstochastic evolution and parametric transformations of probability\ndistributions. \n\n"}
{"id": "1712.07811", "contents": "Title: Multi-dimensional Graph Fourier Transform Abstract: Many signals on Cartesian product graphs appear in the real world, such as\ndigital images, sensor observation time series, and movie ratings on Netflix.\nThese signals are \"multi-dimensional\" and have directional characteristics\nalong each factor graph. However, the existing graph Fourier transform does not\ndistinguish these directions, and assigns 1-D spectra to signals on product\ngraphs. Further, these spectra are often multi-valued at some frequencies. Our\nmain result is a multi-dimensional graph Fourier transform that solves such\nproblems associated with the conventional GFT. Using algebraic properties of\nCartesian products, the proposed transform rearranges 1-D spectra obtained by\nthe conventional GFT into the multi-dimensional frequency domain, of which each\ndimension represents a directional frequency along each factor graph. Thus, the\nmulti-dimensional graph Fourier transform enables directional frequency\nanalysis, in addition to frequency analysis with the conventional GFT.\nMoreover, this rearrangement resolves the multi-valuedness of spectra in some\ncases. The multi-dimensional graph Fourier transform is a foundation of novel\nfilterings and stationarities that utilize dimensional information of graph\nsignals, which are also discussed in this study. The proposed methods are\napplicable to a wide variety of data that can be regarded as signals on\nCartesian product graphs. This study also notes that multivariate graph signals\ncan be regarded as 2-D univariate graph signals. This correspondence provides\nnatural definitions of the multivariate graph Fourier transform and the\nmultivariate stationarity based on their 2-D univariate versions. \n\n"}
{"id": "1712.09222", "contents": "Title: Identification and Estimation of Time-Varying Nonseparable Panel Data\n  Models without Stayers Abstract: This paper explores the identification and estimation of nonseparable panel\ndata models. We show that the structural function is nonparametrically\nidentified when it is strictly increasing in a scalar unobservable variable,\nthe conditional distributions of unobservable variables do not change over\ntime, and the joint support of explanatory variables satisfies some weak\nassumptions. To identify the target parameters, existing studies assume that\nthe structural function does not change over time, and that there are\n\"stayers\", namely individuals with the same regressor values in two time\nperiods. Our approach, by contrast, allows the structural function to depend on\nthe time period in an arbitrary manner and does not require the existence of\nstayers. In estimation part of the paper, we consider parametric models and\ndevelop an estimator that implements our identification results. We then show\nthe consistency and asymptotic normality of our estimator. Monte Carlo studies\nindicate that our estimator performs well in finite samples. Finally, we extend\nour identification results to models with discrete outcomes, and show that the\nstructural function is partially identified. \n\n"}
{"id": "1712.09816", "contents": "Title: Extremal Behavior of Aggregated Data with an Application to Downscaling Abstract: The distribution of spatially aggregated data from a stochastic process $X$\nmay exhibit a different tail behavior than its marginal distributions. For a\nlarge class of aggregating functionals $\\ell$ we introduce the $\\ell$-extremal\ncoefficient that quantifies this difference as a function of the extremal\nspatial dependence in $X$. We also obtain the joint extremal dependence for\nmultiple aggregation functionals applied to the same process. Explicit formulas\nfor the $\\ell$-extremal coefficients and multivariate dependence structures are\nderived in important special cases. The results provide a theoretical link\nbetween the extremal distribution of the aggregated data and the corresponding\nunderlying process, which we exploit to develop a method for statistical\ndownscaling. We apply our framework to downscale daily temperature maxima in\nthe south of France from a gridded data set and use our model to generate high\nresolution maps of the warmest day during the 2003 heatwave. \n\n"}
{"id": "1712.10131", "contents": "Title: Sparse Polynomial Chaos Expansions via Compressed Sensing and D-optimal\n  Design Abstract: In the field of uncertainty quantification, sparse polynomial chaos (PC)\nexpansions are commonly used by researchers for a variety of purposes, such as\nsurrogate modeling. Ideas from compressed sensing may be employed to exploit\nthis sparsity in order to reduce computational costs. A class of greedy\ncompressed sensing algorithms use least squares minimization to approximate PC\ncoefficients. This least squares problem lends itself to the theory of optimal\ndesign of experiments (ODE). Our work focuses on selecting an experimental\ndesign that improves the accuracy of sparse PC approximations for a fixed\ncomputational budget. We propose DSP, a novel sequential design, greedy\nalgorithm for sparse PC approximation. The algorithm sequentially augments an\nexperimental design according to a set of the basis polynomials deemed\nimportant by the magnitude of their coefficients, at each iteration. Our\nalgorithm incorporates topics from ODE to estimate the PC coefficients. A\nvariety of numerical simulations are performed on three physical models and\nmanufactured sparse PC expansions to provide a comparative study between our\nproposed algorithm and other non-adaptive methods. Further, we examine the\nimportance of sampling by comparing different strategies in terms of their\nability to generate a candidate pool from which an optimal experimental design\nis chosen. It is demonstrated that the most accurate PC coefficient\napproximations, with the least variability, are produced with our\ndesign-adaptive greedy algorithm and the use of a studied importance sampling\nstrategy. We provide theoretical and numerical results which show that using an\noptimal sampling strategy for the candidate pool is key, both in terms of\naccuracy in the approximation, but also in terms of constructing an optimal\ndesign. \n\n"}
{"id": "1801.00175", "contents": "Title: New robust confidence intervals for the mean under dependence Abstract: The goal of this paper is to indicate a new method for constructing normal\nconfidence intervals for the mean, when the data is coming from stochastic\nstructures with possibly long memory, especially when the dependence structure\nis not known or even the existence of the density function. More precisely we\nintroduce a random smoothing suggested by the kernel estimators for the\nregression function. Applications are presented to linear processes and\nreversible Markov chains with long memory. \n\n"}
{"id": "1801.00644", "contents": "Title: Matching with Text Data: An Experimental Evaluation of Methods for\n  Matching Documents and of Measuring Match Quality Abstract: Matching for causal inference is a well-studied problem, but standard methods\nfail when the units to match are text documents: the high-dimensional and rich\nnature of the data renders exact matching infeasible, causes propensity scores\nto produce incomparable matches, and makes assessing match quality difficult.\nIn this paper, we characterize a framework for matching text documents that\ndecomposes existing methods into: (1) the choice of text representation, and\n(2) the choice of distance metric. We investigate how different choices within\nthis framework affect both the quantity and quality of matches identified\nthrough a systematic multifactor evaluation experiment using human subjects.\nAltogether we evaluate over 100 unique text matching methods along with 5\ncomparison methods taken from the literature. Our experimental results identify\nmethods that generate matches with higher subjective match quality than current\nstate-of-the-art techniques. We enhance the precision of these results by\ndeveloping a predictive model to estimate the match quality of pairs of text\ndocuments as a function of our various distance scores. This model, which we\nfind successfully mimics human judgment, also allows for approximate and\nunsupervised evaluation of new procedures. We then employ the identified best\nmethod to illustrate the utility of text matching in two applications. First,\nwe engage with a substantive debate in the study of media bias by using text\nmatching to control for topic selection when comparing news articles from\nthirteen news sources. We then show how conditioning on text data leads to more\nprecise causal inferences in an observational study examining the effects of a\nmedical intervention. \n\n"}
{"id": "1801.00865", "contents": "Title: Accounting for unobserved covariates with varying degrees of\n  estimability in high dimensional biological data Abstract: An important phenomenon in high dimensional biological data is the presence\nof unobserved covariates that can have a significant impact on the measured\nresponse. When these factors are also correlated with the covariate(s) of\ninterest (i.e. disease status), ignoring them can lead to increased type I\nerror and spurious false discovery rate estimates. We show that depending on\nthe strength of this correlation and the informativeness of the observed data\nfor the latent factors, previously proposed estimators for the effect of the\ncovariate of interest that attempt to account for unobserved covariates are\nasymptotically biased, which corroborates previous practitioners' observations\nthat these estimators tend to produce inflated test statistics. We then provide\nan estimator that corrects the bias and prove it has the same asymptotic\ndistribution as the ordinary least squares estimator when every covariate is\nobserved. Lastly, we use previously published DNA methylation data to show our\nmethod can more accurately estimate the direct effect of asthma on methylation\nthan previously published methods, which underestimate the correlation between\nasthma and latent cell type heterogeneity. Our re-analysis shows that the\nmajority of the variability in methylation due to asthma in those data is\nactually mediated through cell composition. \n\n"}
{"id": "1801.01220", "contents": "Title: Generalized Similarity U: A Non-parametric Test of Association Based on\n  Similarity Abstract: Second generation sequencing technologies are being increasingly used for\ngenetic association studies, where the main research interest is to identify\nsets of genetic variants that contribute to various phenotype. The phenotype\ncan be univariate disease status, multivariate responses and even\nhigh-dimensional outcomes. Considering the genotype and phenotype as two\ncomplex objects, this also poses a general statistical problem of testing\nassociation between complex objects. We here proposed a similarity-based test,\ngeneralized similarity U (GSU), that can test the association between complex\nobjects. We first studied the theoretical properties of the test in a general\nsetting and then focused on the application of the test to sequencing\nassociation studies. Based on theoretical analysis, we proposed to use\nLaplacian kernel based similarity for GSU to boost power and enhance\nrobustness. Through simulation, we found that GSU did have advantages over\nexisting methods in terms of power and robustness. We further performed a whole\ngenome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative\n(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with\nimaging phenotype. We developed a C++ package for analysis of whole genome\nsequencing data using GSU. The source codes can be downloaded at\nhttps://github.com/changshuaiwei/gsu. \n\n"}
{"id": "1801.01797", "contents": "Title: Monte Carlo integration with a growing number of control variates Abstract: It is well known that Monte Carlo integration with variance reduction by\nmeans of control variates can be implemented by the ordinary least squares\nestimator for the intercept in a multiple linear regression model. A central\nlimit theorem is established for the integration error if the number of control\nvariates tends to infinity. The integration error is scaled by the standard\ndeviation of the error term in the regression model. If the linear span of the\ncontrol variates is dense in a function space that contains the integrand, the\nintegration error tends to zero at a rate which is faster than the square root\nof the number of Monte Carlo replicates. Depending on the situation, increasing\nthe number of control variates may or may not be computationally more efficient\nthan increasing the Monte Carlo sample size. \n\n"}
{"id": "1801.01950", "contents": "Title: High Dimensional Elliptical Sliced Inverse Regression in non-Gaussian\n  Distributions Abstract: Sliced inverse regression (SIR) is the most widely-used sufficient dimension\nreduction method due to its simplicity, generality and computational\nefficiency. However, when the distribution of the covariates deviates from the\nmultivariate normal distribution, the estimation efficiency of SIR is rather\nlow. In this paper, we propose a robust alternative to SIR - called elliptical\nsliced inverse regression (ESIR) for analysing high dimensional, elliptically\ndistributed data. There are wide range of applications of the elliptically\ndistributed data, especially in finance and economics where the distribution of\nthe data is often heavy-tailed. To tackle the heavy-tailed elliptically\ndistributed covariates, we novelly utilize the multivariate Kendall's tau\nmatrix in a framework of so-called generalized eigenvector problem for\nsufficient dimension reduction. Methodologically, we present a practical\nalgorithm for our method. Theoretically, we investigate the asymptotic behavior\nof the ESIR estimator and obtain the corresponding convergence rate under high\ndimensional setting. Quantities of simulation results show that ESIR\nsignificantly improves the estimation efficiency in heavy-tailed scenarios. A\nstock exchange data analysis also demonstrates the effectiveness of our method.\nMoreover, ESIR can be easily extended to most other sufficient dimension\nreduction methods. \n\n"}
{"id": "1801.02512", "contents": "Title: On the one parameter unit-Lindley distribution and its associated\n  regression model for proportion data Abstract: In this paper considering the transformation $X=\\frac{Y}{1+Y}$, where $Y\n\\sim\\text{Lindley}(\\theta)$, we propose the unit-Lindley distribution and\ninvestigate some of its mathematical properties. A important fact associated\nwith this new distribution is that is possible to obtain the analytical\nexpression for bias correction of the maximum likelihood estimator. Moreover,\nit belongs to the exponential family. This distribution allows us to\nincorporate covariates directly in the mean and consequently to quantify the\ninfluence on the average of the response variable. Finally, a practical\napplication is present and it is shown that our model fits much better than the\nBeta regression. \n\n"}
{"id": "1801.05108", "contents": "Title: Factor graph fragmentization of expectation propagation Abstract: Expectation propagation is a general approach to fast approximate inference\nfor graphical models. The existing literature treats models separately when it\ncomes to deriving and coding expectation propagation inference algorithms. This\ncomes at the cost of similar, long-winded algebraic steps being repeated and\nslowing down algorithmic development. We demonstrate how factor graph\nfragmentization can overcome this impediment. This involves adoption of the\nmessage passing on a factor graph approach to expectation propagation and\nidentification of factor graph sub-graphs, which we call fragments, that are\ncommon to wide classes of models. Key fragments and their corresponding\nmessages are catalogued which means that their algebra does not need to be\nrepeated. This allows compartmentalization of coding and efficient software\ndevelopment. \n\n"}
{"id": "1801.05242", "contents": "Title: A Bayesian Conjugate Gradient Method Abstract: A fundamental task in numerical computation is the solution of large linear\nsystems. The conjugate gradient method is an iterative method which offers\nrapid convergence to the solution, particularly when an effective\npreconditioner is employed. However, for more challenging systems a substantial\nerror can be present even after many iterations have been performed. The\nestimates obtained in this case are of little value unless further information\ncan be provided about the numerical error. In this paper we propose a novel\nstatistical model for this numerical error set in a Bayesian framework. Our\napproach is a strict generalisation of the conjugate gradient method, which is\nrecovered as the posterior mean for a particular choice of prior. The estimates\nobtained are analysed with Krylov subspace methods and a contraction result for\nthe posterior is presented. The method is then analysed in a simulation study\nas well as being applied to a challenging problem in medical imaging. \n\n"}
{"id": "1801.05465", "contents": "Title: On a bimodal Birnbaum-Saunders distribution with applications to\n  lifetime data Abstract: The Birnbaum-Saunders distribution is a flexible and useful model which has\nbeen used in several fields. In this paper, a new bimodal version of this\ndistribution based on the alpha-skew-normal distribution is established. We\ndiscuss some of its mathematical and inferential properties. We consider\nlikelihood-based methods to estimate the model parameters. We carry out a Monte\nCarlo simulation study to evaluate the performance of the maximum likelihood\nestimators. For illustrative purposes, three real data sets are analyzed. The\nresults indicated that the proposed model outperformed some existing models in\nthe literature, in special, a recent bimodal extension of the Birnbaum-Saunders\ndistribution. \n\n"}
{"id": "1801.07318", "contents": "Title: Variable Prioritization in Nonlinear Black Box Methods: A Genetic\n  Association Case Study Abstract: The central aim in this paper is to address variable selection questions in\nnonlinear and nonparametric regression. Motivated by statistical genetics,\nwhere nonlinear interactions are of particular interest, we introduce a novel\nand interpretable way to summarize the relative importance of predictor\nvariables. Methodologically, we develop the \"RelATive cEntrality\" (RATE)\nmeasure to prioritize candidate genetic variants that are not just marginally\nimportant, but whose associations also stem from significant covarying\nrelationships with other variants in the data. We illustrate RATE through\nBayesian Gaussian process regression, but the methodological innovations apply\nto other \"black box\" methods. It is known that nonlinear models often exhibit\ngreater predictive accuracy than linear models, particularly for phenotypes\ngenerated by complex genetic architectures. With detailed simulations and two\nreal data association mapping studies, we show that applying RATE enables an\nexplanation for this improved performance. \n\n"}
{"id": "1801.07683", "contents": "Title: Discovering the Signal Subgraph: An Iterative Screening Approach on\n  Graphs Abstract: Supervised learning on graphs is a challenging task due to the high\ndimensionality and inherent structural dependencies in the data, where each\nedge depends on a pair of vertices. Existing conventional methods are designed\nfor standard Euclidean data and do not account for the structural information\ninherent in graphs. In this paper, we propose an iterative vertex screening\nmethod to achieve dimension reduction across multiple graph datasets with\nmatched vertex sets and associated graph attributes. Our method aims to\nidentify a signal subgraph to provide a more concise representation of the full\ngraphs, potentially benefiting subsequent vertex classification tasks. The\nmethod screens the rows and columns of the adjacency matrix concurrently and\nstops when the resulting distance correlation is maximized. We establish the\ntheoretical foundation of our method by proving that it estimates the true\nsignal subgraph with high probability. Additionally, we establish the\nconvergence rate of classification error under the Erdos-Renyi random graph\nmodel and prove that the subsequent classification can be asymptotically\noptimal, outperforming the entire graph under high-dimensional conditions. Our\nmethod is evaluated on various simulated datasets and real-world human and\nmurine graphs derived from functional and structural magnetic resonance images.\nThe results demonstrate its excellent performance in estimating the\nground-truth signal subgraph and achieving superior classification accuracy. \n\n"}
{"id": "1801.07742", "contents": "Title: MCMC methods for inference in a mathematical model of pulmonary\n  circulation Abstract: This study performs parameter inference in a partial differential equations\nsystem of pulmonary circulation. We use a fluid dynamics network model that\ntakes selected parameter values and mimics the behaviour of the pulmonary\nhaemodynamics under normal physiological and pathological conditions. This is\nof medical interest as it enables tracking the progression of pulmonary\nhypertension. We show how we make the fluids model tractable by reducing the\nparameter dimension from a 55D to a 5D problem. The Delayed Rejection Adaptive\nMetropolis (DRAM) algorithm, coupled with constraint nonlinear optimization is\nsuccessfully used to learn the parameter values and quantify the uncertainty in\nthe parameter estimates. To accommodate for different magnitudes of the\nparameter values, we introduce an improved parameter scaling technique in the\nDRAM algorithm. Formal convergence diagnostics are employed to check for\nconvergence of the Markov chains. Additionally, we perform model selection\nusing different information criteria, including Watanabe Akaike Information\nCriteria. \n\n"}
{"id": "1801.07905", "contents": "Title: Discrete Weibull generalised additive model: an application to count\n  fertility data Abstract: Fertility plans, measured by the number of planned children, have been found\nto be affected by education and family background via complex tail\ndependencies. This challenge was previously met with the use of non-parametric\njittering approaches. This paper shows how a novel generalized additive model\nbased on a discrete Weibull distribution provides partial effects of the\ncovariates on fertility plans which are comparable to jittering, without the\ninherent drawback of crossing conditional quantiles. The model has some\nadditional desirable features: both over- and under-dispersed data can be\nmodelled by this distribution, the conditional quantiles have a simple analytic\nform and the likelihood is the same of that of a continuous Weibull\ndistribution with interval-censored data. The latter means that efficient\nimplementations are already available, in the R package gamlss, for a range of\nmodels and inferential procedures, and at a fraction of the time compared to\nthe jittering and COM-Poisson approaches, showing potential for the wide\napplicability of this approach to the modelling of count data. \n\n"}
{"id": "1801.08686", "contents": "Title: Selection-adjusted inference: an application to confidence intervals for\n  cis-eQTL effect sizes Abstract: The goal of eQTL studies is to identify the genetic variants that influence\nthe expression levels of the genes in an organism. High throughput technology\nhas made such studies possible: in a given tissue sample, it enables us to\nquantify the expression levels of approximately 20,000 genes and to record the\nalleles present at millions of genetic polymorphisms. While obtaining this data\nis relatively cheap once a specimen is at hand, obtaining human tissue remains\na costly endeavor. Thus, eQTL studies continue to be based on relatively small\nsample sizes, with this limitation particularly serious for tissues of most\nimmediate medical relevance. Given the high dimensional nature of this datasets\nand the large number of hypotheses tested, the scientific community has adopted\nearly on multiplicity adjustment procedures, which primarily control the false\ndiscoveries rate for the identification of genetic variants with influence on\nthe expression levels. In contrast, a problem that has not received much\nattention to date is that of providing estimates of the effect sizes associated\nto these variants, in a way that accounts for the considerable amount of\nselection. We illustrate how the recently developed conditional inference\napproach can be deployed to obtain confidence intervals for the eQTL effect\nsizes with reliable coverage. The procedure we propose is based on a randomized\nhierarchical strategy that both reflects the steps typically adopted in state\nof the art investigations and introduces the use of randomness instead of data\nsplitting to maximize the use of available data. Analysis of the GTEx Liver\ndataset (v6) suggests that naively obtained confidence intervals would likely\nnot cover the true values of effect sizes and that the number of local genetic\npolymorphisms influencing the expression level of genes might be\nunderestimated. \n\n"}
{"id": "1801.09874", "contents": "Title: Change point analysis in non-stationary processes - a mass excess\n  approach Abstract: This paper considers the problem of testing if a sequence of means\n$(\\mu_t)_{t =1,\\ldots ,n }$ of a non-stationary time series $(X_t)_{t =1,\\ldots\n,n }$ is stable in the sense that the difference of the means $\\mu_1$ and\n$\\mu_t$ between the initial time $t=1$ and any other time is smaller than a\ngiven level, that is $ | \\mu_1 - \\mu_t | \\leq c $ for all $t =1,\\ldots ,n $. A\ntest for hypotheses of this type is developed using a biascorrected monotone\nrearranged local linear estimator and asymptotic normality of the corresponding\ntest statistic is established. As the asymptotic variance depends on the\nlocation and order of the critical roots of the equation $| \\mu_1 - \\mu_t | =\nc$ a new bootstrap procedure is proposed to obtain critical values and its\nconsistency is established. As a consequence we are able to quantitatively\ndescribe relevant deviations of a non-stationary sequence from its initial\nvalue. The results are illustrated by means of a simulation study and by\nanalyzing data examples. \n\n"}
{"id": "1801.10243", "contents": "Title: A scalable estimate of the extra-sample prediction error via approximate\n  leave-one-out Abstract: The paper considers the problem of out-of-sample risk estimation under the\nhigh dimensional settings where standard techniques such as $K$-fold cross\nvalidation suffer from large biases. Motivated by the low bias of the\nleave-one-out cross validation (LO) method, we propose a computationally\nefficient closed-form approximate leave-one-out formula (ALO) for a large class\nof regularized estimators. Given the regularized estimate, calculating ALO\nrequires minor computational overhead. With minor assumptions about the data\ngenerating process, we obtain a finite-sample upper bound for $|\\text{LO} -\n\\text{ALO}|$. Our theoretical analysis illustrates that $|\\text{LO} -\n\\text{ALO}| \\rightarrow 0$ with overwhelming probability, when $n,p \\rightarrow\n\\infty$, where the dimension $p$ of the feature vectors may be comparable with\nor even greater than the number of observations, $n$. Despite the\nhigh-dimensionality of the problem, our theoretical results do not require any\nsparsity assumption on the vector of regression coefficients. Our extensive\nnumerical experiments show that $|\\text{LO} - \\text{ALO}|$ decreases as $n,p$\nincrease, revealing the excellent finite sample performance of ALO. We further\nillustrate the usefulness of our proposed out-of-sample risk estimation method\nby an example of real recordings from spatially sensitive neurons (grid cells)\nin the medial entorhinal cortex of a rat. \n\n"}
{"id": "1801.10516", "contents": "Title: Are `Water Smart Landscapes' Contagious? An epidemic approach on\n  networks to study peer effects Abstract: We test the existence of a neighborhood based peer effect around\nparticipation in an incentive based conservation program called `Water Smart\nLandscapes' (WSL) in the city of Las Vegas, Nevada. We use 15 years of\ngeo-coded daily records of WSL program applications and approvals compiled by\nthe Southern Nevada Water Authority and Clark County Tax Assessors rolls for\nhome characteristics. We use this data to test whether a spatially mediated\npeer effect can be observed in WSL participation likelihood at the household\nlevel. We show that epidemic spreading models provide more flexibility in\nmodeling assumptions, and also provide one mechanism for addressing problems\nassociated with correlated unobservables than hazards models which can also be\napplied to address the same questions. We build networks of neighborhood based\npeers for 16 randomly selected neighborhoods in Las Vegas and test for the\nexistence of a peer based influence on WSL participation by using a\nSusceptible-Exposed-Infected-Recovered epidemic spreading model (SEIR), in\nwhich a home can become infected via autoinfection or through contagion from\nits infected neighbors. We show that this type of epidemic model can be\ndirectly recast to an additive-multiplicative hazard model, but not to purely\nmultiplicative one. Using both inference and prediction approaches we find\nevidence of peer effects in several Las Vegas neighborhoods. \n\n"}
{"id": "1802.00796", "contents": "Title: Bayes Calculations from Quantile Implied Likelihood Abstract: In statistical practice, a realistic Bayesian model for a given data set can\nbe defined by a likelihood function that is analytically or computationally\nintractable, due to large data sample size, high parameter dimensionality, or\ncomplex likelihood functional form. This in turn poses challenges to the\ncomputation and inference of the posterior distribution of the model\nparameters. For such a model, a tractable likelihood function is introduced\nwhich approximates the exact likelihood through its quantile function. It is\ndefined by an asymptotic chi-square confidence distribution for a pivotal\nquantity, which is generated by the asymptotic normal distribution of the\nsample quantiles given model parameters. This Quantile Implied Likelihood (QIL)\ngives rise to an approximate posterior distribution which can be estimated by\nusing penalized log-likelihood maximization or any suitable Monte Carlo\nalgorithm. The QIL approach to Bayesian Computation is illustrated through the\nBayesian analysis of simulated and real data sets having sample sizes that\nreach the millions. The analyses involve various models for univariate or\nmultivariate iid or non-iid data, with low or high parameter dimensionality,\nmany of which are defined by intractable likelihoods. The probability models\ninclude the Student's t, g-and-h, and g-and-k distributions; the Bayesian logit\nregression model with many covariates; exponential random graph model, a\ndoubly-intractable model for networks; the multivariate skew normal model, for\nrobust inference of the inverse-covariance matrix when it is large relative to\nthe sample size; and the Wallenius distribution model. \n\n"}
{"id": "1802.00828", "contents": "Title: Comparing multiple networks using the Co-expression Differential Network\n  Analysis (CoDiNA) Abstract: Biomedical sciences are increasingly recognising the relevance of gene\nco-expression-networks for analysing complex-systems, phenotypes or diseases.\nWhen the goal is investigating complex-phenotypes under varying conditions, it\ncomes naturally to employ comparative network methods. While approaches for\ncomparing two networks exist, this is not the case for multiple networks. Here\nwe present a method for the systematic comparison of an unlimited number of\nnetworks: Co-expression Differential Network Analysis (CoDiNA) for detecting\nlinks and nodes that are common, specific or different to the networks.\nApplying CoDiNA to a neurogenesis study identified genes for neuron\ndifferentiation. Experimentally overexpressing one candidate resulted in\nsignificant disturbance in the underlying neurogenesis' gene regulatory\nnetwork. We compared data from adults and children with active tuberculosis to\ntest for signatures of HIV. We also identified common and distinct network\nfeatures for particular cancer types with CoDiNA. These studies show that\nCoDiNA successfully detects genes associated with the diseases. \n\n"}
{"id": "1802.02821", "contents": "Title: Data-adaptive doubly robust instrumental variable methods for treatment\n  effect heterogeneity Abstract: We consider the estimation of the average treatment effect in the treated as\na function of baseline covariates, where there is a valid (conditional)\ninstrument.\n  We describe two doubly robust (DR) estimators: a locally efficient\ng-estimator, and a targeted minimum loss-based estimator (TMLE). These two DR\nestimators can be viewed as generalisations of the two-stage least squares\n(TSLS) method to semi-parametric models that make weaker assumptions. We\nexploit recent theoretical results that extend to the g-estimator the use of\ndata-adaptive fits for the nuisance parameters.\n  A simulation study is used to compare standard TSLS with the two DR\nestimators' finite-sample performance, (1) when fitted using parametric\nnuisance models, and (2) using data-adaptive nuisance fits, obtained from the\nSuper Learner, an ensemble machine learning method.\n  Data-adaptive DR estimators have lower bias and improved coverage, when\ncompared to incorrectly specified parametric DR estimators and TSLS. When the\nparametric model for the treatment effect curve is correctly specified, the\ng-estimator outperforms all others, but when this model is misspecified, TMLE\nperforms best, while TSLS can result in large biases and zero coverage.\n  Finally, we illustrate the methods by reanalysing the COPERS (COping with\npersistent Pain, Effectiveness Research in Self-management) trial to make\ninference about the causal effect of treatment actually received, and the\nextent to which this is modified by depression at baseline. \n\n"}
{"id": "1802.05342", "contents": "Title: Spatial Coherence of Oriented White Matter Microstructure: Applications\n  to White Matter Regions Associated with Genetic Similarity Abstract: We present a method to discover differences between populations with respect\nto the spatial coherence of their oriented white matter microstructure in\narbitrarily shaped white matter regions. This method is applied to diffusion\nMRI scans of a subset of the Human Connectome Project dataset: 57 pairs of\nmonozygotic and 52 pairs of dizygotic twins. After controlling for\nmorphological similarity between twins, we identify 3.7% of all white matter as\nbeing associated with genetic similarity (35.1k voxels, $p < 10^{-4}$, false\ndiscovery rate 1.5%), 75% of which spatially clusters into twenty-two\ncontiguous white matter regions. Furthermore, we show that the orientation\nsimilarity within these regions generalizes to a subset of 47 pairs of non-twin\nsiblings, and show that these siblings are on average as similar as dizygotic\ntwins. The regions are located in deep white matter including the superior\nlongitudinal fasciculus, the optic radiations, the middle cerebellar peduncle,\nthe corticospinal tract, and within the anterior temporal lobe, as well as the\ncerebellum, brain stem, and amygdalae.\n  These results extend previous work using undirected fractional anisotrophy\nfor measuring putative heritable influences in white matter. Our\nmultidirectional extension better accounts for crossing fiber connections\nwithin voxels. This bottom up approach has at its basis a novel measurement of\ncoherence within neighboring voxel dyads between subjects, and avoids some of\nthe fundamental ambiguities encountered with tractographic approaches to white\nmatter analysis that estimate global connectivity. \n\n"}
{"id": "1802.05530", "contents": "Title: Gaussian process modeling of heterogeneity and discontinuities using\n  Voronoi tessellations Abstract: Many methods for modelling spatial processes assume global smoothness\nproperties; such assumptions are often violated in practice. We introduce a\nmethod for modelling spatial processes that display heterogeneity or contain\ndiscontinuities. The problem of non-stationarity is dealt with by using a\ncombination of Voronoi tessellation to partition the input space, and a\nseparate Gaussian process to model the data on each region of the partitioned\nspace. Our method is highly flexible because we allow the Voronoi cells to form\nrelationships with each other, which can enable non-convex and disconnected\nregions to be considered. In such problems, identifying the borders between\nregions is often of great importance and we propose an adaptive sampling method\nto gain extra information along such borders. The method is illustrated with\nsimulation studies and application to real data. \n\n"}
{"id": "1802.05570", "contents": "Title: Optimal Transport: Fast Probabilistic Approximation with Exact Solvers Abstract: We propose a simple subsampling scheme for fast randomized approximate\ncomputation of optimal transport distances. This scheme operates on a random\nsubset of the full data and can use any exact algorithm as a black-box\nback-end, including state-of-the-art solvers and entropically penalized\nversions. It is based on averaging the exact distances between empirical\nmeasures generated from independent samples from the original measures and can\neasily be tuned towards higher accuracy or shorter computation times. To this\nend, we give non-asymptotic deviation bounds for its accuracy in the case of\ndiscrete optimal transport problems. In particular, we show that in many\nimportant instances, including images (2D-histograms), the approximation error\nis independent of the size of the full problem. We present numerical\nexperiments that demonstrate that a very good approximation in typical\napplications can be obtained in a computation time that is several orders of\nmagnitude smaller than what is required for exact computation of the full\nproblem. \n\n"}
{"id": "1802.06048", "contents": "Title: High-dimensional covariance matrix estimation using a low-rank and\n  diagonal decomposition Abstract: We study high-dimensional covariance/precision matrix estimation under the\nassumption that the covariance/precision matrix can be decomposed into a\nlow-rank component L and a diagonal component D. The rank of L can either be\nchosen to be small or controlled by a penalty function. Under moderate\nconditions on the population covariance/precision matrix itself and on the\npenalty function, we prove some consistency results for our estimators. A\nblockwise coordinate descent algorithm, which iteratively updates L and D, is\nthen proposed to obtain the estimator in practice. Finally, various numerical\nexperiments are presented: using simulated data, we show that our estimator\nperforms quite well in terms of the Kullback-Leibler loss; using stock return\ndata, we show that our method can be applied to obtain enhanced solutions to\nthe Markowitz portfolio selection problem. \n\n"}
{"id": "1802.06156", "contents": "Title: A Parsimonious Personalized Dose Finding Model via Dimension Reduction Abstract: Learning an individualized dose rule in personalized medicine is a\nchallenging statistical problem. Existing methods often suffer from the curse\nof dimensionality, especially when the decision function is estimated\nnonparametrically. To tackle this problem, we propose a dimension reduction\nframework that effectively reduces the estimation to a lower-dimensional\nsubspace of the covariates. We exploit that the individualized dose rule can be\ndefined in a subspace spanned by a few linear combinations of the covariates,\nleading to a more parsimonious model. The proposed framework does not require\nthe inverse probability of the propensity score under observational studies due\nto a direct maximization of the value function. This distinguishes us from the\noutcome weighted learning framework, which also solves decision rules directly.\nUnder the same framework, we further propose a pseudo-direct learning approach\nthat focuses more on estimating the dimensionality-reduced subspace of the\ntreatment outcome. Parameters in both approaches can be estimated efficiently\nusing an orthogonality constrained optimization algorithm on the Stiefel\nmanifold. Under mild regularity assumptions, the results on the asymptotic\nnormality of the proposed estimators are established, respectively. We also\nderive the consistency and convergence rate for the value function under the\nestimated optimal dose rule. We evaluate the performance of the proposed\napproaches through extensive simulation studies and a warfarin pharmacogenetic\ndataset. \n\n"}
{"id": "1802.06310", "contents": "Title: Characterizing and Learning Equivalence Classes of Causal DAGs under\n  Interventions Abstract: We consider the problem of learning causal DAGs in the setting where both\nobservational and interventional data is available. This setting is common in\nbiology, where gene regulatory networks can be intervened on using chemical\nreagents or gene deletions. Hauser and B\\\"uhlmann (2012) previously\ncharacterized the identifiability of causal DAGs under perfect interventions,\nwhich eliminate dependencies between targeted variables and their direct\ncauses. In this paper, we extend these identifiability results to general\ninterventions, which may modify the dependencies between targeted variables and\ntheir causes without eliminating them. We define and characterize the\ninterventional Markov equivalence class that can be identified from general\n(not necessarily perfect) intervention experiments. We also propose the first\nprovably consistent algorithm for learning DAGs in this setting and evaluate\nour algorithm on simulated and biological datasets. \n\n"}
{"id": "1802.06931", "contents": "Title: Empirical Bayes Matrix Factorization Abstract: Matrix factorization methods - including Factor analysis (FA), and Principal\nComponents Analysis (PCA) - are widely used for inferring and summarizing\nstructure in multivariate data. Many matrix factorization methods exist,\ncorresponding to different assumptions on the elements of the underlying matrix\nfactors. For example, many recent methods use a penalty or prior distribution\nto achieve sparse representations (\"Sparse FA/PCA\"). Here we introduce a\ngeneral Empirical Bayes approach to matrix factorization (EBMF), whose key\nfeature is that it uses the observed data to estimate prior distributions on\nmatrix elements. We derive a correspondingly-general variational fitting\nalgorithm, which reduces fitting EBMF to solving a simpler problem - the\nso-called \"normal means\" problem. We implement this general algorithm, but\nfocus particular attention on the use of sparsity-inducing priors that are\nuni-modal at 0. This yields a sparse EBMF approach - essentially a version of\nsparse FA/PCA - that automatically adapts the amount of sparsity to the data.\nWe demonstrate the benefits of our approach through both numerical comparisons\nwith competing methods and through analysis of data from the GTEx (Genotype\nTissue Expression) project on genetic associations across 44 human tissues. In\nnumerical comparisons EBMF often provides more accurate inferences than other\nmethods. In the GTEx data, EBMF identifies interpretable structure that\nconcords with known relationships among human tissues. Software implementing\nour approach is available at https://github.com/stephenslab/flashr \n\n"}
{"id": "1802.08178", "contents": "Title: Correlation-Adjusted Regression Survival Scores for High-Dimensional\n  Variable Selection Abstract: Background: The development of classification methods for personalized\nmedicine is highly dependent on the identification of predictive genetic\nmarkers. In survival analysis it is often necessary to discriminate between\ninfluential and non-influential markers. Usually, the first step is to perform\na univariate screening step that ranks the markers according to their\nassociations with the outcome. It is common to perform screening using Cox\nscores, which quantify the associations between survival and each of the\nmarkers individually. Since Cox scores do not account for dependencies between\nthe markers, their use is suboptimal in the presence highly correlated markers.\nMethods: As an alternative to the Cox score, we propose the\ncorrelation-adjusted regression survival (CARS) score for right-censored\nsurvival outcomes. By removing the correlations between the markers, the CARS\nscore quantifies the associations between the outcome and the set of\n\"de-correlated\" marker values. Estimation of the scores is based on inverse\nprobability weighting, which is applied to log-transformed event times. For\nhigh-dimensional data, estimation is based on shrinkage techniques. Results:\nThe consistency of the CARS score is proven under mild regularity conditions.\nIn simulations, survival models based on CARS score rankings achieved higher\nareas under the precision-recall curve than competing methods. Two example\napplications on prostate and breast cancer confirmed these results. CARS scores\nare implemented in the R package carSurv. Conclusions: In research applications\ninvolving high-dimensional genetic data, the use of CARS scores for marker\nselection is a favorable alternative to Cox scores even when correlations\nbetween covariates are low. Having a straightforward interpretation and low\ncomputational requirements, CARS scores are an easy-to-use screening tool in\npersonalized medicine research. \n\n"}
{"id": "1802.08613", "contents": "Title: Accelerate iterated filtering Abstract: In simulation-based inferences for partially observed Markov process models\n(POMP), the by-product of the Monte Carlo filtering is an approximation of the\nlog likelihood function. Recently, iterated filtering [14, 13] has originally\nbeen introduced and it has been shown that the gradient of the log likelihood\ncan also be approximated. Consequently, different stochastic optimization\nalgorithm can be applied to estimate the parameters of the underlying models.\nAs accelerated gradient is an efficient approach in the optimization\nliterature, we show that we can accelerate iterated filtering in the same\nmanner and inherit that high convergence rate while relaxing the restricted\nconditions of unbiased gradient approximation. We show that this novel\nalgorithm can be applied to both convex and non-convex log likelihood\nfunctions. In addition, this approach has substantially outperformed most of\nother previous approaches in a toy example and in a challenging scientific\nproblem of modeling infectious diseases. \n\n"}
{"id": "1802.08664", "contents": "Title: Modeling goal chances in soccer: a Bayesian inference approach Abstract: We consider the task of determining the number of chances a soccer team\ncreates, along with the composite nature of each chance-the players involved\nand the locations on the pitch of the assist and the chance. We propose an\ninterpretable Bayesian inference approach and implement a Poisson model to\ncapture chance occurrences, from which we infer team abilities. We then use a\nGaussian mixture model to capture the areas on the pitch a player makes an\nassist/takes a chance. This approach allows the visualization of differences\nbetween players in the way they approach attacking play (making assists/taking\nchances). We apply the resulting scheme to the 2016/2017 English Premier\nLeague, capturing team abilities to create chances, before highlighting key\nareas where players have most impact. \n\n"}
{"id": "1802.08717", "contents": "Title: Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art Abstract: Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture. \n\n"}
{"id": "1802.09116", "contents": "Title: Partial Distance Correlation Screening for High Dimensional Time Series Abstract: High dimensional time series datasets are becoming increasingly common in\nvarious fields such as economics, finance, meteorology, and neuroscience. Given\nthis ubiquity of time series data, it is surprising that very few works on\nvariable screening discuss the time series setting, and even fewer works have\ndeveloped methods which utilize the unique features of time series data. This\npaper introduces several model free screening methods based on the partial\ndistance correlation and developed specifically to deal with time dependent\ndata. Methods are developed both for univariate models, such as nonlinear\nautoregressive models with exogenous predictors (NARX), and multivariate models\nsuch as linear or nonlinear VAR models. Sure screening properties are proved\nfor our methods, which depend on the moment conditions, and the strength of\ndependence in the response and covariate processes, amongst other factors.\nDependence is quantified by functional dependence measures (Wu [Proc. Natl.\nAcad. Sci. USA 102 (2005) 14150-14154]) and $\\beta$-mixing coefficients, and\nthe results rely on the use of Nagaev and Rosenthal type inequalities for\ndependent random variables. Finite sample performance of our methods is shown\nthrough extensive simulation studies, and we include an application to\nmacroeconomic forecasting. \n\n"}
{"id": "1802.09117", "contents": "Title: Testability of high-dimensional linear models with non-sparse structures Abstract: Understanding statistical inference under possibly non-sparse\nhigh-dimensional models has gained much interest recently. For a given\ncomponent of the regression coefficient, we show that the difficulty of the\nproblem depends on the sparsity of the corresponding row of the precision\nmatrix of the covariates, not the sparsity of the regression coefficients. We\ndevelop new concepts of uniform and essentially uniform non-testability that\nallow the study of limitations of tests across a broad set of alternatives.\nUniform non-testability identifies a collection of alternatives such that the\npower of any test, against any alternative in the group, is asymptotically at\nmost equal to the nominal size. Implications of the new constructions include\nnew minimax testability results that, in sharp contrast to the current results,\ndo not depend on the sparsity of the regression parameters. We identify new\ntradeoffs between testability and feature correlation. In particular, we show\nthat, in models with weak feature correlations, minimax lower bound can be\nattained by a test whose power has the $\\sqrt{n}$ rate, regardless of the size\nof the model sparsity. \n\n"}
{"id": "1802.09304", "contents": "Title: Marked Self-Exciting Point Process Modelling of Information Diffusion on\n  Twitter Abstract: Information diffusion occurs on microblogging platforms like Twitter as\nretweet cascades. When a tweet is posted, it may be retweeted and henceforth\nfurther retweeted, and the retweeting process continues iteratively and\nindefinitely. A natural measure of the popularity of a tweet is the number of\nretweets it generates. Accurate predictions of tweet popularity can assist\nTwitter to rank contents more effectively and facilitate the assessment of\npotential for marketing and campaigning strategies. In this paper, we propose a\nmodel called the Marked Self-Exciting Process with Time-Dependent Excitation\nFunction, or MaSEPTiDE for short, to model the retweeting dynamics and to\npredict the tweet popularity. Our model does not require expensive feature\nengineering but is capable of leveraging the observed dynamics to accurately\npredict the future evolution of retweet cascades. We apply our proposed\nmethodology on a large amount of Twitter data and report substantial\nimprovement in prediction performance over existing approaches in the\nliterature. \n\n"}
{"id": "1802.09725", "contents": "Title: High-dimensional ABC Abstract: This Chapter, \"High-dimensional ABC\", is to appear in the forthcoming\nHandbook of Approximate Bayesian Computation (2018). It details the main ideas\nand concepts behind extending ABC methods to higher dimensions, with supporting\nexamples and illustrations. \n\n"}
{"id": "1802.10570", "contents": "Title: Statistical shape analysis in a Bayesian framework for shapes in two and\n  three dimensions Abstract: In this paper, we describe a novel shape classification method which is\nembedded in the Bayesian paradigm. We discuss the modelling and the resulting\nshape classification algorithm for two and three dimensional data shapes. We\nconclude by evaluating the efficiency and efficacy of the proposed algorithm on\nthe Kimia shape database for the two dimensional case. \n\n"}
{"id": "1803.00113", "contents": "Title: Approximate Inference for Constructing Astronomical Catalogs from Images Abstract: We present a new, fully generative model for constructing astronomical\ncatalogs from optical telescope image sets. Each pixel intensity is treated as\na random variable with parameters that depend on the latent properties of stars\nand galaxies. These latent properties are themselves modeled as random. We\ncompare two procedures for posterior inference. One procedure is based on\nMarkov chain Monte Carlo (MCMC) while the other is based on variational\ninference (VI). The MCMC procedure excels at quantifying uncertainty, while the\nVI procedure is 1000 times faster. On a supercomputer, the VI procedure\nefficiently uses 665,000 CPU cores to construct an astronomical catalog from 50\nterabytes of images in 14.6 minutes, demonstrating the scaling characteristics\nnecessary to construct catalogs for upcoming astronomical surveys. \n\n"}
{"id": "1803.00698", "contents": "Title: Next Steps for the Colorado Risk-Limiting Audit (CORLA) Program Abstract: Colorado conducted risk-limiting tabulation audits (RLAs) across the state in\n2017, including both ballot-level comparison audits and ballot-polling audits.\nThose audits only covered contests restricted to a single county; methods to\nefficiently audit contests that cross county boundaries and combine ballot\npolling and ballot-level comparisons have not been available.\n  Colorado's current audit software (RLATool) needs to be improved to audit\nthese contests that cross county lines and to audit small contests efficiently.\n  This paper addresses these needs. It presents extremely simple but\ninefficient methods, more efficient methods that combine ballot polling and\nballot-level comparisons using stratified samples, and methods that combine\nballot-level comparison and variable-size batch comparison audits in a way that\ndoes not require stratified sampling.\n  We conclude with some recommendations, and illustrate our recommended method\nusing examples that compare them to existing approaches. Exemplar open-source\ncode and interactive Jupyter notebooks are provided that implement the methods\nand allow further exploration. \n\n"}
{"id": "1803.01328", "contents": "Title: WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling Abstract: To train an inference network jointly with a deep generative topic model,\nmaking it both scalable to big corpora and fast in out-of-sample prediction, we\ndevelop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet\nallocation, which infers posterior samples via a hybrid of stochastic-gradient\nMCMC and autoencoding variational Bayes. The generative network of WHAI has a\nhierarchy of gamma distributions, while the inference network of WHAI is a\nWeibull upward-downward variational autoencoder, which integrates a\ndeterministic-upward deep neural network, and a stochastic-downward deep\ngenerative model based on a hierarchy of Weibull distributions. The Weibull\ndistribution can be used to well approximate a gamma distribution with an\nanalytic Kullback-Leibler divergence, and has a simple reparameterization via\nthe uniform noise, which help efficiently compute the gradients of the evidence\nlower bound with respect to the parameters of the inference network. The\neffectiveness and efficiency of WHAI are illustrated with experiments on big\ncorpora. \n\n"}
{"id": "1803.01984", "contents": "Title: Bayesian Predictive Synthesis with Outcome-Dependent Pools Abstract: This paper reviews background and examples of Bayesian predictive synthesis\n(BPS), and develops details in a subset of BPS mixture models. BPS expands on\nstandard Bayesian model uncertainty analysis for model mixing to provide a\nbroader foundation for calibrating and combining predictive densities from\nmultiple models or other sources. One main focus here is BPS as a framework for\njustifying and understanding generalized \"linear opinion pools,\" where multiple\npredictive densities are combined with flexible mixing weights that depend on\nthe forecast outcome itself, i.e., the setting of outcome-dependent model\nmixing. BPS also defines approaches to incorporating and exploiting\ndependencies across models defining forecasts, and to formally addressing the\nproblem of model set incompleteness within the subjective Bayesian framework.\nIn addition to an overview of general mixture-based BPS, new methodological\ndevelopments for dynamic BPS -- involving calibration and pooling of sets of\npredictive distributions in a univariate time series setting -- are presented.\nThese developments are exemplified in summaries of an analysis in a univariate\nfinancial time series study. \n\n"}
{"id": "1803.01999", "contents": "Title: ABC and Indirect Inference Abstract: This chapter will appear in the forthcoming Handbook of Approximate Bayesian\nComputation (2018).\n  Indirect inference (II) is a classical likelihood-free approach that\npre-dates the main developments of ABC and relies on simulation from a\nparametric model of interest to determine point estimates of the parameters. It\nis not surprising then that some likelihood-free Bayesian approaches have\nharnessed the II literature. This chapter provides an introduction to II and\ndetails the connections between ABC and II. A particular focus is placed on the\nuse of an auxiliary model with a tractable likelihood function, an approach\ncommonly adopted in the II literature, to facilitate likelihood-free Bayesian\ninferences. \n\n"}
{"id": "1803.02174", "contents": "Title: Radio Imaging With Information Field Theory Abstract: Data from radio interferometers provide a substantial challenge for\nstatisticians. It is incomplete, noise-dominated and originates from a\nnon-trivial measurement process. The signal is not only corrupted by imperfect\nmeasurement devices but also from effects like fluctuations in the ionosphere\nthat act as a distortion screen. In this paper we focus on the imaging part of\ndata reduction in radio astronomy and present RESOLVE, a Bayesian imaging\nalgorithm for radio interferometry in its new incarnation. It is formulated in\nthe language of information field theory. Solely by algorithmic advances the\ninference could be sped up significantly and behaves noticeably more stable\nnow. This is one more step towards a fully user-friendly version of RESOLVE\nwhich can be applied routinely by astronomers. \n\n"}
{"id": "1803.02626", "contents": "Title: Inferring health conditions from fMRI-graph data Abstract: Automated classification methods for disease diagnosis are currently in the\nlimelight, especially for imaging data. Classification does not fully meet a\nclinician's needs, however: in order to combine the results of multiple tests\nand decide on a course of treatment, a clinician needs the likelihood of a\ngiven health condition rather than binary classification yielded by such\nmethods. We illustrate how likelihoods can be derived step by step from first\nprinciples and approximations, and how they can be assessed and selected,\nillustrating our approach using fMRI data from a publicly available data set\ncontaining schizophrenic and healthy control subjects. We start from the basic\nassumption of partial exchangeability, and then the notion of sufficient\nstatistics and the \"method of translation\" (Edgeworth, 1898) combined with\nconjugate priors. This method can be used to construct a likelihood that can be\nused to compare different data-reduction algorithms. Despite the\nsimplifications and possibly unrealistic assumptions used to illustrate the\nmethod, we obtain classification results comparable to previous, more realistic\nstudies about schizophrenia, whilst yielding likelihoods that can naturally be\ncombined with the results of other diagnostic tests. \n\n"}
{"id": "1803.04503", "contents": "Title: Improved Neymanian analysis for $2^K$ factorial designs with binary\n  outcomes Abstract: $2^K$ factorial designs are widely adopted by statisticians and the broader\nscientific community. In this short note, under the potential outcomes\nframework (Neyman, 1923; Rubin, 1974), we adopt the partial identification\napproach and derive the sharp lower bound of the sampling variance of the\nestimated factorial effects, which leads to an \"improved\" Neymanian variance\nestimator that mitigates the over-estimation issue suffered by the classic\nNeymanian variance estimator by Dasgupta et al. (2015). \n\n"}
{"id": "1803.04839", "contents": "Title: Optimal estimators in misspecified linear regression model with an\n  application to real-world data Abstract: In this article, we propose the Sample Information Optimal Estimator (SIOE)\nand the Stochastic Restricted Optimal Estimator (SROE) for misspecified linear\nregression model when multicollinearity exists among explanatory variables.\nFurther, we obtain the superiority conditions of proposed estimators over some\nother existing estimators in the Mean Square Error Matrix (MSEM) criterion in a\nstandard form which can apply to all estimators considered in this study.\nFinally, a real world example and a Monte Carlo simulation study are presented\nfor the proposed estimators to illustrate the theoretical results. \n\n"}
{"id": "1803.05127", "contents": "Title: Feature Selection and Model Comparison on Microsoft Learning-to-Rank\n  Data Sets Abstract: With the rapid advance of the Internet, search engines (e.g., Google, Bing,\nYahoo!) are used by billions of users for each day. The main function of a\nsearch engine is to locate the most relevant webpages corresponding to what the\nuser requests. This report focuses on the core problem of information\nretrieval: how to learn the relevance between a document (very often webpage)\nand a query given by user. Our analysis consists of two parts: 1) we use\nstandard statistical methods to select important features among 137 candidates\ngiven by information retrieval researchers from Microsoft. We find that not all\nthe features are useful, and give interpretations on the top-selected features;\n2) we give baselines on prediction over the real-world dataset MSLR-WEB by\nusing various learning algorithms. We find that models of boosting trees,\nrandom forest in general achieve the best performance of prediction. This\nagrees with the mainstream opinion in information retrieval community that\ntree-based algorithms outperform the other candidates for this problem. \n\n"}
{"id": "1803.05403", "contents": "Title: Additive quantile regression for clustered data with an application to\n  children's physical activity Abstract: Additive models are flexible regression tools that handle linear as well as\nnonlinear terms. The latter are typically modelled via smoothing splines.\nAdditive mixed models extend additive models to include random terms when the\ndata are sampled according to cluster designs (e.g., longitudinal). These\nmodels find applications in the study of phenomena like growth, certain disease\nmechanisms and energy consumption in humans, when repeated measurements are\navailable. In this paper, we propose a novel additive mixed model for quantile\nregression. Our methods are motivated by an application to physical activity\nbased on a dataset with more than half million accelerometer measurements in\nchildren of the UK Millennium Cohort Study. In a simulation study, we assess\nthe proposed methods against existing alternatives. \n\n"}
{"id": "1803.06040", "contents": "Title: False discovery rate control for multiple testing based on p-values with\n  c\\`adl\\`ag distribution functions Abstract: For multiple testing based on p-values with c\\`{a}dl\\`{a}g distribution\nfunctions, we propose an FDR procedure \"BH+\" with proven conservativeness. BH+\nis at least as powerful as the BH procedure when they are applied to\nsuper-uniform p-values. Further, when applied to mid p-values, BH+ is more\npowerful than it is applied to conventional p-values. An easily verifiable\nnecessary and sufficient condition for this is provided. BH+ is perhaps the\nfirst conservative FDR procedure applicable to mid p-values. BH+ is applied to\nmultiple testing based on discrete p-values in a methylation study, an HIV\nstudy and a clinical safety study, where it makes considerably more discoveries\nthan the BH procedure. \n\n"}
{"id": "1803.06206", "contents": "Title: Big Data and Reliability Applications: The Complexity Dimension Abstract: Big data features not only large volumes of data but also data with\ncomplicated structures. Complexity imposes unique challenges in big data\nanalytics. Meeker and Hong (2014, Quality Engineering, pp. 102-116) provided an\nextensive discussion of the opportunities and challenges in big data and\nreliability, and described engineering systems that can generate big data that\ncan be used in reliability analysis. Meeker and Hong (2014) focused on large\nscale system operating and environment data (i.e., high-frequency multivariate\ntime series data), and provided examples on how to link such data as covariates\nto traditional reliability responses such as time to failure, time to\nrecurrence of events, and degradation measurements. This paper intends to\nextend that discussion by focusing on how to use data with complicated\nstructures to do reliability analysis. Such data types include high-dimensional\nsensor data, functional curve data, and image streams. We first provide a\nreview of recent development in those directions, and then we provide a\ndiscussion on how analytical methods can be developed to tackle the challenging\naspects that arise from the complexity feature of big data in reliability\napplications. The use of modern statistical methods such as variable selection,\nfunctional data analysis, scalar-on-image regression, spatio-temporal data\nmodels, and machine learning techniques will also be discussed. \n\n"}
{"id": "1803.06249", "contents": "Title: Link prediction for interdisciplinary collaboration via co-authorship\n  network Abstract: We analyse the Publication and Research (PURE) data set of University of\nBristol collected between $2008$ and $2013$. Using the existing co-authorship\nnetwork and academic information thereof, we propose a new link prediction\nmethodology, with the specific aim of identifying potential interdisciplinary\ncollaboration in a university-wide collaboration network. \n\n"}
{"id": "1803.06518", "contents": "Title: Provable Convex Co-clustering of Tensors Abstract: Cluster analysis is a fundamental tool for pattern discovery of complex\nheterogeneous data. Prevalent clustering methods mainly focus on vector or\nmatrix-variate data and are not applicable to general-order tensors, which\narise frequently in modern scientific and business applications. Moreover,\nthere is a gap between statistical guarantees and computational efficiency for\nexisting tensor clustering solutions due to the nature of their non-convex\nformulations. In this work, we bridge this gap by developing a provable convex\nformulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator\nenjoys stability guarantees and its computational and storage costs are\npolynomial in the size of the data. We further establish a non-asymptotic error\nbound for the CoCo estimator, which reveals a surprising \"blessing of\ndimensionality\" phenomenon that does not exist in vector or matrix-variate\ncluster analysis. Our theoretical findings are supported by extensive simulated\nstudies. Finally, we apply the CoCo estimator to the cluster analysis of\nadvertisement click tensor data from a major online company. Our clustering\nresults provide meaningful business insights to improve advertising\neffectiveness. \n\n"}
{"id": "1803.06557", "contents": "Title: Estimation of treatment effects under endogeneous heteroskedasticity Abstract: The empirical literature on program evaluation limits its scope almost\nexclusively to models where treatment effects are homogenous for\nobservationally identical individuals. This paper considers a treatment effect\nmodel in which treatment effects may be heterogeneous, even among\nobservationally identical individuals. Specifically, extending the classical\ninstrumental variables (IV) model with an endogenous binary treatment and a\nbinary instrument, we allow the heteroskedasticity of the error disturbance to\nalso depend upon the treatment variable so that treatment has both mean and\nvariance effects on the outcome. In this endogenous heteroskedasticity IV\n(EHIV) model with heterogeneous individual treatment effects, the standard IV\nestimator can be inconsistent and lead to incorrect inference. After showing\nidentification of the mean and variance treatment effects in a nonparametric\nversion of the EHIV model, we provide closed-form estimators for the linear\nEHIV for the mean and variance treatment effects and the individual treatment\neffects (ITE). Asymptotic properties of the estimators are provided. A Monte\nCarlo simulation investigates the performance of the proposed approach, and an\nempirical application regarding the effects of fertility on female labor supply\nis considered. \n\n"}
{"id": "1803.06711", "contents": "Title: A Dynamic Additive and Multiplicative Effects Model with Application to\n  the United Nations Voting Behaviors Abstract: Motivated by a study of United Nations voting behaviors, we introduce a\nregression model for a series of networks that are correlated over time. Our\nmodel is a dynamic extension of the additive and multiplicative effects network\nmodel (AMEN) of Hoff (2019). In addition to incorporating a temporal structure,\nthe model accommodates two types of missing data thus allows the size of the\nnetwork to vary over time. We demonstrate via simulations the necessity of\nvarious components of the model. We apply the model to the United Nations\nGeneral Assembly voting data from 1983 to 2014 (Voeten (2013)) to answer\ninteresting research questions regarding international voting behaviors. In\naddition to finding important factors that could explain the voting behaviors,\nthe model-estimated additive effects, multiplicative effects, and their\nmovements reveal meaningful foreign policy positions and alliances of various\ncountries. \n\n"}
{"id": "1803.06730", "contents": "Title: Combining Probabilistic Load Forecasts Abstract: Probabilistic load forecasts provide comprehensive information about future\nload uncertainties. In recent years, many methodologies and techniques have\nbeen proposed for probabilistic load forecasting. Forecast combination, a\nwidely recognized best practice in point forecasting literature, has never been\nformally adopted to combine probabilistic load forecasts. This paper proposes a\nconstrained quantile regression averaging (CQRA) method to create an improved\nensemble from several individual probabilistic forecasts. We formulate the CQRA\nparameter estimation problem as a linear program with the objective of\nminimizing the pinball loss, with the constraints that the parameters are\nnonnegative and summing up to one. We demonstrate the effectiveness of the\nproposed method using two publicly available datasets, the ISO New England data\nand Irish smart meter data. Comparing with the best individual probabilistic\nforecast, the ensemble can reduce the pinball score by 4.39% on average. The\nproposed ensemble also demonstrates superior performance over nine other\nbenchmark ensembles. \n\n"}
{"id": "1803.06738", "contents": "Title: Large-Scale Dynamic Predictive Regressions Abstract: We develop a novel \"decouple-recouple\" dynamic predictive strategy and\ncontribute to the literature on forecasting and economic decision making in a\ndata-rich environment. Under this framework, clusters of predictors generate\ndifferent latent states in the form of predictive densities that are later\nsynthesized within an implied time-varying latent factor model. As a result,\nthe latent inter-dependencies across predictive densities and biases are\nsequentially learned and corrected. Unlike sparse modeling and variable\nselection procedures, we do not assume a priori that there is a given subset of\nactive predictors, which characterize the predictive density of a quantity of\ninterest. We test our procedure by investigating the predictive content of a\nlarge set of financial ratios and macroeconomic variables on both the equity\npremium across different industries and the inflation rate in the U.S., two\ncontexts of topical interest in finance and macroeconomics. We find that our\npredictive synthesis framework generates both statistically and economically\nsignificant out-of-sample benefits while maintaining interpretability of the\nforecasting variables. In addition, the main empirical results highlight that\nour proposed framework outperforms both LASSO-type shrinkage regressions,\nfactor based dimension reduction, sequential variable selection, and\nequal-weighted linear pooling methodologies. \n\n"}
{"id": "1803.06823", "contents": "Title: Nonparametric forecasting of multivariate probability density functions Abstract: The study of dependence between random variables is the core of theoretical\nand applied statistics. Static and dynamic copula models are useful for\ndescribing the dependence structure, which is fully encrypted in the copula\nprobability density function. However, these models are not always able to\ndescribe the temporal change of the dependence patterns, which is a key\ncharacteristic of financial data. We propose a novel nonparametric framework\nfor modelling a time series of copula probability density functions, which\nallows to forecast the entire function without the need of post-processing\nprocedures to grant positiveness and unit integral. We exploit a suitable\nisometry that allows to transfer the analysis in a subset of the space of\nsquare integrable functions, where we build on nonparametric functional data\nanalysis techniques to perform the analysis. The framework does not assume the\ndensities to belong to any parametric family and it can be successfully applied\nalso to general multivariate probability density functions with bounded or\nunbounded support. Finally, a noteworthy field of application pertains the\nstudy of time varying networks represented through vine copula models. We apply\nthe proposed methodology for estimating and forecasting the time varying\ndependence structure between the S\\&P500 and NASDAQ indices. \n\n"}
{"id": "1803.07166", "contents": "Title: Latent Space Modeling of Multidimensional Networks with Application to\n  the Exchange of Votes in Eurovision Song Contest Abstract: The Eurovision Song Contest is a popular TV singing competition held annually\namong country members of the European Broadcasting Union. In this competition,\neach member can be both contestant and jury, as it can participate with a song\nand/or vote for other countries' tunes. Throughout the years, the voting system\nhas repeatedly been accused of being biased by the presence of tactical voting,\naccording to which votes would represent strategic interests rather than actual\nmusical preferences of the voting countries. In this work, we develop a latent\nspace model to investigate the presence of a latent structure underlying the\nexchange of votes. Focusing on the period from 1998 to 2015, we represent the\nvote exchange as a multivariate network: each edition is a network, where\ncountries are the nodes and two countries are linked by an edge if one voted\nfor the other. The different networks are taken to be independent replicates of\na common latent space capturing the overall relationships among the countries.\nProximity denotes similarity, and countries close in the latent space are\nassumed to be more likely to exchange votes. Therefore, if the exchange of\nvotes depends on the similarity between countries, the quality of the competing\nsongs might not be a relevant factor in the determination of the voting\npreferences, and this would suggest the presence of bias. A Bayesian\nhierarchical modelling approach is employed to model the probability of a\nconnection between any two countries as a function of their distance in the\nlatent space, and of network-specific parameters and edge-specific covariates.\nThe inferred latent space is found to be relevant in the determination of edge\nprobabilities, however, the positions of the countries in such space only\npartially correspond to their actual geographical positions. \n\n"}
{"id": "1803.07734", "contents": "Title: Adaptive Sequential MCMC for Combined State and Parameter Estimation Abstract: In the case of a linear state space model, we implement an MCMC sampler with\ntwo phases. In the learning phase, a self-tuning sampler is used to learn the\nparameter mean and covariance structure. In the estimation phase, the parameter\nmean and covariance structure informs the proposed mechanism and is also used\nin a delayed-acceptance algorithm. Information on the resulting state of the\nsystem is given by a Gaussian mixture. In on-line mode, the algorithm is\nadaptive and uses a sliding window approach to accelerate sampling speed and to\nmaintain appropriate acceptance rates. We apply the algorithm to joined state\nand parameter estimation in the case of irregularly sampled GPS time series\ndata. \n\n"}
{"id": "1803.08255", "contents": "Title: A non-homogeneous hidden Markov model for partially observed\n  longitudinal responses Abstract: Dropout represents a typical issue to be addressed when dealing with\nlongitudinal studies. If the mechanism leading to missing information is\nnon-ignorable, inference based on the observed data only may be severely\nbiased. A frequent strategy to obtain reliable parameter estimates is based on\nthe use of individual-specific random coefficients that help capture sources of\nunobserved heterogeneity and, at the same time, define a reasonable structure\nof dependence between the longitudinal and the missing data process. We refer\nto elements in this class as random coefficient based dropout models (RCBDMs).\nWe propose a dynamic, semi-parametric, version of the standard RCBDM to deal\nwith discrete time to event. Time-varying random coefficients that evolve over\ntime according to a non-homogeneous hidden Markov chain are considered to model\ndependence between longitudinal responses recorded from the same subject. A\nseparate set of random coefficients is considered to model dependence between\nmissing data indicators. Last, the joint distribution of the random\ncoefficients in the two equations helps describe the dependence between the two\nprocesses. To ensure model flexibility and avoid unverifiable assumptions, we\nleave the joint distribution of the random coefficients unspecified and\nestimate it via nonparametric maximum likelihood. The proposal is applied to\ndata from the Leiden 85+ study on the evolution of cognitive functioning in the\nelderly. \n\n"}
{"id": "1803.08424", "contents": "Title: Autoplotly - Automatic Generation of Interactive Visualizations for\n  Popular Statistical Results Abstract: The autoplotly package provides functionalities to automatically generate\ninteractive visualizations for many popular statistical results supported by\nggfortify package with plotly and ggplot2 style. The generated visualizations\ncan also be easily extended using ggplot2 and plotly syntax while staying\ninteractive. \n\n"}
{"id": "1803.11273", "contents": "Title: High-Dimensional Causal Discovery Under non-Gaussianity Abstract: We consider graphical models based on a recursive system of linear structural\nequations. This implies that there is an ordering, $\\sigma$, of the variables\nsuch that each observed variable $Y_v$ is a linear function of a variable\nspecific error term and the other observed variables $Y_u$ with $\\sigma(u) <\n\\sigma (v)$. The causal relationships, i.e., which other variables the linear\nfunctions depend on, can be described using a directed graph. It has been\npreviously shown that when the variable specific error terms are non-Gaussian,\nthe exact causal graph, as opposed to a Markov equivalence class, can be\nconsistently estimated from observational data. We propose an algorithm that\nyields consistent estimates of the graph also in high-dimensional settings in\nwhich the number of variables may grow at a faster rate than the number of\nobservations, but in which the underlying causal structure features suitable\nsparsity; specifically, the maximum in-degree of the graph is controlled. Our\ntheoretical analysis is couched in the setting of log-concave error\ndistributions. \n\n"}
{"id": "1803.11331", "contents": "Title: Large Multi-scale Spatial Kriging Using Tree Shrinkage Priors Abstract: We develop a multiscale spatial kernel convolution technique with higher\norder functions to capture fine scale local features and lower order terms to\ncapture large scale features. To achieve parsimony, the coefficients in the\nmultiscale kernel convolution model is assigned a new class of \"Tree shrinkage\nprior\" distributions. Tree shrinkage priors exert increasing shrinkage on the\ncoefficients as resolution grows so as to adapt to the necessary degree of\nresolution at any sub-domain. Our proposed model has a number of significant\nfeatures over the existing multi-scale spatial models for big data. In contrast\nto the existing multiscale approaches, the proposed approach auto-tunes the\ndegree of resolution necessary to model a subregion in the domain, achieves\nscalability by suitable parallelization of local updating of parameters and is\nbuttressed by theoretical support. Excellent empirical performances are\nillustrated using several simulation experiments and a geostatistical analysis\nof the sea surface temperature data from the pacific ocean. \n\n"}
{"id": "1804.02090", "contents": "Title: Microsimulation Model Calibration using Incremental Mixture Approximate\n  Bayesian Computation Abstract: Microsimulation models (MSMs) are used to predict population-level effects of\nhealth care policies by simulating individual-level outcomes. Simulated\noutcomes are governed by unknown parameters that are chosen so that the model\naccurately predicts specific targets, a process referred to as model\ncalibration. Calibration targets can come from randomized controlled trials,\nobservational studies, and expert opinion, and are typically summary\nstatistics. A well calibrated model can reproduce a wide range of targets. MSM\ncalibration generally involves searching a high dimensional parameter space and\npredicting many targets through model simulation. This requires efficient\nmethods for exploring the parameter space and sufficient computational\nresources. We develop Incremental Mixture Approximate Bayesian Computation\n(IMABC) as a method for MSM calibration and implement it via a high-performance\ncomputing workflow, which provides the necessary computational scale. IMABC\nbegins with a rejection-based approximate Bayesian computation (ABC) step,\ndrawing a sample of parameters from the prior distribution and simulating\ncalibration targets. Next, the sample is iteratively updated by drawing\nadditional points from a mixture of multivariate normal distributions, centered\nat the points that yield simulated targets that are near observed targets.\nPosterior estimates are obtained by weighting sampled parameter vectors to\naccount for the adaptive sampling scheme. We demonstrate IMABC by calibrating a\nMSM for the natural history of colorectal cancer to obtain simulated draws from\nthe joint posterior distribution of model parameters. \n\n"}
{"id": "1804.02097", "contents": "Title: Multi-view Banded Spectral Clustering with Application to ICD9\n  Clustering Abstract: Despite recent development in methodology, community detection remains a\nchallenging problem. Existing literature largely focuses on the standard\nsetting where a network is learned using an observed adjacency matrix from a\nsingle data source. Constructing a shared network from multiple data sources is\nmore challenging due to the heterogeneity across populations. Additionally, no\nexisting method leverages the prior distance knowledge available in many\ndomains to help the discovery of the network structure. To bridge this gap, in\nthis paper we propose a novel spectral clustering method that optimally\ncombines multiple data sources while leveraging the prior distance knowledge.\nThe proposed method combines a banding step guided by the distance knowledge\nwith a subsequent weighting step to maximize consensus across multiple sources.\nIts statistical performance is thoroughly studied under a multi-view stochastic\nblock model. We also provide a simple yet optimal rule of choosing weights in\npractice. The efficacy and robustness of the method is fully demonstrated\nthrough extensive simulations. Finally, we apply the method to cluster the\nInternational classification of diseases, ninth revision (ICD9), codes and\nyield a very insightful clustering structure by integrating information from a\nlarge claim database and two healthcare systems. \n\n"}
{"id": "1804.02334", "contents": "Title: Individualized Dynamic Prediction of Survival under Time-Varying\n  Treatment Strategies Abstract: Often in follow-up studies intermediate events occur in some patients, such\nas reinterventions or adverse events. These intermediate events directly affect\nthe shapes of their longitudinal profiles. Our work is motivated by two studies\nin which such intermediate events have been recorded during follow-up. The\nfirst study concerns Congenital Heart Diseased patients who were followed-up\nechocardiographically, with several patients undergoing reintervention. The\nsecond study concerns patients who participated in the SPRINT study and\nexperienced adverse events during follow-up. We are interested in the change of\nthe longitudinal profiles after the occurrence of the intermediate event and in\nutilizing this information to improve the accuracy of the dynamic prediction\nfor their risk.\n  To achieve this, we propose a flexible joint modeling framework for the\nlongitudinal and survival data that includes the intermediate event as a\ntime-varying binary covariate in both the longitudinal and survival submodels.\nWe consider a set of joint models that postulate different effects of the\nintermediate event in the longitudinal profile and the risk of the clinical\nendpoint, with different formulations for their association while allowing its\nparametrization to change after the occurrence of the intermediate event. Based\non these models we derive dynamic predictions of conditional survival\nprobabilities which are adaptive to different scenarios with respect to the\noccurrence of the intermediate event. We evaluate the accuracy of these\npredictions with a simulation study using the time-dependent area under the\nreceiver operating characteristic curve and the expected prediction error\nadjusted to our setting. The results suggest that accounting for the changes in\nthe longitudinal profiles and the instantaneous risk for the clinical endpoint\nis important, and improves the accuracy of the dynamic predictions. \n\n"}
{"id": "1804.02605", "contents": "Title: Moving Beyond Sub-Gaussianity in High-Dimensional Statistics:\n  Applications in Covariance Estimation and Linear Regression Abstract: Concentration inequalities form an essential toolkit in the study of high\ndimensional (HD) statistical methods. Most of the relevant statistics\nliterature in this regard is based on sub-Gaussian or sub-exponential tail\nassumptions. In this paper, we first bring together various probabilistic\ninequalities for sums of independent random variables under much more general\nexponential type (namely sub-Weibull) tail assumptions. These results extract a\npart sub-Gaussian tail behavior in finite samples, matching the asymptotics\ngoverned by the central limit theorem, and are compactly represented in terms\nof a new Orlicz quasi-norm - the Generalized Bernstein-Orlicz norm - that\ntypifies such tail behaviors.\n  We illustrate the usefulness of these inequalities through the analysis of\nfour fundamental problems in HD statistics. In the first two problems, we study\nthe rate of convergence of the sample covariance matrix in terms of the maximum\nelementwise norm and the maximum k-sub-matrix operator norm which are key\nquantities of interest in bootstrap, HD covariance matrix estimation and HD\ninference. The third example concerns the restricted eigenvalue condition,\nrequired in HD linear regression, which we verify for all sub-Weibull random\nvectors through a unified analysis, and also prove a more general result\nrelated to restricted strong convexity in the process. In the final example, we\nconsider the Lasso estimator for linear regression and establish its rate of\nconvergence under much weaker than usual tail assumptions (on the errors as\nwell as the covariates), while also allowing for misspecified models and both\nfixed and random design. To our knowledge, these are the first such results for\nLasso obtained in this generality. The common feature in all our results over\nall the examples is that the convergence rates under most exponential tails\nmatch the usual ones under sub-Gaussian assumptions. \n\n"}
{"id": "1804.02742", "contents": "Title: Bayesian Calibration of Force-fields from Experimental Data: TIP4P Water Abstract: Molecular dynamics (MD) simulations give access to equilibrium structures and\ndynamic properties given an ergodic sampling and an accurate force-field. The\nforce-field parameters are calibrated to reproduce properties measured by\nexperiments or simulations. The main contribution of this paper is an\napproximate Bayesian framework for the calibration and uncertainty\nquantification of the force-field parameters, without assuming parameter\nuncertainty to be Gaussian. To this aim, since the likelihood function of the\nMD simulation models are intractable in absence of Gaussianity assumption, we\nuse a likelihood-free inference scheme known as approximate Bayesian\ncomputation (ABC) and propose an adaptive population Monte Carlo ABC algorithm,\nwhich is illustrated to converge faster and scales better than previously used\nABCsubsim algorithm for calibration of force-field of a helium system. The\nsecond contribution is the adaptation of ABC algorithms for High Performance\nComputing to MD simulation within the Python ecosystem ABCpy. We illustrate the\nperformance of the developed methodology to learn posterior distribution and\nBayesian estimates of Lennard-Jones force-field parameters of helium and TIP4P\nsystem of water implemented both for simulated and experimental datasets\ncollected using Neutron and X-ray diffraction. For simulated data, the Bayesian\nestimate is in close agreement with the true parameter value used to generate\nthe dataset. For experimental as well as for simulated data, the Bayesian\nposterior distribution shows a strong correlation pattern between the\nforce-field parameters. Providing an estimate of the entire posterior\ndistribution, our methodology also allows us to perform uncertainty\nquantification of model prediction. This research opens up the possibility to\nrigorously calibrate force-fields from available experimental datasets of any\nstructural and dynamic property. \n\n"}
{"id": "1804.04541", "contents": "Title: A copula-based sensitivity analysis method and its application to a\n  North Sea sediment transport model Abstract: This paper describes a novel sensitivity analysis method, able to handle\ndependency relationships between model parameters. The starting point is the\npopular Morris (1991) algorithm, which was initially devised under the\nassumption of parameter independence. This important limitation is tackled by\nallowing the user to incorporate dependency information through a copula. The\nset of model runs obtained using latin hypercube sampling, are then used for\nderiving appropriate sensitivity measures.\n  Delft3D-WAQ (Deltares, 2010) is a sediment transport model with strong\ncorrelations between input parameters. Despite this, the parameter ranking\nobtained with the newly proposed method is in accordance with the knowledge\nobtained from expert judgment. However, under the same conditions, the classic\nMorris method elicits its results from model runs which break the assumptions\nof the underlying physical processes. This leads to the conclusion that the\nproposed extension is superior to the classic Morris algorithm and can\naccommodate a wide range of use cases. \n\n"}
{"id": "1804.04568", "contents": "Title: A fuzzy process of individuation Abstract: It is shown that an aspect of the process of individuation may be thought of\nas a fuzzy set. The process of individuation has been interpreted as a\ntwo-valued problem in the history of philosophy. In this work, I intend to show\nthat such a process in its psychosocial aspect is better understood in terms of\na fuzzy set, characterized by a continuum membership function. According to\nthis perspective, species and their members present different degrees of\nindividuation. Such degrees are measured from the membership function of the\npsychosocial process of individuation. Thus, a social analysis is suggested by\nusing this approach in human societies. \n\n"}
{"id": "1804.04588", "contents": "Title: Bayesian Modeling of Air Pollution Extremes Using Nested Multivariate\n  Max-Stable Processes Abstract: Capturing the potentially strong dependence among the peak concentrations of\nmultiple air pollutants across a spatial region is crucial for assessing the\nrelated public health risks. In order to investigate the multivariate spatial\ndependence properties of air pollution extremes, we introduce a new class of\nmultivariate max-stable processes. Our proposed model admits a hierarchical\ntree-based formulation, in which the data are conditionally independent given\nsome latent nested $\\alpha$-stable random factors. The hierarchical structure\nfacilitates Bayesian inference and offers a convenient and interpretable\ncharacterization. We fit this nested multivariate max-stable model to the\nmaxima of air pollution concentrations and temperatures recorded at a number of\nsites in the Los Angeles area, showing that the proposed model succeeds in\ncapturing their complex tail dependence structure. \n\n"}
{"id": "1804.04590", "contents": "Title: Mixed-Effect Modeling for Longitudinal Prediction of Cancer Tumor Abstract: In this paper, a mixed-effect modeling scheme is proposed to construct a\npredictor for different features of cancer tumor. For this purpose, a set of\nfeatures is extracted from two groups of patients with the same type of cancer\nbut with two medical outcome: 1) survived and 2) passed away. The goal is to\nbuild different models for the two groups, where in each group,\npatient-specified behavior of individuals can be characterized. These models\nare then used as predictors to forecast future state of patients with a given\nhistory or initial state. To this end, a leave-on-out cross validation method\nis used to measure the prediction accuracy of each patient-specified model.\nExperiments show that compared to fixed-effect modeling (regression),\nmixed-effect modeling has a superior performance on some of the extracted\nfeatures and similar or worse performance on the others. \n\n"}
{"id": "1804.04622", "contents": "Title: Causal Inference via Kernel Deviance Measures Abstract: Discovering the causal structure among a set of variables is a fundamental\nproblem in many areas of science. In this paper, we propose Kernel Conditional\nDeviance for Causal Inference (KCDC) a fully nonparametric causal discovery\nmethod based on purely observational data. From a novel interpretation of the\nnotion of asymmetry between cause and effect, we derive a corresponding\nasymmetry measure using the framework of reproducing kernel Hilbert spaces.\nBased on this, we propose three decision rules for causal discovery. We\ndemonstrate the wide applicability of our method across a range of diverse\nsynthetic datasets. Furthermore, we test our method on real-world time series\ndata and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we\noutperform existing state-of-the-art methods. \n\n"}
{"id": "1804.05015", "contents": "Title: Large-scale diversity estimation through surname origin inference Abstract: The study of surnames as both linguistic and geographical markers of the past\nhas proven valuable in several research fields spanning from biology and\ngenetics to demography and social mobility. This article builds upon the\nexisting literature to conceive and develop a surname origin classifier based\non a data-driven typology. This enables us to explore a methodology to describe\nlarge-scale estimates of the relative diversity of social groups, especially\nwhen such data is scarcely available. We subsequently analyze the\nrepresentativeness of surname origins for 15 socio-professional groups in\nFrance. \n\n"}
{"id": "1804.05079", "contents": "Title: Robust Estimation of the Weighted Average Treatment Effect for A Target\n  Population Abstract: The weighted average treatment effect (WATE) is a causal measure for the\ncomparison of interventions in a specific target population, which may be\ndifferent from the population where data are sampled from. For instance, when\nthe goal is to introduce a new treatment to a target population, the question\nis what efficacy (or effectiveness) can be gained by switching patients from a\nstandard of care (control) to this new treatment, for which the average\ntreatment effect for the control (ATC) estimand can be applied. In this paper,\nwe propose two estimators based on augmented inverse probability weighting to\nestimate the WATE for a well defined target population (i.e., there exists a\ntarget function that describes the population of interest), using observational\ndata. The first proposed estimator is doubly robust if the target function is\nknown or can be correctly specified. The second proposed estimator is doubly\nrobust if the target function has a linear dependence on the propensity score,\nwhich can be used to estimate the average treatment effect for the treated\n(ATT) and ATC. We demonstrate the properties of the proposed estimators through\ntheoretical proof and simulation studies. We also apply our proposed methods in\na comparison of glucagon-like peptide-1 receptor agonists therapy and insulin\ntherapy among patients with type 2 diabetes, using the UK clinical practice\nresearch datalink data. \n\n"}
{"id": "1804.05144", "contents": "Title: Simultaneous Edit and Imputation for Household Data with Structural\n  Zeros Abstract: Multivariate categorical data nested within households often include reported\nvalues that fail edit constraints---for example, a participating household\nreports a child's age as older than his biological parent's age---as well as\nmissing values. Generally, agencies prefer datasets to be free from erroneous\nor missing values before analyzing them or disseminating them to secondary data\nusers. We present a model-based engine for editing and imputation of household\ndata based on a Bayesian hierarchical model that includes (i) a nested data\nDirichlet process mixture of products of multinomial distributions as the model\nfor the true latent values of the data, truncated to allow only households that\nsatisfy all edit constraints, (ii) a model for the location of errors, and\n(iii) a reporting model for the observed responses in error. The approach\npropagates uncertainty due to unknown locations of errors and missing values,\ngenerates plausible datasets that satisfy all edit constraints, and can\npreserve multivariate relationships within and across individuals in the same\nhousehold. We illustrate the approach using data from the 2012 American\nCommunity Survey. \n\n"}
{"id": "1804.06788", "contents": "Title: Validating Bayesian Inference Algorithms with Simulation-Based\n  Calibration Abstract: Verifying the correctness of Bayesian computation is challenging. This is\nespecially true for complex models that are common in practice, as these\nrequire sophisticated model implementations and algorithms. In this paper we\nintroduce \\emph{simulation-based calibration} (SBC), a general procedure for\nvalidating inferences from Bayesian algorithms capable of generating posterior\nsamples. This procedure not only identifies inaccurate computation and\ninconsistencies in model implementations but also provides graphical summaries\nthat can indicate the nature of the problems that arise. We argue that SBC is a\ncritical part of a robust Bayesian workflow, as well as being a useful tool for\nthose developing computational algorithms and statistical software. \n\n"}
{"id": "1804.07864", "contents": "Title: Between perfectly critical and fully irregular: a reverberating model\n  captures and predicts cortical spike propagation Abstract: Knowledge about the collective dynamics of cortical spiking is very\ninformative about the underlying coding principles. However, even most basic\nproperties are not known with certainty, because their assessment is hampered\nby spatial subsampling, i.e. the limitation that only a tiny fraction of all\nneurons can be recorded simultaneously with millisecond precision. Building on\na novel, subsampling-invariant estimator, we fit and carefully validate a\nminimal model for cortical spike propagation. The model interpolates between\ntwo prominent states: asynchronous and critical. We find neither of them in\ncortical spike recordings across various species, but instead identify a narrow\nreverberating regime. This approach enables us to predict yet unknown\nproperties from very short recordings and for every circuit individually,\nincluding responses to minimal perturbations, intrinsic network timescales, and\nthe strength of external input compared to recurrent activation - thereby\ninforming about the underlying coding principles for each circuit, area, state\nand task. \n\n"}
{"id": "1804.08760", "contents": "Title: Randomization Tests to Assess Covariate Balance When Designing and\n  Analyzing Matched Datasets Abstract: Causal analyses for observational studies are often complicated by covariate\nimbalances among treatment groups, and matching methodologies alleviate this\ncomplication by finding subsets of treatment groups that exhibit covariate\nbalance. It is widely agreed upon that covariate balance can serve as evidence\nthat a matched dataset approximates a randomized experiment, but what kind of\nexperiment does a matched dataset approximate? In this work, we develop a\nrandomization test for the hypothesis that a matched dataset approximates a\nparticular experimental design, such as complete randomization, block\nrandomization, or rerandomization. Our test can incorporate any experimental\ndesign, and it allows for a graphical display that puts several designs on the\nsame univariate scale, thereby allowing researchers to pinpoint which design --\nif any -- is most appropriate for a matched dataset. After researchers\ndetermine a plausible design, we recommend a randomization-based approach for\nanalyzing the matched data, which can incorporate any design and treatment\neffect estimator. Through simulation, we find that our test can frequently\ndetect violations of randomized assignment that harm inferential results.\nFurthermore, through simulation and a real application in political science, we\nfind that matched datasets with high levels of covariate balance tend to\napproximate balance-constrained designs like rerandomization, and analyzing\nthem as such can lead to precise causal analyses. However, assuming a precise\ndesign should be proceeded with caution, because it can harm inferential\nresults if there are still substantial biases due to remaining imbalances after\nmatching. Our approach is implemented in the randChecks R package, available on\nCRAN. \n\n"}
{"id": "1804.09088", "contents": "Title: Semi-supervised Content-based Detection of Misinformation via Tensor\n  Embeddings Abstract: Fake news may be intentionally created to promote economic, political and\nsocial interests, and can lead to negative impacts on humans beliefs and\ndecisions. Hence, detection of fake news is an emerging problem that has become\nextremely prevalent during the last few years. Most existing works on this\ntopic focus on manual feature extraction and supervised classification models\nleveraging a large number of labeled (fake or real) articles. In contrast, we\nfocus on content-based detection of fake news articles, while assuming that we\nhave a small amount of labels, made available by manual fact-checkers or\nautomated sources. We argue this is a more realistic setting in the presence of\nmassive amounts of content, most of which cannot be easily factchecked. To that\nend, we represent collections of news articles as multi-dimensional tensors,\nleverage tensor decomposition to derive concise article embeddings that capture\nspatial/contextual information about each news article, and use those\nembeddings to create an article-by-article graph on which we propagate limited\nlabels. Results on three real-world datasets show that our method performs on\npar or better than existing models that are fully supervised, in that we\nachieve better detection accuracy using fewer labels. In particular, our\nproposed method achieves 75.43% of accuracy using only 30% of labels of a\npublic dataset while an SVM-based classifier achieved 67.43%. Furthermore, our\nmethod achieves 70.92% of accuracy in a large dataset using only 2% of labels. \n\n"}
{"id": "1804.10675", "contents": "Title: Estimation of the number of spikes using a generalized spike population\n  model and application to RNA-seq data Abstract: Although a generalized spike population model has been actively studied in\nrandom matrix theory, its application to real data has been rarely explored. We\nfind that most methods for determining the number of spikes based on the\nJohnstone's spike population model choose far too many spikes in RNA-seq gene\nexpression data or often fail to determine the number of spikes by indicating\nthat all components are spikes. In this paper, we propose a new algorithm for\nthe estimation of the number of spikes based on a generalized spike population\nmodel. Also, we suggest a new noise model for RNA-seq data based on population\nspectral distribution ideas, which provides a biologically reasonable number of\nspikes using the proposed algorithm. Furthermore, we propose a graphical tool\nfor assessing the performance of the underlying noise model. \n\n"}
{"id": "1805.00057", "contents": "Title: Identifying Effects of Multivalued Treatments Abstract: Multivalued treatment models have typically been studied under restrictive\nassumptions: ordered choice, and more recently unordered monotonicity. We show\nhow treatment effects can be identified in a more general class of models that\nallows for multidimensional unobserved heterogeneity. Our results rely on two\nmain assumptions: treatment assignment must be a measurable function of\nthreshold-crossing rules, and enough continuous instruments must be available.\nWe illustrate our approach for several classes of models. \n\n"}
{"id": "1805.01271", "contents": "Title: NFL Injuries Before and After the 2011 Collective Bargaining Agreement\n  (CBA) Abstract: The National Football League's (NFL) 2011 collective bargaining agreement\n(CBA) with its players placed a number of contact and quantity limitations on\npractices and workouts. Some coaches and others have expressed a concern that\nthis has led to poor conditioning and a subsequent increase in injuries. We\nsought to assess whether the 2011 CBA's practice restrictions affected the\nnumber of overall, conditioning-dependent, and/or non-conditioning-dependent\ninjuries in the NFL or the number of games missed due to those injuries. The\nstudy population was player-seasons from 2007-2016. We included regular season,\nnon-illness, non-head, game-loss injuries. Injuries were identified using a\ndatabase from Football Outsiders. The primary outcomes were overall,\nconditioning-dependent and non-conditioning-dependent injury counts by season.\nWe examined time trends in injury counts before (2007-2010) and after\n(2011-2016) the CBA using a Poisson interrupted time series model. The number\nof game-loss regular season, non-head, non-illness injuries grew from 701 in\n2007 to 804 in 2016 (15% increase). The number of regular season weeks missed\nexhibited a similar increase. Conditioning-dependent injuries increased from\n197 in 2007 to 271 in 2011 (38% rise), but were lower and remained relatively\nunchanged at 220-240 injuries per season thereafter. Non-conditioning injuries\ndecreased by 37% in the first three years of the new CBA before returning to\nhistoric levels in 2014-2016. Poisson models for all, conditioning-dependent,\nand non-conditioning-dependent game-loss injury counts did not show\nstatistically significant or meaningful detrimental changes associated with the\nCBA. We did not observe an increase in injuries following the 2011 CBA. Other\nconcurrent injury-related rule and regulation changes limit specific causal\ninferences about the practice restrictions, however. \n\n"}
{"id": "1805.02547", "contents": "Title: Learning Gene Regulatory Networks with High-Dimensional Heterogeneous\n  Data Abstract: The Gaussian graphical model is a widely used tool for learning gene\nregulatory networks with high-dimensional gene expression data. Most existing\nmethods for Gaussian graphical models assume that the data are homogeneous,\ni.e., all samples are drawn from a single Gaussian distribution. However, for\nmany real problems, the data are heterogeneous, which may contain some\nsubgroups or come from different resources. This paper proposes to model the\nheterogeneous data using a mixture Gaussian graphical model, and apply the\nimputation-consistency algorithm, combining with the $\\psi$-learning algorithm,\nto estimate the parameters of the mixture model and cluster the samples to\ndifferent subgroups. An integrated Gaussian graphical network is learned across\nthe subgroups along with the iterations of the imputation-consistency\nalgorithm. The proposed method is compared with an existing method for learning\nmixture Gaussian graphical models as well as a few other methods developed for\nhomogeneous data, such as graphical Lasso, nodewise regression and\n$\\psi$-learning. The numerical results indicate superiority of the proposed\nmethod in all aspects of parameter estimation, cluster identification and\nnetwork construction. The numerical results also indicate generality of the\nproposed method: it can be applied to homogeneous data without significant\nharms. \n\n"}
{"id": "1805.02988", "contents": "Title: Hierarchical inference for genome-wide association studies: a view on\n  methodology with software Abstract: We provide a view on high-dimensional statistical inference for genome-wide\nassociation studies (GWAS). It is in part a review but covers also new\ndevelopments for meta analysis with multiple studies and novel software in\nterms of an R-package hierinf. Inference and assessment of significance is\nbased on very high-dimensional multivariate (generalized) linear models: in\ncontrast to often used marginal approaches, this provides a step towards more\ncausal-oriented inference. \n\n"}
{"id": "1805.03309", "contents": "Title: Vecchia approximations of Gaussian-process predictions Abstract: Gaussian processes (GPs) are highly flexible function estimators used for\ngeospatial analysis, nonparametric regression, and machine learning, but they\nare computationally infeasible for large datasets. Vecchia approximations of\nGPs have been used to enable fast evaluation of the likelihood for parameter\ninference. Here, we study Vecchia approximations of spatial predictions at\nobserved and unobserved locations, including obtaining joint predictive\ndistributions at large sets of locations. We consider a general Vecchia\nframework for GP predictions, which contains some novel and some existing\nspecial cases. We study the accuracy and computational properties of these\napproaches theoretically and numerically, proving that our new methods exhibit\nlinear computational complexity in the total number of spatial locations. We\nshow that certain choices within the framework can have a strong effect on\nuncertainty quantification and computational cost, which leads to specific\nrecommendations on which methods are most suitable for various settings. We\nalso apply our methods to a satellite dataset of chlorophyll fluorescence,\nshowing that the new methods are faster or more accurate than existing methods,\nand reduce unrealistic artifacts in prediction maps. \n\n"}
{"id": "1805.04035", "contents": "Title: Scaling limit of the Stein variational gradient descent: the mean field\n  regime Abstract: We study an interacting particle system in $\\mathbf{R}^d$ motivated by Stein\nvariational gradient descent [Q. Liu and D. Wang, NIPS 2016], a deterministic\nalgorithm for sampling from a given probability density with unknown\nnormalization. We prove that in the large particle limit the empirical measure\nof the particle system converges to a solution of a non-local and nonlinear\nPDE. We also prove global existence, uniqueness and regularity of the solution\nto the limiting PDE. Finally, we prove that the solution to the PDE converges\nto the unique invariant solution in long time limit. \n\n"}
{"id": "1805.05232", "contents": "Title: Bayesian forecasting of many count-valued time series Abstract: This paper develops forecasting methodology and application of new classes of\ndynamic models for time series of non-negative counts. Novel univariate models\nsynthesise dynamic generalized linear models for binary and conditionally\nPoisson time series, with dynamic random effects for over-dispersion. These\nmodels allow use of dynamic covariates in both binary and non-zero count\ncomponents. Sequential Bayesian analysis allows fast, parallel analysis of sets\nof decoupled time series. New multivariate models then enable information\nsharing in contexts when data at a more highly aggregated level provide more\nincisive inferences on shared patterns such as trends and seasonality. A novel\nmulti-scale approach-- one new example of the concept of decouple/recouple in\ntime series-- enables information sharing across series. This incorporates\ncross-series linkages while insulating parallel estimation of univariate\nmodels, hence enables scalability in the number of series. The major motivating\ncontext is supermarket sales forecasting. Detailed examples drawn from a case\nstudy in multi-step forecasting of sales of a number of related items showcase\nforecasting of multiple series, with discussion of forecast accuracy metrics\nand broader questions of probabilistic forecast accuracy assessment. \n\n"}
{"id": "1805.05383", "contents": "Title: Spatio-temporal Bayesian On-line Changepoint Detection with Model\n  Selection Abstract: Bayesian On-line Changepoint Detection is extended to on-line model selection\nand non-stationary spatio-temporal processes. We propose spatially structured\nVector Autoregressions (VARs) for modelling the process between changepoints\n(CPs) and give an upper bound on the approximation error of such models. The\nresulting algorithm performs prediction, model selection and CP detection\non-line. Its time complexity is linear and its space complexity constant, and\nthus it is two orders of magnitudes faster than its closest competitor. In\naddition, it outperforms the state of the art for multivariate data. \n\n"}
{"id": "1805.06639", "contents": "Title: Independent Component Analysis via Energy-based and Kernel-based Mutual\n  Dependence Measures Abstract: We apply both distance-based (Jin and Matteson, 2017) and kernel-based\n(Pfister et al., 2016) mutual dependence measures to independent component\nanalysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,\nminimizing empirical dependence measures as an objective function in both\ndeflation and parallel manners. Solving this minimization problem, we introduce\nLatin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization\nmethod, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization\nof the Newton-type local optimization method. The performance of MDMICA is\nevaluated in various simulation studies and an image data example. When the ICA\nmodel is correct, MDMICA achieves competitive results compared to existing\napproaches. When the ICA model is misspecified, the estimated independent\ncomponents are less mutually dependent than the observed components using\nMDMICA, while they are prone to be even more mutually dependent than the\nobserved components using other approaches. \n\n"}
{"id": "1805.06640", "contents": "Title: Testing for Conditional Mean Independence with Covariates through\n  Martingale Difference Divergence Abstract: As a crucial problem in statistics is to decide whether additional variables\nare needed in a regression model. We propose a new multivariate test to\ninvestigate the conditional mean independence of Y given X conditioning on some\nknown effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are\nlinearly related, we reformulate an equivalent notion of conditional mean\nindependence through transformation, which is approximated in practice. We\napply the martingale difference divergence (Shao and Zhang, 2014) to measure\nconditional mean dependence, and show that the estimation error from\napproximation is negligible, as it has no impact on the asymptotic distribution\nof the test statistic under some regularity assumptions. The implementation of\nour test is demonstrated by both simulations and a financial data example. \n\n"}
{"id": "1805.07423", "contents": "Title: Efficient simulation of Gaussian Markov random fields by Chebyshev\n  polynomial approximation Abstract: This paper presents an algorithm to simulate Gaussian random vectors whose\nprecision matrix can be expressed as a polynomial of a sparse matrix. This\nsituation arises in particular when simulating Gaussian Markov random fields\nobtained by the discretization by finite elements of the solutions of some\nstochastic partial derivative equations. The proposed algorithm uses a\nChebyshev polynomial approximation to compute simulated vectors with a linear\ncomplexity. This method is asymptotically exact as the approximation order\ngrows. Criteria based on tests of the statistical properties of the produced\nvectors are derived to determine minimal orders of approximation. \n\n"}
{"id": "1805.07688", "contents": "Title: Bayesian Modeling and Computation for Analyte Quantification in Complex\n  Mixtures Using Raman Spectroscopy Abstract: In this work, we propose a two-stage algorithm based on Bayesian modeling and\ncomputation aiming at quantifying analyte concentrations or quantities in\ncomplex mixtures with Raman spectroscopy. A hierarchical Bayesian model is\nbuilt for spectral signal analysis, and reversible-jump Markov chain Monte\nCarlo (RJMCMC) computation is carried out for model selection and spectral\nvariable estimation. Processing is done in two stages. In the first stage, the\npeak representations for a target analyte spectrum are learned. In the second,\nthe peak variables learned from the first stage are used to estimate the\nconcentration or quantity of the target analyte in a mixture. Numerical\nexperiments validated its quantification performance over a wide range of\nsimulation conditions and established its advantages for analyte quantification\ntasks under the small training sample size regime over conventional\nmultivariate regression algorithms. We also used our algorithm to analyze\nexperimental spontaneous Raman spectroscopy data collected for glucose\nconcentration estimation in biopharmaceutical process monitoring applications.\nOur work shows that this algorithm can be a promising complementary tool\nalongside conventional multivariate regression algorithms in Raman\nspectroscopy-based mixture quantification studies, especially when collecting a\nlarge training dataset with high quality is challenging or resource-intensive. \n\n"}
{"id": "1805.08512", "contents": "Title: Non-parametric Structural Change Detection in Multivariate Systems Abstract: Structural change detection problems are often encountered in analytics and\neconometrics, where the performance of a model can be significantly affected by\nunforeseen changes in the underlying relationships. Although these problems\nhave a comparatively long history in statistics, the number of studies done in\nthe context of multivariate data under nonparametric settings is still small.\nIn this paper, we propose a consistent method for detecting multiple structural\nchanges in a system of related regressions over a large dimensional variable\nspace. In most applications, practitioners also do not have a priori\ninformation on the relevance of different variables, and therefore, both\nlocations of structural changes as well as the corresponding sparse regression\ncoefficients need to be estimated simultaneously. The method combines\nnonparametric energy distance minimization principle with penalized regression\ntechniques. After showing asymptotic consistency of the model, we compare the\nproposed approach with competing methods in a simulation study. As an example\nof a large scale application, we consider structural change point detection in\nthe context of news analytics during the recent financial crisis period. \n\n"}
{"id": "1805.08883", "contents": "Title: Sensitivity of Regular Estimators Abstract: This paper studies local asymptotic relationship between two scalar\nestimates. We define sensitivity of a target estimate to a control estimate to\nbe the directional derivative of the target functional with respect to the\ngradient direction of the control functional. Sensitivity according to the\ninformation metric on the model manifold is the asymptotic covariance of\nregular efficient estimators. Sensitivity according to a general policy metric\non the model manifold can be obtained from influence functions of regular\nefficient estimators. Policy sensitivity has a local counterfactual\ninterpretation, where the ceteris paribus change to a counterfactual\ndistribution is specified by the combination of a control parameter and a\nRiemannian metric on the model manifold. \n\n"}
{"id": "1805.09091", "contents": "Title: Neural networks for post-processing ensemble weather forecasts Abstract: Ensemble weather predictions require statistical post-processing of\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\nTraditionally, this is accomplished with distributional regression models in\nwhich the parameters of a predictive distribution are estimated from a training\nperiod. We propose a flexible alternative based on neural networks that can\nincorporate nonlinear relationships between arbitrary predictor variables and\nforecast distribution parameters that are automatically learned in a\ndata-driven way rather than requiring pre-specified link functions. In a case\nstudy of 2-meter temperature forecasts at surface stations in Germany, the\nneural network approach significantly outperforms benchmark post-processing\nmethods while being computationally more affordable. Key components to this\nimprovement are the use of auxiliary predictor variables and station-specific\ninformation with the help of embeddings. Furthermore, the trained neural\nnetwork can be used to gain insight into the importance of meteorological\nvariables thereby challenging the notion of neural networks as uninterpretable\nblack boxes. Our approach can easily be extended to other statistical\npost-processing and forecasting problems. We anticipate that recent advances in\ndeep learning combined with the ever-increasing amounts of model and\nobservation data will transform the post-processing of numerical weather\nforecasts in the coming decade. \n\n"}
{"id": "1805.09859", "contents": "Title: Measure of gap and inequalities in basic education students\n  proficiencies Abstract: This study uses students performance on standardized tests as evidence of the\nquality of education and introduces a methodology based on the comparison of\nperformance distributions to produce indicators for both the level achieved by\nthe students and the learning gap between social groups, two inseparable\ndimensions of quality of education. In the first case, the study compares the\ndistribution of the group observed with a reference distribution, which\nrepresents an ideal situation of where students should be. In the second, it\ncompares the performance distribution of students belonging to social groups\ndefined by socioeconomic characteristics. This article uses the\nKullback-Leibler divergence to characterize the differences between the\ndistributions. This measure takes into account types of diferences not\nconsidered by other measures and have solid conceptual justifications. The\nproposed methodology is used to describe the quality of Brazilian basic\neducation using the test results applied biannually to all Brazilian students\nof basic education. \n\n"}
{"id": "1805.09873", "contents": "Title: Concave regression: value-constrained estimation and likelihood\n  ratio-based inference Abstract: We propose a likelihood ratio statistic for forming hypothesis tests and\nconfidence intervals for a nonparametrically estimated univariate regression\nfunction, based on the shape restriction of concavity (alternatively,\nconvexity). Dealing with the likelihood ratio statistic requires studying an\nestimator satisfying a null hypothesis, that is, studying a concave\nleast-squares estimator satisfying a further equality constraint. We study this\nnull hypothesis least-squares estimator (NLSE) here, and use it to study our\nlikelihood ratio statistic. The NLSE is the solution to a convex program, and\nwe find a set of inequality and equality constraints that characterize the\nsolution. We also study a corresponding limiting version of the convex program\nbased on observing a Brownian motion with drift. The solution to the limit\nproblem is a stochastic process. We study the optimality conditions for the\nsolution to the limit problem and find that they match those we derived for the\nsolution to the finite sample problem. This allows us to show the limit\nstochastic process yields the limit distribution of the (finite sample) NLSE.\nWe conjecture that the likelihood ratio statistic is asymptotically pivotal,\nmeaning that it has a limit distribution with no nuisance parameters to be\nestimated, which makes it a very effective tool for this difficult inference\nproblem. We provide a partial proof of this conjecture, and we also provide\nsimulation evidence strongly supporting this conjecture. \n\n"}
{"id": "1805.10054", "contents": "Title: Stochastic algorithms with descent guarantees for ICA Abstract: Independent component analysis (ICA) is a widespread data exploration\ntechnique, where observed signals are modeled as linear mixtures of independent\ncomponents. From a machine learning point of view, it amounts to a matrix\nfactorization problem with a statistical independence criterion. Infomax is one\nof the most used ICA algorithms. It is based on a loss function which is a\nnon-convex log-likelihood. We develop a new majorization-minimization framework\nadapted to this loss function. We derive an online algorithm for the streaming\nsetting, and an incremental algorithm for the finite sum setting, with the\nfollowing benefits. First, unlike most algorithms found in the literature, the\nproposed methods do not rely on any critical hyper-parameter like a step size,\nnor do they require a line-search technique. Second, the algorithm for the\nfinite sum setting, although stochastic, guarantees a decrease of the loss\nfunction at each iteration. Experiments demonstrate progress on the\nstate-of-the-art for large scale datasets, without the necessity for any manual\nparameter tuning. \n\n"}
{"id": "1805.10097", "contents": "Title: Penalized polytomous ordinal logistic regression using cumulative\n  logits. Application to network inference of zero-inflated variables Abstract: We consider the problem of variable selection when the response is ordinal,\nthat is an ordered categorical variable. In particular, we are interested in\nselecting quantitative explanatory variables linked with the ordinal response\nvariable and we want to determine which predictors are relevant. In this\nframework, we choose to use the polytomous ordinal logistic regression model\nusing cumulative logits which generalizes the logistic regression. We then\nintroduce the Lasso estimation of the regression coefficients using the\nFrank-Wolfe algorithm. To deal with the choice of the penalty parameter, we use\nthe stability selection method and we develop a new method based on the\nknockoffs idea. This knockoffs method is general and suitable to any regression\nand besides, gives an order of importance of the covariates. Finally, we\nprovide some experimental results to corroborate our method. We then present an\napplication of this regression method for network inference of zero-inflated\nvariables and use it in practice on real abundance data in an agronomic\ncontext. \n\n"}
{"id": "1805.10214", "contents": "Title: Bias correction in daily maximum and minimum temperature measurements\n  through Gaussian process modeling Abstract: The Global Historical Climatology Network-Daily database contains, among\nother variables, daily maximum and minimum temperatures from weather stations\naround the globe. It is long known that climatological summary statistics based\non daily temperature minima and maxima will not be accurate, if the bias due to\nthe time at which the observations were collected is not accounted for. Despite\nsome previous work, to our knowledge, there does not exist a satisfactory\nsolution to this important problem. In this paper, we carefully detail the\nproblem and develop a novel approach to address it. Our idea is to impute the\nhourly temperatures at the location of the measurements by borrowing\ninformation from the nearby stations that record hourly temperatures, which\nthen can be used to create accurate summaries of temperature extremes. The key\ndifficulty is that these imputations of the temperature curves must satisfy the\nconstraint of falling between the observed daily minima and maxima, and\nattaining those values at least once in a twenty-four hour period. We develop a\nspatiotemporal Gaussian process model for imputing the hourly measurements from\nthe nearby stations, and then develop a novel and easy to implement Markov\nChain Monte Carlo technique to sample from the posterior distribution\nsatisfying the above constraints. We validate our imputation model using hourly\ntemperature data from four meteorological stations in Iowa, of which one is\nhidden and the data replaced with daily minima and maxima, and show that the\nimputed temperatures recover the hidden temperatures well. We also demonstrate\nthat our model can exploit information contained in the data to infer the time\nof daily measurements. \n\n"}
{"id": "1805.10244", "contents": "Title: Detecting Influence Campaigns in Social Networks Using the Ising Model Abstract: We consider the problem of identifying coordinated influence campaigns\nconducted by automated agents or bots in a social network. We study several\ndifferent Twitter datasets which contain such campaigns and find that the bots\nexhibit heterophily - they interact more with humans than with each other. We\nuse this observation to develop a probability model for the network structure\nand bot labels based on the Ising model from statistical physics. We present a\nmethod to find the maximum likelihood assignment of bot labels by solving a\nminimum cut problem. Our algorithm allows for the simultaneous detection of\nmultiple bots that are potentially engaging in a coordinated influence\ncampaign, in contrast to other methods that identify bots one at a time. We\nfind that our algorithm is able to more accurately find bots than existing\nmethods when compared to a human labeled ground truth. We also look at the\ncontent posted by the bots we identify and find that they seem to have a\ncoordinated agenda. \n\n"}
{"id": "1805.10742", "contents": "Title: High-dimensional empirical likelihood inference Abstract: High-dimensional statistical inference with general estimating equations are\nchallenging and remain less explored. In this paper, we study two problems in\nthe area: confidence set estimation for multiple components of the model\nparameters, and model specifications test. For the first one, we propose to\nconstruct a new set of estimating equations such that the impact from\nestimating the high-dimensional nuisance parameters becomes asymptotically\nnegligible. The new construction enables us to estimate a valid confidence\nregion by empirical likelihood ratio. For the second one, we propose a test\nstatistic as the maximum of the marginal empirical likelihood ratios to\nquantify data evidence against the model specification. Our theory establishes\nthe validity of the proposed empirical likelihood approaches, accommodating\nover-identification and exponentially growing data dimensionality. The\nnumerical studies demonstrate promising performance and potential practical\nbenefits of the new methods. \n\n"}
{"id": "1805.11414", "contents": "Title: Inference for ergodic diffusions plus noise Abstract: We research adaptive maximum likelihood-type estimation for an ergodic\ndiffusion process where the observation is contaminated by noise. This\nmethodology leads to the asymptotic independence of the estimators for the\nvariance of observation noise, the diffusion parameter and the drift one of the\nlatent diffusion process. Moreover, it can lessen the computational burden\ncompared to simultaneous maximum likelihood-type estimation. In addition to\nadaptive estimation, we propose a test to see if noise exists or not, and\nanalyse real data as the example such that data contains observation noise with\nstatistical significance. \n\n"}
{"id": "1805.11456", "contents": "Title: Elastic Functional Principal Component Regression Abstract: We study regression using functional predictors in situations where these\nfunctions contain both phase and amplitude variability. In other words, the\nfunctions are misaligned due to errors in time measurements, and these errors\ncan significantly degrade both model estimation and prediction performance. The\ncurrent techniques either ignore the phase variability, or handle it via\npre-processing, i.e., use an off-the-shelf technique for functional alignment\nand phase removal. We develop a functional principal component regression model\nwhich has comprehensive approach in handling phase and amplitude variability.\nThe model utilizes a mathematical representation of the data known as the\nsquare-root slope function. These functions preserve the $\\mathbf{L}^2$ norm\nunder warping and are ideally suited for simultaneous estimation of regression\nand warping parameters. Using both simulated and real-world data sets, we\ndemonstrate our approach and evaluate its prediction performance relative to\ncurrent models. In addition, we propose an extension to functional logistic and\nmultinomial logistic regression \n\n"}
{"id": "1805.11505", "contents": "Title: Classification with imperfect training labels Abstract: We study the effect of imperfect training data labels on the performance of\nclassification methods. In a general setting, where the probability that an\nobservation in the training dataset is mislabelled may depend on both the\nfeature vector and the true label, we bound the excess risk of an arbitrary\nclassifier trained with imperfect labels in terms of its excess risk for\npredicting a noisy label. This reveals conditions under which a classifier\ntrained with imperfect labels remains consistent for classifying uncorrupted\ntest data points. Furthermore, under stronger conditions, we derive detailed\nasymptotic properties for the popular $k$-nearest neighbour ($k$nn), support\nvector machine (SVM) and linear discriminant analysis (LDA) classifiers. One\nconsequence of these results is that the knn and SVM classifiers are robust to\nimperfect training labels, in the sense that the rate of convergence of the\nexcess risks of these classifiers remains unchanged; in fact, our theoretical\nand empirical results even show that in some cases, imperfect labels may\nimprove the performance of these methods. On the other hand, the LDA classifier\nis shown to be typically inconsistent in the presence of label noise unless the\nprior probabilities of each class are equal. Our theoretical results are\nsupported by a simulation study. \n\n"}
{"id": "1805.11636", "contents": "Title: Diagnosing Glaucoma Progression with Visual Field Data Using a\n  Spatiotemporal Boundary Detection Method Abstract: Diagnosing glaucoma progression is critical for limiting irreversible vision\nloss. A common method for assessing glaucoma progression uses a longitudinal\nseries of visual fields (VF) acquired at regular intervals. VF data are\ncharacterized by a complex spatiotemporal structure due to the data generating\nprocess and ocular anatomy. Thus, advanced statistical methods are needed to\nmake clinical determinations regarding progression status. We introduce a\nspatiotemporal boundary detection model that allows the underlying anatomy of\nthe optic disc to dictate the spatial structure of the VF data across time. We\nshow that our new method provides novel insight into vision loss that improves\ndiagnosis of glaucoma progression using data from the Vein Pulsation Study\nTrial in Glaucoma and the Lions Eye Institute trial registry. Simulations are\npresented, showing the proposed methodology is preferred over existing spatial\nmethods for VF data. Supplementary materials for this article are available\nonline and the method is implemented in the R package womblR. \n\n"}
{"id": "1805.12217", "contents": "Title: Introducing shrinkage in heavy-tailed state space models to predict\n  equity excess returns Abstract: We forecast S&P 500 excess returns using a flexible Bayesian econometric\nstate space model with non-Gaussian features at several levels. More precisely,\nwe control for overparameterization via novel global-local shrinkage priors on\nthe state innovation variances as well as the time-invariant part of the state\nspace model. The shrinkage priors are complemented by heavy tailed state\ninnovations that cater for potential large breaks in the latent states.\nMoreover, we allow for leptokurtic stochastic volatility in the observation\nequation. The empirical findings indicate that several variants of the proposed\napproach outperform typical competitors frequently used in the literature, both\nin terms of point and density forecasts. \n\n"}
{"id": "1805.12572", "contents": "Title: A comparison of partisan-gerrymandering measures Abstract: We compare and contrast fourteen measures that have been proposed for the\npurpose of quantifying partisan gerrymandering. We consider measures that,\nrather than examining the shapes of districts, utilize only the partisan vote\ndistribution among districts. The measures considered are two versions of\npartisan bias; the efficiency gap and several of its variants; the mean-median\ndifference and the equal vote weight standard; the declination and one variant;\nand the lopsided-means test. Our primary means of evaluating these measures is\na suite of hypothetical elections we classify from the start as fair or unfair.\nWe conclude that the declination is the most successful measure in terms of\navoiding false positives and false negatives on the elections considered. We\ninclude in an appendix the most extreme outliers for each measure among\nhistorical congressional and state legislative elections. \n\n"}
{"id": "1806.02588", "contents": "Title: Designing Experiments to Measure Incrementality on Facebook Abstract: The importance of Facebook advertising has risen dramatically in recent\nyears, with the platform accounting for almost 20% of the global online ad\nspend in 2017. An important consideration in advertising is incrementality: how\nmuch of the change in an experimental metric is an advertising campaign\nresponsible for. To measure incrementality, Facebook provide lift studies. As\nFacebook lift studies differ from standard A/B tests, the online\nexperimentation literature does not describe how to calculate parameters such\nas power and minimum sample size. Facebook also offer multi-cell lift tests,\nwhich can be used to compare campaigns that don't have statistically identical\naudiences. In this case, there is no literature describing how to measure the\nsignificance of the difference in incrementality between cells, or how to\nestimate the power or minimum sample size. We fill these gaps in the literature\nby providing the statistical power and required sample size calculation for\nFacebook lift studies. We then generalise the statistical significance, power,\nand required sample size calculation to multi-cell lift studies. We represent\nour results theoretically in terms of the distributions of test metrics and in\npractical terms relating to the metrics used by practitioners, making all of\nour code publicly available. \n\n"}
{"id": "1806.06295", "contents": "Title: Detecting intrusions in control systems: a rule of thumb, its\n  justification and illustrations Abstract: Control systems are exposed to unintentional errors, deliberate intrusions,\nfalse data injection attacks, and various other disruptions. In this paper we\npropose, justify, and illustrate a rule of thumb for detecting, or confirming\nthe absence of, such disruptions. To facilitate the use of the rule, we\nrigorously discuss background results that delineate the boundaries of the\nrule's applicability. We also discuss ways to further widen the applicability\nof the proposed intrusion-detection methodology. \n\n"}
{"id": "1806.06551", "contents": "Title: A cautionary tale on using imputation methods for inference in matched\n  pairs design Abstract: Imputation procedures in biomedical fields have turned into statistical\npractice, since further analyses can be conducted ignoring the former presence\nof missing values. In particular, non-parametric imputation schemes like the\nrandom forest or a combination with the stochastic gradient boosting have shown\nfavorable imputation performance compared to the more traditionally used MICE\nprocedure. However, their effect on valid statistical inference has not been\nanalyzed so far. This paper closes this gap by investigating their validity for\ninferring mean differences in incompletely observed pairs while opposing them\nto a recent approach that only works with the given observations at hand. Our\nfindings indicate that machine learning schemes for (multiply) imputing missing\nvalues may inflate type-I-error or result in comparably low power in small to\nmoderate matched pairs, even after modifying the test statistics using Rubin's\nmultiple imputation rule. In addition to an extensive simulation study, an\nillustrative data example from a breast cancer gene study has been considered. \n\n"}
{"id": "1806.06696", "contents": "Title: SMOGS: Social Network Metrics of Game Success Abstract: This paper develops metrics from a social network perspective that are\ndirectly translatable to the outcome of a basketball game. We extend a\nstate-of-the-art multi-resolution stochastic process approach to modeling\nbasketball by modeling passes between teammates as directed dynamic relational\nlinks on a network and introduce multiplicative latent factors to study\nhigher-order patterns in players' interactions that distinguish a successful\ngame from a loss. Parameters are estimated using a Markov chain Monte Carlo\nsampler. Results in simulation experiments suggest that the sampling scheme is\neffective in recovering the parameters. We then apply the model to the first\nhigh-resolution optical tracking dataset collected in college basketball games.\nThe learned latent factors demonstrate significant differences between players'\npassing and receiving tendencies in a loss than those in a win. The model is\napplicable to team sports other than basketball, as well as other time-varying\nnetwork observations. \n\n"}
{"id": "1806.07422", "contents": "Title: Doubly Robust Estimation in Observational Studies with Partial\n  Interference Abstract: Interference occurs when the treatment (or exposure) of one individual\naffects the outcomes of others. In some settings it may be reasonable to assume\nindividuals can be partitioned into clusters such that there is no interference\nbetween individuals in different clusters, i.e., there is partial interference.\nIn observational studies with partial interference, inverse probability\nweighted (IPW) estimators have been proposed of different possible treatment\neffects. However, the validity of IPW estimators depends on the propensity\nscore being known or correctly modeled. Alternatively, one can estimate the\ntreatment effect using an outcome regression model. In this paper, we propose\ndoubly robust (DR) estimators which utilize both models and are consistent and\nasymptotically normal if either model, but not necessarily both, is correctly\nspecified. Empirical results are presented to demonstrate the DR property of\nthe proposed estimators, as well as the efficiency gain of DR over IPW\nestimators when both models are correctly specified. The different estimators\nare illustrated using data from a study examining the effects of cholera\nvaccination in Bangladesh. \n\n"}
{"id": "1806.07605", "contents": "Title: Optimal Riemannian quantization with an application to air traffic\n  analysis Abstract: The goal of optimal quantization is to find the best approximation of a\nprobability distribution by a discrete measure with finite support. When\ndealing with empirical distributions, this boils down to finding the best\nsummary of the data by a smaller number of points, and automatically yields a\nK-means-type clustering. In this paper, we introduce Competitive Learning\nRiemannian Quantization (CLRQ), an online algorithm that computes the optimal\nsummary when the data does not belong to a vector space, but rather a\nRiemannian manifold. We prove its convergence and show simulated examples on\nthe sphere and the hyperbolic plane. We also provide an application to real\ndata by using CLRQ to create summaries of images of covariance matrices\nestimated from air traffic images. These summaries are representative of the\nair traffic complexity and yield clusterings of the airspaces into zones that\nare homogeneous with respect to that criterion. They can then be compared using\ndiscrete optimal transport and be further used as inputs of a machine learning\nalgorithm or as indexes in a traffic database. \n\n"}
{"id": "1806.08069", "contents": "Title: Deep Gaussian Process-Based Bayesian Inference for Contaminant Source\n  Localization Abstract: This paper proposes a Bayesian framework for localization of multiple sources\nin the event of accidental hazardous contaminant release. The framework\nassimilates sensor measurements of the contaminant concentration with an\nintegrated multizone computational fluid dynamics (multizone-CFD) based\ncontaminant fate and transport model. To ensure online tractability, the\nframework uses deep Gaussian process (DGP) based emulator of the multizone-CFD\nmodel. To effectively represent the transient response of the multizone-CFD\nmodel, the DGP emulator is reformulated using a matrix-variate Gaussian process\nprior. The resultant deep matrix-variate Gaussian process emulator (DMGPE) is\nused to define the likelihood of the Bayesian framework, while Markov Chain\nMonte Carlo approach is used to sample from the posterior distribution. The\nproposed method is evaluated for single and multiple contaminant sources\nlocalization tasks modeled by CONTAM simulator in a single-story building of 30\nzones, demonstrating that proposed approach accurately perform inference on\nlocations of contaminant sources. Moreover, the DMGP emulator outperforms both\nGP and DGP emulator with fewer number of hyperparameters. \n\n"}
{"id": "1806.08468", "contents": "Title: Personalized Thread Recommendation for MOOC Discussion Forums Abstract: Social learning, i.e., students learning from each other through social\ninteractions, has the potential to significantly scale up instruction in online\neducation. In many cases, such as in massive open online courses (MOOCs),\nsocial learning is facilitated through discussion forums hosted by course\nproviders. In this paper, we propose a probabilistic model for the process of\nlearners posting on such forums, using point processes. Different from existing\nworks, our method integrates topic modeling of the post text, timescale\nmodeling of the decay in post activity over time, and learner topic interest\nmodeling into a single model, and infers this information from user data. Our\nmethod also varies the excitation levels induced by posts according to the\nthread structure, to reflect typical notification settings in discussion\nforums. We experimentally validate the proposed model on three real-world MOOC\ndatasets, with the largest one containing up to 6,000 learners making 40,000\nposts in 5,000 threads. Results show that our model excels at thread\nrecommendation, achieving significant improvement over a number of baselines,\nthus showing promise of being able to direct learners to threads that they are\ninterested in more efficiently. Moreover, we demonstrate analytics that our\nmodel parameters can provide, such as the timescales of different topic\ncategories in a course. \n\n"}
{"id": "1807.01269", "contents": "Title: Estimation of component reliability from superposed renewal processes\n  with masked cause of failure by means of latent variables Abstract: In a system, there are identical replaceable components working for a given\ntask and a failed component is replaced by a functioning one in the\ncorresponding position, which characterizes a repairable system. Assuming that\na replaced component lifetime has the same lifetime distribution as the old\none, a single component position can be represented by a renewal process and\nthe multiple components positions for a single system form a superposed renewal\nprocess. When the interest consists in estimating the component lifetime\ndistribution, there are a considerable amount of works that deal with\nestimation methods for this kind of problem. However, the information about the\nexact position of the replaced component is not available, that is, a masked\ncause of failure. In this work, we propose two methods, a Bayesian and a\nmaximum likelihood function approaches, for estimating the failure time\ndistribution of components in a repairable system with a masked cause of\nfailure. As our proposed estimators consider latent variables, they yield\nbetter performance results compared to commonly used estimators from the\nliterature. The proposed models are generic and straightforward for any\nprobability distribution. Aside from point estimates, interval estimates are\npresented for both approaches. Using several simulations, the performances of\nthe proposed methods are illustrated and their efficiency and applicability are\nshown based on the so-called cylinder problem. \n\n"}
{"id": "1807.02348", "contents": "Title: Data-driven causal path discovery without prior knowledge - a benchmark\n  study Abstract: Causal discovery broadens the inference possibilities, as correlation does\nnot inform about the relationship direction. The common approaches were\nproposed for cases in which prior knowledge is desired, when the impact of a\ntreatment/intervention variable is discovered or to analyze time-related\ndependencies. In some practical applications, more universal techniques are\nneeded and have already been presented. Therefore, the aim of the study was to\nassess the accuracies in determining causal paths in a dataset without\nconsidering the ground truth and the contextual information. This benchmark was\nperformed on the database with cause-effect pairs, using a framework consisting\nof generalized correlations (GC), kernel regression gradients (GR) and absolute\nresiduals criteria (AR), along with causal additive modeling (CAM). The best\noverall accuracy, 80%, was achieved for the (majority voting) combination of\nGC, AR, and CAM, however, the most similar sensitivity and specificity values\nwere obtained for AR. Bootstrap simulation established the probability of\ncorrect causal path determination (which pairs should remain indeterminate).\nThe mean accuracy was then improved to 83% for the selected subset of pairs.\nThe described approach can be used for preliminary dependence assessment, as an\ninitial step for commonly used causality assessment frameworks or for\ncomparison with prior assumptions. \n\n"}
{"id": "1807.03419", "contents": "Title: On Causal Discovery with Equal Variance Assumption Abstract: Prior work has shown that causal structure can be uniquely identified from\nobservational data when these follow a structural equation model whose error\nterms have equal variances. We show that this fact is implied by an ordering\namong (conditional) variances. We demonstrate that ordering estimates of these\nvariances yields a simple yet state-of-the-art method for causal structure\nlearning that is readily extendable to high-dimensional problems. \n\n"}
{"id": "1807.04431", "contents": "Title: Statistical Inference with Local Optima Abstract: We study the statistical properties of an estimator derived by applying a\ngradient ascent method with multiple initializations to a multi-modal\nlikelihood function. We derive the population quantity that is the target of\nthis estimator and study the properties of confidence intervals (CIs)\nconstructed from asymptotic normality and the bootstrap approach. In\nparticular, we analyze the coverage deficiency due to finite number of random\ninitializations. We also investigate the CIs by inverting the likelihood ratio\ntest, the score test, and the Wald test, and we show that the resulting CIs may\nbe very different. We propose a two-sample test procedure even when the MLE is\nintractable. In addition, we analyze the performance of the EM algorithm under\nrandom initializations and derive the coverage of a CI with a finite number of\ninitializations. \n\n"}
{"id": "1807.05405", "contents": "Title: The conditional permutation test for independence while controlling for\n  confounders Abstract: We propose a general new method, the conditional permutation test, for\ntesting the conditional independence of variables $X$ and $Y$ given a\npotentially high-dimensional random vector $Z$ that may contain confounding\nfactors. The proposed test permutes entries of $X$ non-uniformly, so as to\nrespect the existing dependence between $X$ and $Z$ and thus account for the\npresence of these confounders. Like the conditional randomization test of\nCand\\`es et al. (2018), our test relies on the availability of an approximation\nto the distribution of $X \\mid Z$. While Cand\\`es et al. (2018)'s test uses\nthis estimate to draw new $X$ values, for our test we use this approximation to\ndesign an appropriate non-uniform distribution on permutations of the $X$\nvalues already seen in the true data. We provide an efficient Markov Chain\nMonte Carlo sampler for the implementation of our method, and establish bounds\non the Type I error in terms of the error in the approximation of the\nconditional distribution of $X\\mid Z$, finding that, for the worst case test\nstatistic, the inflation in Type I error of the conditional permutation test is\nno larger than that of the conditional randomization test. We validate these\ntheoretical results with experiments on simulated data and on the Capital\nBikeshare data set. \n\n"}
{"id": "1807.05600", "contents": "Title: Modeling Daily Seasonality of Mexico City Ozone using Nonseparable\n  Covariance Models on Circles Cross Time Abstract: Mexico City tracks ground-level ozone levels to assess compliance with\nnational ambient air quality standards and to prevent environmental health\nemergencies. Ozone levels show distinct daily patterns, within the city, and\nover the course of the year. To model these data, we use covariance models over\nspace, circular time, and linear time. We review existing models and develop\nnew classes of nonseparable covariance models of this type, models appropriate\nfor quasi-periodic data collected at many locations. With these covariance\nmodels, we use nearest-neighbor Gaussian processes to predict hourly ozone\nlevels at unobserved locations in April and May, the peak ozone season, to\ninfer compliance to Mexican air quality standards and to estimate respiratory\nhealth risk associated with ozone. Predicted compliance with air quality\nstandards and estimated respiratory health risk vary greatly over space and\ntime. In some regions, we predict exceedance of national standards for more\nthan a third of the hours in April and May. On many days, we predict that\nnearly all of Mexico City exceeds nationally legislated ozone thresholds at\nleast once. In peak regions, we estimate respiratory risk for ozone to be 55%\nhigher on average than the annual average risk and as much at 170% higher on\nsome days. \n\n"}
{"id": "1807.05834", "contents": "Title: A Statistical Approach to Inferring Business Locations Based on Purchase\n  Behavior Abstract: Transaction data obtained by Personal Financial Management (PFM) services\nfrom financial institutes such as banks and credit card companies contain a\ndescription string from which the merchant, and an encoded store identifier may\nbe parsed. However, the physical location of the purchase is absent from this\ndescription. In this paper we present a method designed to recover this\nvaluable spatial information and map merchant and identifier tuples to physical\nmap locations. We begin by constructing a graph of customer sharing between\nbusinesses, and based on a small set of known \"seed\" locations we formulate\nthis task as a maximum likelihood problem based on a model of customer sharing\nbetween nearby businesses. We test our method extensively on real world data\nand provide statistics on the displacement error in many cities. \n\n"}
{"id": "1807.06143", "contents": "Title: Quickest Detection of Dynamic Events in Networks Abstract: The problem of quickest detection of dynamic events in networks is studied.\nAt some unknown time, an event occurs, and a number of nodes in the network are\naffected by the event, in that they undergo a change in the statistics of their\nobservations. It is assumed that the event is dynamic, in that it can propagate\nalong the edges in the network, and affect more and more nodes with time. The\nevent propagation dynamics is assumed to be unknown. The goal is to design a\nsequential algorithm that can detect a \"significant\" event, i.e., when the\nevent has affected no fewer than $\\eta$ nodes, as quickly as possible, while\ncontrolling the false alarm rate. Fully connected networks are studied first,\nand the results are then extended to arbitrarily connected networks. The\ndesigned algorithms are shown to be adaptive to the unknown propagation\ndynamics, and their first-order asymptotic optimality is demonstrated as the\nfalse alarm rate goes to zero. The algorithms can be implemented with linear\ncomputational complexity in the network size at each time step, which is\ncritical for online implementation. Numerical simulations are provided to\nvalidate the theoretical results. \n\n"}
{"id": "1807.07797", "contents": "Title: The Sliding Window Discrete Fourier Transform Abstract: This paper introduces a new tool for time-series analysis: the Sliding Window\nDiscrete Fourier Transform (SWDFT). The SWDFT is especially useful for\ntime-series with local- in-time periodic components. We define a 5-parameter\nmodel for noiseless local periodic signals, then study the SWDFT of this model.\nOur study illustrates several key concepts crucial to analyzing time-series\nwith the SWDFT, in particular Aliasing, Leakage, and Ringing. We also show how\nthese ideas extend to R > 1 local periodic components, using the linearity\nproperty of the Fourier transform. Next, we propose a simple procedure for\nestimating the 5 parameters of our local periodic signal model using the SWDFT.\nOur estimation procedure speeds up computation by using a trigonometric\nidentity that linearizes estimation of 2 of the 5 parameters. We conclude with\na very small Monte Carlo simulation study of our estimation procedure under\ndifferent levels of noise. \n\n"}
{"id": "1807.07911", "contents": "Title: Application of the Iterated Weighted Least-Squares Fit to counting\n  experiments Abstract: Least-squares fits are an important tool in many data analysis applications.\nIn this paper, we review theoretical results, which are relevant for their\napplication to data from counting experiments. Using a simple example, we\nillustrate the well known fact that commonly used variants of the least-squares\nfit applied to Poisson-distributed data produce biased estimates. The bias can\nbe overcome with an iterated weighted least-squares method, which produces\nresults identical to the maximum-likelihood method. For linear models, the\niterated weighted least-squares method converges faster than the equivalent\nmaximum-likelihood method, and does not require problem-specific starting\nvalues, which may be a practical advantage. The equivalence of both methods\nalso holds for binomially distributed data. We further show that the unbinned\nmaximum-likelihood method can be derived as a limiting case of the iterated\nleast-squares fit when the bin width goes to zero, which demonstrates a deep\nconnection between the two methods. \n\n"}
{"id": "1807.07968", "contents": "Title: Scaling in the eigenvalue fluctuations of the empirical correlation\n  matrices Abstract: The spectra of empirical correlation matrices, constructed from multivariate\ndata, are widely used in many areas of sciences, engineering and social\nsciences as a tool to understand the information contained in typically large\ndatasets. In the last two decades, random matrix theory-based tools such as the\nnearest neighbour eigenvalue spacing and eigenvector distributions have been\nemployed to extract the significant modes of variability present in such\nempirical correlations. In this work, we present an alternative analysis in\nterms of the recently introduced spacing ratios, which does not require the\ncumbersome unfolding process. It is shown that the higher order spacing ratio\ndistributions for the Wishart ensemble of random matrices, characterized by the\nDyson index $\\beta$, is related to the first order spacing ratio distribution\nwith a modified value of co-dimension $\\beta'$. This scaling is demonstrated\nfor Wishart ensemble and also for the spectra of empirical correlation matrices\ndrawn from the observed stock market and atmospheric pressure data. Using a\ncombination of analytical and numerics, such scalings in spacing distributions\nare also discussed. \n\n"}
{"id": "1807.08018", "contents": "Title: Information estimation using nonparametric copulas Abstract: Estimation of mutual information between random variables has become crucial\nin a range of fields, from physics to neuroscience to finance. Estimating\ninformation accurately over a wide range of conditions relies on the\ndevelopment of flexible methods to describe statistical dependencies among\nvariables, without imposing potentially invalid assumptions on the data. Such\nmethods are needed in cases that lack prior knowledge of their statistical\nproperties and that have limited sample numbers. Here we propose a powerful and\ngenerally applicable information estimator based on non-parametric copulas.\nThis estimator, called the non-parametric copula-based estimator (NPC), is\ntailored to take into account detailed stochastic relationships in the data\nindependently of the data's marginal distributions. The NPC estimator can be\nused both for continuous and discrete numerical variables and thus provides a\nsingle framework for the mutual information estimation of both continuous and\ndiscrete data. By extensive validation on artificial samples drawn from various\nstatistical distributions, we found that the NPC estimator compares well\nagainst commonly used alternatives. Unlike methods not based on copulas, it\nallows an estimation of information that is robust to changes of the details of\nthe marginal distributions. Unlike parametric copula methods, it remains\naccurate regardless of the precise form of the interactions between the\nvariables. In addition, the NPC estimator had accurate information estimates\neven at low sample numbers, in comparison to alternative estimators. The NPC\nestimator therefore provides a good balance between general applicability to\narbitrarily shaped statistical dependencies in the data and shows accurate and\nrobust performance when working with small sample sizes. \n\n"}
{"id": "1807.08513", "contents": "Title: Geostatistical modeling to capture seismic-shaking patterns from\n  earthquake-induced landslides Abstract: In this paper, we investigate earthquake-induced landslides using a\ngeostatistical model that includes a latent spatial effect (LSE). The LSE\nrepresents the spatially structured residuals in the data, which are\ncomplementary to the information carried by the covariates. To determine\nwhether the LSE can capture the residual signal from a given trigger, we test\nwhether the LSE is able to capture the pattern of seismic shaking caused by an\nearthquake from the distribution of seismically induced landslides, without\nprior knowledge of the earthquake being included in the statistical model. We\nassess the landslide intensity, i.e., the expected number of landslide\nactivations per mapping unit, for the area in which landslides triggered by the\nWenchuan (M 7.9, May 12, 2008) and Lushan (M 6.6, April 20, 2013) earthquakes\noverlap. We chose an area of overlapping landslides in order to test our method\non landslide inventories located in the near and far fields of the earthquake.\nWe generated three different models for each earthquake-induced landslide\nscenario: i) seismic parameters only (as a proxy for the trigger); ii) the LSE\nonly; and iii) both seismic parameters and the LSE. The three configurations\nshare the same set of morphometric covariates. This allowed us to study the\npattern in the LSE and assess whether it adequately approximated the effects of\nseismic wave propagation. Moreover, it allowed us to check whether the LSE\ncaptured effects that are not explained by the shaking levels, such as\ntopographic amplification. Our results show that the LSE reproduced the shaking\npatterns in space for both earthquakes with a level of spatial detail even\ngreater than the seismic parameters. In addition, the models including the LSE\nperform better than conventional models featuring seismic parameters only. \n\n"}
{"id": "1807.10869", "contents": "Title: Residual Balancing: A Method of Constructing Weights for Marginal\n  Structural Models Abstract: When making causal inferences, post-treatment confounders complicate analyses\nof time-varying treatment effects. Conditioning on these variables naively to\nestimate marginal effects may inappropriately block causal pathways and may\ninduce spurious associations between treatment and the outcome, leading to\nbias. To avoid such bias, researchers often use marginal structural models\n(MSMs) with inverse probability weighting (IPW). However, IPW requires models\nfor the conditional distributions of treatment and is highly sensitive to their\nmisspecification. Moreover, IPW is relatively inefficient, susceptible to\nfinite-sample bias, and difficult to use with continuous treatments. We\nintroduce an alternative method of constructing weights for MSMs, which we call\n\"residual balancing.\" In contrast to IPW, it requires modeling the conditional\nmeans of the post-treatment confounders rather than the conditional\ndistributions of treatment, and it is therefore easier to use with continuous\nexposures. Numeric simulations suggest that residual balancing is both more\nefficient and more robust to model misspecification than IPW and its variants.\nWe illustrate the method by estimating (a) the cumulative effect of negative\nadvertising on election outcomes and (b) the controlled direct effect of shared\ndemocracy on public support for war. Open source software is available for\nimplementing the proposed method. \n\n"}
{"id": "1807.11143", "contents": "Title: ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks Abstract: To backpropagate the gradients through stochastic binary layers, we propose\nthe augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low\nvariance, and has low computational complexity. Exploiting variable\naugmentation, REINFORCE, and reparameterization, the ARM estimator achieves\nadaptive variance reduction for Monte Carlo integration by merging two\nexpectations via common random numbers. The variance-reduction mechanism of the\nARM estimator can also be attributed to either antithetic sampling in an\naugmented space, or the use of an optimal anti-symmetric \"self-control\"\nbaseline function together with the REINFORCE estimator in that augmented\nspace. Experimental results show the ARM estimator provides state-of-the-art\nperformance in auto-encoding variational inference and maximum likelihood\nestimation, for discrete latent variable models with one or multiple stochastic\nbinary layers. Python code for reproducible research is publicly available. \n\n"}
{"id": "1807.11591", "contents": "Title: The Efficiency of Geometric Samplers for Exoplanet Transit Timing\n  Variation Models Abstract: Transit timing variations (TTVs) are a valuable tool to determine the masses\nand orbits of transiting planets in multi-planet systems. TTVs can be readily\nmodeled given knowledge of the interacting planets' orbital configurations and\nplanet-star mass ratios, but such models are highly nonlinear and difficult to\ninvert. Markov chain Monte Carlo (MCMC) methods are often used to explore the\nposterior distribution for model parameters, but, due to the high correlations\nbetween parameters, nonlinearity, and potential multi-modality in the\nposterior, many samplers perform very inefficiently. Therefore, we assess the\nperformance of several MCMC samplers that use varying degrees of geometric\ninformation about the target distribution. We generate synthetic datasets from\nmultiple models, including the TTVFaster model and a simple sinusoidal model,\nand test the efficiencies of various MCMC samplers. We find that sampling\nefficiency can be greatly improved for all models by sampling from a parameter\nspace transformed using an estimate of the covariance and means of the target\ndistribution. No one sampler performs the best for all datasets, but several\nsamplers, such as Differential Evolution Monte Carlo and Geometric adaptive\nMonte Carlo, have consistently efficient performance. For datasets with near\nGaussian posteriors, Hamiltonian Monte Carlo samplers with 2 or 3 leapfrog\nsteps obtained the highest efficiencies. Based on differences in effective\nsample sizes per time, we show that the right choice of sampler can improve\nsampling efficiencies by several orders of magnitude. \n\n"}
{"id": "1807.11743", "contents": "Title: Modeling joint probability distribution of yield curve parameters Abstract: US Yield curve has recently collapsed to its most flattened level since\nsubprime crisis and is close to the inversion. This fact has gathered attention\nof investors around the world and revived the discussion of proper modeling and\nforecasting yield curve, since changes in interest rate structure are believed\nto represent investors expectations about the future state of economy and have\nforeshadowed recessions in the United States. While changes in term structure\nof interest rates are relatively easy to interpret they are however very\ndifficult to model and forecast due to no proper economic theory underlying\nsuch events. Yield curves are usually represented by multivariate sparse time\nseries, at any point in time infinite dimensional curve is portrayed via\nrelatively few points in a multivariate space of data and as a consequence\nmultimodal statistical dependencies behind these curves are relatively hard to\nextract and forecast via typical multivariate statistical methods.We propose to\nmodel yield curves via reconstruction of joint probability distribution of\nparameters in functional space as a high degree polynomial. Thanks to adoption\nof an orthonormal basis, the MSE estimation of coefficients of a given function\nis an average over a data sample in the space of functions. Since such\npolynomial coefficients are independent and have cumulant-like interpretation\nie.describe corresponding perturbation from an uniform joint distribution, our\napproach can also be extended to any d-dimensional space of yield curve\nparameters (also in neighboring times) due to controllable accuracy. We believe\nthat this approach to modeling of local behavior of a sparse multivariate\ncurved time series can complement prediction from standard models like ARIMA,\nthat are using long range dependencies, but provide only inaccurate prediction\nof probability distribution, often as just Gaussian with constant width. \n\n"}
{"id": "1807.11887", "contents": "Title: Gaussian Process Landmarking for Three-Dimensional Geometric\n  Morphometrics Abstract: We demonstrate applications of the Gaussian process-based landmarking\nalgorithm proposed in [T. Gao, S.Z. Kovalsky, and I. Daubechies, SIAM Journal\non Mathematics of Data Science (2019)] to geometric morphometrics, a branch of\nevolutionary biology centered at the analysis and comparisons of anatomical\nshapes, and compares the automatically sampled landmarks with the \"ground\ntruth\" landmarks manually placed by evolutionary anthropologists; the results\nsuggest that Gaussian process landmarks perform equally well or better, in\nterms of both spatial coverage and downstream statistical analysis. We provide\na detailed exposition of numerical procedures and feature filtering algorithms\nfor computing high-quality and semantically meaningful diffeomorphisms between\ndisk-type anatomical surfaces. \n\n"}
{"id": "1808.00036", "contents": "Title: Scalable Multi-Task Gaussian Process Tensor Regression for Normative\n  Modeling of Structured Variation in Neuroimaging Data Abstract: Most brain disorders are very heterogeneous in terms of their underlying\nbiology and developing analysis methods to model such heterogeneity is a major\nchallenge. A promising approach is to use probabilistic regression methods to\nestimate normative models of brain function using (f)MRI data then use these to\nmap variation across individuals in clinical populations (e.g., via anomaly\ndetection). To fully capture individual differences, it is crucial to\nstatistically model the patterns of correlation across different brain regions\nand individuals. However, this is very challenging for neuroimaging data\nbecause of high-dimensionality and highly structured patterns of correlation\nacross multiple axes. Here, we propose a general and flexible multi-task\nlearning framework to address this problem. Our model uses a tensor-variate\nGaussian process in a Bayesian mixed-effects model and makes use of Kronecker\nalgebra and a low-rank approximation to scale efficiently to multi-way\nneuroimaging data at the whole brain level. On a publicly available clinical\nfMRI dataset, we show that our computationally affordable approach\nsubstantially improves detection sensitivity over both a mass-univariate\nnormative model and a classifier that --unlike our approach-- has full access\nto the clinical labels. \n\n"}
{"id": "1808.02430", "contents": "Title: Granger Causality Analysis Based on Quantized Minimum Error Entropy\n  Criterion Abstract: Linear regression model (LRM) based on mean square error (MSE) criterion is\nwidely used in Granger causality analysis (GCA), which is the most commonly\nused method to detect the causality between a pair of time series. However,\nwhen signals are seriously contaminated by non-Gaussian noises, the LRM\ncoefficients will be inaccurately identified. This may cause the GCA to detect\na wrong causal relationship. Minimum error entropy (MEE) criterion can be used\nto replace the MSE criterion to deal with the non-Gaussian noises. But its\ncalculation requires a double summation operation, which brings computational\nbottlenecks to GCA especially when sizes of the signals are large. To address\nthe aforementioned problems, in this study we propose a new method called GCA\nbased on the quantized MEE (QMEE) criterion (GCA-QMEE), in which the QMEE\ncriterion is applied to identify the LRM coefficients and the quantized error\nentropy is used to calculate the causality indexes. Compared with the\ntraditional GCA, the proposed GCA-QMEE not only makes the results more\ndiscriminative, but also more robust. Its computational complexity is also not\nhigh because of the quantization operation. Illustrative examples on synthetic\nand EEG datasets are provided to verify the desirable performance and the\navailability of the GCA-QMEE. \n\n"}
{"id": "1808.02763", "contents": "Title: Bayesreef: A Bayesian inference framework for modelling reef growth in\n  response to environmental change and biological dynamics Abstract: Estimating the impact of environmental processes on vertical reef development\nin geological time is a very challenging task. pyReef-Core is a deterministic\ncarbonate stratigraphic forward model designed to simulate the key biological\nand environmental processes that determine vertical reef accretion and\nassemblage changes in fossil reef drill cores. We present a Bayesian framework\ncalled Bayesreef for the estimation and uncertainty quantification of\nparameters in pyReef-Core that represent environmental conditions affecting the\ngrowth of coral assemblages on geological timescales. We demonstrate the\nexistence of multimodal posterior distributions and investigate the challenges\nof sampling using Markov chain Monte-Carlo (MCMC) methods, which includes\nparallel tempering MCMC. We use synthetic reef-core to investigate fundamental\nissues and then apply the methodology to a selected reef-core from the Great\nBarrier Reef in Australia. The results show that Bayesreef accurately estimates\nand provides uncertainty quantification of the selected parameters that\nrepresent the environment and ecological conditions in pyReef-Core. Bayesreef\nprovides insights into the complex posterior distributions of parameters in\npyReef-Core, which provides the groundwork for future research in this area. \n\n"}
{"id": "1808.03109", "contents": "Title: Change Point Estimation in Panel Data with Time-Varying Individual\n  Effects Abstract: This paper proposes a method for estimating multiple change points in panel\ndata models with unobserved individual effects via ordinary least-squares\n(OLS). Typically, in this setting, the OLS slope estimators are inconsistent\ndue to the unobserved individual effects bias. As a consequence, existing\nmethods remove the individual effects before change point estimation through\ndata transformations such as first-differencing. We prove that under reasonable\nassumptions, the unobserved individual effects bias has no impact on the\nconsistent estimation of change points. Our simulations show that since our\nmethod does not remove any variation in the dataset before change point\nestimation, it performs better in small samples compared to first-differencing\nmethods. We focus on short panels because they are commonly used in practice,\nand allow for the unobserved individual effects to vary over time. Our method\nis illustrated via two applications: the environmental Kuznets curve and the\nU.S. house price expectations after the financial crisis. \n\n"}
{"id": "1808.03786", "contents": "Title: Improved Methods for Moment Restriction Models with Marginally\n  Incompatible Data Combination and an Application to Two-sample Instrumental\n  Variable Estimation Abstract: Combining information from multiple samples is often needed in biomedical and\neconomic studies, but the differences between these samples must be\nappropriately taken into account in the analysis of the combined data. We study\nestimation for moment restriction models with data combination from two samples\nunder an ignorablility-type assumption but allowing for different marginal\ndistributions of common variables between the two samples. Suppose that an\noutcome regression model and a propensity score model are specified. By\nleveraging the semiparametric efficiency theory, we derive an augmented inverse\nprobability weighted (AIPW) estimator that is locally efficient and doubly\nrobust with respect to the outcome regression and propensity score models.\nFurthermore, we develop calibrated regression and likelihood estimators that\nare not only locally efficient and doubly robust, but also intrinsically\nefficient in achieving smaller variances than the AIPW estimator when the\npropensity score model is correctly specified but the outcome regression model\nmay be misspecified. As an important application, we study the two-sample\ninstrumental variable problem and derive the corresponding estimators while\nallowing for incompatible distributions of common variables between the two\nsamples. Finally, we provide a simulation study and an econometric application\non public housing projects to demonstrate the superior performance of our\nimproved estimators. \n\n"}
{"id": "1808.04312", "contents": "Title: Analysing Multiple Epidemic Data Sources Abstract: Evidence-based knowledge of infectious disease burden, including prevalence,\nincidence, severity and transmission, in different population strata and\nlocations, and possibly in real time, is crucial to the planning and evaluation\nof public health policies. Direct observation of a disease process is rarely\npossible. However, latent characteristics of an epidemic and its evolution can\noften be inferred from the synthesis of indirect information from various\nroutine data sources, as well as expert opinion. The simultaneous synthesis of\nmultiple data sources, often conveniently carried out in a Bayesian framework,\nposes a number of statistical and computational challenges: the heterogeneity\nin type, relevance and granularity of the data, together with selection and\ninformative observation biases, lead to complex probabilistic models that are\ndifficult to build and fit, and challenging to criticize. Using motivating case\nstudies of influenza, this chapter illustrates the cycle of model development\nand criticism in the context of Bayesian evidence synthesis, highlighting the\nchallenges of complex model building, computationally efficient inference, and\nconflicting evidence. \n\n"}
{"id": "1808.04360", "contents": "Title: Stochastic on-time arrival problem in transit networks Abstract: This article considers the stochastic on-time arrival problem in transit\nnetworks where both the travel time and the waiting time for transit services\nare stochastic. A specific challenge of this problem is the combinatorial\nsolution space due to the unknown ordering of transit line arrivals. We propose\na network structure appropriate to the online decision-making of a passenger,\nincluding boarding, waiting and transferring. In this framework, we design a\ndynamic programming algorithm that is pseudo-polynomial in the number of\ntransit stations and travel time budget, and exponential in the number of\ntransit lines at a station, which is a small number in practice. To reduce the\nsearch space, we propose a definition of transit line dominance, and techniques\nto identify dominance, which decrease the computation time by up to 90% in\nnumerical experiments. Extensive numerical experiments are conducted on both a\nsynthetic network and the Chicago transit network. \n\n"}
{"id": "1808.04401", "contents": "Title: Horseshoe-based Bayesian nonparametric estimation of effective\n  population size trajectories Abstract: Phylodynamics is an area of population genetics that uses genetic sequence\ndata to estimate past population dynamics. Modern state-of-the-art Bayesian\nnonparametric methods for recovering population size trajectories of unknown\nform use either change-point models or Gaussian process priors. Change-point\nmodels suffer from computational issues when the number of change-points is\nunknown and needs to be estimated. Gaussian process-based methods lack local\nadaptivity and cannot accurately recover trajectories that exhibit features\nsuch as abrupt changes in trend or varying levels of smoothness. We propose a\nnovel, locally-adaptive approach to Bayesian nonparametric phylodynamic\ninference that has the flexibility to accommodate a large class of functional\nbehaviors. Local adaptivity results from modeling the log-transformed effective\npopulation size a priori as a horseshoe Markov random field, a recently\nproposed statistical model that blends together the best properties of the\nchange-point and Gaussian process modeling paradigms. We use simulated data to\nassess model performance, and find that our proposed method results in reduced\nbias and increased precision when compared to contemporary methods. We also use\nour models to reconstruct past changes in genetic diversity of human hepatitis\nC virus in Egypt and to estimate population size changes of ancient and modern\nsteppe bison. These analyses show that our new method captures features of the\npopulation size trajectories that were missed by the state-of-the-art methods. \n\n"}
{"id": "1808.04871", "contents": "Title: Rao-Blackwellizing Field Goal Percentage Abstract: Shooting skill in the NBA is typically measured by field goal percentage\n(FG%) - the number of makes out of the total number of shots. Even more\nadvanced metrics like true shooting percentage are calculated by counting each\nplayer's 2-point, 3-point, and free throw makes and misses, ignoring the\nspatiotemporal data now available (Kubatko et al. 2007). In this paper we aim\nto better characterize player shooting skill by introducing a new estimator\nbased on post-shot release shot-make probabilities. Via the Rao-Blackwell\ntheorem, we propose a shot-make probability model that conditions probability\nestimates on shot trajectory information, thereby reducing the variance of the\nnew estimator relative to standard FG%. We obtain shooting information by using\noptical tracking data to estimate three factors for each shot: entry angle,\nshot depth, and left-right accuracy. Next we use these factors to model\nshot-make probabilities for all shots in the 2014-15 season, and use these\nprobabilities to produce a Rao-Blackwellized FG% estimator (RB-FG%) for each\nplayer. We demonstrate that RB-FG% is better than raw FG% at predicting 3-point\nshooting and true-shooting percentages. Overall, we find that conditioning\nshot-make probabilities on spatial trajectory information stabilizes inference\nof FG%, creating the potential to estimate shooting statistics earlier in a\nseason than was previously possible. \n\n"}
{"id": "1808.06316", "contents": "Title: Discovering Context Specific Causal Relationships Abstract: With the increasing need of personalised decision making, such as\npersonalised medicine and online recommendations, a growing attention has been\npaid to the discovery of the context and heterogeneity of causal relationships.\nMost existing methods, however, assume a known cause (e.g. a new drug) and\nfocus on identifying from data the contexts of heterogeneous effects of the\ncause (e.g. patient groups with different responses to the new drug). There is\nno approach to efficiently detecting directly from observational data context\nspecific causal relationships, i.e. discovering the causes and their contexts\nsimultaneously. In this paper, by taking the advantages of highly efficient\ndecision tree induction and the well established causal inference framework, we\npropose the Tree based Context Causal rule discovery (TCC) method, for\nefficient exploration of context specific causal relationships from data.\nExperiments with both synthetic and real world data sets show that TCC can\neffectively discover context specific causal rules from the data. \n\n"}
{"id": "1808.07804", "contents": "Title: Transfer Learning for Estimating Causal Effects using Neural Networks Abstract: We develop new algorithms for estimating heterogeneous treatment effects,\ncombining recent developments in transfer learning for neural networks with\ninsights from the causal inference literature. By taking advantage of transfer\nlearning, we are able to efficiently use different data sources that are\nrelated to the same underlying causal mechanisms. We compare our algorithms\nwith those in the extant literature using extensive simulation studies based on\nlarge-scale voter persuasion experiments and the MNIST database. Our methods\ncan perform an order of magnitude better than existing benchmarks while using a\nfraction of the data. \n\n"}
{"id": "1808.08507", "contents": "Title: Mallows Ranking Models: Maximum Likelihood Estimate and Regeneration Abstract: This paper is concerned with various Mallows ranking models. We study the\nstatistical properties of the MLE of Mallows' $\\phi$ model. We also make\nconnections of various Mallows ranking models, encompassing recent progress in\nmathematics. Motivated by the infinite top-$t$ ranking model, we propose an\nalgorithm to select the model size $t$ automatically. The key idea relies on\nthe renewal property of such an infinite random permutation. Our algorithm\nshows good performance on several data sets. \n\n"}
{"id": "1808.08683", "contents": "Title: Regression adjustments for estimating the global treatment effect in\n  experiments with interference Abstract: Standard estimators of the global average treatment effect can be biased in\nthe presence of interference. This paper proposes regression adjustment\nestimators for removing bias due to interference in Bernoulli randomized\nexperiments. We use a fitted model to predict the counterfactual outcomes of\nglobal control and global treatment. Our work differs from standard regression\nadjustments in that the adjustment variables are constructed from functions of\nthe treatment assignment vector, and that we allow the researcher to use a\ncollection of any functions correlated with the response, turning the problem\nof detecting interference into a feature engineering problem. We characterize\nthe distribution of the proposed estimator in a linear model setting and\nconnect the results to the standard theory of regression adjustments under\nSUTVA. We then propose an estimator that allows for flexible machine learning\nestimators to be used for fitting a nonlinear interference functional form. We\npropose conducting statistical inference via bootstrap and resampling methods,\nwhich allow us to sidestep the complicated dependences implied by interference\nand instead rely on empirical covariance structures. Such variance estimation\nrelies on an exogeneity assumption akin to the standard unconfoundedness\nassumption invoked in observational studies. In simulation experiments, our\nmethods are better at debiasing estimates than existing inverse propensity\nweighted estimators based on neighborhood exposure modeling. We use our method\nto reanalyze an experiment concerning weather insurance adoption conducted on a\ncollection of villages in rural China. \n\n"}
{"id": "1808.09254", "contents": "Title: Estimating seal pup production in the Greenland Sea using Bayesian\n  hierarchical modeling Abstract: The Greenland Sea is an important breeding ground for harp and hooded seals.\nEstimates of the annual seal pup production are critical factors in the\nabundance estimation needed for management of the species. These estimates are\nusually based on counts from aerial photographic surveys. However, only a minor\npart of the whelping region can be photographed, due to its large extent. To\nestimate the total seal pup production, we propose a Bayesian hierarchical\nmodeling approach motivated by viewing the seal pup appearances as a\nrealization of a log-Gaussian Cox process using covariate information from\nsatellite imagery as a proxy for ice thickness. For inference, we utilize the\nstochastic partial differential equation (SPDE) module of the integrated nested\nLaplace approximation (INLA) framework. In a case study using survey data from\n2012, we compare our results with existing methodology in a comprehensive\ncross-validation study. The results of the study indicate that our method\nimproves local estimation performance, and that the increased prediction\nuncertainty of our method is required to obtain calibrated count predictions.\nThis suggests that the sampling density of the survey design may not be\nsufficient to obtain reliable estimates of the seal pup production. \n\n"}
{"id": "1808.09379", "contents": "Title: A transport-based multifidelity preconditioner for Markov chain Monte\n  Carlo Abstract: Markov chain Monte Carlo (MCMC) sampling of posterior distributions arising\nin Bayesian inverse problems is challenging when evaluations of the forward\nmodel are computationally expensive. Replacing the forward model with a\nlow-cost, low-fidelity model often significantly reduces computational cost;\nhowever, employing a low-fidelity model alone means that the stationary\ndistribution of the MCMC chain is the posterior distribution corresponding to\nthe low-fidelity model, rather than the original posterior distribution\ncorresponding to the high-fidelity model. We propose a multifidelity approach\nthat combines, rather than replaces, the high-fidelity model with a\nlow-fidelity model. First, the low-fidelity model is used to construct a\ntransport map that deterministically couples a reference Gaussian distribution\nwith an approximation of the low-fidelity posterior. Then, the high-fidelity\nposterior distribution is explored using a non-Gaussian proposal distribution\nderived from the transport map. This multifidelity \"preconditioned\" MCMC\napproach seeks efficient sampling via a proposal that is explicitly tailored to\nthe posterior at hand and that is constructed efficiently with the low-fidelity\nmodel. By relying on the low-fidelity model only to construct the proposal\ndistribution, our approach guarantees that the stationary distribution of the\nMCMC chain is the high-fidelity posterior. In our numerical examples, our\nmultifidelity approach achieves significant speedups compared to\nsingle-fidelity MCMC sampling methods. \n\n"}
{"id": "1808.10506", "contents": "Title: Maximum Entropy Principle Analysis in Network Systems with Short-time\n  Recordings Abstract: In many realistic systems, maximum entropy principle (MEP) analysis provides\nan effective characterization of the probability distribution of network\nstates. However, to implement the MEP analysis, a sufficiently long-time data\nrecording in general is often required, e.g., hours of spiking recordings of\nneurons in neuronal networks. The issue of whether the MEP analysis can be\nsuccessfully applied to network systems with data from short recordings has yet\nto be fully addressed. In this work, we investigate relationships underlying\nthe probability distributions, moments, and effective interactions in the MEP\nanalysis and then show that, with short recordings of network dynamics, the MEP\nanalysis can be applied to reconstructing probability distributions of network\nstates under the condition of asynchronous activity of nodes in the network.\nUsing spike trains obtained from both Hodgkin-Huxley neuronal networks and\nelectrophysiological experiments, we verify our results and demonstrate that\nMEP analysis provides a tool to investigate the neuronal population coding\nproperties, even for short recordings. \n\n"}
{"id": "1808.10868", "contents": "Title: Generalized probabilistic principal component analysis of correlated\n  data Abstract: Principal component analysis (PCA) is a well-established tool in machine\nlearning and data processing. The principal axes in PCA were shown to be\nequivalent to the maximum marginal likelihood estimator of the factor loading\nmatrix in a latent factor model for the observed data, assuming that the latent\nfactors are independently distributed as standard normal distributions.\nHowever, the independence assumption may be unrealistic for many scenarios such\nas modeling multiple time series, spatial processes, and functional data, where\nthe outcomes are correlated. In this paper, we introduce the generalized\nprobabilistic principal component analysis (GPPCA) to study the latent factor\nmodel for multiple correlated outcomes, where each factor is modeled by a\nGaussian process. Our method generalizes the previous probabilistic formulation\nof PCA (PPCA) by providing the closed-form maximum marginal likelihood\nestimator of the factor loadings and other parameters. Based on the explicit\nexpression of the precision matrix in the marginal likelihood that we derived,\nthe number of the computational operations is linear to the number of output\nvariables. Furthermore, we also provide the closed-form expression of the\nmarginal likelihood when other covariates are included in the mean structure.\nWe highlight the advantage of GPPCA in terms of the practical relevance,\nestimation accuracy and computational convenience. Numerical studies of\nsimulated and real data confirm the excellent finite-sample performance of the\nproposed approach. \n\n"}
{"id": "1809.00266", "contents": "Title: Function-on-Scalar Quantile Regression with Application to Mass\n  Spectrometry Proteomics Data Abstract: Mass spectrometry proteomics, characterized by spiky, spatially heterogeneous\nfunctional data, can be used to identify potential cancer biomarkers. Existing\nmass spectrometry analyses utilize mean regression to detect spectral regions\nthat are differentially expressed across groups. However, given the\ninter-patient heterogeneity that is a key hallmark of cancer, many biomarkers\nare only present at aberrant levels for a subset of, not all, cancer samples.\nDifferences in these biomarkers can easily be missed by mean regression, but\nmight be more easily detected by quantile-based approaches. Thus, we propose a\nunified Bayesian framework to perform quantile regression on functional\nresponses. Our approach utilizes an asymmetric Laplace working likelihood,\nrepresents the functional coefficients with basis representations which enable\nborrowing of strength from nearby locations, and places a global-local\nshrinkage prior on the basis coefficients to achieve adaptive regularization.\nDifferent types of basis transform and continuous shrinkage priors can be used\nin our framework. A scalable Gibbs sampler is developed to generate posterior\nsamples that can be used to perform Bayesian estimation and inference while\naccounting for multiple testing. Our framework performs quantile regression and\ncoefficient regularization in a unified manner, allowing them to inform each\nother and leading to improvement in performance over competing methods as\ndemonstrated by simulation studies. We also introduce an adjustment procedure\nto the model to improve its frequentist properties of posterior inference. We\napply our model to identify proteomic biomarkers of pancreatic cancer that are\ndifferentially expressed for a subset of cancer patients compared to the normal\ncontrols, which were missed by previous mean-regression based approaches.\nSupplementary materials for this article are available online. \n\n"}
{"id": "1809.00420", "contents": "Title: Network estimation via graphon with node features Abstract: Estimating the probabilities of linkages in a network has gained increasing\ninterest in recent years. One popular model for network analysis is the\nexchangeable graph model (ExGM) characterized by a two-dimensional function\nknown as a graphon. Estimating an underlying graphon becomes the key of such\nanalysis. Several nonparametric estimation methods have been proposed, and some\nare provably consistent. However, if certain useful features of the nodes\n(e.g., age and schools in social network context) are available, none of these\nmethods was designed to incorporate this source of information to help with the\nestimation. This paper develops a consistent graphon estimation method that\nintegrates the information from both the adjacency matrix itself and node\nfeatures. We show that properly leveraging the features can improve the\nestimation. A cross-validation method is proposed to automatically select the\ntuning parameter of the method. \n\n"}
{"id": "1809.00694", "contents": "Title: Proper likelihood ratio based ROC curves for general binary\n  classification problems Abstract: Everybody writes that ROC curves, a very common tool in binary classification\nproblems, should be optimal, and in particular concave, non-decreasing and\nabove the 45-degree line. Everybody uses ROC curves, theoretical and especially\nempirical, which are not so. This work is an attempt to correct this\nschizophrenic behavior. Optimality stems from the Neyman-Pearson lemma, which\nprescribes using likelihood-ratio based ROC curves. Starting from there, we\ngive the most general definition of a likelihood-ratio based classification\nprocedure, which encompasses finite, continuous and even more complex data\ntypes. We point out a strict relationship with a general notion of\nconcentration of two probability measures. We give some nontrivial examples of\nsituations with non-monotone and non-continuous likelihood ratios. Finally, we\npropose the ROC curve of a likelihood ratio based Gaussian kernel flexible\nBayes classifier as a proper default alternative to the usual empirical ROC\ncurve. \n\n"}
{"id": "1809.01796", "contents": "Title: Optimal Sparse Singular Value Decomposition for High-dimensional\n  High-order Data Abstract: In this article, we consider the sparse tensor singular value decomposition,\nwhich aims for dimension reduction on high-dimensional high-order data with\ncertain sparsity structure. A method named Sparse Tensor Alternating\nThresholding for Singular Value Decomposition (STAT-SVD) is proposed. The\nproposed procedure features a novel double projection \\& thresholding scheme,\nwhich provides a sharp criterion for thresholding in each iteration. Compared\nwith regular tensor SVD model, STAT-SVD permits more robust estimation under\nweaker assumptions. Both the upper and lower bounds for estimation accuracy are\ndeveloped. The proposed procedure is shown to be minimax rate-optimal in a\ngeneral class of situations. Simulation studies show that STAT-SVD performs\nwell under a variety of configurations. We also illustrate the merits of the\nproposed procedure on a longitudinal tensor dataset on European country\nmortality rates. \n\n"}
{"id": "1809.02385", "contents": "Title: Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers Abstract: In recent years, data have become increasingly higher dimensional and,\ntherefore, an increased need has arisen for dimension reduction techniques for\nclustering. Although such techniques are firmly established in the literature\nfor multivariate data, there is a relative paucity in the area of matrix\nvariate, or three-way, data. Furthermore, the few methods that are available\nall assume matrix variate normality, which is not always sensible if cluster\nskewness or excess kurtosis is present. Mixtures of bilinear factor analyzers\nusing skewed matrix variate distributions are proposed. In all, four such\nmixture models are presented, based on matrix variate skew-t, generalized\nhyperbolic, variance-gamma, and normal inverse Gaussian distributions,\nrespectively. \n\n"}
{"id": "1809.02959", "contents": "Title: MPS: An R package for modelling new families of distributions Abstract: We introduce an \\verb|R| package, called \\verb|MPS|, for computing the\nprobability density function, computing the cumulative distribution function,\ncomputing the quantile function, simulating random variables, and estimating\nthe parameters of 24 new shifted families of distributions. By considering an\nextra shift (location) parameter for each family more flexibility yields. Under\nsome situations, since the maximum likelihood estimators may fail to exist, we\nadopt the well-known maximum product spacings approach to estimate the\nparameters of shifted 24 new families of distributions. The performance of the\n\\verb|MPS| package for computing the cdf, pdf, and simulating random samples\nwill be checked by examples. The performance of the maximum product spacings\napproach is demonstrated by executing \\verb|MPS| package for three sets of real\ndata. As it will be shown, for the first set, the maximum likelihood estimators\nbreak down but \\verb|MPS| package find them. For the second set, adding the\nlocation parameter leads to acceptance the model while absence of the location\nparameter makes the model quite inappropriate. For the third set, presence of\nthe location parameter yields a better fit. \n\n"}
{"id": "1809.03659", "contents": "Title: New models for symbolic data analysis Abstract: Symbolic data analysis (SDA) is an emerging area of statistics concerned with\nunderstanding and modelling data that takes distributional form (i.e. symbols),\nsuch as random lists, intervals and histograms. It was developed under the\npremise that the statistical unit of interest is the symbol, and that inference\nis required at this level. Here we consider a different perspective, which\nopens a new research direction in the field of SDA. We assume that, as with a\nstandard statistical analysis, inference is required at the level of\nindividual-level data. However, the individual-level data are aggregated into\nsymbols - group-based distributional-valued summaries - prior to the analysis.\nIn this way, large and complex datasets can be reduced to a smaller number of\ndistributional summaries, that may be analysed more efficiently than the\noriginal dataset. As such, we develop SDA techniques as a new approach for the\nanalysis of big data. In particular we introduce a new general method for\nconstructing likelihood functions for symbolic data based on a desired\nprobability model for the underlying measurement-level data, while only\nobserving the distributional summaries. This approach opens the door for new\nclasses of symbol design and construction, in addition to developing SDA as a\nviable tool to enable and improve upon classical data analyses, particularly\nfor very large and complex datasets. We illustrate this new direction for SDA\nresearch through several real and simulated data analyses. \n\n"}
{"id": "1809.03759", "contents": "Title: On the aberrations of mixed level Orthogonal Arrays with removed runs Abstract: Given an Orthogonal Array we analyze the aberrations of the sub-fractions\nwhich are obtained by the deletion of some of its points. We provide formulae\nto compute the Generalized Word-Length Pattern of any sub-fraction. In the case\nof the deletion of one single point, we provide a simple methodology to find\nwhich the best sub-fractions are according to the Generalized Minimum\nAberration criterion. We also study the effect of the deletion of 1, 2 or 3\npoints on some examples. The methodology does not put any restriction on the\nnumber of levels of each factor. It follows that any mixed level Orthogonal\nArray can be considered. \n\n"}
{"id": "1809.03878", "contents": "Title: Topological Brain Network Distances Abstract: Existing brain network distances are often based on matrix norms. The\nelement-wise differences in the existing matrix norms may fail to capture\nunderlying topological differences. Further, matrix norms are sensitive to\noutliers. A major disadvantage to element-wise distance calculations is that it\ncould be severely affected even by a small number of extreme edge weights. Thus\nit is necessary to develop network distances that recognize topology. In this\npaper, we provide a survey of bottleneck, Gromov-Hausdorff (GH) and\nKolmogorov-Smirnov (KS) distances that are adapted for brain networks, and\ncompare them against matrix-norm based network distances. Bottleneck and\nGH-distances are often used in persistent homology. However, they were rarely\nutilized to measure similarity between brain networks. KS-distance is recently\nintroduced to measure the similarity between networks across different\nfiltration values. The performance analysis was conducted using the random\nnetwork simulations with the ground truths. Using a twin imaging study, which\nprovides biological ground truth, we demonstrate that the KS distance has the\nability to determine heritability. \n\n"}
{"id": "1809.04235", "contents": "Title: Risk-Limiting Audits by Stratified Union-Intersection Tests of Elections\n  (SUITE) Abstract: Risk-limiting audits (RLAs) offer a statistical guarantee: if a full manual\ntally of the paper ballots would show that the reported election outcome is\nwrong, an RLA has a known minimum chance of leading to a full manual tally.\nRLAs generally rely on random samples. Stratified sampling--partitioning the\npopulation of ballots into disjoint strata and sampling independently from the\nstrata--may simplify logistics or increase efficiency compared to simpler\nsampling designs, but makes risk calculations harder. We present SUITE, a new\nmethod for conducting RLAs using stratified samples. SUITE considers all\npossible partitions of outcome-changing error across strata. For each\npartition, it combines P-values from stratum-level tests into a combined\nP-value; there is no restriction on the tests used in different strata. SUITE\nmaximizes the combined P-value over all partitions of outcome-changing error.\nThe audit can stop if that maximum is less than the risk limit. Voting systems\nin some Colorado counties (comprising 98.2% of voters) allow auditors to check\nhow the system interpreted each ballot, which allows ballot-level comparison\nRLAs. Other counties use ballot polling, which is less efficient. Extant\napproaches to conducting an RLA of a statewide contest would require major\nchanges to Colorado's procedures and software, or would sacrifice the\nefficiency of ballot-level comparison. SUITE does not. It divides ballots into\ntwo strata: those cast in counties that can conduct ballot-level comparisons,\nand the rest. Stratum-level P-values are found by methods derived here. The\nresulting audit is substantially more efficient than statewide ballot polling.\nSUITE is useful in any state with a mix of voting systems or that uses\nstratified sampling for other reasons. We provide an open-source reference\nimplementation and exemplar calculations in Jupyter notebooks. \n\n"}
{"id": "1809.04541", "contents": "Title: Lugsail lag windows for estimating time-average covariance matrices Abstract: Lag windows are commonly used in time series, econometrics, steady-state\nsimulation, and Markov chain Monte Carlo to estimate time-average covariance\nmatrices. In the presence of positive correlation of the underlying process,\nestimators of this matrix almost always exhibit significant negative bias,\nleading to undesirable finite-sample properties. We propose a new family of lag\nwindows specifically designed to improve finite-sample performance by\noffsetting this negative bias. Any existing lag window can be adapted into a\nlugsail equivalent with no additional assumptions. We use these lag windows\nwithin spectral variance estimators and demonstrate its advantages in a linear\nregression model with autocorrelated and heteroskedastic residuals. We further\nemploy the lugsail lag windows in weighted batch means estimators due to their\ncomputational efficiency on large simulation output. We obtain bias and\nvariance results for these multivariate estimators and significantly weaken the\nmixing condition on the process. Superior finite-sample properties are\nillustrated in a vector autoregressive process and a Bayesian logistic\nregression model. \n\n"}
{"id": "1809.05173", "contents": "Title: Distinguishing Between Roles of Football Players in Play-by-play Match\n  Event Data Abstract: Over the last few decades, the player recruitment process in professional\nfootball has evolved into a multi-billion industry and has thus become of vital\nimportance. To gain insights into the general level of their candidate\nreinforcements, many professional football clubs have access to extensive video\nfootage and advanced statistics. However, the question whether a given player\nwould fit the team's playing style often still remains unanswered. In this\npaper, we aim to bridge that gap by proposing a set of 21 player roles and\nintroducing a method for automatically identifying the most applicable roles\nfor each player from play-by-play event data collected during matches. \n\n"}
{"id": "1809.05580", "contents": "Title: Assessing Bayes factor surfaces using interactive visualization and\n  computer surrogate modeling Abstract: Bayesian model selection provides a natural alternative to classical\nhypothesis testing based on p-values. While many papers mention that Bayesian\nmodel selection is frequently sensitive to prior specification on the\nparameters, there are few practical strategies to assess and report this\nsensitivity. This article has two goals. First, we aim educate the broader\nstatistical community about the extent of potential sensitivity through\nvisualization of the Bayes factor surface. The Bayes factor surface shows the\nvalue a Bayes factor takes (usually on the log scale) as a function of\nuser-specified hyperparameters. We provide interactive visualization through an\nR shiny application that allows the user to explore sensitivity in Bayes factor\nover a range of hyperparameter settings in a familiar regression setting. We\ncompare the surface with three automatic procedures. Second, we suggest\nsurrogate modeling via Gaussian processes (GPs) to visualize the Bayes factor\nsurface in situations where computation of Bayes factors is expensive. That is,\nwe treat Bayes factor calculation as a computer simulation experiment. In this\ncontext, we provide a fully reproducible example using accessible GP libraries\nto augment an important study of the influence of outliers in empirical\nfinance. We suggest Bayes factor surfaces are valuable for scientific reporting\nsince they (i) increase transparency, making potential instability in Bayes\nfactors easy to visualize, (ii) generalize to simple and more complicated\nexamples, and (iii) provide a path for researchers to assess the impact of\nprior choice on modeling decisions in a wide variety research areas. \n\n"}
{"id": "1809.05760", "contents": "Title: History of art paintings through the lens of entropy and complexity Abstract: Art is the ultimate expression of human creativity that is deeply influenced\nby the philosophy and culture of the corresponding historical epoch. The\nquantitative analysis of art is therefore essential for better understanding\nhuman cultural evolution. Here we present a large-scale quantitative analysis\nof almost 140 thousand paintings, spanning nearly a millennium of art history.\nBased on the local spatial patterns in the images of these paintings, we\nestimate the permutation entropy and the statistical complexity of each\npainting. These measures map the degree of visual order of artworks into a\nscale of order-disorder and simplicity-complexity that locally reflects\nqualitative categories proposed by art historians. The dynamical behavior of\nthese measures reveals a clear temporal evolution of art, marked by transitions\nthat agree with the main historical periods of art. Our research shows that\ndifferent artistic styles have a distinct average degree of entropy and\ncomplexity, thus allowing a hierarchical organization and clustering of styles\naccording to these metrics. We have further verified that the identified groups\ncorrespond well with the textual content used to qualitatively describe the\nstyles, and that the employed complexity-entropy measures can be used for an\neffective classification of artworks. \n\n"}
{"id": "1809.05935", "contents": "Title: Bayesian Modular and Multiscale Regression Abstract: We tackle the problem of multiscale regression for predictors that are\nspatially or temporally indexed, or with a pre-specified multiscale structure,\nwith a Bayesian modular approach. The regression function at the finest scale\nis expressed as an additive expansion of coarse to fine step functions. Our\nModular and Multiscale (M&M) methodology provides multiscale decomposition of\nhigh-dimensional data arising from very fine measurements. Unlike more complex\nmethods for functional predictors, our approach provides easy interpretation of\nthe results. Additionally, it provides a quantification of uncertainty on the\ndata resolution, solving a common problem researchers encounter with simple\nmodels on down-sampled data. We show that our modular and multiscale posterior\nhas an empirical Bayes interpretation, with a simple limiting distribution in\nlarge samples. An efficient sampling algorithm is developed for posterior\ncomputation, and the methods are illustrated through simulation studies and an\napplication to brain image classification. Source code is available as an R\npackage at https://github.com/mkln/bmms. \n\n"}
{"id": "1809.06023", "contents": "Title: Learning-based attacks in cyber-physical systems Abstract: We introduce the problem of learning-based attacks in a simple abstraction of\ncyber-physical systems---the case of a discrete-time, linear, time-invariant\nplant that may be subject to an attack that overrides the sensor readings and\nthe controller actions. The attacker attempts to learn the dynamics of the\nplant and subsequently override the controller's actuation signal, to destroy\nthe plant without being detected. The attacker can feed fictitious sensor\nreadings to the controller using its estimate of the plant dynamics and mimic\nthe legitimate plant operation. The controller, on the other hand, is\nconstantly on the lookout for an attack; once the controller detects an attack,\nit immediately shuts the plant off. In the case of scalar plants, we derive an\nupper bound on the attacker's deception probability for any measurable control\npolicy when the attacker uses an arbitrary learning algorithm to estimate the\nsystem dynamics. We then derive lower bounds for the attacker's deception\nprobability for both scalar and vector plants by assuming a specific\nauthentication test that inspects the empirical variance of the system\ndisturbance. We also show how the controller can improve the security of the\nsystem by superimposing a carefully crafted privacy-enhancing signal on top of\nthe \"nominal control policy.\" Finally, for nonlinear scalar dynamics that\nbelong to the Reproducing Kernel Hilbert Space (RKHS), we investigate the\nperformance of attacks based on nonlinear Gaussian-processes (GP) learning\nalgorithms. \n\n"}
{"id": "1809.06418", "contents": "Title: Spatial Variable Selection and An Application to Virginia Lyme Disease\n  Emergence Abstract: Lyme disease is an infectious disease that is caused by a bacterium called\nBorrelia burgdorferi sensu stricto. In the United States, Lyme disease is one\nof the most common infectious diseases. The major endemic areas of the disease\nare New England, Mid-Atlantic, East-North Central, South Atlantic, and West\nNorth-Central. Virginia is on the front-line of the disease's diffusion from\nthe northeast to the south. One of the research objectives for the infectious\ndisease community is to identify environmental and economic variables that are\nassociated with the emergence of Lyme disease. In this paper, we use a spatial\nPoisson regression model to link the spatial disease counts and environmental\nand economic variables, and develop a spatial variable selection procedure to\neffectively identify important factors by using an adaptive elastic net\npenalty. The proposed methods can automatically select important covariates,\nwhile adjusting for possible spatial correlations of disease counts. The\nperformance of the proposed method is studied and compared with existing\nmethods via a comprehensive simulation study. We apply the developed variable\nselection methods to the Virginia Lyme disease data and identify important\nvariables that are new to the literature. Supplementary materials for this\npaper are available online. \n\n"}
{"id": "1809.06447", "contents": "Title: Homogeneity testing under finite location-scale mixtures Abstract: The testing problem for the order of finite mixture models has a long history\nand remains an active research topic. Since Ghosh and Sen (1985) revealed the\nhard-to-manage asymptotic properties of the likelihood ratio test, there has\nbeen marked progress. The most successful attempts include the modified\nlikelihood ratio test and the EM-test, which lead to neat solutions for finite\nmixtures of univariate normal distributions, finite mixtures of\nsingle-parameter distributions, and several mixture-like models. The problem\nremains challenging, and there is still no generic solution for location-scale\nmixtures. In this paper, we provide an EM-test solution for homogeneity for\nfinite mixtures of location-scale family distributions. This EM-test has\nnonstandard limiting distributions, but we are able to find the critical values\nnumerically. We use computer experiments to obtain appropriate values for the\ntuning parameters. A simulation study shows that the fine-tuned EM-test has\nclose to nominal type I errors and very good power properties. Two application\nexamples are included to demonstrate the performance of the EM-test. \n\n"}
{"id": "1809.06636", "contents": "Title: Comparison between Suitable Priors for Additive Bayesian Networks Abstract: Additive Bayesian networks are types of graphical models that extend the\nusual Bayesian generalized linear model to multiple dependent variables through\nthe factorisation of the joint probability distribution of the underlying\nvariables. When fitting an ABN model, the choice of the prior of the parameters\nis of crucial importance. If an inadequate prior - like a too weakly\ninformative one - is used, data separation and data sparsity lead to issues in\nthe model selection process. In this work a simulation study between two weakly\nand a strongly informative priors is presented. As weakly informative prior we\nuse a zero mean Gaussian prior with a large variance, currently implemented in\nthe R-package abn. The second prior belongs to the Student's t-distribution,\nspecifically designed for logistic regressions and, finally, the strongly\ninformative prior is again Gaussian with mean equal to true parameter value and\na small variance. We compare the impact of these priors on the accuracy of the\nlearned additive Bayesian network in function of different parameters. We\ncreate a simulation study to illustrate Lindley's paradox based on the prior\nchoice. We then conclude by highlighting the good performance of the\ninformative Student's t-prior and the limited impact of the Lindley's paradox.\nFinally, suggestions for further developments are provided. \n\n"}
{"id": "1809.07401", "contents": "Title: Transmission of Macroeconomic Shocks to Risk Parameters: Their uses in\n  Stress Testing Abstract: In this paper, we are interested in evaluating the resilience of financial\nportfolios under extreme economic conditions. Therefore, we use empirical\nmeasures to characterize the transmission process of macroeconomic shocks to\nrisk parameters. We propose the use of an extensive family of models, called\nGeneral Transfer Function Models, which condense well the characteristics of\nthe transmission described by the impact measures. The procedure for estimating\nthe parameters of these models is described employing the Bayesian approach and\nusing the prior information provided by the impact measures. In addition, we\nillustrate the use of the estimated models from the credit risk data of a\nportfolio. \n\n"}
{"id": "1809.07419", "contents": "Title: Randomization Tests for Weak Null Hypotheses in Randomized Experiments Abstract: The Fisher randomization test (FRT) is appropriate for any test statistic,\nunder a sharp null hypothesis that can recover all missing potential outcomes.\nHowever, it is often sought after to test a weak null hypothesis that the\ntreatment does not affect the units on average. To use the FRT for a weak null\nhypothesis, we must address two issues. First, we need to impute the missing\npotential outcomes although the weak null hypothesis cannot determine all of\nthem. Second, we need to choose a proper test statistic. For a general weak\nnull hypothesis, we propose an approach to imputing missing potential outcomes\nunder a compatible sharp null hypothesis. Building on this imputation scheme,\nwe advocate a studentized statistic. The resulting FRT has multiple desirable\nfeatures. First, it is model-free. Second, it is finite-sample exact under the\nsharp null hypothesis that we use to impute the potential outcomes. Third, it\nconservatively controls large-sample type I errors under the weak null\nhypothesis of interest. Therefore, our FRT is agnostic to the treatment effect\nheterogeneity. We establish a unified theory for general factorial experiments.\nWe also extend it to stratified and clustered experiments. \n\n"}
{"id": "1809.08785", "contents": "Title: Modeling non-linear spectral domain dependence using copulas with\n  applications to rat local field potentials Abstract: This paper intends to develop tools for characterizing non-linear spectral\ndependence between spontaneous brain signals. We use parametric copula models\n(both bivariate and vine models) applied on the magnitude of Fourier\ncoefficients rather than using coherence. The motivation behind this work is an\nexperiment on rats that studied the impact of stroke on the connectivity\nstructure (dependence) between local field potentials recorded at various\nchannels. We address the following major questions. First, we ask whether one\ncan detect any changepoint in the regime of a brain channel for a given\nfrequency band based on a difference between the cumulative distribution\nfunctions modeled for each epoch (small window of time). Our proposed approach\nis an iterative algorithm which compares each successive bivariate copulas on\nall the epochs range, using a bivariate Kolmogorov-Smirnov statistic. Second,\nwe ask whether stroke can alter the dependence structure of brain signals; and\nexamine whether changes in dependence are present only in some channels or\ngeneralized across channels. These questions are addressed by comparing\nVine-copulas models fitted for each epoch. We provide the necessary framework\nand show the effectiveness of our methods through the results for the local\nfield potential data analysis of a rat. \n\n"}
{"id": "1809.09620", "contents": "Title: RAFP-Pred: Robust Prediction of Antifreeze Proteins using Localized\n  Analysis of n-Peptide Compositions Abstract: In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)\nto counter the otherwise lethal intracellular formation of ice. Structures and\nsequences of various AFPs exhibit a high degree of heterogeneity, consequently\nthe prediction of the AFPs is considered to be a challenging task. In this\nresearch, we propose to handle this arduous manifold learning task using the\nnotion of localized processing. In particular an AFP sequence is segmented into\ntwo sub-segments each of which is analyzed for amino acid and di-peptide\ncompositions. We propose to use only the most significant features using the\nconcept of information gain (IG) followed by a random forest classification\napproach. The proposed RAFP-Pred achieved an excellent performance on a number\nof standard datasets. We report a high Youden's index\n(sensitivity+specificity-1) value of 0.75 on the standard independent test data\nset outperforming the AFP-PseAAC, AFP\\_PSSM, AFP-Pred and iAFP by a margin of\n0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB\ndataset is found to be 83.19\\% which is substantially superior to the 57.18\\%\nreported for the iAFP method. \n\n"}
{"id": "1809.09793", "contents": "Title: A new Gini correlation between quantitative and qualitative variables Abstract: We propose a new Gini correlation to measure dependence between a categorical\nand numerical variables. Analogous to Pearson $R^2$ in ANOVA model, the Gini\ncorrelation is interpreted as the ratio of the between-group variation and the\ntotal variation, but it characterizes independence (zero Gini correlation\nmutually implies independence). Closely related to the distance correlation,\nthe Gini correlation is of simple formulation by considering the nature of\ncategorical variable. As a result, the proposed Gini correlation has a lower\ncomputational cost than the distance correlation and is more straightforward to\nperform inference. Simulation and real applications are conducted to\ndemonstrate the advantages. \n\n"}
{"id": "1809.09881", "contents": "Title: Boosting Functional Response Models for Location, Scale and Shape with\n  an Application to Bacterial Competition Abstract: We extend Generalized Additive Models for Location, Scale, and Shape (GAMLSS)\nto regression with functional response. This allows us to simultaneously model\npoint-wise mean curves, variances and other distributional parameters of the\nresponse in dependence of various scalar and functional covariate effects. In\naddition, the scope of distributions is extended beyond exponential families.\nThe model is fitted via gradient boosting, which offers inherent model\nselection and is shown to be suitable for both complex model structures and\nhighly auto-correlated response curves. This enables us to analyze bacterial\ngrowth in \\textit{Escherichia coli} in a complex interaction scenario,\nfruitfully extending usual growth models. \n\n"}
{"id": "1810.02315", "contents": "Title: DER Allocation and Line Repair Scheduling for Storm-induced Failures in\n  Distribution Networks Abstract: Electricity distribution networks (DNs) in many regions are increasingly\nsubjected to disruptions caused by tropical storms. Distributed Energy\nResources (DERs) can act as temporary supply sources to sustain \"microgrids\"\nresulting from disruptions. In this paper, we investigate the problem of\nsuitable DER allocation to facilitate more efficient repair operations and\nfaster recovery. First, we estimate the failure probabilities of DN components\n(lines) using a stochastic model of line failures which parametrically depends\non the location-specific storm wind field. Next, we formulate a two-stage\nstochastic mixed integer program, which models the distribution utility's\ndecision to allocate DERs in the DN (pre-storm stage); and accounts for\nmulti-period decisions on optimal dispatch and line repair scheduling\n(post-storm stage). A key feature of this formulation is that it jointly\noptimizes electricity dispatch within the individual microgrids and the line\nrepair schedules to minimize the sum of the cost of DER allocation and cost due\nto lost load. To illustrate our approach, we use the sample average\napproximation method to solve our problem for a small-size DN under different\nstorm intensities and DER/crew constraints. \n\n"}
{"id": "1810.02456", "contents": "Title: Graph-Theoretic Analysis of Belief System Dynamics under Logic\n  Constraints Abstract: Opinion formation cannot be modeled solely as an ideological deduction from a\nset of principles; rather, repeated social interactions and logic constraints\namong statements are consequential in the construct of belief systems. We\naddress three basic questions in the analysis of social opinion dynamics: (i)\nWill a belief system converge? (ii) How long does it take to converge? (iii)\nWhere does it converge? We provide graph-theoretic answers to these questions\nfor a model of opinion dynamics of a belief system with logic constraints. Our\nresults make plain the implicit dependence of the convergence properties of a\nbelief system on the underlying social network and on the set of logic\nconstraints that relate beliefs on different statements. Moreover, we provide\nan explicit analysis of a variety of commonly used large-scale network models. \n\n"}
{"id": "1810.02954", "contents": "Title: Adapting to Unknown Noise Distribution in Matrix Denoising Abstract: We consider the problem of estimating an unknown matrix $\\boldsymbol{X}\\in\n{\\mathbb R}^{m\\times n}$, from observations $\\boldsymbol{Y} =\n\\boldsymbol{X}+\\boldsymbol{W}$ where $\\boldsymbol{W}$ is a noise matrix with\nindependent and identically distributed entries, as to minimize estimation\nerror measured in operator norm. Assuming that the underlying signal\n$\\boldsymbol{X}$ is low-rank and incoherent with respect to the canonical\nbasis, we prove that minimax risk is equivalent to\n$(\\sqrt{m}\\vee\\sqrt{n})/\\sqrt{I_W}$ in the high-dimensional limit\n$m,n\\to\\infty$, where $I_W$ is the Fisher information of the noise. Crucially,\nwe develop an efficient procedure that achieves this risk, adaptively over the\nnoise distribution (under certain regularity assumptions).\n  Letting $\\boldsymbol{X} =\n\\boldsymbol{U}{\\boldsymbol{\\Sigma}}\\boldsymbol{V}^{{\\sf T}}$ --where\n$\\boldsymbol{U}\\in {\\mathbb R}^{m\\times r}$, $\\boldsymbol{V}\\in{\\mathbb\nR}^{n\\times r}$ are orthogonal, and $r$ is kept fixed as $m,n\\to\\infty$-- we\nuse our method to estimate $\\boldsymbol{U}$, $\\boldsymbol{V}$. Standard\nspectral methods provide non-trivial estimates of the factors\n$\\boldsymbol{U},\\boldsymbol{V}$ (weak recovery) only if the singular values of\n$\\boldsymbol{X}$ are larger than $(mn)^{1/4}{\\rm Var}(W_{11})^{1/2}$. We prove\nthat the new approach achieves weak recovery down to the the\ninformation-theoretically optimal threshold $(mn)^{1/4}I_W^{1/2}$. \n\n"}
{"id": "1810.03296", "contents": "Title: Event History Analysis of Dynamic Communication Networks Abstract: Statistical analysis on networks has received growing attention due to demand\nfrom various emerging applications. In dynamic networks, one of the key\ninterests is to model the event history of time-stamped interactions amongst\nnodes. We propose to model dynamic directed communication networks via\nmultivariate counting processes. A pseudo partial likelihood approach is\nexploited to capture the network dependence structure. Asymptotic results of\nthe resulting estimation are established. Numerical results are performed to\ndemonstrate effectiveness of our proposal. \n\n"}
{"id": "1810.03814", "contents": "Title: SNAP: A semismooth Newton algorithm for pathwise optimization with\n  optimal local convergence rate and oracle properties Abstract: We propose a semismooth Newton algorithm for pathwise optimization (SNAP) for\nthe LASSO and Enet in sparse, high-dimensional linear regression. SNAP is\nderived from a suitable formulation of the KKT conditions based on Newton\nderivatives. It solves the semismooth KKT equations efficiently by actively and\ncontinuously seeking the support of the regression coefficients along the\nsolution path with warm start. At each knot in the path, SNAP converges locally\nsuperlinearly for the Enet criterion and achieves an optimal local convergence\nrate for the LASSO criterion, i.e., SNAP converges in one step at the cost of\ntwo matrix-vector multiplication per iteration. Under certain regularity\nconditions on the design matrix and the minimum magnitude of the nonzero\nelements of the target regression coefficients, we show that SNAP hits a\nsolution with the same signs as the regression coefficients and achieves a\nsharp estimation error bound in finite steps with high probability. The\ncomputational complexity of SNAP is shown to be the same as that of LARS and\ncoordinate descent algorithms per iteration. Simulation studies and real data\nanalysis support our theoretical results and demonstrate that SNAP is faster\nand accurate than LARS and coordinate descent algorithms. \n\n"}
{"id": "1810.05497", "contents": "Title: Probabilistic Blocking with An Application to the Syrian Conflict Abstract: Entity resolution seeks to merge databases as to remove duplicate entries\nwhere unique identifiers are typically unknown. We review modern blocking\napproaches for entity resolution, focusing on those based upon locality\nsensitive hashing (LSH). First, we introduce $k$-means locality sensitive\nhashing (KLSH), which is based upon the information retrieval literature and\nclusters similar records into blocks using a vector-space representation and\nprojections. Second, we introduce a subquadratic variant of LSH to the\nliterature, known as Densified One Permutation Hashing (DOPH). Third, we\npropose a weighted variant of DOPH. We illustrate each method on an application\nto a subset of the ongoing Syrian conflict, giving a discussion of each method. \n\n"}
{"id": "1810.06989", "contents": "Title: Clustering in statistical ill-posed linear inverse problems Abstract: In many statistical linear inverse problems, one needs to recover classes of\nsimilar curves from their noisy images under an operator that does not have a\nbounded inverse. Problems of this kind appear in many areas of application.\nRoutinely, in such problems clustering is carried out at the pre-processing\nstep and then the inverse problem is solved for each of the cluster averages\nseparately. As a result, the errors of the procedures are usually examined for\nthe estimation step only. The objective of this paper is to examine, both\ntheoretically and via simulations, the effect of clustering on the accuracy of\nthe solutions of general ill-posed linear inverse problems. In particular, we\nassume that one observes $X_m = A f_m + \\delta \\epsilon_m$, $m=1, \\cdots, M$,\nwhere functions $f_m$ can be grouped into $K$ classes and one needs to recover\na vector function ${\\bf f}= (f_1,\\cdots, f_M)^T$. We construct an estimators\nfor ${\\bf f}$ as a solution of a penalized optimization problem and derive an\noracle inequality for its precision. By deriving upper and minimax lower bounds\nfor the error, we confirm that the estimator is minimax optimal or nearly\nminimax optimal up to a logarithmic factor of the number of observations. One\nof the advantages of our estimation procedure is that we do not assume that the\nnumber of clusters is known in advance. We conclude that clustering at the\npre-processing step is beneficial when the problem is moderately ill-posed. It\nshould be applied with extreme care when the problem is severely ill-posed. \n\n"}
{"id": "1810.07403", "contents": "Title: Optimal Covariance Estimation for Condition Number Loss in the Spiked\n  Model Abstract: We study estimation of the covariance matrix under relative condition number\nloss $\\kappa(\\Sigma^{-1/2} \\hat{\\Sigma} \\Sigma^{-1/2})$, where $\\kappa(\\Delta)$\nis the condition number of matrix $\\Delta$, and $\\hat{\\Sigma}$ and $\\Sigma$ are\nthe estimated and theoretical covariance matrices. Optimality in $\\kappa$-loss\nprovides optimal guarantees in two stylized applications: Multi-User Covariance\nEstimation and Multi-Task Linear Discriminant Analysis. We assume the so-called\nspiked covariance model for $\\Sigma$, and exploit recent advances in\nunderstanding that model, to derive a nonlinear shrinker which is\nasymptotically optimal among orthogonally-equivariant procedures. In our\nasymptotic study, the number of variables $p$ is comparable to the number of\nobservations $n$. The form of the optimal nonlinearity depends on the aspect\nratio $\\gamma=p/n$ of the data matrix and on the top eigenvalue of $\\Sigma$.\nFor $\\gamma > 0.618...$, even dependence on the top eigenvalue can be avoided.\nThe optimal shrinker has two notable properties. First, when $p/n \\rightarrow\n\\gamma \\gg 1$ is large, it shrinks even very large eigenvalues substantially,\nby a factor $1/(1+\\gamma)$. Second, even for moderate $\\gamma$, certain highly\nstatistically significant eigencomponents will be completely suppressed. We\nshow that when $\\gamma \\gg 1$ is large, purely diagonal covariance matrices can\nbe optimal, despite the top eigenvalues being large and the empirical\neigenvalues being highly statistically significant. This aligns with\npractitioner experience. We identify intuitively reasonable procedures with\nsmall worst-case relative regret - the simplest being generalized soft\nthresholding having threshold at the bulk edge and slope $(1+\\gamma)^{-1}$\nabove the bulk. For $\\gamma < 2$ it has at most a few percent relative regret. \n\n"}
{"id": "1810.08264", "contents": "Title: Quantile Regression Under Memory Constraint Abstract: This paper studies the inference problem in quantile regression (QR) for a\nlarge sample size $n$ but under a limited memory constraint, where the memory\ncan only store a small batch of data of size $m$. A natural method is the\nna\\\"ive divide-and-conquer approach, which splits data into batches of size\n$m$, computes the local QR estimator for each batch, and then aggregates the\nestimators via averaging. However, this method only works when $n=o(m^2)$ and\nis computationally expensive. This paper proposes a computationally efficient\nmethod, which only requires an initial QR estimator on a small batch of data\nand then successively refines the estimator via multiple rounds of\naggregations. Theoretically, as long as $n$ grows polynomially in $m$, we\nestablish the asymptotic normality for the obtained estimator and show that our\nestimator with only a few rounds of aggregations achieves the same efficiency\nas the QR estimator computed on all the data. Moreover, our result allows the\ncase that the dimensionality $p$ goes to infinity. The proposed method can also\nbe applied to address the QR problem under distributed computing environment\n(e.g., in a large-scale sensor network) or for real-time streaming data. \n\n"}
{"id": "1810.08564", "contents": "Title: Nonparametric Bayesian Lomax delegate racing for survival analysis with\n  competing risks Abstract: We propose Lomax delegate racing (LDR) to explicitly model the mechanism of\nsurvival under competing risks and to interpret how the covariates accelerate\nor decelerate the time to event. LDR explains non-monotonic covariate effects\nby racing a potentially infinite number of sub-risks, and consequently relaxes\nthe ubiquitous proportional-hazards assumption which may be too restrictive.\nMoreover, LDR is naturally able to model not only censoring, but also missing\nevent times or event types. For inference, we develop a Gibbs sampler under\ndata augmentation for moderately sized data, along with a stochastic gradient\ndescent maximum a posteriori inference algorithm for big data applications.\nIllustrative experiments are provided on both synthetic and real datasets, and\ncomparison with various benchmark algorithms for survival analysis with\ncompeting risks demonstrates distinguished performance of LDR. \n\n"}
{"id": "1810.09682", "contents": "Title: Bivariate modelling of precipitation and temperature using a\n  non-homogeneous hidden Markov model Abstract: Aiming to generate realistic synthetic times series of the bivariate process\nof daily mean temperature and precipitations, we introduce a non-homogeneous\nhidden Markov model. The non-homogeneity lies in periodic transition\nprobabilities between the hidden states, and time-dependent emission\ndistributions. This enables the model to account for the non-stationary\nbehaviour of weather variables. By carefully choosing the emission\ndistributions, it is also possible to model the dependance structure between\nthe two variables. The model is applied to several weather stations in Europe\nwith various climates, and we show that it is able to simulate realistic\nbivariate time series. \n\n"}
{"id": "1810.09781", "contents": "Title: A Social Network Analysis of Articles on Social Network Analysis Abstract: A collection of articles on the statistical modelling and inference of social\nnetworks is analysed in a network fashion. The references of these articles are\nused to construct a citation network data set, which is almost a directed\nacyclic graph because only existing articles can be cited. A mixed membership\nstochastic block model is then applied to this data set to soft cluster the\narticles. The results obtained from a Gibbs sampler give us insights into the\ninfluence and the categorisation of these articles. \n\n"}
{"id": "1810.09894", "contents": "Title: Heterogeneous large datasets integration using Bayesian factor\n  regression Abstract: Two key challenges in modern statistical applications are the large amount of\ninformation recorded per individual, and that such data are often not collected\nall at once but in batches. These batch effects can be complex, causing\ndistortions in both mean and variance. We propose a novel sparse latent factor\nregression model to integrate such heterogeneous data. The model provides a\ntool for data exploration via dimensionality reduction while correcting for a\nrange of batch effects. We study the use of several sparse priors (local and\nnon-local) to learn the dimension of the latent factors. Our model is fitted in\na deterministic fashion by means of an EM algorithm for which we derive\nclosed-form updates, contributing a novel scalable algorithm for non-local\npriors of interest beyond the immediate scope of this paper. We present several\nexamples, with a focus on bioinformatics applications. Our results show an\nincrease in the accuracy of the dimensionality reduction, with non-local priors\nsubstantially improving the reconstruction of factor cardinality, as well as\nthe need to account for batch effects to obtain reliable results. Our model\nprovides a novel approach to latent factor regression that balances sparsity\nwith sensitivity and is highly computationally efficient. \n\n"}
{"id": "1810.11130", "contents": "Title: Robust Importance Sampling with Adaptive Winsorization Abstract: Importance sampling is a widely used technique to estimate properties of a\ndistribution. This paper investigates trading-off some bias for variance by\nadaptively winsorizing the importance sampling estimator. The novel winsorizing\nprocedure, based on the Balancing Principle (or Lepskii's Method), chooses a\nthreshold level among a pre-defined set by roughly balancing the bias and\nvariance of the estimator when winsorized at different levels. As a\nconsequence, it provides a principled way to perform winsorization with\nfinite-sample optimality guarantees under minimal assumptions. In various\nexamples, the proposed estimator is shown to have smaller mean squared error\nand mean absolute deviation than leading alternatives. \n\n"}
{"id": "1810.11881", "contents": "Title: Bounded Regression with Gaussian Process Projection Abstract: Examples with bound information on the regression function and density abound\nin many real applications. We propose a novel approach for estimating such\nfunctions by incorporating the prior knowledge on the bounds. Specially, a\nGaussian process is first imposed on the regression function whose posterior\ndistribution is then projected onto the bounded space. The resulting projected\nmeasure is then used for inference. The projected sample path has closed form\nwhich facilitates efficient computations. In particular, our projection\napproach maintains a comparable computational efficiency with that of the\noriginal GP. The proposed method yield predictions that respects bound\nconstraints everywhere, while allows varying bounds across the input domain. An\nextensive simulation study is carried out which demonstrates that the\nperformance of our approach dominates that of the competitors. An application\nto real data set is also considered. \n\n"}
{"id": "1811.00591", "contents": "Title: Defining a Metric Space of Host Logs and Operational Use Cases Abstract: Host logs, in particular, Windows Event Logs, are a valuable source of\ninformation often collected by security operation centers (SOCs). The\nsemi-structured nature of host logs inhibits automated analytics, and while\nmanual analysis is common, the sheer volume makes manual inspection of all logs\nimpossible. Although many powerful algorithms for analyzing time-series and\nsequential data exist, utilization of such algorithms for most cyber security\napplications is either infeasible or requires tailored, research-intensive\npreparations. In particular, basic mathematic and algorithmic developments for\nproviding a generalized, meaningful similarity metric on system logs is needed\nto bridge the gap between many existing sequential data mining methods and this\ncurrently available but under-utilized data source. In this paper, we provide a\nrigorous definition of a metric product space on Windows Event Logs, providing\nan embedding that allows for the application of established machine learning\nand time-series analysis methods. We then demonstrate the utility and\nflexibility of this embedding with multiple use-cases on real data: (1)\ncomparing known infected to new host log streams for attack detection and\nforensics, (2) collapsing similar streams of logs into semantically-meaningful\ngroups (by user, by role), thereby reducing the quantity of data but not the\ncontent, (3) clustering logs as well as short sequences of logs to identify and\nvisualize user behaviors and background processes over time. Overall, we\nprovide a metric space framework for general host logs and log sequences that\nrespects semantic similarity and facilitates a wide variety of data science\nanalytics to these logs without data-specific preparations for each. \n\n"}
{"id": "1811.00964", "contents": "Title: The X Factor: A Robust and Powerful Approach to X-chromosome-Inclusive\n  Whole-genome Association Studies Abstract: The X-chromosome is often excluded from genome-wide association studies\nbecause of analytical challenges. Some of the problems, such as the random,\nskewed or no X-inactivation model uncertainty, have been investigated. Other\nconsiderations have received little to no attention, such as the value in\nconsidering non-additive and gene-sex interaction effects, and the inferential\nconsequence of choosing different baseline alleles (i.e.\\ the reference vs.\\\nthe alternative allele). Here we propose a unified and flexible\nregression-based association test for X-chromosomal variants. We provide\ntheoretical justifications for its robustness in the presence of various model\nuncertainties, as well as for its improved power when compared with the\nexisting approaches under certain scenarios. For completeness, we also revisit\nthe autosomes and show that the proposed framework leads to a more robust\napproach than the standard method. Finally, we provide supporting evidence by\nrevisiting several published association studies. Supplementary materials for\nthis article are available online. \n\n"}
{"id": "1811.01467", "contents": "Title: The one comparing narrative social network extraction techniques Abstract: Analysing narratives through their social networks is an expanding field in\nquantitative literary studies. Manually extracting a social network from any\nnarrative can be time consuming, so automatic extraction methods of varying\ncomplexity have been developed. However, the effect of different extraction\nmethods on the analysis is unknown. Here we model and compare three extraction\nmethods for social networks in narratives: manual extraction, co-occurrence\nautomated extraction and automated extraction using machine learning. Although\nthe manual extraction method produces more precise results in the network\nanalysis, it is much more time consuming and the automatic extraction methods\nyield comparable conclusions for density, centrality measures and edge weights.\nOur results provide evidence that social networks extracted automatically are\nreliable for many analyses. We also describe which aspects of analysis are not\nreliable with such a social network. We anticipate that our findings will make\nit easier to analyse more narratives, which help us improve our understanding\nof how stories are written and evolve, and how people interact with each other. \n\n"}
{"id": "1811.02316", "contents": "Title: Stacked Penalized Logistic Regression for Selecting Views in Multi-View\n  Learning Abstract: In biomedical research, many different types of patient data can be\ncollected, such as various types of omics data and medical imaging modalities.\nApplying multi-view learning to these different sources of information can\nincrease the accuracy of medical classification models compared with\nsingle-view procedures. However, collecting biomedical data can be expensive\nand/or burdening for patients, so that it is important to reduce the amount of\nrequired data collection. It is therefore necessary to develop multi-view\nlearning methods which can accurately identify those views that are most\nimportant for prediction. In recent years, several biomedical studies have used\nan approach known as multi-view stacking (MVS), where a model is trained on\neach view separately and the resulting predictions are combined through\nstacking. In these studies, MVS has been shown to increase classification\naccuracy. However, the MVS framework can also be used for selecting a subset of\nimportant views. To study the view selection potential of MVS, we develop a\nspecial case called stacked penalized logistic regression (StaPLR). Compared\nwith existing view-selection methods, StaPLR can make use of faster\noptimization algorithms and is easily parallelized. We show that nonnegativity\nconstraints on the parameters of the function which combines the views play an\nimportant role in preventing unimportant views from entering the model. We\ninvestigate the performance of StaPLR through simulations, and consider two\nreal data examples. We compare the performance of StaPLR with an existing view\nselection method called the group lasso and observe that, in terms of view\nselection, StaPLR is often more conservative and has a consistently lower false\npositive rate. \n\n"}
{"id": "1811.04028", "contents": "Title: An Overview of Computational Approaches for Interpretation Analysis Abstract: It is said that beauty is in the eye of the beholder. But how exactly can we\ncharacterize such discrepancies in interpretation? For example, are there any\nspecific features of an image that makes person A regard an image as beautiful\nwhile person B finds the same image displeasing? Such questions ultimately aim\nat explaining our individual ways of interpretation, an intention that has been\nof fundamental importance to the social sciences from the beginning. More\nrecently, advances in computer science brought up two related questions: First,\ncan computational tools be adopted for analyzing ways of interpretation?\nSecond, what if the \"beholder\" is a computer model, i.e., how can we explain a\ncomputer model's point of view? Numerous efforts have been made regarding both\nof these points, while many existing approaches focus on particular aspects and\nare still rather separate. With this paper, in order to connect these\napproaches we introduce a theoretical framework for analyzing interpretation,\nwhich is applicable to interpretation of both human beings and computer models.\nWe give an overview of relevant computational approaches from various fields,\nand discuss the most common and promising application areas. The focus of this\npaper lies on interpretation of text and image data, while many of the\npresented approaches are applicable to other types of data as well. \n\n"}
{"id": "1811.05063", "contents": "Title: SMERC: Social media event response clustering using textual and temporal\n  information Abstract: Tweet clustering for event detection is a powerful modern method to automate\nthe real-time detection of events. In this work we present a new tweet\nclustering approach, using a probabilistic approach to incorporate temporal\ninformation. By analysing the distribution of time gaps between tweets we show\nthat the gaps between pairs of related tweets exhibit exponential decay,\nwhereas the gaps between unrelated tweets are approximately uniform. Guided by\nthis insight, we use probabilistic arguments to estimate the likelihood that a\npair of tweets are related, and build an improved clustering method. Our method\nSocial Media Event Response Clustering (SMERC) creates clusters of tweets based\non their tendency to be related to a single event. We evaluate our method at\nthree levels: through traditional event prediction from tweet clustering, by\nmeasuring the improvement in quality of clusters created, and also comparing\nthe clustering precision and recall with other methods. By applying SMERC to\ntweets collected during a number of sporting events, we demonstrate that\nincorporating temporal information leads to state of the art clustering\nperformance. \n\n"}
{"id": "1811.05076", "contents": "Title: Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n  and its Statistical Optimality Abstract: We consider the problem of decomposing a higher-order tensor with binary\nentries. Such data problems arise frequently in applications such as\nneuroimaging, recommendation system, topic modeling, and sensor network\nlocalization. We propose a multilinear Bernoulli model, develop a\nrank-constrained likelihood-based estimation method, and obtain the theoretical\naccuracy guarantees. In contrast to continuous-valued problems, the binary\ntensor problem exhibits an interesting phase transition phenomenon according to\nthe signal-to-noise ratio. The error bound for the parameter tensor estimation\nis established, and we show that the obtained rate is minimax optimal under the\nconsidered model. Furthermore, we develop an alternating optimization algorithm\nwith convergence guarantees. The efficacy of our approach is demonstrated\nthrough both simulations and analyses of multiple data sets on the tasks of\ntensor completion and clustering. \n\n"}
{"id": "1811.05648", "contents": "Title: Analysis of Gaussian Spatial Models with Covariate Measurement Error Abstract: Uncertainty is an inherent characteristic of biological and geospatial data\nwhich is almost made by measurement error in the observed values of the\nquantity of interest. Ignoring measurement error can lead to biased estimates\nand inflated variances and so an inappropriate inference. In this paper, the\nGaussian spatial model is fitted based on covariate measurement error. For this\npurpose, we adopt the Bayesian approach and utilize the Markov chain Monte\nCarlo algorithms and data augmentations to carry out calculations. The\nmethodology is illustrated using simulated data. \n\n"}
{"id": "1811.06198", "contents": "Title: Minimax Posterior Convergence Rates and Model Selection Consistency in\n  High-dimensional DAG Models based on Sparse Cholesky Factors Abstract: In this paper, we study the high-dimensional sparse directed acyclic graph\n(DAG) models under the empirical sparse Cholesky prior. Among our results,\nstrong model selection consistency or graph selection consistency is obtained\nunder more general conditions than those in the existing literature. Compared\nto Cao, Khare and Ghosh (2017), the required conditions are weakened in terms\nof the dimensionality, sparsity and lower bound of the nonzero elements in the\nCholesky factor. Furthermore, our result does not require the irrepresentable\ncondition, which is necessary for Lasso type methods. We also derive the\nposterior convergence rates for precision matrices and Cholesky factors with\nrespect to various matrix norms. The obtained posterior convergence rates are\nthe fastest among those of the existing Bayesian approaches. In particular, we\nprove that our posterior convergence rates for Cholesky factors are the minimax\nor at least nearly minimax depending on the relative size of true sparseness\nfor the entire dimension. The simulation study confirms that the proposed\nmethod outperforms the competing methods. \n\n"}
{"id": "1811.06692", "contents": "Title: Subtask Gated Networks for Non-Intrusive Load Monitoring Abstract: Non-intrusive load monitoring (NILM), also known as energy disaggregation, is\na blind source separation problem where a household's aggregate electricity\nconsumption is broken down into electricity usages of individual appliances. In\nthis way, the cost and trouble of installing many measurement devices over\nnumerous household appliances can be avoided, and only one device needs to be\ninstalled. The problem has been well-known since Hart's seminal paper in 1992,\nand recently significant performance improvements have been achieved by\nadopting deep networks. In this work, we focus on the idea that appliances have\non/off states, and develop a deep network for further performance improvements.\nSpecifically, we propose a subtask gated network that combines the main\nregression network with an on/off classification subtask network. Unlike\ntypical multitask learning algorithms where multiple tasks simply share the\nnetwork parameters to take advantage of the relevance among tasks, the subtask\ngated network multiply the main network's regression output with the subtask's\nclassification probability. When standby-power is additionally learned, the\nproposed solution surpasses the state-of-the-art performance for most of the\nbenchmark cases. The subtask gated network can be very effective for any\nproblem that inherently has on/off states. \n\n"}
{"id": "1811.07625", "contents": "Title: Joint reconstruction and prediction of random dynamical systems under\n  borrowing of strength Abstract: We propose a Bayesian nonparametric model based on Markov Chain Monte Carlo\n(MCMC) methods for the joint reconstruction and prediction of discrete time\nstochastic dynamical systems, based on $m$-multiple time-series data, perturbed\nby additive dynamical noise. We introduce the Pairwise Dependent Geometric\nStick-Breaking Reconstruction (PD-GSBR) model, which relies on the construction\nof a $m$-variate nonparametric prior over the space of densities supported over\n$\\mathbb{R}^m$. We are focusing in the case where at least one of the\ntime-series has a sufficiently large sample size representation for an\nindependent and accurate Geometric Stick-Breaking estimation, as defined in\nMerkatas et al. (2017). Our contention, is that whenever the dynamical error\nprocesses perturbing the underlying dynamical systems share common\ncharacteristics, underrepresented data sets can benefit in terms of model\nestimation accuracy. The PD-GSBR estimation and prediction procedure is\ndemonstrated specifically in the case of maps with polynomial nonlinearities of\nan arbitrary degree. Simulations based on synthetic time-series are presented. \n\n"}
{"id": "1811.10223", "contents": "Title: Bayesian Weighted Mendelian Randomization for Causal Inference based on\n  Summary Statistics Abstract: The results from Genome-Wide Association Studies (GWAS) on thousands of\nphenotypes provide an unprecedented opportunity to infer the causal effect of\none phenotype (exposure) on another (outcome). Mendelian randomization (MR), an\ninstrumental variable (IV) method, has been introduced for causal inference\nusing GWAS data. Due to the polygenic architecture of complex traits/diseases\nand the ubiquity of pleiotropy, however, MR has many unique challenges compared\nto conventional IV methods. We propose a Bayesian weighted Mendelian\nrandomization (BWMR) for causal inference to address these challenges. In our\nBWMR model, the uncertainty of weak effects owing to polygenicity has been\ntaken into account and the violation of IV assumption due to pleiotropy has\nbeen addressed through outlier detection by Bayesian weighting. To make the\ncausal inference based on BWMR computationally stable and efficient, we\ndeveloped a variational expectation-maximization (VEM) algorithm. Moreover, we\nhave also derived an exact closed-form formula to correct the posterior\ncovariance which is often underestimated in variational inference. Through\ncomprehensive simulation studies, we evaluated the performance of BWMR,\ndemonstrating the advantage of BWMR over its competitors. Then we applied BWMR\nto make causal inference between 130 metabolites and 93 complex human traits,\nuncovering novel causal relationship between exposure and outcome traits. The\nBWMR software is available at https://github.com/jiazhao97/BWMR. \n\n"}
{"id": "1811.10287", "contents": "Title: A New Standard for the Analysis and Design of Replication Studies Abstract: A new standard is proposed for the evidential assessment of replication\nstudies. The approach combines a specific reverse-Bayes technique with\nprior-predictive tail probabilities to define replication success. The method\ngives rise to a quantitative measure for replication success, called the\nsceptical p-value. The sceptical p-value integrates traditional significance of\nboth the original and replication study with a comparison of the respective\neffect sizes. It incorporates the uncertainty of both the original and\nreplication effect estimates and reduces to the ordinary p-value of the\nreplication study if the uncertainty of the original effect estimate is\nignored. The proposed framework can also be used to determine the power or the\nrequired replication sample size to achieve replication success. Numerical\ncalculations highlight the difficulty to achieve replication success if the\nevidence from the original study is only suggestive. An application to data\nfrom the Open Science Collaboration project on the replicability of\npsychological science illustrates the proposed methodology. \n\n"}
{"id": "1811.11025", "contents": "Title: CVEK: Robust Estimation and Testing for Nonlinear Effects using Kernel\n  Machine Ensemble Abstract: The R package CVEK introduces a suite of flexible machine learning models and\nrobust hypothesis tests for learning the joint nonlinear effects of multiple\ncovariates in limited samples. It implements the Cross-validated Ensemble of\nKernels (CVEK)(Liu and Coull 2017), an ensemble-based kernel machine learning\nmethod that adaptively learns the joint nonlinear effect of multiple covariates\nfrom data, and provides powerful hypothesis tests for both main effects of\nfeatures and interactions among features. The R Package CVEK provides a\nflexible, easy-to-use implementation of CVEK, and offers a wide range of\nchoices for the kernel family (for instance, polynomial, radial basis\nfunctions, Mat\\'ern, neural network, and others), model selection criteria,\nensembling method (averaging, exponential weighting, cross-validated stacking),\nand the type of hypothesis test (asymptotic or parametric bootstrap). Through\nextensive simulations we demonstrate the validity and robustness of this\napproach, and provide practical guidelines on how to design an estimation\nstrategy for optimal performance in different data scenarios. \n\n"}
{"id": "1811.11368", "contents": "Title: First-order Newton-type Estimator for Distributed Estimation and\n  Inference Abstract: This paper studies distributed estimation and inference for a general\nstatistical problem with a convex loss that could be non-differentiable. For\nthe purpose of efficient computation, we restrict ourselves to stochastic\nfirst-order optimization, which enjoys low per-iteration complexity. To\nmotivate the proposed method, we first investigate the theoretical properties\nof a straightforward Divide-and-Conquer Stochastic Gradient Descent (DC-SGD)\napproach. Our theory shows that there is a restriction on the number of\nmachines and this restriction becomes more stringent when the dimension $p$ is\nlarge. To overcome this limitation, this paper proposes a new multi-round\ndistributed estimation procedure that approximates the Newton step only using\nstochastic subgradient. The key component in our method is the proposal of a\ncomputationally efficient estimator of $\\Sigma^{-1} w$, where $\\Sigma$ is the\npopulation Hessian matrix and $w$ is any given vector. Instead of estimating\n$\\Sigma$ (or $\\Sigma^{-1}$) that usually requires the second-order\ndifferentiability of the loss, the proposed First-Order Newton-type Estimator\n(FONE) directly estimates the vector of interest $\\Sigma^{-1} w$ as a whole and\nis applicable to non-differentiable losses. Our estimator also facilitates the\ninference for the empirical risk minimizer. It turns out that the key term in\nthe limiting covariance has the form of $\\Sigma^{-1} w$, which can be estimated\nby FONE. \n\n"}
{"id": "1811.11922", "contents": "Title: Distributed Inference for Linear Support Vector Machine Abstract: The growing size of modern data brings many new challenges to existing\nstatistical inference methodologies and theories, and calls for the development\nof distributed inferential approaches. This paper studies distributed inference\nfor linear support vector machine (SVM) for the binary classification task.\nDespite a vast literature on SVM, much less is known about the inferential\nproperties of SVM, especially in a distributed setting. In this paper, we\npropose a multi-round distributed linear-type (MDL) estimator for conducting\ninference for linear SVM. The proposed estimator is computationally efficient.\nIn particular, it only requires an initial SVM estimator and then successively\nrefines the estimator by solving simple weighted least squares problem.\nTheoretically, we establish the Bahadur representation of the estimator. Based\non the representation, the asymptotic normality is further derived, which shows\nthat the MDL estimator achieves the optimal statistical efficiency, i.e., the\nsame efficiency as the classical linear SVM applying to the entire data set in\na single machine setup. Moreover, our asymptotic result avoids the condition on\nthe number of machines or data batches, which is commonly assumed in\ndistributed estimation literature, and allows the case of diverging dimension.\nWe provide simulation studies to demonstrate the performance of the proposed\nMDL estimator. \n\n"}
{"id": "1812.01412", "contents": "Title: Necessary and Probably Sufficient Test for Finding Valid Instrumental\n  Variables Abstract: Can instrumental variables be found from data? While instrumental variable\n(IV) methods are widely used to identify causal effect, testing their validity\nfrom observed data remains a challenge. This is because validity of an IV\ndepends on two assumptions, exclusion and as-if-random, that are largely\nbelieved to be untestable from data. In this paper, we show that under certain\nconditions, testing for instrumental variables is possible. We build upon prior\nwork on necessary tests to derive a test that characterizes the odds of being a\nvalid instrument, thus yielding the name \"necessary and probably sufficient\".\nThe test works by defining the class of invalid-IV and valid-IV causal models\nas Bayesian generative models and comparing their marginal likelihood based on\nobserved data. When all variables are discrete, we also provide a method to\nefficiently compute these marginal likelihoods.\n  We evaluate the test on an extensive set of simulations for binary data,\ninspired by an open problem for IV testing proposed in past work. We find that\nthe test is most powerful when an instrument follows monotonicity---effect on\ntreatment is either non-decreasing or non-increasing---and has moderate-to-weak\nstrength; incidentally, such instruments are commonly used in observational\nstudies. Among as-if-random and exclusion, it detects exclusion violations with\nhigher power. Applying the test to IVs from two seminal studies on instrumental\nvariables and five recent studies from the American Economic Review shows that\nmany of the instruments may be flawed, at least when all variables are\ndiscretized. The proposed test opens the possibility of data-driven validation\nand search for instrumental variables. \n\n"}
{"id": "1812.01767", "contents": "Title: RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time\n  Series Abstract: Decomposing complex time series into trend, seasonality, and remainder\ncomponents is an important task to facilitate time series anomaly detection and\nforecasting. Although numerous methods have been proposed, there are still many\ntime series characteristics exhibiting in real-world data which are not\naddressed properly, including 1) ability to handle seasonality fluctuation and\nshift, and abrupt change in trend and reminder; 2) robustness on data with\nanomalies; 3) applicability on time series with long seasonality period. In the\npaper, we propose a novel and generic time series decomposition algorithm to\naddress these challenges. Specifically, we extract the trend component robustly\nby solving a regression problem using the least absolute deviations loss with\nsparse regularization. Based on the extracted trend, we apply the the non-local\nseasonal filtering to extract the seasonality component. This process is\nrepeated until accurate decomposition is obtained. Experiments on different\nsynthetic and real-world time series datasets demonstrate that our method\noutperforms existing solutions. \n\n"}
{"id": "1812.03775", "contents": "Title: Sufficient Dimension Reduction for Classification Abstract: We propose a new sufficient dimension reduction approach designed\ndeliberately for high-dimensional classification. This novel method is named\nmaximal mean variance (MMV), inspired by the mean variance index first proposed\nby Cui, Li and Zhong (2015), which measures the dependence between a\ncategorical random variable with multiple classes and a continuous random\nvariable. Our method requires reasonably mild restrictions on the predicting\nvariables and keeps the model-free advantage without the need to estimate the\nlink function. The consistency of the MMV estimator is established under\nregularity conditions for both fixed and diverging dimension (p) cases and the\nnumber of the response classes can also be allowed to diverge with the sample\nsize n. We also construct the asymptotic normality for the estimator when the\ndimension of the predicting vector is fixed. Furthermore, our method works\npretty well when n < p. The surprising classification efficiency gain of the\nproposed method is demonstrated by simulation studies and real data analysis. \n\n"}
{"id": "1812.04103", "contents": "Title: Non-local U-Net for Biomedical Image Segmentation Abstract: Deep learning has shown its great promise in various biomedical image\nsegmentation tasks. Existing models are typically based on U-Net and rely on an\nencoder-decoder architecture with stacked local operators to aggregate\nlong-range information gradually. However, only using the local operators\nlimits the efficiency and effectiveness. In this work, we propose the non-local\nU-Nets, which are equipped with flexible global aggregation blocks, for\nbiomedical image segmentation. These blocks can be inserted into U-Net as\nsize-preserving processes, as well as down-sampling and up-sampling layers. We\nperform thorough experiments on the 3D multimodality isointense infant brain MR\nimage segmentation task to evaluate the non-local U-Nets. Results show that our\nproposed models achieve top performances with fewer parameters and faster\ncomputation. \n\n"}
{"id": "1812.04187", "contents": "Title: Dynamic Sparse Factor Analysis Abstract: Its conceptual appeal and effectiveness has made latent factor modeling an\nindispensable tool for multivariate analysis. Despite its popularity across\nmany fields, there are outstanding methodological challenges that have hampered\npractical deployments. One major challenge is the selection of the number of\nfactors, which is exacerbated for dynamic factor models, where factors can\ndisappear, emerge, and/or reoccur over time. Existing tools that assume a fixed\nnumber of factors may provide a misguided representation of the data mechanism,\nespecially when the number of factors is crudely misspecified. Another\nchallenge is the interpretability of the factor structure, which is often\nregarded as an unattainable objective due to the lack of identifiability.\nMotivated by a topical macroeconomic application, we develop a flexible\nBayesian method for dynamic factor analysis (DFA) that can simultaneously\naccommodate a time-varying number of factors and enhance interpretability\nwithout strict identifiability constraints. To this end, we turn to dynamic\nsparsity by employing Dynamic Spike-and-Slab (DSS) priors within DFA. Scalable\nBayesian EM estimation is proposed for fast posterior mode identification via\nrotations to sparsity, enabling Bayesian data analysis at scales that would\nhave been previously time-consuming. We study a large-scale balanced panel of\nmacroeconomic variables covering multiple facets of the US economy, with a\nfocus on the Great Recession, to highlight the efficacy and usefulness of our\nproposed method. \n\n"}
{"id": "1812.04990", "contents": "Title: Causal inference, social networks, and chain graphs Abstract: Traditionally, statistical and causal inference on human subjects rely on the\nassumption that individuals are independently affected by treatments or\nexposures. However, recently there has been increasing interest in settings,\nsuch as social networks, where individuals may interact with one another such\nthat treatments may spill over from the treated individual to their social\ncontacts and outcomes may be contagious. Existing models proposed for causal\ninference using observational data from networks of interacting individuals\nhave two major shortcomings. First, they often require a level of granularity\nin the data that is practically infeasible to collect in most settings, and\nsecond, the models are high-dimensional and often too big to fit to the\navailable data. In this paper we illustrate and justify a parsimonious\nparameterization for network data with interference and contagion. Our\nparameterization corresponds to a particular family of graphical models known\nas chain graphs. We argue that, in some settings, chain graph models\napproximate the marginal distribution of a snapshot of a longitudinal data\ngenerating process on interacting units. We illustrate the use of chain graphs\nfor causal inference about collective decision making in social networks using\ndata from U.S. Supreme Court decisions between 1994 and 2004 and in\nsimulations. \n\n"}
{"id": "1812.05529", "contents": "Title: High dimensional inference for the structural health monitoring of lock\n  gates Abstract: Locks and dams are critical pieces of inland waterways. However, many\ncomponents of existing locks have been in operation past their designed\nlifetime. To ensure safe and cost effective operations, it is therefore\nimportant to monitor the structural health of locks. To support lock gate\nmonitoring, this work considers a high dimensional Bayesian inference problem\nthat combines noisy real time strain observations with a detailed finite\nelement model. To solve this problem, we develop a new technique that combines\nKarhunen-Lo\\`eve decompositions, stochastic differential equation\nrepresentations of Gaussian processes, and Kalman smoothing that scales\nlinearly with the number of observations and could be used for near real-time\nmonitoring. We use quasi-periodic Gaussian processes to model thermal\ninfluences on the strain and infer spatially distributed boundary conditions in\nthe model, which are also characterized with Gaussian process prior\ndistributions. The power of this approach is demonstrated on a small synthetic\nexample and then with real observations of Mississippi River Lock 27, which is\nlocated near St. Louis, MO USA. The results show that our approach is able to\nprobabilistically characterize the posterior distribution over nearly 1.4\nmillion parameters in under an hour on a standard desktop computer. \n\n"}
{"id": "1812.06575", "contents": "Title: Matching on Generalized Propensity Scores with Continuous Exposures Abstract: In the context of a binary treatment, matching is a well-established approach\nin causal inference. However, in the context of a continuous treatment or\nexposure, matching is still underdeveloped. We propose an innovative matching\napproach to estimate an average causal exposure-response function under the\nsetting of continuous exposures that relies on the generalized propensity score\n(GPS). Our approach maintains the following attractive features of matching: a)\nclear separation between the design and the analysis; b) robustness to model\nmisspecification or to the presence of extreme values of the estimated GPS; c)\nstraightforward assessment of covariate balance. We first introduce an\nassumption of identifiability, called local weak unconfoundedness. Under this\nassumption and mild smoothness conditions, we provide theoretical guarantees\nthat our proposed matching estimator attains point-wise consistency and\nasymptotic normality. In simulations, our proposed matching approach\noutperforms existing methods under settings of model misspecification or the\npresence of extreme values of the estimated GPS. We apply our proposed method\nto estimate the average causal exposure-response function between long-term\nPM$_{2.5}$ exposure and all-cause mortality among 68.5 million Medicare\nenrollees, 2000-2016. We found strong evidence of a harmful effect of long-term\nPM$_{2.5}$ exposure on mortality. Code for the proposed matching approach is\nprovided in the CausalGPS R package, which is available on CRAN and provides a\ncomputationally efficient implementation. \n\n"}
{"id": "1812.07722", "contents": "Title: Active Learning and CSI Acquisition for mmWave Initial Alignment Abstract: Millimeter wave (mmWave) communication with large antenna arrays is a\npromising technique to enable extremely high data rates due to the large\navailable bandwidth in mmWave frequency bands. In addition, given the knowledge\nof an optimal directional beamforming vector, large antenna arrays have been\nshown to overcome both the severe signal attenuation in mmWave as well as the\ninterference problem. However, fundamental limits on achievable learning rate\nof an optimal beamforming vector remain.\n  This paper considers the problem of adaptive and sequential optimization of\nthe beamforming vectors during the initial access phase of communication. With\na single-path channel model, the problem is reduced to actively learning the\nAngle-of-Arrival (AoA) of the signal sent from the user to the Base Station\n(BS). Drawing on the recent results in the design of a hierarchical beamforming\ncodebook [1], sequential measurement dependent noisy search strategies [2], and\nactive learning from an imperfect labeler [3], an adaptive and sequential\nalignment algorithm is proposed.\n  An upper bound on the expected search time of the proposed algorithm is\nderived via Extrinsic Jensen-Shannon Divergence. which demonstrates that the\nsearch time of the proposed algorithm asymptotically matches the performance of\nthe noiseless bisection search up to a constant factor. Furthermore, the upper\nbound shows that the acquired AoA error probability decays exponentially fast\nwith the search time with an exponent that is a decreasing function of the\nacquisition rate.\n  Numerically, the proposed algorithm is compared with prior work where a\nsignificant improvement of the system communication rate is observed. Most\nnotably, in the relevant regime of low (-10dB to 5dB) raw SNR, this establishes\nthe first practically viable solution for initial access and, hence, the first\ndemonstration of stand-alone mmWave communication \n\n"}
{"id": "1812.07935", "contents": "Title: The negative binomial beta prime regression model with cure rate Abstract: This paper introduces a cure rate survival model by assuming that the time to\nthe event of interest follows a beta prime distribution and that the number of\ncompeting causes of the event of interest follows a negative binomial\ndistribution. This model provides a novel alternative to the existing cure rate\nregression models due to its flexibility, as the beta prime model can exhibit\ngreater levels of skewness and kurtosis than those of the gamma and inverse\nGaussian distributions. Moreover, the hazard rate of this model can have an\nupside-down bathtub or an increasing shape. We approach both parameter\nestimation and local influence based on likelihood methods. In special, three\nperturbation schemes are considered for local influence. Numerical evaluation\nof the proposed model is performed by Monte Carlo simulations. In order to\nillustrate the potential for practice of our model we apply it to a real data\nset. \n\n"}
{"id": "1812.08683", "contents": "Title: Robust Estimation of Causal Effects via High-Dimensional Covariate\n  Balancing Propensity Score Abstract: In this paper, we propose a robust method to estimate the average treatment\neffects in observational studies when the number of potential confounders is\npossibly much greater than the sample size. We first use a class of penalized\nM-estimators for the propensity score and outcome models. We then calibrate the\ninitial estimate of the propensity score by balancing a carefully selected\nsubset of covariates that are predictive of the outcome. Finally, the estimated\npropensity score is used to construct the inverse probability weighting\nestimator. We prove that the proposed estimator, which has the sample\nboundedness property, is root-n consistent, asymptotically normal, and\nsemiparametrically efficient when the propensity score model is correctly\nspecified and the outcome model is linear in covariates. More importantly, we\nshow that our estimator remains root-n consistent and asymptotically normal so\nlong as either the propensity score model or the outcome model is correctly\nspecified. We provide valid confidence intervals in both cases and further\nextend these results to the case where the outcome model is a generalized\nlinear model. In simulation studies, we find that the proposed methodology\noften estimates the average treatment effect more accurately than the existing\nmethods. We also present an empirical application, in which we estimate the\naverage causal effect of college attendance on adulthood political\nparticipation. Open-source software is available for implementing the proposed\nmethodology. \n\n"}
{"id": "1812.08927", "contents": "Title: Global and Local Two-Sample Tests via Regression Abstract: Two-sample testing is a fundamental problem in statistics. Despite its long\nhistory, there has been renewed interest in this problem with the advent of\nhigh-dimensional and complex data. Specifically, in the machine learning\nliterature, there have been recent methodological developments such as\nclassification accuracy tests. The goal of this work is to present a regression\napproach to comparing multivariate distributions of complex data. Depending on\nthe chosen regression model, our framework can efficiently handle different\ntypes of variables and various structures in the data, with competitive power\nunder many practical scenarios. Whereas previous work has been largely limited\nto global tests which conceal much of the local information, our approach\nnaturally leads to a local two-sample testing framework in which we identify\nlocal differences between multivariate distributions with statistical\nconfidence. We demonstrate the efficacy of our approach both theoretically and\nempirically, under some well-known parametric and nonparametric regression\nmethods. Our proposed methods are applied to simulated data as well as a\nchallenging astronomy data set to assess their practical usefulness. \n\n"}
{"id": "1812.09181", "contents": "Title: e4clim 1.0 : The Energy for CLimate Integrated Model: Description and\n  Application to Italy Abstract: We develop an open-source Python software integrating flexibility needs from\nVariable Renewable Energies (VREs) in the development of regional energy mixes.\nIt provides a flexible and extensible tool to researchers/engineers, and for\neducation/outreach. It aims at evaluating and optimizing energy deployment\nstrategies with high shares of VRE; assessing the impact of new technologies\nand of climate variability; conducting sensitivity studies. Specifically, to\nlimit the algorithm's complexity, we avoid solving a full-mix cost-minimization\nproblem by taking the mean and variance of the renewable production-demand\nratio as proxies to balance services. Second, observations of VRE technologies\nbeing typically too short or nonexistent, the hourly demand and production are\nestimated from climate time-series and fitted to available observations. We\nillustrate e4clim's potential with an optimal recommissioning-study of the 2015\nItalian PV-wind mix testing different climate-data sources and strategies and\nassessing the impact of climate variability and the robustness of the results. \n\n"}
{"id": "1812.11026", "contents": "Title: Hybrid Wasserstein Distance and Fast Distribution Clustering Abstract: We define a modified Wasserstein distance for distribution clustering which\ninherits many of the properties of the Wasserstein distance but which can be\nestimated easily and computed quickly. The modified distance is the sum of two\nterms. The first term --- which has a closed form --- measures the\nlocation-scale differences between the distributions. The second term is an\napproximation that measures the remaining distance after accounting for\nlocation-scale differences. We consider several forms of approximation with our\nmain emphasis being a tangent space approximation that can be estimated using\nnonparametric regression. We evaluate the strengths and weaknesses of this\napproach on simulated and real examples. \n\n"}
{"id": "1812.11689", "contents": "Title: K-nearest Neighbor Search by Random Projection Forests Abstract: K-nearest neighbor (kNN) search has wide applications in many areas,\nincluding data mining, machine learning, statistics and many applied domains.\nInspired by the success of ensemble methods and the flexibility of tree-based\nmethodology, we propose random projection forests (rpForests), for kNN search.\nrpForests finds kNNs by aggregating results from an ensemble of random\nprojection trees with each constructed recursively through a series of\ncarefully chosen random projections. rpForests achieves a remarkable accuracy\nin terms of fast decay in the missing rate of kNNs and that of discrepancy in\nthe kNN distances. rpForests has a very low computational complexity. The\nensemble nature of rpForests makes it easily run in parallel on multicore or\nclustered computers; the running time is expected to be nearly inversely\nproportional to the number of cores or machines. We give theoretical insights\nby showing the exponential decay of the probability that neighboring points\nwould be separated by ensemble random projection trees when the ensemble size\nincreases. Our theory can be used to refine the choice of random projections in\nthe growth of trees, and experiments show that the effect is remarkable. \n\n"}
{"id": "1901.00834", "contents": "Title: The market nanostructure origin of asset price time reversal asymmetry Abstract: We introduce a framework to infer lead-lag networks between the states of\nelements of complex systems, determined at different timescales. As such\nnetworks encode the causal structure of a system, infering lead-lag networks\nfor many pairs of timescales provides a global picture of the mutual influence\nbetween timescales. We apply our method to two trader-resolved FX data sets and\ndocument strong and complex asymmetric influence of timescales on the structure\nof lead-lag networks. Expectedly, this asymmetry extends to trader activity:\nfor institutional clients in our dataset, past activity on timescales longer\nthan 3 hours is more correlated with future activity at shorter timescales than\nthe opposite (Zumbach effect), while a reverse Zumbach effect is found for past\ntimescales shorter than 3 hours; retail clients have a totally different, and\nmuch more intricate, structure of asymmetric timescale influence. The causality\nstructures are clearly caused by markedly different behaviors of the two types\nof traders. Hence, market nanostructure, i.e., market dynamics at the\nindividual trader level, provides an unprecedented insight into the causality\nstructure of financial markets, which is much more complex than previously\nthought. \n\n"}
{"id": "1901.03090", "contents": "Title: Endemic-epidemic models with discrete-time serial interval distributions\n  for infectious disease prediction Abstract: Multivariate count time series models are an important tool for the analysis\nand prediction of infectious disease spread. We consider the endemic-epidemic\nframework, an autoregressive model class for infectious disease surveillance\ncounts, and replace the default autoregression on counts from the previous time\nperiod with more flexible weighting schemes inspired by discrete-time serial\ninterval distributions. We employ three different parametric formulations, each\nwith an additional unknown weighting parameter estimated via a profile\nlikelihood approach, and compare them to an unrestricted nonparametric\napproach. The new methods are illustrated in a univariate analysis of dengue\nfever incidence in San Juan, Puerto Rico, and a spatio-temporal study of viral\ngastroenteritis in the twelve districts of Berlin. We assess the predictive\nperformance of the suggested models and several reference models at various\nforecast horizons. In both applications, the performance of the\nendemic-epidemic models is considerably improved by the proposed weighting\nschemes. \n\n"}
{"id": "1901.03905", "contents": "Title: Are Clusterings of Multiple Data Views Independent? Abstract: In the Pioneer 100 (P100) Wellness Project (Price and others, 2017), multiple\ntypes of data are collected on a single set of healthy participants at multiple\ntimepoints in order to characterize and optimize wellness. One way to do this\nis to identify clusters, or subgroups, among the participants, and then to\ntailor personalized health recommendations to each subgroup. It is tempting to\ncluster the participants using all of the data types and timepoints, in order\nto fully exploit the available information. However, clustering the\nparticipants based on multiple data views implicitly assumes that a single\nunderlying clustering of the participants is shared across all data views. If\nthis assumption does not hold, then clustering the participants using multiple\ndata views may lead to spurious results. In this paper, we seek to evaluate the\nassumption that there is some underlying relationship among the clusterings\nfrom the different data views, by asking the question: are the clusters within\neach data view dependent or independent? We develop a new test for answering\nthis question, which we then apply to clinical, proteomic, and metabolomic\ndata, across two distinct timepoints, from the P100 study. We find that while\nthe subgroups of the participants defined with respect to any single data type\nseem to be dependent across time, the clustering among the participants based\non one data type (e.g. proteomic data) appears not to be associated with the\nclustering based on another data type (e.g. clinical data). \n\n"}
{"id": "1901.04884", "contents": "Title: Optimistic optimization of a Brownian Abstract: We address the problem of optimizing a Brownian motion. We consider a\n(random) realization $W$ of a Brownian motion with input space in $[0,1]$.\nGiven $W$, our goal is to return an $\\epsilon$-approximation of its maximum\nusing the smallest possible number of function evaluations, the sample\ncomplexity of the algorithm. We provide an algorithm with sample complexity of\norder $\\log^2(1/\\epsilon)$. This improves over previous results of Al-Mharmah\nand Calvin (1996) and Calvin et al. (2017) which provided only polynomial\nrates. Our algorithm is adaptive---each query depends on previous values---and\nis an instance of the optimism-in-the-face-of-uncertainty principle. \n\n"}
{"id": "1901.05885", "contents": "Title: A Learning Framework for An Accurate Prediction of Rainfall Rates Abstract: The present work is aimed to examine the potential of advanced machine\nlearning strategies to predict the monthly rainfall (precipitation) for the\nIndus Basin, using climatological variables such as air temperature,\ngeo-potential height, relative humidity and elevation. In this work, the focus\nis on thirteen geographical locations, called index points, within the basin.\nArguably, not all of the hydrological components are relevant to the\nprecipitation rate, and therefore, need to be filtered out, leading to a\nlower-dimensional feature space. Towards this goal, we adopted the gradient\nboosting method to extract the most contributive features for precipitation\nrate prediction. Five state-of-the-art machine learning methods have then been\ntrained where pearson correlation coefficient and mean absolute error have been\nreported as the prediction performance criteria. The Random Forest regression\nmodel outperformed the other regression models achieving the maximum pearson\ncorrelation coefficient and minimum mean absolute error for most of the index\npoints. Our results suggest the relative humidity (for pressure levels of 300\nmb and 150 mb, respectively), the u-direction wind (for pressure level of 700\nmb), air temperature (for pressure levels of 150 mb and 10 mb, respectively) as\nthe top five influencing features for accurate forecasting the precipitation\nrate. \n\n"}
{"id": "1901.07396", "contents": "Title: Bayesian Prediction of Nitrate Concentration Using a Gaussian\n  Log-Gaussian Spatial Model with Measurement Error in Explanatory Variables Abstract: This article has been removed by arXiv administrators due to falsified\nauthorship. \n\n"}
{"id": "1901.08117", "contents": "Title: Spatial Modeling of Trends in Crime over Time in Philadelphia Abstract: Understanding the relationship between change in crime over time and the\ngeography of urban areas is an important problem for urban planning. Accurate\nestimation of changing crime rates throughout a city would aid law enforcement\nas well as enable studies of the association between crime and the built\nenvironment. Bayesian modeling is a promising direction since areal data\nrequire principled sharing of information to address spatial autocorrelation\nbetween proximal neighborhoods. We develop several Bayesian approaches to\nspatial sharing of information between neighborhoods while modeling trends in\ncrime counts over time. We apply our methodology to estimate changes in crime\nthroughout Philadelphia over the 2006-15 period, while also incorporating\nspatially-varying economic and demographic predictors. We find that the local\nshrinkage imposed by a conditional autoregressive model has substantial\nbenefits in terms of out-of-sample predictive accuracy of crime. We also\nexplore the possibility of spatial discontinuities between neighborhoods that\ncould represent natural barriers or aspects of the built environment. \n\n"}
{"id": "1901.08196", "contents": "Title: Asynchronous Multi-Sensor Change-Point Detection for Seismic Tremors Abstract: We consider the sequential change-point detection for asynchronous\nmulti-sensors, where each sensor observe a signal (due to change-point) at\ndifferent times. We propose an asynchronous Subspace-CUSUM procedure based on\njointly estimating the unknown signal waveform and the unknown relative delays\nbetween the sensors. Using the estimated delays, we can align signals and use\nthe subspace to combine the multiple sensor observations. We derive the optimal\ndrift parameter for the proposed procedure, and characterize the relationship\nbetween the expected detection delay, average run length (of false alarms), and\nthe energy of the time-varying signal. We demonstrate the good performance of\nthe proposed procedure using simulation and real data. We also demonstrate that\nthe proposed procedure outperforms the well-known `one-shot procedure' in\ndetecting weak and asynchronous signals. \n\n"}
{"id": "1901.08941", "contents": "Title: Computational landscape of user behavior on social media Abstract: With the increasing abundance of 'digital footprints' left by human\ninteractions in online environments, e.g., social media and app use, the\nability to model complex human behavior has become increasingly possible. Many\napproaches have been proposed, however, most previous model frameworks are\nfairly restrictive. We introduce a new social modeling approach that enables\nthe creation of models directly from data with minimal a priori restrictions on\nthe model class. In particular, we infer the minimally complex, maximally\npredictive representation of an individual's behavior when viewed in isolation\nand as driven by a social input. We then apply this framework to a\nheterogeneous catalog of human behavior collected from fifteen thousand users\non the microblogging platform Twitter. The models allow us to describe how a\nuser processes their past behavior and their social inputs. Despite the\ndiversity of observed user behavior, most models inferred fall into a small\nsubclass of all possible finite-state processes. Thus, our work demonstrates\nthat user behavior, while quite complex, belies simple underlying computational\nstructures. \n\n"}
{"id": "1901.09472", "contents": "Title: Separable Effects for Causal Inference in the Presence of Competing\n  Events Abstract: In time-to-event settings, the presence of competing events complicates the\ndefinition of causal effects. Here we propose the new separable effects to\nstudy the causal effect of a treatment on an event of interest. The separable\ndirect effect is the treatment effect on the event of interest not mediated by\nits effect on the competing event. The separable indirect effect is the\ntreatment effect on the event of interest only through its effect on the\ncompeting event. Similar to Robins and Richardson's extended graphical approach\nfor mediation analysis, the separable effects can only be identified under the\nassumption that the treatment can be decomposed into two distinct components\nthat exert their effects through distinct causal pathways. Unlike existing\ndefinitions of causal effects in the presence of competing events, our\nestimands do not require cross-world contrasts or hypothetical interventions to\nprevent death. As an illustration, we apply our approach to a randomized\nclinical trial on estrogen therapy in individuals with prostate cancer. \n\n"}
{"id": "1901.09475", "contents": "Title: Causal Discovery with a Mixture of DAGs Abstract: Causal processes in biomedicine may contain cycles, evolve over time or\ndiffer between populations. However, many graphical models cannot accommodate\nthese conditions. We propose to model causation using a mixture of directed\ncyclic graphs (DAGs), where the joint distribution in a population follows a\nDAG at any single point in time but potentially different DAGs across time. We\nalso introduce an algorithm called Causal Inference over Mixtures that uses\nlongitudinal data to infer a graph summarizing the causal relations generated\nfrom a mixture of DAGs. Experiments demonstrate improved performance compared\nto prior approaches. \n\n"}
{"id": "1901.09729", "contents": "Title: Estimation and simulation of the transaction arrival process in intraday\n  electricity markets Abstract: We examine the novel problem of the estimation of transaction arrival\nprocesses in the intraday electricity markets. We model the inter-arrivals\nusing multiple time-varying parametric densities based on the generalized F\ndistribution estimated by maximum likelihood. We analyse both the in-sample\ncharacteristics and the probabilistic forecasting performance. In a rolling\nwindow forecasting study, we simulate many trajectories to evaluate the\nforecasts and gain significant insights into the model fit. The prediction\naccuracy is evaluated by a functional version of the MAE (mean absolute error),\nRMSE (root mean squared error) and CRPS (continuous ranked probability score)\nfor the simulated count processes. This paper fills the gap in the literature\nregarding the intensity estimation of transaction arrivals and is a major\ncontribution to the topic, yet leaves much of the field for further\ndevelopment. The study presented in this paper is conducted based on the German\nIntraday Continuous electricity market data, but this method can be easily\napplied to any other continuous intraday electricity market. For the German\nmarket, a specific generalized gamma distribution setup explains the overall\nbehaviour significantly best, especially as the tail behaviour of the process\nis well covered. \n\n"}
{"id": "1901.10852", "contents": "Title: Detecting multiple generalized change-points by isolating single ones Abstract: We introduce a new approach, called Isolate-Detect (ID), for the consistent\nestimation of the number and location of multiple generalized change-points in\nnoisy data sequences. Examples of signal changes that ID can deal with are\nchanges in the mean of a piecewise-constant signal and changes, continuous or\nnot, in the linear trend. The number of change-points can increase with the\nsample size. Our method is based on an isolation technique, which prevents the\nconsideration of intervals that contain more than one change-point. This\nisolation enhances ID's accuracy as it allows for detection in the presence of\nfrequent changes of possibly small magnitudes. In ID, model selection is\ncarried out via thresholding, or an information criterion, or SDLL, or a hybrid\ninvolving the former two. The hybrid model selection leads to a general method\nwith very good practical performance and minimal parameter choice. In the\nscenarios tested, ID is at least as accurate as the state-of-the-art methods;\nmost of the times it outperforms them. ID is implemented in the R packages\nIDetect and breakfast, available from CRAN. \n\n"}
{"id": "1901.11279", "contents": "Title: Random forests for high-dimensional longitudinal data Abstract: Random forests is a state-of-the-art supervised machine learning method which\nbehaves well in high-dimensional settings although some limitations may happen\nwhen $p$, the number of predictors, is much larger than the number of\nobservations $n$. Repeated measurements can help by offering additional\ninformation but no approach has been proposed for high-dimensional longitudinal\ndata. Random forests have been adapted to standard (i.e., $n > p$) longitudinal\ndata by using a semi-parametric mixed-effects model, in which the\nnon-parametric part is estimated using random forests. We first propose a\nstochastic extension of the model which allows the covariance structure to vary\nover time. Furthermore, we develop a new method which takes intra-individual\ncovariance into consideration to build the forest. Simulations reveal the\nsuperiority of our approach compared to existing ones. The method has been\napplied to an HIV vaccine trial including 17 HIV infected patients with 10\nrepeated measurements of 20000 gene transcripts and the blood concentration of\nhuman immunodeficiency virus RNA at the time of antiretroviral interruption.\nThe approach selected 21 gene transcripts for which the association with HIV\nviral load was fully relevant and consistent with results observed during\nprimary infection. \n\n"}
{"id": "math/0612811", "contents": "Title: Adaptive Allocation Theory in Clinical Trials Abstract: Various adaptive randomization procedures (adaptive designs) have been\nproposed to clinical trials. This paper discusses several broad families of\nprocedures, such as the play-the-winner rule and Markov chain model, randomized\nplay-the-winner rule and urn models, drop-the-loser rule, doubly biased coin\nadaptive design. Asymptotic theories are presented with several pivotal proofs.\nThe effect of delayed responses, the power and variability comparison of these\ndesigns are also discussed. \n\n"}
{"id": "physics/0702148", "contents": "Title: Reliability of rank order in sampled networks Abstract: In complex scale-free networks, ranking the individual nodes based upon their\nimportance has useful applications, such as the identification of hubs for\nepidemic control, or bottlenecks for controlling traffic congestion. However,\nin most real situations, only limited sub-structures of entire networks are\navailable, and therefore the reliability of the order relationships in sampled\nnetworks requires investigation. With a set of randomly sampled nodes from the\nunderlying original networks, we rank individual nodes by three centrality\nmeasures: degree, betweenness, and closeness. The higher-ranking nodes from the\nsampled networks provide a relatively better characterisation of their ranks in\nthe original networks than the lower-ranking nodes. A closeness-based order\nrelationship is more reliable than any other quantity, due to the global nature\nof the closeness measure. In addition, we show that if access to hubs is\nlimited during the sampling process, an increase in the sampling fraction can\nin fact decrease the sampling accuracy. Finally, an estimation method for\nassessing sampling accuracy is suggested. \n\n"}
{"id": "q-bio/0702049", "contents": "Title: The Cyclohedron Test for Finding Periodic Genes in Time Course\n  Expression Studies Abstract: The problem of finding periodically expressed genes from time course\nmicroarray experiments is at the center of numerous efforts to identify the\nmolecular components of biological clocks. We present a new approach to this\nproblem based on the cyclohedron test, which is a rank test inspired by recent\nadvances in algebraic combinatorics. The test has the advantage of being robust\nto measurement errors, and can be used to ascertain the significance of\ntop-ranked genes. We apply the test to recently published measurements of gene\nexpression during mouse somitogenesis and find 32 genes that collectively are\nsignificant. Among these are previously identified periodic genes involved in\nthe Notch/FGF and Wnt signaling pathways, as well as novel candidate genes that\nmay play a role in regulating the segmentation clock. These results confirm\nthat there are an abundance of exceptionally periodic genes expressed during\nsomitogenesis. The emphasis of this paper is on the statistics and\ncombinatorics that underlie the cyclohedron test and its implementation within\na multiple testing framework. \n\n"}
